Journal of Artificial Intelligence Research 50 (2014) 535-572

Submitted 12/13; published 07/14

Integrating Queueing Theory and Scheduling
for Dynamic Scheduling Problems
Daria Terekhov
Tony T. Tran

dterekho@mie.utoronto.ca
tran@mie.utoronto.ca

Department of Mechanical and Industrial Engineering
University of Toronto, Toronto, Ontario, Canada

Douglas G. Down

downd@mcmaster.ca

Department of Computing and Software
McMaster University, Hamilton, Ontario, Canada

J. Christopher Beck

jcb@mie.utoronto.ca

Department of Mechanical and Industrial Engineering
University of Toronto, Toronto, Ontario, Canada

Abstract
Dynamic scheduling problems consist of both challenging combinatorics, as found in
classical scheduling problems, and stochastics due to uncertainty about the arrival times,
resource requirements, and processing times of jobs. To address these two challenges, we
investigate the integration of queueing theory and scheduling. The former reasons about
long-run stochastic system characteristics, whereas the latter typically deals with shortterm combinatorics. We investigate two simple problems to isolate the core differences
and potential synergies between the two approaches: a two-machine dynamic flowshop
and a flexible queueing network. We show for the first time that stability, a fundamental
characteristic in queueing theory, can be applied to approaches that periodically solve combinatorial scheduling problems. We empirically demonstrate that for a dynamic flowshop,
the use of combinatorial reasoning has little impact on schedule quality beyond queueing approaches. In contrast, for the more complicated flexible queueing network, a novel
algorithm that combines long-term guidance from queueing theory with short-term combinatorial decision making outperforms all other tested approaches. To our knowledge, this
is the first time that such a hybrid of queueing theory and scheduling techniques has been
proposed and evaluated.

1. Introduction
To behave intelligently over an extended period of time, a situated agent must be able
to deal with dynamic changes in its tasks and goals. New tasks (e.g., targets for surveillance, see Burns, Benton, Ruml, Yoon, & Do, 2012; deliveries, see Bent & Van Hentenryck,
2007; requests for a drink, see Petrick & Foster, 2013) arrive continuously and must be
incorporated into the ongoing problem-solving process.
Similarly, in a real-world scheduling problem, the set of jobs changes as customer orders
arrive, are processed, and leave the system. Different jobs have different requirements for
processing on different resources, and these characteristics often do not become known until
the arrival of the job. Depending on the scheduling horizon, a scheduler may have a number
of possibly conflicting objectives. Examples of short-term objectives include minimizing
c
2014
AI Access Foundation. All rights reserved.

fiTerekhov, Tran, Down, & Beck

tardiness or makespan, while over the long term a scheduler may want to guarantee that
the facility can handle the expected pattern of demand without catastrophic failures (e.g.,
without the number of jobs waiting for processing becoming extremely large).
Our thesis is that long-term stochastic reasoning as studied in queueing theory can be
usefully combined, both theoretically and algorithmically, with shorter-term combinatorial reasoning that has been traditionally studied in scheduling within artificial intelligence
(AI) and operations research. Such a combination is challenging, as queueing theory and
scheduling have developed independently for many years and, as a consequence, have different performance measures of interest, standard problem settings, and assumptions. In
this paper, we take steps toward the integration of queueing and scheduling by studying
two dynamic scheduling problems and making the following contributions:

 We show that the fundamental queueing theory notion of stability can be used to
analyze periodic scheduling algorithms, the standard approach to dynamic scheduling
from the scheduling literature. We show, for each of the problems studied, that
periodic scheduling algorithms can be proved to be maximally stable: no queueing
policy or scheduling approach will allow the system to operate at a higher load or
achieve a higher throughput.
 We show, in the context of one dynamic scheduling problem, that the long-term,
stochastic reasoning of queueing theory can be combined with short-term combinatorial reasoning to produce a hybrid scheduling algorithm that achieves better performance than either queueing or scheduling approaches alone.

This paper is organized as follows. Section 2 provides the necessary background on
dynamic scheduling algorithms, queueing theory, and stability. Next, we discuss our general
problem settings and assumptions about job arrivals and the time at which various problem
characteristics become known. Section 3 addresses our first, simpler problem, scheduling
of a dynamic two-machine flowshop, presenting both our theoretical results on stability
and our empirical results comparing algorithm performance. Section 4 turns to the second,
more complex environment, a flexible queueing network. For the problem of scheduling
in this setting, in addition to the theoretical and numerical examination of queueing and
scheduling approaches, we propose and analyze a queueing/scheduling hybrid. In Section
5, we discuss the broader implications of our results for scheduling and AI problem solving.
We conclude in Section 6.

2. Preliminaries
This section introduces the background that forms the context for our study. We review the
general dynamic scheduling problem and two solution approaches: combinatorial scheduling
and queueing theory. We then provide a detailed discussion of stability, the fundamental
analytical concept we use in our theoretical contributions. Finally, we discuss our problem
settings and assumptions.
536

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

2.1 Dynamic Scheduling Problems
The dynamic scheduling problems that we are interested in are characterized by a stream
of jobs arriving stochastically over time. Each job requires a combination of resources,
sequentially and/or in parallel, for different processing times. The existence of any particular
job and its corresponding characteristics is not known until its arrival. However, we make
the assumption, as in queueing theory but unlike in typical scheduling settings, that we
have stochastic information of the distribution of job arrivals and characteristics. Jobs may
require a complex routing through the available resources, which may be heterogeneous but
with known and deterministic capacities.
Such a scheduling problem may have both short- and long-term objectives. In typical
scheduling contexts, the goal is to construct a schedule that achieves an optimal level of
performance for a given optimization criterion (e.g., mean flow time) for the jobs that
actually arrive. Often we settle for evaluating algorithm performance over a finite time
horizon; we refer to such criteria as short-term criteria. In contrast, long-term objectives
focus on system-level performance measures such as stability, which establishes whether
the instantaneous number of jobs will remain finite over an infinite horizon for particular
system parameters.
To solve a dynamic scheduling problem, the jobs must be assigned to appropriate resources and start times, respecting the resource and temporal constraints. As jobs arrive,
there must be an online process to make decisions: it is not possible to solve the entire
problem offline.
Solving dynamic scheduling problems is challenging both due to the combinatorics of
the interaction of jobs, resources, and time, and due to the stochastics: to make a decision,
one can use only the information that is known with certainty at a decision point and the
stochastic properties of scenarios that may occur in the future.
2.2 Combinatorial Scheduling
In the classical scheduling literature, it is common to assume full information: all jobs and
their characteristics are known prior to any decision making. For example, the job shop
scheduling problem (JSP) consists of |J| jobs and |M | resources (French, 1982). Each job
consists of a set of |M | completely ordered activities with a corresponding processing time
and resource requirement: the job must use the specified resource for its full, uninterrupted
processing time. While a number of objective functions have been defined, the most common is to minimize the makespan: the time between the start of the first activity and the
end of the last one. Many solution approaches have been proposed, ranging from complete
techniques such as branch-and-bound (Carlier & Pinson, 1989; Brucker, Jurisch, & Sievers,
1994), mixed integer programming (Bowman, 1959; Manne, 1960), and constraint programming (Fox, 1987; Baptiste, Le Pape, & Nuijten, 2001; Beck, 2007; Beck, Feng, & Watson,
2011), to dispatch rules (Pinedo, 2003) and meta-heuristics (Nowicki & Smutnicki, 1996,
2005).
The scheduling literature has also addressed problems with uncertainty. Pinedo (2003)
refers to stochastic scheduling problems as those with random variables for the processing
times and/or release dates (i.e., arrival times) of jobs. While acknowledging the similarity
to the dynamic problems studied in queueing theory, Pinedo points out that a primary
537

fiTerekhov, Tran, Down, & Beck

difference is that stochastic scheduling is typically concerned with optimizing the schedule
for a finite number of jobs rather than the long-term system behaviour over a potentially
infinite stream of arriving jobs (Pinedo, 2003, ch. 11). Approaches to solving such stochastic
scheduling problems often involve the formulation of a determinstic scheduling problem to
optimize the expected value or some other probabilistic measure of the objective function
(Daniels & Carrillo, 1997; Pinedo, 2003; Beck & Wilson, 2007; Wu, Brown, & Beck, 2009),
insertion of temporal or resource redundancy to cope with realizations of random variables
(Leon, Wu, & Storer, 1994; Davenport, Gefflot, & Beck, 2001), or multi-stage stochastic
programming (Herroelen & Leus, 2005).
When solving dynamic problems, the scheduling community typically adopts the periodic scheduling approach of solving a collection of linked static sub-problems (Bidot, Vidal,
Laborie, & Beck, 2009; Ouelhadj & Petrovic, 2009). At a given time point, the static
problem, consisting of the jobs currently present in the system, is solved to optimize some
short-term objective function. That schedule is then executed (wholly or partially) until
the creation of the next sub-problem is triggered. This viewpoint means that methods developed for static scheduling problems become directly applicable to dynamic ones. Such
methods can effectively deal with complex combinatorics and can optimize the quality of
schedules for each static sub-problem. However, they tend to overlook long-run performance
and stochastic properties of the system.
There are a few examples of work that modifies the short-term objective or problemsolving process to address the fact that the static problem is part of the long-term scheduling
problem. Branke and Mattfeld (2002, 2005) address a dynamic job shop problem with the
overall objective of minimizing the mean tardiness of jobs. They use a periodic scheduling
approach based on a genetic algorithm that solves each sub-problem to minimize a combination of tardiness with a preference to place resource idle time toward the end of the
sub-problem schedule. The intuition is that later idle time allows jobs that arrive in the
future to be slotted in to the current schedule with little disruption. Empirical results
demonstrate a lower idle time over the long term compared to simply minimizing tardiness.
Another example is the framework of Online Stochastic Combinatorial Optimization
(OSCO) (Van Hentenryck & Bent, 2006; Mercier & Van Hentenryck, 2007), where the
set of existing jobs plus a sample of future arrivals is used to create a static scheduling
sub-problem. Multiple samples and optimizations are intricately combined to arrive at
a set of decisions for the existing jobs. Mercier and Van Hentenryck (2007) show that
such algorithms scale better than Markov Decision Processes (MDPs) and result in good
performance when there is a small expected difference between the best performance of
clairvoyant and non-clairvoyant decision makers. Similar approaches have been developed
in AI planning (Yoon, Fern, Givan, & Kambhampati, 2008; Burns et al., 2012).
These examples employ the same underlying approach to dynamic scheduling problems:
adapt a static technique by incorporating intuitive (Branke & Mattfeld, 2002) or sampled
(Van Hentenryck & Bent, 2006) information about the future. These approaches serve as
an inspiration for us to develop a more formal and general understanding of how long-term
objectives can be achieved by solving a series of short-term, static scheduling problems.
538

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

2.3 Queueing Theory
Queueing theory is the mathematical study of waiting lines (Gross & Harris, 1998). It
models systems in which one or more servers (machines) at one or more service stations
process arriving customer requests (jobs). Fundamentally, queueing theory focuses on the
formal analysis of the evolution of the queues and related metrics over time given a particular system definition or class of systems. The standard notation to describe a queueing
process, due mainly to Kendall (1953), is represented by A/B/X/Y /Z. A describes the
inter-arrival process (distribution of time between two successive arrivals), B the service
time distribution, X the number of parallel machines, Y the buffer size (maximum job capacity of the system), and Z the queue discipline (scheduling order). Common distributions
found in the literature for A and B are deterministic1 (D), exponential (M) and general
(G). Given a defined arrival and service time distribution, queueing discipline, and number
of machines and buffer capacity, the queueing literature is most commonly interested in
determining steady-state system parameters such as stability conditions, expected waiting
time and expected queue length. Although a significant portion of the literature focuses
on these descriptive models for steady-state performance metrics, transient behaviour and
prescriptive models are also studied, and are of interest in this paper. Markov processes
(Down & Meyn, 1994; Dai & Meyn, 1995; Bolch, Greiner, de Meer, & Trivedi, 2006), fluid
models (Dai, 1995; Dai & Weiss, 1996), and linear programming (LP) models (Andradottir,
Ayhan, & Down, 2003; Down & Karakostas, 2008) are typical approaches in analyzing
queueing systems.
The area of queueing theory that deals with prescriptive models is frequently called the
design and control of queueing systems (Tadj & Choudhury, 2005; Gross & Harris, 1998).
In both queueing design and control problems, the goal is to find optimal values for the
controllable parameters of the queue. These parameters include the number of machines
available for processing arriving jobs, the buffer capacity, the arrival rate of jobs to the
queue(s), the service rates of the machine(s), as well as any combination of these. Queueing
design problems are static: once the optimal value of a controllable parameter is determined,
it becomes a fixed characteristic of the queue. Queueing control problems, on the contrary,
are dynamic: the goal in such problems is usually to determine an optimal action to take
when the system is in a particular state. For example, consider a retail facility with workers
who have to serve stochastically arriving customers and also perform back-room tasks which
are independent of the customer arrival process (Terekhov, Beck, & Brown, 2009). In order
to optimize the performance of such a facility, one has to solve the queueing design problem
of finding the optimal number of cross-trained servers to employ as well as the related
queueing control problem of determining when to switch these workers between the two
task types. We refer the reader to the papers of Tadj and Choudhury (2005) and Crabill,
Gross, and Magazine (1977) for overviews of design and control problems involving queues.
Queueing theory has taken the viewpoint that, since it is impossible to create an optimal
schedule for every single sample path in the evolution of the system, one should aim to
achieve optimal performance in some probabilistic sense (e.g., in expectation) over an infinite
time horizon. This goal could be attained by construction of a policy based on the global
stochastic properties of the system. For example, a policy could specify how start time
1. Deterministic here refers to all inter-arrival or service times having the same value.

539

fiTerekhov, Tran, Down, & Beck

assignments should be made whenever a new job arrives or completes processing. However,
the schedule resulting from such a policy, while being of good quality in expectation, may
be far from optimal for the particular realization of uncertainty that occurs. Moreover,
queueing theory generally studies systems with simple combinatorics, as such systems are
more amenable to rigorous analysis of their stochastic properties.
2.4 Stability
Stability2 is a fundamental concept in queueing theory and forms the main part of our
theoretical analysis. Informally, a system is stable if its queues remain bounded over time.
The stability of a system is dependent on the scheduling policy it employs: for a given set of
problem parameters (e.g., arrival and processing distributions), one policy may stabilize the
system while another might not. Knowledge of whether a system is stable for a given job
arrival rate, processing rate and scheduling policy is considered a precursor to more detailed
analysis and is essential for practical applications (Kumar, 1994; Dai & Weiss, 1996).
Formally, a system operating under a particular queueing discipline is stable if the
Markov process that describes the dynamics of the system is positive Harris recurrent (Dai,
1995). Positive Harris recurrence implies the existence of a unique stationary distribution.
Due to the considerable notation required, we do not formally define positive Harris recurrence here, but instead refer the reader to the work of Dai (1995), Dai and Meyn (1995)
and Bramson (2008).
In the special case when the state space of the Markov process is countable3 and all states
communicate, positive Harris recurrence is equivalent to the more widely-known concept of
positive recurrence (Bramson, 2008). A Markov chain is positive recurrent if every state s
is positive recurrent: the probability that the process starting in state s will return to s is
1, and the expected time to return to this state is finite (Ross, 2003). In particular, this
property guarantees that the system will empty.
2.5 Problem Settings
Both queueing theory and combinatorial scheduling focus on the efficient use of scarce
resources over time. However, their different emphases (i.e., stochastics vs. combinatorics)
indicate that it may be possible to combine them in order to provide a better understanding
of, and stronger solution approaches to, dynamic scheduling problems.
A first challenge in this study is to determine the problem settings and assumptions. As
queueing and scheduling sometimes make differing assumptions, we have been guided by
three heuristics in choosing problems. First, we sought the simplest problem settings where
queueing and scheduling had not arrived at the same solution. For example, in many single
machine dynamic scheduling problems, identical, optimal dispatch rules/queueing policies
exist in both literatures. Second, where assumptions in the two areas are consistent, we
2. Unfortunately, stability has multiple meanings in closely related literature. In the scheduling literature,
a predictive schedule is called stable if its execution is close to what was planned (Bidot et al., 2009).
Similarly, in work on scheduling under uncertainty, stability analysis concerns the identification of the
range of values that the processing times may take while the given schedule remains optimal (Sotskov,
Sotskova, Lai, & Werner, 2010). Here we use the meaning of stability from queueing theory.
3. This process is referred to as a continuous-parameter Markov chain in the book by Gross and Harris
(1998).

540

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

adopted them. And third, when assumptions are contradictory, we attempted to choose
realistic settings that also result in novel and challenging problems.
As a consequence, we make the following assumptions in the two systems studied here.
 As in queueing, we assume that we know the distribution of the job inter-arrival times.
 We assume that the job durations become known upon arrival and are realizations
of the corresponding known distributions. Queueing models typically assume that
the distribution of job durations is known, but the actual processing time of a job is
not available until its completion time.4 In the classical scheduling literature, exact
job durations are known prior to construction of a schedule and jobs processed on the
same machine have, in general, different durations. As knowledge of both distributions
and actual durations upon arrival can be justified by applications in which historical
data is available and in which similar activities are repeatedly executed (e.g., the
serving of the same static web page, the cutting of a piece of wood with standardized
dimensions), we adopt both assumptions.
 For each queue, the sequences of processing times for all machines and the sequence
of inter-arrival times are independent and identically distributed sequences. They are
also mutually independent.
 Mean processing times and mean inter-arrival times are finite.
 The inter-arrival times are unbounded and continuous.
While the final three assumptions are standard in the queueing literature dealing with
stability analysis (Dai, 1995) and satisfied by commonly used distributions (e.g., the exponential distribution), the combination of the first two assumptions typically has not received
much attention in the queueing or scheduling literature. The setting is somewhat similar to
the online-time setting in online scheduling (Pruhs, Sgall, & Torng, 2004) where jobs arrive
to the system dynamically and job processing times become known upon arrival. However,
that setting makes no assumption about known inter-arrival and duration distributional
information.

3. The Two-Machine Dynamic Flowshop
The first problem we consider is a dynamic two-machine permutation flowshop. Within
this setting, we demonstrate the stability of a periodic scheduling approach that uses processing time information, and provide a numerical comparison of queueing and scheduling
approaches.
In a dynamic two-machine permutation flowshop, arriving jobs must be processed first
on machine 1 and then on machine 2, and the order in which jobs are processed on the two
machines must be the same. We assume that the inter-arrival time distribution is general
with mean 1 , while the processing time distributions for machine 1 and 2 are general with
means 11 and 12 , respectively. Thus, the load for machine 1 is 1 = 1 , and the load for
4. One exception is the deterministic distribution, under which the durations of all jobs are assumed to be
identical (Gross & Harris, 1998).

541

fiTerekhov, Tran, Down, & Beck

machine 2 is 2 = 2 . Both 1 and 2 are assumed to be less than 1, as these are known
necessary conditions for stability (Gross & Harris, 1998). Once a job arrives to the system,
its processing times on both machines become known with certainty. Both machines are
of unary capacity. Preemptions are not allowed. The queues in front of machine 1 and
machine 2 are both assumed to be of infinite size. The goal of the problem is to sequence
the jobs on machine 1 and machine 2 in order to minimize the flow time: the time between
the arrival of a job and its completion on machine 2.
The dynamic flowshop (Park, 1988; Sarper & Henry, 1996; El-Bouri, Balakrishnan,
& Popplewell, 2008) is an extension of the classical flowshop environment that has been
extensively studied in the scheduling literature (Widmer & Hertz, 1989; Taillard, 1990). In
the queueing literature, this system is known as a tandem queue or a network of queues in
series and has also received significant attention (Burke, 1956; Reich, 1957; Jackson, 1957;
Gross & Harris, 1998; Andradottir & Ayhan, 2005).
3.1 Algorithms
We consider four periodic scheduling approaches to the two-machine dynamic flowshop
problem. In each case, a schedule is formed by optimizing a value of interest for a subproblem: the set of jobs present in the system at a particular time. In the dynamic flowshop,
the start time of every new sub-problem is equal to the completion time of the last job on
machine 1 in the previous sub-problem, as in Figure 1. Since the jobs in a new sub-problem
can begin execution after all scheduled jobs have been processed on the first machine, at the
time of scheduling there may still be jobs from a previous schedule that need processing on
machine 2. These previous scheduling decisions are not altered when the next sub-problem
is solved.

Figure 1: Dynamic flowshop with three sub-problems and three jobs per sub-problem. The
start of a sub-problem is the start of a set of jobs on machine 1, the end of a sub-problem
is the end of a set of jobs on machine 2.
To the best of our knowledge, no queueing policy has been proved to be optimal for
the flow time objective, even in the expected sense, for a dynamic two-machine flowshop
under our assumptions. Our first two approaches, however, are chosen because of theoretical
results for systems related to ours, as discussed below. For the dynamic flowshop, none of
our periodic scheduling methods make use of distributional information.
542

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

3.1.1 FCFS
Jobs in a sub-problem are sequenced in non-decreasing order of their arrival times to the
system. FCFS achieves the smallest expected flow time in a two-machine dynamic flowshop in the class of work-conserving, non-preemptive policies that do not use processing
time information (Towsley & Baccelli, 1991). Note that the periodic FCFS policy creates
identical schedules to the standard (non-periodic) first-come, first-served policy in queueing
theory, where jobs are processed in the order they arrive.
3.1.2 SPTsum
Jobs in a sub-problem are processed in non-decreasing order of the sum of their durations on
machine 1 and machine 2. This policy choice is motivated by the fact that, in the case when
the server is a single unary resource, shortest processing time first minimizes the expected
flow time (Wierman, Winands, & Boxma, 2007).
3.1.3 CompletionTime
Since minimizing sub-problem flow time under our assumptions is equivalent to minimizing
the sum of job completion times in a sub-problem, a natural sub-problem objective is
the sum of completion times of activities on the second machine. Optimizing the total
completion time will lead to the best short-run performance but it is unclear how this
method will perform with respect to long-run objectives. Minimizing the sum of completion
times in a two-machine flowshop is NP-hard (Pinedo, 2003).
3.1.4 Makespan
The fourth method we employ is motivated by reasoning about a proxy measure that we
speculate may result in strong long-run flow time performance. The minimum makespan
schedule for a sub-problem, by definition, allows the subsequent sub-problem to start as
early as possible on machine 2, implying potentially lower flow times for all future subproblems until the system empties. Therefore, while we are not likely to achieve optimal
flow time performance on each sub-problem, we conjecture that makespan minimization
may lead to better long-run flow time performance.
The optimal makespan schedule for a static two-machine flowshop can be found in
polynomial-time using Johnsons rule (Conway, Maxwell, & Miller, 1967). Johnsons rule
divides jobs into two sets: set I consists of all jobs whose processing time on machine 1
is less than or equal to its processing time on machine 2, and set II consists of all the
remaining jobs. Set I is processed before set II. Note that, as required, Johnsons rule
creates permutation schedules. Within set I, jobs are sequenced in non-decreasing order of
the processing time on machine 1, while within set II, jobs are sequenced in non-increasing
order of the processing times on machine 2.
3.2 Stability
We study and compare the stability of the only one of our policies that does not use
processing time information, i.e., FCFS, and one of our policies that does, i.e., makespan. In
particular, we show that the condition for the stability of FCFS follows trivially from known
543

fiTerekhov, Tran, Down, & Beck

results in the queueing literature. Using this result, more significantly, we show that the
makespan approach is stable under the same condition. To the best of our knowledge, this
is the first example of stability analysis of scheduling policies based on observed processing
times.
We leave the study of stability of our two remaining methods that utilize processing
time information for future work. Showing stability of the completionTime approach may
prove to be especially challenging since it possesses neither a specific structure that can be
utilized in the proof nor a property that makes it easily comparable to FCFS.
Theorem 1. If min{1 ,2 } < 1, then the periodic FCFS policy is stable for the two-machine
dynamic flowshop.
Proof. Under our assumptions, the dynamic two-machine flowshop is a generalized Jackson
network, and the periodic FCFS policy is equivalent to the standard, non-periodic implementation of FCFS. Stability of generalized Jackson networks under the condition that the
load of each machine is strictly less than 1 (in our case, /1 < 1 and /2 < 1) is known
(Dai, 1995).
The above result extends to |M | > 2 machines: fluid model methodology (Dai, 1995) can
be used to show the stability of FCFS in an |M |-machine flowshop under the condition that

minm{1,...,|M |} {m } < 1.
For the makespan policy, we prove a result which holds for every sample path in the
evolution of the system. Let sn and tn be the time points at which sub-problem n starts on
machine 1 and completes on machine 2, respectively, under the makespan approach. Let vn
and vn be the total processing time of all jobs completed by time tn under the makespan
policy and an arbitrary policy , respectively. Following the queueing literature, we further
refer to vn and vn as the work completed by time tn .
Lemma 1. The amount of work completed by tn is maximized by the makespan policy.
That is, vn  vn for all n and all non-idling .
Proof. For any arbitrary non-idling policy , vn can be written as vn1, + vn2, , where vn1, is
the amount of work completed by tn on machine 1 and vn2, is the amount of work completed
by tn on machine 2. (In Figure 2, t0 is 5, v01, = 5 and v02, = 3.) For the makespan approach,
we use the notation without the superscript , i.e., vn = vn1 + vn2 . In the dynamic flowshop,
the amount of work completed by tn on machine 1 is the same for any non-idling policy.
Thus, it remains to prove that vn2  vn2, . We prove this by induction.
Base Case: v02  v02, since any policy other than makespan can complete the set of initial
jobs only at the same time as makespan (t0 ) or later.
Inductive Hypothesis: Assume the property is true for tn , that is, vn2  vn2, .
2,
2
Inductive Step: We need to show the same property for tn+1 , i.e., that vn+1
 vn+1
. For
2
2
the makespan approach, vn+1 = vn + ((sn , sn+1 ]), where ((sn , sn+1 ]) is the total machine
2 workload that arrives in the time period (sn , sn+1 ] and, therefore, is the workload that is
processed in the sub-problem starting at sn+1 and ending at tn+1 .
By the induction hypothesis, we know that at tn , vn2  vn2, . The amount of work
2,
processed on machine 2 by time tn+1 by policy , vn+1
, equals the amount of work processed
by  by time tn plus some fraction of the difference in the amount of work completed by
544

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

 and makespan by tn plus some fraction of the amount of machine 2 work that arrives in
2,
2 .
(sn , sn+1 ]. Thus, vn+1
 vn2, + (vn2  vn2, ) + ((sn , sn+1 ]) = vn2 + ((sn , sn+1 ]) = vn+1

Figure 2: Schedule for policy  for a two-machine flowshop.

Figure 3: Schedule for makespan for the same problem instance as in Figure 2.
For example, consider the schedules in Figures 2 and 3: s0 = 0, s1 = 4, s2 = 9, t0 = 5,
t1 = 11, t2 = 14, v22 = v12 + ((s1 , s2 ]) = 9 + 3 = 12, v22, = 7 + (9  7) + 1  12.5
The lemma does not hold if  is idling since an idling policy may create a better schedule
by waiting and taking more jobs into account.
Theorem 2. If min{1 ,2 } < 1 then the periodic makespan policy is stable for the twomachine dynamic flowshop.
Proof. We know that FCFS is stable under the given condition. By Lemma 1, at every
sub-problem completion time, the makespan approach has finished at least as much work
as FCFS. Therefore, the two-machine dynamic flowshop with the periodic makespan policy
is stable under the same condition as FCFS.
The theorem provides a sufficient condition for stability. As noted above, from the
literature (see e.g., Gross & Harris, 1998), we know that 1 < 1 and 2 < 1 are necessary
conditions for stability of this system. Since ensuring that min{1 ,2 } < 1 is the same as
ensuring 1 < 1 and 2 < 1, we see that min{1 ,2 } < 1 is a necessary and sufficient condition
for stability of the dynamic two-machine flowshop under the makespan policy.
5. We assume that J7, J8 and J9 arrive in the time period (4, 9].

545

fiTerekhov, Tran, Down, & Beck

Figure 4: Schedule for FCFS for the dynamic flowshop with three machines.

Figure 5: Schedule for makespan for the dynamic flowshop with three machines.

Lemma 1 does not hold for an |M |-machine flowshop when |M | > 2. To illustrate this
fact, consider the problem instance in Figures 4 and 5. In this example, sub-problem 0
consists of jobs J0 and J1, and both the makespan policy and FCFS construct the same
schedule, with t0 = 7. The second sub-problem consists of J2 and J3, and the two policies
result in different schedules. At t0 = 7, the amount of work completed by makespan is
v0 = 12, whereas the amount of work completed by FCFS is v0 = 13. Therefore, in this
case, v0 < v0 , which shows that it is possible that the amount of work completed by tn is
not maximized by the makespan policy. We nonetheless conjecture that Theorem 2 extends
to the case with more than two machines and can be proved using a fluid model approach.
3.3 Numerical Results
We present experiments comparing the performance of FCFS, SP Tsum , makespan and completionTime models for minimizing the mean flow time over a long time horizon. The completionTime model was implemented via constraint programming in ILOG Scheduler 6.5
546

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

and uses the completion global constraint (Kovacs & Beck, 2011). The remaining methods
were implemented using C++.
To evaluate the performance of our four methods in a dynamic flowshop, we considered
a system with exponentially distributed inter-arrival times and exponentially distributed
processing times with the same means on both machines. Experiments with uniformly
distributed processing times showed identical performance. All parameters were chosen to
satisfy the stability conditions of Theorems 1 and 2. We fixed the mean inter-arrival time,
1/, to 10, and varied the load on the system from 0.1 to 0.95 by changing the means of
the processing time distributions from 1 to 9.496. Note that by Theorems 1 and 2, these
parameters guarantee stability of FCFS and makespan. The results of our experiments are
shown in Figure 6. Each point in the figure represents the mean flow time over 100 problem
instances of 55,000 jobs each.
In these experiments, the completionTime model was run with a time limit of 1 second per sub-problem in order to ensure reasonable run-times. Moreover, since constraint
programming is more efficient with integer, rather than real-valued, durations, when using
the completionTime approach we set durations to be the ceiling of the actual durations
multiplied by 100; the resulting processing sequence is then used to construct the actual
sub-problem schedule. Note that with a time limit, completionTime does not always find
the optimal sub-problem schedule. When run on a Dual Core AMD 270 CPU with 1
MB cache, 4GB of main memory, and Red Hat Enterprise Linux 4, the completionTime
model is able to solve 100% of sub-problems to optimality at loads 0.1 and 0.3, but this
percentage decreases to an average of 81.3% for instances at the 0.95 load. This change
in performance is due to the increase in sub-problem size as load increases: the average
sub-problem size increases from 1.009 at the 0.1 load to 5.452 at the 0.95 load, while the
maximum sub-problem size encountered over all instances increases from 5 at the 0.1 load
to 172 at the 0.95 load. Experiments with a time limit of 5 seconds showed very similar
performance, with the mean flow time at the 0.95 load decreasing to 368.985 (from 371.434
for the completionTime model with the 1 second time limit) and the percentage of instances
solved to optimality at the 0.95 load increasing to 83.8% on a machine with 4 Intel Core
i7-2600 CPUs @ 3.40GHz with 8 MB cache and 9871 MB main memory, running Red Hat
Enterprise Linux 6.3.
Figure 6 shows that there is very little difference among the methods. completionTime
has a slight advantage over the others for loads of 0.7 and less. The makespan model is
better than FCFS and completionTime at loads above 0.8. SP Tsum is the best-performing
model for loads above 0.8, when the static sub-problems become large. This last observation
is supported by the results of Xia, Shanthikumar, and Glynn (2000), who have shown that
SP Tsum is asymptotically optimal for the static average completion time objective as the
number of jobs in a two-machine flowshop increases. However, this asymptotic result does
not imply that applying SP Tsum to each sub-problem results in the best long-run behaviour
for high loads. FCFS, on the other hand, is the worst performer over all loads, but only
marginally so.
These empirical results are not contradictory to Theorem 1. In particular, the fact that
makespan results in the largest amount of workload being completed by a particular time
point does not imply that it will minimize the mean flow time. For example, consider
sub-problem 1 in Figure 3: both the sequence displayed in the figure, J5  J6  J4, and
547

fiTerekhov, Tran, Down, & Beck

200

FCFS
makespan
SPT_sum
completionTime

0

100

Mean Flow Time

300

Mean Flow Time for Various Queue Loads

0.2

0.4

0.6

0.8

1.0

Loads

Figure 6: Mean flow times in a dynamic two-machine flowshop for FCFS, SP Tsum , completionTime and makespan models as the system load varies.
the sequence J6  J5  J4 will minimize makespan, but the latter sequence will achieve a
lower total completion time (26 as opposed to 28 for the first sequence) and hence a lower
mean flow time for this sub-problem.
3.4 Discussion
Our study of the dynamic two-machine flowshop provides partial support for our thesis of
the utility of combining queueing and scheduling. We have shown that stability analysis
can be applied to periodic scheduling algorithms and that algorithms that reason about
combinatorics can achieve the same stability guarantees as traditional queueing theory
approaches. However, in this system we were not able to empirically demonstrate that
the scheduling algorithms (completionTime and makespan) result in better mean flow time
performance than queueing approaches.
At low loads, all methods perform equivalently due to small sub-problems; at high loads,
the differences in performance lead to the following observations. Firstly, the performance
of completionTime degrades due to its inability to find good solutions for large sub-problems
given the time limit; if it is not able to find a better solution, completionTime defaults to
the FCFS schedule. Secondly, our motivation for using the makespan approach did not
548

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

materialize: the differences between the makespans of the makespan approach and SPTsum
are not large enough to offset the differences in total completion time (which are in favour of
SPTsum ). Finally, we conjecture that an implementation of completionTime which defaults
to the SPTsum schedule would outperform the current approaches at the highest load. A
further improvement should result from finding, for each sub-problem, the optimal total
completion time schedule with the lowest possible makespan.
Subsequent analytical work on this problem setting shows that the completionTime
approach results in a smaller long-run mean flow time over T time periods than makespan if,
for the majority of sub-problems, the difference between completionTime and makespan in
individual sub-problem total completion times is larger than the difference in the makespans
prior to the start of the sub-problem. Furthermore, in a more complex problem setting
(i.e., a polling system with a two-machine flowshop server) it is shown, both analytically
and empirically, that for the objective of minimizing long-run flow time, an algorithm that
minimizes the makespan of the sub-problems outperforms one that minimizes the sum of
completion times (Terekhov, 2013, ch. 7). While these results do not directly address the
combination of queueing and scheduling approaches, they demonstrate that the impact of
sub-problem optimization criteria on long-term performance measures is delicate and not
yet well understood.

4. A Queueing Network with Flexible Servers
To more deeply investigate the combination of queueing theory and scheduling, we examine
a second system with a different combinatorial structure. As in the first problem, we provide
an analysis of stability and a numerical comparison of queueing and scheduling approaches.
However, unlike in the first problem, in this setting we also propose and evaluate a hybrid
queueing/scheduling algorithm.
The system of interest is a queueing network where heterogeneous servers are required to
serve jobs belonging to specific classes. This problem is an example of a classical queueing
system (Bramson & Williams, 2000; Andradottir et al., 2003). The system differs from the
dynamic flowshop in that jobs may be processed on any machine, and the assignment of
the job to a machine must be made. Jobs arrive over time via an arrival process with rate
 and generally distributed inter-arrival times. An arriving job belongs to a class k with
probability pk . We assume that there are no preemptions: once a job begins execution it
must be processed for its full processing time. When a server m is assigned to a job of class
k, the jobs processing time is generally distributed with rate mk ; once a job j arrives to
the system, the processing times on each server become known and are denoted by dmj . We
assume that all servers are able to serve jobs of all classes. If multiple servers are working
on a single class at any point in time, they work in parallel so that each job is served by
exactly one server.
The equivalent static problem from the scheduling literature is the parallel machine
scheduling problem (Pinedo, 2003). However, the scheduling literature either assumes machines are related or unrelated. In a related parallel machine scheduling problem, each
machine has an inherent speed that determines job processing times. If machine a is twice
as fast as machine b, it will always require half the time to process any job than machine
b does. When machines are unrelated, there is no correlation in processing times of a job
549

fiTerekhov, Tran, Down, & Beck

on the different machines. In the queueing network problem, processing times for a given
job class are stochastically related because they are drawn from the same distribution on
a given machine. However, there are no class relationships across different machines so it
may be that ok < mk but ol > ml for m 6= o, k 6= l. We adopt the queueing theory
assumption.
4.1 Algorithms
We present five different approaches for scheduling jobs in the queueing network with flexible
servers: two queueing policies, two scheduling policies, and a hybrid queueing theory and
scheduling method.
To illustrate the five approaches, we present a small example of a possible realization
for a system with two servers and two job classes. There are a total of five jobs: three
belonging to class 1 and two belonging to class 2. Each job is represented as J{k,j}, where
k represents the class of a job and j is the job number. Table 1 provides the arrival and
processing times on the two machines (servers) for each job in our example. Notice that
both servers will process class 2 jobs for the same amount of time, but server 2 is 1.5 times
slower than server 1 on jobs of class 1.
{Class, Job ID}
Arrival Time
Server 1 Processing Time
Server 2 Processing Time

J{1,1}
1
8
12

J{1,2}
3
6
9

J{1,3}
5
4
6

{2,1}
0
6
6

{2,2}
12
6
6

Table 1: Example job arrival and processing times.

The two queueing policies do not employ a periodic scheduler. Instead, they are dispatch
rules used to decide which job should be processed when a server becomes available. In both
policies, a linear program (LP), called the allocation linear program, is used to determine
the portion of total capacity that each server should allocate to each job class (Andradottir
et al., 2003). This LP is solved only once, before any scheduling decisions are made. The
allocation LP is derived from a fluid representation of the queueing network that assumes
that the number of jobs present in the system is so large as to exhibit properties of a
continuous fluid (Chen & Yao, 1992; Dai, 1995). The allocation LP is as follows:
max 
|M |
X
s.t.
mk mk  pk , k  K

(1)

m=1
|K|

X

mk  1,

mM

(2)

k  K; m  M

(3)

k=1

mk  0
where
550

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

: the arrival rate of jobs,
mk : the fractional amount of time that server m should spend on class k jobs,
K: the set of job classes,
M : the set of servers.
The decision variables for this problem are  and mk . The objective is to maximize the
arrival rate of jobs while maintaining stability of the system. Stability is achieved through
constraint (1) which ensures that the total capacity allocated to a job class will be greater
than or equal to the total amount of work generated from that job class.
The solution of the allocation LP provides a tight upper bound on the maximum arrival
rate for which the system can be stabilized,  , and values for the resource allocation
 (Andradottir et al., 2003). However, it does not provide a method to assign
proportions, mk
jobs to servers and does not decide how to sequence the jobs. To make these decisions, the
Round Robin policy (Andradottir et al., 2003) or the linear programming-based affinity
scheduling (LPAS ) heuristic (Al-Azzoni & Down, 2008) can be used.
4.1.1 Round Robin
In the Round Robin policy, each server m cyclically visits classes in Vm , where Vm is an

> 0. While serving a class k  Vm , the server
ordered list of all classes k with mk mk
processes at most lmk jobs before moving on to the next class. If the server is idle at any
point before lmk jobs are served, it switches to the next class in Vm . Given a desired arrival

1
rate , let  =  
+ , mk = mk , and let 1{mk > 0} be a function that is equal to 1 if
 values are sufficiently
mk > 0 and 0 otherwise. Andradottir et al. (2003) show that the mk
tracked to stabilize the system if lmk is chosen to be
P

 > 0})  
(1  )( lK ml 1{mk
mk
lmk =
.
mk
Figure 7 shows the schedule produced by the Round Robin policy for the five-job exam = 1,   = 0,   = 0.5, and   = 0.5.
ple. Assume that the allocation LP results in 11
12
21
22
That is, server 1 handles jobs of class 1 only, and server 2 can handle both classes. Further,
assume the calculated lmk value to be greater than three for all server and class pairs. In
this situation, for our example with at most three jobs per class, the Round Robin policy
will change classes that a server is processing only if there are no available tasks of that
class to start immediately. At time 0, only J{2,1} is present in the system. Since server 2 is
the only server to handle class 2 jobs, the job is assigned to begin immediately on server 2.
When J{1,1} arrives, server 1 is able to start this job instantly. Server 2 completes J{2,1} at
time 6, when there are two jobs of class 1 in queue. Server 2 then begins processing J{1,2}
since it arrived first. Server 1 completes J{1,1} at time 9 and starts processing J{1,3}.
Server 1 completes J{1,3} before server 2 completes J{1,2}. Although J{2,2} is waiting in
 = 0. J{2,2} must wait until
queue at time 13, it is not assigned to server 1 because 12
server 2 is available at time 15. The example schedule ends at time 21 with the completion
of J{2,2}.
4.1.2 LPAS
 > 0 and is expected to complete
LPAS assigns each arriving job to a machine that has mk
the job at the earliest time. The LPAS heuristic is similar to Round Robin, but does not

551

fiTerekhov, Tran, Down, & Beck

Figure 7: Example schedule produced by the Round Robin policy and LPAS.

 to guarantee stability. Rather, the heuristic uses
reason about the relative magnitudes of mk

mk to reason only about which server-class pairs are efficient assignments. The stability of
LPAS is an open question, though it has been shown empirically to perform well in terms
of the number of jobs in the system (Al-Azzoni & Down, 2008).
Figure 7 also presents the schedule produced by LPAS for the five-job example. Similar
to the Round Robin policy, we assume the allocation LP results in server 1 processing class 1
only, and server 2 handling both classes. At time 0, J{2,1} arrives and is assigned to server
2. J{1,1} arrives at time 1 and can be processed by either server. Starting immediately on
server 1 would lead to an earlier completion time for J{1,1}, so we make that assignment.
J{1,2} arrives at time 3 and is also considered by both servers. Starting J{1,2} on server 2
at time 6 leads to the earliest completion time, and hence this assignment is made. J{1,3}
then arrives at time 5 and can complete earliest if assigned to server 1. Finally, J{2,2} must
be assigned to server 2 at the end of the schedule. Therefore, the resulting schedule for this
particular example is the same for the Round Robin policy and for LPAS.

Of the two scheduling models we study, one is a simple dispatch policy similar to that of
the queueing policies presented above, while the second makes use of a periodic scheduler.
4.1.3 MinFT
The dispatch policy MinFT greedily minimizes the flow time of jobs without exploiting the
solution of the allocationP
LP. In this setting, the flow time of a job j, if it is assigned to
server m, is fmj = m + xm dmx + dmj , where m represents thePtotal remaining time
that server m will be busy with the job currently being served, and xm dmx is the sum
of the processing times of all jobs belonging to m , the set of jobs queued for server m.
When a job arrives, it is assigned to the server which results in the smallest flow time for
that job given the jobs already scheduled. Ties are broken arbitrarily. Jobs are sequenced
in first-come, first-served (FCFS) order on each server.
The schedule produced by MinFT for the five-job example is shown in Figure 8. At
the start of the schedule, J{2,1} is considered by both servers, which leads to the same
completion time on either, so MinFT arbitrarily chooses one of the servers. In this case, we
assume servers are chosen lexicographically, and so the job is assigned to server 1. At time
1, assigning J{1,1} to server 2 will lead to a smaller flow time, and hence this assignment
is made. This process continues for each job, which results in the schedule of Figure 8.
552

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

Figure 8: Example schedule produced by MinFT.

4.1.4 Makespan
As in the flowshop studied above, the makespan model periodically solves static makespan
minimization problems. A period is defined as the time from when the scheduler evaluates
the system until any server is once again available. Any jobs belonging to a previous period
but not yet completed stay assigned as before.
This problem is NP-hard, and so there are no polynomial-time algorithms to minimize
the makespan as in the two-machine dynamic flowshop problem. We use mixed-integer
programming (MIP) to solve the minimum makespan scheduling problem. At the beginning
of every period, the following MIP model is solved to minimize the makespan of the set of
unscheduled jobs:
min Cmax
|J|
X
s.t.
xmj dmj + m  Cmax ,

mM

(4)

jJ

(5)

j  J; m  M

(6)

j=1
|M |

X

xmj = 1,

m=1

xmj  {0, 1},
where
Cmax :
xmj :
m :

makespan of the period,
1 if job j is assigned to server m, 0 otherwise,
the remaining time that server m is busy serving jobs belonging to schedules
made during previous periods,
J: the set of jobs to be scheduled during the period.
This model minimizes the makespan, Cmax , which is constrained to be greater than
or equal to the maximum scheduled busy period of a server. m is the amount of time
P|J|
that server m will be busy until it is able to serve new jobs, and j=1 xmj dmj is the total
processing time allotted to the new jobs by server m. The sum of these two terms equals
the total time the server will be busy by following the schedule. Constraint (5) ensures that
each job is assigned to exactly one server.
The MIP model assigns jobs to servers but does not sequence the jobs. Any order will
achieve the same makespan since processing times of all jobs are sequence independent. To
553

fiTerekhov, Tran, Down, & Beck

allow a direct comparison with the policies defined above, FCFS is used once jobs have been
assigned to machines.
Figure 9 gives the schedule produced by makespan for the five-job example. At time
0, there is only 1 job available, so makespan schedules J{2,1}. Assignment to server 1 or
2 leads to the same makespan, so we arbitrarily choose server 1. When J{1,1} arrives, a
new sub-problem is created since server 2 is idle and can begin processing jobs immediately.
Assigning the arriving job to server 2 leads to minimizing the makespan of sub-problem 1.
At time 6, server 1 becomes idle and finds two jobs in queue (J{1,2} and J{1,3}). Minimizing
the makespan of sub-problem 2 requires J{1,2} and J{1,3} to be assigned to server 1. The
final sub-problem starts at time 13 when server 2 finishes J{1,1}. At this point, there is one
job in the queue, and it is assigned to server 2 to minimize the makespan of sub-problem 3.

Figure 9: Example schedule produced by the makespan model.

MinFT does not reason about future jobs, while makespan can be thought of as only
indirectly saving resources for future jobs by minimizing the makespan of the current period.
The ability of the makespan model to deal with complex combinatorics allows it to take
advantage of a systems state information, while Round Robin and LPAS are able to use
knowledge of processing and arrival rates to manage the allocation of servers. An approach
that is able to reason with both types of information may improve overall performance.
Thus, we integrate scheduling and queueing theory on an algorithmic level to create a
hybrid model.
4.1.5 Hybrid Model
The hybrid model is a periodic scheduler that uses the makespan model as a framework and

employs the mk
values from the allocation LP. Recall that these values indicate the best
long-run proportional allocation of servers to job classes. The MIP model solved for each
period is:
554

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

min Cmax + (1  )

|M | |K|
X
X

cmk

m=1 k=1

s.t.

Constraints (4) to (6)
|J|
X
X

xmj dmj  mk
xmj dmj  cmk , k  K; m  M
jSk

(7)

j=1

cmk  0,

k  K; m  M

(8)

where
cmk : the deviation between a realized assignment of server m to class k and what
 suggests,
mk

: an input parameter used to scale the importance of deviation from the mk
values versus makespan,
Sk : the set of jobs that belong to class k.
This MIP model has a bi-criteria objective of minimizing the makespan and the deviation

values. The trade-off between makespan and deviation is expressed by ,
from the mk
0    1. When  = 1, the model is the makespan model.
Figure 10 illustrates the schedule that the hybrid model might produce for the five-job
 = 1,   = 0,   = 0.5, and

values as before, i.e., 11
example assuming the same mk
12
21

22 = 0.5, and assuming  is chosen to be close to 1. Using a high  value makes our
illustration of this models behaviour simpler. Further, this choice ensures that minimizing
 values being used only for breaking ties
makespan is the more important objective, with mk
when different schedules produce the same makespan. At time 0, sub-problem 0 is solved
with only J{2,1} in the system. Although the makespan is the same regardless of which
server is responsible for the job, the hybrid model chooses to make an assignment to server
 = 0 and   = 0.5. Sub-problem 1 starts at time 1 when J{1,1} arrives and is
2, because 12
22
assigned to server 1. At time 6, J{1,2} and J{1,3} are in queue, and sub-problem 2 starts.
J{1,2} is assigned to server 1 and J{1,3} is assigned to server 2 to minimize the makespan
of sub-problem 2. At time 12, sub-problem 3 begins as J{2,2} is scheduled on server 2.

Figure 10: Example schedule produced by the hybrid model.

555

fiTerekhov, Tran, Down, & Beck

4.2 Stability
We know from the work of Andradottir et al. (2003) that the Round Robin policy is stable
for all  <  , the maximum stabilizable arrival rate. The stability of the LPAS heuristic
has not yet been established.
We examine the stability of the two scheduling policies and the hybrid model presented
above. The stability conditions for makespan and the hybrid model are determined through
a comparison with the Round Robin policy. The MinFT model is shown to not guarantee
stability under the same conditions as the Round Robin policy.
4.2.1 Stability of Makespan
Given that Round Robin can achieve any desired capacity  <  , we have the following
theorem.
Theorem 3. If Round Robin is stable for a given arrival process with rate , then the
makespan model is also guaranteed to be stable.
Proof. We prove the stability conditions of makespan by assuming that makespan is unstable
and Round Robin is stable, and deriving a contradiction. Let Jt be the set of jobs in the
system at time t under the makespan policy. Our assumption that makespan (denoted with
ain our notation) is unstable means that limt E(|Jt |) = .
At some time t0 , the makespan model completes a period and schedules the set Jt0 .
Denote the earliest start time and latest end time of jobs in Jt0 under the makespan policy as S(Jt0 ) and F (Jt0 ), respectively. The makespan model will minimize the makespan,
C(Jt0 ) = F (Jt0 )  S(Jt0 ). However, the work remaining from previous periods for all servers,
represented by , must be accounted for. If J 0 is the set of jobs in the previous period, then
the left-over work is bounded by   maxmM ;jJ 0 (dmj ) because any server with residual
work greater than the largest of all the processing times would have had a job reassigned to
the free machine in the previous sub-problem to reduce the sub-problem makespan. In the
worst case, all servers except one are busy for time  and the next period will have only one
server available immediately, delaying the optimal schedule by less than  time units. We
further define the minimum makespan of scheduling Jt0 when residual work can be ignored
as C  (Jt0 ). It is easy to show that C(Jt0 )  C  (Jt0 ) + . There exists some number of jobs
 where
|J|
1


 
C  (J)
C(J)

=1


C(J)
C(J)

  ,   0. This is true because as the number of jobs increases,  becomes
and as |J|
negligible compared to the actual makespan of the schedule. Therefore, as the number of
jobs in the system increases without bound, the makespan converges to the optimal. Since
throughput of the system is the exit rate of jobs and Round Robin can at best match the
 the
minimum makespan, after the system has reached some sufficiently large size of J,
throughput of makespan is at least as large as that of the Round Robin policy. Therefore,
we find a contradiction: makespan cannot be unstable when Round Robin is stable if the
throughput of makespan is at least as large as that of the Round Robin policy. Thus, it is
guaranteed that any stabilizable system is stabilized under makespan.
556

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

   is an aspect of the proof technique, not a
Note that the dependence on |J|
requirement for stability. Stability is a property of the system and does not depend on
the number of jobs in the system. However, our proof depends on achieving the minimum
makespan for each sub-problem, which requires solving an NP-hard problem. We return to
this point in Section 5.
4.2.2 Stability of the Hybrid
We prove stability conditions for the hybrid model in the same way as for makespan. We
 values when
first show that there exists a minimum makespan schedule that tracks the mk
the number of jobs in a period is sufficiently large.
Proposition 1. As the number of jobs to be scheduled in a period approaches , there exists
 values from the allocation LP.
a schedule that has a minimum makespan and tracks the mk
Proof. Denote the set of jobs to schedule during a period as J, the set of jobs assigned to
 , and the makespan
server m from class k as Qmk , the resulting makespan of server m as Cm

of the system overPall machines for the set of jobs J as Cmax . The time server m spends on
jobs of class k is jQmk dmj . The proportion of time each server m spends on a class k,
denoted mk , is
P
P
jQmk dmj
jQmk dmj
P
mk = P
.
=

Cm
kK
jQmk dmj
Using the Law of Large Numbers, we know
X
1
(
dmj ) = 1
mk .
|Q
|
|Qmk |
mk
lim

iQmk

Taking the limit as the number of jobs in the system, |J|, goes to , gives
(
|Qmk |
when lim|J| |Qmk | = ,
lim|J| mk

Cmax
lim mk =
|J|
0
when lim|J| |Qmk | < .

(1)

If we multiply mk by mk and sum over all machines, we get
lim

|J|

|M |
X

mk mk = lim

m=1

1


|J| Cmax

|M |
X
m=1

pk |J|
,

C
|J| max

|Qmk | = lim

which is the left-hand side of constraint (1) in the allocation LP. Re-arranging terms leads
to
|M |
X
mk mk
pk |J|
|J|
= lim
= lim
 .


pk
|J|
|J| pk Cmax
|J| Cmax

lim

m=1

Here, we see that since the allocation LP aims to maximize , it is equivalent to minimizing

the makespan Cmax
, and therefore a schedule exists with a minimum makespan that also

tracks mk .
557

fiTerekhov, Tran, Down, & Beck

Theorem 4. If Round Robin is stable for a given arrival process with rate , then the
hybrid model is also stable.
Proof. The proof is based on the proof of Theorem 3. makespan can be replaced by the
hybrid model (denoted as h when used as a superscript) and the proof follows except for
C h (Jth )   6 C  (Jth ). Due to the bi-criteria objective, the model does not guarantee
minimum makespan schedules.
If we take |J|  , Proposition 1 states that there is a minimum makespan schedule

that will track the mk
values such that cmk goes to 0. Therefore, for a sufficiently large
system size, cmk will be small enough to ensure that the makespan of the schedule converges
to that of makespan and, similarly to Theorem 3, the hybrid model guarantees stability in
a stabilizable system.
4.2.3 Instability of MinFT
To show that the MinFT model cannot handle all  <  , we provide a counter-example.
Assume a system where there are two servers and two job classes. The arrival rate to the
system is  = 1 and p1 = p2 = 0.5. If 11 = 10, 21 = 9, 12 = 9, and 22 = 10, then the
 = 1,   = 0,   = 0, and   = 1 to get  = 20. In order
allocation LP would assign 11
21
12
22
for the system to be stable for a particular , a scheduling algorithm must adequately track
 values. As    , the available freedom for an algorithm to deviate from   goes
the mk
mk
to 0.
Assignments in the MinFT model are made greedily to the server that will minimize
the jobs flow time. Denoting the completion time of the latest job on a server m as m ,
we know that an arriving job j will be served on the faster server m rather than the slower
server l unless the inequality m + dmj > l + dlj is true. When the inequality is true,
the MinFT model will assign the arriving job to server l since its completion time will
be earlier. We define j = dlj  dmj where j << dmj since the difference between the
processing rates is an order of magnitude smaller than the processing rates themselves.
Consider two cases: (1) |1  2 | > j , and (2) |1  2 | < j . In case (1), an arriving
job will be assigned to the server with a smaller m regardless of class because the other
server is busy. The MinFT model follows an efficient assignment to use the faster server
only in case (2). In case (1) the scheduler will make the efficient assignment 50% of the
time because an arriving job has equal probability of belonging to either class. We need to
show that if P (|1  2 | > j ) > 0, then the MinFT model cannot guarantee stability like
the Round Robin policy. Assume the inequality is false, i.e., P (|1  2 | > j ) = 0. In this
scenario, we know the system at any point in time adheres to |1  2 | < j . However, if a
job arrives, it must be scheduled onto one of the servers, making |1 2 | > minm1,2 (dmj )
j > j . This inequality results in a contradiction since, with certainty, the next arriving
job will force the system into case (1). Since case (1) occurs with positive probability, an
inefficient assignment will occur with some non-zero probability. Denote the probability of
inefficient assignments as b1 = P (1  2 > j )p1 and b2 = P (1  2 < j )p2 where
b = b1 = b2  (0.5)(0.5) because of system symmetry. Then the realized proportions of time
each server spends on classes are 11 = 1  b, 21 = b, 12 = b, and 22 = 1  b. Therefore,
the obtainable capacity is
10(1  b) + 9b + 9b + 10(1  b) = 20  2b < 20,
558

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

because b > 0. For this simple system, if   2b <  <  , the MinFT model cannot
guarantee stability.
4.3 Numerical Results
We experimentally compare the mean flow time performance of our proposed models. Two
different cases are tested: two job classes and two servers, and four job classes and four
servers. For each test case, five different loads between 0.8 and 0.99 of the maximum
theoretical load are simulated. At each load, 20 instances are run for 10,000 time units,
resulting in a total of 100 simulations per model for each of the two systems.
Job arrivals follow a Poisson process. In order to maintain the relative processing times
of a single job on each server, an amount of work for each job, wj , is generated using an
exponential distribution with rate 1; it is then augmented linearly by the processing rate
wj
for a server, based on the jobs class, to create the processing time dmj = mk
. We use
processing rates mk ranging from one job per time unit to ten jobs per time unit. Both test
cases are asymmetric: the processing rates of the classes are different for different servers.
Symmetric systems are not examined to emphasize the heterogeneity of the network.
The simulation was implemented in C++. The LP and MIP models use IBM ILOG
CPLEX 12.1. All experiments are performed on a Dual Core AMD 270 CPU with 1 MB
cache, 4GB of main memory, running Red Hat Enterprise Linux 4. Preliminary results
showed that  = 0.6 provided the best performance for the hybrid method (Tran, 2011).

Mean Flow Time

Round Robin
MinFT
LPAS
makespan
Hybrid

1

10

0

10

80

82

84

86
88
90
92
94
Percent of Maximum Theoretical Load

96

98

100

Figure 11: Comparison of mean flow times for a flexible queueing network with two servers
and two classes.

559

fiTerekhov, Tran, Down, & Beck

Mean Flow Time

Round Robin
MinFT
LPAS
makespan
Hybrid

1

10

0

10

80

82

84

86
88
90
92
94
Percent of Maximum Theoretical Load

96

98

100

Figure 12: Comparison of mean flow times for a flexible queueing network with four servers
and four classes.

Figures 11 and 12 present the mean flow times averaged across all problem instances for
every load. Note the log-scale on the y-axes. In Figure 11, we see that the two scheduling
models and the hybrid model create better schedules than Round Robin.
Increasing the system size produces substantially different results. Figure 12 shows that
at lower loads, LPAS obtains the lowest mean flow time. This is in contrast to performing
worse than makespan and the hybrid model at all loads on the smaller system. As the
load increases, the makespan model performance becomes better than that of Round Robin
but still does not become as good as that of LPAS. The hybrid model is able to obtain
performance comparable with LPAS at lower loads and provide the best performance at
very high loads. Thus, the hybrid model maintains robust performance across varying
system loads. Even though the performance of the hybrid falls short of LPAS at lower
loads, waiting times at low loads are almost negligible. At heavy loads where  approaches
 , the hybrid model outperforms all other algorithms by about 17%.
The good performance of the MinFT model is lost in the larger system. At loads of
0.9 and greater, the MinFT model is not able to process jobs quickly enough to dissipate a
build up of jobs. This is empirical confirmation that MinFT is not stable under parameters
where other algorithms are stable, e.g., loads greater than 90% in Figure 12.
To investigate whether the strong performance of the hybrid model is indeed due to
 values and plot the performance
the guidance of the allocation LP values, we alter the mk
in Figure 13. If following the queueing guidance is beneficial, deviating from these values

 . The
should lead to worse schedules. Therefore, we replace all mk
values with 1  mk
560

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

3

10

1

*

mk

*

mk

2

Mean Flow Time

10

1

10

0

10

80

82

84

86
88
90
92
94
Percent of Maximum Theoretical Load

96

98

100

 and 1   
Figure 13: Comparison of mean flow times for the hybrid model guided by mk
mk
for a flexible queueing network with four servers and four classes.

more resources the allocation LP assigns
the fewer will be assigned in this hybrid
P to a class,

model. Although this method allows kK mk > 1, the conceptual goal of scheduling to
avoid the allocation LP solution is still maintained, and the validity of the model is not
compromised since the resource capacity limit is essential only when solving the fluid model.
We find that the incorrectly guided hybrid performs poorly at all loads; at loads above 0.9,
the system size grows rapidly and behaves as if it were unstable. We conclude that using
proper guidance from the allocation LP is crucial.
4.3.1 Performance Analysis
Our numerical results show that the makespan model can be improved by incorporating
queueing analysis. We can understand this result at low loads by considering a system with
two servers and two classes. With equal arrival rates and processing rates 11 = 9, 12 =
 = 0,   = 1,   = 0.5, and
2, 21 = 5, and 22 = 1, solving the allocation LP gives 11
12
21

22 = 0.5. If the system is lightly loaded with only two class 1 jobs present, both having
1
processing times equal to the expected value ( m1
: 19 for server 1 and 15 for server 2), then
makespan will schedule one job on each of the servers. The hybrid model would consider the
 = 0 and depending on the  parameter chosen, could decide to place both jobs
fact that 11
on the second server in an attempt to perform well in the long run. Although the hybrid is
 , this guidance is sufficient for performance improvement over
only partially guided by mk
makespan.
561

fiTerekhov, Tran, Down, & Beck

2

10

Round Robin
LPAS
makespan
Hybrid
1

Mean Variance

10

0

10

1

10

2

10

80

82

84

86
88
90
92
94
Percent of Maximum Theoretical Load

96

98

100

Figure 14: Comparison of class flow time variance for a flexible queueing network with four
servers and four classes.


and so, contrary
At heavy loads, Proposition 1 states that makespan will track mk
to our results, we would expect the makespan and hybrid models to perform similarly.
However, from the time a server becomes busy until it is once again idle, the first few
scheduling periods are expected to have fewer jobs than those in the middle of this time
span, because there have not yet been enough arrivals for significant queues to build up.
Therefore, the first few periods resemble a lightly loaded system. It is not necessarily the
 until a larger queue is formed. These early decisions will
case that makespan will track mk
propagate through to the subsequent periods and affect all later jobs within the busy period.
The hybrid model, from the start, is able to make better long-run decisions. Although the
hybrid model does not guarantee that it can always make the best choice, we speculate that
it does increase the probability that these better choices are made.

In the instances with four servers and four job classes, LPAS is better than the hybrid
at low loads and worse at high loads. Deeper analysis shows that this performance pattern
is largely due to the poor performance of the hybrid policy at low loads. At such loads,
the hybrid will make assignments that are inefficient in the long term as there are few jobs
in the system. We argued above that the hybrid model is making better assignments than
makespan because it is guided by the queueing analysis. However, a policy that maintains a
 values, such as LPAS and Round Robin, reserves servers for
more strict adherence to the mk
job classes that are efficient in the long term. Even though the hybrid model incorporates

the mk
values in the decision-making process, it is not guaranteed at low loads that the
562

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

 . This is especially
model will always make globally optimal decisions that adhere to mk
true when  is chosen to be high.
The hybrid model outperforms LPAS at heavy loads because optimizing the makespan
for each period has long-run impact. At heavy loads the hybrid model is able to both match
 values and minimize makespan, whereas Round Robin can only do the former and
the mk
LPAS cannot guarantee either. If the hybrid model reduces the makespan by t time units
compared to LPAS or Round Robin, all jobs in the next period will be able to start t time
units earlier and thereby have a net effect of reducing the mean flow time by approximately
t for the subsequent set. The reductions will propagate to subsequent periods until a server
is once again idle.
The propagation effect from reducing the makespan of an earlier period is a property
of the hybrid model that is inherited from the makespan model. As such, we can expect
makespan to exhibit this behaviour as well. Our numerical results show, as expected, the
performance of makespan is good at heavy loads. The makespan model only lacks the
queueing guidance to ensure even higher quality schedules earlier in a busy period. We
expected similar results from the makespan model in the dynamic flowshop problem of
Section 3 as well. However, the experimental results from Section 3 show our assumption
to be incorrect. We conjecture that one of the reasons why we see better performance with
makespan in the queueing network with flexible servers and not in the dynamic flowshop
is because in the latter the differences between optimal sub-problem makespans and the
makespans found by other methods are not substantial. In the queueing network with
flexible servers, the makespan may change more significantly when using different policies.
Given the complex nature of these systems, further investigation into performance trade-offs
resulting from different system parameters is necessary in future work. We hope that such
an investigation will lead to derivations of general dynamic scheduling principles.

4.3.2 Beyond Mean Flow Time
Though our primary quantity of interest is the mean flow time, variance is also an important
criterion. From the perspective of a customer who has submitted a job for processing, a
high variance indicates that the actual flow time of a given job is unlikely to be accurately
predicted by the mean flow time.
Figure 14 shows that the variance in the mean flow time of each job class for four servers
and four classes when using Round Robin or LPAS is much larger than that of the makespan
and hybrid policies.
We see a substantial difference (note the log scale) among the four algorithms. The
larger variance observed in the queueing models occurs because the policies use less information about the state of the system. The policies may overcompensate to serve a job
class immediately rather than delaying service to achieve a fairer allocation. In contrast,
the scheduling models make use of state information to perform better load balancing and
therefore exhibit lower variance.

5. Discussion
Our motivation for studying the integration of scheduling and queueing was that as the
two areas address similar problems in different ways, their combination in terms of problem
563

fiTerekhov, Tran, Down, & Beck

settings, performance measures and algorithms provides a richer set of domains, goals and
tools. Given the nascent nature of such a study, we looked at two simple problem settings
and demonstrated the following contributions.
1. In both problem settings, we showed that it is possible to establish stability of periodic
scheduling approaches, enhancing the periodic scheduling framework with a guarantee
of stability that has traditionally been available for queueing approaches only. As far
as we are aware, this is the first time that stability has been established for algorithms that use observed job characteristics instead of, or in addition to, stochastic
information. Similarly, as far as we are aware, the concept of stability as it appears in
queueing theory has not been examined by the combinatorial scheduling community.
We believe this is an oversight arising from the areas focus on short-term combinatorics and suggest that, as in queueing theory, stability is an important criterion for
any problem that must deal with a stream of arriving resource requests.
2. In our second problem setting, a flexible queueing network, but not in our first, a
dynamic flowshop, we demonstrated that solution approaches that combine guidance
from long-run stochastic reasoning with short-term combinatorial reasoning can perform better than queueing or scheduling approaches alone. However, the differences
between our two problem settings, as well as other work in more complex problem
domains (Tran, 2011), shows that such a combination is non-trivial and further work
is needed to place such hybrid algorithms on a more formal foundation.
We see a variety of additional ways to integrate ideas from queueing theory and scheduling in the future. For example, we would like to: extend our analysis to more general
dynamic scheduling environments, such as job shops; compare the methods discussed in
this paper with more complex queueing approaches and alternative dynamic scheduling
approaches, such as OSCO; investigate stability of dynamic scheduling approaches such as
OSCO or those based on approximation algorithms (i.e., polynomial time approximation
schemes applied to every sub-problem); and develop more sophisticated hybrid queueing
theory and scheduling models.
A broader direction for future work is to more fully investigate the fact that, for a
number of applications, it is now reasonable to assume that both data about probability
distributions and the actual job characteristics (at arrival time) are available. The best way
to integrate these different data sources is an open question that this paper has begun to
investigate from the perspective of hybridization of existing tools.
5.1 Distributional Assumptions
Our work assumes that, for each queue, the sequences of processing times for all machines and the sequence of inter-arrival times are independent and identically distributed sequences, and that these sequences are also mutually independent. In many AI applications,
such assumptions may not be justified, as the data may be non-stationary (time-dependent)
and/or correlated. Both non-stationary models and simple correlation structures are also
considered in the queueuing theory literature (Prabhu & Zhu, 1989; Massey, 2002; Gupta,
Harchol-Balter, Scheller-Wolf, & Yechiali, 2006; Liu & Whitt, 2012).
564

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

Importantly, we note that the proof of Lemma 1 holds in a non-stationary setting and
that the scheduling methods used in the dynamic flow shop are distribution-independent.
For the flexible queueing network, the majority of analysis is based on the allocation LP,
which is based on the fluid representation of a system. For fluid analysis of stability, the
i.i.d. assumption is not required; instead, it is necessary for the Law of Large Numbers
to hold for the inter-arrival and processing time sequences. Thus, our analysis can be
extended to including correlation structures as long as the Law of Large Numbers holds.
On the algorithmic side, the Round Robin, LPAS and hybrid methods can be adapted to a
non-stationary setting by periodically re-solving the allocation LP and using the updated
  values. In fact, this approach is a special case of a general principle that needs to be
considered in dynamic scheduling problems: the distributions may need to be updated as
new data becomes available. Not surprisingly, in queueing theory as in scheduling, nonstationary situations require periodic approaches.
5.2 Computational Complexity
A substantial difference between periodic scheduling approaches and most traditional queueing theory-based policies is that the scheduling approaches may have to solve NP-hard
problems. This is indeed the case for the completionTime algorithm for the two-machine
dynamic flowshop problem (Section 3) and both the makespan and hybrid algorithms for
the queueing network (Section 4). Our assumption in this work, consistent with that in
related work such as OSCO (Van Hentenryck & Bent, 2006; Mercier & Van Hentenryck,
2007), is that dynamism of the system is slow enough to allow such problem solving. We do
not report run-times for solving our problems because we can make them look arbitrarily
good or bad by making assumptions about the time granularity. For example, if a time
unit in the problem corresponds to one hour, algorithm run-times of 10 or 20 seconds are
unlikely to be significant.
In applications with a finer time granularity, we may not be able to solve these problems
and traditional queueing policies are more appropriate. However, our work does raise the
interesting question of the stability of polynomial-time algorithms that provide approximation guarantees. As our proofs of Theorems 2 and 4 depend on finding optimal makespans,
there does not seem to be an easy generalization.
5.3 Online Scheduling
Online scheduling is an alternative approach to dynamic scheduling problems (Pruhs et al.,
2004), different from both classical combinatorial scheduling and queueing theory. The
online scheduling literature focuses on competitive analysis, proving worst-case bounds on
how much worse deterministic and randomized online algorithms are, compared to a fullinformation algorithm. As with queueing theory, the rigorous mathematical approach of
online scheduling tends to limit the combinatorial structure that is addressed. However,
unlike queueing theory, it is uncommon to assume knowledge of stochastic distributions
from which job arrivals and characteristics are drawn. Indeed, often the results showing
differences between deterministic and randomized online algorithms arise from the analysis
of systems where an adversary has full knowledge of the online algorithm and can manipulate
job characteristics arbitrarily.
565

fiTerekhov, Tran, Down, & Beck

While we have chosen not to include online scheduling in this paper, we believe that it
is important to understand how results and insights of this area can be integrated with our
work in order to obtain an even deeper understanding of dynamic scheduling problems.
5.4 Relevance to AI
Dynamic scheduling requires a series of scheduling and resource allocation decisions to be
made online before future tasks arrive and their characteristics, if any, are known. This is
the challenge of sequential decision making under uncertainty, a problem that has received a
significant amount of attention in AI. Indeed, the requirement for an agent to make decisions
and take actions without full knowledge of the future states of the world would appear to
be a central requirement for an embodied, intelligent agent. Investigation of sequential
decision making in applications of interest to AI include planning and task allocation under
uncertainty (Keller & Eyerich, 2012; Alighanbari & How, 2008), land purchases for the
conservation of endangered species (Xue, Fern, & Sheldon, 2012), and multi-player and
strategy games (Sturtevant, 2008; Balla & Fern, 2009).
Most methodological approaches to such problems in AI rely in some way on Markov
Decision Processes (MDPs) (Puterman, 1994), notably in the area of decision-theoretic
planning (Boutilier, Dearden, & Goldszmidt, 2000). The challenge arising from direct applications of MDPs is the well-known curse of dimensionality, where the state space is so
large that problems cannot be solved. Substantial work has therefore focused on approaches
such as factored MDPs (Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean, & Boutilier,
1998; Boutilier et al., 2000; Guestrin, Koller, & Parr, 2003), approximate dynamic programming (Powell, 2010), Monte Carlo Tree Search (Chaslot, 2011; Browne, Powley, Lucas,
Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, & Colton, 2012; Bellemare, Naddaf,
Veness, & Bowling, 2013) and Online Stochastic Combinatorial Optimization (Van Hentenryck & Bent, 2006).
Queueing theory is another approach to sequential decision making under uncertainty,
one that emphasizes time and resources and one that, to our knowledge, has not been
considered for solving problems studied in AI. Given the concern with time and resources,
dynamic scheduling is a natural problem choice for investigation of the incorporation of
queueing theory into the toolbox of AI techniques. As the richness of decision-making
problems in AI extends to questions of time and resources (e.g., in temporal planning
problems, see Coles, Coles, Fox, & Long, 2012) and, in fact, much of the underlying analysis
of prescriptive queueing theory approaches is founded on MDPs (Stidham & Weber, 1993;
Sennott, 1999; Meyn, 2008), we believe that the application of queueing theory to AI and the
hybridization of queueing and scheduling as proposed in this paper, are promising directions
for fundamental and applied research.

6. Conclusion
In this paper, we considered the combination of long-run stochastic reasoning with shortterm combinatorial reasoning to solve dynamic scheduling problems. More specifically, we
investigated the integration of queueing theory and scheduling in two simple scheduling
problems: a dynamic two-machine flowshop and a flexible queueing network. We provided
both analytical and empirical results.
566

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

Analytically, we demonstrated for both of our problem settings that scheduling approaches that make use of the observed job characteristics, as opposed to stochastic information about job classes, can be proved to be stable. That is, if it is possible to manage
the system such that the number of jobs waiting for processing remains finite, then a combinatorial scheduling algorithm can do so.
Empirically, we demonstrated that for a common scheduling criterion it is possible to
create a hybrid algorithm guided by long-term stochastic reasoning and short-term combinatorial reasoning. Furthermore, such an algorithm can be shown to be stable and to
empirically out-perform pure scheduling and pure queueing approaches. However, this result was shown for only one of our two problem settings, suggesting that we currently do not
have an understanding that will allow us to systematically design such successful hybrids.
We believe that our novel investigation of the integration of problem settings, performance criteria, and algorithms of queueing theory and combinatorial scheduling opens a
number of interesting research directions surrounding approaches to dynamic scheduling
and, more broadly, to sequential decision making under uncertainty.

Acknowledgements
The authors would like to thank the reviewers and the associate editor for their comments,
which helped improve the paper.
Section 3 of this paper is based on previously published workshop and conference papers
(Terekhov, Tran, & Beck, 2010; Terekhov, Tran, Down, & Beck, 2012). The work in Section
4 forms part of a Masters dissertation (Tran, 2011) but has not appeared in any peerreviewed publication.
This research has been supported by the Natural Sciences and Engineering Research
Council of Canada, the Canadian Foundation for Innovation, the Ontario Research Fund,
the Ontario Ministry for Research and Innovation, the Ireland Industrial Development
Agency, Alcatel- Lucent, Microway Inc., IBM ILOG, University of Toronto School of Graduate Studies Doctoral Completion Award, and the Department of Mechanical and Industrial
Engineering at the University of Toronto.

References
Al-Azzoni, I., & Down, D. G. (2008). Linear programming-based affinity scheduling of
independent tasks on heterogeneous computing systems. Parallel and Distributed
Systems, IEEE Transactions on, 19 (12), 16711682.
Alighanbari, M., & How, J. P. (2008). A robust approach to the UAV task assignment
problem. International Journal of Robust and Nonlinear Control, 18, 118134.
Andradottir, S., & Ayhan, H. (2005). Throughput maximization for tandem lines with two
stations and flexible servers. Operations Research, 53 (3), 516531.
Andradottir, S., Ayhan, H., & Down, D. G. (2003). Dynamic server allocation for queueing
networks with flexible servers. Operations Research, 51 (6), 952968.
567

fiTerekhov, Tran, Down, & Beck

Balla, R.-K., & Fern, A. (2009). UCT for tactical assault planning in real-time strategy
games. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI09), pp. 4045.
Baptiste, P., Le Pape, C., & Nuijten, W. (2001). Constraint-based Scheduling. Kluwer
Academic Publishers.
Beck, J. C. (2007). Solution-guided multi-point constructive search for job shop scheduling.
Journal of Artificial Intelligence Research, 29, 4977.
Beck, J. C., Feng, T., & Watson, J. P. (2011). Combining constraint programming and local
search for job-shop scheduling. INFORMS Journal on Computing, 23 (1), 114.
Beck, J. C., & Wilson, N. (2007). Proactive algorithms for job shop scheduling with probabilistic durations. Journal of Artificial Intelligence Research, 28, 183232.
Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence
Research, 47, 253279.
Bent, R., & Van Hentenryck, P. (2007). Waiting and relocation strategies in online stochastic
vehicle routing.. In Proceedings of the 20th International Joint Conference on Artificial
Intelligence (IJCAI07), pp. 18161821.
Bidot, J., Vidal, T., Laborie, P., & Beck, J. C. (2009). A theoretic and practical framework
for scheduling in a stochastic environment. Journal of Scheduling, 12 (3), 315344.
Bolch, G., Greiner, S., de Meer, H., & Trivedi, K. S. (2006). Queueing networks and
Markov chains: modeling and performance evaluation with computer science applications. Wiley-Interscience.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121, 49107.
Bowman, E. (1959). The schedule-sequencing problem. Operations Research, 7 (5), 621624.
Bramson, M. (2008). Stability of queueing networks. Probability Surveys, 5, 169345.
Bramson, M., & Williams, R. J. (2000). On dynamic scheduling of stochastic networks in
heavy traffic and some new results for the workload process. In Proceedings of the
39th IEEE Conference on Decision and Control, Vol. 1, pp. 516521.
Branke, J., & Mattfeld, D. C. (2002). Anticipatory scheduling for dynamic job shop problems. In Proceedings of the ICAPS02 Workshop on On-line Planning and Scheduling,
pp. 310.
Branke, J., & Mattfeld, D. C. (2005). Anticipation and flexibility in dynamic scheduling.
International Journal of Production Research, 43 (15), 31033129.
Browne, C., Powley, E., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D.,
Samothrakis, S., & Colton, S. (2012). A survey of Monte Carlo tree search methods.
IEEE Transactions on Computational Intelligence and AI in Games, 4 (1), 149.
Brucker, P., Jurisch, B., & Sievers, B. (1994). A branch and bound algorithm for the
job-shop scheduling problem. Discrete Applied Mathematics, 49 (1), 107127.
Burke, P. (1956). The output of a queuing system. Operations Research, 4 (6), 699704.
568

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

Burns, E., Benton, J., Ruml, W., Yoon, S., & Do, M. B. (2012). Anticipatory on-line planning. In Proceedings of the Twenty-Second International Conference on Automated
Planning and Scheduling (ICAPS12), pp. 333337.
Carlier, J., & Pinson, E. (1989). An algorithm for solving the job-shop problem. Management Science, 35 (2), 164176.
Chaslot, G. M. J.-B. (2011). Monte-Carlo Tree Search. Ph.D. thesis, Universeteit Maastricht.
Chen, H., & Yao, D. D. (1992). A fluid model for systems with random disruptions. Operations Research, 41 (2), 239247.
Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2012). COLIN: Planning with continuous
linear numeric change. Journal of Artificial Intelligence Research, 44, 196.
Conway, R. W., Maxwell, W. L., & Miller, L. W. (1967). Theory of Scheduling. AddisonWesley.
Crabill, T., Gross, D., & Magazine, M. (1977). A classified bibliography of research on
optimal design and control of queues. Operations Research, 25 (2), 219232.
Dai, J. G. (1995). On positive Harris recurrence of multiclass queueing networks: A unified
approach via fluid limit models. The Annals of Applied Probability, 5 (1), 4977.
Dai, J. G., & Meyn, S. P. (1995). Stability and convergence of moments for multiclass
queueing networks via fluid limit models. IEEE Transactions on Automatic Control,
40 (11), 18891904.
Dai, J. G., & Weiss, G. (1996). Stability and instability of fluid models for reentrant lines.
Mathematics of Operations Research, 21 (1), 115134.
Daniels, R., & Carrillo, J. (1997). -robust scheduling for single-machine systems with
uncertain processing times. IIE Transactions, 29, 977985.
Davenport, A., Gefflot, C., & Beck, J. C. (2001). Slack-based techniques for robust schedules.
In Proceedings of the Sixth European Conference on Planning (ECP-2001).
Down, D., & Meyn, S. (1994). A survey of Markovian methods for stability of networks.
In 11th International Conference on Analysis and Optimization of Systems: Discrete
Event Systems, pp. 490504. Springer.
Down, D. G., & Karakostas, G. (2008). Maximizing throughput in queueing networks with
limited flexibility. European Journal of Operational Research, 187 (1), 98112.
El-Bouri, A., Balakrishnan, S., & Popplewell, N. (2008). Cooperative dispatching for minimizing mean flowtime in a dynamic flowshop. International Journal of Production
Economics, 113 (2), 819833.
Fox, M. S. (1987). Constraint-directed Search: A Case Study of Job-Shop Scheduling.
Morgan-Kaufmann Publishers Inc.
French, S. (1982). Sequencing and Scheduling: An Introduction to the Mathematics of the
Job-shop. Ellis Horwood.
Gross, D., & Harris, C. (1998). Fundamentals of Queueing Theory. John Wiley & Sons,
Inc.
569

fiTerekhov, Tran, Down, & Beck

Guestrin, C., Koller, D., & Parr, R. (2003). Efficient solution algorithms for factored MDPs.
Journal of Artificial Intelligence Research, 19, 399468.
Gupta, V., Harchol-Balter, M., Scheller-Wolf, A., & Yechiali, U. (2006). Fundamental characteristics of queues with fluctuating load. ACM SIGMETRICS Performance Evaluation Review, 34 (1), 203215.
Herroelen, W., & Leus, R. (2005). Project scheduling under uncertainty: Survey and research
potentials. European Journal of Operational Research, 165 (2), 289306.
Jackson, J. (1957). Networks of waiting lines. Operations Research, 5 (4), 518521.
Keller, T., & Eyerich, P. (2012). PROST: Probabilistic planning based on UCT. In Proceedings of the 22nd International Conference on Automated Planning and Scheduling
(ICAPS12), pp. 119127.
Kendall, D. G. (1953). Stochastic processes occurring in the theory of queues and their
analysis by the method of the imbedded Markov chain. The Annals of Mathematical
Statistics, 24 (3), 338354.
Kovacs, A., & Beck, J. C. (2011). A global constraint for total weighted completion time
for unary resources. Constraints, 16 (1), 100123.
Kumar, P. R. (1994). Scheduling semiconductor manufacturing plants. IEEE Control Systems Magazine, 14 (6), 3340.
Leon, V. J., Wu, S. D., & Storer, R. H. (1994). Robustness measures and robust scheduling
for job shop. IIE Transactions, 26 (5), 3243.
Liu, Y., & Whitt, W. (2012). The Gt /GI/st + GI many-server fluid queue. Queueing
Systems, 71 (4), 405444.
Manne, A. (1960). On the job-shop scheduling problem. Operations Research, 8 (2), 219223.
Massey, W. A. (2002). The analysis of queues with time-varying rates for telecommunication
models. Telecommunication Systems, 21 (24), 173204.
Mercier, L., & Van Hentenryck, P. (2007). Performance analysis of online anticipatory
algorithms for large multistage stochastic integer programs. In Proceedings of the
20th International Joint Conference on Artificial Intelligence, pp. 19791984. Morgan
Kaufmann Publishers Inc.
Meuleau, N., Hauskrecht, M., Kim, K. E., Peshkin, L., Kaelbling, L. P., Dean, T., &
Boutilier, C. (1998). Solving very large weakly coupled Markov decision processes. In
Proceedings of the 15th National Conference on Artificial Intelligence (AAAI98).
Meyn, S. P. (2008). Control Techniques for Complex Networks. Cambridge University Press.
Nowicki, E., & Smutnicki, C. (1996). A fast taboo search algorithm for the job shop problem.
Management Science, 42 (6), 797813.
Nowicki, E., & Smutnicki, C. (2005). An advanced tabu algorithm for the job shop problem.
Journal of Scheduling, 8, 145159.
Ouelhadj, D., & Petrovic, S. (2009). A survey of dynamic scheduling in manufacturing
systems. Journal of Scheduling, 12 (4), 417431.
570

fiIntegrating Queueing and Scheduling for Dynamic Scheduling Problems

Park, B. Y. (1988). An evaluation of static flowshop scheduling heuristics in dynamic
flowshop models via a computer simulation. Computers & Industrial Engineering,
14 (2), 103112.
Petrick, R. P. A., & Foster, M. E. (2013). Planning for social interaction in a robot bartender
domain. In Proceedings of the 23rd International Conference on Automated Planning
and Scheduling, pp. 389397.
Pinedo, M. L. (2003). Scheduling: Theory, Algorithms, and Systems (2nd edition). PrenticeHall.
Powell, W. (2010). Merging AI and OR to solve high-dimensional stochastic optimization
problems using approximate dynamic programming. INFORMS Journal on Computing, 22 (1), 217.
Prabhu, N. U., & Zhu, Y. (1989). Markov-modulated queueing systems. Queueing Systems,
5 (13), 215245.
Pruhs, K., Sgall, J., & Torng, E. (2004). Online scheduling. In Leung, J. Y.-T. (Ed.),
Handbook of Scheduling: Algorithms, Models and Performance Analysis, chap. 15.
CRC Press.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Reich, E. (1957). Waiting times when queues are in tandem. The Annals of Mathematical
Statistics, 28 (3), 768773.
Ross, S. M. (2003). Introduction to Probability Models, chap. 6  Continuous-Time Markov
Chains, pp. 349399. Academic Press.
Sarper, H., & Henry, M. C. (1996). Combinatorial evaluation of six dispatching rules in a
dynamic two-machine flow shop. Omega, 24 (1), 7381.
Sennott, L. I. (1999). Stochastic Dynamic Programming and the Control of Queueing Systems. John Wiley & Sons, Inc.
Sotskov, Y. N., Sotskova, N. Y., Lai, T.-C., & Werner, F. (2010). Scheduling Under Uncertainty: Theory and Algorithms. Belorussian Science.
Stidham, Jr., S., & Weber, R. (1993). A survey of Markov decision models for control of
networks of queues. Queueing Systems, 13, 291314.
Sturtevant, N. (2008). An analysis of UCT in multi-player games. In Proceedings of the
Sixth International Conference on Computers and Games (CG2008), pp. 3749.
Tadj, L., & Choudhury, G. (2005). Optimal design and control of queues. Sociedad de
Estadstica e Investigacion Operativa, Top, 13 (2), 359412.
Taillard, E. (1990). Some efficient heuristic methods for the flow shop sequencing problem.
European Journal of Operational Research, 47 (1), 6574.
Terekhov, D. (2013). Integrating Combinatorial Scheduling with Inventory Management and
Queueing Theory. Ph.D. thesis, Department of Mechanical and Industrial Engineering,
University of Toronto.
571

fiTerekhov, Tran, Down, & Beck

Terekhov, D., Beck, J. C., & Brown, K. N. (2009). A constraint programming approach for
solving a queueing design and control problem. INFORMS Journal on Computing,
21 (4), 549561.
Terekhov, D., Tran, T. T., & Beck, J. C. (2010). Investigating two-machine dynamic flow
shops based on queueing and scheduling. In Proceedings of ICAPS10 Workshop on
Planning and Scheduling Under Uncertainty.
Terekhov, D., Tran, T. T., Down, D. G., & Beck, J. C. (2012). Long-run stability in dynamic scheduling. In Proceedings of the 22nd International Conference on Automated
Planning and Scheduling (ICAPS12), pp. 261269.
Towsley, D., & Baccelli, F. (1991). Comparisons of service disciplines in a tandem queueing
network with real time constraints. Operations Research Letters, 10 (1), 4955.
Tran, T. T. (2011). Using queueing analysis to guide combinatorial scheduling in dynamic
environments. Masters thesis, Department of Mechanical and Industrial Engineering,
University of Toronto.
Van Hentenryck, P., & Bent, R. (2006). Online Stochastic Combinatorial Optimization.
MIT Press.
Widmer, M., & Hertz, A. (1989). A new heuristic method for the flow shop sequencing
problem. European Journal of Operational Research, 41 (2), 186193.
Wierman, A., Winands, E., & Boxma, O. (2007). Scheduling in polling systems. Performance Evaluation, 64, 10091028.
Wu, C. W., Brown, K. N., & Beck, J. C. (2009). Scheduling with uncertain durations:
Modeling -robust scheduling with constraints. Computers & Operations Research,
36 (8), 23482356.
Xia, C. H., Shanthikumar, J. G., & Glynn, P. W. (2000). On the asymptotic optimality of
the SPT rule for the flow shop average completion time problem. Operations Research,
48 (4), 615622.
Xue, S., Fern, A., & Sheldon, D. (2012). Scheduling conservation designs via network cascade
optimization. In Proceedings of the 26th AAAI Conference on Artificial Intelligence
(AAAI12), pp. 391397.
Yoon, S. W., Fern, A., Givan, R., & Kambhampati, S. (2008). Probabilistic planning via
determinization in hindsight. In Proceedings of the 23rd AAAI Conference on Artificial
Intelligence (AAAI08), pp. 10101016.

572

fiJournal of Artificial Intelligence Research 50 (2014) 639696

Submitted 02/14; published 07/14

Planning through Automatic Portfolio Configuration:
The PbP Approach
Alfonso Emilio Gerevini
Alessandro Saetti

alfonso.gerevini@unibs.it
alessandro.saetti@unibs.it

Dipartimento di Ingegneria dellInformazione
Universita degli Studi di Brescia
Via Branze 38, I-25123 Brescia, Italy

Mauro Vallati

m.vallati@hud.ac.uk

School of Computing and Engineering
University of Huddersfield
Huddersfield, West Yorkshire, HD1 3DH, UK

Abstract
In the field of domain-independent planning, several powerful planners implementing
different techniques have been developed. However, no one of these systems outperforms
all others in every known benchmark domain. In this work, we propose a multi-planner
approach that automatically configures a portfolio of planning techniques for each given
domain. The configuration process for a given domain uses a set of training instances to:
(i) compute and analyze some alternative sets of macro-actions for each planner in the
portfolio identifying a (possibly empty) useful set, (ii) select a cluster of planners, each
one with the identified useful set of macro-actions, that is expected to perform best, and
(iii) derive some additional information for configuring the execution scheduling of the
selected planners at planning time. The resulting planning system, called PbP (Portfoliobased Planner), has two variants focusing on speed and plan quality. Different versions of
PbP entered and won the learning track of the sixth and seventh International Planning
Competitions. In this paper, we experimentally analyze PbP considering planning speed
and plan quality in depth. We provide a collection of results that help to understand PbPs
behavior, and demonstrate the effectiveness of our approach to configuring a portfolio of
planners with macro-actions.

1. Introduction
During the last fifteen years, the field of automated plan generation has achieved significant
advancements, and several powerful domain-independent planners are today available, e.g.,
for propositional planning, FF (Hoffmann & Nebel, 2001), LPG (Gerevini, Saetti, & Serina,
2003), SGPlan (Chen, Hsu, & Wah, 2006), Fast Downward (Helmert, 2006), and LAMA
(Richter & Westphal, 2010). Moreover, while each of such systems performs very well on a
(more or less large) class of planning domains and problems, it is well-known that no one
outperforms all the others in every available benchmark domain (see, e.g., Roberts & Howe,
2009). It would then be useful to have a multi-planner system that automatically selects
and combines the most efficient planner(s) in a portfolio for each given domain.
c
2014
AI Access Foundation. All rights reserved.

fiGerevini, Saetti, & Vallati

The performance of the current planning systems is typically affected by the structure
of the search space, which depends on the considered planning domain. For many domains,
the planning performance can be improved by exploiting some knowledge about the domain
structure that is not explicitly given as part of the input formalization, but that can be
automatically derived from it. In particular, several approaches encoding additional knowledge in the form of macro-actions have been proposed (e.g., Botea, Enzenberger, Muller,
& Schaeffer, 2005; Newton, Levine, Fox, & Long, 2007). A macro-action (macro for short)
is a sequence of actions that can be planned at one time like a single action. When using
macros there is an important tradeoff to consider. While their use can speedup the planning
process, because it reduces the number of search steps required to reach a solution, it also
increases the search space size, which could slow down the planning process. Moreover,
it is known that the effectiveness of macros can depend on the planning algorithm: a set
of macros can increase the performance of a planner, but decrease it, or be irrelevant, for
another.
In this paper, we propose an approach to automatically configuring a portfolio of existing
planners, possibly using a useful set of macros for each of them. The configuration relies on
a statistical analysis of the performance of the planners in the portfolio and the usefulness of
some automatically generated sets of macros, considering a set of training problem instances
in a given domain. The configuration knowledge that is automatically generated by this
analysis consists of a cluster of planners defined by: an ordered subset of the planners in
the initial portfolio, which at planning time are combined using a round-robin strategy; a
set of useful macros for each planner; and some sets of planning time slots. The planning
time slots specify the amount of CPU time to be allocated to each planner in the cluster
during planning. The resulting planning system is called PbP (Portfolio-based Planner).
The current implementation of PbP incorporates two systems for the generation of
macros and nine efficient planners, but its architecture is open to consider any other (current
or future) planner as an additional or alternative system. If PbP is used without configuration knowledge, all planners in the portfolio are scheduled (without macros) by a simple
round-robin strategy where some predefined CPU-time slots are assigned to the (randomly
ordered) planners. If PbP is used with the configuration knowledge for the domain under consideration, only the selected cluster of planners (possibly using the relative selected
sets of macros) is scheduled, their ordering favors the planners that during configuration
performed best, and the planning time slots are defined by the computed configuration
knowledge. As for the selection and exploitation of macros in PbP, it is worth noting that
the planners in the portfolio configured by PbP do not necessarily use the macros learned
for them. In the configuration process, the system evaluates each planner in the portfolio
with each set of macros computed for it, as well as with the empty macro set, as if they
were independent planning systems.
PbP has two main variants: PbP.s, focusing on speed, and PbP.q, focusing on plan
quality. A preliminary implementation of PbP.s (Gerevini, Saetti, & Vallati, 2009) entered
the learning track of the sixth international planning competition (IPC6) and was the overall
winner of this competition track (Fern, Khardon, & Tadepalli, 2011).1 More recently, a
1. As observed by the IPC6 organizers, surprisingly, for the IPC6 problems the use of the configuration
knowledge does not considerably speedup this version of PbP.s. The reasons are some implementation bugs concerning both the configuration phase and the planning phase, and the inefficient use of

640

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

revised and optimized version of PbP with both the speed and quality variants entered
the learning track of the seventh competition (IPC7), and it was again the winner of this
competition track (Coles, Coles, Olaya, Celorrio, Lopez, Sanner, & Yoon, 2012).
A large experimental analysis presented in this paper provides a collection of results that
help to understand the performance behavior of PbP and the effectiveness of its portfolio
configuration methods. In particular, the analysis (i) confirms the very good performance
of PbP in the context of the IPC6-7 benchmarks, (ii) compares PbP with other existing
approaches to configure a planner portfolio, (iii) evaluates the accuracy of PbPs approach
to identify an effective cluster of planners and the strength of using a (configured and
unconfigured) multi-planner with respect to a single planner, (iv) investigates the usefulness
of macros in the considered benchmarks, showing that PbP selects useful macro sets, and
(v) examines the execution scheduling configuration of PbP for the selected planners in the
configured portfolio, demonstrating that its default strategy works well compared to other
possible strategies considered in the analysis.
Several ideas and techniques investigated in the context of PbP use or build on previous
work. Besides presenting and evaluating an effective approach to configuring a planner
portfolio, the research presented in this paper corroborates, validates or evaluates some
hunches and empirical studies done by other researchers in planning. In particular, our
experimental analysis confirms that certain sets of macros can be very useful to accelerate
planning speed or improve plan quality (Botea et al., 2005; Coles & Smith, 2007; Newton
et al., 2007) while others are harmful, that diversity of the planning techniques is important
in the construction of an effective planner portfolio, as observed by, e.g., Roberts and Howe
(2009), and that the round-robin scheduling of the planner execution times is a robust
strategy for a planner portfolio (Howe, Dahlman, Hansen, vonMayrhauser, & Scheetz, 1999;
Roberts & Howe, 2006).
The remainder of the paper is organized as follows. Section 2 discusses related work;
Section 3 describes the PbP approach; Section 4 presents the results of our experimental
study; finally, Section 5 gives the conclusions.

2. Related Work
In this section, after a brief presentation of the most prominent work on algorithm portfolio
design in automated reasoning, we describe related work by others on planner portfolio
design in the automated planning, pointing out some important differences between PbP
and the most related work. Other specific differences and similarities will be indicated in
the following sections presenting our technical results.
2.1 Algorithm Portfolio Design in Automated Reasoning
In the field of automated reasoning, the idea of using a portfolio of techniques has been
investigated by several researchers. A prominent example is the work by Gomes and Selman
(2001), who conducted a theoretical and experimental study on the parallel run of stochastic
algorithms for solving computationally hard search problems. Their work shows under what
some Linux shell scripts (evident especially for small or easy problems), which were corrected after the
competition obtaining much better results (Gerevini et al., 2009).

641

fiGerevini, Saetti, & Vallati

conditions running different stochastic algorithms in parallel can give a computational gain
over running multiple copies of the same stochastic algorithm in parallel.
Many papers on algorithm portfolio design concern the definition of models to select
the best algorithm(s) for an instance of a certain problem according to the values of some
predetermined features of the instance (Rice, 1976). For example, algorithm portfolios have
been designed with this aim to solve instances of SAT, MaxSAT, and QBF (Matos, Planes,
Letombe, & Marques-Silva, 2008; Pulina & Tacchella, 2007; Xu, Hutter, Hoos, & LeytonBrown, 2008). SATzilla is a prominent example of an algorithm portfolio designed for SAT
(Xu et al., 2008). SATzilla uses machine learning techniques to build a predictor of the
runtime of a class of SAT solvers. When SATzilla attempts to solve an instance of the SAT
problem, it computes the values of some features of the instance, predicts the performance
of the SAT solvers it incorporates, selects the most promising SAT solvers and order them
accordingly to their predicted performance, and finally runs the selected SAT solvers using
the established ordering and the predicted required CPU times.
Matos et al. (2008) propose an algorithm portfolio solving the MaxSAT problem. The
portfolio computes the values of several features of a given instance of the MaxSAT problem,
estimates the runtime for each solver in the portfolio, and then solves the instance with
the estimated fastest solver. The estimation is done using a (linear) model configured by
performing ridge regression (Marquardt & Snee, 1975). Similarly, Pulina and Tacchella
(2007) study an algorithm portfolio solving the QBF problem. They identify some features
of the QBF problem, and investigate the usage of four inductive models to select the best
solver to use according to the values of the identified features.
2.2 Planner Portfolio Design in Automated Planning
Regarding automated planning, some prominent planners combining one or more algorithms
have been proposed. Blackbox (Kautz & Selman, 1999) can use a variety of satisfiability engines (the initial version also included the Graphplan algorithm), and FF (Hoffmann & Nebel,
2001), LPG (Gerevini et al., 2003; Gerevini, Saetti, & Serina, 2006) and SGPlan5 (Chen et al.,
2006) include a backup strategy using an alternative search technique that is run when
the default method fails. The algorithm combination in these systems is straightforward
and does not use an automatic portfolio configuration.
Previous work on planner portfolios includes the approach proposed by Howe and collaborators (Howe et al., 1999; Roberts & Howe, 2007, 2009; Roberts, Howe, Wilson, &
desJardins, 2008). In the rest of the paper, we will refer to Howe and collaborators approach using the name of their first planner portfolio, BUS (Howe et al., 1999), although
our analysis of this approach will consider their most recent techniques for the planner
portfolio configuration. Their approach learns models of performance for a set of planners.
At planning time, the round-robin policy is used to schedule the runs of the planners in
such a set, and the learned models are exploited to determine the order of the runs. The
configuration-knowledge derived by this approach is domain-independent: the performance
models of the planners are built by using several predictive models of the WEKA data mining
package (Witten & Frank, 2005), and the set of planners forming the portfolio is determined
through a set covering algorithm over the solved training problems across several different
planning domains.
642

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

The work on BUS originally inspired our approach. PbP has some similarities with it, but
it computes and uses very different configuration knowledge, and the methods for selecting
and ordering the portfolio planners are considerably different. The portfolio configuration
of PbP generates domain-optimized clusters of planners, and the selection and ordering of
PbP is based on a statistical analysis of the planners performance over a set of training
problems using the Wilcoxon sign-rank test, also known as the Wilcoxon matched pairs
test (Wilcoxon & Wilcox, 1964).2 Finally, their system does not compute, analyze or use
macros, and does not consider plan quality.
Similarly to the work of Howe et al. (1999), and Roberts and Howe (2007), the techniques used by Cenamor, de la Rosa, and Fernandez (2013), and Fawcett, Vallati, Hutter,
Hoffmann, Hoos, and Leyton-Brown (2014) learn models of performance of a set of planners
according to some predetermined features. In the work of Cenamor et al. (2013), such features are derived from the SAS+ representation of the planning problem. In this approach,
the learned models are used to determine which planners should be run, in which order, and
for how long. The selected planners run sequentially either using an amount of CPU time
uniformly assigned or determined from the predicted execution time. The experimental results in the work of Cenamor et al. (2013) show that for problems in domains different from
those used to learn the models, the configured portfolios perform worse than running an
unconfigured portfolio consisting of all the incorporated planners with uniform CPU time
assigned to each of them.
The work described by Fawcett et al. (2014) is focused on generating models for accurately predicting planners runtime. Such models exploit a large set of instance features,
derived from the PDDL and SAS+ representations of the problem, a SAT encoding of the
planning problem, and (short) runs of planners. The experimental results in the work of
Fawcett et al. (2014) indicate that the generated performance models are able to produce
very accurate runtime predictions.
Fast Downward Stone Soup (here abbreviated FDSS) is an approach to selecting and combining a set of forward-state planning techniques (Helmert, Roger, & Karpas, 2011). Using
the IPC6 scoring function, FDSS evaluates a class of candidate techniques on the basis
of their performance over a set of training problem instances from different domains, and
builds a domain-independent sequential portfolio of forward planners by a hill-climbing algorithm searching a space of possible sequential combinations of the evaluated candidate
techniques. The automatic portfolio configuration in FDSS and PbP aims at building different types of planning systems: a single efficient domain-independent planner portfolio
in FDSS; an efficient domain-optimized portfolio planner for each given domain in PbP.
The configuration processes and the resulting configured portfolios of FDSS and PbP are
significantly different. In particular, PbP configures a portfolio of generic planners (using
different styles of planning), each one with a (possibly empty) set of useful learned macros,
which are not considered in FDSS because of its domain-independent purpose. Moreover,
the execution scheduling strategy of PbP runs the selected planners in round-robin rather
than sequentially.
2. In the context of planning, the Wilcoxon sign-rank test has been previously used also in the work of
Long and Fox (2003), Gerevini, Haslum, Long, Saetti, and Dimopoulos (2009), Gerevini et al. (2009),
and Roberts and Howe (2009).

643

fiGerevini, Saetti, & Vallati

ParLPG (Vallati, Fawcett, Gerevini, Hoos, & Saetti, 2013b) and Fast Downward-autotune
(Fawcett, Helmert, Hoos, Karpas, Roger, & Seipp, 2011) configure the parameters of planners LPG and Fast Downward (Helmert, 2006), respectively, using a set of training problems
of a given domain in order to obtain combinations of parameters for these two planners that
perform especially well in the given domain. Both these frameworks uses the stochastic local
search procedure ParamILS to search for high-performance configurations of parameters by
evaluating promising configurations (Hutter, Hoos, & Stutzle, 2007; Hutter, Hoos, LeytonBrown, & Stutzle, 2009). An extended version of FDSS (Seipp, Braun, Garimort, & Helmert,
2012) involves twenty one configurations of Fast Downward, obtained by configuring its parameters through Fast Downward-autotune for twenty one different domains (Fawcett et al.,
2011), that are combined by several alternative sequential strategies allocating the CPU
times to them.
ASAP (Vallati, Chrpa, & Kitchin, 2013a) is a recent system for selecting the most promising planner from a set of candidates planners that derives much of its power from the use
of entanglements (Chrpa & Bartak, 2009; Chrpa & McCluskey, 2012). Entanglements are
relations between planning operators and predicates used to reformulate the domain model
by removing unpromising operator instances or restricting the applicability of some actions
to certain states. A problem over the resulting modified domain can become significantly
easier to solve for a planner. On the other hand, since ASAP uses an approximate method to
decide entanglements, which is PSPACE-complete (Chrpa, McCluskey, & Osborne, 2012), a
problem that is solvable with the original domain can become unsolvable with the reformulated domain. Given a planning domain modified by entanglements and a set of planners,
ASAP identifies the most promising of these planners as the one with the highest IPC score
(Jimenez, Coles, & Coles, 2011) over a set of training problems.

3. Automated Planner Portfolio Design in PbP
In this section, after introducing some preliminaries defining the problem of configuring a
planner portfolio and its execution to solve planning problems, we describe the architecture
and techniques of our approach to configure and execute a planner portfolio.
3.1 Preliminaries on Configuring and Executing a Planner Portfolio
Differently from most of the existing work on algorithm portfolio design of which we are
aware, PbP does not design the planner portfolio for solving a specific instance of the
planning problem according to the values of some predetermined features of the instance.
Instead, planning problems are gathered according to their planning domains, and the planner portfolio is designed for the whole domain. The basis of this choice is the empirical
observation that often there exists a single planner or a combination of planners that performs generally better for all or most of the problems of a domain. This seems something
peculiar to automated planning that does not hold for other types of reasoning problems,
and it makes PbP somewhat atypical in the general literature on algorithm portfolio design.
Let D be a planning domain, T a CPU-time limit, and P a set of n planners (initial
portfolio), each of which with its predefined parameter values. The problem of configuring
P for D consists of computing a set of triples {hPi , Mi , Si i | i = 1 . . . m}, where: 1  m  n,
Pi  P, Mi is a (possibly empty) set of macro operators learned for Pi in domain D, and Si
644

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

is a sequence of increasing CPU times. These CPU times (real numbers) are called planning
time slots, and are such that each time is lower than or equal to T .
The output set of triples identified by a portfolio configuration algorithm is the configured
(planner) portfolio of P for D, which in the rest of the paper will also be called a selected
planner cluster (or simply cluster). Depending on how planners, macros and planning
time slots are chosen, there can be many candidate solutions to a portfolio configuration
problem. A special case, that we call the unconfigured (planner) portfolio, is defined as
{hPi , , Spre i | i = 1 . . . |P|}, where Spre is predefined as h0.1, 1, 10, 100, 1000i (in seconds).
Like BUS, PbP uses the round-robin policy for scheduling the runs of the planners in the
configured portfolio. Let  = {hPi , Mi , Si i | i = 1 . . . m} be a planner portfolio configured
for a domain D. Portfolio  is executed to solve a planning problem in D by a roundrobin scheduling of m processes where: each process corresponds to running a planner Pi
with macros Mi (Pi + Mi for short), according to an order and time slices derived from
sequences S1...m . More precisely, the circular order of the m planners in  is determined
by considering the m values t1...m defined by the first planning time slot in each of the m
sequences S1 . . . Sm . If ti < tj , Pi is ordered before Pj ; if ti = tj , the relative order of Pi and
Pj is arbitrarily decided (i.e., in this case Pi runs before Pj iff i < j), for every i, j  1 . . . m
with i 6= j. Each planner Pi + Mi is initially run until the total CPU time allocated to this
process is ti , or the planner terminates earlier. If a planner Pi + Mi does not terminate
within the assigned planning time slot ti , then it is suspended, and it resumes the next time
a time slot is assigned to it. No additional CPU time is assigned to those processes that
have already terminated. When, according to the circularity of the order, a planner Pi + Mi
resumes its execution, the total CPU time assigned to it (from the start of its execution)
is equal to the next unprocessed time slot in Si (i.e., the j-th value of Si for the j-th time
Pi + Mi runs).
Figure 1 shows an example of the round-robin scheduling for portfolio {hP1 , M1 ,
h10, 40, 160, . . . ii, hP2 , M2 , h20, 60, 180, . . . ii}, assuming that P1 + M1 terminates after using
80 CPU time units, and P2 + M2 after using 120 CPU time units. P1 + M1 runs before
planner P2 + M2 , because the first time slot of P1 + M1 (i.e., 10) is lower than the first
time slot of P2 + M2 (i.e., 20). The round-robin scheduler suspends P1 + M1 after 10 time
units, and gives P2 + M2 20 time units of CPU time. This process is repeated suspending
P1 + M1 when the total execution of P1 + M1 has consumed 40 time units, and suspending
P2 + M2 when the total execution of P2 + M2 has consumed 60 time units. At the next
iteration, P1 + M1 should be suspended when its total execution time reaches 80 time units,
but, before the end of its third time slot, i.e., at time 140, P1 + M1 terminates and needs
no more CPU time. Then, P2 + M2 resumes its run, and terminates at time 200. In this
example, the planners of the portfolio use only their first three time slots.
Given a set of training problems in a domain D, we propose an approach to configuring
an initial planner portfolio for D through a statistical analysis about the performance of
the planners in the initial portfolio with some alternative sets of computed macros. The
effectiveness of the determined configured portfolios can then be evaluated over a set of test
problems in D, that in our experimental analysis are disjoint from the training problem set
and that, if not specified otherwise, are always formed by known benchmark problems.
The proposed approach is implemented in a planning system called PbP (Portfolio-based
Planner). In the following, depending on the context, PbP will be used to indicate either
645

fiGerevini, Saetti, & Vallati

P1 with M1
P2 with M2
10

30

60

100

140

200

Time

Figure 1: An example of the round-robin scheduling of PbP when running portfolio {hP1 , M1 , h10, 40, 160, . . . ii, {hP2 , M2 , h20, 60, 180, . . . ii} on a given planning
problem, assuming that planner P1 using macros M1 takes a total of 80 CPUtime units to terminate and P2 with M2 takes a total of 120 CPU-time units.

its method for configuring the planner portfolio, or the generated configured portfolio. In
the experimental evaluation of the configured portfolios generated by PbP, as a baseline
planner portfolio, we will use the unconfigured planner portfolio, that will be also called
the unconfigured version of PbP and denoted with PbP-nok (while PbP will indicate the
generated configured planner portfolio).
3.2 Architecture and Components of PbP
The architecture of PbP consists of the following five main components, which are combined
as described in Figure 2.
3.2.1 Macro-Actions Computation
For each integrated planner, PbP computes some sets of alternative macros using the following two approaches.
 Wizard, PhD thesis version (Newton et al., 2007). This system implements three
learning techniques based on offline evolutionary methods, which use genetic operators
to compute macros for a given planner from the plans solving a set of training problem
instances of an input domain. The three learning techniques are called chunking,
bunching, and clumping: chunking learns individual macros from the original domain
operators; bunching learns bunches of macros from a given pool of macros (such
as the macros learned by the chunking process); and clumping learns both individual
macros and sets of macros simultaneously. The learned macros are filtered by a fitness
value. The fitness value reflects some filtering criteria including the number of solved
problems and the CPU time required to solve the training problems using the domain
operators augmented with the learned macros. For each computed macros, if the
fitness value of a macro is lower than a threshold, the macro is discarded. Therefore,
for each planner incorporated into PbP (expect Macro-FF), PbP using Wizard can
generate at most three sets of macros for the planner. In order to determine the sets
of macros to be used in the configured portfolio, the performance of the planner will
then be evaluated by PbP with/without using the sets of learned and filtered macros
over the training problems. This evaluation is performed by the Planner cluster
646

fiPlanning

Planning through Automatic Portfolio Configuration: The PbP Approach

Incorporated Planners:

Domain and
problem to solve

Multi-planner by
round-robin scheduling
Time limit t

Fast-downward (Helmert, 2006)

Cluster of planners with macros

Planning time
Time slots
slots computation

LPG-td (Gerevini, Saetti & Serina, 2006)
Macro-FF (Botea et al., 2005)
Marvin (Coles & Smith, 2007)
Metric-FF (Hoffmann & Nebel, 2005)
SGPLAN5 (Chen, Wah & Hsu, 2006)

Portfolio configuration

LAMA (Richter & Westphal, 2010)

YAHSP (Vidal, 2004)

Solution plan
or failure

Planner cluster
selection & ordering

Performance of planners with macros

Macro-actions
computation
Wizard

Planners with macros

Performance
measurement

MacroFF

Planners

Domain and
training probs

Time limit T

ParLPG (Vallati et al., 2011)

Figure 2: A sketch of PbPs architecture.
selection and ordering component. For simplicity, the sets of learned macros will be
identified by the names of the techniques used to derive them.
 Macro-FF (Botea et al., 2005; Botea, Muller, & Schaeffer, 2007b). The approach
implemented in Macro-FF (Botea et al., 2005) computes macros by analyzing the solutions of a set of training problem instances, so that the macros that appear frequently
and that significantly reduce the required search effort are preferred. In particular,
first Macro-FF solves the training problems using an enhanced version of FF; then
it generates macros by considering the frequency the sequences of actions forming
macros appear in the computed solutions.3 After the macro generation, Macro-FF
solves the training problems using the computed macros, ranks the macros in terms
of the obtained search effort gain, and using this ranking selects at most five sets of
macros M1..5 , where Mi with i = 1..5 is the set of macros formed by the i best learned
macros. The version of the approach integrated into PbP contains the enhancements
described by Botea et al. (2007b). Since the macros learned by Macro-FF are coded
using an ad-hoc language, in PbP the five learned sets of macros M1..5 are used only
by the Macro-FF planner.

3.2.2 Planner Performance Measurement
This is the most expensive computation step in the configuration of the portfolio. PbP
runs each integrated planner expect Macro-FF with and without the three sets of macros
3. In our experiments presented in Section 4, we observed that Macro-FF computes no macros only if the
enhanced version of FF solves no training problem.

647

fiGerevini, Saetti, & Vallati

learned for it by Wizard on the input training problem set, using the input CPU time
limit T for each planner run. Similarly, Macro-FF runs with and without the five sets of
macros learned by itself. The current implementation of PbP incorporates eight well-known
successful planners, Fast Downward (Helmert, 2006), LAMA (Richter & Westphal, 2010),
LPG-td (Gerevini et al., 2006), Macro-FF (Botea et al., 2005, 2007b), Marvin (Coles & Smith,
2007), Metric-FF (Hoffmann, 2003), SGPlan5 (Chen et al., 2006), YAHSP (Vidal, 2004) and
a recent version of LPG (ParLPG) using a dedicated configuration phase to automatically
optimize the setting of a collection of parameters governing the behavior of several parts of
the system (Vallati et al., 2013b). Basically, running ParLPG consists in running LPG using
a domain-specific parameter configuration. Every other incorporated planner runs using its
default parameter configuration. For Marvin, this implies that during planning it can learn
and memorize macros to escape from plateaus. For each run, PbP measures the planner
performance in terms of: number of problems solved within T , CPU time required for solving
the training problems, and quality of the computed solutions. For the incremental planners,
i.e., LPG, ParLPG and LAMA, PbP measures the quality of all the solutions generated for
each problem and the corresponding CPU times. Finally, note that for the macro-actions
computation Macro-FF and Wizard already run the incorporated planners and hence, in
principle, the performance of the planners with macros could be measured when Macro-FF
and Wizard compute them. However, this has some technical difficulties and, for simplicity,
PbP duplicates the runs of the (incorporated) planners.
3.2.3 Planning Time Slots Computation
The method for computing the planning time slots in PbP is a variant of the CPU-time allocation strategy proposed by Roberts and Howe (2007) for the round-robin planner scheduling. Let hp1 , . . . , pn i be a sequence of increasing percentages, and tpi (i  {1, . . . , n}) the
minimum CPU time required by a planner P with a set of macros M learned for it (P + M
for short) in order to solve a percentage of training problems equal to pi . During PbPs
configuration of the planner portfolio, the planning time slots S of P + M are defined as
S = htp1 , . . . , tpn i.
The difference between the planning time slots in PbP and in the approach of Roberts
and Howe can be explained by the following example. Assume that the computed planning
time slots for planner A using macros MA (A+MA ) are h0.20, 1.40, 4.80, 22.50, . . . i and that
those for planner B using macros MB (B + MB ) are h14.5, 150.8, . . . i. Then, for this pair
of planners, differently from the approach of Roberts and Howe, PbP extends the first time
slot for A + MA (0.20) to 4.80, i.e., to the greatest time slot of A + MA which is smaller than
the first time slot of B + MB ; similarly for the subsequent time slots. If the first time slot
of A + MA were not extended, the slowest planner B + MB would initially run for a CPU
time much greater than the CPU time initially assigned to the fastest planner A + MA ,
and for many problems that planner A + MA quickly solves (e.g., using one CPU second),
PbP would perform much slower. It is worth noting that using this time slot extension we
observed a high performance gain only for small and easy problems.
In the rest of the paper, the sequence of increasing percentages hp1 , ..., pn i used to define
the planning time slots is called the problem coverage percentage vector (PCPV). The default
648

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

PCPV in PbP is the sequence h25, 50, 75, 80, 85, 90, 95, 97, 99i (n = 9), which is the same
used in the work of Roberts and Howe (2007).
3.2.4 Planner Cluster Selection and Ordering
This is the last step of the configuration process of PbP. PbP selects a cluster of planners
in the initial portfolio (as described in Section 3.3), each one with a (possibly empty) set
of useful macros, according to the measured performance and the computed planning time
slots.
As for the macro selection, note that PbP has no explicit independent mechanism for
selecting the macros to be used in the configured portfolio, and that macros are not shared
between planners because the tools used to learn them (Wizard and Macro-FF) generate
macro sets for a specific input planner. Planners and their macro sets are selected together,
since the planner cluster selection of PbP considers a candidate planner using two different
sets of macros learned for it as two different candidate planners.
The execution order of the planners in the selected cluster is implicitly defined by the
increasing first planning time slots associated with the planners. Section 3.3 describes the
planner cluster selection in detail.
3.2.5 Multi-Planner by Round-Robin Scheduling
After PbP has configured the planner portfolio for the domain under consideration, when
a problem instance of this domain is encountered, PbP runs the selected ordered cluster
of planners (each one using the relative selected set of macro-actions) by a round-robin
scheduling algorithm using the computed planning time slots, that is similar to the one investigated in many portfolio algorithms (see, e.g., Howe et al., 1999; Roberts & Howe, 2006,
2007). Alternative planner scheduling strategies are possible, such as sequential execution
or/and using configured planning time slots. However, according to the experimental results
that will be presented in Section 4.8, the default round-robin strategy with the planning
time slots derived from the default PCPV is robust and performs generally well. Concerning
termination of the resulting multi-planner, PbP.s terminates if either a given (execution)
CPU-time limit t is exceeded, returning failure, or one among the selected planners computes a solution (output of PbP.s); PbP.q terminates if either time t is exceeded, or all
the selected planners terminate. If PbP.q generates no solution within t, it returns failure;
otherwise, it returns the best computed solution.
3.3 Selecting a Planner Cluster
After the performance measurement and time slot computation phases, PbP analyzes the
obtained results to identify the best cluster of planners and macros for the domain under
consideration and the given CPU-time limit T . This is done by simulating, for every cluster
C of at most k planners, each with a (possibly empty) set of macros, the round-robin execution of the planners in C for solving the same training problems used in the performance
measurement phase.4 The simulation is done using the data from the runs conducted for
4. In our experiments parameter k is set to 3. If k were greater than 3, we experimentally observed that
for the considered benchmark domains and problems the cluster selected by PbP would be the same.
The maximum number of possible combinations between the planners currently incorporated into PbP

649

fiGerevini, Saetti, & Vallati

the performance measurement phase (the planners are not re-run), ignoring the data of the
planners that always perform worse than another incorporated planner (i.e., any planner
that performs worse than another one across all the training problems of the domain is
discarded). The CPU-time limit for each simulated execution of a cluster is T (the same
time given to each run of a single planner during the performance measurement phase). The
performances of the simulated cluster runs are compared by a statistical analysis based on
the Wilcoxon sign-rank test (Wilcoxon & Wilcox, 1964). This test applies to a set of paired
observations (a sample from a larger population), and tells us if it is plausible to assume
that there is no correlation between the pairwise observed quantities. In our case, these
paired observations are, e.g., the simulated runtimes of two clusters on the same training
problem instance, and no correlation between them means it is equally likely that we will
see one cluster solving a problem faster than the other as it is that we will see the opposite
on a sample of problems.
For our purposes, the Wilcoxon sign-rank test is appropriate because it does not require
us to know the sample distribution, and makes no assumption about this distribution. That
is, we have no way to know a priori how hard a planning problem is, and hence we have
no distribution of the simulated performance of the clusters. Consequently, as stated by
Gibbons and Chakraborti (2003), it is critical that we use a non-parameterized test, such
as the Wilcoxon sign-rank test. We have also investigated the usage of other methods to
compare the performance of the simulated runs of planner clusters, including the IPC score
function that was also used by Vallati et al. (2013a). However, we experimentally observed
that, for the IPC7 domains, such a method is less effective than the usage of the Wilcoxon
sign-rank test.
In PbP, the performance measure considers either the CPU time (PbP.s) or the plan
quality (PbP.q). The data for carrying out the test in PbP.s are derived as follows. For
each planning problem, the system computes the difference between the simulated execution
times of the compared clusters. If a planner cluster does not solve a problem, the corresponding simulated time is twice the CPU-time limit;5 if no cluster solves the problem,
this problem is not considered. The difference between the simulated times is normalized
by the value of the best simulated time under comparison (e.g., if cluster C1 requires 200
seconds and cluster C2 220, then the difference is 10% in favor of C1 ). The absolute values
of these differences are then ranked by increasing numbers, starting from the lowest value.
(The lowest value is ranked 1, the next lowest value is ranked 2, and so on.) The ranks
of the positive differences and the ranks of the negative differences are summed, yielding
two values r+ and r , respectively. If the performance of the two compared clusters is not
significantly different, then the number of the positive differences r+ is approximately equal
to the number of the negative differences r , and the sum of the ranks in the set of the
positive differences is approximately equal to the sum of the ranks in the other set. Intuitively, the test considers a weighted sum of the number of times a cluster performs better
and the considered sets of macros
hence, with k = 3, the maximum number of clusters that can be
P is 38;
38
evaluated by run simulation is i=k
= 9177. This is the number of clusters with at most 3 different
i=1
i
combinations of planners and macros over the 38 in the current implementation.
5. This is the minimum value that ensures the performance gap for a problem solved by one cluster of
planners and unsolved by the other compared cluster is bigger than the performance gap for any problem
solved by both the compared clusters.

650

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

than the other compared one. The sum is weighted because the test uses the performance
gap to assign a rank to each performance difference.
When the number of samples is sufficiently large, the T-distribution used by the Wilcoxon
sign-rank test is approximately a normal distribution, which is characterized by two parameters called the z-value and the p-value. The higher the z-value, the more significant the
difference of the performance is. The p-value represents the level of significance in the
performance gap. If the p-value is greater than 0.05, then the null hypothesis that the
performance of the compared pair of planners is statistically similar is refused, and the alternative hypothesis that their performance is statistically different is accepted. Otherwise,
there is no statistically significant evidence that they perform differently, and PbP considers
that they perform pretty much similarly.
The results of the Wilcoxon sign-rank test are used to form a directed graph where
the nodes are the compared clusters, and an edge from a cluster C1 to another cluster C2
indicates that C1 performs better than C2 . Such a graph has already been used by Long
and Fox to present the results of the 3rd International Planning Competition (Long & Fox,
2003). Each strongly connected component of this graph is collapsed into a single node
representing the elements in the clusters of the collapsed nodes. From the resulting DAG,
PbP considers only the nodes without incoming edges (the graph root nodes). If there is
only one root node, this is the selected cluster, otherwise PbP uses some secondary criteria
to select the most promising cluster among the root nodes. These criteria are the number of
solved problems, the sums of the ratios between the (simulated) CPU times of the planners
in the compared clusters, and the first planning CPU-time slots of the involved planners.
Specifically, PbP selects the cluster among the root nodes such that its simulation solves the
highest number of training problems. To break the ties, for every pair of selected clusters x
|sx sy |
and y PbP computes the ratio max{s
, where sx and sy are the sums of the (simulated)
x ,sy }
CPU times of clusters x and y, respectively; if such a ratio is greater than threshold 0.05,
the compared cluster with the worst sum of CPU times is discarded. If the number of
remaining clusters is still greater than one, PbP selects the cluster with the lowest first
planning CPU-time slots of the involved planners. Finally, the remaining ties are broken by
selecting the cluster randomly, but in our experiments no cluster has ever been randomly
selected.
The method used to select a cluster of planners and macros in PbP.q is similar, but it
applies to the plan qualities resulting from the cluster execution simulation, rather than to
the CPU times as done by PbP.s. For this simulation, PbP.q considers also the intermediate
solutions (i.e., those that are generated before the last one, which has the best quality)
and the relative CPU times computed by the basic incremental planners in the considered
clusters. If these solutions were ignored, the simulated plan quality for the clusters including
incremental planners could be much worse than the actual quality. For example, assume
that the CPU-time limit is 900 seconds, FF computes a solution with quality 50 using 100
seconds, LAMA computes two solutions with quality 20 and 19 using 120 and 880 seconds,
respectively. If the intermediate solutions of LAMA were ignored, the estimated plan quality
for the cluster formed by planners FF and LAMA would be equal to the quality of the plan
generated by FF (the second solution generated by LAMA could be computed by the cluster
using 980 seconds, but this is greater than the CPU time limit), although the intermediate
(first) solution of LAMA is much better than the FFs solution.
651

fiGerevini, Saetti, & Vallati

Finally, note that if the performance of the incorporated planners is measured with
CPU-time limit T , then the portfolio of PbP.s/q can be (re)configured for any time limit
t  T by simply ignoring the solutions computed after time t in the simulation of the
planner cluster performance. If t  T , then t  T is equally distributed among the planners
in the selected cluster. If a planner terminates before its allocated time, the remaining time
is also equally distributed to the other planners that are still running.
3.4 The Integrated Basic Planners
In this subsection, we give a very brief description of each of the nine basic planners that
are currently incorporated in PbP. Much more detailed information is available from the
corresponding referred papers.
Metric-FF (Version 2.1; Hoffmann, 2003). Metric-FF inherits the main ideas used in FF
(Hoffmann & Nebel, 2001). FFs search strategy is a variation of hill-climbing over the space
of the world states, and in FF the goal distance is estimated by solving a relaxed task for
each successor world state. Compared to the first version of FF, Metric-FF is enhanced with
goal orderings pruning techniques and with the ordering knowledge provided by the goal
agenda. Moreover, it deals with level 2 of pddl2.1 (Fox & Long, 2003), i.e., numerical state
variables, numerical action preconditions and effects.
YAHSP (Version 1.1; Vidal, 2004). YAHSP extends the search procedure of FF with some
information extracted from FFs relaxed plan. For each evaluated world state, YAHSP
exploits a look-ahead strategy in a complete best-first search by employing actions from the
relaxed plans in order to find the beginning of a valid plan that can lead to a reachable
world state.
MacroFF (Botea et al., 2005, 2007b). Macro-FF extends FF with support for using macrooperators during the search, and with engineering enhancements. One of the main features
of the planner version integrated into PbP is the use of iterative macros (Botea et al., 2007b),
i.e., runtime combinations of macro operators, which are instantiated by attempting to use
as many actions from FFs relaxed plan as possible. In the search procedure of FF, the
iterative macros that can be successfully instantiated are considered for the generation of
the next world states.
Marvin (Release 1; Coles & Smith, 2007). Marvin is another planner based on FF. The
main improvement w.r.t. FF is memorizing the plateau-escaping action sequences discovered
during the (local) search of FF. These action sequences form macros, which can be applied
later when plateaus are once-again encountered by FFs search in order to escape from these
plateaus quickly.
SGPlan (Version 5.22; Chen et al., 2006) with domain-modification script (Coles & Coles,
2011). SGPlan5 exploits a partition-and-resolve strategy to partition the mutual-exclusion
constraints of a planning problem by its subgoals into subproblems, solves the subproblems individually using a modified version of the Metric-FF planner, and resolves those
violated global constraints iteratively across the subproblems. It has been observed that
the performance of SGPlan are affected by some rules detecting the domain name and the
number of domain operators (Coles & Coles, 2011). In our work, we intend to consider the
652

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

available implemented systems that have chances to perform well (possibly in combination
with others) for at least one domain over a range of varied existing benchmark domains.
SGPlan is definitely one of these systems. However, in order to prevent the usage of the
domain-specific detection rules in SGPlan that, differently from the other planners incorporated in PbP, would make SGPlan domain-specific for some domains, we have induced
SGPlan to behave domain-independently by a domain modification script, as proposed by
Coles and Coles (2011). Such a script changes the domain name, adds a never-applicable
action to the domain, and then runs SGPlan over the obtained domain. In addition, our
domain-modification script also changes the names of domain operators.
Fast Downward (Version 1.0.1; Helmert, 2006). Fast Downward (abbreviated with FD)
translates the input pddl problem specification into its multi-valued state variable representation SAS+ (Backstrom & Nebel, 1995), and searches for a plan in the space of the
world states using a heuristic derived from the causal graph, a particular graph representing
the causal dependencies of SAS+ variables. PbP integrates the 2006 version of the planner.
The main improvement compared to the earlier version of the planner that in 2004 won the
propositional satisficing track of IPC4 is the addition of safe abstraction, a form of problem
simplification that allows the planner to solve certain kinds of simple problems without
search.
LAMA (Version 2008; Richter & Westphal, 2010). LAMA is built on Fast Downward, using
SAS+ state variables and multi-heuristic search. Its core feature is the use of a pseudoheuristic derived from landmarks, propositions that must be true in every solution of a
planning task. Moreover, a weighted A? search is used with iteratively decreasing weights,
so that the planner continues to search for plans of better quality.
LPG-td (Gerevini et al., 2006). LPG-td inherits the main ideas used in LPG (Gerevini et al.,
2003). LPG uses stochastic local search in a space of partial plans represented through action
graphs. The search steps are certain graph modifications transforming an action graph into
another one. LPG-td includes more accurate heuristics for selecting the graph modifications
than those in LPG.
ParLPG (Version IPC7; Vallati et al., 2013b). ParLPG is a recent system based on the
idea of automatically configuring a generic, parameterized planner using a set of training
planning problems in order to obtain speed-optimized planners that perform especially well
for the domains of these problems. ParLPG uses the FocusedILS variant of the off-the-shelf,
state-of-the-art automatic algorithm configuration procedure ParamILS (Hutter et al., 2007,
2009), and the planning system LPG (ver. 1.0), which has several components that can be
configured very flexibly via many exposed configurable parameters.

4. Experimental Analysis
In this section, we present the results of a large experimental study about PbP with the
following main goals:
(G1) describing the configured portfolios and analyzing the configuration process of PbP
(Section 4.2);
653

fiGerevini, Saetti, & Vallati

(G2) analyzing the efficiency of PbP.s/q in terms of speed and plan quality in the context
of the planning competitions IPC6-7 (Section 4.3);
(G3) comparing the performance of the planner portfolio configured by PbP.s/q versus other
planning systems based on planner portfolios (Section 4.4);
(G4) evaluating the effectiveness of using the (automatically computed) domain-specific
configuration knowledge in PbP.s/q (Section 4.5);
(G5) comparing the performance of the planner portfolio configured by PbP.s/q versus the
single basic planners in the portfolio, and evaluating the accuracy of the planner
cluster selection in PbP.s/q (Section 4.6);
(G6) analyzing which kind of macros is selected by PbP for the planners in the configured
portfolio, evaluating the effectiveness of using the selected macro set, and understanding PbP.s/qs accuracy for selecting the most useful set (Section 4.7);
(G7) investigating some possible alternative methods for scheduling the execution of the
planners in the selected cluster, and understanding the effectiveness of the default
round-robin strategy in PbP.s/q (Section 4.8).
This experimental study uses various versions of PbP, the most important of which
are listed in Table 1. For G1, we show the CPU time of each configuration step, and we
evaluate if the size of the training problem set can be important to derive effective configured
portfolios. For G2, PbP is compared with the planners that entered the learning track of
IPC6-7 and the winner of the deterministic track of IPC7. For G3, the performance of
PbP is analyzed w.r.t. FDSS (Seipp et al., 2012) and BUS, the portfolio approach proposed
by Roberts and Howe (2007). Although both BUS and FDSS propose to design domainindependent planner portfolios, in principle they can also be used, like PbP, to generate
domain-optimized planning systems. We will experimentally investigate also such an use of
these approaches, comparing them with PbP. For G4, we show the results of three different
experimental comparisons: comparison between PbP configured using the learned domainspecific knowledge (DSK), the unconfigured version of PbP (PbP-nok) and the randomly
configured version of PbP (PbP-rand); comparison of the performance gaps of PbP and
PbP-nok w.r.t. the gaps of the IPC6-7 planners with/without their learned knowledge; and
comparison of PbP using the DSK, PbP configured for a single domain but without using
macros, and PbP configured across all IPC7 domains (PbP-allD). For G5, we have conducted
three experiments in which: the performance of PbP and of each incorporated planners are
compared; the performance of PbP is analyzed w.r.t. the best incorporated planner (without
using macros) for every IPC7 domain; and, finally, PbP is compared with the best cluster of
incorporated planners (possibly using macros) for every IPC7 domain. For G6, we compare
the performance of the planners forming the clusters selected by PbP using (i) no macros,
(ii) the set of macros selected by PbP, and (iii) the best performing set of macros; moreover,
we show and comment some features of the sets of macros selected and used by PbP. Finally,
for G7 we perform two experimental analysis: comparison of the clusters selected by PbP
using some different scheduling strategies, and comparison of the performance of PbP using
different PCPVs (PbP with R1-R2/S1-S2).
654

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

PbP (default)
PbP-IPC6
PbP-IPC7
PbP-nok
PbP-rand
PbP-noM
PbP-allD
PbP with S1
PbP with S2
PbP with R1  PbP
PbP with R2
PbP with 10/30/60

PbP versions
Last version of PbP configured by computing the domain-specific knowledge (DSK)
Version of PbP that entered IPC6 configured using DSK
Version of PbP that entered IPC7 configured using DSK
Unconfigured portfolio
Randomly configured portfolio
Configuration without macros
Configuration without macros and across all IPC7 domains
Configuration using sequential scheduling of the planners with uniform time slots
Configuration using sequential scheduling of the planners with non-uniform time slots
Configuration using round-robin scheduling of the planners with the default PCPV
Configuration using round-robin scheduling of the planners with different PCPVs
Configuration using 10/30/60 training problems

Table 1: Main variants of PbP generating different types of planner portfolio configurations
used in the experimental analysis.

Before presenting and discussing the results of the experimental analysis, we describe
the experimental settings.
4.1 Experimental Settings
The experiment evaluating PbP.s/q with respect to the other IPC6-7 planners considers
all IPC6-7 benchmark domains (Fern et al., 2011; Jimenez et al., 2011), while the other
experiments focus on the most recent IPC7 domains. Regarding the training problems used
in the experiments, for the IPC6 domains they are the same as those of IPC6; for the IPC7
domains, they are a set of 540 problems of various sizes (60 problems for each IPC7 domain,
unless otherwise specified for the particular experiment under consideration) that have been
generated using the problem generator made available by the organizers of IPC7 (for IPC7,
no explicit set of training problems was provided). The training problems are used for both
learning macros and configuring the portfolio. Since the learning procedure of Wizard can
run a planner over the training problems several times, in order to make the training not
too much time consuming, half of the training problem set was designed to be formed by
problems that took up to 30 seconds to solve by some planner; the other half is formed by
problems that took up to about 450 seconds (half of the CPU time limit used in the testing
phase) to solve.
Regarding the test problems, we used the same problems as those used in IPC6-7: the
IPC6 test problems were used for evaluating the performance of PbP.s/q with respect to
the planners that entered IPC6; the IPC7 test problems, that are generally larger and much
more difficult than the IPC6 problems, were used for evaluating PbP.s/q with respect to
the IPC7 planners, as well as for all other experiments in our analysis.
All our experiments have been conducted using the last version of PbP.s/q, which is not
exactly the same as the one that entered and won IPC7 (PbP-IPC7 for short) for three reasons:6 (a) PbP-IPC7 was not properly compiled because of the lack of some C-libraries on the
competition machine, which was discovered only after competition; (b) PbP-IPC7 contains a
very minor syntax bug about the format of the output plans that for few IPC7 domains made
all generated plans invalid to the program validating them used in the competition (Howey,
6. The code of the last version of PbP is available from http://chronus.ing.unibs.it/pbp/.

655

fiGerevini, Saetti, & Vallati

Long, & Fox, 2004); and (c) PbP-IPC7.s uses SGPlan5 without the domain-modification
script that induces SGPlan5 to behave domain-independently. Point (a) negatively affected
the performance of PbP-IPC7.s/q, because one of the incorporated planners (Macro-FF)
could not run when selected. For (b), many valid plans generated by PbP-IPC7.s/q were
rejected by the plan validator of IPC7. Point (c) changed the composition of some clusters selected by PbP-IPC7.q that include SGPlan5, but it does not make the performance
of PbP.q and PbP-IPC7.q substantially different. The only difference between the planner
clusters selected by PbP-IPC7.s and those of PbP.s concerns domain Blocksworld, as the
cluster of PbP-IPC7.s consists of ParLPG without macros, while the cluster selected by PbP.s
is ParLPG using the Bunching set of macros computed by Wizard.
For the comparison with the IPC6 planners, the results of PbP.s/q were obtained by
running its last version on a machine similar to (same CPU frequency and amount of RAM)
the one used to obtain the official IPC6 data (an Intel Core(tm) Quad-processor Q6600 with
3 Gbytes of RAM). For the comparison of PbP.s/q and the IPC7 planners, all systems were
run using the same machine of IPC7 (a Quad-core Intel Xeon 2.93 GHz with 4 Gbytes of
RAM) that the IPC-organizers made available to us for this experiment. Unless otherwise
specified, the other experiments were conducted using a Quad-core Intel Xeon(tm) 3.16
GHz with 4 Gbytes of RAM.
The experimental analysis required many days of CPU time. Unless otherwise indicated,
as in IPC6-7, the CPU-time limit of each run of PbP.s/q was 15 minutes, PbP.s/q used the
default configuration process (the CPU-time limit for each simulated execution of a planner
cluster was 15 minutes), and the planners of the configured portfolio were run by the roundrobin scheduling described in Section 3.2. The performance data of each planner in PbP.s/q
incorporating a randomized algorithm (i.e., LPG, ParLPG and LAMA) were obtained by a
single run for each considered problem instance.
The experimental comparisons with the test instances will generally use three alternative
methods: the average of the performance data, the IPC7 score function (Jimenez et al.,
2011), and the same Wilcoxon sign-rank test used for the planner cluster selection during
configuration. Given two compared planners and a problem set, the average CPU time of
each planner is computed over the problems in the set that are solved by at least one of the
compared planners, and using the CPU-time limit (900 seconds) as the CPU time of the
planner when it does not solve a problem; the average plan quality is computed over the
problems solved by both the compared planners.
The IPC7 score function is defined as follows. Concerning planning speed, if a planner
P solves a problem  using t CPU time, it gets a time score equal to 1+log 1 (t/t ) , where
10
t is the best time over the times required by the planners under comparison for solving
. Concerning plan quality, if P generates a plan with l actions solving , it gets a quality

score equal to ll , where l is the number of actions in the shortest plan over those computed
by the compared planners for . If P does not solve , then it gets zero score (both for
speed and quality). Given a domain D, the time (quality) score of planner P for D is the
sum of the time (quality) scores assigned to P over all the considered test problems in D.
The IPC7 score function for speed is a refinement of the IPC6 score function. Both the
IPC6 and IPC7 time scores are defined according to how much slower a planner performs
than the best performing one, but the IPC6 score penalizes slowdowns more heavily than
656

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

the IPC7 score. For our experiments, we observed that using the IPC6 function, instead of
the IPC7 function, gives similar general results that are slightly more favorable to PbP.s.
As for the Wilcoxon sign-rank test, the null hypothesis is that the performance of a
compared pair of planning systems is statistically similar. The level of confidence we used
is p = 0.001. If the analysis involves the comparison of more than two planning systems,
then, in order to maintain the confidence level used when only one hypothesis is tested
(i.e., only a pair of planners is compared), the confidence level has been modified by the
Bonferronis correction (Shaffer, 1995). For our analysis, the usage of the Bonferronis
correction implies that, if the experimental result we obtain by the Wilcoxon sign-rank test
derives from the comparison of n planning systems, then the used confidence level is 0.001
n1 .
Moreover, for the plan quality comparison using the Wilcoxon sign-rank test, the quality of
the plans computed by two compared planners is normalized by the length of the best plan
for all the test problems solved by these planners. Since the Wilcoxon sign-rank test uses a
ranking of the differences between values in each sample pair, if we compared the absolute
plan length directly, without normalization, such differences in values between domains
could result in an unintended bias, with small relative differences in a benchmark domain
with large solution plans weighted as more important than larger relative differences in a
domain with small plans.
4.2 Overview of the Configured Portfolios Generated by PbP
This section concerns experimental goal G1: we give some information about the configured portfolios (multi-planners) generated by the default version of PbP.s/q (see Table 1),
the relative CPU times used for the automated portfolio configuration, and the size of the
training problem set used for configuring PbP. Table 2 shows the planners in the clusters selected by PbP for every IPC6-7 domain. For each planner in the cluster, the table
also indicates in brackets the sets of macros selected by PbP, which are available from
http://chronus.ing.unibs.it/pbp (the computed planning time slots in the clusters are
omitted for brevity and clarity). For example, for Depots, PbP.q selects the cluster formed
by (i) Macro-FF with the two learned macros that most frequently appear in the Macro-FFs
plans solving the training problems, (ii) ParLPG without any of the computed macros, and
(iii) SGPlan5 with the set of macros obtained by the chunking macro generation method of
Wizard. From the configured portfolios in Table 2 we can derive the following observation:
Experimental result 4.2.1 The planner clusters selected by PbP often are formed by different sets of planners and macros: overall all nine basic planners are helpful (each of them
is selected by PbP.s/q at least once), and different sets of macros are considered more helpful
than others, including, in few cases, the empty set.
Concerning planning speed, we observe that for most domains PbP.s relies on a single
planner possibly using a set of macros. In particular, for 7 of the 15 considered domains
ParLPG outperforms the other incorporated planners, and hence for these domains the
selected cluster contains only ParLPG. The main reason of the better performance of ParLPG
is that it uses LPG with a parameter configuration that is (automatically) optimized for
every considered domain, and this can greatly speedup the planner (Vallati et al., 2013b).
657

fiGerevini, Saetti, & Vallati

Domains
IPC6 domains
Gold-miner
Matching-BW
N-puzzle
Parking
Sokoban
Thoughtful
IPC7 domains
Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP

PbP.s

PbP.q

YAHSP (Cl)
ParLPG ()
ParLPG ()
Macro-FF (M2)
ParLPG ()
FF (), YAHSP ()

Macro-FF (M1), LAMA (B), LPG (0)
Marvin (0), LAMA (), LPG (B)
Fast Downward (), LAMA (), LPG (0)
FF (0), LAMA ()
Macro-FF (M2), LPG (B)
Macro-FF (M5), Marvin (), LAMA ()

SGPlan5 (B)
ParLPG (B)
Macro-FF (M2), ParLPG (0)
ParLPG ()
Macro-FF (M2)
ParLPG ()
ParLPG ()
ParLPG ()
Macro-FF (M1)

SGPlan5 (Cl), FF (), LAMA ()
ParLPG (), LPG (B)
Macro-FF (M2), ParLPG (0), SGPlan5 (Ch)
Marvin (), ParLPG ()
FF (0), LAMA ()
LAMA (), ParLPG ()
ParLPG (), Marvin (0)
LPG ()
LAMA (), SGPlan5 (Ch)

Table 2: Planners and sets of macros (in round brackets) in the cluster selected by PbP
for the IPC6-7 domains.  and 0 indicate that no macros was generated and
selected, respectively; Ch, B and Cl abbreviate the three sets of macros
Chunking, Bunching and Clumping generated by Wizard, respectively; M1M5 are
the five sets of macros generated by Macro-FF. The order of the planners listed in
the clusters corresponds to the order in which they run.

We observed that, in a previous version of PbP that entered IPC6 without ParLPG, the
selected clusters were even more varied.
It is interesting to observe that when PbP selects Macro-FF for the configured portfolio
this planner always uses a non-empty set of macros. The fact that in the selected cluster Macro-FF always uses one among the learned sets of macros indicates that the macro
construction and exploitation methods incorporated into Macro-FF are effective for this
planning system.
Table 3 gives the CPU times used by PbP.s for the different phases of the portfolio
configuration applied to the IPC7 domains, for which a machine with a Quad-core Intel
Xeon(tm) 3.16 GHz and 4 Gbytes of RAM was used.7 The configuration times of PbP.q
are similar to those of PbP.s for the macro extraction and cluster simulation phases, while
they are higher for the performance measurement, because the incorporated incremental
planners can use the whole CPU-time limit in order to find good quality plans. Although
configuring PbP for a specific domain requires a considerable amount of CPU time, it should
be considered that such a configuration needs to be done only once, since the generated
configured portfolio (selected planner cluster) can be used for all the problems in the domain.
Finally, in order to understand if small sets of training problems can be sufficient to
derive informative DSK for test problems that are larger than the training ones, we have
7. For every IPC7 domain, the parameter configuration of ParLPG required about 1400 hours.

658

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7
Domains
Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP

Macro Extraction
Macro-FF Wizard
37.5
28.5
16.4
44.2
7.1
82.9
45.3
12.9
23.8
86.5
18.2
0.0
14.1
41.3
5.25
0.8
19.3
3.9

Performance
Measure
121.6
92.7
92.4
96.8
163.0
57.3
60.0
110.7
34.5

Simulation &
Selection
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02

Total
187.6
153.4
182.4
155.0
273.3
75.6
115.4
116.8
57.8

Table 3: CPU hours used by the configuration of PbP.s for the IPC7 domains: extraction of
macros with Macro-FF and Wizard (2nd and 3rd columns), performance measurement phase (4th column), cluster run simulation and best cluster selection (5th
column), total configuration time (6th column).

compared the performance of PbP configured using the default number of 60 training problems and using half and one-sixth of these training problems. (The range of the problem
size is the same for each of the three sets of training problems.) The results of this analysis
are in Table 4. Of course, the lower the number of training problems is, the cheaper the
training of PbP is. On the other hand, the DSK computed using few training problems can
sometime be much less effective and informative than the DSK obtaining using larger sets.
For Depots, PbP.s with the DSK derived from 60 training problems performs much
better than with the DSKs derived from 30 and 10 training problems; for all the other
domains, the performance of PbP.s with the three compared DSKs is similar or the same. It
is interesting to observe that Depots is the only domain for which the cluster of PbP.s has two
planners. For this domain, the cluster of PbP.s derived from 60 training problems consists
of Macro-FF and ParLPG: for 16 training problems ParLPG hands a solution to PbP.s, while
for the other 44 training problems the solution of PbP.s is obtained by Macro-FF. If the
DSK is derived from 30 or 20 training problems, either Macro-FF or ParLPG is not part of
the configured cluster of PbP.s and this makes PbP.s performing worse.
For Depots, Satellite and TPP, PbP.q with the DSK derived from 60 training problems
performs much better than with the DSK derived from 30 or 10 training problems. For all
the other domains, the performance of PbP.q is similar or the same.
4.3 Performance of PbP and the IPC6-7 Planners
This section concerns experimental goal G2: we experimentally evaluate the performance
of PbP in the context of IPC6-7 with the aim of showing that it is competitive with other
recent planning systems using domain specific learned knowledge. Since at the time of
writing several IPC6-7 planners and the relative domain specific knowledge are not available,
for this experiment we used the official competition data (CPU times, plan qualities and
number of solved problems) and the results we obtained by running the last version of PbP.
659

fiGerevini, Saetti, & Vallati

IPC7
Domains

Depots
Parking
All domains
IPC7
Domains

Blocksworld
Depots
Parking
Satellite
TPP
All domains

Time score
60
30
26.0
2.6
7.4
4.9
33.4
7.0

10
2.6
5.8
8.4

Mean CPU time
60
30
10
31.9
312.5 312.5
172.8 127.7 281.3
110.2 209.8 292.6

# solved problems
60
30
10
26
4
4
8
5
7
34
9
11

Quality score
60
30
10
29.9
29.6 29.6
24.7
8.6
9.2
4.8
2.8
2.0
29.6
0.0
27.8
14.8
7.7
0.0
133.8 78.7 98.6

Mean plan length
60
30
10
269.9 272.8 272.8
151.7 156.2 151.2
76.0
61.0
61.0






325.3 326.6 326.0

# solved problems
60
30
10
30
30
30
26
10
10
5
3
2
30
0
28
15
8
0
136
81
100

Table 4: Time/quality score, average CPU time/plan length and number of solved problems
of PbP.s/q configured with DSK computed by using a set of either 60 (default
version of PbP), 30 or 10 training problems. The domains considered are the
IPC7 domains for which the training phase of PbP.s/q derives different DSKs for
training problem sets with different sizes.

In the learning track of IPC6 and IPC7, the competing teams were not aware of the
domains used for the evaluation before submitting their systems. After code submission,
the contest had two phases. In the first phase, the domains were released and the learning
parts of the planners were run to automatically derive, for each domain, some additional
knowledge using a set of training problems in the domain. In the second phase, after submitting the learned knowledge to the IPC organizers, the planners were run with the relative
learned knowledge, and the resulting performance data were compared using the IPC score
function. The interested reader can find more details about the IPC6-7 organization as
well as a collection of short papers describing the IPC6-7 planners that entered the learning
track in the work of Fern et al. (2011), and Jimenez et al. (2011).
For PbP, the knowledge derived in the first phase of the competition is the portfolio configuration knowledge described in the previous section of the paper. The knowledge learned by IPC6 planner ObtuseWedge consists of some special patterns, that extend
the notion of an n-gram to include argument relations, and are used with the aim of
speeding up the enforced hill-climbing search (Yoon, Fern, & Givan, 2008). The IPC6
planning systems Wizard+FF and Wizard+SGPlan learn a set of macro actions for planners FF and SGPlan5, respectively. As for the IPC7 planners, Bootstrap-Planner learns a
domain-specific heuristic by combining a set of existing heuristics with weights obtained
by evaluating the performance of the heuristics on the training problems (Arfaee, Zilles, &
Holte, 2010). Finally, the knowledge learned by OALDAEYASHP (Brendel & Schoenauer,
2011), ParLPG, Fast Downward-autotune-speed and Fast Downward-autotune-quality (Fawcett
et al., 2011) consists of domain-specific parameter configurations.
Table 5 gives an overall experimental evaluation of the best-performing planners in
IPC6 (using the IPC6 domains) and of the best-performing planners in IPC7 (using the
660

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

Best IPC6 planners
PbP.s
PbP.q
ObtuseWedge
PbP-IPC6.s
PbP-IPC6.q
RFA1
Wizard+FF
Wizard+SGPlan
Best IPC7 planners
PbP.s
PbP.q
Bootstrap-Planner
Fast Downward-autotune-speed
Fast Downward-autotune-quality
OALDAEYASHP
ParLPG
PbP-IPC7.s
PbP-IPC7.q
LAMA-2011

Problem Solved
(%)
97.0
95.0
65.0
95.6
92.8
47.2
56.7
51.1

Time score
(max = 180)
105.5
6.89
73.5
88.2
6.43
14.7
47.3
56.1

Quality score
(max = 180)
111.2
169.1
75.6
111.4
132.3
52.3
72.4
65.2

Problem Solved
(%)
87.4
83.7
4.07
77.0
32.2
7.41
57.04
71.48
70.37
37.67

Time score
(max = 270)
232.7
76.2
3.28
110.0
35.3
5.70
104.0
178.1
71.1
37.9 (1st sol.)

Quality score
(max = 270)
202.5
221.7
10.93
170.8
64.3
3.76
146.0
172.5
192.7
82.4 (last sol.)

Table 5: Percentage of solved problems within 15 CPU minutes, and time and quality scores
of PbP.s/q and the (best performing) planners that took part in the learning track
of IPC6-7 for the domains and problems of IPC6-7. Larger scores indicate better
performances. PbP-IPC6 and PbP-IPC7 indicate the versions of PbP that took part
in IPC6 and IPC7, respectively; LAMA-2011 is the winner of the deterministic track
of IPC7.

IPC7 domains), in terms of percentage of solved problems, planning speed and plan quality.
All compared planners were run with the relative learned knowledge. From the data in
Table 5, the following general experimental result can be derived.
Experimental result 4.3.1 For the IPC6-7 domains and problems, PbP.s is generally
faster than the compared IPC6-7 planners, PbP.q performs generally better in terms of plan
quality, and PbP.s/q solves many more problems.8
Remarkably, PbP.s/q solves a very high percentage of the IPC6-7 benchmark problems
within 15 CPU minutes, and PbP.q almost always computes a plan that is better than the
plan computed by any other competitor. In contrast, the time score of PbP.q is low, since
8. The version of PbP used for the comparison does not suffer the technical problems indicated in Section
4.1 that affected the performance of PbP at IPC7. At IPC7 other planners may have suffered similar
problems, and their implementation might also have improved versions which we have not considered.
However, we note that even the version of PbP.s/q that entered IPC7 performs generally better than the
other competing planners.

661

fiGerevini, Saetti, & Vallati

PbP.q usually runs more than one planner and stops only when all the selected planners
terminate or the CPU-time limit is exceeded.
An analysis of the competition results (planner CPU times and plan qualities) using the
Wilcoxon sign-rank test instead of the IPC score functions for the performance comparison
confirms that PbP.q generates significantly better quality plans (z = 3.920, p < 0.001
7 ). The
p-value obtained by this analysis is 0.004 (with z-value equal to 2.846). Since the p-value is
not below the adjusted critical value of 0.001
7 , the null hypothesis (the performance of PbP is
similar to the performance of the other IPC6-7 planners in terms of speed) is accepted, and
thus the research hypothesis (the performance of PbP is statistically different) is rejected.
However, it is worth pointing out that the critical value of 0.001 is quite hard to reach,
especially given that we also apply an experiment-wise error adjustment. If we had set a
less stringent critical value, say 0.05, then the adjusted critical value would be 0.05
7 = 0.0071,
and p-value of 0.004 would be significant.
Table 6 gives details about the performance comparison for each IPC7 domain. In
terms of speed, PbP.s has the best performance in eight out of the nine domains considered in the analysis; the only domain where it does not perform best is Parking, where
Fast Downward-Autotune-speed performs better. Similarly, in terms of quality, PbP.q has
the best performance in seven out of nine domains, it performs as well as ParLPG and
PbP.q in one domain (Spanner), and it performs worse than Fast Downward-Autotune-speed
in two domains (Parking and TPP). It is worth noting that in principle a portfolio approach
should incorporate the planners most promising for attempting the problems of a domain.
The current version of PbP integrates the planners that have established the state-of-the-art
when PbP was developed, and at that time Fast Downward-Autotune-speed was not available. The results in Table 6 indicate that our portfolio-based approach would reach better
performance, if it also incorporated such a planner. For instance, it is likely that PbP would
select this planner for domain Parking, greatly improving its performance for this domain.
Finally, we comment on the relative performance of PbP and the winner of the deterministic satisficing track of IPC7, the 2011 version of LAMA. Of course, we cannot expect
that a domain-independent planner, such LAMA, performs better than a planner exploiting
(learned) specific domain knowledge. On the other hand, it is definitely a desired property
that the other way around holds: a planning system that uses some form of (automatically
acquired) domain specific knowledge is effective only if it performs better than a state-ofthe-art domain-independent planner that does not use such additional knowledge.
The last lines of Tables 5 and 6 indicate the global and domain-by-domain performance
of LAMA-2011 with respect to the planners of the learning track of IPC7, considering the
score functions of this competition track.9 For this comparison, the CPU time limit used
to run LAMA is 15 minutes, the same time limit as the one used to run PbP.s/q and the
other planners that took part in the learning track of IPC7. It is worth noting that the
IPC7 domains of the learning track are propositional, and the IPC7 problems do not require
the optimization of an explicit specified plan metric; for these problems, both LAMA and
PbP minimize the number of actions. It can be seen that PbP.s/q performs substantially
9. Although the experimental comparison considers both planning time scores and plan quality scores, it
should be noted that the deterministic track of IPC7 focused on plan quality, and hence LAMA-2011 has
presumably been developed focusing on quality rather than speed. In this sense, the results about plan
quality in our comparison with LAMA-2011 are more meaningful than those about planning speed.

662

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7 Planners
Bootstrap-Planner
FDA-speed
FDA-quality
OALDAEYASHP
ParLPG
PbP-IPC7.s
PbP-IPC7.q
PbP.s
PbP.q
LAMA-11

Solved problems

Barman

BW

Depots

Gripper

Parking

Rovers

Sat

Spanner

TPP

0
30
0
0
0
29
30
30
30
2

11
29
27
20
30
29
30
30
30
29

0
20
0
0
17
26
10
26
26
0

0
30
1
0
30
30
30
30
30
0

0
20
9
0
0
0
5
8
5
5

0
30
30
0
27
27
30
27
30
30

0
19
7
0
30
30
30
30
30
13

0
0
0
0
30
30
30
30
30
0

0
30
14
0
15
0
0
25
15
20

Barman

BW

Depots

Gripper

Parking

Rovers

Sat

Spanner

TPP

0.0
14.3
0.0
0.0
0.0
28.8
22.9
28.8
22.9
0.52

3.28
12.1
9.31
5.70
16.3
17.3
8.27
30.0
8.27
10.6

0.0
10.9
0.0
0.0
9.18
25.1
3.05
25.1
7.95
0.0

0.0
9.84
0.55
0.0
15.3
30.0
8.47
30.0
8.47
0.0

0.0
18.7
5.67
0.0
0.0
0.0
2.62
7.14
2.62
2.8

0.0
13.5
11.4
0.0
17.5
27.0
8.21
27.0
8.21
10.0

0.0
6.39
1.92
0.0
17.7
30.0
10.2
30.0
10.2
3.5

0.0
0.81
0.51
0.0
16.2
30.0
7.34
30.0
7.34
0.0

0.0
23.5
5.96
0.0
11.9
0.0
0.0
24.8
7.37
10.5

Barman

BW

Depots

Gripper

Parking

Rovers

Sat

Spanner

TPP

0.0
26.9
0.0
0.0
0.0
28.4
30.0
28.4
30.0
1.86

3.76
13.2
13.3
10.9
21.7
24.1
29.8
21.1
29.8
21.8

0.0
20.0
0.0
0.0
8.31
16.8
9.01
16.8
23.0
0.0

0.0
28.8
0.0
0.0
28.6
27.5
29.9
27.5
29.9
0.0

0.0
17.2
9.00
0.0
0.0
0.0
4.09
9.08
4.09
3.8

0.0
24.2
22.8
0.0
21.4
19.3
30.0
19.3
30.0
24.7

0.0
15.7
6.67
0.0
28.5
26.5
30.0
26.5
30.0
11.0

0.0
0.0
0.0
0.0
30.0
30.0
30.0
30.0
30.0
0.0

0.0
24.8
12.6
0.0
7.54
0.0
0.0
23.9
14.9
19.3

IPC7 Planners
Bootstrap-Planner
FDA-speed
FDA-quality
OALDAEYASHP
ParLPG
PbP-IPC7.s
PbP-IPC7.q
PbP.s
PbP.q
LAMA-11 (1st sol.)

Time score

IPC7 Planners
Bootstrap-Planner
FDA-speed
FDA-quality
OALDAEYASHP
ParLPG
PbP-IPC7.s
PbP-IPC7.q
PbP.s
PbP.q
LAMA-11 (last sol.)

Quality score

Table 6: Number of solved problems, and time/quality scores of the (best performing)
IPC7 planners for each IPC7 domain. FDA, LAMA-11, BW and Sat abbreviate
Fast Downward-Autotune, LAMA-2011, Blocksworld and Satellite, respectively.

better than LAMA-2011. The results in Table 5 show that PbP.s/q solves many more IPC7
problems, and it achieves considerably better overall time and quality scores with respect
to LAMA-2011s first and best quality solutions, respectively. The results in Table 6 show
that: PbP.s has a much higher speed performance for every domain, and a much higher
quality performance for most of the domains; PbP.q has a much higher quality performance
for seven domains, while it performs similarly for the other two domains, and it has a much
higher speed performance for most of the domains.
Moreover, since in the deterministic track of IPC7 the CPU-time limit was 30 minutes,
we compared LAMA-2011 and PbP.s/q over the problems of the learning track using this
limit for the first planner, but keeping 15 CPU minutes for the second. The extra CPU
time for LAMA-2011 does not considerably change the results of the comparison: overall,
663

fiGerevini, Saetti, & Vallati

the total time scores of LAMA-2011 and PbP.s/q are 61.9 and 231.9/116.5, respectively;
the total quality scores of LAMA-2011 and PbP.s/q are 80.7 and 227.6/206.2, respectively;
LAMA-2011 solves 101 problems while PbP.s/q solve 238/230 problems.
The previous experimental analysis of PbP.s/q and LAMA-2011 is summarized in the
following claim, suggesting that if a portfolio-based planner is (automatically) configured for
a given domain, it can perform much better than a state-of-the-art fully domain-independent
planner.
Experimental result 4.3.2 For the benchmark domains of the learning track of IPC7,
the configured versions of PbP.s/q perform better than the IPC7 winner of the deterministic
track.
Since PbP without configuration knowledge (PbP-nok) is a fully domain-independent
planner, it is also interesting to see how well PbP-nok performs w.r.t. LAMA-2011. For this
experimental comparison, we also used the benchmark domains and problems of the deterministic track of IPC7, with the same CPU-time limit of IPC7 for each run (30 minutes).
Moreover, since the deterministic track of IPC7 focused on plan quality, measured as total
action cost, we considered only the quality version of PbP-nok. While LAMA-2011 optimizes
total action cost, PbP-nok.q and the incorporated planners consider number of actions for
plan quality. Although our analysis relies on the total action cost, and hence is somewhat
in favor of LAMA-2011, we observed that PbP-nok.q is competitive with LAMA-2011. For
the problems of the IPC7 deterministic track, the total quality score and number of solved
problems are slightly lower for PbP-nok.q than for LAMA-2011 (214.8 against 232.2, and
239 against 250, respectively). The lower quality score of PbP.q is mainly because of two of
the fourteen IPC7 domains (Elevator and Parcprinter), where PbP-nok.q obtains much
lower scores (1.0 against 19.0 and 3.0 against 19.6, respectively). For the test problems of
the learning track of IPC7, PbP-nok.q performs even better than LAMA-2011 (IPC quality
score: 168.8 versus 97.5; solved problems: 181 versus 105).

Experimental result 4.3.3 For the benchmark domains of the deterministic and learning
tracks of IPC7, PbP.q without configuration knowledge (PbP-nok.q) is competitive with the
winner of the IPC7 deterministic track.

Given that PbP.q without configuration performs already well, a performance improvement obtained by exploiting the computed configuration knowledge is even more notable.
Section 4.5 shows that the portfolio configuration of PbP.s/q is very useful to improve
performance.
4.4 Performance of PbP and Other Planner Portfolios
This section concerns experimental goal G3: we compare PbP with two planner portfolio
approaches: FDSS (Helmert et al., 2011) and BUS (Roberts & Howe, 2007).
664

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

4.4.1 PbP versus FDSS
Table 7 shows the performance of PbP.s/q w.r.t. FDSS with and without using macros.10
The results of this comparison can be summarized as follows.
Experimental result 4.4.1 For the benchmark domains of the learning track of IPC7, in
terms of number of solved problems PbP.s/q performs always better than FDSS, except for
domains Rovers and TPP, where FDSS solves few problems more than PbP.s and PbP.q,
respectively. In terms of time score, PbP.s always performs better than FDSS. In terms of
quality score, PbP.q performs always better except for TPP.
We think there are at least four reasons why in our experiments PbP performed generally
better than FDSS. The main reason is that, while PbP is separately configured for every
considered domain, FDSS always uses the same configuration determined from the problem
instances of IPC16, that were designed using problem distributions quite different from
those of the learning track of IPC7 (Seipp et al., 2012). Other reasons are (a) the diversity
of the planning methods implemented in the planners incorporated into PbP and FDSS, (b)
the usage of macros in PbP.s/q, and (c) the different portfolio configuration techniques of
the two compared systems. Concerning (a), if we consider for instance domain Spanner,
PbP.s/q outperforms FDSS because PbPs configured portfolios use ParLPG/LPG (see Table
2). While every planner incorporated into FDSS uses heuristic forward search techniques,
ParLPG/LPG uses heuristic techniques searching a space of partial plans, which seems more
effective for this domain. As for (b), we tried to learn macros for FDSS using Wizard, but
unfortunately no useful macro was learned for this planning system. Therefore, we tested
the performance of FDSS using the same macros learned by Wizard and selected by PbP.s/q
for the planners in the configured portfolios (see Table 2). The results in Table 7 indicate
that, while using these macros sometimes greatly improves the performance of PbP, they
are not really effective for FDSS.
Finally, in order to better understand the importance of (c), we also developed and compared with PbP a new variant of FDSS, called FDSSd , restricting the differences between
FDSS and PbP to their configuration techniques. Specifically, FDSSd has the following similarities and differences w.r.t. the original FDSS. While FDSSd uses the same configuration
techniques of FDSS, it configures the planner portfolio separately for each input domain
(instead of for a set of domains altogether), uses macros, and integrates the same planners
as PbP (instead of a set of forward-state planners). Then, the most important differences
between PbP and FDSSd are the method for the planner cluster selection and the scheduling
strategy used for running the planners forming the clusters that, as described in Section 2,
are substantially different.
Like for PbP, we computed two sets of domain-optimized portfolio configurations of
FDSSd : FDSS.sd focusing on speed, and FDSS.qd focusing on plan quality. For all the IPC7
domains except Depots, the planner clusters selected by FDSS.s are the same as those of
PbP.s. For Depots, the cluster of FDSS.s consists of Macro-FF using macro set M1 and
Macro-FF using macro set M2, while the cluster of PbP.s consists of ParLPG using no macro
and Macro-FF using macro set M2. For all domain in which FDSS.s and PbP.s have the
10. The version of FDSS that was run in this experiment uses 15 of the 38 variants of Fast Downward analyzed
by Helmert et al. (2011).

665

fiGerevini, Saetti, & Vallati

Planners
PbP.s
PbP.q
FDSS
FDSS+M
PbP-nok.s
PbP-nok.q

# solved problems

Barman

BW

Depots

Gripper

Parking

Rovers

Sat

Spanner

TPP

30
30
0
0
23
23

30
30
21
10
17
18

26
26
0
0
8
8

30
30
0
0
24
25

8
5
0
0
0
0

27
30
28
28
20
17

30
30
14
14
11
11

30
30
0
0
13
13

25
15
17
17
7
10

Barman

BW

Depots

Gripper

Parking

Rovers

Sat

Spanner

TPP

30.0
23.0
0.0
0.0
6.7
6.2

30.0
8.27
9.7
3.6
7.1
5.4

26.0
8.03
0.0
0.0
5.1
3.0

30.0
8.47
0.0
0.0
10.3
7.9

7.8
2.54
0.0
0.0
0.0
0.0

27.0
8.21
17.1
17.1
9.8
5.8

30.0
10.2
8.8
8.8
4.7
4.1

30.0
7.3
0.0
0.0
5.3
5.3

23.7
7.1
8.5
9.0
3.2
3.8

Barman

BW

Depots

Gripper

Parking

Rovers

Sat

Spanner

TPP

28.4
30.0
0.0
0.0
22.5
22.5

21.3
29.9
13.4
12.9
10.3
12.3

17.2
26.0
0.0
0.0
7.6
7.4

27.6
30.0
0.0
0.0
18.1
19.9

5.4
5.0
0.0
0.0
0.0
0.0

18.2
27.9
24.9
24.9
18.7
15.8

26.3
29.5
12.2
12.2
8.9
9.8

30.0
30.0
0.0
0.0
13.0
13.0

20.3
14.2
16.1
15.4
6.3
9.3

Planners
PbP.s
PbP.q
FDSS
FDSS+M
PbP-nok.s
PbP-nok.q

Time score

Planners
PbP.s
PbP.q
FDSS
FDSS+M
PbP-nok.s
PbP-nok.q

Total
236
226
80
69
123
125

Total
234.5
83.2
44.1
38.5
52.2
41.5

Quality score
Total
194.7
222.5
66.6
65.4
105.4
110.0

Table 7: Number of solved problems, and time/quality scores of PbP, PbP-nok, FDSS
with/without using macros for each IPC7 domain. FDSS+M, BW and Sat abbreviate FDSS using macros, Blocksworld and Satellite, respectively.

same cluster, the cluster is formed by a single planner. Hence, running it by the sequential
scheduling and by the round-robin scheduling is the same thing, and the compared planner
portfolios have the same performance.
The planner clusters selected by FDSS.q are in Table 8. For domains Gripper, Satellite
and TPP, they are the same as those of PbP.q but, in these cases they are formed by more
than one planner. For domains Satellite and Gripper we observed that FDSS.q performs
differently from PbP.q because of the different scheduling strategy. Table 9 shows the results of the experimental comparison between PbP and FDSSd (results are omitted when
the compared clusters are the same and they are formed by a single planner). Overall, we
can derive the following observation.
Experimental result 4.4.2 For almost all the benchmark problems and domains of the
learning track of IPC7, PbP.s is as fast as FDSS.sd , and for Depots it is slightly faster;
PbP.q computes plans that are always as good as or better than those computed by FDSS.qd ,
and solves more problems.
The performance gap between PbP and FDSSd is lower than the gap between PbP and
FDSS, but for Depots PbP.s performs slightly better in terms of speed and number of
solved problems, and over all the IPC7 domains PbP.q performs considerably better in
terms of plan quality. A rationale for this behavior is that, as we will show in Section 4.6,
666

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7 Domain
Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP

FDSS.qd
S (Cl), FF, L, M
M (B), FF (Ch), MFF (M1), FF (B),P, L, LPG (Ch), LPG (B)
LPG, M, FF (Ch), MFF (M1), FF (Cl), S (Ch), S (Cl), P, MFF (M2), L
M, P
FF (Ch), FF, L
FF, MFF (M1), L, LPG
P, M
P
L (), S (Ch)

Table 8: Planners and sets of macros (in round brackets) in the cluster selected by FDSSd
for the IPC7 domains. S, L, M, MFF and P abbreviate SGPlan5, LAMA, Marvin,
Macro-FF and ParLPG, respectively; Ch, B and Cl are the three sets of
macros Chunking, Bunching and Clumping generated by Wizard; M1M5 are the
five sets of macros generated by Macro-FF.

running planner clusters by a round-robin scheduling can be more robust than running them
sequentially using possibly inadequate values of planning time slots. Another explanation,
especially for the high performance difference in terms of plan quality, is the different way
in which PbP and FDSSd explore their portfolio configuration spaces. FDSSd searches the
planner cluster to use by a hill-climbing algorithm over the space of possible clusters, while
PbP explores the whole space of possible clusters (with a bound on the number of planners
in clusters). The selected clusters of PbP.s and FDSS.sd are almost always the same because
for the IPC7 domains and the considered training problems configuring the planner portfolio
focusing on speed is quite easy, as in most cases a single planner (possibly using macros)
outperforms every other planner. On the contrary, for these domains and the training
problems, configuring the planner portfolio focusing on plan quality is more difficult for
FDSSd , because its search space contains local minima that prevent FDSSd from finding the
best-performing configuration (planner cluster), while a complete exploration of the search
space allows PbP to identify it.
It is worth noting that the space of the planner clusters of PbP is much smaller than the
spaces of FDSS and FDSSd , since in the space of PbP there cannot be two different clusters
formed by the same planners and the same relative macros, but different relative sequences
of planning time slots (the sequence of planning time slots for a planner with the relative
set of macros is derived according to the default PCPV). If this were not the case, the space
of the clusters of PbP would be orders of magnitude greater, and the time required by PbP
for simulating the cluster execution would not be negligible w.r.t. the time for the other
configuration phases (see Table 3).
The performance comparison of PbP.s and FDSSd using the Wilcoxon sign-rank test
gives a statistical result that is compatible with the performance data in Table 9 and
Experimental result 4.4.2: over all the IPC7 domains, there is no statistical difference
between the planning CPU times of PbP.s and FDSS.sd (z = 1.257, p = 0.209), while,
667

fiGerevini, Saetti, & Vallati

IPC7
Domains
Depots

Max
score
30

Time score
PbP.s
FDSS.sd
22.2
20.1

Mean CPU time
PbP.s
FDSS.sd
53.1
126.3

# solved problems
PbP.s
FDSS.sd
26
23

IPC7
Domains
Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains

Max
score
30
30
30
30
30
30
30
30
30
270

Quality score
PbP.q
FDSS.qd
30.0
30.0
30.0
14.4
24.2
19.2
30.0
23.7
4.8
2.8
29.8
17.3
30.0
25.0
30.0
30.0
15.0
15.0
223.8
177.4

Mean plan length
PbP.q
FDSS.qd
448.3
448.3
233.5
340.2
159.9
169.1
574.0
581.7
70.6
71.7
580.3
600.2
775.2
775.2
326.0
326.0
370.1
370.1
433.1
448.7

# solved problems
PbP.q
FDSS.qd
30
30
30
20
26
21
30
24
5
3
30
18
30
25
30
30
15
15
226
186

Table 9: Maximum score, time/quality score, average CPU time/plan length and number
of solved problems of PbP and FDSSd on benchmark problems from Depots for
planning speed, and from all the IPC7 domains for plan quality.

in terms of plan quality, PbP.q performs significantly better than FDSS.qd (z = 5.767,
p < 0.001).
4.4.2 PbP versus BUS
Although BUS was originally designed to generate a domain-independent configured planner portfolio, like FDSS, in principle it can also be used to build domain-specific configured
portfolios. Domain specificity can be obtained simply by having all the training problems over the same domain. A fully-automated executable of BUS is not available, as the
experimental results presented by Roberts and Howe (2007) were derived by simulation
(Roberts & Howe, 2012). Thereby, in order to compare PbP and BUS, we implemented the
BUS approach using the same planners and macros integrated into PbP, and we generated
domain-specific configured portfolios using this implementation of BUS.
BUS selects the planners for the configured portfolio through a greedy set covering
approximation algorithm over the sets of problems solved by the incorporated planners,
and then the planners forming the clusters are ordered according to the ranking algorithm
by Simon and Kadane (1975). The greedy set covering approximation algorithm iteratively
selects a planner and reduces the set covering problem into a smaller one, until the original
input set is fully covered (Cormen, Stein, Rivest, & Leiserson, 2001). Let D be the planning
domain, P the set of selected planners, and S the set of test problems to cover. Initially, P
is empty and S contains all 60 training problems. At each iteration, the algorithm chooses
the planner with the largest set of solved problems in S, removes these problems from
S, and adds the selected planner to P . If the number of planners with the largest set of
solved problems in S is greater than one, the algorithm selects the first evaluated planner
668

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

(the planner evaluation order is random). The process terminates when S is empty. The
resulting set P contains the planners of the configured portfolio.
We experimentally observed that for almost every considered domain, since more than
one incorporated planner solves all training problems in the domain, the set of planners
forming the cluster selected by BUS for the domain consists of only one planner (except
for domain Parking that has two selected planners, LAMA and FF using macro set Clumping). Moreover, the choice of this planner among those that solve all training problems is
drastically affected by the random order in which the greedy set covering approximation
algorithm evaluates the coverage of the planners. Hence, to derive an indication about the
performance that can be reached by our implementation of BUS, we ran the portfolio configuration of BUS nine times, tested the obtained nine configured portfolios, and analyzed
three sets of experimental results for the CPU time and three sets of experimental results
for the plan quality. These three sets were derived using: the median performing configured
portfolio over the nine generated for each considered domain, and the best/worst performing configured portfolio over all the possible portfolios that can be generated by the greedy
set covering approximation algorithm of BUS. The results of this experimental comparison
are given in Table 10 and summarized in the following observation.
Experimental result 4.4.3 For the benchmark domains of the learning track of IPC7, in
terms of time score and average CPU time, PbP.s/q performs much better than the worst
and the median configured portfolios derived by BUS; PbP.s performs slightly better than
the best configured portfolio that an oracle would select among those that can be derived by
BUS, while PbP.q performs slightly worse. In terms of problem coverage (the criterion used
by BUS to select the planners in the cluster), PbP.s solves the same number of problems as
the best configured portfolio that can be derived by BUS.
The results of Table 10 show that the performance obtained by the configured portfolios
generated by BUS varies greatly, indicating that the planner-selection method of BUS is
not very accurate to derive efficient domain-specific configured portfolios. We think that
the main reason of this is that for the planner selection BUS only considers the problem
coverage and ignores the CPU time and plan quality of the incorporated planners. However,
it is important to note that the planner-selection method of BUS was originally proposed for
different kinds of data sets (problem instances over a set of domains considered altogether
and different from those used in our experiment) and with a different purpose (generating a
domain-independent planner portfolio), for which BUS is a prominent approach that showed
good performance (Roberts & Howe, 2009).
4.5 Effectiveness of the Computed Configuration Knowledge
This section concerns experimental goal G4. In order to understand the effectiveness of the
automated portfolio configuration in PbP, we compare the performance of PbP with the
computed configuration knowledge (PbP.s/q), with no configuration (PbP-nok.s/q), and
with a random configuration (PbP-rand.s/q). In PbP-nok.s/q, all planners in the initial
portfolio are selected, macros are not used, the planning time slots are the same for all
planners, and their execution order is random. PbP-rand.s/q, is the same as PbP-nok.s/q
except that a subset of at most three randomly chosen planners with a (possibly empty)
669

fiGerevini, Saetti, & Vallati

IPC7
Domains

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP

PbP.s

All domains

30.0
29.8
20.6
29.5
7.9
26.9
30.0
30.0
25.0
229.7

IPC7
Domains

PbP.q

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains

30.0
29.9
24.4
29.0
4.3
25.1
30.0
30.0
14.5
217.2

Time score
BUS
W.s
M.s
0.0
27.3
7.4
16.1
7.6
14.3
17.3
17.5
7.2
7.2
1.7
8.7
0.0
0.0
15.0
22.4
0.0
8.2
56.2
121.7
Quality score
BUS
W.q
M.q
0.0
29.5
9.4
29.7
6.0
13.5
15.2
15.2
7.8
7.8
4.4
16.9
0.0
0.0
30.0
30.0
0.0
14.7
72.8
157.9

Mean CPU time
BUS
W.s
M.s
B.s
2.0
900.0
2.7
2.0
9.9
603.2
135.1
19.7
191.4
590.9
205.0 132.5
18.2
183.8
87.3
18.3
364.1
428.1
428.1
428.1
50.5
840.4
411.5
50.5
28.3
900.0
900.0
70.2
16.9
208.1
151.1
16.9
121.2
900.0
508.6
175.3
63.2
627.0
299.6
70.3

# solved problems
PbP.s
BUS
W.s M.s B.s
30
0
30
30
30
10
30
30
26
11
22
26
30
25
30
30
8
8
8
8
27
5
18
27
30
0
0
30
30
30
30
30
25
0
15
25
236
89
183 236

Mean plan length
BUS
W.q
M.q


210.6
198.4
175.8
231.8
876.6
876.6
77.4
77.4
482.8
522.4


326.0
326.0


459.8
464.9

# solved problems
PbP.q
BUS
W.q M.q B.q
30
0
30
30
30
10
30
30
26
11
22
26
30
30
30
30
5
8
8
8
29
5
18
29
30
0
0
30
30
30
30
30
15
0
15
25
226
94
183 238

PbP.s

B.s
30.0
23.5
21.1
28.9
7.2
26.9
23.0
30.0
21.3
211.9

PbP.q
B.q
30.0
29.9
22.8
29.9
7.8
29.0
28.5
30.0
21.5
229.4


198.4
143.7
577.0
87.8
498.8

326.0

367.6

B.q

198.4
190.5
554.7
77.4
420.0

326.0

358.7

Table 10: Time/quality score, mean CPU time/plan length and number of solved problems
of PbP.s/q, and the worst, median and best portfolios that can be derived by
using BUS for each IPC7 domains. W.s, M.s and B.s denote the worst, median
and best portfolios among those that BUS can derive with the lowest, median, and
highest time score for each considered IPC7 test problems, respectively; similarly
W.q, M.q and B.q denote the worst, median and best portfolios with the lowest,
median, and highest quality score, respectively.

randomly chosen set of learned macros is used, instead of all planners, and a different
random configuration of PbP.s/q has been generated for every IPC7 problem.
Figure 3 gives an overall picture of the results for all problems in the IPC7 domains
considering different amounts of CPU times for the portfolio configuration; specifically, the
time on the x-axis is the CPU-time limit given to each run of a planner (with a set of
macros) during the performance measurement simulation phase, to each (simulated) run of
the candidate clusters of planners during the planning cluster selection and ordering phase,
and to each run of the configured portfolio during the test phase. The marked points on the
curves for PbP.s/q correspond to performance scores of the different configured portfolios
obtained for the different considered CPU-time limits. These results indicate that, for every
considered CPU-time limit in the configuration phase, PbP.s/q clearly performs better than
PbP-nok and PbP-rand. Moreover, a refined analysis considering each domain separately
shows that PbP.s/q has the best performance also for every single considered domain, and
that in terms of problem coverage for every considered CPU-time limit the gaps between
670

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7 domains

Time score
240

PbP.s
PbP-nok.s
PbP-rand.s

200

IPC7 domains

Quality score
240

PbP.q
PbP-nok.q
PbP-rand.q

200

160

160

120

120

80

80

40

40

0

0
1

10

100

1000

1

10

100

1000

Figure 3: Time score (left plot) and quality score (right plot) of PbP, PbP-nok and PbP-rand
with respect to an increasing CPU-time limit T (ranging from 1 to 1000 seconds)
for the IPC7 domains.

PbP.s/q and the other two compared version of PbP are very similar to the gaps in the plots
of Figure 3.
Experimental result 4.5.1 The computed configuration knowledge can considerably improve the performance of PbP.s/q w.r.t. the unconfigured and randomly configured versions
of PbP (PbP-nok and PbP-rand, respectively).
In terms of planning speed, the performance comparison of the three considered versions of PbP.s/q, using the Wilcoxon sign-rank test gives a similar general result: PbP.s
is statistically faster than the other versions (z = 12.578, p < 0.001
2 ). In terms of plan
quality, PbP.q performs statistically better than the unconfigured version (z = 13.205,
p < 0.001
2 ). For the comparison between PbP.q and PbP-rand.q, we analyzed 47 out of the
230 problems solved by PbP.q, because PbP-rand.q solves few problems and for the plan
quality comparisons we consider only problems that are solved by both the compared planners. The results of the Wilcoxon test indicates that PbP.q performs similarly to PbP-rand.q
(z = 1.662, p = 0.096). However, it should be noted that the low number of considered
problems makes the statistical comparison through the Wilcoxon sign-rank test not very
accurate and informative for deriving general conclusions about the relative performance in
this case.
We have also tested a version of PbP-nok in which the incorporated planners are run
using the predetermined time slot sequence Spre and the planner runs are ordered using
the same method used by PbP, which considers the relative performance of the planners on
the set of the training problems instead of the random order. Overall the performance of
PbP-nok remains much worse than the performance of (the planner cluster selected by) the
configured version of PbP.
Table 11 analyzes the impact on performance of using DSK (i.e., for PbP, the computed
configuration knowledge) in the best-performing planners that entered the learning track of
IPC6-7. The results of this comparison confirm the strong positive impact of PbPs DSK.
671

fiGerevini, Saetti, & Vallati

Planner
Best IPC6 planners
ObtuseWedge
PbP-IPC6.s
PbP-IPC6.q
Wizard+FF
Wizard+SGPlan
PbP.s
PbP.q
Best IPC7 planners
BootstrapPlanner
Fast Downward-autotune-speed
Fast Downward-autotune-quality
OALDAEYASHP
ParLPG-speed
PbP-IPC7.s
PbP-IPC7.q
PbP.s
PbP.q

Solved (%)

Time

Quality

+17.3
+3.6
+0.9
6.6
1.7
+5.0
+3.3

+34.1
+ 65.2
+5.8
+21.0
+17.6
+82.5
+6.3

+23.7
3.0
+0.7
15.2
3.1
3.2
+37.5

+4.1
+43.3
+14.1
18.9
+9.3
+5.6
+7.4
+21.48
+20.74

+3.3
+65.3
+16.0
17.3
+42.1
+116.5
+24.3
+171.1
+29.4

+10.9
+99.1
+26.4
40.4
+15.6
+16.8
+40.9
+46.8
+69.8

Table 11: Performance gaps of the best-performing IPC6-7 planners with/without DSK in
terms of percentage of solved problems, time and quality scores for the IPC6-7
benchmark domains and problems. Planner RFA1 is omitted because it works
only with DSK.

Experimental result 4.5.2 For the IPC6 domains and problems, the DSK computed for
PbP.s and PbP.q has the strongest impact among the DSK of the IPC6 planners in terms
of improved speed (time) and plan quality (quality), respectively. The DSK computed
for ObtuseWedge has the strongest impact in terms of percentage of additional solved IPC6
problems.
The reason why the impact of the DSK computed by PbP is quite low in terms of additional
solved IPC6 problems is that PbP.s/q solves almost all these problems even without DSK.
Experimental result 4.5.3 For the IPC7 domains and problems, the DSK computed for
PbP.s has the strongest impact in terms of improved speed (time) among the DSK of the
IPC7 planners. The use of the computed DSK in Fast Downward-Autotune-speed has the
strongest impact in terms of percentage of additional solved problems and improved plan
quality.
Although in terms of percentage of additional solved problems and improved plan quality the use of DSK in PbP.s/q has not the highest impact, it leads to high improvements also in PbP.s/q, allowing it to achieve performance that is generally better than
Fast Downward-Autotune-speed (see the Quality Score column of Table 5).
Finally, we conducted an experiment to understand if configuring PbP for a specific domain generates DSK that leads to better performance w.r.t. configuring the planner portfolio
672

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

over a set of domains altogether. Table 12 compares the performance of PbP.s/q with the
DSK, the DSK obtained without using macros (PbP-noM.s/q), and the configuration knowledge computed across all IPC7 domains (PbP-allD.s/q). The planner cluster of PbP-allD.s
is formed by LPG and SGPlan5, while the planner cluster of PbP-allD.q is formed by LAMA,
Marvin and SGPlan5. The results in Table 12 indicate that, even without considering the
usage of macros, the portfolio configuration over all the considered domains together greatly
decreases the performance of PbP.
Experimental result 4.5.4 For the IPC7 domains, in terms of time score, average CPU
time and number of solved problems, PbP.s performs much better than both PbP-noM.s and
PbP-allD.s. In terms of quality score and number of solved problems, PbP.q performs much
better than both PbP-noM.q and PbP-allD.q. In terms of average plan length, both PbP.q
and PbP-noM.q perform usually better than PbP-allD.q.
The results of the Wilcoxon sign-rank test applied to the comparison between PbP
and PbP-noM confirm that, over all the IPC7 domains, PbP.s is significantly faster than
PbP-noM.s (z = 7.699, p < 0.001) and, in terms of plan quality, PbP.q performs significantly better than PbP-noM.q (z = 5.465, p < 0.001). The high performance gap between
PbP and PbP-noM, that is in favor of PbP, clearly indicates the usefulness of using macros,
showing that a portfolio of planners and macros can be much more efficient than a portfolio
of only planners.
4.6 Accuracy of the Planner Cluster Selection
This section concerns experimental goal G5. In order to test the accuracy of the planner
cluster selection in PbP, we carried out three related experiments in which the performance
of PbP using the computed configuration knowledge was compared with the performance
of (a) every basic planner incorporated in the initial portfolio, (b) the best performing
incorporated planner (without using macros) in each considered domain, and (c) the best
performing planner cluster (possibly using macros) in each considered domain. In the
following, Section 4.6.1 presents experiments (a) and (b), Section 4.6.2 experiment (c).
4.6.1 PbP and the Basic Portfolio Planners
Figure 4 gives an overall picture of the performance of PbP.s/q w.r.t. the performance of the
basic planners (without macros) in terms of speed and plan quality, using a CPU-time limit
for each run ranging from 1 to 1000 seconds. The time/quality scores of each compared
system was derived by summing up the corresponding scores obtained by the system in each
IPC7 domain. This analysis indicates that, for every considered CPU-time limit, PbP.s with
DSK is generally much faster than the incorporated basic planners, and PbP.q generates
better quality plans.
Experimental result 4.6.1 For the IPC7 domains, there is no basic planner in the considered input portfolio of PbP that achieves an overall performance better than or similar
to the performance of PbP.s for speed, and of PbP.q for plan quality (except for very low
CPU-time limits, where all compared planners perform similarly in terms of plan quality).
673

fiGerevini, Saetti, & Vallati

IPC7
Domains

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains
IPC7
Domains

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains

Time score
PbP.s
noM
30.0
12.0
30.0
17.3
22.3
16.5
30.0
30.0
7.8
6.2
27.0
27.0
30.0
30.0
30.0
30.0
23.7
11.9
232.3 181.3

allD
10.9
8.3
8.7
15.0
0.0
13.6
18.3
11.3
0.0
86.1

Mean
PbP.s
2.0
5.9
28.3
18.2

16.9
28.3
13.5

25.8

CPU time
noM
allD
72.9
138.6
48.1
311.4
49.3
134.8
18.2
175.1


16.9
192.7
28.3
151.0
13.5
272.0


44.7
194.4

# solved problems
PbP.s noM allD
30
30
30
30
30
21
26
21
13
30
30
30
8
7
0
27
27
26
30
30
30
30
30
25
25
13
0
236
219
175

Quality score
PbP.q
noM
allD
29.9
29.8
0.0
29.9
18.5
13.4
25.8
7.2
2.8
30.0
30.0
21.4
4.8
4.8
2.0
29.6
29.6
22.1
29.9
29.9
8.8
30.0
30.0
8.0
14.8
14.8
12.3
224.7 194.5 91.2

Mean
PbP.q

222.6
153.0
578.9
76.0
634.9
715.1
284.8
370.1
484.5

plan length
noM
allD


307.0 340.8
163.5 171.0
578.9 675.9
76.0
61.0
634.9 650.6
715.1 731.9
284.8 284.8
370.1 374.5
498.8 535.4

# solved problems
PbP.q noM allD
30
30
0
30
26
16
26
9
3
30
30
25
5
5
2
30
30
23
30
30
9
30
30
8
15
15
13
226
205
99

Table 12: Time/quality score, average CPU time/plan length and number of solved problems for the speed and quality versions of PbP, PbP-noM (abbreviated with noM)
and PbP-allD (abbreviated with allD) for the IPC7 domains.

The results of the Wilcoxon sign-rank test applied to this experiment confirm that PbP.s
is significantly faster than every incorporated planner (z = 5.773, p < 0.001
9 ), and that
in terms of plan quality PbP.q performs significantly better than all them (z = 3.920,
p < 0.001
9 ) except ParLPG. According to the Wilcoxon sign-rank test, there is no statistical
difference between the quality performances of PbP.q and ParLPG. The discrepancy between
the results of this analysis and those in Figure 4 is generated by the different ways in which
unsolved problems are handled by the quality score function and the Wilcoxon sign-rank
test for comparing plan quality performance: the first considers all problems attempted by
the compared planners (explicitly penalizing a planner with zero score for each unsolved
problem), while the second considers only the subset of the test problems solved by both
the compared planners; PbP.q solves many more problems than ParLPG (230 against 179),
and this is reflected in the relative curves of Figure 4 for plan quality.
We observed that for domains Rovers, Satellite and Gripper all solutions of PbP.q
are computed by ParLPG; for domains Blocksworld and Depots, PbP.q using ParLPG solves
5 and 3 problems, respectively; for the other considered domains, ParLPG is not part of the
selected cluster of running planners. To better understand the importance of ParLPG in PbP,
we have analyzed the performance of a version of PbP that does not incorporate ParLPG.
674

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7 domains

Time score
240

PbP.s
FD
LAMA (1st sol.)
LPG-td (1st sol.)
Macro-FF
Marvin
Metric-FF
SGPlan5
YAHSP
ParLPG

200
160
120
80

IPC7 domains

Quality score
210

PbP.q
FD
LAMA (last sol.)
LPG-td (last sol.)
Macro-FF
Marvin
Metric-FF
SGPlan5
YAHSP
ParLPG

180
150
120
90
60

40

30

0

0
1

10

100

1000

1

10

100

1000

Figure 4: Time (left plot) and quality (right plot) scores of PbP.s/q with the relative computed configuration knowledge compared to the time and quality scores of the
basic incorporated planners for the IPC7 domains, using an increasing CPU-time
limit. FD abbreviates Fast Downward.

For the IPC7 domains, if PbP.s/q does not incorporate ParLPG, the problems solved by
PbP.s/q decrease by about 10/12%, and, in terms of time score, PbP.s without ParLPG
performs worse than ParLPG (156.1 vs. 176.5). However, in terms of quality score, PbP.q
without ParLPG performs still much better than ParLPG (183.1 vs. 144.5). The results of
this analysis show that the performance of PbP in terms of speed is drastically affected by
ParLPG. On the other hand, the importance of having ParLPG in PbP.q is limited because
the parameter configuration of ParLPG focused on speed.
The two main reasons explaining the observation derived from Experimental result 4.6.1
about the globally best performance of PbP.s/q are that no basic incorporated planner
(even ParLPG) outperforms all the others in every considered benchmark domain, and that
PbP effectively selects and combines the most efficient planners for each domain under
consideration (possibly using a useful set of macro-actions).
One may wonder if the picture can be different when PbP.s/q is compared with the basic
incorporated planners using a (possibly empty) set of macros. Figure 5 shows the results
of this comparison, using a CPU-time limit for each run ranging from 1 to 1000 seconds.
For the sake of readability, the names of the 38 combinations of basic incorporated planners
and sets of macros (learned by Wizard and Macro-FF) have been omitted. The time/quality
scores of each compared system was derived by summing up the corresponding scores obtained by any compared system in each IPC7 domain. If for a domain the combination of
a planner P and a macro set M has M empty, then for that domain the combination is
restricted to P .
The results in Figure 5 show that, in terms of CPU time, for the IPC7 domains there is
no basic planner in PbP that, by using a learned macro set, achieves an overall performance
better than or similar to the performance of PbP.s (except for very low CPU-time limits,
where some compared planners with macros perform similarly). In terms of plan quality, for
CPU-time limits lower than 20 seconds, there exist some basic incorporated planners using
macros that perform better than PbP.q; for high CPU-time limits, PbP.q performs much
better than every compared planner with macros. The combinations of basic incorporated
675

fiGerevini, Saetti, & Vallati

IPC7 domains

Time score
240

PbP.s
Any planner with a set of macros

200

IPC7 domains

Quality score
240

PbP.q
Any planner with a set of macros

200

160

160

120

120

80

80

40

40

0

0
1

10

100

1000

1

10

100

1000

Figure 5: Time (left plot) and quality (right plot) scores of PbP.s/q with the relative computed configuration knowledge compared to the time and quality scores of the 38
combinations of incorporated planners and sets of macros for the IPC7 domains.

planners and sets of macros that for low CPU-time limits perform better than PbP.q are
SGPlan5 using any set of learned macros, ParLPG using macro set bunching, and YAHSP
using macro set clumping. For low CPU-time limits, these combinations of planners and
macros have an overall performance better than PbP.q, essentially because they dominate
over a single domain: Barman for SGPlan5, Blocksworld for ParLPG and YAHSP.
Since the analysis of Figure 4 considered the test domains altogether, in order to verify
the supposition that also for a given single domain PbP performs better or not worse than
every basic incorporated planner, we have compared PbP.s/q with the best-performing basic
planner (according to the test problems and the relative IPC scores) for each considered
domain. Such a planner, indicated with BestP.s/q, is the single planner (without macros)
that we would use if we had an oracle specifying the best basic incorporated planner for the
test problems of a specific domain. The results of this experiment are shown in Table 13.
For domains Gripper, Rovers, Satellite and Spanner the planner cluster of PbP.s is
the same as BestP.s. For the other considered domains, the time score and the average
CPU time of PbP.s are much better than BestP.s. In terms of problem coverage, for three
domains PbP.s solves a much higher number of problems; while for the other domains
problem coverage is the same as BestP.s. These results show that, in order to achieve
higher planning speed, using a cluster of planners or a useful set of macro-actions selected
by PbP.s can be much better than using a single planner without macros. Sections 4.6.2
and 4.7 will study the usefulness of using a properly selected cluster of planners and a
non-empty set of macros, respectively.
Experimental result 4.6.2 There is no IPC7 domain for which any basic planner in the
considered input portfolio of PbP.s is faster, achieves better time score, or solves more
problems than PbP.s.
Concerning plan quality, BestP.q contributes a great deal to the success of PbP.q, since
for all domains except Barman and Spanner it is included in the cluster selected by PbP.q
(see Table 2). For Barman, Gripper, Parking, Rovers, Satellite, Spanner, and TPP, in
most cases BestP.q provides the solution to PbP.q.
676

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7
Domains

BestP.s

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP

SGPlan5
ParLPG
ParLPG
ParLPG
FF
ParLPG
ParLPG
ParLPG
ParLPG


All domains
IPC7
Domains

BestP.q

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP

SGPlan5
ParLPG
ParLPG
ParLPG
FF
ParLPG
ParLPG
ParLPG
LAMA


All domains

Max
score
30
30
30
30
30
30
30
30
30
270

Time score
PbP.s
BestP.s
30.0
12.0
30.0
17.3
23.6
18.3
30.0
30.0
6.8
3.0
27.0
27.0
30.0
30.0
30.0
30.0
24.4
11.2
232.8
178.8

Mean CPU time
PbP.s
BestP.s
2.0
72.9
9.9
95.3
109.5
229.7
18.2
18.2
364.1
460.9
25.1
25.1
28.3
28.3
16.9
16.9
121.2
531.4
39.9
79.3

# solved problems
PbP.s
BestP.s
30
30
30
30
26
21
30
30
8
7
27
27
30
30
30
30
25
14
236
219

Max
score
30
30
30
30
30
30
30
30
30
270

Quality score
PbP.q
BestP.q
30.0
29.8
29.9
29.9
24.5
10.4
29.0
29.9
4.8
6.8
30.0
30.0
29.6
29.8
30.0
30.0
14.6
14.7
222.4
210.8

Mean plan length
PbP.q
BestP.q
449.3
452.9
269.9
272.8
160.1
163.1
577.3
570.1
79.0
80.6
694.7
694.7
785.2
782.8
326.0
326.0
370.1
366.3
472.6
481.8

# solved problems
PbP.q
BestP.q
30
30
30
30
26
11
30
30
5
7
30
30
30
30
30
30
15
15
226
213

Table 13: Maximum score, time/quality score, average CPU time/plan length, and number
of problems solved by PbP.s/q and the best planner (BestP.s/q) for the IPC7
domains.

Experimental result 4.6.3 For the IPC7 domains, in terms of plan quality, the relative
performance of PbP.q and the best-performing basic planner (BestP.q) that an oracle would
choose is generally slightly in favor of PbP.q: for Blocksworld and Depots PbP.q performs
better, for Parking BestP.q performs slightly better, and in the rest of the IPC7 domains
they perform similarly.
Concerning Parking, Table 13 shows that, for the used benchmark problems, the
BestP.q-planner is FF, which is correctly contained in the cluster selected by PbP.q for this
domain (see Table 2). However, this cluster also includes an additional planner (LAMA)
that, for the tested problems and the considered CPU-time limit, does not give a useful
contribution to PbP.q (no solution is found by LAMA), introducing some noise in the
cluster selection. This and the fact that for Parking no useful set of macros is computed by
PbP.q are the main reasons why PbP.q performs slightly worse than the BestP.q-planners
for the considered test problems in domain Parking.
The Wilcoxon sign-rank test applied to this experiment confirms that, overall, PbP.s is
significantly faster than the BestP.s-planner of each domain (z = 3.134, p  0.001); while
in terms of plan quality, the test results indicate that the performances of PbP.q and the
677

fiGerevini, Saetti, & Vallati

BestP.q-planner are not significantly different (z = 1.157, p = 0.247); in other words, the
test cannot derive that one system performs statistically better than the other.
Finally, we have compared PbP.s/q and the best-performing combination P + M of a
basic planner P with a non-empty set M of macros learned for P in each IPC7 domain,
except Spanner for which no macro are computed. In this experiment, the best macro set
M for P in a domain D was chosen considering the performance of P + M over the training
problems of D. Overall, in terms of speed score and problem coverage, PbP.s performs
similarly to P + M in five domains, while it performs much better in three domains; in
terms of quality score, PbP.q performs similarly in four domains and much better in other
four domains. One of the reasons why P + M can perform worse than PbP.s/q is that in
some domains macros are harmful, and PbP.s/q correctly decides not to use them. This
will be discussed also in the context of an experiment presented in Section 4.7, where we
analyze the usefulness of macros and the accuracy of their selection in PbP.s/q.
4.6.2 PbP and the Best-Performing Portfolio Configuration
In order to test the accuracy of the planner cluster selection in PbP.s/q, we have also
compared PbP with the computed configuration knowledge and the best-performing cluster
of planners (with the useful macros) for each considered test domain. (The worst-performing
cluster solves no problem.) Table 14 shows the results of this experiment considering two
best-performing clusters with at most three planners: for each considered IPC7 domain,
BestC.s is the planner cluster with the highest time score among those that can be obtained
by PbP.s using the default PCPV; similarly, BestC.q is the planner cluster with the highest
quality score. Therefore the data in the time/quality score columns BestC.s/q are the
maximum values over the time/quality score sums of all planner clusters for the set of test
problems of each IPC7 domain.
For every domain except Depots, the time score of PbP.s is the same as the one of the
best cluster and is much greater than zero (and thus much better than the score of the
worst cluster). Also in terms of average CPU time and problem coverage the performance
of PbP.s and the best cluster are almost always the same. Only for domain Depots PbP.s
and BestC.s perform slightly differently; in this case, the planners and relative macros in
the cluster of PbP.s are different from those in BestC.s. In particular, Macro-FF is selected
with a different set of macros, and this makes PbP.s slightly slower.
Concerning PbP.q, overall, in terms of plan quality there is no high performance gap
with respect to the best cluster, although PbP.q performs worse for domain TPP. For this
domain, the training problems used by PbP.q are not informative enough. This observation
is supported by the fact that the best cluster computed using the training problems, instead
of the test problems, is different from the one derived for the test problems. On the other
hand, we observed that, if the size of the training problems is similar to the size of the test
problems, the configured portfolios of PbP.q and BestC.q are the same.
The Wilcoxon sign-rank test confirms that, overall, the performance of PbP.s/q and the
best cluster is not statistically significantly different (z = 0.422, p = 0.673, for the speed
analysis; z = 2.432, p = 0.015, for the quality analysis). Moreover, we have also observed
that PbP.s/q without configuration (PbP-nok.s/q) performs generally much worse than the
678

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7
Domains

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains
IPC7
Domains

BestC.s
SGPlan5 (B)
ParLPG (B)
Macro-FF (M1)
ParLPG ()
Macro-FF (M2)
ParLPG ()
ParLPG ()
ParLPG ()
Macro-FF (M1)

BestC.q

Barman
SGPlan5 (Cl)
Blocksworld
ParLPG ()
Depots
MFF(M1),MFF(M2)
Gripper
ParLPG ()
Parking
FF (0)
Rovers
ParLPG ()
Satellite
ParLPG ()
Spanner
LPG ()
TPP
Macro-FF (M1)
All domains

-

Max
score
30
30
30
30
30
30
30
30
30
270

Time score
PbP.s BestC.s
30.0
30.0
30.0
30.0
21.2
24.5
30.0
30.0
8.0
8.0
27.0
27.0
30.0
30.0
30.0
30.0
25.0
25.0
231.2
234.5

Mean CPU time
PbP.s
BestC.s
2.0
2.0
9.9
9.9
165.9
82.5
18.2
18.2
364.1
364.1
25.1
25.1
28.3
28.3
16.9
16.9
121.2
121.2
47.4
36.7

# solved probs
PbP.s BestC.s
30
30
30
30
26
28
30
30
8
8
27
27
30
30
30
30
25
25
236
238

Max
score
30
30
30
30
30
30
30
30
30
270

Quality score
PbP.q BestC.q
29.9
30.0
29.9
29.9
23.9
26.7
29.0
29.9
4.8
6.8
30.0
30.0
29.5
29.8
30.0
30.0
14.8
24.4
221.8
237.5

Mean plan length
PbP.q
BestC.q
449.3
448.3
269.9
272.8
160.1
165.1
577.3
570.1
79.0
80.6
694.7
694.7
785.2
782.8
326.0
326.0
370.1
379.5
461.2
487.6

# solved probs
PbP.q BestC.q
30
30
30
30
26
28
30
30
5
7
30
30
30
30
30
30
15
25
226
240

Table 14: Maximum score, time/quality score, average CPU time/plan length, and number
of problems solved by PbP.s/q and the best cluster (BestC.s/q) for the IPC7
domains. MFF abbreviates Macro-FF. The order of the planners listed in the
cluster for Depots corresponds to the order in which they are run.

best cluster for speed and quality. Overall, from our experimental results we can derive the
following observation.
Experimental result 4.6.4 For the IPC7 benchmarks, in terms of time score, average
CPU time and problem coverage, PbP.s performs as well as or, for Depots, similarly to
BestC.s. In terms of quality score, average plan length and problem coverage, PbP.q performs as well as or similarly to BestC.q, except for TPP, in which the plan quality score and
problem coverage of PbP.q are worse.
Table 14 also shows that very often an oracle would use a single planner to either quickly
solve the IPC7 problems or compute high-quality plans for them. Hence, one may argue
that using clusters formed by more than one planner (possibly with a set of useful macros)
is not useful. The rationale why the best clusters in Table 14 are formed by a single planner
is that often any incorporated planner (even using macros) requires almost all the CPU
time to solve each IPC7 test problem (except for domain Depots); thus the remaining time
is usually not enough to improve the coverage or the quality of the (first) computed plan
by running more than one planner. For the purpose of computing high-quality plans, if we
use a set of test problems smaller than the IPC7 problems, then the picture is different.
679

fiGerevini, Saetti, & Vallati

Table 15 compares the performance of PbP and the best performing cluster of planners for
some sets of randomly generated medium-size problems of the IPC7 domains (i.e., with
size ranging between the largest training problems and the smallest testing problems). In
this table, BestC.s/q indicates the clusters that an oracle would use to solve these sets of
medium-size problems.
Experimental result 4.6.5 For test problems over the IPC7 domains with sizes ranging
between the training problem sizes and the IPC7 test problem sizes, for most of the IPC7
domains the best planner clusters for deriving high quality plans are formed by more than
one planner.
In general, a cluster of planners containing a certain planner performs worse than this
planner alone when most of the planning problems of the domain for which the planner
portfolio is configured are efficiently solved by the planner alone, and thus running also the
other planners of the cluster is a waste of CPU time. A cluster formed by more than one
planner performs better than any single portfolio planner only if for the considered domain
there is no planner dominating all others in terms of either problem coverage and CPU
time, or problem coverage and plan quality.
Interestingly, we observed that sometimes the cluster selected by PbP.q and the best
cluster for the intermediate-size test problems are formed by a planner that solves all the
problems, but produces low-quality plans, and other planners that produce higher-quality
plans, but solve few problems. This is the case for Barman and TPP. For these domains,
although the quality of the plans of SGPlan5 is low, having SGPlan5 in the cluster is very
useful because it contributes to greatly improve the problem coverage of the cluster.
Finally, the results in Table 15 also indicate that sometimes the effectiveness of the
configured portfolio can be greatly affected by the difference between the size/hardness of the
training problems and the size/hardness of the test problems. In particular, the performance
gap between PbP.q and the best cluster for the considered randomly generated intermediatesize problems of domain TPP is lower than between PbP.q and the best cluster for the IPC7
test problems of TPP. This indicates that, in terms of plan quality, the effectiveness of the
planner portfolio configuration in PbP.q computed using relatively small training problems
can gradually decrease when the size/hardness of the test problems is increased.
4.7 Macro Usefulness and Selection Accuracy
This section concerns experimental goal G6: we analyze the effectiveness of using the set
of macros selected by PbP for each planner, and the accuracy of PbP for selecting the most
useful set of macros among those computed by Wizard or Macro-FF for each planner in the
configured portfolio. While it has been shown that Wizard and Macro-FF can often generate
useful sets of macros that speed up planners (Botea et al., 2007b; Newton et al., 2007), it
is also known that there is no guarantee that using macros always leads to improving the
speed of a planner, and a bad set of macros could even make a planner slower. Moreover,
usually the degree of usefulness of a set of macros depends on the specific planner that uses
them.
Concerning macros in PbP.s, for each IPC7 domain with at least one non-empty set of
computed macros and each planner in the selected cluster (see Table 2), we compared the
680

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7 domains

BestC.s

(medium probs)

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains
IPC7 domains

S (B)
ParLPG (B)
MFF (M1), ParLPG (0)
ParLPG ()
MFF (M2)
ParLPG ()
ParLPG ()
ParLPG ()
Macro-FF (M1)

BestC.q

(medium probs)

Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains

S (Cl), FF (), M ()
P (), MFF (M1), LPG (B)
MFF (M1), P (), LPG ()
ParLPG ()
FF (), LAMA ()
ParLPG ()
ParLPG (), Marvin ()
LPG ()
MFF (M1), L (), S (CH)


Time score
PbP.s BestC.s
30.0
30.0
30.0
30.0
28.9
29.7
30.0
30.0
22.0
22.0
30.0
30.0
30.0
30.0
30.0
30.0
30.0
30.0
260.9
261.7

Mean CPU time
PbP.s
BestC.s
1.5
1.5
7.3
7.3
57.4
52.5
13.2
13.2
308.7
308.7
17.6
17.6
14.3
14.3
13.6
13.6
93.6
93.6
50.9
50.4

# solved probs
PbP.s BestC.s
30
30
30
30
30
30
30
30
22
22
30
30
30
30
30
30
30
30
262
262

Quality score
PbP.q BestC.q
29.7
29.8
29.5
29.6
26.4
28.5
30.0
30.0
20.0
20.0
30.0
30.0
30.0
30.0
30.0
30.0
24.9
29.3
221.8
237.5

Mean plan length
PbP.q
BestC.q
327.2
327.1
174.7
173.2
143.1
145.4
472.3
472.3
63.1
63.1
694.7
694.7
524.6
524.6
257.2
257.2
219.4
220.3
462.6
463.2

# solved probs
PbP.q BestC.q
30
30
30
30
28
30
30
30
20
20
30
30
30
30
30
30
25
30
226
240

Table 15: Time/quality score, average CPU time/plan length, and number of problems
solved by PbP.s/q and the best cluster (BestC.s/q) for sets of medium-size
problems of the IPC7 domains. S, M, MFF, P, L abbreviate SGPlan5, Marvin,
Macro-FF, ParLPG, and LAMA, respectively. The order of the planners listed in
the clusters corresponds to the order in which they run.

number of solved problems, number of visited search nodes, average CPU time and time
score using: (a) no macros, (b) the set of macros identified by PbP.s as useful for the planner,
and (c) the set of macros among those computed for the planner that in terms of time score
makes it perform best over the test problems. From the results of this experiment, which
are given in Table 16, the following general observation can be derived.
Experimental result 4.7.1 For the IPC7 domains, very often there is a candidate set of
macros for a planner (computed by Wizard or Macro-FF) that greatly increases the speed
performance of the configured portfolio, and PbP.s correctly selects it.
Table 16 also indicates that, for most of the considered domains, the performance of
the selected planners obtained using their sets of macros identified as useful by PbP.s is
usually the same as the performance they can achieve when using their best sets of macros.
This gives strong positive evidence about the effectiveness of PbP.ss approach to selecting
a useful set of macros for each planner in the configured portfolio. In particular, the best
set of macros is the same as the set of macros selected by PbP.s (see Table 2). The only
exception where the sets of macros identified by PbP.s is different from the best set is the
681

fiGerevini, Saetti, & Vallati

Domain &
Planner
Barman
SGPlan5
Blocksworld
ParLPG
Depots
Macro-FF
Parking
Macro-FF
TPP
Macro-FF

#S

with no macros
#N
T

TS

#S

with PbP.s macros
#N
T
TS

#S

with best macros
#N
T
TS

30



72.9

12.0

30



1.8

30.0

30



1.8

30.0

30

3361

95.3

17.3

30

218.0

9.9

30.0

30

218.0

9.9

30.0

0

242678



0.0

26

33654

203.3

22.2

28

21231

105.1

26.2

2

1739

406.9

0.6

8

880.9

92.3

8.0

8

880.9

92.3

8.0

0

71594

600.0

0.0

25

2990

121.2

25.0

25

2990

121.2

25.0

Table 16: Number of solved problems (#S), number of visited search nodes (#N), average
CPU time (T) and time score (TS) of the planners forming the cluster selected
by PbP.s using no macro, the set of macros selected by PbP.s, and the best performing set of computed macros. The domains considered are the IPC7 domains
with at least one non-empty set of computed macros. We indicate with  that
the number of nodes visited by SGPlan5 could not be measured.

case of Macro-FF in domain Depots. However, as shown in Table 2, for Depots PbP.s
selects a cluster that contains both Macro-FF with macro set M2 and ParLPG, obtaining an
overall performance that we experimentally observed to be very similar to the performance
of Macro-FF with the best set of macros, M1. It is worth noting that the candidate sets of
macros computed for ParLPG and Depots are harmful (i.e., they make its speed performance
much worse) and PbP.s correctly detects this, choosing to run ParLPG with zero macros
(denoted with ParLPG (0) in Table 2).
The study of computing and using macros has usually been pursued with the main goal
of speeding up planning, possibly making the quality of the computed plan lower than when
macros are not used. Interestingly, in the context of PbP.q, in several cases macros are useful
also for improving plan quality. Specifically, for nine over the fifteen IPC6-7 domains, the
configuration phase of PbP.q selects clusters of planners with at least one planner using a
non-empty set of macros (see Table 2). We experimentally observed, with both the training
problems and the test problems, that there are two reasons why macros are useful to PbP.q:
 For some domains there are individual planners for which using macros leads to better
quality plans. This is the case, e.g., for domains Barman and Blocksworld using planners SGPlan5 and LPG (first solution), respectively. This behavior has been observed
also by Botea et al. (2005), Coles and Smith (2007), and Newton et al. (2007).
 If the selected cluster includes a planner configured to use a set of macros, usually
such a planner quickly computes a solution. This can be somewhat helpful also for
the test problems that another planner in the cluster can solve with better solutions,
if it has enough CPU time, because a quick termination of the planner with macros
leaves more CPU time to run the other cluster planner(s). Having more CPU time,
can be important especially for the incremental planner(s) included in the selected
682

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

cluster, like LAMA and ParLPG. There are many problem instances of domains Depots,
Satellite and TPP for which we observed this behavior.
Experimental result 4.7.2 For the IPC7 domains, the use of the macros selected by
PbP.q can lead to better quality solutions.
In general, the use of macros can make the plan search more effective because, e.g.,
by planning multiple actions in one search step the size of the possible plateaus and the
depth of the local minima can be reduced. On the other hand, if a large number of macros
is added to the domain, the size of the search space can drastically increase, making the
problem harder to solve. In the rest of this section, we analyze the kind and number of
macros selected and used by PbP. We consider both macro operators, i.e., parameterized
macros defined as sequences of (primitive) domain operators, and macro actions, i.e., macros
derived by instantiating the parameters of the macro operators.
Table 17 describes the macro operators in the sets selected by PbP.s for a planner
in the configured portfolio (see Table 2) in terms of: number of aggregated operators,
number of involved parameters, average numbers of macro-actions and primitive actions in
the augmented domain, average plan lengths obtained by the considered planners without
using macros, and using them but counting each planned macro actions as a single action.
From the data in Table 17, we can derive some interesting observations about the macros
used by PbP for the considered domains. First, the macro operators used by PbP for a
planner are no more than three, and often they aggregate few primitive operators. Secondly,
for the planners that handle macros by simply adding instantiated macro operators to
the domain definition (SGPlan5 and ParLPG), the average number of macro actions in
the augmented domains is much lower or comparable to the number of primitive domain
actions, even for domain Barman where SGPlan5 uses a large macro operators involving
seven primitive operators and six parameters. Hence, for these planners and domains,
macro actions do not drastically increase the search space. The picture is quite different for
Macro-FF, for which the macro operators selected by PbP.s in domains Depots, Parking
and TPP, if instantiated, generate a number of macro actions that on average is one or more
orders of magnitude greater than the number of primitive domain actions. The reason why
Macro-FF can successfully use macro operators even if the number of domain macro actions
is huge is that this planner instantiates macro operators and filters macro actions at search
time, according to a relaxed-plan heuristic applied to the current search state, rather than
simply adding all macro actions to the original domain before planning.
The fact that in our experiment PbP never generates configured portfolios with large
sets of macro actions added to the domain description seems to indicate that, if the number
of macro actions is very high w.r.t. the number of primitive actions, this macro exploitation
method usually makes the performance of a planner using them much worse. This observation was confirmed by an additional experiment in which we added the PDDL description
of the macro operators learned by Macro-FF for domain Depots to the original description
of Depots, and run Macro-FF using the resulting augmented domain. As shown in Table 17,
for Depots the number of learned macro actions is about one order of magnitude greater
than the number of primitive actions. We experimentally observed that with the augmented
domain Macro-FF (without its own method of using macros) solves no Depots problem.
683

fiGerevini, Saetti, & Vallati

Domain &
Planner
Barman
SGPlan5+B
Blocksworld
ParLPG+B
Depots
Macro-FF+M2
Parking
Macro-FF+M2
TPP
Macro-FF+M1

#operators
for every m.
7,4

#parameters
for every m.
6,4

2,3,2

2,2,2

2,2

5,6

5,2

8,5

6

9

#grounded
macros
1645
(397)
17757
(5812)
224053
(114600)
billions
(billions)
billions
(billions)

#actions
15610
(3767)
11983
(5270)
16005
(8269)
243223
(151979)
133145
(78545)

Plan length
without m.
452
(57)
415
(107)

143
(23)


Plan length
with m.
374
(45)
153
(42)
119
(26)
64
(11)
238
(51)

Table 17: Number of (primitive) operators forming each of the selected macro operators,
number of parameters in each macro operator, average number of instantiated
macro actions, average number of domain (primitive) actions, average plan length
without using macros, and average plan length using macros and counting each
planned macro action as a single action. Each number in the 2nd and 3rd columns
refers to a different macro operator. Numbers in brackets are standard deviations. The domains considered are the IPC7 domains with at least one non-empty
set of learned macros selected by PbP.s. B abbreviates the Bunching macro
set learned by Wizard; M1M2 are two of the five sets of macros generated by
Macro-FF. We indicate with  that no solution was found within the given
CPU-time limit.

Moreover, the results about the average plan length in Table 17, show that plans with
macro actions are much shorter than those computed from the original domain, if we count
each macro as a single action. Given that during planning the application to the current
search state of a macro (or possibly a combination of macros in Macro-FF) generates a single
successor state, for the considered planners and domains, on average the distance between
the initial search state and a goal state is much shorter when the search space includes
macros, and hence searching a solution plan in this space can be much faster.
To conclude, we note that the usefulness of macros can also depend on factors different
from those considered in our analysis, such as, e.g., the ratio between the number of useful instantiations of a macro operator (providing shortcuts towards a goal state) and the
number of instantiations that guides the search towards a wrong direction (Botea, Muller,
& Schaeffer, 2007a). Further factors that might affect the usefulness of macro-operators in
planning are conjectured in the work by McCluskey and Porteous (1997).
4.8 Planner Cluster Scheduling
This section concerns experimental goal G7: we experimentally analyze some possible alternative strategies for scheduling the execution of the planners during the portfolio configuration of PbP and at planning time. In the first experiment, we investigate the use in
PbP of four sequential and round-robin strategies with predefined and configured planning
time slots. In the second experiment, we study the importance of choosing a specific PCPV
684

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

defining the planning time slots (as described in Section 3.1) and in particular of PbPs
default PCPV.
Let T be the input CPU-time limit, k the maximum number of planners in the cluster,
and n the number of single planners, combined with a set of macros, in the portfolio (in
our experiment, T = 900 seconds, k = 3, and 9  n  38 depending on the number
of computed macro sets). We experimentally compare the performance of PbP using the
following strategies for the planner cluster execution during the portfolio configuration:11
S1. Sequential execution of each tuple of at most k planners with Tk seconds for the run of

P
every planner; the number of candidate configured portfolios is ki=1 i! ni . For this
and the next (S2) strategies, when a planner terminates before the end of its time
slot, the remaining time of this slot is used to (uniformly) increase the slots of the
subsequently running planners.
S2. For every combination of time slots t1...k such that ti  {0, 90, 180, 270, 360, 450,
540, 630, 720, 810, 900}, i  {1    k} and t1 +    + tk = T , sequential execution of
each tuple of k planners with ti seconds for the runP
of the i-th
 planner in the sequence;
the number of candidate configured portfolios is ki=1 ni  O(ui1 ), where u is the
number of non-zero planning time slots lower than 900 (in our experiment u = 9).
R1. Round-robin execution of each set of at most k planners with the planning time slots
derived from the default PCPV defined in Section 3.1 (this isPPbPs default
scheduling

strategy); the number of candidate configured portfolios is ki=1 ni .
R2. For every PCPV p = hp1 , ..., p9 i in set P (defined below), round-robin execution
of each set of at most k planners with the planning
 time uslots derived from p; the
Pk
n
number of candidate configured portfolios is i=1 i  O(s ), where s is the number
of increments considered for each pi (in our experiment s = 4).
Set P in R2 is formed by more than 100,000 PCPVs obtained by setting each percentage
in the PCPV to a value ranging from li to ui , with: l1 , ..., l9 equal to 10, 15, 20, 25, 30, 35,
40, 45, 50; u1 , ..., u9 equal to 70, 75, 80, 85, 90, 95, 98, 99, 100; and increment step of pi
equal to ui 4li . For instance, if i = 1, we have that the increment step of p1 is 7010
= 15.
4
Consequently, the values used for the first percentage p1 of the considered PCPVs are 10,
25, 40, 55, 70.
Concerning the execution order of the planners in a cluster, for each considered sequence
in strategies S1 and S2, the order is defined by the planner order in the sequence (two
sequences formed by the same planners are considered different clusters if the planners are
differently ordered or they use different time slots); for each cluster of planners in strategies
R1 and R2, the execution order is determined according to the increasing planning time slots
associated with the planners in the cluster (this is the default execution order strategy).
The configuration phase of PbP using each of the four scheduling strategies generates four
alternative clusters of planners, with relative planning time slots, which, at planning time,
are run with the same corresponding scheduling strategies that were used at configuration
time. It should be noted that the portfolio configuration using strategies S2 and R2 is
11. The planners of each candidate cluster are executed by simulation, as described in Section 3.2.

685

fiGerevini, Saetti, & Vallati

computationally much heavier than the configuration using S1 and R1, respectively, since
many more candidate configured portfolios are considered. On the other hand, since PbP
with S2 and R2 examines larger portfolio configuration spaces, in principle, it could obtain
more accurate configured portfolios.
Tables 18 and 19 compare the performance of PbP configured using S1-S2 and R1-R2
for solving the IPC7 domains and problems. We observed that, in terms of speed, for
all IPC7 benchmark domains except Depots, the considered scheduling strategies do not
affect the selection of the best cluster, since PbP.s always selects the cluster formed by a
single planner (possibly using macros). For Depots, as shown in Tables 18 and 19, PbP.s
with the round-robin scheduling strategies solves more problems and is faster than with the
sequential scheduling strategies.
Concerning plan quality, the best cluster selected by PbP.q contains more than one
planner for every IPC7 domain. Overall, the following observation can be derived:
Experimental result 4.8.1 For the IPC7 benchmark domains and problems, PbP.q with
R1-R2 solves more problems than PbP.q with S1-S2 and, in terms of plan quality, overall
it performs similarly to PbP.q with S1-S2.
We think that the explanation why PbP.q with R1-R2 performs better in terms of number of solved problems is that using a round-robin strategy makes PbP.q more robust than
using a sequential strategy with respect to possible incorrect ordering of the planner runs
and inadequate values of the planning time slots decided at configuration time. When the
training problems are not as difficult as those used at testing time (usually they are easier),
some inaccurate estimation about the effectiveness of the learned configuration knowledge
can arise. An under estimation of the time slot values or an incorrect planner execution
order can damage more severely the sequential execution of the planners in the selected
cluster, since each of these planners is run only once, using at most the estimated time slot,
while in the round-robin execution each of them is iteratively run with its (multiple) time
slots, until the total CPU-time limit is reached or all planners terminate.
In terms of plan quality evaluated through the IPC quality scores, PbP.q with R1-R2
tends to perform better than PbP.q with S1-S2. The main reason is that PbP.q with R1R2 solves more problems than PbP.q with S1-S2, and the quality score for an unsolved
problem is zero. If we consider the average plan quality (last four columns of Tables 18
and 19), we observe mixed results: in two domains PbP.q with R1-R2 performs best, in
two worse, and in the other ones about the same. The discrepancy in the evaluation results
using quality scores and average plan qualities is only apparent, since the quality score
and the average quality evaluations have different assumptions about the way they consider
unsolved problems. For the average plan quality, only the subset of the test problems solved
by PbP using all the compared strategies are considered; while for the quality score, all test
problems are considered.
Seipp et al. (2012) show that a sequential portfolio of 21 domain-independent statebased forward planners can solve more problems when the planning time slots are uniform,
rather than configured over a set of training problems, because, for the considered planners
and test problems, a planner either quickly solves a problem or does not solve it at all.
In our context, we observed that if we sequentially run all n planners of PbP.q (i.e., up
to 38 combinations of the 9 basic planners with/without the computed sets of macros)
686

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

IPC7
Domains
Depots

S1
20.8

Time Score of PbP.s
S2
R1
R2
17.8
22.2
21.0

Problems Solved by PbP.s
S1
S2
R1
R2
26
20
27
26

IPC7
Domains
Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains

Quality Score of PbP.q
S1
S2
R1
R2
30.0
30.0
30.0
30.0
16.7
16.7
30.0
30.0
6.1
6.1
24.4
25.2
29.9
29.9
28.9
28.9
3.7
4.6
4.3
4.3
29.0
29.0
25.2
25.2
29.8
21.0
29.5
29.7
30.0
30.0
30.0
30.0
13.7
13.7
14.7
12.8
188.9 181.0 217.0 216.1

Problems Solved by PbP.q
S1
S2
R1
R2
30
30
30
30
21
21
30
30
8
8
25
27
30
30
30
30
4
5
5
5
29
29
30
30
30
21
30
30
30
30
30
30
14
14
15
13
196
188
225
225

Table 18: Time/quality score and number of solved problems of PbP.s/q using scheduling
strategies S1-S2 and R1-R2 for the IPC7 benchmark domains and problems.

using uniform time slots, then only 137 test problems are solved (against the 225 solved
by PbP); the n-planners uniform strategy performs as well as PbP.q only if the CPU-time
limit is increased by several times (keeping 900 seconds for PbP.q). Differently from what
observed in the work of Seipp et al. (2012), our experimental evaluation includes many
problems that the n planners of PbP.q solve using considerable CPU time (e.g., the number
of problems that can be solved by any planner incorporated in PbP, even using macros,
within 10 seconds is only 80). Probably a reason of this different behavior is that the test
problems of the IPC7 learning track are on average more difficult than the problems of
the IPC7 deterministic track, which are the test problems used in the work of Seipp et al.
(2012).
On the other hand, if PbP sequentially runs at most 3 planners, as in strategies S1S2, instead of all the 38 possible combinations between the incorporated planners and the
learned macros, we obtain a behavior similar to that observed by Seipp et al. (2012). In
particular, the results in Tables 18 and 19 show that in terms of number of solved problems
and speed, configuring the planning time slots for the sequential scheduling in some cases
can even degrade the performance of PbP w.r.t. using the uniform distribution of CPU time
(see the results in Tables 18 and 19 of PbP.s using S1 and S2 for Depots and of PbP.q using
S1 and S2 for Satellite). However, in our context the uniform distribution of CPU time
over the planners in the cluster selected by PbP is not the best one, since we experimentally
observed that PbP with S2 clearly outperforms PbP with S1 if the configuration is done
using the test problems rather than the training problems. We believe that the main reason
for this behavior is that in our experiment the training problems are much smaller and
easier than the test problems, which in several cases makes PbP with S2 (configured with
the training problems) underestimate the CPU times required to solve the test problems.
687

fiGerevini, Saetti, & Vallati

IPC7
Domains
Depots

Average CPU Time of PbP.s
S1
S2
R1
R2
256.2 360.8 185.2
185.0

Std. Dev. of CPU Time of PbP.s
S1
S2
R1
R2
122.3
250.4
206.1
78.2

IPC7
Domains
Barman
Blocksworld
Depots
Gripper
Parking
Rovers
Satellite
Spanner
TPP
All domains

Average Plan Quality of
S1
S2
R1
449.3 449.3 449.3
310.3 310.3 236.7
220.0 220.0 154.3
570.1 570.1 588.7
84.0
83.3
82.0
583.9 583.9 703.4
747.8 747.8 751.6
326.0 326.0 326.0
364.0 364.0 362.6
466.8 466.8 477.8

Std. Dev. of plan
S1
S2
55.3
55.3
89.1
89.1
118.2
118.2
45.2
45.2
11.8
27.4
158.2
158.2
161.3
128.5
52.1
52.1
101.4
101.4
217.9
188.9

PbP.q
R2
449.3
236.7
159.5
588.7
82.0
703.4
751.6
326.0
362.6
477.6

quality of PbP.q
R1
R2
55.3
55.3
68.6
68.6
32.5
32.8
38.0
38.0
24.8
24.8
194.9
194.9
183.8
183.8
52.1
52.1
99.1
99.1
238.4
238.8

Table 19: Average and standard deviation of the CPU time/plan quality of PbP.s/q using
scheduling strategies S1-S2 and R1-R2 for the IPC7 benchmark domains and
problems.

Contrary to PbP with S1-S2, PbP with R1-R2 performs similarly according to all three
evaluation criteria (solved problems, speed and plan quality). This result indicates that
configuring the planning time slots by considering many alternative PCPVs does not lead
to high improvements with respect to using the default predefined planning time slots,
that in PbP configuring the values of the planning time slots is less crucial when using a
round-robin strategy than when using a sequential strategy, and that PbP with R1-R2 is
less sensitive to the different size of the problems used for configuration and testing.
Experimental result 4.8.2 For the IPC7 benchmark domains and problems, PbP.s/q
with R1-R2 is less sensitive to the definition of the planning time slots than PbP.s/q with
S1-S2.
In the rest of this section, we study the problem of configuring the PCPV used to define
the planning time slots in the round-robin planner scheduling of PbP. In particular, we
address the following questions focusing on the IPC7 benchmarks: how important is setting
the PCPV to a particular value for a given domain? If we had an oracle specifying the best
PCPV for the test problems of a specific domain, how good would the default PCPV be with
respect to it?
The data used in this analysis were obtained as follows. For each PCPV p in the set P
defined as well as for the scheduling strategy R2 of the previous experiment, PbP.s/q was
run using the cluster selected by simulating the round-robin scheduling with the planning
time slots derived from p as described in Section 3.1. Thereby PbP.s/q was configured more
than 100,000 times with different predefined PCPVs and, consequently, different predefined
planning time slots. The resulting configured portfolios were then run (by simulation) over
the test problems of the learning track of IPC7.
688

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

Time score

Quality score

30

30

25

25

20

20

15

15

10

10

5

5

0

0
Barman

BW

Depots Grip. Parking Rovers

Sat. Spanner TPP

Barman

BW

Depots Grip. Parking Rovers

Sat. Spanner TPP

Figure 6: Distribution of the time (left plot) and quality (right plot) scores of PbP.s/q using
more than 100,000 PCPVs for the IPC7 problems. BW, Grip. and Sat. abbreviate
Blocksworld, Gripper and Satellite, respectively.

Figure 6 analyzes the time and quality scores of these configured portfolios through box
and whisker plots. In each plot, the bottom of a whisker is the worst score; the bottom
of a box is the lower quartile score; the band in a box is the median score; the top of a
box is the upper quartile score; the top of a whisker is the best score; finally, each cross is
the score of PbP.s/q for a domain using the default predefined PCPV. In the following, the
PCPV corresponding to the configured portfolio obtaining the best time or quality score for
a domain is called the best-performing PCPV for that domain. Since the best performing
PCPV is derived from the observed performance on the test problems, it can be considered
the best PCPV over P that an oracle would give us. From the experimental data used for
Figure 6, we derive the following observation.
Experimental result 4.8.3 Different IPC7 domains have different best-performing PCPVs
for PbP.
For each IPC7 domain where the length of the whisker in Figure 6 is zero, the cluster
selected by PbP.s/q with any PCPV is formed by only a single planner, and hence for these
cases the definition of the PCPV used to derive the planning time slots do not affect the
performance of PbP (all available CPU time is assigned to the single selected planner). In
the plot about speed this happens for all domains except Depots, while in the plot about
plan quality, it happens only in domain Spanner. For domain Barman, all clusters selected
by PbP.q using the configured PCPVs include SGPlan5 with a learned set of macros, and
this is the only planner in the cluster finding solutions for the test problems of this domain.
For the domains in which PbP.s/q does not always select the same singleton planner
cluster for all the PCPVs considered, the specific used PCPV can have a high impact on
PbPs performance, as shown especially for domains Depots, Gripper and Satellite in
the quality-score plot of Figure 6. Interestingly, we can observe that the default predefined
PCPV used in PbP.s/q is generally a good choice, since very often the crosses in the plots
appear at (or near to) the top position of the corresponding whiskers.
689

fiGerevini, Saetti, & Vallati

Experimental result 4.8.4 For every IPC7 domain, the cluster selected by PbP.s/q using
the default PCPV h25, 50, 75, 80, 85, 90, 95, 97, 99i performs similarly to PbP.s/q using the
best-performing PCPV, except for PbP.q in domains Parking and TPP.
For Parking, the best performance is obtained by running planners FF and LAMA with
PCPV equal to h10, 15, 60, 65, 70, 75, 80, 95.5, 96.5i; for TPP, it is obtained by running planners LAMA, Macro-FF and SGPlan5 with PCPV equal to h10, 15, 20, 25, 30, 35, 40, 45, 50i.
For these two domains, PbP.q with the default PCPV does not perform as well as with
the best-performing PCPV (but still better than the median-performing PCPV). The main
reason is that for these domains the IPC7 test problems are much larger (and harder) than
those used for the training, which, as also observed in Section 4.6, can affect the accuracy
of the portfolio configuration for the test problems in terms of the selected planner cluster
and configured PCPVs.
Overall, the results of the experiment about configured and default PCPVs for PbP
indicate that, if the round-robin planner scheduling is used, tuning the PCPV (and consequently the planning time slots) for a specific IPC7 domain does not greatly improve the
performance of the resulting configured portfolio, since very often the default PCPV performs as well as the best PCPV specified by an oracle. Consequently, given that without the
PCPV tuning the portfolio configuration is much simpler and faster, PbP uses the default
version.

5. Conclusions
The existing automated-planning technology offers a large, growing set of powerful techniques and efficient domain-independent planners, but none of them outperforms all the
others in every planning domain. From a practical perspective, it is then useful to consider
a portfolio-based approach to planning that involves several techniques and planners. In
this paper, we have proposed an approach to automatically configuring a portfolio of planners and learned macros for any given domain, that is implemented in the portfolio-based
planner PbP. The computed configuration knowledge consists of a promising combination
of basic planners in the portfolio, each one with a (possibly empty) set of useful macros,
and some scheduling information for specializing their execution at planning time. The
configured portfolio is obtained through an automated statistical analysis about the performance of a set of candidate clusters of planners and relative candidate sets of macros, using
a collection of training problems in the given domain. The planner cluster performance is
computed by simulating the cluster execution using the performance data from the runs of
the individual basic planners (and relative sets of macros) in the portfolio.
The proposed approach to the portfolio planner configuration has been evaluated through
a large experimental analysis, focusing on the IPC6-7 domains, with the aim of demonstrating its high efficiency, understanding the effectiveness of the automatic configuration, and
investigating the importance of the main design choices. Several results have been derived
from the various experiments of this analysis. The most important experimental results
indicate that:
 the configured planner portfolios generated by PbP.s/q perform very well compared
to other state-of-the-art planning systems using learning techniques, and much better
690

fiPlanning through Automatic Portfolio Configuration: The PbP Approach








than PbP-nok, i.e., the unconfigured planner portfolio of PbP (which is competitive
with LAMA, a state-of-the-art domain independent planner);
PbP.s/q performs much better than the other existing domain-independent portfoliobased planners, and often better than other domain-optimized planner portfolio approaches;
the computed configuration knowledge is very useful and the selection of the planner
cluster forming the configured portfolio is generally accurate for the given planning
domain;
while macros in a planning domain are not always helpful to a planner for improving
its planning speed or plan quality, PbP.s/q generally selects helpful sets of macros;
in the context of the proposed approach, the round-robin scheduling strategy of the
planner cluster execution is a robust strategy with respect to the execution order of
the cluster planners and their planning time slots; moreover, configuring the planning
time slots is not crucial given the good default technique for deriving them currently
implemented in PbP.s/q.

Besides evaluating the approach of PbP to configuring a planner portfolio with macros,
the experimental analysis corroborates and validates some results, observations or empirical
studies in previous work on other researchers in planning. These include the usefulness or
harmfulness of macros for a set of prominent existing planners, the importance of diversity
of the planning techniques in the construction of an effective planner portfolio, and the
robustness of the round-robin scheduling of the execution times in a multi-planner system.
While the current version of PbP uses a portfolio formed by a specific set of selected
techniques for plan synthesis, computation of macros and planner-parameter tuning, the
architecture of PbP is open in the sense that additional or alternative (current or future)
techniques can be integrated. Moreover, although we have chosen the Wilcoxon sign-rank
test for comparing the candidate planner clusters and macro sets, demonstrating its effectiveness, other methods could be considered.
A limit of the current approach, which affects also other systems relying on knowledge
learned from examples, is that when the training problem set is not representative of the
test problems (e.g., most problems are much smaller or easier than the test problems), the
computed portfolio configuration might not be very accurate for these problems. Knowing
at configuration time enough information characterizing the test problems can obviously
be very useful for generating representative training problem sets. For planning with PbP,
we experimentally observed that, when the minimum/maximum number of objects involved
in test problems is known, randomly generated training problem sets under these object
bounds are sufficiently representative for an effective configuration of PbP.
We think that in future work it will be important to study and incorporate into PbP
additional methods supporting the problem-based configuration of the portfolio planner.
Such methods could refine the current domain-based configuration so that problems with
different size or heuristically estimated hardness can have different, specialized configured
portfolios. Moreover, it will also be important to extend PbP.q so that plan quality is
measured in terms of plan action costs rather than number of plan actions.
Other directions for further research are investigating the use of PbP.s/q for optimal
planning and for metric-temporal domains (Fox & Long, 2003), and extending the portfolios
691

fiGerevini, Saetti, & Vallati

with additional automatically extracted domain-specific knowledge, such as entanglements
(Vallati et al., 2013a). Finally, we intend to investigate the idea of making PbP fully domainindependent by computing many portfolio configurations (planner clusters) for different
known domains, and using a classifier to match a new domain with the most promising
stored configuration in terms of expected performance for the new domain. A similar idea
was successfully developed for SAT (e.g., Xu et al., 2008).

Acknowledgments
Many ideas, techniques, and systems investigated in the paper use or build on important
previous work in planning and portfolio design, without which our research would have
not been possible. We thank all authors of such work, and in particular the authors of
the planning systems and macro generators incorporated in PbP. A special thank to Mark
Roberts and Adele Howe for clarifications on the configuration of their planner portfolio,
to Beniamino Galvani for his help with the implementation of part of a preliminary version
of PbP.s, and to the IPC7 organizers for letting us use the competition machine in one
of the experiments conducted after the the competition. We would also like to thank the
organizers of IPC6 and IPC7 for having developed and made available a large collection of
useful benchmark domains, problems and some software tools that we used in our analysis.
Finally, we thank the anonymous Reviewers and the Associate Editor for their very helpful
and detailed comments.

References
Arfaee, S., J., Zilles, S., & Holte, R., C. (2010). Bootstrap learning of heuristic functions.
In Proceedings of the Third Annual Symposium on Combinatorial Search (SOCS-10),
pp. 5260. AAAI Press.
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 134.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI
planning with automatically learned macro-operators. Journal of Artificial Intelligence Research, 24, 581621.
Botea, A., Muller, M., & Schaeffer, J. (2007a). Fast planning with iterative macros. In
Proceedings of the Twentieth International Joint Conference on Artificial Intelligence
IJCAI-07, pp. 18281833. AAAI Press.
Botea, A., Muller, M., & Schaeffer, J. (2007b). Learning partial-order macros from solutions.
In Proceedings of the Fifteenth International Conference on Automated Planning and
Scheduling (ICAPS-05), pp. 231240. AAAI Press.
Brendel, M., & Schoenauer, M. (2011). Instance-based parameter tuning for evolutionary AI planning. In Proceedings of the Thirteenth Annual Genetic and Evolutionary
Computation Conference (GECCO-11), pp. 259260. ACM.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2013). Learning predictive models to configure planning portfolios. In Proceedings of the ICAPS-13 Workshop on Planning and
Learning.
692

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

Chen, Y., Hsu, C., & Wah, B. (2006). Temporal planning using subgoal partitioning and
resolution in SGPlan. Journal of Artificial Intelligence Research, 26, 323369.
Chrpa, L., & Bartak, R. (2009). Reformulating planning problems by eliminating unpromising actions. In Proceedings of the Eighth Symposium on Abstraction, Reformulation,
and Approximation, (SARA-09), pp. 5057. AAAI press.
Chrpa, L., & McCluskey, T., L. (2012). On exploiting structures of classical planning problems: Generalizing entanglements. In Proceedings of the Twentieth European Conference on Artificial Intelligence (ECAI-12), pp. 240245. IOS Press.
Chrpa, L., McCluskey, T., & Osborne, H. (2012). Reformulating planning problems: A
theoretical point of view. In Proceedings of the Twenty-Fifth International Florida
Artificial Intelligence Research Society Conference (FLAIRS-12), pp. 1419. AAAI
Press.
Coles, A., & Coles, A. (2011). LPRPG-P: Relaxed plan heuristics for planning with preferences. In Proceedings of the Twenty-First International Conference on Automated
Planning and Scheduling (ICAPS-11), pp. 2633. AAAI Press.
Coles, A., Coles, A., Olaya, A., Celorrio, S., Lopez, C., Sanner, S., & Yoon, S. (2012). A
survey of the seventh international planning competition. AI Magazine, 33 (1).
Coles, A., & Smith, K., A. (2007). Marvin: A heuristic search planner with online macroaction learning. Journal of Artificial Intelligence Research, 28, 119156.
Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction to Algorithms
(3rd edition). McGraw-Hill.
Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:
Domain-specific configuration using Fast Downward. In Proceedings of the ICAPS-11
Workshop on Planning and Learning.
Fawcett, C., Vallati, M., Hutter, F., Hoffmann, J., Hoos, H., H., & Leyton-Brown, K. (2014).
Improved features for runtime prediction of domain-independent planners. In Proceedings of the 24th International Conference on Automated Planning and Scheduling
(ICAPS), pp. 355359. AAAI Press.
Fern, A., Khardon, R., & Tadepalli, P. (2011). The first learning track of the international
planning competition. Machine Learning, 84 (1), 81107.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. Journal of Artificial Intelligence Research, 20, 61124.
Gerevini, A., Haslum, P., Long, D., Saetti, A., & Dimopoulos, Y. (2009). Deterministic
planning in the fifth international planning competition: PDDL3 and experimental
evaluation of the planners. Artificial Intelligence, 173 (5-6), 619668.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239290.
Gerevini, A., Saetti, A., & Serina, I. (2006). An approach to temporal planning and scheduling in domains with predictable exogenous events. Journal of Artificial Intelligence
Research, 25, 187231.
693

fiGerevini, Saetti, & Vallati

Gerevini, A., Saetti, A., & Vallati, M. (2009). An automatically configurable portfolio-based
planner with macro-actions: PbP. In Proceedings of the Nineteenth International
Conference on Automated Planning & Scheduling (ICAPS-09), pp. 350353. AAAI
Press.
Gibbons, J., & Chakraborti, S. (2003). Nonparametric Statistical Inference, Fourth Edition:
Revised and Expanded. Statistics: A Series of Textbooks and Monographs. CRC Press.
Gomes, C., P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126 (1-2),
4362.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M., Roger, G., & Karpas, E. (2011). Fast Downward Stone Soup: A baseline for
building planner portfolios. In Proceedings of the ICAPS-11 Workshop of Planning
and Learning.
Hoffmann, J. (2003). The Metric-FF planning system: Translating ignoring delete lists
to numeric state variables. Journal of Artificial Intelligence Research, 20, 291341.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Howe, A., Dahlman, E., Hansen, C., vonMayrhauser, A., & Scheetz, M. (1999). Exploiting
competitive planner performance. In Proceedings of the Fifth European Conference
on Planning (ECP-99), pp. 6272. Springer.
Howey, R., Long, D., & Fox, M. (2004). VAL: Automatic plan validation, continuous effects
and mixed initiative planning using PDDL. In Proceedings of the Sixteenth IEEE
International Conference on Tools with Artificial Intelligence (ICTAI-04), pp. 294
301. IEEE.
Hutter, F., Hoos, H., H., & Stutzle, T. (2007). Automatic algorithm configuration based on
local search. In Proceedings of the Twenty-second National Conference on Artificial
Intelligence (AAAI-07), pp. 11521157. AAAI Press.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: an automatic
algorithm configuration framework. Journal of Artificial Intelligence Research, 36,
267306.
Jimenez, S., C., Coles, A., & Coles, A. (2011). Seventh International Planning Competition
IPC7  learning part. In http://www.plg.inf.uc3m.es/ipc2011-learning.
Kautz, H., A., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In
Proceedings of the Sixteenth International Joint Conferences on Artificial Intelligence
(IJCAI-99), pp. 318325. AAAI Press.
Long, D., & Fox, M. (2003). The third International Planning Competition: Results and
analysis. Journal of Artificial Intelligence Research, 20, 159.
Marquardt, D., W., & Snee, D. (1975). Ridge regression in practice. The American Statistician, 29(1), 320.
694

fiPlanning through Automatic Portfolio Configuration: The PbP Approach

Matos, P., Planes, J., Letombe, F., & Marques-Silva, J. (2008). A MAX-SAT algorithm
portfolio. In Proceedings of the Eighteenth European Conference on Artificial Intelligence (ECAI-08), pp. 911912. IOS Press.
McCluskey, T., L., & Porteous, J., M. (1997). Engineering and compiling planning domain
models to promote validity and efficiency. Artificial Intelligence, 95 (1), 165.
Newton, M., Levine, J., Fox, M., & Long, D. (2007). Learning macro-actions for arbitrary
planners and domains. In Proceedings of the Seventeenth International Conference on
Automated Planning & Scheduling (ICAPS-07), pp. 256263. AAAI Press.
Pulina, L., & Tacchella, A. (2007). A multi-engine solver for quantified boolean formulas.
In Proceedings of the Thirteenth International Conference on Principles and Practice
of Constraint Programming (CP-07), pp. 574589. Springer.
Rice, J. R. (1976). The algorithm selection problem. Advances in Computers, 15, 65118.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. Journal of Artificial Intelligence Research, 39, 127177.
Roberts, M., & Howe, A. (2006). Directing a portfolio with learning. In Proceedings of the
AAAI 2006 Workshop on Learning for Search, pp. 129135.
Roberts, M., & Howe, A. (2007). Learned models of performance for many planners. In
Proceedings of the ICAPS-07 Workshop of AI Planning and Learning.
Roberts, M., & Howe, A. (2009). Learning from planner performance. Artificial Intelligence,
173 (5-6), 536561.
Roberts, M., & Howe, A. (2012). Personal communication. December 14.
Roberts, M., Howe, A., E., Wilson, B., & desJardins, M. (2008). What makes planners
predictable?. In Proceedings of the Eighteenth International Conference on Automated
Planning and Scheduling (ICAPS-08), pp. 288295. AAAI Press.
Seipp, J., Braun, M., Garimort, J., & Helmert, M. (2012). Learning portfolios of automatically tuned planners. In Proceedings of the Twenty-second International Conference
on Automated Planning & Scheduling (ICAPS-12), pp. 368372. AAAI Press.
Shaffer, J., P. (1995). Multiple hypothesis testing. Annual Review of Psych, 46, 561584.
Simon, H., & Kadane, J. (1975). Optimal problem-solving search: All-or-none solutions.
Artificial Intelligence, 6, 235247.
Vallati, M., Chrpa, L., & Kitchin, D. (2013a). An automatic algorithm selection approach
for planning. In IEEE International Conference on Tools with Artificial Intelligence
(ICTAI), pp. 18. IEEE.
Vallati, M., Fawcett, C., Gerevini, A., Hoos, H., & Saetti, A. (2013b). Automatic generation of efficient domain-optimized planners from generic parametrized planners. In
Proceedings of the 6th Annual Symposium on Combinatorial Search (SOCS-13), pp.
184192. AAAI Press.
Vidal, V. (2004). A lookahead strategy for heuristic search planning. In Proceedings of the
Fourteenth International Conference on Automated Planning and Scheduling (ICAPS04), pp. 150159. AAAI Press.
695

fiGerevini, Saetti, & Vallati

Wilcoxon, F., & Wilcox, R., A. (1964). Some Rapid Approximate Statistical Procedures.
American Cyanamid Co., Pearl River, N.Y.
Witten, I., H., & Frank, E. (2005). Data Mining: Practical machine learning tools and
techniques. Morgan Kaufmann, San Francisco.
Xu, L., Hutter, F., Hoos, H., H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based
algorithm selection for SAT. Journal of Artificial Intelligence Research, 32, 565606.
Yoon, S., Fern, A., & Givan, R. (2008). Learning control knowledge for forward search
planning. Journal of Machine Learning Research, 9, 683718.

696

fiJournal of Artificial Intelligence Research 50 (2014) 141-187

Submitted 9/13; published 5/14

Enhanced Partial Expansion A*
Meir Goldenberg
Ariel Felner
Roni Stern
Guni Sharon

MGOLDENBE @ GMAIL . COM
FELNER @ BGU . AC . IL
RONI . STERN @ GMAIL . COM
GUNISHARON @ GMAIL . COM

Ben-Gurion University of the Negev
Beer-Sheva, Israel

Nathan Sturtevant

S TURTEVANT @ CS . DU . EDU

The University of Denver,
Denver, USA

Robert C. Holte
Jonathan Schaeffer

HOLTE @ CS . UALBERTA . CA
JONATHAN @ CS . UALBERTA . CA

The University of Alberta
Edmonton, Canada

Abstract
When solving instances of problem domains that feature a large branching factor, A* may
generate a large number of nodes whose cost is greater than the cost of the optimal solution. We
designate such nodes as surplus. Generating surplus nodes and adding them to the OPEN list may
dominate both time and memory of the search. A recently introduced variant of A* called Partial
Expansion A* (PEA*) deals with the memory aspect of this problem. When expanding a node n,
PEA* generates all of its children and puts into OPEN only the children with f = f (n). n is reinserted in the OPEN list with the f -cost of the best discarded child. This guarantees that surplus
nodes are not inserted into OPEN.
In this paper, we present a novel variant of A* called Enhanced Partial Expansion A* (EPEA*)
that advances the idea of PEA* to address the time aspect. Given a priori domain- and heuristicspecific knowledge, EPEA* generates only the nodes with f = f (n). Although EPEA* is not
always applicable or practical, we study several variants of EPEA*, which make it applicable to
a large number of domains and heuristics. In particular, the ideas of EPEA* are applicable to
IDA* and to the domains where pattern databases are traditionally used. Experimental studies
show significant improvements in run-time and memory performance for several standard benchmark applications. We provide several theoretical studies to facilitate an understanding of the new
algorithm.

1. Introduction
A* and its derivatives such as IDA* (Korf, 1985) and RBFS (Korf, 1993) are general state-based
search solvers guided by the cost function f (n) = g(n) + h(n). Given an admissible (i.e. nonoverestimating) heuristic function h, A* is guaranteed to find an optimal solution. Until the paper
by Yoshizumi, Miura, and Ishida (2000), performance studies of A* mostly concentrated on the
number of nodes that A* expanded.1 Furthermore, a general result about optimality of A* with
1. Papers on memory-bounded A* such as the work of Russell (1992) and the more recent work of Zhou and Hansen
(2002) form one notable exception to this statement. However, saving time of heap operations by reducing the size
of the OPEN list was not the central focus of these papers.
c
2014
AI Access Foundation. All rights reserved.

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

respect to the expanded nodes has been established (Dechter & Pearl, 1985). However, the number
of expanded nodes clearly does not represent the performance of A* accurately enough. Rather,
the run-time performance of A* is determined by the net total of many factors, such as the number
of generated nodes and the time of updating the data structures (the OPEN and CLOSED lists).
Motivated by the need to view the performance of A* in this general context, we focus on both the
number of nodes that A* expands and the number of nodes that A* generates.
Let C  be the cost of the optimal solution for a particular problem instance. A* has to expand all
nodes n with f (n) < C  to guarantee optimality of the solution. These nodes have to be generated
as well. However, A* generates many nodes with f (n) = C  and nodes with f (n) > C  as well.
We call the latter group of nodes surplus (with f (n) > C  ). Surplus nodes are never expanded by
A* and thus do not contribute to finding an optimal solution. Non-surplus nodes are called useful.
Many important problem domains feature a large branching factor. When solving these problems, A* may generate a large number of surplus nodes. In fact, the execution time spent on
generating these nodes may be much larger than the time spent on generating and expanding the
useful nodes. When solving instances of such domains, the number of generated nodes is a central
performance issue.
A first step to address this problem in a general manner was taken by Yoshizumi et al. (2000).
They introduced a variant of A* called Partial Expansion A* (PEA*), which deals with the memory
aspect of the surplus nodes problem. Throughout the paper, n denotes the current node being
expanded (unless a different meaning is explicitly specified), while nc denotes a child of n. When
PEA* expands n, it generates all of ns children and adds to OPEN only the children with f (nc ) =
f (n). This guarantees that surplus nodes are not inserted into OPEN. The rest of the children are
discarded. n is re-inserted into OPEN with the f -cost of the best discarded child with f (nc ) > f (n)
(the one with minimal f -value). A node n can be expanded in this way several times, each time
with a different f (n). Note that PEA* addresses only the problem of high memory consumption
caused by storing surplus nodes in OPEN. At the same time, PEA* incurs a high price in terms of
run-time performance, since it may have to generate children of a given node more than once.
We preface the presentation of our contributions with the following observation. We know that
A* is optimal with respect to the number of expanded nodes, but does not give any optimality
guarantees with respect to the number of generated nodes. The reason for the optimality of A* is
the way it consults the heuristic to decide which node to expand next (i.e. the node with a minimal
g(n) + h(n) among the nodes in OPEN). However, A* does not take h(n) into consideration at
all when it generates the children of n. If A* could know, based on h, that nc is surplus without
actually generating nc , then we could avoid generating the surplus nodes altogether.
This observation leads directly to the first and most important contribution of this paper: a variant of A* called Enhanced Partial Expansion A* (EPEA*) (Section 3). EPEA* generates only those
children with f (nc ) = f (n). In contrast to PEA*, the other children are not even generated. This
is enabled by using a priori domain- and heuristic-specific knowledge to compute the list of operators that lead to the children with the needed f -cost without actually generating any children.
This knowledge and the algorithm for using it (also domain- and heuristic-specific) form an Operator Selection Function (OSF). For a given node expansion, an OSF returns the set of children with
f (nc ) = f (n) and the smallest f -value among the children with f (nc ) > f (n).
The following example gives a preview of EPEA*. Consider the four-connected grid-based
single-agent pathfinding domain (SAPF) with the Manhattan distance heuristic. The OSF represents
the following piece of domain- and heuristic-specific knowledge: if the goal is to the North-West of
142

fiE NHANCED PARTIAL E XPANSION A*

N

G
W

+0
+0

n

E
S

+2

+2

Figure 1: EPEA* uses domain and heuristic-specific knowledge to obtain the set of children nc of
the node n being expanded with f (nc ) = f (n).

the current location of the agent, then moving North or West will result in the same f -value, while
moving South or East will increase the f -value by two. An example of this knowledge is shown in
Figure 1. When EPEA* expands node n for the first time, it generates only the children nodes with
f (nc ) = f (n). The OSF tells EPEA* that these nodes result from applying the operators North and
West. Also, the OSF determines that the cost of n can be increased by 2, corresponding to the cost
of the best child that is not being generated. EPEA* keeps n with the new cost in OPEN. When n
is re-expanded, the children of n corresponding to the remaining operators (i.e. South and East) are
generated.
OSFs are domain and heuristic-dependent. The minimum prerequisite for applying EPEA* to
solve problem instances of a particular domain with a particular heuristic is the existence of the
full-checking OSF that we will introduce in Section 5 for those specific domain and heuristic.
The paper is organized as follows. PEA* is described in Section 2. We note that PEA* can
be described in terms of Collapsing the Frontier Nodes (CFN)  a general technique that has been
used by a number of well known algorithms. Section 3 presents EPEA*. Section 4 extends our
ideas to IDA* resulting in a variant of IDA* called Enhanced Partial Expansion IDA* (EPEIDA*).
Sections 5-8 describe and study experimentally the different kinds of OSFs. Sections 9 and 10
presents analytical insights into aspects of EPEA*. Section 11 touches upon the topic of using PEA*
and EPEA* with an inconsistent heuristic and exposes the trade-off that exists in that situation.
The paper introduces a significant amount of terminology. For the readers convenience, we
provided a glossary of terms in Appendix A.
A preliminary version of this paper was presented at AAAI-2012 (Felner et al., 2012). The current paper extends the conference version by a deeper treatment of the EPEA* algorithm including
an extended set of experimental and theoretical studies.

2. Background Knowledge
EPEA* advances the idea of Partial Expansion A* (PEA*) (Yoshizumi et al., 2000). We describe
PEA* in the general context of the technique called Collapsing Frontier Nodes (CFN).
143

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Figure 2: The subtree rooted at the node n is collapsed into n, which receives the new stored value
F (n) = 8.

2.1 Collapsing Frontier Nodes
Designate the set of leaf nodes of the search tree as the frontier of the search. Best-first search
algorithms choose a frontier node with the lowest cost for expansion. When n is expanded, n is
removed from the frontier and its children are added to the frontier. The frontier can be maintained
either in memory (OPEN in A*) or only logically (IDA*, RBFS). For example, the frontier of
IDA* for a given iteration are all the nodes that have been generated but not expanded in that
iteration (these nodes would have been stored in OPEN if A* was used). In either case, the following
admissibility invariant is maintained: for every node n in the frontier, the cost of n does not exceed
the cost of any possible solution that passes through n.
Let n be a non-leaf node of the search tree and R be a subset of the frontier, such that, for all r 
R, the path in the search tree from the root to r passes through n. That is, n is a common ancestor
of the nodes in R. The Collapsing Frontier Nodes (CFN) technique relies on the observation that
it is possible to obtain a smaller frontier by replacing R with n. Furthermore, the cost of n can be
increased, without violating the admissibility invariant, to the minimum cost among the nodes in R.
This is illustrated in Figure 2 where the part of the frontier corresponding to the entire left subtree
(i.e. R consists of three nodes with costs 9, 9 and 8) is collapsed into the node n. The cost of n is
now modified to be the minimum cost among the collapsed leaves (8 in this case).
Such collapse actions have been used by many algorithms. In SMA* (Russell, 1992), for example, once OPEN is too large, areas with the highest f -values in OPEN are collapsed. As another
example, consider IDA*. Once an IDA* iteration ends, it can be seen as collapsing the entire frontier into the start node, which gets the f -value of the least cost frontier node  the threshold for
the next iteration. In ITS (Ghosh, Mahanti, & Nau, 1994), IDA* is allowed to use a given amount
of memory. When that memory limit is exceeded, collapsing is used to remove the nodes with the
largest cost. Another important example is RBFS. Here only one branch of the tree plus the children
of nodes in that branch are kept in memory at all times. Any frontier below these nodes is collapsed.
We use the terminology coined by Korf (1993). The regular f = g + h value of a node is
designated as its static value and denoted by f (n). The f -value of the leaves that was propagated
up to their common ancestor n by a collapse action is called the stored value of n and denoted by
F (n). The static value of n in Figure 2 is 5, while its stored value after the collapse action is 8.
When we reach a node n in OPEN with a stored value F (n) = x such that x > f (n), we know
that n has been expanded before and that its least-cost frontier node had the static f -value of x. It is
144

fiE NHANCED PARTIAL E XPANSION A*

Figure 3: Example of PEA* and EPEA*. Two expansions of the node a are shown. Solid circles
denote the generated children, while the non-solid circles denote the discarded/collapsed
children.

important to note that each time a set of nodes is collapsed into n, the stored value of n may grow.
The stored value of a node never decreases.
2.2 Partial Expansion A*
Partial Expansion A* (PEA*) (Yoshizumi et al., 2000) is a variant of A* that reduces the memory
overhead of A* by not putting the surplus nodes into OPEN. PEA* preserves admissibility by using
collapse actions. EPEA*, the new variant of A* introduced in this paper, works similarly to PEA*.
Therefore, a clear grasp of PEA* will be essential for understanding EPEA*. In the following, we
will (1) define PEA* and (2) discuss the performance overheads of PEA* that EPEA* addresses.
For simplicity, we assume a consistent heuristic.2 We will touch upon the case of an inconsistent
heuristic in Section 11.
PEA* maintains both the static and the stored value for each node. The stored value is obtained
by collapse actions as we describe below. PEA* expands nodes in the order of the least stored value.
To avoid putting surplus nodes into OPEN, PEA* distinguishes two kinds of nc :
1. Provably useful, i.e. the children with f (nc )  F (n). Since n satisfies the admissibility
invariant, we have F (n)  C  . We infer that f (nc )  C  , which means that nc is useful.
Thus, checking that f (nc )  F (n) proves that nc is useful. In other words the children with
f (nc )  F (n) are provably useful in the sense that A* cannot be optimal without putting
these nodes into the OPEN list. That a node is provably useful does not necessary imply that
A* will expand this node.
2. Possibly surplus, i.e. the children with f (nc ) > F (n). Such a node nc might be useful, but
this cannot be proven at this point in the search. Note that a possibly surplus nc can become
provably useful when n is re-expanded with a higher stored value.
2. A heuristic h is called consistent if, for every node n and its child nc the inequality h(n)  h(nc ) + cost(n, nc )
holds. Equivalently, a heuristic h is consistent if it results in monotonically increasing f -values, i.e. if for every node
n and its child nc , the inequality f (nc )  f (n) holds.

145

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

When PEA* expands n, it generates all children of n, but inserts into OPEN only the provably
useful children with f (nc ) = F (n). Since F (n) can only grow, the provably useful children with
f (nc ) < F (n) are known to have been put into OPEN by previous expansions of n. The possibly
surplus children of n are collapsed into n, which is re-inserted into OPEN with the stored value
obtained by this collapse action. If n does not have possibly surplus children, it is not re-inserted
into OPEN, but is moved to CLOSED instead.
We refer to the provably useful children with f (nc ) = F (n) as currently needed. We refer to
the other children (i.e. the possibly surplus children and the provably useful children with f (nc ) <
F (n)) as currently unneeded. In other words, currently needed children of n are the ones that
PEA* would insert into OPEN during the current expansion of n. We designate operators that result
in currently needed children as currently needed. The other operators are designated as currently
unneeded.
The idea of PEA* is demonstrated in Figure 3. In the top part, when the node a is expanded, all
of its children (x, y, z, w) are generated. However, only the children with the f -value of 3 (x and
y) are currently needed. They are inserted into OPEN. All the other children are possibly surplus.
They are collapsed back into a, who gets the new stored value F (a) = 4 (since f (z) = 4 is the least
static value among the possibly surplus children of a). Following this, a is re-inserted into OPEN
with F (a) = 4. There are now two cases.
 No re-expansion: Assume that a node g with f (g) = 3 is the goal. In this case, a will not be
expanded again and the children of a with costs larger than 3 (z and w) will never be put into
OPEN.
 Re-expansion of a: When a is chosen for re-expansion with F (a) = 4 (Figure 3 (bottom)),
all its children are generated. Only the node z with f (z) = 4 is currently needed. It is placed
into OPEN. The possibly surplus node w is collapsed into a who gets the new stored value of
F (a) = 5. The other currently unneeded nodes (x and y) are discarded. Since a has possibly
surplus children, it is put back into OPEN.
The pseudo-code of PEA* is shown in Procedure 1. To clearly show the differences between
the different variants of A*, we show A*, PEA* and EPEA* in the same pseudo-code. To make
the pseudo-code easier to read, we assign a dummy stored value to the nodes in A*, although A*
does not really ever use it. When PEA* generates a new node, it sets the new nodes stored value to
be equal to its static value (lines 2, 18).3 Nodes are expanded in the order of the best stored value
(line 5). When expanding n, PEA* generates all of its children (line 7) and inserts into OPEN only
the children with f (nc ) = F (n) (lines 11-18). These nodes are currently needed. The children with
f (nc ) > F (n) are possibly surplus. They are collapsed into n, whose stored value becomes the
lowest static value among these children (line 14). The children with f (nc ) < F (n) are discarded
(line 15). If n has no possibly surplus children (i.e. all children of n have been inserted into OPEN),
then n is put into CLOSED (line 20). Otherwise, n is re-inserted into OPEN with the new stored
value (line 22).
3. Although the notion of stored value is defined only in the context of collapsing, we can view an yet unexpanded node
as being a frontier node that has been collapsed into itself.

146

fiE NHANCED PARTIAL E XPANSION A*

Procedure 1 A*, PEA* and EPEA*.
1: Generate the start node ns
2: Compute h(ns ) and set F (ns )  f (ns )  h(ns )
3: Put ns into OPEN
4: while OPEN is not empty do
5:
Get n with lowest F (n) from OPEN
6:
if n is goal then exit
// optimal solution is found!
7:
For A* and PEA*: set N  set of all children of n and initialize Fnext (n)  
8:
For EPEA*: set (N, Fnext (n))  OSF (n).
9:
for all nc  N do
10:
Compute h(nc ), set g(nc )  g(n) + cost(n, nc ) and f (nc )  g(nc ) + h(nc )
11:
For PEA*:
12:
if f (nc ) 6= F (n) then
13:
if f (nc ) > F (n) then
14:
Set Fnext (n)  min(Fnext (n), f (nc ))
15:
Discard nc
16:
continue
// To the next nc
17:
Check for duplicates
18:
Set F (nc )  f (nc ) and put nc into OPEN
19:
if Fnext (n) =  then
20:
Put n into CLOSED
// For A*, this is always done
21:
else
22:
Set F (n)  Fnext (n) and re-insert n into OPEN
// Collapse
23: exit
// There is no solution.

2.3 The Parameter C and Comparison of EPEA* with PEA*
PEA* saves memory by not putting surplus nodes into OPEN, but incurs a large time performance
overhead, since, whenever it re-expands a node n, it generates all of the children nodes of n and
computes their f -values, thus repeating the work that it did when it expanded n for the first time.
In domains where children of n can assume a large number of different static values, this run-time
overhead is large. To make PEA* practicable for these domains, the authors of PEA* introduced
a C-parameter, which determines the trade-off between the amount of memory saved and the runtime overhead paid. When a node n is expanded, the children with F (n)  f (nc )  F (n) + C are
added into OPEN (the change is made in line 12 of the above pseudo-code). When C = 0 maximal
memory savings are obtained. This is the variant shown in the pseudo-code. When C = , PEA*
becomes equivalent to A*. The best choice of C is domain-, heuristic- and instance-dependent and
no policy for selecting C has been reported.
EPEA* does not have this memory-time trade-off. It saves the same amount of memory as
PEA* with C = 0, while saving run-time as well. Therefore, in our experiments, we always
compare EPEA* to PEA* with C = 0.
We will show in Section 9 that, although PEA* is motivated by trying to make OPEN smaller,
it sometimes has exactly the opposite effect.
147

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

3. Enhanced Partial Expansion A*
This section presents our new variant of A*, the Enhanced Partial Expansion A* (EPEA*). We start
with an important observation.
3.1 The Three Phases on the Way to OPEN
One of key observations used in EPEA* is that every node passes three phases from its inception to
its entry into OPEN:
1. Discovering and checking an operator. The search algorithm (a) discovers an applicable
operator and (b) checks whether this operator should be applied. In many commonly used
domains, the list of applicable operators (step (a) above) is determined easily by looking at the
current state. In planning domains, discovering an operator consists of checking the operators
preconditions. In either case, both A* and PEA* generate children nodes corresponding to
all applicable operators. They do not perform step (b) above. Checking an operator refers
to deciding whether to apply an available operator. Until now, this step has gone unnoticed.
Distinguishing this step is a key idea of EPEA*.
2. Applying an operator. We will also refer to this phase as generating a node, since the search
algorithm generates a child node by applying an operator to the node being expanded. In
A*, generating a node is preceded by making a copy of the parent, while in IDA* this is not
needed.
3. Inserting a node into OPEN. The new node becomes part of the frontier of nodes maintained
by the search algorithm.
3.2 The Operator Selection Function (OSF)
The difference between EPEA* and PEA* can be summarized as follows. PEA* generates all
children nc of n with two objectives:
1. To put into OPEN the children with f (nc ) = F (n) and
2. To update the stored value F (n).
EPEA* achieves these objectives without generating all children. Instead, it employs a domainand heuristic-specific Operator Selection Function (OSF) to both generate only the children nc of
n with f (nc ) = F (n) and compute the next F (n). In this section, we describe OSF. In the next
section, we will present the EPEA* algorithm.
An OSF consists of two components:
1. The Knowledge Component  domain- and heuristic-specific knowledge.
2. The Algorithmic Component  an algorithm that uses the knowledge component to attain the
stated above objectives of the OSF.
For an example of OSF, consider the single-agent pathfinding domain (SAPF) in Figure 4 (left).
The agent is placed into a 4-connected grid (the shaded cell in the figure). The agents arsenal of
actions consists of 4 cardinal moves and a Wait move (where the agent stands still). Assume that
148

fiE NHANCED PARTIAL E XPANSION A*

N

G

W

3
3 4 5
5

E
S

f (nc )
0
1
2

h(nc )
-1
0
1

Operators
North, West
Wait
South, East

Fnext (n)
1
2


Figure 4: The knowledge component of an OSF for the single-agent pathfinding with the Manhattan
distance heuristic. The part of the knowledge component for the case of the goal located
to the north-west of the current location is shown.

all moves cost 1 and the heuristic function is Manhattan distance (MD). In the figure, the numbers
inside the cells are the corresponding h-values.
Consider the case when the goal is located to the north-west of the current location. The knowledge component of the OSF for this example is shown in Figure 4 (right) in the form of a table. This
table uses the following convenient notation, which we will continue to use throughout the paper.
Consider an expansion of n and the operator that produces the child nc . We denote the change in
heuristic value resulting from applying this operator by h(nc ) = h(nc )  h(n) and the change
in the f -value by f (nc ) = f (nc )  f (n). The current difference between the stored and the
static value of n is denoted by F (n) = F (n)  f (n). The next stored value of n is denoted by
Fnext (n), while the F (n) that corresponds to that stored value is denoted by Fnext (n). The
table groups the operators according to f (nc ) and orders the groups according to the increasing
order of this quantity. Note that, since the quantities f (nc ) and h(nc ) differ by cost(n, nc ),
which is a constant in this domain, ordering the operators by either f (nc ) or h(nc ) produces the
same result.
When EPEA* expands a node n with the stored value F (n) and needs to generate the children
nc with f (nc ) = F (n), it would invoke the algorithmic component of this OSF to:
1. Find the row that correspond to f (nc ) = F (n)  f (n),
2. Generate the currently needed children by applying the operators from that row, and
3. Return the next stored value of n: Fnext (n) = f (n) + Fnext (n).
OSFs are domain and heuristic-dependent. Using EPEA* to solve problem instances of a particular domain with a particular heuristic requires the creation of an OSF for those specific domain
and heuristic. We provide a classification of OSFs in Section 5. In particular, the minimum prerequisite for applying EPEA* is the existence of the full-checking OSF described in that section for a
given domain and heuristic. The classification of OSFs will also serve as general guideline for OSF
construction.
An OSF bears some resemblance to the concept of preferred operators, often used by domain
independent planners such as FF (Hoffmann & Nebel, 2001) and Fast Downward (Helmert, 2006;
Richter & Helmert, 2009).4 Preferred operators are a subset of actions that are assumed to be
more likely to lead to a goal. When expanding a node, FF generates nodes by using only the
4. In FF, preferred operators are called helpful actions.

149

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

preferred operators while ignoring all other actions. This often reduces search time at the cost of
the loss of completeness. Fast Downward uses preferred operators in a more conservative manner
that preserves completeness. Nodes generated by the preferred operators are prioritized so as to be
expanded more often. This is done by maintaining an additional OPEN list containing only nodes
generated by the preferred operators. Fast Downward alternates between expanding a node from the
additional OPEN list and the regular OPEN list containing all the generated nodes.
Preferred operators impose a binary partition over the set of operators: an operator is either preferred or not. By contrast, an OSF provides a finer grained operator partitioning, grouping operators
according to their f values. Moreover, neither FF nor Fast Downward is able to completely avoid
generating surplus nodes by using preferred operators, while the EPEA* algorithm described below
does exactly that by using an OSF.
Note that Fast Downward uses preferred operators in conjunction with another technique called
deferred heuristic evaluation (Helmert, 2006; Richter & Helmert, 2009). Deferred heuristic evaluation saves heuristic computation time by inserting generated nodes into OPEN with the f -value
of their parent. The heuristic of a node is computed only when the node reaches the top of the
OPEN list. Thus, deferred heuristic evaluation trades memory and the time spent on the OPEN list
operations for saving on the heuristic computation time. One can view deferred heuristic evaluation
as the opposite of the collapse action (Section 2.1): a collapse saves memory and the time needed
by the OPEN list operations.
We are now ready to describe the EPEA* algorithm.
3.3 Definition of EPEA*
The flow of EPEA* (Procedure 1) is identical to that of PEA*: (1) it puts nodes into OPEN and
expands them in the same order as PEA* and (2) it collapses the possibly surplus children nodes.
Therefore, the pseudo-code of EPEA* is similar to that of PEA*. The few differences stem from
the fact that EPEA* uses a domain- and heuristic-specific Operator Selection Function (OSF):
1. Instead of generating all children nodes and discarding the currently unneeded ones (PEA*,
lines 7 and 15), EPEA* uses the OSF to generate only the currently needed children nodes to
start with (line 8).
2. Instead of looking at all children of n to compute its next stored value (PEA*, line 14), EPEA*
receives that value from the OSF (line 8).
We have already seen the operation of PEA* for the example in Figure 3. Consider the operation
of EPEA* for the same example. In Figure 3 (top), EPEA* uses an OSF to generate only the
currently needed children x and y. It does not generate the nodes z and w, since they are possibly
surplus. This OSF also determines the next stored value of a (F (a) = 4)  the lowest cost among
the nodes z and w. In Figure 3 (bottom), the children x, y and z are provably useful. However,
only z is currently needed and is generated by the OSF, since the costs of x and y (3) are lower
than F (a) (4). It is important to clearly see the distinction between PEA* and EPEA*: the nodes
generated and discarded by PEA* are the same nodes that EPEA* does not generate.
For an example with a concrete OSF, consider the operation of EPEA* for the SAPF example
in Figure 4 (left) with the OSF shown in Figure 4 (right) and described in Section 3.2. Suppose that
n (with the agent in the shaded cell) is expanded for the first time with f (n) = F (n) = 10. EPEA*
uses the OSF to get the children with f (nc ) = F (n) = 10. That is, we are interested in the children
150

fiE NHANCED PARTIAL E XPANSION A*

with f (nc ) = 0. The OSF uses the information in Figure 4 (right) to produce the children that
correspond to the operators North and West. The children with f (nc ) > F (n) are collapsed into n,
which gets the stored value determined by f -value of the operator in the next row. In this case, the
next row contains the operator Wait with f = 1, so the next stored value of n will be Fnext (n) =
f (n) + 1 = 11. For convenience, the next value of F (n), Fnext (n) = Fnext (n)  f (n), is
shown in the rightmost column of the table.
If n is re-expanded with f (n) = 10 and F (n) = 11, the OSF returns the nodes nc with f (nc ) =
F (n) = 11 or, equivalently, f (nc ) = 1. This corresponds to applying the Wait operator. The
remaining children with f (nc ) > F (n) will be collapsed into n, which will get the stored value of
f (n) + Fnext (n) = 10 + 2 = 12.
Note that the table in Figure 4 (right) is only for expanding n located to the South-East of the
goal. The complete knowledge component of this OSF for this domain contains seven more tables
for the other possible locations of n relative to the goal.
3.4 A High-Level Comparison of A*, PEA* and EPEA*
In this section, we compare A*, PEA* and EPEA* with respect to their memory and run-time
performance.
3.4.1 M EMORY P ERFORMANCE
A* puts into OPEN every node that it generates and that has not been generated before with the
same or lower cost. PEA* improves on the memory performance of A* by not putting into OPEN
the currently unneeded nodes. The actual memory saving is determined by the parameter C (see
Section 2.2), with the greatest saving achieved by setting C = 0. EPEA* puts into OPEN the same
nodes as PEA* with C = 0. Therefore, EPEA* affords the same memory savings as the most
memory-effective variant of PEA*.
3.4.2 RUN -T IME P ERFORMANCE
First, compare the node performance of the three algorithms:
 A* is known to be optimal with respect to the number of expanded nodes (Dechter & Pearl,
1985). It does not give any guarantees with respect to the number of generated nodes.
 Consider the first expansion of each node expanded by PEA*. These are the same expansions
that A* performs. Also, PEA* performs these expansions in the same order as A*. Therefore,
PEA* is optimal with respect to the number of unique node expansions.5 However, PEA* may
have to re-expand the same node many times. During each re-expansion, PEA* generates all
of the nodes children. Therefore, PEA* may perform many more node generations than A*.
 EPEA* expands the same nodes as PEA* in the same order. Therefore, just like PEA*,
EPEA* is optimal with respect to the number of unique node expansions. Unlike PEA*,
EPEA* does not generate the currently unneeded nodes. Therefore, EPEA* may skip generating some states that A* would generate. In fact, optimality results with respect to the
5. Although the number of unique node expansions is not of practical interest, it is interesting as a means of understanding the algorithm and its performance. Therefore, we measure the number of unique node expansions in some of our
experiments.

151

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Variant

Which nodes are generated

Which nodes are put into OPEN

A*
PEA*
EPEA*, full-checking OSF

All
All
Currently needed,
obtained by checking all operators
Currently needed,
obtained by direct computation

All
Currently needed
Currently needed

EPEA*, direct-computation OSF

Currently needed

Table 1: High-level comparison of A*, PEA* and EPEA*.
number of generated nodes were recently proven about variants of EPEA* (Goldenberg, Felner, Stutervant, Holte, & Schaeffer, 2013).
Now consider the run-time performance of the three algorithms:
 Depending on the domain and the heuristic, A* may spend much time generating surplus
nodes. Also, A* may have to pay the penalty related to its usage of large amounts of memory.
In particular, the OPEN list operations become expensive when the size of the search frontier
grows.
 In most cases (see Section 9), PEA* uses a smaller amount of memory than A*, but pays the
overhead of possible re-expansions of a single node and generating all children of a node for
each re-expansion.
 Similar to PEA*, EPEA* has to pay the price of many possible re-expansions of a single node.
However, EPEA* avoids the two run-time overheads that PEA* suffers from: generating the
same useful node many times and generating possibly surplus nodes. The significance of
these savings depends on the size of the states of the domain and the amount of computation
that needs to be performed to apply an operator.
Depending on the kind of OSF being used (the different kinds of OSFs will be considered in
Section 5), EPEA* may be able to avoid not only generating the currently unneeded nodes,
but also checking the operators that result in these nodes (i.e. the currently unneeded operators). The differences between A*, PEA* and EPEA* with respect to the phases described
in Section 3.1 are summarized in Table 1. We study the time performance of EPEA* more
deeply in Section 10.

4. Enhanced Partial Expansion IDA* (EPEIDA*)
We begin by noting that IDA* can be viewed as having partial expansion built into the algorithm.
Namely, consider the current iterations threshold as the stored value of all nodes during this iteration. When IDA* completes an iteration, it collapses all of the frontier nodes of the current iteration
into the root node, which gets an updated stored value  the next iterations threshold. Suppose that
the current iterations threshold is T . Once n is expanded, all its children are generated. At the
next level of the depth-first search, the children nc of n with f (nc )  T are expanded, while the
children with f (nc ) > T are discarded. This is partial expansion with the following one difference.
152

fiE NHANCED PARTIAL E XPANSION A*

In PEA*, the currently needed children are the children with f (nc ) = F (n). The children with
f (nc ) < F (n) do not need to be put into OPEN, since such children were put into OPEN during
the previous expansions of n. IDA* does not store information from the previous iterations and
therefore it needs to search the children with f (nc ) < T as well. Therefore, in the context of IDA*,
we need to re-define the notion of currently needed children of n to include all children nc with
f (nc )  T .
We now describe EPEIDA*. To clearly see the distinctions between IDA* and EPEIDA*, both
variants are shown in the same pseudo-code (Procedure 2). The first difference is that EPEIDA* uses
an OSF to obtain both the list of currently needed children (those with f (nc )  T ) and the lowest
cost among the currently unneeded children (line 10). The latter cost is used to update the threshold
Tnext for the next iteration (line 11). This threshold is initialized to infinity at the beginning of the
current iteration (not shown in the pseudo-code). In contrast, IDA* generates all children of n and
updates the value of the threshold for the next iteration by iterating through the list of these children
(IDA*, lines 5, 8). The second difference is that EPEIDA* does not need to check the threshold
condition (IDA*, line 4). This is because the threshold condition is equivalent to the condition in
our definition of a currently needed node for IDA*. Since OSF generates only the currently needed
children, no additional check is needed.
The number of nodes that EPEIDA* generates is approximately b times smaller than the number
of nodes generated by IDA*, where b is the average branching factor of the domain. To verify
this, let X be the number of nodes expanded by the last iteration of IDA*. The number of nodes
generated by this iteration can be approximated by bX. On the other hand, EPEIDA* generates up
to X + (b  1)d nodes (more details are given in Section 10.1), where d is the depth of the search.
Since (b  1)d is usually very small compared to X, the ratio between bX and X + (b  1)d is
approximately equal to b. We will repeatedly point to this fact in the discussion of our experimental
results in Sections 6-7.
The experimental results will compare the performance of EPEIDA* and the performance of
IDA* for several domains and heuristics. When studying those results, it is important to have in
mind that IDA* can be viewed as having partial expansion built into the algorithm. Therefore, when
we compare EPEIDA* with IDA*, we are really comparing EPEIDA* with the iterative deepening
version of PEA*.

5. Classification of OSFs
Recall the high-level comparison of A*, PEA* and EPEA* in Table 1. In that table, a principle
distinctions between EPEA* and PEA* is shown: EPEA* applies only the currently needed operators to generate only the currently needed children, while PEA* generates all children of the node
n being expanded. Furthermore, Table 1 shows that two possibilities exist for obtaining the list of
currently needed operators. These possibilities correspond to two classes of OSFs:
1. A full-checking OSF obtains the list of currently needed operators by checking all operators,
including the currently unneeded ones.
2. A direct-computation OSF obtains the list of currently needed operators by means of direct
computation.
These two kinds of OSF are the focus of this section. In addition to these two pure classes,
we will encounter a hybrid OSF. This will happen in domains and heuristics for which we are
153

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Procedure 2 IDA* and EPEIDA*.
Variables:
i  current depth in the search tree
T  threshold of the current iteration
Tnext  threshold for the next iteration, set to infinity before calling DFS().
1: DFS(n, i, T )
2:
Compute h(n) and set f (n)  g(n) + h(n)
3:
For IDA*:
4:
if f (n) > T then
5:
Set Tnext = min(Tnext , f (n))
6:
return
// Threshold cut-off
7:
if n is goal then halt
// The optimal solution is found!
8:
For IDA*: Set N  set of all children of n
9:
For EPEIDA*:
10:
Set N, f BestN otN eeded  OSF (n)
11:
Set Tnext = min(Tnext , f BestN otN eeded)
12:
for all nc  N do
13:
Set g(nc )  g(n) + cost(n, nc )
14:
Call DF S(nc , i + 1, T )

able to construct a direct-computation OSF to generate only the currently needed nodes for certain
values of f (nc ), but not for the other values of f (nc ). For these domains, we construct a
hybrid OSF, which can behave either as a direct-computation or as a full-checking OSF for different
(re-)expansions of n depending on the required f (nc ).
We will show below (Sections 6-8) how an OSF can be constructed for some well known domains and heuristics. For now, we explain the two pure classes of OSFs using the SAPF domain as
a running example.
5.1 Full-Checking OSFs
The knowledge component of the full-checking OSF for SAPF consists of eight tables. Each table
corresponds to one of the eight possible positions of n relative to the goal. The algorithmic component of this OSF determines which of these tables must be used for a given n based on the position
of n relative to the goal. For example, the table for the case of n located South-East of the goal (e.g.
the shaded cell in Figure 5 (left)) is shown in Figure 5 (right). From now on, we use the table in
Figure 5 (right) as the running example.
For each operator applicable to n, the table records the resulting changes in the h- and f -values.
The OSFs algorithmic component needs to check each of the operators to decide which children
nodes are currently needed. It then generates only these children. Also, it computes the next stored
value of n, Fnext (n), as follows. Fnext (n) is initialized to infinity. Whenever the OSF checks an
operator that results in nc and determines that nc is possibly surplus (i.e. f (nc ) > F (n)), it updates
Fnext (n) to be Fnext (n) = min(Fnext (n), f (nc )).
154

fiE NHANCED PARTIAL E XPANSION A*

N

G

W

3
3 4 5
5

E
S

Operator
Wait
East
South
West
North

h(nc )
0
1
1
-1
-1

f (nc )
1
2
2
0
0

Figure 5: The knowledge component of a full-checking OSF for the single-agent pathfinding with
the Manhattan distance heuristic. The part of the knowledge component for the case of
the goal located to the north-west of the current location is shown.

5.2 Direct-Computation OSFs
A direct-computation OSF computes the list of the currently needed operators directly, without the
need to check all the applicable operators. In the case of SAPF, the knowledge component of a
direct-computation OSF can be obtained from the described above full-checking OSF by ordering
the rows of each of the eight tables in the order of increasing f (nc ) and merging the rows corresponding to the same f (nc ). For the table in Figure 5 (right), the corresponding knowledge
component of the direct-computation OSF is shown in Figure 4 (right). Given the required f (nc ),
the row with the currently needed operators can be found by the algorithmic component quickly
without checking all operators.6 EPEA* will then apply these operators to generate the corresponding currently needed children nodes.
In general, a direct-computation OSF can be constructed when the states of the domain can be
classified into several classes, such that, for each class C, one of the following holds:
 A table with operators applicable to states in C ordered by f (nc ) can be computed and
stored before the main search begins. This is the option that we used in the above OSF for
SAPF. In that case, each class corresponded to one of the eight possible locations of n with
respect to the goal.
 The set of operators with a given f (nc ) can be computed on-the-fly during the search. This
is the option that we will use in Section 6.2 for the pancake puzzle with the GAP heuristic (Helmert, 2010).
Even if one or both of the above conditions are satisfied, it may be impossible or impractical
to construct a direct-computation OSF for a given domain and heuristic for one or more of the
following reasons:
1. To construct the tables of operators ordered by f (nc ), a large number of state classes must
be defined, resulting in large time and memory requirements to pre-compute and store these
tables.
2. To determine which class a given state belongs to, all operators applicable to that state need
to be checked.
6. In a domain with a large number of f (nc )-values, binary search or a hashing mechanism can be used.

155

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Both these reasons will be clarified in Section 6.1, where we will explain why we cannot construct
a direct-computation OSF for the 15-Puzzle with the Manhattan distance heuristic.
Note that even in the case when a direct-computation OSF is not available for a given domain
and heuristic, we can still construct a direct-computation OSF for each given search node. Namely,
when n is expanded for the first time, we can compute a table of operators applicable to n ordered
by f (nc ) and store this table together with n in OPEN. During the subsequent re-expansions of
n, this table can be used as the knowledge component of a direct-computation OSF. The decision of
whether to construct a direct-computation OSF for individual search nodes depends on the following
time-memory trade-off:
 Using this technique eliminates the need to check all operators at each re-expansion of n. The
amount of saved execution time depends on how many times n is re-expanded, which in turn
depends on how many different f -values are taken by the children of n.
 Storing a table of applicable operators with every node in OPEN may be prohibitively expensive (memory-wise) for the domains with high average branching factor.

6. Experimental Study of the Different Kinds of OSFs
In this and the two following sections, we construct an OSF and show experimental results for
several domains and heuristics. We start with relatively simple OSFs in this section and move
towards the more complicated PDBs-based OSFs in Section 7. We then move to the yet more
complicated additive PDBs-based OSFs in Section 8. Each description of an OSF is immediately
followed by an experimental study of that OSF.
We start with EPEIDA* for two domains and heuristics:
 The 15-puzzle with the Manhattan distance heuristic. For this domain, we construct a
full-checking OSF and explain why constructing a direct-computation OSF is impractical.
 The pancake puzzle with the GAP heuristic. For this domain, we construct a hybrid OSF.
We will see that, in practice, this OSF behaves as a direct-computation OSF for most expansions.
One can easily come up with interesting domains and heuristics, for which a pure direct-computation
OSF can be constructed. However, we decided to focus on well known benchmark domains. We
will be able to show direct-computation OSFs for such domains in the context of OSFs based on
pattern databases (Culberson & Schaeffer, 1998; Felner, Korf, & Hanan, 2004) in Sections 7 and 8.7
6.1 A Full-Checking OSF for the 15-puzzle
The 15-puzzle consists of a 4  4 square frame containing 15 numbered square tiles, and an empty
position called the blank. The legal operators are to slide any tile that is horizontally or vertically
adjacent to the blank into the blank position. The problem is to rearrange the tiles from some random
initial configuration, e.g. Figure 6 (left), into the configuration in Figure 6 (middle). Manhattan
distance (MD) is the classic heuristic function for this puzzle. It is computed by counting the
7. We saw above a direct-computation OSF for SAPF. However, searches in the state space of SAPF feature a small
number of surplus nodes, which makes it an uninteresting application of EPEA*.

156

fiE NHANCED PARTIAL E XPANSION A*

6

1

5

8

13

3

9

4 14

7

1

2

3

4

5

6

7

8

9 10 11

2

12 10 15 11

Operator
13 moves East
6 moves South
3 moves West
14 moves North

12 13 14 15

h(nc )
1
-1
1
1

f (nc )
2
0
2
2

Figure 6: Left: a possible state of the 15-Puzzle. Middle: the goal state of the 15-Puzzle. Right: part
of the knowledge component of an OSF for the 15-Puzzle with the Manhattan distance
heuristic for the case when the set of applicable operators is as in the state on the left.

number of grid units that each tile is displaced from its goal position, and summing these values
over all tiles.
6.1.1 T HE OSF
In the 15-puzzle, the quantity f (nc ) of each operator applicable to n is completely determined by:
1. The location of the blank,
2. The identity of the tile being moved by the operator, and
3. The position of that tile relative to the blank.
Therefore, we can construct the knowledge component of a full-checking OSF for the 15-puzzle as
a three-dimensional array with dimensions 16  15  4. These dimensions stand for the 16 possible
locations of the blank, 15 possible identities of the tile being moved and 4 possible positions of the
tile being moved relative to the blank. Each element in this array is the f (nc ) of the corresponding
operator.
Example. For the operators applicable to the node n in Figure 6 (left), the knowledge component of the OSF would store the f (nc )-values shown in Figure 6 (right). For example, for
the operator that moves 13 into the blank position, the f (nc )-value (2) is stored in the element
[6][13][0], where 6 denotes the position of the blank, 13 is the identity of the tile being moved and 0
denotes the operator that moves the tile to the West of the blank into the blank position. For the same
node n, the algorithmic component of this OSF would (1) check the operators in Figure 6 (right) by
looking up the corresponding entries in the three-dimensional array (i.e. the knowledge component),
(2) generate the currently needed nodes by applying the operators with f (nc ) = F (n)  f (n),
and (3) compute the next stored value of n using the minimal f (nc ) among the possibly surplus
children: Fnext (n) = f (n) + f (nc ).
To construct a direct-computation OSF for this domain, we need to classify states such that two
states s1 and s2 are in the same class if and only if (1) the location of blank is same in s1 and s2 and
(2) the identities of tiles surrounding the blank and their positions relative to the blank are same in
s1 and s2 . Therefore, we need to define 16(15141312) classes. Pre-computing and storing
a table of operators for each of these classes is a computation and memory overhead. Furthermore,
to decide which class a given state belongs to, the OSF will need to look at the identities of the tiles
157

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Gen Nodes
Time (ms)

IDA*
363,028,079
17,537

EPEIDA*
184,336,705
14,020

Ratio
1.97
1.25

Table 2: Comparison of generated nodes and time performance of IDA* and EPEIDA* for the 15Puzzle.

surrounding the blanks, which is equivalent to checking all operators. We conclude that constructing
a direct-computation OSF for the 15-puzzle domain with the MD heuristic is impractical. The
provided below experimental results were obtained with the full-checking OSF.
6.1.2 E XPERIMENTAL R ESULTS
Optimal solutions to random instances of the 15-puzzle were first found by Korf (1985) using IDA*
and the MD heuristic. Korf has graciously made this code available to the public. In this code
(known to be highly optimized), a look-up table is pre-computed to give the heuristic value based
on the current location of the tile, the operator chosen and the tile currently occupying the proposed
new location of the blank. Note that this information is exactly the knowledge component of the
full-checking OSF described above. However, Korfs code did not exploit this information to avoid
generating the currently unneeded nodes. Instead, it generates all the children nodes and uses the
look-up tables only for the heuristic calculation.
Table 2 presents the results of running Korfs IDA* code and EPEIDA* on his 100 random
instances. We observe a factor of 1.97 reduction in the number of generated nodes. This number is
close to the asymptotic branching factor of this domain when only the reverse moves are eliminated,
which was reported to be 2.13 (Edelkamp & Korf, 1998). This reduction in the number of generated
nodes translates to a 1.25-fold improvement in run-time, which is significant given the well known
efficiency of Korfs code. The timing results were obtained on Dell Optiplex 760.
6.2 A Hybrid OSF for the Pancake Puzzle
The pancake puzzle (Dweighter, 1975) is analogous to a waiter navigating a busy restaurant with
a stack of N pancakes. The waiter wants to sort the pancakes ordered by size. Having only one
free hand, the only available operation is to lift a top portion of the stack and reverse it. A state is
a permutation of the values 1...N . Each state has N  1 children, with the k th successor formed
by reversing the order of the first k + 1 elements of the permutation (1  k < N ). For example,
if N = 4 the children of state (1, 2, 3, 4) are (2, 1, 3, 4), (3, 2, 1, 4) and (4, 3, 2, 1). However,
we may safely not consider two successors  the parent position and the complete reverse of the
pancakes (the latter can be ignored for most search nodes when the GAP heuristic described below
is used). Therefore, the branching factor of this domain is approximately equal to the number of
pancakes minus two. Since all states are reachable from any start state, the size of the state space
is N !. A number of heuristics based on pattern databases have been used for this puzzle (Zahavi,
Felner, Holte, & Schaeffer, 2008; Felner, Zahavi, Holte, Schaeffer, Sturtevant, & Zhang, 2011;
Yang, Culberson, Holte, Zahavi, & Felner, 2008), but the GAP heuristic discussed by Helmert
significantly outperforms them all (Helmert, 2010).
158

fiE NHANCED PARTIAL E XPANSION A*

6

1

3

4

5

7

2

Figure 7: A state of the Pancake Puzzle with seven pancakes. An operator can affect the gap at one
location only.

We now describe the GAP heuristic. Two integers a and b are consecutive if |a  b| = 1. For a
given state, a gap at location j occurs when the pancakes at location j and j + 1 are not consecutive.
The goal can be defined such that pancake 1 is at location 1 and there are no gaps. The GAP heuristic
iterates through the state and counts the number of gaps. Since an operator can reduce the number
of gaps by at most one, this heuristic is admissible.
To gather the insight necessary to construct an OSF, consider the example in Figure 7. There are
seven pancakes, with pancake 6 occupying location 1. Consider the operator that reverses the first
five pancakes. We note that this operator does not affect the number of gaps between the pancakes
being reversed (i.e. the gaps at locations 1-5 between the pancakes 6, 1, 3, 4, 5). Indeed, only the
gaps adjacent to the extreme pancakes being reversed (i.e. pancakes 6 and 5) are affected. In our
case, the consecutive pancakes 6 and 7 become adjacent, while the gap between pancakes 5 and 7
ceases to exist. Thus, just by looking at three pancakes (in this case, 5, 6, and 7), we know that the
number of gaps (and hence the GAP heuristic) has been decreased by one and f (nc ) = 0.
More generally, an operator that reverses j pancakes affects only the gaps formed by the three
pancakes at locations 1 (pancake P ), j (pancake X), and j + 1 (pancake Y ). Three cases are
possible:
1. X and Y were not consecutive but P and Y are. In this case, one gap is removed and the
heuristic value decreases by one. This corresponds to f (nc ) = 0.
2. Both the pancakes X, Y and the pancakes P, Y form or do not form a gap. In this case the
heuristic value does not change. This corresponds to f (nc ) = 1.
3. The pancakes X, Y do not form a gap, while the pancakes P, Y do. In this case, a new gap is
introduced and the heuristic grows by one. This corresponds to f (nc ) = 2.
Suppose that we need to generate children of n with f (nc ) = 0. We note that this can be done
without checking the currently unneeded operators (i.e. operators with f (nc ) > 0). To do this,
we classify the states based on the identity of the pancake in location 1. The state shown in Figure 7
belongs to the class with the pancake 6 in location 1. We enable the OSF to compute operators
with f (nc ) = 0 for each class on-the-fly by maintaining the following additional information for
each node of the main search. For each pancake, we keep the current location of the pancakes that
are consecutive to it (note that there are at most two such pancakes; in the example, the pancakes
consecutive to the pancake 6 are 5 and 7 in locations 5 and 6, respectively). Now, suppose that n
has pancake P in location 1. Then, all we have to do is to use the information stored with P to
locate pancakes P  1 and P + 1 and check the pancakes directly to the left of these pancakes. For
example, if P + 1s left neighbor is any pancake other than P + 2, then reversing the pancakes to
159

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

#

IDA*

20
30
40
50
60
70

18,592
241,947
1,928,771
13,671,072
92,816,534
754,845,658

Generated Nodes
EPEIDA*
Total
Full-checking OSF
1,042
8,655
50,777
284,838
1,600,315
11,101,091

12
25
49
131
121
1,118

ratio

IDA*

Time (ms)
EPEIDA*

ratio

17.84
27.95
37.98
47.99
57.99
67.99

1.5
24.9
247
2,058
16,268
155,037

0.1
1.2
8.5
57
359
2,821

11.23
20.00
30.75
36.15
45.32
54.90

Table 3: Comparison of generated nodes and time performance of IDA* and EPEIDA* for the
Pancake Puzzle.

the left of P + 1 will decrease the GAP heuristic and result in f (nc ) = 0. Thus, we can construct
a direct-computation OSF for the case of f (nc ) = 0.
We cannot construct a direct-computation OSF for all cases of f (nc ), since we have to check
all operators to find the operators resulting in f (nc ) = 1 or f (nc ) = 2. For these cases,
a full-checking OSF is constructed. This OSF has no knowledge component. The algorithmic
component simply checks all the operators applicable to n and, for each operator, computes the
resulting f (nc ) by looking at the three affected pancakes as described above. If the operator is
currently needed, then the corresponding child node is generated.
The experimental results for 100 random instances for 20 to 70 pancakes are given in Table 3.
In this table, we compare the performance of EPEIDA* and IDA*, both using the GAP heuristic.
For 70 pancakes, EPEIDA* generated (the column titled Total) 68 times fewer nodes than IDA*.
Most of this is reflected in the running time (54-fold). To the best of our knowledge, these are the
state-of-the-art results for this puzzle. Note that for all versions tested of this domain, the reduction
in the number of generated nodes is almost the same as the branching factor of the domain.
To better understand the behavior of the hybrid OSF for this domain, compare the numbers in
the two columns for the EPEIDA* generated nodes. The column on the left shows the total number
of generated nodes, while the column on the right shows how many nodes were generated by using
a full-checking OSF. The comparison between these two columns reveals that, for the vast majority
of expansions, only the operators with f (nc ) = 0 were currently needed, whereby EPEIDA*
applied the direct-computation OSF.

7. OSFs Based on Pattern Databases
Pattern Databases (PDBs) (Culberson & Schaeffer, 1998; Felner et al., 2004) are a powerful method
for automatically building admissible memory-based heuristics based on domain abstractions. After
a short background section, we explain how PDB-based full-checking and direct-computation OSFs
can be constructed and provide experimental results for the Rubiks cube domain with the CornersPDB heuristic.
7.1 Background
View a state of a domain as an assignment of values to a number of domain variables (hereafter
variables). The main idea of PDBs is to abstract the state space by only considering a subset of
160

fiE NHANCED PARTIAL E XPANSION A*

the variables. For a concrete choice of variables, this abstraction is formalized as an abstraction
mapping, which we denote by  and immediately define. For each state s of the original state, the
abstract state (or pattern) (s) is the projection of s onto the variables that participate in  (or, for
short, the projection of s onto ).
A PDB for a given  is constructed by performing a full breadth-first search in the abstract state
space from the abstract goal, i.e. (g), where g is the goal state. Distances to all abstract states
are calculated and stored in a lookup table (PDB). These values are then used throughout the search
as admissible heuristics for states in the original state space. Formally, for each state s, the PDB
contains the distance in the abstract space from (s) to (g). When a heuristic value is required for
n, one simply looks up PDB[(n)].
An interesting variation of PDBs are the instance-dependent pattern databases (IDPDBs) (Felner
& Adler, 2005; Zhou & Hansen, 2004; Silver, 2005). IDPDBs can be built lazily during the search
and are particularly effective in domains where the abstract space is too big to be stored completely
in memory. At first, a directed search in the pattern space is performed from the goal pattern to
the start pattern (unlike regular PDBs where a complete breadth-first search is performed). All the
patterns seen in this search are saved in the PDBs. Then, the main search in the real state space
begins. As more nodes are generated, the search in the pattern space is continued lazily and more
PDB values are found and stored. Hierarchical search algorithms, such as Hierarchical A* (Holte,
Perez, Zimmer, & MacDonald, 1996) and Switchback (Larsen, Burns, Ruml, & Holte, 2010), use a
hierarchy of abstract spaces to create a hierarchy of PDBs, also created lazily in a similar manner.
We will use IDPDBs in our experiments in Section 8.4.
We now present a method for constructing a PDB-based OSF.
7.2 PDB-Based OSF
Let  be an abstraction mapping for a given domain. Note that each operator in the original space
has a projection onto   an abstract operator that modifies the variables of (s) in the same way
that the original operator modifies these variables in s.
For a given , we can construct either a full-checking or a direct-computation OSF. Intuitively,
the knowledge components of these OSFs are similar to the tables that we constructed for SAPF in
Sections 5.1 and 5.2. Recall that, for SAPF, the knowledge component of both OSFs consisted of
eight tables. Each table corresponded to one of the eight possible positions of n relative to the goal.
Each table recorded the operators applicable to n and the resulting f (nc ). Depending on the kind
of OSF, the tables were sorted by either the operators or the f (nc )-values. We adopt this method
to construct an OSF based on  as follows. The knowledge component of the OSF contains:
 A table for each abstract state a. This table records the abstract operators applicable to a as
well as the resulting change in the f -value. We will denote an abstract operator by , the
result of applying  to a by ac (= (a)) and the resulting change in the f -value by f (ac ).
 Depending on whether we are building a full-checking or a direct-computation OSF, each
table is sorted either by abstract operators or by f (ac ).
We call the data structure employed by this knowledge component a -PDB and distinguish a
-PDB sorted by operators (used as the knowledge component of the full-checking OSF) and a
-PDB sorted by f (ac ) (used as the knowledge component of the direct-computation OSF).
It is important to note that:
161

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

1. A -PDB does not include the regular PDB that it is based on. Rather, -PDBs are additional
data structures that enable a PDB-based OSF. In some domains, there is a trade-off between
(1) building a -PDB, which affords using EPEA* and (2) building an additional PDB, which
affords a more accurate heuristic. We do not explore this trade-off.
2. The entry of a -PDB is different than the entry of a regular PDB. An entry of a regular PDB
contains one value  the distance from a given abstract state to the abstract goal. An entry of
a -PDB contains a table of abstract operators and f (ac )-values for a given abstract state.
The algorithmic component of the full-checking PDB-based OSF operates as follows. For each
operator o applicable to n, the OSF (1) locates the table for (n) in the -PDB ordered by operators
and (2) looks up in this table the f (ac )-value of the projection of o onto . If this value is the
required f (nc ), nc is included in the set of currently needed children.
The algorithmic component of the direct-computation OSF operates as follows. For a given n
and required f (nc ), the OSF (1) locates the table for (n) in the -PDB ordered by f (ac )values and (2) looks up in this table the abstract operators with the f (ac )-value equal to the
required f (nc ). For each such abstract operator, the OSF determines the set of operators in the
original space that correspond to these abstract operators. Thus, a direct-computation OSF can be
constructed only if every abstract operator can be efficiently mapped to the corresponding operator(s) in the original space.
It should be clear that, unless the branching factor of the abstract space is large, the improvement
in the run-time performance afforded by using the direct-computation OSF rather than the fullchecking OSF cannot be large. This is because the algorithmic components of both kinds of OSF
need to perform only one lookup in the -PDBs. The only difference is that the full-checking OSF
needs to scan through the array of f (ac )-values corresponding to all abstract operators, which is
usually not a large overhead.
In the next section, we consider an OSF for Rubiks Cube with the Corner PDBs heuristic as
a simple example of a PDB-based OSF. A more involved example  an OSF based on additive
PDBs (Felner et al., 2004; Yang et al., 2008) for Multi-Agent Pathfinding domain (MAPF) (Standley, 2010; Sharon, Stern, Goldenberg, & Felner, 2011, 2012) will be given in Section 8.
7.3 An OSF for Rubiks Cube
Rubiks Cube was invented in 1974 by Erno Rubik of Hungary. The standard version consists of a
3  3  3 cube with different colored stickers on each of the exposed squares of the sub-cubes, or
cubies. There are 20 movable cubies and 6 stable cubies in the center of each face. The movable
cubies can be divided into eight corner cubies, with three faces each, and twelve edge cubies, with
two faces each. Corner cubies can only move among corner positions, and edge cubies can only
move among edge positions.
Each one of the 6 faces of the cube can be rotated 90, 180, or 270 degrees relative to the rest
of the cube. This results in 18 possible moves for each state. Since twisting the same face twice
in a row is redundant, the branching factor after the first move can be reduced to 15. In addition,
movements of opposite faces are independent. For example, twisting the left face and then the right
face leads to the same state as performing the same moves in the opposite order. Pruning redundant
moves results in a search tree with an asymptotic branching factor of about 13.34847 (Korf, 1997).
162

fiE NHANCED PARTIAL E XPANSION A*

#

12
13
14
15

IDA*

EPEIDA*
ratio
IDA* EPEIDA*
Corner PDB
Generated Nodes - Thousands
Time (mm:ss)
45,800
3,441 13.31
0:05
0.01
434,671
32,610 13.32
0:53
0:15
3,170,960
237,343 13.37
5:31
1:32
100,813,966 7,579,073 13.30 175:25
47:16

ratio

4
3.53
3.68
3.71

Table 4: Comparison of generated nodes and time performance of IDA* and EPEIDA* for the
Rubiks Cube.

In the goal state, all the squares on each side of the cube are the same color. The puzzle is
scrambled by making a number of random moves, and the task is to restore the cube to its original
unscrambled state. There are about 4  1019 different reachable states.
A classic abstraction mapping for Rubiks cube is the PDB based on the corner cubies (Korf,
1997). This abstraction has 88, 179, 840 abstract states. The branching factor of the abstract space
is same as the branching factor of the original space. In fact, each operator in the abstract space is
trivially one-to-one-mapped to an operator in the original space.
We experimented with both the full-checking and the direct-computation OSFs. As expected,
the branching factor of 18 was not large enough to achieve a run-time advantage by using a directcomputation OSF. In fact, the full-checking OSF was marginally faster in our experiments.
The results for the full-checking OSF are given in Table 4. Each line is the average over 100
instances of depth 12-15. The reduction (ratio column) in the number of nodes generated is a factor
of 13.3 (again very close to the known effective branching factor), while the time improvement is
only 3.7-fold. The reason for the discrepancy is that the constant time per node of EPEIDA* is
larger than that of IDA* since it includes the time to retrieve values from the -PDB.

8. Additive PDBs-Based OSF
This section is motivated by the Multi-Agent Pathfinding domain (MAPF), which has recently attracted significant attention of researchers (Standley, 2010; Sharon et al., 2011, 2012). During the
recent workshop dedicated to this problem, a way to use additive PDBs (Felner et al., 2004; Yang
et al., 2008) to solve instances of this problem was presented (Goldenberg, Felner, Stern, & Schaeffer, 2012). We develop an additive PDBs-based OSF, so that generation of surplus nodes can be
avoided when solving instances of MAPF.
To keep this section as simple as possible, we leave the description of technically complicated
background and details of implementation of OSF to Appendices C and D. The current section is
organized as follows:
1. A brief description of MAPF (Section 8.1). Standley (2010) introduced two MAPF-specific
algorithmic enhancements to A* that make problem instances of MAPF solvable within reasonable time resources. We mention these enhancements and describe them in Appendix C
for the purpose of being self-contained.
163

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

2. The definition of additive PDBs and the explanation of how such PDBs can be built for MAPF
(Section 8.2).
3. The description of the basic additive PDBs-based OSF for MAPF (Section 8.3). We have a
number of performance enhancements for this OSF, which are described in Appendix D.
4. The report on our experimental results (Section 8.4).
8.1 The Multi-Agent Pathfinding Domain (MAPF)
Consider the following commonly used variant of MAPF (Standley, 2010; Sharon et al., 2011,
2012). The input consists of: (1) A graph G(V, E) and (2) k agents labeled a1 , a2 . . . ak . Every
agent ai is coupled with a start and a goal vertices: si and gi . At the initial time point t = 0 every
agent ai is located at the location si . Between successive time points, each agent can perform a
move action to a neighboring location or can wait (i.e. stay idle) at its current location. An operator
consists of an action (which may be Wait) for every agent. Every legal operator has to respect two
constraints:
1. Each vertex can be occupied by at most one agent at a given time and
2. If x and y are neighboring vertices, two different agents cannot simultaneously traverse the
connecting edge in opposite directions (from x to y and from y to x). However, agents are
allowed to follow each other, i.e., agent ai could move from x to y at the same time as agent
aj moves from y to z.
The task is to find a sequence of legal operators that bring each agent to its goal position while
minimizing a global cost function. In our variant of the problem, the cost function is the summation
(over all agents) of the number of time steps required to reach the goal location. Therefore, both
Move and Wait actions cost 1, except for the case when the Wait action is applied at an agents goal
location, in which case it costs 0. An exception is the case when an agent waits m times at its goal
location and then moves: the cost of that move is m + 1.
A*-based optimal solvers of MAPF use the Sum of the Individual Costs (SIC) heuristic, which
is the optimal cost of solving the problem when the legal operator constraints are ignored. The first
use of PDBs for MAPF was recently reported (Goldenberg et al., 2012). We will explain below
why applying abstraction-based heuristics is challenging for MAPF. Two techniques have become
standard in A*-based optimal MAPF solvers (Standley, 2010):8
 Independence Detection (ID) tries to reduce the part of a MAPF problem instances complexity that is due to agents interactions (i.e. the legal operator constraints).
 Operator Decomposition (OD) reduces the number of surplus nodes generated by A*. We
call the resulting variant of A* the Operator Decomposition A* (ODA*).
Both ID and ODA* are described in more detail in Appendix C. In that appendix, we also introduce
the Partial Expansion ODA* (PEODA*), our hybrid between ODA* and PEA*.
8. The recent papers by Roger and Helmert (2012) and Bouzy (2013) stand out as far as solving MAPF non-optimally
is concerned.

164

fiE NHANCED PARTIAL E XPANSION A*

8.2 Additive PDBs for MAPF
In this section, we define additive PDBs and show why constructing such PDBs for MAPF is challenging.
8.2.1 T HE D EFINITION OF A DDITIVE PDB S
Consider a set of abstractions  = {1 , 2 , . . . , k } for some given domain. For each abstraction
i , a PDB, denoted P DBi can be built. For an arbitrary state s, P DBi [i (s)] stores the distance
from s to the goal in the abstract
X space of i . The set  is called additive if, for any state s of the
original state space, the sum
P DBi [i (s)] is not greater than the distance from s to the goal
i

in the original space. PDBs based on additive abstractions are called additive PDBs (Felner et al.,
2004; Yang et al., 2008).
Suppose that we have built and stored a -PDB ordered by f (ac ) for each abstraction in the
additive set of abstractions . We denote these -PDBs by -PDB1 , -PDB2 , . . . , -PDBk . We
make two assumptions motivated by MAPF:
1. Each variable participates in at least one of the abstractions in .
2. Given a node n and an operator that results in nc , f (nc ) is given by:
f (nc ) =

k
X

f (i (nc )),

(1)

i=1

Intuitively, this means that each operator may affect all of the variables and so all of the
-PDBs have to be looked up to determine f (nc ) of a particular operator.
8.2.2 A DDITIVE PDB S FOR MAPF
For a given node n, we say that agents a1 , a2 , . . . , am are in conflict if the SIC heuristic for these
agents (i.e. when all other agents are ignored) is not perfect. Intuitively this definition means that,
whatever optimal plan is chosen for each individual agent, these plans cannot be executed simultaneously without agents colliding into each other. PDBs for MAPF can provide useful heuristic
information only if they are built for agents that will be in conflict for many nodes during the main
search. However, this information is not known a priori. We solve this problem for the case of
abstractions that consist of two variables (i.e. locations of two agents). We call PDBs based on such
abstractions pairwise PDBs.
Recall that ID is used on top of EPEA* (the reader unfamiliar with ID should refer to Appendix C at this point). The idea of our solution is to consider the merge actions of ID to be an
indication that the agents being merged are in conflict for many nodes of the main search. Namely,
whenever ID merges two agents into a group, we use this information to build pairwise PDBs at
later stages of ID. We build additive PDBs that consist of pairwise PDBs built this way.
Suppose, for example, an instance with 10 agents. Consider an execution of ID, while ignoring
all operations except the operation of merging two groups into a single group. Suppose that ID
merged agents {1, 5}, then merged agents {2, 8} and then merged the two groups together, forming
the group consisting of agents {1, 2, 5, 8}. When looking for an optimal path for this group, we will
use two 2-agent PDBs: one with states projected onto agents {1, 5} and the other using projections
165

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

f ([1 (n)])



f ([2 (n)])



f ([3 (n)])



0
3
5

11 = {11 , 12 }
12 = {13 , 14 }
13 = {15 }

0
1
3

21 = {21 }
22 = {22 }
23 = {23 }

0
1
4

31 = {31 }
32 = {32 }
33 = {33 , 34 }

Figure 8: Entries of -PDBs for a given node n.
onto agents {2, 8}. In our experiments, we used instance dependent pattern databases described
above.
In the following section, we describe a direct-computation OSF constructed for additive PDBs
built as described above. It is important to note that, in MAPF, an operator affects all of the agents
and Equation 1 holds.
Note that SIC is the trivial case of PDBs, since the heuristic based on single-agent additive PDBs
is exactly the SIC heuristic. Therefore, the OSF based on additive abstractions that we will develop
below will be applicable to SIC as well.
8.3 Constructing a Direct-Computation OSF for Additive PDBs
Recall from Section 5.2 that one way to build a direct-computation OSF is by defining a classification of the states. In particular, when we built a direct-computation OSF for a single PDB in
Section 7.2, we defined these classes by putting each abstract state into a class of its own. In contrast, a set of abstractions does not define a classification of the states. Therefore, we do not use
classes (this is equivalent to putting all states into one class). Instead, the algorithmic component
of the OSF will compute the set of operators with the required value of f (nc ) on-the-fly as we
describe below.
For example, suppose three additive abstractions: 1 , 2 , 3 . Figure 8 shows the entries that the
corresponding -PDBs ordered by f (ac ) might contain for a particular node n being expanded.
Let ij denote the j th abstract operator applicable to i (n). Figure 8 groups the abstract operators
that result in the same f (ac ). These groups are denoted by , i.e. ij is the j th group of abstract
operators applicable to i (n). In our example, the group 11 contains two abstract operators: 11
and 12 . There are 5, 3 and 4 abstract operators available for projections of n onto each of the three
abstractions, respectively. This means that there could be up to 5  3  4 = 60 operators for n in
the original space. However, it may be that not every combination is a legal operator in the original
space. For example, it could be that the changes to the variables performed by 14 and the changes
to the variables performed by 21 cannot be applied at the same time according to the rules of the
domain (e.g. because two agents cannot occupy the same location in MAPF).
Suppose that we need to compute the set of operators applicable to n with f (nc ) = 8. According to Equation 1, we need to find all ways to choose one abstract operator for each of the three
projections of n, such that the sum of corresponding f (ac )-values would equal eight. One such
choice is 13 , 22 , 33 (these abstract operators and their f (ac )-values are shown in Figure 8 in
bold). Furthermore, we could choose any one operator from each of the groups 12 , 22 , 33 .
In general, for k abstractions, we find all operators with the required f (nc ) by finding all ways
to choose one abstract operator for each abstraction, such that the sum of corresponding f (ac )values is equal to the required f (nc ). We note that this is a combinatorial problem that is exponential in the number of abstractions. Since this problem has to be solved for every expansion, it
166

fiE NHANCED PARTIAL E XPANSION A*

Procedure 3 Algorithmic component of an additive PDBs-based OSF for MAPF.
Variables:
k  number of abstractions
i  current abstraction
C  sum of f (ac )-values selected from abstractions 1, 2, . . . , i  1
sum  sum of f (ac )-values selected from abstractions 1, 2, . . . , i
nOpsi  number of abstract operators applicable to i (n)
opi  the current choice of abstract operator for i (n)
op  the current choice of abstract operators for abstractions 1, 2, . . . , i
OP  the set of all ops that result in the required f (nc ). Some of these ops may be illegal
N  the set of currently needed nodes returned by the OSF
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

FindAbsOpSets(i, C, op)
for j from 1 to nOpsi do
Set opi  ij
Set sum  C + f (ij [i (n)])
if sum > f (nc ) then
Set Fnext (n)  min(Fnext (n), sum)
return
if i == k then
if sum == f (nc ) then append op to OP
continue
Call FindAbsOpSets(i + 1, sum, op)
OSF(f (nc ))
Set N  , OP   and Fnext (n)  
Call FindAbsOpSets(1, 0, )
for each op in OP do
if op is legal then
Append nc corresponding to op to N
return N, Fnext (n)

is critical that this problem be solved efficiently. We present a simple basic algorithm for solving
this problem in Procedure 3. Enhancements to this simple algorithm are presented in Appendix D.
In addition to finding operators with the required f (nc ), this algorithm computes the next stored
value of n, Fnext (n). Thus, Procedure 3 is a full-fledged direct-computation OSF.
In Procedure 3, the main part of the OSF starts on line 12. First, the set of currently needed
operators, N , is initialized to be empty and the next stored value, Fnext (n) is initialized to infinity.
We also initialize to empty the set OP , whose meaning is explained below. Then a call to the
FindAbsOpSets (which stands for find abstract operator sets) procedure is made. This procedure
takes three parameters:
1. i  the current abstraction,
2. C  the sum of f (ac )s corresponding to the abstract operators chosen for abstractions 1
though i  1, and
167

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

3. op  the current choice of abstract operators for abstractions 1, 2, . . . , i.
and produces two results:
1. OP , the set of choices of abstract operators, one per abstraction, that result in the required
f (nc ) and
2. Fnext (n).
The OSF computes N by filtering out the illegal operators from OP and returns (lines 15-end).
We now walk through FindAbsOpSets, which implements a simple recursive algorithm. When a
given level of recursion begins, an abstract operator is chosen for each of the abstractions 1, 2, . . . , i
1. These operators are stored in op. The sum of f (ac )s corresponding to these abstract operators
is stored in C. There are two cases:
1. The current abstraction is k (i.e. i = k). This is the base of the recursion. In this case, we
look for the abstract operators for the current abstraction with f (ac ) that complements C
to the required f (nc ) (C + f (ac ) is denoted by sum in Procedure 3). For each chosen
operator, the tuple op appended by this operator is stored in OP (lines 8-10).
2. The current abstraction is not k (i.e. i < k). In this case, we look for the abstract operators for
the current abstraction such that sum = C + f (ac ) would not exceed the required f (nc )
and invoke the next level of recursion (line 11).
In addition, whenever an abstract operator that results in sum = C + f (ac ) that exceeds the
required f (nc ) is encountered, we can update Fnext (n) and not look at the following abstractions (lines 5-7).
In Appendix D, we present four enhancements to the basic algorithm just described.
8.4 Experimental Results
Tables 5 and 6 compare node and time performance, respectively, of six A*-variants for optimally
solving MAPF. For EPEA*, two variants are shown: one using the SIC heuristic and the other using
an additive PDBs-based OSF as described above. The -PDBs were constructed on-the-fly. That
is, when a node n was expanded for the first time, the entries of the -PDBs corresponding to the
abstractions of n were computed if they had not existed already.
There were a total of 1,000 instances. In all instances, agents were placed onto a four-connected
8x8 grid with no obstacles. We varied the number of agents. All algorithmic variants were run under
the ID framework as described in the footnote.9 All variants were given up to two minutes and two
gigabytes of memory per instance. The results were bucketed according to the number of agents (k,
9. Since the ID framework can produce different results due to reasons that are not related to the performance properties of the different A* variants for MAPF (see Appendix C), we compared these variants using the following
approach (Sharon et al., 2011). For each given instance, we first ran ODA* under the ID framework and saved the
largest group of inter-dependent agents. Then, the original instance was substituted by another instance, where the
agents not in the largest group were discarded. The variants were then compared on these instances without the use
of ID.

168

fiE NHANCED PARTIAL E XPANSION A*

Unique Nodes Generated, 103
k Ins
A* ODA* PEA* PEODA* EPEA* EPEA* with abstractions
Instances solved by both A* and PEA* within two minutes and 2GB memory
2-6 794
46.35
1.27 0.08
0.38
0.08
0.06
7-8 34 1,261.04
3.26 0.11
0.88
0.10
0.05
9-10
0
n/a
n/a
n/a
n/a
n/a
n/a
Instances solved by neither A* nor PEA* within two minutes and 2GB memory
2-6
1
n/a 335.34
n/a
105.14
9.94
9.31
7-8 25
n/a 219.11
n/a
67.04
7.82
4.41
9-10 13
n/a 705.76
n/a
211.54 17.57
10.01

Table 5: Comparison of nodes performance of six algorithms for the Multi-Agent Pathfinding.
Run-Time, ms
k Ins
A* ODA* PEA* PEODA* EPEA* EPEA* with abstractions
Instances solved by both A* and PEA* within two minutes and 2GB memory
2-6 794
647
5
606
5
2
33
7-8 34 22,440
14 14,886
12
2
100
9-10
0
n/a
n/a
n/a
n/a
n/a
n/a
Instances solved by neither A* nor PEA* within two minutes and 2GB memory
2-6
1
n/a 2,153
n/a
1,803
278
354
7-8 25
n/a 1,637
n/a
1,312
335
232
9-10 13
n/a 16,660
n/a
8,846 3,062
1,089

Table 6: Comparison of time performance of six algorithms for the Multi-Agent Pathfinding.
shown in the first column of Tables 5 and 6) and results for instances falling into the same bucket
were averaged.
Since the basic A* and PEA* perform much worse than the other variants, we split the tables
into two halves. The upper part of the table shows results for instances that were solved within the
allowed resources by both A* and PEA*. We see that EPEA* generates four orders of magnitude
less nodes and is three orders of magnitude faster than A*. Also, it generates an order of magnitude
less nodes and is up to seven times faster than ODA*. EPEA* with an additive PDBs-based OSF
generates the fewest number of nodes among all variants, but is not the fastest in terms of time. This
is because, for these simple instances, the overhead of building the (-)PDBs does not pay off.
The lower part of the table shows results for instances that were not solved by either A* or
PEA*, but solved by the other variants. The following trends can be observed. Applying partial
expansion on top of ODA* results in three-fold reduction in the number of generated nodes and up
to two times speed-up over ODA*. However, EPEA* generates yet another order of magnitude less
nodes and is up to four times faster. EPEA* with additive PDBs-based OSF is the clear winner for
the hard instances, running up to three times faster than EPEA* based on the SIC heuristic.
The next two sections are a theoretical study of PEA* and EPEA*.

9. The Size of OPEN: A Limitation of PEA* and EPEA*
In this section, we will show that, although the purpose of PEA* is to make OPEN smaller, it
sometimes has exactly the opposite effect.
169

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

b s2

g2

a s1

g1

Figure 9: For this instance of MAPF, PEA*/EPEA* will not close even the start node.

Figure 10: Open (checkerboard) and closed (gray) nodes after 16 node expansions for A* (left) and
PEA*/EPEA* (right).

When PEA*/EPEA* expands a node n, it puts into OPEN only the currently needed children
of n. If n has possibly surplus children, then n will be put back into OPEN with an updated stored
value. This is in contrast to A*, which always puts n into CLOSED after the expansion. This may
result in PEA*/EPEA* maintaining a much larger OPEN than A*. Note that OPEN is frequently
implemented as a heap and heap operations become slower as the number of elements stored in
the heap grows. Therefore, the time performance may suffer considerably when the size of OPEN
increases.
Consider the MAPF example in Figure 9. In this instance, two agents start at locations (s1 , s2 )
and need to get to locations (g1 , g2 ). The SIC heuristic at the start state is 8, while the optimal
solution length is 9 (namely, one agent has to wait for the other agent to pass). However, the
highest-cost (i.e. with the agents at (a, b)) child of the start node has the f -value of 12. Therefore,
while A* closes all the expanded nodes, PEA*/EPEA* will never close any of the nodes, not even
the start node.
Furthermore, it is possible for a node n not to be closed by PEA*/EPEA* even though the
neighbors of n are surely not surplus. Figure 10 (left) shows this with an instance of the singleagent pathfinding problem on a four-connected grid, where the shortest path is needed from cell S
to cell G. Black cells are obstacles. We assume the Manhattan distance heuristic. Note that the
shortest path is of length 10 but the 16 gray cells all have f -value of 8. Thus, A* starts expanding
states with f = 8. After 16 expansions all these states are closed, and the 8 states around them
(marked with a checkerboard) are in the OPEN list, all with f = 10.
Consider the operation of PEA*/EPEA* on the same example. When PEA*/EPEA* expands
the gray nodes it does not close them because each has a child with f = 10. For example, when
expanding state 10, operators North and West have f (nc ) = 0, while the operators East and South
170

fiE NHANCED PARTIAL E XPANSION A*

have f (nc ) = 2. The cell east of 10 is the cell 1 which was already generated with a lower gvalue and with f = 8. However, PEA*/EPEA* do not perform duplicate detection for the currently
unneeded nodes. Thus, cell 10 is re-inserted into the OPEN list with F = 10, received from cell 1.
Consequently, after expanding all the gray cells for the first time, PEA*/EPEA* has 16 OPEN nodes
as shown in Figure 10 (right) and no CLOSED nodes. In the case of PEA*, this problem can be fixed
at the expense of the run-time overhead of performing a hash look up for the currently unneeded
children before discarding them (note that this fix does not address the large OPEN problem shown
in the MAPF example above). In the case of EPEA*, such duplicate detection is impossible, since
EPEA* does not actually generate the currently unneeded children.
To summarize, A* stores only the perimeter of the generated states in the OPEN list, while
PEA*/EPEA*, in the worst case, stores in the OPEN list all of the states that have ever been generated during the search. For polynomial domains (defined in the footnote10 ), this considerably affects
the time performance of PEA*/EPEA*. For exponential domains, this factor is of lesser importance,
since, for these domains, A* stores most of the nodes in OPEN as well. That is why we did not
observe that EPEA* was slower than A* in any of our experiments besides the experiments with
SAPF (not shown), where EPEA* was slightly slower than A*.

10. Performance analysis
The purpose of this section is to analytically estimate how the time performance of EPEA*/EPEIDA*
and the regular A*/IDA* compare. To do this, we identify the basic operations within each of the
algorithms and give notation to the time costs of these operations. We then use this notation to give
precise conditions for obtaining a time speed-up by using the enhanced partial expansion variant of
A*/IDA*.
The operations of the algorithms under consideration are listed in Table 7. For each operation,
we denote its applicability to a given algorithm by putting a + in the respective column. If an
operation is applicable only when a full-checking OSF or a direct-computation OSF is used, we
denote this fact by +(FC) and +(DC), respectively. The operations appear in the same order in
which they appear in Procedures 1 and 2.11 To introduce as little notation as possible, we group
some of these operations together and assign notation to the total time cost of the operations in each
group as shown in Table 8. Note that these time costs are the averages over all node expansions
or generations, whichever is applicable to the particular group of operations. We skip the word
average in the text for brevity. We will explain these operations and groups on-the-fly when they
are used in the analysis. Analysis of EPEIDA* is simpler than the analysis of EPEA*. Therefore
we start with the analysis of EPEIDA*.
10.1 Analysis of EPEIDA*
For simplicity, we restrict our analysis to the last iteration of IDA*. Let X be the number of nodes
expanded by this iteration. Let b be the average branching factor of the domain. We approximate the
10. Polynomial domains are domains where the number of distinct states at depths up to d of the breadth-first search tree
starting from any given state is (dm ), where m is a domain-specific constant. SAPF is a classical example of a
polynomial domain. Exponential domains are domains where the number of distinct states at depths up to d of the
breadth-first search tree starting from any given state is (bd ), where b is the average branching factor of the domain.
15-puzzle, Rubiks cube and pancake puzzle are examples of exponential domains.
11. Hence the heuristic computation appears twice: operation 3 is for A*, while operation 9 is for IDA*.

171

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

#
1
2
3
4
5
6
7
8
9
10

Operation

Applicability
A* PEA* EPEA* IDA* EPEIDA*
Remove from OPEN
+
+
+
Recursive call of IDA*
+
+
Compute the nodes heuristic (IDA*) +
+
Check the threshold condition of IDA* +
+
Compute a child nodes heuristic
+(FC)
+(FC)
when the node has not been generated
Check one operator
+(FC)
+(FC)
using a full-checking OSF
Check one operator
+(DC)
+(DC)
using a direct-computation OSF
Apply an operator
+
+
+
+
+
(in A*, includes copying the parent)
Compute a child nodes heuristic
+
+
when the node has been generated
Insert one child into OPEN
+
+
+
-

Table 7: Summary of operations performed by the five algorithms under consideration. A plus sign
stands for an operation that is performed by the given algorithm, while a minus sign stands
for an operation that is not applicable in the context of the given algorithm.
Notation
te
tr
tof
tod
tm
t0m

Operations
1
8, 2, 3, 4
6
7
8, 9, 10
8, 10

Comments
Has different cost in A* and EPEA*,
so, in EPEA*, we use t0e
In the case of EPEA*/EPEIDA*, also calls operation 5.
For A*
For EPEA*

Table 8: Notation for the time costs of the groups of operations. The operations are denoted by the
numbers introduced by Table 7.

number of nodes generated by IDA* by bX. Since EPEIDA* generates only the currently needed
children (Section 4), it expands all of the nodes that it generates. The only exception from this rule
is when the solution has been found. In that case, some nodes generated at the upper levels of the
depth-first tree may remain unexpanded. Therefore, EPEIDA* generates up to X + (b  1)d nodes.
Since (b  1)d is usually very small compared to X, we will ignore this quantity in the following
analysis. This will allow us to express the ratio between the run-time costs of IDA* and EPEIDA*
in terms of the parameters of the domain and the implementation without the usage of X.
For each operator available for a given node n, IDA* generates a child nc by applying the
operator (operation 8), makes the recursive call of IDA* (operation 2), computes h(nc ) (operation
3) and checks the threshold condition (operation 4). We denote the total time cost of these operations
172

fiE NHANCED PARTIAL E XPANSION A*

by tr . Since these operations are performed by IDA* for each generated node, the run-time cost of
IDA* is:
bXtr .
(2)
10.1.1 A NALYSIS OF EPEIDA* WITH A F ULL -C HECKING OSF
In EPEIDA* with a full-checking OSF, when a node n is expanded, the cost tr is spent only on
currently needed operators applicable to n. The other operators are checked (operation 6), but the
children are not generated. After denoting the time cost of checking an operator by a full-checking
OSF by tof , the run-time cost of EPEIDA* is given by:
bXtof + Xtr .

(3)

The ratio between the run-time costs of IDA* and EPEIDA* is expressed by:
btr
.
btof + tr

(4)

EPEIDA* is faster than IDA* when this ratio is greater than one, i.e. when
tof < tr

b1
.
b

(5)

10.1.2 A NALYSIS OF EPEIDA* WITH A D IRECT-C OMPUTATION OSF
EPEIDA* with a direct-computation OSF checks only the operators leading to the X expanded
nodes. After denoting the time cost of checking an operator by a direct-computation OSF (operation
7) by tod , the run-time cost of EPEIDA* is:
X(tod + tr ).

(6)

The ratio between the run-time costs of IDA* and EPEIDA* is expressed by:
b

tr
.
tod + tr

(7)

EPEIDA* is faster than IDA* when this ratio is greater than one, i.e. when
tod < tr (b  1).

(8)

10.1.3 A NALYSIS OF EPEIDA* WITH A H YBRID OSF
Recall that a hybrid OSF can behave either as a direct-computation or as a full-checking OSF for
different expansions depending on the required f (nc ). For example, the OSF for the pancake
puzzle introduced in Section 6.2 is a direct-computation-OSF when nodes with f (nc ) = 0 are
needed and is a full-checking-OSF when nodes with f (nc ) = 1 or f (nc ) = 2 are needed.
Let G0  X be the number nodes for whose generation a direct-computation OSF is used. Also,
let E 0  X be the number of nodes for whose expansion a direct-computation OSF is used. Note
that we cannot easily express G0 in terms of E 0 (i.e. it would be wrong to state that G0 = bE 0 ), since
the direct-computation OSF generated only the currently needed children of n. The run-time cost
of EPEIDA* is given by
G0 tod + b(X  E 0 )tof + Xtr .
(9)
173

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

The ratio between the run-time costs of IDA* and EPEIDA* is expressed by:
G0 tod

bXtr
.
+ b(X  E 0 )tof + Xtr

(10)

G0 and E 0 are not known and we need to use a model that allows for a more detailed analysis
to estimate them. Fortunately, a suitable model has been reported (Zahavi, Felner, Burch, & Holte,
2010). Since using this model to estimate G0 and E 0 is technically involved and does not provide an
immediate insight into the efficiency of EPEA*, we defer this analysis to Appendix B.
10.2 Analysis of EPEA*
To analyze the time performance of EPEA*, we will need to introduce two auxiliary parameters that
characterize the performance of EPEA*. Let  be the average number of times that any given node
is expanded (each time with a different stored value). Let  be the average number of currently
needed children during any given node expansion. In particular, when  = 1 and  = b (b is
the average branching factor), EPEA* is equivalent to A*. Note that these parameters are domain,
heuristic and problem instance-dependent.
Suppose that A* performs X expansions and bX generations. Since EPEA* expands each node
an average of  times, it will perform a total of X expansions. Since EPEA* performs an average
of  generations per expansion, it will perform a total of X generations.
Let te denote the time cost of getting the least cost node from OPEN (operation 1). For each
operator applicable to n, A* generates a child nc (operation 8), computes h(nc ) (operation 9) and
inserts nc into OPEN (operation 10). We denote the total cost of these three operations by tm . The
run-time cost of the regular of A* is given by:
Xte + bXtm .

(11)

10.2.1 A NALYSIS OF EPEA* WITH A F ULL -C HECKING OSF
Recall from Section 5.2 that one can trade memory for speed by imitating a direct-computation OSF
even when such an OSF is not available. The following analysis is for the pure version of EPEA*
with a full-checking OSF where this trade-off is not used.
EPEA* performs X expansions, each time checking b operators, followed by generating 
currently needed children nodes. Denote the time cost of expanding a node by EPEA* by t0e . Note
that te and t0e may be different for the same problem instance, since the size of OPEN differs between
the expansions of A* and EPEA*. Similarly, we denote the total time cost of generating a child node
nc , computing h(nc ) and putting nc into OPEN by EPEA* by t0m , which might differ from tm . The
run-time cost of EPEA* is given by:
Xt0e + bXtof + Xt0m .

(12)

The ratio between the run-time costs of the regular A* and EPEA* is expressed by:


t0e

te + btm
.
+ btof + t0m

(13)

EPEA* is faster than the regular A* when this ratio is greater than one, i.e. when
tof <

te + btm  (t0e + t0m )
.
b
174

(14)

fiE NHANCED PARTIAL E XPANSION A*

10.2.2 A NALYSIS OF EPEA* WITH A D IRECT-C OMPUTATION OSF
With a direct-computation OSF, EPEA* performs X expansions, each time checking  operators
and generating  currently needed children nodes. The run-time cost of EPEA* is given by:
Xt0e + X(tod + t0m ).

(15)

The ratio between the run-time costs of the regular A* and EPEA* is expressed by:


t0e

te + btm
.
+ (tod + t0m )

(16)

EPEA* is faster than the regular A* when this ratio is greater than one, i.e. when
tod <

btm  t0e + te
 t0m .


(17)

10.3 Conclusion from the Performance Analysis
From the above analysis we see that whether EPEA*/EPEIDA* is faster or slower than A*/IDA*
depends on:
1. The average branching factor b of the domain,
2. The efficiency of implementation of A*/IDA* (expressed by te , tr and tm ),
3. The efficiency of the OSF being used (expressed by tof or tod ), and
4. For EPEA*, the domain and heuristic-dependent parameters  and .
To obtain experimental evidence of the analysis of this section, one would have to find a way to
reliably estimate the quantities in Table 8. However, such estimation meets with two challenges:
1. The operations under discussion are of very fine granularity. The time performance of these
operations cannot be measured directly, but has to be measured by a sampling procedure.
2. For EPEA*, estimating the parameters  and  presents and additional challenge.
3. The time performance of some of the operations under discussion is not constant throughout
the search. For example, the cost of inserting a node into OPEN depends on the current size
of OPEN. Also, there are domains where nodes have variable branching factors.
The latter observation opens a possibility for an algorithm that, depending on the current state of
the search, switches between A*/IDA* and EPEA*/EPEIDA*.

11. EPEA* With Inconsistent Heuristics
A heuristic h is called consistent if f -value does not decrease along any path, i.e. f (nc )  f (n).
Otherwise, h is called inconsistent. So far, we assumed a consistent heuristic. In this section,
we will (1) show the changes that need to be made to EPEA* (Procedure 1 of Section 2.2) when an
inconsistent heuristic h is used and (2) point out that one has to make a choice between using EPEA*
175

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Procedure 4 A*, PEA* and EPEA* for an inconsistent heuristic.
1: Generate the start node ns
2: Compute h(ns ) and set F (ns )  f (ns )  h(ns )
3: Put ns into OPEN
4: while OPEN is not empty do
5:
Get n with lowest F (n) from OPEN
6:
if n is goal then exit
// optimal solution is found!
7:
For A* and PEA*: set N  set of all children of n and initialize Fnext (n)  
8:
For EPEA*:
9:
if f (n) = F (n) then
10:
Set (N, Fnext (n))  OSFe (n).
11:
else
12:
Set (N, Fnext (n))  OSF (n).
13:
for all nc  N do
14:
Compute h(nc ), set g(nc )  g(n) + cost(n, nc ) and f (nc )  g(nc ) + h(nc )
15:
For PEA*:
16:
if f (nc ) 6= F (n) then
17:
if f (nc ) > F (n) then
18:
Set Fnext (n)  min(Fnext (n), f (nc ))
19:
if f (nc ) > F (n) or F (n) 6= f (n) then
20:
Discard nc
21:
continue
// To the next nc
22:
Check for duplicates
23:
Set F (nc )  f (nc ) and put nc into OPEN
24:
if Fnext (n) =  then
25:
Put n into CLOSED
// For A*, this is always done
26:
else
27:
Set F (n)  Fnext (n) and re-insert n into OPEN
// Collapse
28: exit
// There is no solution.
and using heuristic value propagation techniques (such as bidirectional pathmax (BPMX), Zahavi,
Felner, Schaeffer, & Sturtevant, 2007) for inconsistent heuristics. A deep experimental study of
EPEA* with inconsistent heuristics including the trade-off due to the aforementioned choice is
beyond the scope of this paper.
11.1 Changes to PEA* and EPEA*
We will start by discussing PEA*, since PEA* makes the choice of what nodes to insert into OPEN
explicitly (line 12 of Procedure 1).
When the heuristic is inconsistent, it is possible that there are children nc of n with f (nc ) <
f (n). These children nodes are not surplus and PEA* will need to insert them into OPEN during
the first expansion of n. Thus, during the first expansion of n the definition of a currently needed
node is changed to include all children with f (nc )  F (n).
This is demonstrated in Figure 11. When the node a is expanded, all of its children (x, y, z, w)
are generated. However, only the children with the f -values not exceeding 3 (x and y) are currently
176

fiE NHANCED PARTIAL E XPANSION A*

Figure 11: Example of PEA* with an inconsistent heuristic. The first expansions of the node a is
shown. Even the children nodes whose f -value is less than f (a) are generated.

8
BPMX
6

10
4

BPMX

2

10

9
6

5
4
8
6

7

7
2

6

Figure 12: Two examples of value propagation by BPMX.

needed. They are inserted into OPEN. All other children are possibly surplus. They are collapsed
back into a, who gets the new stored value F (a) = 4. Following this, a is re-inserted into OPEN.
From here on, a is treated as for the consistent heuristic case.
The pseudo-code of A*, PEA* and EPEA* for the case of an inconsistent heuristic is shown in
Procedure 4. Compared to Procedure 1 in Section 2.2, PEA* handles the inconsistent heuristic by
performing the check in line 19 before discarding nc .
In EPEA*, the case of inconsistent heuristic is also handled by checking whether n is being
expanded for the first time (line 9). For the first expansion of n, the OSF that returns the set of
children with f (nc )  F (n) is used. We call this an extended OSF and denote it by OSFe (line 10).
Note that a similar OSF is used by EPEIDA* (Section 4) as well. For other expansions of n, no
change in the OSF is needed (line 12).
11.2 The Trade-Off
When an inconsistent heuristic is used, heuristic value propagation techniques can be applied to take
advantage of regions of the state space with high heuristic values. One of the most effective such
techniques is the bidirectional pathmax (BPMX) (Zahavi et al., 2007). An example of BPMXs
operation is shown in Figure 12 (right). Assuming unit edge costs, the h-value of the left grandchild
(10) is propagated up and then down the search tree, increasing heuristic estimates of all states in its
neighborhood except for the gray node. In general, for two arbitrary states a, b  V , the heuristic
estimate h(a, g) can be updated to be max {h(a, g), h(b, g)  d(a, b)} (shown in Figure 12 (left))
and BPMX uses this rule in both directions of the search tree.
177

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

When EPEA* is used, the possibly surplus children of n are not generated and, therefore, the
propagation of values from these children to n is not available. This introduces an interesting tradeoff:
 EPEA* saves both memory and time by not generating surplus nodes, but
 The opportunity to increase the quality of heuristic estimates by using the non-monotonicity
of f is being missed.
A separate study might be dedicated to this trade-off.12

12. Conclusions and Future Work
We have presented the Enhanced Partial Expansion A* (EPEA*), a novel variant of A* which
avoids generating the surplus nodes. This is enabled by using a priori domain- and heuristic-specific
knowledge in the form of an Operator Selection Function (OSF) to compute the list of operators
that lead to the children with the needed f -cost without actually generating any children of the
node being expanded. We studied several kinds of OSF, including the OSFs based on (additive)
pattern databases. We extended the principles of EPEA* to IDA* resulting in the Enhanced Partial
Expansion IDA* (EPEIDA*). Experimental results with the 15-puzzle, the pancake puzzle, the
Rubiks cube and the multi-agent pathfinding show that EPEA*/EPEIDA* achieves state-of-the-art
run-time performance. Furthermore, EPEA* fully maintains the memory savings offered by PEA*.
EPEA* is most effective for domains and heuristics that meet the following criteria:
1. The domain possesses a large branching factor. The large branching factor is an indicator that
A* may generate a large number of surplus nodes. EPEA* will save this overhead.
2. The set of possible f -values of the children of a given node is small. This means that EPEA*
will not re-expand the same node many times.
3. The operators can be classified according to their f (nc ) value, so that operators of the
needed class can be applied without the need to check all operators of the node being expanded. In other words, a direct-computation OSF is available.
If only some of these conditions are met by the specific domain and heuristic of interest, then a
more thorough assessment, such as experimenting with a prototype implementation, is needed. In
addition, we explained that there is a possibility of poor performance of EPEA* in polynomial
domains.
Future work will seek applications of EPEA* to other domains and heuristics. In particular,
it would be very interesting to see whether EPEA* can be implemented for the best heuristics
used in domain-independent planning. It seems that EPEA* can be easily implemented for the
12. Related to this trade-off is the following observation (Felner et al., 2011, p. 22). They studied the following trade-off
in the context of applying BPMX to IDA*. Once propagation from children to a parent results in a cut-off, IDA* can
either backtrack immediately (this option is called lazy propagation) or look at the other children in hopes to obtain an
even higher value for the parent and obtain more cut-offs in the future. They concluded that backtracking immediately
was preferable for all domains that they studied. However, we cannot conclude from this that one should always use
EPEA* and forgo propagation of values from children to parents, since IDA* with lazy BPMX propagation does
perform an effective propagation before backtracking.

178

fiE NHANCED PARTIAL E XPANSION A*

STRIPS (Fikes & Nilsson, 1971) and the variable abstraction heuristics (Edelkamp, 2001). However, it remains to see whether EPEA* can be applied for merge-and-shrink abstractions (Helmert,
Haslum, & Hoffmann, 2007) and landmarks (Karpas & Domshlak, 2009).
A promising direction for future work is to implement an OSF by using symbolic representation,
such as BDDs (Binary Decision Diagrams). Jensen, Bryant, and Veloso (2002) proposed a method
to group state transitions that have the same effect on the f -value. Such grouping of state transitions
was referred to as an improvement partitioning and was used to avoid generating node with f -values
larger than a given upper bound (Jensen, Hansen, Richards, & Zhou, 2006). Thus, we believe that
it may be possible to create an OSF with a similar method.
Another interesting direction is to see how EPEA* can be used for non-optimal searches. In
these searches, the notion of a surplus node needs to be defined based on the required quality of the
solution.
Furthermore, we only touched upon the topic of EPEA* with inconsistent heuristics and pointed
out that a trade-off exists between using EPEA* and leveraging the full power of value propagation
techniques. An experimental study of this trade-off remains a subject for future work.

13. Acknowledgements
This research was supported by the Israeli Science Foundation (ISF) grant 305/09 to Ariel Felner
and the Natural Sciences and Engineering Research Council of Canada grant to Jonathan Schaeffer.
Acknowledgements are due to Kobi Shmerkovich, Tal Beja, Idan Morad and Shaked Zimmermann for performing some of the experiments for this paper.

Appendix A. Glossary of EPEA* Terms
Table 9 lists the terms introduced in this paper in the alphabetical order. It provides a brief definition
of each term and a reference to the place where this term is defined in the paper.

Term

Brief definition

Section(s)

Algorithmic component of OSF

An algorithm that OSF employs to generate only the currently
needed nodes and compute the next stored value of the node
being expanded.

3.2

CFN

See collapsing frontier nodes.

2.1

Checking an operator

Deciding whether to apply an available operator to generate a
child node.

3.1

Collapse action

The operation of substituting some nodes in the search frontier
by their common ancestor n, while increasing the cost of n. See
stored value.

2.1

Collapsing frontier nodes (CFN)

The technique used by a number of algorithms. See collapse
action.

2.1

179

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Currently needed
child node

A node that A*/IDA* has to generate during the current expansion to guarantee optimality. The definition differs for A* and
IDA*.

2.2 and 4

Currently needed
operator

An operator the results in a currently needed child node.

2.2 and 4

Currently
needed
node

A child node that is not currently needed.

2.2 and 4

Currently
unneeded operator

An operator that is not currently needed.

2.2 and 4

Directcomputation
OSF

An OSF that does not need to check all operators to generate
only the currently needed nodes.

5

Full-checking
OSF

An OSF that needs to check all operators to generate only the
currently needed nodes.

5

Hybrid OSF

An OSF that behaves either as a direct-computation or as a fullchecking OSF depending on the needed change in the f -value
(f (nc )).

5

Knowledge component of OSF

A domain- and heuristic-specific data structure that the OSF
stores. The algorithmic component uses the knowledge component to generate only the currently needed nodes and compute
the next stored value of the node being expanded.

3.2

Operator
selection
function (OSF)

A function that uses domain- and heuristic-specific knowledge
to generate only the currently needed nodes and compute the
next stored value of the node being expanded.

3.2

OSF

See operator selection function.

3.2

Possibly surplus
child node

A child node that is not provably useful.

2.2

Provably useful
child node

A child node that can be proved to be useful. A child node nc is
provably useful if f (nc )  F (n).

2.2

Pure OSF

Non-hybrid OSF.

5

SAPF

The single-agent pathfinding domain.

3.2

Static value

The regular g + h cost of a node, denoted f (n).

2.1

Stored value

The cost of a node obtained by the collapse action and denoted
F (n).

2.1

unchild

180

fiE NHANCED PARTIAL E XPANSION A*

Surplus node

A node whose static value is greater than the cost of the optimal
solution.

1

Useful node

A node that is node surplus.

1

Table 9: The terminology used in the paper.

Appendix B. Analysis of the Hybrid OSF
We start by describing a model that aims to predict the number of nodes expanded by IDA* (Zahavi
et al., 2010) (ZFBH).13 Later in this section, we will show how the same model can be used to
estimate the quantities G0 and E 0 introduced in Section 10.1.3. The model uses the assumption of
unit cost operators. Our analysis will make this assumption as well.
We start with a result that puts our study of the hybrid OSF into a more general framework than
just the pancake puzzle example.
Lemma 1. Let H be a natural number such that the hybrid OSF is direct when nodes with f (nc ) 
H are needed. Then, whenever a node n with f (n)  T  H is expanded, where T is the threshold
of the current iteration, this hybrid OSF is direct.
Proof. Suppose that n with f (n)  T  H is being expanded. Since in IDA* only the nodes
with f (nc )  T are needed, we need to generate only the nodes with f (nc ) = f (nc )  f (n) 
f (nc )  T + H  H. But for these nodes our OSF is direct.
Remark 1. In the pancake puzzle, H is zero.
Remark 2. We chose to focus on the simple model with two ranges of values of f (nc ). The results
of this section can be generalized to a model with a number of ranges designated by H1 , H2 , . . . , Hm .
B.1 The Model of ZFBH.
ZFBH define Ni (s, d, v) to be the number of nodes that IDA* will generate at level i with a heuristic
value equal to v when s is the start state and d is the IDA*s iteration threshold. They give a recursive
formula to approximate this quantity as follows:
d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp )  bvp  p(v|vp ).

(18)

vp =0

In this equation, p(v|vp ) is the probability of a child node having the heuristic value of v given that
its parent heuristic value is vp , bvp is the average branching factor of nodes whose heuristic value is
vp . Both these quantities can be obtained by sampling the state space (we refer the reader to ZFBH
for details of this sampling process). We now explain the reasoning behind this equation. Since the
parent node is located at level i  1 of the depth-search tree, it could have been expanded only if
13. We use only the basic one-step model of ZFBH.

181

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

its heuristic was less than or equal to d  (i  1) (otherwise, the threshold condition would have
failed). For each heuristic value vp , there are Ni1 (s, d, vp ) parent nodes, each of whom have bvp
children. However, only the children with the heuristic value of v are of interest. Since, the parents
heuristic value is fixed at vp , we multiply by the conditional probability.
The total number of nodes expanded by this iteration of IDA* is then approximated by:
X

d X
di
X

Ni (s, d, v).

(19)

i=0 v=0

The first summation runs through all depths of the current iterations search tree. At a given depth i,
only the nodes with heuristic value is less than or equal to d  i satisfy the threshold condition. The
second summation runs through all such heuristic values. Once both the depth i and the heuristic
value v are fixed, Ni (s, d, v), given by Equation 18, approximates the number of nodes expanded
by IDA*.
B.2 Estimating E 0 .
By Lemma 1, our hybrid OSF is direct if f (n)  d  H. Subtracting the level number i from both
d and f (n), we get the equivalent condition:
v  (d  i)  H.

(20)

Therefore, we can modify Equation 19 to estimate E 0 :
E0 

d
X

di
X

Ni (s, d, v).

(21)

i=0 v=(di)H

B.3 Estimating G0 .
Let G0i (s, d, v) denote the number of nodes that EPEIDA* will generate by using direct-computation
OSF at level i with a heuristic value equal to v when s is the start state and d is the IDA*s iteration
threshold. We can approximate G0i (s, d, v) by restricting the summation in Equation 18 to the
parent nodes that are expanded with a direct-computation OSF. From Equation 20, this condition is:
vp  [d  (i  1)]  H. We have:
d(i1)

G0

i (s, d, v)

X

=

Ni1 (s, d, vp )  bvp  p(v|vp ).

(22)

vp =[d(i1)]H

We estimate G0 as:
0

G 

d X
di
X

G0 i (s, d, v).

(23)

i=0 v=0

Once G0 and E 0 are estimated, Equation 10 gives the time speed-up of EPEIDA* over IDA*.
It is of interest to note that we can also estimate the number of nodes whose generation EPEIDA*
will save compared to IDA* at each particular level i:
+
X

Ni (s, d, v).

v=di+1

182

(24)

fiE NHANCED PARTIAL E XPANSION A*

In this summation, v starts at d  i + 1 because only nodes with heuristic values in the range
[0 . . . d  i] will be expanded at level i.

Appendix C. Algorithmic Enhancements for Solving MAPF
In this section we describe three MAPF-specific algorithmic enhancements to A*. Two of them,
the Independence Detection (ID) and the Operator Decomposition A* (ODA*) are due to Standley
(2010). The Partial Expansion ODA* (PEODA*) is our new hybrid between ODA* and PEA*.
C.1 The Independence Detection (ID)
Two groups of agents are called independent if there is an optimal solution for each group such that
the two solutions do not conflict. The basic idea of Independence Detection (ID) is to divide the
agents into independent groups. Initially each agent is placed in its own group. Shortest paths are
found for each group separately. The resulting paths of all groups are simultaneously performed
until a conflict occurs between two (or more) groups. Then, all agents in the conflicting groups are
unified into a new group, i.e. the two groups are merged. Whenever a new group of k  1 agents
is formed, this new k-agent problem is solved optimally by an A*-based search. This process
is repeated until no conflicts between groups occur. Standley observed that since the problem is
exponential in k, the A*-search of the largest group dominates the running time of solving the
entire problem, as all other searches involve smaller groups (for more details on ID, see Standley,
2010).
Note that ID can be combined with any optimal MAPF solver. In the paper of Standley (2010),
ID is combined with ODA*. Our approach combines ID and EPEA* with an OSF based on additive abstractions. In fact, we use the feedback received from ID to determine which variables are
included in each of the additive abstractions as will be explained below.
C.2 The Operator Decomposition (OD)
Standley (2010) introduced Operator Decomposition (OD) which reduces the number of surplus
nodes generated for MAPF as follows. OD introduces intermediate nodes between the regular states
of the A* search as follows. Agents are assigned an arbitrary (but fixed) order. When a regular A*
state is expanded, OD considers only the moves of the first agent, which results in generating the so
called intermediate nodes. At these nodes, only the moves of the second agent are considered and
more intermediate nodes are generated. When an operator is applied to the last agent, a regular node
is generated. Once the solution is found, intermediate nodes in OPEN are not developed further into
regular nodes, so that the number of surplus nodes is significantly reduced. This variant of A* is
referred to as ODA*. ODA* can still generate surplus nodes, both intermediate and regular. By
contrast, EPEA* never generates any surplus nodes.
C.3 Partial Expansion ODA*
We introduce Partial Expansion ODA* (PEODA*), a hybrid between ODA* and PEA*. This variant
operates similarly to ODA* with one exception. When PEODA* generates intermediate children nc
of n, it puts nc into OPEN only if f (nc ) = F (na ), where na is the standard node ancestor of nc . In
particular, if n is a standard node, then na is n.
183

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Appendix D. Enhancements of the OSF Based on Additive PDBs
In this section, we present four enhancements to the OSF described in Section 8.3. Each enhancement is presented in a separate subsection.
D.1 Avoiding Linear Search for i = k
The most obvious enhancement of FindAbsOpSets is that, at the last level of recursion (i = k), we
can use the fact that -PDBk is ordered by f (ac ) to quickly locate (e.g. by binary search) the
f (ac )-values that complete sum to the required f (nc ).
D.2 Obtaining More Cut-offs in Line 5
We note that it is possible to obtain a stronger condition for the cut-off in line 5 of Procedure 3. To
do this, at the first expansion of n, we compute and store with n in OPEN the array of k (the number
of abstractions) elements:
smallSums[i] =

k
X

f (l1 [l (n)])

l=i+1

Intuitively, smallSums[i] is the sum of the smallest entries in -PDBs for n in the abstractions
i + 1 through to k. We note that a cut-off can be performed whenever sum + smallSums[i] >
f (nc ) holds. Intuitively, this means that sum is provably too large, since even choosing the smallest entries for the remaining abstractions would result in exceeding the required f (nc ). Whenever
such a cut-off occurs, Fnext (n) is set to the minimum between the current value of Fnext (n) and
sum + smallSums[i].
D.3 Obtaining and Additional Cut-off
Just as we can obtain a cut-off when sum is provably too large, we can obtain a cut-off when sum
is provably too small. To do this, at the first expansion of n, we compute and store with n in OPEN
the array of k (the number of abstractions) elements:
largeSums[i] =

k
X

f (l,nOpsl [l (n)])

l=i+1

A cut-off can be performed whenever sum + largeSums[i] < f (nc ) holds.
D.4 Per-Group Search
We already observed above that, once a choice of f (ac )-value for each of the abstractions that
results in the required f (nc ) is found, we can take any one abstract operator from the groups
resulting in those f (ac )-values. Therefore, we can change the loop in line 2 to go through the
possible groups instead of the possible abstract operators. For our example, two choices of groups
of operators result in f (nc ) = 8:
{(12 , 22 , 33 ), (13 , 23 , 31 )}.
184

fiE NHANCED PARTIAL E XPANSION A*

Once FindAbsOpSets returns all choices of groups, one per abstractions that result in the required f (nc ) (line 14), a post-processing step would compute the corresponding choices of abstract operators. For example, this post-processing step would take the choice (12 , 22 , 33 ) and
compute four choices of abstract operators:
{(13 , 22 , 33 ), (14 , 22 , 33 ), (13 , 22 , 34 ), (14 , 22 , 34 )}.
For a further enhancement, in domains such as MAPF, illegal operator pruning can be integrated
with this post-processing step. In our example, if 22 cannot be performed together with 13 , then
there is no need to consider the choice of operator of the third abstraction. In MAPF, this results in
a further significant time speed-up.

References
Bouzy, B. (2013). Monte-carlo fork search for cooperative path-finding. In IJCAI Workshop on
Computer Games.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14(3),
318334.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies and the optimality of A*.
Journal of the Association for Computing Machinery, 32(3), 505536.
Dweighter, H. (1975). Problem e2569. American Mathematical Monthly, 82, 1010.
Edelkamp, S. (2001). Planning with pattern databases. In European Conference on Planning (ECP01), pp. 1324.
Edelkamp, S., & Korf, R. E. (1998). The branching factor of regular search spaces. In AAAI, pp.
299304.
Felner, A., & Adler, A. (2005). Solving the 24-puzzle with instance dependent pattern databases.
In SARA, pp. 248260.
Felner, A., Goldenberg, M., Sharon, G., Stutervant, N., Stern, R., Beja, T., Schaeffer, J., & Holte, R.
(2012). Partial-expansion A* with selective node generation. In AAAI.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal of Artificial
Intelligence Research (JAIR), 22, 279318.
Felner, A., Zahavi, U., Holte, R., Schaeffer, J., Sturtevant, N., & Zhang, Z. (2011). Inconsistent
heuristics in theory and practice. Artificial Intelligence, 175(9-10), 15701603.
Fikes, R. E., & Nilsson, N. J. (1971). Strips: A new approach to the application of theorem proving
to problem solving. Tech. rep. 43R, AI Center, SRI International.
Ghosh, S., Mahanti, A., & Nau, D. S. (1994). ITS: An efficient limited-memory heuristic tree search
algorithm. In AAAI, pp. 13531358.
Goldenberg, M., Felner, A., Stern, R., & Schaeffer, J. (2012). A* variants for optimal multi-agent
pathfinding. In Workshop on Multiagent Pathfinding.
Goldenberg, M., Felner, A., Stutervant, N., Holte, R., & Schaeffer, J. (2013). Optimal-generation
variants of EPEA*. In International Symposium on Combinatorial Search (SoCS).
185

fiG OLDENBERG , F ELNER , S TERN , S HARON , S TURTEVANT, H OLTE , & S CHAEFFER

Helmert, M. (2006). The fast downward planning system.. Journal of Artificial Intelligence Research (JAIR), 26, 191246.
Helmert, M. (2010). Landmark heuristics for the pancake problem. In International Symposium on
Combinatorial Search (SoCS), pp. 745750.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics for optimal sequential planning. In ICAPS, pp. 176183.
Hoffmann, J., & Nebel, B. (2001). The FF Planning System: Fast Plan Generation Through Heuristic Search. Journal of Artificial Intelligence Research (JAIR), 14, 253302.
Holte, R. C., Perez, M. B., Zimmer, R. M., & MacDonald, A. J. (1996). Hierarchical A*: Searching
abstraction hierarchies efficiently. In AAAI, pp. 530535.
Jensen, R. M., Hansen, E. A., Richards, S., & Zhou, R. (2006). Memory-efficient symbolic heuristic
search. In ICAPS, pp. 304313.
Jensen, R. M., Bryant, R. E., & Veloso, M. M. (2002). SetA*: An Efficient BDD-Based Heuristic
Search Algorithm. In AAAI/IAAI, pp. 668673.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In IJCAI, pp. 1728
1733.
Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible treesearch. Artificial
Intelligence, 27(1), 97109.
Korf, R. E. (1993). Linear-space best-first search. Artificial Intelligence, 62(1), 4178.
Korf, R. E. (1997). Finding optimal solutions to Rubiks Cube using pattern databases. In AAAI,
pp. 700705.
Larsen, B. J., Burns, E., Ruml, W., & Holte, R. (2010). Searching without a heuristic: Efficient use
of abstraction. In AAAI.
Richter, S., & Helmert, M. (2009). Preferred operators and deferred evaluation in satisficing planning.. In ICAPS, pp. 273280.
Roger, G., & Helmert, M. (2012). Non-optimal multi-agent pathfinding is solved (since 1984). In
International Symposium on Combinatorial Search (SoCS), pp. 15.
Russell, S. J. (1992). Efficient memory-bounded search methods. In ECAI-92.
Sharon, G., Stern, R., Goldenberg, M., & Felner, A. (2011). The increasing cost tree search for
optimal multi-agent pathfinding. In IJCAI, pp. 662667.
Sharon, G., Stern, R., Goldenberg, M., & Felner, A. (2012). Meta-agent conflict-based search for optimal multi-agent path finding. In International Symposium on Combinatorial Search (SoCS).
Silver, D. (2005). Cooperative pathfinding. In AIIDE, pp. 117122.
Standley, T. (2010). Finding optimal solutions to cooperative pathfinding problems. In AAAI, pp.
173178.
Yang, F., Culberson, J., Holte, R. C., Zahavi, U., & Felner, A. (2008). A general theory of additive
state space abstractions. Journal of Artificial Intelligence Research (JAIR), 32, 631662.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* with partial expansion for large branching factor
problems. In AAAI/IAAI, pp. 923929.
186

fiE NHANCED PARTIAL E XPANSION A*

Zahavi, U., Felner, A., Burch, N., & Holte, R. C. (2010). Predicting the performance of IDA* (with
BPMX) with conditional distributions. Journal of Artificial Intelligence Research (JAIR), 37,
4183.
Zahavi, U., Felner, A., Holte, R. C., & Schaeffer, J. (2008). Duality in permutation state spaces and
the dual search algorithm. Artificial Intelligence, 172(45), 514540.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. R. (2007). Inconsistent heuristics. In AAAI,
pp. 12111216.
Zhou, R., & Hansen, E. (2004). Space-efficient memory-based heuristics. In AAAI, pp. 677682.
Zhou, R., & Hansen, E. A. (2002). Memory-bounded A* graph search. In Florida Artificial Intelligence Research Society (FLAIRS-02), pp. 203209.

187

fiJournal of Artificial Intelligence Research 50 (2014) 409-446

Submitted 12/13; published 6/14

A Multivariate Complexity Analysis of Lobbying in
Multiple Referenda
Robert Bredereck
Jiehua Chen
Sepp Hartung
Stefan Kratsch
Rolf Niedermeier

robert.bredereck@tu-berlin.de
jiehua.chen@tu-berlin.de
sepp.hartung@tu-berlin.de
stefan.kratsch@tu-berlin.de
rolf.niedermeier@tu-berlin.de

TU Berlin, Germany

Ondrej Suchy

ondrej.suchy@fit.cvut.cz

Czech Technical University in Prague, Czech Republic

Gerhard J. Woeginger

gwoegi@win.tue.nl

TU Eindhoven, The Netherlands

Abstract
Assume that each of n voters may or may not approve each of m issues. If an agent (the
lobby) may influence up to k voters, then the central question of the NP-hard Lobbying
problem is whether the lobby can choose the voters to be influenced so that as a result
each issue gets a majority of approvals. This problem can be modeled as a simple matrix
modification problem: Can one replace k rows of a binary nm-matrix by k all-1 rows such
that each column in the resulting matrix has a majority of 1s? Significantly extending on
previous work that showed parameterized intractability (W[2]-completeness) with respect
to the number k of modified rows, we study how natural parameters such as n, m, k, or
the maximum number of 1s missing for any column to have a majority of 1s (referred
to as gap value g) govern the computational complexity of Lobbying. Among other
results, we prove that Lobbying is fixed-parameter tractable for parameter m and provide a
greedy logarithmic-factor approximation algorithm which solves Lobbying even optimally
if m  4. We also show empirically that this greedy algorithm performs well on general
instances. As a further key result, we prove that Lobbying is LOGSNP-complete for
constant values g  1, thus providing a first natural complete problem from voting for this
complexity class of limited nondeterminism.

1. Introduction
Campaign management comprises all sorts of activities for influencing the outcome of an
election, including well-known scenarios such as bribery (Faliszewski, Hemaspaandra, &
Hemaspaandra, 2009; Dorn & Schlotter, 2012; Schlotter, Elkind, & Faliszewski, 2011;
Elkind, Faliszewski, & Slinko, 2012) and control (Bartholdi III, Tovey, & Trick, 1992; Elkind,
Faliszewski, & Slinko, 2011; Erdelyi, Piras, & Rothe, 2011). While these works relate to
campaigning in case of classical voting scenarios where one typically wants to make a specific
candidate win or to prevent him from winning, Christian, Fellows, Rosamond, and Slinko
(2007) introduced the scenario of lobbying in multiple referenda. Intuitively, the point here
is that there are n voters, each of them providing a yes- or no-answer to each of m issues. In
other words, we have a referendum on m issues the voters can decide on. Naturally, before
c
2014
AI Access Foundation. All rights reserved.

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

the referendum is held, campaigns will be run by various parties and interest groups to
influence the outcome of the referendum. Assuming complete knowledge about the current
voter opinions and assuming the extreme scenario that an external agentthe lobbygains
complete control over specific voters, Christian et al. modeled this very basic scenario as
0/1-matrix modification problem, where the rows represent voters, the columns represent
issues to vote on with yes (1) or no (0), and the lobby goal is represented by a 0/1-vector.
Lobbying
Input:
A matrix A  {0, 1}nm and an integer k  0.
Question: Can one modify the entries of at most k rows in A such that in the resulting
matrix every column has more 1s than 0s?
In our context, modifying a row means to simply flip all 0s to 1s. Hence, modifying a
minimum number of rows can be interpreted as the lobby influencing a minimum number
of voters to reach a certain goal. In difference to Christian et al., we assume that the
desired outcome of each column is 1 instead of providing a goal vector of the lobby. That is,
1 corresponds to agreement with the lobby goal and 0 corresponds to disagreement. Clearly,
by appropriately flipping all entries of a column this can be always ensured. Furthermore,
we assume that any column with a majority of 1s is removed from the input matrix. The
following example, which is an extract from the real-world data from Section 6.2, illustrates
our model.
Example 1. Consider the following four issues and voting behavior of the five faction
leaders extracted from the recorded votes of the German parliament in 2013. (See Section 6.2
for details about the full data set.)
Selected issues:
1. Water access is a human right.
2. Forbid the Nationalist Party.
3. Financial help for Ireland.
4. Financial help for Cyprus.

Bruderle
Gysi
Kauder
Kunast
Steinmeier

1
No
Yes
No
Yes
No

2
No
Yes
No
Yes
Yes

3
Yes
No
Yes
Yes
Yes

4
Yes
No
Yes
Yes
Yes

Assume that the lobby wants to approve the first two issues and disapprove the last two
issues. Then, the matrix translates into the following binary matrix.
0
1
0
1
0

0
1
0
1
1

0
1
0
0
0

0
1
0
0
0

The second column can be removed because
the majority of voters already agree with the
lobby. Modifying the first and third row
yields a solution.

Lobbying is NP-complete (Christian et al., 2007). Moreover, in the setting of parameterized complexity analysis (Downey & Fellows, 2013; Flum & Grohe, 2006; Niedermeier,
2006), Christian et al. showed that it is W[2]-complete with respect to the parameter number k of rows to modify, that is, even if only a small number of voters shall be influenced
the problem is computationally intractable. In this work, we provide a significantly more
410

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

refined view on the parameterized and multivariate computational complexity (Fellows,
Jansen, & Rosamond, 2013; Niedermeier, 2010) of Lobbying. To this end, we identify the
following parameters naturally occurring in Lobbying and analyze their influence on its
computational complexity. The studied parameters are:
 n: number of rows;
 m: number of columns1 ;
 k: number of rows to modify;
 k 0 :=d(n + 1)/2e  k: below-guarantee parameter;2
 g:=maxm
j=1
Pngj : maximum gap value over all columns, where the gap value gj :=d(n +
1)/2e  i=1 Ai,j is the number of missing 1s to make column j have more 1s than
0s;
 s: maximum number of 1s per row;
 t: maximum number of 0s per row.
1.1 Parameter Choice
The parameters n, m, and k are naturally occurring parameters as they measure the input
size and the solution size, respectively. Scenarios with only few voters or issues are clearly
interesting and realistic restrictions. Furthermore, also the restriction to instances with
small solutions is very natural as a limited budget of the lobby may only allow a (very)
small amount of bribery or, more positively, advertisement. Additionally, k 0 complements
the parameter k and seems promising because it measures the distance from a trivial type
of yes-instances. Moreover, observe that the maximum gap value g is a lower bound on the
number of rows that have to be modified; it can be precomputed in linear time. Furthermore,
sets of issues where each single issue needs only a small amount of additional approvals or
disapprovals bear good prospects for a lobby with limited budget. Finally, s and t measure
the density of the input matrix. In our model the density of the matrix can be seen as the
degree of agreement between the voters and the lobby. Hence, it is worth to investigate
whether high or low density leads to computational tractability.
1.2 Parameter Relations
There are various relations between the parameters values above. For instance, columns
containing a majority of 1s can be safely removed (also implying positive gap values for all
columns). As this implies that the input matrix has at least as many 0s as 1s, it follows
that there has to be at least one row where at least half of the entries are 0s. Hence,
t  m  2t. In addition, if the input matrix contains a column only consisting of 0s, then
one has to modify d(n + 1)/2e rows, implying that the corresponding Lobbying instance is
1. Christian et al. (2007) argued that m seldom exceeds 20.
2. Clearly, lobbying d(n + 1)/2e voters, that is, modifying d(n + 1)/2e rows in the input matrix yields always
the desired solution, making d(n + 1)/2e a trivial upper bound for k.

411

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

(even for constant parameter values)

LOGSNP-hard
maximum
gap value g

NP-hard

maximum number
s of 1s per row

below-guarantee
parameter k 0

(s, k 0 )

W[2]-hard
number
k of rows
to modify

(s, k)

(s, g)

(m, g)

number
n of rows

(m, k)

(k, k 0 )

ILP-FPT

(m, k 0 )

no polynomial kernel
(s, n)

trivial FPT
(parameters provide upper bounds
of the input size)

maximum number
t of 0s per row

number m
of columns

(m, n)

FPT
(fixed-parameter tractable)

(t, n)

Figure 1: Parameterized complexity of Lobbying and relations between the parameters
considered in this paper. An arc from x to y means that there is some function
f with x  f (y). We omit combined parameters where one component is upperbounded by a function of the other component, for example (n, k) is omitted,
because k  n. ILP-FPT means that the fixed-parameter tractability bases on a
formulation as integer linear program.

a yes-instance if and only if k  d(n + 1)/2e. Also, we assume m  n  s as otherwise there
has to be a column consisting only of 0s which is a trivial input instance since we would
only have to check whether k  d(n + 1)/2e. The relations between these parameters and
the combinations considered in this paper are illustrated in Figure 1.
Finally, we remark that in this article we focus on the plain lobbying in multiple
referenda scenarios. We leave considerations of more fine-grained aspects of the lobbying
process (such as allowing different forms of modifying the input matrix) as a natural next
step for future work.
412

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

1.3 Related Work
Our central point of reference is the work by Christian et al. (2007) whose key result
is the proof of W[2]-completeness of Lobbying with respect to the number k. Erdelyi,
Hemaspaandra, Rothe, and Spakowski (2007) generalized Lobbying to a weighted scenario and showed that an efficient greedy algorithm achieves a logarithmic approximation
factor (1 + ln(m  d(n + 1)/2e)) for the weighted case. Moreover, they showed that essentially no better approximation ratio can be proven for their algorithm. Later, models for
lobbying in a probabilistic environment were proposed by Binkele-Raible, Erdelyi, Fernau,
Goldsmith, Mattei, and Rothe (2014), providing (parameterized) hardness and tractability
results; they interpret lobbying as a form of bribery.
As a special case of the combinatorial reverse auction problem (Sandholm, Suri, Gilpin,
& Levine, 2002), the optimization version of Lobbying is relevant to combinatorial markets
in multi-agent systems. In combinatorial reverse auctions, there are different items that a
buyer wants to acquire at the lowest cost from the sellers. More precisely, the buyer wants
to have an amount of units from each item, U = (u1 , u2 , . . . , um ). Each seller i submits
how many goods he has for each item, (i1 , i2 , . . . , im ) and the price pi for which he would
sell. The goal is to minimize the cost while acquiring the required amount of units of each
item. It is easy to see how to translate our lobbying problem into a combinatorial reverse
auction problem: Each issue j corresponds to an item j. The buyers requirement for each
item j is the gap value of the corresponding column, uj := gj , j  {1, . . . , m}. Each row i
corresponds to a seller with offer ij := 1  Ai,j , j  {1, . . . , m} and price pi := 1.
Lobbying is also closely related to bribery in judgment aggregation (Baumeister, Erdelyi,
& Rothe, 2011) where the judges submit binary opinions on different propositions and the
goal is to bribe as few judges as possible in order to obtain a certain outcome. A Lobbying
instance can be formulated as an equivalent instance of the bribery in judgment aggregation
problem using a premise-based procedure. More precisely, given a binary matrix A, each
propositional variable vj corresponds to a column j, each judge i corresponds to a row i with
the judgment set consisting of those variables vj satisfying Ai,j = 1, and the agenda consists
of all propositional variables as premises and no conclusions. Now the goal of modifying
as few rows as possible in order to have more 1s than 0s in each column is equivalent to
bribing as few judges as possible in order to have all possible variables approved by more
than half of the judges.
Finally, we refer to survey articles for a discussion on the importance of parameterized
complexity results in the context of artificial intelligence (AI) (Gottlob, Scarcello, & Sideri,
2002; Gottlob & Szeider, 2008) and voting (Betzler, Bredereck, Chen, & Niedermeier, 2012).
1.4 Our Contributions
We initiate a systematic parameterized and multivariate complexity analysis for the Lobbying problem. We contribute to the theoretical as well as to the empirical side. See the
preliminaries in Section 2 for definitions of parameterized complexity classes and the like.
Our complexity results are summarized in Table 1. Let us sketch a few highlights.
 In Section 3, we show that Lobbying remains NP-hard for s = 3 and k 0 = 1. Thus,
there is no hope for fixed-parameter tractability with respect to the parameters s or k 0 .
Moreover, a special highlight of our work is to show that even if the gap parameter g
413

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

m
m
t
s
k0
g

t

s
k0
g
k
n
ILP-FPT (Thm. 9, Cor. 2)
FPT (Thm. 10)
m
m
(2m )2.52 +o(2 )  m  n
(g + 1)m  n2  m
s  3: NP-c (Thm. 1)
FPT (Cor. 3)
s  2: P (Thm. 6)
NP-c
(g + 1)4s  s  n2 + 16g  m
(Thm. 1) XP, FPT for const. g
FPT (Prop. 2)
0
2g+1
2k
m
 2 (Thm. 11)
2n  m
a
W[2]-h
LOGSNP-c (Thm. 2)
W[2]-ca

k
n
a. (Christian et al., 2007)

Table 1: Summary of results. For each parameter combination (row by column) the entries
indicate whether Lobbying is fixed-parameter tractable (FPT), fixed-parameter
tractable based on a formulation as integer linear program (ILP-FPT), polynomialtime solvable for constant parameter values (XP), W[2]-hard (W[2]-h), W[2]complete (W[2]-c), LOGSNP-complete, meaning completeness for a class of limited
nondeterminism lying between P and NP for constant parameter values (LOGSNPc), or NP-complete even for constant parameter values (NP-c). Entries on the main
diagonal represent results with respect to single parameters. Furthermore, for
(m, n), (t, n), and (s, n), we have problem kernels of polynomial size because these
parameters naturally bound the input size (see Figure 1). For (m, k 0 ) we are not
aware of kernel size bounds. For all other parameter combinations above, under
some reasonable complexity-theoretic assumptions, there cannot be a polynomialsize problem kernel (see Section 4).

is equal to one, Lobbying remains intractable in the sense of being complete for the
limited nondeterminism class LOGSNP (Papadimitriou & Yannakakis, 1996) (also see
the survey in Goldsmith, Levy, & Mundhenk, 1996). This is of particular interest for
at least two reasons. First, it provides a first natural voting problem complete for
LOGSNP. Second, this is one of the so far rare examples where this complexity class
is used in the context of parameterized complexity analysis.
 In Section 4, we reveal limitations of effective polynomial-time preprocessing for Lobbying, that is, we prove that polynomial-size problem kernels are unlikely to exist.
 In Section 5, we show that Lobbying is fixed-parameter tractable for parameter m by
means of describing an ILP formulation with at most 2m variables. We further provide
two efficient algorithms yielding provably optimal results for input matrices with up
to four columns. One of these two algorithms is based on a simple greedy strategy and
provides a logarithmic approximation ratio for cases with more than four columns.
414

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

Furthermore, we develop several fixed-parameter algorithms for various parameter
combinations and show that Lobbying is polynomial-time solvable for s  2.
 Finally, in Section 6, we experimentally compare our greedy heuristic, with the heuristic in the work of Erdelyi et al. (2007), and an implementation of our ILP formulation
providing solutions guaranteed to be optimal. Our empirical results with random data
and real-world data indicate that Lobbying can be solved efficiently.

2. Preliminaries
Our aim is to provide a deeper understanding of the computational complexity of the NPcomplete Lobbying problem. To this end, we employ classical complexity classes such as P
(polynomial time) and NP (nondeterministic polynomial time) (Garey & Johnson, 1979) as
well as the class LOGSNP of limited nondeterminism (lying between P and NP) (Papadimitriou & Yannakakis, 1996; Goldsmith et al., 1996) and parameterized complexity classes
such as FPT (fixed-parameter tractability), W[2] (second level of the weft hierarchy of
presumable parameterized intractability), and XP (Downey & Fellows, 2013; Flum & Grohe,
2006; Niedermeier, 2006). Throughout this paper, we denote by log the logarithm to base
two.
2.1 LOGSNP
LOGSNP has been introduced by Papadimitriou and Yannakakis (1996) to precisely characterize the computational complexity of certain problems in NP that are neither known
to be NP-complete nor known to be solvable in polynomial time. LOGSNP is a subclass of
problems in NP which can be decided in polynomial time with an initial phase of O(log2 N )
nondeterministic steps, where N is the overall input size. However, LOGSNP does not include all problems decidable in polynomial time after the nondeterministic phase since it
puts additional restrictions on the computation. Roughly speaking, one is only allowed to
use some constant number of elements of the guessed solution.
It is widely believed that LOGSNP is properly intermediate between P and NP. Problems complete for LOGSNP under polynomial-time reductions include Rich Hypergraph
Cover (see Section 3 for the definition) and Log Adjustment (Papadimitriou & Yannakakis, 1996), the latter being of particular importance in artificial intelligence. In Log
Adjustment, a boolean expression in conjunctive normal form with r variables and a truth
assignment T are given, and the question is whether there is a satisfying truth assignment
whose Hamming distance from T is at most log r.
Alternative characterizations of LOGSNP exist (Cai, Chen, Downey, & Fellows, 1997;
Flum & Grohe, 2006, Sec. 15.2).
2.2 Fixed-Parameter Tractability and XP
The concept of parameterized complexity was pioneered by Downey and Fellows (2013) (see
also further textbooks: Flum & Grohe, 2006; Niedermeier, 2006). The fundamental goal is to
find out whether the seemingly unavoidable combinatorial explosion occurring in algorithms
to solve NP-hard problems can be confined to certain problem-specific parameters. If such
a parameter assumes only small values in applications, then an algorithm with a running
415

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

time that is exponential exclusively with respect to this parameter may be efficient. For
Lobbying, we suggest a number of (potentially small) parameters. A combined parameter
(which is a vector of parameters) can, for the sake of convenience, be simply seen as the
sum of its components.
Formally, a problem is fixed-parameter tractable (equivalently, contained in the parameterized complexity class FPT) with respect to a parameter p (equivalently, when parameterized by p) if any instance (I, p) of this problem can be solved in f (p)  |I|O(1) time,
where f solely depends on p. If the problem can only be solved in polynomial running time
where the degree of the polynomial depends on p (such as |I|f (p) ), then, for parameter p,
the problem is said to lie in thestrictly larger (Downey & Fellows, 2013)parameterized
complexity class XP. Note that containment in XP ensures polynomial-time solvability for a
constant parameter p whereas FPT additionally ensures that the degree of the corresponding
polynomial is independent of the parameter p.
2.3 Kernelization
Another way of showing fixed-parameter tractability is through kernelization. A kernelization algorithm takes as input a problem instance I together with a parameter p and
transforms it in polynomial time into an instance I 0 with parameter p0 such that (I, p) is
a yes-instance if and only if (I 0 , p0 ) is a yes-instance and there is a function f such that
p0  f (p) and |I 0 |  f (p). The function f measures the size of the (problem) kernel (I 0 , p0 ).
A problem kernel is said to be a polynomial kernel if f is polynomially bounded. Note that it
is well-known that a decidable problem is fixed-parameter tractable with respect to a parameter if and only if it is kernelizable (Cai et al., 1997). The corresponding kernels, however,
may have exponential size and it is of particular interest to determine which problems, with
respect to which parameter(s), allow for polynomial-size problem kernels (Bodlaender, 2009;
Guo & Niedermeier, 2007) since the total running time complexity may vary dependent on
the kernel size. Using techniques developed by Bodlaender, Downey, Fellows, and Hermelin
(2009) and by Fortnow and Santhanam (2011), Szeider (2011) recently proposed to examine
the power of kernelization for several problems in Artificial Intelligence. See Section 4 for
more discussion.
2.4 Parameterized Intractability
Downey and Fellows (2013) also introduced a framework of parameterized intractability. In
this framework, the two basic complexity classes of parameterized intractability are W[1]
and W[2], and a problem is shown to be W[1]- or W[2]-hard by providing a parameterized
reduction from a W[1]- or W[2]-hard problem to the problem in question. A parameterized
reduction from a problem 1 to another problem 2 is a function f computable in FPT
time such that there is a function g and the instance (I2 , p2 ) produced by f on instance
(I1 , p1 ) satisfies the following:
 (I1 , p1 ) is a yes-instance of problem 1 if and only if (I2 , p2 ) is a yes-instance of
problem 2 , and
 p2  g(p1 ).
416

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

W[1]-complete problems include Clique parameterized by the solution size (Downey &
Fellows, 2013) while W[2]-complete problems include Set Cover parameterized by the
solution size (see Section 3.1 for the formal definition).
If a problem is shown to be NP-hard even if parameter p is a constant, then it cannot
be contained in XP unless P = NP.

3. Intractable Cases
In this section, we examine the worst-case computational complexity of Lobbying. Besides
NP-completeness in one very restricted case, we also prove a LOGSNP-completeness result
for the case that in each column only one 1-entry is missing to reach a majority of 1s.
3.1 NP-Completeness
For some of the hardness reductions presented in this work, NP-complete variants of the
following Set Cover (SC) problem are used.
Set Cover (SC)
Input:
A family of sets S = {S1 , . . . , S` } over a universe U = {u1 , . . . , ur } of elements
and an integer h  0.
Question: Is there a size-h set cover, that is, a collection of h sets in S whose union is
U?
Note that even 3-Set Cover (3-SC), where each set from S is of size at most three, is
NP-complete (Karp, 1972). In order to make the reduction work, we need to let h, |S|, and
the number of occurrences of every element in the sets in S fulfill the following properties
whose correctness is easy to see:
1. We add (multiple copies of) singletons to the family S to ensure that |S|  2h + 1  0
and to ensure that every element appears in at least h sets in S.
2. We also add a new element u to the universe U, add h copies of the singleton {u }
to the family S, and set h := h + 1 to ensure that each element appears in at most
|S|  h sets in S.
A common starting point for each of our SC reductions is to transform S (and U) into
a binary matrix in a similar way as Christian et al. (2007) transformed Dominating Set
instances into binary matrices. The corresponding |S|  |U|-matrix is called SC-matrix and
is defined as
(
1 if uj  Si ,
M (S, U) = (xi,j ) with xi,j :=
0 otherwise.
Observe that each column j  {1, . . . , |U|} has 1s in at most (|S|  h) rows. We will make
use of the SC-matrix in Section 4 and in the following theorem.
An instance of Lobbying with k  d(n + 1)/2e (that is, more than half of the rows can
be modified) is a yes-instance. Following general ideas of Mahajan and Raman (1999) in the
context of above and below guarantee parameterization, we investigate the complexity of
Lobbying in case the maximum number of 1s per row is bounded by a constant and k is
slightly below the bound d(n + 1)/2e. Such a parameterization is called below guarantee.
417

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

U = {1, 2, 3, 4, 5}
S = { S1 = {2, 4}
S2 = {3, 5}
S3 = {1, 2, 5}
S4 = {1, 2, 3}
S5 = {4, 5} }

S1 :
S2 :
S3 :
S4 :
S5 :

1
0
0
1
1
0
0
0

2
1
0
1
1
0
0
0

3
0
1
0
1
0
0
0

4
1
0
0
0
1
0
0

5
0
1
1
0
1
0
0

6
0
0
0
0
0
1
0

7
0
0
0
0
0
0
1

k := |S|  h = 3

h=2

Figure 2: Illustration of the reduction from 3-SC to Lobbying. Left: an 3-SC instance;
Right: the constructed Lobbying instance. The SC-matrix together with two
dummy rows and two dummy columns (inside the dashed polygon) ensure that
any set of rows which are not selected by some solution of size three corresponds
to a set cover. The 3-SC solution (S4 and S5 ) is highlighted in boldface on the
left. The Lobbying solution (modifying the first three rows) is highlighted by
gray backgrounds.

Theorem 1. Lobbying remains NP-complete for input matrices where the maximum number s of ones per row is three and for k 0 = 1 of the below guarantee parameter k 0 =
(d(n + 1)/2e  k).
Proof. Obviously, given a matrix A and k = d(n + 1)/2e  1 rows of A, we can check in
polynomial time whether each row of A has at most three 1s and whether modifying the
k given rows makes every column have more 1s than 0s. Hence, Lobbying with s = 3 and
k 0 = 1 remains in NP.
For the NP-hardness result, we describe a polynomial-time reduction from 3-SC to
Lobbying where each row contains at most three 1s. The reduction is illustrated with an
example in Figure 2. Let (S, U, h) denote a 3-SC instance. First, compute the SC-matrix
M (S, U), which we refer to as original rows and original columns in the following. Second,
add |S|2h+1 additional dummy rows and |S|2h+1 additional dummy columns containing
only 0s. Recall that we can assume that |S|  2h + 1  0. Finally, for 1  i  |S|  2h + 1,
change the entry in the ith dummy column of the ith dummy row to 1. To complete the
construction we set k := |S|  h.
Each original row has at most three 1s since each set of S has at most three elements.
Each dummy row has exactly one 1. Further, the below guarantee parameter k 0 = d(n +
1)/2e  k = d(2|S|  2h + 1)/2e  (|S|  h) = 1. The quota for a column to have more 1s
than 0s is |S|  h + 1. Since every original column has 1s in at most |S|  h original rows
and no 1s in the dummy rows, no original column has more 1s than 0s in the matrix. More
precisely, the jth original column has gap value |S|  h + 1  |{Si | uj  Si }|. Each dummy
column has gap value |S|  h. Hence the maximum gap value g is |S|  h.
418

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

Obviously, the reduction runs in polynomial time. Now, we show that (S, U, h) is a
yes-instance of 3-SC if and only if the constructed matrix can be modified to have more 1s
than 0s in every column by changing at most k rows to all-1-rows.
For the only if part, suppose that (S, U, h) is a yes-instance for 3-SC. Let S 0  S
denote a set cover of size h. By modifying each original row i that corresponds to a set
Si 
/ S 0 , every column has more 1s than 0s, that is, at least (|S|  h + 1) 1s: Each jth
dummy column gains (|S|  h) 1s from the modified original rows and another 1 from the
jth dummy row. Each original column gains (|S|h) 1s from the modified original rows and
another 1 from an unmodified original row, since the sets corresponding to the unmodified
original rows form a set cover.
For the if part, suppose that the constructed matrix has more 1s than 0s in every
column by modifying at most k = |S|  h rows. First, observe that the number of modified
rows is exactly |S|  h since the maximum gap value is |S|  h. Second, no dummy row
is modified: Assume towards a contradiction that the i0 th dummy row is modified. Since
the i0 th dummy column has only one 1 in the matrix, namely, at the i0 th dummy row, this
column cannot get a majority of (|S|  h + 1) 1s by the modification of any |S|  h rows
including the i0 th dummy rowa contradiction. Third, we show that the sets corresponding
to the unmodified original rows form a size-h set cover for (S, U, h). Each original column j 0
gets exactly (|S|  h) 1s from the modified original rows and no 1 from any dummy row.
To get a majority of (|S|  h + 1) 1s, column j 0 must contain another 1 in some unmodified
original row. Hence, the sets corresponding to the unmodified rows form a set cover of
size k. This shows the correctness of the construction.
Proposition 1. Lobbying remains NP-complete for every constant integer value k 0 > 1
of the below-guarantee parameter k 0 = (d(n + 1)/2e  k).
Proof. The NP-containment follows analogously to Theorem 1. To show the NP-hardness
for any constant k 0 > 1, we take the SC-matrix M (S, U) and add x additional dummy
columns and xk 0 additional dummy rows where x := (|S|  2h  1)/k 0 + 2. Note that we
can add singletons to the family S to make sure k 0 is a divisor of (|S|  2h  1). We fill
the added entries as follows: For each j  {1, . . . , |U|}, set the entries of k 0  1 arbitrary
dummy rows at the original column j to 1. For 1  j  x, set the entry in the jth dummy
column of the [(j  1)k 0 + 1]th, [(j  1)k 0 + 2]th, . . ., [(j  1)k 0 + k 0 ]th dummy rows to 1.
Set k := |S|  h.
In total, the constructed matrix has 2|S|  2h + 2k 0  1 rows and |U| + x columns. The
quota for a column to have more 1s than 0s is q = |S|  h + k 0 . Each original column has
gap value q  (|{Si | uj  Si }| + k 0  1) = |S|  h + 1  (|{Si | uj  Si }|) and each dummy
column has gap value q  k 0 = |S|  h.
The reduction runs in polynomial time. Now, it remains to show the correctness.
Suppose that (S, U, h) has a set cover S 0  S of size h. By modifying each original row
i that corresponds to a set Sj 
/ S 0 , every column has more 1s than 0s.
Conversely, suppose that the constructed matrix has more 1s than 0s after modifying
at most k rows. Using the analogous reasoning as in the proof for Theorem 1, we can
show that exactly k original rows are modified and the original rows which are not modified
correspond to the sets which can form a set cover for (S, U, h).
419

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

3.2 LOGSNP-Completeness
In this section, we show that Lobbying for instances with constant maximum gap value g =
1 is LOGSNP-complete (Papadimitriou & Yannakakis, 1996) (see also Section 2 for some
discussion about the class). This implies that,
 unless LOGSNP = P, Lobbying restricted to instances with constant value g cannot
be solved in polynomial time, and hence, Lobbying parameterized by g is not in XP,
and
 unless LOGSNP = NP, Lobbying restricted to instances with constant value g is not
NP-hard.
Theorem 2. Lobbying for instances with constant maximum gap value g is in LOGSNP.
Proof. First of all, we show that if k  g  (blog mc + 1), then the Lobbying instance (A, k)
is a yes-instance. Recall that there is a row containing 0s in at least half of the columns
(see the parameter discussion in Section 1). Modify this row and continue with the columns
in which nothing was modified so far. Again, there has to be a row containing 0s in at
least half of the columns, which one can modify. We repeat this until each column has been
touched at least once, decreasing g by at least one. This takes at most (blog mc + 1) steps.
Then, we remove all columns with gap value zero. We repeat this procedure at most g times
to end up with an empty matrix, modifying altogether at most g  (blog mc + 1) rows.
It follows from the observations above that any Lobbying instance (A, k) can be solved
by modifying at most g  (blog mc + 1) rows (independent of the value of k). Since each row
can be identified by O(log n) bits, there is a certificate for yes-instances of the problem that
uses only O(log2 (n + m)) many bits. This implies that the Lobbying problem belongs to
problems decidable in polynomial time after O(log2 (n + m)) non-deterministic steps. This
already indicates that Lobbying with constant maximum gap lies somewhere between P
and NP. To show the containment of LOGSNP, we reduce Lobbying with constant maximum gap to the LOGSNP-complete Rich Hypergraph Cover problem (Papadimitriou
& Yannakakis, 1996) in polynomial time.
Rich Hypergraph Cover (RHC)
Input:
A family of sets S = {S1 , . . . , S` } over a universe U = {u1 , . . . , ur } of an even
number r of elements, where |Sj |  r/2 for all Sj , and an integer h  0.
Question: Is there a subset U 0  U of size at most h that has a non-empty intersection
with every set Sj  S?
Let (A  {0, 1}nm , k) be an input instance to Lobbying with constant maximum gap
value g. First, for each column j  {1, . . . , m}, let Zj be the set of rows that have 0s
at column j and let gj be the gap value of column j, that is, the number of missing 1s
to make column j have more 1s than 0s. We assume that (A, k) is non-trivial, that is,
no column consists of only 0s and all gap values are positive. Now, consider all possible
size-(|Zj |  gj + 1) subsets of row set Zj . If there is a set X of rows which intersects each
of these subsets, then modifying all rows in X make column j have more 1s than 0s. We
exploit this observation and construct an instance of RHC from (A  {0, 1}nm , k).
420

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

For each row i  {1, . . . , n}, add an element ui to the universe U. For each column j 
{1, . . . , m}, add to the family S all possible size-(|Zj |  gj + 1) subsets of elements in
set {ui | i  Zj }. We say that all these newly added subsets correspond to column j. Let
the solution size h equal k. This completes the reduction.
Since g is constant and at most m  ng different subsets are added to S, the reduction
runs in polynomial time. It remains to show the correctness, that is, (A, k) is a yes-instance
to Lobbying if and only if the constructed instance (S, U, h) is a yes-instance to Rich
Hypergraph Cover.
For the only if part, assume that (A, k) is a yes-instance. Let R0 be a set of rows of
size at most k such that modifying them makes every column have more 1s than 0s. Let
U 0 = {ui | i  R0 } be the set of corresponding elements. We show that U 0 intersects with
every set S  S. Suppose for the sake of contradiction that there is a set S   S with
S   U 0 = . By construction of the subsets in S, let set S  correspond to column j and,
hence, |S  | = |Zj |  gj + 1. Then R0 can contain at most (gj  1) rows from Zj which makes
column j not have more 1s than 0sa contradiction.
For the if part, assume that (S, U, h) is a yes-instance. Let U 0 be a set of elements
of size at most h which intersects every set S  S. Let R0 = {i | ui  U 0 } be the set
of corresponding rows. We show that modifying all rows in R0 results in a matrix where
every column has more 1s than 0s. Suppose for the sake of contradiction that there is a
column j which has at most as many 1s as 0s after modifying all rows in R0 . This means
that |Zj  R0 |  gj  1, and hence Zj \ R0 contains at least |Zj |  gj + 1 rows. Thus, it is
possible to find a size-(|Zj |  gj + 1) set of elements in {ui | i  Zj \ R0 }  Zj which does
not intersect with U 0 a contradiction.
As a consequence of the above proof, we can assume that any non-trivial instance of
Lobbying fulfills k < g  (blog mc + 1). If we try all combinations of at most k rows to
modify, and for each of them check whether every column has more 1s than 0s, then we
obtain an algorithm solving Lobbying in O(ng(dlog me+1)  m) time. Now, suppose that
Lobbying with constant g were NP-hard, implying there is a polynomial-time reduction
from some NP-complete problem P to Lobbying with constant g. Then we would have
n = |I|O(1) and m = |I|O(1) for the constructed Lobbying instance with |I| being the input
size of problem P. Thus, P would be decidable in |I|O(log |I|) time, implying the following
corollary.
Corollary 1. Lobbying for instances with constant maximum gap value g is not NP-hard
unless all problems in NP can be solved in |I|O(log |I|) time, where |I| is the size of the input.
Next, we show that Lobbying is LOGSNP-hard even if each column needs only one
additional 1 to reach a majority of 1s, that is, g = 1.
Theorem 3. Lobbying for instances with maximum gap value g = 1 is LOGSNP-hard.
Proof. We reduce in polynomial time from Rich Hypergraph Cover (RHC) to Lobbying with maximum gap value g = 1. The definition of RHC is already given in the proof of
Theorem 2.
Consider an arbitrary RHC instance (S, U, h) with |S| = ` and |U| = r. As already
mentioned by Papadimitriou and Yannakakis (1996), we can assume that h  blog `c + 1 as
421

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

otherwise (S, U, h) is a trivial yes-instance: Since each set in S contains at least half of the
elements from U, there should be one element which appears in at least half of the sets in
S. Add to the solution an element which appears in at least half of the subsets and delete
all sets in S which contain this element, and repeat. Now, let w := max(dlog `e + 2, dlog re).
This implies r/2  2w1 , h  w  1 and 2w  8` + 2r, that is, 2w is upper-bounded by a
polynomial in ` and r. Furthermore, let ~vi with 1  i  2w  1 be an enumeration of all
0/1-vectors of length w with the exception of the all-zero vector. For every such vector ~vi ,
let V(i) denote the set of all ~vi0 (1  i0  2w  1) for which the inner product ~vi  ~vi0 is odd;
it is easily seen that |V(i)| = 2w1 .
Now, we are ready to construct an instance (A, k) for Lobbying. Add to matrix A
one element row i for each element ui  U and one so-called dummy row i0 for each 0/1vector ~vi0 (1  i0  2w  1) of length w except for the all-zero vector. The total number n
of rows in A is r + 2w  1, which is odd, because r is even. The quota for a column to
have more 1s than 0s is q = r/2 + 2w1 . Let matrix A have a total of m = `  (2w  1)
columns, one column c(j,z) for each pair (j, z) with 1  j  ` and 1  z  2w  1. We fill
the entries of A as follows: let column c(j,z) have 0 at element row i if the corresponding
set Sj  S contains element ui , and 1 otherwise. Further, let column c(j,z) have 0s at q |Sj |
arbitrarily chosen dummy rows i0 with ~vi0  V(z) (that is, the inner product ~vi0  ~vz is odd),
and have 1s in the remaining dummy rows. Note that our choice of w implies |Sj |  q and
0  q  |Sj | = r/2 + 2w1  |Sj |  2w1 such that there are enough dummy rows i0 with
~vi0  V(z). In this way, we fix the entries of matrix A such that each column has exactly
q  1 ones and q zeros, which means that the maximum gap value g equals one.
Finally, set the parameter k to be h. This completes the reduction. Obviously, it runs
in polynomial time. It remains to show that the RHC instance (S, U, h) is a yes-instance if
and only if the constructed Lobbying instance (A, k) is a yes-instance.
For the only if part, assume that the RHC instance (S, U, h) is a yes-instance, which
is certified by a subset U 0 of size at most h = k. By construction of matrix A, modifying
the element rows corresponding the elements in U 0 makes every column c(j,z) gain at least
one additional 1.
For the if part, assume that modifying at most k rows in matrix A results in a matrix
where each column has more 1s than 0s. Let R0 be the set of modified element rows,
and define D = {d1 , d2 , . . . , dy } so that d1 , . . . , dy are the modified dummy rows. Thus,
y  k = h  w  1. Let U 0 = {ui | i  R0 } be the set of elements that correspond to the
element rows from R0 . For the sake of contradiction, suppose that there is a set Sj  S
which does not intersect with U 0 . Then, for every 1  z  2w  1, column c(j,z) has 1s
in all element rows from R0 . We show that there even is a column c(j,z) which has 1s at
every dummy row from D. In order to find this column, we first observe that by standard
linear algebra there exists a vector ~vz (1  z  2w  1) that simultaneously satisfies all
equations ~vdi  ~vz = 0 over the finite field of size two for 1  i  y. (The equation system
is homogeneous, the dimension of the underlying space is w, and there are only y  w  1
constraining equations.) Then, by definition, none of the vectors ~vd1 , . . . , ~vdy are contained
in the set V(z). Thus, column c(j,z) has 1s in all dummy rows d1 , . . . , dy a contradiction
since c(j,z) does not gain an additional 1 after modifying rows from R0  D.
422

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

4. Limits of Preprocessing
Efficient preprocessing and data reduction techniques are of key importance when trying
to (exactly) solve NP-hard problems (Guo & Niedermeier, 2007); we also refer to Szeider
(2011) for a general account on showing limits of preprocessing for AI problems. In this
section, we prove that for almost all fixed-parameter tractability results stated in Table 1,
Lobbying does not admit, under a reasonable complexity-theoretic assumption, an efficient
preprocessing algorithm formalized as a polynomial kernelization in parameterized algorithmics. A sufficient assumption for almost all known lower bounds for kernelization is that
NP * coNP/poly; it is known that NP  coNP/poly would imply a collapse of the polynomial
hierarchy to its third level (Yap, 1983). One way for showing the non-existence of polynomial kernels is to give a so-called polynomial time and parameter transformation from an
NP-hard problem that is already shown unlikely to admit a polynomial kernel (Bodlaender,
Thomasse, & Yeo, 2011). A polynomial time and parameter transformation is a parameterized reduction that is required to be computable in polynomial time and the parameter
in the instance where one reduces to is polynomially upper-bounded by the parameter in
the instance that one reduces from.
We prove that Lobbying admits neither a polynomial-size kernel with respect to the
combined parameter (m, k) nor with respect to n. By simple further observations on the relations between parameters, these two results imply the non-existence of polynomial kernels
for most parameters and parameter combinations as listed in Table 1. Recall Figure 1 and
the parameter discussion in Section 1 for details on the parameter relations. In some cases
Lobbying is NP-hard even if the parameters are constants (see s, k 0 , and (s, k 0 )), in other
cases the parameters can be bounded by functions only depending on n (see g, k, k 0 , n,
and combinations of them) or the parameters can be bounded by functions only depending
on m and k (see t, m, g, k, and combinations of them). For (m, n), (t, n), and (s, n) we
have problem kernels of linear ((m, n) and (t, n)) or polynomial ((s, n)) size, because these
parameters naturally bound the input size. The question whether there are polynomial kernels remains open for the parameters (m, k 0 ) and (t, k 0 ), which are equivalent with respect
to this question.
To prove our conditional non-existence result for Lobbying parameterized by (m, k), we
employ an incompressibility result for Set Cover due to the work of Dom, Lokshtanov,
and Saurabh (2009).
Theorem 4. Unless NP  coNP/poly, Lobbying does not admit a polynomial-size problem kernel with respect to the combined parameter (m, k), that is, the number of columns
combined with the number of rows to modify .
Proof. Dom et al. (2009) showed that, unless NP  coNP/poly, Set Cover (SC) does not
admit a polynomial kernel with respect to the combined parameter (|U|, h). To show that
this result transfers to Lobbying with respect to (m, k) we describe a polynomial time and
parameter transformation from SC with respect to (|U|, h).
Let (S, U, h) be an SC-instance. Recall that adding multiple copies of each set to S
does not change the answer to the instance, we can assume without loss of generality that
each element occurs in at least h and in at most |S|  h sets. First, compute the SC-matrix
M (S, U), which we refer to as original rows and original columns in the following, and
423

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

invert 0s and 1s. Second, add |S|  2h + 1 dummy rows containing only 0s. In every original
column j, invert |S|  |{Si : uj  Si }|  h + 1 of the 0s from arbitrary dummy rows to 1s.
Add one dummy column containing 1s in the dummy rows, and 0s elsewhere. Finally, let
k := h.
Note that the number of 0s in the dummy column is |S|, the number of 0s in any other
column is |S|  h + 1, and the total number of rows is 2|S|  2h + 1. Thus, the gap of each
original column is exactly one and the gap in the dummy column is h.
We show that (S, U, h) is a yes-instance of Set Cover if and only if the constructed
matrix can be modified to have more 1s than 0s in every column by changing at most k = h
rows to all-1-rows.
For the only if part, assume that (S, U, h) is a yes-instance. Let S 0  S be a size-h
set cover. Modifying all (original) rows corresponding to the sets of S 0 results in a matrix
where each column has more 1s than 0s: Since each of the h modified original rows has
a 0 in the dummy column, the dummy column gains h additional 1s. Furthermore, each
original row gets at least one additional 1, because S 0 is a set cover.
For the if part, suppose that the constructed instance is a yes-instance. This means
that modifying exactly k = h rows results in a matrix where each column has more 1s
than 0s since the maximum gap value is h. Since the gap value in the dummy column is h
and no dummy row contains a 0 in the dummy column, a solution cannot modify any dummy
row. Since the gap value of each original column is exactly one, the sets corresponding to
the modified rows of any Lobbying solution must form a set cover.
This reduction is a polynomial time and parameter transformation from SC parameterized by |U| and h to Lobbying parameterized by m and k: The matrix in the resulting
Lobbying instance has |U| + 1 columns and we ask to modify at most k = h rows. Since
Lobbying is contained in NP and since SC is NP-hard, a polynomial kernel for Lobbying
with respect to (m, k) would imply a polynomial kernel for SC with respect to (|U|, h) which
is not possible unless NP  coNP/poly (Dom et al., 2009).
While Lobbying is trivially fixed-parameter tractable with respect to the number n of
rows, it is unlikely to have a polynomial-size problem kernel under this parameterization.
Theorem 5. Unless NP  coNP/poly, Lobbying does not admit a polynomial-size problem
kernel with respect to the number n of rows (voters).
Proof. Observe that in the polynomial time and parameter transformation in the proof of
Theorem 4 the number of rows in the constructed input matrix for Lobbying is at most
twice the number of sets of the SC instance. Hence, if SC does not admit a polynomial
kernel with respect to the number of sets (unless NP  coNP/poly), then Lobbying also
does not admit a polynomial kernel with respect to n (unless NP  coNP/poly). Next, we
show that, indeed, SC does not admit a polynomial kernel with respect to the number of
sets.
SC is strongly related to the Hitting Set (HS) problem, in which, given a family of
sets SH over a universe UH and an integer h  1, one is asked to choose a set U 0  UH of
size at most h such that each set in SH has a non-empty intersection with U 0 . Dom et al.
(2009) showed that, unless NP  coNP/poly, HS does not admit a polynomial kernel with
respect to |UH |.
424

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

There is a simple polynomial time and parameter transformation from HS parameterized
by |UH | to SC parameterized by |S|: For each set Si  SH in the HS instance add an
element si to the universe set U of the SC instance. Finally, for each element e  UH of the
HS instance add a set to the set family S of the SC instance that contains the element si
for each Si  SH with e  Si . It is easy to verify that (SH , UH , h) is a yes-instance of HS if
and only if (S, U, h) is a yes-instance of SC.

5. Tractable Cases
In this section, contrasting the intractability results of the previous sections, we demonstrate that Lobbying is efficiently solvable in practically relevant cases. To this end, we
study numerous quantities (that is, parameters) and how they influence the computational
complexity of Lobbying. For instance, in Section 3 we have seen that Lobbying remains
NP-hard for input matrices with at most three 1s per row (Theorem 1)here we show that
it becomes polynomial-time solvable for matrices with at most two 1s per row. Moreover, we
show fixed-parameter tractability of Lobbying with respect to the number m of columns
(issues), for up to four columns even being linear-time solvable. Finally, we shed light on
several further structural restrictions on the input matrix that can make solving Lobbying
feasible.
Note that instances with only few rows (voters) are tractable, because already the very
naive brute-force approach which simply tries to modify all size-k subsets of rows leads to
an algorithm with FPT running time with respect to the parameter number n of rows.
Proposition 2. Lobbying is solvable in O(2n  m) time.
5.1 At Most Two Ones per Row
The following result complements Theorem 1, altogether yielding a complexity dichotomy
between containment in P and NP-completeness with respect to the maximum number s of
1s per row.
Theorem 6. Lobbying restricted to input matrices with at most two 1s per row, that is,
s  2, is solvable in O(nm log m) time.
Proof. The idea for the algorithm is to use the special structure of rows that are not modified
and the fact that each row has at most two 1s. If we may modify k  d(n + 1)/2e rows then
we can trivially answer yes; otherwise each column requires b := d(n + 1)/2e  k additional
1s outside the k modified rows in order to reach d(n + 1)/2e 1s. Our algorithm will seek
a selection of rows such that (1) each column has b 1s inside the selected rows and (2)
there are at least k unselected rows left out of which any k can be modified. Modifying any
k unselected rows then gives a total of at least b + k = d(n + 1)/2e 1s in each column. Note
that any solution with k modified rows requires that each column initially contains at least
b 1s; otherwise we can safely answer no.
We will see that the problem of finding the desired selection of rows with b 1s per column
comes down to a certain matching problem in an auxiliary graph G, which we will construct
next. The graph G gets one vertex for each column, and we add to G an edge for each row
which contains 1s in the corresponding two columns (a vertex pair may be connected by
425

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

multiple edges). A b-matching in our graph is a subset of the edges such that each vertex is
endpoint of at most b edges in that subset. Our final algorithm will essentially come down
to the computation of a maximum cardinality b-matching in G. For clarity, however, let us
first show how b-matchings in G and solutions to our lobbying question interact.
First, assume that there exists a solution for the Lobbying instance. Let R be the set
of all rows and let R be a choice of k rows such that modifying these gives each column at
least d(n+1)/2e 1s. (Should R be smaller than k then further rows could be added without
harm.) It follows that all other rows, say, R0 = R\R , contribute at least b = d(n+1)/2ek
1s in each column. From R0 we will get a large b-matching in G as follows: For each column
we arbitrarily mark b rows that have a 1 in this column (so each row is marked at most
twice since s  2). Let M  R0 denote the set of marked rows, and let M2  M denote
the set of rows that are marked twice. We observe that the size of M is b  m  |M2 | since
we marked b  m times and used two marks for each row of M2 . Considering the edges of G
which correspond to the elements in M2 it is easy to see that each vertex is incident to at
most b of them (we only marked b times per vertex, and only the twice marked rows are
considered). Thus the edges corresponding to M2 constitute a b-matching in G.
Second, consider the set M2 of rows that correspond to the edges of any maximum
b-matching of G, that is, each vertex is incident to at most b edges; clearly |M2 |  |M2 |.
Now greedily add to M2 rows such that each column has exactly b 1s in the created set of
rows. (This is possible since each column contains at least b 1s; otherwise we answered no
at the beginning.) We let M  denote the obtained set of rows; i.e., those from M2 and the
added ones. We observe that |M  | = b  m  |M2 |, since we add b  m  2|M2 | further rows
to M2 : We need b  m 1s total, M2 contributes 2|M2 | 1s, and the greedily added rows give
one 1 each. It follows that
|M  | = b  m  |M2 |  b  m  |M2 | = |M |  |R0 | = |R|  k.
Thus, there are at least k rows left outside of M  which can be modified to all-1 rows.
Together with the rows of M  this gives the required number d(n + 1)/2e = k + b of 1s for
each column.
Thus, any solution gives rise to a b-matching and any maximum b-matching leads to
a solution (if one exists). Our algorithm therefore begins by computing a maximum bmatching for G. Then, as in the previous paragraph we take the corresponding rows and
greedily extend the set to get b 1s in each column. Finally, if this leaves at least k rows
unused then modifying any k of them must give a solution (as we just argued). Crucially,
if there is a solution, then we showed that this gives a sufficiently large b-matching in G.
The computation of a maximum b-matching is the most costly part; according to Gabow
(1983), a maximum b-matching can be computed in O(nm log m) time (see also Schrijver
(2003, Chapter 31) for a more general notion of b-matchings).
5.2 Very Few and Few Columns
As already mentioned in the introductory section, Christian et al. (2007) pointed out that
the number m of issues in multiple referenda rarely exceeds the value 20. This makes m a
well-motivated, practically relevant parameter and leads to the question how m influences
the computational complexity of Lobbying. In this subsection, we demonstrate how to
426

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

solve Lobbying efficiently when m  4 (by a matching-based and by a greedy algorithm).
Moreover, we formulate Lobbying as an integer linear program with a number of variables
bounded by a function in m and deduce fixed-parameter tractability from this. These
findings are complemented by a no-polynomial-kernel result from Section 4 (Theorem 4)
and experimental evaluations in Section 6. We start with a matching-based linear-time
algorithm for Lobbying.
Theorem 7. For input matrices with at most four columns Lobbying is solvable in linear
time.
Proof. We modify the matching-based algorithm from Theorem 6 to cover all cases with
m  4. If m = 1 or m = 2, then no modification is needed. For the cases m = 3 and m = 4
observe first that the matching-based algorithm can be asked to modify k rows such that
there will be bi 1s in column i for i  {1, . . . , m}.
Now, if m = 3 and the input matrix contains c all-1 rows, then we can remove them and
execute the matching-based algorithm from Theorem 6 on the resulting matrix (where now
every row contains at most two 1s) and ask it to achieve d(n + 1)/2e  c 1s in each column.
For the case m = 4 we claim that there is an optimal solution in which at most one
row with three 1s is modified. To prove that, consider a solution modifying the minimum
number of such rows. Further assume that these rows are the last to be modified and
consider the situation just before any row with three 1s is modified. Now suppose without
loss of generality that row vector 0111 is modified. Then there is a positive gap in the first
column and whenever there is a row with 0 in this column, then it is a 0111 row. Hence, the
number of 0s in this column is at least d(n + 1)/2e as it is not yet satisfied. But the number
is at most d(n + 1)/2e, as otherwise we would have more than d(n + 1)/2e row vectors 0111
and the columns 2, 3, and 4 would be satisfied in the given matrixa contradiction. Thus,
the gap in the first column is 1 and it is enough to modify the row vector 0111 to satisfy
the first column. If there was another row with three 1s to be modified, say the row vector
1011, then this one together with all 0111 row vectors would make the gap values of the
third and the fourth columns negativea contradiction.
In summary, we can solve the case m = 4 by branching into at most five cases: One
0111 row is modified, one 1011 row is modified, one 1101 row is modified, one 1110 row is
modified, and no row with three 1s is modified. Then, we modify the appropriate row and
remove all rows containing at least three 1s. We count for each j the number cj of 1s in
column j of the removed rows. Finally, we ask the matching-based algorithm to modify at
most k  1 (or at most k, according to the chosen branch) of the remaining rows such that
column j contains d(n + 1)/2e  cj 1-entries. The running time follows from Theorem 6.
To attack matrices with more than four columns, we next develop a simple greedy
strategy for this case. Indeed, it provides optimal results for input matrices with at most
four columns. We further show that it is a logarithmic-factor approximation algorithm
for an unlimited number of columns. In Section 6, we demonstrate its excellent heuristic
properties.
Let A  {0, 1}nm be an input matrix. Recall that the gap gj of a column j  {1, . . . , m}
is the number of additional 1s this column needs to gain a majority of 1s. Let G := {gj |
j  {1, . . . , m}} be the set of different gap values, let hGi be the sequence of the elements
427

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

from G in decreasing order, and let hGij denote the jth element from hGi. Further, let
gain(R) be the gain vector of a row vector R with respect to matrix A, defined as
gain(R) := (z1 , z2 , . . . , z|G| ),
where zj denotes the number of zeros in row R in columns with gap value hGij . To compare
two gain vectors, we use the lexicographic order . A row R has the highest gain if R is
the greatest element with respect to , that is, for every row R0 in the matrix it holds that
R0  R.
We now present the algorithm MaxGapZeros which employs a greedy heuristic to decide
which rows to modify. The basic idea is to repeatedly choose a row vector with highest gain
in A and set it to all 1s until all columns are satisfied. Note that the gap value of each
column may change after one row is modified. Hence, a row vectors gain may also change.
Algorithm 1 MaxGapZeros (A  {0, 1}nm )
compute the gap values of all columns
while a column with positive gap value exists do
compute the gain vector of each row vector
modify an arbitrary row vector with highest gain
update the gap values for all columns
return modified rows

Theorem 8. Given an input matrix with n rows, m columns, and maximum gap g, MaxGapZeros finds a solution of size at most dlog me  g in O(m  n2 ) time; it even finds an
optimal solution for Lobbying in O(n2 ) time if m  4.
Proof. First, we show the running time bound. Computing the gap values of all columns
takes O(mn) time and MaxGapZeros performs at most d(n + 1)/2e iterations. In each
iteration computing the gain vectors and finding a row with maximum gain takes O(mn)
time, because we can sort the m columns according to their gap values (positive integers of
size at most n) in O(m+n) time using counting sort, and compute row-wise the gain vectors
while comparing with the current maximum gain vector in O(mn) time. Updating the gap
values takes O(m) time. Altogether, MaxGapZeros runs in O(mn + n(m + n + mn + m)) =
O(m  n2 ) time.
Second, we show the logarithmic approximation factor for m > 1. MaxGapZeros takes
in each iteration a row with a maximum number of 0s in columns with maximum gap value.
We show that, doing this, MaxGapZeros reduces the maximum gap value by one in at most
dlog me iterations.
Let mg denote the number of the columns with gap value g, where g is the maximum gap
value over all columns. If g > 1, then there is always a row with strictly more than (mg /2)
0s in columns with gap value g, because otherwise the matrix restricted to the columns with
gap value g would contain at most as many 0s as 1s. MaxGapZeros will select such row
because the gain vector of every row with less than (mg /2) 0s in columns with gap value g
is smaller. Hence, the maximum gap is reduced by one in at most dlog(mg )e  dlog me
iterations.
428

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

If g = 1, it is possible that there is no row with strictly more than (mg /2) 0s, because the
matrix restricted to columns with gap one can have as many 0s as 1s. However, in the ith
iteration MaxGapZeros satisfies at least mg  2i columns, since there is always a row that
contains at least as many 0s as 1s in columns with gap one. Hence, after dlog(mg )e iterations,
at most one column with gap one survives. Thus, by showing that actually MaxGapZeros
satisfies more than mg /2 columns in the first iteration or MaxGapZeros satisfies more than
mg /4 columns in the second iteration, we show that in total MaxGapZeros needs dlog(mg )e
iterations to satisfy all columns with gap one. Assume that MaxGapZeros satisfies exactly
mg /2 columns in the first iteration. Without loss of generality, let the row that was selected
by MaxGapZeros in the first iteration contain 1s in the first mg /2 columns with gap one.
Since these columns have gap one, there must be more 0s than 1s in the remaining rows of
the matrix restricted to these columns. Thus, there is a row with more than (mg /4) 0s in
columns with gap one that is selected by MaxGapZeros in the second iteration. Hence, all
columns with maximum gap one can be satisfied in dlog(mg )e  dlog me iterations.
Summarizing, MaxGapZeros terminates in at most dlog me  g iterations. Clearly, every
solution of minimum size must contain at least g rows.
Third, we show that MaxGapZeros finds an optimal solution when m  4. For input matrices with one or two columns, MaxGapZeros clearly finds a solution of minimum
size. In the remainder of the proof, we show that MaxGapZeros also finds a minimum-size
solution when the input matrix contains three or four columns. To this end, we analyze
the stepwise modification of the input matrix A by MaxGapZeros and compare with the
stepwise modification of the input matrix A following a minimum-size solution.
For some multiset of row vectors X, let A(X) denote the matrix resulting from the
modification of all row vectors from X in A, that is, replacing the row vectors from X with
the same number of all-1-rows. Furthermore, hXi denotes a sequence of the row vectors
from X and hXii denotes the ith element in this sequence.
We analyze the stepwise modification process as follows: Let hRMGZ i be the sequence of
the row vectors as modified by MaxGapZeros and let hROPT i be an arbitrary sequence of
the row vectors from some minimum size solution ROPT . Furthermore, let Ai be the matrix
before the ith row modification from hRMGZ i, that is, Ai := A({hRMGZ ii0 | i0 < i}) and
gj (Ai ) denotes the gap value of the jth column in matrix Ai . Now, for i = 1 to |RMGZ |,
we compare hRMGZ ii with hROPT ii . Note that hROPT ii will not run out of the sequence
since we will show that hRMGZ ii0 = hROPT ii0 , 1  i0 < i, and it is clear that if hROPT i and
hRMGZ i are identical in the first i  1 positions, then either both contain at least i elements
or none. When comparing hRMGZ ii with hROPT ii , whenever hRMGZ ii 6= hROPT ii we replace
or reorder elements in hROPT i such that afterwards hRMGZ ii0 = hROPT ii0 , 1  i0  i, and,
as an invariant, hROPT i still corresponds to a solution of minimum size. This implies that
Ai = A({hRMGZ ii | i0 < i}) = A({hROPT ii | i0 < i}).
In the following, we assume i  0 with hRMGZ ii0 = hROPT ii0 , i0 < i. For two row
vectors r1 , r2 from {0, 1}m we write r1  r2 if r1 [x] = 0  r2 [x] = 0, 1  x  m. It
is easy to see that @i00  i with hRMGZ ii  hROPT ii00  hRMGZ ii 6= hROPT ii00 , because
MaxGapZeros would have selected hROPT ii00 instead of hRMGZ ii already in iteration i. For
each 1  i  |hRMGZ i|, if hRMGZ ii 6= hROPT ii , then at least one of the following four
cases occurs. Note that the case that hRMGZ ii is an all-0s row is subsumed by Case 1.
429

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

Furthermore, when there are only three columns Case 2 is also subsumed by Case 1. Cases 2
4 implicitly assume that Case 1 does not apply.
Case 1. i0  i with hROPT ii0  hRMGZ ii . If i00 > i with hROPT ii00 = hRMGZ ii , then
swap the ith and i00 th element of hROPT i. Otherwise, replace the i0 th element of
hROPT i by hRMGZ ii and swap the ith and i0 th element of hROPT i if i0 6= i.
Case 2. hRMGZ ii contains exactly three 0s. Without loss of generality let hRMGZ ii
have 0s in the first three columns. This implies that the last columns gap value
can not be larger than each of the first three columns gap values, that is, gx (Ai ) 
g4 (Ai ), x  {1, 2, 3}. Since Case 1 does not apply, for every position i0  i it holds
that hROPT ii0 has a 0 in column 4, but at least one 1 in some other column since
otherwise our greedy algorithm would have selected this all-0-row hROPT ii0 at step i.
Thus, there are more than g4 (Ai ) such positions and we can safely replace the ith
element in hROPT i by hRMGZ ii .
Case 3. hRMGZ ii contains exactly two 0s. Without loss of generality, hRMGZ ii has 0s
in the first two columns. Due to the design of MaxGapZeros it holds that the first two
columns gap values are at least as large as the last two columns gap values, that is,
gx (Ai )  gy (Ai ), x  {1, 2}, y  {3, 4}. There is no position i0  i in hROPT i such that
hROPT ii0 also has 0s in the first two columns, because otherwise hRMGZ ii  hROPT ii0 ,
which is not possible since MaxGapZeros would have selected hROPT ii0 instead of
hRMGZ ii already in iteration i. Thus, there are at least g1 (Ai ) positions i1  i with
hROPT ii1 containing a 1 in the second column and at least g2 (Ai ) positions i2  i
with hROPT ii2 containing a 1 in the first column. Moreover, since Case 1 does not
apply and, hence, hROPT ii0 6 hRMGZ ii for every i0 > i, each of the corresponding row
vectors has at least two 0s. If g1 (Ai ) = g2 (Ai ), then by its approximation guarantee
MaxGapZeros also needs at most dlog 4e  g1 (Ai ) = g1 (Ai ) + g2 (Ai ) further rows.
Thus, we can safely replace the i0 th element of hROPT i by hRMGZ ii0 for each i0  i.
If g1 (Ai ) > g2 (Ai ), then there must be some position i0  i such that hROPT ii0
contains a 0 in column y  {3, 4}, but the column y is already satisfied in the matrix
A({hROPT ii00 | i00 < i0 }). If there is any position i  i with hROPT ii containing two
0s: one 0 in column y and one 0 in column 1 or 2, then replace the i th element of
hROPT i by hRMGZ ii and swap the ith and i th element of hROPT i if i 6= i. Otherwise,
for every position i  i it holds that if hROPT ii contains a 0 in column y then
hROPT ii contains exactly three 0: one 0 in column 3, one 0 in column 4, and one 0
in column 1 or 2. This implies that both column 3 and column 4 do not have positive
gap in the matrix A({hROPT ii00 | i00 < i0 }). Thus, replace the i0 the element of hROPT i
by hRMGZ ii and swap the ith and i0 th element of hROPT i if i0 6= i.
Case 4. hRMGZ ii contains exactly one 0. We show that in this case there are at most
two columns with positive gap values in Ai . Let j  be the only column where hRMGZ ii
has one 0. For each column j, let R0 (j) be the index set of rows containing a 0 in
column j. Consider any other column j 0 with positive gap value. Ai contains no rows
with 0s in both j  and j 0 , that is, R0 (j  )  R0 (j 0 ) = , because otherwise our greedy
algorithm would have selected such rows. Thus, the gap values of both j  and j 0 must
be one, implying that the maximum gap value is one. If the matrix contains at least
430

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

three columns with maximum gap one, then there must be a row containing 0s in at
least two of these columns which should have been selected by our greedy algorithm.
Thus, there are at most two columns with positive gap in Ai . Since MaxGapZeros is
optimal for at most two columns, we can safely replace the i0 th element of hROPT i by
hRMGZ ii0 for each i0  i.
Finally, we obtain a minimum-size solution ROPT with ROPT = RMGZ .
There are five-column input matrices where our algorithm may not provide an optimal
solution. For example, the optimal solution for the 5  6-matrix
0
0
0
0
1
1

0
0
1
1
0
1

0
1
0
1
0
1

1
0
1
1
0
0

1
1
0
1
0
0

contains two rows (row two and three) while our algorithm may output three rows as a
possible solution since the first three row vectors all have the highest gain (1, 2). Note that
the first column has gap 2 while all other columns have gap 1. If MaxGapZeros decides to
modify the first row before the other two rows, then it needs two more rows to satisfy all
columns.
Theorems 7 and 8 show that in case of at most four issues Lobbying can be solved very
efficiently. On the contrary, parameterized by the number m of columns, we only have a
theoretical fixed-parameter tractability result; it is based on a famous theorem in mathematical programming of Lenstra (1983) and further improved by Frank and Tardos (1987)
and Kannan (1987). Roughly speaking, it says that solving integer linear programs with a
number of variables depending solely on parameter p is fixed-parameter tractable with respect to p. However, the (worst-case) upper bound on the running time of the corresponding
algorithm is impractical and of classification nature only. Nevertheless, we experimented
with the practical usefulness of the subsequent ILP formulation (see Section 6).
Theorem 9. Lobbying is fixed-parameter tractable with respect to the parameter number m
of columns.
Proof. We describe an integer linear program (ILP) with at most 2m variables that solves
Lobbying.3 Then, Lobbying is fixed-parameter tractable with respect to m, because any
ILP with  variables and L input bits can be solved in O(2.5+o() L) time (Kannan, 1987;
Frank & Tardos, 1987).
There are at most 2m different rows in a binary matrix with m columns. Let r1 , . . . , rl
be an arbitrary ordering of all pairwise different rows in A, and for all 1  i  l let c(ri )
be the number of occurrences of ri . For 1  i  l and 1  j  m let Bj (ri ) = 1 if the
jth column of row ri has value 0 and, otherwise, Bj (ri ) = 0. For 1  i  l we introduce
3. Dorn and Schlotter (2012) already mentioned that their ILP for the Swap Bribery problem could be
adapted to Lobbying.

431

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

the integer variable bi , where 0  bi  c(ri ); the value of bi indicates how often one has to
modify a row of type ri . The ILP is formulated as follows. Recall that gj is the number of
missing 1s to make column j have more 1s than 0s and k is the number of rows to modify.
l
X

bi  k,

(1)

i=1

0  bi  c(ri )
gj 

l
X

bi  Bj (ri )

1  i  l,

(2)

1  j  m.

(3)

i=1

Constraint (1) ensures that at most k rows are modified. Constraint (2) ensures that
the amount of rows to be modified from each type ri is available in the input matrix.
Constraint (3) ensures that for each column j at least gj rows with a 0-entry in the jth
position are modified. Hence, the ILP provides a solution for Lobbying with at most k
modified rows.
Since m  2t, fixed-parameter tractability for m (Theorem 9) implies the following.
Corollary 2. Lobbying is fixed-parameter tractable with respect to the parameter maximum number t of zeros per row.
5.3 Dynamic Programming for Few Columns and Small Gap
If we use the reduction proof by Christian et al. (2007), then we immediately gain a W[2]hardness result with respect to the maximum gap value g for Lobbying. Furthermore, as
discussed in Section 3.2, Lobbying is LOGSNP-complete even if g is 1. So, under reasonable
complexity-theoretic assumptions, it is neither NP-hard nor in XP for constant g. In this
section, we prove tractability of Lobbying with respect to parameter g when combining it
either with the number m of columns (Theorem 10) or even with the maximum number s
of 1s per row (Corollary 3).
Theorem 10. Lobbying is solvable in O((g + 1)m  n2  m) time.
Proof. Let A  {0, 1}nm and k  N be a Lobbying input instance and let g1 , g2 , . . . , gm
be the corresponding gap values in A. We solve the problem via Dynamic Programming
employing a boolean table T [i, l, g1 , . . . , gm ], where i  {0, . . . , n}, l  {0, . . . , k}, and
gj  {0, . . . , gj } for all j. An entry T [i, l, g1 , . . . , gm ] is set to True if it is possible to reduce
the gap of column j by at least gj by modifying exactly l rows in the first i rows of A.
Otherwise set the entry to False. Clearly, (A, k) is a yes-instance of Lobbying if and only
if T [n, k, g1 , . . . , gm ] = True.
To initialize the table, we set T [0, 0, g1 , . . . , gm ] to be True if gj = 0 for all j and
False otherwise. To compute an entry T [i, l, g1 , . . . , gm ] we check two cases: First, we set
T [i, l, g1 , . . . , gm ] to True if T [i  1, l, g1 , . . . , gm ] = True (treating the case where row i is not
0 ]=
contained in a solution). Second, we set T [i, l, g1 , . . . , gm ] to True if T [i1, l1, g10 , . . . , gm
True where gj0 = gj if row i has 1 in the column j and gj0 = max{0, gj  1} if row i has 0 in
the column j. If none of the two cases applies, then we set T [i, l, g1 , . . . , gm ] = False.
432

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

Table T has (g + 1)m  (k + 1)  (n + 1) entries and each table entry can be computed in
O(m) time, resulting in an overall running time of O((g + 1)m  k  n  m).
Finally, we harvest a further fixed-parameter tractability result for Lobbying by
simply relating parameter values.
Corollary 3. Lobbying is solvable in O((g + 1)4s s  n2 + 16g  m) time.
Proof. First, we provide some useful inequality between (functions of) parameter values.
Count the number of 1s in the input matrix. Since there are at most s of them in each row,
there are at most ns of them in the whole matrix. In addition, there are at least n/2  g of
them in each column and, hence, the total number of 1s is at least (n/2  g)m. It follows
that (n/2  g)m  ns.
Now, we employ the just derived inequality to deduce fixed-parameter tractability for
the combined parameter (g, s). If g  n/4, then nm/4  (n/2  g)m  ns, hence m  4s
and we can use the O((g + 1)m m  n2 ) = O((g + 1)4s s  n2 ) time algorithm from Theorem 10.
Otherwise n < 4g and we can solve the problem by brute force, testing all possible subsets
of rows to be modified in O(2n  m) = O(24g  m) time.
5.4 Close to Guarantee and Small Gap
Under some reasonable complexity-theoretic assumptions Lobbying is neither in XP for the
below-guarantee parameter k 0 :=d(n+1)/2ek (see Theorem 1) nor in XP for the maximum
gap value g over all columns (see Theorem 3). However, using some relations between the
parameters and the brute-force algorithm from Proposition 2 we can show that Lobbying
is in XP for the combined parameter (g, k 0 ). More precisely, we show that Lobbying is
fixed-parameter tractable with respect to k 0 if g is a constant.
0

Theorem 11. Lobbying is solvable in O(m2g+1  22k ) time.
Proof. From the definition of k 0 it follows that k + k 0  n/2. Furthermore, from the
logarithmic factor approximation (see Theorem 8) it follows that k  g  log m. Combining
this gives n  2(g  log m + k 0 ). Thus, using the brute-force algorithm from Proposition 2,
0
0
Lobbying is solvable in O(2n  m) = O(22glog m  22k  m) = O(m2g+1  22k ) time.
It remains open whether Lobbying is fixed-parameter tractable for the combined parameter (g, k 0 ) (not assuming that g is a constant).

6. Experimental Evaluation of the Greedy and ILP Algorithms
Recall that we consider the number m of columns to be the parameter with currently
strongest support in terms of doing a parameterized complexity analysis. This is because
m seldom exceeds 20 (Christian et al., 2007). Hence, in this section we present the experimental results we obtained from testing our greedy heuristic (MaxGapZeros) and our ILP
formulation (ILP) from Section 5.2 as well as the slightly simpler greedy algorithm (MaxZeros) by Erdelyi et al. (2007). Roughly speaking, whereas MaxZeros simply picks a row with a
maximum number of 0s, MaxGapZeros uses a gain measure where 0s in columns with higher
433

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

gap value are given higher attention. Thus, by comparing MaxGapZeros and MaxZeros we
evaluated the effect of our refined gain measure.
We evaluated both efficiency and solution quality of the greedy algorithms, comparing
with exact solutions delivered by the ILP. Note that the heuristic by Erdelyi et al. (2007)
was designed to solve the more general weighted variant of Lobbying, and hence, we do
not expect MaxZeros to outperform MaxGapZeros in terms of solution quality. However,
we are not aware of any other algorithm solving Lobbying to compare with.
We used Gurobi 5.0.1 as our (integer) linear program solver to handle the ILP formulation of Lobbying given in the proof of Theorem 9. Recall that the number of variables
in the ILP formulation depends only on the number of different rows and thus can be
upper-bounded by 2m .
We tested the algorithms on a large set ( 3.3  105 matrices) of random instances and
instances generated from the actual voting records of the German parliament. We used
two types of random models where each was controlled by several density parameters,
determining the fraction of the number of 1s to the number of 0s. Our row-oriented
model randomly chooses a density for each row and then sets each row entry at random
such that it equals 1 with a probability equal to the chosen density value. In a similar
way, our column-oriented approach randomly chooses a density for each column and then
generates the entries in this column at random using this density value as the probability
for generating a 1. Section 6.1 contains a more detailed description of our random models.
In Section 6.3 we present our experimental findings.
All our experiments were performed on an Intel Xeon E5-1620 3.6GHz machine with
64GB of memory running the Debian GNU/Linux 6.0 operating system. Both greedy
heuristics are implemented in C++. The ILP implementation uses Gurobi4 5.0.1 through
its Java API. The source code including the data generator is freely available.5
6.1 Random Instance Generation
In both models, the row-oriented and the column-oriented, the process of creating a random
instance is controlled by two density parameters a and b with 0  a  b  1. Subsequently,
when describing our two models in detail, randomly choosing a number always refers to
a random number generated using an i.i.d. process and a uniform distribution.
6.1.1 Row-Oriented Model
For certain values of n, m, a, and b with a  b, the row-oriented model creates a binary
matrix A  {0, 1}nm as follows. For each row i, it chooses a random number di from the
interval [a, b]. Then, for all 1  j  m, entry Ai,j is set to 1 with probability di . Using
the row-oriented model, we created instances for all combinations of a  {0.1, 0.2, 0.3, 0.4}
and b  {0.5, 0.6, 0.65, 0.7, 0.8} where a + b < 1. Since the expected fraction of 1s in row i
is di and the expected fraction of 1s in A is (a + b)/2, implying that in case of a + b  1
a high fraction of columns have gaps at most zero, we required that a + b < 1. Thus, for
each combination of n and m, we created 13 instances according to this model. Herein,
we started with m = 10 and increased m by 10% in each step, rounding up to the nearest
4. http://www.gurobi.com/
5. http://akt.tu-berlin.de/menue/software/

434

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

integer if necessary (formally, m  dm  1.1e), until m was larger than 300. We assigned n
110 equidistant values within [10, 991] (formally, n  n + 9). Summarizing, we had 33
different values for m and 110 different values for n, obtaining a total of 47 190 instances in
the row-oriented model.
6.1.2 Column-Oriented Model
Here, to create a matrix A  {0, 1}nm for fixed a and b, for each column i the columnoriented model chooses a random number di from [a, b]. Then, denoting the maximum possible gap d(n+1)/2e by gmax , the model assigns 1s to (1di )gmax many entries in column i,
and 0 to the remaining entries. Observe that this ensures that the gap of column i is di gmax
and thus the expected gap of a column is ((a+b)/2)gmax . We created instances for all combinations of a  {0, 0.1, 0.2, 0.3, 0.4, 0.5} and b  {0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}
where a  b. Thus, for each combination of n and m we created 51 instances. Thereby,
we started with m = 10 and increased m by 10% in each step , once again rounding up
to the nearest integer if necessary (formally, m  dm  1.1e) as long as it was smaller than
100. We assigned n 110 equidistant values within [10, 991] (formally, n  n + 9). Thus we
had 21 different values for m and 110 for n, obtaining a total of 142 810 instances from the
column-oriented model.
We discarded instances containing a column with gap at most zero. Furthermore, we do
not discuss instances with less than 10 columns because all tested algorithms were extremely
fast (less than 0.01 seconds on average) and our greedy heuristic computed optimal solutions
for more than 99% of these instances.
6.2 Real-World Data Generation
To obtain a data set representing realistic binary preferences of politicians we use the welldocumented voting records of the German parliament, the Bundestag. Votes on issues in
the Bundestag can be anonymous or recorded votes. Whenever a party from the parliament
or 5% of the members request it, each voters decision is recorded together with the voters
name. We generated our instances from the recorded votes in 2012 and 2013 which are
freely available from www.bundestag.de:
 The issue of each recorded vote became an issue in our model. When there were
several very similar issues (for example similar amendments on the same topic) we
only took the last one.
 Each member of the Bundestag became a voter in our model. When a Bundestag
member did not show up or abstained from voting we assume that his or her opinion
was consistent with the majority of his or her party. When a Bundestag member left
the parliament during the period, we did not collect his or her votes. (This happened
twice.)
This resulted in a matrix with 67 columns and 620 rows. Approximately half of the columns
have a small gap value (less than 25) and the other half has gap values greater than 90.
Roughly one third of the columns have gap values greater than 150. Even the maximum
possible gap value of 311 occurs twice.
435

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

6.2.1 The Goal of the Lobby
Since lobby actions are usually not very well-documented and one cannot (easily) identify
voters that have been bribed, we decided to guess the goal of the lobby. Recall that in our
model, a 1 (resp. a 0) in column j of row i of the input matrix means that voter i agrees
(resp. disagrees) with the lobby with respect to issue j. Hence, to get input matrices for
our model, we need the set of issues for which the lobby disagrees with a majority of the
voters. Issues for which the goal of the lobby is consistent with the majority of voters can
always be ignored.
6.2.2 Test Series
We performed three experiments, one with randomly selected issues of small gap values
(g < 30), one with randomly selected issues of high gap values (g > 90), and one with
randomly selected issues without any restriction. For each number m of columns from
{5, 10, . . . , 30} we extracted 100 instances for each of the first two experiments. For each m
from {5, 10, . . . , 60} we generated 100 instances for the last experiment. This covers a wide
range of scenarios which differ in the number of issues the lobby wants to change as well as
in the amount of changes each issue needs to reach a majority of approvals.
6.3 Results
We evaluated the experimental results concerning time efficiency and, for the greedy heuristics, additionally with respect to solution quality (closeness to optimal solutions), that is,
the distance of the derived solutions from an optimal one.
To evaluate the efficiency for some fixed attribute values (e.g., the number of columns)
we computed the average running times over all corresponding instances. The running times
of the greedy algorithms were very small (below 0.1 seconds on average). To get reliable
values we ran both greedy heuristics 20 times for each instance and stored the average value.
The ILP algorithm solved more than 95% of the instances within five minutes. We counted
the running time for the remaining instances as five minutes to get a lower bound for the
correct average running times of the ILP algorithm.
To evaluate the optimality we computed the percentage of optimally solved instances as
well as the average difference between the optimal number of lobbied voters and the number
of voters lobbied by the heuristic solution. In the following, we denote this as the distance
from optimality.
6.3.1 Efficiency for the Row-Oriented Model
As expected, the heuristic algorithms were much faster than the ILP algorithm. Whereas
both heuristics needed less than 0.1 seconds on instances with more than 300 columns or
up to 1000 rows, the ILP needed more than ten seconds on average for instances with at
least 150 columns or with at least 500 rows. See Figure 3 for details. Somewhat surprisingly, MaxGapZeros turned out to be faster than MaxZeros for instances with more than
24 columns as well as for instances with at least 100 rows. The reason seems to be that
MaxGapZeros produces smaller solutions than MaxZeros allowing for earlier termination
(see Figure 4).
436

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

101

running time in seconds

101

100
100
101

101
102

102

103

0

50

100

150

200

250

0

300

100 200 300 400 500 600 700 800 900 1,000

number of rows

number of columns
MaxGapZeros

MaxZeros

ILP

Figure 3: Row-oriented model: Running time depending on the number of columns and the
number of rows, respectively.

10
100
9

distance from optimality

%optimal solutions

90
80
70
60
50
40
30
20
10

8
7
6
5
4
3
2
1
0

0
0

50

100

150

200

250

300

0

50

100

150

200

250

300

number of columns

number of columns
MaxGapZeros

MaxZeros

Figure 4: Row-oriented model: Percentage of optimal solutions and average distance from
the optimal solution for both greedy algorithms, depending on the number of
columns.

437

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

running time in seconds

101
101
100

100

101

102

101

103

102

10

20

30

40

50

60

70

80

90

100

0

100 200 300 400 500 600 700 800 900 1,000

number of columns
MaxGapZeros

number of rows
MaxZeros

ILP

Figure 5: Column-oriented model: Running time depending on the number of columns and
the number of rows, respectively.

6.3.2 Optimality for the Row-Oriented Model
Our greedy algorithm (MaxGapZeros) performed very well for this data set in terms of
solution size. More than 50% of the instances were optimally solved, even those with more
than 300 columns. In contrast, the simpler greedy algorithm (MaxZeros) could only solve
very few instances optimally. As for the distance to optimality, MaxGapZeros results have
an average distance of less than one whereas the average distance to optimality of the
MaxZeros results exhibits a logarithmic growth with respect to the number of issues and it
is always greater than two. See Figure 4 for details.
6.3.3 Efficiency for the Column-Oriented Model
Similarly to the row-oriented model, the heuristic algorithms were extremely fast for all
tested instances. For all instances (which all have less than 100 columns), MaxZeros was
slightly faster than MaxGapZeros but the difference between the average running time of
MaxGapZeros and MaxZeros decreased with an increasing number of issues. See Figure 5
for details.
6.3.4 Optimality for the Column-Oriented Model
Similarly to the row-oriented model, MaxGapZeros computed solutions which are relatively
close to the optimum. Whereas the percentage of instances having between 30 and 100
columns which were optimally solved by MaxGapZeros is slightly lower than for the roworiented model, the percentage of instances which were optimally computed by MaxZeros is
twice as high as for the row-oriented model. In contrast, as for the distance to optimality,
438

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

10
100
9

distance from optimality

%optimal solutions

90
80
70
60
50
40
30
20
10

8
7
6
5
4
3
2
1
0

0
10

20

30

40

50

60

70

80

90

100

10

number of columns

20

30

40

50

60

70

80

90

100

number of columns

MaxGapZeros

MaxZeros

Figure 6: Column-oriented model: Percentage of optimal solutions and average distance
from the optimal solution for both greedy algorithms depending on the number
of columns.

the results for the column-oriented model behaved similarly to the results for the roworiented model. Again, the average distance to optimality of the MaxZeros results exhibits
a logarithmic growth with respect to the number of issues and is always greater than three,
while the average distance to optimality of the MaxGapZeros results is lower than one for
all tested instances. See Figure 6 for details.
6.3.5 Efficiency for the Real-World Data Set
Surprisingly, all algorithms including the ILP algorithm could solve every single instance
extremely fast. MaxZeros was slightly faster than MaxGapZeros and even the ILP algorithm
needed less than 0.07 seconds for each instance. A reason seems to be that politicians of
the same party often vote similarly so that there are few different rows in the matrices and,
hence, the ILP has much less variables to handle than in the worst case (only up to 103
instead of 620). As expected, instances with small maximum gap values could be computed
(slightly) faster.
6.3.6 Optimality for the Real-World Data Set
Somewhat unexpectedly, MaxGapZeros could solve all instances optimally. Recall that this
was not the case for random data instances of similar sizes. In contrast to MaxGapZeros,
for most instances MaxZeros could not find an optimal solution. Especially for the instances
with only high gap values and for the instances without restrictions on the gap values, the
distance of MaxZeross solution size to the optimal solution size is quite large. See Figure 7
and Figure 8 for details. Interestingly, the optimal solution size equals the maximum gap
439

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

8
100

distance from optimality

7

%optimal solutions

90

6

80

5

70

4

60

3

50
40

2

30

1

20
0
10
5

10

15

20

25

30

5

10

number of columns

15

20

25

30

number of columns

MaxGapZeros

MaxZeros

Figure 7: Real-world data, instances with low gap values only: Percentage of optimal solutions and average distance from the optimal solution for both greedy algorithms
depending on the number of columns.

100
100

distance from optimality

90

%optimal solutions

90
80
70
60
50
40
30
20
10

80
70
60
50
40
30
20
10
0

0
0

10

20

30

40

50

0

60

10

20

30

40

50

60

number of columns

number of columns
MaxGapZeros

MaxZeros

Figure 8: Real-world data, instances with low and high gap values: Percentage of optimal
solutions and average distance from the optimal solution for both greedy algorithms depending on the number of columns. The results for instances with high
gap values only are very similar.

440

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

value for almost all instances. Note that this cannot be the only explanation for tractability
since such instances are in general still NP-hard to compute; see the reduction used in the
proof of Theorem 1.
6.3.7 Conclusion
We showed that both heuristics are very efficient on synthetic random data of reasonable
size, that is, for up to 300 columns and up to 1000 rows. Whereas the greedy heuristic by
Erdelyi et al. (2007) computes solutions that are relatively far away from the optimum, our
greedy algorithm computed optimal solutions for most instances. However, note that our
greedy algorithm is directly designed to solve Lobbying whereas the algorithm by Erdelyi
et al. (2007) is designed for the more general weighted variant of Lobbying. Somewhat
unexpectedly, also the exact ILP algorithm solved most of our instances within five minutes.
Even all instances with up to 37 columns and up to 100 rows were optimally solved by the
ILP within five minutes.
On the data set based on real-world data, the algorithms behaved very similar to the
synthetic case. Our greedy algorithm could solve all instances optimally and the ILP algorithm could solve all instances very fast.

7. Conclusion
Lobbying as studied here is a fundamental matrix modification problem with a rich combinatorial structure.6 We started exploiting this structure in terms of a number of natural
parameterizations and a corresponding parameterized and multivariate complexity analysis.
Table 1 in Section 1 summarizes our results. Indeed, the space of investigations could be
extended by introducing further parameters and also looking at further parameter combinations, all the time with the ultimate goal to obtain (improved) fixed-parameter tractability
results (Komusiewicz & Niedermeier, 2012). So far, our results indicate that making use
of the parameter number m of columns is particularly promising albeit only a theoretical
fixed-parameter tractability result based on integer linear programming could be proven.
We found a very well performing greedy algorithm that turns out to deliver provably optimal results for input matrices with up to four columns. Based on our positive experimental
results, we suggest this algorithm to be the current method of choice in solving also larger
Lobbying instances getting close to optimal solutions.
On the methodological side, our probably most innovative contribution is to show
LOGSNP-completeness (Papadimitriou & Yannakakis, 1996) in case of input matrices which
only have gap value 1 for all columns, that is, a situation where the lobby may hope to
achieve the goal at low cost (meaning few modified rows). This result is of particular interest
since only few natural LOGSNP-complete problems are known and since it seems to be the
first use of this tool in assessing the parameterized complexity of parameterized problems
between XP and the class of problems that are NP-hard even for constant parameter values.
This can be of independent future interest when studying the parameterized complexity of
other problems.
6. Very recently, Lobbying was very useful in assessing the computational complexity of a matrix modification problem arising in machine learning (Froese, van Bevern, Niedermeier, & Sorge, 2013).

441

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

In future work, our findings for the plain Lobbying problem may be extended to natural
variants and generalizations. These include
 to allow the lobby to only partially influence the voters (that is, the rows may not be
changed to all-1 rows),
 to only head for getting a majority of approvals for a certain (pre-specified) percentage
of issues (columns),
 to consider the modification operation of adding voters (rows), and
 to consider the modification operation of deleting voters (rows).
Moreover, it is of interest to extend the multivariate studies to the scenarios of probabilistic
and weighted lobbying as have been studied in previous work (Binkele-Raible et al., 2014;
Erdelyi et al., 2007).
We conclude with a few concrete open questions for future research on Lobbying.
 Can one replace the integer linear program (see Theorem 9 in Section 5.2) by a direct
(more efficient) combinatorial algorithm in the fixed-parameter tractability result for
Lobbying parameterized by the number m of columns?
 Our fixed-parameter tractability results for the parameter combinations (m, k), (m, g),
(m, s), (k, s), and (g, s) are of theoretical nature onlycan they be made practical?
 We showed that Lobbying is fixed-parameter tractable with respect to k 0 when g is
a constant. Is it also fixed-parameter tractable for the combined parameter (g, k 0 )?
 For the combined parameters (m, n), (t, n), and (s, n) there are trivial polynomial-size
problem kernels for Lobbying because there are polynomials only depending on the
respective parameters which upper-bound the input size. We showed that, except for
(m, k 0 ) which remains open, for all of our other fixed-parameter tractability results
there are no polynomial-size problem kernels, unless NP  coNP/poly. Is there any
possibility of (provable) efficient and effective preprocessing and data reduction for
Lobbying?
 All our results adhere to a worst-case analysiswhat is the complexity of Lobbying
on average? Our findings with the greedy heuristic suggest that there is reason to
believe that Lobbying is computationally not so hard as worst-case analysis suggests. The situation is similar in the recent studies concerning the seemingly pathological NP-hardness of manipulation and its very good solvability in experimental
results (Betzler, Niedermeier, & Woeginger, 2011; Davies, Katsirelos, Narodytska, &
Walsh, 2011; Davies, Narodytska, & Walsh, 2012; Walsh, 2011). A more thorough
investigation in this direction concerning Lobbying seems promising.

442

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

Acknowledgments
An extended abstract of this work (without coauthor G.J. Woeginger) appeared in the
Proceedings of the 26th Conference on Artificial Intelligence (AAAI 12) (Bredereck, Chen,
Hartung, Kratsch, Niedermeier, & Suchy, 2012). In this long version, now exclusively
focusing on the plain Lobbying problem, we provide numerous details that have been
omitted in the extended abstract. Moreover, we have the following new contributions: We
prove LOGSNP-completeness for the case of gap-1 instances. We show that our greedy
algorithm as already presented in the extended abstract has a logarithmic approximation
ratio. Finally, we present experimental results with greedy algorithms and an integer linear
program formulation for Lobbying.
Robert Bredereck is supported by the German Research Foundation (DFG), research
project PAWS (NI 369/10). Jiehua Chen is supported by the Studienstiftung des Deutschen
Volkes. Main work was done while Stefan Kratsch was with Utrecht University supported by the Netherlands Organization for Scientific Research (NWO), project KERNELS
(OND1336203), and visiting TU Berlin, Germany. Ondrej Suchy is supported by the DFG
Cluster of Excellence on Multimodal Computing and Interaction (MMCI) and the DFG
project DARE (GU 1023/1-2). Main work was done while he was with the Universitat des
Saarlandes, Saarbrucken and visiting TU Berlin, Germany. Gerhard J. Woeginger is supported by DIAMANT (a mathematics cluster of the Netherlands Organization for Scientific
Research NWO). Main work was done while he was staying as a recipient of a Humboldt
Research Award at TU Berlin.
We are grateful to the anonymous referees of JAIR for providing numerous insightful
remarks that helped to significantly improve the paper. We thank Kolja Stahl for his great
support in extracting and converting the real-world data for our experiments.

References
Bartholdi III, J. J., Tovey, C. A., & Trick, M. A. (1992). How hard is it to control an
election?. Mathematical and Computer Modeling, 16 (8-9), 2740.
Baumeister, D., Erdelyi, G., & Rothe, J. (2011). How hard is it to bribe the judges? A
study of the complexity of bribery in judgment aggregation. In Proceedings of the
2nd International Conference on Algorithmic Decision Theory, Vol. 6992 of LNCS,
pp. 115. Springer.
Betzler, N., Bredereck, R., Chen, J., & Niedermeier, R. (2012). Studies in computational
aspects of votinga parameterized complexity perspective. In The Multivariate Algorithmic Revolution and Beyond, Vol. 7370 of LNCS, pp. 318363. Springer.
Betzler, N., Niedermeier, R., & Woeginger, G. J. (2011). Unweighted coalitional manipulation under the Borda rule is NP-hard. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence, pp. 5560. AAAI Press.
Binkele-Raible, D., Erdelyi, G., Fernau, H., Goldsmith, J., Mattei, N., & Rothe, J. (2014).
The complexity of probabilistic lobbying. Discrete Optimization, 11, 121.
443

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

Bodlaender, H. L. (2009). Kernelization: New upper and lower bound techniques. In Proceedings of the 4th International Workshop on Parameterized and Exact Computation,
Vol. 5917 of LNCS, pp. 1737. Springer.
Bodlaender, H. L., Downey, R. G., Fellows, M. R., & Hermelin, D. (2009). On problems
without polynomial kernels. Journal of Computer System Sciences, 75 (8), 423434.
Bodlaender, H. L., Thomasse, S., & Yeo, A. (2011). Kernel bounds for disjoint cycles and
disjoint paths. Theoretical Computer Science, 412 (35), 45704578.
Bredereck, R., Chen, J., Hartung, S., Kratsch, S., Niedermeier, R., & Suchy, O. (2012). A
multivariate complexity analysis of lobbying in multiple referenda. In Proceedings of
the 26th Conference on Artificial Intelligence, pp. 12921298. AAAI Press.
Cai, L., Chen, J., Downey, R. G., & Fellows, M. R. (1997). Advice classes of parameterized
tractability. Annals of Pure and Applied Logic, 84 (1), 119138.
Christian, R., Fellows, M., Rosamond, F., & Slinko, A. (2007). On complexity of lobbying
in multiple referenda. Review of Economic Design, 11 (3), 217224.
Davies, J., Katsirelos, G., Narodytska, N., & Walsh, T. (2011). Complexity of and algorithms
for Borda manipulation. In Proceedings of the 25th AAAI Conference on Artificial
Intelligence, pp. 657662. AAAI Press.
Davies, J., Narodytska, N., & Walsh, T. (2012). Eliminating the weakest link: Making
manipulation intractable?. In Proceedings of the 26th AAAI Conference on Artificial
Intelligence, pp. 13331339. AAAI Press.
Dom, M., Lokshtanov, D., & Saurabh, S. (2009). Incompressibility through colors and IDs.
In Proceedings of the 36th International Colloquium on Automata, Languages, and
Programming, Vol. 5555 of LNCS, pp. 378389. Springer.
Dorn, B., & Schlotter, I. (2012). Multivariate complexity analysis of swap bribery. Algorithmica, 64 (1), 126151.
Downey, R. G., & Fellows, M. R. (2013). Fundamentals of Parameterized Complexity. Texts
in Computer Science. Springer.
Elkind, E., Faliszewski, P., & Slinko, A. (2011). Cloning in elections: Finding the possible
winners. Journal of Artificial Intellligence Research, 42, 529573.
Elkind, E., Faliszewski, P., & Slinko, A. (2012). Clone structures in voters preferences.
In Proceedings of the 13th ACM Conference on Electronic Commerce, pp. 496513.
ACM.
Erdelyi, G., Hemaspaandra, L. A., Rothe, J., & Spakowski, H. (2007). On approximating
optimal weighted lobbying, and frequency of correctness versus average-case polynomial time. In Proceedings of the 16th International Symposium on Fundamentals of
Computation Theory, Vol. 4639 of LNCS, pp. 300311. Springer.
Erdelyi, G., Piras, L., & Rothe, J. (2011). The complexity of voter partition in Bucklin
and fallback voting: Solving three open problems. In Proceedings of the 10th International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 837844.
IFAAMAS.
444

fiA Multivariate Complexity Analysis of Lobbying in Multiple Referenda

Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. A. (2009). How hard is bribery in
elections?. Journal of Artificial Intelligence Research, 35, 485532.
Fellows, M. R., Jansen, B. M., & Rosamond, F. (2013). Towards fully multivariate algorithmics: Parameter ecology and the deconstruction of computational complexity.
European Journal of Combinatorics, 34 (3), 541566.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer.
Fortnow, L., & Santhanam, R. (2011). Infeasibility of instance compression and succinct
PCPs for NP. Journal of Computer System Sciences, 77 (1), 91106.
Frank, A., & Tardos, E. (1987). An application of simultaneous diophantine approximation
in combinatorial optimization. Combinatorica, 7 (1), 4965.
Froese, V., van Bevern, R., Niedermeier, R., & Sorge, M. (2013). A parameterized complexity analysis of combinatorial feature selection problems. In Proceedings of the
38th International Symposium on Mathematical Foundations of Computer Science,
Vol. 8087 of LNCS, pp. 445456. Springer.
Gabow, H. N. (1983). An efficient reduction technique for degree-constrained subgraph and
bidirected network flow problems. In Proceedings of the 15th Annual ACM Symposium
on Theory of Computing, pp. 448456. ACM.
Garey, M. R., & Johnson, D. S. (1979). Computers and IntractabilityA Guide to the
Theory of NP-Completeness. W. H. Freeman and Company.
Goldsmith, J., Levy, M. A., & Mundhenk, M. (1996). Limited nondeterminism. SIGACT
News, 27 (2), 2029.
Gottlob, G., Scarcello, F., & Sideri, M. (2002). Fixed-parameter complexity in AI and
nonmonotonic reasoning. Artificial Intelligence, 138 (1-2), 5586.
Gottlob, G., & Szeider, S. (2008). Fixed-parameter algorithms for artificial intelligence,
constraint satisfaction and database problems. The Computer Journal, 51 (3), 303
325.
Guo, J., & Niedermeier, R. (2007). Invitation to data reduction and problem kernelization.
SIGACT News, 38 (1), 3145.
Kannan, R. (1987). Minkowskis convex body theorem and integer programming. Mathematics of Operations Research, 12 (3), 415440.
Karp, R. M. (1972). Reducibility among combinatorial problems. In Complexity of Computer Computations, pp. 85103. Plenum Press.
Komusiewicz, C., & Niedermeier, R. (2012). New races in parameterized algorithmics. In
Proceedings of the 37th Mathematical Foundations of Computer Science, Vol. 7464 of
LNCS, pp. 1930. Springer.
Lenstra, H. W. (1983). Integer programming with a fixed number of variables. Mathematics
of Operations Research, 8 (4), 538548.
Mahajan, M., & Raman, V. (1999). Parameterizing above guaranteed values: MaxSat and
MaxCut. Journal of Algorithms, 31 (2), 335354.
Niedermeier, R. (2006). Invitation to Fixed-Parameter Algorithms. Oxford University Press.
445

fiBredereck, Chen, Hartung, Kratsch, Niedermeier, Suchy, & Woeginger

Niedermeier, R. (2010). Reflections on multivariate algorithmics and problem parameterization. In Proceedings of the 27th International Symposium on Theoretical Aspects
of Computer Science, Vol. 5 of Leibniz International Proceedings in Informatics, pp.
1732.
Papadimitriou, C. H., & Yannakakis, M. (1996). On limited nondeterminism and the complexity of the V-C dimension. Journal of Computer and System Sciences, 53 (2),
161170.
Sandholm, T., Suri, S., Gilpin, A., & Levine, D. (2002). Winner determination in combinatorial auction generalizations. In Proceedings of the First International Conference
on Autonomous Agents and Multiagent Systems, pp. 6976. ACM.
Schlotter, I., Elkind, E., & Faliszewski, P. (2011). Campaign management under approvaldriven voting rules. In Proceedings of the 25th AAAI Conference on Artificial Intelligence, pp. 726731. AAAI Press.
Schrijver, A. (2003).
Springer.

Combinatorial Optimization: Polyhedra and Efficiency, Vol. A.

Szeider, S. (2011). Limits of preprocessing. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence, pp. 9398. AAAI Press.
Walsh, T. (2011). Is computational complexity a barrier to manipulation?. Annals of
Mathematics and Artificial Intelligence, 62 (1-2), 726.
Yap, C.-K. (1983). Some consequences of non-uniform conditions on uniform classes. Theoretical Computer Science, 26 (3), 287300.

446

fiJournal of Artificial Intelligence Research 50 (2014) 573601

Submitted 12/13; published 07/14

False-Name Manipulation in Weighted Voting Games is Hard for
Probabilistic Polynomial Time
Anja Rey
Jrg Rothe

REY @ CS . UNI - DUESSELDORF. DE
ROTHE @ CS . UNI - DUESSELDORF. DE

Institut fr Informatik
Heinrich-Heine-Universitt Dsseldorf
40225 Dsseldorf
Germany

Abstract
False-name manipulation refers to the question of whether a player in a weighted voting game
can increase her power by splitting into several players and distributing her weight among these
false identities. Relatedly, the beneficial merging problem asks whether a coalition of players
can increase their power in a weighted voting game by merging their weights. For the problems
of whether merging or splitting players in weighted voting games is beneficial in terms of the
ShapleyShubik and the normalized Banzhaf index, merely NP-hardness lower bounds are known,
leaving the question about their exact complexity open. For the ShapleyShubik and the probabilistic Banzhaf index, we raise these lower bounds to hardness for PP, probabilistic polynomial
time, a class considered to be by far a larger class than NP. For both power indices, we provide
matching upper bounds for beneficial merging and, whenever the new players weights are given,
also for beneficial splitting, thus resolving previous conjectures in the affirmative. Relatedly, we
consider the beneficial annexation problem, asking whether a single player can increase her power
by taking over other players weights. It is known that annexation is never disadvantageous for
the ShapleyShubik index, and that beneficial annexation is NP-hard for the normalized Banzhaf
index. We show that annexation is never disadvantageous for the probabilistic Banzhaf index either, and for both the ShapleyShubik index and the probabilistic Banzhaf index we show that it
is NP-complete to decide whether annexing another player is advantageous. Moreover, we propose a general framework for merging and splitting that can be applied to different classes and
representations of games.

1. Introduction
Algorithmic game theory has been studied intensely in recent years (Nisan, Roughgarden, Tardos, &
Vazirani, 2007; Chalkiadakis, Elkind, & Wooldridge, 2011). This paper focuses on the algorithmic
and complexity-theoretic aspects of problems from cooperative game theory. The central question
studied is whether merging or splitting players in a coalitional game (with transferable utilities) can
raise their power in the game. Power indices measure how influential a player is for forming winning
coalitions in simple games. The ShapleyShubik and the Banzhaf power indices are most prominent
among such measures (Shapley & Shubik, 1954; Banzhaf III, 1965). Roughly speaking, in simple
games they indicate, respectively, in how many orders of support and with what probability a
player can swing the outcome of a coalition by joining or leaving it.
Weighted voting games are an important class of succinctly representable, simple games. They
can be used to model cooperation among players in scenarios where each player is assigned a
weight, and a coalition of players wins if and only if their joint weight meets or exceeds a given
c
2014
AI Access Foundation. All rights reserved.

fiR EY & ROTHE

quota. Typical real-world applications of weighted voting games include decision-making in legislative bodies (e.g., parliamentary voting) and shareholder voting (for further concrete applications and literature pointers see Chalkiadakis et al., 2011). In particular, the algorithmic and
complexity-theoretic properties of problems related to weighted voting have been studied in depth
(for an overview see, e.g., Elkind, Chalkiadakis, & Jennings, 2008; Elkind, Goldberg, Goldberg, &
Wooldridge, 2009; Bachrach, Elkind, Meir, Pasechnik, Zuckerman, Rothe, & Rosenschein, 2009;
Zuckerman, Faliszewski, Bachrach, & Elkind, 2012; Elkind, Pasechnik, & Zick, 2013; Chalkiadakis
et al., 2011).
For weighted voting games, Bachrach and Elkind (2008) were the first to study the complexity
of false-name manipulation: Is it possible for a player to increase her power by splitting into several
players and distributing her weight among them? Such a player may have an incentive to manipulate
the game via introducing false names. Relatedly, they also ask whether merging their weights can
help any two players in a weighted voting game to increase their power.
PI-B ENEFICIAL M ERGE
 open question 

PI-B ENEFICIAL S PLIT
 NP-hard (ShapleyShubik
index, m = 2) * 

PI-A NNEXATION
 never disadv. (Shapley
Shubik index) # 

 NP-hard (ShapleyShubik
index)  

 never disadv. (probabilistic Banzhaf index) 

 in PP (ShapleyShubik index, kSk = 2) 

 NP-complete (kSk = 1) 

 in P (probabilistic Banzhaf
index, kSk = 2) 

 in P (probabilistic Banzhaf
index, m = 2) 

 PP-complete 

 PP-hard 






#

 NP-hard 

(Bachrach & Elkind, 2008)
(Aziz & Paterson, 2009)
(Aziz, Bachrach, Elkind, & Paterson, 2011)
(Faliszewski & Hemaspaandra, 2009)
this paper
(Felsenthal & Machover, 1995)

Table 1: Overview of the history of complexity results of beneficial merging, splitting, and annexation for the probabilistic Banzhaf index and the ShapleyShubik index. Key: kSk denotes
the size of a merging coalition and m is the number of players a given player splits into.

Table 1 gives an overview of the development of these problems complexity results for the
power indices studied here, the probabilistic Banzhaf index and the ShapleyShubik index, and
we briefly elaborate on them now. Merging and extending the results of Bachrach and Elkind
(2008) and Aziz and Paterson (2009), Aziz et al. (2011) in particular study the problem of whether
574

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

merging or splitting players in weighted voting games is beneficial in terms of the ShapleyShubik
index (Shapley, 1953; Shapley & Shubik, 1954) and the normalized Banzhaf index (Banzhaf III,
1965) (see Section 2 for formal definitions). These results, however, provide merely NP-hardness
lower bounds. Aziz et al. (2011, p. 72, Remark 13) note that it is quite possible that our problems
are not in NP (and thus are not NP-complete). Faliszewski and Hemaspaandra (2009) provide the
best known upper bound for the beneficial merging problem for two players with respect to the
ShapleyShubik index: It is contained in the class PP, probabilistic polynomial time, which is
considered to be by far a larger class than NP, and they conjecture that this problem is PP-complete.
We observe that the same arguments give a PP upper bound for beneficial merging also in terms
of the probabilistic Banzhaf index.1 In contrast to the normalized Banzhaf index and the Shapley
Shubik index, we show for the probabilistic Banzhaf index that the problems of raising power by
merging or splitting are in P for coalitions of size two and a split into two players, respectively.
Furthermore, we bridge the gap between the NP-hardness lower bound and the PP upper bound
by proving that beneficial merging and splitting (if the new weights are given in the problem instance) are PP-complete problems both for the ShapleyShubik and the probabilistic Banzhaf index.
Beneficial splitting in general (i.e., if the number of new false identities is given, but not their actual
weights) is PP-hard for the same two indices. Thus, none of these six problems can be in NP, unless
the polynomial hierarchy collapses to its first level, which is considered highly unlikely.
Felsenthal and Machover (1995) distinguish two types of merging, voluntarily and involuntarily, and show for the latter, a bloc of two players (a.k.a. annexation of one player by another), that
it is never disadvantageous for the ShapleyShubik index, but can be disadvantageous for the normalized Banzhaf index. For the latter index, Aziz et al. (2011) show that it is NP-hard to decide
whether a player can benefit from annexing other players. We show that it is NP-complete to decide
whether annexing another player is advantageous for the ShapleyShubik index, as well as for the
probabilistic Banzhaf index. We also show that annexation can never be disadvantageous for the
probabilistic Banzhaf index, which thus behaves like the ShapleyShubik index in this regard.
A further contribution of our paper is that we propose a general framework for merging and splitting that can be applied to various classes or representations of games. Introducing new properties
of merging and splitting functions, consistency and independence, which in particular are satisfied
by the standard merging and splitting functions for weighted voting games, we can generalize the P
results for the probabilistic Banzhaf index. On the one hand, these properties are desirable for the
design of a merging or splitting function; on the other hand, they are an approach for an axiomatic
evaluation of such functions. The analysis of further properties are an interesting task for future
work. As an example of applying this more general framework to a concrete class of games, we
consider threshold network flow games on hypergraphs, a model adapted here from the threshold
network flow games introduced by Bachrach and Rosenschein (2009). In unanimity games and with
respect to the probabilistic Banzhaf index, we show that splitting is always disadvantageous or neutral, whereas merging is neutral for size-two coalitions, yet advantageous for coalitions with at least
three players. This strongly contrasts with the results by Aziz et al. (2011) showing that merging
is always disadvantageous and splitting is always advantageous for the normalized Banzhaf index
in unanimity weighted voting games. These are only two examples of how different properties of a
game or restrictions caused by a certain representation can lead to different behavior when consid1. Note that the same arguments cannot be transferred immediately to the corresponding problem for the normalized
Banzhaf index.

575

fiR EY & ROTHE

ering merging and splitting (for an overview of different classes and representations of games see,
for example, Shoham & Leyton-Brown, 2009).

2. Preliminaries
We start by providing the needed concepts and notation from cooperative game theory and complexity theory.
2.1 Basic Notions from Cooperative Game Theory
We will need the following concepts from cooperative game theory, see, e.g., the textbooks by
Chalkiadakis et al. (2011) and Peleg and Sudhlter (2003).
A coalitional game with transferable utilities, G = (N, v), consists of a set N = {1, . . . , n} of
players (or, synonymously, agents) and a coalitional function v : P(N)  R with v(0)
/ = 0, where
P(N) denotes the power set of N. We consider different classes of games with certain properties.
For example, a coalitional game is monotonic if v(B)  v(C) whenever B  C for coalitions B,C  N,
and it is simple if it is monotonic and v : P(N)  {0, 1} maps each coalition C  N to a value that
indicates whether C is successful (i.e., C wins: v(C) = 1) or not (i.e., C loses: v(C) = 0), where we
require that the grand coalition N is always winning (i.e., v(N) = 1).
Since the number of coalitions is exponential in the number of players, specifying coalitional
games by listing all values of their coalitional function would require exponential space. For algorithmic purposes, however, it is important that these games can be represented succinctly. Weighted
voting games (a.k.a. weighted threshold games) are an important class of simple games that are
compactly representable (Bilbao, Fernndez, Jimnez, & Lpez, 2002). In such games, each player
has a given weight, and a coalition of players is successful if and only if the sum of their weights
reaches or exceeds a given threshold, called the quota. Formally, a weighted voting game (WVG)
G = (w1 , . . . , wn ; q) consists of a quota q  N and nonnegative integer2 weights wi , 1  i  n, where
wi is the ith players weight. For each coalition C  N, letting w(C) denote iC wi , C wins if
w(C)  q, and it loses otherwise. Requiring the quota to satisfy 0 < q  w(N) ensures that the
empty coalition loses and the grand coalition wins. Weighted voting games have been intensely
studied from a computational complexity point of view, see, e.g., the work of Elkind et al. (2009)
and the book by Chalkiadakis et al. (2011, ch. 4) for an overview.
A weighted majority game (WMG) is the special case of a WVG where the quota is set to
q = bw(N)/2c + 1. WVGs are compactly representable, although not fully expressive (for a simple
game that cannot be represented as a WVG see, e.g., Chalkiadakis et al., 2011, Example 4.17).
More precisely, every simple game can be represented as a k-weighted voting game, which is the
intersection of k weighted voting games; the smallest such k is the dimension of the simple game;
and weighted voting games are fully expressive only for the class of one-dimensional simple games
(again, see the book by Chalkiadakis et al. (2011) for more details).
For a coalitional game G = (N, v), let dG (C, i) = v(C  {i})  v(C) be the marginal contribution
of a player i  N to a coalition C  N r {i}. In a simple game, a player i is called pivotal (or
crucial or critical) for a coalition C  N r {i} if C  {i} is successful, but C is not. We have
dG (C, i) = 1 if player i is pivotal for C, and dG (C, i) = 0 otherwise. This term can be generalized
to nonsimple games, referring to player i as pivotal for C if dG (C, i) > 0. A power index measures
2. See the work of Chalkiadakis et al. (2011, Thm. 4.2) for why nonnegative integer weights and quotas may be assumed.

576

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

a players influence in a simple game. Banzhaf III (1965), who rediscovered a notion originally
introduced by Penrose (1946), defined the raw Banzhaf power index by
Banzhaf (G , i) =



dG (C, i)

CNr{i}

for a game G = (N, v) and a player i in N. In a simple game, this indicates the number of coalitions
a player is pivotal for. However, since the ratios of these indices are more important than their
magnitudes, it is useful to normalize them. In fact, two different ways of normalization have been
proposed for the Banzhaf index.
In the original definition of the normalized Banzhaf power index (Banzhaf III, 1965), the raw
Banzhaf index of a given player is divided by the sum of all players raw Banzhaf indices:
Banzhaf(G , i) =

Banzhaf (G , i)
,
nj=1 Banzhaf (G , j)

so that all players normalized Banzhaf indices add up to one. This index has been analyzed in
detail by Dubey and Shapley (1979), who introduce an alternative normalization, which divides the
raw Banzhaf index of a given player by the total number of coalitions without that player and which
they dub the probabilistic Banzhaf power index:
Banzhaf(G , i) =

Banzhaf (G , i)
.
2n1

Intuitively, this index measures the probability that a player is pivotal for any possible coalition.
Based on a comprehensive analysis comparing various mathematical properties of these two power
indices, Dubey and Shapley (1979, p. 102) view the probabilistic Banzhaf index as being in many
respects more natural than the normalized Banzhaf index. In particular, the probabilistic Banzhaf
index satisfies four fundamental axioms: (1) symmetry,3 (2) dummy player,4 (3) additivity,5 and
(4) a property we call valuation.6 Neither valuation nor additivity is satisfied by the normalized
Banzhaf index. It is beyond the purpose of this paper to give a full explanation of these axioms,
but we refer the reader to the work of Dubey and Shapley (1979) for a careful, detailed discussion.
As the normalized Banzhaf index lacks their fourth axiom, Dubey and Shapley (1979, Footnote 21)
conclude: This may be taken as an initial sign of trouble with the normalization [of the normalized Banzhaf index]. They also note that the probabilistic Banzhaf index is better behaved when
analyzing convergence (Dubey & Shapley, 1979, p. 116). Also, the normalized Banzhaf index is
subject to the so-called bloc paradox (see Felsenthal & Machover, 1995), that is, a player can lose
power by taking over another players weight. For the probabilistic Banzhaf index this paradox
doesnt hold. (See Sections 3 and 4 for the computational complexity of this annexation problem.)
On the other hand, the normalized Banzhaf power index has its advantages as well, depending
on which setting one considers. Aziz et al. (2011, p. 61) argue that: Although the probabilistic
3. In each game G , Banzhaf(G , i) = Banzhaf(G , j) whenever i and j are symmetric, i.e., v(C  {i}) = v(C  { j}) for all
coalitions C  N r {i, j}.
4. In each game G , Banzhaf(G, i) = 0 whenever i is a dummy player, i.e., v(C  {i}) = v(C) for each coalition C  N.
5. For any two games G1 = (N, v1 ) and G2 = (N, v2 ), Banzhaf(G1 + G2 , i) = Banzhaf(G1 , i) + Banzhaf(G2 , i) for all
players i  N, where G1 + G2 = (N, v1 + v2 ) is defined via (v1 + v2 )(C) = v1 (C) + v2 (C) for all coalitions C  N.
6. For any two simple games G1 = (N, v1 ) and G2 = (N, v2 ), it holds that Banzhaf(Gv1 v2 , i) + Banzhaf(Gv1 v2 , i) =
Banzhaf(G1 , i) + Banzhaf(G2 , i) for each i  N, where the games Gv1 v2 = (N, v1  v2 ) and Gv1 v2 = (N, v1  v2 ) are
defined by (v1  v2 )(C) = max(v1 (C), v2 (C)) and (v1  v2 )(C) = min(v1 (C), v2 (C)) for all coalitions C  N.

577

fiR EY & ROTHE

Banzhaf index is more useful measuring the actual probability of influencing a decision, it does
not fit the framework of using power indices to share resources or power, because the probabilistic
Banzhaf index is not normalized. Here, normalization is done with respect to the number of players/coalitions, which makes games with different numbers of players better comparable. Due to its
normalization with respect to all players in a game, in monotonic games the normalized Banzhaf
index yields an imputation (i.e., a payoff vector (Banzhaf(G , 1), Banzhaf(G , 2), . . . , Banzhaf(G , n))
satisfying efficiency, ni=1 Banzhaf(G , i) = v(N), and individual rationality, Banzhaf(G , i)  v({i})
for all i  N), whereas the probabilistic Banzhaf index is not efficient.
There is a unique imputation satisfying all four axioms mentioned above, based on the Shapley
Shubik power index (Shapley & Shubik, 1954), which describes the marginal contributions of a
player to all possible coalitions with respect to the order in which players enter the coalitions:
ShapleyShubik (G , i) =

kCk!  (n  1  kCk)!  dG (C, i),


CNr{i}

normalized by

ShapleyShubik (G , i)
.
n!
For coalitional games, this is also known as the Shapley value (Shapley, 1953).
Felsenthal and Machover (2005, 1995) carefully discuss the differences between these power
indices, and we refer the reader to their work.
ShapleyShubik(G , i) =

2.2 Basic Notions from Computational Complexity Theory
We assume that the reader is familiar with the basic notions of complexity theory such as the complexity classes P (deterministic polynomial time) and NP (nondeterministic polynomial time),
p
the polynomial-time many-one reducibility, denoted by m , and the notions of hardness and comp
pleteness (of decision problems for complexity classes) with respect to m . For more background
and details, see, e.g., the textbooks by Garey and Johnson (1979), Papadimitriou (1995), and Rothe
(2005).
Valiant (1979) introduces #P as the class of functions that give the number of solutions of the
instances of NP problems. For a decision problem A  NP, we denote this function by #A. For
example, letting SAT denote the satisfiability problem from propositional logic, #SAT denotes the
function mapping any boolean formula  to the number of truth assignments satisfying . There are
several notions of reducibility for functional problems and, consequently, there are several notions
of hardness and completeness for complexity classes of functions such as #P. Let f and g be two
functions mapping from  to N. We say f many-one-reduces to g if there exist two polynomialtime computable functions,  and , such that for each x   , f (x) = (g((x))). This notion
of functional many-one reducibility is due to Zank (1991); it is the analogue of (polynomial-time)
many-one reducibility for sets. The special case where  is the identity yields the parsimonious
reducibility, which preserves the number of solutions: We say f parsimoniously reduces to g if
there exists a polynomial-time computable function  such that for each x   , f (x) = g((x)).
Intuitively, parsimonious reductions preserve the number of solutions (for a more detailed discussion
of functional reducibilities see Faliszewski & Hemaspaandra, 2009). We say g is #P-parsimonioushard if every function f  #P parsimoniously reduces to g. If g is #P-parsimonious-hard and g 
#P, then g is #P-parsimonious-complete. The notions of #P-many-one-hardness and #P-many-onecompleteness are defined analogously. It is known that, given a WVG G and a player i, computing
578

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

the raw Banzhaf index is #P-parsimonious-complete (Prasad & Kelly, 1990), whereas computing
the raw ShapleyShubik index is not (Faliszewski & Hemaspaandra, 2009), although it is #P-manyone-complete. For other recent #P-completeness results, we refer to the work of Aziz, Brandt, and
Brill (2013).
Gill (1977) introduces the class PP (probabilistic polynomial time) that contains all decision
problems X for which there exist a function f  #P and a polynomial p such that for all instances x,
x  X if and only if f (x)  2 p(|x|)1 . It is easy to see that NP  PP; in fact, PP is considered to
be by far a larger class than NP, due to Todas theorem (1991): PP is at least as hard (in terms of
polynomial-time Turing reductions) as any problem in the polynomial hierarchy (i.e., PH  PPP ).
NPPP , the second level of Wagners counting hierarchy (1986), is the class of problems solvable by
an NP machine with access to a PP oracle. Mundhenk, Goldsmith, Lusena, and Allender (2000)
identify NPPP -complete problems related to finite-horizon Markov decision processes. Littman,
Goldsmith, and Mundhenk (1998) obtain NPPP -completeness results when analyzing a variant of
the satisfiability problem and questions related to probabilistic planning.

3. Definition of Beneficial Merging, Splitting, and Annexation
Aziz et al. (2011) introduce the merging and splitting operations for WVGs. We use the following
notation. Given a WVG G = (w1 , . . . , wn ; q) and a nonempty7 coalition S  {1, . . . , n}, let G&S =
(w(S), w j1 , . . . , w jnkSk ; q) with { j1 , . . . , jnkSk } = N r S denote the new WVG in which the players
in S have been merged into one new player of weight w(S).8 For a power index PI, the beneficial
merging problem is defined as follows.
PI-B ENEFICIAL M ERGE
Given:
Question:

A WVG G = (w1 , . . . , wn ; q) and a nonempty coalition S  {1, . . . , n}.
Is it true that PI(G&S , 1) > iS PI(G , i)?

Similarly, given a WVG G = (w1 , . . . , wn ; q), a player i, and an integer m  2, define the set of
WVGs Gim = (w1 , . . . , wi1 , wi+1 , . . . , wn , wn+1 , . . . , wn+m ; q) in which i with weight wi is split into
m new players n + 1, . . . , n + m with weights wn+1 , . . . , wn+m such that mj=1 wn+ j = wi . (Note that
there is a set of such WVGs Gim , since there might be several possibilities of distributing is weight
wi to the new players n + 1, . . . , n + m satisfying mj=1 wn+ j = wi .)
We distinguish between two different splitting problems.9 First, for a power index PI, consider
the problem where a weighted voting game, a player i, and the number m of false identities i splits
into are given in the problem instance, but not the weights of the new players:
PI-B ENEFICIAL S PLIT
Given:
Question:

A WVG G = (w1 , . . . , wn ; q), a player i, and an integer m  2.
Is it possible to split i into m new players n + 1, . . . , n + m with weights wn+1 , . . . , wn+m
satisfying mj=1 wn+ j = wi such that in this new WVG, call it Gim , it holds that
mj=1 PI(Gim , n + j) > PI(G , i)?

7. We omit the empty coalition, since this would slightly change the idea of the problem.
8. Note that the players order doesnt matter when considering the normalized or probabilistic Banzhaf index.
9. This distinction wouldnt make sense for beneficial merging or annexation.

579

fiR EY & ROTHE

As mentioned above, for an instance (G , i, m) of PI-B ENEFICIAL S PLIT, there might be various
ways of distributing is weight to her false identities, giving rise to various new games Gim . In
the second (more special) variant of the problem we consider, the new players weights are given
explicitly in the problem instance (and thus the number of false identities is given implicitly). In
this case, there is only one unique new game Gim , and splitting is the inverse function to merging.
We will use PI-B ENEFICIAL S PLIT to denote the more general problem without explicitly given
weights, and will explicitly mention it whenever we speak of the more restricted variant.
While these problems deal with voluntary actions by players, Felsenthal and Machover (1995)
study the question of whether it is possible for a player to change her power by taking another
players weight over without this players consent. They introduce the bloc paradox, stating that it
is possible to lose power by annexing another players weight. For instance, the normalized Banzhaf
index is subject to this paradox. For the ShapleyShubik index, Felsenthal and Machover show that
annexation is never disadvantageous. Nevertheless, one can still ask the question of whether it is
in fact advantageous. The following problem has been studied by Aziz et al. (2011). Let PI be a
power index.
PI-B ENEFICIAL A NNEXATION
Given:
Question:

A WVG G = (w1 , . . . , wn ; q), a player i  N, and a coalition S  {1, . . . , n} r {i}.
Is it true that PI(G&(S{i}) , 1) > PI(G , i)?

In particular, Aziz et al. (2011) show that it is NP-hard to decide whether annexation is beneficial
for the normalized Banzhaf index, and that with respect to this power index it is always beneficial
for a player to annex another player with a larger weight. They also introduce the annexation
nonmonotonicity paradox, which says that it sometimes can be more useful for a player to annex
another player of small weight than to annex another player of large weight.
The goal of this paper is to classify these problems in terms of their complexity for both the
ShapleyShubik and the probabilistic Banzhaf index. First, since we allow players with zero weight,
the analysis of the beneficial splitting problem requires another simple fact, which we will use in
the upcoming proofs of Theorems 4.7 and 4.12.
Lemma 3.1. For both the probabilistic Banzhaf index and the ShapleyShubik index, given a
weighted voting game, adding a player with weight zero does not change the original players
power indices, and the new players power index is zero.
The proof of Lemma 3.1 is straightforward and therefore omitted.

4. Complexity of Beneficial Merging, Splitting, and Annexation in Weighted Voting
Games
Aziz et al. (2011) analyze the problems ShapleyShubik-B ENEFICIAL M ERGEand ShapleyShubikB ENEFICIAL S PLIT as well as Banzhaf-B ENEFICIAL M ERGE and Banzhaf-B ENEFICIAL S PLIT in
terms of their complexity. They provide NP-hardness lower bounds, but leave it open whether these
problems are in NP. Indeed, Aziz et al. (2011, p. 72, Remark 13) note that it is quite possible
that our problems are not in NP, which raises a natural question: What about the upper bounds?
Faliszewski and Hemaspaandra (2009) establish an upper bound for the restriction of the beneficial
merging problem for weighted voting games that was originally proposed by Bachrach and Elkind
580

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

(2008) (Can any two players increase their joint ShapleyShubik index via merging?): This problem is in the complexity class PP. In that paper, Faliszewski and Hemaspaandra study the problem
PI-P OWER C OMPARE in weighted voting games, where PI is some power index. They prove PPcompleteness of this problem for the probabilistic Banzhaf and the ShapleyShubik index.
In this section we prove that beneficial merging and splitting is PP-hard, and we provide matching upper bounds for beneficial merging and splitting (the latter in the variant where the new players
weights are given), both for the ShapleyShubik and the probabilistic Banzhaf index.
4.1 The Probabilistic Banzhaf Power Index
Both the beneficial merging problem for a coalition S of size 2 and the beneficial splitting problem
for m = 2 false identities can trivially be decided in polynomial time for the probabilistic Banzhaf
index, since the sum of power (in terms of this index) of two players is always equal to the power
of the player that is obtained by merging them.
Proposition 4.1. Let G be a weighted voting game and S  {1, . . . , n} be a coalition of its players.
1. Banzhaf-B ENEFICIAL M ERGE is in P for instances (G , S) with kSk = 2.
2. Banzhaf-B ENEFICIAL S PLIT is in P for instances (G , i, 2).
Proof. Let G = (w1 , . . . , wn ; q) be a weighted voting game. Without loss of generality (see Footnote 8), let S = {1, n}. We obtain a new game G&S = (w1 + wn , w2 , . . . , wn1 ; q), where the first
player is the new player merging S. Letting vG and vG&S denote the corresponding coalitional functions, it holds that
Banzhaf(G&S , 1)  (Banzhaf(G , 1) + Banzhaf(G , n))
!
1
= n2
 (vG&S (C  {1})  vG&S (C))
2
C{2,...,n1}


=

1
2n1
1

2n1

!



(vG (C  {1})  vG (C)) +

C{2,...,n}





(vG (C  {n})  vG (C))

C{1,...,n1}

(2(vG&S (C  {1})  vG&S (C))

C{2,...,n1}

 (vG (C  {1})  vG (C))  (vG (C  {1, n})  vG (C  {n}))
!
 (vG (C  {n})  vG (C))  (vG (C  {n, 1})  vG (C  {1})))
=

1
2n1

!



(2vG&S (C  {1})  2vG (C  {1, n}) + 2vG (C)  2vG&S (C))

C{2,...,n1}

In the case of splitting, it similarly holds that
Banzhaf(Gn2 , n + 1) + Banzhaf(Gn2 , n + 2)  Banzhaf(G , n) = 0
581

= 0.

fiR EY & ROTHE

for a weighted voting game G = (N, v), m = 2, and, without loss of generality, player n in G splitting
into players n + 1 and n + 2 in a new game Gn2 .
q

Although it may seem as if Proposition 4.1 implied that merging (and splitting) were never
beneficial regarding this index, this cannot be generalized to merging (or splitting into) more than
two players, by repeatedly applying the above result to pairs of players step by step. For example, as
soon as two players merge, a third players probabilistic Banzhaf index might have already changed
in the new game, before merging her with another player in a subsequent step. Suppose three players
{1, 2, 3} want to merge in a game G . Let Bi = Banzhaf(G , i), 1  i  3, be their original probabilistic
Banzhaf indices. Let B be their common Banzhaf index after the merge. After merging the first two
players, let B01 and B03 be the indices of {1, 2} and 3, respectively. Then, due to Proposition 4.1, B =
B01 +B03 = B1 +B2 +B03 . Hence, B > B1 +B2 +B3 if and only if B03 > B3 . That is, for the probabilistic
Banzhaf index, beneficial merging of three players boils down to comparing the index of one player
in two gamesthe original game and the one where the other two players have merged. If these were
two arbitrary games, the result for PI-P OWER C OMPARE by Faliszewski and Hemaspaandra (2009)
would have applied. Here, however, the indices need to be compared in two closely related games;
this requires a different proof. Indeed, next we show that it is by far harder (unless the polynomial
hierarchy collapses to its first level) to decide whether merging three players is beneficial in terms
of the probabilistic Banzhaf index than for two players. We will use the following result due to
Faliszewski and Hemaspaandra (2009, Lemma 2.3).

Lemma 4.2 (Faliszewski & Hemaspaandra, 2009). Let F be a #P-parsimonious-complete function. The problem C OMPARE-F = {(x, y) | F(x) > F(y)} is PP-complete.

The well-known NP-complete problem S UBSET S UM (which is a special variant of the K NAP problem) asks, given a sequence (a1 , . . . , an ) of positive integers and a positive integer q,
do there exist x1 , . . . , xn  {0, 1} such that ni=1 xi ai = q? It is known that #S UBSET S UM is #Pparsimonious-complete (for parsimonious reductions from #3-SAT via #E XACT C OVER B Y 3-S ETS
to #S UBSET S UM see, e.g., Hunt, Marathe, Radhakrishnan, & Stearns, 1998; Papadimitriou, 1995).
Hence, by Lemma 4.2, we have the following.
SACK

Corollary 4.3. C OMPARE-#S UBSET S UM is PP-complete.
p

Our goal is to provide a m -reduction from C OMPARE-#S UBSET S UM to Banzhaf-B ENEFICIAL M ERGE. However, to make this reduction work, it will be useful to consider two restricted variants of C OMPARE-#S UBSET S UM, which we denote by C OMPARE-#S UBSET S UM-R and C OM PARE-#S UBSET S UM -RR, show their PP-hardness, and then reduce C OMPARE-#S UBSET S UM -RR
to Banzhaf-B ENEFICIAL M ERGE. This will be done in Lemmas 4.4 and 4.5 and in Theorem 4.6. In
all restricted variants of C OMPARE-#S UBSET S UM we may assume, without loss of generality, that
the target value q in a related #S UBSET S UM instance ((a1 , . . . , an ), q) satisfies 1  q    1, where
 = ni=1 ai .
582

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

C OMPARE -#S UBSET S UM -R
Given:
Question:

A sequence A = (a1 , . . . , an ) of positive integers and two positive integers q1 and q2
with 1  q1 , q2    1, where  = ni=1 ai .
Is the number of subsequences of A summing up to q1 greater than the number of
subsequences of A summing up to q2 , that is, does it hold that
#S UBSET S UM((a1 , . . . , an ), q1 ) > #S UBSET S UM((a1 , . . . , an ), q2 )?
p

Lemma 4.4. C OMPARE-#S UBSET S UM m C OMPARE-#S UBSET S UM-R.
Proof. Given an instance (X,Y ) of C OMPARE-#S UBSET S UM, X = ((x1 , . . . , xm ), qx ) and Y =
((y1 , . . . , yn ), qy ), construct a C OMPARE-#S UBSET S UM-R instance (A, q1 , q2 ) as follows. Let  =
m
i=1 xi and define A = (x1 , . . . , xm , 2y1 , . . . , 2yn ), q1 = qx , and q2 = 2qy . This construction can
obviously be achieved in polynomial time.
It holds that integers from A can only sum up to qx    1 if they do not contain multiples of
2, thus #S UBSET S UM(A, q1 ) = #S UBSET S UM((x1 , . . . , xm ), qx ). On the other hand, q2 cannot be
obtained by adding any of the xi s, since this would yield a non-zero remainder modulo 2, because
m
i=1 xi =  is too small. Thus, it holds that #S UBSET S UM (A, q2 ) = #S UBSET S UM ((y1 , . . . , yn ), qy ).
It follows that (X,Y ) is in C OMPARE-#S UBSET S UM if and only if (A, q1 , q2 ) is in C OMPARE #S UBSET S UM -R.
q
In order to perform the next step, we need to ensure that all integers in a C OMPARE-#S UBSETS UM-R instance are divisible by 8. This can easily be achieved, by multiplying each integer in an
instance ((a1 , . . . , an ), q1 , q2 ) by 8, obtaining ((8a1 , . . . , 8an ), 8q1 , 8q2 ) without changing the number
of solutions for both related S UBSET S UM instances. Thus, from now on, without loss of generality,
we assume that for a given C OMPARE-#S UBSET S UM-R instance ((a1 , . . . , an ), q1 , q2 ), it holds that
ai , q j  0 mod 8 for 1  i  n and j  {1, 2}.
Now, we consider our even more restricted variant of this problem.
C OMPARE -#S UBSET S UM -RR
Given:
Question:

A sequence A = (a1 , . . . , an ) of positive integers.
Is the number of subsequences of A summing up to (/2)2, where  = ni=1 ai , greater
than the number of subsequences of A summing up to (/2)  1, i.e., is it true that
#S UBSET S UM((a1 , . . . , an ), (/2)  2) > #S UBSET S UM((a1 , . . . , an ), (/2)  1)?
p

Lemma 4.5. C OMPARE-#S UBSET S UM-R m C OMPARE-#S UBSET S UM-RR.
Proof. Given an instance (A, q1 , q2 ) of C OMPARE-#S UBSET S UM-R, where we assume that A =
(a1 , . . . , an ), q1 , and q2 satisfy ai , q j  0 mod 8 for 1  i  n and j  {1, 2}, we construct an instance
B of C OMPARE-#S UBSET S UM-RR as follows. (This reduction is inspired by the standard reduction
from S UBSET S UM to PARTITION due to Karp, 1972.)
Letting  = ni=1 ai , define
B = (a1 , . . . , an , 2  q1 , 2 + 1  q2 , 2 + 3 + q1 + q2 , 3).
583

fiR EY & ROTHE

This instance can obviously be constructed in polynomial time. Observe that
!
n

T=

 ai

+ (2  q1 ) + (2 + 1  q2 ) + (2 + 3 + q1 + q2 ) + 3 = 10 + 4,

i=1

and therefore, (T/2)2 = 5 and (T/2)1 = 5 +1. We show that (A, q1 , q2 ) is in C OMPARE-#S UB SET S UM -R if and only if B is in C OMPARE-#S UBSET S UM -RR.
First, we examine which subsequences of B sum up to 5. Consider two cases.
Case 1: If 3 is added, 2 +3+q1 +q2 cannot be added, as it would be too large. Also, 2 +1q2
cannot be added, leading to an odd sum. So, 2  q1 has to be added, as the remaining 
are too small. Since 3 + 2  q1 = 5  q1 , 5 can be achieved by adding some ai s if and
only if there exists a subset A0  {1, . . . , n} such that iA0 ai = q1 (i.e., A0 is a solution of the
S UBSET S UM instance (A, q1 )).
Case 2: If 3 is not added, but 2 + 3 + q1 + q2 , an even number can only be achieved by adding
2 + 1  q2 , thus,   4  q1 remain. 2  q1 is too large, while no subsequence of A sums up
to  4q1 , because of the assumption of divisibility by 8. If neither 3 nor 2 +3+q1 +q2
are added, the remaining 5 + 1  q1  q2 are too small.
Thus, the only possibility to obtain 5 is to find a subsequence of A adding up to q1 . Thus,
#S UBSET S UM(A, q1 ) = #S UBSET S UM(B, 5).
Second, for similar reasons, a sum of 5 + 1 can only be achieved by adding 3 + (2 +
1  q2 ) and a term iA0 ai , where A0 is a subset of {1, . . . , n} such that iA0 ai = q2 . Hence,
#S UBSET S UM(A, q2 ) = #S UBSET S UM(B, 5 + 1).
Thus, the relation #S UBSET S UM(A, q1 ) > #S UBSET S UM(A, q2 ) holds if and only if #S UBSETS UM(B, 5) > #S UBSET S UM(B, 5 + 1), which completes the proof.
q
We now are ready to prove the main theorem of this section.
Theorem 4.6. Banzhaf-B ENEFICIAL M ERGE is PP-complete, even if only three players of equal
weight merge.
Proof. Membership of Banzhaf-B ENEFICIAL M ERGE in PP follows from the fact that the raw
Banzhaf index is in #P and that #P is closed under addition and multiplication by two,10 and, furthermore, since comparing the values of two #P functions on two (possibly different) inputs reduces
to a PP-complete problem. This technique (which was proposed by Faliszewski & Hemaspaandra,
p
2009, and applies their Lemma 2.10) works, since PP is closed under m -reducibility.
p
We show PP-hardness of Banzhaf-B ENEFICIAL M ERGE by means of a m -reduction from C OM PARE-#S UBSET S UM -RR, which is PP-hard by Corollary 4.3 via Lemmas 4.4 and 4.5.
Given an instance A = (a1 , . . . , an ) of C OMPARE-#S UBSET S UM-RR, construct the following
instance for Banzhaf-B ENEFICIAL M ERGE. Let  = ni=1 ai . Define the WVG
G = (2a1 , . . . , 2an , 1, 1, 1, 1; )
10. Again, note that this idea cannot be transferred straightforwardly to the normalized Banzhaf index, since in different
games the indices have possibly different denominators, not only different by a factor of some power of two, as is the
case for the probabilistic Banzhaf index.

584

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

with n + 4 players, and let the merging coalition be S = {n + 2, n + 3, n + 4}.
Letting N = {1, . . . , n}, it holds that
(
fi
)
fi

1 

fi

Banzhaf(G , n + 2) = n+3  C  {1, . . . , n + 1, n + 3, n + 4} fi  wi =   1 

fi iC

2
(
fi
(
fi
)
)

fi


fi

1
 0
fi

 0
fi

= n+3  A  N fi  2ai =   1  + 3   A  N fi 1 +  2ai =   1 

fi iA0


fi

2
iA0
fi
fi
(
)!
) (
fi
fi


 
fi
fi

 

+ 3   A0  N fi 2 +  2ai =   1  +  A0  N fi 3 +  2ai =   1 
fi
fi


 
iA0
iA0
(
fi
fi
) (
)!

fi
 
fi

1

fi
 
fi

= n+3 3   A0  N fi  2ai =   2  +  A0  N fi  2ai =   4  ,

fi iA0
 
fi iA0

2

(1)
(2)

since the 2ai s can only add up to an even number. The first of the four sets in (1) and (2) refers
to those coalitions that do not contain any of the players n + 1, n + 3, and n + 4; the second, third,
and fourth set in (1) and (2) refers to those coalitions containing either one, two, or three of them,
respectively. Since the players in S have the same weight, players n + 3 and n + 4 have the same
probabilistic Banzhaf index as player n + 2.
Furthermore, the new game after merging is G&{n+2,n+3,n+4} = (3, 2a1 , . . . 2an , 1; ) with n + 2
players, and similarly as above the Banzhaf index of the first player is calculated as follows:

Banzhaf G&{n+2,n+3,n+4} , 1
(
fi
)
fi

1 

fi

C

{2,
.
.
.
,
n
+
2}
w

{

3,


2,


1}
=

fi

 i
fi iC

2n+1 
(
fi
)

fi

1
 0
fi

=
A

N
2a

{

3,


2,


1}

fi  i

fi iA0

2n+1 
(
fi
)!

fi

 0
fi

+  A  N fi 1 +  2ai  {  3,   2,   1} 

fi

iA0
fi
fi
(
)!
) (

fi
fi
 

1
fi

  0
fi
 0
A

N
2a
=


4
+
2

A

N
2a
=


2
=
 .


fi
fi

i
i


n+1

fi iA0
fi iA0
 

2
Altogether, it holds that
Banzhaf G&{n+2,n+3,n+4} , 1







Banzhaf(G , i)

i{n+2,n+3,n+4}

fi
(
fi
) (
)!
fi


fi
 
 0
fi
  0
fi

=
A

N
2a
=


2
+
A

N
2a
=


4
2


fi


fi

i
i


n+1

fi iA0
 
fi iA0

2
(
fi
fi
) (
)!

fi
 
fi

3

fi
 
fi

 n+3 3   A0  N fi  2ai =   2  +  A0  N fi  2ai =   4 

fi iA0
 
fi iA0

2
1

585

fiR EY & ROTHE

fi
(
)


fi

 0
fi


2


3
A

N
=
2a
=


2

fi

i

n+1
n+3

fi iA0

2
2
fi
(
)



fi

1
3

fi

+ n+1  n+3  A0  N fi  2ai =   4 

fi iA0

2
2
(
fi
(
)
fi

1 

1 

fi


=  n+3   A0  N fi  ai =  1  + n+3   A0  N

fi iA0
 2

2
2


1

3

which is greater than zero if and only if
(
fi
) (

fi
 

 0
fi
  0
A

N

2
a
=

fi  i
> A N

fi iA0
 
2

fi
fi
fi
fi
fi

fi
fi
fi
fi
fi

)




2
a
=
,
0 i 2

iA


0 ai = 2  1
iA

)


,


which in turn is the case if and only if the original instance A is in C OMPARE-#S UBSET S UM-RR. q
Analogously to the proof of Theorem 4.6, it can be shown that the beneficial splitting problem
for at least three false identities with given new weights is PP-complete. However, for the more
general beneficial splitting problem where the new players weights are not given, a PP upper bound
cannot be shown straightforwardly. Yet, it can be shown that this problem is PP-hard, even for three
false identities.
Theorem 4.7. Banzhaf-B ENEFICIAL S PLIT is PP-hard (even if the given player can only split into
three players of equal weight).
Proof. In order to show PP-hardness for Banzhaf-B ENEFICIAL S PLIT, we use the same techniques
as in Theorem 4.6, appropriately modified. In fact, we will now show PP-hardness for m = 3 false
identities. This result can be expanded to all fixed m  3 by splitting into additional players with
weight 0. More precisely, if m > 3, we consider the same game G as below and split into three
players of weight 1 each and m  3 players of weight 0 each. By Lemma 3.1, the sum of all m
new players Banzhaf power is equal to the combined Banzhaf power of the three players. Thus,
PP-hardness will hold for splitting into m > 3 players by essentially the same arguments as given
below for splitting into three players.
First, we slightly change the definition of C OMPARE-#S UBSET S UM-RR by switching (/2) 
2 and (/2)  1. The problem (call it C OMPARE-#S UBSET S UM-R R) of whether the number of
subsequences of a given sequence A of positive integers summing up to (/2)  1 is greater than
the number of subsequences of A summing up to (/2)  2, is PP-hard by the same proof as in
Lemma 4.5 with the roles of q1 and q2 exchanged.
Now, we reduce this problem to Banzhaf-B ENEFICIAL S PLIT by constructing the following instance of the beneficial splitting problem from an instance A = (a1 , . . . , an ) of C OMPARE-#S UBSETS UM-R R. Let G = (2a1 , . . . , 2an , 1, 3; ), where  = nj=1 a j , and let i = n + 2 be the player to be
split. G is (apart from the order of players) equivalent to the game obtained by merging in the proof
of Theorem 4.6. Thus, letting N = {1, . . . , n}, Banzhaf(G , n + 2) equals
(
fi
fi
) (
)!

fi
 
fi

1
 0
fi
  0
fi

2

A

N
2a
=


2
+
A

N
2a
=


4

fi


fi
 .
j
j
0
0

fi jA
 
fi jA

2n+1
586

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

Allowing players with weight zero, there are different possibilities to split player n + 2 into three
players. By Lemma 3.1, splitting n + 2 into one player with weight 3 and two others with weight 0
is not beneficial. Likewise, splitting n + 2 into two players with weights 1 and 2 and one player with
weight 0 is not beneficial, by Lemma 3.1 and since splitting into two players is not beneficial (by
Theorem 1). Thus, the only possibility left is splitting n + 2 into three players of weight 1 each. This
corresponds to the original game in the proof of Theorem 4.6, Gi3 = (2a1 , . . . , 2an , 1, 1, 1, 1; ).
Therefore,
Banzhaf(Gi3 , n + 2) = Banzhaf(Gi3 , n + 3) = Banzhaf(Gi3 , n + 4)
fi
fi
(
) (
)!
fi
fi

 

1
fi
fi
 0
  0

A

N
A

N
2a
=


2
+
2a
=


4
=
3

fi

fi


 .
0 j
0 j
fi jA
fi jA

 

2n+3
Altogether, as in the proof of Theorem 4.6, the sum of the three new players probabilistic
Banzhaf indices minus the probabilistic Banzhaf index of the original player is greater than zero if
and only if
(
fi
fi
) (
)

fi
 
fi

 0
fi


fi

0


 A  N fi  a j = ( /2)  1  >  A  N fi  a j = ( /2)  2  ,

fi jA0
 
fi jA0

which is true if and only if A is in C OMPARE-#S UBSET S UM-R R.

q

Remark 4.8. As an upper bound for the general beneficial splitting problem, we can only show
membership in NPPP , whenever the number of false identities is given in unary, and we conjecture
that this problem is even complete for this class. When the number m of false identities but not their
weights are given in unary, there are exponentially many possibilities to distribute the split players
weight among her false identities. Nondeterministically guessing such a distribution and then, for
each distribution guessed, asking an appropriate PP oracle to check in polynomial time whether
their combined Banzhaf power in the new game is greater than the original players Banzhaf power
in the original game, shows that Banzhaf-B ENEFICIAL S PLIT is in NPPP .
Whenever the number of false identities is given in the standard binary input format, even this
upper bound might no longer be valid.
For a given weighted voting game G and two players i and j in G , Proposition 4.1 implies that
Banzhaf(G&{i, j} , 1)  Banzhaf(G , i) = Banzhaf(G , j)  0.

(3)

Therefore, it is never disadvantageous for player i to annex player j. Furthermore, we have the
following result on the complexity of beneficial annexation for the probabilistic Banzhaf index.
Theorem 4.9. Banzhaf-B ENEFICIAL A NNEXATION is NP-complete for instances (G , i, S) such that
kSk = 1.
Proof. By Equation (3) above, the question of whether the new players probabilistic Banzhaf
index is greater than the original players probabilistic Banzhaf index is equivalent to the question
of whether the annexed player has a positive value in the original game. This property can be
decided in nondeterministic polynomial time and is NP-hard by a result due to Prasad and Kelly
(1990).
q

587

fiR EY & ROTHE

Remark 4.10. While Banzhaf-B ENEFICIAL A NNEXATION immediately inherits NP-hardness from
the special case in Theorem 4.9, that problems NP upper bound does not generalize straightforwardly.
4.2 The ShapleyShubik Power Index
In order to prove PP-hardness for the merging and splitting problems with respect to the Shapley
Shubik index, we need to take a further step back.
E XACT C OVER BY 3-S ETS (X3C, for short) is another well-known NP-complete decision problem: Given a set B of size 3k and a family S of subsets of B that have size three each, does there
exist a subfamily S 0 of S such that B is exactly covered by S 0 ?
Theorem 4.11. ShapleyShubik-B ENEFICIAL M ERGE is PP-complete, even if only two players of
equal weight merge.
Proof. The PP upper bound, which has already been observed for two players by Faliszewski and
Hemaspaandra (2009), can be shown analogously to the proof of Theorem 4.6.
For proving the lower bound, observe that the size of a coalition a player is pivotal for is crucial for determining the players ShapleyShubik index. Pursuing the techniques by Faliszewski and
Hemaspaandra, we examine the problem COMPARE-#X3C, which is PP-complete by Lemma 4.2.
We will apply the following useful properties of X3C instances shown by Faliszewski and Hemaspaandra (2009, Lemma 2.7): Every X3C instance (B0 , S 0 ) can be transformed into an X3C instance
(B, S ), where kBk = 3k and kS k = n, that satisfies k/n = 2/3 without changing the number of solutions, i.e., #X3C(B, S ) = #X3C(B0 , S 0 ). Now, by the properties of the standard reduction from
X3C to S UBSET S UM (which in particular preserves the number of solutions, i.e., #X3C parsimoniously reduces to #S UBSET S UM, as well as the input size n and the solution size k, see, e.g.,
Papadimitriou, 1995), we can assume that in a given C OMPARE -#S UBSET S UM instance each subsequence summing up to the given integer q is of size 2n/3. Following the track of the reductions
from C OMPARE -#S UBSET S UM via C OMPARE-#S UBSET S UM-R to C OMPARE-#S UBSET S UM-RR
in Lemmas 4.4 and 4.5, a solution A0  {1, . . . , n} to a given instance A = (a1 , . . . , an ) of the latter problem (A0 satisfying either iA0 ai = (/2)  2 or iA0 ai = (/2)  1, where  = ni=1 ai )
can be assumed to satisfy kA0 k = k = (n+2)/3. Under this assumption, we show PP-hardness of
ShapleyShubik-B ENEFICIAL M ERGE via a reduction from C OMPARE -#S UBSET S UM -RR. Given
such an instance, we construct the WVG G = (a1 , . . . , an , 1, 1; /2) and consider coalition S = {n +
1, n + 2}. Let N = {1, . . . , n} and define X = #S UBSET S UM(A, (/2)  1) and Y = #S UBSET S UM(A,
(/2)  2). Then,
ShapleyShubik(G , n + 1) = ShapleyShubik(G , n + 2)
 

=


 

1
 


kCk!(n
+
1

kCk)!
+
(kCk
+
1)!(n

kCk)!






(n + 2)! CN such that

 CN such that
 ai =(/2)1

 ai =(/2)2

iC

=



iC

1
(X  k!(n + 1  k)! +Y  (k + 1)!(n  k)!) .
(n + 2)!
588

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

Merging the players in S, we obtain G&S = (2, a1 , . . . , an ; /2). The ShapleyShubik index of the
new player in G&S is
ShapleyShubik(G&S , 1) =

1
(n + 1)!



kCk!(n  kCk)!

CN such that
 ai  {(/2)1,(/2)2}
iC

1
=
(X +Y )  (k + 1)!(n  k)!.
(n + 1)!
All in all,
ShapleyShubik(G&S , 1)  (ShapleyShubik(G , n + 1) + ShapleyShubik(G , n + 2))
(X +Y )  (k + 1)!(n  k)! 2 (X  k!(n + 1  k)! +Y  (k + 1)!(n  k)!)
=

(n + 1)!
(n + 2)!
k!(n  k)!
=
(n  2k)(X +Y ).
(n + 2)!

(4)

Since we assumed that k = (n+2)/3 and we can also assume that n > 4 (because we added four
integers in the construction in the proof of Lemma 4.5), it holds that
n  2k =

n4
> 0.
3

Thus the term (4) is greater than zero if and only if Y is greater than X, which is true if and only if
A is in C OMPARE -#S UBSET S UM -RR.
q
Analogously to the probabilistic Banzhaf index, we can show that also for the ShapleyShubik
index it is PP-complete to decide if splitting a player into players with given weights is beneficial.
For the more general case where the number of false identities but no actual weights are given,
we can raise the previously known lower bound to PP-hardness. However, the upper bound of PP
cannot be transferred straightforwardly.
Theorem 4.12. ShapleyShubik-B ENEFICIAL S PLIT is PP-hard (even if the given player can only
split into two players of equal weight).
Proof. PP-hardness can be shown analogously to the proof of Theorem 4.7, appropriately modified to use the arguments from the proof of Theorem 4.11 instead of those from the proof of Theorem 4.6.
q
An upper bound of NPPP holds due to analogous arguments as in the proof of Theorem 4.7,
whenever m is given in unary.
Felsenthal and Machover (1995) have shown that annexation is never disadvantageous for the
ShapleyShubik index. Still, the question of whether it is advantageous is hard to decide.
Theorem 4.13. ShapleyShubik-B ENEFICIAL A NNEXATION is NP-complete for instances (G , i, S)
with kSk = 1.
589

fiR EY & ROTHE

Proof. Let G = (w1 , . . . , wn ; q) be a weighted voting game and, without loss of generality, let
player 1 annex player n. It holds that
ShapleyShubik(G&{1,n} , 1)  ShapleyShubik(G , 1)
=

1
 ((v(C  {1, n}  v(C  {1}))  kCk!(n  1  kCk)!
n! C{2,...,n1}
+ (v(C  {n}  v(C))  (kCk + 1)!(n  2  kCk)!).

Unlike for the probabilistic Banzhaf index, this term is in general not equal to ShapleyShubik(G , n),
but is greater than zero if and only if player n is pivotal for at least one coalition C  {1, . . . , n 1} in
the original game. So, analogously to Theorem 4.9, this property can be decided in nondeterministic
polynomial time and is NP-hard by a result due to Prasad and Kelly (1990) (see also Deng &
Papadimitriou, 1994).
q
Remark 4.14. Analogously to annexation with respect to the probabilistic Banzhaf index, while
ShapleyShubik-B ENEFICIAL A NNEXATION immediately inherits NP-hardness from the special case
in Theorem 4.13, that problems NP upper bound does not generalize straightforwardly.

5. Generalizing Merging and Splitting Functions
We extend the definition of merging and splitting functions from weighted voting games to general
classes (or representations) G of coalitional games; one may think of G as being the class of simple
games or the family of weighted voting games or any representation of simple games such as the
vector weighted voting games (Chalkiadakis et al., 2011), or the threshold network flow games due
to Bachrach and Rosenschein (2009), or even the class of all coalitional games.
A merging function on G,
G : {G = (N, v) | G  G}  (P(N) r 0)
/  G,
turns a given coalitional game G = (N, v) in suitable representation and a given nonempty coalition
S  N into a new game G (G , S) = (N 0 , v0 ), where N 0 = {i&S }  (N r S) contains a new player i&S
merging S, and v0 : P(N 0 )  R is the new coalitional function whose values are to be specified
according to the type of games in class G. For example, for weighted voting games a possible v0
has been specified in Section 3.
Similarly, a splitting function on G,
G : {G = (N, v) | G  G}  N  (N r {0, 1})  P(G),
turns a given coalitional game G = (N, v), a given player i  N, and a given integer m  2 into a
set of new games of the form (N 0 , v0 ), where player i is split into m players such that N 0 = {n +
1, . . . , n + m}  (N r {i}) and v0 : P(N 0 )  R is the new coalitional function whose values are to be
specified according to the type of games in class G. Again, for weighted voting games v0 has been
specified in Section 3, and for other classes of coalitional games, v0 needs to be suitably defined. For
example, if G is the class of monotonic coalitional games, v0 must be defined so that monotonicity is
590

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

maintained, and since there are various possibilities of doing so, various distinct splitting functions
can be defined for this class of games.
As an example, let wvg and wvg denote the merging and splitting functions for weighted voting
games as defined in Section 3. That is,
 for a weighted voting game G = (w1 , . . . , wn ; q) and a coalition S  N = {1, . . . , n}, define
wvg (G , S) = G&S , and
 given a weighted voting game G = (w1 , . . . , wn ; q), a player i, and an integer m  2, define
wvg (G , i, m) to be the set of weighted voting games Gim .
We define the following properties of merging and splitting functions.
Definition 5.1. Let G be a class of coalitional games and let G be a merging function on G and
G be a splitting function on G.
1. We say G satisfies consistency if for each G = (N, v)  G and for each coalition S  N, if
G (G , S) = (N 0 , v0 ) then v(C  S) = v0 (C  {i&S }) holds for each coalition C  N r S.
2. We say G satisfies independence if for each G = (N, v)  G and for each coalition S  N, if
G (G , S) = (N 0 , v0 ) then v(C) = v0 (C) holds for each coalition C  N r S.
3. We say G satisfies consistency if for each G = (N, v)  G, for each player i  N, and for
each integer m  2, if (N 0 , v0 )  G (G , i, m) then v(C  {i}) = v0 (C  {n + 1, . . . , n + m}) for
each coalition C  N r {i}.
4. We say G satisfies independence if for each G = (N, v)  G, for each player i  N, and for
each integer m  2, if (N 0 , v0 )  G (G , i, m) then v(C) = v0 (C) for each coalition C  N r {i}.
Intuitively, consistency means that the value of a coalition subject to merging or splitting should
be the same before and after these operations. Independence means that the value of a coalition not
affected by merging or splitting should remain the same in the new game, i.e., it depends only on
the players in this coalition. In weighted voting games, both wvg and wvg satisfy consistency and
independence, since the weight of the new player in G (G , S) equals iS wi for merging, and since
mj=1 wn+ j = wi for splitting.
The following example presents a merging function for the class of weighted majority games
such that neither consistency nor independence is satisfied.
Example 5.2. Let wmg be the merging function that maps a given weighted majority game G =
(w1 , . . . , wn ) and a given coalition S  N to a new weighted majority game, where each player not
in S keeps his or her weight, and the new player i&S merging S receives weight wi&S = iS wi .
Consider the game G = (2, 3, 4, 4) and the coalition S = {1, 3}. Then, the game wmg (G , S) =
(8, 3, 4) is formed. The value of the merged player in the new game is v0 ({i&S }) = 1, whereas the
value of S in the original game is v(S) = 0. Thus, wmg is not consistent. On the other hand, the
value of the coalition of the other players ({2, 4} in G and {2, 3} in wmg (G , S)) decreases from 1
to 0. Thus, wmg is not independent.
A similar example is obtained by using, e.g., the maximum or minimum weight of the coalitions
players instead of the product of their weights, or any other function that is not additive.
591

fiR EY & ROTHE

In general, a merging function csg on the class of constant-sum games (i.e., games G = (N, v)
such that v(C) + v(N r C) = v(N) holds for each coalition C  N) is neither consistent nor independent whenever for G = (N, v), some coalition S  N, and csg (G , S) = (N 0 , v0 ), we have that
v(S) 6= v0 ({i&S }) and v(N) = v0 (N 0 ), since then v(N r S) 6= v0 (N r {i&S }).
As pointed out by an anonymous reviewer, natural merging and splitting functions may also
exist in other important classes of coalitional games with transferable utilities where each player
posesses a certain amount of a divisible resource, such as fractional matching games, bankruptcy
games, or market games (see, e.g., Shoham & Leyton-Brown, 2009). Moreover, one could consider
the class of path-disruption games (Bachrach & Porat, 2010; Rey & Rothe, 2011, 2012; Marple,
Rey, & Rothe, 2014), where merging any two unconnected vertices might influence the value of
a coalition of other players. An approach on how to define a merging and splitting function on
network flow games can be found in Section 5.2.
5.1 Beneficial Merging and Splitting Generalized: The Case of Two Players
We can now define the beneficial merging and splitting problems in general. Let G be a merging
function and G be a splitting function on a class G of coalitional games and let PI be a power
index. Define the following generalized problems.
G -PI-B ENEFICIAL M ERGE
Given:
Question:

A game G = (N, v) in G and a nonempty coalition S  N.
Is it true that PI(G (G , S), i&S ) > iS PI(G , i), where G (G , S) = (N 0 , v0 ) with N 0 =
{i&S }  (N r S)?
G -PI-B ENEFICIAL S PLIT

Given:
Question:

A game G = (N, v) in G, N = {1, . . . , n}, a player i  N, and an integer m  2.
Is there a game G 0 = (N 0 , v0 )  G (G , i, m) with N 0 = {n + 1, . . . , n + m}  (N r {i})
such that mj=1 PI(G 0 , n + j) > PI(G , i)?

Intuitively, G -PI-B ENEFICIAL M ERGE is the problem of whether a coalition of players can benefit from merging via G by raising their power in terms of f . Similarly, G -PI-B ENEFICIAL S PLIT
is the problem of whether a player can benefit from splitting into a number of new players via G
by raising his or her power in terms of PI.
Generalizing Proposition 4.1, if consistency and independence are satisfied by the merging function, a coalition of two players cannot benefit from merging nor can a player benefit from splitting
into two players considering the probabilistic Banzhaf index.
Theorem 5.3. Let G be a merging function and let G be a splitting function, both satisfying
consistency and independence.
1. G -Banzhaf-B ENEFICIAL M ERGE is in P for instances (G , S) with kSk = 2.
2. G -Banzhaf-B ENEFICIAL S PLIT is in P for instances (G , i, 2).
Proof. Let G = (N, v) be a coalitional game and let G be a consistent and independent merging
function. Without loss of generality (see Footnote 8), let S = {n  1, n}. We obtain a new game
592

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

G (G , S) = ({1, . . . , n  1}, v0 ), where n  1 is the new player merging S in G . It holds that
Banzhaf(G (G , S), n1)  (Banzhaf(G , n1) + Banzhaf(G , n))

=

1 

 2(v0 (C  {n1})  v0 (C))
2n1 C{1,...,n2}


(v(C  {n1})  v(C))



CNr{n1,n}



(v(C  {n1})  v(C))



CNr{n1},
nC




(v(C  {n})  v(C))





CNr{n,n1}


=

1 


2n1




(v(C  {n})  v(C))

CNr{n},
n1C





 0

0
2v (C  {n1})  2v(C  {n1, n}) + 2v(C)  2v (C)  = 0.
|
{z
}
|
{z
}
C{1,...,n2}



= 0 (by consistency)

= 0 (by independence)

In the case of splitting, consider a game G = (N, v) with n players, a consistent and independent
splitting function G , and, without loss of generality, player n in G splitting into players n + 1 and
n + 2, which results in a new game G 0  G (G , n, 2). Now, it similarly holds that
Banzhaf(G 0 , n + 1) + Banzhaf(G 0 , n + 2)  Banzhaf(G , n) = 0,
q

as claimed.

In particular, this immediately implies Proposition 4.1 for wvg and wvg . As another example,
we next consider threshold network flow games on hypergraphs, a class of compactly representable
simple coalitional games.
5.2 Example: Threshold Network Flow Games on Hypergraphs
Bachrach and Rosenschein (2009) analyze threshold network flow games on graphs. A threshold
network flow game (TNFG, for short) is defined on an edge-weighted graph with n agents that each
control one edge, a source vertex s  V and a target vertex t  V , and a threshold k  R. The
coalitional function is the success function, where a coalition of agents is successful if and only if a
data flow of size k can be sent from s to t over the edges represented by the agents in the coalition.
How can merging and splitting be defined in this setting? Since agents control single edges,11
merging two or more agents would yield one new agent who controls more than one edge and so
would be qualitively different from the remaining agents. Similarly, splitting an agent into several
subagents would mean to split the original agents edge, and it is unclear how to do that. Our
approach for solving this issue is to consider threshold network flow games on hypergraphs rather
than on graphs. A hyperedge in a hypergraph is any subset of the vertex set (so a graph is the special
case of a hypergraph with hyperedges of size two only). Of course, agents in a hypergraph can have
control over hyperedges of different sizes, but that is merely a quantitative difference.
11. Kalai and Zemel (1982a, 1982b) propose a model where each agent controls a set of edges, not only a single edge.
As suggested by an anonymous reviewer, a natural merging function would then assign a merging coalition to the
union of sets of edges controlled by players in the coalition of the original game.

593

fiR EY & ROTHE

Definition 5.4. A threshold hypergraph network flow game (THNFG, for short) G = (N, v) is set
on a weighted hypergraph H = (V, E) with vertex set V and a set E = {e1 , . . . , en } of n weighted
hyperedges (where agent i represents hyperedge ei ), a weight function w : E  N (represented as a
list (w1 , . . . , wn ) with wi = w(ei )), a source vertex s  V and a target vertex t  V , and a threshold
k  R. The coalitional function v : P(N)  {0, 1} is defined by v(C) = 1 if a data flow of size k
from s to t is possible in H|C , the subhypergraph of H induced by the hyperedge set {ei | i  C}, and
v(C) = 0 otherwise.
Example 5.5. The THNFG with four agents in Figure 1 may help to visualize the definitions and
theorems given below. It shows the hypergraph (in the standard bipartite graph representation for
hypergraphs) related to the game G = ({1, . . . , 5}, v) with
G = (H, s,t, w, k) = (({v1 , . . . , v5 }, {{v4 , v5 }, {v1 , v3 , v4 }, {v2 , v3 , v5 }, {v3 , v5 }}), v3 , v5 , (1, 2, 3, 4), 5).
| {z } | {z } | {z } | {z }
e1

v1

v2

e1

e2

e3

v3

v4

e3

e2

e4

v5

e4

Figure 1: A THNFG for Example 5.5
Possible applications of THNFGs can be found in grid computing where manyoften thousands ofcomputers collaborate to solve a common task by distributing various subtasks to certain
clusters of computers, which represent the agents (respectively, the hyperedges). Connecting a number of such computer clusters corresponds to forming a coalition of agents. Modeled as a game, a
coalition (i.e., a set of clusters) is successful if it connects the source to the target and allows a
sufficient network data flow within the capacity of the computers in the clusters, and in this case
these clusters can be assigned the desired subtask. As another example of a possible application of
THNFGs, we propose to use this model for smart power grids that deliver electricity from suppliers
to consumers. Success of a coalition would mean here that a certain threshold is to be exceeded
according to the consumers current demands, thus allowing a sufficiently large power flow.
For TNFGs, determining the raw Banzhaf index is #P-complete, while the ShapleyShubik
index is only known to be at least as hard as problems in NP (Bachrach & Rosenschein, 2009).
However, for THNFGs, the following results are consequences of the corresponding results for
weighted voting games. The easy hardness proof simply reduces the problem for WVGs to the
corresponding problem for THNFGs by mapping a given WVG G = (w1 , . . . , wn , q) to the THNFG
H = (({v0 , v1 , . . . , vn+1 }, {e1 , . . . en }), v0 , vn+1 , (w1 , . . . , wn ), q),
where ei = {v0 , vi , vn+1 } for each i, 1  i  n, with the same weights and threshold. Since the value
of each coalition C in G equals the value of C in H , we have Banzhaf (G , i) = Banzhaf (H , i)
594

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

and ShapleyShubik (G , i) = ShapleyShubik (H , i) for each player i. Since this reduction is parsimonious, #P-parsimonious-hardness of Banzhaf for THNFGs is inherited from that for WVGs,
and #P-many-one-hardness of ShapleyShubik for THNFGs is inherited from that for WVGs.
Proposition 5.6. Computing the raw Banzhaf power index in THNFGs is #P-parsimonious-complete,
while computing the raw ShapleyShubik power index in THNFGs is #P-many-one-complete.
The power compare problem for weighted voting games with respect to a power index PI
(originally introduced by Faliszewski & Hemaspaandra, 2009) can be defined analogously for
THNFGs:
PI-THNFG-P OWER C OMPARE
Given:
Question:

Two THNFGs, G and G 0 , and a player i occurring in both games.
Is is true that PI(G , i) > PI(G 0 , i)?

By suitably extending the reduction given right above Proposition 5.6, we have the following.
Corollary 5.7. PI-THNFG-P OWER C OMPARE is PP-hard for PI  {ShapleyShubik, Banzhaf}.
The use of hyperedges makes it possible that, in THNFGs, a coalition of agents can be merged
into a single new agent who controls the hyperedge that corresponds to the union of vertices belonging to the hyperedges of the coalitions original agents. In the example of grid computing, a merge
of two clusters allows new connections as well as the sum of the computing power of the computers
in the clusters as a total weight. Similarly, it is possible for an agent in such a setting to split into
several subagents by partitioning this agents hyperedge into subsets that each are controlled by one
of the new subagents. We define the merging function and the splitting function for THNFGs as
follows:
 The merging function thnfg on THNFGs maps a given THNFG G = (H, s,t, w, k), with hypergraph H = (V, E), and a given coalition S of agents to the new THNFG thnfg (G , S) =
(H&S , s,t, w&S , k), where the new hypergraph is H&S = (V, E&S ) with the new set of hyperS
edges E&S = (E r {ei | i  S})  {e&S }, the new agent i&S controls hyperedge e&S = iS ei ,
and the new weight function w&S is given by w&S (ei ) = wi for i 6 S, and w&S (e&S ) = iS wi .
 The splitting function thnfg on THNFGs maps a given THNFG G = (H, s,t, w, k), with
hypergraph H = (V, E), a given agent i, and a given integer m  2 to the new THNFG
thnfg (G , i, m) = (Him , s,t, wim , k), where the new hypergraph is Him = (V, Eim ) with
Eim = (E r {ei })  {en+1 , . . . , en+m }, agent i is split into m agents n + 1, . . . , n + m such
S
that mj=1 en+ j = ei and en+ j  e` = 0/ for `  {1, . . . , n} r {i}, and the new weight function
wim is given by wim (e` ) = w` for ` 6= i, and the new agents weights wn+ j = wim (en+ j ),
1  j  m, satisfy mj=1 wn+ j = wi .
In contrast to weighted voting games, consistency is not satisfied in general for THNFGs, neither
by thnfg nor by thnfg . On the one hand, merging two agents via thnfg can create new connections
between vertices and thus allows new data flows to emerge. On the other hand, existing connections
can get lost by splitting an agent via thnfg (i.e., splitting the corresponding hyperedge). Therefore,
merging and splitting by thnfg and thnfg might be advantageous for the probabilistic Banzhaf index,
even for size-two coalitions or a split into two players.
595

fiR EY & ROTHE

Example 5.8 (continuing Example 5.5). Consider again the THNFG G from Example 5.5. Merging
the agents of coalition S = {1, 3} via thnfg , allows new connections, but is disadvantagous for the
probabilistic Banzhaf index in the new game
thnfg (G , {1, 3}) = (({v1 . . . , v5 }, {{v2 , v3 , v4 , v5 }, {v1 , v3 , v4 }, {v3 , v5 }}), v3 , v5 , (4, 2, 4), 5).
|
{z
} | {z } | {z }
e&{1,3}

e2

e4

However, merging the agents of coalition S = {1, 2} via thnfg , is beneficial for the probabilistic
Banzhaf index in the new game
thnfg (G , {1, 2}) = (({v1 . . . , v5 }, {{v1 , v3 , v4 , v5 }, {v2 , v3 , v5 }, {v3 , v5 }}), v1 , v5 , (3, 3, 4), 5).
|
{z
} | {z } | {z }
e&{1,2}

e3

e4

For comparison, let G 0 be the corresponding weighted voting game with G 0 = (1, 2, 3, 4; 5) (i.e., with
0
the same weights and threshold). Merging the agents of S = {1, 3} in G 0 via wvg (G 0 , S) = G&{1,3}
=
0
0
0
(4, 2, 4; 5) as well as merging the agents of S = {1, 2} in G via wvg (G , S) = G&{1,2} = (3, 3, 4; 5)
are neutral for the probabilistic Banzhaf index.
However, just as wvg and wvg for weighted voting games, both thnfg and thnfg satisfy independence for THNFGs: The value of a coalition only depends on the hyperedges of the agents
within the coalition, not on other hyperedges that might have been merged or split.
5.3 Merging and Splitting in Unanimity Games
A simple game G = (N, v) is called a unanimity game if only the grand coalition wins, i.e., v(C) = 1
if C = N, and v(C) = 0 if C ( N. For example, a weighted voting game G = (w1 , . . . , wn ; q) is a
unanimity weighted voting game if and only if ni=1 wi  miniN wi < q  ni=1 wi .
There is only one possible merging function for unanimity games. Let G be a unanimity game
and let S  N be a coalition. Define ug (G , S) = (N 0 , v0 ) by N 0 = {i&S }  (N r S) and v0 (C) = 1 if
C = N 0 , and v0 (C) = 0 if C ( N 0 . Obviously, ug satisfies consistency and independence.
For unanimity weighted voting games, Aziz et al. (2011) show that for the normalized Banzhaf
index, merging is always disadvantageous, whereas splitting is always advantageous. (Thus one can
decide in polynomial time whether or not merging or splitting is beneficial.) In strong contrast, we
show that in unanimity games with respect to the probabilistic Banzhaf index, splitting is always
disadvantageous or neutral, whereas merging is neutral for size-two coalitions, yet advantageous for
coalitions with at least three players.
Theorem 5.9. Let G be a unanimity game with player set N.
1. (G , S) 6 ug -Banzhaf-B ENEFICIAL M ERGE for each S  N with kSk = 2,
2. (G , S)  ug -Banzhaf-B ENEFICIAL M ERGE for each S  N with kSk  3.
3. (G , i, m) 6 ug -Banzhaf-B ENEFICIAL S PLIT for each i  N and m  2.
Proof. The first statement follows immediately from Theorem 5.3.
To prove the second statement, note that in a unanimity game, any player i can be pivotal only
for the coalition S = N r {i}, and i always is pivotal for this coalition. Thus the raw Banzhaf
596

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

index of each i is always equal to one. It follows that Banzhaf(G , i) = 1/2n1 for each player i  N.
If an arbitrary coalition S merges, the Banzhaf index of a player i in the new game ug (G , S) is
Banzhaf(ug (G , S), i) = 1/2nkSk . Since kSk  3, we have
Banzhaf(ug (G , S), i&S )   Banzhaf(G , i) =
iS

2kSk1  kSk
> 0.
2n1

The third statement can be shown by similar arguments. In particular, for any possible split into
players with integer weights, we have
m

 Banzhaf(ug (G , i, m), n + j)  Banzhaf(G , i) =

j=1

m  2m1
 0.
2n+m2
q

This completes the proof.

6. Conclusions and Future Work
We have analyzed the beneficial merging, splitting, and annexation problems in terms of their complexity. In particular, our results complementby considering the probabilistic Banzhaf power
indexthose by Aziz et al. (2011) for the normalized Banzhaf power index in weighted voting
games. To some extent, these results differ from those for the normalized Banzhaf power index: For
the probabilistic Banzhaf power index, beneficial merging and splitting turns out to be tractable for
a merger of size-two coalitions and a split into two players.
One of our main results is that, solving previous conjectures in the affirmative, we have pinpointed the precise complexity of the beneficial merging problem in weighted voting games for the
ShapleyShubik and the probabilistic Banzhaf index by showing that it is PP-complete in both cases.
On the one hand, this result is interesting in itself from a theoretical point of view. On the other
hand, this provides a PP-completeness result for a natural problem from game theory, which are by
far rarer than NP-complete problems in this field. Since merging can be seen as manipulative behavior, a high complexity can be interpreted as a protection shield against such strategic interference.
While there are several known methods to circumvent NP-hardnesssuch as approximation, fixedparameter tractability, typical case analyses (for a discussion of applying such methods to NP-hard
voting problems see, e.g., Rothe & Schend, 2013), or a recent algebraic approach (Berghammer &
Schnoor, 2014), such methods are less applicable to circumvent hardness for higher complexity
classes. Since PP is considered to be a much larger complexity class than NP, PP-hardness can be
seen as leading to a potentially higher degree of protection than mere NP-hardness. Still, the hardness of our problems rests on the hardness to compute the related power indices. Note that there
are good approximation schemes and dynamic methods known for computing the ShapleyShubik
index (see, e.g., Bachrach, Markakis, Resnik, Procaccia, Rosenschein, & Saberi, 2010; Fatima,
Wooldridge, & Jennings, 2008; Bilbao, Fernndez, Jimnez, & Lpez, 2000; Matsui & Matsui,
2000; Shapley, 1953), though there is not much known about the exact case.
We have obtained the same PP-completeness result for beneficial splitting (a.k.a. false-name
manipulation) whenever the new players weights are given. For a given number of false identities,
but unknown weights, we raised the lower bound (known for the ShapleyShubik index) from NPhardness to PP-hardness and showed that it is contained in NPPP whenever the number of false
597

fiR EY & ROTHE

identities is given in unary. For this problem, it remains open whether it can be shown to be complete
for NPPP , a huge complexity class thatby Todas theorem (1991)contains the entire polynomial
hierarchy. NPPP is an interesting class, but somewhat sparse in natural complete problems. The only
(natural) NPPP -completeness results we are aware of are due to Littman et al. (1998), who analyze
a variant of the satisfiability problem and questions related to probabilistic planning, and due to
Mundhenk et al. (2000), who study problems related to finite-horizon Markov decision processes.
Another interesting open question is whether our results can be transferred also to the beneficial
merging and splitting problems for the normalized Banzhaf index or other power indices.
For the beneficial annexation problem with a takeover of a single player, we showed NPcompleteness for both the ShapleyShubik and the probabilistic Banzhaf index.
Finally, we have proposed a general framework for merging and splitting that can be applied
to various classes of coalitional games with transferable utilities. An interesting task for future
research is to study useful properties of merging and splitting functions, such as consistency and
independence, in general and when applied to particular classes of games like network flow games or
market games. Another interesting question, raised by an anonymous reviewer, is how to naturally
extend the idea of merging to classes of games where players control several resources. Which
properties do we want to hold in that case? Also, can a merging function that satisfies independence
and consistence be unique for a certain class of games? For unanimity games we have observed that
there is only one possible merging function that guarantees unanimity. For weighted voting games,
however, such a uniqueness result does not hold, since there are different ways to distribute the
players weights that lead to the same coalitional function. For instance, the games (1, 3, 4; 8) and
(2, 3, 4; 8) are semantically the same, even if players merge. Restricting to other classes or requiring
other properties might imply uniqueness. Although consistency seems to be an essential property
for a merging or splitting function, we have seen a natural merging function on threshold hypergraph
network flow games that does not satisfy this property, and we made similar observations for other
classes of games.

Acknowledgments
Preliminary versions of parts of this paper appear in the proceedings of the 19th European Conference on Artificial Intelligence (ECAI10) (Rey & Rothe, 2010a), the 5th European Starting AI
Researcher Symposium (STAIRS10) (Rey & Rothe, 2010b), and the 11th Latin American Theoretical Informatics Symposium (LATIN14) (Rey & Rothe, 2014). We are grateful to the anonymous
JAIR, ECAI10, STAIRS10, LATIN14, and CoopMAS14 reviewers for their helpful comments
on this paper. This work was supported in part by DFG grants RO 1202/11-1, RO 1202/12-1 (within
the ESF EUROCORES program LogICCC), and RO 1202/14-1.

References
Aziz, H., Bachrach, Y., Elkind, E., & Paterson, M. (2011). False-name manipulations in weighted
voting games. Journal of Artificial Intelligence Research, 40, 5793.
Aziz, H., Brandt, F., & Brill, M. (2013). The computational complexity of random serial dictatorships. Economic Letters, 121(3), 341345.
598

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

Aziz, H., & Paterson, M. (2009). False name manipulations in weighted voting games: Splitting,
merging and annexation. In Proceedings of the 8th International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 409416. IFAAMAS.
Bachrach, Y., & Elkind, E. (2008). Divide and conquer: False-Name manipulations in weighted voting games. In Proceedings of the 7th International Joint Conference on Autonomous Agents
and Multiagent Systems, pp. 975982. IFAAMAS.
Bachrach, Y., Elkind, E., Meir, R., Pasechnik, D., Zuckerman, M., Rothe, J., & Rosenschein, J.
(2009). The cost of stability in coalitional games. In Proceedings of the 2nd International
Symposium on Algorithmic Game Theory, pp. 122134. Springer-Verlag Lecture Notes in
Computer Science #5814.
Bachrach, Y., Markakis, E., Resnik, E., Procaccia, A., Rosenschein, J., & Saberi, A. (2010). Approximating power indices: Theoretical and empirical analysis. Journal of Autonomous Agents and
Multi-Agent Systems, 20(2), 105122.
Bachrach, Y., & Porat, E. (2010). Path disruption games. In Proceedings of the 9th International
Joint Conference on Autonomous Agents and Multiagent Systems, pp. 11231130. IFAAMAS.
Bachrach, Y., & Rosenschein, J. (2009). Power in threshold network flow games. Journal of Autonomous Agents and Multi-Agent Systems, 18(1), 106132.
Banzhaf III, J. (1965). Weighted voting doesnt work: A mathematical analysis. Rutgers Law
Review, 19, 317343.
Berghammer, R., & Schnoor, H. (2014). Control of condorcet voting: Complexity and a relationalgebraic approach (extended abstract). In Proceedings of the 13th International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 13651366. IFAAMAS.
Bilbao, J., Fernndez, J., Jimnez, N., & Lpez, J. (2000). Generating functions for computing
power indices efficiently. Top, 8(2), 191213.
Bilbao, J., Fernndez, J., Jimnez, N., & Lpez, J. (2002). Voting power in the European Union
enlargement. European Journal of Operational Research, 143(1), 181196.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational Aspects of Cooperative
Game Theory. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan
and Claypool Publishers.
Deng, X., & Papadimitriou, C. (1994). On the complexity of comparative solution concepts. Mathematics of Operations Research, 4(2), 257266.
Dubey, P., & Shapley, L. (1979). Mathematical properties of the Banzhaf power index. Mathematics
of Operations Research, 4(2), 99131.
Elkind, E., Chalkiadakis, G., & Jennings, N. (2008). Coalition structures in weighted voting games.
In Proceedings of the 18th European Conference on Artificial Intelligence, pp. 393397. IOS
Press.
Elkind, E., Goldberg, L., Goldberg, P., & Wooldridge, M. (2009). On the computational complexity
of weighted voting games. Annals of Mathematics and Artificial Intelligence, 56(2), 109131.
599

fiR EY & ROTHE

Elkind, E., Pasechnik, D., & Zick, Y. (2013). Dynamic weighted voting games. In Proceedings of
the 12th International Joint Conference on Autonomous Agents and Multiagent Systems, pp.
515522. IFAAMAS.
Faliszewski, P., & Hemaspaandra, L. (2009). The complexity of power-index comparison. Theoretical Computer Science, 410(1), 101107.
Fatima, S., Wooldridge, M., & Jennings, N. (2008). A linear approximation method for the shapley
value. Artificial Intelligence, 172(14), 16731699.
Felsenthal, D., & Machover, M. (1995). Postulates and paradoxes of relative voting power  A
critical re-appraisal. Theory and Decision, 38(2), 195229.
Felsenthal, D., & Machover, M. (2005). Voting power measurement: A story of misreinvention.
Social Choice and Welfare, 25(2), 485506.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman and Company.
Gill, J. (1977). Computational complexity of probabilistic Turing machines. SIAM Journal on
Computing, 6(4), 675695.
Hunt, H., Marathe, M., Radhakrishnan, V., & Stearns, R. (1998). The complexity of counting
problems. SIAM Journal on Computing, 27(4), 11421167.
Kalai, E., & Zemel, E. (1982a). Generalized network problems yielding totally balanced games.
Operations Research, 30(5), 9981008.
Kalai, E., & Zemel, E. (1982b). Totally balanced games and games of flow. Mathematics of Operations Research, 7(3), 476478.
Karp, R. (1972). Reducibility among combinatorial problems. In Miller, R., & Thatcher, J. (Eds.),
Complexity of Computer Computations, pp. 85103. Plenum Press.
Littman, M., Goldsmith, J., & Mundhenk, M. (1998). The computational complexity of probabilistic
planning. Journal of Artificial Intelligence Research, 9(1), 136.
Marple, A., Rey, A., & Rothe, J. (2014). Bribery in multiple-adversary path-disruption games is
hard for the second level of the polynomial hierarchy (extended abstract). In Proceedings
of the 13th International Joint Conference on Autonomous Agents and Multiagent Systems.
IFAAMAS. To appear.
Matsui, T., & Matsui, Y. (2000). A survey of algorithms for calculating power indices of weighted
majority games. Journal of the Operation Research Society of Japan, 43(1), 7186.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (2000). Complexity results for finitehorizon Markov decision process problems. Journal of the ACM, 47(4), 681720.
Nisan, N., Roughgarden, T., Tardos, ., & Vazirani, V. (Eds.). (2007). Algorithmic Game Theory.
Cambridge University Press.
Papadimitriou, C. (1995). Computational Complexity (Second edition). Addison-Wesley.
Peleg, B., & Sudhlter, P. (2003). Introduction to the Theory of Cooperative Games. Kluwer
Academic Publishers.
Penrose, L. (1946). The elementary statistics of majority voting. Journal of the Royal Statistical
Society, 109(1), 5357.
600

fiFALSE -NAME M ANIPULATION IN WVG S IS H ARD FOR PP

Prasad, K., & Kelly, J. (1990). NP-completeness of some problems concerning voting games. International Journal of Game Theory, 19(1), 19.
Rey, A., & Rothe, J. (2010a). Complexity of merging and splitting for the probabilistic Banzhaf
power index in weighted voting games. In Proceedings of the 19th European Conference on
Artificial Intelligence, pp. 10211022. IOS Press.
Rey, A., & Rothe, J. (2010b). Merging and splitting for power indices in weighted voting games
and network flow games on hypergraphs. In Proceedings of the 5th European Starting AI
Researcher Symposium, pp. 277289. IOS Press.
Rey, A., & Rothe, J. (2011). Bribery in path-disruption games. In Proceedings of the 2nd International Conference on Algorithmic Decision Theory, pp. 247261. Springer-Verlag Lecture
Notes in Artificial Intelligence #6992.
Rey, A., & Rothe, J. (2012). Probabilistic path-disruption games. In Proceedings of the 20th European Conference on Artificial Intelligence, pp. 923924. IOS Press. An extended version
appears in the proceedings of the 6th European Starting AI Researcher Symposium, IOS Press,
pages 264269, August 2012.
Rey, A., & Rothe, J. (2014). False-name manipulation in weighted voting games is hard for probabilistic polynomial time. In Proceedings of the 11th Latin American Theoretical Informatics
Symposium, pp. 6071. Springer-Verlag Lecture Notes in Computer Science #8392.
Rothe, J. (2005). Complexity Theory and Cryptology. An Introduction to Cryptocomplexity. EATCS
Texts in Theoretical Computer Science. Springer-Verlag.
Rothe, J., & Schend, L. (2013). Challenges to complexity shields that are supposed to protect
elections against manipulation and control: A survey. Annals of Mathematics and Artificial
Intelligence, 68(13), 161193.
Shapley, L. (1953). A value for n-person games. In Kuhn, H., & Tucker, A. (Eds.), Contributions
to the Theory of Games, Vol. II of Annals of Mathematics Studies 40. Princeton University
Press.
Shapley, L., & Shubik, M. (1954). A method of evaluating the distribution of power in a committee
system. The American Political Science Review, 48(3), 787792.
Shoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems. Algorithmic, Game-Theoretic, and
Logical Foundations. Cambridge University Press.
Toda, S. (1991). PP is as hard as the polynomial-time hierarchy. SIAM Journal on Computing,
20(5), 865877.
Valiant, L. (1979). The complexity of computing the permanent. Theoretical Computer Science,
8(2), 189201.
Wagner, K. (1986). The complexity of combinatorial problems with succinct input representations.
Acta Informatica, 23, 325356.
Zank, V. (1991). #P-completeness via many-one reductions. International Journal of Foundations
of Computer Science, 2(1), 7682.
Zuckerman, M., Faliszewski, P., Bachrach, Y., & Elkind, E. (2012). Manipulating the quota in
weighted voting games. Artificial Intelligence, 180-181(0), 119.

601

fiJournal of Artificial Intelligence Research 50 (2014) 369-407

Submitted 10/13; published 6/14

HC-Search: A Learning Framework for Search-based
Structured Prediction
Janardhan Rao Doppa

doppa@eecs.oregonstate.edu

School of EECS, Oregon State University
Corvallis, OR 97331-5501, USA

Alan Fern

afern@eecs.oregonstate.edu

School of EECS, Oregon State University
Corvallis, OR 97331-5501, USA

Prasad Tadepalli

tadepall@eecs.oregonstate.edu

School of EECS, Oregon State University
Corvallis, OR 97331-5501, USA

Abstract
Structured prediction is the problem of learning a function that maps structured inputs
to structured outputs. Prototypical examples of structured prediction include part-ofspeech tagging and semantic segmentation of images. Inspired by the recent successes of
search-based structured prediction, we introduce a new framework for structured prediction
called HC-Search. Given a structured input, the framework uses a search procedure guided
by a learned heuristic H to uncover high quality candidate outputs and then employs a
separate learned cost function C to select a final prediction among those outputs. The
overall loss of this prediction architecture decomposes into the loss due to H not leading
to high quality outputs, and the loss due to C not selecting the best among the generated
outputs. Guided by this decomposition, we minimize the overall loss in a greedy stage-wise
manner by first training H to quickly uncover high quality outputs via imitation learning,
and then training C to correctly rank the outputs generated via H according to their true
losses. Importantly, this training procedure is sensitive to the particular loss function of
interest and the time-bound allowed for predictions. Experiments on several benchmark
domains show that our approach significantly outperforms several state-of-the-art methods.

1. Introduction
We consider the problem of structured prediction, where the predictor must produce a
structured output given a structured input. For example, in Part-Of-Speech (POS) tagging, the structured input is a sequence of words and structured output corresponds to the
POS tags for those words. Image scene labeling is another example, where the structured
input is an image and the structured output is a semantic labeling of the image regions.
Structured prediction tasks such as above arise in several domains ranging from natural
language processing (e.g., named entity recognition, coreference resolution, and semantic
parsing) and computer vision (e.g., multi-object tracking and activity recognition in videos)
to speech (e.g., text-to-speech mapping and speech recognition) and compuational biology
(e.g., protein secondary structure prediction and gene prediction).
Viewed as a traditional classification problem, the set of possible classes in structured
prediction is exponential in the size of the input. Thus, the problem of producing an
c
2014
AI Access Foundation. All rights reserved.

fiDoppa, Fern, & Tadepalli

output is combinatorial in nature, which introduces the non-trivial choice of selecting a
computational framework for producing outputs. Importantly, this framework needs to
balance two conflicting criteria: 1) It must be flexible enough to allow for complex and
accurate structured predictors to be learned, and 2) It must support inference of outputs
within the computational time constraints of an application. One of the core research
challenges in structured prediction has been to achieve a balance between these criteria.
A standard approach to structured prediction is to learn a cost function C(x, y) for
scoring a potential structured output y given a structured input x. Given such a cost
function and a new input x, the output computation involves solving the so-called Argmin
problem, which is to find the minimum cost output for a given input.
y = arg minyY(x) C(x, y)

(1)

For example, approaches such as Conditional Random Fields (CRFs) (Lafferty, McCallum,
& Pereira, 2001), Max-Margin Markov Networks (Taskar, Guestrin, & Koller, 2003) and
Structured SVMs (Tsochantaridis, Hofmann, Joachims, & Altun, 2004) represent the cost
function as a linear model over template features of both x and y. Unfortunately, exactly
solving the Argmin problem is often intractable. Efficient solutions exist only in limited
cases such as when the dependency structure among features forms a tree. In such cases, one
is forced to simplify the features to allow for tractable inference, which can be detrimental
to prediction accuracy. Alternatively, a heuristic optimization method can be used such
as loopy belief propagation or variational inference. While such methods have shown some
success in practice, it can be difficult to characterize their solutions and to predict when
they are likely to work well for a new problem.
We are inspired by the recent successes of output-space search approaches (Doppa, Fern,
& Tadepalli, 2012; Wick, Rohanimanesh, Bellare, Culotta, & McCallum, 2011), which place
few restrictions on the form of the cost function. These methods learn and use a cost
function to conduct a search through the space of complete outputs via a search procedure
(e.g., greedy search), and return the least cost output that is uncovered during the search as
the prediction. The search procedure only needs to be able to efficiently evaluate the cost
function at specific input-output pairs, which is generally straightforward even when the
corresponding Argmin problem is intractable. Thus, these methods are free to increase the
complexity of the cost function without considering its impact on the inference complexity.
While these approaches have achieved state-of-the-art performance on a number of
benchmark problems, a primary contribution of this paper is to highlight a fundamental
deficiency that they share. In particular, prior work uses a single cost function to serve the
dual roles of both: 1) guiding the search toward good outputs, and 2) scoring the generated
outputs in order to select the best one. Serving these dual roles often means that the cost
function needs to make unclear tradeoffs, increasing the difficulty of learning. Indeed, in
the traditional AI search literature, these roles are typically served by different functions,
mainly a heuristic function for guiding search, and a cost/evaluation function (often part
of the problem definition) for selecting the final output.
In this paper, we study a new framework for structured prediction called HC-Search that
closely follows the traditional search literature. The key idea is to learn distinct functions
for each of the above roles: 1) a heuristic function H to guide the search and generate a set
of high-quality candidate outputs, and 2) a cost function C to score the outputs generated
370

fiHC-Search: A Learning Framework for Search-based Structured Prediction

by the heuristic H. Given a structured input, predictions are made by using H to guide a
search strategy (e.g., greedy search or beam search) until a time bound to generate a set of
candidate outputs and then returning the generated output of least cost according to C.
While the move to HC-Search might appear to be relatively small, there are significant
implications in terms of both theory and practice. First, the regret of the HC-Search
approach can be decomposed into the loss due to H not leading to high quality outputs, and
the loss due to C not selecting the best among the generated outputs. This decomposition
helps us target our training to minimize each of these losses individually in a greedy stagewise manner. Second, as we will show, the performance of the approaches with a single
function can be arbitrarily bad when compared to that of HC-Search in the worst case.
Finally, we show that in practice HC-Search performs significantly better than the single
cost function search and other state-of-the-art approaches to structured prediction.
The effectiveness of the HC-Search approach for a particular problem depends critically
on: 1) the quality of the search space over complete outputs being used, where quality is
defined as the expected depth at which target outputs (zero loss outputs) can be located, 2)
our ability to learn a heuristic function for effectively guiding the search to generate highquality candidate outputs, and 3) the accuracy of the learned cost function in selecting the
best output among the candidate outputs generated by the heuristic function. In this work,
we assume the availability of an efficient search space over complete outputs and provide an
effective training regime for learning both heuristic function and cost function within the
HC-Search framework.

1.1 Summary of Contributions
The main contributions of our work are as follows: 1) We introduce the HC-Search framework, where two different functions are learned to serve the purposes of search heuristic
and cost function as in the search literature; 2) We analyze the representational power and
computational complexity of learning within the HC-Search framework; 3) We identify a
novel decomposition of the overall regret of the HC-Search approach in terms of generation
loss, the loss due to heuristic not generating high-quality candidate outputs, and selection
loss, the loss due to cost function not selecting the best among the generated outputs; 4)
Guided by the decomposition, we propose a stage-wise approach to learning the heuristic
and cost functions based on imitation learning; 5) We empirically evaluate the HC-Search
approach on a number of benchmarks, comparing it to state-of-the-art methods and analyzing different dimensions of the framework.
The remainder of the paper proceeds as follows. In Section 2, we introduce our problem
setup, give a high-level overview of our framework, and analyze the complexity of HC-Search
learning problem. We describe our approaches to heuristic and cost function learning in
Section 3. Section 4 presents our experimental results followed by an engineering methodology for applying our framework to new problems in Section 5. Finally, Sections 6 and 7
discuss related work and future directions.
371

fiDoppa, Fern, & Tadepalli

2. HC-Search Framework
In this section, we first state the formal problem setup and then describe the specifics of
the search spaces and search strategies that we will investigate in this work. Next, we give
a high-level overview of our HC-Search framework along with its learning objective.
2.1 Problem Setup
A structured prediction problem specifies a space of structured inputs X , a space of structured outputs Y, and a non-negative loss function L : X Y Y 7 <+ such that L(x, y 0 , y  )
is the loss associated with labeling a particular input x by output y 0 when the true output is y  . We are provided with a training set of input-output pairs {(x, y  )} drawn from
an unknown target distribution D. The goal is to return a function/predictor from structured inputs to outputs whose predicted outputs have low expected loss with respect to
the distribution D. Since our algorithms will be learning heuristic and cost functions over
input-output pairs, as is standard in structured prediction, we assume the availability of a
feature function  : X  Y 7 <n that computes an n dimensional feature vector for any
pair. Importantly, we can employ two different feature functions H and C for heuristic
and cost function noting that they are serving two different roles: the heuristic is making
local decisions to guide the search towards high-quality outputs and the cost function is
making global decisions by scoring the candidate outputs generated by the heuristic in this
framework.
2.2 Search Spaces and Search Strategies
We overview some basic search concepts in the context of our search-based framework below.
2.2.1 Search Spaces
Our approach is based on search in a space So of complete outputs, which we assume to
be given. Every state in a search space over complete outputs consists of an input-output
pair (x, y), representing the possibility of predicting y as the output for structured input
x. Such a search space is defined in terms of two functions: 1) An initial state function I
such that I(x) returns an initial state for input x, and 2) a successor function S such that
for any search state (x, y), S((x, y)) returns a set of next states {(x, y1 ),    , (x, yk )} that
share the same input x as the parent. For example, in a sequence labeling problem, such
as part-of-speech tagging, (x, y) is a sequence of words and corresponding part-of-speech
(POS) labels. The successors of (x, y) might correspond to all ways of changing one of the
output labels in y, the so-called flipbit space. Figure 1 provides an illustration of the
flipbit search space for the handwriting recognition task.
Search Space Quality. The effectiveness of our HC-Search framework depends on
the quality of the search space that is used. The quality of a search space can in turn be
understood in terms of the expected amount of search needed to uncover the correct output
y  . For most search procedures, the time required to find a target output y  will grow as
a function of the depth of the target. Thus, one way to quantify the expected amount of
search, independently of the specific search strategy, is by considering the expected depth
of target outputs y  . In particular, for a given input-output pair (x, y  ), the target depth d
372

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Figure 1: An example Flipbit search space for the handwriting recognition problem. Each
search state consists of a complete input-output pair and the complete output at
every state differs from that of its parent by exactly one label. The highlighted
state corresponds to the one with true output y  at the smallest depth, which is
equal to the number of errors in the initial state.

373

fiDoppa, Fern, & Tadepalli

is defined as the minimum depth at which we can find a state corresponding to the target
output y  (d = 5 in the example flipbit space shown in Figure 1). Clearly according to this
definition, the expected target depth of the flipbit space is equal to the expected number of
errors in the output corresponding to the initial state.
A variety of search spaces, such as the above flipbit space, Limited Discrepancy Search
(LDS) space (Doppa et al., 2012), and those defined based on hand-designed proposal
distributions (Wick et al., 2011) have been used in past research. While our work applies to
any such space, we will focus on the LDS space in our experiment, which has been shown to
effectively uncover high-quality outputs at relatively shallow search depths (Doppa et al.,
2012).
The LDS space is defined in terms of a recurrent classifier h which uses the next input
token, e.g. word, and output tokens in a small preceding window, e.g. POS labels, to
predict the next output token. The initial state of the LDS space consists of the input
x paired with the output of the recurrent classifier h on x. One problem with recurrent
classifiers is that when a recurrent classifier makes a mistake, its effects get propagated
to down-stream tokens. The LDS space is designed to prevent this error propagation by
immediately correcting the mistakes made before continuing with the recurrent classifier.
Since we do not know where the mistakes are made and how to correct them, all possible
corrections, called discrepancies, are considered. Hence the successors of any state (x, y) in
the LDS space consist of the results of running the recurrent classifier after changing exactly
one more label, i.e., introducing a single new discrepancy, somewhere in the current output
sequence y while preserving all previously introduced discrepancies. In previous work, the
LDS space has been shown to be effective in uncovering high-quality outputs at relatively
shallow search depths, as one would expect with a good recurrent classifier (Doppa et al.,
2012). The Appendix contains more details and examples of the LDS space we employ in
this work.
2.2.2 Search Strategies
Recall that in our HC-Search framework, the role of the search procedure is to uncover highquality outputs. We can consider both uninformed and informed search strategies. However,
uninformed search procedures like depth bounded breadth-first search will only be practical
when high-quality outputs exist at small depths and even when they are feasible, they are
not a good choice because they dont use the search time bound in an intelligent way to
make predictions. For most structured prediction problems, informed search strategies that
take heuristic functions into account, such as greedy search or best-first search are a better
choice, noting that their effectiveness depends the quality of the search heuristic H. Prior
work (Doppa et al., 2012; Wick et al., 2011) has shown that greedy search (hill climbing
based on the heuristic value) works quite well for a number of structured prediction tasks
when used with an effective search space. Thus, in this work, we focus our empirical work
on the HC-Search framework using greedy search, though the approach applies more widely.
2.3 HC-Search Approach
Our approach is parameterized by a search space over complete outputs So (e.g., LDS
space), a heuristic search strategy A (e.g., greedy search), a learned heuristic function
374

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Figure 2: A high level overview of our HC-Search framework. Given a structured input x
and a search space definition So , we first instantiate a search space over complete
outputs. Each search node in this space consists of a complete input-output pair.
Next, we run a search procedure A (e.g., greedy search) guided by the heuristic
function H for a time bound  . The highlighted nodes correspond to the search
trajectory traversed by the search procedure, in this case greedy search. The
scores on the nodes correspond to cost values, which are different from heuristic scores (not shown in the figure). We return the least cost output y that is
uncovered during the search as the prediction for input x.

H : X  Y 7 <, and a learned cost function C : X  Y 7 <. Given an input x and a
prediction time bound  , HC-Search makes predictions as follows. It traverses the search
space starting at I(x) using the search procedure A guided by the heuristic function H until
the time bound is exceeded. Then the cost function C is applied to find return the least-cost
output y that is generated during the search as the prediction for input x. Figure 2 gives a
high-level overview of our HC-Search framework.
More formally, let YH (x) be the set of candidate outputs generated using heuristic H
for a given input x. The output returned by HC-Search is y the least cost output in this
set according to C, i.e.,
y = arg minyYH (x) C(x, y)
375

fiDoppa, Fern, & Tadepalli

Figure 3: An example that illustrates that C-Search can suffer arbitrarily large loss compared to HC-Search.

The expected loss of the HC-Search approach E(H, C) for a given heuristic H and C can be
defined as
E (H, C) = E(x,y )D L (x, y, y  )
(2)
Our goal is to learn a heuristic function H and corresponding cost function C  that minimize
the expected loss from their respective spaces H and C, i.e.,
(H , C  ) = arg min(H,C)HC E (H, C)

(3)

In contrast to our framework, existing approaches for output space search (Doppa et al.,
2012; Wick et al., 2011) use a single function (say C) to serve the dual purpose of heuristic
and cost function. This raises the question of whether HC-Search, which uses two different functions, is strictly more powerful in terms of its achievable losses. The following
proposition shows that the expected loss of HC-Search can be arbitrarily smaller than when
restricting to using a single function C.
Proposition 1. Let H and C be functions from the same function space. Then for all
learning problems, minC E(C, C)  min(H,C) E(H, C). Moreover there exist learning problems
for which minC E(C, C) is arbitrarily larger (i.e. worse) than min(H,C) E(H, C).
Proof. The first part of the proposition follows from the fact that the first minimization is
over a subset of the choices considered by the second.
To see the second part, consider a problem with a single training instance with search
space shown in Figure 3. The search procedure will be greedy search that is either guided
by H for HC-Search, or by C when only one function is used. L(n) and (n) represents the
true loss and the feature vector of node n respectively. The cost and heuristic functions are
linear functions of (n). Node 7 corresponds to the lowest-loss output and greedy search
must follow the trajectory of highlighted nodes in order to reach that output. First consider
HC-Search. For the highlighted path to be followed the heuristic H needs to satisfy the
following constraints: H(3)<H(2), H(7)<H(6), and the weights wH = [1, 1, 1] result in a
376

fiHC-Search: A Learning Framework for Search-based Structured Prediction

heuristic that satisfies the constraints. Given this heuristic function, in order to return node
7 as the final output, the cost function must satisfy the following constraints: C(7)<C(1),
C(7)<C(2), C(7)<C(3), C(7)<C(6), and the weights wC = [1, 1, 0] solve the problem.
Thus we see that HC-Search can achieve zero loss on this problem.
Now consider the case where a single function C is used for the heuristic and cost
function. Here in order to generate a loss of zero, the function C must satisfy the combined
set of constraints from above that were placed on the heuristic and cost function. However,
it can be verified that there is no set of weights that satisfies both C(3)<C(2) and C(7)<C(1),
and hence, there is no single function C in our space that can achieve a loss of zero. By
scaling the losses by constant factors we can make the loss suffered arbitrarily high.
Thus, we see that there can be potential representational advantages to following the HCSearch framework. In what follows, we consider the implications of this added expressiveness
in terms of the worst-case time complexity of learning.
2.4 Learning Complexity
We now consider the feasibility of efficient, optimal learning in the simplest setting of greedy
search using linear heuristic and cost functions represented by their weight vectors wH and
wC respectively. In particular, we consider the HC-Search Consistency Problem, where the
input is a training set of structured examples, and we must decide whether or not there
exists wH and wC such that HC-Search using greedy search will achieve zero loss on the
training set. We first note, that this problem can be shown to be NP-Hard by appealing
to results on learning for beam search (Xu, Fern, & Yoon, 2009a). In particular, results
there imply that in all but trivial cases, simply determining whether or not there is a linear
heuristic wH that uncovers a zero loss search node is NP-Hard. Since HC-Search can only
return zero loss outputs when the heuristic is able to uncover them, we see that our problem
is also hard.
Here we prove a stronger result that provides more insight into the HC-Search framework. In particular, we show that even when it is easy to learn a heuristic that uncovers
all zero loss outputs, the consistency problem is still hard. This shows, that in the worst case
the hardness of our learning problem is not simply a result of the hardness of discovering
good outputs. Rather our problem is additionally complicated by the potential interaction
between H and C. Intuitively, when learning H in the worst case there can be ambiguity
about which of many small loss outputs to generate, and for only some of those will we
be able to find an effective C to return the best one. This is formalized by the following
theorem, whose proof is in the Appendix.
Theorem 1. The HC-Search Consistency Problem for greedy search and linear heuristic
and cost functions is NP-Hard even when we restrict to problems for which all possible
heuristic functions uncover a zero loss output.

3. Learning Approach
The above complexity result suggests that, in general, learning the optimal (H , C  ) pair
is impractical due to their potential interdependence. In this section, we develop a greedy
377

fiDoppa, Fern, & Tadepalli

stage-wise learning approach that first learns H and then a corresponding C. The approach
is motivated by observing a decomposition of the expected loss into components due to H
and C. Below, we first describe the decomposition and the staged learning approach that it
motivates. Next we describe our approaches for learning the heuristic and cost functions.
3.1 Loss Decomposition and Staged Learning
For any heuristic H and cost function C, the expected loss E (H, C) can be decomposed into
two parts: 1) the generation loss H , due to H not generating high-quality outputs, and
2) the selection loss C|H , the additional loss (conditional on H) due to C not selecting the
 be the best loss output in the
best loss output generated by the heuristic. Formally, let yH
set YH (x), i.e.,

yH
= arg minyYH (x) L(x, y, y  )

We can express the decomposition as follows:


E (H, C) = E(x,y )D L (x, yH
, y  ) + E(x,y )D L (x, y, y  )  L (x, yH
, y)
|
{z
}
|
{z
}
H

(4)

C|H

Note that given labeled data, it is straightforward to estimate both the generation and
selection loss, which is useful for diagnosing the HC-Search framework. For example, if one
observes that a system has high generation loss, then there will be little payoff in working
to improve the cost function. In our empirical evaluation we will further illustrate how the
decomposition is useful for understanding the results of learning.
In addition to being useful for diagnosis, the decomposition motivates a learning approach that targets minimizing each of the errors separately. In particular, we optimize the
overall error of the HC-Search approach in a greedy stage-wise manner. We first train a
heuristic H in order to optimize the generation loss component H and then train a cost
function C to optimize the selection loss C|H conditioned on H.
H  arg minHH H
C  arg minCC C|H
Note that this approach is greedy in the sense that H is learned without considering the
 While the proof of Theorem 1 hinges on this coupling, we have
implications for learning C.
found that in practice, learning H independently of C is a very effective strategy.
In what follows, we first describe a generic approach for heuristic function learning that
is applicable for a wide range of search spaces and search strategies, and then explain our
cost function learning algorithm.
3.2 Heuristic Function Learning
Most generally, learning a heuristic can be viewed as a Reinforcement Learning (RL) problem where the heuristic is viewed as a policy for guiding search actions and rewards
378

fiHC-Search: A Learning Framework for Search-based Structured Prediction

are received for uncovering high quality outputs (Zhang & Dietterich, 1995). In fact, this
approach has been explored for structured prediction in the case of greedy search (Wick,
Rohanimanesh, Singh, & McCallum, 2009) and was shown to be effective given a carefully
designed reward function and action space. While this is a viable approach, general purpose
RL can be quite sensitive to the algorithm parameters and specific definition of the reward
function and actions, which can make designing an effective learner quite challenging. Indeed, recent work (Jiang, Teichert, Daume III, & Eisner, 2012), has shown that generic RL
algorithms can struggle for some structured prediction problems, even with significant effort
put forth by the designer. Hence, in this work, we follow an approach based on imitation
learning, that makes stronger assumptions, but has nevertheless been very effective and
easy to apply across a variety of problems.
Algorithm 1 Heuristic Function Learning via Exact Imitation
Input: D = Training examples, (I, S) = Search space definition, L = Loss function, A =
Rank-based search procedure, max = search time bound
Output: H, the heuristic function
1: Initialize the set of ranking examples R = 
2: for each training example (x, y  )  D do
3:
s0 = I(x) // initial state of the search tree
4:
M0 = {s0 } // set of open nodes in the internal memory of the search procedure
5:
for each search step t = 1 to max do
6:
Select the state(s) to expand: Nt =Select(A, L, Mt1 )
7:
Expand every state s  Nt using the successor function S: Ct =Expand(Nt , S)
8:
Prune states and update the internal memory state of the search procedure:
Mt =Prune(A, L, Mt1  Ct \ Nt )
9:
Generate ranking examples Rt to imitate this search step
10:
Add ranking examples Rt to R: R = R  Rt // aggregation of training data
11:
end for
12: end for
13: H =Rank-Learner(R) // learn heuristic function from all the ranking examples
14: return learned heuristic function H
Our heuristic learning approach is based on the observation that for many structured
prediction problems, we can quickly generate very high-quality outputs by guiding the
search procedure using the true loss function L as a heuristic. Obviously this can only be
done for the training data for which we know y  . This suggests formulating the heuristic
learning problem in the framework of imitation learning by attempting to learn a heuristic
that mimics the search decisions made by the true loss function on training examples. The
learned heuristic need not approximate the true loss function uniformly over the output
space, but need only make the distinctions that were important for guiding the search. The
main assumptions made by this approach are: 1) the true loss function can provide effective
heuristic guidance to the search procedure, so that it is worth imitating, and 2) we can
learn to imitate those search decisions sufficiently well.
This imitation learning approach is similar to prior work on learning single cost functions
for output-space search (Doppa et al., 2012). However, a key distinction here is that learning
379

fiDoppa, Fern, & Tadepalli

is focused on only making distinctions necessary for uncovering good outputs (the purpose
of the heuristic) and hence requires a different formulation. As in prior work, in order to
avoid the need to approximate the loss function arbitrarily closely, we restrict ourselves to
rank-based search strategies. A search strategy is called rank-based if it makes all its
search decisions by comparing the relative values of the search nodes (their ranks) assigned
by the heuristic, rather than being sensitive to absolute values of heuristic. Most common
search procedures such as greedy search, beam search, and best-first search fall under this
category.
3.2.1 Imitating Search Behavior
Given a search space over complete outputs S, a rank-based search procedure A, and a
search time bound  , our learning procedure generates imitation training data for each
training example (x, y  ) as follows. We run the search procedure A for a time bound of 
for input x using a heuristic equal to the true loss function, i.e. H(x, y) = L(x, y, y  ). During
the search process we observe all of the pairwise ranking decisions made by A using this
oracle heuristic and record those that are sufficient (see below) for replicating the search.
If the state (x, y1 ) has smaller loss than (x, y2 ), then a ranking example is generated in the
form of the constraint H(x, y1 )<H(x, y2 ). Ties are broken using a fixed arbitrator1 . The
aggregate set of ranking examples collected over all the training examples is then given to
a learning algorithm to learn the weights of the heuristic function.
If we can learn a function H from hypothesis space H that is consistent with these
ranking examples, then the learned heuristic is guaranteed to replicate the oracle-guided
search on the training data. Further, given assumptions on the base learning algorithm
(e.g. PAC), generic imitation learning results can be used to give generalization guarantees
on the performance of search on new examples (Khardon, 1999; Fern, Yoon, & Givan, 2006;
Syed & Schapire, 2010; Ross & Bagnell, 2010). Our experiments show, that the simple
approach described above, performs extremely well on our problems.
Algorithm 1 describes our approach for heuristic function learning via exact imitation of
search guided by the loss function. It is applicable to a wide-range of search spaces, search
procedures and loss functions. The learning algorithm takes as input: 1) D = {(x, y  )}, a
set of training examples for a structured prediction problem (e.g., handwriting recognition);
2) So = (I, S), a search space over complete outputs (e.g., LDS space), where I is the initial
state function and S is the successor function; 3) L, a task loss function defined over
complete outputs (e.g., hamming loss); 4) A, a rank-based search procedure (e.g., greedy
search); 5) max , the search time bound (e.g., number of search steps).
The algorithmic description of Algorithm 1 assumes that the search procedure A can
be described in terms of three steps that are executed repeatedly on an open list of search
nodes: 1) selection, 2) expansion and 3) pruning. In each execution, the search procedure
selects one or more open nodes from its internal memory for expansion (step 6) based on
heuristic value, and expands all the selected nodes to generate the candidate set (step 7).
It retains only a subset of all the open nodes after expansion in its internal memory and
prunes away all the remaining ones (step 8) again based on heuristic value. For example,
1. For the LDS Space that we employed in this work, we implemented an arbitrator which breaks the ties
based on the position of the discrepancy (prefers earlier discrepancies).

380

fiHC-Search: A Learning Framework for Search-based Structured Prediction

greedy search maintains only the best node, best-first beam search retains only the best b
nodes for a fixed beam-width b, and pure best first search does not do any pruning.
Algorithm 1 loops through each training example and collects a set of ranking constraints. Specifically, for example (x, y  ), the search procedure is run for a time bound of
max using the true loss function L as the heuristic (steps 2-12). During each search step a
set of pairwise ranking examples is generated that are sufficient for allowing the search step
to be imitated (step 9) as described in more detail below. After all such constraints are
aggregated across all search steps of all training examples, they are given to a rank-learning
algorithm (e.g., Perceptron or SVM-Rank) to learn the weights of the heuristic function
(step 13).
The most important step in our heuristic function learning algorithm is the generation
of ranking examples to imitate each step of the search procedure (step 9). In what follows,
we will give a generic description of sufficient pairwise decisions to imitate the search,
and illustrate them for greedy search through a simple example.
3.2.2 Sufficient Pairwise Decisions
Above we noted that we only need to collect and learn to imitate the sufficient pairwise
decisions encountered during search. We say that a set of constraints is sufficient for a
structured training example (x, y  ), if any heuristic function that is consistent with the
constraints causes the search to follow the same trajectory of open lists encountered during
search. The precise specification of these constraints depends on the actual search procedure
that is being used. For rank-based search procedures, the sufficient constraints can be
categorized into two types:
1. Selection constraints, which ensure that the search node(s) from the internal memory
state that will be expanded in the next search step is (are) ranked better than all
other nodes.
2. Pruning constraints, which ensure that the internal memory state (set of search nodes)
of the search procedure is preserved at every search step. More specifically, these
constraints involve ranking every search node in the internal memory state better
(lower H-value) than those that are pruned.
Below, we will illustrate these constraints concretely for greedy search noting that similar
formulations for other rank-based search procedures are straightforward (See (Doppa, Fern,
& Tadepalli, 2014a) for beam search formulation).
3.2.3 Constraints for Greedy Search
This is the most basic rank-based search procedure. For a given input x, it traverses the
search space by selecting the next state as the successor of the current state that looks best
according to the heuristic function H. In particular, if si is the search state at step i, greedy
search selects si+1 = arg minsS(si ) H(s), where s0 = I(x). In greedy search, the internal
memory state of the search procedure at step i consists of only the best open (unexpanded)
node si .
381

fiDoppa, Fern, & Tadepalli

Figure 4: An example search tree that illustrates greedy search with loss function. Each
node represents a complete input-output pair and can be evaluated using the loss
function. The highlighted nodes correspond to the trajectory of greedy search
guided by the loss function.

Let (x, yi ) correspond to the input-output pair associated with state si . Since greedy
search maintains only a single open node si in its internal memory at every search step i,
there are no selection constraints. Let Ci+1 be the candidate set after expanding state si ,
i.e., Ci+1 = S(si ). Let si+1 be the best node in the candidate set Ci+1 as evaluated by the
loss function, i.e., si+1 = arg minsCi+1 L(s). As greedy search prunes all the nodes in the
candidate set other than si+1 , pruning constraints need to ensure that si+1 is ranked better
than all the other nodes in Ci+1 . Therefore, we include one ranking constraint for every
node (x, y)  Ci+1 \ (x, yi+1 ) such that H(x, yi+1 ) < H(x, y).
We will now illustrate these ranking constraints through an example. Figure 4 shows
an example search tree of depth two with associated losses for every search node. The
highlighted nodes correspond to the trajectory of greedy search with loss function that our
learner has to imitate. At the first search step, {H(3) < H(2), H(3) < H(4)} are the pruning
constraints. Similarly, {H(10) < H(8), H(10) < H(9)} form the pruning constraints at the
second search step. Therefore, the aggregate set of constraints needed to imitate the greedy
search behavior shown in Figure 4 are:
{H(3) < H(2), H(3) < H(4), H(10) < H(8), H(10) < H(9)}.
3.3 Cost Function Learning
Given a learned heuristic H, we now want to learn a cost function that correctly ranks the
potential outputs generated by the search procedure guided by H. More formally, let YH (x)
be the set of candidate outputs generated by the search procedure guided by heuristic H for
a given input x, and lbest be the loss of the best output among those outputs as evaluated by
the true loss function L, i.e., lbest = minyYH (x) L(x, y, y  ). In an exact learning scenario,
the goal is to find the parameters of a cost function C such that for every training example
382

fiHC-Search: A Learning Framework for Search-based Structured Prediction

(x, y  ), the loss of the minimum cost output y equals lbest , i.e., L(x, y, y  ) = lbest , where
y = arg minyYH (x) C(x, y). In practice, when exact learning isnt possible, the goal is to
find a cost function such that the average loss over the training data of the predicted output
using the cost function is minimized.
Algorithm 2 Cost Function Learning via Cross Validation
Input: D = Training examples, So = Search space definition, L = Loss function, A =
Search procedure, max = search time bound
Output: C, the cost function
1: Divide the training set D into k folds D1 , D2 ,    , Dk
2: // Learn k different heuristics H1 ,    , Hk
3: for i = 1 to k do
4:
Ti = j6=i Dj // training data for heuristic Hi
5:
Hi = Learn-Heuristic(Ti , So , L, A, max ) // heuristic learning via Algorithm 1
6: end for
7: // Generate ranking examples for cost function training
8: Intialize the set of ranking examples R = 
9: for i = 1 to k do
10:
for each training example (x, y  )  Di do
11:
Generate outputs by running the search procedure A with heuristic Hi for time
bound max : YHi (x) = Generate-Outputs(x, So , A, Hi , max )
12:
Compute the set of best loss outputs: Ybest = {y  YHi (x)|L(x, y, y  ) = lbest },
where lbest = minyYH (x) L(x, y, y  )
i
13:
for each pair of outputs (ybest , y)  Ybest  YHi (x) \ Ybest do
14:
Add ranking example C(x, ybest ) < C(x, y) to R
15:
end for
16:
end for
17: end for
18: // Train cost function on all the ranking examples
19: C = Rank-Learner(R)
20: return learned cost function C
We formulate the cost function training problem as an instance of rank learning problem
(Agarwal & Roth, 2005). More specifically, we want all the best loss outputs in YH (x) to
be ranked better than all the non-best loss outputs according to our cost function, which is
a bi-partite ranking problem. Let Ybest be the set of all best loss outputs from YH (x), i.e.,
Ybest = {y  YH (x)|L(x, y, y  ) = lbest }. We generate one ranking example for every pair of
outputs (ybest , y)  Ybest  YH (x) \ Ybest , requiring that C(x, ybest )<C(x, y). If the search
procedure was able to generate the target output y (i.e., lbest = 0), this is similar to the
standard learning in CRFs and SVM-Struct, but results in a much simpler rank-learning
problem (cost function needs to rank the correct output above only the incorrect outputs
generated during search). When the set of best loss outputs Ybest is very large, bi-partite
ranking may result in a highly over-constrained problem. In such cases, one could relax the
problem by attempting to learn a cost function that ranks at least one output in Ybest higher
than all the non-best loss outputs. This can be easily implemented in an online-learning
383

fiDoppa, Fern, & Tadepalli

framework as follows. If there is an error (i.e., the best cost output according to the current
weights y 
/ Ybest ), the weights are updated to ensure that the best cost output ybest  Ybest
according to the current weights is ranked better than all the outputs in YH (x) \ Ybest .
It is important to note that both in theory and practice, the distribution of outputs
generated by the learned heuristic H on the testing data may be slightly different from the
one on training data. Thus, if we train C on the training examples used to train H, then C
is not necessarily optimized for the test distribution. To mitigate this effect, we train our
cost function via cross validation (see Algorithm 2) by training the cost function on the
data, which was not used to train the heuristic. This training methodology is commonly
used in Re-ranking style algorithms (Collins, 2000) among others.
Algorithm 2 describes our approach for cost function training via cross validation. There
are four main steps in the algorithm. First, we divide the training data D into k folds.
Second, we learn k different heuristics, where each heuristic Hi is learned using the data
from all the folds excluding the ith fold (Steps 3-6). Third, we generate ranking examples
for cost function learning as described above using each heuristic Hi on the data it was not
trained on (Steps 9-17). Finally, we give the aggregate set of ranking examples R to a rank
learner (e.g., Perceptron, SVM-Rank) to learn the cost function C (Step 19).
3.4 Rank Learner
In this section, we describe the specifics of the rank learner that can be used to learn both
the heuristic and cost functions from the aggregate sets of ranking examples produced by the
above algorithms. We can use any off-the-shelf rank-learning algorithm (e.g., Perceptron,
SVM-Rank) as our base learner to train the heuristic function from the set of ranking
examples R. In our specific implementation we employed the online Passive-Aggressive
(PA) algorithm (Crammer, Dekel, Keshet, Shalev-Shwartz, & Singer, 2006) as our base
learner. Training was conducted for 50 iterations in all of our experiments.
PA is an online large-margin algorithm, which makes several passes over the training
examples R, and updates the weights whenever it encounters a ranking error. Recall that
each ranking example is of the form H(x, y1 ) < H(x, y2 ) for heuristic training and C(x, y1 ) <
C(x, y2 ) for cost function training, where x is a structured input with target output y  ,
y1 and y2 are potential outputs for x such that L(x, y1 , y  ) < L(x, y2 , y  ). Let >0 be
the difference between the losses of the two outputs involved in a ranking example. We
experimented with PA variants that use margin scaling (margin scaled by ) and slack
scaling (errors weighted by ) (Tsochantaridis, Joachims, Hofmann, & Altun, 2005). Since
margin scaling performed slightly better than slack scaling, we report the results of the PA
variant that employs margin scaling. Below we give the full details of the margin scaling
update.
Let wt be the current weights of the linear ranking function. If there is a 
ranking error
when cycling through the training data, i.e., wt  (x, y2 )  wt  (x, y1 ) < , the new
weights wt+1 that corrects the error can be obtained using the following equation.
wt+1 = wt + t ((x, y2 )  (x, y1 ))
384

fiHC-Search: A Learning Framework for Search-based Structured Prediction

where the learning rate t is given by
wt  (x, y1 )  wt  (x, y2 ) +
t =
k(x, y2 )  (x, y1 )k2





This specific update has been previously used for cost-sensitive multiclass classification
(Crammer et al., 2006) (See Equation 51) and for structured output problems (Keshet,
Shalev-Shwartz, Singer, & Chazan, 2005) (See Equation 7).

4. Experiments and Results
In this section we empirically investigate our HC-Search approach and compare it against
the state-of-the-art in structured prediction.
4.1 Datasets
We evaluate our approach on the following four structured prediction problems including
three benchmark sequence labeling problems and a 2D image labeling problem.
 Handwriting Recognition (HW). The input is a sequence of binary-segmented
handwritten letters and the output is the corresponding character sequence [a  z]+ .
This dataset contains roughly 6600 examples divided into 10 folds (Taskar et al.,
2003). We consider two different variants of this task as in the work of Hal Daume
III, Langford, and Marcu (2009). In the HW-Small version, we use one fold for training
and the remaining 9 folds for testing, and vice-versa in HW-Large.
 NETtalk Stress. This is a text-to-speech mapping problem, where the task is to
assign one of the 5 stress labels to each letter of a word. There are 1000 training
words and 1000 test words in the standard dataset. We use a sliding window of size
3 for observational features.
 NETtalk Phoneme. This is similar to NETtalk Stress except that the task is to
assign one of the 51 phoneme labels to each letter of the word.
 Scene labeling. This data set contains 700 images of outdoor scenes (Vogel &
Schiele, 2007). Each image is divided into patches by placing a regular grid of size
1010 over the entire image, where each patch takes one of the 9 semantic labels (sky,
water, grass, trunks, foliage, field, rocks, flowers, sand ). Simple appearance features
including color, texture and position are used to represent each patch. Training was
performed with 600 images, and the remaining 100 images were used for testing.
4.2 Experimental Setup
For our HC-Search experiments, we use the Limited Discrepancy Space (LDS) exactly as
described in the work of Doppa et al. (2012) as our search space over structured outputs.
Prior work with HC-Search has shown that greedy search works quite well for most structured prediction tasks, particularly when using the LDS space (Doppa et al., 2012). Hence,
we consider only greedy search in our experiments. We would like to point out that experiments not shown using beam search and best first search produce similar results. During
385

fiDoppa, Fern, & Tadepalli

training and testing we set the search time bound  to be 25 search steps for all domains
except for scene labeling, which has a much larger search space and uses  = 150. We
found that using values of  larger than these did not produce noticeable improvement. For
extremely small values of  , performance tends to be worse, but it increases quickly as  is
made larger. We will also show results for the full spectrum of time bounds later. For all
domains, we learn linear heuristic and cost functions over second order features unless otherwise noted. In this case, the feature vector measures features over neighboring label pairs
and triples along with features of the structured input. We measure error with Hamming
loss unless otherwise noted.
4.3 Comparison to State-of-the-Art
We compare the results of our HC-Search approach with other structured prediction algorithms including CRFs (Lafferty et al., 2001), SVM-Struct (Tsochantaridis et al., 2004),
Searn (Hal Daume III et al., 2009), Cascades (Weiss & Taskar, 2010) and C-Search,
which is identical to HC-Search except that it uses a single-function for output space search
(Doppa et al., 2012). We also show the performance of Recurrent, which is a simple
recurrent classifier trained exactly as in the work of Doppa et al. (2012). The top section
of Table 1 shows the error rates of the different algorithms. For scene labeling it was not
possible to run CRFs, SVM-Struct, and Cascades due to the complicated grid structure
of the outputs (hence the - in the table). We report the best published results of CRFs,
SVM-Struct, and Searn. Cascades was trained using the implementation (Weiss, 2014) provided by the authors, which can be used for sequence labeling problems with Hamming loss.
We would like to point out that the results of cascades differ from those that appear in the
work of Doppa, Fern, and Tadepalli (2013) and are obtained using an updated2 version of
cascades training code. Across all benchmarks, we see that results of HC-Search are comparable or significantly better than the state-of-the-art including C-Search, which uses a single
function as both heuristic function and cost function. The results in the scene labeling
domain are the most significant improving the error rate from 27.05 to 19.71. These results
show that HC-Search is a state-of-the-art approach across these problems and that learning
separate heuristic and cost functions can significantly improve output-space search.
4.4 Higher-Order Features
One of the advantages of our approach compared to many frameworks for structured prediction is the ability to use more expressive feature spaces without paying a huge computational
price. The bottom part of Table 1 shows results using third-order features (compared to
second-order above) for HC-Search, C-Search and Cascades. Note that it is not practical to
run the other methods using third-order features due to the substantial increase in inference
time. The overall error of HC-Search with higher-order features slightly improved compared
to using second-order features across all benchmarks and is still better than the error-rates
of C-Search and Cascades with third-order features, with the exception of Cascades on
HW-Large. In fact, HC-Search using only second-order features is still outperforming the
third-order results of the other methods on three out of five domains.
2. Personal communication with the author

386

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Algorithms
HW-Small

HW-Large

Datasets
Stress Phoneme

Scene labeling

HC-Search
C-Search
CRF
SVM-Struct
Recurrent
Searn
Cascades

a. Comparison to state-of-the-art
12.81
03.23
17.58
16.91
17.03
07.16
21.07
20.81
19.97
13.11
21.48
21.09
19.64
12.49
22.01
21.70
34.33
25.13
27.18
26.42
17.88
09.42
23.85
22.74
13.02
03.22
20.41
17.56

19.71
27.05
43.36
37.69
-

HC-Search
C-Search
Cascades

b. Results with Third-Order Features
10.04
02.21
16.32
14.29
14.15
04.76
19.36
18.19
10.82
02.16
19.51
17.41

18.25
25.79
-

Table 1: Error rates of different structured prediction algorithms.
4.5 Loss Decomposition Analysis
We now examine HC-Search and C-Search in terms of their loss decomposition (see Equation 4) into generation loss H and selection loss C|H . Both of these quantities can be
easily measured for both HC-Search and C-Search by keeping track of the best loss output
generated by the search (guided either by a heuristic or the cost function for C-Search)
across the testing examples. Table 2 shows these results, giving the overall error HC and
its decomposition across our benchmarks for both HC-Search and C-Search.
We first see that generation loss H is very similar for C-Search and HC-Search across the
benchmarks with the exception of scene labeling, where HC-Search generates slightly better
outputs. This shows that at least for the LDS search space the difference in performance
between C-Search and HC-Search cannot be explained by C-Search generating lower quality
outputs. Rather, the difference between the two methods is most reflected by the difference
in selection loss C|H , meaning that C-Search is not as effective at ranking the outputs
generated during search compared to HC-Search. This result clearly shows the advantage
of separating the roles of C and H and is understandable in light of the training mechanism
for C-Search. In that approach, the cost function is trained to satisfy constraints related to
both the generation loss and selection loss. It turns out that there are many more generation
loss constraints, which we hypothesize biases C-Search toward low generation loss at the
expense of selection loss.
These results also show that for both methods the selection loss C|H contributes significantly more to the overall error compared to H . This shows that both approaches are
able to uncover very high-quality outputs, but are unable to correctly rank the generated
outputs according to their losses. This suggests that a first avenue for improving the results
of HC-Search would be to improve the cost function learning component, e.g. by using
non-linear cost functions.
387

fiDoppa, Fern, & Tadepalli

4.6 Ablation Study
To futher demonstrate that having two separate functions (heuristic and cost function) as
in HC-Search will lead to more accurate predictions compared to using a single function
as in C-Search, we perform some ablation experiments. In this study, we take the learned
heuristic function H and cost function C in the HC-Search framwork, and use only one of
them to make predictions. For example, HH-Search corresponds to the configuration when
we use the function H as both heuristic and cost function. Similarly, CC-Search corresponds
to the configuration when we use the function C as both heuristic and cost function.
Table 2b shows the results for these ablation experiments. We can make several interesting observations from these results. First, the overall error of HC-Search is significantly
better than that of HH-Search and CC-Search. Second, the selection loss for HH-Search
increases compared to that of HC-Search. This is understandable because H is not trained
to score the candidate outputs that are generated during search. Third, the generation
loss for CC-Search increases compared to that of HC-Search and this behavior is significant
(increases to 11.24 compared to 5.82) for the scene labeling task. All these results provide
further evidence for the importance of separating the training of the heuristic and cost
functions.
HW-Small
C|H
H

Stress
C|H
H

HC

03.2
07.1

a. HC-Search vs. C-Search
00.7 02.7 17.5 02.7 14.7
00.9 06.2 21.0 03.0 18.0

16.9
20.8

03.4
04.1

13.4
16.6

19.7
27.0

05.8
07.8

13.8
19.2

07.9
06.6

b. Results for Ablation study
00.7 7.2
22.5 02.7 19.7
01.7 04.9 19.1 03.2 15.8

22.1
21.6

03.4
04.3

18.7
17.3

32.1
25.3

07.8
11.2

24.3
14.0

c. Results with heuristic function training via DAgger
08.1 03.1 00.4 02.6 17.2 02.2 15.0 16.8 03.0
09.9 05.1 00.8 03.6 20.3 02.8 17.1 19.0 03.9

13.8
14.7

18.0
24.2

03.7
05.9

14.3
18.3

11.7

16.3

00.3

16.0

Datasets
Error

HC

HC-Search
C-Search

12.8
17.5

04.7
04.9

08.0
12.6

HH-Search
CC-Search

18.4
16.2

04.7
05.3

13.7
10.9

HC-Search
C-Search

12.0
15.1

03.9
04.6

HC

HW-Large
C|H
H

HC

Phoneme
C|H
H

HC

Scene
C|H
H

d. Results with Oracle Heuristic
LC-Search
(Oracle H)

10.1

00.2

09.9

03.0

00.5

02.5

14.1

00.2

13.9

12.2

00.5

Table 2: HC-Search: Error decomposition of heuristic and cost function.
4.7 Results for Heuristic Training via DAgger
Our heuristic learning approach follows the simplest approach to imitation learning, exact
imitation, where the learner attempts to exactly imitate the observed expert trajectories
(here imitate search with the oracle heuristic). While our experiments show that exact
imitation performs quite well, it is known that exact imitation has certain deficiencies in
general. In particular, functions trained via exact imitation can be prone to error propagation (Kaariainen, 2006; Ross & Bagnell, 2010), where errors made at test time change the
distribution of decisions encountered in the future compared to the training distribution.
To address this problem, more sophisticated imitation learning algorithms have been developed, with a state-of-the-art approach being DAgger (Ross, Gordon, & Bagnell, 2011).
388

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Here we consider whether DAgger can improve our heuristic learning and in turn overall
accuarcy.
DAgger is an iterative algorithm, where each iteration adds imitation data to an aggregated data set. The first iteration follows the exact imitation approach, where data are
collected by observing an expert trajectory (or a number of them). After each iteration an
imitation function (here a heuristic) is learned from the current data. Successive iterations
generate trajectories by following a mixture of expert suggestions (in our case ranking decisions) and suggestions of the most recently learned imitation function. Each decision point
along the trajectory is added to the aggregate data set by labeling it by the expert decision.
In this way, later iterations allow DAgger to learn from states visited by its possibly erroneous learned functions and correct its mistakes using the expert input. Ross et al. (2011)
show that during the iterations of DAgger just using the learned policy without mixing
the expert policy performs very well across diverse domains. Therefore, we use the same
approach in our DAgger experiments. In our experiments we run 5 iterations of DAgger,
noting that no noticable improvement was observed after 5 iterations.
Table 2c shows the results of HC-Search and C-Search obtained by training with DAgger. For HC-Search, the generation loss (H ) improved slightly on the sequence labeling
problems as there is little room for improvement, but DAgger leads to significant improvement in the generation loss on the more challenging problem of scene labeling. We can also
see that the overall error of HC-Search for scene labeling reduces due to improvement in
generation loss showing that cost function is able to leverage the better outputs produced
by the heuristic. Similarly, the overall error of C-Search also improved with DAgger across
the board and we see most significant improvements for handwiriting and scene labeling
domains. It is interesting to note that unlike HC-Search, the improvement in C-Search is
mostly due the improvement in the selection loss (C|H ) except for scene labeling task, where
it is due to the improvement in both generation loss and selection loss.
These results show that improving the heuristic learning is able to improve overall
performance. What is not clear is whether further improvement, perhaps due to future
advances in imitation learning, would yet again lead to overall improvement. That is, while
it may be possible to further improve the generation loss, it is not clear that the cost function
will be able to exploit such improvments. To help evaluate this we ran an experiment where
we gave HC-Search the true loss function to use as a heuristic (an oracle heuristic), i.e.,
H(x, y) = L(x, y, y  ), during both training of the cost function and testing. This provides
an assessment of how much better we might be able to do if we could improve heuristic
learning. The results in Table 2, which we label as LC-Search (Oracle H) show that when
using the oracle heuristic, H is negligible as we might expect and smaller than observed for
HC-Search. This shows that it may be possible to further improve our heuristic learning
via better imitation.
We also see from the oracle results that the overall error HC is better than that of
HC-Search, but for HW-Small and Scene labeling tasks, the selection error C|H got slightly
worse.. This indicates that our cost function learner is able to leverage, to varying degrees, the better outputs produced by the oracle heuristic. This suggests that improving
the heuristic learner in order to reduce the generation loss could be a viable way of further
reducing the overall loss of HC-Search, even without altering the current cost learner. However, as we saw above there is much less room to improve the heuristic learner for these
389

fiDoppa, Fern, & Tadepalli

data sets and hence the potential gains are less than for directly trying to improve the cost
learner.
4.8 Results for Training with Different Time bounds

Train

We also trained HC-Search for different time bounds (i.e., number of greedy search steps) to
see how the overall loss, generation loss and selection loss vary as we increase the training
time bound. In general, as the time bound increases, the generation loss will monotonically
decrease, since strictly more outputs will be encountered. On the other hand the difficulty
of cost function learning can increase as the time bound grows since it must learn to distinguish between a larger set of candidate outputs. Thus, the degree to which the overall
error decreases (or grows) with the time bound depends on a combination of how much
the generation loss decreases and whether the cost function learner is able to accurately
distinguish improved outputs.
Figure 5 shows the performance of HC-Search for the full spectrum of time bounds.
Qualitatively, we see that the generation loss, due to the heuristic, decreases remarkably
fast and for most benchmarks improves very little after the initial decrease. We also see that
the cost function learner achieves a relatively stable selection loss in a short time, though it
does increase a bit with time in most cases. The combined effect is that we see the overall
error HC improves quickly as we increase the time bound and the improvement tends to be
very small beyond certain time bound. Also, in some cases (e.g., phoneme prediction and
scene labeling) performance tends to get slightly worse for very large time bounds, which
happens when the increase in selection loss is not counteracted by a decreased generation
loss.
Loss Function
Hamming
VC

Test
Hamming
VC
1757
4658
1769
4620

Table 3: Results for training with non-hamming loss functions.

4.9 Results for Training with Non-Hamming Loss functions
One of the advantages of HC-Search compared to many other approaches for structured
prediction is that it is sensitive to the loss function used for training. So we trained HCSearch with different loss functions on the handwriting domain to verify if this is true in
practice or not. We used hamming loss (uniform misclassification cost of 1 for all characters)
and Vowel-Consonant (VC) loss (different misclassification costs for vowels and consonants)
for this experiment. For VC loss, we used misclassification costs of 4 and 2 for vowels and
consonants respectively. Training was done on 5 folds and the remaining 5 folds were used
for testing. Table 4.8 shows the results for training and testing with the two loss functions.
We report cumulative loss over all the testing examples. As we can see, for any testing
loss function, training with the same loss function gives slightly better performance than
training using a different loss function. This shows that our HC-Search learning approach
390

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Figure 5: HC-Search results for training with different time bounds. We have training time
bound (i.e., no. of greedy search steps) on x-axis and error on y-axis. There are
three curves in each graph corresponding to overall loss HC , generation loss H
and selection loss C|H .

391

fiDoppa, Fern, & Tadepalli

is sensitive to the loss function. However, this result may not hold generally and very much
depends on the problem structure, loss function and the ability of our cost function to
capture that loss.
4.10 Discussion on Efficiency of the HC-Search Approach
In our HC-Search framework, the basic computational elements include generating candidate
states for a given state; computing the heuristic function features via H and cost function
features via C for all the candidate states; and computing the heuristic and cost scores via
the learned heuristic and cost function pair (H, C). The computational time for generating
the candidate states depends on the employed search space So = (I, S), where I is the initial
state function and S is the successor function. For example, the generation of candidates
will be very efficient with Flipbit space compared to the LDS space (involves running the
recurrent classifier for every action specified by the successor function S). Therefore, the
efficiency of the overall approach depends on the size of the candidate set and can be
greatly improved by generating fewer candidate states (e.g., via pruning) or parallelizing the
computation. We have done some preliminary work in this direction by introducing sparse
versions of both LDS and Flipbit search spaces by pruning actions based on the recurrent
classifier scores (as specified by the prunining parameter k). This simple pruning strategy
resulted in 10-fold speedup with little or no loss in accuracy across several benchmark
problems (Doppa et al., 2014a). However, more work needs to be done on learning pruning
rules to improve the efficiency of the HC-Search approach.

5. Engineering Methodology for Applying HC-Search
In this section, we describe an engineering methodology for applying our HC-Search framework to new problems. At a very high-level, the methodology involves selecting an effective
time-bounded search architecture (search space, search procedure, and search time-bound),
and leveraging the loss decomposition in terms of generation and selection loss for training
and debugging the heuristic and cost functions. Below we describe these steps in detail.
5.1 Selection of Time-bounded Search Architecture
A time-bounded search architecture can be instantiated by selecting a search space, search
strategy, and search time-bound. As we mentioned before, the effectiveness of HC-Search
depends critically on the quality of the search space (i.e., search depth at which target
outputs can be found) that is being employed. In fact, our prior work empirically demonstrated that the performance gap of the search architectures with Flipbit space and LDS
space grows as the difference between their target depths increase (Doppa et al., 2014a).
Therefore, it is important to select/design a high-quality search space for the problem at
hand.
If there exists a greedy predictor for the structured prediction problem, one could leverage it to define an appropriate variant of the LDS space. Fortunately, there are greedy
predictors for several problems in natural language processing, computer vision, relational
networks, and planning with preferences. For example, transition-based parsers for dependency parsing (Nivre, 2008; Goldberg & Elhadad, 2010); greedy classifiers for co-reference
392

fiHC-Search: A Learning Framework for Search-based Structured Prediction

resolution (Chang, Samdani, & Roth, 2013; Stoyanov & Eisner, 2012) and event extraction
(Li, Ji, & Huang, 2013); sequential labelers for boundary detection of objects in images
(Payet & Todorovic, 2013); iterative classifiers for collective inference in relational networks
(Sen, Namata, Bilgic, Getoor, Gallagher, & Eliassi-Rad, 2008; Doppa, Yu, Tadepalli, &
Getoor, 2009, 2010); classifier chains for multi-label prediction (Read, Pfahringer, Holmes,
& Frank, 2011); and greedy planners for planning with preferences (Xu, Fern, & Yoon,
2010). In general, designing high-quality search spaces is a key research topic and more
work needs to be done in this direction. Learning search operators (macro actions) or
transformation rules as in Transformation-based Learning (TBL) (Brill, 1995) to optimize
the search space is one of the many possibilities. Sometimes problem structure can also help
in designing effective search spaces. For example, in most multi-label prediction problems,
the outputs which are binary vectors have a small number of active labels (highly sparse).
So a simple flipbit space initialized with the null vector can be very effective (Doppa, Yu,
Ma, Fern, & Tadepalli, 2014b).
After picking the search space, we need to select an appropriate search procedure and
search time-bound. The effectiveness of a search architecture can be measured by performing
oracle search (true loss function used as both heuristic and cost function) on the training
data. So one could perform oracle search (LL-Search) with different search procedures (e.g.,
greedy and beam search) for different time-bounds and select the search procedure that is
more effective. We did not see benefit with beam search for the problems we considered, but
we expect that this can change for harder problems with non-Hamming loss functions (e.g.,
B-Cubed score for co-reference resolution). If the search space is not redundant, then we
can fix the search time-bound to a value where the performance of the search architecture
stagnates. Otherwise, one should allow some slack so that the search procedure can recover
from errors. In our experiments, we found that T (size of the structured output) is a
reasonable value for the time-bound (Figure 5 provides justification for this choice).
5.2 Training and Debugging
The training procedure involves learning the heuristic H and cost function C to optimize
the performance of the selected time-bounded search architecture on the training data.
Following our staged learning approach, one could start with learning a heuristic via exact
imitation of the oracle search. After that, the learned heuristic H should be evaluated by
measuring the generation loss (HL-Search configuration). If the performance of the HLSearch configuration is acceptable with respect to the performance of LL-Search, we can
move to cost function learning part. Otherwise, we can try to improve the heuristic by either
employing more sophisticated imitation learning algorithms (e.g., DAgger), enriching the
feature function H , or employing a more powerful rank learner. Similarly, after learning
the cost function C conditioned on the learned heuristic, we can measure the selection loss.
If the selection loss is very high, we can try to improve the cost function by either adding
expressive features to C or employing a more powerful rank learner.

6. Comparison to Related Work
As described earlier, the majority of structured prediction work has focused on the use
of exact inference for computing outputs when it is tractable, and approximate inference
393

fiDoppa, Fern, & Tadepalli

techniques, such as loopy belief propagation and relaxation methods, when it is not. Learning then is focused on tuning the cost function parameters in order to optimize various
objective functions, which differ among learning algorithms (Lafferty et al., 2001; Taskar
et al., 2003; Tsochantaridis et al., 2004; McAllester, Hazan, & Keshet, 2010). There are also
approximate cost function learning approaches that do not employ any inference routine
during training. For example, piece-wise training (Sutton & McCallum, 2009), Decomposed
Learning (Samdani & Roth, 2012) and its special case pseudo-max training (Sontag, Meshi,
Jaakkola, & Globerson, 2010) fall under this category. These training approaches are very
efficient, but they still need an inference algorithm to make predictions during testing.
In these cases, one could employ the Constrained Conditional Models (CCM) framework
(Chang, Ratinov, & Roth, 2012) with some declarative (global) constraints to make predictions using the learned cost function. The CCM framework relies on the Integer Linear
Programming (ILP) inference method (Roth & tau Yih, 2005). More recent work has attempted to integrate (approximate) inference and cost function learning in a principled
manner (Meshi, Sontag, Jaakkola, & Globerson, 2010; Stoyanov, Ropson, & Eisner, 2011;
Hazan & Urtasun, 2012; Domke, 2013). Researchers have also worked on using higher-order
features for CRFs in the context of sequence labeling under the pattern sparsity assumption
(Ye, Lee, Chieu, & Wu, 2009; Qian, Jiang, Zhang, Huang, & Wu, 2009). However, these
approaches are not applicable for the graphical models where the sparsity assumption does
not hold.
An alternative approach to addressing inference complexity is cascade training (Felzenszwalb & McAllester, 2007; Weiss & Taskar, 2010; Weiss, Sapp, & Taskar, 2010), where
efficient inference is achieved by performing multiple runs of inference from a coarse level
to a fine level of abstraction. While such approaches have shown good success, they place
some restrictions on the form of the cost functions to facilitate cascading. Another potential drawback of cascades and most other approaches is that they either ignore the loss
function of a problem (e.g. by assuming Hamming loss) or require that the loss function be
decomposable in a way that supports loss augmented inference. Our approach is sensitive
to the loss function and makes minimal assumptions about it, requiring only that we have
a blackbox that can evaluate it for any potential output.
Classifier-based structured prediction algorithms avoid directly solving the Argmin problem by assuming that structured outputs can be generated by making a series of discrete
decisions. The approach then attempts to learn a recurrent classifier that given an input
x is iteratively applied in order to generate the series of decisions for producing the target
output y. Simple training methods (e.g. Dietterich, Hild, & Bakiri, 1995) have shown
good success and there are some positive theoretical guarantees (Syed & Schapire, 2010;
Ross & Bagnell, 2010). However, recurrent classifiers can be prone to error propagation
(Kaariainen, 2006; Ross & Bagnell, 2010). Recent work, e.g. SEARN (Hal Daume III
et al., 2009), SMiLe (Ross & Bagnell, 2010), and DAgger (Ross et al., 2011), attempts to
address this issue using more sophisticated training techniques and have shown state-of-theart structured-prediction results. However, all these approaches use classifiers to produce
structured outputs through a single sequence of greedy decisions. Unfortunately, in many
problems, some decisions are difficult to predict by a greedy classifier, but are crucial for
good performance. In contrast, our approach leverages recurrent classifiers to define good
394

fiHC-Search: A Learning Framework for Search-based Structured Prediction

quality search spaces over complete outputs, which allows decision making by comparing
multiple complete outputs and choosing the best.
There are also non-greedy methods that learn a scoring function to search in the space of
partial structured outputs (DaumeIII & Marcu, 2005; Daume III, 2006; Xu, Fern, & Yoon,
2009b; Huang, Fayong, & Guo, 2012; Yu, Huang, Mi, & Zhao, 2013). All these methods
perform online training, and differ only in the way search errors are defined and how the
weights are updated when errors occur. Unfortunately, training the scoring function can
be difficult because it is hard to evaluate states with partial outputs and the theoretical
guarantees for the learned scoring function (e.g., convergence and generalization results)
rely on strong assumptions (Xu et al., 2009b).
Our work is most closely related to the output space search approaches (Doppa et al.,
2012; Wick et al., 2011), which use a single cost function to serve as both search heuristic
and also to score the candidate outputs. Serving these dual roles often means that the cost
function needs to make unclear tradeoffs, increasing the difficulty of learning. Our HCSearch approach overcomes this deficiency by learning two different functions, a heuristic
function to guide the search to generate high-quality candidate outputs, and a cost function
to rank the candidate outputs. Additionally, the error decomposition of HC-Search in terms
of heuristic error and cost function error allows the human designers of the learning system
to diagnose failures and take corrective measures.
Our approach is also related to Re-Ranking (Collins, 2002), which uses a generative
model to propose a k-best list of outputs, which are then ranked by a separate ranking
function. In contrast, rather than restricting to a generative model for producing potential
outputs, our approach leverages generic search over efficient search spaces guided by a
learned heuristic function that has minimal representational restrictions, and employs a
learned cost function to rank the candidate outputs. Recent work on generating multiple
diverse solutions in a probabilistic framework can be considered as another way of producing
candidate outputs. A representative set of approaches in this line of work are diverse Mbest (Batra, Yadollahpour, Guzman-Rivera, & Shakhnarovich, 2012), M-best modes (Park
& Ramanan, 2011; Chen, Kolmogorov, Zhu, Metaxas, & Lampert, 2013) and Determinantal
Point Processes (Kulesza & Taskar, 2012).
The general area of speedup learning studied in the planning and search community is
also related to our work (Fern, 2010). In these problems, the cost function is typically known
and the objective is to learn control knowledge (i.e., heuristic function) for directing a search
algorithm to a low-cost terminal node in the search space. For example, STAGE (Boyan &
Moore, 2000) learns an evaluation function over the states to improve the performance of
search, where value of a state corresponds to the performance of a local search algorithm
starting from that state, (Zhang & Dietterich, 1995) use Reinforcement Learning (RL)
methods to learn heuristics for job shop scheduling with the goal of minimizing the duration
of the schedule. Unlike the problems in planning and combinatorial optimization, such a
cost function is not given for the structured prediction problems. Therefore, our HC-Search
approach learns a cost function to score the structured outputs along with a heuristic
function to guide the search towards low cost outputs.
395

fiDoppa, Fern, & Tadepalli

7. Summary and Future Work
We introduced the HC-Search framework for structured prediction whose principal feature
is the separation of the cost function from search heuristic. We showed that our framework
yields significantly superior performance to state-of-the-art results, and allows an informative error analysis and diagnostics.
Our investigation showed that the main source of error of existing output-space approaches including our own approach (HC-Search) is the inability of cost function to correctly rank the candidate outputs produced by the output generation process. This analysis
suggests that learning more powerful cost functions, e.g., Regression trees (Mohan, Chen,
& Weinberger, 2011), with an eye towards anytime performance (Grubb & Bagnell, 2012;
Xu, Weinberger, & Chapelle, 2012) would be productive. Our results also suggested that
there is room to improve overall performance with better heuristic learning. Thus, another
direction to pursue is heuristic function learning to speed up the process of generating
high-quality outputs (Fern, 2010).
Future work includes applying this framework to more challenging problems in natural language processing (e.g., co-reference resolution, dependency parsing, and semantic
parsing) and computer vision (e.g., object detection in biological images Lam, Doppa, Hu,
Todorovic, Dietterich, Reft, & Daly, 2013, and multi-object tracking in complex sports
videos Chen, Fern, & Todorovic, 2014). The effectiveness of HC-Search approach depends
on the quality of the search space, and therefore, more work needs to be done in learning
to optimize search spaces by leveraging the problem structure. Similarly, studying pruning
techniques to further improve the efficiency of both learning and inference is another useful
direction.
Acknowledgements
The authors would like to thank the anonymous reviewers and Jason Eisner, the associate
editor, for their comments and feedback. The first author would also like to thank Tom
Dietterich for his encouragement and support throughout this work. This work was supported in part by NSF grants IIS 1219258, IIS 1018490 and in part by the Defense Advanced
Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under
Contract No. FA8750-13-2-0033. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect
the views of the NSF, the DARPA, the Air Force Research Laboratory (AFRL), or the
US government. A preliminary version of this article was published at AAAI-2013 (Doppa
et al., 2013)

Appendix A. Limited Discrepancy Search (LDS) Space
The Limited Discrepancy Search (LDS) space (Doppa et al., 2012, 2014a) is defined in terms
of a learned recurrent classifier h. Thus, we start by describing recurrent classifier and then
explain the key idea behind LDS space. For simplicity, we explain the main ideas using
a sequence labeling problem (handwriting recognition task) noting that they generalize to
non-sequence labeling problems (for full details see Doppa et al., 2012, 2014a).
396

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Figure 6: Illustration of recurrent classifier for handwriting recognition problem. The classifier predicts the labels in a left-to-right order. It makes the labeling decision
at each position greedily based on the character image and the predicted label
at previous position (shown by the dotted box). In this particular example, the
classifier makes a mistake at the first position and this error propagates to other
positions leading to a very bad output.

A.1 Recurrent Classifier
In a sequence labeling problem, the recurrent classifier produces a label at each position in
sequence, based on an input in that position and the predicted labels at previous positions
(Dietterich et al., 1995). If the learned classifier is accurate, then the number of incorrect
labeling decisions will be relatively small. However, even a small number of errors can
propagate and cause poor outputs.
Figure 6 illustrates recurrent classifier for a handwriting recognition example. The
classifier predicts the labels in a left-to-right order. It makes the labeling decision at each
position greedily based on the character image and the predicted label at the previous
position (shown by the dotted box). In this particular example, the classifier makes a
mistake at the first position and this error propagates leading to a very bad output (5
errors).
A.2 Limited Discrepancy Search (LDS)
LDS was originally introduced in the context of problem solving using heuristic search
(Harvey & Ginsberg, 1995). The key idea behind LDS is to realize that if the classifier
prediction was corrected at a small number of critical errors, then a much better output
397

fiDoppa, Fern, & Tadepalli

(a)

(b)

Figure 7: Illustration of Limited Discrepancy Search (LDS) for handwriting recognition
problem. For any given discrepancy set D, we can generate a unique output
by running the recurrent classifier with the changes from D. (a) LDS with one
discrepancy. If introduce a discrepancy at the first position with label s (shown
in red) and run the classifier, it is able to correct the two subsequent labels. (b)
LDS with two discrepancies. If we introduce an additional discrepancy at fifth
position with label c (shown in red) and run the classifier, we recover the target
output struct.

would be produced. LDS conducts a (shallow) search in the space of possible corrections in
the hope of finding an output better than the original.
Given a classifier h and a sequence of length T , a discrepancy is a pair (i, l) where
i  {1, . . . , T } is the index of sequence position and l is a label, which generally is different
from the prediction of the classifier at position i. For any set of discrepancies D, we
can generate a unique output h[D](x) by running the classifier with changes in D. The
discrepancies in D can be viewed as overriding the prediction of h at particular positions,
possibly correcting for errors, or introducing new errors. At one extreme, when D is empty,
we get the original output produced by the greedy classifier (see Figure 6). At the other
extreme, when D specifies a label at each position, the output is not influenced by h at
all and is completely specified by the discrepancy set. Figure 7 illustrates LDS for the
same handwriting example. If we introduce a discrepancy at the first position with label
s (shown in red) and run the classifier, it is able to correct the two subsequent labels (see
Figure 7(a)). If we introduce an additional discrepancy at fifth position with label c (shown
in red) and run the classifier, we recover the target output struct (see Figure 7(b)).
398

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Figure 8: An example Limited Discrepancy Search (LDS) space for handwriting recognition
problem. The highlighted state corresponds to the one with true output y  at
the smallest depth.

In practice, when h is reasonably accurate, we will be primarily interested in small
discrepancy sets relative to the length of the sequence. The problem is that we do not know
where the corrections should be made and thus LDS conducts a search over the discrepancy
sets, usually from small to large sets.

A.3 LDS Space
Given a recurrent classifier h, we define the corresponding limited-discrepancy search space
over complete outputs as follows. Each state in this search space is represented as (x, D)
where x is a input sequence and D is a discrepancy set. We view a state (x, D) as equivalent
to the input-output state (x, h[D](x)). The initial state function I simply returns (x, )
which corresponds to the original output of the recurrent classifier. The successor function
S for a state (x, D) returns the set of states of the form (x, D0 ), where D0 is the same as D,
but with an additional discrepancy. In this way, a path through the LDS search space starts
at the output generated by the recurrent classifier and traverses a sequence of outputs that
differ from the original by some number of discrepancies. Given a reasonably accurate h,
we expect that high-quality outputs will be generated at relatively shallow depths of this
search space and hence will be generated quickly.
399

fiDoppa, Fern, & Tadepalli

Figure 8 illustrates3 the limited-discrepancy search space. Each state consists of the
input x, a discrepancy set D and the output produced by running the classifier with the
specified discrepancy set, i.e., h[D](x). The root node has an empty discrepancy set. Nodes
at level one contain discrepancy sets of size one. The highlighted state corresponds to the
smallest depth state containing the target output.

Appendix B. Hardness Proof for HC-Search Consistency Problem
Theorem 2. The HC-Search Consistency Problem for greedy search and linear heuristic
and cost functions is NP-Hard even when we restrict to problems for which all possible
heuristic functions uncover a zero loss output.
Proof. We reduce from the Minimum Disagreement problem for linear binary classifiers,
which was proven to be NP-complete in the work of Hoffgen, Simon, and Horn (1995).
In one statement of this problem we are given as input a set of N , p-dimensional vectors
T = {x1 , . . . , xN } and a positive integer k. The problem is to decide whether or not there
is a p-dimensional real-valued weight vector w such that w  xi < 0 for at most k of the
vectors.
We first sketch the high-level idea of the proof. Given an instance of Minimum Disagreement, we construct an HC-Search consistency problem with only a single structured
training example. The search space corresponding to the training example is designed such
that there is a single node n that has a loss of zero and all other nodes have a loss of
1. Further for all linear heuristic functions all greedy search paths terminate at n , while
generating some other set of nodes/outputs on the path there. The search space is designed
such that each possible path from the initial node to n corresponds to selecting k or fewer
vectors from T , which we will denote by T  . By traversing the path, the set of nodes
generated (and hence must be scored by C), say N , includes feature vectors corresponding
to those in T  T  along with the negation of the feature vectors in T  . We further define
n to be assigned the zero vector, so that the cost of that node is 0 for any weight vector.
In order to achieve zero loss given the path in consideration, there must be a weight
vector wC such that wC  x  0 for all x  N . By our construction this is equivalent to
wC  x < 0 for x  T  . If this is possible then we have found a solution to the Minimum
Disagreement problem since |T  |  k. The remaining details show how to construct this
space so that there is a setting of the heuristic weights that can generate paths corresponding
to all possible T  in a way that all paths end at n . For completeness we describe this
construction below.
Each search node in the space other than n is a tuple (i, m, t) where 1  i  N ,
0  m  k, and t is one of 5 node types from the set {d, s+ , s , x+ , x }. Here i should
be viewed as indexing an example xi  T and m effectively codes how many instances in
T have been selected to be mistakes and hence put in T  . Finally, t encodes the type
of the search node with the following meanings which will become more clear during the
construction: d (decision), s+ (positive selection), s (negative selection), x+ (positive
instance), x (negative instance). The search space is constructed so that each example xi
3. It may not be clear from this example, but we allow over-riding the discrepancies to provide the opportunity to recover from the search errors.

400

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Figure 9: An example search space for T = {x1 , x2 , x3 } and k = 1. All greedy paths
terminate at the zero loss node n and no path selects more than one instance to
include in the mistake set T  .

is considered in order and a choice is made about whether to count it as a mistake (put
it in T  ) or not. This choice is made at decision nodes, which all have the form (i, m, d),
indicating that a decision is to be made about example i and that there have already been m
examples selected for T  . Each such decision node with m < k has two children (i, m, s )
and (i, m, s+ ), which respectively correspond to selecting xi to be in the mistake set or not.
Later we will show how features are assigned to nodes so as to allow the heuristic to make
any selection desired.
Each selection node has a single node as a child. In particular, a positive selection node
(i, m, s+ ) has the positive instance node (i, m, x+ ) as a child, while negative selection nodes
(i, m, s ) has the negative instance node (i, m, x ) as a child. Each such instance node
effectively implements the process of putting xi into T  or not as will become clear when
feature vectors are described below. After arriving at either a positive or negative instance
node, the consideration of xi is complete and we must move on to the decision for the next
example xi+1 . Thus, a positive instance node (i, m, x+ ) has the single child decision node
401

fiDoppa, Fern, & Tadepalli

(i + 1, m, d), while a negative instance node has a single child decision node (i + 1, m + 1, d),
noting that the number of mistakes is incremented for negative nodes.
The final details of the search space structure ensure that no more than k mistakes
are allowed and force all search paths to terminate at n . In particular, for any decision
node (i, m, d) with m = k, we know that no more mistakes are allowed and hence no more
decisions should be allowed. Thus, from any such node we form a path from it to n that
goes through positive instance nodes (i, m, x+ ), . . . , (N, m, x+ ), which reflects that none of
{xi , . . . , xN } will be in T  . Figure 9 shows an example search space for our construction.
Given the above search space, which has polynomial size (since k  N ), one can verify
that for any set of k or fewer instances T  there is a path from the root to n that goes
through the negative instance nodes for instances in T  and positive instance nodes for
instances in T  T  . Further, each possible path goes through either a positive or negative
instance node for each instance and no more than k negative nodes. Thus there is a direct
correspondence between paths and mistake sets T  .
We now describe how to assign features to each node in a way that allows for the
heuristic function to select each path and effectively construct the set T  . For any node
u the feature vector (u) = (x, s, b). The component x is an p-dimensional feature vector
and will correspond to one of the xi . The component s is an N -dimensional vector where
si  {1, 1} will implement the selection of instances. Finally b is a binary value that is
equal to 1 for all non-instance nodes and is 0 for both positive and negative instance nodes.
The mapping from nodes to feature vectors is as follows. Each decision node (i, m, d), is all
zeros, except for b = 1. Each positive selection node (i, m, s+ ) is all zeros except for si = 1
and b = 1. Negative selection nodes are similar except that si = 1. For a positive instance
node (i, m, x+ ) the feature vector is (xi , 0, 0) and for negative instance nodes (i, m, x ) the
feature vector is (xi , 0, 0). Finally the feature vector for n is all zeros.
The key idea to note is that the heuristic function can effectively select a positive or
negative selection node by setting the weight for si to be positive or negative respectively.
In particular, the set of negative selection nodes visited (and hence negative instance nodes)
correspond to the first k or fewer negative weight values for the s component of the feature
vector. Thus, the heuristic can select any set of negative nodes that it wants to go through,
but no more than k. On such a path there will be three types of nodes encountered that the
cost function must rank. First, there will be control nodes (decision and selection nodes)
that all have b = 1. Next there will be positive instance nodes that will have a feature
vector (xi , 0, 0) and no more than k negative instance nodes with feature vectors (xi , 0, 0).
The cost function can easily rank n higher than the control nodes by setting the weight for
b to be negative. Further if it can find heuristic weights for the x component that allows n
to be ranked highest then that is a solution to the original minimum disagreement problem.
Further if there is a solution to the disagreement problem it is easy to see that there will
also be a solution to the HC-Search consistency problem by selecting a heuristic that spans
the proper set T  .

References
Agarwal, S., & Roth, D. (2005). Learnability of Bipartite Ranking Functions. In Proceedings
of International Conference on Learning Theory (COLT), pp. 1631.
402

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Batra, D., Yadollahpour, P., Guzman-Rivera, A., & Shakhnarovich, G. (2012). Diverse MBest Solutions in Markov Random Fields. In Proceedings of European Conference on
Computer Vision (ECCV), pp. 116.
Boyan, J. A., & Moore, A. W. (2000). Learning Evaluation Functions to Improve Optimization by Local Search. Journal of Machine Learning Research (JMLR), 1, 77112.
Brill, E. (1995). Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics, 21 (4),
543565.
Chang, K.-W., Samdani, R., & Roth, D. (2013). A Constrained Latent Variable Model
for Coreference Resolution. In Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 601612.
Chang, M.-W., Ratinov, L.-A., & Roth, D. (2012). Structured Learning with Constrained
Conditional Models. Machine Learning Journal (MLJ), 88 (3), 399431.
Chen, C., Kolmogorov, V., Zhu, Y., Metaxas, D., & Lampert, C. H. (2013). Computing
the M Most Probable Modes of a Graphical Model. In Proceedings of International
Conference on Artificial Intelligence and Statistics (AISTATS).
Chen, S., Fern, A., & Todorovic, S. (2014). Multi-Object Tracking via Constrained Sequential Labeling. In To appear in Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).
Collins, M. (2000). Discriminative Reranking for Natural Language Parsing. In Proceedings
of International Conference on Machine Learning (ICML), pp. 175182.
Collins, M. (2002). Ranking Algorithms for Named Entity Extraction: Boosting and the
Voted Perceptron. In ACL.
Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., & Singer, Y. (2006). Online PassiveAggressive Algorithms. Journal of Machine Learning Research (JMLR), 7, 551585.
Daume III, H. (2006). Practical Structured Learning Techniques for Natural Language
Processing. Ph.D. thesis, University of Southern California, Los Angeles, CA.
DaumeIII, H., & Marcu, D. (2005). Learning as Search Optimization: Approximate Large
margin methods for Structured Prediction. In ICML.
Dietterich, T. G., Hild, H., & Bakiri, G. (1995). A Comparison of ID3 and Backpropagation
for English Text-to-Speech Mapping. Machine Learning Journal (MLJ), 18 (1), 5180.
Domke, J. (2013). Structured Learning via Logistic Regression. In Proceedings of Advances
in Neural Information Processing Systems (NIPS), pp. 647655.
Doppa, J. R., Fern, A., & Tadepalli, P. (2012). Output Space Search for Structured Prediction. In Proceedings of International Conference on Machine Learning (ICML).
Doppa, J. R., Fern, A., & Tadepalli, P. (2013). HC-Search: Learning Heuristics and Cost
Functions for Structured Prediction. In Proceedings of AAAI Conference on Artificial
Intelligence (AAAI).
Doppa, J. R., Fern, A., & Tadepalli, P. (2014a). Structured Prediction via Output Space
Search. Journal of Machine Learning Research (JMLR), 15, 13171350.
403

fiDoppa, Fern, & Tadepalli

Doppa, J. R., Yu, J., Ma, C., Fern, A., & Tadepalli, P. (2014b). HC-Search for Multi-Label
Prediction: An Empirical Study. In To appear in Proceedings of AAAI Conference on
Artificial Intelligence (AAAI).
Doppa, J. R., Yu, J., Tadepalli, P., & Getoor, L. (2009). Chance-Constrained Programs
for Link Prediction. In Proceedings of NIPS Workshop on Analyzing Networks and
Learning with Graphs.
Doppa, J. R., Yu, J., Tadepalli, P., & Getoor, L. (2010). Learning Algorithms for Link
Prediction based on Chance Constraints. In Proceedings of European Conference on
Machine Learning (ECML), pp. 344360.
Felzenszwalb, P. F., & McAllester, D. A. (2007). The Generalized A* Architecture. Journal
of Artificial Intelligence Research (JAIR), 29, 153190.
Fern, A. (2010). Speedup Learning. In Encyclopedia of Machine Learning, pp. 907911.
Fern, A., Yoon, S. W., & Givan, R. (2006). Approximate Policy Iteration with a Policy
Language Bias: Solving Relational Markov Decision Processes. Journal of Artificial
Intelligence Research (JAIR), 25, 75118.
Goldberg, Y., & Elhadad, M. (2010). An Efficient Algorithm for Easy-First Non-Directional
Dependency Parsing. In Proceedings of Human Language Technologies: Conference of
the North American Chapter of the Association of Computational Linguistic (HLTNAACL), pp. 742750.
Grubb, A., & Bagnell, D. (2012). SpeedBoost: Anytime Prediction with Uniform NearOptimality. Journal of Machine Learning Research - Proceedings Track, 22, 458466.
Hal Daume III, Langford, J., & Marcu, D. (2009). Search-based Structured Prediction.
Machine Learning Journal (MLJ), 75 (3), 297325.
Harvey, W. D., & Ginsberg, M. L. (1995). Limited Discrepancy Search. In Proceedings of
International Joint Conference on Artificial Intelligence (IJCAI), pp. 607615.
Hazan, T., & Urtasun, R. (2012). Efficient Learning of Structured Predictors in General
Graphical Models. CoRR, abs/1210.2346.
Hoffgen, K.-U., Simon, H.-U., & Horn, K. S. V. (1995). Robust Trainability of Single
Neurons. Journal of Computer and System Sciences, 50 (1), 114125.
Huang, L., Fayong, S., & Guo, Y. (2012). Structured Perceptron with Inexact Search.
In Proceedings of Human Language Technology Conference of the North American
Chapter of the Association of Computational Linguistics (HLT-NAACL), pp. 142
151.
Jiang, J., Teichert, A., Daume III, H., & Eisner, J. (2012). Learned Prioritization for
Trading Off Accuracy and Speed. In Proceedings of Advances in Neural Information
Processing (NIPS).
Kaariainen, M. (2006). Lower Bounds for Reductions. In Atomic Learning Workshop.
Keshet, J., Shalev-Shwartz, S., Singer, Y., & Chazan, D. (2005). Phoneme Alignment based
on Discriminative Learning. In Proceedings of Annual Conference of the International
Speech Communication Association (Interspeech), pp. 29612964.
404

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Khardon, R. (1999). Learning to Take Actions. Machine Learning Journal (MLJ), 35 (1),
5790.
Kulesza, A., & Taskar, B. (2012). Determinantal Point Processes for Machine Learning.
Foundations and Trends in Machine Learning, 5 (2-3), 123286.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: Probabilistic
Models for Segmenting and Labeling Sequence Data. In Proceedings of International
Conference on Machine Learning (ICML), pp. 282289.
Lam, M., Doppa, J. R., Hu, X., Todorovic, S., Dietterich, T., Reft, A., & Daly, M. (2013).
Learning to Detect Basal Tubules of Nematocysts in SEM Images. In ICCV Workshop
on Computer Vision for Accelerated Biosciences (CVAB). IEEE.
Li, Q., Ji, H., & Huang, L. (2013). Joint Event Extraction via Structured Prediction with
Global Features. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 7382.
McAllester, D. A., Hazan, T., & Keshet, J. (2010). Direct Loss Minimization for Structured
Prediction. In Proceedings of Advances in Neural Information Processing Systems
(NIPS), pp. 15941602.
Meshi, O., Sontag, D., Jaakkola, T., & Globerson, A. (2010). Learning Efficiently with
Approximate Inference via Dual Losses. In Proceedings of International Conference
on Machine Learning (ICML), pp. 783790.
Mohan, A., Chen, Z., & Weinberger, K. Q. (2011). Web-Search Ranking with Initialized
Gradient Boosted Regression trees. Journal of Machine Learning Research - Proceedings Track, 14, 7789.
Nivre, J. (2008). Algorithms for Deterministic Incremental Dependency Parsing. Computational Linguistics, 34 (4), 513553.
Park, D., & Ramanan, D. (2011). N-Best Maximal Decoders for Part Models. In Proccedings
of IEEE International Conference on Computer Vision (ICCV), pp. 26272634.
Payet, N., & Todorovic, S. (2013). SLEDGE: Sequential Labeling of Image Edges for
Boundary Detection. International Journal of Computer Vision (IJCV), 104 (1), 15
37.
Qian, X., Jiang, X., Zhang, Q., Huang, X., & Wu, L. (2009). Sparse Higher Order Conditional Random Fields for Improved Sequence Labeling. In Proceedings of International
Conference on Machine Learning (ICML).
Read, J., Pfahringer, B., Holmes, G., & Frank, E. (2011). Classifier Chains for Multi-Label
Classification. Machine Learning, 85 (3), 333359.
Ross, S., & Bagnell, D. (2010). Efficient Reductions for Imitation Learning. Journal of
Machine Learning Research - Proceedings Track, 9, 661668.
Ross, S., Gordon, G. J., & Bagnell, D. (2011). A Reduction of Imitation Learning and
Structured Prediction to No-Regret Online Learning. Journal of Machine Learning
Research - Proceedings Track, 15, 627635.
405

fiDoppa, Fern, & Tadepalli

Roth, D., & tau Yih, W. (2005). Integer Linear Programming Inference for Conditional
Random Fields. In Proceedings of International Conference on Machine Learning
(ICML), pp. 736743.
Samdani, R., & Roth, D. (2012). Efficient Decomposed Learning for Structured Prediction.
In Proceedings of International Conference on Machine Learning (ICML).
Sen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B., & Eliassi-Rad, T. (2008). Collective Classification in Network Data. AI Magazine, 29 (3), 93106.
Sontag, D., Meshi, O., Jaakkola, T., & Globerson, A. (2010). More data means less inference:
A pseudo-max approach to structured learning. In Proceedings of Advances in Neural
Information Processing Systems (NIPS), pp. 21812189.
Stoyanov, V., & Eisner, J. (2012). Easy-first Coreference Resolution. In Proceedings of
International Conference on Computational Linguistics (COLING), pp. 25192534.
Stoyanov, V., Ropson, A., & Eisner, J. (2011). Empirical Risk Minimization of Graphical
Model Parameters Given Approximate Inference, Decoding, and Model Structure.
In Proceedings of International Conference on Artificial Intelligence and Statistics
(AISTATS), pp. 725733.
Sutton, C. A., & McCallum, A. (2009). Piecewise Training for Structured Prediction.
Machine Learning Journal (MLJ), 77 (2-3), 165194.
Syed, U., & Schapire, R. (2010). A Reduction from Apprenticeship Learning to Classification. In Proceedings of Advances in Neural Information Processing Systems (NIPS),
pp. 22532261.
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-Margin Markov Networks. In Proceedings
of Advances in Neural Information Processing Systems (NIPS).
Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support Vector Machine Learning for Interdependent and Structured Output Spaces. In Proceedings of
International Conference on Machine Learning (ICML).
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large Margin Methods
for Structured and Interdependent Output Variables. Journal of Machine Learning
Research (JMLR), 6, 14531484.
Vogel, J., & Schiele, B. (2007). Semantic Modeling of Natural Scenes for Content-Based
Image Retrieval. International Journal of Computer Vision (IJCV), 72 (2), 133157.
Weiss, D. (2014). Structured Prediction Cascades code. http://code.google.com/p/
structured-cascades/.
Weiss, D., Sapp, B., & Taskar, B. (2010). Sidestepping Intractable Inference with Structured
Ensemble Cascades. In Proceedings of Advances in Neural Information Processing
Systems (NIPS), pp. 24152423.
Weiss, D., & Taskar, B. (2010). Structured Prediction Cascades. Journal of Machine
Learning Research - Proceedings Track, 9, 916923.
Wick, M. L., Rohanimanesh, K., Bellare, K., Culotta, A., & McCallum, A. (2011). SampleRank: Training Factor Graphs with Atomic Gradients. In Proceedings of International
Conference on Machine Learning (ICML).
406

fiHC-Search: A Learning Framework for Search-based Structured Prediction

Wick, M. L., Rohanimanesh, K., Singh, S., & McCallum, A. (2009). Training Factor Graphs
with Reinforcement Learning for Efficient MAP Inference. In Proceedings of Advances
in Neural Information Processing Systems (NIPS), pp. 20442052.
Xu, Y., Fern, A., & Yoon, S. (2009a). Learning Linear Ranking Functions for Beam Search
with Application to planning. The Journal of Machine Learning Research, 10, 1571
1610.
Xu, Y., Fern, A., & Yoon, S. W. (2009b). Learning Linear Ranking Functions for Beam
Search with Application to Planning. Journal of Machine Learning Research (JMLR),
10, 15711610.
Xu, Y., Fern, A., & Yoon, S. W. (2010). Iterative Learning of Weighted Rule Sets for
Greedy Search. In Proceedings of International Conference on Automated Planning
and Systems (ICAPS), pp. 201208.
Xu, Z., Weinberger, K., & Chapelle, O. (2012). The Greedy Miser: Learning under Test-time
Budgets. In Proceedings of International Conference on Machine Learning (ICML).
Ye, N., Lee, W. S., Chieu, H. L., & Wu, D. (2009). Conditional Random Fields with
High-Order Features for Sequence Labeling. In Proceedings of Advances in Neural
Information Processing Systems (NIPS), pp. 21962204.
Yu, H., Huang, L., Mi, H., & Zhao, K. (2013). Max-Violation Perceptron and Forced
Decoding for Scalable MT Training. In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP), pp. 11121123.
Zhang, W., & Dietterich, T. G. (1995). A Reinforcement Learning Approach to job-shop
Scheduling. In Proceedings of International Joint Conference on Artificial Intelligence
(IJCAI), pp. 11141120.

407

fiJournal of Artificial Intelligence Research 50 (2014) 235-264

Submitted 12/13; published 06/14

Reconnection with the Ideal Tree: A New Approach to
Real-Time Search
Nicolas Rivera

nicolas.rivera@kcl.ac.uk

Kings College London
London, WC2R 2LS, UK

Leon Illanes
Jorge A. Baier

lillanes@uc.cl
jabaier@ing.puc.cl

Departamento de Ciencia de la Computacion
Pontificia Universidad Catolica de Chile
Vicuna Mackenna 4860, Santiago, Chile

Carlos Hernandez

chernan@ucsc.cl

Departamento de Ingeniera Informatica
Universidad Catolica de la Santsima Concepcion
Caupolican 491, Concepcion, Chile

Abstract
Many applications, ranging from video games to dynamic robotics, require solving
single-agent, deterministic search problems in partially known environments under very
tight time constraints. Real-Time Heuristic Search (RTHS) algorithms are specifically
designed for those applications. As a subroutine, most of them invoke a standard, but
bounded, search algorithm that searches for the goal. In this paper we present FRIT, a
simple approach for single-agent deterministic search problems under tight constraints and
partially known environments that unlike traditional RTHS does not search for the goal
but rather searches for a path that connects the current state with a so-called ideal tree T .
When the agent observes that an arc in the tree cannot be traversed in the actual environment, it removes such an arc from T and then carries out a reconnection search whose
objective is to find a path between the current state and any node in T . The reconnection
search is done using an algorithm that is passed as a parameter to FRIT. If such a parameter
is an RTHS algorithm, then the resulting algorithm can be an RTHS algorithm. We show,
in addition, that FRIT may be fed with a (bounded) complete blind-search algorithm. We
evaluate our approach over grid pathfinding benchmarks including game maps and mazes.
Our results show that FRIT, used with RTAA*, a standard RTHS algorithm, outperforms
RTAA* significantly; by one order of magnitude under tight time constraints. In addition,
FRIT(daRTAA*) substantially outperforms daRTAA*, a state-of-the-art RTHS algorithm,
usually obtaining solutions 50% cheaper on average when performing the same search effort.
Finally, FRIT(BFS), i.e., FRIT using breadth-first-search, obtains best-quality solutions
when time is limited compared to Adaptive A* and Repeated A*. Finally we show that
Bug2, a pathfinding-specific navigation algorithm, outperforms FRIT(BFS) when planning
time is extremely limited, but when given more time, the situation reverses.

1. Introduction
Real-Time Heuristic Search (Korf, 1990) is an approach to solving single-agent search problems when a limit is imposed on the amount of computation that can be used for deliberac
2014
AI Access Foundation. All rights reserved.

fiRivera, Illanes, Baier, & Hernandez

tion. It is used for solving problems in which agents have to start moving before a complete
search algorithm can solve the problem and is especially suitable for problems in which the
environment is only partially known in advance.
An application of real-time heuristic search algorithms is goal-directed navigation in
video games (Bulitko, Bjornsson, Sturtevant, & Lawrence, 2011) in which computer characters are expected to find their way in partially known terrain. Game-developing companies
impose a constant time limit on the amount of computation per move close to one millisecond for all simultaneously moving characters (Bulitko et al., 2011). As such, real-time
search algorithms are applicable since they provide the main loop with quick moves that
allow implementing continuous character moves.
Most standard real-time heuristic search algorithmse.g., LRTA* (Korf, 1990) or LSSLRTA* (Koenig & Sun, 2009)are not algorithms of choice for videogame developers, since
they will require characters to re-visit many states in order to escape so-called heuristic
depressions, producing back-and-forth movements, also referred to as scrubbing (Bulitko
et al., 2011). The underlying reason for this behavior is that the heuristic used to guide
search must be updatedin a process usually referred to as heuristic learningwhenever
new obstacles are found. To exit so-called heuristic depressions, the agent may need to
revisit a group of states many times (Ishida, 1992).
By exploiting preprocessing (e.g., Bulitko, Bjornsson, Lustrek, Schaeffer, & Sigmundarson, 2007; Bulitko, Bjornsson, & Lawrence, 2010; Hernandez & Baier, 2011), one can produce Real-Time Heuristic Search algorithms that will control the agent in a way that is
sensible to the human observer. Give a map of the terrain, these algorithms compute information offline that can later be utilized online by a Real-Time Search algorithm to find
paths very quickly.
Unfortunately, preprocessing is not applicable in all settings. For example if one wants
to implement an agent which has no knowledge of the terrain, there is no map that is
available prior to search and hence no preprocessing can be carried out. On the other hand,
when knowledge about the terrain is only partial (i.e., the agent may know the location
of some of the obstacles but not all of them), using a plain Real-Time Heuristic Search
along with partial information about the map obtained from preprocessing (i.e., a perfect
heuristic computed for the partially known map) may still result in the same performance
issues described above.
In this paper we present FRIT, a real-time search algorithm that does not necessarily
rely on heuristic learning to control the agent, and that produces high-quality solutions in
partially known environments. While easily motivated by game applications, our algorithm
is designed for general search problems. An agent controlled by our algorithm always
follows the branch of a tree containing a family of solutions. We call such a tree the ideal
tree because the paths it contains are solutions in the world that is currently known to the
agent, but such solutions may not be legal in the actual world. As the agent moves through
the states in the ideal tree it will usually encounter states that are not accessible and which
block a solution in the ideal tree. When this happens, a secondary algorithmwhich is
given as a parameteris used to perform a search and reconnect the current state with
another state known to be in the ideal tree. After reconnection succeeds the agent is again
on a state of an updated ideal tree, and it can continue following a branch of the tree.
236

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

We discuss two different ways in which the algorithm given as a parameter to FRIT
can be useful for real-time scenarios. The first alternative is to feed FRIT with a real-time
search algorithm. This produces a standard real-time search algorithm, but not a so-called
agent-centered search algorithm (Koenig, 2001) since to verify that a node is connected to
the ideal tree it may need to consider states that are far away from its current position.
The second option to make FRIT amenable for real-time scenarios consists of feeding
FRIT with a bounded complete search algorithm; i.e., a complete search algorithm which in
each iteration expands a bounded number of states. After performing such a limited number
of expansions, the search algorithm may have not found a reconnecting path. In this case
the agent may or may not perform an action depending on domain-specific considerations.
In our implementation we chose to perform no move at all.
We evaluated our algorithm over standard game and maze pathfinding benchmarks using
both a blind, breadth-first search algorithm and two different real-time search algorithms
for reconnection. Even though our algorithm does not guarantee optimality, solutions returned, in terms of quality and total time, are significantly better than those returned by the
state-of-the-art real-time heuristic search algorithms we compared to, when the search effort
is comparable. Upon inspection of the route followed by the agent, we observe that when
using blind-search algorithms for reconnection they do not contain back-and-forth, irrational movements, and that indeed they look similar to solutions returned by so-called bug
algorithms (LaValle, 2006; Taylor & LaValle, 2009) developed by the robotics community.
As such, it usually detects states that do not need to be visited againsometimes referred
to as dead-ends or redundant states (Sturtevant & Bulitko, 2011; Sharon, Sturtevant, &
Felner, 2013)without implementing a specific mechanism to detect them.
We also compared our algorithm to incremental heuristic search algorithms that can be
modified to behave like a real-time search algorithm. We find that, although FRIT does not
reach the same solution quality, it can obtain solutions that are significantly better when
the time deadline is tight (under 40 sec).
Our algorithm is extremely easy to implement and, in case there is sufficient time for
pre-processing, can utilize techniques already described in the literature, like so-called compressed path databases (Botea, 2011), to compute an initial ideal tree. Furthermore, we
provide proofs for termination of the algorithm using real-time search and blind-search for
reconnection, and provide a bound on the number of moves required to find a solution in
arbitrary graphs.
Some of the contributions presented in this article have been published in a conference
paper (Rivera, Illanes, Baier, & Hernandez, 2013b). This articles extends the work and
includes new material. In particular:
 We describe a method to use our algorithm with a real-time search algorithm passed
as a parameter, and evaluate the results obtained when using two different real-time
algorithms.
 We provide proofs for the termination of algorithms obtained by using the aforementioned method, and a general proof for convergence applicable to all the algorithms
we propose.
 We incorporate a small optimization that affects the InTree[c] function described in
Section 3.
237

fiRivera, Illanes, Baier, & Hernandez

 We extend the previous empirical results by including maze benchmarks, which had
not been previously considered, and by evaluating on more problem instances. In
addition, we compare our algorithm with the Bug2 (Lumelsky & Stepanov, 1987)
pathfinding algorithm.
The rest of the paper is organized as follows. In Section 2 we describe the background
necessary for the rest of the paper. In Section 3 we describe a simple version of our algorithm
that is not real-time. In Section 4 we describe two alternative ways to make the algorithm
satisfy the real-time property. In Section 5 we present a theoretical analysis, followed by
a description of our experimental evaluation in Section 6. We then describe other related
work, and finish with a summary.

2. Background
The search problems we deal with in this paper can be described by a tuple P = (G, c, sstart , g),
where G = (S, A) is a directed graph that represents the search space. The set S represents
the states and the arcs in A represent all available actions. State sstart  S is the initial
state and state g  S is the goal state. We assume that S is finite, that A does not contain
elements of the form (s, s), that G is such that g is reachable from all states reachable from
sstart . In addition, we have a non-negative cost function c : A  R which associates a cost
with each of the available actions. Naturally, the cost of a path in the graph is the sum of
the costs of the arcs in the path. Finally g  S is the goal state. Note that even though
our definition considers a single goal state it can still model problems with multiple goal
states since we can always transform a multiple-goal problem into a single-goal problem by
adding a new state g to the graph and connecting the goals in the original problem to g
with a zero-cost action.
We define the distance function dG : S  S  R such that dG (s, t) denotes the cost
of a shortest path from s to t in the graph G. A heuristic for a search graph G is a nonnegative function h : S  R such that h(s) estimates dG (s, g). We say that h is admissible
if h(s)  dG (s, g), for all s  S. In addition, we say a heuristic h is consistent if for every
pair (s, t)  A it holds that h(s)  c(s, t) + h(t), and furthermore that h(g) = 0. It is simple
to prove that consistency implies admissibility.
2.1 Real-Time Search
Given a search problem P = (G, c, sstart , g), the objective of a real-time search algorithm is
to move an agent from sstart to g, through a low-cost path. The algorithm should satisfy
the real-time property, which means that the agent is given a bounded amount of time
for deliberating, independent of the size of the problem. After deliberation, the agent is
expected to move. After such a move, more time is given for deliberation and the loop
repeats.
Most Real-Time Heuristic Search algorithms rely on the execution of a bounded but
standard state-space search algorithm (e.g., A*, Hart, Nilsson, & Raphael, 1968). In order
to apply such an algorithm in partially known environments, they carry out their search
in a graph which may not correspond to the graph describing the actual environment. In
particular, in pathfinding in grid worlds, it is assumed that the dimensions of the grid are
238

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

known, and to enable search a free-space assumption (Zelinsky, 1992) is made, whereby grid
cells are regarded as obstacle-free unless they are known to be blocked.
Below we define a version of the free-space assumption for use with general search
problems. We assume a certain search graph GM is given as input to the agent. Such
a graph reflects what the agent knows about the environment, and is kept in memory
throughout execution. We assume that this graph satisfies the following generalized version
of the free-space assumption: if the actual search graph is G = (S, A), then GM is a spanning
supergraph of G, i.e. GM = (S, A0 ), with A  A0 . Note that because GM is a supergraph of
G then dGM (s, t)  dG (s, t) for all s, t  S, and that if h is admissible for GM then so it is
for G.
While moving through the environment, we assume the agent is capable of observing
whether or not some of the arcs in its search graph GM = (S, A0 ) are present in the actual
graph. Specifically, we assume that if the agent is in state s, it is able to sense whether
(s, t)  A0 is traversable in the actual graph. If an arc (s, t) is not traversable, then t is
inaccessible and hence the agent removes from GM all arcs that lead to t. Note that this
means that if GM satisfies the free-space assumption initially, it will always satisfy it during
execution.
Note the following fact implicit to our definitions: the environment is static. This is
because G, unlike GM , never changes. The free-space assumption also implies that the
agent cannot discover arcs in the environment that are not present in its search graph GM .
Many standard real-time search algorithms have the structure of Algorithm 1, which
solves the search problem by iterating through a loop that runs four procedures: lookahead,
heuristic learning, movement, and observation. The lookahead phase (Line 3) runs a timebounded search algorithm that returns a path that later determines how the agent moves.
The heuristic learning procedure (Line 4) changes the h-value of some of the states in the
search space to make them more informed. Finally, in the movement and observation phase
(Line 5), the agent moves along a path in the graph previously computed by the lookahead
search procedure. While moving, the agent observes the environment, and prunes away
from GM any arc that is perceived to be absent in the actual environment.
Algorithm 1: A Generic Real-Time Search Algorithm
Input: A search graph GM , a heuristic function h, a goal state g
Effect: The agent is moved from the initial state to a goal state if a trajectory exists
1 while the agent has not reached the goal state do
2
scurr  the current state.
3
path  LookAhead(scurr , g).
4
Update the heuristic function h.
5
Move the agent through path. While moving, observe the environment and
update GM , removing any non traversable arcs. Stop if an arc in path is removed
or if path has been traversed completely

RTAA* (Koenig & Likhachev, 2006) is an instance of Algorithm 1. In its lookahead
phase, it runs a bounded A* from scurr towards the goal state, which executes as regular
A* does but execution is stopped as soon the node with lowest f -value in Open is a goal
239

fiRivera, Illanes, Baier, & Hernandez

state or as soon as k nodes have been expanded. The path returned is the one that connects
scurr and the best state in Open (i.e., the state with lowest f-value in Open). On the other
hand, heuristic learning is carried out using Algorithm 2, which resets the heuristic of all
states expanded by the lookahead according to the f -value of the best state in Open. Koenig
and Likhachev (2006) prove that Algorithm 2 maintains the consistency of h if h is initially
consistent.
Algorithm 2: RTAA*s heuristic learning.
1
2
3
4

procedure Update ()
f   minsOpen g(s) + h(s)
for each s  Closed do
h(s)  f   g(s)

LRTA* (Korf, 1990) is also instance of Algorithm 1; indeed, LRTA* is an instance of
RTAA* when the k parameter is set to 1. In a nutshell, the most simple version of LRTA*
decides where to move to by just looking at the best of scurr s neighbors, and updates the
heuristic of scurr also based on the heuristic of its neighbors.
It is easy to see that both RTAA* and LRTA* satisfy the real-time property since
all operations carried out prior to movement take constant time. These algorithms are
also completein the sense that they always find a solution if one existswhen the input
heuristic is consistent. To prove completeness, heuristic learning is key. First, because
learning guarantees that the state the agent moves to has a lower heuristic value compared
to h(scurr ). Second, because the learning procedure guarantees that the heuristic is always bounded (in the case of RTAA*, and many other algorithms, consistency, and hence
admissibility is preserved during execution).
Finally, bounds for the number of execution steps are known for some of these algorithms.
LRTA*, for example, can solve any search problem in (|S|2  |S|)/2 iterations, where |S| is
the number nodes in the search graph (Edelkamp & Schrodl, 2011, Ch. 11).

3. Searching via Tree Reconnection
The algorithm we propose below moves an agent towards the goal state in a partially known
environment by following the arcs of a so-called ideal tree T . Whenever an arc in such a
tree cannot be traversed in the actual environment, it carries out a search to reconnect the
current state with a node in T . In this section we describe a simple version of our algorithm
which still does not satisfy the real-time property. Prior to that, we describe how T is built
initially.
3.1 The Ideal Tree
The ideal tree intuitively corresponds to a family of paths that connect some states of the
search space with the goal state. The tree is ideal because some of the arcs in the tree may
not exist in the actual search graph. Formally,

240

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

Definition 1 (Ideal Tree) Given a search problem P = (G, c, sstart , g), and a graph GM
that satisfies the generalized free-space assumption with respect to G, the ideal tree T over
P and GM is a directed acyclic subgraph of GM such that:
1. the goal state g is in T and has no parent (i.e., it is the root), and
2. if t is a child of s in T , then (t, s) is an arc in GM .
Note that arcs in an ideal tree are directed and point from the children to the parent
(Figure 1 depicts an ideal tree over a grid world). Properties 1 and 2 of Definition 1 imply
that given an ideal tree T and a node s in GM it suffices to follow the arcs in T (which are
also in GM ) to reach the goal state g. Property 2 corresponds to the intuition of T being
ideal : the arcs in T may not exist in the actual search graph because they correspond to
arcs in GM but not necessarily in G.
We note that in search problems in which the search graph is defined using a successor
generator (as is the case of standard planning problems) it is possible to build an ideal tree
by first setting which states will represent the leaves of the tree, and then computing a path
to the goal from those states. A way of achieving this is to relax the successor generator
(perhaps by removing preconditions), which allows including arcs in T that are not in the
original problem. As such, Property 2 does not require the user to provide an inverse of the
successor generator in planning problems.
The internal representation of an ideal tree T is straightforward. For each node s  S
we store a pointer to the parent of s, which we denote by p(s). Formally p : S  {null} 
S  {null}, p(null) = null and p(g) = null. Notice that this representation can actually be
used to describe a forest. Below, we sometimes refer to this forest as F and use the concept
of paths in F, that correspond to paths in some connected component of F that might or
not be T .
At the outset of search, the algorithm we present below starts off with an ideal tree
that is also spanning, i.e., such that it contains all the states in S. In the general case, a
spanning ideal tree can be computed by running Dijkstras algorithm from the goal node in
a graph like GM but in which all arcs are inverted. Indeed, if h(s) is defined as the distance
from g to s in such a graph, an ideal tree can be constructed using the following rules: for
every s  S \ {g} we define p(s) = arg minu:(s,u)A[GM ] c(s, u) + h(u), where A[GM ] are the
arcs of GM .
In some applications like real-time pathfinding in video games, when the environment is
partially known a priori it is reasonable to assume that there is sufficient time for preprocessing (Bulitko et al., 2010). In preprocessing time, one could run Dijkstras algorithm for
every possible goal state. If memory is a problem, one could use so-called compressed path
databases (Botea, 2011), which actually define spanning ideal trees for every possible goal
state of a given grid.
Moreover, in gridworld pathfinding in unknown terrain, an ideal tree over an obstaclefree GM can be quickly constructed using the information given by a standard heuristic.
This is because both the Manhattan distance and the octile distance correspond to the
value returned by a Dijkstra call from the goal state in 4-connected and 8-connected grids,
respectively. In cases in which the grid is completely or partially known initially but there
is no time for preprocessing, one can still feed the algorithm with an obstacle-free initial
241

fiRivera, Illanes, Baier, & Hernandez

graph in which obstacles are regarded as accessible from neighbor states. Thus, a call to an
algorithm like Dijkstra does not need to be made if there is insufficient time.
In the implementation of our algorithm for gridworlds we further exploit the fact that
the tree can be built on the fly. Indeed, we do not need to set p(s) for every s before starting
the search; instead, we set p(s) when it is needed for the first time. As such, no time is
spent initializing an ideal tree before search. More generally, depending on the problem
structure, specific implementations can exploit the fact that T need not be an explicit tree.
3.2 Following and Reconnecting
Our search algorithm, Follow and Reconnect with the Ideal Tree (FRIT, Algorithm 3)
receives as input a search graph GM , an initial state sstart , a goal state g, and a graph
search algorithm A. GM is the search graph known to the agent initially, which we assume
satisfies the generalized free-space assumption with respect to the actual search graph. A is
the algorithm used for reconnecting with the ideal tree. We require A to receive the following
parameters: an initial state, a search graph, and a goal-checking boolean function, which
receives a state as parameter.
Algorithm 3: FRIT: Follow and Reconnect with The Ideal Tree
Input: A search graph GM , an initial state sstart , a goal state g, and a search
algorithm A
1 Initialization: Let T be an ideal tree for GM .
2 Set s to sstart .
3 Set c to 0 and the color of each state in GM to 0.
4 Set hobstacle to .
5 while s 6= g do
6
Observe the environment around s.
7
for each newly discovered inaccesible state o do
8
if h(o) < hobstacle then
9
hobstacle  h(o).
10
11
12
13
14

Prune from T and GM any arcs that lead to o.
if p(s) = null then
cc+1
Reconnect (A, s, GM , InTree[c]()).
Movement: Move the agent from s to p(s) and set s to the new position of the
agent.

Algorithm 4: Reconnect component of FRIT
Input: A search algorithm A, an initial state s, a search graph GM and a goal
function fGOAL ()
1 Let  be the path returned by a call to A(s, GM , fGOAL ()).
2 Assuming  = s0 s1 , . . . sn make p(si ) = si+1 for every i  {0, . . . , n  1}.

242

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

In its initialization (Lines 14), it sets up an ideal tree T over graph GM . As discussed
above, the tree can be retrieved from a database, if pre-processing was carried out. If there
is no time for pre-processing but a suitable heuristic is available for GM , then it computes T
on the fly. In addition it sets the value of the variable c and the color of every state to 0, and
sets the variable hobstacle to . Note that if T is computed on the fly, then state colors can
also be initialized on the fly. hobstacle is used to maintain a record of the smallest heuristic
value observed in an inaccessible state. The role of state colors and hobstacle will become
clear below, when we describe reconnection and the InTree[c] function. After initialization,
in the main loop (Lines 614), the agent observes the environment and prunes from GM and
from T those arcs that do not exist in the actual graph. Additionally, it updates hobstacle if
needed. If the current state is s and the agent observes that its parent in T is not reachable
in the actual search graph, it sets the parent pointer of s, p(s), to null. Now the agent will
move immediately to state p(s) unless p(s) = null. In the latter case, s is disconnected
from the ideal tree T , and a reconnection search is carried out as shown in Algorithm 4.
This procedure calls algorithm A. The objective of this search is to reconnect to some state
in T : the goal function InTree[c]() returns true when invoked over a state in T and false
otherwise. Once a path is returned, we reconnect the current state with T through the path
found and then move to the parent of s. The main loop of Algorithm 3 finishes when the
agent reaches the goal.
3.2.1 The InTree[c] Function
A key component of reconnection search is the InTree[c] function that determines whether
or not a state is in T . Our implementationshown in Algorithm 5follows the parent
pointers of the state being queried and returns true when it reaches the goal state or a state
whose h-value is smaller than hobstacle . This last condition exploits the fact that the way T
is built (i.e.: the free-space assumption) ensures that all states that are closer to the goal
than any observed obstacle must still be in T . This is merely an optimization technique,
and removing it will incur in a small performance reduction, but no change in the actions
of the agent. In addition, it paints each visited state with a color c, given as a parameter.
The algorithm returns false if a state visited does not have a parent or has been painted
with c (i.e., it has been visited before by some previous call to InTree[c] while in the same
reconnection search).
Algorithm 5: InTree[c] function
Input: a vertex s
1 while s 6= g do
2
if h(s) < hobstacle then
3
return true

6

Paint s with color c.
if p(s) = null or p(s) has color c then
return false

7

s  p(s)

4
5

8

return true

243

fiRivera, Illanes, Baier, & Hernandez

Figure 1 shows an example execution of the algorithm in an a priori unknown grid
pathfinding task. As can be observed, the agent moves until a wall is encountered, and
then continues bordering the wall until it solves the problem. It is simple to see that, had
the vertical been longer, the agent would have traveled beside the wall following a similar
down-up pattern.
This example reflects a general behavior of this algorithm in grid worlds: the agent
usually moves around obstacles, in a way that resembles bug algorithms (LaValle, 2006;
Taylor & LaValle, 2009). This occurs because the agent believes there is a path behind
the wall currently known and always tries to move to such a state unless there is another
state that allows reconnection and that is found first. A closer look shows that some times
the agent does not walk exactly besides the wall but moves very close to it, performing a
sort of zig-zag movement. This can occur if the search used does not consider the cost of
diagonals. Breadth-First Search (BFS) or Depth-First Search (DFS) may sometimes prefer
using two diagonals instead of two edges with cost 1.
To avoid this problem we can use a variant of BFS, that, for a few iterations, generates
first the non-diagonal successors and later the diagonal ones. For nodes deeper in the search
it uses the standard ordering (e.g., clockwise). Such a version of BFS achieves in practice
a behavior very similar to a bug algorithm.1 This approach was explored in previous
work (Rivera et al., 2013b), and the overall improvements were shown to be small. For
this paper, we use standard BFS. See Section 6.4 for a more detailed comparison to bug
algorithms.
Note that our algorithm does not perform any kind of update to the heuristic h. This
contrasts with traditional real-time heuristic search algorithms, which rely on increasing
the heuristic value of h to exit the heuristic depressions generated by obstacles. In such a
process they may need to revisit the same cell several times.

4. Satisfying the Real-Time Property
FRIT, as presented, does not satisfy the real-time property. There are two reasons for this:
R1. the number of states expanded by a call to the algorithm passed as a parameter, A,
depends on the search graph GM rather than on a constant; and,
R2. during the execution of A, each time A checks whether or not a state is connected to
the ideal tree T , function InTree[c] may visit a number of states dependent on the
size of the search graph GM .
Below we present two natural approaches to making FRIT satisfy the real-time property.
The first approach is to use a slightly modified, generic real-time heuristic search algorithm
as a parameter to the algorithm. The resulting algorithm is a real-time search algorithm
both because it satisfies the real-time property and because the time between movements
is bounded by a constant. The second approach limits the amount of reconnection search
but does not guarantee that the time between movements is limited by a constant.
1. Videos can be viewed at http://web.ing.puc.cl/~jabaier/index.php?page=research.

244

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

1

2

3

4

5

A

6

7

8

1

2

3

4

5

6

7

A

8

1

2

3

4

5

6

7

A

8

1

B

B

B

B

C

C

C

C

D

D

D

D

E

E

E

F

F

(a)

g

g

F

(b)

2

3

4

5

6

7

8

A

E

g

F

(c)

(d)

Figure 1: An illustration of some of the steps of an execution over a 4-connected grid
pathfinding task, where the initial state is cell D3, and the goal is E6. The search algorithm
A is breadth-first search, which, when expanding a cell, generates the successors in clockwise
order starting with the node to the right. The position of the agent is shown with a black
dot. (a) shows the true environment, which is not known a priori by the agent. (b) shows the
p pointers which define the ideal tree built initially from the Manhattan heuristic. Following
the p pointers, the algorithm leads the agent to D4, where a new obstacle is observed. D5 is
disconnected from T and GM , and a reconnection search is initiated. (c) shows the status
of T after reconnection search expands state D4, finding E4 is in T . The agent is then
moved to E4, from where a new reconnection search expands the gray cells shown in (d).
The problem is now solved by simply following the p pointers.
4.1 FRIT with Real-Time Heuristic Search Algorithms
A natural way of addressing R1 is by using a real-time search algorithm as parameter to
FRIT. It turns out that it is not possible to plug into FRIT a real-time search algorithm
directly without modifications. However, the modifications we need to make to Algorithm 1
are simple. We describe them below.
The following two observations justify the changes that need to be made to the pseudocode of the generic real-time search algorithm. First we observe that the objective of the
lookahead search procedure of real-time heuristic algorithms like Algorithm 1 is to search
towards the goal and thus the heuristic h estimates the distance to the goal. However,
FRIT carries out search with the sole objective of reconnecting with the ideal tree, which
means that both the goal condition and the heuristic have to be changed. Second, one of
the main ideas underlying FRIT is to use and maintain the ideal tree T ; that is, when the
agent has found a reconnecting path, the p function needs to be updated accordingly.
Algorithm 6 shows the pseudocode for the modified generic real-time heuristic search
algorithm, which has two main differences with respect to Algorithm 1. First, the goal
condition is now given by function gT , which returns true if evaluated with a state that is
in T . Second, Line 5 of Algorithm 6 connects the states in the path found by the lookahead
search to T . This implies also that the Reconnect procedure described in Algorithm 4
needs to be changed by that described in Algorithm 7.
Now we turn our attention to how we can guide the search towards reconnection using
reconnecting heuristics. Before giving a formal definition for these heuristics, we introduce
a little notation. Given the graph GM = (S, A) and the ideal tree T for GM over a problem
P with goal state g, we denote by ST the set of states in T . Now we are ready to define
reconnecting heuristics formally.

245

fiRivera, Illanes, Baier, & Hernandez

Algorithm 6: A Generic Real-Time Search Algorithm for FRIT
Input: A search graph GM , a heuristic function h, a goal function gT () that
receives a state as parameter.
Effect: The agent is moved from the initial state to a goal state if a trajectory
exists. The ideal tree T is updated.
1 while the agent has not reached a goal state do
2
scurr  the current state
3
path  LookAhead(scurr , gT ())
4
Update the heuristic function h.
5
Given path = s0 s1 . . . sn , update T so that p(si ) = si+1 for every
i  {0, . . . , n  1}.
6
Move the agent through the path. While moving, observe the environment and
update GM and T , removing any non traversable arcs and updating hobstacle if
needed. Stop if the current state has no parent in T .

Algorithm 7: Reconnect component for FRIT with a real-time algorithm
Input: A real-time search algorithm A, an initial state s, a search graph GM and a
goal function fGOAL ()
1 Call A(s, GM , fGOAL ).

Definition 2 (Admissible Reconnecting Heuristic) Given an ideal tree T over graph
GM and a subset B of ST , we say function h : S  R+
0 is an admissible reconnecting
heuristic with respect to B iff for every s  S it holds that h(s)  dGM (s, s0 ), for any
s0  B.
Intuitively, a reconnecting heuristic with respect to B is an admissible heuristic over
the graph GM where the set of goal states is defined as B. As such, when Algorithm 6
is initialized with a reconnecting heuristic, search will be guided towards those connected
states.
Depending on how we choose B, we may obtain a different heuristic. At first glance, it
may seem sensible to choose B as ST . However, it is not immediately obvious how one would
maintain (i.e., learn) such a heuristic efficiently. This is because both T and ST change
when new obstacles are discovered. Initially ST contains all states but during execution,
some states in S cease to belong to ST as an arc is removed and other become members
after reconnection is completed.
In this paper we propose to use an easy-to-maintain reconnecting heuristic, which, for
all s is initialized to zero and then is updated in the standard way. Below, we prove that
if the update procedure has standard properties, such an h corresponds to a reconnecting
heuristic for the subset B = V (E) of ST , where V (E) is defined as follows:
B = V (E) = {s  ST : s has not been visited by the agent and s 6 E}.
In addition, E must be set to the set of states whose heuristic value has been potentially
updated by the real-time search algorithm. The reason for this is that, by definition, all
246

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

states in B should have their h-value set to zero and thus we do not want to include in B
states that have been potentially modified.
Now we prove that a simple heuristic initialized as 0 for all states and updated in a
standard way is indeed a reconnecting heuristic.
Proposition 1 Let FRIT be modified to initialize h as the null heuristic. Let E be defined
as the set of states that the update procedure has potentially updated.2 Furthermore, assume
that A is an instance of Algorithm 6 satisfying:
P1. gT (s) returns true iff s  ST .
P2. Heuristic learning maintains consistency; i.e., if h is consistent prior to learning, then
it remains as such after learning.
Then, along the execution of FRIT(A), h is a reconnecting heuristic with respect to
B = V (E).
Proof: First we observe that initially h is a reconnecting heuristic because it is set to
zero for every state. Let s be any state in S and s0 be any state in B. We prove that
h(s)  d(s, s0 ). Indeed, let  = s0 s1 . . . sn , with s0 = s and sn = s, be a shortest path
between s and s0 . Since h is consistent, it holds that
h(si )  c(si , si+1 ) + h(si+1 ),

(1)

for any i  {0, . . . , n  1}. From where we can write
h(s)  h(s0 ) =

n1
X

h(si )  h(si+1 ) 

i=0

n1
X

c(si , si+1 ) = d(s, s0 )

(2)

i=0

Now observe that because s0  B, then the h-value of s0 could have not been updated by
the algorithm and therefore h(s0 ) = 0, which substituted in Inequality 2, proves the desired
result.

4.1.1 Tie-Breaking
In pathfinding, the standard approach to tie-breaking among states with equal f-values is
to select the state with highest g-value. For the reconnection search, we propose a different
strategy based on selecting a state based on a user-given heuristic that should guide search
towards the final goal state. For example, in our experiments on grids we break ties by
selecting the state with smallest octile distance to the goal. Intuitively, among two otherwise
equal states, we prefer the one that seems to be closer to the final goal. This seems like a
reasonable way to use information that is commonly used by other search algorithms, but
unavailable to the reconnection search due to the initial use of the null heuristic.
2. Note that in practice, E is a very natural set of states. For example if RTAA* is used, the set of states
that have potentially been updated are those that were expanded by some A* lookahead search.

247

fiRivera, Illanes, Baier, & Hernandez

4.1.2 Making InTree[c] Real-Time
At the beginning of Section 4 we identified R1 and R2 as the two reasons why FRIT does not
satisfy the real-time property, and then discussed how to address R1 by using a real-time
search algorithm. Now we discuss how to address R2.
To address R2, we simply make InTree[c] a bounded algorithm. All real-time search
algorithms receive a parameter that allows them to bound the computation carried out per
search. Assume that Algorithm 6 receives k as parameter. Furthermore, assume without
loss of generality that lookahead search is implemented with an algorithm that constantly
expands states (such as bounded A*). Then we can always choose implementation-specific
constants NE and NT , associated respectively to the expansions performed during lookahead
and the operation that follows the p pointer in the InTree[c] function. Given that e is the
number of expansions performed by lookahead search and f is the number of times the p
pointer has been followed in a run of the real-time search algorithm, we modify the stop
condition of InTree[c] to return false if NE  e + NT  f > k. Also, we modify lookahead
search to stop if the same condition holds true.
Henceforth we call FRITRT the algorithm that addresses R1 and R2 using a real-time
search algorithm and a bounded version of InTree[c]. Note that because the computation per iteration of FRITRT is bounded, the time between agent moves is bounded, and
thus FRITRT can be considered a standard real-time algorithm, as originally defined by
Korf (1990). Note however that the time to reach a state connected to the ideal tree is not
bounded, as several calls to the real-time algorithm may be required before reaching such
a state.
4.2 FRIT with Bounded Complete Search Algorithms
In the previous section we proposed to use a standard real-time heuristic search algorithm
to reconnect with the ideal tree. A potential downside of such an approach is that those
algorithms usually find suboptimal solutions which sometimes require re-visiting the same
state many timesa behavior usually referred to as scrubbing (Bulitko et al., 2011). In
applications in which the quality of the solution is important, but in which there are still
real-time constraints it is possible to make FRIT satisfy the real-time property in a different
way.
Imagine for example, that we are in a situation in which FRIT is given a sequence of
time frames, each of which is very short. After each time frame FRIT is allowed to return a
movement which is performed by the agent. Such a model for real-time behavior has been
termed as the game time model (Hernandez, Baier, Uras, & Koenig, 2012b) since it has a
clear application to video games in which the games main cycle will reserve a fixed and
usually short amount of time to plan the next move for each of the automated characters.
To accommodate this behavior in FRIT we can apply the same simple idea already described in Section 4.1.2, but using a complete search algorithm for reconnection rather than a
real-time search algorithm. As described above this simply involves choosing implementationspecific constants NE and NT , associated respectively to the expansions performed by the
(now complete) search algorithm for reconnection and the operation that follows the p
pointer in the InTree[c] function. As before, given that e is the number of expansions
performed by reconnection search and f is the number of times the p pointer we modify
248

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

the Reconnect algorithm to return an empty path as soon as NE e + NT f > k and save
all local variables used by A and InTree[c]. Once Reconnect is called again, search is
resumed at the same point it was in the previous iteration and e and f are set to 0.
Note that instead of returning an empty path other implementations may choose to
move the agent in a fashion that is meaningful for the specific application. We leave a
thorough discussion on how to implement such a movement strategy out of the scope of this
paper since we believe that such a strategy is usually application-specific. If a movement
ought to be carried out after each time frame, the agent could choose to move back-andforth, or choose any other moving strategy that allows it to follow the reconnection path
once it is found. Later, in our experimental evaluation, we choose not to move the agent
if computation exceeds the parameter and discuss why this seems a good strategy in the
application we chose.
Note that if a non-empty path is returned after each given time frame, then FRIT,
modified in the way described above, is also a real-time search algorithm, as originally
defined by Korf (1990). Finally, we note that implementing the stop-and-resume mechanism
described above is easy for most search algorithms.

5. Theoretical Analysis
The results described in this section prove the termination of the algorithms and present
explicit bounds on the number of agent moves performed by FRIT and FRITRT before
reaching the goal. Additionally, we show that both algorithms converge in the second run
so that subsequent executions of the algorithm result in identical paths. Our first theorem
is correctness of the InTree[c] function.
5.1 Proofs for InTree[c]
To determine whether or not a state s belongs to the ideal tree, our InTree[c] function
(Algorithm 5) follows the p pointers until the goal is reached or until some state whose
h-value is smaller than hobstacle is reached. Here we prove that InTree[c] is correct in the
sense that it returns true iff a state s belongs to the ideal tree. We start by proving the
following intermediate result.
Lemma 1 Let H = {s : h(s) < hobstacle }. Reconnection search never modifies the parent
pointer of a state s  H.
Proof: Take any state s  H. It is clear that any call to InTree[c](s) will immediately
return true (Algorithm 5, Lines 2 and 3). This effectively ends the search, and a path that
ends in s is selected. This path does not change the parent of s.

Note that the property described in the Lemma holds both for FRIT and FRITRT . The
bounded version of InTree[c] used for FRITRT will always answer true when called for
a state in H. Indeed, all states in H are part of the reconnection target set B, and are
correctly identified as such during execution.
Theorem 1 When T is initialized as described in Section 3 and the color c is set to increment for each reconnection search, InTree[c], as described in Algorithm 5, returns true
for a state s iff s  T .
249

fiRivera, Illanes, Baier, & Hernandez

Proof: Note that besides the exit condition established in Lines 2 and 3, the algorithm is
trivially correct. It follows the parent pointers, returns true only if it reaches the goal, and
returns false if it reaches a dead end or a state that has already been checked.
Let H be as defined in Lemma 1. We need to prove that all states s  H are in T . We
know that all such states have their original parent pointers set through the construction
of T described in Section 3. Note that all the paths in the initial Ideal Tree are monotonic;
for every state s different from the goal it holds that h(s)  h(p(s)). From this, we know
that for any state s  H, p(s)  H is true. This proves that all ancestors of s are in H, and
therefore they represent a path that existed in the initial T and has not been modified. 
5.2 Termination and Bound for FRITRT
Our first result proves termination of the algorithm when it uses a real-time search algorithm
as parameter. We provide an explicit bound on the number of agent moves until reaching
the goal.
Theorem 2 Consider the same conditions of Proposition 1 and let A be a modified real-time
search algorithm as described in Algorithm 6, that requires at most fA (x) agent moves to
solve any problem with x states, and such that it never updates the h-value of the goal state.
Then an agent controlled by FRITRT (A) reaches a goal state performing O(|S|fA (|S|))
moves.
Proof: Let M denote the elements in the state space S that are inaccessible from any
state in the connected component that contains sstart . Furthermore, let T be the ideal tree
computed at initialization. Note that, by Proposition 1, we know that we use a reconnecting
heuristic. By definition, this means the heuristic is always admissible for some subset of
states in T that will always contain at least g. Therefore, we know that all reconnections are
eventually successful and that each reconnection takes at most fA (|S|) steps. Notice that
the agent moves at most |S| steps in the Ideal Tree before it reaches an inaccessible state.
Because reconnection search is only invoked after a new inaccessible state is detected, it can
be invoked at most |M| times. By the definition of T , we know that after |M| reconnections,
the agent must be able to reach the goal by following T . Therefore, the total number of
steps is at most |S| + |M|(fA (|S|) + |S|)  O(|S|fA (|S|)).

The average length of the paths found by FRITRT can be expected to be much lower. Indeed, the number of reconnections is bounded by the number of obstacles that are reachable
by some state in GM , which in many cases is much lower than the total number inaccessible
states.
5.3 Termination and Bound for FRIT
The following result provides a bound on the length of the solutions found by FRIT.
Theorem 3 Given an initial tree GM that satisfies the generalized free-space assumption,
2
then FRIT solves P in at most (|S|+1)
agent moves.
4
250

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

Proof: Let M and T be as described in the proof of Theorem 2. Note that the goal
state g is always part of T , thus T can never become empty and reconnection will always
succeed. As for FRITRT , reconnection search can be invoked at most |M| times. Between
two consecutive calls to reconnection search, the agent moves in a tree and thus cannot visit
any state twice. Hence, the number of states visited between two consecutive reconnection
searches is at most |S|  |M|. We conclude that the number of moves until the algorithm
terminates is
(|M| + 1)(|S|  |M|),
(3)
which is largest when |M| =


|S|1
2 .

Substituting this value in (3) gives the desired result.

Again, the average complexity can be expected to be much lower than this bound.
5.4 Convergence
The following results prove that after termination of either FRIT or FRITRT , the agent
knows a solution to the problem that is possibly shorter than the one just found.
Lemma 2 Let F be the forest defined by the p pointers. Throughout the execution of either
FRIT or FRITRT , there is a path  in F that goes from sstart to the current position of the
agent.
Proof: The proof is done by induction over the number of steps taken by the agent. Let s
represent the current position of the agent. Initially, the proposition is trivial, as sstart = s.
Let s0 be the position of the agent after moving. By the induction hypothesis, we know
there is a path  from sstart to s. If s0 is not in , we know that the parent pointers of the
states in  different from s have not been modified, and therefore the path that extends 
by appending s0 is valid and satisfies the property. If s0 is in , we know that the parent
pointers of states in  that appear before s0 have not been modified, and therefore there is
a valid subpath of  that goes from sstart to s0 which satisfies the property.

Theorem 4 Running the algorithm for a second time over the same problem, without reinitializing the ideal tree, results in an execution that never runs reconnection search and finds
a potentially better solution than the one found in the first run.
Proof: The proof is straightforward from Lemma 2. At the end of the execution, there is
a path in F, and specifically in T , from sstart to g. Note that all states on the path have
necessarily been visited during the first execution, which ensures that this new path is at
most as long as the one resulting from the first execution.

Note that Theorem 4 implies that our algorithm can return a different path in a second
trial, which can be viewed as an optimized solution that does not contain the loops that
the first solution had. The second execution of the algorithm is naturally very fast, because
reconnection search is not required.
It is interesting to note that this approach could be used with any other real-time search
algorithm. By storing for each visited state the direction in which the agent moved away
from it, a path with no loops that goes from sstart to g can be immediately extracted as
soon as the execution finishes.
251

fiRivera, Illanes, Baier, & Hernandez

6. Empirical Evaluation
The objective of our experimental evaluation was to compare the performance of our algorithm with various state-of-the-art algorithms on the task of pathfinding with real-time
constraints. We chose this application since it seems to be the most straightforward application of real-time search algorithms.
We compared three classes of search algorithms. For the first class, we considered stateof-the-art real-time heuristic search algorithms and the corresponding versions of FRITRT
that result when it is fed with these. Specifically, we compare to RTAA* (Koenig &
Likhachev, 2006) and daRTAA* (Hernandez & Baier, 2012), a variant of RTAA* that
may outperform it significantly. In both versions of the algorithm, the tree ideal tree is not
built at the outset of search but rather built on-the-fly, using the heuristic function.
For the second class, we compared FRIT fed with a breadth-first-search algorithm to the
incremental heuristic search algorithms Repeated A* (RA*) and Adaptive A* (AA*). We
chose them on the one hand because it is fairly obvious how to modify them to satisfy the
real-time property following the same approach we follow with FRIT, and on the other hand
because they have reasonable performance. Indeed, we do not include D* Lite (Koenig &
Likhachev, 2002) since it has been shown that Repeated A* is faster than D* Lite in most
instances of the problems we evaluate here (Hernandez, Baier, Uras, & Koenig, 2012a).
Other incremental search algorithms are not included since it is not the focus of this paper
to propose strategies to make various algorithms satisfy the real-time property.
Finally we compare our algorithm to Bug2 (Lumelsky & Stepanov, 1987) a so-called bug
algorithm, which is an algorithm specifically designed for path-planning. Bug algorithm
need very limited computational requirements to make decisions.
Repeated A* and Adaptive A* both run a complete A* search over the currently known
search graph until the goal is reached. Then the path found is followed. While following
the path, the search graph is updated with newly found obstacles. The agent stops when
it reaches the goal is reached or when the path is blocked by an obstacle. When this
happens, they iterate by running another A* to the goal. To make both algorithms satisfy
the real-time property, we follow an approach similar to that employed in the design of the
algorithm Time-Bounded A* (Bjornsson, Bulitko, & Sturtevant, 2009). In each iteration,
we only allow the algorithm to expand at most k states. If no path to the goal is found the
agent does not move. Otherwise (the agent has found a path to the goal), the agent makes
a single move on the path.
For the case of FRIT(BFS), we satisfy the real-time property as discussed above by
setting both constants, NE and NT , to 1. This means that in each iteration, if the current
state has no parent then only k states can be expanded/visited during the reconnection
search and if no reconnection path is found the agent is not moved. Otherwise, if the
current state has a non-null parent pointer, the agent follows the pointer.
Therefore in each iteration of FRIT(BFS), Repeated A* or Adaptive A* two things can
happen: either the agent is not moved or the agent is moved one step. This moving strategy
is sensible for applications like video games where, although characters are expected to move
fluently, we do not want to force the algorithm to return an arbitrary move if a path has
not been found, since that would introduce moves that may be perceived as pointless by
the users. In contrast, real-time search algorithms return a move at each iteration.
252

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

4 mazes - 500 runs

4 mazes - 500 runs
10000000

FRIT_rt(RTAA)
FRIT_rt(daRTAA)
RTAA
daRTAA

1000000

Average Solution Cost (log-scale)

Average Solution Cost (log-scale)

10000000

100000

10000

1000

0

20

40

60

80

100

120

FRIT_rt(RTAA)
FRIT_rt(daRTAA)
RTAA
daRTAA
1000000

100000

10000

140

Average time per planning episode (us)

0

20

40

60

80

100

120

Average time per planning episode (us)

(a) Real-time algorithms in games.

(b) Real-time algorithms in mazes.

Figure 2: Real-time algorithms: Total Iterations versus Time per Episode
We use eight-neighbor grids in the experiments since they are often preferred in practice,
for example in video games (Bulitko et al., 2011). The algorithms are evaluated in the
context of goal-directed navigation in a priori unknown grids. The agent is capable of
detecting whether or not any of its eight neighboring cells is blocked and can then move to
any one of the unblocked neighboring cells. The user-given h-values are the octile distances
(Bulitko & Lee, 2006).
To carry out the experiments, we used twelve maps from deployed video games and four
different mazes. Six of the maps are taken from the game Dragon Age, and the remaining
six are taken from the game StarCraft. Both the maps and the mazes were retrieved from
Nathan Sturtevants pathfinding repository (Sturtevant, 2012).3
We averaged our experimental results over 500 test cases with a reachable goal cell for
each map. For each test case the start and goal cells were chosen randomly. All realtime algorithms were run with 10 different parameter values. The incremental algorithms
were run to completion once per test case, after which the results were processed to show
the behavior corresponding to using 150,000 different values for the k parameter. All the
experiments were run on a 2.00GHz QuadCore Intel Xeon machine running Linux.
6.1 Analysis of the Results for Real-Time Search Algorithms
Figure 2 shows two plots of the average solution cost versus average time per planning
episode for the four real-time search algorithms in games and mazes benchmarks.
We observe that for the games benchmarks FRITRT outperforms RTAA* and daRTAA*
substantially. FRITRT (daRTAA*) finds solutions of about half the cost of those found by
daRTAA* for any given time deadline. Moreover, the average planning time per episode
needed by FRITRT (daRTAA*) to obtain a particular solution quality is about one half of
that needed by daRTAA*. The improvements are more pronounced with FRITRT (RTAA*),
3. Maps used from Dragon Age: brc202d, orz702d, orz900d, ost000a, ost000t and ost100d whose sizes are
481  530, 939  718, 656  1491, 969  487, 971  487, and 1025  1024 cells respectively. Maps from
StarCraft: ArcticStation, Enigma, Inferno, JungleSiege, Ramparts and WheelofWar of sizes 768  768,
768  768, 768  768, 768  768, 512  512 and 768  768 cells respectively.
The four mazes all have the same size, 512  512, and different corridor widths: 4, 8, 16 and 32.

253

fiRivera, Illanes, Baier, & Hernandez

100000000
FRIT(BFS)
RA
AA
optimal

1000000

Average Solution Length (log-scale)

Average Solution Length (log-scale)

10000000

100000

10000

1000

0

100

200

300

400

500

600

Average time per planning episode (us)

FRIT(BFS)
RA
AA
optimal

10000000

1000000

100000

10000

1000

0

200

400

600

800

1000

Average time per planning episode (us)

(a) Incremental algorithms in games.

(b) Incremental algorithms in mazes.

Figure 3: Incremental algorithms: Total Iterations versus Time per Episode
where solutions for a given time deadline are at least three times cheaper than pure RTAA*
and up to one order of magnitude cheaper for very small time frames. It is interesting to
note that even though daRTAA* improves significantly over RTAA*, FRITRT (daRTAA*)
is only marginally better than FRITRT (RTAA*).
For mazes, the FRITRT variants seem to be slightly better than daRTAA*, with bigger improvements in performance noticeable as the time deadlines are increased. The
best solutions found by daRTAA* and FRITRT (daRTAA*) are of comparable lengths, but
FRITRT (daRTAA*) finds these solutions requiring slightly more than half of the time per
planning episode than daRTAA*.
6.2 Analysis of the Results for Incremental Algorithms Modified to Satisfy
the Real-Time Property
Figure 3 shows two plots of the average number of agent steps versus average time per
planning episode for the incremental search algorithms used as real-time algorithms as
described aboved in games and mazes benchmarks. Figure 4 shows the regions of the same
plots as Figure 3 in which FRIT(BFS) appears.
We observe that FRIT(BFS) returns significantly better solutions when time constraints
are very tight. Indeed, for games benchmarks our algorithm does not need more than 41 sec
per planning episode to return its best solution. Given such a time as a limit per episode,
AA* requires over four times as many iterations on average. Furthermore, to obtain a
solution of the quality returned by FRIT(BFS) at 41 sec, AA* needs around 220 sec;
i.e., more than 5 times as long as FRIT(BFS). This behavior is more extreme in the case
of mazes, where the best solutions for FRIT(BFS) are obtained with less than 19 sec per
planning episode. With this time limit, the number of steps required on average by AA* is
a whole order of magnitude larger than the number required by FRIT(BFS).
Generally, FRIT(BFS) behaves much better than both RA* and AA*, requiring fewer
iterations and less time. Nevertheless, when provided more time, FRIT(BFS) does not take
advantage of it and the resulting solutions cease to improve. This can be seen both in
254

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

100000000
FRIT(BFS)
RA
AA
optimal

1000000

Average Solution Length (log-scale)

Average Solution Length (log-scale)

10000000

100000

10000

1000

0

10

20

30

40

50

FRIT(BFS)
RA
AA
optimal

10000000

1000000

100000

10000

1000

0

Average time per planning episode (us)

5

10

15

20

Average time per planning episode (us)

(a) Incremental algorithms in games.

(b) Incremental algorithms in mazes.

Figure 4: Incremental algorithms: Total Iterations versus Time per Episode (zoomed)
FRIT(BFS)
k Avg. Its Time/ep No
( s)
1 1508631 0.0430
5 303483 0.2148
10 152858 0.4283
50 32401 2.0940
100 17370 4.0678
500
5449 16.115
1000
4035 24.840
5000
3073 39.316
10000
3026 40.487
50000
3011 40.851
100000
3011 40.869

RA*
moves Avg. Its Time/ep No
(%)
( s)
99.80 3505076 0.3754
99.01 702029 1.8761
98.03 351648 3.7499
90.71 71343 18.655
82.67 36305 37.077
44.74
8304 175.89
25.38
4901 322.74
2.046
2261 915.66
0.501
1947 1171.9
0.030
1726 1458.9
0.007
1711 1484.7

AA*
moves Avg. Its Time/ep No
(%)
( s)
99.95 1144680 0.4152
99.76 229967 2.0727
99.51 115628 4.1376
97.60 24156 20.378
95.29 12723 40.004
79.42
3607 172.41
65.13
2583 274.35
24.44
1854 474.29
12.26
1775 514.88
1.041
1728 524.55
0.133
1726 543.66

moves
(%)
99.84
99.25
98.51
92.86
86.44
52.15
33.20
6.904
2.764
0.117
0.014

Table 1: Relationship between search expansions and number of iterations in which the
agent does not move in games maps. The table shows a parameter k for each algorithm. In
the case of AA* and Repeated A* the parameter corresponds to the number of expanded
states. In case of FRIT, the parameter corresponds to the number of visited states during
an iteration. In addition, it shows average time per search episode (Time/ep), and the
percentage of iterations in which the agent did not move with respect to the total number
of iterations (No moves).

Figure 3, across both sets of benchmarks, and Table 1. As an example of this, we can see
that for k = 5000 to k = 100000 the number of iterations required to solve the problem only
decreases by 62 steps, and the time used per search episode only increases by 1.55 sec.
Effectively, this means that the algorithm does not use the extra time in an advantageous
way. This is in contrast to what is usually expected for real-time search algorithms.
255

fiRivera, Illanes, Baier, & Hernandez

An interesting variable to study is the number of algorithm iterations in which the
agent did not return a move because the algorithm exceeded the amount of computation
established by the parameter without finishing search. As we can see in Table 1, FRIT,
using BFS as its parameter algorithm, has the best relationship between time spent per
episode and the percentage of no-moves over the total number of moves. To be comparable
to other real-time heuristic search algorithms, it would be preferrable to reduce the number
of incomplete searches as much as possible. With this in mind, we can focus on the time
after which the amount of incomplete searches is reduced to less than 1%. Notice that for
FRIT(BFS) this is somewhere around 40 s, whereas for AA* and RA* this requires times
of over 514 s and 1458 s respectively.
6.3 Comparison of the Two Approaches
Figure 5 shows a plot of the average time per planning episode versus average number
of agent steps for both FRITRT (daRTAA*) and FRIT(BFS) in games benchmarks. We
observe that FRIT(BFS) obtains better resuts for most time limits. Indeed, for any given
time deadline of more than 10 sec, FRIT(BFS) finds a solution that is about half as long
as that found by FRITRT (daRTAA*). For smaller time deadlines the results are similar for
both algorithms. Furthermore, the best solution obtained by FRIT(BFS) is, on average,
less than 60% as long as the best solution obtained by FRITRT (daRTAA*). As mentioned
above, this particular solution requires a time deadline of less than 41 sec per planning
episode. The number of no-moves incurred with this time limit in our experiments was of
only 465 iterations throughout all the experiments in games benchmarks, which corresponds
to approximately one no-move every 40, 000 moves.
12 maps - 500 runs
Average Solution Length (log-scale)

10000000
FRIT_rt(daRTAA)
FRIT(BFS)
1000000

100000

10000

1000

0

10

20

30

40

50

60

Average time per planning episode (us)

Figure 5: Comparison of FRIT using a real-time algorithm versus FRIT as an incremental
algorithm in games benchmarks.

256

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

1

2

3

4

5

6

7

8

9

10

11

12

13

1

A

2

3

4

5

6

7

8

9

10

11

12

13

A

B

B

C

C

D

D

E

E

T

F

F

G

G

H

H

(a)

T

(b)

Figure 6: Bug2 (a) and FRIT (b) in a pathfinding scenario in which the goal cell is E10
and the initial cell is E2. The segmented line shows the path followed by the agent and the
filled arrow is the m-line.

6.4 Comparison to the Bug2 Algorithm
Bug algorithms (LaValle, 2006; Taylor & LaValle, 2009) are a family of pathfinding algorithms for continuous 2D terrain. They make their decisions based on sensory input, require
very limited time and memory resources, and are inspired by the behavior of insects while
finding their way through obstacles. Bug algorithms are not heuristic as they do not utilize
a heuristic function to make decisions (Rao, Kareti, Shi, & Iyengar, 1993).
Bug2 (Lumelsky & Stepanov, 1987) is a bug algorithm that is simple to implement and
is guaranteed to reach the goal. An agent controlled by Bug2 will follow a straight line
connecting the initial position with the final positionthe so-called m-line, until encountering an obstacle or reaching the goal. If an obstacle is encountered, it saves the position
at which the obstacle was hit in a variable called hit point and then starts following the
boundary of the obstacle (either clockwise or counterclockwise) until the m-line is encountered again. Then, if the current position is closer to the goal than the hit point, the agent
starts following the m-line again and the process repeats.
Figure 6 compares the behaviors of FRIT and Bug2. In this particular situation, Bug2
does not make a good decision and FRIT solves the problem fairly quickly. Of course it is
possible to contrive families of problems in which Bug2 will always outperform FRIT.
We implemented Bug2 for 8-connected grid worlds. We forbid diagonal movements
between two obstacles, as this essentially has the effect of changing the direction in which an
obstacles boundary is being followed. To make the comparison fair, we also ran FRIT(BFS)
with this additional restriction. We used the same game maps, and generated 500 solvable
random problems for each of them.
Results for FRIT(BFS) are shown in Table 2. The average cost of solutions obtained
by Bug2 was 6546, requiring 5766 iterations. In addition, the average runtime was 2317
s, which yields, on average, 0.4 s spent per iteration. FRIT(BFS) spends around 0.4
per search episode when k = 12, and requires about 20 times more iterations to solve the
problem yielding about 97% of no-moves. To obtain a solution comparable to that of Bug2,
FRIT(BFS) requires k to be set to around 462, which yields an average time per search
episode close to 12 s, returning a no-move on 47% of the iterations. Finally, if more time
257

fiRivera, Illanes, Baier, & Hernandez

k Avg. Its Time/ep ( s) No moves (%)
1 1544087
0.0346
99.81
5 310579
0.1725
99.03
10 156411
0.3442
98.07
50 33314
1.6830
90.91
100 14715
3.2712
83.01
500
5521
13.009
45.45
1000
4070
20.132
26.02
5000
3079
32.162
2.207
10000
3027
33.193
0.542
50000
3012
33.520
0.026
100000
3011
33.532
0.006
Table 2: Performance Indicators for FRIT(BFS) when no diagonal movements are allowed
between obstacles.
is available, FRIT(BFS) returns solutions on average more than 50% cheaper than those
obtained by Bug2.
As a conclusion we observe that in pathfinding applications Bug2 runs faster than any
other search algorithm we tried. As a disadvantage, Bug2 is specific to pathfinding and
cannot exploit additional time per episode to obtain a better solution, yielding solutions
that are longer than those obtained by FRIT(BFS) when more time is available. Therefore
bug algorithms seem to be recommended for real-time pathfinding applications in which
there is very little time available per iteration. When more time is available FRIT(BFS)
is the recommended algorithm, leaving AA* as a choice for applications in which there is
significantly more time available.
6.5 On the Usefulness of Reconnecting Heuristics
Definition 2 introduced the idea of admissible reconnecting heuristics, which we argued are
important to guide search towards reconnection. A natural question to ask is whether or
not these heuristics are key to the performance of FRITRT . Indeed, an admissible heuristic
for the problem is formally an admissible reconnecting heuristic since we simply need to
define B = {g} in Definition 2. Nevertheless, intuitively the problems heuristic does not
guide towards reconnection with the ideal tree.
To evaluate the usefulness of reconnecting heuristics, we implemented a version of
FRITRT (RTAA*) that:
1. uses the problems heuristic to guide the A* search,
2. breaks ties in favor of states with greater g-value, and that
3. learns h using RTAA*s learning rule.
We compared it to our standard FRITRT (RTAA*), which uses the admissible reconnection
h = 0 to guide the A* search, uses the problem heuristic as a first tie breaker, uses the
258

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

g-value as a second tie breaker, and learns h using RTAA*s learning rule. We ran the
algorithms over the same 12 game maps, using the same configuration described above.
Figure 7 shows the relative performance of the algorithms, confirming that indeed in
these pathfinding applications, using reconnection heuristics is key to performance. Using a
goal heuristic to guide for reconnection performs more similar to the baseline (RTAA*). This
is because FRITRT , used with the problems heuristic, can be seen as a version of RTAA*
that stops the A* search early if it expands a state the agent believes to be connected to
the goal (i.e., in the ideal tree). Although stopping search early saves time with respect
to RTAA*, the more expensive goal conditionwhich verifies that a state is in the ideal
treeseems to counter time savings unless the lookahead parameter is high.
12 game maps - 500 runs

Average Solution Cost (log-scale)

10000000
FRIT_rt(RTAA) [h0=0]
FRIT_rt(RTAA) [h0=octile]
RTAA

1000000

100000

10000

1000

0

20

40

60

80

100

120

Average time per planning episode (us)

Figure 7: Comparison of FRITRT using a reconnecting heuristic (h=0) to guide search
versus FRITRT using the problems heuristic.

7. Related Work
Incremental Heuristic Search and Real-time Heuristic Search are two heuristic search approaches to solving search problems in partially known environments using the free-space
assumption that are related to the approach we propose here. Incremental search algorithms based on A*, such as D* Lite (Koenig & Likhachev, 2002), Adaptive A* (Koenig
& Likhachev, 2005) and Tree Adaptive A* (Hernandez, Sun, Koenig, & Meseguer, 2011),
reuse information from previous searches to speed up the current search. The algorithms
can solve sequences of similar search problems faster than Repeated A*, which performs
repeated A* searches from scratch.
During runtime, most incremental search algorithms, like our algorithm, store a graph
in memory reflecting the current knowledge of the agent. In the first search, they perform
a complete A* (backward or forward), and in the subsequent searches they perform less
intensive searches. In contrast to our algorithm, such searches return optimal paths connecting the current state with the goal. FRIT is similar to incremental search algorithms
in the sense that it uses the ideal tree, which is information that, in some cases, may have
259

fiRivera, Illanes, Baier, & Hernandez

been computed using search, but differs from them in that the objective of the search is not
to compute optimal paths to the goal. Our algorithm leverages the speed of simple blind
search and does not need to deal with a priority queue, which is computationally expensive
to handle.
Many state-of-the-art real-time heuristic search algorithms (e.g., Koenig & Sun, 2009;
Koenig & Likhachev, 2006; Sturtevant & Bulitko, 2011; Hernandez & Baier, 2012; Rivera,
Baier, & Hernandez, 2013a), which satisfy the real-time property, rely on updating the
heuristic to guarantee important properties like termination. Our algorithm, on the other
hand, does not need to update the heuristic to guarantee termination. Like incremental
search algorithms, real-time heuristic search algorithms usually carry out search for a path
between the current node and the goal state. Real-time heuristic search algorithms cannot
return a likely better solution after the problem is solved without performing any search
at all (cf. Theorem 4). Instead, when running multiple trials they eventually converge to
an optimal solution or offer guarantees on solution quality. Our algorithm does not offer
guarantees on solution quality, even though experimental results are positive.
HCDPS (Lawrence & Bulitko, 2010) is a real-time heuristic algorithm that does not
employ learning. This algorithm is tailored to problems in which the agent knows the map
initially, and in which there is time for preprocessing.
The idea of reconnecting with a tree rooted at the goal state is not new and can be traced
back to bi-directional search (Pohl, 1971). A recent Incremental Search algorithm, Tree
Adaptive A* (Hernandez et al., 2011), exploits this idea to make subsequent searches faster.
Real-Time D* (RTD*) (Bond, Widger, Ruml, & Sun, 2010) uses bi-directional search to
perform searches in dynamic environments. RTD* combines Incremental Backward Search
(D*Lite) with Real-Time Forward Search (LSS-LRTA*).
Finally, our notion of generalized free-space assumption is related to that proposed by
Bonet and Geffner (2011), for the case of planning in partially observable environments.
Under certain circumstances, they propose to set unobserved variables in action preconditions in the most convenient way during planning time, which indeed corresponds to adding
more arcs to the original search graph.

8. Summary and Future Work
We presented FRIT, a search algorithm that follows a path in a treethe ideal tree
that represents a family of solutions in the graph currently known by the agent. The
algorithm is simple to describe and implement, and does not need to update the heuristic
to guarantee termination. FRIT uses a secondary search algorithm to search for a state
in the ideal tree when the agent becomes disconnected from it. We show that, with slight
modifications, we can a real-time search algorithm to search for reconnection, and we obtain
a real-time version of FRIT, FRITRT . In addition, we propose a different way of using FRIT
in some applications that use real-time search by feeding it with bounded, complete search
algorithms.
We provide theoretical results proving that both FRIT and FRITRT always find solutions
if they exist. Furthermore, we give explicit bounds on the length of the obtained solutions.
Finally, we prove that both algorithms converge after two trial runs.
260

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

Our experiments show that the proposed algorithms return solutions faster than other
state-of-the-art real-time search algorithms. In particular FRIT(daRTAA*) substantially
improves performance over daRTAA*, a state-of-the-art Real-Time Search algorithm. Larger
performance improvements are observed when time constraints are tighter. Additionally,
we compare both our approaches and show that FRIT(BFS)that is, FRIT fed with the
breadth first search algorithmproduces similar or better results for tight time constraints.
In a comparison with the pathfinding-specific Bug2 algorithm, we concluded than when
very little time per search episode is given Bug2 delivers best-quality solutions, followed by
FRIT(BFS), and eventually, when significantly more time is available, followed by state-ofthe-art incremental search algorithms.
As a disadvantage of our approach, we note that FRIT cannot exploit more computational time as other algorithms do. Indeed, other incremental heuristic search algorithms
will return better quality solutions if allowed large time constraints, while FRIT will generally not converge asymptotically to the optimal path if given arbitrary time.
We have left out of the scope of the paper how FRIT could be used in more general
search spaces like the ones that can be described using a planning language like STRIPS
(Fikes & Nilsson, 1971) or PDDL (McDermott, 1998). In planning domains, search graphs
are implicitly defined by actions defined in terms of preconditions an effects. Computing
ideal trees using Dijkstras algorithm, as we suggested above, is not simple since it requires
the generation of a number of states exponential in the size of the problem description.
Moreover, it is not immediately obvious either how to build an ideal tree from a standard
domain-independent heuristic, as we have done in pathfinding. Indeed, while there exist
domain-independent planning heuristics that are admissible (e.g., Haslum & Geffner, 2000),
it is easy to show that they do not correspond to a perfect heuristic over a spanning supergraph of the original search space, which implies that it is not possible to use them directly
to construct an ideal tree, as loops may be easily formed. Therefore, further investigation is
needed in order to adapt the techniques we have presented here for planning applications.
Acknowledgments
Nicolas Rivera, Leon Illanes, and Jorge Baier were partly funded by Fondecyt Project
Number 11110321. We thank the anonymous JAIR reviewers, for their valuable input, and
the SoCS2013 reviewers for their comments on an earlier version of this paper.

References
Bjornsson, Y., Bulitko, V., & Sturtevant, N. R. (2009). TBA*: Time-bounded A*. In Proc.
of the 21st Intl Joint Conf. on Artificial Intelligence (IJCAI), pp. 431436.
Bond, D. M., Widger, N. A., Ruml, W., & Sun, X. (2010). Real-time search in dynamic
worlds. In Proc. of the 3rd Symposium on Combinatorial Search (SoCS), Atlanta,
Georgia.
Bonet, B., & Geffner, H. (2011). Planning under partial observability by classical replanning: Theory and experiments. In Proc. of the 22nd Intl Joint Conf. on Artificial
Intelligence (IJCAI), pp. 19361941, Barcelona, Spain.
261

fiRivera, Illanes, Baier, & Hernandez

Botea, A. (2011). Ultra-fast Optimal Pathfinding without Runtime Search. In Proc. of the
7th Annual Intl AIIDE Conference (AIIDE), Palo Alto, California.
Bulitko, V., & Lee, G. (2006). Learning in real time search: a unifying framework. Journal
of Artificial Intelligence Research, 25, 119157.
Bulitko, V., Bjornsson, Y., & Lawrence, R. (2010). Case-based subgoaling in real-time
heuristic search for video game pathfinding. Journal of Artificial Intelligence Research,
38, 268300.
Bulitko, V., Bjornsson, Y., Lustrek, M., Schaeffer, J., & Sigmundarson, S. (2007). Dynamic
control in path-planning with real-time heuristic search. In Proc. of the 17th Intl
Conf. on Automated Planning and Scheduling (ICAPS), pp. 4956.
Bulitko, V., Bjornsson, Y., Sturtevant, N., & Lawrence, R. (2011). Real-time Heuristic
Search for Game Pathfinding, pp. 130. Applied Research in Artificial Intelligence for
Computer Games. Springer Verlag.
Edelkamp, S., & Schrodl, S. (2011). Heuristic Search: Theory and Applications. Morgan
Kaufmann.
Fikes, R., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2 (3/4), 189208.
Hart, P. E., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimal cost paths. IEEE Transactions on Systems Science and Cybernetics, 4 (2),
100107.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proc. of the
5th Intl Conf. on Artificial Intelligence Planning and Systems (AIPS), pp. 140149.
Hernandez, C., & Baier, J. A. (2011). Fast subgoaling for pathfinding via real-time search.
In Proc. of the 21th Intl Conf. on Automated Planning and Scheduling (ICAPS),
Freiburg, Germany.
Hernandez, C., & Baier, J. A. (2012). Avoiding and escaping depressions in real-time
heuristic search. Journal of Artificial Intelligence Research, 43, 523570.
Hernandez, C., Baier, J. A., Uras, T., & Koenig, S. (2012a). Position paper: Incremental
search algorithms considered poorly understood. In Proc. of the 5th Symposium on
Combinatorial Search (SoCS).
Hernandez, C., Baier, J. A., Uras, T., & Koenig, S. (2012b). TBAA*: Time-Bounded
Adaptive A*. In Proc. of the 10th Intl Joint Conf. on Autonomous Agents and Multi
Agent Systems (AAMAS).
Hernandez, C., Sun, X., Koenig, S., & Meseguer, P. (2011). Tree adaptive A*. In Proc. of
the 10th Intl Joint Conf. on Autonomous Agents and Multi Agent Systems (AAMAS),
Taipei, Taiwan.
Ishida, T. (1992). Moving target search with intelligence. In Proc. of the 10th National
Conf. on Artificial Intelligence (AAAI), pp. 525532.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22 (4), 109131.
262

fiReconnection with the Ideal Tree: A New Approach to Real-Time Search

Koenig, S., & Likhachev, M. (2002). D* lite. In Proc. of the 18th National Conf. on Artificial
Intelligence (AAAI), pp. 476483.
Koenig, S., & Likhachev, M. (2005). Adaptive A*. In Proc. of the 4th Intl Joint Conf. on
Autonomous Agents and Multi Agent Systems (AAMAS), pp. 13111312.
Koenig, S., & Likhachev, M. (2006). Real-time Adaptive A*. In Proc. of the 5th Intl Joint
Conf. on Autonomous Agents and Multi Agent Systems (AAMAS), pp. 281288.
Koenig, S., & Sun, X. (2009). Comparing real-time and incremental heuristic search for
real-time situated agents. Autonomous Agents and Multi-Agent Systems, 18 (3), 313
341.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42 (2-3), 189211.
LaValle, S. M. (2006). Planning algorithms. Cambridge University Press.
Lawrence, R., & Bulitko, V. (2010). Taking learning out of real-time heuristic search for
video-game pathfinding. In Australasian Conf. on Artificial Intelligence, pp. 405414.
Lumelsky, V. J., & Stepanov, A. A. (1987). Path-planning strategies for a point mobile
automaton moving amidst unknown obstacles of arbitrary shape. Algorithmica, 2,
403430.
McDermott, D. V. (1998). PDDL  The Planning Domain Definition Language. Tech.
rep. TR-98-003/DCS TR-1165, Yale Center for Computational Vision and Control.
Pohl, I. (1971). Bi-directional heuristic search. In Machine Intelligence 6, pp. 127140.
Edinburgh University Press, Edinburgh, Scotland.
Rao, N. S., Kareti, S., Shi, W., & Iyengar, S. S. (1993). Robot navigation in unknown terrains: Introductory survey of non-heuristic algorithms. Tech. rep. ORNL-TM-12410,
Oak Ridge National Laboratory.
Rivera, N., Baier, J. A., & Hernandez, C. (2013a). Weighted real-time heuristic search. In
Proc. of the 11th Intl Joint Conf. on Autonomous Agents and Multi Agent Systems
(AAMAS).
Rivera, N., Illanes, L., Baier, J. A., & Hernandez, C. (2013b). Reconnecting with the ideal
tree: An alternative to heuristic learning in real-time search. In Proceedings of the 6th
Symposium on Combinatorial Search (SoCS).
Sharon, G., Sturtevant, N., & Felner, A. (2013). Online detection of dead states in realtime agent-centered search. In Proc. of the 6th Symposium on Combinatorial Search
(SoCS), Leavenworth, WA, USA.
Sturtevant, N. R. (2012). Benchmarks for grid-based pathfinding. IEEE Transactions
Computational Intelligence and AI in Games, 4 (2), 144148.
Sturtevant, N. R., & Bulitko, V. (2011). Learning where you are going and from whence
you came: h- and g-cost learning in real-time heuristic search. In Proc. of the 22nd
Intl Joint Conf. on Artificial Intelligence (IJCAI), pp. 365370, Barcelona, Spain.
Taylor, K., & LaValle, S. M. (2009). I-bug: An intensity-based bug algorithm. In Proc. of
the 2009 IEEE Intl Conf. on Robotics and Automation (ICRA), pp. 39813986.
263

fiRivera, Illanes, Baier, & Hernandez

Zelinsky, A. (1992). A mobile robot exploration algorithm. IEEE Transactions on Robotics
and Automation, 8 (6), 707717.

264

fiJournal of Artificial Intelligence Research 50 (2014) 487-533

Submitted 12/13; published 06/14

Improving Delete Relaxation Heuristics Through Explicitly
Represented Conjunctions
Emil Keyder

emilkeyder@gmail.com

Jorg Hoffmann

hoffmann@cs.uni-saarland.de

Saarland University
66123 Saarbrucken, Germany

Patrik Haslum

patrik.haslum@anu.edu.au

The Australian National University & NICTA
Canberra ACT 0200, Australia

Abstract
Heuristic functions based on the delete relaxation compute upper and lower bounds
on the optimal delete-relaxation heuristic h+ , and are of paramount importance in both
optimal and satisficing planning. Here we introduce a principled and flexible technique
for improving h+ , by augmenting delete-relaxed planning tasks with a limited amount of
delete information. This is done by introducing special fluents that explicitly represent
conjunctions of fluents in the original planning task, rendering h+ the perfect heuristic h
in the limit. Previous work has introduced a method in which the growth of the task is
potentially exponential in the number of conjunctions introduced. We formulate an alternative technique relying on conditional effects, limiting the growth of the task to be linear
in this number. We show that this method still renders h+ the perfect heuristic h in the
limit. We propose techniques to find an informative set of conjunctions to be introduced in
different settings, and analyze and extend existing methods for lower-bounding and upperbounding h+ in the presence of conditional effects. We evaluate the resulting heuristic
functions empirically on a set of IPC benchmarks, and show that they are sometimes much
more informative than standard delete-relaxation heuristics.

1. Introduction
Planning as heuristic search is one of the most successful approaches to planning. Some
of the most informative heuristic functions for domain-independent planning are obtained
as the estimated cost of the delete relaxation of the original planning task. The delete
relaxation simplifies planning tasks by assuming that every variable value, once achieved,
persists during the execution of the rest of the plan. The cost of an optimal plan for the
resulting relaxed planning task, denoted h+ , is NP-complete to compute, however whether
some plan for the delete-relaxed task exists can be checked in polynomial time (Bylander,
1994). For satisficing planning, where the heuristic does not have to be admissible, the
latter fact can be exploited to upper-bound h+ , by generating some not necessarily optimal
plan for the delete-relaxed task (Hoffmann & Nebel, 2001). For optimal planning, lowerbounding methods have been devised based on the analysis of landmarks, logical formulas
over the set of actions that state necessary properties of delete-relaxed plans (Karpas &
c
2014
AI Access Foundation. All rights reserved.

fiKeyder, Hoffmann, & Haslum

Domshlak, 2009; Helmert & Domshlak, 2009). These cost estimates for the delete-relaxed
task can then be used to guide heuristic search in the state space of the original task.
Since delete relaxation heuristics were first proposed (Bonet & Geffner, 2001), much
work has been done to improve them. One approach focuses on better approximation
schemes for h+ , obtaining tighter upper bounds and thus better non-admissible estimates
(Hoffmann & Nebel, 2001; Keyder & Geffner, 2008, 2009), or tighter lower bounds that
correspond to more informative admissible heuristics (Helmert & Domshlak, 2009; Bonet
& Helmert, 2010). In many domains, however, it is important that the heuristic be able to
take into account delete information (Hoffmann, 2005), and indeed there is a long tradition
of works proposing heuristics that do so. Several of these extend the delete relaxation to
capture strictly more information (Fox & Long, 2001; Helmert, 2006; Helmert & Geffner,
2008; Cai, Hoffmann, & Helmert, 2009; Katz, Hoffmann, & Domshlak, 2013), while some
consider only the delete relaxation but attempt to find low-conflict relaxed plans (Baier &
Botea, 2009), or generate modified heuristic values based on taking conflicts into account
to some extent (Do & Kambhampati, 2001; Gerevini, Saetti, & Serina, 2003). Here, we
approach this problem by taking inspiration from the admissible hm family of heuristics
(Haslum & Geffner, 2000). An important property of the heuristics we introduce, shared
with some other recent work in this direction, is that our technique renders h+ the perfect
heuristic h in the limit. In other words, the technique offers a trade-off between the amount
of delete information considered and the computational overhead of doing so. At one end
of that continuum, delete-relaxed plans become plans for the original task.
The hm heuristic function considers the cost of making true simultaneously sets of fluents
of size  m. The cost of the planning task is then estimated by recursively taking the cost
of a set of fluents, such as the goal or a set of action preconditions, to be the cost of its
most costly subset of size  m, and ignoring the cost of achieving the remaining fluents in
the set. As each possible subset of size m of the fluents in the task must be considered, the
size of the representation required to compute hm is exponential in m. The hm heuristics
provide the guarantee that there exists an m such that hm = h (trivially satisfied when m
is the total number of fluents in the task). However, the value of m required to achieve this
is usually so large as to make computing h with this method infeasible in practice.
The hm heuristic has recently been recast as the hmax = h1 cost of a planning task m
with no deletes (Haslum, 2009). This is achieved by representing conjunctions of fluents c
of size  m in the original task with new fluents c , here called -fluents, and modifying
the initial state, goal, and operators of the planning task so as to capture the reasoning
performed by hm over these sets within the computation of hmax . However, h+ (m ) is
not admissible (since a separate copy of the same action may be needed to establish each
-fluent), and thus the m compilation is not useful for obtaining admissible estimates that
are more informative than h+ . The more recent C construction (Haslum, 2012) fixes this
issue (introducing an action copy for every subset of -fluents that may be established), at
the cost of growing the task representation exponentially in the number of -fluents rather
than linearly as in the m representation. On the other hand, C offers the possibility of a
more fine-grained tradeoff between representation size and heuristic accuracy, by allowing
the choice of an arbitrary set of conjunctions C and corresponding -fluents (which need not
all be of the same size). This stands in contrast to the hm heuristic and the m compilation,
in which all sets or conjunctions of size  m are represented.
488

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Haslum (2012) proposed to repeatedly solve C optimally, within an iterative procedure
that adds new conjunctions to the set C in each iteration. The relaxed plans that are
computed therefore gradually become closer, in some sense, to being plans for the original
task. We instead explore the idea of using this kind of construction for obtaining heuristic
functions for guiding search.
C
We introduce a related construction C
ce that is similar to  , but that makes use of
conditional effects to limit the growth of the task to be worst case linear, rather than
exponential, in |C|. This gain in size comes at the price of some information loss relative to
C . However, as we show, this information loss does not affect the fundamental property
of tending towards the perfect heuristic h if enough conjunctions are introduced: Like
C ,  C
ce is perfect in the limit, i. e. there always exists a set of conjunctions C such that

h+ (C
ce ) = h . Furthermore, while information may be lost, this is not always the case.
Indeed, it is possible to construct families of planning tasks for which C
ce can represent the
+
C
same heuristic function as C for the same set of conjunctions C, i. e., h+ (C
ce ) = h ( ),
C
but for which the representation of ce occupies exponentially less space.
Having said that, the theoretical advantage of C
ce does not tend to materialize in practice
(or at least in the commonly used benchmarks): While without further optimizations C
indeed grows too quickly to be practical, it turns out that mutex pruning techniques (eliminating compiled actions with conflicting preconditions) are extremely effective at keeping
C
the size of C at bay. We therefore consider both C
ce and  , evaluating their usefulness
for devising improved heuristic functions. We focus on two main questions:
(A) How to obtain upper and lower bounds for h+ in compiled tasks?
(B) How to choose a set of conjunctions C so as to maximize the information gained from
their addition to the planning task?
In response to question (A), we analyze and extend three state-of-the-art methods for
estimating h+ . In the satisficing setting (upper-bounding h+ ), we consider the problem
of finding low-cost relaxed plans that can be scheduled so as to minimize the cost of the
sequence of actions required to trigger a given set of conditional effects, avoiding unnecessary
repeated applications of the same action. This problem has been only scantily addressed
in previous work. Here we show that the problem of optimal action scheduling for a given
set of effects is NP-complete, and generalize the approximation technique used in the FF
planner (Hoffmann & Nebel, 2001).
In the optimal setting (lower-bounding h+ ), we consider the LM-cut heuristic (Helmert
& Domshlak, 2009) as well as admissible heuristics based on fluent landmarks (Karpas
& Domshlak, 2009). For the former, our findings are mostly negative: First, we show
that even though the introduction of -fluents cannot decrease hmax or h+ , which lowerand upper-bound LM-cut, respectively, the LM-cut heuristic value can decrease. Second,
we show that neither of the two straightforward adaptations of the LM-cut algorithm to
problems with conditional effects maintains both admissibility and domination of hmax .1 For
the latter, we show that C
ce can be used to generate more informative fluent landmarks.
Recent work (Keyder, Richter, & Helmert, 2010) extracts landmarks from the m task.
1. A more sophisticated adaptation of LM-cut, based on the idea of context splitting, has recently been
proposed by ? (?). It maintains both properties.

489

fiKeyder, Hoffmann, & Haslum

This allows the discovery of -fluent landmarks corresponding to conjunctive landmarks in
the original task, but suffers due to the large number of -fluents that must be considered.
The C
ce compilation offers the possibility of discovering interesting conjunctive landmarks
of unbounded size, while avoiding growing the size of the compilation unnecessarily.
In response to question (B), we devise a range of strategies depending on the purpose
C
for which the C
ce or  compilation is to be used. Most of these are parameterized in terms
of the allowed growth of the compiled task relative to the original task, and thus allow a
trade-off between the informativeness of the heuristic and its computational overhead. We
evaluate the resulting heuristics on a wide range of benchmarks from the International Planning Competition, varying the relevant algorithm parameters to determine their individual
effect on performance. As the results show, in several domains our heuristics are much more
informative than previous ones, leading to significantly improved performance.
We next define the basic concepts (Section 2), before moving on to the formal definition
m
C
of the C
ce compilation and the previously introduced  and  compilations (Section 3).
C
m
C
In Section 4, we analyze ce and its relation to  and  from a theoretical perspective.
Section 5 discusses the practical issues that arise in using the compilations for the purpose
of satisficing planning, and describes the obtained experimental results, while Section 6 does
the same for the case of optimal planning. Finally, Section 7 summarizes the main points
of the paper and indicates some possible future research directions.

2. Preliminaries
Our planning model is based on the propositional STRIPS formalization, to which we add
action costs and conditional effects. States and operators are defined in terms of a set F
of propositional variables, or fluents. A state s  F is given by the set of fluents that are
true in that state. A planning task is described by a 4-tuple  = hF, A, I, Gi, where F
is a set of such variables, A is the set of actions, I  F is the initial state, and G  F
describes the set of goal states, given by {s | G  s}. Each action a  A consists of
a 4-tuple hpre(a), add(a), del(a), ce(a)i, where pre(a), add(a), and del(a) are subsets of F .
The action has a cost cost(a)  R+
0 . By ce(a) = {ce(a)1 , . . . , ce(a)n }, we denote the set of
conditional effects of action a, each of which is a triple hc(a)i , add(a)i , del(a)i i of subsets of
F . To simplify some of our notations, we require that add(a)  del(a) = ; we do not need
to impose any restrictions on the deletes del(a)i of conditional effects, because conditional
effects will be used only within the delete relaxation. If ce(a) =  for all a  A,  has no
conditional effects, and we say that it is a STRIPS planning task.
An action a is applicable in s if pre(a)  s. The result of applying it is given by
[
[
s[a] = (s \ (del(a) 
del(a)i ))  (add(a) 
add(a)i )
{i|c(a)i s}

{i|c(a)i s}

A plan for s is a sequence
Pn of actions  = a1 , . . . , an whose application in s results in a goal
state. The cost of  is i=1 cost(ai ).  is optimal if its cost is minimal among all plans for
s; we will often denote optimal plans with   . A plan for I is also called a plan for , or
simply a plan.
A heuristic for  is a function h mapping states of  into R+
0 . The perfect heuristic

h maps each state s to the cost of an optimal plan for s. A heuristic h is admissible if
490

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

h(s)  h (s) for all s. By h(0 ), we denote a heuristic function for  whose value in s is
given by estimating the cost of the corresponding state s0 in a modified task 0 . We specify
0 in terms of the transformation of  = hF, A, I, Gi into 0 = hF 0 , A0 , I 0 , G0 i; s0 is obtained
by applying to s the same transformation used to obtain I 0 from I. It is sometimes useful
to make explicit that h is a heuristic computed on  itself; we will denote that by h().
Note that the modified task 0 is used only for the computation of the heuristic function.
In particular, the actual search for a plan is performed in the state space of the original
planning task .
The delete relaxation + of a planning task  is obtained by discarding all delete
effects. Formally, + = hF, A+ , I, Gi, where A+ = {hpre(a), add(a), , ce+ (a)i | a  A}, and
ce+ (a) = {hc(a)i , add(a)i , i | ce(a)i  ce(a)}. The cost of each action a+  A+ is the same
as the cost of the corresponding action cost(a). The optimal delete relaxation heuristic h+
is defined as the cost h (+ ) of an optimal plan for + .
We denote the power set of F with P(F ) = {c | c  F }. In the context of hm ,
C
 , and C
ce , we refer to fluent subsets c  P(F ) as sets or conjunctions interchangeably.
Throughout the paper, we assume that conjunctions are non-unit, i. e., |c| > 1.
A landmark in a planning task is a logical formula  over the set of fluents F such that
every valid plan  makes  true in some state (Hoffmann, Porteous, & Sebastia, 2004).
Orderings between landmarks are statements about the order in which these states occur.
A natural ordering 1 n 2 means that if a state sj satisfies 2 , there is some state si
occurring before sj in which 1 is satisfied. A necessary ordering 1 nec 2 means that 1
is always true in the state immediately before the state in which 2 becomes true, while a
greedy necessary ordering 1 gn 2 means that this relationship holds the first time that
2 is made true. Note that the necessary ordering 1 nec 2 implies the greedy necessary
ordering 1 gn 2 , but not vice versa. A landmark graph G is a directed graph whose
nodes are landmarks, and whose labelled edges correspond to the known orderings between
these landmarks.

3. The m , C and C
ce Compilations
The m compilation (Haslum, 2009) was the first technique proposed that made use of
the idea of -fluents that explicitly represent conjunctions in the original task. Given a
conjunction c  F , c is a new fluent c 6 F unique to c, i. e., if c 6= c0 then c 6= c0 .
In defining m and the other compilations that we discuss, we use the shorthand X C =
X  {c | c  C  c  X}, where X  F is a set of fluents, and C  P(F ) is a set of
conjunctions. In other words, X C consists of the set of fluents X itself, together with new
fluents c whose intention is to represent the conjunctions c  C that are contained in X,
c  X.
Definition 1 (The m compilation) Given a STRIPS planning task  = hF, A, I, Gi
and a parameter m  Z+ , m is the planning task hF C , AC , I C , GC i, where C = {c | c 
F  1 < |c|  m}, and AC contains A as well as an action ac for each pair a  A, c  C
such that del(a)  c =  and add(a)  c 6= , and ac is given by del(ac ) = , ce(ac ) = , and
pre(ac ) = (pre(a)  (c \ add(a)))C
add(ac ) = add(a)  {c0 | c0  C  c0  (add(a)  c)}
491

fiKeyder, Hoffmann, & Haslum

The parameter m here indicates the maximum size of the conjunctions to be represented
explicitly in the resulting compiled task. A -fluent is inserted (by definition of F C , cf.
above) for each c  F where 1 < |c|  m. These c are then added to all fluent sets in the
task (such as the initial state, action preconditions, and goals) containing the associated
set c. Furthermore, a linear (in |C|) number of representatives of each action a are added
to the task to model the situation in which the elements of c that are not made true by a
are already true before a is applied, and a adds the remaining fluents in c while deleting
none of them, thereby making every fluent in c, and therefore c , true. This compilation
allows the admissible hm cost of the original task to be computed as the hmax cost of this
compiled task.
The non-admissibility of h (m ) = h+ (m ) is due to the construction of the action representatives ac : Sets of fluents that are simultaneously made true with a single application
of an action a in  may require several representatives of a to explicitly achieve the same
effect in m . Consider for example an action a adding a fluent p in a state in which q and
r are already true. In  this makes the fluents p, q, and r true simultaneously, whereas in
2 , two different representatives of a are required: one with c = {p, q} adding {p,q} , and
one with c = {p, r} adding {p,r} .
The C compilation solves this problem by instead creating a number of representatives of a exponential in the number of -fluents which may be made true by a. Each
of these representatives corresponds to an application of a that makes a set of -fluents
true (Haslum, 2012). Following the above example, separate representatives of a would be
introduced for each of the -fluent sets , {{p,q} }, {{p,r} }, and {{p,q} , {p,r} }, and the
representative resulting from the last of these could be applied to make the two -fluents
true simultaneously. C also differs from m in that it allows the choice of a set C  P(F ),
and introduces fluents c for only those c  C, rather than for all subsets of size at most
m:2
Definition 2 (The C compilation) Given a STRIPS planning task  = hF, A, I, Gi
and a set of non-unit conjunctions C  P(F ), C is the planning task hF C , AC , I C , GC i,
0
where AC contains an action aC for each pair a  A, C 0  C such that c0  C 0 ,
(1) del(a)  c0 =   add(a)  c0 6= , and
(2) c  C((c  c0  add(a)  c 6= ) = c  C 0 ),
0

0

0

and aC is given by del(aC ) = , ce(aC ) = , and
[
0
pre(aC ) = (pre(a) 
(c0 \ add(a)))C
c0 C 0
C0

add(a ) = (add(a)  (pre(a) \ del(a)))C  {c0 | c0  C 0 }
2. There are three differences between our definition and Haslums (2012) definition of the the actions in
C . First, Haslums definition features delete effects, ensuring that real (non-relaxed) plans correspond
to plans in the original task. Since we only consider delete relaxations of the compiled task, we can safely
omit these. Second, we allow the sets C 0 used in the construction of actions to contain conjunctions c
0
with c  add(a)  (pre(a) \ del(a)); and third, add(aC ) contains the -fluents c where c  pre(a) \ del(a).
These latter two differences keep our definitions simpler. The redundant action representatives and
redundant add effects that they cause are easily pruned in practice.

492

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

0

The representatives aC of a enforce, for every c0  C 0 , that no part of c0 is deleted,
0
and that the non-added part of c0 is true already before aC is executed. Constraint (2)
0
ensures a form of non-redundancy: if aC adds a -fluent c0 , then it also adds all -fluents
c such that c  c0 , as all fluents in c necessarily become true with the application of
the action. Note that, differently than m , add effects in C include -fluents representing
conjunctions of fluents added by the action with prevail fluents (non-deleted preconditions).
This is necessary for admissibility of h+ (the primary purpose of C ), but not needed for
the computation of h1 (the primary purpose of m ).
C enumerates all possible subsets of C in constructing the representatives of each
action and therefore grows exponentially in |C|. This exponentiality is reminiscent of the
canonical conditional effects compilation used to convert planning tasks with conditional
effects into classical STRIPS planning tasks with exponentially more actions (Gazen &
Knoblock, 1997). The C
ce compilation that we introduce here is the result of applying
roughly the reverse transformation to C , resulting in a closely related planning task with
a linear (in |C|) number of conditional effects:
Definition 3 (The C
ce compilation) Given a STRIPS planning task  = hF, A, I, Gi
C
C
C
C
and a set of non-unit conjunctions C  P(F ), C
ce is the planning task hF , Ace , I , G i
where
C
C
C
C
AC
ce = {hpre(a ), add(a ), del(a ), ce(a )i | a  A},

and aC is given by
pre(aC ) = pre(a)C
add(aC ) = (add(a)  (pre(a) \ del(a)))C
del(aC ) = 
ce(aC ) = {h(pre(a)  (c \ add(a)))C , {c }, i
| c  C  c  del(a) =   c  add(a) 6= }
Rather than enumerating the sets of -fluents that may be made true by an action, C
ce
uses conditional effects to implicitly describe the conditions under which each is made true.
The only information lost in doing so is the information encoded by cross-context -fluents
in preconditions, which appear in action representatives in C , but not in the preconditions
C 0 in
or effect conditions of the corresponding actions in C
ce . For action representatives a
0
C , these are -fluents y  pre(aC ) where there exists no c  C 0 s.t. y  (c \ add(a)) 
pre(a). In the situation discussed above, for example, {q,r} is a precondition for the action
representative that adds both {p,q} and {p,r} in C , but does not appear in the condition
of any conditional effects of the corresponding action in C
ce . Since effect conditions are
determined individually for each c , such conditions are never included. We will return to
this below when discussing the theoretical relationship between C and C
ce .
Example 1 Consider the STRIPS planning task (adapted from Helmert & Geffner, 2008)
with variables {x0 , . . . , xn , y}, initial state I = {x0 , y}, goal G = {xn }, and unit-cost actions
a : h, {y}, , i

bi : h{xi , y}, {xi+1 }, {y}, i
493

fiKeyder, Hoffmann, & Haslum

for i = 0, . . . , n  1.
The optimal solution to this planning task takes the form b0 , a, b1 , a, . . . , bn1 , and has
cost 2n1. In the delete relaxation of the task, the fact that y is deleted after each application
of bi is ignored, and the optimal plan has cost n.
When a -fluent xi ,y is introduced in the C
ce compilation, it is added to the precondition
of the action bi , and new conditional effects ce(a)i of the form h{xi }, {{xi ,y} }, i are created
for action a. No conditional effects are added to any of the b-actions, as each deletes y and
therefore cannot be an achiever of any -fluent. This increases the optimal delete relaxation
cost of the task by 1, as a new instance of a must be added to the relaxed plan to achieve
the newly introduced precondition of bi . If all -fluents of the form {xi ,y} are introduced,
the delete relaxation cost of C
ce becomes 2n  1, the optimal cost.
The same set of conjunctions renders the delete relaxation cost of C perfect (i. e.,
2n  1). However, the size of C given that conjunction set is exponential in n: As action
a may in principle achieve any subset of the conjunctions, every such subset C 0 induces a
0
separate representative aC in AC .
Regarding the m compilation, h2 = hmax (2 ) also gives the optimal cost of this task.
However, its computation requires the consideration of (n2 ) fluent pairs, rather than the
linear number of -fluents that need to be introduced in C
ce . As we shall see below (Theorem 6), the example can be easily extended so that m must scale with n for hm to become
C
m
perfect, thus showing an exponential separation between C
ce and both  and  .
An important practical optimization for both C and C
ce is mutex pruning. If mutex
information about the original planning task is available, specifically if we are given (some)
m-tuples of fluents that are not reachable in conjunction, then we can discard from the
compiled task any action representatives and conditional effects which require such an
m-tuple, without losing admissibility of the compilation. Namely, the value of h+ (C )
(respectively h+ (C
ce )) after this mutex pruning is bounded from above by the value of
0
0
0
h+ (C ) (respectively h+ (C
ce )) for a larger set C  C of conjunctions: If we include all
m
-fluents of size at most m, then all h mutexes are found, i. e., none of the respective fluents is reachable in the compiled task. Exploiting available mutex information allows us
to make the compilation more informed without having to add all these additional -fluents,
helping to keep the compilation small.
Another optimization we use is to eliminate dominated preconditions. Whenever we add
a fluent c to the precondition of an action, or to the condition of a conditional effect, we
remove from that condition all fluents p  c and -fluents {c0 | c0  c}. That is because
achieving c implies achieving these fluents as well, and methods that count their cost
separately (such as, for example, hadd and related heuristics) would incur an overestimation.
Note, however, that this does not eliminate the duplication caused by -fluents representing different fluent sets that have a non-empty intersection. Consider, for example, an
action a with pre(a) = {p, q, r}. If C = {{p, q}, {q, r}}, then pre(a) = {{p,q} , {q,r} }, and
the cost of achieving q will implicitly be counted twice in the hadd estimate of the cost of
applying a. As a possible solution, we considered replacing overlapping -fluents c , c0 with
cc0 . This, however, did not consistently improve any of the heuristics that we compute
from the compiled tasks.
494

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

4. Theoretical Properties of C
ce
+
C
We now discuss some theoretical properties of C
ce , considering the cost h (ce ) of its
optimal solutions instead of more practical approximations (note that for C
ce and the version
of C considered here, h+ = h as no delete effects are present). Where only proof sketches
are shown, the full proofs can be found in Appendix A. We first show the fundamental and
expected property:

Theorem 1 (Consistency and admissibility) h+ (C
ce ) is consistent and admissible.
Proof: Regarding consistency, given s, a such that s[a] = s0 in , we need to show that
+
C
0
 0C
0C in C . Then
h+ (C
ce )(s)  cost(a) + h (ce )(s ). Let  (s ) be an optimal plan for s
ce
0C  sC [aC ] and C is a task with no
aC    (s0C ) is necessarily a plan for sC in C
ce , as s
ce
deletes. Admissibility follows from consistency together with the fact that h+ (C
ce )(s) = 0
on goal states s.
Furthermore, the (ideal) delete relaxation lower bound can only improve as we add -fluents:
Theorem 2 (h+ (C
ce ) grows monotonically with C) Given a planning task  and sets
0
+
C0
C  C of non-unit conjunctions, h+ (C
ce )  h (ce ).
C
C
0
Proof: This follows from the fact that given any plan  = aC
1 , . . . , an0 for0 ce , 0 =
0
0
0
C
C
C
C
C
C
a1 , . . . , an constitutes a plan for ce . We show by induction that I [a1 ] . . . [ai ] 
C
0
C0
C0 =
I C [aC
1 ] . . . [ai ] \ {c | c  C \ C }, which shows the result since the goal of ce is G
GC \ {c | c  C \ C 0 }, and GC  sC [] if  is a valid plan.
0
For i = 0, the induction hypothesis holds since I C = I C \ {c | c  C \ C 0 } by definition.
0
0
0
0
C
C
C
C0
C
0
For i > 0, aC
i is applicable in I [a1 ] . . . [ai1 ] since pre(ai ) = pre(ai ) \ {c | c  C \ C },
0
0
0
C
0
C
C C
and I C [aC
1 ] . . . [ai1 ]  I [a1 ] . . . [ai ] \ {c | c  C \ C } by the induction hypothesis.
C
C
C
C
C
C
For {c | c  (I [a1 ] . . . [ai ] \ I [a1 ] . . . [ai1 ])  c  C 0 }, either c  add(ai )C , which
0
C0
implies c  add(aC
i ) due to the definition of ce , or there exists some conditional effect
C
0
cej (aC
i ) = h(pre(a)  (c \0 add(a))) , {c }, i. Since c  C , there must exist a 0corresponding
0
C
C0
conditional effect in ce by definition, and its condition must be true in I C [aC
1 ] . . . [ai1 ]
by the induction hypothesis.

For the special case where C 0 = , Theorem 2 gives us:
+
Corollary 1 (h+ (C
ce ) dominates h ()) Given a planning task  and a set of non-unit
+
C
+
conjunctions C, h (ce )  h ().

The domination can be strict, as follows trivially from convergence to h (Theorem 5 below).
We now consider the relationship between the C and C
ce compilations. As mentioned
above, information encoded by cross-context preconditions is lost when moving from the
C
exponential C to the linear C
ce . Estimates obtained from ce may therefore be inferior to
those obtained from C :
Theorem 3 (h+ (C ) dominates h+ (C
ce )) Given a planning task  and a set of nonunit conjunctions C, h+ (C )  h+ (C
).
There are cases in which the inequality is strict.
ce
495

fiKeyder, Hoffmann, & Haslum

Proof sketch: The standard conditional effects compilation into STRIPS (Gazen & Knoblock,
C
1997), applied to C
ce , is equivalent to  except for the presence of cross-context preconditions in C . Given this, any plan for C is also a plan for C
ce , yet the inverse is not the
C C
C
1
n
,
.
. . , aC
case. To show the first part, we show by induction that I C [aC
n ]  I [a1 , . . . , an ]ce ,
1
where I[. . . ]ce denotes the result of applying a sequence of actions to the initial state I C in
C
C
ce . Since the goal of both tasks is defined as G , this shows the desired result.
The strictness result follows from the fact that it is possible to construct tasks in which
the cross-context preconditions discussed above play a role, leading to situations in which
C
there exist plans for C
ce that are shorter than the minimum-length plans for  .
In the proof of strictness (Appendix A), we show a planning task for which the h+ (C )
value is strictly larger than the h+ (C
ce ) value when C is chosen to be all conjunctions
of size 2. This implies that there exist tasks in which it is necessary to consider strictly
larger conjunctions in C
ce to obtain equally good heuristic estimates as are obtained with
C
C . This is not necessarily problematic however, as differently from hm , C
ce and  do not
introduce all conjunctions of a given size, and are therefore not exponential in the maximum
size of the conjunctions considered.
C
The advantage of C
ce over  is that it is potentially exponentially smaller in |C|; the
above domination therefore must be qualified against this reduction in size. Furthermore,
C
ce preserves the ability to compute a perfect heuristic given a sufficiently large set C of
conjunctions. We first consider the equivalent result for C , already proved by Haslum
(2012). We provide an alternative proof here that can be conveniently adapted to show the
C
same property for C
ce . The key to our proof for  is the following equivalence between
h1 (m ) and h1 (C ):
Lemma 1 Given a planning task , and C = {c  P(F ) | 1 < |c|  m}, h1 (m ) = h1 (C ).
Proof sketch: m and C are identical except for their action sets. h1 values are computed
by considering only a single add effect at a time. The inequality h1 (m )  h1 (C ) is
0
then easy to see by verifying that, for every add effect c of an action aC in C (unless
0
c  pre(aC ) and thus is redundant), the action ac in m dominates it, i. e., c  add(ac )
0
and pre(ac )  pre(aC ). The proof is similar for the inequality h1 (C )  h1 (m ), observing
that for any action ac in m and non-redundant add effect, there exists a dominating action
0
aC in C .
Theorem 4 (h+ (C ) is perfect in the limit) Given a planning task , there exists C
such that h+ (C ) = h ().
Proof: It is known that h () = hm () for sufficiently high values of m (Haslum &
Geffner, 2000), and as shown by Haslum (2009), hm () = h1 (m ). By Lemma 1, for
C = {c  P(F ) | 1 < |c|  m}, we have h1 (m ) = h1 (C ). Choosing an appropriate
m and the corresponding C, we thus have that h () = hm () = h1 (m ) = h1 (C ).
Together with the fact that h1 (C )  h+ (C ), and since h+ (C )  h () by admissibility
of h+ (C ), the claim follows.
1
C
+
C
To show the same claim for C
ce , all that remains is to relate h ( ) to h (ce ):

496

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Lemma 2 Given a planning task  and a set of non-unit conjunctions C, h1 (C ) 
h+ (C
ce ).
C
Proof sketch: Consider a planning task C
no-cc identical to  except that it drops cross1
context -fluents from preconditions. We show that (A) h (C )  h1 (C
no-cc ), and (B)
1
C
+
C
h (no-cc )  h (ce ).
Similarly to the proof of Lemma 1, (A) is easy to see by showing that every add effect
0
C 00 in C : we simply set C 00 to
c of an action aC in C
no-cc is dominated by an action a
the minimal subset of C 0 that contains c and satisfies condition (2) of Definition 2 (in other
words, we reduce C 0 to get rid of any cross-context -fluents).
+
C
For (B), it suffices to show that h+ (C
no-cc )  h (ce ). This holds because, for any action
C
0
a in a relaxed plan for ce , if C is the set of conjunctions that are added by conditional
0
effects of a when it is applied in the plan, then the action representative aC in C
no-cc has
the same preconditions as a, and can be used to achieve the same set of fluents.

Theorem 5 (h+ (C
ce ) is perfect in the limit) Given a planning task , there exists C
such that h+ (C
)
=
h ().
ce
Proof: Choosing an appropriate m and C, we have h () = hm () = h1 (m ), and,
by Lemma 1, h1 (m ) = h1 (C ). With Lemma 2, we get h1 (C )  h+ (C
ce ). Since, by
 (), this shows the claim.
Theorem 1, h+ (C
)

h
ce
Note that, with Theorem 3, Theorem 4 is actually a corollary of Theorem 5. Our presentation is chosen to make the relation between the two results, and the role of the two lemmas,
clearer.
The proofs of Theorems 4 and 5 rely on obtaining perfect hm , which is clearly unfeasible
in general since this involves enumerating all subsets of fluents (and hence all possible states)
in the worst case. However, C and C
ce offer flexibility in allowing us to choose the set
C: while selecting all subsets guarantees a perfect heuristic, this may be achieved with
much less effort, which is especially beneficial when using C
ce whose growth in |C| is linear.

Indeed, there are task families for which obtaining h takes exponential effort with hm , and
requires exponentially-sized C , yet for which C
ce remains small:
m and C ) There exist parameterized task
Theorem 6 (Expressive power of C
ce vs. h
families k such that

1. if hm (k ) = h (k ) then m  k,

C
2. h+ (C
k ) = h (k ) implies that the number of action representatives in k is exponential in k, and

3. for any k there exists Ck such that |Ck | (and therefore the number of conditional effects
+
C

in (k )C
ce ) is polynomial in k, and (b) h ((k )ce ) = h (k ).
Proof: Members of one such family are given by the combination of k planning tasks of
the type shown in Example 1, each of size k, that share among them the action a and the
fluent y that needs to be made true after each step. k then has k goals, and hm = h iff
m  k.
497

fiKeyder, Hoffmann, & Haslum

C
For both C
k and (k )ce to be perfect, k -fluents {xi1 , y}, . . . , {xik , y} must be introduced for each of the individual subtasks i, leading to a total of k 2 -fluents. If any one of
these -fluents is not present, then the precondition for the action bij in (k )C
ce , or similarly
its representative with C 0 = {} in C
,
have
only
the
individual
fluent
preconditions
y, xij ,
k
and in consequence one of the a actions reestablishing y can be left out of the plan. The
number of conditional effects created in (k )C
ce is linear in the number of -fluents added.
However, the number of action representatives in (k )C is exponential in k: the action a
adds the fluent y, that belongs to all -fluents, and hence a has one representative for each
subset of the -fluents.

When using h+ (C
ce ) in practice, we will not typically be able to choose a C that results
in a perfect heuristic. Instead, we try to pick a set C that yields an informative heuristic
without making the size of the representation impractical to work with.

5. Heuristics for Satisficing Planning
We now consider the practical issues involved in using C
ce for satisficing planning. Section 5.1 deals with the extraction of relaxed plans, and Section 5.2 deals with strategies for
choosing the set of conjunctions C. Section 5.3 presents our experiments with the resulting
setup.
5.1 Relaxed Planning with Conditional Effects
Techniques for extracting relaxed plans in the presence of conditional effects have long been
known (Hoffmann & Nebel, 2001). Here, we refine and extend those techniques. They are
particularly important in our context as, unlike in most IPC benchmarks, the structure
of the conditional effects in C
ce can be rather complex, involving multiple dependencies
between different actions, and even between different executions of the same action.3
Non-admissible delete-relaxation heuristics are typically obtained with a relaxed plan
extraction algorithm (Keyder & Geffner, 2008). The different variants of this algorithm
are characterized by the best-supporter function bs : F 7 A they use. In all cases, bs(p)
is an action adding p that minimizes some estimate of the cost of making p true. When
no conditional effects are present, the algorithms compute a set of actions  that can be
scheduled to form a relaxed plan for the planning task. Formally, the algorithms construct
a relaxed plan  according to the following equations (Keyder & Geffner, 2008):
(
{}
if p  s
(p) =
bs(p)  (pre(bs(p))) otherwise
[
(P ) =
(p)
pP

Existing methods for choosing best supporters, such as hadd or hmax , can easily be
extended to conditional effects by treating each conditional effect in the task as a separate
3. We remark that similar issues arise in approaches compiling uncertainty into classical planning with
conditional effects (Palacios & Geffner, 2009; Bonet, Palacios, & Geffner, 2009), so our techniques may
turn out to be useful there as well.

498

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

action. In particular, this is the method employed, using hmax , to compute the FF heuristic
function (Hoffmann & Nebel, 2001). More precisely, for each relaxed conditional effect
ce(a)+
i with condition c(a)i and add add(a)i , an action ai with the same add effect add(ai ) =
add(a)  add(a)i and precondition pre(ai ) = pre(a)  c(a)i is created. The set of effects (G)
as defined by the rules above then forms a relaxed plan. The presence of conditional effects,
however, implies that there is a problem of how to schedule that relaxed plan: different
schedules may require different numbers of action applications, as multiple applications of
a single action a can be avoided by making the conditions of multiple desired effects true
before a given application of a.
For illustration, consider a planning task where an action move-briefcase has n conditional effects, each of which conditionally transports an object from location A to location
B if it is inside the briefcase. Using the representation above, a distinct moving action is
generated for each conditional effect. So one possible schedule of the relaxed plan repeatedly
puts an object in the briefcase, applies move-briefcase, then proceeds to the next object.
This plan is n  1 steps longer than the optimal relaxed plan, which first places all of the
objects in the briefcase and then applies move-briefcase once.
In other words, as a single action execution may trigger several conditional effects at
once, there may exist a relaxed plan of length less than |(G)|. The question then arises of
how to optimally schedule the relaxed plan, minimizing the number of action applications
required. FF uses a simple approximate solution to this problem, that we outline and
improve upon below. But we first note that the problem of scheduling conditional relaxed
plans (SCRP) is actually NP-complete:
Theorem 7 (Scheduling conditional relaxed plans) Let + be a relaxed planning task
with conditional effects and (G) a set of effects that, viewed as a set of independent actions,
constitutes a plan for + . Deciding whether there exists a sequence of actions of length  k
such that all conditional effects in (G) are triggered is NP-complete.
Proof: Membership follows from the fact that given a sequence of k actions, it can easily be
checked in polynomial time whether all conditional effects in (G) are triggered. Hardness
follows by reduction from the shortest common supersequence problem (SCS) (Garey &
Johnson, 1979). A supersequence of a string x = d0 . . . dm over the alphabet  is a string
over the same alphabet that belongs to the language L =  d0  . . .  dm  . Given an
instance of the SCS problem with strings x0 , . . . , xn over the alphabet {0, 1} that asks
whether there exists a supersequence of these strings with length  k, we construct a
planning task with conditional effects  = hF, A, I, Gi, where
S
 F = ni=0 {yij | 0  j  |xi |}
 A = {a0 , a1 }, where az = {, , , ce(az )}, and ce(az ) is given by the set of conditional
effects
n |x[
i |1
[
{hyij , yij+1 , i | xij = z}
i=0 j=0

 I = {y00 , . . . , yn0 }
 G = {y0|x0 | , . . . , yn|xn | }
499

fiKeyder, Hoffmann, & Haslum

The two actions a0 and a1 correspond to the addition of the symbols 0 and 1 respectively
to the supersequence that is implicitly being constructed, and a fluent yij encodes the fact
that the current string constitutes a supersequence for a prefix xi0 , . . . , xij1 . It can then
be seen that a valid plan for the planning task must trigger all of the conditional effects
of the task, yet such a sequence of actions with length  k exists iff there is a common
supersequence of x0 , . . . , xn with length  k. This transformation of the SCS problem into
a planning task with conditional effects is polynomial, which shows the claim.
Note that Theorem 7 does not relate to the (known) hardness of optimal relaxed planning:
we wish only to schedule effects that we have already selected and which we know to form
a relaxed plan. This source of complexity has, as yet, been overlooked in the literature.
Given this hardness result, we employ a greedy minimization technique that we call
conditional effect merging. Starting with the trivial schedule containing one action execution
for each effect in (G), we consider pairs of effects e, e0  (G) that are conditional effects of
the same action a. The two effects are merged into a single execution of a if their conditions
can be achieved without the use of either of their add effects. FFs approximation method
applies similar reasoning, but captures only a special case in which this condition holds:
when both e and e0 appear in the same layer of the relaxed planning graph, which trivially
implies that the conditions of these effects are independently achievable. However, the
same may also be the case for effects in different layers of the relaxed planning graph. Here
we devise a strictly more general technique, capturing this form of independence between
effects using what we call the best supporter graph (BSG) representation of the relaxed plan
(for simplicity, we assume here that the task has a single goal fluent G0 , which if needed can
be achieved by introducing a new action end whose preconditions are the original goals,
and that adds G0 ):
Definition 4 (Best supporter graph) Given a relaxed planning task + and a best supporter function bs, the best supporter graph is a directed acyclic graph  = hV, Ei, where
V = (G), with (G) as above, E = {hv, v 0 i | p  pre(v 0 )  v = bs(p)}, each vertex is
labeled with the action whose conditional effect it represents, and each edge is labelled with
the set of preconditions {p | p  pre(v 0 )  v = bs(p)}.
The nodes of this graph represent conditional effects that appear in the relaxed plan, and
there exists an edge hv, v 0 i between two nodes if the effect represented by v is the best
supporter of a (pre)condition of the effect represented by v 0 .4 bs being a valid best supporter
function (i. e., the relaxed plan (G) generated by bs being sound) is a sufficient condition
for  being acyclic, and it can easily be shown that any topological sort of  is a sound
relaxed plan. This implies that, if there is no path in  between two conditional effects of
the same action, they can occur as the result of the same action application, and therefore
can be merged into a single occurrence of the action. These nodes are then removed from
the BSG, and a new node is added that represents both effects, combining their incoming
and outgoing edges. This process can be repeated until no further node merges are possible.
The algorithm runs in polynomial time and is sound in that it results in a BSG of which
any topological sort constitutes a relaxed plan for . It does not, however, guarantee an
optimal scheduling of the original plan.
4. The edge labels will be used in our procedure choosing the conjunction set C, described in Section 5.2.

500

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

For example, consider again the task where move-briefcase has n conditional effects
transporting an object from location A to location B if it is inside the briefcase. The nodes
in the BSG are n put-into-briefcase(oi ) actions (one for each object oi ), as well as n copies
of move-briefcase(A, B) (one for the conditional effect regarding each object oi ). There is
one edge from put-into-briefcase(oi ) to the respective copy of move-briefcase(A, B), labeled
with in-briefcase(oi ). There is therefore no path in the graph from any move-briefcase(A, B)
node to another, and they will be merged into a single node by the conditional effect merging
algorithm. All topological sorts of that merged BSG correspond to optimal relaxed plans.
5.2 Choosing C for Relaxed Planning
Algorithm 1 shows the main procedure for computing the set of conjunctions C used to form
the C
ce task. The algorithm is applied once, before the start of the search, to the initial
C
state of the planning task. The resulting C
ce (or  ) task is then used for all subsequent
heuristic evaluations. Conditional effect merging is not used during the conflict extraction
phase in any configuration discussed below, i. e., we use the original non-merged BSG as
stated in Definition 4.
Algorithm 1: Choosing C for relaxed plan heuristics.
C=
 = RelaxedPlan(C
ce )
while  not a plan for  and size(C
ce ) < bound do
C = C  FindConflicts()
 = RelaxedPlan(C
ce )
Algorithm 1 is, at a high level, very similar to the procedure previously introduced for
computing incremental cost lower bounds based on the C construction (Haslum, 2012).
The algorithm repeatedly generates relaxed plans for the initial state of the current compiled
task. It adds new conjunctions to C based on the conflicts that are found in the current
plan, i. e., based on how the current relaxed plan fails when executed in the original planning
task . The process stops when either no further conflicts can be found, implying that the
current relaxed plan for C
ce is a plan for the original planning task, or when a user-specified
C
bound on the size of ce is reached. We will express this bound in terms of the size of C
ce
compared to  (see below). We will also sometimes impose a bound on the runtime of the
algorithm.
If no bound is specified, and if FindConflicts() returns at least one new conjunction as
long as  is not a plan for , Algorithm 1 is a complete planning algorithm in its own right.
We report results for this usage of the algorithm in our experiments below. If the relaxed
plan generated in each iteration is optimal, Algorithm 1 can be used to compute a sequence
of admissible cost estimates that converges to the optimal plan cost (Haslum, 2012). Our
focus, however, is on the use of C
ce for generating inadmissible heuristic functions. We
therefore use a tractable, non-optimal, relaxed planning procedure, and impose a bound
that typically stops Algorithm 1 before a plan for the original task has been found.
It remains to specify the FindConflicts procedure: Given a relaxed plan that fails to
execute in the original planning task , how to select the set of new conjunctions C? One
501

fiKeyder, Hoffmann, & Haslum

answer to this question has been provided by the previous use of Algorithm 1 to compute
plan cost lower bounds (Haslum, 2012). Because our aim is different  computing heuristics
for satisficing search-based planning  we make a number of changes to the previously
proposed version of FindConflicts. Section 5.2.1 summarizes the original procedure, and
Section 5.2.2 describes the changes we make to it.
5.2.1 Conflict Extraction for Incremental Plan Cost Lower Bounds
Given an optimal relaxed plan that is not a plan for the original planning task, Haslums
(2012) version of FindConflicts returns a set of conjunctions C that prevents the same relaxed
plan from being a solution in the next iteration. This ensures progress, in the sense that
the cost of the relaxed plan will eventually increase, or prove to be the real plan cost. To
describe the conflict extraction procedure, we need two definitions:
Definition 5 (Relaxed Plan Dependency Graph) Let  be a non-redundant plan for
the relaxed planning task + . Construct a directed graph G () with one node va for each
action a in , plus a node vG representing the goal. Let pre(v) denote the precondition of
node v, which is pre(a) for a node va and G for node vG . G (S) has a directed edge from
va to v 0 iff pre(v 0 ) is not relaxed reachable using the set of actions in  minus {a}. The
edge is labelled with the subset of pre(v 0 ) that is relaxed unreachable with these actions. The
relaxed plan dependency graph, RPDG(), is the transitive reduction of G ().
The RPDG is similar to the BSG above (Definition 4), but encodes the necessary dependencies between actions in a relaxed plan. A path from a node va to a node vb in the
RPDG implies that a precedes b in every valid sequencing of ; in this case, vb is said
to be ordered after va . In contrast, the BSG encodes the intentions of the relaxed plan
heuristic, in the form of the chosen best supporters, and may impose orderings that need
not be respected in every valid sequencing of the plan (e. g. if a fluent p is added by another
action in the relaxed plan that is not the best supporter of p). Because the relaxed plan is
non-redundant, meaning that no action can be removed without invalidating it, there is a
path from every action node in the RPDG to the goal node.
Definition 6 (Dependency Closure) Let  be a non-redundant plan for the relaxed planning task + , and let v and v 0 be nodes in RPDG(), where v 0 is ordered after v. A simple
q1
q2
qm
dependency path is a path v  v1  . . .  v 0 from v to v 0 in RPDG(), where each edge
is labelled by one fluent, chosen arbitrarily, from the edge label in RPDG(). (Whenever
v 0 is ordered after v, a simple dependency path from v to v 0 exists.) A dependency closure
from v to v 0 is a minimal, w.r.t. subset, union of paths, such that (1) it contains a simple
dependency path from v to v 0 , and (2) if q is the fluent that labels an edge from a node v 00
in the closure, and a is an action with q  add(a), where a is not the action associated with
v 00 , then the closure contains a simple dependency path from v to the node corresponding to
a. (Such a path is guaranteed to exist.)
Recall that input to FindConflicts is a plan, , that is valid for the delete relaxation +
but not for the original planning task  when delete effects are considered. Because  is
valid for + , the preconditions of all actions in , as well as all goals, must be made true at
502

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

...
vd
p1
r
...
vf
q1

p
vd

q1

...

qn

vf

pn
vj
qm

(b)

(a)

Figure 1: Relaxed plan failure scenarios. Wavy edges show deletions of a precondition.
some point. Thus, if  fails to solve the original task  it must be the case that some action
d, which we call the deleter, deletes a precondition of some other action f , which we called
the failed action. Note that the failed action here can also be the goal. Let p  pre(f )
be the deleted fluent. The procedure distinguishes two cases, based on the relation between
nodes vd and vf in the RPDG:
In the first case, illustrated in Figure 1 (a), vf is ordered after vd . Choose a dependency
closure from vd to vf , and let L be the set of fluents labelling the edges in this closure: the
set of conflicts generated is {{p, q} | q  L}. (Note that p 6 L, and thus each conflict is a
proper conjunction.)
If the first case does not hold, then vd and vf are unordered. They must have a nearest
common descendant node, vj , in the RPDG, so we are in the situation illustrated in Figure 1
(b). Choose a dependency closure from vd to vj , and let L1 be the set of fluents labelling
the edges in this closure. Likewise, choose a dependency closure from vf to vj , and let L2
be the set of fluents labelling the edges in this closure. The set of conflicts generated is then
{{q, q 0 } | q  L1 , q 0  L2  {p}}.
Theorem 8 (Haslum, 2012, Theorem 6) Let  = a1 , . . . , an be a non-redundant plan
for the delete relaxed task + that is not valid for the original task , and let C be a set of
conjunctions extracted by the procedure described above. No action sequence  0 = a01 , . . . , a0n
such that each a0i is a representative of ai is a valid plan for C .
5.2.2 Changes to Conflict Extraction for Satisficing Planning
There are a number of differences between our setting and that of Haslum (2012). In
particular, although -fluents are collected only in the initial state, the resulting C
ce task
will be used for heuristic evaluations of all states encountered in the search, and growth
in the size of the C
ce task will incur an overhead on each heuristic evaluation. Thus, our
objective is to find a set C that will make the heuristic more accurate across all states,
while keeping the size of C limited. On the other hand, computing non-optimal relaxed
plans is computationally far cheaper than optimal relaxed planning, so we can afford more
iterations in Algorithm 1.
Therefore, we make the following modifications to the strategy: First, we use the BSG
instead of the RPDG. The necessity of orderings in the latter does not extend beyond
the current (initial) state, and therefore is not useful for our purpose. The BSG is more
representative of the relaxed plans found by the non-optimal relaxed planning procedure.
Second, we introduce just a single -fluent in each iteration of Algorithm 1. Most of the
time, this does cause a new relaxed plan to be found, which allows the algorithm to focus
on finding a small number conflicts that are useful in a wide range of states. The chosen
conflict is {p, qn } in the case depicted in Figure 1 (a), and {pn , qm } in that of Figure 1 (b).
503

fiKeyder, Hoffmann, & Haslum

Intuitively, this works better in our setting because the set of all conflicts generated by the
same plan failure tends to be redundant, and thus needlessly grows the size of the task
leading to slow evaluation times without much gain in informativeness.
These changes do not affect the fundamental property of Algorithm 1, that it converges
to a real plan. To show convergence, the only property that FindConflicts must have is that
it returns at least one new conjunction whenever  fails to solve the original task. Our
variant still gives that guarantee:
Lemma 3 Assume we eliminate dominated preconditions 5 in C . Let  = a1 , . . . , an be
a non-redundant plan for C that is not valid for the original task , and let c be the
conjunction extracted by the procedure described above. Then c 6 C.
Proof: This is simply because, in both possible relaxed plan failure scenarios (Figure 1), the
chosen conjunction c = {x, y} ({x, y} = {p, qn } respectively {x, y} = {pn , qm }) is contained
in the precondition of the failed action f . Assuming that c = {x, y}  C, as we eliminate
dominated preconditions, no action precondition in C contains both x and y. Hence, in
that case, c cannot be the chosen conjunction.
Theorem 9 (Convergence of conflict extraction) Assume we eliminate dominated preconditions in C , and Algorithm 1 is run without a size bound. Then eventually  will be
a plan for .
Proof: Follows from Lemma 3 as the set of possible conjunctions is finite.
Contrasting Theorem 9 with Haslums variant (Theorem 8), the latter gives a stronger
convergence guarantee (only) in the sense that it guarantees a certain minimum progress
is made in each iteration.
Lemma 3 (and thus Theorem 9) holds in the same way for C
ce , i. e., when  is a sequence
C
of conditional effects in ce , that, viewed as a set of independent actions, constitutes a nonredundant plan for C
ce . We rely on eliminating dominated preconditions here as that
makes the proof very simple, and we use the technique in practice anyway. We did not
verify whether or not convergence holds also if dominated preconditions are not eliminated;
we conjecture that it does.
Since there can be multiple conflicts in the BSG of a relaxed plan, in our experiments
we choose (arbitrarily) one that minimizes the number of conditional effects (or STRIPS
actions, in the case of C ) created. We place a bound on the factor x by which C
ce exceeds
the size of the original planning task . Precisely, when x = 1, no -fluents or conditional
+
effects are added, and C
ce =  , resulting in a standard relaxed plan heuristic. For growth
bounds x > 1, -fluents are added until the number of conditional effects in the task reaches
(x  1)  |A|. For C , x limits the total number of actions in the task as a multiple of |A|.

5. Recall that eliminating dominated preconditions means that, whenever we add a fluent c to the precondition of an action, or to the condition of a conditional effect, we remove from that condition all fluents
p  c and -fluents {c0 | c0  c}.

504

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Example 2 Consider again the STRIPS planning task from Example 1, with variables
{x0 , . . . , xn , y}, initial state I = {x0 , y}, goal G = {xn }, and unit-cost actions
a : h, {y}, , i

bi : h{xi , y}, {xi+1 }, {y}, i

for i = 0, . . . , n  1.
As previously discussed, setting C = {x1 ,y , . . . , xn1 ,y } renders the delete relaxation
perfect, i. e., results in the relaxed plan having to re-establish y in between every two bactions. Exactly that set C is iteratively selected by our procedure.
Assuming a best supporter function based on either of hadd or hmax , in the first iteration
of Algorithm 1 the BSG will be:

b0

x1

b1

x2

...

b2

bn2

xn1

bn1

The relaxed plan fails to execute when trying to apply the second action, b1 . The corresponding failure scenario matches Figure 1 (a):

y
b0

b1

x1

The chosen conflict thus is {y, x0 }. With the now non-empty set of conjunctions C containing just that single conjunction, the precondition of b1 contains {x1 ,y} which must be
established using action a, so that the BSG now takes the form (note that the dominated
preconditions y and xi of b1 are eliminated):

b0

x1

a

{x1 ,y}

b1

b2

x2

...

bn2

xn1

bn1

The relaxed plan now fails to execute when trying to apply the fourth action, b2 . The
corresponding failure scenario is:

y
b1

x2

b2

The chosen conflict now is {y, x2 }. Iterating the procedure will, in the same manner, select
exactly the set C above one-by-one, at the end of which the relaxed plan will solve the original
planning task.
5.3 Experiments
We evaluate the impact of using the C
ce compilation in a relaxed plan heuristic in the context of a greedy search. The expected impact of using a heuristic based on the improved
relaxation is two-fold. On the one hand, it should make the heuristic more informative,
505

fiKeyder, Hoffmann, & Haslum

enabling the search to find plans with fewer node evaluations. On the other hand, there is a
computational overhead associated with the growth of the problem, slowing down heuristic
evaluations. We examine both of these effects individually, as well as their combined influence on coverage, or the set of problems that the planner is able to solve within given time
and memory bounds, which we take to be the main measure of performance.
In this study, we do not consider the objective of producing plans of high quality (as
measured by plan length or cost). That is not because plan quality is unimportant. Rather,
the rationale for this decision is methodological: Seeking a high quality plan is not the
same problem as seeking to find a plan with minimum search effort  particularly when
quality is measured by non-unit action costs  and the requirements on heuristics for the
two problems are quite different. Here, we have chosen to focus on one, viz. search efficiency,
as measured by coverage and node evaluations, rather than conflate the two. The choice of
a plain greedy search algorithm is also motivated by this decision. As a consequence, we
treat all actions as having a unit cost of 1. Previous experiments have shown that in the
context of greedy search, distinguishing action costs in heuristic calculation tends to result
in lower coverage (Richter & Westphal, 2010). However, to at least assess the impact of our
heuristics on plan quality, we do report data regarding plan length.
We next describe the experiment setup and baseline. We then discuss heuristic informativeness, computational overhead, the impact of conditional effect merging, the impact on
plan length of using C
ce heuristics, a comparison with other state-of-the-art heuristics for
the same problem, the difference between using the C and C
ce compilations, and finding
plans with no search.
5.3.1 Experiment Setup and Baseline
The compilation and associated heuristics were implemented in the Fast Downward planner
(Helmert, 2006), and used in a greedy best-first search, with lazy evaluation and a second
open list (with boosting) for states resulting from preferred operators. The planners were
tested on all of the STRIPS domains from the 19982011 editions of the International
Planning Competition (IPC). For domains from the last two IPCs, only the most recent
sets of instances were used. All experiments were run on Opteron 2384 processors with the
settings used in the competition: a memory limit of 2Gb and a time limit of 30 minutes.
The baseline planner configuration uses the relaxed plan heuristic, with best supporters
identified by hadd , on the unmodified planning task (i. e., with the growth bound x = 1).
It is a known fact that greedy search, and in particular greedy search with lazy evaluation
and a strong bias towards preferred operators, can be highly sensitive to small changes
in the relaxed plan, even changes that do not alter the heuristic value but rather only the
operators that are preferred. Unfortunately, this fact is very rarely taken into account when
heuristics are compared in the context of greedy search. Since the introduction of -fluents
alters the structure of the relaxed plan, we believe it is particularly important to determine
whether the resulting differences in planner performance are really due to the relaxed plan
being more (or less) informative.
Therefore, as a first step towards accounting for the brittleness of experiments with
greedy heuristic search, we introduce a simple variance measure and use it to decide when the
results of our experiments should be considered significant. Variance in the performance
506

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

of the baseline planner is measured by randomizing the choice of supporters with equal
hadd values in the construction of the relaxed plan and measuring the maximum deviation
from the results of the baseline planner over five repeated runs. The results are shown in
columns labeled MaD (Tables 1 and 4). For each domain and for the problem set as a
whole, the deviation is defined by the differences in coverage and in the median number of
heuristic evaluations. Note that we are not interested in whether randomization helps
or hurts the search, but rather in the magnitude of the variation that it causes. When
C
comparing the results of the planner using heuristics based on C
ce or  under different
growth bounds with the results of the baseline planner, we consider the difference between
them to be significant if it is greater in magnitude than the maximum deviation observed
with randomization of the baseline. This should not be interpreted as significance in the
statistical sense (although, if we assumed that randomization affects all heuristics equally,
we could estimate the probability of the hypothesis of no difference), but simply as setting
a reasonable threshold for what counts as a substantial difference in search performance.

5.3.2 Heuristic Informativeness
The comparison of heuristic informativeness is summarized in the right half of Table 1,
which shows the ratio of the median (per domain, over tasks solved by both planners)
number of heuristic evaluations for the baseline planner to that of the planners using the
C
ce -based heuristics. In just under half the domains, the difference in informativeness of the
C
ce -based heuristics compared to the baseline does not exceed the threshold for significance
set by the sensitivity study (shown in the MaD column). Among the domains where
there is a significant difference, in the majority using the C
ce -based heuristics reduces the
number of node evaluations, indicating that the augmented heuristics are more informative.
In most of these cases, the ratio grows as more -fluents are added, i.e., as the growth bound
x is increased. The most drastic example can be seen in the Floortile domain, where all
C
ce -based heuristics evaluate four or more orders of magnitude fewer nodes, compared to
the standard delete relaxation heuristic. This allows them to easily solve all of the instances
in the domain. By comparison, no planner in IPC 2011 was able to solve more than 9 of
the 20 instances in this domain. In the Woodworking domain, C
ce heuristics are more than
two orders of magnitude more informative, but there is no associated increase in coverage
as all tasks are solved by all configurations.
In roughly a third of the domains there is a consistent (or nearly consistent) loss of
informativeness, though in most of them it is not significant. Note that this loss of informativeness does not always correlate with a loss in coverage. This can be attributed to different
factors, including the small magnitudes of loss, as well as the fact that the ratio of node
evaluations is taken only over tasks solved by both the planners compared. Another issue is
that dramatic coverage losses are often due to the computational overhead incurred by the
C
ce compilation. In particular, in the Openstacks and Satellite domains, the decrease in the
number of tasks solved with the C
ce -based heuristics matches almost exactly the number of
tasks for which the conflict selection and compilation process fails to complete within the
1800 seconds allocated per task. We get back to this in the next subsection.
507

fiKeyder, Hoffmann, & Haslum

It is worth noting that the quality of C
ce -based heuristics can be highly sensitive to the
precise choice of -fluents used in the compilation.6 Hence, there may exist better policies
for making this choice than the relatively simple one we have used here.
The HO and PO colums in Table 1 (coverage only) examine the effect of the new
heuristic function, respectively the new preferred operators returned by that function, in
separation. PO corresponds to a configuration that uses the relaxed plan from the C
ce task
(built with x = 1.5 and a timeout of t = 60s, discussed in Section 5.3.3) only to identify
preferred operators, together with the heuristic value from the baseline (x = 1) heuristic.
HO, on the other hand, uses the heuristic values obtained with x = 1.5 and the preferred
operators from x = 1. Interestingly, either heuristic values or preferred operators alone are
sufficient to greatly improve coverage in Floortile, the domain in which our techniques have
the greatest impact. Both the HO and PO configurations are able to solve every instance
of this domain.7 The effect in other domains is mixed, with both configurations solving
sometimes more, sometimes fewer instances.
5.3.3 Computational Overhead
The computational overhead of the C
ce -based heuristics, compared to the standard relaxed
plan heuristic, stems from two sources: (1) the time spent on computing the set of -fluents
to add to the problem, and (2) the greater overhead of heuristic evaluation in the C
ce task.
Table 2 shows three measures of their impact.
The first four columns (under Timeouts) show the number of instances in which the
construction of the C
ce task does not finish within 1800 seconds, while the second set of
four columns (under > 60 sec) shows the number of instances for which the construction
time exceeds 60 seconds (inclusive of those instances in the first set of columns). Note that
this behavior  spending a large amount of time on the C
ce construction without reaching
the growth bound  is partly due to our strategy for selecting -fluents, since we purposely
choose those -fluents which will increase the size of the compiled task the least. While
there are several domains in which construction time frequently exceeds 60 seconds, this
does not happen in those domains where the C
ce -based heuristic are most informative, such
as Floortile and Woodworking. This suggests that imposing a time limit on the construction
of the C
ce task will incur only a small loss of informativeness. We present coverage results
for such a strategy (using a 60 second time limit) in Table 4 below. It is significantly better
than the baseline planner, and compares favourably with other state of the art heuristics.
As expected, in most domains evaluating heuristics on the C
ce task is slower than on the
standard delete relaxation, and tends to slow down more as the growth bound x increases,
due to the larger number of fluents and actions in the compiled planning task. The median
slowdown per domain is typically of the same order as x itself, and exceeds one order of
6. Indeed, the results reported in our earlier paper (Keyder, Hoffmann, & Haslum, 2012) show an increase
in informativeness in the Barman and Parcprinter domains.
7. A plausible explanation for this is the behavior with respect to dead-end states (intuitively, where the
robot has painted itself into a corner) that are unrecognized under the standard delete relaxation
heuristic, i. e., where a relaxed plan for  exists. It appears that C
ce is highly effective at fixing this
issue: While under x = 1 search encounters millions of states with hFF () = , HO encounters only
FF
few states with hFF (C
(C
ce ) =  (suggesting that h
ce ) prunes dead-ends early on), and PO encounters
no such states at all (suggesting that hFF (C
)
preferred
operators prevent the search from entering the
ce
dead-end regions in the first place).

508

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain
x=1 MaD
Airport (50)
36
Barman (20)
13
Blocksworld (35)
35
Depots (22)
19
Driverlog (20)
20
Elevators (20)
19
Floortile (20)
6
FreeCell (80)
79
Grid (5)
5
Gripper (20)
20
Logistics00 (28)
28
Logistics98 (35)
34
Miconic (150)
150
Mprime (35)
35
Mystery (30)
16
Nomystery (20)
9
Openstacks (20)
20
Parcprinter (20)
16
Parking (20)
20
Pathways (30)
30
Pegsol (20)
20
Pipes-NoTk (50)
42
Pipes-Tank (50)
38
PSR (50)
50
Rovers (40)
40
Satellite (36)
35
Scanalyzer (20)
18
Sokoban (20)
19
Tidybot (20)
16
TPP (30)
30
Transport (20)
11
Trucks (30)
14
Visitall (20)
19
Woodwork (20)
20
Zenotravel (20)
20
Total (1126)
1002

+2

+5

+0

+1

+0

+3

+0

+2

+0

+0

+0

+1

+0

+0

+0

+2

+0

+7

+1

+2

+0

+1

+4

+0

+0

+1

+1

+1

+3

+0

+1

+0

+1

+0

+1

+19


Coverage
x =
PO HO
1.5 2 2.5 3 1.5 1.5
+0
+1 +3 +1 +2 +1 
+6 +3 +1 3 3 +5
+0 
+0 
+0 
+0 
+0 
+0

+0 1
+2 +2 +2 +2 
+0 
+0 
+0 
+0 
+0 
+0

+1 +1 +1 +1 +1 +1
+14 +14 +14 +14 +14 +14
+0 +1
+1 3 4 2 
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+0 1 
+0 +1 +1
+1 
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+3 +3 +3 +3 +3 +1
2 3 3 3 1 2
9 9 9 9 1 1
11 8 7 7 4 10
+0 6
2 8 7 5 
+0
1 2 5 1 1 
+0 
+0 
+0 
+0 
+0 
+0

+0 1 
+0 
+0 1 
+0

+0 +2 1 +3
+3 +2 
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 1 
+0
1 
+0
2 5 6 8 +1 
+2 +2 +2 +2 1 +2
+0 2
2 2 3 3 
+0 
+0 1 1 
+0
2 
+0 
+0 
+0 
+0 
+0 
+0

+2 2 +1 +1 7 3
+0 +2
+1 +3 +4 +2 
+1 +1 2 2 2 2
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+6 9 17 14 3 +3

Median Node Evaluations Ratio
x =
1.5
2
2.5
3
4.32
1.36 : 1
1.34 : 1
1.40 : 1
1.50 : 1
9.91
1 : 1.63
1 : 2.22
1 : 8.33
1 : 25
5
2.84 : 1
2.90 : 1
2.90 : 1
3.02 : 1
12.13
2.84 : 1
3.26 : 1
3.65 : 1
8.55 : 1
1.16 2.17 : 1
2.64 : 1
2.65 : 1
2.31 : 1
1.31
1.25 : 1
1.47 : 1
1.34 : 1
1.22 : 1
6.42 15013 : 1
15110 : 1
14757 : 1
19674 : 1
1.28
1 : 1.11
1 : 1.28
1 : 1.17
1 : 1.29
1.21 1.27 : 1
2.83 : 1
1.27 : 1
1.27 : 1
1
1 : 1.05
1 : 1.51
1 : 1.49 1.23 : 1
1.64
1.47 : 1
1.47 : 1
1.53 : 1
1.60 : 1
1.45 2.53 : 1
2.57 : 1
2.83 : 1
2.70 : 1
1 1.16 : 1
1.22 : 1
1.29 : 1
1.36 : 1
1.09 2.33 : 1
2.33 : 1
2.33 : 1
2.33 : 1
1.07 1.21 : 1
1.27 : 1
1.29 : 1
1.29 : 1
13.93
1 : 1.44
2.58 : 1
9.95 : 1
10.13 : 1
1.08 2.52 : 1
1.31 : 1
1 : 1.23
1 : 1.11
1.58
1 : 1.04
1 : 1.04
1 : 1.08
1 : 1.09
7.06
1 : 1.12
3.02 : 1
1.61 : 1
1.18 : 1
1.21
1 : 1.03
1:1
1 : 1.05
1 : 1.11
3.42
1.44 : 1
1.09 : 1
1.66 : 1
1.41 : 1
16.47
1 : 1.07
1.08 : 1
1.14 : 1
1.29 : 1
2.46
1:1
1:1
1:1
1 : 1.05
1
1:1
1.02 : 1
1.12 : 1
1.12 : 1
1.33
1.12 : 1
1.15 : 1
1.12 : 1
1.20 : 1
1.36
1.15 : 1
1.36 : 1
1.30 : 1
1.29 : 1
1.85
1.17 : 1
1.80 : 1
3.43 : 1
3.25 : 1
1.46
1.17 : 1
1 : 1.09
1.01 : 1
1.06 : 1
1.22
1.15 : 1
1 : 1.25
1 : 1.38
1 : 1.08
2.29
1 : 1.20
1 : 1.13
1 : 1.19
1 : 1.01
2.73
1.20 : 1
1 : 1.11
1 : 1.29
1 : 1.01
1.66
1.16 : 1
1.97 : 1
7.27 : 1
4.57 : 1
1.34
1.01 : 1
1 : 1.08
1 : 1.09
1 : 1.49
52.78 245.72 : 1
263.3 : 1
245.72 : 1
245.72 : 1
1.28
1.22 : 1
1.24 : 1
1.36 : 1
1.42 : 1
MaD

Table 1: Planner coverage and heuristic informativeness using C
ce with varying growth bounds,
without conditional effect merging. Coverage shows the number of problems solved for the baseline
configuration (x = 1), and the difference (increase/decrease) relative to the baseline for the other
configurations; PO uses only the preferred operators obtained from the C
ce compilation with
x = 1.5 and t = 60s, returning the x = 1 heuristic value, while HO uses the heuristic values
obtained with x = 1.5 and t = 60s, and the preferred operators from x = 1. Heuristic informativeness
is measured by the ratio of the per-domain median number of node evaluations, comparing the
baseline to our configurations (across those instances solved by both configurations), normalized so
that the smaller value is 1. That is, an entry m : 1 means that the baseline planner requires m times
as many heuristic evaluations as the other planner. Columns labeled MaD show the magnitude
of the maximum deviation (in coverage and ratio) from the baseline in our sensitivity study: values
in bold are those that exceed this threshold, and which we therefore consider to be significant.

magnitude only in the Floortile domain when x = 1.5. Somewhat surprisingly, there are
domains in which heuristic evaluations become faster as more -fluents are added. A possible
explanation for this is that because we eliminate dominated preconditions (cf. Section 3),
the number of action preconditions decreases and the delete-relaxation hypergraph of the
C
ce becomes more graph-like as a result.
509

fiKeyder, Hoffmann, & Haslum

Domain
Airport (50)
Barman (20)
Blocksworld (35)
Depots (22)
Driverlog (20)
Elevators (20)
Floortile (20)
FreeCell (80)
Grid (5)
Gripper (20)
Logistics00 (28)
Logistics98 (35)
Miconic (150)
Mprime (35)
Mystery (30)
Nomystery (20)
Openstacks (20)
Parcprinter (20)
Parking (20)
Pathways (30)
Pegsol (20)
Pipes-NoTank (50)
Pipes-Tank (50)
PSR (50)
Rovers (40)
Satellite (36)
Scanalyzer (20)
Sokoban (20)
Tidybot (20)
TPP (30)
Transport (20)
Trucks (30)
Visitall (20)
Woodwork (20)
Zenotravel (20)
Total (1126)

Timeouts
x =
1.5 2 2.5 3
1
1
1
1

1.5
15

> 60 sec
x =
2
2.5
20
23

3
26

1

9

3

9

6

16

10

10

12

1

1

3

6

9

10

9

9

16

16

16

16

14

16
3

16
6

16
7

5
9
3

5
11
3

5
13
3

5
13
4

6

1
5
12
2
8

1
5
16
2
9

1
6
17
4
10

96

118

134

148

7

2

13

10

20

9

3

23

4
11

Ratio of Median Evaluations/sec
x =
1.5
2
2.5
3
1.08 : 1
1.19 : 1
1.30 : 1
1.26 : 1
1.35 : 1
1.88 : 1
2.42 : 1
3.09 : 1
1.50 : 1
1.60 : 1
1.56 : 1
1.42 : 1
1.65 : 1
1.81 : 1
2.03 : 1
2.31 : 1
1.56 : 1
2.38 : 1
3.17 : 1
4.07 : 1
1.87 : 1
2.92 : 1
3.99 : 1
5.45 : 1
17.67 : 1
8.28 : 1
5.97 : 1
5.49 : 1
1.20 : 1
1.31 : 1
1.39 : 1
1.66 : 1
1.27 : 1
2.55 : 1
3.04 : 1
3.04 : 1
1 : 2.13
1 : 2.04
1 : 2.13
1 : 1.69
1.09 : 1
1 : 1.76
1 : 1.35
1 : 1.22
1.20 : 1
1.64 : 1
2.55 : 1
3.72 : 1
1.06 : 1
1.10 : 1
1.18 : 1
1.27 : 1
1.60 : 1
1.80 : 1
1.60 : 1
1.80 : 1
1.28 : 1
1.35 : 1
1.42 : 1
1.42 : 1
1.43 : 1
2.20 : 1
2.53 : 1
3.26 : 1
1.16 : 1
1.73 : 1
2.49 : 1
3.17 : 1
1 : 8.33
1 : 8.33
1 : 6.25
1 : 1.81
2.32 : 1
3.61 : 1
4.84 : 1
6.20 : 1
1.36 : 1
2.04 : 1
1.01 : 1
1.64 : 1
1.25 : 1
1.08 : 1
1.48 : 1
1.11 : 1
1.41 : 1
1.94 : 1
2.31 : 1
2.78 : 1
1.61 : 1
2.37 : 1
2.42 : 1
3.39 : 1
1:1
1:1
1.02 : 1
1 : 1.05
1.11 : 1
1.43 : 1
1.48 : 1
1.83 : 1
1.14 : 1
1.07 : 1
1 : 1.01 1.30 : 1
1.09 : 1
2.22 : 1
2.42 : 1
2.73 : 1
1.23 : 1
1.38 : 1
1.62 : 1
1.82 : 1
1.48 : 1
2.03 : 1
3.36 : 1
2.22 : 1
1.59 : 1
1.70 : 1
2.38 : 1
2.84 : 1
2.38 : 1
4.04 : 1
6.50 : 1
8.80 : 1
1.62 : 1
2.29 : 1
2.52 : 1
3.30 : 1
1 : 1.49
1 : 1.33
1 : 1.15
1 : 1.07
1 : 2.63
1 : 2.08 3.29 : 1
6.32 : 1
1.09 : 1
1:1
1.06 : 1
1.10 : 1

Table 2: Computational overhead of C
ce . The first set of columns (Timeouts) shows the number

of tasks for which the C
ce construction does not finish within the 1800 second time limit, and the
second set (> 60 sec) shows the number of tasks for which construction time exceeds 60 seconds
(inclusive of those in the first set of columns). To improve readability, only non-zero entries are
shown (i.e., the blank cells in these columns are zeroes). The last set of columns shows the median
(per domain, over commonly solved tasks) ratio of heuristic evaluations per second for the baseline
planner (x = 1) to that of the other planner (x > 1). An entry m : 1 means that the baseline planner
performs m times as many heuristic evaluations per second as the other planner.

5.3.4 Conditional Effect Merging
In a majority of domains, conditional effect merging slightly increases or does not change
the informativeness of the C
ce -based heuristics. Some exceptions to this are the Logistics00
and Gripper domains, where merging results in a heuristic that is twice as informative
(using the same ratio of median number of evaluations metric as presented in Table 1), the
Nomystery domain where it is an order of magnitude more informative (for the problems
that are solved by both heuristics), and the Barman domain where it is four times less
informative. In general, higher informativeness occurs in domains in which all tasks are
510

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

solved by all planners, so it does not result in increased coverage. Indeed, as shown in
Table 4 below, conditional effect merging proves to be detrimental to the overall coverage
C
of the planner using the C
ce -based heuristic: the best ce configuration with conditional
effect merging solves, in total, only 4 more tasks than the standard relaxed plan heuristic,
while the same configuration without conditional effect merging solves 20 more tasks.
The runtime overhead of the merging procedure is quite small, as the transitive closure
operation required to check whether there is a path between two nodes of the BSG can be
implemented very efficiently when the graph is known to be directed acyclic, as is the case
here. For x = 1.5, comparing the C
ce -based heuristic with conditional effect merging to
that without, the ratio of the median number of heuristic evaluations per second (the same
metric as used on the right-hand side of Table 2) shows a maximal per-domain slow-down
of 2.04, and an across-domain average of 1.11. Coverage decreases in domains other than
Barman therefore appear to be due to the sensitivity of search to small changes in the
heuristic function (rather than due to the time taken to compute that function).
5.3.5 Plan Length with C
ce
To determine the effect of using C
ce heuristics on plan quality, we compare the length of the
plans found with C
heuristics
to
those found with x = 1, the standard delete relaxation
ce
heuristic. The plan length measure is equivalent to plan quality in the unit-cost setting.
We consider the median ratio of plan length found with the standard delete relaxation
heuristic to that found with the C
ce heuristic, over the set of instances that are solved by
both configurations (Table 3). In general, we do not observe large differences, with the
median ratio staying close to 1. One notable exception is the blocksworld domain, in which
heuristics based on the C
ce compilation consistently find significantly shorter plans. This
C
results from the ce heuristics ability to deduce implicit ordering constraints in the domain,
avoiding actions that lead to temporary improvements during greedy search but that later
need to be reversed, adding to plan length. C
ce also leads to shorter plans in the Gripper,
Mprime, and Woodworking domains, but tends to result in longer plans in the Barman and
Grid domains.
5.3.6 Comparison to the State of the Art
Table 4 shows coverage for a variety of heuristics and planners. The best configurations
for each of the two compilations with x > 1 achieve better overall coverage results than
the standard relaxed plan heuristic. The best-performing heuristics obtained with the C
ce
compilation without conditional effect merging, and the C compilation, give coverages of
1022 and 1023 respectively. The difference between these and the coverage of the baseline
planner is greater than the significance threshold. These numbers also far exceed the coverage obtained with the hcea heuristic, but fall short of the 1039 instances solved with the
dual heuristic approach used by LAMA. However, combining LAMA and the best C
ce -nm
configuration in a portfolio planner that runs LAMA for 1500 seconds and search with the
C
ce -nm heuristic for 300 seconds results in a coverage of 1063 out of 1115 solvable problems.
Almost all of this difference results from the C
ce -based heuristics superior performance in
the Floortile, and to a lesser extent, Airport domains.
511

fiKeyder, Hoffmann, & Haslum

Domain
Airport
Barman
Blocksworld
Depots
Driverlog
Elevators
Floortile
FreeCell
Grid
Gripper
Logistics00
Logistics98
Miconic
Mprime
Mystery
Nomystery
Openstacks
Parcprinter
Parking
Pathways
Pegsol
Pipesworld
Pipesworld
PSR
Rovers
Satellite
Scanalyzer
Sokoban
Tidybot
TPP
Transport
Trucks
Visitall
Woodwork
Zenotravel

x = 1.5
1 : 1.00
1 : 1.11
1.65 : 1
1.08 : 1
1.04 : 1
1 : 1.06
1 : 1.10
1 : 1.02
1 : 1.24
1.04 : 1
1 : 1.14
1.07 : 1
1 : 1.00
1.12 : 1
1 : 1.00
1 : 1.00
1 : 1.02
1 : 1.00
1.26 : 1
1 : 1.01
1 : 1.04
1 : 1.09
1 : 1.12
1 : 1.00
1 : 1.01
1 : 1.00
1 : 1.00
1 : 1.00
1 : 1.06
1 : 1.04
1 : 1.10
1 : 1.00
1.01 : 1
1.21 : 1
1 : 1.00

x
1
1
1.68
1.03
1
1
1
1
1
1.16
1
1.05
1
1.17
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.21
1

=
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

2
1.00
1.12
1
1
1.00
1.05
1.15
1.03
1.17
1
1.13
1
1.00
1
1.00
1.02
1.02
1.00
1.01
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.03
1.19
1.02
1.16
1.00
1.00
1
1.00

x = 2.5
1 : 1.00
1 : 1.23
1.69 : 1
1.04 : 1
1 : 1.00
1 : 1.05
1 : 1.05
1 : 1.04
1 : 1.17
1.31 : 1
1 : 1.13
1.05 : 1
1 : 1.00
1.12 : 1
1 : 1.00
1 : 1.02
1 : 1.02
1 : 1.00
1.04 : 1
1 : 1.01
1 : 1.03
1 : 1.17
1 : 1.04
1 : 1.00
1.01 : 1
1 : 1.00
1 : 1.00
1.01 : 1
1 : 1.16
1 : 1.00
1 : 1.06
1 : 1.00
1 : 1.04
1.21 : 1
1 : 1.00

x
1
1
1.64
1.06
1
1
1
1
1
1.31
1
1.06
1.01
1.17
1
1
1
1.04
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.21
1

=
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

3
1.00
1.32
1
1
1.00
1.12
1.03
1.06
1.17
1
1.12
1
1
1
1.00
1.02
1.01
1
1.00
1.02
1.02
1.14
1.06
1.00
1.00
1.01
1.00
1.03
1.13
1.00
1.13
1.00
1.08
1
1.00

Table 3: Median ratio of length of plans found with x = 1, to the length of plans found with C
ce
with different values of x, over instances solved by both planners. An entry m : 1 means that the
baselines plans are m times longer than those of the other planner. No conditional effect merging
was used.

5.3.7 C vs. C
ce
Given a fixed number of -fluents, the difference in size between the C and the C
ce
compilations is exponential in the worst case. Mutex pruning, however, can mitigate much
of the growth of C . Consider, for example, an action in the C
ce compilation that has n
different conditional effects. If mutexes are not considered, one would expect the number of
action representatives generated for the same set of -fluents in C to be 2n . If, however,
each of the n -fluents generating the conditional effects can be shown to be mutex with
one another, the number of action representatives generated in C is also only n.
We have found in our experiments that this effect leads to much slower growth in C
than what might be expected. Consider Figure 2. Each point on the graph represents a
512

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain
Airport (50)
Barman (20)
Blocksworld (35)
Depots (22)
Driverlog (20)
Elevators (20)
Floortile (20)
FreeCell (80)
Grid (5)
Gripper (20)
Logistics00 (28)
Logistics98 (35)
Miconic (150)
Mprime (35)
Mystery (30)
Nomystery (20)
Openstacks (20)
Parcprinter (20)
Parking (20)
Pathways (30)
Pegsol (20)
Pipes-NoTank (50)
Pipes-Tank (50)
PSR (50)
Rovers (40)
Satellite (36)
Scanalyzer (20)
Sokoban (20)
Tidybot (20)
TPP (30)
Transport (20)
Trucks (30)
Visitall (20)
Woodwork (20)
Zenotravel (20)
Total (1126)

x=1
36
13
35
19
20
19
6
79
5
20
28
34
150
35
16
9
20
16
20
30
20
42
38
50
40
35
18
19
16
30
11
14
19
20
20
1002

MaD
+2

+5

+0

+1

+0

+3

+0

+2

+0

+0

+0

+1

+0

+0

+0

+2

+0

+7

+1

+2

+0

+1

+4

+0

+0

+1

+1

+1

+3

+0

+1

+0

+1

+0

+1

+
19

C
ce
37
14
35
22
20
19
20
80
4
20
28
35
150
35
18
9
20
9
12
24
20
42
41
50
40
36
19
18
14
30
11
16
18
20
20
1006

Coverage
C
-nm
C
ce
36
38
19
18
35
35
21
21
20
20
20
19
20
20
80
79
5
5
20
20
28
28
35
35
150
150
35
35
19
19
7
11
20
20
5
10
18
15
29
29
20
20
42
42
41
40
50
50
40
40
36
36
20
20
17
17
14
16
30
30
15
11
15
16
20
18
20
20
20
20
1022
1023

hFF
34
20
35
18
20
19
6
79
5
20
28
33
150
35
16
10
20
20
20
30
20
43
39
50
40
36
18
19
14
30
11
19
3
20
20
1000

hcea
42
0
35
18
20
20
6
79
5
20
28
35
150
35
19
7
19
12
20
28
20
40
32
50
40
36
20
3
16
29
17
15
3
8
20
947

LAMA
31
20
35
21
20
20
5
79
5
20
28
35
150
35
19
13
20
20
20
30
20
44
43
50
40
36
20
19
17
30
19
15
20
20
20
1039

PF
36
20
35
22
20
20
20
79
5
20
28
35
150
35
19
13
20
20
20
30
20
44
44
50
40
36
20
19
17
30
19
16
20
20
20
1062

Table 4: Comparison of state-of-the-art-heuristics for satisficing planning. Columns x = 1 and
MaD are as in Table 1. Column C
ce shows coverage for the best configuration (in terms of overall
coverage) with that compilation when using conditional effect merging (namely x = 1.5, t = 60);
C
C
ce -nm is the best ce configuration without conditional effect merging (which happens to use the
C
same x and t);  is the best C configuration where x > 1 (which again happens to use the same
x and t). Entries in bold in these columns are those where the difference from the baseline planner
exceeds our threshold for significance (given by the MaD column). Column PF shows coverage
for a portfolio planner that runs LAMA for 1500 seconds and C
ce for 300 seconds.
single problem instance (from the same instance set as before), paired with a value of x.
C
For each of C
ce (x-axis) and  (y-axis), we measure the ratio of growth in |A| over growth
in |F |, i. e., the factor by which the compilation increased the size of the action set encoding
(measured by the number of actions in C and by the number of conditional effects in C
ce ),
divided by the factor by which the compilation increased the number of fluents. In other
513

fiKeyder, Hoffmann, & Haslum

1e+08
Action set growth/uent set growth, C

Action set growth/uent set growth, C

4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

Action set growth/uent set growth, Cce

1e+07
1e+06
100000
10000
1000
100
10
1

0

50

100

150

200

250

300

350

Action set growth/uent set growth, Cce

(a)

(b)

Figure 2: Growth in problem size as ratio of growth in |A| to growth in |F |, with (a) and without
(b) mutex pruning. Each point corresponds to a single instance and value of x. f (x) = x is also
shown for reference.

words, we assess the growth of the encoding over the number of conjunctions |C|, which in
theory is worst-case exponential for C but linear for C
ce .
When mutex pruning is not used (Figure 2 (b)), the growth of A in C is rapid and the
ratio quickly increases to millions; with mutex pruning (Figure 2 (a)), the growth in C is
still faster than in C
ce , but the difference is much smaller.
5.3.8 Finding Plans With No Search
When no growth or time limit is imposed on the construction of the C or C
ce tasks,
Algorithm 1 can be used as a complete planning algorithm. While it is not competitive with
heuristic search methods, it is nevertheless interesting to observe some performance details
of this algorithm in various domains. The coverage obtained with this algorithm using
the C and C
ce compilations, as well as some statistics about the growth of the compiled
C
tasks, are shown in Table 5. The difference between C
ce and  is much more visible here,
since the number of -fluents added is, in general, much larger than in the growth-bounded
constructions we have used for heuristic computation. C
ce is able to rapidly add a much
larger number of -fluents, and can therefore find relaxed plans that are solutions to the
C
original planning task as well. Overall, C
ce solves 568 tasks compared to 404 for  , and
solves an equal or greater number of tasks in all except 4 domains.
Considering individual domains, it can be seen that C and C
ce are able to solve all or
almost all of the tasks in certain domains such as Logistics00, Mprime, Mystery, Parcprinter,
PSR, and Woodworking. In some of these domains, even the addition of a small amount of
information is sufficient to obtain relaxed plans that are plans for the original task, and the
maximum x values required to solve all tasks are quite low. This is the case in the Mprime,
Mystery, and Woodworking domains, where the maximum required x values are 4.07, 4.62,
and 1.35, respectively. In others such as Elevators, Openstacks, Transport, and Visitall,
where even the smallest tasks are quite large and many different plans are possible, it is not
possible to introduce enough -fluents to disqualify all of the possible relaxed plans that do
not constitute real plans, and no tasks can be solved.
514

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain
Airport (50)
Barman-sat (20)
Blocksworld (35)
Depots (22)
Driverlog (20)
Elevators-sat11 (20)
Floortile-sat11 (20)
FreeCell (80)
Grid (5)
Gripper (20)
Logistics00 (28)
Logistics98 (35)
Miconic (150)
Mprime (35)
Mystery (30)
Nomystery-sat11 (20)
Openstacks-sat11 (20)
Parcprinter-sat11 (20)
Parking-sat11 (20)
Pathways-noneg (30)
Pegsol-sat11 (20)
Pipes-NoTank (50)
Pipes-Tank (50)
PSR (50)
Rovers (40)
Satellite (36)
Scanalyzer-sat11 (20)
Sokoban-sat11 (20)
Tidybot-sat11 (20)
TPP (30)
Transport-sat11 (20)
Trucks (30)
Visitall-sat11 (20)
Woodwork-sat11 (20)
Zenotravel (20)
Total (1126)

Cov.
34
0
33
16
15
0
4
1
4
16
27
24
106
35
19
5
0
20
0
5
19
10
9
50
21
15
9
0
1
20
0
15
0
20
15
568

Min
1.00
1.21
1.82
1.00
31.47
91.41
1.29
5.37
1.27
1.22
1.20
1.02
1.02
16.72
3.66
1.77
69.36
3.51
2.69
1.03
1.35
4.22
1.90
22.42
1.17
5.54
1.00
1.00

C
ce
Max
44.50
141.79
90.93
37.11
216.18
91.41
22.53
208.75
91.10
41.16
175.11
4.07
4.62
59.80
76.04
51.72
707.06
187.41
94.33
77.38
459.66
43.61
47.75
22.42
43.12
21.36
1.35
92.26

C
Med
2.31
11.44
8.48
8.92
112.50
91.41
1.41
67.69
6.79
31.14
55.01
1.15
1.25
41.14
9.95
7.26
137.98
11.60
13.46
7.29
7.40
19.16
13.93
22.42
13.75
13.56
1.24
7.25

Cov.
31
0
30
14
14
0
9
2
4
6
27
21
36
35
19
3
0
9
0
3
0
5
8
49
14
7
10
1
0
7
0
8
0
20
12
404

Min
1.00
1.21
1.82
1.00
48.92
212.24
1.31
6.77
1.27
1.22
1.20
1.02
1.02
21.91
1.60
2.32
12.61
2.56
1.05
1.70
5.22
2.38
2.89
1.17
6.32
1.00
1.00

Max
168.45
400.94
221.41
74.97
633.11
272.12
41.22
501.28
585.53
149.91
2706.86
13.22
8.34
43.50
276.81
203.41
738.88
110.51
1052.26
645.00
399.10
161.42
2.89
560.95
100.15
1.40
191.54

Med
3.56
15.49
12.45
9.96
107.21
242.18
1.54
76.12
14.11
5.80
54.47
1.14
1.20
33.27
88.23
89.79
242.54
11.71
16.04
26.04
22.09
44.21
2.89
11.27
41.05
1.25
11.14

Table 5: Solving planning tasks with no search. Table shows Coverage for the C and C
ce compilations, and the Minimum, Maximum, and Median values of x for solved tasks.

6. Heuristics for Optimal Planning
We now consider admissible heuristics, for optimal planning. Section 6.1 considers the
LM-cut heuristic, showing that there are certain complications that make it difficult to
obtain improved heuristic estimates from both C and C
ce . In Section 6.2, we consider an
alternative method to lower-bound h+ , namely admissible cost-partitioning heuristics based
on the conjunctive landmarks that can be obtained from C
ce . We detail how to choose C
in that setting, and present our experimental results in Section 6.3.
515

fiKeyder, Hoffmann, & Haslum

a1
i

a2
a3

g2

aC
3

g1

g1
i

a1
a2
a3

g2

aC
2

0

g3

g3
(a)

0

{g1 ,g2 ,g3 }
aC
1

0

(b)

Figure 3: LM-cut in , and in the C compilation, for Example 3.
6.1 LM-cut
The state-of-the-art admissible approximation of h+ is computed by the LM-cut algorithm
(Helmert & Domshlak, 2009). A logical approach to obtaining admissible heuristics from
C and C
ce is therefore to apply LM-cut to these compilations. Unfortunately, it turns out
that there are several serious obstacles to this. Before discussing these issues, we first give
a brief description of the LM-cut algorithm, and then present the simpler case of the C
compilation, where the additional complication of conditional effects is not present.
Before LM-cut can be computed on a planning task with no deletes, a simple transformation is first applied that replaces the goal set G with a single goal that can only be achieved
with a goal-achievement action whose precondition set is G, and adds a dummy precondition to all actions whose precondition set is empty. LM-cut then initializes hLM-cut := 0
and, repeats the following steps until hmax (G) becomes 0: (1) Compute hmax ; (2) apply a
precondition choice function (PCF) to each action precondition pre(a) that removes from
pre(a) all but one of the fluents p  pre(a) for which hmax (p) is maximal; (3) construct the
justification graph whose vertices are the fluents and whose arcs are the precondition/effect
pairs according to the PCF; (4) find a cut L between the initial state and the goal in the
justification graph, given by the set of actions that enters the goal zone, i. e., the set of
fluents from which the goal can be reached at 0 cost; and (5) add costmin := minaL cost(a)
to the heuristic value hLM-cut , and reduce the cost of each a  L by costmin . As proved by
Helmert and Domshlak (2009), this algorithm has two fundamental properties, namely (i)
admissibility, hLM-cut  h+ , and (ii) domination of hmax , hmax  hLM-cut .
While it would be expected that the heuristic obtained in this manner from C would
be strictly more informative than that obtained from the original planning task , this
turns out not to be the case. Indeed, the heuristic can become strictly less informative:
Example 3 Let  = hF, A, I, Gi be given by F = {g1 , g2 , g3 }, A = {a1 , a2 , a3 }, where
ai = h, {gi }, , i, I = , and G = {g1 , g2 , g3 } (Figure 3). In words,  has three goals, each
of which is achievable by a single action. Valid plans for  apply each action once in any
order, to make all of the goals true. The cuts found by the LM-cut algorithm for this task are
{a1 }, {a2 }, and {a3 }, regardless of the PCFs chosen, and the LM-cut algorithm therefore
always computes the optimal cost 3. Consider now the C compilation that results from the
set C = {{g1 , g2 , g3 }}. F C then contains the single -fluent {g1 ,g2 ,g3 } , and a representative
0
0
of each action aC
i constructed with the sole non-empty subset C = {{g1 , g2 , g3 }} of C. The
first cut found by LM-cut then contains these three representatives, as each adds the most
expensive goal {g1 ,g2 ,g3 } . For any of the possible PCFs, the next cut is then the last, and
the final heuristic estimate is only 2 as only two cuts have been found. If, for example, the
516

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

0

0

C
precondition choice function chooses g1 as the hmax justifier for aC
2 and a3 , and g2 as the
0
max
C
h
justifier for a1 , the cut is {a1 , a2 }. After that cut, the goal can be reached at 0 cost
{{g ,g ,g }}
via a1 , a2 , a3 1 2 3 , so hmax is 0 and LM-cut stops.

+
C
Note that (similarly to h+ (C
ce ), cf. Theorem 2) it is not possible for either h ( ) or
hmax (C ) to decrease with the addition of -fluents, and that in this example the hmax
cost of the task actually increases (from 1 to 2) with the addition of the -fluent {g1 ,g2 ,g3 } .
However, the type of interactions that are introduced are difficult for the LM-cut algorithm
to reason about, resulting in worse admissible bounds in practice. LM-cut of course continues to dominate hmax , proving that if a sufficient number of -fluents are added, LM-cut
will eventually tend towards the optimal cost of the task.

The weakness pointed out by Example 3 is inherited in the application of the LM-cut
algorithm to the C
ce compilation. Furthermore, that application involves an additional complication that proves formidable: LM-cut is not defined for conditional effects, and therefore
cannot be directly applied to the C
ce task. It turns out that of the two straightforward
adaptations of the algorithm to problems with conditional effects, neither preserves both
properties (i) admissibility and (ii) domination of hmax .
To see why (ii) is at stake, consider a planning task  with a single action a that
has two conditional effects ce(a)1 = h{p}, {q}, i and ce(a)2 = h{q}, {r}, i, initial state
{p}, and goal {r}. We have h+ () = hmax () = 2 due to the critical path ha, ai, and the
justification graph considered by LM-cut consists of this same sequence. The first cut found
is {a}. When the cost of a is reduced, the remaining task has hmax cost 0, resulting in the
cost estimate hLM-cut = 1.
The issue here is that different conditional effects of an action may be part of the same
critical path. A natural approach is therefore to reduce costs per individual conditional
effect, rather than for all of the effects of the action at once. Unfortunately, it turns out that
this does not preserve admissibility (i). Indeed, as we detail in Example 4 (Appendix A),
there exist STRIPS tasks  whose C
ce compilations have the following property: there
exists an action a such that reducing its cost globally when it is first encountered in a cut
leads to a heuristic estimate that is less than hmax (C
ce ), while treating each of its effects
+
C
separately leads to an estimate greater than h (ce ) = h ().
There is therefore no simple strategy for dealing with conditional effects that preserves
both (i) and (ii) on all planning tasks. Since admissibility cannot be sacrificed, we must
reduce costs globally and give up on dominating hmax . As a particular implication of doing
1
C

so, despite Theorem 5 which shows that hmax (C
ce ) = h (ce ) converges to h (), such
convergence is not guaranteed for hLM-cut (C
ce ). This could of course be fixed by using
max(hmax , hLM-cut ) as the heuristic value, yet as hmax is typically not informative, this
strategy is not useful in practice.
As we detail in Section 6.3 below, on the IPC benchmarks, using A with LM-cut computed on either C or C
ce often results in larger search spaces as more -fluents are introduced. In all but a few cases, overall performance is worse with hLM-cut (C ) or hLM-cut (C
ce )
than with hLM-cut (). It remains an open question whether this can be improved.
517

fiKeyder, Hoffmann, & Haslum

6.2 C
ce Landmarks
Landmarks in planning tasks are formulas  over the set of fluents F that have the property that they are made true in some state during the execution of any valid plan. As
the problem of checking whether even a single fluent is a landmark for a planning task
is PSPACE-complete, approaches to finding landmarks have in the past focused on the
delete relaxation, in which setting whether a fluent is a landmark or not can be checked
in polynomial time. It has recently been shown that the maximum fixpoint solution to a
set of simple recursive equations defines the complete set of single fact delete-relaxation
landmarks, in other words those landmark formulas that consist of only a single literal
 = p (Keyder et al., 2010). This solution can be computed by an algorithm that repeatedly updates the set of such landmarks for each fluent and action in the planning task,
until convergence. This method can naturally handle conditional effects by treating them
as independent actions, as described in Section 5.1.
It has also been shown that these equations can be applied to any AND/OR graph
structure, not necessarily corresponding to the delete relaxation of a planning task. This
insight has been used to obtain landmarks from the m task. Single -fluent landmarks
in m correspond to conjunctive landmarks in  that are not necessarily landmarks of the
delete relaxation + . This approach suffers, however, from the large number of -fluents
that are considered in m , rendering landmark generation impractical for the m compilations of larger tasks. Here we aim to take advantage of the flexibility of the C
ce compilation
to obtain non-delete-relaxation landmarks for the original task, while considering only a
focused set of -fluents and not all those of a given size m. As before, this allow us to
consider larger conjunctions while keeping the size of the delete relaxation task low.
When using C
ce for landmark finding, to focus the technique and keep its overhead at
bay, we choose the set of conjunctions C so as to guarantee that every -fluent is a landmark
in C
ce (and therefore in the original planning task). This is accomplished by extracting from
the landmark graph sets of landmarks that are simultaneously achieved :
Definition 7 (Simultaneously achieved landmarks) A set of landmarks Ls = {1 ,
. . . , n } is simultaneously achieved if Lc = 1  ...  n is a landmark in .
Maximal sets of simultanously achieved landmarks can easily be extracted from a set of
landmarks and orderings. Given an initial set of landmarks L and a set of orderings, the
following are sets of sets of simultaneously achieved landmarks:
 LG = {{ | G |= }}
 Lnec = {{ |  nec } |   L}
 Lgn = {{ |  gn } |   L}
LG contains a single set that is made up of the landmarks in L that are entailed by G. Since
any valid plan must make all goals true in its final state, they are necessarily simultaneously
achieved. Given a landmark , Lnec contains a set which has as its elements all those landmarks  that are ordered necessarily before . Due to the definition of necessary orderings,
all of these  must be simultaneously true in every state that immediately precedes a state
in which  becomes true. Lgn is a similar set, yet since greedy necessary orderings are
518

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

weaker than necessary orderings, can sometimes contain sets that do not appear in Lnec ,
and therefore result in a larger overall set of conjunctive landmarks. Note that all necessary orderings are also greedy necessary orderings, and each conjunctive landmark that
results from a set of necessary orderings is therefore a subset of some conjunctive landmark
that results from greedy necessary orderings. We include conjunctive landmarks that result
from necessary orderings as they result in stronger necessary orderings between the added
conjunctive landmark and . Landmark heuristics can then sometimes infer that these
conjunctive landmarks must be reachieved if a landmark that they are ordered necessarily
before has to be reachieved. This is not the case for the conjunctive landmarks derived
from greedy-necessary orderings, as they only need to be achieved to make the landmarks
they are ordered before true for the first time.
Algorithm 2: Choosing C for landmark generation.
C=
L = FindLandmarks(C
ce )
repeat
C = C  SimultaneouslyAchieved(L)
L = FindLandmarks(C
ce )
until SimultaneouslyAchieved(L)  C

Our strategy for choosing C for landmark generation is shown in Algorithm 2. While
new conjunctive landmarks L = p1      pn can be discovered, the corresponding fluents
{p1 ,...,pn } are added to C
ce and the landmark computation step is repeated. Note that the
process may go through several iterations, and is run until a fixpoint is reached, as the
addition of the new -fluents to the C
ce task can result in the discovery of new landmarks.
The process terminates when all the new conjunctive landmarks that are discovered already
exist as -fluents in C
ce . We note that this method of choosing C does have the desired
C
property mentioned above: all -fluents introduced in C
ce represent fact landmarks in ce
and conjunctive landmarks in the original task .
The above strategy works especially well in domains in which many landmarks have several landmarks that are necessarily or greedy necessarily ordered before them. One domain
where this occurs is in Blocksworld (see an illustration in Figure 4), where the method is
able to find extremely informative conjunctive landmarks that allow it to optimally solve
more tasks than any other heuristic we tested.
6.3 Experiments
We consider the performance of the LM-cut heuristic hLM-cut on the C and C
ce compilations, and that of the admissible landmark cost-partitioning heuristic hLM introduced by
Karpas and Domshlak (2009) with different landmark generation schemes, including the
LM-cut is used with the A search algorithm, while for hLM
landmarks obtained from C
ce . h

we use LM-A , a variant which is more effective when there are known fluent landmarks
(Karpas & Domshlak, 2009). The benchmarks, computers, and time/memory limits are the
same as those used in in Section 5.3.
519

fiKeyder, Hoffmann, & Haslum

Informativeness
Coverage
Domain
C , 1.5
C
,
1.5
Orig.
x
=
1
C , 1.5 C
ce
ce , 1.5
Airport
1 : 49.02
1 : 52.91
28
28
19
18
Barman-opt
1 : 1.06
1 : 1.12
4
4
4
4
Blocksworld
4.22 : 1
1 : 1.71
28
28
28
27
Depots
1 : 1.96
1 : 6.49
7
5
4
4
Driverlog
1 : 23.7
1 : 48.31
13
13
10
10
Elevators-opt11
1.65 : 1
1 : 1.03
18
18
16
15
Floortile-opt11
19.23 : 1
13.93 : 1
7
6
12
12
FreeCell
1.07 : 1
1 : 2.12
15
15
13
9
Grid
3.47 : 1
1 : 1.35
2
2
2
1
Gripper
1:1
1:1
7
7
6
6
Logistics00
1 : 9.38
1 : 10.47
20
20
16
15
Logistics98
1 : 7.69
1 : 18.87
6
6
2
3
Miconic
1 : 232.55
1 : 769.23 141 141
50
45
Mprime
13.08 : 1
1 : 1.13
22
22
28
22
Mystery
1.03 : 1
1.07 : 1
16
16
17
17
Nomystery-opt11
1 : 126.58
1 : 588.24
14
14
8
8
Openstacks-opt11
1:1
1:1
14
14
14
14
Parcprinter-opt11
1 : 1.14
1 : 4.23
13
13
13
12
Parking-opt11
2
1
1
0
Pathways-noneg
1 : 15.72
1 : 51.81
5
5
4
4
Pegsol-opt11
1.08 : 1
1.08 : 1
17
17
17
17
Pipes-NoTank
1 : 1.48
1 : 2.09
17
17
15
14
Pipes-Tank
1 : 1.30
1 : 2.25
11
10
8
7
PSR
1.04 : 1
1 : 1.03
49
49
49
49
Rovers
1 : 1.72
1 : 3.56
7
7
7
6
Satellite
1 : 3.77
1 : 33.33
7
7
6
6
Scanalyzer-opt11
1 : 1.36
1 : 1.06
11
11
4
5
Sokoban-opt11
1 : 1.28
1 : 1.31
20
20
20
20
Tidybot-opt11
1 : 4.79
1 : 13.19
13
13
7
6
TPP
1 : 5.14
1 : 1.70
6
6
6
6
Transport-opt11
1 : 1.87
1.35 : 1
6
6
6
7
Trucks
1 : 5.94
1 : 10.26
10
10
7
6
Visitall-opt11
1 : 3.86
1 : 3.32
10
10
10
10
Woodwork-opt11
4.07 : 1
1 : 2.36
11
11
7
5
Zenotravel
1 : 39.37
1 : 153.85
12
12
8
8
Total
589 584
444
418
C
Table 6: LM-cut with C
ce and  . The two columns on the left show the ratio of the summed

number of heuristic evaluations for tasks solved by both configurations, comparing the standard
LM-cut that results from x = 1 to LM-cut computed on C and C
ce with x = 1.5. For example,
the first entry in the table, 1 : 49.02, shows that LM-cut computed on C with a growth bound of
x = 1.5 evaluates, in sum over the commonly solved tasks, nearly 50 times as many states as LM-cut
computed on the standard delete relaxation. The last 4 columns show coverage. Column Original
shows results obtained with Fast Downwards implementation of LM-cut (which applies only to the
standard delete relaxation), while column x = 1 shows the results of our implementation of LMcut on the unmodified delete relaxation (with any differences between the two being purely due to
implementation details). Entries in bold indicate the highest coverage in each domain, and in total.

520

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

clear(a)
gn
clear(b)
handempty
ontable(b)

clear(a)
holding(b)

nat

nat

clear(b)
handempty
on(b, a)
clear(c)
handempty
ontable(c)

clear(b)
clear(d)
gn
on(b, a)
holding(c)
nat ontable(d)
gn

on(b, a)
on(c, b)
on(d, c)

nec

clear(c)
clear(c)
clear(d)
on(b, a) gn handempty
on(c, b)
on(b, a)
holding(d)
on(c, b)
ontable(d)

Figure 4: Landmarks graph found with the C
ce compilation for a small Blocksworld task, in which
all blocks are initially on the table and G = {on(b, a), on(c, b), on(d, c)}. Some smaller conjunctive
landmarks and single fluent landmarks are omitted.

6.3.1 LM-cut with C and C
ce
To evaluate the impact of using the C and C
ce compilations on LM-cut, we constructed
C
C
the  and ce tasks following the same procedure as described in Section 5.3, repeatedly
selecting conflicts until the increase in the size of the compiled task reached a fixed growth
bound x. Conflict selection was based on hmax supporters rather than hadd supporters, as
hmax plays a key role in the computation of LM-cut, and also resulted in better performance.
Other than that, the procedure used to generate the C and C
ce tasks was the same. We
C
tested each value of x from the set {1.5, 2, 2.5, 3} for both  and C
ce . We observed that
x = 1.5 dominated the larger values of x on a domain-by-domain and overall basis, and
therefore report results only for these two configurations. The only exception to this is the
Mystery domain, in which C with x = 2.5 and x = 3 solved 18 tasks compared to 17 for
x = 1.5.
Overall, the heuristic computed by the LM-cut algorithm on the standard delete relaxation + dominates that computed on C and C
ce , both in terms of informativeness and
in terms of coverage. The first two columns of Table 6 show that for the large majority
of domains, search using LM-cut computed on C and C
ce performs many more heuristic
evaluations in tasks that are solved by both configurations. In the Airport domain, for
instance, LM-cut on the standard delete relaxation requires approximately 50 times fewer
heuristic evaluations to solve the same set of tasks as either C or C
ce . In most other domains, the situation is less extreme, but the standard delete relaxation continues to give the
better heuristic estimates. The exceptions to this are the Blocksworld, Elevators, Floortile,
FreeCell, Grid, Mprime, Mystery, Pegsol, PSR, Transport and Woodworking domains, in
+
which at least one of C or C
ce yields more informative heuristic estimates than  . Most
impressively, C and C
ce give estimates that are respectively 19 and 14 times more informative than the estimates obtained from + in the Floortile domain, and estimates using
C are 13 times more informative than those of + on the Mprime domain. In terms of
coverage, this translates into 12 tasks solved with both C and C
ce in the Floortile domain,
compared to 7 for the standard version of LM-cut, and 28 tasks solved with C in the
521

fiKeyder, Hoffmann, & Haslum

Mprime domain, compared to 22. In the Mystery domain, coverage is increased by 1. In all
other domains, the coverage achieved with C and C
ce -based LM-cut is less than or equal
to the coverage achieved with standard LM-cut. Overall, the standard version of LM-cut
solves 589 planning tasks as compared to 445 for C and 418 for C
ce . Though a large part
of this difference (90 tasks) comes from the Miconic domain, the difference in the remaining
domains is still significant.
Comparing C and C
ce , it can be seen that the additional loss of information resulting
from the treatment of conditional effects in LM-cut leads to worse heuristic estimates when
C
using C
ce . As expected from the theoretical result that ce grows only linearly with the
number of -fluents, the number of -fluents that are added to the task when using the
C
C
ce compilation is almost always higher than when using  . However the treatment of
conditional effects in LM-cut (described above) turns out to greatly degrade performance,
C
and LM-cut using C
ce is more informative than LM-cut using  in only 4 domains.
6.3.2 Admissible Landmark Heuristics with C
ce Landmarks
The admissible landmark heuristic, hLM , uses action cost partitioning to derive heuristic
values from a collection of (ordered) landmarks, distributing the cost of each action over the
set of landmarks it achieves (Karpas & Domshlak, 2009). Cost partitioning can be done in
different ways: optimal cost partitioning is tractable, and yields the best possible heuristic
value for a given set of landmarks, but in practice is so slow that coverage suffers; uniform
partitioning generally achieves a better time/informativeness trade-off, and therefore better
coverage.
To evaluate the potential informativeness of the landmarks obtained with C
ce using the
iterative technique described in Section 6.2, we used these landmarks in the optimal cost
partitioning setting, since this setting makes the best possible use of information present
in the given landmarks. We compared their informativeness to that of the heuristic using
landmarks obtained from the m compilation with m = 1 and m = 2 and the sound and
complete landmark generation algorithm (Keyder et al., 2010). The results are shown in the
first two columns of Table 7. They show the ratio of total number of heuristic evaluations,
per domain, over all tasks solved by all configurations, for hLM using landmarks from 1 only
to the heuristic using landmarks from 2 and C
ce , respectively. Note that the landmarks
2 compilations contain the landmarks obtained from 1
generated from both the C
and

ce
as a subset, and that hLM with optimal partitioning over the 2 or C
ce landmarks therefore
dominates hLM with optimal partitioning over 1 landmarks. Hence the ratio is always
greater than 1.
In 9 of the 35 domains considered, neither the addition of 2 landmarks nor C
ce landmarks leads to a more informative heuristic (cases in which both columns show the value 1).
Of the remaining 26 domains, both schemes improve over 1 landmarks to an equal degree
in 7, and 2 improves over 1 to a greater degree than C
ce in 17. In one case, Blocksworld,
C
landmarks
are
much
more
informative
than
landmarks
found by both the other methce
ods, and improve informativeness over the baseline heuristic using only 1 landmarks by a
factor of 122.
Uniform cost partitioning divides the cost of each action evenly over the set of landmarks
that it achieves, rather than searching for a partitioning that maximizes the heuristic value
522

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain

Airport
Barman-opt
Blocksworld
Depots
Driverlog
Elevators-opt11
Floortile-opt11
FreeCell
Grid
Gripper
Logistics00
Logistics98
Miconic
Mprime
Mystery
Nomystery-opt11
Openstacks-opt11
Parcprinter-opt11
Parking-opt11
Pathways-noneg
Pegsol-opt11
Pipes-NoTank
Pipes-Tank
PSR
Rovers
Satellite
Scanalyzer-opt11
Sokoban-opt11
Tidybot-opt11
TPP
Transport-opt11
Trucks
Visitall-opt11
Woodwork-opt11
Zenotravel
Total

Informativeness
Coverage
(optimal partitioning) (uniform partitioning)
2
C
1
2
C
ce
ce
1.03
1.03
27
11
27
4
4
4
25.65
122.31
26
28
32
4.28
1.11
7
7
7
1.02
1.01
10
9
9
1.54
1.54
12
12
12
3.28
1.02
2
2
2
1.32
1.01
60
38
39
1.12
1.12
2
2
2
1
1
7
6
7
13.65
13.65
20
22
22
1.25
1.25
3
3
3
3.91
3.91
142
142
143
1.39
1
20
20
20
2.17
1
15
15
15
3.08
1.69
20
20
18
1
1
12
7
11
4.09
1
10
8
10
9.84
1
3
0
0
1
1
4
4
4
1.25
1
17
15
17
1.37
1.27
16
16
16
1.64
1.13
13
10
11
2.68
1.23
49
49
49
1
1
6
6
5
1.06
1.06
6
6
6
1.54
1.01
6
3
6
1.02
1
20
14
18
1.18
1.05
14
9
14
1
1
6
6
6
1
1
6
6
6
1
1
8
6
7
1
1
16
9
9
3.04
1.16
7
4
5
1
1
8
8
8
604
527
570

Table 7: hLM with landmarks generated from the delete relaxation (1 ), 2 (Keyder et al., 2010)
and C
ce . The two columns on the left show the ratio of the summed number of heuristic evaluations
for tasks solved by both configurations, comparing 2 and C
ce to the baseline of using only landmarks
from 1 . Using optimal cost partitioning in hLM , more landmarks can only yield better lower bounds,
and indeed all the ratios are  1 (which is why we do not use a m : 1 presentation, differently from
the previous tables). The right-most three columns show coverage, using uniform cost partitioning.
The heuristic with uniform partitioning solves more tasks than with optimal cost partitioning under
all the landmark generation schemes considered. Entries in bold indicate best results, per domain
and in total.

523

fiKeyder, Hoffmann, & Haslum

in each state. This can make the hLM heuristic weaker, though typically not by much, but
also makes it much faster to compute, leading to better coverage in general. We confirmed
that uniform cost partitioning results in higher coverage than optimal cost partitioning in
all domains for all three of the landmark generation schemes we considered.
The three right-most columns in Table 7 show the coverage achieved with hLM and the
three landmark generation schemes in this setting. It can be seen that using only 1 landmarks results in greater coverage than combining them with either 2 or C
ce landmarks.
2
C
Compared to the heuristic using  landmarks, using ce landmarks solve as many or more
tasks in every domain except the Nomystery and Rovers domains. C
ce landmarks outperform 1 landmarks in two domains: Blocksworld, where using C
landmarks
the planner
ce
finds optimal solutions to 32 out of the 35 tasks, more than any other tested heuristic, and
in Miconic, by 1 instance. In the other domains, the use of C
ce landmarks either has no
1
effect or worsens coverage compared to  . Interestingly, while the informativeness of the
LM-cut heuristic increases greatly with the C and C
ce compilations in the Floortile domain, there is no corresponding increase when the compilations are used to find landmarks.
This is because few conjunctive landmarks, besides the goal, are found.

7. Conclusions and Open Questions
There is a long tradition of works attempting to devise heuristics taking into account some
delete effects. However, techniques rendering h+ perfect in the limit  and thus allowing
to smoothly interpolate between h+ and h  have been proposed only quite recently, by
Haslum (2012) and Katz et al. (2013) respectively. We have extended Haslums approach
by introducing a new compilation method with linear (vs. worst-case exponential) growth,
and demonstrated the machinery needed for using the approach to generate heuristics. Our
evaluation shows that, in some domains, informedness can be dramatically improved at a
small cost in terms of computational overhead.
The main open issue lies in our use of the words in some domains here. In most
domains, the gain in informativeness is small, and in some domains overall performance
suffers dramatically. While no domain-independent planning technique can work well in
every domain, and while a simple portfolio approach (cf. column PF in Table 4) suffices
to improve the state of the art in satisficing planning, the extent of the per-domain performance variation of our technique is dramatic. Can we obtain an understanding of what
causes these phenomena, and ultimately exploit that understanding to devise more reliable/effective practical methods? Is the unchanged or worse performance in many domains
due to fundamental limitations of the technique, or only due to its particular instantiation
(especially the selection of -fluents) as run in our experiments?
From a practical perspective, answering these questions comes down to the exploration
of techniques for predicting the impact of adding -fluents, and for making more informed
decisions about which -fluents to add. We have observed that changes in domain formulation, random reorderings, and small changes to the heuristic criteria used in -fluent
selection can have a large impact on both heuristic informativeness and coverage. Further research to formulate new heuristic criteria and improve existing ones therefore could,
potentially, provide better performance across a wide range of domains. It might also be in524

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

teresting to systematically explore the impact of random/arbitrary changes, and to attempt
building complementary-strength compilations to be combined into effective portfolios.
From a theoretical perspective, we are currently approaching the above questions in
terms of analyzing the conditions under which a small (polynomial-size) set of -fluents
suffices to render h+ perfect. Applied to individual domains, this analysis offers a way
of answering the question of whether a lack of performance improvement is due to an
essential limitation or only due to choosing the wrong set of -fluents. Our hope is
to eventually obtain syntactic criteria (e. g., based on causal graph structure) that can be
automatically applied to arbitrary planning task descriptions, serving to select the -fluents
(or to exclude subsets of -fluents from consideration) in a targeted manner. Our first
results in that direction have already been published in HSDIP14 (Hoffmann, Steinmetz,
& Haslum, 2014).
Our observations in optimal planning pose many questions for future work. A simple one
is whether more effective C
ce landmarks could be extracted by not restricting our techniques
to adding only -fluents guaranteed to be landmarks. The more daunting challenges regard
LM-cut. Our observations suggest that the methods we use suffer greatly from suboptimal
choices of precondition choice functions (PCFs). It would therefore be worthwhile to investigate new methods for obtaining better PCFs. Another important direction is to develop
extensions of LM-cut to conditional effects that guarantee both admissibility and domination of hmax . A simple yet impractical method is to multiply out the conditional effects
(enumerating all subsets thereof). A more sophisticated method based on context splitting, where distinctions between different occurences of the same action are introduced in
a targeted manner only where necessary, has recently been proposed (Roger, Pommerening,
& Helmert, 2014).
In summary, explicitly represented conjunctions clearly exhibit the potential to dramatically improve delete relaxation heuristics. But much remains to be done in order to
understand and use them effectively.

Acknowledgments
Part of the work leading to this publication was carried out while Emil Keyder and Jorg
Hoffmann were working at INRIA Grand Est, Nancy, France. NICTA is funded by the
Australian Government through the Department of Communications and the Australian
Research Council through the ICT Centre of Excellence Program. We thank the University
of Freiburg for allowing us to use their computional resources.

Appendix A. Proofs
Theorem 3 (h+ (C ) dominates h+ (C
ce )) Given a planning task  and a set of con+
C
+
C
junctions C, h ( )  h (ce ). There are cases in which the inequality is strict.
Proof: This follows from the fact that any plan for C is also a plan for C
ce , yet the
Cn i be a plan for C .
1
inverse is not the case. To show the first part, let  = haC
,
.
.
.
,
a
n
1
We show that the same sequence of actions constitutes a plan for C
ce , by showing by
Cn ]  I C [aC , . . . , aC ] , where I[. . . ]
1
induction that I C [aC
,
.
.
.
,
a
denotes
the result of
ce
n
n ce
1
1
525

fiKeyder, Hoffmann, & Haslum

applying a sequence of actions to the initial state I C in C
ce . Since the goal in both tasks
is defined to be GC , this shows the desired result. For the base case, the initial state in
C
both C and C
ce is I , and the subset relation holds. For the inductive case, assume
Ci1
C1
C
C
C
C
C
I [a1 , . . . , ai1 ]  I [aC
1 , . . . , ai1 ]ce . Since the precondition of ai in ce is a subset of
C
C
C C
C
i
the precondition of aC
i in  for all a and all Ci , ai can be applied in I [a1 , . . . , ai1 ]ce by
C
i
the induction hypothesis. We then need to show that all fluents added by aC
i in  are also
Ci
C
C C
C
C
added by aC
i in ce when applied in I [a1 , . . . , ai1 ]ce . The add effect of ai in  consists
C
of the union of two sets, (add(a)  (pre(a) \ del(a))) , which is also the add effect of aC
i in
Ci
C
0
C
0
ce and therefore added, and {c | c  Ci }. Since ai was applicable in  , each of its
S
Ci1
1
preconditions (pre(a) c0 Ci (c0 \add(a)))C must be true in I C [aC
1 , . . . , ai1 ], and therefore
C
0
in I C [aC
1 , . . . , ai1 ]ce , by the induction hypothesis. For c  C (and therefore c  Ci , as Ci 
C
C
C), aC
i in ce has a conditional effect with effect c and condition (pre(a)  (c \ add(a))) ,
Ci
C
which applies because its condition is a subset of the precondition of ai in  . This shows
the desired property.
For strictness, consider the planning task with fluent set F = {p1 , p2 , r, g1 , g2 }, initial
state I = {p1 }, goal G = {g1 , g2 }, and actions
ap2 : h{p1 }, {p2 }, {r, p1 }, i ar : h, {r}, , i
ag1 : h{p1 , r}, {g1 }, , i ag2 : h{p2 , r}, {g2 }, , i
Let C = {c  F | |c| = 2}. The only optimal plan for both  and C is the sequence
har , ag1 , ap2 , ar , ag2 i. In the case of C this follows from the fact that the plan must include
ag1 and ag2 as they are the only actions achieving the two goals, and therefore must achieve
their precondition -fluents {p1 ,r} and {p2 ,r} , respectively. Each of these -fluents can
be achieved only with ar , as no action achieves either of the p fluents without deleting
r. There is no single representative of ar that achieves both {p1 ,r} and {p2 ,r} , as such a
representative would have the precondition {p1 ,p2 } , which is unreachable, since the only
action achieving p2 deletes p1 . A plan in C therefore must contain ag1 , ag2 , at least two
instances of ar , and ap2 .
This no longer holds, however, when considering C
ce , for which the action sequence
hap2 , ar , ag1 , ag2 i is a plan that contains only 4 actions. In C
ce , the two possible -fluents
added by ar , {p1 ,r} and {p2 ,r} , are treated independently, and a separate conditional effect
is created for each, with the conditions p1 and p2 respectively. Once p1 and p2 have been
achieved separately, a single application of the action ar is then sufficient to achieve the
two -fluents, without making true the (unreachable) cross-context -fluent {p1 ,p2 } . In this
and similar cases, there exist plans for C
ce that are shorter than the minimum length plans
for C .
Given a STRIPS task  = hF, A, I, G, costi, the h1 heuristic for a set P of fluents, is
defined as follows (Bonet & Geffner, 2001):

0
if p  s
1
h (p) =
min{a|padd(a)} h1 (pre(a)) + cost(a) otherwise
h1 (P ) = max h1 (p)
pP

526

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

The value of the heuristic for a given planning task is taken to be the h1 cost of the goal
G, h1 () = h1 (G).
Lemma 1 Given a planning task  and C = {c  P(F ) | 1 < |c|  m}, h1 (C ) = h1 (m ).
Proof: Let  = hF, A, I, Gi. m and C are identical except for the action sets. We
denote the action set of m by AC (m ) and that of C by AC (C ). There are no deletes
and conditional effects in either of AC (m ) and AC (C ), so we will ignore these in what
follows.
We first show that h1 (C )  h1 (m ), then that h1 (m )  h1 (C ). Each direction
is based on the following two observations. First, for any STRIPS planning task, we can
split up the actions over their singleton add effects, without affecting h1 . Precisely, given
an action a and p  add(a) \ pre(a), we denote by a[p] the action where pre(a[p]) = pre(a)
and add(a[p]) = {p}. Replacing each a with all its split-up actions a[p] (i. e. generating a
split-up action a[p] for every non-redundant add effect of a), h1 remains the same. Second,
say that every split-up action a[p] in action set A is dominated by an action a0 in action set
A0 , i. e., pre(a0 )  pre(a[p]) and add(a0 )  add(a[p]). Then h1 using A0 is a lower bound on
h1 using A.
We now prove that h1 (C )  h1 (m ). For every a  A and c  F so that 1 < |c|  m,
del(a)  c = , and add(a)  c 6= , AC (m ) contains the action ac given by pre(ac ) =
(pre(a)  (c \ add(a)))C , and add(ac ) = add(a)  {c0 | c0  C  c0  (add(a)  c)}. Let
0
p  add(ac ). If p  add(a), then aC for C 0 =  dominates ac [p]. Say p = c0 where
c0 6 pre(ac ). To obtain a dominating action in AC (C ), we define:
C 0 := {c00  C | del(a)  c00 = , add(a)  c00 6= , c00  c0 }
We have C 0  C, and for all c00  C 0 the conditions (1) del(a)  c00 =   add(a)  c00 6=  and
(2) c  C : ((c  c00  add(a)  c 6= ) = c  C 0 ) of Definition 2 are
S obviously satisfied.
0
0
Thus AC (C ) contains the action aC given by pre(aC ) = (pre(a)  c00 C 0 (c00 \ add(a)))C
0
and add(aC ) = (add(a)  (pre(a) \ del(a)))C  {c00 | c00  C 0 }. We now prove that (a)
0
0
pre(aC )  pre(ac ) and (b) p = c0  add(aC ).
Regarding (a), for every c00  C 0 we have
S
c00 \ add(a)  c0 \ add(a)  c \ add(a), and thus c00 C 0 (c00 \ add(a))  c0 \ add(a)  c \ add(a).
Regarding (b), we need to prove that c0  C, del(a)c0 = , add(a)c0 6= , and c0  c0 . The
first and the last of these properties are obvious, the second one is direct from construction.
As for the third one, add(a)  c0 6= , this is true because otherwise we would have c0 
c \ add(a) implying in contradiction to construction that c0  pre(ac ).
It remains to prove that h1 (m )  h1 (C ). For every a  A and C 0  C with conditions
0
0
(1) and (2) as stated above, AC (C ) contains the action aC . Let p  add(aC ). If p is not
a -fluent, then either p  add(a) or p  pre(a) \ del(a). The latter case is irrelevant (and
0
no split-up action is generated); in the former case, setting c := add(a) we get that aC [p]
is dominated by ac in AC (m ). Say p = c . Then at least one of the following cases
must hold: (a) c  C 0 or (b) c  (pre(a) \ del(a)) or (c) c  (add(a)  (pre(a) \ del(a)))
and c  add(a) 6= . In case (a), it follows directly by definition that ac  AC (m ) which
0
0
dominates aC [p]. In case (b), p = c  pre(aC ) so that case is irrelevant. In case (c),
0
c  add(a) 6=  and c  del(a) =  so again ac  AC (m ) which dominates aC [p]. This
concludes the proof.
527

fiKeyder, Hoffmann, & Haslum

Lemma 2 Given a planning task  and a set of non-unit conjunctions C, h1 (C ) 
h+ (C
ce ).
C
Proof: Consider the planning task C
no-cc that is identical to  except that it does not
include cross-context preconditions. That is, the precondition of each action representative
0
aC is modified to be the following:

0

pre(aC ) = pre(a)C 

[

(pre(a)  (c0 \ add(a)))C

c0 C 0

1
C
+
C
We show that (A) h1 (C )  h1 (C
no-cc ), and (B) h (no-cc )  h (ce ).

We first prove (A). As in the proof of Lemma 1, it suffices to prove that, for every split0
C
up action aC [p] of C
no-cc , there exists a dominating action in  . If p is not a -fluent,
00
0
then aC in C for C 00 =  dominates aC [p]. Otherwise, say p = c0 . Then at least one of
the following cases must hold: (a) c0  C 0 or (b) c0  (add(a)  (pre(a) \ del(a))). In case (b),
00
0
again aC in C for C 00 =  dominates aC [p]. In case (a), to obtain a dominating action
00
aC in C , we define
C 00 := {c00  C | del(a)  c00 = , add(a)  c00 6= , c00  c0 }
All c00  C 00 satisfy the conditions (1) del(a)  c00 =   add(a)  c00 6=  and (2) c  C :
00
((c  c00  add(a)  c 6= ) = c  C 0 ) of Definition 2, so indeed aC is an action in C .
00
00
We obviously have c0  C 00 and thus p  add(aC ). It remains to prove that pre(aC ) 
0
pre(aC [p]). This is so, intuitively, because C 00 corresponds to the single conjunction c0 (plus
subsumed conjunctions) and hence no cross-context fluents arise.S Specifically, for every
00
c00  C 00 we have c00 \add(a)  c0 \add(a). Thus pre(aC ) = (pre(a) c00 C 00 (c00 \add(a)))C =
0
(pre(a)  (c0 \ add(a)))C . The latter is obviously contained in pre(aC [p]), concluding the
proof of (A).
+
C
+
C
It remains to prove (B). Since h1 (C
no-cc )  h (no-cc ), it suffices to prove that h (no-cc )
+
C
+
C
h (ce ). Consider a state s, and a relaxed plan ce for s in ce . For each action ace in
+ , representing action a of the original task , let C 0 be the set of conjunctions c whose
ce
+ in C . C 0 obviously qualifies for
-fluents are added by ace during the execution of ce
ce

constraint (1) in Definition 2; it qualifies for constraint (2) because any conditional effect
for c with that property will be triggered by ace if the conditional effect for a suitable c0 is
C 0 of a. Define the action sequence  +
triggered. Thus C
no-cc includes the representative a
C 0 in  + adds the same fluents as a ,
C0
in C
ce
no-cc to be the sequence of these a . Obviously, a
and its precondition is the union of that of ace and of its conditional effects that fire. Thus
+
C
+
C
 + is a relaxed plan for C
no-cc . It follows that h (no-cc )  h (ce ) as desired.

Example 4 Consider the STRIPS planning task  with variables {i, p, q, r, z, g1 , g2 , g3 },
initial state I = {i}, goal G = {g1 , g2 , g3 }, and actions A as follows:
528



fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Name
pre
add del ce cost
aqz
{i}
{q,
z}
 
4
i
ari
{i}
{r}
 
1
pz
{i, q}
{p} {z} 
1
aiq
apz
{r}
{p}
{z}

4
r
g1
{p, z} {g1 }
 
1
apz
agiq2
{i, q} {g2 }
 
1
g3
ar
{r} {g3 }
 
1
We set C = {{i, q}, {p, z}}. The only operator adding part of {i, q} is aqz
i which adds q.
qz
pz pz
The only operators adding part of {p, z} are ai which adds z, and aiq , ar which add p;
pz
since aiq
and apz
both delete z, they cannot be used to establish the conjunction {p, z}.
r
Thus the actions of C
ce are:
Name
pre
add del
ce cost
qz
qz
ai
{i} {q, z}
 ce(ai )
4
ari
{i}
{r}


1
{i,
q,

}
{p}


1
apz
i,q
iq
pz
ar
{r}
{p}


4
{p, z, p,z } {g1 }


1
agpz1
g2
aiq
{i, q, i,q } {g2 }


1
{r} {g3 }


1
agr3
where ce(aqz
i ) contains two conditional effects:
Name
c
add del
i,q
ei
{i} {i,q }


ep p,z
{p} {p,z }

pz
Clearly, with respect to hmax , the -fluents in the preconditions of aiq
, agpz1 , and agiq2 dominate the respective other preconditions of these actions (as pointed out in Section 5.1, in our
implementation we actually remove the other preconditions). Thus LM-cuts justification
graph on C
ce would have the structure shown in Figure 5.
+
C
We have hmax (C
ce ) = 10 due to the cost of achieving g1 . As for h (ce ), to construct
qz pz
a plan for C
ce the only choice we have is how to establish p. We can use ai , aiq , or we
can use ari , apz
r . In the latter case, we make do with a single application of the conditionalg2 g3
r pz qz g1
effects action aqz
i , by the relaxed plan 1 = hai , ar , ai , apz , aiq , ar i, whose cost is 12. In
qz
the former case, we must use ai twice  first for i,q , then for p,z  yielding the relaxed
pz qz g1
g2 r g3
+
C
plan 2 = haqz
i , aiq , ai , apz , aiq , ai , ar i, whose cost is 13. Thus h (ce ) = 12. Since, in
pz
the execution of 1 , the only delete is that of ar , which is not true anyhow in the state of
execution, 1 also solves the original task  and we get h () = h+ (C
ce ) = 12.
Now consider LM-cut, and say that we produced the cut for the conditional-effects action
p,z
aqz
i that connects p to p,z via the conditional effect ep . The two options discussed in
Section 6.1 are to (A) reduce the cost of aqz
i globally, sticking to the original definition of


LM-cut; or to (B) reduce the cost only of ep p,z , because the other conditional effect ei i,q is
p,z
part of an optimal-cost path to ep and thus serves to justify its hmax value. Each of these
options violates one of the essential properties of LM-cut:

529

fiKeyder, Hoffmann, & Haslum



z

p



i,q
aqz
i : ei

aqz
i

i

q

aqz
i

:


ei i,q

:

r

arg3

p,z

apz
iq


ei i,q

i,q

apz
r

ari

p,z
aqz
i : ep

agiq2

agpz1

g1

g2

g3

Figure 5: Illustration of LM-cut justification graphs for C
ce in Example 4. The dashed
edges correspond to preconditions that are not critical (hmax -maximizing) at the start, but
that become critical at some point during the execution of LM-cut.


p,z
(A) In this configuration, LM-cut produces the cuts {agpz1 } [cost 1], {aqz
i : ep } [cost 4],
pz pz
g2
[cost 1], {ar , aiq } [cost 1], {aiq } [cost 1], {ari } [cost 1]. Note here that, after the
i,q
p,z
qz
qz
cut {aqz
i : ep }, the cost of ai is reduced to 0 globally; in particular, the cut {ai : ei }
is not produced. We get the heuristic value hLM-cut = 9 < hmax (C
ce ) = 10, so here LM-cut
does not dominate hmax .
(B) In this configuration, LM-cut can produce the following cuts. At the start, for every
p,z
possible precondition choice function (pcf ), we get the cuts {agpz1 } [cost 1] and {aqz
i : ep }
[cost 4]. Now hmax is 5 for each of g1 , g2 ; say the pcf selects g2 , and we get the cut {agiq2 }
pz
max is 4
[cost 1]. Now, the pcf has to select g1 , getting the cut {apz
r , aiq } [cost 1]. Then, h
g1
for each of g1 , g2 ; say the pcf selects g1 . Say further that the pcf selects p,z for apz (another
choice would be z), and selects i,q for apz
(another choice would be q), thus remaining
iq
i,q pz
in the non-dashed part of Figure 5. Then we get the cut {aqz
i : ei , ar } [cost 3] because
g1 can be reached at 0 cost from both p and i,q (we would get the same cut for any pcf
selecting g1 , at this point). Now, hmax is 2 for g3 and 1 for each of g1 , g2 , so we get the cut
{agr3 } [cost 1]. At this point, hmax is 1 for all goal facts; say LM-cut selects g3 , and thus
we get the cut {ari } [cost 1] because that is the only way to achieve r. Then finally hmax
i,q
is 1 for g2 only, yielding the cut {aqz
i : ei } [cost 1]. Overall, we get the heuristic value
hLM-cut = 13 > h () = h+ (C
ce ) = 12, so here LM-cut is not admissible.

{agr3 }

Bibliography
Baier, J. A., & Botea, A. (2009). Improving planning performance using low-conflict relaxed
plans. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the
530

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

19th International Conference on Automated Planning and Scheduling (ICAPS09),
pp. 1017, Thessaloniki, Greece. AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. In
Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th European
Conference on Artificial Intelligence (ECAI10), pp. 329334, Lisbon, Portugal. IOS
Press.
Bonet, B., Palacios, H., & Geffner, H. (2009). Automatic derivation of memoryless policies
and finite-state controllers using classical planners. In Gerevini, A., Howe, A., Cesta,
A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS09), pp. 3441, Thessaloniki, Greece. AAAI
Press.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (12), 165204.
Cai, D., Hoffmann, J., & Helmert, M. (2009). Enhancing the context-enhanced additive
heuristic with precedence constraints. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning
and Scheduling (ICAPS09), pp. 5057, Thessaloniki, Greece. AAAI Press.
Do, M. B., & Kambhampati, S. (2001). Sapa: A domain-independent heuristic metric temporal planner. In Cesta, A., & Borrajo, D. (Eds.), Recent Advances in AI Planning. 6th
European Conference on Planning (ECP01), Lecture Notes in Artificial Intelligence,
pp. 109120, Toledo, Spain. Springer-Verlag.
Fox, M., & Long, D. (2001). STAN4: A hybrid planning strategy based on subproblem
abstraction. The AI Magazine, 22 (3), 8184.
Garey, M. R., & Johnson, D. S. (1979). Computers and IntractabilityA Guide to the
Theory of NP-Completeness. Freeman, San Francisco, CA.
Gazen, B. C., & Knoblock, C. (1997). Combining the expressiveness of UCPOP with
the efficiency of Graphplan. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI
Planning. 4th European Conference on Planning (ECP97), Lecture Notes in Artificial
Intelligence, pp. 221233, Toulouse, France. Springer-Verlag.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239290.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Chien, S.,
Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the 5th International Conference on Artificial Intelligence Planning Systems (AIPS00), pp. 140149, Breckenridge, CO. AAAI Press.
Haslum, P. (2009). hm (P ) = h1 (P m ): Alternative characterisations of the generalisation
from hmax to hm . In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning and Scheduling
(ICAPS09), pp. 354357, Thessaloniki, Greece. AAAI Press.
531

fiKeyder, Hoffmann, & Haslum

Haslum, P. (2012). Incremental lower bounds for additive cost planning problems. In
Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd
International Conference on Automated Planning and Scheduling (ICAPS12), pp.
7482, Sao Paulo, Brasil. AAAI Press.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats
the difference anyway?. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.),
Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS09), pp. 162169, Thessaloniki, Greece. AAAI Press.
Helmert, M., & Geffner, H. (2008). Unifying the causal graph and additive heuristics. In
Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings of the 18th
International Conference on Automated Planning and Scheduling (ICAPS08), pp.
140147, Sydney, Australia. AAAI Press.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal
of Artificial Intelligence Research, 22, 215278.
Hoffmann, J., Steinmetz, M., & Haslum, P. (2014). What does it take to render h+ ( c )
perfect?. In Proceedings of the 6th Workshop on Heuristics and Search for Domain
Independent Planning, at ICAPS14.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Boutilier, C.
(Ed.), Proceedings of the 21st International Joint Conference on Artificial Intelligence
(IJCAI09), pp. 17281733, Pasadena, California, USA. Morgan Kaufmann.
Katz, M., Hoffmann, J., & Domshlak, C. (2013). Who said we need to relax all variables?.
In Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings of the
23rd International Conference on Automated Planning and Scheduling (ICAPS13),
pp. 126134, Rome, Italy. AAAI Press.
Keyder, E., & Geffner, H. (2008). Heuristics for planning with action costs revisited. In
Ghallab, M. (Ed.), Proceedings of the 18th European Conference on Artificial Intelligence (ECAI08), pp. 588592, Patras, Greece. Wiley.
Keyder, E., & Geffner, H. (2009). Trees of shortest paths vs. Steiner trees: Understanding
and improving delete relaxation heuristics. In Boutilier, C. (Ed.), Proceedings of the
21st International Joint Conference on Artificial Intelligence (IJCAI09), pp. 1734
1749, Pasadena, California, USA. Morgan Kaufmann.
Keyder, E., Hoffmann, J., & Haslum, P. (2012). Semi-relaxed plan heuristics. In Bonet, B.,
McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd International Conference on Automated Planning and Scheduling (ICAPS12), pp. 128136,
Sao Paulo, Brasil. AAAI Press.
532

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Keyder, E., Richter, S., & Helmert, M. (2010). Sound and complete landmarks for And/Or
graphs. In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th
European Conference on Artificial Intelligence (ECAI10), pp. 335340, Lisbon, Portugal. IOS Press.
Palacios, H., & Geffner, H. (2009). Compiling uncertainty away in conformant planning
problems with bounded width. Journal of Artificial Intelligence Research, 35, 623
675.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. Journal of Artificial Intelligence Research, 39, 127177.
Roger, G., Pommerening, F., & Helmert, M. (2014). Optimal planning in the presence of
conditional effects: Extending LM-Cut with context-splitting. In Schaub, T. (Ed.),
Proceedings of the 21st European Conference on Artificial Intelligence (ECAI14),
Prague, Czech Republic. IOS Press. To appear.

533

fiJournal of Artificial Intelligence Research 50 (2014) 1-30

Submitted 12/13; published 05/14

Topic-Based Dissimilarity and Sensitivity Models
for Translation Rule Selection
Min Zhang

MINZHANG @ SUDA . EDU . CN

Provincial Key Laboratory for Computer Information Processing Technology,
Soochow University, Suzhou, China

Xinyan Xiao

XIAOXINYAN @ ICT. AC . CN

IIP Key Lab, Institute of Computing Technology,
Chinese Academy of Sciences, China

Deyi Xiong

DYXIONG @ SUDA . EDU . CN

Provincial Key Laboratory for Computer Information Processing Technology,
Soochow University, Suzhou, China

Qun Liu

LIUQUN @ ICT. AC . CN

CNGL, School of Computing, Dublin City University, Ireland
IIP Key Lab, Institute of Computing Technology,
Chinese Academy of Sciences, China

Abstract
Translation rule selection is a task of selecting appropriate translation rules for an ambiguous
source-language segment. As translation ambiguities are pervasive in statistical machine translation, we introduce two topic-based models for translation rule selection which incorporates global
topic information into translation disambiguation. We associate each synchronous translation rule
with source- and target-side topic distributions.With these topic distributions, we propose a topic
dissimilarity model to select desirable (less dissimilar) rules by imposing penalties for rules with a
large value of dissimilarity of their topic distributions to those of given documents. In order to encourage the use of non-topic specific translation rules, we also present a topic sensitivity model to
balance translation rule selection between generic rules and topic-specific rules. Furthermore, we
project target-side topic distributions onto the source-side topic model space so that we can benefit
from topic information of both the source and target language. We integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrase-based machine translation for synchronous
translation rule selection. Experiments show that our topic-based translation rule selection model
can substantially improve translation quality.

1. Introduction
Translation rules are bilingual segments1 that establish translation equivalences between the source
and target language. They are widely used in statistical machine translation (SMT) with various representations ranging from word pairs to bilingual phrases and synchronous rules in word-, phraseand syntax-based SMT respectively. Normally, a large number of translation rules can be learnt
from bilingual training data for a single source segment which occurs in different contexts. For
example, Xiong, Zhang, and Li (2012) observe that each Chinese verb can be translated with more
1. Here a segment is defined as a string of terminals and/or nonterminals.

c
2014
AI Access Foundation. All rights reserved.

fiZ HANG , X IAO , X IONG , & L IU

than 140 different translation rules on average. Therefore how to select an appropriate translation
rule for an ambiguous source segment is a very crucial issue in SMT.
Traditionally the appropriateness of a translation rule is measured with multiple probabilities
estimated from word-aligned data, such as bidirectional translation probabilities (Koehn, Och, &
Marcu, 2003). As such probabilities fail to capture local and global contexts of highly ambiguous
source segments, they are not sufficient to select correct translation rules for these segments. Therefore various approaches have been proposed to capture rich contexts at the sentence level to help
select proper translation rules for phrase- (Carpuat & Wu, 2007a) or syntax-based SMT (Chan, Ng,
& Chiang, 2007; He, Liu, & Lin, 2008; Liu, He, Liu, & Lin, 2008). These studies show that local
features, such as surrounding words, syntactic information and so on, are helpful for translation rule
selection.
Beyond these contextual features at the sentence level, we conjecture that translation rules are
also related to high-level global information, such as the topic (Hofmann, 1999; Blei, Ng, & Jordan,
2003) information at the document level. In order to visualize the relatedness between translation
rules and document topics, we show four hierarchical phrase-based translation rules with their topic
distributions in Figure 1. From the figure, we can observe that
 First, translation rules can be divided into two categories in terms of their topic distributions:
topic-sensitive rules (i.e., topic-specific rules) and topic-insensitive rules (i.e., non-topic specific or generic rules). The former rules, e.g., the translation rule (a), (b) and (d) in Figure
1, have much higher distribution probabilities on a few specific topics than other topics. The
latter rules, e.g., the translation rule (c) of Figure 1, have an even distribution over all topics.
 Second, topic information can be used to disambiguate ambiguous source segments. In Figure
1, translation rule (b) and (c) have the same source segment. However their topic distributions
are quite different. Rule (b) distributes on the topic about international relations with the
highest probability, which suggests that rule (b) is much more related to this topic than other
topics. In contrast, rule (c) has an even distribution over all topics. Therefore in a document
on international relations, rule (b) will be more appropriate than rule (c) for the source
X1 .
segment 



These two observations suggest that different translation rules have different topic distributions and
document-level topic information can be used to benefit translation rule selection.
In this article, we propose a framework for translation rule selection that exactly capitalizes on
document-level topic information. The proposed topic-based translation rule selection framework
associates each translation rule with a topic distribution (rule-topic distribution) on both the source
and target side. Each source document is also annotated with its corresponding topic distribution
(document-topic distribution). Dissimilarity between the document-topic distribution and rule-topic
distribution is calculated and used to help select translation rules that are related to documents in
terms of topics. In particular,
 Given a document to be translated, we use a topic dissimilarity model to calculate the dissimilarity of each translation rule to the document based on their topic distributions. Our
translation system will penalize candidate translations with high dissimilarities.2
2. Section 6 explains why our system penalizes candidate translations with high dissimilarities.

2

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

0.6

0.6

0.4

0.4

0.2

0.2

0

 U  operational capability

1

(a)

5

10

15

20

25

0
30

1

0.6

0.6

0.4

0.4

0.2

0.2

0
1

5

(c)

 X
10

1

0
15

20

25

5

(b)

30

1

 give X1

(d) X1

 X
10

15

1 ! X
5

20

25

30

25

30

 grants X1

1

10

2

15

20

 held talks X1 X2

Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its
topic distribution, where the X-axis shows the topic index and the Y-axis the topic probability. Notably, the rule (b) and rule (c) shares the same source Chinese string, but they
have different topic distributions due to the different English translations.

 The dissimilarity between a topic-insensitive translation rule and a given source document
computed by our topic dissimilarity model is often very high as documents are normally
topic-sensitive. We dont want to penalize these generic topic-insensitive rules. Therefore
we further propose a topic sensitivity model which rewards topic-insensitive rules so as to
complement the topic dissimilarity model.
 We associate each translation rule with a rule-topic distribution on both the source and target side. In order to calculate the dissimilarity between target-side rule-topic distributions
of translation rules and source-side document-topic distributions of given documents during
decoding, we project the target-side rule-topic distributions of translation rules onto the space
of source-side document topic model by one-to-many mapping.
We use a hierarchical phrase-based SMT system (Chiang, 2007) to validate the effectiveness of
our topic-based models for translation rule selection. Experiments on Chinese-English translation
tasks (Section 7) show that our method outperforms the baseline hierarchial phrase-based system
by +1.2 B LEU points on large-scale training data.
The use of topic-based dissimilarity and sensitivity models to improve SMT was first presented
in our previous paper (Xiao, Xiong, Zhang, Liu, & Lin, 2012). In this article, we provide more
detailed comparison to related work and formulations of the two models as well as the integration

3

fiZ HANG , X IAO , X IONG , & L IU

procedure. More importantly, we carry out large-scale experiments with more bilingual and monolingual training data and incorporate a detailed analysis of the output of topic-based dissimilarity
and sensitivity models at both the document and translation hypothesis level.
The rest of this article is organized as follows. Section 2 introduces related work. Section 3
provides background knowledge about statistical machine translation and topic modeling. Section 4
elaborates our topic-based translation rule selection framework, including the topic dissimilarity and
topic sensitivity model. Section 5 discusses how we estimate rule-topic and document-topic distributions and how we project target-side rule-topic distributions onto the source-side topic space in a
one-to-many mapping fashion. Section 6 presents the integration of the topic-based translation rule
selection models into hierarchical phrase-based SMT. Section 7 describes a series of experiments
that verify the effectiveness of our approach. Section 8 provides a detailed analysis of the output of
our models. Section 9 gives some suggestions for bilingual topic modeling from the perspective of
machine translation. Finally, we conclude in Section 10 with future directions.

2. Related Work
Our topic-based dissimilarity and sensitivity models for translation rule selection are related to three
categories of work in SMT: translation rule selection, topic models for SMT and document-level
translation. In this section, we introduce related approaches of the three categories and highlight the
differences of our method from previous work.
2.1 Translation Rule Selection
As we mentioned before, translation rule selection is a very important task in SMT. Several approaches have been proposed for it recently. Carpuat and Wu explore both word and phrase sense
disambiguation (WSD and PSD) for translation rule selection in phrase-based SMT (Carpuat & Wu,
2007a, 2007b). Their WSD and PSD system integrate sentence-level local collocation features. Experiments show that multi-word PSD can improve phrase selection. Also following the WSD line,
Chan et al. (2007) integrate a WSD system into hierarchical phrase-based SMT for lexical selection
or the selection of short phrases of length 1 or 2. Their WSD system also adopts sentence-level
features of local collocations, surrounding words and so on.
Different from lexical or phrasal selection using WSD/PSD, He et al. (2008) propose a maximum entropy (MaxEnt) based model for context-dependent synchronous rule selection in hierarchical phrase-based SMT. Local context features such as phrase boundary words and part-of-speech
information are incorporated into the model. Liu et al. (2008) extends the selection method of He
et al. to integrate a similar MaxEnt-based rule selection model into a tree-to-string syntax-based
SMT system (Liu, Liu, & Lin, 2006). Their model uses syntactic information from source parse
trees as features.
The significant difference between our topic-based rule selection framework and previous approaches on translation rule selection is that we use global topic information to help select translation rules for ambiguous source segments rather than sentence-level local context features.
2.2 Topic Models for SMT
Topic modeling (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering underlying
topic structures of documents. Recent years have witnessed that topic models have been explored
4

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

for SMT. Zhao and Xing (2006, 2007) and Tam, Lane, and Schultz (2007) have proposed topicspecific lexicon translation adaptation models to improve translation quality. Such models focus on
word-level translations. They first estimate word translation probabilities conditioned on topics, and
then adapt lexical translation probabilities of phrases by these topic-conditioned probabilities. Since
modern SMT systems use synchronous rules or bilingual phrases to translate sentences, we believe
that it is more reasonable to incorporate topic models for phrase or synchronous rule selection than
lexical selection.
Gong, Zhang, and Zhou (2010) adopt a topic model to filter out phrase pairs that are not consistent with source documents in terms of their topics. They assign a topic for each document to
be translated. Similarly, each phrase pair is also assigned with one topic. A phrase pair will be
discarded if its topic mismatches the document topic. The differences from their work are twofold.
First, we calculate the dissimilarities of translation rules to documents based on their topic distributions instead of comparing the best topics assigned to translation rules and those of documents.
Second, we integrate topic information into SMT in a soft-constraint manner via our topic-based
models. They explore topic information in a hard-constraint fashion by discarding translation rules
with unmatched topics.
Topic models are also used for domain adaptation on translation and language models in SMT.
Foster and Kuhn (2007) describe a mixture model approach for SMT adaptation. They divide a
training corpus into different domains, each of which is used to train a domain-specific translation
model. During decoding, they combine a general domain translation model with a specific domain
translation model that is selected according to various text distances calculated by topic model.
Tam et al. (2007) and Ruiz and Federico (2011) use a bilingual topic model to project latent topic
distributions across languages. Based on the bilingual topic model, they apply source-side topic
weights onto the target-side topic model so as to adapt the target-side n-gram language model.
2.3 Document-Level Machine Translation
Since we incorporate document topic information into SMT, our work is also related to documentlevel machine translation. Tiedemann (2010) integrates cache-based language and translation models that are built from recently translated sentences into SMT. Gong, Zhang, and Zhou (2011) further
extend this cache-based approach by introducing two additional caches: a static cache that stores
phrases extracted from documents in training data which are similar to the document in question and
a topic cache with target language topic words. Xiao, Zhu, Yao, and Zhang (2011) try to solve the
translation consistency issue in document-level translation by introducing a hard constraint where
ambiguous source words are required to be consistently translated into the most frequent translation options. Ture, Oard, and Resnik (2012) soften this consistency constraint by integrating three
counting features into the decoder. These studies normally focus on the surface structure to capture inter-sentence dependencies for document-level machine translation while we explore the topic
structure of a document for document translation.

3. Preliminaries
We establish in this section some background knowledge about both statistical machine translation
and topic modeling. Although the introduction here is short, it is sufficient for understanding our

5

fiZ HANG , X IAO , X IONG , & L IU

Sub-models
PI
logP (ei |f i )
P1I
logP (f i |ei )
P1I
logPlex (ei |f i )
P1I
logPlex (f i |ei )
P1|e|
logP (ei |e1 ...ei1 )
PI1
1 log(ei , f i )
|e|
I

Descriptions
direct translation probabilities
inverse translation probabilities
direct lexical translation probabilities
inverse lexical translation probabilities
language model
reordering model
word count
rule count

Table 1: The most widely-used sub-models of statistical machine translation. I is the number of
translation rules that are used to generate the target sentence e given the source sentence
f . ei and f i are the target and source side of a translation rule ri .

topic-based dissimilarity and sensitivity models that try to bridge the gap between topic modeling
and statistical machine translation.
3.1 Statistical Machine Translation
Given a source sentence f , most SMT systems find the best translation e among all possible translations as follows.
hP
i 

M

 exp

h
(f,
e)
m m
1
hP
i
e = argmax P
M

 
e
e exp
1 m hm (f, e )
#)
(
" M
X
(1)
m hm (f, e)
= argmax exp
e

= argmax
e

m=1

(

M
X

)

m hm (f, e)

m=1

where hm (f, e) is a feature function defined on the source sentence f and the corresponding
transla-i
hP
P
M

tion e, m is the weight of the feature function. Since the normalization e exp
1 m hm (f, e )

is constant for all possible translations e , we do not need to calculate it during decoding.
The weighted model in the equation (1) is a log-linear model. The feature functions hm (f, e)
are also referred to as sub-models3 as they are components of the log-linear model. In Table 1,
we show the most widely-used feature functions in SMT. Most of them can be easily factored over
translation rules, which facilitates the application of dynamic programming in decoding. We will
show that our proposed topic-based dissimilarity and sensitivity models can be also easily factorized
in Section 4.
3. This notation is used when we want to emphasize that a sub-model is a component of the log-linear model. Otherwise
we just call them models, such as a language model, a reordering model and so on.

6

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

In the log-linear model of SMT, the sub-models are trained separately and combined under
the assumption that they are independent of each other. The associated weights s can be tuned
using minimum error rate training (MERT) (Och, 2003) or the Margin Infused Relaxed Algorithm
(MIRA) (Chiang, Marton, & Resnik, 2008). Note that the normalization factor in the equation (1)
is not calculated in these training algorithms. This is because these algorithms directly optimize the
log-linear model of SMT towards some translation quality measure such as BLEU. Feature weights
that are optimized towards criteria such as Maximum Mutual Information (MMI) are not necessarily
optimal with respect to translation quality (Och, 2003).
As we integrate the proposed two models into the log-linear model of a hierarchical phrasebased SMT system (Section 6) in order to validate the effectiveness of the two models, we provide
more details about hierarchical phrase-based SMT (Chiang, 2005) in this section. Translation rules
in hierarchial phrase-based SMT are synchronous context-free grammar rules, which can be denoted
as follows.
X  h, , i
(2)
where X is an undifferentiated nonterminal,  and  are strings of terminals and nonterminals4 on
the source and target side respectively,  denotes the one-to-one mapping between nonterminals in
 and nonterminals in . These rules can be automatically extracted from word-aligned bilingual
training data. In addition to these rules, two special rules are also introduced into hierarchical
phrase-based SMT.
S  hX1 , X1 i
S  hS0 X1 , S0 X1 i

(3)

These two rules are used to serially concatenate nonterminal Xs in a monotonic manner to form an
initial symbol S, the start symbol of the grammar of hierarchical phrase-based SMT.
The log-linear model of hierarchical phrase-based SMT can be formulated as follows.
!
X
log(t(r)) + lm logPlm (e) + wp |e| + rp I
(4)
w(D) = exp
rD

where D is a derivation defined as a set of triples (r, i, j), each of which denotes an application of a
translation rule that spans words i from j on the source side. I is the number of translation rules in
D. The probability of a translation rule r is defined as
t(r) = P (|)1 P (|)2 Plex (|)3 Plex (|)4

(5)

where the lexical translation probabilities Plex (|) and Plex (|) estimate the probabilities that
the words in  translate the words in  in a word-by-word fashion (Koehn et al., 2003).
3.2 Topic Modeling
Topic modeling is used to discover topics that occur in a collection of documents. Both Latent
Dirichlet Allocation (LDA) (Blei et al., 2003) and Probabilistic Latent Semantic Analysis (PLSA)
4. In order to simplify the decoder implementation, at most two nonterminals are allowed in hierarchical translation
rules.

7

fiZ HANG , X IAO , X IONG , & L IU

(Hofmann, 1999) are topic models. As LDA is the most widely used topic model, we exploit it to
mine topics for our translation rule selection.
LDA views each document as a mixture of various topics, each of which is a probability distribution over words. More particularly, LDA works in a generative process as follows.
 For each document Dj , sample a document-topic distribution (per-document topic distribution) j from a Dirichlet distribution Dir(): j  Dir();
 for each word wj,i of Nj words in the document Dj ,
 Sample a topic assignment zj,i  Multinomial(j );
 Sample the word wj,i  Multinomial(zj,i ) where zj,i is the per-topic word distribution of topic zj,i drawn from Dir().
Generally speaking, LDA contains two groups of parameters. The first group of parameters
characterizes document-topic distributions (j ), which record the distribution of each document over
topics. The second group of parameters is used for topic-word distributions (k ), which represent
each topic as a distribution over words.
Given a document collection with observed words w = {wj,i }, the goal of LDA inference is to
compute the values for these two sets of parameters  and  as well as the latent topic assignments
z = {zj,i }. The inference is complicated due to the latent topic assignments z. An efficient inference
algorithm that has been proposed to address this problem is Collapsed Gibbs Sampling (Griffiths
& Steyvers, 2004), where the two sets of parameters  and  are integrated out of the LDA model,
and only the latent topic assignments z are sampled from P (z|w). Once we obtain the values of z,
we can estimate  and  by recovering their posterior distributions given z and w. In Section 4, we
will use these two sets of estimated parameters and the topic assignments of words to calculate the
parameters of our models.

4. Topic-based Dissimilarity and Sensitivity Models
In this section, we elaborate our topic-based models for translation rule selection, including a topic
dissimilarity model and a topic sensitivity model.
4.1 Topic Dissimilarity Model
Sentences should be translated in accordance with their topics (Zhao & Xing, 2006, 2007; Tam
et al., 2007). Take the translation rule (b) in Figure 1 as an example. If the source side of rule
(b) occurs in a document on international relations, we hope to encourage the application of rule
(b) rather than rule (c). This can be achieved by calculating the dissimilarity between probability
distributions of a translation rule and a document over topics.
In order to calculate such a topic dissimilarity for translation rule selection, we associate both
the source and target side of a translation rule with a rule-topic distribution P (z |r ), where  is
the placeholder for the source side f or target side e, r is the source or target side of a translation
rule r, and z is the corresponding topic of r . Therefore each translation rule has two rule-topic
distributions: P (zf |rf ) on the source side and P (ze |re ) on the target side.
8

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

Supposing there are K topics, the two distributions can be represented by a K-dimension vector. The k-th component P (z = k|r ) denotes the probability of topic k given r . The sourceand target-side rule-topic distributions are separately estimated from training data. The estimation
method is described in Section 5, where we also discuss the reason why we estimate them in a
separate manner.
Analogously, we represent the topic information of a document d to be translated by a documenttopic distribution P (z|d), which is also a K-dimension vector. The k-th dimension P (z = k|d)
is the topic proportion for topic k in document d. Different from the rule-topic distribution, the
document-topic distribution can be directly inferred by an off-the-shelf LDA tool.
Based on the defined rule-topic and document-topic distributions, we can measure the dissimilarity of a translation rule to a document so as to decide whether the rule is suitable for the document
in translation. Traditionally, the similarity of two probability distributions is calculated by information measurements such as Jensen-Shannon divergence (Lin, 2006) or Hellinger distance (Blei &
Lafferty, 2007).
Here we adopt the Hellinger distance (HD) to measure the topic dissimilarity, which is symmetric and widely used for comparing two probability distributions (Blei & Lafferty, 2007). Given
a rule-topic distribution P (z |r ) and a document-topic distribution P (z|d), HD is computed as
follows.
K p
2
X
p
P (z = k|d)  P (z = k|r )
(6)
HD(P (z|d), P (z |r )) =
k=1

Let D be a derivation as defined in Section 3.1. Let P(z|r) represent corresponding rule-topic
distributions for all rules in D. Our topic dissimilarity model Dsim(P (z|d), P(z|r)) on a derivation
D is defined on the HD of the equation (6) as follows
X
Dsim(P (z|d), P(z|r)) =
HD(P (z|d), P (z |r ))
(7)
rD

Obviously, the larger the Hellinger distance between a candidate translation yielded by a derivation
and a document, the larger the dissimilarity between them. With the topic dissimilarity model
defined above, we aim to select translation rules that are similar to the document to be translated in
terms of their topics.
4.2 Topic Sensitivity Model
Before we introduce the topic sensitivity model, lets revisit Figure 1. We can easily find that the
probability of rule (c) distributes evenly over all topics. This indicates that it is insensitive to topics,
and can be therefore applied on any topics. In contrast, the distributions of the other three rules
peak on a few topics. Generally speaking, a topic-insensitive rule has a fairly flat distribution over
all topics, while a topic-sensitive rule has a sharp distribution over a few topics.
As a document typically focuses on a few topics, it has a sharp distribution over these topics.
In other words, documents are normally topic-sensitive. Since the distribution of a topic-insensitive
rule is fairly flat, the dissimilarity between a topic-insensitive rule and a topic-sensitive document
will be very low. Therefore, our system with the proposed topic dissimilarity model will punish
topic-insensitive rules.
9

fiZ HANG , X IAO , X IONG , & L IU

However, topic-insensitive rules may be more preferable than topic-sensitive rules if neither of
them are similar to given documents. For a document about a topic of love, the rule (b) and (c) in
Figure 1 are both dissimilar to the document as rule (b) relates to the international relations topic
and rule (c) is topic-insensitive. Nevertheless, since rule (c) occurs more frequently across various
topics, we prefer rule (c) to rule (b) when we translate a document about love.
To address such issue of the topic dissimilarity model, we further propose a topic sensitivity
model. The model employs an entropy based metric to measure the topic sensitivity of a rule as
follows
K
X
P (z = k|r )  log(P (z = k|r ))
(8)
H(P (z |r )) = 
k=1

According to this equation, a topic-insensitive rule normally has a large entropy while a topicsensitive rule has a smaller entropy.
Given a derivation D and rule-topic distributions P(z|r) for rules in D, the topic sensitivity
model is defined as follows.
X
H(P (z |r ))
(9)
Sen(P(z|r)) =
rD

Incorporating the topic sensitivity model with the topic dissimilarity model, we enable our SMT
system to balance the selection of topic-sensitive and topic-insensitive rules. Given rules with approximately equal values of topic dissimilarity, we prefer topic-insensitive rules.

5. Estimation
Unlike document-topic distributions that can be directly learned by LDA tools, we need to estimate
rule-topic distributions for translation rules. As we want to exploit topic information of both the
source and target language, we separately train two monolingual topic models on the source and
target side, and learn correspondences between the two topic models via word alignments in the
bilingual training data.
Particularly, we adopt two rule-topic distributions for each translation rule: 1) the source-side
rule-topic distribution P (zf |rf ) and the 2) the target-side rule-topic distribution P (ze |re ), both of
which are defined in Section 4.1. These two rule-topic distributions are estimated using trained
topic models in the same way (Section 5.1). Notably, only source-language documents are available
during decoding. In order to compute the dissimilarity between the target-side rule-topic distribution
of a translation rule and the source-side document-topic distribution of a given document we need
to project the target-side rule-topic distribution of a translation rule onto the space of the source-side
topic model (Section 5.2).
We can also establish alternative approaches to the estimation of rule-topic distributions via
multilingual topic models (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Boyd-Graber
& Blei, 2009) or bilingual topic models that also infer word-to-word alignments in document pairs
(Zhao & Xing, 2006, 2007). The former multilingual topic models only require that documents in
different languages are comparable in terms of content similarity. In contrast, the latter bilingual
topic models require that documents are parallel, i.e., translations of each other, so as to capture
word alignments.



10

fiT OPIC -BASED D ISSIMILARITY

Z

N

AND

S ENSITIVITY M ODELS

Z

W

N

M

Z

W
Z

topic
correspondence

N1

Z

M

k

k

target

source

(a)

W

NL

N

M

word
alignment

N

I

W

M

M
W

Z

Z

e

J

W
1

L

1

L

k

M

f

J

a

S

k

k

B

T

target

source

(a*)

(b)

(c)

Figure 2: Graphical model representations of (a) our bilingual topic model, (b) polylingual topic
model of Mimno et al. (2009), and (c) bilingual topic model of Zhao and Xing (2007)
where S is the number of parallel sentence pairs in a document, a is the word alignment
between a source and target sentence. For simplicity, we do not display HMM transitions
among word alignments a. Subfigure (a*) shows how we build topic correspondences between the source and target language after source and target topics are separately learned
as shown in (a).

The biggest difference between our method and these multilingual/bilingual topic models is that
they use the same per-tuple topic distribution  for all documents in the same tuple. Here we define
the tuple as a set of documents in different languages. A per-tuple topic distribution is similar to
a per-document topic distribution. The only difference between them is that the per-tuple topic
distribution is shared by all documents in the tuple.
Topic assignments for words in these languages are naturally connected since they are sampled
from the same topic distribution. In contrast, we assume that each document on the source/target
side has its own sampled document-specific distribution over topics. Topic correspondences between the source and target document are learned by projection via word alignments. We visualize
this difference in Figure 2.
Yet another difference between our models and the topic-specific lexicon translation model of
Zhao and Xing (2007) is that they use their bilingual topics to improve SMT at the word level
instead of the rule level. Since a synchronous rule is rarely factorized into individual words, we
believe that it is more reasonable to incorporate the topic model directly at the rule level rather than
the word level. In Section 7.2.3, we empirically compare our model with the topic-specific lexicon
translation model.
Tam et al. (2007) also construct two monolingual topic models for parallel source and target
documents. They build the topic correspondences between source and target documents by enforcing a one-to-one topic mapping constraint. We project target-side topics onto the space of the
source-side topic model in a one-to-many fashion. In Section 7.3.1, we compare these two different
methods for building topic correspondences.

11

fiZ HANG , X IAO , X IONG , & L IU

5.1 Rule-Topic Distribution Estimation
We estimate rule-topic distributions from word-aligned bilingual training corpus with document
boundaries explicitly given. The source- and target-side rule-topic distributions are estimated in the
same way. Therefore, for simplicity, we only describe the estimation of the source-side rule-topic
distribution P (zf |rf ) of a translation rule in this section.
The estimation of rule-topic distributions is analogous to the traditional estimation of rule translation probabilities (Chiang, 2007). In addition to the word-aligned corpus, the input for rule-topic
distribution estimation also contains source-side document-topic distributions inferred by LDA tool.
We first extract translation rules from bilingual training data in a traditional way. When the
source side of a translation rule rf is extracted from a source-language document df with a documenttopic distribution P (zf |df ), we obtain an instance (rf , P (zf |df ), ), where  is the fraction count
of an instance as described by Chiang (2007). In this way, we can collect a set of instances I
= {(rf , P (zf |df ), )} with different document-topic distributions for each translation rule. Using
these instances, we calculate the probability P (zf = k|rf ) of rf over topic k as follows:
P
  P (zf = k|df )
(10)
P (zf = k|rf ) = PK II
P

k  =1
II   P (zf = k |df )

Based on this equation, we can obtain two rule-topic distributions P (zf |rf ) and P (ze |re ) for each
rule using the source- and target-side document-topic distributions P (zf |df ) and P (ze |de ) respectively.
5.2 Target-Side Rule-Topic Distribution Projection

As described in the previous section, we also estimate target-side rule-topic distributions. However, we can not directly use the equation (6) to calculate the dissimilarity between the target-side
rule-topic distribution P (ze |re ) of a translation rule and the source-side document-topic distribution
P (zf |df ) of a source-language document that is to be translated. In order to measure this dissimilarity, we need to project target-side topics onto the source-side topic space. The projection takes
the following two steps.
 First, we calculate a correspondence probability p(zf |ze ) for each pair of a target-side topic
ze and a source-side topic zf , which are inferred by the two separately trained monolingual
topic models respectively.
 Second, we project the target-side rule-topic distribution of a translation rule onto the sourceside topic space using the correspondence probabilities learned in the first step.
In the first step, we estimate the topic-to-topic correspondence probabilities using co-occurrence
counts of topic assignments of source and target words in the word-aligned corpus. The topic assignments of source/target words are inferred by the two monolingual topic models. With these topic
assignments, we characterize a sentence pair (f, e) as (zf , ze , a), where zf and ze are two vectors
containing topic assignments for words in the source and target sentence f and e respectively, and a
is a set of word alignment links {(i, j)} between the source and target sentence. Particularly, a link
(i, j) represents that a source-side position i aligns to a target-side position j.
12

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

With these notations, we calculate the co-occurrence count of a source-side topic kf and a
target-side topic ke as follows.
X X
(zfi , kf )  (zej , ke )
(11)
(zf ,ze ,a) (i,j)a

where zfi and zej are topic assignments for words fi and ej respectively, (x, y) is the Kronecker
function, which is 1 if x = y and 0 otherwise.
We then compute the topic-to-topic correspondence probability of P (zf = kf |ze = ke ) by
normalizing the co-occurrence count as follows.
P
P
(zf ,ze ,a)
(i,j)a (zfi , kf )  (zej , ke )
P
P
(12)
P (zf = kf |ze = ke ) =
(zf ,ze ,a)
(i,j)a (zej , ke )

Overall, after the first step, we obtain a topic-to-topic correspondence matrix MKe Kf , where the
item Mi,j represents the probability P (zf = i|ze = j).
In the second step, given the correspondence matrix MKe Kf , we project the target-side ruletopic distribution P (ze |re ) to the source-side topic space by multiplication as follows.
T (P (ze |re )) = P (ze |re )  MKe Kf

(13)

In this way, we get a second distribution for a translation rule in the source-side topic space, which
we call projected target-side topic distribution T (P (ze |re )).
Word alignment noises may be introduced in the equation (11), which in turn may flatten the
sharpness of the projected topic distributions calculated in the equation (13). In order to decrease
the flattening effects of word alignment noises, we take the following action in practice: if the
topic-to-topic correspondence probability P (zf = kf |ze = ke ) calculated via word alignments is
1
where K is the predefined number of topics, we set it to 0 and then re-normalize all
less than K
other correspondence probabilities of the target-side topic ke .
Obviously, our projection method allows one target-side topic ze to align to multiple source-side
topics. This is different from the one-to-one correspondence used by Tam et al. (2007). We investigate the correspondence matrix MKe Kf obtained from our training data. We find that the topic
correspondence between the source and target language is not necessarily one-to-one. Typically, the
correspondence probability P (zf = kf |ze = ke ) of a target-side topic mainly distributes over two
or three source-side topics. Table 2 shows an example of a target-side topic with its three mainly
aligned source-side topics.

6. Integration
We incorporate our topic dissimilarity and sensitivity model as two new features into a hierarchical
phrase-based system (Chiang, 2007) under the log-linear discriminative framework (Och & Ney,
2002). The dissimilarity values are positive as Hellinger distances are positive. The weight of this
dissimilarity feature tuned by MERT will be negative. Therefore the log-linear model will favor
those candidate translations with lower values of the dissimilarity feature (less dissimilar). In other
words, translation rules that are more similar to the document to be translated in terms of their topics
will be selected.
13

fiZ HANG , X IAO , X IONG , & L IU

e-topic
enterprises
rural
state
agricultural
market
reform
production
peasants
owned
enterprise
P (zf |ze )

(agricultural)
~(rural)
(peasant)
U(reform)
(finance)
(social)
(safety)
N(adjust)
(policy)
\(income)

f-topic 1

(enterprise)
|(market)
Ik(state)
i(company)
7K(finance)
1(bank)
(investment)
+n(manage)
U(reform)
E(operation)

f-topic 2

u(develop)
L(economic)
E(technology )
I(China)
E(technique)
(industry)
((structure)
M#(innovation)
\(accelerate)
U(reform)

f-topic 3

0.38

0.28

0.16

Table 2: An example of topic-to-topic correspondence. The last line shows the correspondence
probability. Each column shows a topic represented by its top-10 topical words. The first
column is a target-side topic, while the remaining three columns are source-side topics.

One possible side-effect of the integration of such a dissimilarity feature is that our system will
favour translations generated by fewer translation rules against those generated by more translation
rules because more translation rules result in higher dissimilarity (see the equation (7)). That is to
say, the topic-based dissimilarity feature also acts as a translation rule count penalty on derivations.
Fortunately, however, we also use a translation rule count feature (see the last row in Table 1) which
normally favours translations yielded by a derivation with a large number of translation rules. This
feature will balance against the mentioned side-effect of our topic-based dissimilarity feature.
As each translation rule is associated with a source-side rule-topic distribution and a projected
target-side rule-topic distribution during decoding, we add four features as follows.5
 Dsim(P (zf |d), P(zf |rf )) (or DsimSrc): Topic dissimilarity feature on source-side rule-topic
distributions.
 Dsim(P (zf |d), T (P(ze |re ))) (or DsimTrg): Topic dissimilarity feature on projected targetside rule-topic distributions.
 Sen(P(zf |rf )) (or SenSrc): Topic sensitivity feature on source-side rule-topic distributions.
 Sen(T (P(ze |re )) (or SenTrg): Topic sensitivity feature on projected target-side rule-topic
distributions.
The source-side and projected target-side rule-topic distributions for translation rules can be
calculated before decoding as described in the last section. During decoding, we first infer the
topic distribution P (zf |d) for a given document of the source language. When a translation rule is
adopted in a derivation, the scores of the four features will be updated correspondingly according to
the equation (7) and (9). Obviously, the computational cost of these features is rather small.
5. Since the glue rule and rules of unknown words are not extracted from training data, we just set the values of the four
features for these rules to zero.

14

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

For topic-specific lexicon translation models (Zhao & Xing, 2007; Tam et al., 2007), they first
calculate topic-specific translation probabilities by normalizing the entire lexicon translation table
and then adapt the lexical weights of translation rules correspondingly during decoding. This makes
the decoder run slower. Therefore, comparing with previous topic-specific lexicon translation methods, our method provides a more efficient way for incorporating topic models into SMT.

7. Experiments
In this section, we conducted two groups of experiments to validate the effectiveness of our topicbased translation rule selection framework. In the first group of experiments, we use medium-scale
bilingual data to train our SMT system and topic models. The purpose of this group of experiments
is to quickly answer the following questions:
 Is our topic dissimilarity model able to improve translation rule selection in terms of B LEU?
Furthermore, are the source-side and target-side rule-topic distributions complementary to
each other?
 Is it helpful to introduce the topic sensitivity model to distinguish topic-insensitive and topicsensitive rules?
 Is our topic-based method better than previous topic-specific lexicon translation method (Zhao
& Xing, 2007) in terms of both B LEU and decoding speed?
After we confirm the efficacy of our topic-based dissimilarity and sensitivity model on mediumscale training data, we conducted a second group of experiments on large-scale training data to
further investigate the following questions:
 Is our one-to-many target-side rule-topic projection method better than previous methods
proposed by Zhao and Xing (2007) or Tam et al. (2007)?
 What are the effects of our models on various types of rules, such as phrase rules and rules
with non-terminals?
 What else can we achieve if we use more monolingual data to train topic models?
7.1 Setup
We carried out our experiments on NIST Chinese-to-English translation. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as the test sets. The
numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. Case-insensitive
NIST B LEU (Papineni, Roukos, Ward, & Zhu, 2002) was used to measure translation performance.
We used minimum error rate training (Och, 2003) to optimize the feature weights.
In our medium-scale experiments, we used the FBIS corpus as our bilingual training data, which
contains 10,947 documents, 239K sentence pairs with 6.9M Chinese words and 9.14M English
words. In our large-scale experiments, the bilingual training data consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News).

15

fiZ HANG , X IAO , X IONG , & L IU

These selected corpora contain 103,236 documents and 2.80M sentences. On average, each document has 28.4 sentences.
We obtained symmetric word alignments of training data by first running GIZA++ (Och & Ney,
2003) in both directions and then applying the refinement rule grow-diag-final-and (Koehn et al.,
2003). Our hierarchical phrase translation rules were extracted from word-aligned training data.
We used the SRILM toolkit (Stolcke, 2002) to train language models on the Xinhua portion of the
GIGAWORD corpus, which contains 238M English words. We trained a 4-gram language model
for our medium-scale experiments and a 5-gram language model for our large-scale experiments.
In order to train the two monolingual topic models on the source and target side of our bilingual
training data, we used the open source LDA tool GibbsLDA++.6 GibssLDA++ is an implementation
of LDA using gibbs sampling for parameter estimation and inference. The source- and targetside topic models were separately estimated from the Chinese and English part of the bilingual
training data. We set the number of topic K = 30 for both the source- and target-side topic models,
and used the default setting of the tool for training and inference.7 During decoding, we inferred
the document-topic distribution for each document in the dev/test sets before translation using the
trained source-side topic model. Note that the topic inference on the dev/test sets was performed
after all parameters of the two topic models were estimated on the training data.
The case-insensitive BLEU-4 was used as our evaluation metric. We performed the statistical
significance in BLEU differences using the paired bootstrap re-sampling (Koehn, 2004). In order
to alleviate the impact of the instability of MERT, we ran the tuning process three times for all our
large scale experiments and presented the average BLEU scores on the three runs following the
suggestion by Clark, Dyer, Lavie, and Smith (2011)
7.2 Medium-Scale Experiments
In this section, we conducted medium-scale experiments to investigate the effectiveness of our two
topic-based models for translation rule selection.
7.2.1 E FFECT

OF

T OPIC D ISSIMILARITY M ODEL

We quickly investigated the effectiveness of our topic dissimilarity and sensitivity model using
medium-scale training data. Results are shown in Table 3. From the table, we can observe that
 If we use the topic dissimilarity model only with the source-side or projected target-side ruletopic distributions (DsimSrc/DsimTrg in the table, see descriptions in Section 5), we can
obtain an absolute improvement of 0.48/0.38 B LEU points over the baseline.
 If we combine the two topic dissimilarity features together, we can achieve a further improvement of 0.16 B LEU points over DsimSrc.
These two observations show that our topic dissimilarity model is able to improve translation quality
in terms of B LEU.
6. http://gibbslda.sourceforge.net/
7. We determine K by testing {15, 30, 50, 100, 200} in our preliminary experiments. We find that K = 30 produces
a slightly better performance than other values. In order to improve the stability of the topic estimation, we run the
tool multiple times and use the best model with respect to the log-likelihood.

16

fiT OPIC -BASED D ISSIMILARITY

System
Baseline
TopicLex
DsimSrc
DsimTrg
DsimSrc+DsimTrg
Dsim+Sen

MT06
30.20
30.65
30.41
30.51
30.73
30.95

AND

S ENSITIVITY M ODELS

MT08
21.93
22.29
22.69
22.39
22.69
22.92

Avg
26.07
26.47
26.55
26.45
26.71
26.94

Speed
12.6
3.3
11.5
11.7
11.2
10.2

Table 3: Results of our topic dissimilarity and sensitivity model in terms of B LEU and speed (words
per second), comparing with the traditional hierarchical system (Baseline) and the system with the topic-specific lexicon translation model (TopicLex). DsimSrc and DsimTrg are topic dissimilarity features on the source-side and projected target-side ruletopic distributions respectively. Dsim+Sen activates both the two dissimilarity features
and the two sensitivity features as described in Section 6. Avg denotes average B LEU
scores on the two test sets. Scores in bold are significantly better than Baseline (p < 0.01).
Speed denotes the number of words translated per second.
Rule Type
Phrase
Monotone
Reordering
All

Count
3.9M
19.2M
5.7M
28.8M

Src-Sen(%)
83.4
85.3
85.9
85.1

Trg-Sen(%)
84.4
86.1
86.8
86.0

Table 4: Percentages of topic-sensitive rules listed by rule types according to entropies of their
source-side (Src) and target-side (Trg) rule-topic distributions. Phrase rules are fully
lexicalized, while monotone and reordering rules contain nonterminals.

In order to gain insights into why the topic dissimilarity model is helpful for translation rule
selection, we further investigate how many rules are topic-sensitive. As described in Section 4.2,
we use entropy to measure whether a translation rule is topic-sensitive based on its rule-topic distribution. If the entropy of a translation rule calculated by the equation (8) is smaller than a certain
threshold, the rule is topic-sensitive. Since documents often focus on a few topics, we use the average entropy of document-topic distributions of all training documents as the threshold. We compare
entropies of source-side and target-side rule-topic distributions against this threshold. Our findings
are shown in Table 4. 85.5% translation rules are topic-sensitive rules if we compare entropies of
their source-side rule-topic distributions against the threshold. If we compare entropies of targetside rule-topic distributions against the threshold, topic-sensitive rules account for 86%. These
strongly suggest that most rules only occur in documents with specific topics and topic information
can be used to improve translation rule selection.
7.2.2 E FFECT

OF

T OPIC S ENSITIVITY M ODEL

As we can see from Table 4, there are still about 15% translation rules which are generic, not sensitive to any topics. These rules are also widely used in documents. As mentioned before, our
17

fiZ HANG , X IAO , X IONG , & L IU

topic dissimilarity model always punishes such rules as documents are normally topic-specific. We
therefore introduce a topic sensitivity model to complement the topic dissimilarity model. The experiment result of this model is show in the last line of Table 3. We obtain a further improvement of
0.23 B LEU points when incorporating the topic sensitivity model. This indicates that it is necessary
to distinguish topic-insensitive and topic-sensitive rules.
7.2.3 C OMPARISON

WITH

T OPIC -S PECIFIC L EXICON T RANSLATION M ODEL

We also compared our topic models against the topic-specific lexicon translation model proposed by
Zhao and Xing (2007). They introduce a framework to combine Hidden Markov Model (HMM) and
LDA topic model for SMT, which is shown in Figure 2. In their framework, each bilingual sentence
pair has a single topic assignment sampled from the document-pair topic distribution . Then all
words of the target language (e.g., English) are sampled given the sentence-pair topic assignment
and a monolingual per-topic word distribution . After that, word alignments and words of the
source language are sampled from a first-order Markov process and a topic-specific translation
lexicon respectively.
Zhao and Xing integrate the topic-specific word-to-word translation lexicons estimated from
their bilingual topic model described above into the topic-specific lexicon translation model, which
is formulated as follows.
P (we |wf , df )  P (wf |we , df )P (we |df )
X
=
P (wf |we , z = k)P (we |z = k)P (z = k|df )

(14)

k

In this model, the probability of a candidate translation we for a source word wf in a source document df is calculated by marginalizing over all topics and corresponding topic-specific translation
lexicons. We simplify the estimation of p(wf |we , z = k) by directly computing these probabilities
on our word-aligned corpus associated with target-side topic assignments that are inferred from the
target-side topic model. Despite this simplification, the improvement of our implementation is comparable with the improvement obtained by Zhao and Xing (2007). Given a new document, we need
to adapt the lexical translation weights of rules. The adapted lexicon translation model is integrated
as a new feature into the log-linear discriminative framework.
We show the comparison results in Table 3. The topic-specific lexicon translation model is
better than the baseline by 0.4 B LEU points. However, our topic-based method (the combination of
topic dissimilarity and sensitivity models) outperforms the baseline by 0.87 B LEU points.
We also compare these two methods in terms of the decoding speed (words/second). The baseline translates 12.6 words per second, while the system with the topic-specific lexicon translation
model only translates 3.3 words in one second. The overhead of the topic-specific lexicon translation model mainly comes from the adaptation of lexical weights. It takes 72.8% of the time to do
the adaptation. In contrast, our method has a speed of 10.2 words per second for each sentence on
average, which is three times faster than the topic-specific lexicon translation method.
7.3 Large-Scale Experiments
In this section, we investigated deeper into our models with the second group of experiments on
large-scale training data.
18

fiT OPIC -BASED D ISSIMILARITY

7.3.1 E FFECT

OF

AND

S ENSITIVITY M ODELS

O NE - TO -M ANY P ROJECTION

As we discussed in Section 5.2, we need to project target-side topics onto source-side topic space
so as to calculate the dissimilarity between a target-side rule-topic distribution and a source-side
document-topic distribution. We propose a one-to-many projection method for this issue. In order
to investigate the effectiveness of this method, we conducted experiments with large-scale training
data to compare it with the following 3 other methods.
 One-to-One Mapping We enforce a one-to-one mapping between source-side and target-side
topics, similar to the method by Tam et al. (2007). We achieve this by aligning a target-side
topic to the corresponding source-side topic with the largest correspondence probability as
calculated in Section 5.2.
 Marginalization over Word Alignments Following Zhao and Xing (2007), we first obtain
topics on the target side using LDA and then retrieve topics of the source language through a
marginalization over word alignments as follows.
X
P (wf |k) =
P (wf |we )P (we |z = k)
(15)
we

 Combination of the source and target language documents We concatenate each target document and its aligned source document into one document. We then run the LDA tool on these
combined documents to train one topic model with mixed-language words. During decoding,
we use the trained topic model to infer topics only on source documents.
In order to compare our one-to-many projection method with the three methods described above,
we only add the target-side topic dissimilarity feature (DsimTrg) to the log-linear translation model.
The experiment results are reported in Table 5. Clearly, all the four methods achieve improvements
over the baseline. However, our one-to-many projection method performs better than all three other
methods. In particular,
 Our method outperforms the one-to-one topic mapping method, which indicates that sourceside and target-side topics do not exactly match in a one-to-one correspondence manner.
 The reason that the marginalization method performs the worse among the four methods may
be that the topic model is trained only on target documents.
 Surprisingly, the combination method performs quite well. This shows that the LDA model
can find hidden topics even on mixed-language documents.

7.3.2 E FFECT
RULES

OF THE

T OPIC -BASED RULE S ELECTION F RAMEWORK

ON

VARIOUS T YPES

OF

We conducted experiments to further investigate the effect of our topic-based models for various
types of rules selection. Particularly, we divide translation rules in hierarchical phrase-based SMT
into three types: 1) phrase rules, which only contain terminals and are the same as bilingual phrase
pairs used in phrase-based system; 2) monotone rules, which contain non-terminals and produce
19

fiZ HANG , X IAO , X IONG , & L IU

System
Baseline
One-to-One
Marginalization
Combination
One-to-Many

MT06
31.77
32.15
32.23
32.17
32.44

MT08
24.89
25.32
24.99
25.56
25.54

Avg
28.33
28.73
28.61
28.86
28.99

Table 5: Effect of our one-to-many topic projection method against other methods. Marginalization: Marginalization over Word Alignments; Combination: Combination of the source
and target language documents.
System
Baseline
Phrase rule
Monotone rule
Reordering rule
All

MT06
31.77
32.43
32.24
31.82
32.77

MT08
24.89
25.53
25.62
25.15
26.29

Avg
28.33
28.98
28.93
28.48
29.53

Table 6: Effect of our topic-based rule selection models on three types of rules. Phrase rules are
fully lexicalized, while monotone and reordering rules contain nonterminals.

monotone translations; and finally 3) reordering rules, which also contain non-terminals but change
the order of translations. We define the monotone and reordering rules according to Chiang et al.
(2008).
When we study the impact of our topic-based models on translation rule type A, we activate all
of the four features described in Section 6 only on those rules of type A. Topic dissimilarity and
sensitivity features on the other two types of translation rules are deactivated.
Table 6 shows the experiment results. From the table, we can observe that
 Our topic-based models achieve the highest improvement of 0.65 B LEU points over the baseline on phrase rules among the three types of translation rules. This is reasonable as phrase
rules consist of topical words.
 We also obtain improvements of 0.6 and 0.15 B LEU points over the baseline on the monotone
and reordering rules respectively. This shows that our models are also able to help select
appropriate translation rules with non-terminals.
 When we activate the topic dissimilarity and sensitivity models on all translation rules, we can
still achieve an additional improvement of 0.55 B LEU points. In total, our models outperform
the baseline by an absolute improvement of 1.2 B LEU points.
7.3.3 E FFECT

OF

M ORE M ONOLINGUAL DATA

Comparing Table 6 and Table 3, we find that our topic-based dissimilarity and sensitivity models
trained with medium-scale data (about 10K documents) collectively achieve an improvement of 0.87
20

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

System
Baseline
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg

MT06
31.77
32.77
32.70
32.37
32.61

MT08
24.89
26.29
25.91
25.80
25.66

Avg
28.33
29.53
29.31
29.09
29.13

Table 7: Effect of using more monolingual data to train topic models. The features in bold are the
topic-based dissimilarity/sensitivity model where the LDA topic model is trained using the
combination of source/target part of the large-scale bilingual data and the corresponding
monolingual corpus.

B LEU points over the baseline while the two models trained with large-scale data (about 100K documents) obtain an improvement of 1.2 B LEU points. This suggests that further performance gains
may be obtained if we have more data. As parallel bilingual data with document boundaries provided is not easily accessible, we try to collect monolingual data of the source or/and target language.
Our interest is to study whether we can gain further improvements by using more monolingual data
to train our topic models.
We used a Chinese monolingual corpus where documents were collected from the Chinese Sohu
weblog in 2009.8 The collected Chinese corpus contains 500K documents with 273.8M Chinese
words. We also used an English monolingual corpus where documents were collected from the
English Blog Authorship corpus (Schler, Koppel, Argamon, & Pennebaker, 2006). The English
monolingual corpus consists of 371K documents with 98M English words. We combined this new
Chinese corpus with the source part of our large-scale bilingual data to train a source-side LDA topic
model ST . The English monolingual corpus is also combined with the target part of the large-scale
bilingual data to train a target-side LDA topic model T T .
We then used the two topic models ST and T T to infer topics for the test sets. Topic information on the source and target part of the large-scale bilingual training data inferred by ST and T T
was used to estimate source-side rule-topic distributions and projected target-side rule-topic distributions. In this way, we can obtain a new topic-based dissimilarity and sensitivity model on the
source/target side.
Experiment results are shown in Table 7. Unfortunately, we can not obtain any further improvements by training topic models on larger data, such as the combination of Chinese monolingual
corpus and the source part of our bilingual training data. Instead, the performance drops from 29.53
to 29.31 if we use the topic model ST to build the source-side dissimilarity and sensitivity features
and to 29.09 if we adopt the topic model T T to build the target-side dissimilarity and sensitivity
features.
One reason for the lower performance with larger topic model training data may be that we only
use 30 topics. Using more topics may improve our models on these larger corpora. In order to
investigate this, we conducted new experiments with more topics than 30. We trained our sourceside topic model using the combination of source part of the large-scale bilingual data and the Sohu
weblog data. Based on this topic model, we built our source-side topic dissimilarity model and
8. http://blog.sohu.com/.

21

fiZ HANG , X IAO , X IONG , & L IU

System
Baseline
K = 30
K = 50
K = 100
K = 200

MT06
31.77
32.32
31.96
32.26
32.16

MT08
24.89
25.41
25.73
25.53
25.28

Avg
28.33
28.86
28.85
28.90
28.72

Table 8: Experiment results with different number of topics (K). Only the source-side topic dissimilarity model (DsimSrc) is integrated into the SMT system.
Test Set
MT06
MT08

MonoSrc
0.359
0.232

BiSrc
0.238
0.136

MonoBiSrc
0.297
0.261

Table 9: The Hellinger distances of the MT06/08 test sets to the Chinese monolingual corpus
(MonoSrc) and the source part of the bilingual training data (BiSrc) as well as their combination (MonoBiSrc) in terms of their average document-topic distributions.

integrated it into our SMT system. Experiment results are shown in Table 8. From this table, we
find that using more topics is not able to improve our model on these corpora.
Yet another reason may be that the additional monolingual corpus is not similar to the test sets in
terms of their topic distributions. In order to examine this hypothesis, we inferred document-topic
distributions for all documents in the test sets, the Chinese monolingual corpus and the source part of
the bilingual corpus using the topic model ST . We then average these document-topic distributions
and obtain four average document-topic distributions for MT06, MT08, the Chinese monolingual
corpus and the source part of the bilingual corpus respectively. These average topic distributions can be approximated as the corpus-topic distributions over the four corpora. We calculate the
Hellinger distances between the corpus-topic distributions of the test sets and those of the Chinese
monolingual corpus and the source part of the bilingual training data, which are shown in Table 9.
From the table, we can clearly find that the additional monolingual corpus is much less similar
to the test sets comparing with the bilingual training corpus. The Hellinger distance of the test
set MT08 to the MonoBiSrc corpus is almost twice as large as that to the bilingual training data
(0.261 vs. 0.136). A topic model trained on such an enlarged corpus will make our topic-based
models select translation rules that are not similar to the documents of the test sets in terms of topic
distributions. This suggests that we should select additional monolingual data that are similar to the
test sets if we want to obtain further improvements.
We further conducted a new group of experiments to empirically examine this hypothesis by
translating a web-domain test set that is similar to the additional weblog corpus in terms of their
topics. We used the web portion of the NIST MT06 set as our new development set and the web
portion of the NIST MT08 as the new test set. Results are displayed in Table 10, which show that
the additional monolingual data can improve the performance this time. This again suggests that we
should select monolingual corpus that is similar to our test sets to learn topics for our topic-based
dissimilarity and sensitivity models.
22

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

System
Baseline
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg

MT08-web
20.45
21.42
21.77

Table 10: Results of translating a web-domain test set with our topic-based models trained on the
data augmented with the monolingual weblog corpus. The features in bold are the topicbased dissimilarity/sensitivity model where the LDA topic model is trained using the
combination of source/target part of the large-scale bilingual data and the corresponding
monolingual corpus. MT08-web is the web portion of the NIST MT08 test set.

8. Analysis
In this section, we will study more details of our topic-based models for translation rule selection by
looking at the differences that they make on target documents and individual translation hypotheses.
These differences will help us gain some insights into how the presented models improve translation
quality. In the analysis, both the baseline system and the system that is enhanced with the proposed
topic-based models (all four features in Section 6 activated) are trained with the large-scale bilingual
data as described in Section 7.1. For notational convenience, hereafter we refer to the baseline
system as BASE and the system enhanced with our topic-based dissimilarity and sensitivity models
as TOPSEL.
8.1 Differences on Target Documents
In order to measure the impact that our topic-based models have on target documents, we calculate
the Hellinger distances between target documents generated by the BASE / TOPSEL system and reference documents generated by human in terms of their topics inferred by the target-side LDA topic
model according to the following 4 steps. The target-side LDA topic model is trained on the target
part of the large-scale bilingual data described in Section 7.1.
 Use the target-side LDA topic model to infer document-topic distribution for each document
in reference translations (called reference distribution).
 Use the target-side LDA topic model to infer document-topic distribution for each target document generated by the BASE system (called BASE distribution).
 Similarly, we can obtain
TOPSEL system.

TOPSEL

distribution on each target document generated by the

 Calculate the dissimilarity between the BASE and reference distribution as well as that between the TOPSEL and reference distribution according to the equation (6). These dissimilarities are first averaged on all documents and then averaged on four reference translations.
Table 11 shows the calculated dissimilarities. According to the equation (6), the smaller the
Hellinger distance between two items, the more similar they are. The average Hellinger distance
between TOPSEL and reference documents is 0.123 while the distance between BASE and reference
23

fiZ HANG , X IAO , X IONG , & L IU

System
BASE
TOPSEL

MT06
0.119
0.116

MT08
0.137
0.129

Avg
0.128
0.123

Table 11: Dissimilarities (measured by Hellinger distance) between reference documents and target
documents generated by the BASE and TOPSEL system in terms of their topics according
to the equation (6).

More similar (+)
Less similar (-)
p<

MT06
49
30
0.05

Table 12: The number of target documents generated by
erence documents than those by BASE.

MT08
72
37
0.01
TOPSEL

that are more/less similar to ref-

documents is 0.128. Therefore, the target documents generated by the TOPSEL system in both
MT06 and MT08 are more similar to the documents in reference translations than those by the
baseline system. We further calculate the number of target documents generated by TOPSEL that
are more/less similar to reference documents than those by BASE based on the average Hellinger
distances. These numbers are shown in Table 12. According to a sign test using these numbers,
TOPSEL is statistically significantly better than the baseline system in terms of the similarity of
translations generated by the two systems to human-generated translations.
8.2 Differences on Translation Hypotheses
We now look deeper into translation hypotheses to understand how our models select translation
rules. Table 13 shows three translation examples that compare the baseline against the system
enhanced with our topic-based models. In order to conduct a quantitative comparison, we calculate
dissimilarity values (measured by Hellinger distance) of all underlined phrases in Table 13 using
our topic-based dissimilarity model. The dissimilarity values are computed between the projected
target-side rule-topic distributions of the underlined phrases and the source-side document-topic
distributions of the corresponding documents where these phrase are used. The values are shown in
Table 14.
From the two tables, we can easily observe that the system with the topic-based dissimilarity
model prefers those target phrases that have smaller Hellinger distances to the documents where they
occur in terms of topic distributions. In contrast, the baseline is not able to use this document-level
topic information for translation rule selection. Figure 3 further shows the topic distributions of the
source-side document, the TOPSEL phrase allow and the BASE phrase permit in Eg. 2. The
major topics of the source-side document are topic 12 and 36. The TOPSEL phrase allow mainly
distributes over 12 different topics9 including topic 12 and 36 while the BASE phrase permit
mainly over 10 different topics which do not include topic 12.
9. The distribution probability over these topics is larger than 0.03.

24

fiT OPIC -BASED D ISSIMILARITY

Source

S ENSITIVITY M ODELS

  7    8   { "

he described the  northern limit line  and unlawful .
he referred to the  northern limit line  is not legitimate .
Reference he pointed out that the  northern limit line  is not legitimate .
Source
BASE
how would permit its love others also accepted by the people .
TOPSEL
will allow their love of love others also accepted by the people
Reference how would someone allow the person he loves to accept other peoples
love at the same time
Source
BASE
at present , the internet is not entitled to such a statutory right to leave
TOPSEL
at present the internet does not enjoy such a statutory right to leave
Reference at present the internet does not enjoy these rights
BASE

Eg. 1

AND

TOPSEL

No  #N gC O  <    O<  O

Eg. 2

8 p  k  { N  |

Eg. 3

Table 13: Translation examples from the NIST MT06/08 test sets, comparing the baseline with
the system enhanced with the topic-based models. The underlined words highlight the
difference between the enhanced models and the baseline.
Phrase
unlawful
not legitimate
permit
allow
entitled to
enjoy

HD
3.08
2.27
3.75
3.47
3.45
3.24

Table 14: Dissimilarity values (measured by Hellinger distance) of the underlined phrases in Table 13 between their projected target-side rule-topic distributions and the corresponding
source-side document-topic distributions of documents calculated by our topic-based dissimilarity model.

9. Discussion on Bilingual Topic Modeling
Although topic models are widely adopted in monolingual text analysis, bilingual or multilingual
topic models are less explored, especially those tailored for multilingual tasks such as machine
translation. In this section we try to provide some suggestions for bilingual topic modeling from
the perspective of statistical machine translation as well as our practice on the integration of topic models into SMT. These suggestions are listed as follows, some of which are also our future
directions.
 Investigation on Topic divergences across different languages Cross-language divergences
are pervasive and become one of big challenges for machine translation (Dorr, 1994). Such
language-level divergences hint that divergences at the topic or concept level may also exist
across languages. This may explain why our one-to-many topic projection from the target side
25

fiZ HANG , X IAO , X IONG , & L IU

Figure 3: Topic distributions of the source-side document (a), the
the BASE phrase permit shown in Eg. 2 of Table 13.

TOPSEL

phrase allow (b) and

to the source side is better than the one-to-one mapping. Although Mimno et al. (2009) have
studied on topic divergences using Wikipedia articles, we believe that a deeper and wider
investigation on topic divergence is needed as it will shed new light on how we can build
better bilingual topic models.
 Adding more linguistic assumptions into topic modeling Practices in SMT show that integrating more linguistic knowledge into machine translation normally generates better translations
(Chiang et al., 2008). We believe that adding more linguistic assumptions beyond bag-ofwords will also improve topic modeling. A flexible topic modeling framework that allows us
to integrate rich linguistic knowledge in the form of features will definitely further facilitate
the application of topic models in natural language processing.
 Joint modeling of topic induction and synchronous grammar induction Synchronous grammar induction for machine translation is a task of automatically learning translation rules from
bilingual data (Blunsom, Cohn, Dyer, & Osborne, 2009; Xiao & Xiong, 2013). As Bayesian
approaches are successfully used in both topic modeling and synchronous grammar induction, joint modeling of them is an very interesting direction, which will also benefit grammar
adaptation from one domain to another domain in machine translation.

10. Conclusions
In this article we have presented a topic-based translation rule selection framework which incorporates the topic information from both the source and target language for translation rule disambiguation. Particularly, we use a topic dissimilarity model to select appropriate translation rules for
documents according to the similarities between translation rules and documents. We also adopt a
26

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

topic sensitivity model to complement the topic dissimilarity model in order to balance translation
rule selection between topic-sensitive and topic-insensitive rules. In order to calculate dissimilarities between source- and target-side topic distributions, we project topic distributions on the target
side onto the source-side topic model space in a new and efficient way.
We have integrated our topic-based rule selection models into a hierarchical phrase-based SMT
system. Experiments on medium/large-scale training data show that
 Our topic dissimilarity and sensitivity model are able to substantially improve translation
quality in terms of B LEU and improve translation rule selection on various types of rules (i.e.,
phrase/monotone/reordering rules).
 Our method is better than previous topic-specific lexicon translation method in both translation quality and decoding speed.
 The proposed one-to-many projection method also outperforms various other methods such
as one-to-one mapping, marginalization via word alignments and so on.
 If we want to use additional monolingual corpus to train topic models, we should first investigate whether the new monolingual corpus is similar to the test data in terms of topic
distributions.
Topic models can provide global and document-level information for machine translation. In
the future, we would like to use topic models to address document-level machine translation issues,
such as coherence and cohesion (Barzilay & Lee, 2004; Hardmeier, Nivre, & Tiedemann, 2012). We
also want to integrate our topic-based models into linguistically syntax-based machine translation
for syntactic translation rule selection (Liu et al., 2006).

Acknowledgments
The work was sponsored by the National Natural Science Foundation of China under projects
61373095 and 61333018. Qun Lius work was partially supported by Science Foundation Ireland
(Grant No. 07/CE/I1142) as part of the CNGL at Dublin City University. We would like to thank
three anonymous reviewers for their insightful comments. The corresponding author of this article
is Deyi Xiong.

References
Barzilay, R., & Lee, L. (2004). Catching the drift: Probabilistic content models, with applications to
generation and summarization. In Susan Dumais, D. M., & Roukos, S. (Eds.), HLT-NAACL
2004: Main Proceedings, pp. 113120, Boston, Massachusetts, USA. Association for Computational Linguistics.
Blei, D. M., & Lafferty, J. D. (2007). A correlated topic model of science. AAS, 1(1), 1735.
Blei, D. M., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. JMLR, 3, 9931022.

27

fiZ HANG , X IAO , X IONG , & L IU

Blunsom, P., Cohn, T., Dyer, C., & Osborne, M. (2009). A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP, pp. 782790, Suntec, Singapore. Association for Computational Linguistics.
Boyd-Graber, J., & Blei, D. M. (2009). Multilingual topic models for unaligned text. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 09, pp. 7582,
Arlington, Virginia, United States. AUAI Press.
Carpuat, M., & Wu, D. (2007a). How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine Translation, pp. 4352.
Carpuat, M., & Wu, D. (2007b). Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp.
6172, Prague, Czech Republic. Association for Computational Linguistics.
Chan, Y. S., Ng, H. T., & Chiang, D. (2007). Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 3340, Prague, Czech Republic. Association for Computational
Linguistics.
Chiang, D. (2005). A hierarchical phrase-based model for statistical machine translation. In Proc.
ACL 2005.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33(2), 201
228.
Chiang, D., Marton, Y., & Resnik, P. (2008). Online large-margin training of syntactic and structural
translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 224233, Honolulu, Hawaii. Association for Computational
Linguistics.
Clark, J. H., Dyer, C., Lavie, A., & Smith, N. A. (2011). Better hypothesis testing for statistical
machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies,
pp. 176181, Portland, Oregon, USA.
Dorr, B. J. (1994). Machine translation divergences: A formal description and proposed solution.
Computational Linguistics, 20(4), 597633.
Foster, G., & Kuhn, R. (2007). Mixture-model adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pp. 128135, Prague, Czech Republic.
Gong, Z., Zhang, M., & Zhou, G. (2011). Cache-based document-level statistical machine translation. In Proc. EMNLP 2011.
Gong, Z., Zhang, Y., & Zhou, G. (2010). Statistical machine translation based on LDA. In Proc.
IUCS 2010, p. 286 290.



Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1), 52285235.

28

fiT OPIC -BASED D ISSIMILARITY

AND

S ENSITIVITY M ODELS

Hardmeier, C., Nivre, J., & Tiedemann, J. (2012). Document-wide decoding for phrase-based statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning,
pp. 11791190, Jeju Island, Korea. Association for Computational Linguistics.
He, Z., Liu, Q., & Lin, S. (2008). Improving statistical machine translation using lexicalized rule
selection. In Proceedings of the 22nd International Conference on Computational Linguistics
(Coling 2008), pp. 321328, Manchester, UK. Coling 2008 Organizing Committee.
Hofmann, T. (1999). Probabilistic latent semantic analysis. In Proc. of UAI 1999, pp. 289296.
Koehn, P. (2004). Statistical significance tests for machine translation evaluation. In Proceedings
of EMNLP 2004, pp. 388395, Barcelona, Spain.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. In Proc. HLT-NAACL
2003.
Lin, J. (2006). Divergence measures based on the Shannon entropy. IEEE Trans. Inf. Theor., 37(1),
145151.
Liu, Q., He, Z., Liu, Y., & Lin, S. (2008). Maximum entropy based rule selection model for syntaxbased statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 8997, Honolulu, Hawaii. Association for
Computational Linguistics.
Liu, Y., Liu, Q., & Lin, S. (2006). Tree-to-string alignment template for statistical machine translation. In Proc. ACL 2006.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual
topic models. In Proc. of EMNLP 2009.
Och, F. J., & Ney, H. (2002). Discriminative training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Och, F. J. (2003). Minimum error rate training in statistical machine translation. In Proc. ACL 2003.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: a method for automatic evaluation
of machine translation. In Proc. ACL 2002.
Ruiz, N., & Federico, M. (2011). Topic adaptation for lecture translation through bilingual latent
semantic models. In Proceedings of the Sixth Workshop on Statistical Machine Translation.
Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. W. (2006). Effects of age and gender on
blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, pp.
199205.
Stolcke, A. (2002). SRILM  an extensible language modeling toolkit. In Proc. ICSLP 2002.
Tam, Y.-C., Lane, I. R., & Schultz, T. (2007). Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4), 187207.
Tiedemann, J. (2010). Context adaptation in statistical machine translation using models with exponentially decaying cache. In Proceedings of the 2010 Workshop on Domain Adaptation for

29

fiZ HANG , X IAO , X IONG , & L IU

Natural Language Processing, pp. 815, Uppsala, Sweden. Association for Computational
Linguistics.
Ture, F., Oard, D. W., & Resnik, P. (2012). Encouraging consistent translation choices. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 417426, Montreal, Canada. Association for Computational Linguistics.
Xiao, T., Zhu, J., Yao, S., & Zhang, H. (2011). Document-level consistency verification in machine
translation. In Proceedings of the 2011 MT summit XIII, pp. 131138, Xiamen, China.
Xiao, X., & Xiong, D. (2013). Max-margin synchronous grammar induction for machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pp. 255264, Seattle, Washington, USA. Association for Computational Linguistics.
Xiao, X., Xiong, D., Zhang, M., Liu, Q., & Lin, S. (2012). A topic similarity model for hierarchical phrase-based translation. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 750758, Jeju Island, Korea.
Association for Computational Linguistics.
Xiong, D., Zhang, M., & Li, H. (2012). Modeling the translation of predicate-argument structure
for SMT. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 902911, Jeju Island, Korea. Association for Computational Linguistics.
Zhao, B., & Xing, E. P. (2007). HM-BiTAM: Bilingual topic exploration, word alignment, and
translation. In Proc. NIPS 2007.
Zhao, B., & Xing, E. P. (2006). BiTAM: Bilingual topic admixture models for word alignment. In
Proc. ACL 2006.

30

fiJournal of Artificial Intelligence Research 50 (2014) 723762

Submitted 12/13; published 08/14

Sentiment Analysis of Short Informal Texts
Svetlana Kiritchenko
Xiaodan Zhu
Saif M. Mohammad

Svetlana.Kiritchenko@nrc-cnrc.gc.ca
Xiaodan.Zhu@nrc-cnrc.gc.ca
Saif.Mohammad@nrc-cnrc.gc.ca

National Research Council Canada
1200 Montreal Rd., Ottawa, ON, Canada

Abstract
We describe a state-of-the-art sentiment analysis system that detects (a) the sentiment
of short informal textual messages such as tweets and SMS (message-level task) and (b)
the sentiment of a word or a phrase within a message (term-level task). The system is
based on a supervised statistical text classification approach leveraging a variety of surfaceform, semantic, and sentiment features. The sentiment features are primarily derived from
novel high-coverage tweet-specific sentiment lexicons. These lexicons are automatically
generated from tweets with sentiment-word hashtags and from tweets with emoticons. To
adequately capture the sentiment of words in negated contexts, a separate sentiment lexicon
is generated for negated words.
The system ranked first in the SemEval-2013 shared task Sentiment Analysis in Twitter (Task 2), obtaining an F-score of 69.02 in the message-level task and 88.93 in the
term-level task. Post-competition improvements boost the performance to an F-score of
70.45 (message-level task) and 89.50 (term-level task). The system also obtains state-ofthe-art performance on two additional datasets: the SemEval-2013 SMS test set and a
corpus of movie review excerpts. The ablation experiments demonstrate that the use of
the automatically generated lexicons results in performance gains of up to 6.5 absolute
percentage points.

1. Introduction
Sentiment Analysis involves determining the evaluative nature of a piece of text. For example, a product review can express a positive, negative, or neutral sentiment (or polarity).
Automatically identifying sentiment expressed in text has a number of applications, including tracking sentiment towards products, movies, politicians, etc., improving customer
relation models, detecting happiness and well-being, and improving automatic dialogue systems. Over the past decade, there has been a substantial growth in the use of microblogging
services such as Twitter and access to mobile phones world-wide. Thus, there is tremendous
interest in sentiment analysis of short informal texts, such as tweets and SMS messages,
across a variety of domains (e.g., commerce, health, military intelligence, and disaster management).
Short informal textual messages bring in new challenges to sentiment analysis. They
are limited in length, usually spanning one sentence or less. They tend to have many
misspellings, slang terms, and shortened forms of words. They also have special markers
such as hashtags that are used to facilitate search, but can also indicate a topic or sentiment.
This paper describes a state-of-the-art sentiment analysis system addressing two tasks:
(a) detecting the sentiment of short informal textual messages (message-level task) and
c
2014
National Research Council Canada. All rights reserved.

fiKiritchenko, Zhu, & Mohammad

(b) detecting the sentiment of a word or a phrase within a message (term-level task).
The system is based on a supervised statistical text classification approach leveraging a
variety of surface-form, semantic, and sentiment features. Given only limited amounts of
training data, statistical sentiment analysis systems often benefit from the use of manually or
automatically built sentiment lexicons. Sentiment lexicons are lists of words (and phrases)
with prior associations to positive and negative sentiments. Some lexicons can additionally
provide a sentiment score for a term to indicate its strength of evaluative intensity. Higher
scores indicate greater intensity. For example, an entry great (positive, 1.2) states that
the word great has positive polarity with the sentiment score of 1.2. An entry acceptable
(positive, 0.1) specifies that the word acceptable has positive polarity and its intensity is
lower than that of the word great.
In our sentiment analysis system, we utilize three freely available, manually created,
general-purpose sentiment lexicons. In addition, we generated two high-coverage tweetspecific sentiment lexicons from about 2.5 million tweets using sentiment markers within
them. These lexicons automatically capture many peculiarities of the social media language
such as common intentional and unintentional misspellings (e.g., gr8, lovin, coul, holys**t),
elongations (e.g., yesssss, mmmmmmm, uugghh), and abbreviations (e.g., lmao, wtf ). They
also include words that are not usually considered to be expressing sentiment, but that are
often associated with positive/negative feelings (e.g., party, birthday, homework ).
Sentiment lexicons provide knowledge on prior polarity (positive, negative, or neutral)
of a word, i.e., its polarity in most contexts. However, in a particular context this prior
polarity can change. One such obvious contextual sentiment modifier is negation. In a
negated context, many words change their polarity or at least the evaluative intensity. For
example, the word good is often used to express positive attitude whereas the phrase not
good is clearly negative. A conventional way of addressing negation in sentiment analysis
is to reverse the polarity of a word, i.e. change a words sentiment score from s to s
(Kennedy & Inkpen, 2005; Choi & Cardie, 2008). However, several studies have pointed
out the inadequacy of this solution (Kennedy & Inkpen, 2006; Taboada, Brooke, Tofiloski,
Voll, & Stede, 2011). We will show through experiments in Section 4.3 that many positive
terms, though not all, tend to reverse their polarity when negated, whereas most negative
terms remain negative and only change their evaluative intensity. For example, the word
terrible conveys a strong negative sentiment whereas the phrase wasnt terrible is mildly
negative. Also, the degree of the intensity shift varies from term to term for both positive and negative terms. To adequately capture the effects of negation on different terms,
we propose a corpus-based statistical approach to estimate sentiment scores of individual
terms in the presence of negation. We build two lexicons: one for words in negated contexts
(Negated Context Lexicon) and one for words in affirmative (non-negated) contexts (Affirmative Context Lexicon). Each word (or phrase) now has two scores, one in the Negated
Context Lexicon and one in the Affirmative Context Lexicon. When analyzing the sentiment of a textual message, we use scores from the Negated Context Lexicon for words
appearing in a negated context and scores from the Affirmative Context Lexicon for words
appearing in an affirmative context.
Experiments are carried out to asses both, the performance of the overall sentiment
analysis system as well as the quality and value of the automatically created tweet-specific
lexicons. In the intrinsic evaluation of the lexicons, their entries are compared with the
724

fiSentiment Analysis of Short Informal Texts

entries of the manually created lexicons. Also, human annotators were asked to rank a
subset of lexicon entries by the degree of their association with positive or negative sentiment
and this ranking is compared with the ranking produced by an automatic lexicon. In
both experiments we observe high agreement between the automatic and manual sentiment
annotations.
The extrinsic evaluation is performed on two tasks: unsupervised and supervised sentiment analysis. On the supervised task, we assess the performance of the full sentiment
analysis system and examine the impact of the features derived from the automatic lexicons
on the overall performance. As a testbed, we use the datasets provided for the SemEval2013 competition on Sentiment Analysis in Twitter (Wilson, Kozareva, Nakov, Rosenthal,
Stoyanov, & Ritter, 2013).1 There were datasets provided for two tasks, message-level task
and term-level task, and two domains, tweets and SMS. However, the training data were
available only for tweets. Among 77 submissions from 44 teams, our system placed first
in the competition in both tasks on the tweet test set, obtaining a macro-averaged F-score
of 69.02 in the message-level task and 88.93 in the term-level task. Post-competition improvements to the system boost the performance to an F-score of 70.45 (message-level task)
and 89.50 (term-level task). We also applied our classifier on the SMS test set without
any further tuning. The classifier obtained the first position in identifying sentiment of
SMS messages (F-score of 68.46) and the second position in detecting the sentiment of
terms within SMS messages (F-score of 88.00; only 0.39 points behind the first-ranked system). With post-competition improvements, the system achieves an F-score of 69.77 in the
message-level task and an F-score of 88.20 in the term-level task on that test set.
In addition, we evaluate the performance of our sentiment analysis system on the domain
of movie review excerpts (message-level task only). The system is re-trained on a collection
of about 7,800 positive and negative sentences extracted from movie reviews. When applied
on the test set of unseen sentences, the system is able to correctly classify 85.5% of the test
set. This result exceeds the best result obtained on this dataset by a recursive deep learning
approach that requires access to sentiment labels of all syntactic phrases in the trainingdata sentences (Socher, Perelygin, Wu, Chuang, Manning, Ng, & Potts, 2013). For the
message-level task, we do not make use of sentiment labels of phrases in the training data,
as that is often unavailable in real-world applications.
The ablation experiments reveal that the automatically built lexicons gave our system
the competitive advantage in SemEval-2013. The use of the new lexicons results in gains
of up to 6.5 percentage points over the gains obtained through the use of other features.
Furthermore, we show that the lexicons built specifically for negated contexts better model
negation than the reversing polarity approach.
The main contributions of this paper are three-fold. First, we present a sentiment
analysis system that achieves state-of-the-art performance on three domains: tweets, SMS,
and movie review excerpts. The system can be replicated using freely available resources.
Second, we describe the process of creating the automatic, tweet-specific lexicons and
demonstrate their superior predictive power over several manually and automatically created general-purpose lexicons. Third, we analyze the impact of negation on sentiment and
propose an empirical method to estimate the sentiment of words in negated contexts by
1. SemEval is an international forum for natural-language shared tasks. The competition we refer to is
SemEval-2013 Task 2 (http://www.cs.york.ac.uk/semeval-2013/task2).

725

fiKiritchenko, Zhu, & Mohammad

creating a separate sentiment lexicon for negated words. All automatic lexicons described
in the paper are made available to the research community.2
The paper is organized as follows. We begin with a description of related work in Section 2. Next, we describe the sentiment analysis task and the data used in this research
(Section 3). Section 4 presents the sentiment lexicons used in our system: existing manually
created, general-purpose lexicons (Section 4.1) and our automatic, tweet-specific lexicons
(Section 4.2). The lexicons built for affirmative and negated contexts are described in Section 4.3. The detailed description of our supervised sentiment analysis system, including
the classification method and the feature sets, is presented in Section 5. Section 6 provides
the results of the evaluation experiments. First, we compare the automatically created lexicons with human annotations derived from the manual lexicons as well as collected through
Amazons Mechanical Turk service3 (Section 6.1). Next, we evaluate the new lexicons on
the extrinsic task of unsupervised sentiment analysis (Section 6.2.1). The purpose of these
experiments is to compare the predictive capacity of the individual lexicons without influence of other factors. Then, in Section 6.2.2 we assess the performance of the entire
supervised sentiment analysis system and examine the contribution of the features derived
from our lexicons to the overall performance. Finally, we conclude and present directions
for future work in Section 7.

2. Related Work
Over the last decade, there has been an explosion of work exploring various aspects of
sentiment analysis: detecting subjective and objective sentences; classifying sentences as
positive, negative, or neutral; detecting the person expressing the sentiment and the target
of the sentiment; detecting emotions such as joy, fear, and anger; visualizing sentiment
in text; and applying sentiment analysis in health, commerce, and disaster management.
Surveys by Pang and Lee (2008) and Liu and Zhang (2012) give a summary of many of
these approaches.
Sentiment analysis systems have been applied to many different kinds of texts including
customer reviews, news paper headlines (Bellegarda, 2010), novels (Boucouvalas, 2002;
John, Boucouvalas, & Xu, 2006; Francisco & Gervas, 2006; Mohammad & Yang, 2011),
emails (Liu, Lieberman, & Selker, 2003; Mohammad & Yang, 2011), blogs (Neviarouskaya,
Prendinger, & Ishizuka, 2011; Genereux & Evans, 2006; Mihalcea & Liu, 2006), and tweets
(Mohammad, 2012). Often these systems have to cater to the specific needs of the text
such as formality versus informality, length of utterances, etc. Sentiment analysis systems
developed specifically for tweets include those by Pak and Paroubek (2010), Agarwal, Xie,
Vovsha, Rambow, and Passonneau (2011), Thelwall, Buckley, and Paltoglou (2011), Brody
and Diakopoulos (2011), Aisopos, Papadakis, Tserpes, and Varvarigou (2012), Bakliwal,
Arora, Madhappan, Kapre, Singh, and Varma (2012). A recent survey by Martnez-Camara,
Martn-Valdivia, Urenalopez, and Montejoraez (2012) provides an overview of the research
on sentiment analysis of tweets.
Several manually created sentiment resources have been successfully applied in sentiment
analysis. The General Inquirer has sentiment labels for about 3,600 terms (Stone, Dunphy,
2. www.purl.com/net/sentimentoftweets
3. https://www.mturk.com/mturk/welcome

726

fiSentiment Analysis of Short Informal Texts

Smith, Ogilvie, & associates, 1966). Hu and Liu (2004) manually labeled about 6,800
words and used them for detecting sentiment of customer reviews. The MPQA Subjectivity
Lexicon, which draws from the General Inquirer and other sources, has sentiment labels for
about 8,000 words (Wilson, Wiebe, & Hoffmann, 2005). The NRC Emotion Lexicon has
sentiment and emotion labels for about 14,000 words (Mohammad & Turney, 2010). These
labels were compiled through Mechanical Turk annotations.
Semi-supervised and automatic methods have also been proposed to detect the polarity
of words. Hatzivassiloglou and McKeown (1997) proposed an algorithm to determine the
polarity of adjectives. SentiWordNet (SWN) was created using supervised classifiers as well
as manual annotation (Esuli & Sebastiani, 2006). Turney and Littman (2003) proposed a
minimally supervised algorithm to calculate the polarity of a word by determining if its
tendency to co-occur with a small set of positive seed words is greater than its tendency
to co-occur with a small set of negative seed words. Mohammad, Dunne, and Dorr (2009)
automatically generated a sentiment lexicon of more than 60,000 words from a thesaurus.
We use several of these lexicons in our system. In addition, we create two new sentiment
lexicons from tweets using hashtags and emoticons. In Section 6, we show that these tweetspecific lexicons have a higher coverage and a better predictive power than the lexicons
mentioned earlier.
Since manual annotation of data is costly, distant supervision techniques have been actively applied in the domain of short informal texts. User-provided indications of emotional
content, such as emoticons, emoji, and hashtags, have been used as noisy sentiment labels.
For example, Go, Bhayani, and Huang (2009) use tweets with emoticons as labeled data for
supervised training. Emoticons such as :) are considered positive labels of the tweets and
emoticons such as :( are used as negative labels. Davidov, Tsur, and Rappoport (2010) and
Kouloumpis, Wilson, and Moore (2011) use certain seed hashtag words such as #cute and
#sucks as labels of positive and negative sentiment. Mohammad (2012) developed a classifier to detect emotions using tweets with emotion word hashtags (e.g., #anger, #surprise)
as labeled data.
In our system too, we make use of the emoticons and hashtag words as signals of positive
and negative sentiment. We collected 775,000 sentiment-word hashtagged tweets and used
1.6 million emoticon tweets collected by Go et al. (2009). However, unlike previous research,
we generate sentiment lexicons from these datasets and use them (along with a relatively
small hand-labeled training dataset) to train a supervised classifier. This approach has the
following benefits. First, it allows us to incorporate large amounts of noisily labeled data
quickly and efficiently. Second, the classification system is robust to the introduced noise
because the noisy data are incorporated not directly as training instances but indirectly
as features. Third, the generated sentiment lexicons can be easily distributed among the
research community and employed in other applications and on other domains (Kiritchenko,
Zhu, Cherry, & Mohammad, 2014).
Negation plays an important role in determining sentiment. Automatic negation handling involves identifying a negation word such as not, determining the scope of negation
(which words are affected by the negation word), and finally appropriately capturing the
impact of the negation. (See work by Jia, Yu, and Meng (2009), Wiegand, Balahur, Roth,
Klakow, and Montoyo (2010), Lapponi, Read, and Ovrelid (2012) for detailed analyses of
negation handling.) Traditionally, the negation word is determined from a small hand727

fiKiritchenko, Zhu, & Mohammad

crafted list (Taboada et al., 2011). The scope of negation is often assumed to begin from
the word following the negation word until the next punctuation mark or the end of the
sentence (Polanyi & Zaenen, 2004; Kennedy & Inkpen, 2005). More sophisticated methods
to detect the scope of negation through semantic parsing have also been proposed (Li, Zhou,
Wang, & Zhu, 2010).
A common way to capture the impact of negation is to reverse the polarities of the
sentiment words in the scope of negation. Taboada et al. (2011) proposed to shift the
sentiment score of a term in a negated context towards the opposite polarity by a fixed
amount. However, in their experiments the shift-score model did not agree with human
judgment in many cases, especially for negated negative terms. More complex approaches,
such as recursive deep models, address negation through semantic composition (Socher,
Huval, Manning, & Ng, 2012; Socher et al., 2013). The recursive deep models work in a
bottom-top fashion over a parse-tree structure of a sentence to infer the sentiment label of
the sentence as a composition of the sentiment expressed by its constituting parts: words
and phrases. These models do not require any hand-crafted features or semantic knowledge,
such as a list of negation words. However, they are computationally intensive and need
substantial additional annotations (word and phrase-level sentiment labeling) to produce
competitive results (Socher et al., 2013). In this paper, we propose a simple corpus-based
statistical method to estimate the sentiment scores of negated words. As will be shown in
Section 6.2.2, this simple method is able to achieve the same level of accuracy as the recursive
deep learning approach. Additionally, we analyze the impact of negation on sentiment scores
of common sentiment terms.
To promote research in sentiment analysis of short informal texts and to establish a
common ground for comparison of different approaches, an international competition was
organized by the Conference on Semantic Evaluation Exercises (SemEval-2013) (Wilson
et al., 2013). The organizers created and shared tweets for training, development, and
testing. They also provided a second test set consisting of SMS messages. The purpose of
having this out-of-domain test set was to assess the ability of the systems trained on tweets
to generalize to other types of short informal texts. The competition attracted 44 teams;
there were 48 submissions from 34 teams in the message-level task and 29 submissions from
23 teams in the term-level task. Most participants (including the top 3 systems in each task)
chose a supervised machine learning approach exploiting a variety of features derived from
ngrams, stems, punctuation, POS tags, and Twitter-specific encodings (e.g., emoticons,
hashtags, abbreviations). Only one of the top-performing systems was entirely rule-based
with hand-written rules (Reckman, Baird, Crawford, Crowell, Micciulla, Sethi, & Veress,
2013). Twitter-specific pre-processing (e.g., tokenization, normalization) as well as negation
handling were commonly applied. Almost all systems benefited from sentiment lexicons:
MPQA Subjectivity Lexicon, SentiWordNet, and others. Existing, low-coverage lexicons
were sometimes extended with distributionally similar words (Proisl, Greiner, Evert, &
Kabashi, 2013) or sentiment-associated words collected from noisily labeled data (Becker,
Erhart, Skiba, & Matula, 2013). Those extended lexicons, however, were still an order of
magnitude smaller than the tweet-specific lexicons we created. For the full results of the
competition and further details we refer the reader to Wilson et al. (2013).
Some research approaches sentiment analysis as a two-tier problem: first a piece of text
is marked as either objective or subjective, and then only the subjective text is assessed
728

fiSentiment Analysis of Short Informal Texts

to determine whether it is positive, negative, or neutral (Wiebe, Wilson, & Cardie, 2005;
Choi & Cardie, 2010; Johansson & Moschitti, 2013; Yang & Cardie, 2013). However, this
can lead to a propagation of errors (for example, the system may mark a subjective text
as objective). Further, one can argue that even objective statements can express sentiment
(for example, the sales of Blackberries are 0.002% of what they used to be 5 years back).
We model sentiment directly as a three-class problem: positive, negative, or neutral.
Also, this paper focuses on sentiment analysis alone and does not consider the task
of associating the sentiment with its targets. There has been interesting work studying
the latter problem (e.g., Jiang, Yu, Zhou, Liu, & Zhao, 2011; Sauper & Barzilay, 2013).
In (Kiritchenko et al., 2014) we show how our approach can be adapted to identify the
sentiment for a specified target. The system ranked first in the SemEval-2014 shared task
Aspect Based Sentiment Analysis.

3. Task and Data Description
In this work, we follow the definition of the task and use the data provided for the SemEval2013 competition: Sentiment Analysis in Twitter (Wilson et al., 2013). This competition
had two tasks: a message-level task and a term-level task. The objective of the messagelevel task is to detect whether the whole message conveys a positive, negative, or neutral
sentiment. The objective of the term-level task is to detect whether a given target term (a
single word or a multi-word expression) conveys a positive, negative, or neutral sentiment
in the context of a message. Note that the same term may express different sentiments
in different contexts. For example, the word unpredictable expresses positive sentiment in
sentence The movie has an unpredictable ending; whereas, it expresses negative sentiment
in sentence The car has unpredictable steering.
Two test sets  one with tweets and one with SMS messages  were provided to the
participants for each task. Training and development data were available only for tweets.
Here we briefly describe how the data were collected and annotated (for more details see
the task description paper (Wilson et al., 2013)). Tweets were collected through the public
streaming Twitter API during a period of one year: from January 2012 to January 2013. To
reduce the data skew towards the neutral class, messages that did not contain any polarity
word listed in SentiWordNet 3.0 were discarded. The remaining messages were annotated for
sentiment through Mechanical Turk.4 Each annotator had to mark the positive, negative,
and neutral parts of a message as well as to provide the overall polarity label for the message.
Later, the annotations were combined through intersection for the term-level task and by
majority voting for the message-level task. The details on data collection and annotation
were released to the participants after the competition.
The data characteristics for both tasks are shown in Table 1. The training set was
distributed through tweet ids and a download script. However, not all tweets were accessible.
For example, a Twitter user could have deleted her messages, and thus these messages
would not be available. Table 1 shows the number of the training examples we were able
to download. The development and test sets were provided in full by FTP.
4. Messages presented to annotators did not have polarity words marked in any way.

729

fiKiritchenko, Zhu, & Mohammad

Table 1: Data statistics for the SemEval-2013 training set, development set and two testing
sets. # of tokens per mess. denotes the average number of tokens per message
in the dataset. Vocab. size represents the number of unique tokens excluding
punctuation and numerals.

Dataset
Positive
Message-level task:
Training set
3,045 (37%)
Development set
575 (35%)
Tweet test set
1,572 (41%)
SMS test set
492 (23%)
Term-level task:
Training set
4,831 (62%)
Development set
648 (57%)
Tweet test set
2,734 (62%)
SMS test set
1,071 (46%)

Number of instances
Negative
Neutral
1,209
340
601
394

(15%)
(20%)
(16%)
(19%)

2,540
430
1,541
1,104

(33%)
(38%)
(35%)
(47%)

4,004
739
1,640
1,208

(48%)
(45%)
(43%)
(58%)

385
57
160
159

(5%)
(5%)
(3%)
(7%)

Total

# tokens
per mess.

Vocab.
size

8,258
1,654
3,813
2,094

22.09
22.19
22.15
18.05

21,848
6,543
12,977
3,513

7,756
1,135
4,435
2,334

22.55
22.93
22.63
19.95

15,238
3,909
10,383
2,979

The tweets are comprised of regular English-language words as well as Twitter-specific
terms, such as emoticons, URLs, and creative spellings. Using WordNet 3.05 (147,278
word types) supplemented with a large list of stop words (571 words)6 as a repository of
English-language words, we found that about 45% of the vocabulary in the tweet datasets
are out-of-dictionary terms. These out-of-dictionary terms fall into different categories, e.g.,
named entities (names of people, places, companies, etc.) not found in WordNet, hashtags,
user mentions, etc. We use the Carnegie Mellon University (CMU) Twitter NLP tool to
automatically identify the categories. The tool was shown to achieve 89% tagging accuracy
on tweet data (Gimpel, Schneider, OConnor, Das, Mills, Eisenstein, Heilman, Yogatama,
Flanigan, & Smith, 2011). Table 2 shows the distribution of the out-of-dictionary terms by
category.7 One can observe that most of the out-of-dictionary terms are named entities as
well as user mentions, URLs, and hashtags. There is also a moderate amount of creatively
spelled regular English words and slang words used as nouns, verbs, and adjectives. In the
SMS test set, out-of-dictionary terms constitute a smaller proportion of the vocabulary,
about 25%. These are mostly named entities, interjections, creative spellings, and slang.
The SemEval-2013 training and development data are used to train our supervised sentiment analysis system presented in Section 5. The performance of the system is evaluated
on both test sets, tweets and SMS (Section 6.2.2). The test data are also used in the
experiments on comparing the performance of sentiment lexicons in unsupervised settings
(Section 6.2.1).
5. http://wordnet.princeton.edu
6. The SMART stopword list built by Gerard Salton and Chris Buckley for the SMART information retrieval
system at Cornell University (http://www.lextek.com/manuals/onix/stopwords2.html) is used.
7. The percentages in the columns do not sum up to 100% because some terms can be used in multiple
categories (e.g., as a noun and a verb).

730

fiSentiment Analysis of Short Informal Texts

Table 2: The distribution of the out-of-dictionary tokens by category for the SemEval-2013
tweet and SMS test sets.
Category of tokens
named entities
user mentions
URLs
hashtags
interjections
emoticons
nouns
verbs
adjectives
adverbs
others

Tweet test set
31.84%
21.23%
16.92%
10.94%
2.56%
1.40%
8.52%
3.05%
1.43%
0.70%
4.00%

SMS test set
32.63%
0.11%
0.84%
0%
10.32%
1.89%
25.47%
18.95%
4.84%
6.21%
15.69%

In addition to the SemEval-2013 datasets, we evaluate the system on a dataset of movie
review excerpts (Socher et al., 2013). The task is to predict the sentiment label (positive
or negative) of a given sentence, extracted from a longer movie review (message-level task).
The dataset is comprised of 4,963 positive and 4,650 negative sentences split into the training (6,920 sentences), development (872 sentences), and test (1,821 sentences) sets. Since
detailed phrase-level annotations are not available for most real-world applications, we use
only sentence-level annotations and ignore the phrase-level annotations and the parse-tree
structures of the sentences provided with the data. We train our sentiment analysis system
on the training and development subsets and evaluate its performance on the test subset.
The results of these experiments are reported in Section 6.2.2.

4. Sentiment Lexicons Used by Our System
4.1 Existing, General-Purpose, Manually Created Sentiment Lexicons
Most of the lexicons that were created by manual annotation tend to be domain free and
include a few thousand terms. The lexicons that we use include the NRC Emotion Lexicon
(Mohammad & Turney, 2010), Bing Lius Lexicon (Hu & Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). The NRC Emotion Lexicon is comprised of frequent
English nouns, verbs, adjectives, and adverbs annotated for eight emotions (joy, sadness,
anger, fear, disgust, surprise, trust, and anticipation) as well as for positive and negative
sentiment. Bing Lius Lexicon provides a list of positive and negative words manually extracted from customer reviews. The MPQA Subjectivity Lexicon contains words marked
with their prior polarity (positive or negative) and a discrete strength of evaluative intensity
(strong or weak). Entities in these lexicons do not come with a real-valued score indicating
the fine-grained evaluative intensity.
731

fiKiritchenko, Zhu, & Mohammad

4.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons
4.2.1 Hashtag Sentiment Lexicon
Certain words in tweets are specially marked with a hashtag (#) and can indicate the topic
or sentiment. Mohammad (2012) showed that hashtagged emotion words such as #joy,
#sad, #angry, and #surprised are good indicators that the tweet as a whole (even without
the hashtagged emotion word) is expressing the same emotion. We adapted that idea to
create a large corpus of positive and negative tweets. From this corpus we then automatically
generated a high-coverage, tweet-specific sentiment lexicon as described below.
We polled the Twitter API every four hours from April to December 2012 in search
of tweets with either a positive-word hashtag or a negative-word hashtag. A collection
of 77 seed words closely associated with positive and negative sentiment such as #good,
#excellent, #bad, and #terrible were used (30 positive and 47 negative). These terms were
chosen from entries for positive and negative in Rogets Thesaurus8 . About 2 million tweets
were collected in total. We used the metadata tag iso language code to identify English
tweets. Since this tag is not always reliable, we additionally discarded tweets that did not
have at least two valid English content words from Rogets Thesaurus.9 This step also
helped discard very short tweets and tweets with a large proportion of misspelled words.
A set of 775,000 remaining tweets, which we refer to as Hashtag Sentiment Corpus,
was used to generate a large wordsentiment association lexicon. A tweet was considered
positive if it had one of the 30 positive hashtagged seed words, and negative if it had one
of the 47 negative hashtagged seed words. The sentiment score for a term w was calculated
from these pseudo-labeled tweets as shown below:
Sentiment Score (w) = PMI (w , positive)  PMI (w , negative)

(1)

PMI stands for pointwise mutual information:
PMI (w , positive) = log2

freq (w , positive)  N
freq (w )  freq (positive)

(2)

where freq (w, positive) is the number of times a term w occurs in positive tweets, freq (w)
is the total frequency of term w in the corpus, freq (positive) is the total number of tokens
in positive tweets, and N is the total number of tokens in the corpus. PMI (w, negative) is
calculated in a similar way. Thus, equation 1 is simplified to:
Sentiment Score (w) = log2

freq (w , positive)  freq (negative)
freq (w , negative)  freq (positive)

(3)

Since PMI is known to be a poor estimator of association for low-frequency events, we
ignore terms that occurred less than five times in each (positive and negative) group of
tweets.10
8. http://www.gutenberg.org/ebooks/10681
9. Any word in the thesaurus was considered a content word with the exception of the words from the
SMART stopword list.
10. The same threshold of five occurrences in at least one class (positive or negative) is applied for all
automatic tweet-specific lexicons discussed in this paper. There is no thresholding on the sentiment
score.

732

fiSentiment Analysis of Short Informal Texts

A positive sentiment score indicates a greater overall association with positive sentiment,
whereas a negative score indicates a greater association with negative sentiment. The
magnitude is indicative of the degree of association. Note that there exist numerous other
methods to estimate the degree of association of a term with a category (e.g., cross entropy,
Chi-squared, and information gain). We have chosen PMI because it is simple and robust
and has been successfully applied in a number of NLP tasks (Turney, 2001; Turney &
Littman, 2003).
The final lexicon, which we will refer to as Hashtag Sentiment Base Lexicon (HS Base)
has entries for 39,413 unigrams and 178,851 bigrams. Entries were also generated for
unigramunigram, unigrambigram, and bigrambigram pairs that were not necessarily
contiguous in the tweets corpus. Pairs where at least one of the terms is punctuation (e.g.,
,, ?, .), a user mention, a URL, or a function word (e.g., a, the, and) were
removed. The lexicon has entries for 308,808 non-contiguous pairs.
4.2.2 Sentiment140 Lexicon
The Sentiment140 Corpus (Go et al., 2009) is a collection of 1.6 million tweets that contain
emoticons. The tweets are labeled positive or negative according to the emoticon. We
generated the Sentiment140 Base Lexicon (S140 Base) from this corpus in the same manner
as described above for the hashtagged tweets using Equation 1. This lexicon has entries
for 65,361 unigrams, 266,510 bigrams, and 480,010 non-contiguous pairs. In the following
section, we further build on the proposed approach to create separate lexicons for terms in
affirmative contexts and for terms in negated contexts.
4.3 Affirmative Context and Negated Context Lexicons
A word in a negated context has a different evaluative nature than the same word in an
affirmative (non-negated) context. This difference may include the change in the polarity
category (positive becomes negative or vice versa), the evaluative intensity, or both. For
example, highly positive words (e.g., great) when negated tend to experience both, polarity
change and intensity decrease, forming mildly negative phrases (e.g., not great). On the
other hand, many strong negative words (e.g., terrible) when negated keep their negative
polarity and just shift their intensity. The conventional approach of reversing polarity is
not able to handle these cases properly.
We propose an empirical method to determine the sentiment of words in the presence of
negation. We create separate lexicons for affirmative and negated contexts. In this way, two
sentiment scores for each term w are computed: one for affirmative contexts and another
for negated contexts. The lexicons are created as follows. The Hashtag Sentiment Corpus
is split into two parts: Affirmative Context Corpus and Negated Context Corpus. Following
the work by Pang, Lee, and Vaithyanathan (2002), we define a negated context as a segment
of a tweet that starts with a negation word (e.g., no, shouldnt) and ends with one of the
punctuation marks: ,, ., :, ;, !, ?. The list of negation words was adopted from
Christopher Potts sentiment tutorial.11 Thus, part of a tweet that is marked as negated
is included into the Negated Context Corpus while the rest of the tweet becomes part
of the Affirmative Context Corpus. The sentiment label for the tweet is kept unchanged
11. http://sentiment.christopherpotts.net/lingstruc.html

733

fiKiritchenko, Zhu, & Mohammad

Table 3: Example sentiment scores from the Sentiment140 Base, Affirmative Context
(AffLex) and Negated Context (NegLex) Lexicons.
Term

Sentiment140 Lexicons
Base AffLex NegLex

Positive terms
great
beautiful
nice
good
honest

1.177
1.049
0.974
0.825
0.391

1.273
1.112
1.149
1.167
0.431

-0.367
0.217
-0.912
-1.414
-0.123

Negative terms
terrible
shame
bad
ugly
negative

-1.766
-1.457
-1.297
-0.899
-0.090

-1.850
-1.548
-1.674
-0.964
-0.261

-0.890
-0.722
0.021
-0.772
0.389

in both corpora. Then, we generate the Affirmative Context Lexicon (HS AffLex) from
the Affirmative Context Corpus and the Negated Context Lexicon (HS NegLex) from the
Negated Context Corpus using the technique described in Section 4.2. We will refer to
the sentiment score calculated from the Affirmative Context Corpus as score AffLex (w ) and
the score calculated from the Negated Context Corpus as score NegLex (w ). Similarly, the
Sentiment140 Affirmative Context Lexicon (S140 AffLex) and the Sentiment140 Negated
Context Lexicon (S140 NegLex) are built from the Affirmative Context and the Negated
Context parts of the Sentiment140 tweet corpus. To employ these lexicons on a separate
dataset, we apply the same technique to split each message into affirmative and negated
contexts and then match words in affirmative contexts against the Affirmative Context
Lexicons and words in negated contexts against the Negated Context Lexicons.
Computing a sentiment score for a term w only from affirmative contexts makes
score AffLex (w ) more precise since it is no longer polluted by negation. Positive terms get
stronger positive scores and negative terms get stronger negative scores. Furthermore, for
the first time, we create lexicons for negated terms and compute score NegLex (w ) that reflects the behaviour of words in the presence of negation. Table 3 shows a few examples of
positive and negative terms with their sentiment scores from the Sentiment140 Base, Affirmative Context (AffLex) and Negated Context (NegLex) Lexicons. In Fig. 1, we visualize
the relationship between score AffLex (w ) and score NegLex (w ) for a set of words manually annotated for sentiment in the MPQA Subjectivity Lexicon. The x-axis is score AffLex (w ), the
sentiment score of a term w in the Sentiment140 Affirmative Context Lexicon; the y-axis
is score NegLex (w ), the sentiment score of a term w in the Sentiment140 Negated Context
Lexicon. Dots in the plot correspond to words that occur in each of the MPQA Subjectivity Lexicon, the Sentiment140 Affirmative Context Lexicon, and the Sentiment140 Negated
Context Lexicon. Furthermore, we discard the terms whose polarity category (positive or
negative) in the Sentiment140 Affirmative Context Lexicon does not match their polarity in
the MPQA Subjectivity Lexicon. We observe that when negated, 76% of the positive terms
734

fiSentiment Analysis of Short Informal Texts

3

scoreNegLex(w)

2

1

scoreAffLex(w)
0
-4.5

-3.5

-2.5

-1.5

-0.5

0.5

1.5

2.5

3.5

4.5

-1

-2

-3

Figure 1: The sentiment scores from the Sentiment140 AffLex and the Sentiment140 NegLex
for 480 positive and 486 negative terms from the MPQA Subjectivity Lexicon.
The x-axis is score AffLex (w ), the sentiment score of a term w in the Sentiment140
Affirmative Context Lexicon; the y-axis is score NegLex (w ), the sentiment score of
a term w in the Sentiment140 Negated Context Lexicon. Each dot corresponds to
one (positive or negative) term. The graph shows that positive and negative terms
when negated tend to convey a negative sentiment. Negation affects sentiment
differently for each term.

reverse their polarity whereas 82% of the negative terms keep their polarity orientation and
just shift their sentiment scores. (This behaviour agrees well with human judgments from
the study by Taboada et al. (2011).) Changes in evaluative intensity vary from term to
term. For example, score NegLex (good ) < score AffLex (good ) whereas score NegLex (great) >
score AffLex (great).
We also compiled a list of 596 antonym pairs from WordNet and compare the scores
of terms in the Sentiment140 Affirmative Context Lexicon with the scores of the terms
antonyms in the Sentiment140 Negated Context Lexicon. We found that 51% of negated
positive terms are less negative than their corresponding antonyms (e.g.,
score NegLex (good ) > score AffLex (bad )), but 95% of negated negative terms are more negative
than their positive antonyms (e.g., score NegLex (ugly) < score AffLex (beautiful )).
These experiments reveal the tendency of positive terms when negated to convey a
negative sentiment and the tendency of negative terms when negated to still convey a
negative sentiment. Moreover, the degree of change in evaluative intensity appears to be
term-dependent. Capturing all these different behaviours of terms in negated contexts by
means of the Negated Context Lexicons empower our automatic sentiment analysis system
as we demonstrate through experiments in Section 6. Furthermore, we believe that the
Affirmative Context Lexicons and the Negated Context Lexicons can be valuable in other
735

fiKiritchenko, Zhu, & Mohammad

applications such as textual entailment recognition, paraphrase detection, and machine
translation. For instance in the paraphrase detection task, given two sentences The hotel
room wasnt terrible. and The hotel room was excellent. an automatic system can
correctly infer that these sentences are not paraphrases by looking up score NegLex (terrible)
and score AffLex (excellent) and seeing that the polarities and intensities of these terms do
not match (i.e., score AffLex (excellent) is highly positive and score NegLex (terrible) is slightly
negative). At the same time, a mistake can easily be made with conventional lexicons
and the polarity reversing strategy, according to which the strong negative term terrible is
assumed to convey a strong positive sentiment in the presence of negation and, therefore,
the polarities and intensities of the two terms would match.
4.4 Negated Context (Positional) Lexicons
We propose to further improve the method of constructing the Negated Context Lexicons
by splitting a negated context into two parts: the immediate context consisting of a single token that directly follows a negation word, and the distant context consisting of the
rest of the tokens in the negated context. We refer to these lexicons as Negated Context
(Positional) Lexicons. Each token in a Negated Context (Positional) Lexicon can have two
scores: immediate-context score and distant-context score. The benefits of this approach
are two-fold. Intuitively, negation affects words directly following a negation word more
strongly than the words farther away. Compare, for example, immediate negation in not
good and more distant negation in not very good, not as good, not such a good idea. Second,
immediate-context scores are less noisy. Our simple negation scope identification algorithm
can occasionally fail and include into negated context parts of a tweet that are not actually
negated (e.g., if a punctuation mark is missing). These errors have less effect on immediate
context. When employing these lexicons, we use an immediate-context score for a word
immediately preceded by a negation word and use distant-context scores for all other words
affected by a negation. As before, for non-negated parts of a message, sentiment scores from
an Affirmative Context Lexicon are used. Assuming that words occur in distant contexts
more often than in immediate contexts, this approach can introduce more sparseness to the
lexicons. Thus, we apply a back-off strategy: if an immediate-context score is not available
for a token immediately following a negation word, its distant-context score is used instead.
In Section 6, we experimentally show that the Negated Context (Positional) Lexicons provide additional benefits to our sentiment analysis system over the regular Negated Context
Lexicons described in the previous section.
4.5 Lexicon Coverage
Table 4 shows the number of positive and negative entries in each of the sentiment lexicons
discussed above. The automatically generated lexicons are an order of magnitude larger
than the manually created lexicons. We can see that all manual lexicons contain more
negative terms than positive terms. In the automatically generated lexicons, this imbalance
is less pronounced (49% positive vs. 51% negative in the Hashtag Sentiment Base Lexicon)
or even reversed (61% positive vs. 39% negative in the Sentiment140 Base Lexicon). The
Sentiment140 Base Lexicon was created from an equal number of positive and negative
tweets. Therefore, the prevalence of positive terms corresponds to the general trend in
736

fiSentiment Analysis of Short Informal Texts

Table 4: The number of positive and negative entries in the sentiment lexicons.
Lexicon
NRC Emotion Lexicon
Bing Lius Lexicon
MPQA Subjectivity Lexicon
Hashtag Sentiment Lexicons (HS)
HS Base Lexicon
- unigrams
- bigrams
HS AffLex
- unigrams
- bigrams
HS NegLex
- unigrams
- bigrams
Sentiment140 Lexicons (S140)
S140 Base Lexicon
- unigrams
- bigrams
S140 AffLex
- unigrams
- bigrams
S140 NegLex
- unigrams
- bigrams

Positive
2,312 (41%)
2,006 (30%)
2,718 (36%)

Negative
3,324 (59%)
4,783 (70%)
4,911 (64%)

Total
5,636
6,789
7,629

19,121 (49%)
69,337 (39%)

20,292 (51%)
109,514 (61%)

39,413
178,851

19,344 (51%)
67,070 (42%)

18,905 (49%)
90,788 (58%)

38,249
157,858

936 (14%)
3,954 (15%)

5,536 (86%)
22,258 (85%)

6,472
26,212

39,979 (61%)
135,280 (51%)

25,382 (39%)
131,230 (49%)

65,361
266,510

40,422 (63%)
133,242 (55%)

23,382 (37%)
107,206 (45%)

63,804
240,448

1,038 (12%)
5,913 (16%)

7,315 (88%)
32,128 (84%)

8,353
38,041

language and supports the Polyanna Hypothesis (Boucher & Osgood, 1969), which states
that people tend to use positive terms more frequently and diversely than negative. Note,
however, that negative terms are dominant in the Negated Context Lexicons since most
terms, both positive and negative, tend to convey negative sentiment in the presence of
negation. The overall sizes of the Negated Context Lexicons are rather small since negation
occurs only in 24% of the tweets in the Hashtag and Sentiment140 corpora and only part
of a message with negation is actually negated.
Table 5 shows the differences in coverage between the lexicons. Specifically, it gives the
number of additional terms a lexicon in row X has in comparison to a lexicon in column
Y and the percentage of tokens in the SemEval-2013 tweet test set covered by these extra
entries of lexicon X (numbers in brackets). For instance, almost half of Bing Lius Lexicon
(3,457 terms) is not found in the Sentiment140 Base Lexicon. However, these additional
terms represent only 0.05% of all the tokens from the tweet test set. These are terms that
are rarely used in short informal writing (e.g., acrimoniously, bestial, nepotism). Each of
the manually created lexicons covers extra 23% of the test data compared to other manual
lexicons. On the other hand, the automatically generated lexicons cover 60% more tokens
in the test data. Both automatic lexicons provide a number of terms not found in the other.
737

fiKiritchenko, Zhu, & Mohammad

Table 5: Lexicons supplemental coverage: for row X and column Y, the number of Lexicon
Xs entries that are not found in Lexicon Y and (in brackets) the percentage of
tokens in the SemEval-2013 tweet test set covered by these extra entries of Lexicon X. NRC stands for NRC Emotion Lexicon, B.L. is for Bing Lius Lexicon,
MPQA is for MPQA Subjectivity Lexicon, HS is for Hashtag Sentiment Base
Lexicon, S140 is for Sentiment140 Base Lexicon.
Lexicon
NRC
B.L.
MPQA
HS
S140
NRC
3,179 (2.25%)
3,010 (2.00%)
2,480 (0.09%) 1,973 (0.05%)
B.L.
4,410 (1.72%)
1,383 (0.70%)
4,001 (0.07%) 3,457 (0.05%)
MPQA
3,905 (3.37%)
1,047 (2.60%)
3,719 (0.07%) 3,232 (0.04%)
HS
36,338 (64.23%) 36,628 (64.73%) 36,682 (62.84%)
15,185 (0.59%)
S140
61,779 (64.13%) 62,032 (64.65%) 62,143 (62.74%) 41,133 (0.53%)
-

5. Our System
5.1 Classifier
Our system, NRC-Canada Sentiment Analysis System, employs supervised statistical machine learning. For both tasks, message-level and term-level, we train a linear-kernel Support Vector Machine (SVM) (Chang & Lin, 2011) classifier on the available training data.
SVM is a state-of-the-art learning algorithm proved to be effective on text categorization
tasks and robust on large feature spaces. In the preliminary experiments, a linear-kernel
SVM outperformed a maximum-entropy classifier. Also, a linear-kernel SVM showed better performance than an SVM with another commonly used kernel, radial basis function
(RBF).
The classification model leverages a variety of surface-form, semantic, and sentiment
lexicon features described below. The sentiment lexicon features are derived from three
existing, general-purpose, manual lexicons (NRC Emotion Lexicon, Bing Lius Lexicon,
and MPQA Subjectivity Lexicon), and four newly created, tweet-specific lexicons (Hashtag
Sentiment Affirmative Context, Hashtag Sentiment Negated Context (Positional), Sentiment140 Affirmative Context, and Sentiment140 Negated Context (Positional)).
5.2 Features
5.2.1 Message-Level Task
For the message-level task, the following pre-processing steps are performed. URLs and
user mentions are normalized to http://someurl and @someuser, respectively. Tweets are
tokenized and part-of-speech tagged with the CMU Twitter NLP tool (Gimpel et al., 2011).
Then, each tweet is represented as a feature vector. We employ commonly used text classification features such as ngrams and part-of-speech tag counts, as well as common Twitterspecific features such as emoticon and hashtag counts. In addition, we introduce several
lexicon features that take advantage of the knowledge present in manually and automatically created lexicons. These features are designed to explicitly handle negation. Table 6
738

fiSentiment Analysis of Short Informal Texts

Table 6: Examples of features that the system would generate for message GRRREAT
show!!! Hope not to miss the next one :). Numeric features are presented in the
format: <feature name>:<feature value>. Binary features are italicized; only
features with value of 1 are shown.
Feature group
word ngrams
character ngrams
all-caps
POS
automatic lexicon
features
manual lexicon
features
punctuation
emoticons
elongated words
clusters

Examples
grrreat, show, grrreat show, miss NEG, miss NEG the
grr, grrr, grrre, rrr, rrre, rrrea
all-caps:1
POS N:1 (nouns), POS V:2 (verbs), POS E:1 (emoticons),
POS ,:1 (punctuation)
HS unigrams positive count:4, HS unigrams negative total score:1.51,
HS unigrams POS N combined total score:0.19,
HS bigrams positive total score:3.55, HS bigrams negative max score:1.98
MPQA positive affirmative score:2, MPQA negative negated score:1,
BINGLIU POS V negative negated score:1
punctuation !:1
emoticon positive:1, emoticon positive last
elongation:1
cluster 11111001110, cluster 10001111

provides some example features for tweet GRRREAT show!!! Hope not to miss the next
one :).
The features:
 word ngrams: presence or absence of contiguous sequences of 1, 2, 3, and 4 tokens;
non-contiguous ngrams (ngrams with one token replaced by *);
 character ngrams: presence or absence of contiguous sequences of 3, 4, and 5 characters;
 all-caps: the number of tokens with all characters in upper case;
 POS: the number of occurrences of each part-of-speech tag;
 hashtags: the number of hashtags;
 negation: the number of negated contexts. Negation also affects the ngram features:
a word w becomes w NEG in a negated context;
 sentiment lexicons:
 Automatic lexicons The following sets of features are generated separately for
the Hashtag Sentiment Lexicons (HS AffLex and HS NegLex (Positional)) and the
Sentiment140 Lexicons (S140 AffLex and S140 NegLex (Positional)). For each
token w occurring in a tweet and present in the lexicons, we use its sentiment
score (score AffLex (w ) if w occurs in an affirmative context and score NegLex (w ) if
w occurs in a negated context) to compute:
 the number of tokens with score(w) 6= 0;
P
 the total score = wtweet score(w);
 the maximal score = max wtweet score(w);
739

fiKiritchenko, Zhu, & Mohammad

 the score of the last token in the tweet.
These features are calculated for all positive tokens (tokens with sentiment scores
greater than zero), for all negative tokens (tokens with sentiment scores less than
zero), and for all tokens in a tweet. Similar feature sets are also created for
each part-of-speech tag and for hashtags. Separate feature sets are produced for
unigrams, bigrams, and non-contiguous pairs.
 Manual lexicons For each of the three manual sentiment lexicons (NRC Emotion Lexicon, Bing Lius Lexicon, and MPQA Subjectivity Lexicon), we compute
the following four features:





the
the
the
the

sum
sum
sum
sum

of
of
of
of

positive scores for tweet tokens in affirmative contexts;
negative scores for tweet tokens in affirmative contexts;
positive scores for tweet tokens in negated contexts;
negative scores for tweet tokens in negated contexts.

Negated contexts are identified exactly as described earlier in Section 4.3 (the
method for creating the Negated Context Corpora). The remaining parts of the
messages are treated as affirmative contexts. We use the score of +1 for positive
entries and the score of -1 for negative entries for the NRC Emotion Lexicon
and Bing Lius Lexicon. For MPQA Subjectivity Lexicon, which provides two
grades of the association strength (strong and weak), we use scores +1/-1 for
weak associations and +2/-2 for strong associations. The same feature sets are
also created for each part-of-speech tag, for hashtags, and for all-caps tokens.
 punctuation:
 the number of contiguous sequences of exclamation marks, question marks, and
both exclamation and question marks;
 whether the last token contains an exclamation or question mark;
 emoticons: The polarity of an emoticon is determined with a regular expression
adopted from Christopher Potts tokenizing script:12
 presence or absence of positive and negative emoticons at any position in the
tweet;
 whether the last token is a positive or negative emoticon;
 elongated words: the number of words with one character repeated more than two
times, for example, soooo;
 clusters: The CMU Twitter NLP tool provides token clusters produced with the
Brown clustering algorithm on 56 million English-language tweets. These 1,000 clusters serve as alternative representation of tweet content, reducing the sparcity of the
token space.
 the presence or absence of tokens from each of the 1000 clusters.
12. http://sentiment.christopherpotts.net/tokenizing.html

740

fiSentiment Analysis of Short Informal Texts

5.2.2 Term-level Task
The pre-processing steps for the term-level task include tokenization and stemming with
Porter stemmer (Porter, 1980).13 Then, each tweet is represented as a feature vector with
the following groups of features:
 word ngrams:
 presence or absence of unigrams, bigrams, and the full word string of a target
term;
 leading and ending unigrams and bigrams;
 character ngrams: presence or absence of two- and three-character prefixes and suffixes
of all the words in a target term (note that the target term may be a multi-word
sequence);
 upper case:
 whether all the words in the target start with an upper case letter followed by
lower case letters;
 whether the target words are all in uppercase (to capture a potential named
entity);
 stopwords: whether a term contains only stop-words. If so, a separate set of features
indicates whether there are 1, 2, 3, or more stop-words;
 negation: similar to the message-level task;
 sentiment lexicons: for each of the manual sentiment lexicons (NRC Emotion Lexicon, Bing Lius Lexicon, and MPQA Subjectivity Lexicon) and automatic sentiment
lexicons (HS AffLex and HS NegLex (Positional), and S140 AffLex and S140 NegLex
(Positional) Lexicons), we compute the following three features:
 the sum of positive scores;
 the sum of negative scores;
 the total score.
For the manual lexicons, the polarity reversing strategy is applied to negation.14 Note
that words themselves and not their stems are matched against the sentiment lexicons.
 punctuation: presence or absence of punctuation sequences such as ?! and !!!;
 emoticons: the numbers and categories of emoticons that a term contains15 ;
 elongated words: presence or absence of elongated words;
 lengths:
 the length of a target term (number of words);
13. Some differences in implementation, such as the use of a stemmer, are simply a result of different team
members working on the two tasks.
14. In the experiments on the development dataset, these manual lexicon features showed better performance
on the term-level task than the set of four features used for the message-level task.
15. http://en.wikipedia.org/wiki/List of emoticons

741

fiKiritchenko, Zhu, & Mohammad

 the average length of words (number of characters) in a term;
 a binary feature indicating whether a term contains long words;
 position: whether a term is at the beginning, at the end, or at another position in a
tweet;
 term splitting: when a term contains a hashtag made of multiple words (e.g., #biggestdaythisyear ), we split the hashtag into component words;
 others:
 whether a term contains a Twitter user name;
 whether a term contains a URL.
The above features are extracted from target terms as well as from the rest of the
message (the context). For unigrams and bigrams, we use four words on either side of the
target as the context. The window size was chosen through experiments on the development
set.

6. Experiments
This section presents the evaluation experiments that demonstrate the state-of-the-art performance of our sentiment analysis system on three domains: tweets, SMS, and movie
review excerpts. The experiments also reveal the superior predictive power of the new,
tweet-specific, automatically created lexicons over existing, general-purpose lexicons. Furthermore, they show that the Negated Context Lexicons can bring additional gains over the
standard polarity reversing strategy of handling negation.
We begin with intrinsic evaluation of the automatic lexicons by comparing them to the
manually created sentiment lexicons and to human annotated sentiment scores. Next, we
assess the value of the lexicons as part of a sentiment analysis system in both, supervised
and unsupervised settings. The goal of the experiments in unsupervised sentiment analysis
(Section 6.2.1) is to compare the predictive capacity of the lexicons with the simplest setup
to reduce the influence of other factors (such as the choice of features) as much as possible.
Also, we evaluate the impact of the amount of data used to create an automatic lexicon
on the quality of the lexicon. Then, in Section 6.2.2 we evaluate the performance of our
supervised sentiment analysis system and analyze the contributions of features derived from
different sentiment lexicons.
6.1 Intrinsic Evaluation of the Lexicons
To intrinsically evaluate our tweet-specific, automatically created sentiment lexicons, we first
compare them to existing manually created sentiment lexicons (Section 6.1.1). However,
existing manual lexicons tend to only have discrete labels for terms (positive, negative,
neutral) but no real-valued scores indicating the intensity of sentiment. In Section 6.1.2,
we show how we collected human annotated real-valued sentiment scores using the MaxDiff
method of annotation (Louviere, 1991). We then compare the association scores in the
automatically generated lexicons with these human annotated scores.
742

fiSentiment Analysis of Short Informal Texts

Table 7: Agreement in polarity assignments between the Sentiment140 Affirmative Context
Lexicon and the manual lexicons. Agreement between two lexicons is measured
as the percentage of shared terms given the same sentiment label (positive or
negative) by both lexicons. The agreement is calculated for three sets of terms:
(1) all shared terms; (2) shared terms whose sentiment score in S140 AffLex has
an absolute value greater than or equal to 1 (|score(w)|  1); and (3) shared terms
whose sentiment score in S140 AffLex has an absolute value greater than or equal
to 2 (|score(w)|  2). Sentiment scores in S140 AffLex range from -5.9 to 6.8.
Lexicon
NRC Emotion Lexicon
Bing Lius Lexicon
MPQA Subjectivity Lexicon

Number of
shared terms
3,472
3,213
3,105

All terms
73.96%
78.24%
75.91%

Agreement
|score(w)|  1 |score(w)|  2
89.96%
98.61%
92.32%
99.45%
90.26%
98.59%

6.1.1 Comparing with Existing Manually Created Sentiment Lexicons
We examine the terms in the intersection of a manual lexicon and an automatic lexicon and
measure the agreement between the lexicons as the percentage of the shared terms having
the same polarity label (positive or negative) assigned by both lexicons. Table 7 shows the
results for the Sentiment140 Affirmative Context Lexicon and three manual lexicons: NRC
Emotion Lexicon, Bing Lius Lexicon, and MPQA Subjectivity Lexicon. Similar figures (not
shown in the table) are obtained for other automatic lexicons (HS Base Lexicon, HS AffLex,
and S140 Base): the agreement for all terms ranges between 71% and 78%. If we consider
only terms whose sentiment scores in the automatic lexicon have higher absolute values,
the agreement numbers substantially increase. Thus, automatically generated entries with
higher absolute sentiment values prove to be more reliable.
6.1.2 Comparing with Human Annotated Sentiment Association Scores
Apart from polarity labels, the automatic lexicons provide sentiment scores indicating the
degree of the association of the term with positive or negative sentiment. It should be noted
that the individual scores themselves are somewhat meaningless other than their ability to
indicate that one word is more positive (or more negative) than another. However, there
exists no resource that can be used to determine if the real-valued scores match human
intuition. In this section, we describe how we collected human annotations of terms for
sentiment association scores using crowdsourcing.
MaxDiff method of annotation: For people, assigning a score indicating the degree
of sentiment is not natural. Different people may assign different scores to the same target
item, and it is hard for even the same annotator to remain consistent when annotating a
large number of items. In contrast, it is easier for annotators to determine whether one word
is more positive (or more negative) than the other. However, the latter requires a much
larger number of annotations than the former (in the order of N 2 , where N is the number
of items to be annotated). MaxDiff is an annotation scheme that retains the comparative
743

fiKiritchenko, Zhu, & Mohammad

aspect of annotation while still requiring only a small number of annotations (Louviere,
1991).
The annotator is presented with four words and asked which word is the most positive
and which is the least positive. By answering just these two questions five out of the six
inequalities are known. Consider a set in which a respondent evaluates: A, B, C and D.
If the respondent says that A is most positive and D is least positive, these two responses
inform us that:
A > B, A > C, A > D, B > D, C > D
Each of these MaxDiff questions can be presented to multiple annotators. The responses
to the MaxDiff questions can then be easily translated into a ranking of all the terms and
also a real-valued score for all the terms (Orme, 2009). If two words have very different
degrees of association (for example, A >> D), then A will be chosen as most positive much
more often than D and D will be chosen as least positive much more often than A. This
will eventually lead to a ranked list such that A and D are significantly farther apart, and
their real-valued association scores are also significantly different. On the other hand, if
two words have similar degrees of association with positive sentiment (for example, A and
B), then it is possible that for MaxDiff questions having both A and B, some annotators
will choose A as most positive, and some will choose B as most positive. Further, both A
and B will be chosen as most positive (or most negative) a similar number of times. This
will result in a list such that A and B are ranked close to each other and their real-valued
association scores will also be close in value.
The MaxDiff method is widely used in market survey questionnaires (Almquist & Lee,
2009). It was also used for determining relation similarity of pairs of items by Jurgens,
Mohammad, Turney, and Holyoak (2012) in a SemEval-2012 shared task.
Term selection: For the evaluation of the automatic lexicons, we selected 1,455 highfrequency terms from the Sentiment140 Corpus and the Hashtag Sentiment Corpus. This
subset of terms includes regular English words, Twitter-specific terms (e.g., emoticons, abbreviations, creative spellings), and negated expressions. The terms were chosen as follows.
All terms from the corpora, excluding URLs, user mentions, stop words, and terms with
non-letter characters, were ordered by their frequency. To reduce the subset skew towards
the neutral class, terms were selected from different ranges of sentiment values. For this,
the full range of sentiment values in the automatic lexicons was divided into 10 equal-size
bins. From each bin, naff most frequent affirmative terms and nneg most frequent negated
terms were selected to form the initial list.16 . naff was set to 200 and nneg was 50 for all the
bins except for the two middle bins that contain words with very weak association to sentiment (i.e., neutral words). For these two middle bins, naff = 80 and nneg = 20. Then, the
initial list was manually examined, and ambiguous terms, rare abbreviations, and extremely
obscene words (243 terms) were removed. The resulting list was further augmented with
25 most frequent emoticons. The final list of 1,455 terms contains 1,202 affirmative terms
and 253 negated terms; there are 946 words found in WordNet and 509 out-of-dictionary
terms. Each negated term was presented to the annotators as a phrase negator + term,
16. Some bins may contain fewer than naff affirmative or fewer than nneg negated terms. In this case, all
available affirmative/negated terms were selected.

744

fiSentiment Analysis of Short Informal Texts

where the negator chosen was the most frequent negator for the term (e.g., no respect,
not acceptable).
Annotation process: The term list was then converted into about 3,000 MaxDiff
subsets with 4 terms each. The terms for the subsets were chosen randomly from the term
list. No duplicate terms were allowed in a subset, and each subset was unique. For each
MaxDiff subset, annotators were asked to identify the term with the most association to
positive sentiment (i.e., the most positive term) and the term with the least association
to positive sentiment (i.e., the most negative term). Each subset was annotated by 10
annotators. For any given question, we will refer to the option chosen most often as the
majority answer. If a question is answered randomly by the annotators, then only 25%
of the annotators are expected to select the majority answer (as each question has four
options). In our task, we observed that the majority answer was selected by 72% of the
annotators on average.
The answers were then converted into scores using the counting procedure (Orme, 2009).
For each term, its score was calculated as the percentage of times the term was chosen as
the most positive minus the percentage of times the term was chosen as the most negative.
The scores were normalized to the range [0,1]. Even though annotators might disagree
about answers to individual questions, the aggregated scores produced with this counting
procedure and the corresponding term ranking are consistent. We verified this by randomly
dividing the sets of answers to each question into two groups and comparing the scores and
rankings obtained from these two groups of annotations. On average, the scores differed
only by 0.04, and the Spearman rank correlation coefficient between the two sets of rankings
was 0.97. In the rest of the paper, we use the scores and term ranking produced from the full
set of annotations. We will refer to these scores as human annotated sentiment association
scores.
Comparing human annotated and automatic sentiment scores: The human
annotated scores are used to evaluate the sentiment scores in the automatically generated,
tweet-specific lexicons. The scores themselves are not very meaningful other than their
ability to rank terms in order of increasing (or decreasing) association with positive (or
negative) sentiment. If terms t1 and t2 are such that rank (t1 ) > rank (t2 ) as per both
rankings (human and automatic), then the term pair (t1 , t2 ) is considered to have the same
rank order.17 We measure the agreement between human and automatic sentiment rankings
by the percentage of term pairs for which the rank order is the same.18
When two terms have a very similar degree of association with sentiment, then it
is more likely that humans will disagree with each other regarding their order. Similarly, the greater the difference in true sentiment scores, the more likely that humans will
agree with each other regarding their order. Thus, we first create several sets of term
pairs pertaining to various minimal differences in human sentiment scores, and calculate
agreement for each of these sets. Every set pairsk has all term pairs (t1 , t2 ) for which
Human Score (t1 ) 6= Human Score (t2 ) and |Human Score (t1 )  Human Score (t2 )|  k,
where k is varied from 0 to 0.8 in steps of 0.1. Thus, pairs0 includes all term pairs (t1 , t2 )
for which Human Score (t1 ) 6= Human Score (t2 ). Similarly, pairs0 .1 includes all term pairs
for which |Human Score (t1 )  Human Score (t2 )|  0.1, and so on. The agreement for a
17. One can swap t2 with t1 without loss of generality.
18. The measure of agreement we use is similar to Kendalls tau rank correlation coefficient.

745

fiKiritchenko, Zhu, & Mohammad

100

agreement

95

90

HS Base Lexicon
S140 Base Lexicon
HS AffLex and HS NegLex
S140 AffLex and S140 NegLex

85

80

75

min. abs. score difference

70

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Figure 2: Agreement in pair order ranking between automatic lexicons and human annotations. The agreement (y-axis) is measured as the percentage of term pairs with
the same rank order obtained from a lexicon and from human annotations. The
x-axis represents the minimal absolute difference in human annotated scores of
term pairs (k). The results for HS AffLex and HS NegLex are very close to the results for the HS Base Lexicon, and, therefore, the two curves are indistinguishable
in the graph.

given set pairsk is the percentage of term pairs in this set for which the rank order is the
same as per both human annotations and automatically generated scores. We expect higher
rank-order agreement for sets pertaining to higher ksets with larger difference in human
(or true) scores. We plot the agreement between the human annotations and an automatic
lexicon as a function of k (x-axis) in Figure 2.
The agreement for pairs0 can be used as the bottom-line overall agreement score between
human annotations and the automatically generated scores. One can observe that the overall
agreement for all automatic lexicons is about 7578%. The agreement curves monotonically
increase with the difference in human scores getting larger, eventually reaching 100%. The
monotonic increase is expected because as we move farther right along the x-axis, term pair
sets with a higher average difference in human scores are considered. This demonstrates
that the automatic sentiment lexicons correspond well with human intuition, especially on
term pairs with larger difference in human scores.
6.2 Extrinsic Evaluation of the Lexicons
6.2.1 Lexicon Performance in Unsupervised Sentiment Analysis
In this set of experiments, we evaluate the performance of each individual lexicon on the
message-level sentiment analysis task in unsupervised settings. No training and/or tuning
is performed. Since most of the lexicons provide the association scores for the positive and
negative classes only, in this subsection, we reduce the problem to a two-way classification
task (positive or negative). The SemEval-2013 tweet test set and SMS test set are used for
evaluation. The neutral instances are removed from both datasets.
746

fiSentiment Analysis of Short Informal Texts

To classify a message as positive or negative, we add up the scores for all matches in
a particular lexicon and assign a positive label if the cumulative score is greater than zero
and a negative label if the cumulative score is less than zero. Again, we use scores +1/-1 for
the NRC Emotion Lexicon and Bing Lius Lexicon and scores +1/-1 for weak associations
and +2/-2 for strong associations in the MPQA Subjectivity Lexicon. A message is left
unclassified when the score is equal to zero or when no matches are found.
Table 8 presents the results of unsupervised sentiment analysis for (1) manually created, general-purpose lexicons (NRC Emotion Lexicon, Bing Lius Lexicon, and MPQA
Subjectivity Lexicon), (2) automatically created, general-purpose lexicons (SentiWordNet
3.0 (Baccianella, Esuli, & Sebastiani, 2010), MSOL (Mohammad et al., 2009), and Osgood
Evaluative Factor Ratings (Turney & Littman, 2003)), and (3) our automatically created,
tweet-specific lexicons (Hashtag Sentiment and Sentiment140 Lexicons). Only unigram
entries are used from each lexicon. The automatic general-purpose lexicons are large, opendomain lexicons providing automatically generated sentiment scores for words taken from
hand-built general thesauri such as WordNet and Macquarie Thesaurus.19 The predictive
performance is assessed through precision and recall on the positive and negative classes
as well as the macro-averaged F-score of the two classes. Observe that for most of the
lexicons, both precision and recall on the negative class are lower than on the positive class.
In particular, this holds for all the manual lexicons (rows ac) despite the fact that they
have significantly more negative terms than positive terms. One possible explanation for
this phenomenon is that people can express negative sentiment without using many or any
clearly negative words.
The threshold of zero seems natural for separating the positive and negative classes in
unsupervised polarity detection; however, better results are possible with other thresholds.
For example, predictions produced by the Osgood Evaluative Factor Ratings (rows f) are
highly skewed towards the positive class (recall of 95.42 on the positive class and 31.28
on the negative class), which negatively affects its macro-averaged F-score. To avoid the
problem of setting the optimal threshold in unsupervised settings, we report the Area Under
the ROC curve (AUC), which takes into account the performance of the classifier at all
possible thresholds (see the last column in Table 8). To calculate AUC, the cumulative
scores assigned by a lexicon to the test messages are ordered in the decreasing order. Then,
taking every score as a possible threshold, the true positive ratio is plotted against the false
positive ratio and the area under this curve is calculated. It has been shown that the AUC
of a classifier is equivalent to the probability that the classifier will rank a randomly chosen
positive instance higher than a randomly chosen negative instance. This is also equivalent
to the Wilcoxon test of ranks (Hanley & McNeil, 1982).
All automatically generated lexicons match at least one token in each test message while
the manual lexicons are unable to cover 1020% of the tweet test set. Paying attention to
negation proves important for all general-purpose lexicons: both the macro-averaged Fscore and AUC are improved by 14 percentage points. However, this is not the case for
the Hashtag Sentiment Base (rows g) and the Sentiment140 Base Lexicons (rows k). The
polarity reversing strategy fails to improve over the simple baseline of disregarding negation
on these lexicons.
19. The SentiWordNet 3.0 has 30,821 unigrams, the MSOL Lexicon has 55,141 unigrams, and the Osgood
Evaluative Factor Ratings Lexicon contains ratings for 72,905 unigrams.

747

fiKiritchenko, Zhu, & Mohammad

Table 8: Prediction performance of the unigram lexicons in unsupervised sentiment analysis
on the SemEval-2013 tweet test set. Cover. denotes coverage  the percentage of
tweets in the test set with at least one match from the lexicon; P is precision; R
is recall; Favg is the macro-averaged F-score for the positive and negative classes;
AUC is the area under the ROC curve.

Lexicon
Manual general-purpose lexicons
a. NRC Emotion Lexicon
- disregarding negation
- reversing polarity
b. Bing Lius Lexicon
- disregarding negation
- reversing polarity
c. MPQA Subjectivity Lexicon
- disregarding negation
- reversing polarity
Automatic general-purpose lexicons
d. SentiWordNet 3.0
- disregarding negation
- reversing polarity
e. MSOL
- disregarding negation
- reversing polarity
f. Osgood Evaluative Factor Ratings
- disregarding negation
- reversing polarity
Automatic tweet-specific lexicons
g. HS Base Lexicon
- disregarding negation
- reversing polarity
h. HS AffLex
- disregarding negation
- reversing polarity
i. HS AffLex and HS NegLex
j. HS AffLex and HS NegLex (Posit.)
k. S140 Base Lexicon
- disregarding negation
- reversing polarity
l. S140 AffLex
- disregarding negation
- reversing polarity
m. S140 AffLex and S140 NegLex
n. S140 AffLex and S140 NegLex (Posit.)
- no tweet-specific entries

Cover.

Positive
P
R

Negative
P
R

Favg

AUC

76.30
76.30

84.77 58.78 56.83 34.61 56.22 70.66
86.20 59.61 59.02 35.94 57.58 72.83

77.59
77.59

90.73 61.64 65.94 45.42 63.60 79.08
92.02 61.64 66.74 48.75 65.09 80.20

88.36
88.36

82.90 71.56 58.57 38.10 61.49 73.01
84.56 71.06 60.09 43.09 63.71 75.33

100.00
100.00

82.40 71.76 44.93 59.73 64.00 71.51
85.08 71.12 47.42 67.22 66.54 75.15

100.00
100.00

77.18 74.43 38.66 27.79 54.06 63.44
77.35 74.30 41.70 30.95 55.66 63.80

100.00
100.00

75.65 97.65 74.31 17.80 56.99 75.30
78.41 95.42 72.31 31.28 64.88 80.11

100.00
100.00

89.15 72.65 51.79 76.87 70.97 82.52
88.03 72.07 50.45 74.38 69.69 80.21

100.00
100.00
100.00
100.00

87.53 80.41 57.75 70.05 73.56 83.06
87.04 79.07 55.84 69.22 72.34 82.21
89.44 77.04 55.92 76.21 73.64 84.61
89.60 77.29 56.30 76.54 73.94 84.62

100.00
100.00

88.60 77.61 55.78 73.88 73.15 84.47
87.78 77.23 54.68 71.88 72.14 83.21

100.00
100.00
100.00
100.00
100.00

85.96
87.19
89.65
89.79
87.26

748

86.45
85.31
83.21
83.33
86.26

64.02
63.56
63.03
63.31
65.11

63.06
67.05
74.88
75.21
67.05

74.87
75.75
77.37
77.59
76.41

84.94
86.04
86.88
87.14
86.55

fiSentiment Analysis of Short Informal Texts

Compared to the Base Lexicons, the lexicons created only from affirmative contexts
(rows h and l) are more precise and slightly improve the predictive performance. More
substantial improvements are obtained by adding the Negated Context Lexicons (rows i and
m). Furthermore, the Sentiment140 Negated Context (Positional) Lexicon (row n) offers
additional gain of 0.26 percentage points in AUC over the regular Sentiment140 Negated
Context Lexicon (row m). Overall, the Affirmative Context Lexicons and the Negated
Context (Positional) Lexicons outperform the Base Lexicons by over 2 percentage points in
AUC.
The automatically created general-purpose lexicons (rows df) have a substantially
higher coverage; however, they do not show better performance than the manual lexicons.
On the other hand, all our tweet-specific automatic lexicons demonstrate a predictive power
superior to that of both, the manually and automatically created, general-purpose lexicons.
The differences are especially pronounced for the Affirmative Context Lexicons and the
Negated Context Lexicons. While keeping the level of precision close to that of the manual
lexicons, the automatic tweet-specific lexicons are able to substantially improve the recall
on both positive and negative classes. This increase in recall is particularly noticeable on
the negative class where the differences reach forty percentage points.
To investigate the impact of tweet-specific subset of the vocabulary (e.g., emoticons,
hashtags, misspellings) on the performance of the automatic lexicons, we conduct the same
experiments on a reduced lexicon. Terms that are not punctuation, numerals, or stop
words, and that are not found in WordNet have been removed from S140 AffLex and
S140 NegLex (Positional) Lexicons. The performance of the reduced lexicon (last row of
the table) drops about 0.6 percentage points in AUC demonstrating the value of tweetspecific terms. Nevertheless, the results achieved with the subset of S140 AffLex and S140
NegLex (Positional) Lexicons are still superior to that obtained with any other automatic
or manual lexicon. This experiment suggests that the high-coverage automatic lexicons can
also be successfully employed as general-purpose sentiment lexicons and, therefore, applied
on other, non-tweet domains. In the next section, we show that the features derived from
these lexicons are extremely helpful in automatic sentiment analysis not only on tweets,
but also on SMS and movie review data. Furthermore, in (Kiritchenko et al., 2014) we
demonstrate the usefulness of the lexicons in the domains of restaurant and laptop customer
reviews.
In the unsupervised sentiment analysis experiments on the SMS test set (Table 9), one
can see trends similar to the ones observed on the tweet test set above. The automatic
lexicons built separately for affirmative and negated contexts (rows i and m) perform 36
percentage points better than the corresponding Base Lexicons in combination with the polarity reversing strategy (rows g and k). Moreover, the use of the Sentiment140 Affirmative
Context Lexicon and Negated Context (Positional) Lexicon (row n) again results in higher
performance than that obtained with any other manually or automatically created lexicon
we used.
To get a better understanding of the impact of the amount of data used to create an
automatic lexicon on the quality of the lexicon, we compare the performance of the automatic lexicons built from subsets of the available data. We split a tweet corpus (Hashtag
Sentiment Corpus or Sentiment140 Corpus) into smaller chunks by the tweets time stamp.
Fig. 3 shows the performance of the Hashtag Sentiment Base, Hashtag Sentiment Affirma749

fiKiritchenko, Zhu, & Mohammad

Table 9: Prediction performance of the unigram lexicons in unsupervised sentiment analysis
on the SemEval-2013 SMS test set. The polarity reversing strategy is applied to
negation for all lexicons except for the Negated Context Lexicons. Cover. denotes
coverage  the percentage of SMS in the test set with at least one match from the
lexicon; P is precision; R is recall; Favg is the macro-averaged F-score for the
positive and negative classes; AUC is the area under the ROC curve.
Positive
P
R

Negative
P
R

Lexicon

Cover.

Favg

AUC

Manual general-purpose lexicons
a. NRC Emotion Lexicon
b. Bing Lius Lexicon
c. MPQA Subjectivity Lexicon

70.88
69.75
83.86

85.11 56.91 80.17 47.21 63.82 79.66
87.90 61.99 86.36 48.22 67.30 83.24
81.69 72.56 77.95 52.03 69.63 82.42

Automatic general-purpose lexicons
d. SentiWordNet 3.0
e. MSOL
f. Osgood Evaluative Factor Ratings

100.00
100.00
100.00

77.36 79.88 73.87 70.30 75.32 81.34
69.88 73.58 69.14 44.92 63.07 72.49
66.15 95.33 87.01 39.09 66.02 84.01

Automatic tweet-specific lexicons
g. HS Base Lexicon
i. HS AffLex and HS NegLex
j. HS AffLex and HS NegLex (Posit.)

100.00
100.00
100.00

88.41 41.87 56.20 93.15 63.47 75.49
92.03 46.95 58.90 94.92 67.44 81.67
92.00 46.75 58.81 94.92 67.31 82.05

k. S140 Base Lexicon
m. S140 AffLex and S140 NegLex
n. S140 AffLex and S140 NegLex (Posit.)

100.00
100.00
100.00

85.71 73.17 71.67 84.77 78.31 86.07
88.38 78.86 76.73 87.06 82.46 89.34
88.69 79.67 77.48 87.31 83.02 89.60

tive Context and Hashtag Sentiment Negated Context Lexicons, Sentiment140 Base, and
Sentiment140 Affirmative Context and Sentiment140 Negated Context Lexicons built from
these partial corpora as a function of the corpus size. As above, the performance of the
lexicons is evaluated in terms of AUC in unsupervised sentiment analysis on the SemEval2013 tweet test set. We can see that the Sentiment140 Lexicons generated from half of the
available tweet set still have higher predictive power than the full Hashtag Sentiment Lexicons. Interestingly, both Hashtag Sentiment Lexicons seem to stabilize at the corpus size
of 400,000500,000 tweets whereas both Sentiment140 Lexicons stabilize at about 800,000
tweets. However, better results might still be possible with corpora that are orders of
magnitude larger.
6.2.2 Lexicon Performance in Supervised Sentiment Analysis
In this section, we evaluate our supervised sentiment analysis system (described in Section 5) on a three-class problem (positive, negative, and neutral) on both the message-level
task and the term-level task. We use the data provided for the SemEval-2013 competition.
We examine the contribution of various feature groups, including the features derived from
the sentiment lexicons: manually created lexicons (NRC Emotion Lexicon, Bing Lius Lexicon, and MPQA Subjectivity Lexicon) and our automatically created lexicons (Hashtag
750

fiSentiment Analysis of Short Informal Texts

88

AUC

86
84
82

HS Base Lexicon
S140 Base Lexicon
HS AffLex and HS NegLex
S140 AffLex and S140 NegLex

80
78
76

# of tweets (millions)

74

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

Figure 3: Performance of the automatic tweet-specific lexicons in unsupervised sentiment
analysis on the SemEval-2013 tweet test set for different sizes of the tweet corpora.
AUC denotes the Area Under the ROC Curve.

Sentiment and Sentiment140 Lexicons). Finally, we compare the performance of different
strategies to process negation.
For both tasks, we train an SVM classifier on the provided training data and evaluate the
performance of the learned models on an unseen tweet test set. The same models are applied,
without any change, to the test set of SMS messages. We evaluate the performance with the
bottom-line evaluation measure used by the organizers of the SemEval-2013 competition 
the macro-averaged F-score of the positive and negative classes:
Favg =

Fpos + Fneg
2

(4)

Note that this measure does not give any credit for correctly classifying neutral instances.
Nevertheless, the system has to predict all three classes (positive, negative, and neutral) to
avoid being penalized for misclassifying neutral instances as positive or negative. We report
the results obtained by our system on the training set (ten-fold cross-validation), development set (when trained on the training set), and test sets (when trained on the combined
set of tweets in the training and development sets). Significance tests are performed using
a one-tailed paired t-test with approximate randomization at the p < .05 level (Yeh, 2000).
In order to test our system on a different domain, we conduct experiments on classifying
movie review sentences as positive or negative (message-level task only). We use the dataset
and the evaluation setup provided by Socher et al. (2013). We train the system on the
training and development subsets of the movie review excerpts dataset and apply the learned
model on the test subset. To compare with published results on this dataset, we use accuracy
as the evaluation measure.
6.2.3 Results for the Message-Level Task
(a) On the SemEval-2013 data: The results obtained by our system on the SemEval2013 message-level task are presented in Table 10. Our official submission on this task
751

fiKiritchenko, Zhu, & Mohammad

Table 10: Message-level task: The macro-averaged F-scores on the SemEval-2013 datasets.
Classifier
a. Majority baseline
b. SVM-unigrams
c. Our system:
c.1. official SemEval-2013 submission
c.2. best result

Train.
Set
26.94
36.95

Dev.
Set
26.85
36.71

Test Sets
Tweets SMS
29.19
19.03
39.61
39.29

67.09
68.19

68.72
68.43

69.02
70.45

68.46
69.77

(row c.1) obtained a macro-averaged F-score of 69.02 on the tweet test set and 68.46 on
the SMS test set. Out of 48 submissions from 34 teams, our system ranked first on both
datasets.20 After replacing the Base Lexicons with the Affirmative Context Lexicons and
the Negated Context (Positional) Lexicons and with some improvements to the feature set,
we achieved the scores of 70.45 on the tweet set and 69.77 on the SMS set (row c.2).21 The
differences between the best scores and the official scores on both test sets are statistically
significant. The table also shows the baseline results obtained by a majority classifier that
always predicts the most frequent class (row a). The bottom-line F-score is based only
on the F-scores of the positive and negative classes (and not on neutral), so the majority
baseline chooses the most frequent class among positive and negative, which in this case
is the positive class.22 We also include the baseline results obtained using an SVM and
unigram features alone (row b).
Table 11 shows the results of the ablation experiments where we repeat the same classification process but remove one feature group at a time. The most influential features turn
out to be the sentiment lexicon features (row b): they provide gains of 810 percentage
points on all SemEval-2013 datasets. Note that the contribution of the automatic tweetspecific lexicons (row b.2) substantially exceeds the contribution of the manual lexicons
(row b.1). This is especially noticeable on the tweet test set where the use of the automatic
lexicons results in improvement of 6.5 percentage points. Also, the use of bigrams and
non-contiguous pairs (row b.5) bring additional gains over using only the unigram lexicons.
The second most important feature group for the message-level task is ngrams (row c):
word ngrams and character ngrams. Part-of-speech tagging (row d) and clustering (row
e) provide only small improvements. Also, removing the sentiment encoding features like
hashtags, emoticons, and elongated words (row f) has little impact on performance, but
this is probably because the discriminating information in them is also captured by some
other features such as character and word ngrams.
Next, we compare the different strategies of processing negation (Table 12). Observe
that processing negation benefits the overall sentiment analysis system: all methods we test
20. The second-best results were 65.27 on the tweet set and 62.15 on the SMS set.
21. The contributions of the different versions of the automatic lexicons to the overall systems performance
are presented later in this subsection.
22. The majority baseline is calculated as follows. Since all instances are predicted as positive, Fneg = 0,
Rpos = 1, and Ppos = Npos /N, where Npos is the number of positive instances and N is the total number
of instances in the dataset. Then, the macro-averaged F-score of the positive and negative classes Favg
= (Fpos + Fneg )/2 = Fpos /2 = (Ppos * Rpos )/(Ppos + Rpos ) = Ppos /(Ppos + 1) = Npos /(Npos + N).

752

fiSentiment Analysis of Short Informal Texts

Table 11: Message-level task: The macro-averaged F-scores obtained on the SemEval-2013
datasets when one of the feature groups is removed. Scores marked with * are
statistically significantly different (p < .05) from the corresponding scores in row
a.
Experiment

Train.
Set
68.19

Dev.
Set
68.43

Test Sets
Tweets SMS
70.45
69.77

b. all - lexicons
b.1. all - manual lexicons
b.2. all - automatic lexicons
b.3. all - Sentiment140 Lexicons
b.4. all - Hashtag Sentiment Lexicons
b.5. all - automatic lexicons of bigrams
& non-contiguous pairs

60.08*
66.59*
65.17*
66.84*
67.65*

58.98*
66.24*
64.15*
66.80*
67.82

60.51*
69.52*
63.89*
66.58*
67.64*

59.94*
67.26*
66.46*
67.61*
71.16*

67.65*

66.84

67.44*

69.42

c. all - ngrams
c.1. all - word ngrams
c.2. all - character ngrams

64.07*
66.64*
67.64*

65.68*
66.70*
68.28

67.49*
68.29*
68.74*

66.93*
67.64*
69.11

d. all - POS
e. all - clusters
f. all - encodings (elongated, emoticons,
punctuations, all-caps, hashtags)

67.54*
68.21*

67.64
68.33

70.47
70.00

68.42*
68.56*

67.99*

68.66

70.79

69.82

a. all features

outperform the baseline of disregarding negation (row a.1). Employing the Affirmative Context Lexicons and the Negated Context Lexicons (row b) provides substantial improvement
over the standard polarity reversing strategy on the Base Lexicons (row a.2). Replacing the
Negated Context Lexicons with the Negated Context (Positional) Lexicons (row c) results
in some additional gains for the system.
(b) On the Movie Reviews data: The results obtained using our system on the
movie review excerpts dataset is shown in Table 13. Our system, trained on the sentencelevel annotations of the training and development subsets, is able to correctly classify 85.5%
of the test subset. Note that we ignore the annotations on the word and phrase level as
well as the parse tree structure used by Socher et al. (2013). Even on a non-tweet domain,
employing the automatically generated, tweet-specific lexicons significantly improves the
overall performance: without the use of these lexicons, the performance drops to 83.9%.
Furthermore, our system demonstrates the state-of-the-art performance surpassing the previous best result obtained on this dataset (Socher et al., 2013).
6.2.4 Results for the Term-level Task
Table 14 shows the performance of our sentiment analysis system on the SemEval-2013
term-level task. Our official submission (row c.1) obtained a macro-averaged F-score of
88.93 on the tweet set and was ranked first among 29 submissions from 23 participating
753

fiKiritchenko, Zhu, & Mohammad

Table 12: Message-level task: The macro-averaged F-scores on the Semeval-2013 datasets
for different negation processing strategies. Scores marked with * are statistically
significantly different (p < .05) from the corresponding scores in row c (our best
result).
Experiment
a. Base automatic lexicons
a.1. disregarding negation
a.2. reversing polarity
b. AffLex and NegLex
c. AffLex and NegLex (Positional)

Train.
Set

Dev.
Set

Test Sets
Tweets SMS

66.62*
67.61*
68.13*
68.19

67.36
68.04
68.41
68.43

67.99*
68.95*
69.95*
70.45

65.29*
66.96*
69.59
69.77

Table 13: Message-level task: The results obtained on the movie review excerpts dataset.
System
a. Majority baseline
b. SVM-unigrams
c. Previous best result (Socher et al., 2013)
d. Our system

Accuracy
50.1
71.9
85.4
85.5

teams.23 Even with no tuning specific to SMS data, our system ranked second on the SMS
test set with an F-score of 88.00. The score of the first ranking system on the SMS set was
88.39. A post-competition bug-fix and the use of the Affirmative Context Lexicons and the
Negated Context (Positional) Lexicons resulted in F-score of 89.50 on the tweets set and
88.20 on the SMS set (row c.2). The difference between the best score and the official score
on the tweet test set is statistically significant. The table also shows the baseline results
obtained by a majority classifier that always predicts the most frequent class as output (row
a), and an additional baseline result obtained using an SVM and unigram features alone
(row b).
Table 15 presents the results of the ablation experiments where feature groups are alternately removed from the final model. Observe that the sentiment lexicon features (row
b) are again the most useful groupremoving them leads to a drop in F-score of 45 percentage points on all datasets. Both manual (row b.1) and automatic (row b.2) lexicons
contribute significantly to the overall sentiment analysis system, with the automatic lexicons
consistently showing larger gains.
The ngram features (row c) are the next most useful group on the term-level task. Note
that removing just the word ngram features (row c.1) or just the character ngram features
(row c.2) results in only a small drop in performance. This indicates that the two feature
groups capture similar information.
The last two rows in Table 15 show the results obtained when the features are extracted
only from the context of the target (and not from the target itself) (row f) and when they
are extracted only from the target (and not from its context) (row g). Observe that even
23. The second-best system that used no additional labeled data obtained the score of 86.98 on the tweet
test set.

754

fiSentiment Analysis of Short Informal Texts

Table 14: Term-level task: The macro-averaged F-scores on the SemEval-2013 datasets.
Classifier
a. Majority baseline
b. SVM-unigrams
c. Our system:
c.1. official SemEval-2013 submission
c.2. best result

Train.
Set
38.38
78.04

Dev.
Set
36.34
79.76

Test Sets
Tweets SMS
38.13
32.11
80.28
78.71

86.80
87.03

86.49
87.07

88.93
89.50

88.00
88.20

Table 15: Term-level task: The macro-averaged F-scores obtained on the SemEval-2013
datasets when one of the feature groups is removed. Scores marked with * are
statistically significantly different (p < .05) from the corresponding scores in row
a.
Experiment
a. all features

Train.
Set
87.03

Dev.
Set
87.07

Test Sets
Tweets SMS
89.50
88.20

b. all - lexicons
b.1. all - manual lexicons
b.2. all - automatic lexicons

82.77*
86.16*
85.28*

81.75*
86.22
85.66*

85.56*
88.21*
88.02*

83.52*
87.27*
86.39*

c. all - ngrams
c.1. all - word ngrams
c.2. all - char. ngrams

84.08*
86.65*
86.67*

84.94*
86.30
87.58

85.73*
88.51*
89.20

82.94*
87.02*
87.15*

d. all - stopwords
e. all - encodings (elongated words, emoticons,
punctuation, uppercase)

87.07*

87.08

89.42*

88.07*

87.11

87.08

89.44

88.17

f. all - target
g. all - context

72.65*
83.76*

71.72*
83.95*

74.12*
85.56*

69.37*
86.63*

though the target features are substantially more useful than the context features, adding
the context features to the system improves the F-scores by roughly 2 to 4 points.
The performance of the sentiment analysis system is significantly higher in the term-level
task than in the message-level task. The difference in performance on these two tasks can
also be observed for the SVM-unigrams baseline. We analyzed the provided labeled data
to determine why unigrams performed so strongly in the term-level task, and found that
most of the test target tokens (85.1%) occur as target tokens in the training data. Further,
the distribution of occurrences of a target term in different polarities is skewed towards one
polarity or other. On average, a word appears in target phrases of the same polarity 80.8%
of the time. These facts explain, at least in part, the high overall result and the dominant
role of unigrams in the term-level task. To evaluate the impact of different feature groups
on the test data with unseen target terms, we split the SemEval-2013 tweet test set into
three subsets. Every instance in the first subset, targets fully seen in training, has a
target X (X can be a single word or a multi-word expression) with the following property:
there exist instances in the training data with exactly the same target. The first subset
755

fiKiritchenko, Zhu, & Mohammad

Table 16: Term-level task: The macro-averaged F-scores obtained on the different subsets
of the SemEval-2013 tweet test set with one of the feature groups removed. The
number in brackets is the difference with the scores in row a. Scores marked with
* are statistically significantly different (p < .05) from the corresponding scores
in row a.
Classifier

a. all features
b. all - lexicons
b.1. all - manual lexicons
b.2. all - automatic lexicons
c. all - ngrams

Targets
fully seen
in training
93.31
92.96 (-0.35)
92.94 (-0.37)
92.98 (-0.33)
89.30 (-4.01)*

Targets
partially seen
in training
85.42
81.26 (-4.16)*
84.51 (-0.91)
84.08 (-1.34)
81.61 (-3.81)*

Targets
unseen
in training
84.09
69.55 (-14.54)*
79.33 (-4.76)*
79.41 (-4.68)*
80.62 (-3.47)*

Table 17: Term-level task: The macro-averaged F-scores on the SemEval-2013 datasets for
different negation processing strategies. Scores marked with * are statistically
significantly different (p < .05) from the corresponding scores in row c (our best
result).
Experiment
a. Base automatic lexicons
a.1. disregarding negation
a.2. reversing polarity
b. AffLex and NegLex
c. AffLex and NegLex (Positional)

Train.
Set

Dev.
Set

Test Sets
Tweets SMS

85.88*
86.85
86.89
87.03

86.37*
86.48*
86.60*
87.07

88.38*
89.10*
89.33
89.50

86.77*
88.34
87.89
88.20

comprises 55% of the test set. Every instance in the second subset, targets partially seen in
training, has a target X with the following property: there exist instances in the training
data whose target expression includes one or more, but not all, tokens in X. The second
subset comprises 31% of the test set. Every instance in the third subset, targets unseen
in training, has a target X with the following property: there are no instances in the
training data whose target includes any of the tokens in X. The third subset comprises
14% of the test set. Table 16 shows the results of the ablation experiments on these three
subsets. Observe that on the instances with unseen targets the sentiment lexicons play a
more prominent role, providing a substantial gain (14.54 percentage points).
In the next set of experiments, we compare the performance of different approaches
to negation handling on the term-level task (Table 17). Similar to the message-level task,
processing negation proves beneficial on the term-level task as well. All tested negation
processing approaches show better results than the default strategy of disregarding negation
(row a.1). The use of the Affirmative Context Lexicons and the Negated Context Lexicons
(row b) and especially the Negated Context (Positional) Lexicons (row c) provides additional
gains over the results obtained through the use of the polarity reversing method (row a.2).
756

fiSentiment Analysis of Short Informal Texts

7. Conclusions
We created a supervised statistical sentiment analysis system that detects the sentiment of
short informal textual messages such as tweets and SMS (message-level task) as well as the
sentiment of a term (a word or a phrase) within a message (term-level task). The system
ranked first in both tasks at the SemEval-2013 competition Sentiment Analysis in Twitter.
Moreover, it demonstrated the state-of-the-art performance on two additional datasets: the
SemEval-2013 SMS test set and a corpus of movie review excerpts.
In this system, we implemented a variety of features based on surface form and lexical
categories. We also included features derived from several sentiment lexicons: (1) existing,
manually created, general-purpose lexicons and (2) high-coverage, tweet-specific lexicons
that we generated from tweets with sentiment-word hashtags and from tweets with emoticons. Our experiments showed that the new tweet-specific lexicons are superior in sentiment
prediction on tweets in both unsupervised and supervised settings.
Processing negation plays an important role in sentiment analysis. Many previous studies adopted a simple technique to reverse polarity of words in the scope of negation. In
this work, we demonstrated that this polarity reversing method may not be always appropriate. In particular, we showed that when positive terms are negated, they tend to
convey a negative sentiment. In contrast, when negative terms are negated, they tend to
still convey a negative sentiment. Furthermore, the evaluative intensity for both positive
and negative terms changes in a negated context, and the amount of change varies from
term to term. To adequately capture the impact of negation on individual terms, we proposed to empirically estimate the sentiment scores of terms in negated context from large
tweet corpora, and built two lexicons, one for terms in negated contexts and one for terms
in affirmative (non-negated) contexts. By using these Affirmative Context Lexicons and
Negated Context Lexicons we were able to significantly improve the performance of the
overall sentiment analysis system on both tasks. In particular, the features derived from
these lexicons provided gains of up to 6.5 percentage points over the other feature groups.
Our system can process 100 tweets in a second. Thus, it is suitable for small- and bigdata versions of applications listed in the introduction. We recently annotated 135 million
tweets over a cluster of 50 machines in 11 hours. We have already employed the sentiment analysis system within larger systems for detecting intentions behind political tweets
(Mohammad, Kiritchenko, & Martin, 2013), for detecting emotions in text (Mohammad &
Kiritchenko, 2014), and for detecting sentiment towards particular aspects of target entities
(Kiritchenko et al., 2014). We are also interested in applying and evaluating the lexicons
generated from tweets on data from other kinds of text such as blogs and news articles.
In addition, we plan to adapt our sentiment analysis system to languages other than English. Along the way, we continue to improve the sentiment lexicons by generating them
from larger amounts of data, and from different kinds of data, such as tweets, blogs, and
Facebook posts. We are especially interested in algorithms that gracefully handle all kinds
of sentiment modifiers including not only negations, but also intensifiers (e.g., very, hardly),
and discourse connectives (e.g., but, however).
757

fiKiritchenko, Zhu, & Mohammad

Acknowledgments
We thank Colin Cherry for providing his SVM code and for helpful discussions.

References
Agarwal, A., Xie, B., Vovsha, I., Rambow, O., & Passonneau, R. (2011). Sentiment analysis
of twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM
11, pp. 3038, Portland, Oregon.
Aisopos, F., Papadakis, G., Tserpes, K., & Varvarigou, T. (2012). Textual and contextual
patterns for sentiment analysis over microblogs. In Proceedings of the 21st International Conference on World Wide Web Companion, WWW 12 Companion, pp.
453454, New York, NY, USA.
Almquist, E., & Lee, J. (2009). What do customers really want?. Harvard Business Review.
Baccianella, S., Esuli, A., & Sebastiani, F. (2010). SentiWordNet 3.0: an enhanced lexical
resource for sentiment analysis and opinion mining. In Proceeding of the 7th International Conference on Language Resources and Evaluation, Vol. 10 of LREC 10, pp.
22002204.
Bakliwal, A., Arora, P., Madhappan, S., Kapre, N., Singh, M., & Varma, V. (2012). Mining sentiments from tweets. In Proceedings of the 3rd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis, WASSA 12, pp. 1118, Jeju, Republic of Korea.
Becker, L., Erhart, G., Skiba, D., & Matula, V. (2013). Avaya: Sentiment analysis on
twitter with self-training and polarity lexicon expansion. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pp. 333340, Atlanta,
Georgia, USA.
Bellegarda, J. (2010). Emotion analysis using latent affective folding and embedding. In
Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to
Analysis and Generation of Emotion in Text, Los Angeles, California.
Boucher, J. D., & Osgood, C. E. (1969). The Pollyanna Hypothesis. Journal of Verbal
Learning and Verbal Behaviour, 8, 18.
Boucouvalas, A. C. (2002). Real time text-to-emotion engine for expressive internet communication. Emerging Communication: Studies on New Technologies and Practices
in Communication, 5, 305318.
Brody, S., & Diakopoulos, N. (2011). Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word
lengthening to detect sentiment in microblogs. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP 11, pp. 562570,
Stroudsburg, PA, USA.
Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2 (3), 27:127:27.
Choi, Y., & Cardie, C. (2008). Learning with compositional semantics as structural inference
for subsentential sentiment analysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP 08, pp. 793801.
758

fiSentiment Analysis of Short Informal Texts

Choi, Y., & Cardie, C. (2010). Hierarchical sequential learning for extracting opinions
and their attributes. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics, ACL 10, pp. 269274.
Davidov, D., Tsur, O., & Rappoport, A. (2010). Enhanced sentiment learning using Twitter hashtags and smileys. In Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING 10, pp. 241249, Beijing, China.
Esuli, A., & Sebastiani, F. (2006). SENTIWORDNET: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th Conference on Language Resources
and Evaluation, LREC 06, pp. 417422.
Francisco, V., & Gervas, P. (2006). Automated mark up of affective information in English
texts. In Sojka, P., Kopecek, I., & Pala, K. (Eds.), Text, Speech and Dialogue, Vol.
4188 of Lecture Notes in Computer Science, pp. 375382. Springer Berlin / Heidelberg.
Genereux, M., & Evans, R. P. (2006). Distinguishing affective states in weblogs. In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing
Weblogs, pp. 2729, Stanford, California.
Gimpel, K., Schneider, N., OConnor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M.,
Yogatama, D., Flanigan, J., & Smith, N. A. (2011). Part-of-speech tagging for Twitter:
Annotation, features, and experiments. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, ACL 11.
Go, A., Bhayani, R., & Huang, L. (2009). Twitter sentiment classification using distant
supervision. Tech. rep., Stanford University.
Hanley, J., & McNeil, B. (1982). The meaning and use of the area under a Receiver Operating Characteristic (ROC) curve. Radiology, 143, 2936.
Hatzivassiloglou, V., & McKeown, K. R. (1997). Predicting the semantic orientation of adjectives. In Proceedings of the 8th Conference of European Chapter of the Association
for Computational Linguistics, EACL 97, pp. 174181, Madrid, Spain.
Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews. In Proceedings of
the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD 04, pp. 168177, New York, NY, USA. ACM.
Jia, L., Yu, C., & Meng, W. (2009). The effect of negation on sentiment analysis and
retrieval effectiveness. In Proceedings of the 18th ACM Conference on Information
and Knowledge Management, CIKM 09, pp. 18271830, New York, NY, USA. ACM.
Jiang, L., Yu, M., Zhou, M., Liu, X., & Zhao, T. (2011). Target-dependent Twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics, ACL 11, pp. 151160.
Johansson, R., & Moschitti, A. (2013). Relational features in fine-grained opinion analysis.
Computational Linguistics, 39 (3), 473509.
John, D., Boucouvalas, A. C., & Xu, Z. (2006). Representing emotional momentum within
expressive internet communication. In Proceedings of the 24th International Conference on Internet and Multimedia Systems and Applications, pp. 183188, Anaheim,
CA. ACTA Press.
759

fiKiritchenko, Zhu, & Mohammad

Jurgens, D., Mohammad, S. M., Turney, P., & Holyoak, K. (2012). Semeval-2012 task 2:
Measuring degrees of relational similarity. In Proceedings of the 6th International
Workshop on Semantic Evaluation, SemEval 12, pp. 356364, Montreal, Canada.
Kennedy, A., & Inkpen, D. (2005). Sentiment classification of movie and product reviews
using contextual valence shifters. In Proceedings of the Workshop on the Analysis
of Informal and Formal Information Exchange during Negotiations, Ottawa, Ontario,
Canada.
Kennedy, A., & Inkpen, D. (2006). Sentiment classification of movie reviews using contextual
valence shifters. Computational Intelligence, 22 (2), 110125.
Kiritchenko, S., Zhu, X., Cherry, C., & Mohammad, S. M. (2014). NRC-Canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval 14, Dublin, Ireland.
Kouloumpis, E., Wilson, T., & Moore, J. (2011). Twitter sentiment analysis: The Good
the Bad and the OMG!. In Proceedings of the 5th International AAAI Conference on
Weblogs and Social Media.
Lapponi, E., Read, J., & Ovrelid, L. (2012). Representing and resolving negation for sentiment analysis. In Vreeken, J., Ling, C., Zaki, M. J., Siebes, A., Yu, J. X., Goethals,
B., Webb, G. I., & Wu, X. (Eds.), ICDM Workshops, pp. 687692. IEEE Computer
Society.
Li, J., Zhou, G., Wang, H., & Zhu, Q. (2010). Learning the scope of negation via shallow semantic parsing. In Proceedings of the 23rd International Conference on Computational
Linguistics, COLING 10, pp. 671679, Beijing, China.
Liu, B., & Zhang, L. (2012). A survey of opinion mining and sentiment analysis. In
Aggarwal, C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 415463. Springer US.
Liu, H., Lieberman, H., & Selker, T. (2003). A model of textual affect sensing using realworld knowledge. In Proceedings of the 8th International Conference on Intelligent
User Interfaces, IUI 03, pp. 125132, New York, NY. ACM.
Louviere, J. J. (1991). Best-worst scaling: A model for the largest difference judgments.
Working Paper.
Martnez-Camara, E., Martn-Valdivia, M. T., Urenalopez, L. A., & Montejoraez, A. R.
(2012). Sentiment analysis in Twitter. Natural Language Engineering, 128.
Mihalcea, R., & Liu, H. (2006). A corpus-based approach to finding happiness. In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing
Weblogs, pp. 139144. AAAI Press.
Mohammad, S. M. (2012). #Emotional tweets. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, *SEM 12, pp. 246255, Montreal, Canada.
Mohammad, S. M., Dunne, C., & Dorr, B. (2009). Generating high-coverage semantic
orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the
Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP
09, pp. 599608.
760

fiSentiment Analysis of Short Informal Texts

Mohammad, S. M., & Kiritchenko, S. (2014). Using hashtags to capture fine emotion
categories from tweets. To appear in Computational Intelligence.
Mohammad, S. M., Kiritchenko, S., & Martin, J. (2013). Identifying purpose behind electoral tweets. In Proceedings of the 2nd International Workshop on Issues of Sentiment
Discovery and Opinion Mining, WISDOM 13, pp. 19.
Mohammad, S. M., & Turney, P. D. (2010). Emotions evoked by common words and
phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the
NAACL-HLT Workshop on Computational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Mohammad, S. M., & Yang, T. W. (2011). Tracking sentiment in mail: How genders differ on
emotional axes. In Proceedings of the ACL Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis, WASSA 11, Portland, OR, USA.
Neviarouskaya, A., Prendinger, H., & Ishizuka, M. (2011). Affect analysis model: novel
rule-based approach to affect sensing from text. Natural Language Engineering, 17,
95135.
Orme, B. (2009). Maxdiff analysis: Simple counting, individual-level logit, and HB. Sawtooth Software, Inc.
Pak, A., & Paroubek, P. (2010). Twitter as a corpus for sentiment analysis and opinion
mining. In Proceedings of the 7th Conference on International Language Resources
and Evaluation, LREC 10, Valletta, Malta.
Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends
in Information Retrieval, 2 (12), 1135.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP 02, pp. 7986, Philadelphia, PA.
Polanyi, L., & Zaenen, A. (2004). Contextual valence shifters. In Exploring Attitude and
Affect in Text: Theories and Applications (AAAI Spring Symposium Series).
Porter, M. (1980). An algorithm for suffix stripping. Program, 3, 130137.
Proisl, T., Greiner, P., Evert, S., & Kabashi, B. (2013). Klue: Simple and robust methods for
polarity classification. In Proceedings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), pp. 395401, Atlanta, Georgia, USA.
Reckman, H., Baird, C., Crawford, J., Crowell, R., Micciulla, L., Sethi, S., & Veress, F.
(2013). teragram: Rule-based detection of sentiment phrases using sas sentiment
analysis. In Proceedings of the 7th International Workshop on Semantic Evaluation
(SemEval 2013), pp. 513519, Atlanta, Georgia, USA.
Sauper, C., & Barzilay, R. (2013). Automatic aggregation by joint modeling of aspects and
values. Journal of Artificial Intelligence Research, 46, 89127.
Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP 12, Jeju, Korea.
761

fiKiritchenko, Zhu, & Mohammad

Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., & Potts,
C. (2013). Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP 13, Seattle, USA.
Stone, P., Dunphy, D. C., Smith, M. S., Ogilvie, D. M., & associates (1966). The General
Inquirer: A Computer Approach to Content Analysis. The MIT Press.
Taboada, M., Brooke, J., Tofiloski, M., Voll, K., & Stede, M. (2011). Lexicon-based methods
for sentiment analysis. Computational Linguistics, 37 (2), 267307.
Thelwall, M., Buckley, K., & Paltoglou, G. (2011). Sentiment in Twitter events. Journal
of the American Society for Information Science and Technology, 62 (2), 406418.
Turney, P. (2001). Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In
Proceedings of the Twelfth European Conference on Machine Learning, pp. 491502,
Freiburg, Germany.
Turney, P., & Littman, M. L. (2003). Measuring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on Information Systems, 21 (4).
Wiebe, J., Wilson, T., & Cardie, C. (2005). Annotating expressions of opinions and emotions
in language. Language resources and evaluation, 39 (2-3), 165210.
Wiegand, M., Balahur, A., Roth, B., Klakow, D., & Montoyo, A. (2010). A survey on the
role of negation in sentiment analysis. In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, NeSp-NLP 10, pp. 6068, Stroudsburg,
PA, USA.
Wilson, T., Kozareva, Z., Nakov, P., Rosenthal, S., Stoyanov, V., & Ritter, A. (2013).
SemEval-2013 Task 2: Sentiment analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval 13, Atlanta, Georgia, USA.
Wilson, T., Wiebe, J., & Hoffmann, P. (2005). Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT 05, pp. 347354,
Stroudsburg, PA, USA.
Yang, B., & Cardie, C. (2013). Joint inference for fine-grained opinion extraction. In
Proceedings of the Annual Meeting of the Association for Computational Linguistics,
ACL 13.
Yeh, A. (2000). More accurate tests for the statistical significance of result differences. In
Proceedings of the 18th conference on Computational linguistics - Volume 2, COLING
00, pp. 947953, Stroudsburg, PA, USA.

762

fiJournal of Artificial Intelligence Research 50 (2014) 603-637

Submitted 02/14; published 07/14

Probabilistic Inference in Credal Networks:
New Complexity Results
Denis Deratani Maua

denis.maua@usp.br

Escola Politecnica, Universidade de Sao Paulo
Av. Prof. Luciano Gualberto, travessa 3, 380
Sao Paulo, 05508-010 Brazil

Cassio Polpo de Campos
Alessio Benavoli
Alessandro Antonucci

cassio@idsia.ch
alessio@idsia.ch
alessandro@idsia.ch

Istituto Dalle Molle di Studi sullIntelligenza Artificiale
Galleria 2
Manno, 6928 Switzerland

Abstract
Credal networks are graph-based statistical models whose parameters take values in a
set, instead of being sharply specified as in traditional statistical models (e.g., Bayesian
networks). The computational complexity of inferences on such models depends on the
irrelevance/independence concept adopted. In this paper, we study inferential complexity
under the concepts of epistemic irrelevance and strong independence. We show that inferences under strong independence are NP-hard even in trees with binary variables except
for a single ternary one. We prove that under epistemic irrelevance the polynomial-time
complexity of inferences in credal trees is not likely to extend to more general models
(e.g., singly connected topologies). These results clearly distinguish networks that admit
efficient inferences and those where inferences are most likely hard, and settle several open
questions regarding their computational complexity. We show that these results remain
valid even if we disallow the use of zero probabilities. We also show that the computation
of bounds on the probability of the future state in a hidden Markov model is the same
whether we assume epistemic irrelevance or strong independence, and we prove a similar
result for inference in naive Bayes structures. These inferential equivalences are important
for practitioners, as hidden Markov models and naive Bayes structures are used in real
applications of imprecise probability.

1. Introduction
Bayesian networks are multivariate probabilistic models where stochastic independence assessments are compactly represented by an acyclic directed graph whose nodes are identified
with variables (Pearl, 1988). In addition to its acyclic directed graph, the specification of
a Bayesian network requires the specification of a conditional probability distribution for
every variable and every assignment of its parents. When information is costly to acquire,
specifying these conditional probabilities can be a daunting task, whether they are estimated from data or elicited from experts. This causes the inferences drawn with the model
to contain imprecisions and arbitrarinesses (Kwisthout & van der Gaag, 2008).
An arguably more principled approach to coping with the imprecision in the numerical
parameters is by incorporating it into the formalism. One way of doing so is by means of
c
2014
AI Access Foundation. All rights reserved.

fiMaua, de Campos, Benavoli & Antonucci

closed and convex sets of probability distributions, which are called credal sets (Levi, 1980).1
Bayesian networks whose numerical parameters are specified by conditional credal sets are
known as credal networks (Cano, Cano, & Moral, 1994; Cozman, 2000, 2005). Credal
networks have been successfully applied to robust pattern recognition,2 and to knowledgebased systems, where it has been argued that allowing parameters to be imprecisely specified
facilitates elicitation from experts.3
A Bayesian network provides a concise representation of the (single) joint probability
distribution that is consistent with the network parameters and satisfies (at least) the set
of stochastic independences encoded in its underlying graph. Analogously, a credal network
provides a concise representation of the credal set of joint distributions that are consistent
with the local credal sets and satisfy (at least) the irrelevances encoded in its underlying
graph. The precise characterization of that joint credal set depends however on the concept
of irrelevance adopted.
The two most commonly used irrelevance concepts in the literature are strong independence and epistemic irrelevance. Two variables X and Y are strongly independent if
their joint credal set can be regarded as originating from a number of precise probability
distributions under each of which the two variables are stochastically independent. Strong
independence follows a sensitivity analysis interpretation, which regards imprecision in the
model as arising out of partial ignorance about an ideal precise model. Epistemic irrelevance, on the other hand, is defined irrespective of the existence of any ideal precise model.
A variable X is epistemically irrelevant to a variable Y if the marginal credal set of Y
coincides with the conditional credal set of Y given X. Unlike strong independence, epistemic irrelevance is an asymmetric concept and cannot in general be characterized by the
properties of the elements of the credal set alone (de Cooman et al., 2010).
If on the one hand the flexibility provided by credal sets arguably facilitates model
building, on the other, it imposes a great burden on the computation of inferences. For
example, whereas the posterior probability of a variable is polynomial-time computable
in polytree-shaped Bayesian networks, the analogous task of computing upper and lower
bounds on the posterior probability of a given variable in a polytree-shaped credal networks
is an NP-hard task (de Campos & Cozman, 2005). There are however exceptional cases,
such as the case of inference in polytree-shaped credal networks with binary variables,
which can be solved in polynomial time under strong independence (Fagiuoli & Zaffalon,
1998). Like in Bayesian networks, the theoretical and practical tractability of inferences
in credal networks depends strongly on the network topology and the cardinality of the
variable domains. Credal networks however include another dimension in the parametrized
complexity of inference, given by the type of irrelevance concept adopted, which in the
1. Other approaches include random sets (Kendall, 1974), evidence theory (Shafer, 1976; Shenoy & Shafer,
1988), possibility theory (Zadeh, 1978), conditional plausibility measures (Halpern, 2001), and coherent
lower previsions (Walley, 1991; de Cooman & Miranda, 2012), the last one being largely equivalent to
credal sets (there is an one-to-one correspondence between credal sets and coherent lower previsions).
2. For example, see the works of Zaffalon, Wesnes, and Petrini (2003), Zaffalon (2005), de Campos, Zhang,
Tong, and Ji (2009), Antonucci, Bruhlmann, Piatti, and Zaffalon (2009), Corani, Giusti, Migliore, and
Schmidhuber (2010), Antonucci, de Rosa, and Giusti (2011), de Campos and Ji (2011).
3. For example, see the works of Walley (2000), Antonucci, Piatti, and Zaffalon (2007), Salvetti, Antonucci,
and Zaffalon (2008), de Campos and Ji (2008), Antonucci et al. (2009), Piatti, Antonucci, and Zaffalon
(2010), Antonucci, Huber, Zaffalon, Luginbuhl, Chapman, and Ladouceur (2013).

604

fiProbabilistic Inference in Credal Networks: New Complexity Results

Bayesian case is usually fixed. For instance, computing probability bounds in tree-shaped
credal networks under the concept of epistemic irrelevance can be performed in polynomial
time (de Cooman et al., 2010), whereas we show here that the same task is NP-hard under
strong independence.
In the rest of this paper, we properly define credal networks and the inference problem
we address (sect. 2), and investigate the parametrized theoretical computational complexity
of inferences in credal networks (sect. 3), both under strong independence and epistemic
irrelevance. We show that a particular type of inference in imprecise hidden Markov models
(i.e., hidden Markov models with uncertainty quantified by local credal sets) is invariant
to the choice of either irrelevance concept, being thus polynomial-time computable (as this
is known to be the case under epistemic irrelevance). We obtain as corollaries of that
result that inferences under strong independence and epistemic irrelevance coincide also
in tree-shaped networks if no evidence is given, and in naive Bayes structures. We also
show that even in tree-shaped credal networks inferences are NP-hard if we assume strong
independence, and that this is the same complexity of inference in polytree-shaped credal
networks for both irrelevance concepts, even if we assume that all variables are (at most)
ternary. We prove that the so-called precise-vacuous models, that is, credal networks that
have vacuous root nodes and precise non-root nodes, lead to the same inferences whether
we assume epistemic irrelevance or strong independence, and that the same is true (apart
from an arbitrarily small error) when vacuous nodes are replaced by near-vacuous ones,
avoiding the problematic case of zero probabilities. This last fact proves that our hardness
results hold true even in cases where the lower probability of any possible event is strictly
positive.

2. Updating Credal Networks
In this section, we review the necessary concepts and definitions, and formalize the problem
of inference in credal networks.
2.1 Bayesian Networks
Consider a finite set X = {X1 , . . . , Xn } of categorical variables, and let Z  X be some
set of variables. A probability distributionPp of Z is a non-negative real-valued function on
the space of assignments of Z such that zZ p(z) = 1, where the notation z  Z entails
that z is an arbitrary (joint) assignment or configuration of the variables in Z. Any joint
probability distribution p induces a probability measure Pp on the sigma-field of all subsets
of assignments of Z.
Let G be an acyclic directed graph (DAG) with nodes N = {1, . . . , n}. We denote
the parents of a node i in G by Pa(i). The set of non-descendants of i, written Nd(i),
contains the nodes not reachable from i by a directed path. Note that Pa(i)  Nd(i).
Fix a probability measure P on the sigma-field of subsets of X and associate every node
i with a variable Xi . The DAG G represents the following set of stochastic independence
assessments known as local Markov conditions:
P(Xi = xi |XNd(i) = xNd(i) ) = P(Xi = xi |XPa(i) = xPa(i) ) ,
605

(1)

fiMaua, de Campos, Benavoli & Antonucci

for all i  N , and x  X. In words, every variable is stochastically independent from its
non-descendant non-parents given its parents under a suitable measure P.
A Bayesian network is a triple (X, G, Q), where Q is the set of conditional probability
assessments
P(Xi = xi |XPa(i) = xPa(i) ) = q(xi |xPa(i) ) ,
(2)
for all i  N , xi  Xi and xPa(i)  XPa(i) , where q(Xi |xPa(i) ) is a probability distribution of
Xi . By assumption, a Bayesian network defines a joint probability distribution p of X by
Y
p(x) =
q(xi |xPa(i) ) ,
(3)
iN

for all x  X. It is not difficult to see that (1) and (2) imply (3) by the Chain Rule using a
topological ordering of variables. That (3) and (2) imply (1) is a bit more intricate to see
but also true (Cowell, Dawid, Lauritzen, & Spiegelhalter, 2007). Thus, these two seemingly
different approaches to specifying a probability measure are virtually equivalent. To be
more explicit: given a Bayesian network, the probability measure that satisfies (1) and
(2) is the same probability measure that satisfies (3) and (2), and we can choose any pair
of assumptions to define a (single) measure for a network. As we shall see, an analogous
equivalence is not observed when probabilities are imprecisely specified, which leads to
different definitions of credal networks with different computational complexity.
2.1.1 Probabilistic Inference In Bayesian Networks
An essential task in many applications of probabilistic modeling is to compute a certain
probability value implied by a Bayesian network. We call such a computational task the
BN-INF problem, and define it as follows.
BN-INF
Input: A Bayesian network (X, G, Q), a target node t, a target value xt of Xt ,
a (possibly empty) set of evidence nodes O, and an assignment xO to XO .
Output: The conditional probability P(Xt = xt |XO = xO ), where P is the probability measure specified by the network.
In the problem above we assume that when P(XO = xO ) = 0 the output is a special symbol
(e.g. ) indicating the problem solution is undefined.
Roth (1996) showed that BN-INF is #P-hard, defining a lower bound to the complexity
of the problem. All known (exact) algorithms take time at least exponential in the treewidth
of the network in the worst case. The treewidth is a measure of the resemblance of a network
to a tree; small treewidth suggests a tree-like structure and the treewidth of a tree is one
and is minimal (Koller & Friedman, 2009). Recently, Kwisthout, Bodlaender, and van der
Gaag (2010) proved that contingent on the hypothesis that satisfiability of Boolean formulas
takes exponential time in the worst-case (known as ETH) this is the best performance an
algorithm for BN-INF can achieve. As we shall see in the next section, Bayesian networks
are particular instances of credal networks. As such, these complexity results set lower
bounds on the complexity of inference in credal networks.
A DAG is said to be singly connected if there is at most one undirected path connecting
any two nodes in the graph; it is a tree if additionally each node has at most one parent. If
606

fiProbabilistic Inference in Credal Networks: New Complexity Results

a graph is not singly connected, we say it is multiply connected. Singly connected directed
graphs are also called polytrees. Pearls belief propagation algorithm (Pearl, 1988) computes
BN-INF in polynomial time in singly connected Bayesian networks. More generally, the
junction tree propagation algorithm (Cowell et al., 2007) solves BN-INF in polynomial time
in any network of bounded treewidth, which includes singly connected networks of bounded
in-degree (i.e., maximum number of parents).
2.2 Credal Networks
In this section we describe credal sets, irrelevance concepts, credal networks and probabilistic
inference in credal networks.
2.2.1 Credal Sets
Let Z  X. A credal set M is a closed and convex set of joint probability distributions
on the same domain, say z  Z (Levi, 1980). The vacuous credal set of Z is the largest
credal set on that domain, and is denoted by V (Z). An extreme distribution of a credal set
is an element of the set that cannot be written as a convex combination of other elements
in the same set. We denote the set of extreme distributions of a credal set M by ext M .
A credal set is finitely generated if it contains a finite number of extreme distributions. A
finite representation of a finitely generated credal set by means of its extreme distributions
is called vertex-based. Any finitely generated credal set of Z defines a (bounded) polytope
in the probability simplex of distributions of Z, and can be specified through a finite set of
linear inequalities of the form
def

Ep (fl ) =

X

fl (z)p(z)  0 ,

(4)

zZ

where {fl } is a finite collection of real-valued functions of Z (Cozman, 2000). The converse
is also true: any finite set of linear inequalities of the form above determines a (bounded)
polytope in the probability simplex (Boyd & Vandenberghe, 2004, ch. 2), and hence a
finitely generated credal set. Thus, an alternative finite representation of a credal set is
by means of a finite set of functions defining linear inequalities of the type above. Such a
representation is called constraint-based.
Example 1. Consider X = {X1 , X2 }, where X1 takes values in {0, 1, 2} and X2 takes
values in {0, 1}. The vacuous set of X1 is the probability simplex on the plane, drawn as a
triangle with vertices (p(0), p(1), p(2)) = (0, 0, 1), (0, 1, 0) and (1, 0, 0) in Figure 1. Let
M (X1 |X2 = 0) = {p  V (X1 ) : p(k)  1/3, k = 1, 2}
and
M (X1 |X2 = 1) = {p  V (X1 ) : p(0)  p(1)  p(2)}
be conditional credal sets for X1 given X2 , and M (X2 ) be the singleton containing the
distribution p of X2 such that p(0) = p(1) = 1/2. The first two sets are depicted in Figure 1.
607

fiMaua, de Campos, Benavoli & Antonucci

(0, 1, 0)

(0, 1, 0)

p5
p3

(0, 0, 1)

p2

p4

p3
p1
(1, 0, 0)

p1
(1, 0, 0)

(0, 0, 1)

(a) M (X1 |X2 = 0)

(b) M (X1 |X2 = 1)

Figure 1: Barycentric coordinate-system visualization of the conditional credal sets in Example 1 (hatched regions) and their corresponding extreme distributions (black circles).
Let us represent a generic function f on {0, . . . , m} by the m-tuple (f (0), . . . , f (m)), and
define
p1 = (1, 0, 0) ,

p2 = (2/3, 1/3, 0) ,

p3 = (1/3, 1/3, 1/3) ,

p4 = (2/3, 0, 1/3) ,

p5 = (1/2, 1/2, 0) ,

f1 = (1, 2, 1) ,

f2 = (1, 1, 2) ,

f3 = (1, 1, 0) ,

f4 = (0, 1, 1) ,

f5 = (1, 1) .

Then the set M (X1 |X2 = 0) can be represented in vertex- and constraint-based form, respectively, as M (X1 |X2 = 0) = co{p1 , p2 , p3 , p4 } (co denotes the convex hull operator) and
M (X1 |X2 = 0) = {p  V (X1 ) : Ep (f1 )  0, Ep (f2 )  0}, while the set M (X1 |X2 = 1) is
represented in vertex- and constraint-based forms as M (X1 |X2 = 1) = co{p1 , p3 , p5 } and
M (X1 |X2 = 1) = {p  V (X1 ) : Ep (f3 )  0, Ep (f4 )  0}, respectively. Similarly, M (X2 )
can be represented as M (X2 ) = {(1/2, 1/2)} in vertex-based form, and as M (X2 ) = {p 
V (X2 ) : Ep (f5 )  0, Ep (f5 )  0} in constraint-based form.
Vertex- and constraint-based representations of the same credal set can have very different sizes. To see this, consider a single variable X taking values in {0, . . . , m}, and let
M = {p  V (X) : p(k)  1/(m + 1), k = 1, . . . , m}. The set M is isomorphic to an mdimensional hypercube, and therefore has 2m extreme distributions,4 whereas the same set
can be represented in constraint-based form by m degenerate functions of X translated by
1/(m + 1). Moving from a vertex-based to a constraint-based representation can also result
in an exponential increase in the size ofP
the input. Consider a variable XPtaking values in
m
{0, . . . , m} and let M = {f (X)  0 : m
k=1 |f (k)  1/(2m)|  1/(2m),
k=0 f (k) = 1}.
The set M is affinely equivalent to the m-dimensional cross-polytope, and hence requires
2m inequalities to be described in constraint-based form, while it can be represented in
4. For any non-negative integer k not greater than m and any (potentially empty) subset S of {1, . . . , m} of
cardinality k, any distribution that assigns mass (m + 1  k)/(m + 1) to p(0), mass 1/(m + 1) to p(j) such
that j is in S, and zero mass elsewhere, is in M , since it satisfies all the constraints in M and is a valid
distribution. There are 2m such distributions, and each one cannot be written as a convex combination
of any other distribution in the set.

608

fiProbabilistic Inference in Credal Networks: New Complexity Results

vertex-based form by its 2m extreme distribution (Kalai & Ziegler, 2000, p. 11).5 Tessem
(1992) and de Campos, Huete, and Moral (1994) studied the representation of credal sets
defined by linear constraints of the form lx  p(x)  ux , where lx and ux are real numbers,
and showed that these credal sets can have exponentially many extreme distributions in the
number of constraints. Wallner (2007) proved an attainable upper bound of m! extreme
distributions on credal sets more generally defined by a coherent lower probability function
of an m-ary variable. More recently, Miranda and Destercke (2013) investigated the number
of extreme distributions in credal sets defined by linear constraints of the form p(x)  p(x0 )
for x 6= x0 , and proved an attainable upper bound of 2m1 extreme distributions for the
case of an m-ary variable. Importantly, both vacuous credal sets (of variables of any cardinality) and credal sets of binary variables can be succinctly represented in either vertexor constraint-based form. Some of the complexity results we obtain later on use vacuous
credal sets and/or binary variables and are thus representation independent.
2.2.2 Graph-Based Representation
So far we have only considered the explicit representation of a finitely generated credal set
by a finite number of functions representing either the vertices of the set or a set of linear
inequalities. Our final goal is however to be able to specify credal sets on large domains
x  X. For a large set X, such an explicit representation is both too difficult to obtain and
too large to manipulate in a computer. Thus, analogously to the more efficient graph-based
representation of a large probability distribution given by a Bayesian network, a large joint
credal set is usually more efficiently represented implicitly as the credal set that satisfies all
irrelevances encoded in a given graph while agreeing on its projection with all local credal
sets, where the latter are credal sets that can be efficiently represented (either in vertex- or
constraint-based form) explicitly by functions of only small subsets of X.
A (separately specified) credal network N is a triple (X, G, Q), where G is a DAG with
nodes N , and Q is a set of imprecise probability assessments
X
X
f :
f (xi )P(Xi = xi |XPa(i) = xPa(i) ) 
min
f (xi )q(xi ) ,
(5)
qQ(Xi |xPa(i) )

xi Xi

xi Xi

one for every i  N and xPa(i)  XPa(i) , where each Q(Xi |xPa(i) ) is a credal set of Xi and
f is an arbitrary real-valued function of Xi . Note that we left unspecified how these credal
sets are represented.
Example 2. Consider the credal network N over variables X1 , X2 and X3 that take values
in {0, 1}, and with graph structure as shown in Figure 2. The local credal sets are
Q(X1 ) = {p  V (X1 ) : 0.5  p(1)  0.6} = co{(0.4, 0.6), (0.5, 0.5)} ,
Q(X2 ) = {p  V (X2 ) : 0.5  p(1)  0.6} = co{(0.4, 0.6), (0.5, 0.5)} ,
and Q(X3 |X1 = i, X2 = j) = {pij } for any i and j, where pij is the probability distribution
on {0, 1} such that pij (1) = 0 if i = j and pij (1) = 1 otherwise.
P
5. The m-dimensional cross-polytope is the set {f (X) : x |f (x)|  1}, whose extreme distributions are
{e1 , . . . , em }, where ek (k = 1, . . . , m) is the degenerate distribution placing all mass at X = k. The
fact that that set cannot be written with less than 2m inequalities follows from it being the dual of the
m-dimensional hypercube.

609

fiMaua, de Campos, Benavoli & Antonucci

1

2
3

Figure 2: DAG of the credal network in Example 2.

A node i in the network and its associated variable Xi are said to be precise (i.e., precisely specified) if the corresponding conditional credal sets Q(Xi |xPa(i) ) are all singletons,
otherwise they are said to be imprecise (i.e., imprecisely specified). In Example 2 variable
X3 and its corresponding associated node are precise, while X1 and X2 are imprecise. If all
of its local credal sets are vacuous, the node and its corresponding variable are said to be
vacuous. A Bayesian network is simply a credal network with all nodes precise.
The DAG G of a credal network specifies a set of conditional irrelevances between sets
of variables which generalize the Markov condition in Bayesian networks. More specifically,
for any node i in G, the set XNd(i)\Pa(i) of non-descendant non-parent variables of Xi is
assumed irrelevant to Xi conditional on its parent variables XPa(i) . The precise definition of
this statement requires the definition of an irrelevance concept. For instance, if stochastic
independence is adopted as irrelevance concept, then the DAG G describes a set of Markov
conditions as a Bayesian network (stochastic irrelevance implies stochastic independence).
In the credal network formalism, the two most common irrelevance concepts used are strong
independence and epistemic irrelevance.
Fix a joint credal set M of probability distributions of X, and consider subsets Y,
Z and W of X. We say that a set of variables Y is strongly independent of a set of
variables Z given variables W if Y and Z are stochastically independent conditional on
W under every extreme distribution p  ext M (X), which implies for every y, z and w
that Pp (Y = y|Z = z, W = w) = Pp (Y = y|W = w). We say that a set of variables Z
is epistemically irrelevant to a set of variables Y conditional on variables W if for every
function f on y  Y and assignments z and w it follows that
min

pM

X

f (y)Pp (Y = y|Z = z, W = w) = min
pM

yY

X

f (y)Pp (Y = y|W = w) ,

(6)

yY

which is equivalent to say that the projection of M on Y conditioned on W = w and Z = z
equals the projection of M on Y conditioned only on W = w. It is an immediate conclusion
that strong independence implies epistemic irrelevance (and the converse is not necessarily
true) (Cozman, 2000; de Cooman & Troffaes, 2004). Variables Y and Z are epistemically
independent conditional on W if, given any assignment w, Y and Z are epistemically
irrelevant to each other (Walley, 1991, ch. 9).
The strong extension of a credal network N = (X, G, Q) is the largest credal set KS of
distributions of X whose extreme distributions satisfy the strong independence assessments
in G (viz. that every variable is strongly independent of its non-descendant non-parents
given its parents), and whose projections on local domains lie inside the local credal sets
specified in Q, that is, KS is the convex hull of the set of distributions of X whose induced
measure satisfies (1) and (5). One can show that the strong extension can be equivalently
610

fiProbabilistic Inference in Credal Networks: New Complexity Results

defined as (Antonucci & Zaffalon, 2008; Antonucci, de Campos, & Zaffalon, 2014)
(
)
Y xPa(i)
xPa(i)
def
qi
KS = co p  V (X) : p(x) =
(xi ), qi
 ext Q(Xi |xPa(i) ) .

(7)

iN

The epistemic extension of a credal network is the largest joint credal set KE of X that
satisfies the epistemic irrelevance assessments in G (viz. the non-descendant non-parents
are irrelevant to a variable given its parents), and the assessments in Q. Equivalently, the
epistemic extension is the credal set KE defined by the set of probability distributions p of
X such that
X
X
f (xi )Pp (xi |xNd(i) ) 
min
f (xi )q(xi ) ,
(8)
qQ(Xi |xPa(i) )

xi

xi

for all functions f of Xi , and assignment xNd(i) . Note that these inequalities can be turned
into linear inequalities of the form (4) by multiplying both sides by Pp (xNd(i) ) and rearranging terms.
The following example is largely based on Example 9.3.4 of the work of Walley (1991).
Example 3. Consider the network in Example 2, and represent a function f of a binary
variable as the pair (f (0), f (1)). The strong extension KS is the credal set whose extreme
distributions are the four joint probability distributions p  V (X1 , X2 , X3 ) such that
p(x1 , x2 , x3 ) = p1 (x1 )p2 (x2 )px3 1 x2 (x3 )

for x1 , x2 , x3  {0, 1} ,

where
p1  {(0.4, 0.6), (0.5, 0.5)} ,

p2  {(0.4, 0.6), (0.5, 0.5)} ,

p00
3

10
p01
3 = p3 = (0, 1) .

=

p11
3

= (1, 0) ,

Note that the strong extension contains four extreme distributions. The epistemic extension
KE is the set of joint probability distributions p  V (X1 , X2 , X3 ) that satisfies the system
of linear inequalities
0.5 =

min q(1)  Pp (X1 = 1|x2 )  max q(1) = 0.6

[x2 = 0, 1] ,

min q(1)  Pp (X2 = 1|x1 )  max q(1) = 0.6

[x1 = 0, 1] ,

qQ(X1 )

0.5 =

qQ(X2 )

qQ(X1 )

qQ(X2 )

Pp (X3 = 1|X1 = x, X2 = x) = 0

[x = 0, 1] ,

Pp (X3 = 1|X1 = 0, X2 = 1) = Pp (X3 = 1|X1 = 1, X2 = 0) = 1 .
One can verify that the set KE has the following six extreme distributions:
p1 = (0.25, 0, 0, 0.25, 0, 0.25, 0.25, 0) ,

p2 = (0.16, 0, 0, 0.36, 0, 0.24, 0.24, 0) ,

p3 = (0.2, 0, 0, 0.3, 0, 0.2, 0.3, 0) ,

p4 = (0.2, 0, 0, 0.3, 0, 0.3, 0.2, 0) ,

p5 = (2/9, 0, 0, 3/9, 0, 2/9, 2/9, 0) ,

p6 = (2/11, 0, 0, 3/11, 0, 3/11, 3/11, 0) ,

where the tuples on the right-hand side represent distributions p(x1 , x2 , x3 ) by
(p(0, 0, 0), p(1, 0, 0), p(0, 1, 0), p(1, 1, 0), p(0, 0, 1), p(1, 0, 1), p(0, 1, 1), p(1, 1, 1)) .
We observe that distributions p1 to p4 are extreme distributions of the strong extension,
whereas p5 and p6 are not in the strong extension.
611

fiMaua, de Campos, Benavoli & Antonucci

The example above shows an interesting and well-known relation between epistemic
and strong extensions, namely, that the latter is always contained in the former, and thus
produces more precise results (Walley, 1991, ch. 9.2).
We have already discussed how the choice of a representation of credal sets can affect
the complexity. The following result connects vertex- and constraint-based credal networks
under strong independence.
Proposition 1. Any vertex-based (separately specified) credal network can be efficiently
reduced to a constraint-based credal network over a larger set of variables that induces the
same strong extension when projected on the original set of variables.
Proof. Let Xi be a variable whose local credal set Q(Xi |xPa(i) ) is specified by the extreme distributions p1 , . . . , pm , for a given assignment of the parents. Insert a new vacuous
variable X taking values in {1, . . . , m}, and with Xi as its child and XPa(i) as its parents, and redefine Q(Xi |xPa(i) ) as the singleton that contains the conditional distribution
q(xi |xPa(i) , x = k) = pk (xi ). One can verify that the strong extension of the new network
after marginalizing X coincides with the original strong extension.
The result above cannot be applied to derive the complexity of singly connected networks
since the reduction used in the proof inserts (undirected) cycles in the network. Thus,
it is not true that hardness results obtained for vertex-based singly connected networks
immediately extend to constraint-based singly connected networks, even though this is
always the case in the results we present here (for instance, we only use credal sets that are
easily translated from one representation to the other in our hardness results). Conversely,
tractability of constraint-based singly connected networks does not immediately extend to
vertex-based singly connected networks. It is unclear whether constraint-based networks
can be efficiently reduced to vertex-based form by inserting new variables, but we conjecture
that this is true.
2.2.3 Probabilistic Inference
Similarly to Bayesian networks, a primary use of credal networks is in deriving bounds for
probabilities implied by the model. The precise characterization depends on the choice of an
irrelevance concept. We define the inference problem under strong independence as follows.
STRONG-INF
Input: A credal network (X, G, Q), a target node t, an assignment xt of Xt , a
(possibly empty) set of evidence nodes O, and an assignment xO of XO .
Output: The numbers
min Pp (Xt = xt |XO = xO ) and max Pp (Xt = xt |XO = xO ) ,

pKS

pKS

where KS is the strong extension of the network.
An analogous inference problem can be defined for epistemic irrelevance, simply by
replacing the strong extension in the output of the problem above by the epistemic extension:
612

fiProbabilistic Inference in Credal Networks: New Complexity Results

EPISTEMIC-INF
Input: A credal network (X, G, Q), a target node t, an assignment xt of Xt , a
(possibly empty) set of evidence nodes O, and an assignment xO of XO .
Output: The numbers
min Pp (Xt = xt |XO = xO ) and max Pp (Xt = xt |XO = xO ) ,

pKE

pKE

where KE is the epistemic extension of the network.
We assume in both problems that when the lower probability of the evidence is zero (i.e.,
minp Pp (XO = xO ) = 0), then any value is a solution (that is, the minimization may achieve
zero and the maximization one).6 For a recent treatment of the zero probability case, see
the work of de Bock and de Cooman (2013). We emphasize that our complexity results hold
true regardless of how zero probabilities are treated, because we take the appropriate care
to avoid any conditioning event with zero probability in our reductions (as it will become
clear later on).
Example 4. Consider again the network in Example 2, and assume that the target node
is t = 3, xt = 0 and XO is the empty set. The strong extension KS has been defined in
Example 3. The outcome of STRONG-INF is
X
min P(X3 = 0) = min
p1 (x1 )p2 (x2 )px3 1 x2 (0)
pKS

x1 ,x2

= 1 + min{2p1 (0)p2 (0)  p1 (0)  p2 (0)}
= 1  (2  1/2  1/2  1/2  1/2) = 1/2 ,
where the minimizations on the right are performed over p1 and p2 , and
X
max P(X3 = 0) = max
p1 (x1 )p2 (x2 )px3 1 x2 (0)
pKS

x1 ,x2

= 1 + max{2p1 (0)p2 (0)  p1 (0)  p2 (0)}
= 1  (2  0.4  0.4  0.4  0.4) = 0.52 .
The outcome of EPISTEMIC-INF are the values of the solutions of the linear programs
min{p(0, 0, 0) + p(1, 1, 0) : p  KE } = 5/11 < 1/2
and
max{p(0, 0, 0) + p(1, 1, 0) : p  KE } = 5/9 > 0.52 ,
where KE is the epistemic extension defined in Example 3.
The fact that the lower bound (resp., upper bound) of EPISTEMIC-INF in the example
above is smaller (resp., greater) than the lower bound (upper bound) of STRONG-INF is
a direct consequence of the fact that the strong extension is contained in the epistemic
extension.
6. If the upper probability of the evidence is positive, the regular extension can be used to compute nonvacuous inferences (see Walley, 1991, Appendix J).

613

fiMaua, de Campos, Benavoli & Antonucci

Let also z denote the Kroneckers delta function at z, which returns one at z and zero
elsewhere. Since there is an one-to-one mapping between expectation and probability, we
can state the inference problems in a slightly different but equivalent way.
STRONG-INF2
Input: A credal network (X, G, Q), a target node t, an assignment xt of Xt , a
(possibly empty) set of evidence nodes O, and an assignment xO of XO .
Output: The solution  of optpKS Ep ([  xt ]xO ) = 0, where KS is the strong
extension of the network, with opt  {min, max}.
EPISTEMIC-INF2
Input: A credal network (X, G, Q), a target node t, an assignment xt of Xt , a
(possibly empty) set of evidence nodes O, and an assignment xO of XO .
Output: The solution  of optpKE Ep ([  xt ]xO ) = 0, where KE is the
epistemic extension of the network, and opt  {min, max}.
The main advantage of these reformulations is that a linear-fractional programming
problem is transformed into a linear programming problem, which facilitates obtaining
some results. We will refer to both reformulations interchangeably, having in mind their
equivalence.

3. Complexity Results
In this section we study the complexity of inference in credal networks with respect to the
irrelevance concept adopted, the network topology and the variable domain cardinality.
3.1 Previously Known Results
Computing STRONG-INF is notoriously a hard task, whose complexity strongly depends
on the topology of the DAG and the cardinality of the variable domains. Cozman et al.
(2004) proved this problem to be NPPP -hard. De Campos and Cozman (2005) studied
the parametrized complexity and concluded that the problem is NP-hard even on singly
connected networks of bounded treewidth. A long-known positive result is the 2U algorithm of Fagiuoli and Zaffalon (1998), which solves the problem in polynomial time if the
underlying graph is a polytree and all variables are binary. For networks assuming a naive
Bayes topology (i.e., containing a single root variable which is the single parent of all remaining variables), Zaffalon (2002) showed that STRONG-INF can be computed efficiently
when the query is the root variable. Zaffalon and Fagiuoli (2003) showed that the problem
is polynomial-time solvable in tree-shaped networks if there is no evidence. De Campos
and Cozman (2005) showed that obtaining approximate solutions with a provably maximum error bound is impossible unless P equals NP, even in polytrees. On the other hand,
Maua, de Campos, and Zaffalon (2013) showed that when both variable cardinalities and
treewidth are assumed bounded, there is a fully polynomial-time approximation scheme
that finds solutions withing a given error  in time polynomial in the input and in 1/. This
is the only known positive result regarding the complexity of approximate inference with
credal networks when the exact solution is NP-hard.
614

fiProbabilistic Inference in Credal Networks: New Complexity Results

Much fewer is known about the complexity of EPISTEMIC-INF. A positive result was
given by de Cooman et al. (2010), who developed a polynomial-time algorithm for computations in credal trees.
3.2 Outline of the Contributions
Our first contribution (sect. 3.3) is the development of credal networks with polynomial-time
computable numbers. We show that such networks allow us to approximate arbitrarily well
any network while still inducing positive lower probabilities on any event. This is useful to
extend our subsequent complexity results (some of them involving events of lower probability
zero) to the case where the lower probability of any event is strictly positive.
We then proceed to derive complexity results about STRONG-INF and EPISTEMICINF. The first new complexity result concerns precise-vacuous networks, which are credal
networks comprised of vacuous root nodes and precise non-root nodes (sect. 3.4). Any credal
network can be transformed into a precise-vacuous network on which STRONG-INF provides
the same results as the original network; moreover, STRONG-INF is known to be NPPP hard in such models. We show that the solutions of STRONG-INF and EPISTEMIC-INF
coincide in precise-vacuous networks, which implies the NPPP -hardness of EPISTEMIC-INF.
The hardness result holds even in the case of binary variables (and multiply connected
networks).
We next show that both problems remain NP-hard in singly connected credal networks
even if we constraint variables to take on at most three values and bound the treewidth on
two (sect. 3.5). We show in the sequence that STRONG-INF is NP-hard already in credal
trees (sect. 3.6); as discussed, EPISTEMIC-INF is polynomial-time computable in this case.
Imprecise hidden Markov models are tree-shaped credal networks that extend standard
(precise) Hidden Markov Models (HMMs) to allow for imprecisely specified parameters
in the form of credal sets. HMMs are commonly used to represent time-dependent process and have wide applicability. Since imprecise HMMs are particular instances of credal
trees, EPISTEMIC-INF can be performed in polynomial time in such models. We show
in Section 3.7 that when the target node is the last node in the longest directed path of
the network inferences under strong independence and epistemic irrelevance coincide (in the
context of time-series prediction, such an inference is known as filtering). As a consequence,
STRONG-INF is also polynomial-time computable for such queries. This is despite the fact
that the strong and the epistemic extensions might disagree on such models, as we show by
a counter-example with a different type of query. We leave open the complexity of more
general inferences in imprecise HMMs under strong independence.
As corollaries of the equivalence of a certain type of inference in HMMs, we obtain that
STRONG-INF and EPISTEMIC-INF also coincide in marginal inference (i.e., with no evidence) in tree-shaped networks, in last-node inference in imprecise Markov chains (sect. 3.8)
and in naive Bayes structures (sect. 3.9). These results have been previously obtained independently for each irrelevance concept and their tractability was thought to be coincidental.
We organize the presentation of the above mentioned results according to the complexity
of the underlying DAG, listing results from the most complex structures to the simplest
ones. The reason to proceed in this fashion is to allow obtaining some results for simpler
615

fiMaua, de Campos, Benavoli & Antonucci

models such as imprecise Markov chains and naive Bayes structures as corollaries of results
for more complex models such as imprecise hidden Markov models.
3.3 Networks Specified with Computable Numbers
Before presenting the complexity results, we need to introduce the concept of polynomialtime computable numbers, and to discuss some properties of networks specified with such
numbers. This will be used within later proofs in an essential way, including (but not only)
showing that the hardness results hold true even if we assume that any possible event has
positive probability.
A number r is polynomial-time computable if there exists a transducer Turing machine
Mr that, for an integer input b (represented through a binary string), runs in time at most
poly(b) (the notation poly(b) denotes an arbitrary polynomial function of b, and might
indicate a different polynomial at each time it is used) and outputs a rational number
r0 (represented through its numerator and denominator in binary strings) such that |r 
r0 | < 2b . Of special relevance to us are numbers of the form (2v1  2v2 )/(1 + 2v3 ),
with v1 , v2 and v3 being non-negative rationals no greater than two. For any rational v
between zero and two we can build a machine that outputs a rational r0 that approximates
2v with precision b in time poly(b) by computing the Taylor expansions of 2v around
zero with sufficiently many terms (depending on the value of b) similar to the proof of
Lemma 4 of the work of Maua et al. (2013). The desired numbers can then be obtained
by the corresponding fractional expression. The following lemmas ensure that the outcome
of STRONG-INF on networks specified with polynomial-time computable numbers can be
approximated arbitrarily well using a network specified only with positive rational numbers.
It allows us to specify the desired precision and for which nodes of the network the numerical
parameters will be approximated by positive rational numbers.
Lemma 1. Consider a vertex-based credal network N whose numerical parameters are
specified with polynomial-time computable numbers encoded by their respective machines (or
directly given as rational numbers), and let b be the size of the encoding of N . Given a
subset of the nodes N 0  N of N and a rational number 1    2poly(b) , we can construct
in time poly(b) a vertex-based credal network N 0 over the same variables whose numerical
parameters that specify the credal sets of nodes N 0 are all rational numbers greater than
2poly(b) (numerical parameters related to nodes not in N 0 are kept unchanged), and such
that there is a polynomial-time computable surjection (p, p0 ) that associates any extreme p
of the strong extension of N with an extreme p0 of the strong extension of N 0 satisfying
max |Pp0 (XA = xA )  Pp (XA = xA )|   ,
xA

for any subset XA  X of the variables.
Proof. Take N 0 to be equal to N except that each computable number r used in the specification of N for nodes N 0 is replaced by a rational r0 such that |r0  r| < 2(n+1)(v+1)1 ,
where n is the number of variables, and v is the maximum cardinality of the domain of any
variable in N . Because   2poly(b) , we can run the Turing machine Mr used to represent r
on input poly(b)+(n+1)(v+1)+1 to obtain r0 in time O(poly(poly(b) + (n + 1)(v + 1) + 1)),
which is O(poly(b)). After obtaining r0 , add to it 2(n+1)(v+1)1  to ensure that r < r0 <
616

fiProbabilistic Inference in Credal Networks: New Complexity Results

r + 2(n+1)(v+1) , that is, the approximation is from above. However, exactly one of the
probability values in each distribution used to represent an extreme of a local credal set in
N 0 is not approximated in that way but is computed as one minus the sum of the other
numbers to ensure that its distribution adds up exactly to one; we can choose the greatest
value for that (by trying each of the (at most) v states, which probability value is at least
1/v), and its error with respect to the corresponding original computable number will be
at most (v  1)  2(n+1)(v+1)  < 2n(v+1) . This construction ensures that every created
rational number is greater than 2(n+1)(v+1)1  > 2poly(b) and have an error of at most
2n(v+1)  to the original corresponding number.
Let qi (xi |xPa(i) ) and qi0 (xi |xPa(i) ) denote, respectively, the parameters of N and N 0 (i.e.
they are corresponding extreme distributions of the local credal sets Q(Xi |xPa(i) ) in the two
networks) such that qi0 (xi |xPa(i) ) is the approximated version computed from qi (xi |xPa(i) )
as explained. Consider an assignment x to all variables in N (or in NQ0 ). Let also p be
an extreme of the strong extension of N . Then p factorizes as p(x) = iN qi (xi |xPa(i) ),
for some combination of extreme distributions qi (|xPa(i) ) from Q(Xi |xPa(i) ), i  N . Finally,
let p0 be an extreme distribution in the strong extension of N 0 that satisfies p0 (x) =
Q
0
0
n(v+1) . It follows
iN qi (xi |xPa(i) ). By construction, |qi (xi |xPa(i) )  qi (xi |xPa(i) )|  2
from the binomial expansion of the factorization of p0 (x) on any x that

Y
Y
p0 (x) =
qi0 (xi |xPa(i) ) 
2n(v+1)  + qi (xi |xPa(i) )
iN

iN

=

X

(2nvn )n|A|

Y

qi (xi |xPa(i) )

iA

AN

 2n 2nvn  +

Y

qi (xi |xPa(i) )

iN

= p(x) + 2nv  .
The second inequality follows from the fact that there is one term for p(x) in the expansion
and 2n  1 terms that can be written as a product of 2n(v+1)  by non-negative numbers
less than or equal to one. With a similar reasoning, we can show that

Y
p0 (x) 
qi (xi |xPa(i) )  2n(v+1)   p(x)  2nv  .
iN

Thus, maxx |p0 (x)  p(x)|  2nv . Now consider a subset of the variables XA and an
assignment xA  XA . Since
X
Pp0 (XA = xA ) =
p0 (x0 ) ,
x0 :x0A =xA

each term p0 (x0 ) in that sum satisfies p0 (x0 )  p(x0 ) + 2nv , and because there are less than
v n  2vn terms being summed, it follows that
X

Pp0 (XA = xA ) 
p(x) + 2vn   Pp (XA = xA ) +  .
x0 :x0A =xA

An analogous argument can be used to show that Pp0 (XA = xA )  Pp (XA = xA )  . Note
that the obtained mapping (p, p0 ) is a surjection by construction.
617

fiMaua, de Campos, Benavoli & Antonucci

The above lemma has the following direct consequence on the computation of the
STRONG-INF with polynomial-time computable numbers. The only restriction for its application is that the computable numbers must be either zero or greater than some  that
is not exponentially close to zero.
Corollary 1. Consider a vertex-based credal network N whose numerical parameters are
specified with polynomial-time computable numbers encoded by their respective machines (or
directly given as rational numbers), such that no such number lies properly in ]0, [, for
some 0    1. Let b be the size of the encoding of the network. Given a subset of the
nodes N 0  N of N and a rational number  with 2poly(b)   < , we can construct
in time poly(b) a vertex-based credal network N 0 over the same variables whose numerical
parameters defining credal sets related to nodes N 0 are all strictly positive rational numbers
greater than 2poly(b) (numbers defining credal sets of nodes not in N 0 are kept unchanged),
and such that 7
|STRONG-INF(N 0 , t, xt , O, xO )  STRONG-INF(N , t, xt , O, xO )|   ,
for any query t, xt , O, xO such that either O =  or minp Pp (xO ) > 0 in N .
Proof. According to Lemma 1, there is a polynomial-time computable network N 0 whose
numerical parameters that specify the credal sets related to nodes N 0 are positive rational numbers and a polynomial-time computable surjection (p, p0 ) such that p and p0
are, respectively, extreme distributions of the strong extension of N and N 0 , and satisfy
|Pp0 (xA )  Pp (xA )|  n+1 /3 for all XA  X and xA  XA . It follows that
Pp0 (xt |xO ) =

Pp0 (xt , xO )
Pp (xt , xO )  n+1 /3

,
Pp0 (xO )
Pp (xO ) + n+1 /3

where p0 is the image of p according to the surjection. If Pp (xt , xO ) = 0, this equation
is useless and vanishes. Otherwise, by Lemma 7 of the work of de Campos and Cozman
(2013), we have that
Pp (xt , xO )  n+1 /3
2n+1 /3

P
(x
|x
)

 Pp (xt |xO )   .
p
t
O
Pp (xO ) + n+1 /3
n
The other side of the inequality is obtained analogously (using once more Lemma 7 of the
work of de Campos & Cozman, 2013, except for the case of Pp (xt , xO ) = 0, when the
following reasoning is valid without the need of that lemma):
Pp (xt , xO ) + n+1 /3
2n+1 /3

P
(x
|x
)
+
p
t
O
Pp (xO )  n+1 /3
n  n+1 /3
2
 Pp (xt |xO ) +
 Pp (xt |xO ) + .
3

Pp0 (xt |xO ) 

Hence, |Pp0 (xt |xO )Pp (xt |xO )|  . Let p be an extreme distribution in the strong extension
of N such that Pp (xt |xO ) = minqKS Pq (xt |xO ), where KS denotes the strong extension of
7. We abuse notation of STRONG-INF, as it is defined for opt  {min, max}. We intend to mean that the
equation is valid for both options of opt.

618

fiProbabilistic Inference in Credal Networks: New Complexity Results

N . We have that
min Pq (xt |xO ) = Pp (xt |xO )  Pp0 (xt |xO )    min0 Pq0 (xt |xO )   ,
q 0 KS

qKS

where p0 in the first inequality is the image of p according to the surjection, and KS0 in the
last inequality is the strong extension of N 0 . It follows from the above that
min Pq0 (xt |xO )  min Pq (xt |xO )   .

q 0 KS0

qKS

The other side comes by contradiction. Suppose that
min Pq (xt |xO )  min0 Pq0 (xt |xO ) >   min Pq (xt |xO )   > min0 Pq0 (xt |xO ) .

qKS

q 0 KS

q 0 KS

qKS

Hence there would exist an extreme q 0  KS0 such that |Pq0 (xt |xO )  Pq (xt |xO )| >  for any
q  KS , which is impossible because the mapping (q, q 0 ) is a surjection. An analogous proof
works for showing that the upper bounds according to the two networks do not differ by
more than .
3.4 Precise-Vacuous Networks
To show that EPISTEMIC-INF is NPPP -hard in arbitrary networks, we need the following
result, which shows that inferences under strong independence and epistemic irrelevance
coincide on precise-vacuous networks.
Proposition 2. Consider a credal network whose root nodes are vacuous and non-root
nodes are precise. Let t be a non-root node, xt an arbitrary value of Xt and O = . Then
STRONG-INF equals EPISTEMIC-INF.
Proof. Let XR be the vacuous variables associated with the root nodes (hence to vacuous
local credal sets), and XI denote the remaining variables (which are precise). For every
x
precise node i in I, let qi Pa(i) (xi ) be the single distribution in the associated credal set
Q(Xi |xPa(i) ). Consider an arbitrary distribution p in the epistemic extension KE , and let
< be a topological ordering of the nodes. For an assignment x to X, we write x<i to denote
the coordinates j < i of x according to the topological ordering. For every node i the set
{j  N : j < i} is a subset of Nd(i), and it follows from the definition of epistemic extension
x
that Pp (xi |x<i ) = qi Pa(i) (xi ) for every precise node i and assignments xi and x<i . By the
Chain Rule we have that
x  X :

Pp (x) = Pp (xR )

Y

Pp (xi |x<i ) = q(xR )

iI

Y

x

qi Pa(i) (xi ) ,

iI

where q is any distribution of XR (since these nodes are vacuous, any distribution satisfies
the constraints in KE for them). As stated, let xt be the value of interest of Xt . The result
619

fiMaua, de Campos, Benavoli & Antonucci

of EPISTEMIC-INF is thus given by
min Pp (Xt = xt ) =

pKE

=

min
qV (XR )

min
qV (XR )

=

min
qV (XR )

X

xt (xt )  q(xR ) 

x

X

q(xR )

xR

X

XY

Y

x

qi Pa(i) (xi )

iI
xPa(i)
qi
(xi )xt (xt )

xI iI

q(xR )  g(xR ) ,

xR

xPa(i)
def P Q
where g(xR ) =
(xi ). According to the last equality, the lower
xI
iI xt (xt )qi
marginal probability of Xt = xt is a convex combination of g(xR ). Hence,
X Y xPa(i)
min Pp (Xt = xt )  min
qi
(xi ) .
pKE

xR

xI\{q} iI

The rightmost minimization is exactly the value of the lower marginal probability returned
by STRONG-INF, and since the strong extension is contained in the epistemic extension,
the inequality above is tight. An analogous result can be obtained for the upper probability
by substituting minimizations with maximization and inverting the inequality above.
The class of networks considered in the result above might seem restrictive at first
sight. However, Antonucci and Zaffalon (2008) showed that STRONG-INF in any credal
network whose local credal sets are represented in vertex-based form can be reduced in
linear time to the same problem in a credal network containing only vacuous and precise
nodes. Such a network can then be transformed in linear time into a precise-vacuous network
(i.e., one in which root nodes are vacuous and non-root nodes are precise) by applying
Transformation 6 in the work of Maua, de Campos, and Zaffalon (2012a),8 which increases
the treewidth of the network by at most three. Hence, any vertex-based credal network
can be reduced in polynomial time into a precise-vacuous network for which STRONGINF provides the same result as for the original network (and whose treewidth remains
bounded, if it originally were). The hardness of EPISTEMIC-INF in precise-vacuous credal
networks follows immediately from the hardness of inference under strong independence
and Proposition 2, as the following corollary shows.
Corollary 2. STRONG-INF and EPISTEMIC-INF are NPPP -hard even if all variables are
binary and all numerical parameters are strictly positive.
Proof. Cozman et al. (2004) used a reduction from E-MAJSAT to STRONG-INF without
evidence in a binary credal network whose root nodes are vacuous and non-root nodes are
precise to show that such inference is NPPP -hard. Since according to Proposition 2 the result
of EPISTEMIC-INF is the same, EPISTEMIC-INF is also NPPP -hard. In order to show that
the result is valid also if all numerical parameters are strictly positive, we will only sketch
the proof so as to avoid repeating all the formulation for the E-MAJSAT problem. Using
Lemma 1 with epsilon  = 2poly(b) smaller than the precision of any number involved in
8. Strictly speaking, the work of Maua et al. (2012a) deals with influence diagrams; the link between those
and credal networks was established by Antonucci and Zaffalon (2008) and de Campos and Ji (2008).

620

fiProbabilistic Inference in Credal Networks: New Complexity Results

any calculation, we build a new network where all numerical parameters are strictly positive
and the variation in the result of STRONG-INF is negligible such that it can still decide
E-MAJSAT (further details on how small  has to be are omitted for simplicity, but the
2
gap between instances is large enough that 2O((n+m) ) will suffice, where n, m are the
number of variables and clauses in the specification of E-MAJSAT, see Park & Darwiche,
2004, Thm. 2). Because EPISTEMIC-INF contains STRONG-INF, its result after applying
Lemma 1 will be between the results of STRONG-INF in the new network and in the old
network (the latter equals that of EPISTEMIC-INF). Hence, EPISTEMIC-INF in the new
network with strictly positive numerical parameters also decides E-MAJSAT.
Note that the result holds irrespective of how the local credal sets are represented, since
vacuous and precise nodes can be mapped from constraint-based to vertex-based form and
vice-versa in polynomial time.
3.5 Singly Connected Networks
We now turn our attention to singly connected networks. A first result is a direct consequence of Proposition 2 is the NP-hardness of EPISTEMIC-INF in singly connected credal
networks, since STRONG-INF is NP-hard in singly connected networks, even if we admit
imprecision only on root nodes:
Corollary 3. EPISTEMIC-INF is NP-hard in singly connected credal networks.
Proof. In the work of de Campos and Cozman (2005) it has been shown that STRONG-INF
is NP-hard in precise-vacuous singly connected networks. Since Proposition 2 shows that
EPISTEMIC-INF can be reduced to STRONG-INF on the same input, the result follows.
The proof of NP-hardness of STRONG-INF provided by de Campos and Cozman (2005)
requires the variable domain cardinalities to be unbounded. We present here the stronger
result of NP-hardness of credal inference in networks where imprecise variables are binary
and precise ones are at most ternary. We can now show NP-hardness of credal inference in
singly connected networks with bounded variable cardinality.
Theorem 1. STRONG-INF and EPISTEMIC-INF are NP-hard even if the network is singly
connected and has treewidth at most two, all imprecise variables are binary, and all precise
variables are (at most) ternary. Moreover, all numerical parameters in the network are
strictly positive.
Proof. We defer the treatment of zero numerical parameters to the final part. We build
a singly connected credal network with underlying graph as in Figure 3. The variables
(associated with nodes) on the upper row are binary and vacuous, namely X1 , . . . , Xk ,
while the remaining variables Xk+1 ,    , X2k+1 are ternary and precise. The local credal
sets associated with precise nodes are singletons such that Q(Xk+1 ) contains a uniform
distribution q(xk+1 ) = 1/3, and, for i = k + 2, . . . , 2k + 1, Q(Xi |xi1 , xik1 ) contains the
conditional distribution q(Xi |xi1 , xik1 ) specified in Table 1. The rational numbers vi in
the table shall be defined later on. Consider an extreme distribution p(x) of the strong
621

fiMaua, de Campos, Benavoli & Antonucci

k+1

1

2

3

k+2

k+3

k+4

k



2k+1

Figure 3: Credal network structure used to prove Theorem 1. The shaded node is the
target.
q(xi |xi1 , xik1 )

xi = 1

xi = 2

xi = 3

xi1 = 1, xik1 = 1
xi1 = 2, xik1 = 1
xi1 = 3, xik1 = 1
xi1 = 1, xik1 = 0
xi1 = 2, xik1 = 0
xi1 = 3, xik1 = 0

2vi

0
1
0
0

1  2vi
0
1
0
1  2vi
1

0
0
1
0
0

2vi
0

Table 1: Local probability distributions used to prove Theorem 1
extension of the network. It follows for all x that
p(x) = q(xk+1 )

2k+1
Y

q(xi |xi1 , xik1 )

Y
iA

i=k+2

1 (xi )

Y

0 (xi ) ,

iA
/

for some A  {1, . . . , k}. This is because the extreme distributions of local vacuous sets
over binary variables are 0 and 1 , and each choice of a local extreme for a root node can
be associated with a choice of either including or excluding its corresponding node in/from
def

A. Let A = {1, . . . , k} \ A denote the complement of a set A with respect to {1, . . . , k}.
We have that
( Q
2k+1
1
q(xi |xi1 , xik1 ), if xA = 1 and xA = 0 ;
p(x) = 3 i=k+2
0,
otherwise.
It follows that
Pp (X2k+1 = 1) =

X

p(x)1 (x2k+1 ) =

x

2

P

iA

3

vi

and Pp (X2k+1 = 2) =

2

P

iA

3

vi 2

.

We show the NP-hardness of credal inference by reducing the NP-complete PARTITION
problem (Garey & Johnson, 1979) to the computation of maxpKS Pp (X2n+1 = 3). We
define PARTITION as follows.
PARTITION
Input: List of positive integers z1 , . . . , zk .
Output: Is there a subset A  {1, . . . , k} such that
X
X
zi =
zi ?
iA

iA

622

fiProbabilistic Inference in Credal Networks: New Complexity Results

h(v)

1.2

1.1

1
0

0.5

1
v

1.5

2

Figure 4: Function used in the reduction in the proof of Theorem 1.
Notice that the above equality is equivalent to
k

X

zi /z = 1 ,

1X
where z =
zi .
2
i=1

iA

def
def P
Define the exponents
in
Table
1
as
v
=
z
/z,
and
let
v
= iA vi . It follows for any A
i
i
A
P
that vA = 2 iA vi . If an instance of the PARTITION is a yes-instance (i.e., if the output
of PARTITION is yes), then there is A for which vA = 1, whereas if it is a no-instance (i.e.,
if the output is no), then for any A, it follows that |vA  1|  1/(2z) because the numbers
in the input are integers and hence the sums of two different sets are either equal or differ
by at least one. Consider the function

h(vA ) =

2(vA 1) + 2vA 1
.
2

The graph of the function is depicted in Figure 4. Seen as a function of a continuous variable
vA  [0, 2], the function above is strictly convex, symmetric around one, and achieves the
minimum value of one at vA = 1. Thus, if PARTITION returns yes then minA h(vA ) = 1,
while if it returns no we have that
4

min h(vA )  21/(2z)1 + 21/(2z)1  2(2z)
A

> 1 + (2z)4 /2 = 1 + 1/(32z 4 ) ,

where the second inequality is due to Lemma 24 in the work of Maua et al. (2012a), and
4

def

the strict inequality follows from the first-order Taylor expansion of 2(2z) . Let  =
(1 + z 4 /64)/3. By computing STRONG-INF with query X2n+1 = 3 and no evidence, we can
decide PARTITION, as
1  max Pp (X2k+1 = 3) = min (Pp (X2k+1 = 1) + Pp (X2k+1 = 2)) = min
p

A

h(vA )

3

if and only if the result of PARTITION is yes. It remains to show that we can polynomially
encode the numbers 2zi /z . This is done by applying Lemma 1 with a small enough 
623

fiMaua, de Campos, Benavoli & Antonucci

0
1

2

3



k

k+1

k+2

k+3



2k

Figure 5: DAG of the credal tree used to prove Theorem 2.
computable in time polynomial in the size of the partition problem:  = 1/(3  64z 4 )
suffices. Note that if we only apply Lemma 1 to the non-root nodes and leave the root
nodes untouched as vacuous, then according to Proposition 2, the outcome of EPISTEMICINF is the same, proving also its NP-hardness. Now, by further applying Lemma 1 to root
vacuous nodes, we ensure all numerical parameters are strictly positive and still yield a
result that can be used to decide PARTITION. Because EPISTEMIC-INF contains STRONGINF, its result after applying Lemma 1 will be between the results of STRONG-INF in the
new network and in that network with vacuous root nodes. Hence, EPISTEMIC-INF in the
new network with strictly positive numerical parameters also decides PARTITION.
3.6 Credal Trees
The previous complexity results showed that, from a theoretical standpoint, computing the
EPISTEMIC-INF is just as difficult as solving STRONG-INF. When the underlying graph is
a tree, de Cooman et al. (2010) showed that EPISTEMIC-INF can be computed efficiently,
and it was previously unknown whether a similar result could be obtained for STRONG-INF.
The next result shows that in this case the equivalence on the tractability under the two
different irrelevance concepts does not hold unless P equals NP.
Theorem 2. STRONG-INF in tree-shaped credal networks is NP-hard, even if only one variable is ternary and precise and all the rest are binary, and even if all numerical parameters
are strictly positive.
Proof. We show hardness by a reduction from PARTITION as defined previously. As before,
P
def
def P
we define vi = zi /z, and vA = iA vi , and note that vA = 2 iA vi . We also let h(vA )
to be as before (thus h is strictly convex on [0, 2], symmetric around one, and achieves the
minimum value of one at vA = 1). Given an instance of PARTITION (i.e., a list of integers),
we build a credal tree N over variables X0 , . . . , X2k with DAG as in Figure 5. The root
variable X0 takes values in {1, 2, 3}, and is precise and uniformly distributed (i.e., its local
credal set contains only the distribution q0 (x0 ) = 1/3). The remaining variables are all
binary and take values in {0, 1}. For i = 1, . . . , k, we specify the local conditional credal
sets Q(Xi |x0 ) as singletons {qix0 } such that

vi
vi

2 /(1 + 2 ),
qix0 (1) = 1/(1 + 2vi ),


1/2,
624

if x0 = 1,
if x0 = 2,
if x0 = 3.

fiProbabilistic Inference in Credal Networks: New Complexity Results

For i = 1 + k, . . . , 2k we specify the local credal sets Q(Xi |xik ) = {p  V (Xi ) :   p(1) 
1}, where  = 2k3 /(64z 4 ). Each of these local credal sets can be represented either in
vertex-based form by two extreme distributions or by a couple of constraints.
Let
Pp (X0 = 3, XO = xO )
def
 = max Pp (X0 = 3|XO = xO ) = max
.
pKS
pKS
Pp (XO = xO )
Hence,  is the solution of
"
#
X
max
(3 (x0 )  )Pp (X0 = x0 , XO = xO ) = 0 .
pKS

x0

By definition, any extreme distribution p in the strong extension KS satisfies for x  X
such that xk+1 = xk+2 =    = x2k = 1 the equality
p(x) = q0 (x0 )

k
Y

qix0 (xi )ixi ,

i=1

ixi

where each
is a number in [, 1]. Let O = {k + 1, . . . , 2k} and xO = (1, . . . , 1). It follows
that  is the solution of
max

X

(3 (x0 )  )q0 (x0 )

x0 ,...,xk

k
Y

qix0 (xi )ixi = 0 ,

i=1

where the maximization is performed on i0 , i1 , for i = 1, . . . , k. Consider j  {1, . . . , k}
and let
x def
j j =

X

X

(3 (x0 ) 

) q0 (x0 )qjx0 (xj )

x0 ,...,xj1 xj+1 ,...,xk

k
Y

[qix0 (xi )ixi ] .

i=1,i6=j

Then,
X

max

(3 (x0 )  ) q0 (x0 )

x0 ,...,xk

j0

k
Y


[qix0 (xi )ixi ] = max j0 j0 + j1 j1 .

i=1

j1

Since
and
are both positive, the maximization in the right-hand side above equals
zero only if both j0 and j1 are zero or they have different signs. In the former case, any
value of j0 and j1 maximizes the expression, and we can assume that (j0 , j1 ) equals (, 1)
or (1, ). In the latter case, j0 < j1 implies that (j0 , j1 ) equals (, 1) in order to maximize
the expression, and (j0 , j1 ) = (1, ) would do it otherwise. Since we selected j arbitrarily,
the result holds for all j. Thus, the maximization is equivalent to selecting, for i = 1, . . . , k,
a value yi in {0, 1} such that i0 = 1yi and i1 = yi . It follows that
max

X
x0 ,...,xk

(3 (x0 )  ) q0 (x0 )

k
Y

[qix0 (xi )ixi ] =

i=1

max

y{0,1}k

X

(3 (x0 )  ) q0 (x0 )

x0 ,...,xk

k
Y
[qix0 (xi )(1xi )(1yi ) xi yi ] .
i=1

625

fiMaua, de Campos, Benavoli & Antonucci

By rearranging terms, we obtain
max

y{0,1}k

X

(3 (x0 )  ) q0 (x0 )

x0

k
Y
 x0

qi (0)1yi + qix0 (1)yi ,
i=1

which by construction equals
k

k

k

i=1

i=1

i=1

 Y 1yi
 Y vi 1yi
1Y1+
max 
 [
+ 2vi yi ] + 
[2 
+ yi ] +
3
3
3
2
y{0,1}k

!
,

Q
where  = ki=1 qi2 (1) (recall that qi2 (1) is the conditional probability value of Xi = 1
given X0 = 2). The binary vector y can be seen as the characteristic vector of a subset
A  {1, . . . , k}. Define
Y
Y
def
(1 + 2vi )
(2vi + )
bA =
iA

iA

for every subset A. The optimization on y can be rewritten as the following optimization
over subsets A: find  such that
!


1 1+ k


+ max   (bA + bA ) = 0 .
A
3
2
3
Solving the expression above for , we get to

=

1+

2
1+

!1

k

 min (bA + bA )
A

.

Define the function g(a) as


def

g(a) = 1 +

2
1+

k
(1 + a)

def

for any real number a, and let aA = bA + bA  1 for any A  {1, . . . , k}. Now  =
(minA g(aA ))1 . Note that g(aA ) > 1 + (1 + aA )2k , because  > 2k (this will be used
later). It follows from the Binomial Theorem that the value of bA is very close to the value
of 2vA from above.
2vA  bA  (2vA + 2k )(1 + )k
 (2vA + 2k )(1 + 2k)
 2vA + 2k+2  ,
where we have used the inequality (1 + r/c)c  1 + 2r valid for r  [0, 1] and positive integer
c (Maua, de Campos, & Zaffalon, 2011, Lemma 37). Thus we conclude that the value aA
is very close (again from above) to h(vA )  1.
h(vA )  1  aA  h(vA )  1 + 2k+3  = h(vA )  1 + 1/(64z 4 ) .
626

fiProbabilistic Inference in Credal Networks: New Complexity Results

Now, if the partition problem is a yes-instance, then h(vA ) = 1 (recall the behavior of h
from the proof of Theorem 3) and thus aA  1/(64z 4 ), while if it is a no-instance, then
h(vA ) > 1 + 1/(32z 4 ) and thus aA > 1/(32z 4 ). Hence, there is a gap of at least 1/(64z 4 ) in
the value of aA between yes- and no-instances, and we can decide the partition problem by
verifying whether   g(3/(128z 4 ))1 . This proof shall be completed with the guarantee
that we can approximate in polynomial time the irrational numbers used to specify the
credal tree and g(a) well enough so that g(3/(128z 4 ))1 falls exactly in the middle of the
gap between the values of  for yes- and no-instances (because g is linear in a). First, note
that





k
1
1
1
2
g
g
=
,
32z 4
64z 4
64z 4 1 + 
which is greater than 2k /(64z 4 ) (since  > 2k ). The gap in the value of  is at least
1
1
g(1/(32z 4 ))  g(1/(64z 4 ))

=
g(1/(64z 4 )) g(1/(32z 4 ))
g(1/(64z 4 ))g(1/(32z 4 ))
g(1/(32z 4 ))  g(1/(64z 4 ))
>
g(1/(32z 4 ))2
k
2 /(64z 4 )
2k
>
.
>
1
k )2
4  64z 4
(1 + (1 + 32z
4 )2
k

2
0
So we apply Corollary 1 with  = 12 464z
4 to obtain from N a network N made only of
positive rational numbers. Such  guarantees that the separation between yes-instances and
no-instances of PARTITION will continue to exist.

The credal network used in the reduction that proves the previous result is in a sense
the simplest tree-shaped network on which solving STRONG-INF is hard, since the problem
would be polynomial-time solvable if the root node were replaced with a binary variable. It is
also interesting as it describes a naive Bayes structure with a single layer of latent variables,
a useful topology for robust classification problems on non-linearly separable feature spaces.
3.7 Imprecise Hidden Markov Models
An imprecise hidden Markov model (HMM) is a credal tree whose nodes can be partitioned
into hidden and manifest nodes such that the hidden nodes form a chain (i.e., a sequence
of nodes with one node linking to the next and to no other in the sequence), and manifest
nodes are leaves of the graph. HMMs are widely used to represent discrete dynamic systems
whose output at any given time step can be stochastically determined by the current state
of the system, which is assumed to be only partially observable.
Since an HMM is simply a credal tree, the algorithm of de Cooman et al. (2010) can
be used to efficiently compute EPISTEMIC-INF in HMMs, while 2U can be used to solve
STRONG-INF if all variables are binary. For networks with variables taking on more than
two values, no polynomial-time is known for STRONG-INF. In this section, we show that
when there is no evidence on variables farther (in the sense of number of nodes in the
path) from the root node than the target variable, the outcomes of the STRONG-INF and
EPISTEMIC-INF coincide. On these cases, we can run de Cooman et al.s (2010) algorithm
627

fiMaua, de Campos, Benavoli & Antonucci

1

2

4

3

Figure 6: Credal HMM in Example 5.
to compute STRONG-INF in polynomial time. This is however not always true, that is,
there are types of queries in which the results of STRONG-INF and EPISTEMIC-INF differ,
as the following example shows.
Example 5. Consider an HMM of length two whose topology is depicted in Figure 6. All
variables are binary and take values in {0, 1}. Variables X1 and X2 are hidden, while
variables X3 and X4 are manifest. The local credal sets are given by Q(X1 ) = Q(X2 |0) =
Q(X4 |0) = {p  V (X4 ) : p(1) = 1/4}, Q(X2 |1) = Q(X4 |1) = {p  V (X4 ) : p(1) = 3/4},
and Q(X3 |0) = {p  V (X3 ) : 1/2  p(1)  3/4} and Q(X3 |1) = {p  V (X3 ) : 1/4  p(1) 
1/2}. Thus, variable X3 is imprecise, and the remaining variables are precise. Consider
a query with target X4 = 0 and evidence X3 = 0. The lower bound of STRONG-INF is the
value of  that solves the equation
X
X
min
q3x2 (0)g (x2 ) =
min q3x2 (0)g (x2 ) = 0 ,
x2

x2

where the minimizations are performed over q3x2  Q(X3 |x2 ), x2 = 0, 1, and
def X
g (x2 ) =
(0 (x4 )  ) q1 (x1 )q2x1 (x2 )q4x1 (x4 ) ,
x1 ,x4

with q1 = q20 = q40 = (3/4, 1/4) and q21 = q41 = (1/4, 3/4). The values of q3x2 (0) depend only
on the signs of g (0) and g (1), which ought to be different for the expression to vanish.
Solving for  for each of the four possibilities, and taking the minimum value of , we find
that  = 4/7 > 1/2.
The lower bound of EPISTEMIC-INF is the value of  that solves
min

X

q1 (x1 )q2x1 (x2 )q4x1 (x4 )qx1 ,x2 ,x4 (0)h (x4 ) =

x1 ,x2 ,x4

(1  )

X

q1 (x1 )q2x1 (x2 )q4x1 (0) min qx1 ,x2 ,0 (0)

x1 ,x2



X

q1 (x1 )q2x1 (x2 )q4x1 (1) max qx1 ,x2 ,1 (0) = 0 ,

x1 ,x2

where h (x4 ) = 0 (x4 )  , q1 , q2x1 and q4x1 are defined as before, and qx1 ,x2 ,x4  Q(X3 |x2 )
for every x1 , x2 , x4 . Solving the equation above for  we obtain  = 13/28 < 1/2.
The above example shows that STRONG-INF and EPISTEMIC-INF might differ, even in
the simple case of HMMs with binary variables. It is currently unknown whether this type of
inference is hard for STRONG-INF. The following result shows that at least for a particular
case, the computations of the STRONG-INF and EPISTEMIC-INF in HMMs coincide.
628

fiProbabilistic Inference in Credal Networks: New Complexity Results

0

2

n-2

1

3

n-1

n

Figure 7: DAG of the HMM considered in Theorem 3.
Theorem 3. Consider a separately specified HMM over variables X0 , . . . , Xn . The variables
associated with odd numbers are manifest, and the remaining variables are hidden (see
Figure 7). Consider also the target hidden node Xn = xn , and evidence XO = xO on a
subset O of the manifest nodes. Then the outcomes of STRONG-INF and EPISTEMIC-INF
are the same.
def

Proof. Define f (xn ) = xn (xn )   for any given , and consider the distribution p in the
epistemic extension KE that minimizes
X
f (xn )p(x) .
xX:xO =xO

Let < be any topological ordering
of the nodes. By the Chain Rule, we have for all x that p
Qn
factorizes as p(x) = Pp (x0 ) i=1 Pp (xi |x<i ), where x<i denotes the coordinates xj of x with
j < i according to the topological ordering (we also write xi and x>i to denote analogous
projections). Assume that for some non-negative integer i less than or equal to n it holds
that
X
X
Y
Y xPa(j)
f (xn )p(x) 
f (xn )
Pp (xj |x<j )
pj
(xj ) ,
x:xO =xO
x
pj Pa(j) is

j<i

x:xO =xO

ji

where each
recursively defined as the extreme distribution of the local credal set
Q(Xj |xPa(j) ) that minimizes either
X xPa(j)
X
Y xPa(k)
pj
(xj )
f (xn )
pk
(xk ) ,
xj

if j is not in O, or

x>j
x

pj Pa(j) (xj )

X

k>j

f (xn )

x>j

Y

x

pk Pa(k) (xk ) ,

k>j

if j is in O, where xj is the value of Xj compatible with xO . We will show by induction in
i = n, . . . , 0 that the assumption is true. If i  1 is not in O then
X
Y
Y xPa(j)
f (xn )
Pp (xj |x<j )
pj
(xj ) =
j<i

x:xO =xO

X

ji

Y

Pp (xj |x<j )

x<i1 :xO =xO j<i1

X

Y

x<i1 :xO =xO j<i1

Pp (xj |x<j )

X

Pp (xi1 |x<i1 )

xi1

X

f (xn )

xi

X

min

qQ(Xi1 |xPa(i1) )

X
x:xO =xO

629

q(xi1 )

xi1

f (xn )

j<i1

x

pk Pa(k) (xk ) 

ki

X
xi

Y

Y

f (xn )

Y

x

pk Pa(k) (xk ) =

k>i

Pp (xj |x<j )

Y
ji1

x

pj Pa(j) (xj ) ,

fiMaua, de Campos, Benavoli & Antonucci

where
the inequality follows from the definition
P
P of epistemic extension, which implies that
h(x
)P
(x
|x
)

min
i
p i Nd(i)
qQ(Xi |xPa(i) )
xi
xi h(xi )q(xi ) for any function h on xi (note
that Nd(i)  {j < i}, and that the minimization on the right is constant w.r.t. values
xj<i:j Pa(i)
). The case of a node i in O is analogous with the sum substituted by a single
/
term. For i = n, it follows that
X
X
X
Y
f (xn )Pp (xt |x<n )
f (xn )p(x) =
Pp (xj |x<j )
xt

xj<n :xO =xO jn

x:xO =xO



X

f (xn )pnxn2 (xn )

Y

Pp (xj |x<j ) ,

j<n

x:xO =xO

so that the basis of the induction holds. For i = 0, we have that
X
Y xPa(j)
X
f (xn )p(x) 
f (xn )p0 (x0 )
pj
(xj ) ,
x:xO =xO

iN

x:xO =xO

which is the lower bound of STRONG-INF. Thus, since the epistemic extension contains the
strong extension, the inequality above is tight. In particular, the equality holds if  is the
lower bound of EPISTEMIC-INF, and it follows that
X
X
min
f (x)p(x) = min
f (x)p(x) = 0 ,
pKS

x:xO =xO

pKE

x:xO =xO

where KS denotes the strong extension. An analogous proof shows that also the upper
bounds coincide.
The previous result shows that at least for the particular case where one seeks the probability of the last variable, STRONG-INF can be computed in polynomial time. Although
restrictive, this type of inference is highly relevant, as it corresponds to predicting the future
state of a partially observable dynamic system whose future state depends in some level
only on its current (unknown) state. There is also another type of inference in trees which
is insensitive to the irrelevance concept adopted, which is the case of marginal inferences:
Corollary 4. Consider a tree-shaped network N and a target Xt = xt . Then
STRONG-INF(N , t, xt , , ) = EPISTEMIC-INF(N , t, xt , , ) .
Proof. We say that a node is barren if it is not an ancestor of any target or evidence node.
It is well-known that removing barren nodes from a Bayesian network does not affect the
outcome of BN-INF (Koller & Friedman, 2009). Since inference under strong independence
can be seen as (exponentially many) inferences in Bayesian networks, the result of STRONGINF is also unaltered if remove barren nodes. Moreover, since N is a tree, removing barren
nodes leaves with a chain of ancestors of t. According to Theorem 15 of the work of
Cozman (2000), the epistemic extension of N projected on the ancestors of t, that is, the
set of marginal distributions on xA  XA induced from joint distributions in the epistemic
extension, where A denotes the ancestors of t, is the epistemic extension of the network we
get by removing nodes not in A. This implies that barren nodes can be discarded also in
inferences under epistemic irrelevance, and the result follows.
630

fiProbabilistic Inference in Credal Networks: New Complexity Results

0

1

2

3

Figure 8: DAG of a naive Bayes with 3 feature variables.
Zaffalon and Fagiuoli (2003) developed a linear-time algorithm to compute marginal
inferences under strong independence in trees as a by-product of their work on imprecise
tree-augmented naive Bayes classifiers. The result above shows that the same algorithm
can be used to compute marginal inferences in trees under epistemic irrelevance; conversely,
de Cooman et al.s (2010) algorithm for epistemic trees can be used to compute marginal
inferences in strong trees.
3.8 Imprecise Markov Chains
The simplest DAG structure forming a connected graph is that of a chain, that is, of a
network in which each variable has at most one parent and one child. Credal chains are
more usually known as (imprecise) Markov chains. As a chain is also a tree, computing
EPISTEMIC-INF can be done in polynomial time; this is also the case for STRONG-INF on
chains of binary variables, as this is a subcase of binary polytrees. A chain can be seen
as an HMM where the values of the manifest variables are deterministically determined by
the values of the hidden variables. As such, the equivalence of both types of inference in
certain types of HMMs extends to chains:
Corollary 5. Consider a credal chain X0      Xn , a target Xn = xn on the single leaf
variable of a separately specified (imprecise) Markov chain, and some evidence XO = xO
on arbitrary non-leaf variables. Then the outcomes of STRONG-INF and EPISTEMIC-INF
coincide.
Proof. The same proof of Theorem 3 applies here, if we omit manifest nodes.
3.9 Imprecise Naive Bayes
A widely used DAG structure is the naive Bayes, where a node (usually called class) has all
other nodes (called features) as its children, and no other arc is present. Figure 8 depicts
a naive Bayes structure with class variable X0 and features X1 , X2 and X3 . Such a DAG
constitutes the structure behind the Naive Bayes and the Naive Credal Classifiers (Zaffalon,
2002). As it is a tree, computing EPISTEMIC-INF can be done in polynomial time; this is
also the case for STRONG-INF when the target node is the class variable (Zaffalon, 2002).
We show next that this similar tractability is not coincidental: both inferences yield the
same result, even if the target is not the class node. We achieve such a result by building
an HMM where the first hidden variable is the class and all other hidden variables have the
same state space as the class and are deterministically determined by its value, while the
manifest variables are the features in the naive Bayes structure. As such, the equivalence of
inferences under both types of irrelevance extends to queries in any node of a naive Bayes
structure:
631

fiMaua, de Campos, Benavoli & Antonucci

MODEL

STRONG-INF

EPISTEMIC-INF

*Naive Bayes
*Imprecise HMM (query on last node)
Imprecise HMMs
*Credal trees (no evidence)
Credal trees
Credal polytrees with binary variables
Credal polytrees with ternary variables
Bounded treewidth networks
Credal networks
*Precise-vacuous

P
P
Unknown
P
NP-hard
P
NP-hard
NP-hard
NPPP -hard
NPPP -hard

P
P
P
P
P
Unknown
NP-hard
NP-hard
NPPP -hard
NPPP -hard

Table 2: Parametrized complexity of the inference in credal networks.

Corollary 6. Consider a credal network N with the naive Bayes structure X0  X1 , X0 
X2 ,    , X0  Xn , a target Xt = xt on a node t, and some evidence XO = xO on arbitrary
features (leaf variables). Then the outcomes of STRONG-INF and EPISTEMIC-INF coincide.
Proof. Let X00 = X0 and X10 ,    , Xn0 be precise variables with the same state space as the
class X00 and probability distributions q(x0i |x0i1 ) = 1 if x0i = x0i1 and zero otherwise, for all
i = 1, . . . , n. Define Q(Xi |Xi0 = x0i ) by using the credal set Q(Xi |X0 = x0 ) of the original
network N , whenever x0i = x0 , for i = 1, . . . , n. Without loss of generality, assume that
t = n (if the query was in X0 , then use Xn0 as query instead of Xn ). This procedure creates
an imprecise HMM with hidden nodes X00 , . . . , Xn0 , manifest nodes X1 , . . . , Xn1 , and a final
query t = n with Xt = xt . This HMM clearly yields the same inferential result as does the
naive Bayes network N for STRONG-INF. By Theorem 3, the results of STRONG-INF and
EPISTEMIC-INF coincide in this HMM, hence the result of EPISTEMIC-INF in this HMM is
equal to the result of STRONG-INF in N . By construction, EPISTEMIC-INF in this HMM
contains the result of EPISTEMIC-INF in N (that is, the latter is equal or lies inside the
former). Because EPISTEMIC-INF always contains STRONG-INF, and in particular in the
naive Bayes structure, they must all coincide.
3.10 Summary of the Complexity Results
The complexity results obtained in this section suggest that inference in credal networks is
computationally difficult for a wide variety of model structures and dimensionalities. This
is the case, for instance, in precise-vacuous networks and singly connected networks with
ternary variables, according to the negative results we have shown. A few importation
exceptions have been obtained including last-node inference (filtering) in imprecise HMMs
and Markov chains and inference in naive Bayes structures. These few positive results are
important as such structures have applications in pattern recognition tasks such as activity
recognition (Antonucci et al., 2011) and robust classification (Zaffalon et al., 2003). The
previously known and new inferential complexity results are summarized in Table 2. The
star indicates models in which inferences under both irrelevance concepts coincide.
632

fiProbabilistic Inference in Credal Networks: New Complexity Results

Yet another irrelevance concept adopted in imprecisely specified models is Kuznetsov
independence. We can define the Kuznetsov extension analogously to the definitions of
strong and epistemic extensions, and define the problem of inference under Kuznetsov independence accordingly. It is known that the Kuznetsov extension lies between the epistemic
and the strong extensions (Cozman & de Campos, 2014). This implies that the outcomes
of inferences under Kuznetsov independence coincide with those under strong independence and epistemic irrelevance whenever the last two coincide. We hence get as corollaries
of the results shown here that inference under Kuznetsov independence is NPPP -hard in
precise-vacuous networks, NP-hard in singly connected networks with ternary variables,
polynomial-time computable in HMMs and Markov chains if the target variable is the lastnode, and polynomial-time computable in naive Bayes structures if the target variable is
the root node.

4. Conclusion
Credal networks generalize Bayesian networks to allow for the representation of uncertain
knowledge in the form of credal sets, closed and convex sets of probability distributions.
The use of credal sets arguably facilitates the constructions of complex models, but presents
a challenge to the computation of inferences with the model.
In this paper we studied the theoretical complexity of inferences in credal networks,
in what concerns the topology of the network, the semantics of the arcs (i.e., whether
epistemic irrelevance or strong independence is assumed), and the cardinality of variable
domains. In a nutshell, computing with credal networks is NP-hard except in the cases of
tree-shaped models under epistemic irrelevance, and binary polytree-shaped models under
strong independence. A notable exception is the computation of probability bounds on
the value of the last variable in a imprecise hidden Markov models, in which case we have
shown that inferences under epistemic irrelevance and strong independence coincide, which
implies that the latter is polynomial-time computable. We leave as an open question the
complexity of generic inferences in imprecise HMMs under strong independence.
Another possible avenue for future research is investigating the complexity of approximate inference. De Campos and Cozman (2005) showed that approximating inference
under strong independence is NP-hard, even if we consider only singly connected networks
of bounded treewidth. This is however not the case if variables are binary, as in this case
we can run the 2U algorithm to obtain the exact value. Maua, de Campos, and Zaffalon
(2012b) showed that for any network of bounded treewidth whose variables have bounded
cardinality there exists a fully polynomial time approximation scheme for performing inference under strong independence, that is, an algorithm that given a rational  > 0 finds
solutions which are within a factor 1 +  of the true value in time polynomial in the input size and in 1/. Apart from its tractability on credal trees, nothing is known about
the complexity of approximate inference under epistemic irrelevance, unless for the case
of precise-vacuous networks, which we showed here to provide the same inferences under
strong independence or epistemic irrelevance, so the results for approximate inference under
the former extend to the latter.

633

fiMaua, de Campos, Benavoli & Antonucci

Acknowledgments
The first author received financial support from PNPD/CAPES and the Sao Paulo Research
Foundation (FAPESP) grant no. 2013/23197-4. The second and third authors received
financial support from the Swiss National Science Foundation grants no. 200021-146606/1
and no. 200020-137680/1, respectively. A shorter version of this paper appeared in (Maua,
de Campos, Benavoli, & Antonucci, 2013).

References
Antonucci, A., Bruhlmann, R., Piatti, A., & Zaffalon, M. (2009). Credal networks for
military identification problems. International Journal of Approximate Reasoning,
50, 666679.
Antonucci, A., de Campos, C., & Zaffalon, M. (2014). Probabilistic graphical models.
In Augustin, T., Coolen, F., de Cooman, G., & Troffaes, M. (Eds.), Introduction to
Imprecise Probabilities, pp. 207229. John Wiley & Sons.
Antonucci, A., de Rosa, R., & Giusti, A. (2011). Action recognition by imprecise hidden Markov models. In Proceedings of the 2011 International Conference on Image
Processing, Computer Vision and Pattern Recognition (IPCV), pp. 474478.
Antonucci, A., Huber, D., Zaffalon, M., Luginbuhl, P., Chapman, I., & Ladouceur, R. (2013).
CREDO: a military decision-support system based on credal networks. In Proceedings
of the 16th Conference on Information Fusion (FUSION 2013).
Antonucci, A., Piatti, A., & Zaffalon, M. (2007). Credal networks for operational risk
measurement and management. In International Conference on Knowledge-Based and
Intelligent Information & Engineering Systems (KES), Vol. LNCS 4693, pp. 604611.
Antonucci, A., & Zaffalon, M. (2008). Decision-theoretic specification of credal networks: a
unified language for uncertain modeling with sets of Bayesian networks. International
Journal of Approximate Reasoning, 49 (2), 345361.
Boyd, S. P., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
Cano, A., Cano, J. E., & Moral, S. (1994). Convex sets of probabilities propagation by simulated annealing. In Proceedings of the Fith International Conference on Information
Processing and Management of Uncertainty in Knowledge Based Systems (IPMU),
pp. 48.
Corani, G., Giusti, A., Migliore, D., & Schmidhuber, J. (2010). Robust texture recognition
using credal classifiers. In Proceedings of the British Machine Vision Conference
(BMVA), pp. 78.178.10.
Cowell, R., Dawid, P., Lauritzen, S., & Spiegelhalter, D. (2007). Probabilistic Networks and
Expert Systems: Exact Computational Methods for Bayesian Networks. Statistics for
Engineering and Information Science Series. Springer.
Cozman, F. G. (2000). Credal networks. Artificial Intelligence, 120 (2), 199233.
Cozman, F. G. (2005). Graphical models for imprecise probabilities. International Journal
of Approximate Reasoning, 39 (23), 167184.
634

fiProbabilistic Inference in Credal Networks: New Complexity Results

Cozman, F. G., de Campos, C. P., Ide, J. S., & da Rocha, J. C. F. (2004). Propositional
and relational Bayesian networks associated with imprecise and qualitative probabilistic assessments. In Proceedings of the 20th Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 104111.
Cozman, F., & de Campos, C. (2014). Kuznetsov independence for interval-valued expectations and sets of probability distributions: Properties and algorithms. International
Journal of Approximate Reasoning, 55 (2), 666682.
de Bock, J., & de Cooman, G. (2013). Allowing for probability zero in credal networks
under epistemic irrelevance. In Proceedings of the 8th International Symposium on
Imprecise Probabilty: Theories and Applications (ISIPTA).
de Campos, C., & Ji, Q. (2011). Bayesian networks and the imprecise Dirichlet model applied
to recognition problems. In Liu, W. (Ed.), Symbolic and Quantitative Approaches to
Reasoning With Uncertainty, Vol. 6717 of Lecture Notes in Computer Science, pp.
158169. Springer, Berlin / Heidelberg.
de Campos, C. P., & Cozman, F. G. (2005). The inferential complexity of Bayesian and
credal networks. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pp. 13131318.
de Campos, C. P., & Cozman, F. G. (2013). Complexity of inferences in polytree-shaped
semi-qualitative probabilistic networks. In Proceedings of the 27th AAAI Conference
on Advances in Artificial Intelligence, pp. 217223.
de Campos, C. P., & Ji, Q. (2008). Strategy selection in influence diagrams using imprecise probabilities. In Proceedings of the 24th Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 121128.
de Campos, C. P., Zhang, L., Tong, Y., & Ji, Q. (2009). Semi-qualitative probabilistic
networks in computer vision problems. Journal of Statistical Theory and Practice,
3 (1), 197210.
de Campos, L., Huete, J., & Moral, S. (1994). Probability intervals: a tool for uncertain
reasoning. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 2, 167196.
de Cooman, G., Hermans, F., Antonucci, A., & Zaffalon, M. (2010). Epistemic irrelevance in
credal nets: the case of imprecise Markov trees. International Journal of Approximate
Reasoning, 51 (9), 10291052.
de Cooman, G., & Miranda, E. (2012). Irrelevant and independent natural extension for
sets of desirable gambles. Journal of Artificial Intelligence Research, 45, 601640.
de Cooman, G., & Troffaes, M. C. M. (2004). Coherent lower previsions in systems modelling: Products and aggregation rules. Reliability Engineering & System Safety, 85 (1
3), 113134.
Fagiuoli, E., & Zaffalon, M. (1998). 2U: An exact interval propagation algorithm for polytrees with binary variables. Artificial Intelligence, 106 (1), 77107.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman.
635

fiMaua, de Campos, Benavoli & Antonucci

Halpern, J. (2001). Conditional plausibility measures and Bayesian networks. Journal of
Artificial Intelligence Research, 14, 359389.
Kalai, G., & Ziegler, G. N. M. (2000). Polytopes: Combinatorics and Computation. DMV
Seminar. Birkhauser Verlag.
Kendall, D. G. (1974). Foundations of a theory of random sets. In Harding, E., & Kendall,
D. G. (Eds.), Stochastic Geometry, pp. 322376. John Wiley & Sons.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models. MIT press.
Kwisthout, J., & van der Gaag, L. C. (2008). The computational complexity of sensitivity
analysis and parameter tuning. In Proceedings of the 24th Conference on Uncertainty
in Artificial Intelligence (UAI), pp. 349356.
Kwisthout, J. H. P., Bodlaender, H. L., & van der Gaag, L. C. (2010). The necessity of
bounded treewidth for efficient inference in Bayesian networks. In Proceedings of the
19th European Conference on Artificial Intelligence (ECAI), pp. 237242.
Levi, I. (1980). The Enterprise of Knowledge. MIT Press.
Maua, D. D., de Campos, C. P., Benavoli, A., & Antonucci, A. (2013). On the complexity
of strong and epistemic credal networks. In Proceedings of the 29th Conference on
Uncertainty in Artificial Intelligence (UAI), pp. 391400.
Maua, D. D., de Campos, C. P., & Zaffalon, M. (2011). Solving limited memory influence
diagrams. CoRR, abs/1109.1754.
Maua, D. D., de Campos, C. P., & Zaffalon, M. (2012a). Solving limited memory influence
diagrams. Journal of Artificial Intelligence Research, 44, 97140.
Maua, D. D., de Campos, C. P., & Zaffalon, M. (2012b). Updating credal networks is
approximable in polynomial time. International Journal of Approximate Reasoning,
53 (8), 11831199.
Maua, D. D., de Campos, C. P., & Zaffalon, M. (2013). On the complexity of solving
polytree-shaped limited memory influence diagrams with binary variables. Artificial
Intelligence, 205, 3038.
Miranda, E., & Destercke, S. (2013). Extreme points of the credal sets generated by elementary comparative probabilities. In Proceedings of the 12th European Conference on
Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU),
Vol. 7958, pp. 424435.
Park, J. D., & Darwiche, A. (2004). Complexity results and approximation strategies for
MAP explanations. Journal of Artificial Intelligence Research, 21, 101133.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.
Piatti, A., Antonucci, A., & Zaffalon, M. (2010). Building Knowledge-Based Systems by
Credal Networks: A Tutorial. Nova Science.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82 (12),
273302.
636

fiProbabilistic Inference in Credal Networks: New Complexity Results

Salvetti, A., Antonucci, A., & Zaffalon, M. (2008). Spatially distributed identification of
debris flow source areas by credal networks. In Transactions of the 4th International
Congress on Environmental Modelling and Software Integrating Sciences and Information Technology for Environmental Assessment and Decision Making (iEMSs), pp.
380387.
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton University Press.
Shenoy, P. P., & Shafer, G. (1988). Axioms for probability and belief-function propagation. In Proceedings of the Fourth Annual Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 169198.
Tessem, B. (1992). Interval probability propagation. International Journal of Approximate
Reasoning, 7 (34), 95120.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities. Chapman and Hall.
Walley, P. (2000). Towards a unified theory of imprecise probability. International Journal
of Approximate Reasoning, 24 (23), 125148.
Wallner, A. (2007). Extreme points of coherent probabilities in finite spaces. International
Journal of Approximate Reasoning, 44 (3), 339357.
Zadeh, L. A. (1978). Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems,
1 (1), 328.
Zaffalon, M. (2002). The naive credal classifier. Journal of Statistical Planning and Inference, 105 (1), 521.
Zaffalon, M. (2005). Credible classification for environmental problems. Environmental
modelling and software, 20 (8), 10031012.
Zaffalon, M., & Fagiuoli, E. (2003). Tree-based credal networks for classification. Reliable
Computing, 9 (6), 487509.
Zaffalon, M., Wesnes, K., & Petrini, O. (2003). Reliable diagnoses of dementia by the
naive credal classifier inferred from incomplete cognitive data. Artificial Intelligence
in Medicine, 29 (12), 6179.

637

fiJournal of Artificial Intelligence Research 50 (2014) 885-922

Submitted 4/14; published 8/14

Demand Side Energy Management via Multiagent Coordination in
Consumer Cooperatives
Andreas Veit

ANDREAS @ CS . CORNELL . EDU

Department of Computer Science
Cornell University
Ithaca, NY 14853 USA

Ying Xu
Ronghuo Zheng

YINGX 1@ ANDREW. CMU . EDU
RONGHUOZ @ ANDREW. CMU . EDU

Tepper School of Business
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA

Nilanjan Chakraborty

NILANJAN . CHAKRABORTY @ STONYBROOK . EDU

Department of Mechanical Engineering
Stony Brook University
Stony Brook, NY 11794 USA

Katia Sycara

KATIA @ CS . CMU . EDU

Robotics Institute, School of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA

Abstract

A key challenge in creating a sustainable and energy-efficient society is to make consumer
demand adaptive to the supply of energy, especially to the renewable supply. In this article, we
propose a partially-centralized organization of consumers (or agents), namely, a consumer cooperative that purchases electricity from the market. In the cooperative, a central coordinator buys the
electricity for the whole group. The technical challenge is that consumers make their own demand
decisions, based on their private demand constraints and preferences, which they do not share with
the coordinator or other agents. We propose a novel multiagent coordination algorithm, to shape
the energy demand of the cooperative. To coordinate individual consumers under incomplete information, the coordinator determines virtual price signals that it sends to the consumers to induce
them to shift their demands when required. We prove that this algorithm converges to the central optimal solution and minimizes the electric energy cost of the cooperative. Additionally, we
present results on the time complexity of the iterative algorithm and its implications for agents
incentive compatibility. Furthermore, we perform simulations based on real world consumption
data to (a) characterize the convergence properties of our algorithm and (b) understand the effect
of differing demand characteristics of participants as well as of different price functions on the cost
reduction. The results show that the convergence time scales linearly with the agent population
size and length of the optimization horizon. Finally, we observe that as participants flexibility of
shifting their demands increases, cost reduction increases and that the cost reduction is not sensitive
to variation in consumption patterns of the consumers.
2014 AI Access Foundation. All rights reserved.

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

1. Introduction
Two key issues in creating a sustainable and energy-efficient society are to increase the penetration
of renewable sources, and to manage supply and demand so as to reduce demand peaks while
maintaining supply and demand balance. One way, which is most commonly used, to achieve
the demand supply balance is to supply all the requested demand whenever it occurs. However,
attempting to achieve demand supply balance by adjusting only the supply side leads to the use
of flexible (usually diesel operated) power plants that can be expensive, inefficient, and emit large
amount of carbon. An alternative to adjusting the supply side only, is to also adjust the demand
of the consumers (Palensky & Dietrich, 2011) via demand response programs. Demand Response
is defined as the changes in electricity consumption by end users from their normal consumption
patterns in response to changes in the price of electricity over time (Albadi & El-Saadany, 2007).
Several different forms of demand response programs have been developed (for an overview
see Albadi & El-Saadany, 2007). A typical example of an incentive based program, where customers receive payment for their participation, are Direct Load Control programs, where utilities
remotely control the power consumption of consumers appliances by switching them on/off. In
small scale pilot studies, direct load control has been successful in reducing peak energy consumption, however consumers were uncomfortable with yielding control of their appliances to utility
companies (Rahimi & Ipakchi, 2010; Medina, Muller, & Roytelman, 2010). Another type of demand management program is price based, where energy rates are variable and follow the real cost
of electricity. The objective of this indirect method is to control the overall demand by incentivizing consumers to flatten the demand curve through shifting energy from peak to off-peak times.
A typical example of these programs is Time of Use pricing, where the price during peak times
is higher than the price during off-peak times. Recent technological advances in smart meters and
smart appliances have created the potential to enable direct and real time participation of individual
consumers in the energy market and thus make real-time price based demand management programs
a reality. However, there are two key problems in realizing this potential. First, despite the presence
of small pilot programs, utilities consider individual consumers of insufficient size to be considered
for demand response services. Second, if consumers participate in the market directly, rather than
through the utilities, the stability of the system may be compromised (e.g., herding) (Ramchurn,
Vytelingum, Rogers, & Jennings, 2012). Considering these challenges, Mohsenian-Rad, Wong,
Jatskevich, Schober, and Leon-Garcia (2010) argue that a good demand side management program
should focus on controlling the aggregate demand (which is also important for economic load dispatching, Wood & Wollenberg, 1996) of a group of consumers instead of individual consumers.
In this paper, the problem of coordinating a group of consumers called consumer cooperative is
introduced and studied.
A consumer cooperative, or collective, allows partial centralization of consumers represented
by a group coordinator (mediator) agent, who purchases electricity from utilities or the market on
their behalf. Such consumer configurations can potentially increase energy efficiency via aggregation of demand to reduce peak power demand. The coordinator is neither a market maker nor a
traditional demand response aggregator (Jellings & Chamberlin, 1993), since it does not set energy
prices or aims to incur profits by selling to the market. Rather, its role is akin to a social planners, in
the sense that it manages the demand of its associated consumer group so that (a) the electricity cost
to the group is minimized, and (b) individual group members autonomously and in a decentralized
manner decide how to shift demands, while maintaining privacy of individual demand preferences
and constraints. The members of such a cooperative are typically geographically co-located in close
proximity to one another, for example a small neighborhood of households and/or enterprises. In
practice, close proximity is required due to limitations in the distribution infrastructure, geograph886

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

ically separated system operators, markets, and electricity suppliers. The proposed coordination
algorithm could also benefit already existing structures such as universities, malls, industrial parks,
commercial estates, and large residential complexes. Although such organizations may already purchase electricity in a centralized manner, their constituent members (e.g. firms in an industrial park)
are not currently coordinated so as to keep their constraints private.
Consumer cooperatives offer advantages to both energy utility companies and consumers. From
a utility companys perspective, the consumer groups are large enough to be useful in demand response programs and have more predictable demand shifts compared to individual consumers. Barbose, Goldman, and Neenan (2004) give a survey of utility experience with price based programs
and conclude that most participants were large industrial customers. For individual consumers, participation in such energy groups allows them to retain control of their own appliances. In addition,
the consumers can obtain electricity at better prices than they would have, if they had purchased
electricity individually. The price advantage is due to three reasons: First, the groups size allows
the group to enter into more flexible purchase contracts, so that the price paid by the consumers
reflects the actual cost of production more accurately. This is not the case in current long term fixed
contract structures (Kirschen, 2003). Second, by buying collectively, the group can benefit from
volume discounts analogous to group insurance programs. Third, in negotiated electricity contracts,
the price usually consists of two components, one reflecting the actual energy production cost and
the other as a premium against volatility in the energy demand and/or supply. Buying as a group
can help reduce the premium against volatility, provided that the demands of the group members
are coordinated, making their total demand more stable.
Our aim in this paper is to design effective schemes for coordinating the electricity demand of
agents who are purchasing electricity as a consumer cooperative. The technical challenge of this
endeavor is the fact that the central coordinator does not know the constraints of the individual
consumers, and thus cannot compute the optimal demand schedule on its own. Furthermore, the
actual cost of electricity consumption depends on the aggregate consumption profile of all agents.
However, the agents may not want to share their demand patterns or constraints with other agents or
the coordinator. Therefore, we present an algorithm designed to enable the central agent to coordinate the consumers to achieve the optimal centralized load, while the individual agents decide their
demand shifting autonomously and retain their private knowledge about their demand constraints.1
The papers contributions are as follows. First, we present an iterative coordination algorithm
to minimize the energy cost of a consumer cooperative that preserves the privacy of individual
demand constraints and costs of the consumers. Second, we prove that the algorithm converges to
the centralized optimal solution and provide computational complexity results. Third, we provide
formal arguments on the incentive compatibility of the coordination scheme. Fourth, we present and
discuss extensive simulation results based on real world data. A preliminary version of this work
appeared in the work of Veit, Xu, Zheng, Chakraborty, and Sycara (2013).
This paper is organized as follows: In Section 2 we give an overview of the related work and
point out the differences to the approach in this paper. In Section 3 we formulate the cost optimization problem of the consumer cooperative. Then, in Section 4 we introduce the demand scheduling
algorithms for the consumer cooperative. In particular, in Section 4.1 we introduce the basic iterative algorithm and in Section 4.2 we prove its convergence to the optimal solution. In Section 5,
we introduce the general iterative algorithm and in Section 5.3 we prove its convergence in general
1. This problem has some surface resemblance to problems where a centralized coordinator determines resource allocations for agents with private preferences, which the coordinator tries to elicit. Such problems are typically addressed
via Vickrey-Clark-Groves mechanisms. Our problem differs from those in fundamental ways that will be discussed
in the Related Work section.

887

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

settings. In Section 5.4, we provide complexity analysis of the algorithms convergence and in Section 5.5 we provide formal arguments on the incentive compatibility of the consumers. In Section 6
we evaluate the coordination algorithms using simulations based on real world consumption data.
Finally, in Section 7, we summarize the main contributions of this paper and offer a perspective on
future work.

2. Related Work
As mentioned in the introduction, the demand response programs vary from classical direct load
control to price based programs with real time prices. In this paper, we introduce an algorithm
that uses variable price signals to coordinate the energy consumption of a consumer cooperative.
Therefore, we restrict this discussion to demand management using variable price signals.
Current literature on price-based demand shaping mostly operates under the assumption that
it is desirable to have an automated or autonomous system (e.g. smart meter) that receives price
signals and uses them to help a consumer schedule its demand so as to minimize its electricity cost,
while satisfying its demand preferences and constraints. There are two types of price signals that
can be used: dynamic and deterministic prices. Most literature on price-based demand response
programs considers the case of deterministic prices, in which the electricity prices of all time slots
are known before consumption. This case applies to all long-term contracts and day-ahead markets,
if the planning horizon is sufficiently short (see Vytelingum, Ramchurn, Rogers, & Jennings, 2010;
Ramchurn, Vytelingum, Rogers, & Jennings, 2011). With known electricity prices, consumers can
compute their consumption schedule ahead of time, e.g., a daily consumption plan, or an annual
production plan. Our paper falls in this category of demand scheduling under deterministic prices.
The approaches for demand scheduling proposed in the literature differ in some important characteristics. First, they differ in the level at which the problem is studied and secondly they differ in
the objective of the demand scheduling. The different levels at which the problem has been studied include the level of the single consumer, the market maker, or the grid operator. The different
objectives include minimizing the cost of a single consumer, minimizing the total cost of power
generation, reducing the peak-to-average ratio in demand, and optimizing grid stability. There are
only a few papers studying the demand scheduling problem at the level of grid operators, where
the major concern is grid stability. In particular, objectives include the minimization of power
flow fluctuations (Tanaka et al., 2011), and the minimization of power losses and voltage deviations (Clement-Nyns, Haesen, & Driesen, 2010).
Most work on demand scheduling is done at the level of the consumers. An important characteristic of this regime is that the electricity prices are often exogenously fixed and not influenced by
the demand scheduling; i.e., the consumer is a price taker. Almost all papers in this regime focus on
the (micro) demand scheduling of one or multiple appliances in a single residential household or a
commercial building, with the typical objective to minimize the incurred electricity cost. For example, Chu and Jong (2008) study air-conditioning load control; Pedrasa, Spooner, and MacGill (2010)
optimize the operation schedule of various distributed energy resources including space heater and
pool pump, etc. There are other papers studying the problem from different perspectives. For example, Philpott and Pettersen (2006) and Samadi, Mohsenian-Rad, Wong, and Schober (2013) address
the challenge of the uncertainty in the loads of the consumers energy consumption. This stream
of research is close to our work, since we as well study the demand management at the level of
consumers. However, instead of studying a single consumer, we consider a group of consumers in a
consumer cooperative, who buys the electricity from a utility company under a known price scheme.
Our work differs from the above literature in the following aspects: size of shift, system stability,
888

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

indirect control, and, most crucially, information privacy. First, while all the above literature only
considers the control of a single consumer, in practice, it may be difficult for utility companies
or grid operators to deal with individual consumers in such demand response programs. The demand shift of an individual consumer might be too small in magnitude compared to the aggregated
necessary shift. Thus, it is unclear whether such a scheme will induce a shift of sufficient size. In
contrast, from the utilitys perspective, the consumer cooperative studied in our work is large enough
to be useful in demand response programs. Second, most of the above literature assumes that the
consumers can participate in the markets directly. However, there have been concerns voiced that,
without control by the utility companies, the stability of the system may be compromised with such
uncontrolled distributed interactions (Kirschen, 2003). In contrast, the demands of consumers in
our consumer cooperative are coordinated by a central coordinator to minimize the electricity procurement cost of the cooperative. Buying as a group helps to reduce the demand volatility, and
the coordinated demand management can achieve higher stability of demand and reduce demand
peaks. Third, the common assumption in the above literature is that the demand manager has direct
control over the appliances in the household and perfect knowledge about their loads and operation
constraints. However, in our cooperative, the coordinator lacks such control on demand scheduling
for individual consumers. Moreover, the demand constraints and preferences are private knowledge
to the individual consumer and not known either to the coordinator or other consumers.
Another stream of research related to our work is the demand scheduling at the level of the market maker. In contrast to the previous approaches, in this regime, the coordinator (i.e., the market
maker) can set the electricity prices for consumers, and often use the price lever to influence the
demand of a number of consumers. The studies in this stream can be categorized into two groups:
centralized and decentralized. Dietrich, Latorre, Olmos, and Ramos (2012) compare demand response programs in an electric system with high wind penetration under two different settings (i.e.,
centralized vs. decentralized) and show that the centralized approach often reaches higher overall
cost savings, but has the disadvantage that central knowledge of consumers constraints and preferences is necessary. Our partially decentralized approach avoids this limitation and keeps consumers
constraints and preferences private while also achieving proven optimality of the solution.
When the coordinator uses prices to incentivize consumers to shift demand, the coordinator
needs to pay close attention to the possibility of a herding phenomenon, whereby agents move their
demand towards the low price times simultaneously and thus cause a spike in demand and bring
instability to the system. To address this issue, in addition to a price signal, some papers adopt
auxiliary methods to make agents gradually change their loads. Voice, Vytelingum, Ramchurn,
Rogers, and Jennings (2011) charge agents an additional fee based on how much they change their
demand profile from one period to the next. Ramchurn et al. (2011) introduce an adaptive mechanism controlling the rate and frequency at which the agents are allowed to adapt their loads and to
readjust their demand profile. Vytelingum et al. (2010) introduce a compensation signal that is sent
to the agents, providing an estimate of how much they should aim to change their behavior. These
approaches may perform well, but, unlike our work, they do not provide any formal guarantees that
their proposed algorithms will converge to the optimal solution. Moreover, these approaches require
that the coordinator be a market maker who can charge consumers arbitrarily, e.g., by imposing additional fees, a feature that may not be implementable and acceptable in practice. In contrast, our
coordinator is akin to a social planner who must ensure that the total charge for the energy demand
of agents should equal the actual amount to be paid to the energy supplier.
Other papers address the decentralized demand-side management problem from a game theoretic perspective. They mainly focus on deriving a charging mechanism that can make all consumers
reach a stable demand equilibrium, which achieves a central objective. These papers often share an
889

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

underlying assumption that an individual consumers unit electricity cost increases in other consumers aggregate consumption. This assumption holds under the setting that the energy cost in each
period increases with the difference between demand and supply (Wu, Mohsenian-Rad, & Huang,
2012) or with the Peak-To-Average ratio (Mohsenian-Rad et al., 2010; Nguyen, Song, & Han,
2012), or the unit market price per period increases with the aggregate demand (Vytelingum, Voice,
Ramchurn, Rogers, & Jennings, 2011; Atzeni, Ordonez, Scutari, Palomar, & Fonollosa, 2013). In
the work of Mohsenian-Rad et al. (2010), a Nash equilibrium of demand achieves optimality via a
designed billing strategy under which individual cost is proportional to the total energy cost. However, under this billing strategy, a consumer must have an estimate of other consumers demand
before making its own decision. Therefore, to implement the proposed algorithms, the consumers
needs to coordinate in an iterative manner and exchange their demand profiles with each other in
each iteration, which may cause invasions of the consumers privacy. In other works of Wu et al.
(2012), Nguyen et al. (2012), Vytelingum et al. (2011) and Atzeni et al. (2013), individual consumers dont need to interact with each other, but only communicate with the central coordinator.
In these papers, the coordinator maintains the equilibrium by designing the unit price, based on the
aggregate demand. The algorithm is often implemented in a distributed and iterative scheme: in
each iteration the agents report their loads to the coordinator and the coordinator updates the price
accordingly. Although these papers ensure a minimum information exchange, none of them has
theoretically proven that their algorithms would converge to the centralized optimal solution. In
contrast, our paper not only safeguards the privacy of consumers, but also proves that the algorithm
converges to the centralized optimal solution.
Our problem has surface resemblance to problems, where a central coordinator determines resource allocations for agents with private preferences, which the coordinator tries to elicit. Such
problems are typically addressed via Vickrey-Clark-Groves mechanisms. The reason for not considering VCG-type mechanisms for our problem is that one of the key requirements of our application is that the coordinator has to pay the supplier for the groups electricity demand at the prices
given by the supplier. Thus, the revenue, namely the sum of the payments individual agents make
should equal the actual amount to be paid to the supplier. In other words, in the jargon of mechanism design, budget balance is a key requirement for our problem. It is well-documented in the
literature that VCG-type mechanisms do not guarantee budget balance (Green & Laffont, 1977;
Hurwicz, 1975)2 and it is impossible to design a mechanism that achieves all the three properties,
namely, efficiency, budget balance and strategy-proofness. For our problem, in addition to budget
balance, allocation efficiency is also a desired social goal. Our algorithm achieves both budget balance (Lemma 2) and allocation efficiency (Theorem 3). Although the algorithm cannot guarantee
strategy-proofness, due to the impossibility results, we prove that no manipulation strategy exists
that dominates truth reporting (Theorem 5).

3. Problem Formulation
In our model, the consumer group consists of N members with the planning period divided into
M discrete time slots. The number of discrete time slots depends on the market price structure,
which can differ depending on the utility companies. For example, M = 2 for time-of-use pricing
with different prices during day and night, whereas M = 24 for hourly time of use pricing. Let
R be an N  M matrix where each row of the matrix, ri is the electricity demand of the agent i,
2. In fact, as has been shown by Ausubel and Milgrom (2006), in the context of broadcast spectrum allocation, the
revenue obtained from VCG-type mechanisms can even be zero! In our case, this would mean the agents would not
pay any money to the coordinator, making it impossible to pay the supplier.

890

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

20

Twolevel Threshold Rate

13
12

high load

11

9

low load

7
6
5
4
0

high load
(Price: 10.34 Cents/kWh)

16
14
12
10

threshold hj

Total Cost

Marginal price pM
(j)
j

10

8

Twolevel Threshold Rate

18

(Dollars)

(Cents/kWh)

14

8

low load
(Price: 6.9 Cents/kWh)

threshold hj

6
4
2

50

100

Aggregated demand j

150

0
0

200

(kWh)

(a) Marginal electricity prices.

50

100

Aggregated demand j

150

200

(kWh)

(b) Total electricity cost.

Figure 1: Sample two-level increasing threshold pricing model as used by BC Hydro
i  {1, 2, . . . , N }. We call ri the demand profile of agent i. Each entry rij is the electricity demand
P
of agent i for time slot j. The total aggregated demand in time slot j is j = N
i=1 rij . The average
market price of a unit of electricity for the consumer group at time slot j is defined as pj (j ).
We assume a typical market price function, where the prices are different in each time slot and
the price has a threshold structure. This means that the marginal electricity prices differ among different demand levels. For each time slot, every unit of electricity consumed below a specified price
threshold is charged at a lower price, while any additional unit exceeding that threshold is charged
at a higher price. Thus, the marginal electricity price in a time slot, denoted by pm
j (j ), is a nondecreasing function of the total demand. The marginal price at a given demand level is the payment
increment (decrement) for adding (reducing) one unit of electricity. Figure 1a shows an example of
a two-level increasing threshold pricing model adopted from BC Hydro.3 The(marginal price of a
pH
j > hj
j
two-level threshold structure can formally be written as follows: pm
(
)
=
with
j
j
L
p j  j  hj
L
+
pH
j > pj , where hj is the price threshold in time slot j. Let x denote the positive value of a term
x, i.e., x+ = max {0, x}, and x denote the negative value respectively, i.e., x = min {0, x}. The
total energy cost for time slot j is thus the integral of the marginal prices. Figure 1b shows the total
electricity cost for the aggregated demand based on the two-level threshold pricing model. The total
electricity cost can be computed as:
+

L
L
pj (j ) j = pH
j (j  hj ) + pj (j  hj ) + pj hj

(1)

The demand profile of each agent ri must satisfy its individual constraints. The overall demand
consists of two types of loads: shiftable loads and non-shiftable loads. Mohsenian-Rad et al. (2010),
Mohsenian-Rad and Leon-Garcia (2010) and Wu et al. (2012) model the demand constraints of
shiftable and non-shiftable loads. An example for non-shiftable loads is typically a refrigerator and
3. BC Hydro is a Canadian utility company. This pricing model is obtained from www.bchydro.com.

891

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

shiftable loads include dishwasher, electric vehicles, washer/dryer, etc. The appliances are modeled
by a total demand that is required over the course of the planning horizon, upper and lower bounds
on the demand in each time slot as well as earliest start and latest end times. These constraints form
a convex set. For our problem, P
we also assume that the total demand of each agent during the whole
planning period is fixed, i.e., M
j=1 rij = i , where i is the total demand for agent i. Further,
we also consider loads, where the demand constraints form a convex set and denote it as Xi . This
constraint set is private knowledge and an agent does not share it, neither with other firms nor with
the coordinator. In some application scenarios, when an agent determines its energy demand profile,
it has to consider additional costs associated with the demand schedule. For example, in any given
factory the energy is most commonly used for production. Changing the energy demand schedule,
therefore, may mean changing the production process and, thereby, the production cost. For agent i,
this cost is denoted by gi (ri ). We assume this cost function to be convex. The overall cost function
P
of each agent is then M
j=1 pj (j ) rij + gi (ri ).
With the objective to minimize the sum of all agents costs, the overall energy allocation problem can be written as:
P
PN PM
pj (j )rij + N
min C (R) :=
i=1 gi (ri )
i=1
j=1
PM
(2)
s.t. ri  Xi , j=1 rij = i .
where the energy allocations rij are the optimization variables and R is the matrix of the demand
profiles of all the agents. Note that the above problem is defined on a convex
Xi . Although the
P setP
M
objective function is non-linear, it is convex because of the following. First, N
j=1 pj (j ) rij =
i=1
PM
j=1 pj (j ) j is convex and non-decreasing in j as indicated by Equation 1. Together with
PM
PN
j =
j=1 pj (j ) j is convex in rij , i, j, (Boyd & Vandeni=1 rij , we can conclude that
berghe, 2004). Since gi (ri ) is also convex, the total cost function C (R) is a summation of convex
functions and so also convex. Thus, Problem 2 is a convex minimization problem.

4. Solution Approach
Although the Problem 2 is a convex optimization problem, since the constraints and preferences of
the agents are private knowledge, the optimal demand profiles cannot be computed directly by the
central coordinator. The objective function, although a sum of the individual costs of each agent,
is coupled, because the price of electricity in any time slot j depends on the aggregated demand
of all agents j . However, since the constraints in Problem 2 are agent-specific, they are naturally
separable. Therefore, a primal decomposition approach (Bertsekas & Tsitsiklis, 1989) is used to
solve the problem in which the sub-problems correspond to each agent optimizing its own energy
cost subject to its individual constraints. The central coordinator has to compute the appropriate
information to be sent to the agents so as to guide the demand pattern towards time slots with lower
prices (this corresponds to the master problem in primal decomposition methods).
Since the agents know the electricity market prices, they individually optimize their demand
according to those prices. Let the resulting demand profile be called the uncoordinated demand
profile. Figure 2a depicts such an aggregated uncoordinated demand profile in a setting with three
time slots. It can be seen that the aggregated demand in time slot 2 is above the threshold, 2 > h2 ,
and the aggregated demand in time slots 1 and 3 is below, 1 < h1 , 3 < h3 . Thus, a shift of
demand from time slot 2 to the other time slots would reduce the total cost for the group. However,
since the agents dont know the demand of the other agents, they cannot shift their demand. An
intuitive solution approach, for the coordinator, to coordinate the demand would be to inform the
892

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

2
1.5

Threshold
h2

High price
Low price 
1

h1

3

1

1
0.5
0

h3
3

3

1

2.5

h1

2

High price

Threshold
3
h3

1.5

2

1
0.5

1

2

Time slot j

3

0

Aggregated demand

3.5

h2

Low price

(c) Coordinated Scenario

4

Aggregated demand

3.5

2

3
2.5

(b) Herding Scenario

4

2
Aggregated demand j

Aggregated demand j

Aggregated demand

Aggregated demand j

(a) Initial Scenario

4
3.5

Threshold

3

2

2.5
2
1.5

High price
h
Low price  1
1

h2
3

h3

1
0.5

1

2

Time slot j

3

0

1

2

Time slot j

3

Figure 2: A comparison of demand profiles in Uncoordinated, Herding and Coordinated scenarios.
agents about the aggregated demand in each time slot. Knowing the market price, the agents could
then solve their individual optimization problems. This approach is problematic, because the agents
dont know the demand constraints and preferences of the other agents in the group, while their costs
strongly depend on the demand of the other agents. For example, all agents knowing the market
price and the current aggregated demand could shift as much demand as possible to a supposedly
cheap time slot. This would lead to load synchronization, herding phenomenon, where all agents
shift demand to the supposedly cheap time slot, resulting in a new demand peak in that time slot
and thus increasing the total cost. The effect of this herding phenomenon is shown in Figure 2b,
where too much demand was shifted from time slot 2 resulting in demand above the threshold in
time slots 1 and 3. Thus, the key challenge is to design the information that the coordinator sends
to the agents, the virtual price signal, to enable the system to minimize the overall cost.
A virtual price signal is not the final price the agents have to pay, but information about what
they would have to pay, given the current aggregated demand. The virtual price signal enables the
agents to foresee the possible price increment/reduction caused by their demand shifting. Therefore, the virtual price signal for agent i in time slot j, svij (rij |R), is a function of the variable rij ,
denoting the new demand of agent i in time slot j. The price signal is computed based on the previous aggregate demand profile R, which is therefore included in the price function. For ease of
readability, time is not made explicit in this notation and will only be used in the proofs. The superscript v indicates the virtual price, in contrast to the real market prices pj (j ). To design the virtual
price signal, the coordinator first computes the amount of demand that should be ideally shifted in
each time slot. As shown in Figure (2a), this amount, denoted by j , j = 1, 2, 3, is the difference
between the total aggregated demand and the price threshold in each time slot. For readability, we
will refer to j as delta increment, noting that the delta could have negative values, i.e., it could
be a decrement. To avoid herding, the delta increment needs to be divided among the agents and a
threshold price signal needs to be designed for each agent, so that the price below the threshold is
lower than the price above the threshold. This serves to penalize the total demand in a time slot going above the threshold. Thus, the agents know the maximum amount of demand they could shift at
what prices and can solve their individual optimization problem. The exact calculation of the price
signal svij (rij |R) is shown in Section 4.1.2. Given the price signal, the virtual cost optimization
problem each agent solves is
P
v
min Cvi (ri |R) := min M
j=1 sij (rij |R) rij + gi (ri )
P
(3)
M
s.t. ri  Xi , j=1 rij = i .
Note that this problem, like the overall problem, is a convex optimization problem and thus solvable. However, because of their individual constraints and cost functions, some agents might not be
able to shift as much demand as was assigned to them by means of the virtual price signal. This
893

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

implies that the aggregated demand shift can be less than the amount that could have been achieved.
Figure 2c shows this case, where the total demand in the second time slot remains above the threshold, because not the whole 2 could be shifted. In order to shift the remaining demand, another
price signal, dividing the remaining amount, would be necessary. This motivates us to design an
iterative algorithm for the coordinator to update the virtual price signal based on the consumers
feedback and thus gradually adjust the individual demands to the central optimal solution.
4.1 Basic Coordination Algorithm
We present the basic coordination algorithm and the details of the virtual price signal design for the
energy allocation in the setting with a planning horizon of only two time slots, M = 2, and no cost
for shifting demand, g() = 0. Although called basic setting in this paper, this setting has practical
relevance, because it represents the commonly used Time of Use pricing schemes that divide the
planning horizon in two time slots (Albadi & El-Saadany, 2007). One time slot with typically high
load has high prices and one time slot with typically low load has low prices for electricity.
4.1.1 OVERVIEW OF A LGORITHM
Recall that ri denotes the demand profile of agent i and that R is the matrix of the demand profiles
of all agents. Let r0i be the updated demand profile of agent i after an iteration and R0 be the new
demand profile of all agents.
Initialization: Each agent computes an initial uncoordinated electricity demand profile ri by
solving Problem 3 based on the market prices and sends it to the coordinator.
1. The coordinator adds up the individual demands to determine the aggregated demand j for
each time slot j and then calculates the delta increment j of demand to be shifted in or out
of each time slot. Finally, the coordinator divides that demand among all agents and computes
the virtual price signals svij (rij |R) for each agent i and time slot j.
2. The coordinator sends the virtual price signals to all agents.
3. After receiving the virtual price signal, each agent individually calculate its new demand
profiles r0i according to the optimization Problem 3.
4. The agents send their new demand profiles back to the coordinator.
5. The coordinator compares the new demand profiles to the old profiles. If no agent changed its
demand profile, i.e., R = R0 , the coordinator stops the algorithm. Otherwise, it sets R = R0
and goes to step (1).
4.1.2 C OORDINATION WITH V IRTUAL P RICE S IGNAL
The virtual price signal for one agent in one time slot is a threshold price function of the demand
in that time slot. The demand up to a specified threshold is charged at a low price and the demand
above the threshold is charged at a higher price. The virtual price signal is therefore parameterized
H
by the low marginal price, pL
j , the high marginal price, pj , and the price threshold, hij (R), that
specifies the demand levels at which the prices apply. The virtual price, svij , for agent i in time slot j
is computed based on these parameters as follows
( pL h (R)+pH (r h (R))
ij
ij
j ij
j
rij > hij (R)
rij
svij (rij |R) =
(4)
L
pj
rij  hij (R)
894

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

Aggregated demand before iteration

1.75

Agent 1 demand r

1j

4

3.5

2
2

2.5

h2

2

2j

1

0.5

0

1

0.75
0.5
0.25

1.75

1

1

12

h12

1

r11

h11

11
1

2

Time slot j

h1

1.5

r12

1.5
1.25

0

Agent 2 demand r

Aggregated demand j

3

Agent 1 demand before local optimization

r22

1.5
1.25
1

h21

0.75
0.5
0.25

22

h22

21
r21

0

2

Time slot j

Agent 2 demand before local optimization

1

2

Time slot j

Figure 3: Division of j among agents for the virtual price signal. The yellow demand belongs to
agent 1 and the red demand to agent 2. Here 1 is the amount of demand that can be shifted into
time slot 1, while 2 indicates the amount of demand that needs to be shifted from time slot 2.

Agent 1 demand after local optimization

1.5

r

1.25

h

1

3.5

12

12

0.75
0.5
0.25

3

h

11

r
11

0

1

2

Time slot j

Agent 2 demand after local optimization


2j

1.75

Agent 2 demand r

Aggregated demand after iteration

4

1.5
1.25
1
0.75
0.5

h21

r21

h22

Aggregated demand j

Agent 1 demand r


1j

1.75

r22

2.5

h2

2

1.5

2

h1
1

1

0.5

0.25
0

1

0

2

Time slot j

1

Time slot j

2

Figure 4: Demand after agents local optimization. Agent 1 cannot reduce its entire demand from
time slot 2 (i.e., r12 > h12 ), because of its own demand constraints (left top plot). Thus, the
aggregated demand in time slot 1 is still below the threshold (i.e., 01 < h1 ). Therefore, the central
coordinator again sends a modified price signal to the agents and the algorithm continues until
agents stop shifting their demands.

895

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

L
v
Although the prices pH
j and pj are derived from the market prices, sij is called a virtual price
signal, because the threshold hij (R) changes, if the agents change their demand profiles. The
coordinator chooses hij (R) < rij to induce the agents to reduce demand or hij (R) > rij to
increase demand in one time slot. With ij as the amount the coordinator wants agent i to change
demand in time slot j, the threshold hij (R) is updated based on the demand profiles submitted in
the last iteration:
hij (R) = rij + ij
(5)

Thus, the agents know that, at the current market price, they can at most change their demand
in time slot j by ij . For the demand exceeding hij (R), they need to pay a higher price. The
demand, j , the coordinator wants to change in time slot j is calculated as the difference between
the current aggregated demand and the threshold of the market price:
j = hj  j

(6)

Since the coordinator
wants the total demand change of the agents to be less than j , it has to
P
ensure that i ij  j . The allowable shift for an agent is proportional to the agents share of
 r
the aggregated demand in that time slot, i.e., ij = Pj rijij . Figure 3 shows how the total demand
i
change is divided among the agents in order to create the individual virtual price signals. The left
side shows the initial aggregated demand of two agents. The yellow demand belongs to agent 1 and
the red demand to agent 2. Since the aggregated demand is below the threshold in time slot 1 (i.e.,
1 < h1 ) and above the threshold in time slot 2 (i.e., 2 > h2 ), the coordinator wants the agents to
shift demand from time slot 2 to time slot 1. The amount of demand that can be shifted into time
slot 1 is 1 , while 2 is the amount of demand that should to be shifted out of time slot 2. The right
side shows the individual thresholds of the agents as determined by the central coordinator using
the procedure described above. The allocation of the demand change to the agents is illustrated by
the dashed arrows. The current demand of agent 1 in time slot 1 is r11 and its threshold for time slot
1 is h11 (the other notations can be interpreted similarly).
4.1.3 T HE AGENT  S R ESPONSE TO THE V IRTUAL P RICE S IGNAL
Having received the virtual price signal, the agents will independently optimize their demand profiles in order to minimize their cost according to Problem 3. The agents objective function Cvi (ri |R)
P
v
= M
j=1 sij (rij |R) rij + gi (ri ) can be written as:
Cvi

(ri |R) =

M
X



+

L
L
pH
j (rij  hij (R)) + pj (rij  hij (R)) + pj hij (R) + gi (ri )

(7)

j=1

Since the agents have to pay the high price pH
j for demand exceeding their individual threshold,
no agent will shift too much demand, based on a false impression of possible cost reduction. Figure 4 shows the agents demand profiles after their individual optimization. The left side shows the
individual problems of the agents, after they have optimized their demand profile. In comparison to
the right side in Figure 3, it can be seen that agent 2 shifted the whole allocated amount, but agent 1
only shifted some part of it (e.g., due to its constraints). The right side in Figure 4 shows the central
problem after the agents individual optimization. It can be seen that there is still demand left to be
shifted from time slot 2 to time slot 1. This remaining demand would again be divided among the
agents in the subsequent iteration.
896

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

4.1.4 PAYMENT RULE
After the algorithm has converged, the agents have to pay the virtual unit price for their demand in
each time slot. In the last iteration the virtual unit price equals the average unit price for the aggregate demand of the whole cooperative, because no agent changed its demand profile, Equation 9.
With R as final demand profiles of the whole cooperative and ri as final demand profile of agent i,
the payment for agent i is given as
paymenti =

M
X

svij (rij |R) rij

(8)

j=1

4.2 Convergence of the Basic Algorithm
In this section we prove that the basic iterative procedure always converges to an optimal solution
L
in the basic setting with M = 2, gi () = 0 and pH
j > pk , j, k. In Lemma 1, we show that the
algorithm strictly reduces cost in every iteration. This fact will be used in Theorem 1 to show that
the algorithm always converges. Then, in Theorem 2 we show that, when M = 2, gi () = 0 and
L
pH
j > pk , j, k, the converged solution is an optimal solution. Subsequently, we show that the
algorithm can get stuck in a suboptimal solution in general settings if M > 2 (Lemma 3) or if
gi () 6= 0 (Lemma 4).
Lemma 1. The algorithm strictly reduces the total cost in every iteration: C (R0 ) < C (R).
Proof. Lets first introduce some notation that will be used throughout this proof. Let R be the total
demand profile at the end of iteration round t and R0 be the total demand profile at the end of round
t + 1. Similarly, let C(R) be the total cost at the end of iteration t and C(R0 ) be the total cost at
the end of iteration t + 1. The virtual prices for round t + 1 are computed by the central coordinator
using R. Let Cvi (ri |R) be the cost for agent i computed according to the virtual price signal for
demands at the beginning of round t + 1 and Cvi (r0i |R) at the end of round t + 1.
At the beginning of each iteration the total cost for the consumer group based on market prices
(given by the objective function in Problem 2) equals the sum of the individual cost of the agents
P
v
based on the virtual price signals (given by Problem 3), i.e., N
i=1 Ci (ri |R) = C (R):
N X
M
X

svij

(rij |R) rij +

gi (ri )

i=1

i=1 j=1
N X
M
X

N
X

"



+
(hj  j ) rij
P
=
rij  rij +
i rij
i=1 j=1



(hj  j ) rij
P
+ pL
r

r
+
ij
ij
j
i rij

# X
N
(hj  j ) rij
L
P
+ pj rij +
+
gi (ri )
i rij
i=1
=

pH
j

M h
N
i X
X
+

L
L
pH
(

h
)
+
p
(

h
)
+
p
h
+
gi (ri )
j
j
j
j
j
j
j
j
j=1

=

(9)

N X
M
X
i=1 j=1

i=1

pj (j ) rij +

N
X

gi (ri )

i=1

897

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

If the algorithm has not stopped, at least one agent has changed its demand profile, i.e., i with
r0i 6= ri . Agents only change their demand profile, if that reduces their cost according to Problem 3.
Thus, given the virtual price, signal for agent i the cost of the new demand profile r0i is strictly lower
than of its previous demand profile ri :

Cvi r0i |R < Cvi (ri |R)
(10)
After all agents
submitted their new demand profile, the new aggregated demand is comPN have
0
0
puted as: j = i=1 rij . Next, we show that the sum of the agents individual cost according to the
virtual price signals is an upper bound on the total central cost at market prices. Thus, the total cost,
based on the new aggregated demand, is lower or equal to the sum of the agents individual cost,
based on their new demand. This fact is very important, because it prevents the herding behavior:
No solution that reduces the cost for the agents individual problems can lead to a worse solution in
the central problem. This is proved by showing with Equations 1 and 7 that for every time slot j the
difference between the central cost for the aggregated demand and the sum of the agents individual
cost is less than or equal to 0. The following is a sketch of the proof omitting some algebraic steps
for ease of readability. Consult Appendix A for the complete proof. For any time slot, j, we have
N
X

N
X
 0
 0
0
pj 0j rij

svij rij
|R rij

i=1

i=1

(PN
=


0
rij
 hij (R) 0j
 0

H
pL
rij  hij (R) 0j
j  pj
 0
 i
L
pH

p
r

h
(R)
0
ij
j
j
ij
i


+
H
0
pL
rij
 hij (R)
0
j  pj

i: r 0 h
PN ij ij
0 >h
i: rij
ij

hP
N

i=1
= hPN


i=1

L
pH
j  pj



> hj
 hj

(11)

0j > hj
0j  hj


 
PN v  0
0 j, it also holds for the sum of all time slots:
0
0
Since i=1 pj j rij  i=1 sij rij |R rij
P
v
0
C (R0 )  N
i=1 Ci (ri |R).
From Equations 9, 10 and 11 we can conclude that:
PN

N
N
 X
 X
C R0 
Cvi r0i |R <
Cvi (ri |R) = C (R)
i=1

(12)

i=1

Thus, the total cost is strictly reduced in each iteration.
Theorem 1. The basic iterative algorithm for solving Problem 2 always converges.
Proof. From the definition we have that Problem 2 is convex and a lower bound on the total cost can
be obtained by the sum of the individual initial demand profile costs at market prices. From Lemma
1 we have that the algorithm reduces the total cost in each iteration. Thus, it can be concluded that
the algorithm converges.

P
PN v
N
Lemma 2. The real market cost, N
i=1 pj i=1 rij rij , equals the virtual cost
i=1 sij (rij |R) rij
in a time slot j, if either i, rij  hij (R) or i, rij  hij (R).
Proof. If i, rij  hij (R), then j  hj and from the first case in Equation 11 follows the cost are
equal. If i, rij  hij (R), then j  hj and from the second case in Equation 11 follows the cost
are equal.
898

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

Theorem 2. The solution R of the basic algorithm is optimal, in either of the following cases
A) j, R satisfies j 6= hj with no restrictions on M and gi ()
B) R satisfies j, s.t. j = hj and M = 2, gi () = 0 and the high price in any time slot is
L
greater than the low price in any other time slot, i.e., pH
j > pk , j, k
Proof. We prove by contradiction that when the algorithm has converged to the solution R that
satisfies Case A, there is no other solution R0 with lower cost. Suppose there exists such a solution
R0 , i.e., C(R0 ) < C(R). We will show that this contradicts the convergence conditions of the
algorithm that i, ri is the solution of the agents individual problem, ri = arg minxi Xi Cvi (xi |R).
Denote R# () = R+(1  ) R0 ,   (0, 1), a linear combination of R and R0 . Since the central
cost function is convex and C(R0 ) < C(R), we also have C(R# ()) < C(R). If j, R satisfies
j 6= hj , then for the demand in the beginning of the iteration rij we have j either i, rij > hij (R)
or i, rij < hij (R), because if j > 0 then i, ij > 0 and similarly if j < 0 then i, ij < 0.
#
#
It follows,   (0, 1), s.t. j, either i, rij
()  hij (R) or i, rij
(1 )  hij (R).


PN
#
v
= C(R# ()). Moreover, we have
Therefore, by Lemma 2,
i=1 Ci ri () |R
P
v
C(R# ()) < C(R) and from Equation 9 we also have N
i=1 Ci (ri |R) = C(R). It follows,
v
v
i, s.t. Cvi r#
i () |R < Ci (ri |R), which conflicts with ri = arg minxi Xi Ci (xi |R), i.e., ri is
not the solution of the agents individual Problem.
We prove by contradiction that when the algorithm has converged to the solution R that satisfies
Case B, there is no other solution R0 with lower cost. Assume there exists a solution R0 with
C(R0 ) < C(R). Let the 2 time slots be {j, k}, where 0j < j and 0k > k (without loss of
m
generality). Note that pm
j (j ) > pk (k ), because otherwise the new cost would not be lower. If
H
L
m
j = hj , k = hk then pm
j (j ) = pj (as j decreases) and pk (k ) = pk (as k increases).
L
m
m
From the market price structure we have pH
k > pj . It follows pj (j ) < pk (k ), which leads to
m
L
H
a contradiction. If j 6= hj , k = hk and if j < hj then pj (j ) = pj and pm
k (k ) = pk thus
H
H
H
m
H
m
m
pm
j (j ) < pk (k ). If j > hj then pj (j ) = pj and pk (k ) = pk . We have pk > pj , because
otherwise a shift from j to k would be beneficial for at least one agent and the algorithm would
m
not have stopped. It follows pm
j (j ) < pk (k ), which again leads to a contradiction. The case of
j = hj , k 6= hk works similarly.
It follows that R is the optimal solution, as no solution with lower cost exists. Thus, the proposed iterative algorithm converges to the optimal solution. Since the problem is convex that solution is also the global optimal solution (Boyd & Vandenberghe, 2004).
Lemma 3. The basic algorithm could converge to a suboptimal solution R in settings, where R
satisfies j, s.t. j = hj , M > 2 and gi () = 0.
Proof. We now prove by presenting a counterexample that the basic algorithm can converge to a
suboptimal solution in settings, where R satisfies j, s.t. j = hj , M > 2 and gi () = 0. Consider
a population of 2 agents, N = 2, and a planning horizon of 3 time slots, M = 3. The agents
constraints are in a form so that in the converged solution the aggregated demand is below the
threshold in one time slot, directly at the threshold in another time slot and in one time slot above
the threshold.
Let the price function for the three time slots be given as:
H
L H
L H
(pL
1 , p1 ) = (3, 6), h1 = 10; (p2 , p2 ) = (2, 5), h2 = 10; (p3 , p3 ) = (1, 4), h3 = 10;

899

fi8
6

p1H=6

p2H=5

p3H=4

p1L=3

p2L=2

p3L=1

(KWh)

10

demand agent 1

demand agent 1

(KWh)

V EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

4
2
0

1

2

3

10
8
6
4
2
0

1

(KWh)

10
8
6
4
2
0

1

2

2

3

time slot

demand agent 2

demand agent 2

(KWh)

time slot

3

time slot

10
8
6
4
2
0

1

2

3

time slot

(b) Optimal Solution R0

(a) Converged Solution R(T )

Figure 5: Converged and Optimal solution for the example in Lemma 3. The top row shows the
demand of agent 1 and the bottom row the demand of agent 2. The dashed lines indicate the upper
and lower bounds of the demands of the agents in a time slot. The solid black lines show the price
thresholds for each time slot.
The individual constraints on the agents demand are: (a) upper and lower bounds on the demand
in each time slot and (b) constant total demand over all time slots. Specifically,
r11  [1, 4], r12  [1, 9], r13  [1, 9], r11 + r12 + r13 = 1 = 17
r21  [1, 9], r22  [1, 9], r23  [8, 9], r21 + r22 + r23 = 2 = 17
Let R(t) denote the demand profile of the agents in the tth iteration and let t = 1 be the initial
iteration and t = T be the final iteration at convergence. At the beginning the agents compute their
(1)
(1)
initial demand profiles based on the market
prices r1 = (1, 7, 9), r2 = (1, 7, 9). The cost based

on the initial demand profiles is C R(1) = 88. At convergence, the profiles of the two agents are
(T )

(T )

r1 = (4, 5, 8), r2 = (4, 5, 8) (see Figure 5a for a graphical representation). The cost based on
these demand profiles is C R(T ) = 78. However, a different demand profile of the agents exists
with r01 = (4, 7, 6), r02 = (6, 3, 8) (see Figure 5b for a graphical representation). This profile is
feasible and leads to lower total cost, i.e., C (R0 ) = 76. It follows that the algorithm has stopped in
a suboptimal solution.

Lemma 4. The basic algorithm could converge to a suboptimal solution R in settings, where R
satisfies j, s.t. j = hj , M = 2 and gi () 6= 0.
Proof. In Appendix B a counterexample is presented, proving that the basic algorithm can converge
to a suboptimal solution in settings, where R satisfies j, s.t. j = hj , M = 2 and gi () 6= 0.
900

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

5. General Coordination Algorithm
In the previous sections we introduced a basic iterative coordination algorithm for optimal energy
allocation in the basic setting with a planning horizon of only two time slots, M = 2, and no cost
for shifting demand, g() = 0. Moreover, we presented counter examples showing that, in general
settings with M > 2 or g() 6= 0, the algorithm can get stuck in a suboptimal solution. In this
section, we present a general coordination algorithm for the energy allocation in general settings
with more than two time slots and non-zero individual costs for shifting demand.
5.1 The Additional Phase
The reason for convergence to a suboptimal solution is that the coordinator cannot determine the
agents marginal valuation for their demand, if the aggregated demand in some time slot is at a
threshold. The agents might have different marginal valuations, so that a shift from an agent with a
lower valuation to an agent with a higher valuation might be beneficial. However, this opportunity
for improvement cannot be realized, because the coordinator does not know the marginal valuations
of the agents.
We now introduce an additional phase to the algorithm to address this problem. When the basic
algorithm has converged, the coordinator checks whether the aggregated demand is at the threshold
for at least one time slot. If that is not the case, the algorithm stops and the solution is optimal.
However, if the demand is at the threshold in some time slot, the coordinator initiates another phase.
The basic idea is that, besides the virtual price signal, the coordinator sends an additional query
to the agents, requesting their valuations for an -increase and -decrease of threshold in the time
slots, where the aggregated demand is at the threshold. Then, the coordinator uses this information
to adjust the virtual price signals.
+
Let vij
denote the value for agent i of an -increase of its threshold in time slot j. Let further
ij+
R
be the demand profile, where in the resulting price signal for agent
in time slot
 i the threshold

j+
v
ij+
ij+
be its virtual cost,
j is increased by , i.e., hij (R ) = hij (R) + . Moreover, let Ci ri |R
where the threshold in time slot j is increased by .
Definition 1. Agent is marginal valuation for an -increase of the threshold in time slot j is the
difference in its virtual cost between the original virtual problem and the virtual problem with an
-increased threshold in time slot j. The valuation for an -decrease is defined analogously.







j

+
ij+
0
v
ij
vij
(R) = Cvi rj+
|R

C
r
|R
;
v
(R)
=
C
r
|R
 Ci r0i |R .
i
i
i
i
ij
i

(13)

When the agents receive their virtual price signal and the query for their marginal valuations,
they first compute their optimal demand profile r0i , based on the price signal. Second, they compute
+

their marginal valuations vij
(R) and vij
(R) for every time slot, with the aggregated demand at
+
the threshold. Let the vector vi consist of the valuations for -increments and vi for decrements
respectively. Finally, the agents send ri , vi+ and vi back to the coordinator.
Having received the agents information, the coordinator computes the virtual price signals for
the next iteration. If the aggregated demand is at thenthreshold,
it finds the agents with the lowest
o
+
cost for an -increase of the threshold: l = arg mini vij and the lowest cost for an -decrease of
n o


+
the threshold: k = arg mini vij
. If the combined cost is negative vkj
+ vlj
< 0, a beneficial shift
901

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

from agent l to agent k exists. In that case, the coordinator updates the thresholds of agents l, k as
lj = , kj = .
The algorithm stops, if the demand is not at the threshold in any time slot or if for all time slots at

+
the thresholds no pair {l, k} exists with vkj
+ vlj
< 0. It follows, when the algorithm converges,
in every time slot with the aggregated demand at the threshold, j = hj , for all agents, the cost
reduction of an -increase of the threshold is less than the additional cost from a -decrease of the
threshold for all other agents, i.e.,
+

vlj
 vkj
, l, k  {1, ..., N } and j s.t. j = hj .

(14)

Otherwise, the coordinator would change the price signals and the algorithm would not stop.
5.2 General Algorithm
The general algorithm is designed as an iterative algorithm, where the coordinator updates the virtual
price signals based on the consumers feedback and thus gradually adjusts the individual demands
to the central optimal solution. The overall algorithm is shown in Algorithm 1. Each iteration
consists of two steps: First, the central coordinator aggregates the demand submitted by the agents
and computes virtual price signals for each agent. The virtual price signals are computed in the
subroutine CalculateVirtualPriceSignals (Algorithm 2). Second, the individual agents use the
virtual price signal to solve their individual cost optimization problem and compute their marginal
valuation for their electricity demand and report them to the coordinator. The agents response is
computed in CalculateDemandProfileAndValuation (Algorithm 3).
The general algorithm first only runs the basic algorithm until convergence and subsequently
performs the additional phase, if necessary. In particular, the additional phase is performed, if the basic algorithm has converged (Algorithm 1 line 8) and in at least one time slot the aggregated demand
is not at the threshold (Algorithm 1 line 9). The algorithm reaches the optimal solution when R =
+

R0 and either j 6= hj , j or in the additional phase vlj
 vkj
, l, k  {1, ..., N }, j s.t. j = hj .
5.3 Convergence of the General Algorithm to an -optimal Solution
Please note that Theorem 1 still holds for the general algorithm, because the algorithm still reduces
the total cost in each iteration. Thus, the extended algorithm converges. In Theorem 3 we prove that
the solution of the general algorithm lies within an -neighborhood of the optimal solution.
Theorem 3. The converged solution R of the general algorithm lies within an  neighborhood of the
optimal solution R , where  is the amount for which the agents compute their marginal valuations.
Proof. We now prove by contradiction that when the general algorithm has converged to the solution R, then no other solution R0 exists with lower cost with respect to the central Problem 2
outside of an -neighborhood around R. Suppose there exists a solution R0 having a lower total energy cost than R, i.e., C(R0 ) < C(R). We will show that this contradicts the convergence
conditions of the general algorithm that i, ri is the solution of the agents individual problem,
+

ri = arg minxi Xi Cvi (xi |R), and that vlj
 vkj
, l, k  {1, ..., N } and j s.t. j = hj .
The only case left from Theorem 2 is that j, j = hj , which implies i, rij = hij (R),
and that
fi
fi
fi
0 > h (R) and k  K, r 0 < h (R), where l, k s.t. r 0  r fi 
L, K 6= , s.t. l  L, rlj
fi lj
lj
kj
lj fi
kj
fi
fi
fi 0
fi
, firkj
 rkj fi  . We will show that this case conflicts with the convergence conditions, too.
902

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

Algorithm 1: Overall algorithm.
Data: Scenario with electricity contract and agent definitions.
Result: Optimal demand schedule R .
1
2
3
4

5
6

7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

Initialization: All agents compute an initial energy demand profile ri by solving Problem 3
based on the market prices and send it to the coordinator.;
addP hase = f alse;
while demand schedule R is not optimal do
Coordinator calculates the price signals using Algorithm 2:
svij (rij |R) , i, j  CalculateVirtualPriceSignals (ri , vi+ , vi , addP hase);
The coordinator sends the virtual price signals to all agents;
Agents calculate demand profiles and valuations for all time slots, j, using Algorithm 3:
i: r0i , vi+ , vi  CalculateDemandProfileAndValuation (svij (rij |R), addP hase);
The agents send their new demand profiles and valuations back to the coordinator;
if algorithm has converged, i.e., R = R0 then
if j 6= hj , j then
the demand schedule R0 is optimal;
else
+

if addP hase and vlj
 vkj
, l, k  {1, ..., N }, j s.t. j = hj then
the demand schedule R0 is optimal;
else
start additional phase, i.e., addP hase = true;
end
end
else
R  R0 ;
end
end

fi
fi
fi
fi #
()  rlj fi =
We start with the case in which |L| = |K| = 1. Then we have   (0, 1), s.t. firlj
fi
fi
fi #
fi
firkj ()  rkj fi = . Moreover, denote R## () s.t.,
##
rit


() =

#
rit
() if i  L  K, t = j
rit
otherwise

a demand profile reflecting the changes in demand in time slot j so that in the resulting individ#
#
ual problems again all agents have their demand on the threshold, rlj
= hlj (R## ) and rkj
=
##
hkj (R ). Thus, with Lemma 2 we get that the sum of the virtual cost is equal to the central cost.
N
X







#
#
v
v
##
##
Cvi r#
()
|R
+
C
r
()
|R
()
+
C
r
()
|R
()
l
k
i
l
k

i=1,i6=l,k

=

C(R# ()) < C(R) =

N
X

Cvi (ri |R)

i=1

903

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

Algorithm 2: Calculation of the virtual price signal by the coordinator.
1

2
3
4

CalculateVirtualPriceSignals (ri , vi+ , vi , addP hase);
Data: demand profiles and valuations of all agents, ri vi+ , vi , i.
Result: virtual price signals for all agents, svij (rij |R) , i, j.
P
Compute the aggregated demand: j  N
i=1 rij ;
Compute demand to be shifted in each time slot: j  hj  j ;
 r
Divide that demand among all agents: ij  Pj rijij ;
i

5
6
7
8
9
10
11

if addP hase then
n
o
+

Find time slot j and agents l, k s.t. min vlj
 vkj
;
+

if vlj
+ vkj
< 0 then
Adapt demand to be shifted for agent l: lj   and agent k: kj  ;
end
end
Compute thresholds based on demands to be shifted: hij  rij + ij ;

Algorithm 3: Calculation of the individual demand profiles and marginal valuation of energy
for agent i.
1 CalculateDemandProfileAndValuation (sv
ij (rij |R), addP hase);
Data: virtual price signal svij (rij |R), j = 1, . . . , M .
Result: new demand profile r0i and valuations vi+ , vi .
2
3
4
5
6
7
8

Compute new demand profile r0i according to virtual price signal:
ri = arg minxi Xi Cvi (xi |R);
if addP hase then
foreach time slot j at the threshold, rij = hij do



+
ij+  Cv (r0 |R);
Compute valuation of increased threshold: vij
 Cvi rj+
|R
i i
i


ij  Cv (r0 |R);
Compute valuation of decreased threshold: vij
 Cvi rj
i i
i |R

end
end

which implies either
N
X



Cvi r#
i () |R <

i=1,i6=l,k



N
X

Cvi (ri |R)

i=1,i6=l,k



v
i, s.t. Cvi r#
i () |R < Ci (ri |R)

which conflicts with the convergence condition ri = arg minxi Xi Cvi (xi |R);
or







#
v
##
##
Cvl r#
()
|R
()
+
C
r
()
|R
()
< Cvl (rl |R) + Cvk (rk |R)
k
l
k


h 

i
##
##
Cvl r#
()  Cvl (rl |R) <  Cvk r#
()  Cvk (rk |R)
l () |R
k () |R

904

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

fi
fi




fi #
fi
## () is equal to Cv rj+ |Rij+
For firkj
()  rkj fi =  we get that Cvl r#
()
|R
and
l
l
l




## () is equal to Cv rj |Rij . Thus, with Equation 13 follows
Cvk r#
k
k () |R
k
+

vlj
(R) < vkj
(R)

+

which conflicts with the convergence condition, l, k  {1, 2,    , N }, vlj
(R)  vkj
(R).
For the general case of |L| , |K|  1, we can get
N
X




 X
 X

##
##
()
() +
Cvk r#
Cvl r#
Cvi r#
i () |R +
k () |R
l () |R

i=1,iLK
/

=

kK

lL

C(R# ()) < C(R) =

N
X

Cvi (ri |R)

i=1

which implies either
N
X
i=1,iLK
/



i, s.t. Cvi

N
X



Cvi r#
()
|R
<
i

Cvi (ri |R)

i=1,iLK
/



r#
i



() |R <

Cvi

(ri |R)

which again conflicts with the convergence condition ri = arg minxi Xi Cvi (xi |R);
or
X


 X

 X
X
#
v
##
##
Cvk (rk |R)
Cvl r#
()
|R
()
+
C
r
()
|R
()
<
Cvl (rl |R) +
k
l
k

lL



X
lL

kK

Cvl



lL

kK

"
#
 X

 X
X
##
##
r#
() 
Cvl (rl |R) < 
Cvk r#
() 
Cvk (rk |R)
l () |R
k () |R
lL

kK

kK

Since the amount of demand increased in time slot j is equal to the demand decreased, and the cost
decrease on the LHS is greater than the increase in cost on the RHS, there exists a pair of agents,
l, k such that the marginal cost decrease of l is larger than the marginal cost increase of k
+

 l  L, k  K, vlj
(R) < vkj
(R)

+

which conflicts with the convergence condition, l, k  {1, 2,    , N }, vlj
(R)  vkj
(R).

The solution R lies within an  neighborhood of the optimal solution R , where  is the amount
for which the agents compute their marginal valuations. When no beneficial shift of the size greater
or equal to  exists any more, the algorithm stops. However, a beneficial shift smaller than  might
still exist.
5.4 Convergence Time of the Algorithm
In this section, we give an upper bound on the number of steps the algorithm needs to converge.
First, we give a bound for the basic algorithm in Lemma 5 and the extended phase in Lemma 6.
Second, we give a bound for the general algorithm in Theorem 4.
905

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

Lemma 5. The basic algorithm converges within a maximum of d log   N e steps, where N is the
number of agents and  is the distance of the converged solution to the optimal solution of the basic
algorithm.
Proof. We prove this by showing an upper bound on the number of iterations needed in one time
slot, say, time slot j, to get from the initial demand level into the neighborhood of the optimal
demand level of the basic algorithm. This is sufficient, because when the demand in the last time
slot has reached its optimal level, the demand in all other time slots also reached their optimal level.
In this proof we will consider a time slot with its initial demand below the optimal demand. This
is sufficient, because every shift consists of a demand decrease in at least one time slot and also an
increase in at least one time slot. Thus, when all time slots that were below their optimal demand
level, reached their optimal level, the remaining time slots also reached their optimal levels.
Since all variables in this proof are for time slot j, we omit the subscript j for increased readability. Let Dt be the difference between the optimal aggregated demand and the aggregated demand
after iteration t. Since the demand is assumed to be below the optimal demand, Dt > 0. Without
loss of generality, assume the initial value of Dt is 1 (D0 = 1) and the optimum is 0; for each
agent i, its initial demand rij = 1. The worst case is that only one agent (e.g., agent 1) can increase
its demand. In this case, only agent 1 can increase its demand in each iteration, and its increment
equals the amount of demand shift assigned by the coordinator.
We denote at as the increment of agent 1s demand in iteration t and ct as agent 1s demand
after iteration t (c0 = 1). With N denoting the total number of agents, the demand shift assigned
 r
to agent i is given based on the definition of the price signal by ij = PNj ij . Then, we have for
i=1 rij

agent 1s increment

ct1 Dt1
.
(15)
N  1 + ct1
Agent 1s demand after iteration t is its initial demand plus its increments in the first t iterations.
Similarly, the difference Dt is the initial difference minus agent 1s increments.
at =

ct = 1 +

t
X

al , Dt = 1 

l=1

t
X

al  at = ct  ct1 = Dt + Dt1 , ct = 2  Dt .

l=1

By substituting the above into Equation 15, we have
(2  Dt1 ) Dt1
N + 1  Dt1
2
= 2Dt1  Dt1
(N  1) Dt1
=
N + 1  Dt1

Dt + Dt1 =
2
(N + 1) (Dt + Dt1 ) + Dt Dt1  Dt1

Dt
Since n  1, Dt  (0, 1), we get



(N  1) Dt1
N 1 n
Dt <
 Dt <
N
N


l
m
log 

Thus,  > 0, t = loglogN1 = log N
log(N 1) , s.t., t > t , we have Dt < . Moreover,
N

since logarithm is strictly increasing and concave, we have log N  log (N  1) >
 > 0, t = d log   N e, s.t., t > t , we have Dt < .
906

1
N.

Thus,

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

Lemma 6. The extended phase converges at maximum within a number of
0

l

(C0  C )
v

m

steps, where



C is the total cost in the solution from the basic algorithm, C is the total cost in the optimal
solution and v is the minimum value for the marginal valuations of an -increase and decrease of
the threshold in one time slot.
Proof. We will prove this by showing an upper bound on the number of iterations needed to reduce
the total cost from the solution of the basic algorithm, i.e., C0 , to the total cost of the optimal
solution, i.e., C . In each iteration of the extended phase, the thresholds are adapted for two agents,

+
say k, j. Thus, the total cost is reduced by vkj
+ vlj
(Equation 13). Let v be the minimum value

+
for vkj + vlj . Then, as long as the algorithm does not converge, after each iteration, the total cost is
l 0  m
reduced at least by v . Thus, the algorithm stops at most within (C vC ) steps.
Theorem 4. The time complexity of the general algorithm lies is O(N + (C0 -C )), i.e., the algorithm is linear in the number of agents and pseudopolynomial in (C 0  C  ).
Proof. From Lemma 5 we get the upper bound on the number of iterations of the basic algorithm
as dN log e. With  log  being a constant, which is independent of N , the time complexity of
the basic algorithm is O(N
l 0 ). From
m Lemma 6 we get the upper bound on the number of iterations of
(C  C )
the extended phase as
. The cost difference (C0  C ) does not depend on N or M , but
v
on the specific characteristics of the agents. The minimum valuation v is proportional to the size
of the threshold increment/decrement . Thus v is a constant for any constant choice of . Thus,
the bound of the extended phase is pseudopolynomial in (C0  C ).
5.5 Incentive Compatibility
In general, for allocation problems, there are three key properties of interest, namely, efficiency,
strategy-proofness, and budget-balance. In the context of our problem, efficiency implies that the
overall electricity consumption cost is minimized. Strategy-proofness implies that telling the truth
is the dominant strategy for the agents and hence they report truthfully. Budget-balance implies
that the total amount paid by the members of the cooperative is equal to the actual electricity consumption cost. There are two well known facts about these three properties in the literature: First,
if the allocation is efficient, the only way to implement payments that guarantee strategy-proofness
is to use VCG-type mechanisms, see the work of Green and Laffont (1977), Hurwicz (1975). Second, as Green and Laffont (1977) and Hurwicz (1975) also prove, payments obtained by VCG-type
mechanisms cannot achieve budget balance. Thus, it is impossible to design a mechanism that
achieves all the three properties, namely, efficiency, budget balance and strategy-proofness.
As we have stated in our discussion in the related work section, budget balance is a key requirement of our problem from the application point of view. Furthermore, allocation efficiency is a desired social goal. Our algorithm achieves both budget balance (Lemma 2) and allocation efficiency
(Theorem 3). Therefore, from the impossibility results, we cannot achieve strategy-proofness, i.e.,
truth telling cannot be a dominant strategy. However, we believe each agent may choose to truthfully respond, because agents cannot anticipate the future development of the algorithm and, as we
prove below, no manipulation strategy exists that dominates truth reporting. Therefore, we define a
notion of weak incentive compatibility.
Definition 2. An algorithm satisfies weak incentive compatibility or is weakly incentive compatible,
if and only if no strategy dominates telling the truth in the algorithm.
907

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

Theorem 5. The general algorithm is weakly incentive compatible.
Proof. In our algorithm the agents have two possible ways to deviate from truthful reporting: First,
they could report a demand profile that does not minimize their individual cost according to the
current virtual price signals with the intent that this misreporting would benefit them for the final
payment. Second, they could report marginal valuations that differ from their true marginal valuations. For our analysis we assume rational agents, i.e., agents always prefer lower cost.
Note that in the payment rule (Equation 8) each agent pays for its electricity demand based on
its final price signal. In each iteration, a truthful demand report minimizes an agents cost given
the price signal. Therefore, if an agent does not report truthfully at the final price signal (i.e., final
iteration) it will incur a higher cost. Furthermore, agents do not know whether any given price
signal is the final price signal or not. This is, because agents have limited knowledge and do not
know the demand profiles of other agents or their constraints and preferences. Thus, manipulation
at any iteration may result in higher cost than a truthful report. Therefore, no manipulation strategy
involving misreported demand profiles dominates truthful reporting.
Regarding the reporting of the marginal valuations (Definition 1), it is obvious that deflating its
valuation hurts the agent. We now demonstrate with an extension of the example from Lemma 3,
that deviating from truth revelation through inflating the reported marginal valuation can also hurt
an agent. The aggregated demand is at the threshold in time slot 2 so that the coordinator asks for
the agents valuations for an -increase of the threshold. Without loss of generality assume  = 1.
+
L
= pH
Agent 1s true valuation is (reduce in time slot 3 and increase in 2) v12
3  p2 = 2 and agent 2s
+
L
is (reduce in time slot 1 and increase in 2) v22 = pH
1  p2 = 1. Lets assume agent 2 misreports its
+
valuation so that v22 > 2. Then, the coordinator increases the threshold in time slot 2 for agent 2
and decreases the threshold for agent 1. It follows that agent 1 has to shift demand from time slot 2
to time slot 3 and that agent 2 can now shift demand from time slot 1 to time slot 2. This leads
to the demand profiles: rdeviate
= (4, 4, 9), rdeviate
= (3, 6, 8). However, this profile does not
1
2
only lead to an increase in total cost, but also to an increase of the individual cost for the deviating
agent 2. Table 1 shows the individual cost.4 Since misreporting the valuations may result in higher
electricity cost than a truthful report for the misreporting agent, no manipulation strategy involving
misreported marginal valuations dominates truthful reporting.
cost
truthful reports
agent 2 deviates

agent 1s cost
37.143
40.118

agent 2s cost
38,857
38.882

total cost
76
79

Table 1: Cost of different demand profiles for individual agents.

6. Simulation
We perform simulations based on real world consumption data to (a) characterize the convergence
properties of our algorithm and (b) understand the effect of different parameters that characterize
the electricity demand profile on the cost reduction through coordination by our algorithm. The
results show that the algorithm scales linearly with the number of agents and time slots. Further,
we observe that as the participants flexibility of shifting their demands increases, cost reduction
4. Since Agent 2s deviation increases both agent 1s as well as agent 2s cost, one might argue that increasing the other
agents cost could also be an incentive for deviation. However, note that the cost of an agent is not visible to others.
Therefore, we believe that each agent may choose to report truthfully.

908

fielectricity price (Euro cents/kWh)

M ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

dayahead
spot market
price

6

4

2

0

2

4

6

8

10

12

14

16

18

20

22

24

time slot

Figure 6: Day-ahead spot market prices from EPEX Spot for an average Tuesday in 2013
increases as well and that the cost reduction is not very sensitive to whether the consumers use more
electricity during the evening or during the day.
6.1 Datasets
CER Electricity Consumption Data: We use real electricity consumption data to generate consumer demand profiles. The data was gathered by the Irish Commission for Energy Regulation
(CER) in the context of a smart metering study. In this study, the electricity consumption data of
485 small and medium enterprises and 4, 225 private households was collected over a period of
about 1.5 years. The observation period is divided into time intervals of 30 minutes. One data
point represents the average electricity consumption in kilowatts of one participant during a 30
minute interval. In particular, we used the data from 46 enterprises. The participating enterprises
also answered questionnaires about, among others, their number of employees and typical hours of
operation. We will refer to this data set as the CER data set5 .
EEX Electricity Prices Market Data: We generate the cooperatives electricity prices using real
day-ahead market electricity prices, gathered from the European Energy Exchange (EEX), which
is the leading energy exchange in Europe. In particular, we use the average hourly day-ahead
prices from the EPEX Spot market from the 20 Tuesdays from January 1st 2013 to May 14th 2013.
Figure 6 shows the average day-ahead spot market prices of the observation period. We will refer
to the hourly market price data as the EEX data set6 .
6.2 Simulation Parameters
As of the model in the work of Mohsenian-Rad et al. (2010), weP
define each agent by its predetermined total electricity demand over the planning horizon, i = M
r , and by upper and lower
h j=1 iji
bounds on the electricity demand in each time slot, i.e., rij  rij , rij . Regarding the individual cost functions gi , we assume gi = 0, because we do not have data on industry specific cost
functions.
5. The data set is available at http://www.ucd.ie/issda/data/commissionforenergyregulation/.
6. The data is available at http://www.eex.com/en/Market%20Data.

909

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

6.2.1 M ODELING THE C OMPOSITION OF C OOPERATIVES :
Agents can have a variety of different demand characteristics. To reflect this diversity, we use
the CER data set to identify two classes of consumers with similar characteristics. To group
consumers with similar characteristics, we clustered the participants of the study using k-means
clustering based on the information from the questionnaires and the electricity consumption data.
In particular, we used the consumption profiles of Tuesdays, because Tuesday is the most average
work day. Here, Class 1 represents consumers consuming most of their electricity during the day,
whereas Class 2 represents consumers having a stable consumption during the day, but have a higher
consumption at night. Based on these classes, cooperatives with varying compositions can be simulated. The parameter p fracAgent defines the composition of the cooperative by specifying the
fraction of agents from Class 1 (and Class 2 by 1  p f racAgent).
6.2.2 G ENERATING THE N OMINAL D EMAND OF AGENTS :
An agents nominal demand is the value around which the agent can vary its demand. We simulate
scenarios with M  {12, 24, 48} time slots and use time intervals of 30, 60 and 120 minutes, which
we generate by interpolating and averaging the data points from the two data sets. Then, j = 1
represents the first and j = M the last time slot of the day. To get different agents, the nominal
demand for each agent and time slot is the outcome xij of the random variable XCj . Let y Cj be

the mean of the average demands of all participants in class C in time slot j and let s y Cj be the
corrected sample standard deviationof their average
 demands. Thenominal demand is drawn from
the uniform distribution XCj  U y Cj  s y Cj , y Cj + s y Cj . The distributions of XCj for
the two Classes
are shown in Figure 7. The total demand of an agent for the whole day, is computed
P
x
by i = M
j=1 ij .
6.2.3 M ODELING THE F LEXIBILITY OF C ONSUMERS :
Agents can have different flexibilities of changing their demand profiles. Here, this ability is expressed through the agents upper and lower bound constraints on their electricity demand in each
time slot. The parameter p flexShift defines the flexibility of the agents by specifying the percentage, by which agents can vary their demands above and below their nominal demand. Thus, the
larger p f lexShif t, the further apart are the upper and lower bounds. The parameter is the same
for all agents and fixed for one scenario. Based on the flexibility and the nominal demand, the upper
and lower bounds can be computed as
rij = xij (1  p f lexShif t)
rij = xij (1 + p f lexShif t)

(16)

6.2.4 M ODELING THE E LECTRICITY P RICE :
The cooperatives electricity price function in each time slot is defined by the marginal price for the
H
low load, pL
j , and for the high load, pj , and the price threshold, hj , specifying at which demand
levels the marginal prices apply. We use the prices from the EEX data set as exemplar prices to
H
generate pL
j and pj . Let mpj be the average spot market price from EPEX Spot in time slot j. We
910

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

14

14
exemplary
nominal
demand

10

12

electricity demand (kW)

electricity demand (kW)

12

distribution
of nominal
demand

8

6

4

10

2

0

8

6

4

2

0

2

4

6

8

10

12

14

16

18

20

22

0

24

0

2

4

6

time slot

8

10

12

14

16

18

20

22

24

time slot

(a) Class 1 - day consumer

(b) Class 2 - night consumer

Figure 7: Distributions of the nominal demand for the two different consumer classes.
compute the marginal electricity prices as
pL
j = mpj
pH
j = mpj +

max

x{1,...,M }

{mpx } 

min

x{1,...,M }

(17)

{mpx }

We keep these marginal prices fixed across all simulations. However, the price thresholds may
vary across the different simulation scenarios. For example, the price threshold could be above
the aggregated nominal demand so that the demand can be increased or it could be below so that
the demand should be reduced. The parameter p distThresh defines the distance between the
aggregated nominal demand and the price thresholds by specifying the percentage the thresholds
are above or below the aggregated nominal demand. With a negative value of p distT hresh, the
thresholds lie below and with a positive value they lie above the aggregated nominal demand. For
each scenario there is only one fixed value for all time slots. Figure 8a illustrates scenarios that have
different values of p distT hresh. In addition, the price thresholds could either be different in each
time slot (e.g., following the profile of the aggregated nominal demand) or could be the same for
each time slot (flat thresholds). Here, we compute flat thresholds by defining them as the moving
average of the aggregated nominal demand. The parameter p flatThresh defines the flatness of
the threshold by specifying the width of the interval over which the thresholds are flattened by a
moving average. For p f latT hresh = 0 the thresholds follow exactly the aggregated nominal
demand, whereas for p f latT hresh = M the thresholds are the same in every time slot. Figure 8b
illustrates scenarios with different values p f latT hresh. The thresholds are computed by
1
hj =
1 + 2p f latT hresh

j+p f latT
X hresh
jp f latT hresh

!
(1 + p distT hresh)

X

xij

(18)

i

6.2.5 S IMULATION S CENARIOS :
To create different scenarios, we vary the input parameters as follows: p f racAgent  {0, 0.25, 0.5,
0.75, 1}, p f lexShif t  {0.1, 0.2, 0.3}, p f latT hresh  {0, 12, 24}, p distT hresh  {0.2,
911

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

500

500
values for
p_distThresh
0.2

400

0
0.1

300

0.2

200

100

0

0

400

0.1

aggregated demand (kW)

aggregated demand (kW)

values for
p_flatThresh

6
24

300

200

100

0

2

4

6

8

10

12

14

16

18

20

22

0

24

time slot

0

2

4

6

8

10

12

14

16

18

20

22

24

time slot

(a) Price thresholds for different values of p distT hresh. (b) Price thresholds for different values of p f latT hresh.

Figure 8: Price thresholds for different values of the parameters p distT hresh and p f latT hresh.

0.1, 0, 0.1, 0.2}, #agents  {20, 40, 60, 80, 100}, M  {12, 24, 48} and   {0.5, 1, 2}. This
leads to 3375 different scenarios. We define the algorithm to be converged, when the cost reduction
in one iteration gets less than 0.00001%, i.e., C (R) / C (R0 ) < 1.0000001.
6.3 Simulation Results
We first present the effects of the different parameters on the cost reduction through the algorithm
and subsequently discuss the convergence time.
Definition 3. The cost reduction is the difference between the cost of the uncoordinated profile C0
(chosen by each agent when optimizing its cost according to the market price) and the
coordinated

profile C as a percentage of the cost of the uncoordinated profile, i.e., CR = C0C0C  100.
Figure 9 shows a subset of the results for p f racAgent  {0, 0.5, 1}, M = 24, #agents = 40
and  = 1. Figure 9a shows the cost reduction in cooperatives only consisting of consumers mainly
consuming electricity at night. Figure 9b shows the cost reductions in cooperatives that consist of
both consumer classes in equal proportions. Finally, Figure 9c shows the cost reductions in cooperatives consisting of only consumers with their main demand during the day. The x-axis represents
the flexibility of shifting demand, the y-axis the flatness of the thresholds and the z-axis the height
of the thresholds. In all figures are scenarios with high cost reduction (white) and scenarios with
less cost reduction (dark red). Over all scenarios, we observed a mean cost reduction of 2.57%,
with results varying from 0% to 7.44%. However, the sampled cases represent all combinations
of the input parameters and we cannot say how realistic individual settings are. For example, in
many scenarios no optimization is possible and we put in a cost reduction of 0%. No optimization
is possible, if the price threshold provided by the supplier is higher than the sum of the agents upper
bound constraints or if the price threshold is lower than the sum of the lower bound constraints. Not
taking into account those scenarios, we observe a mean of 2.96%. Moreover, looking at the cost
reduction from the initial nominal demand profile, we get a mean of 3.7%, with results up to 11.1%.
912

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

0.2

0.07

0.07

0.06

0.06

0.2

0.05

0.05

0.03
0.1

0.1
0.04

0
0.03
0.1

0.3

p_flexShift

0

12

0.01
0.1

24
0

p_flatThresh

0.02
0.2

0.01
0.2

0.03

0.02
0.2

0.1

0.04
0

0.1

0.02
0.2

p_distThresh

0.04

p_distThresh

0.1

0

0.06

0.2

0.05

0.1

p_distThresh

0.07

0.2
0.3

0

12

0

p_flatThresh

p_flexShift

0.01
0.1

24

0.2
0.3

p_flexShift

0

12

24
0

p_flatThresh

(a) Only consumers who mainly con-(b) Consumers from both classes have(c) Only consumers who mainly consume at night, p f racAgent = 0.
equal fractions, p f racAgent = 0.5 sume during day, p f racAgent = 1.

Figure 9: The cost reduction through the algorithm as a function of the four input parameters:
p f racAgent, p f latT hresh, p distT hresh and p f lexShif t. Scenarios with high cost reduction are white and scenarios with less cost reduction are dark red.
6.3.1 S ENSITIVITY A NALYSIS
The purpose of this sensitivity analysis is to understand the effect of the different input parameters
on the cost reduction through our algorithm. For the sensitivity analysis we perform a multiple
linear regression. As dependent variable (criterion) we choose the cost reduction (Definition 3). As
independent variables (predictors) we choose the smallest interpretable model that provides a good
adjusted R-squared. The resulting model is shown in Table 2. The model explains 76.36% of the
variance with an adjusted R-squared of 0.7571. The remaining variance cannot be explained by the
model, because the agents demand constraints are generated randomly.
predictors
p f racAgent
p f latT hresh
p f lexShif t
|p distT hresh|
p distT hresh  p f lexShif t
p f racAgent  p f latT hresh

coefficient
0.0019
0.0077
0.0114
-0.0072
0.0018
-0.0016

stand. error
0.0011
0.0017
0.0010
0.0008
0.0002
0.0005

p-value
0.0910
1.66E-05
1.35E-25
2.08E-16
6.67E-17
0.0035

Table 2: Multiple linear regression model; adjusted R2 = 0.7571
Result 1: As agents flexibility of shifting their demands increases, cost reduction increases, i.e.,
p f lexShif t %  G %.
The multiple linear regression model summarized in Table 2 shows that the effect of p f lexShif t
is highly significant with p < 0.01. The positive value of the coefficient (0.0114) shows that an
increase of the flexibility leads to increased cost reduction.
Intuitively, with higher flexibility, more demand from time slots with high prices can be shifted
to those with lower prices. Consequently, higher cost reduction can be achieved. For example, if
p f lexShif t = 0, no demand can be shifted and no cost reduced. However, another reason for the
high cost reduction is that the uncoordinated profile before the algorithm has higher cost. Recall
that for the uncoordinated demand profile the agents optimize their demand according to the hourly
913

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

550

550

500

500

450

aggregated
nominal demand

400

aggregated
optimal demand

aggregated demand (kW)

aggregated demand (kW)

price thresholds

350
300
250
200
150
100

450
400
350
300
250
200
150

0

2

4

6

8

10

12

14

16

18

20

22

100

24

time slot

0

2

4

6

8

10

12

14

16

18

20

22

24

time slot

(a) Lower flexibility, p f lexShif t = 0.1.

(b) Higher flexibility, p f lexShif t = 0.3.

Figure 10: Example scenarios with different flexibilities for the agents of shifting their demands.

market prices. Consequently, the higher the agents flexibility, the more they adapt their demand
schedules to the hourly prices. However, since not being coordinated, all agents shift as much
demand as possible to cheap time slots. This leads to load synchronization, so that the total demand
in the former cheap slots exceeds the thresholds, leading to higher costs (herding behavior). Thus,
high flexibility of uncoordinated consumers leads to highly synchronized demand. These high initial
costs allow for more cost reduction through the algorithm. Figure 10 illustrates settings with low
(p f lexShif t = 0.1) and high (p f lexShif t = 0.3) flexibility. The figure shows clearly that the
initial demand peak is higher in the scenario with more flexibility. Recall from Figure 6 that slots
14, 15, 16 and 17 are relatively low priced slots with 17 being the lowest. The area between the
red dashed curve of the uncoordinated demand profile and the coordinated demand profile in gray
multiplied by the respective marginal prices shows the cost reduction through coordination. With
the increasing freedom in Figure 10b it can be seen that the area between the two curves increases.
The reason the height of the peak stays the same, is that at first the agents shift as much demand
as possible from time slots with aggregated demands above the thresholds (peaks) to time slots
with demand levels below the thresholds. When all agents reach their upper bounds in those time
slots, they shift the remaining demand to the time slots with the lowest prices above the threshold.
According to the hourly prices, the cheapest remaining time slot is 17. Since the freedom allows
demanding most of the remaining electricity in that time slot, the height of the peak does not change.
Result 2: As the absolute distance of the demand thresholds to the aggregated load profile
decreases, cost reduction increases, i.e., |p distT hresh| &  G %.
The multiple linear regression model summarized in Table 2 shows that the effect of |p distT hresh|
is highly significant with p < 0.01. The negative coefficient (-0.0072) shows that the further away
the thresholds are from the aggregated load profile the less cost reduction can be achieved.
Figure 11 illustrates settings with low (p distT hresh = 0.2), normal (p distT hresh = 0)
and high (p distT hresh = 0.2) thresholds. The cost reduction is limited by how much the agents
can decrease demand above and increase below the thresholds. The demand above the thresholds
is represented by the area between the initial demand profile (red dashed line) and the thresholds
(yellow flat line) above the thresholds, and the unused demand below the thresholds by the area
between the two lines below the thresholds. These amounts are also affected by the flatness of
914

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

550

550

500

500

500

400

aggregated
nominal demand

350

aggregated
optimal demand

aggregated demand (kW)

aggregated demand (kW)

price thresholds

450

450

450

400

400

350

300

350

300

250

300

250

200

250

200

150
100

aggregated demand (kW)

550

200

150
0

2

4

6

8

10

12

14

time slot

16

18

20

22

24

100

150
0

2

4

6

8

10

12

14

time slot

16

18

20

22

24

100

0

2

4

6

8

10

12

14

16

18

20

22

24

time slot

(a) Thresholds below the average load,(b) Thresholds at the average load,(c) Thresholds above the average load,
p distT hresh = 0.2.
p distT hresh = 0.
p distT hresh = 0.2.

Figure 11: Example scenarios with different heights of the price thresholds.

the thresholds, i.e., the flatter the thresholds, the larger the two areas become. Thus, with flatter
thresholds the cost reduction increases as well. The regression model supports this, because the
parameter p f latT hresh has a positive coefficient (0.0077) and is highly significant with p < 0.01.
However, the cost reduction in the scenario with high thresholds in Figure 11c is greater than
in the case of normal thresholds in Figure 11b. That happens, because the flexibility of shifting
demand is a limiting factor. Since the absolute distance between the upper and lower bounds is
smaller in time slots with low demand, the demand hits the upper bounds in those time slots before
it reaches the lower bounds in the time slots with high demand. In this situation, an increment
of the thresholds leads to more time slots below the thresholds and thus more possibilities to shift
demand. Consequently, if the flexibility is a limiting factor, the cost reduction increases, if the
thresholds are slightly increased. This is supported by the regression model, because the interaction
term p distT hresh  p f lexShif t is highly significant with p < 0.01.
Result 3: We observe similar cost reductions in groups with mostly consumers from one class
and groups with agents from both classes in similar fractions.
The multiple linear regression model in Table 2 shows that the parameter p f racAgent is not a
significant predictor for the cost reduction with p > 0.05. This supports the observation that we can
see cost reductions from the coordinated behavior in each of the two classes (day consumers and
night consumers) as well as mixed groups. Figure 12 illustrates settings with only night consumers
(Figure 12a), a mixed group (Figure 12b), and only day consumers (Figure 12c). However, in these
examples, with an increasing fraction of day consumers, the cost reduction decreases to a larger
extent than indicated by the parameter p f racAgent in the regression model. This effect is due to
an interaction effect between the composition of the cooperative and the flatness of the thresholds.
The multiple linear regression model shows that the interaction term p f racAgent  p f latT hresh
is a significant predictor with p < 0.01. The negative value of the coefficient (-5.219e-04) shows
that as the flatness of the thresholds increases and the fraction of day consumers also increases,
cost reduction decreases. In Figure 12 it can be seen that the thresholds are lying outside of the
agents demand bounds in many time slots, because the thresholds are the same in each time slot. In
this situation not the thresholds, but the demand bounds limit the potential cost reduction through
coordination. Consequently, as the thresholds get flatter, they get out of reach of the agents demand
bounds. Since the load profiles of day consumers have a more significant difference between peak
and low load, this effect is stronger for groups with a larger fraction of day consumers.
915

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

650

600

600

price thresholds

500

aggregated
nominal demand

aggregated demand (kW)

550

450

550

500

450

450

400

350

400

350

300

350

300

250

300

250

200

250

200

150
100

550

500

aggregated
optimal demand

400

aggregated demand (kW)

650

600

aggregated demand (kW)

650

200

150
0

2

4

6

8

10

12

14

time slot

16

18

20

22

24

100

150
0

2

4

6

8

10

12

14

16

18

20

time slot

22

24

100

0

2

4

6

8

10

12

14

16

18

20

22

24

time slot

(a) Only consumers who mainly con-(b) Consumers from both classes have(c) Only consumers mainly consuming
sume at night, p f racAgent = 0.
equal fractions, p f racAgent = 0.5. during the day, p f racAgent = 1.

Figure 12: Example scenarios with different compositions of the consumer cooperative.
6.3.2 C ONVERGENCE P ROPERTIES OF THE A LGORITHM
Let the first iteration be the submission of the agents uncoordinated initial demand profiles. Then,
in the second iteration the coordinator sends the first virtual price signals to the agents. Recall
that in the beginning the basic algorithm is performed and after the basic algorithm converged, the
additional phase is started, if necessary. Also recall that we defined the algorithm to have converged,
when the cost reduction in one iteration gets less than 0.00001%, i.e., C (R) / C (R0 ) < 1.0000001.
In our simulations, the basic algorithm converges in average within 9.57 iterations. In the cases that
need the additional phase, the general algorithm converges in average in additional 35.10 iterations.
The experiment was setup using one HP Pavilion dmt4-1000 with an Intel Core i5 520M with
two cores operating at 2.4GHz and 8 Gigabyte of DDR3 physical memory. Using this setup, one
iteration of the basic algorithm took on average 25.12 seconds and one iteration of the extended
phase took on average 52.13 seconds. Looking at the agents demand profiles at convergence, we
observe that for each agent in average in 67.6% of the time slots the demand bounds are tight. In all
sampled scenarios, there is at least one time slot, where the demand bound is not tight, because the
agents have to satisfy their constant total demand over the whole planning horizon.
Definition 4. The convergence accuracy is the difference between the cost of the converged solution
and the optimal solution as percentage of the difference between the cost of the uncoordinated profile
T )C(R )
and the optimal solution, i.e., C(R
 100.
C(R1 )C(R )
Result 4: In our simulations, the basic algorithm achieves the optimal solution in 44.9% of all
sampled scenarios.
We found that in 44.9% of all sampled cases the basic algorithm achieves the optimal solution. In
the remaining scenarios the basic algorithm comes in average 1.06% close to the optimal solution.
This indicates that in most cases the basic algorithm can provide good results.
Result 5: Our simulations indicate that larger step sizes, , lead to faster convergence at the
cost of small reductions in accuracy.
The parameter  defines the step size of the extended phase. Intuitively, larger steps lead to a faster
convergence, at a cost of reduced accuracy. Table 3 shows the convergence accuracy and the average
number of iterations until convergence for varying   {0.5, 1, 2}. The number of agents and time
slots are fixed as N = 40 and M = 24. The results indicate that as  increases, the number of
iterations until convergence as well as the accuracy decreases. However, the reduction in accuracy
is very small and the algorithm achieves good results for all . Our results indicate that in order
916

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

to achieve accurate solutions within few iterations, the step size should be adapted at run-time. In
particular, the value of  should be reduced over time.


Convergence accuracy

0.5
1
2

0.22%
0.33%
0.55%

Number of iterations
until convergence
36.5
26.1
17.9

Table 3: Convergence accuracy and time for varying step sizes
Result 6: Our simulations indicate that the convergence time scales linearly with the agent
population size and number of time slots.
To analyze the influence on the convergence time of the algorithm, we vary the number of agents
from 20 to 100 and time slots from 12 to 48. Table 4a shows the convergence accuracy and Table 4b
the number of iterations until convergence. The step size is fixed at  = 1. The results indicate that
the number of iterations until convergence increases linearly with the number of agents and time
slots. The accuracy does not change much, as we increase the agent population and time slots.
Ptime
P slots
agents PPPP

PP

20
40
60
80
100

12

24

48

0.21%
0.24%
0.23%
0.26%
0.25%

0.19%
0.29%
0.26%
0.26%
0.28%

0.18%
0.20%
0.28%
0.30%
0.38%

(a) Convergence accuracy

Ptime
P slots
agents PPPP
20
40
60
80
100

PP

12

24

48

9.8
12.4
13.5
15.2
16.9

16
20.1
23.7
27.1
28.2

22.4
30.2
35.4
40.3
43.2

(b) Number of iterations until convergece

Table 4: Convergence accuracy and time for varying number of agents and time slots

7. Conclusion and Future Work
In this paper, we presented an iterative coordination algorithm to minimize the energy cost of a
consumer cooperative, given that information about the agents individual demand constraints and
preferences remains private. We proved that the algorithm converges to the optimal demand schedule and presented results on the time complexity of the iterative algorithm and the agents incentive
compatibility. Additionally, we conducted evaluations on the algorithm using multiagent simulation based on real world consumption data. Through simulations, we characterized the convergence
properties of our algorithm and the effect of differing demand characteristics in the cooperative and
price functions on the cost reduction through our algorithm. The results show that the convergence
time scales linearly with the population size and length of the optimization horizon. Finally, we observe that as participants flexibility of shifting their demands increases, cost reduction increases and
that the cost reduction is not very sensitive to variation in consumption patterns of the consumers.
This work can be extended in several directions. Future work can investigate settings in which
the agents might not be able to compute a guaranteed optimal solution of their individual problem,
but only a provably good approximation. This could apply to settings with more detailed load
917

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

models for the agents. The overall demand can come from two types of loads: shiftable loads
and non-shiftable loads. These loads can be divided further into interruptible and non-interruptible
loads. In addition, these loads can be subject to temporal constraints. This can lead to the situation
where the individual problems for the agents are no longer convex, and thus no agent can solve its
individual problem optimally. Luo, Chakraborty, and Sycara (2013) present a distributed iterative
algorithm for the generalized task assignment problem in the context of a multirobot system (MRGAP). Based on the (approximate) best responses from the agents, this algorithm has a provable
approximate ratio. It would be interesting to investigate such a distributed algorithm in the context
of this problem.
In this paper, the demand of the cooperative in each time slot solely consists of the aggregated
demand of the agents. Future work can consider problems with generation and/ or storage (that can
be centralized, i.e., owned by the cooperative, or distributed, i.e., owned by an individual agent).
Another avenue of future work is to consider a problem formulation where the cooperative faces
uncertainty in electricity prices. For example, consider a 24-hour planning horizon and instead of
a long term contract the electricity is bought from an hourly spot market. Here, for scheduling
demand, one only knows the price for the next hour and the prices for the future hourly time slots
are uncertain. The spot market electricity price depends on many factors that are not controlled
by the coordinator. Hence, for planning purposes, the prices can be assumed to be an externally
specified stochastic process. Under this assumption, the goal would be to design algorithms for (1)
determining policies (for generation, storage, and price signals to be sent to the firms) for the central
coordinator and (2) determine the schedules for the individual firms, such that the expected cost of
buying electricity is minimized.

Acknowledgements
This work was partially supported by NSF award IIS-1218542.

Appendix A. Proof: Virtual Cost is an Upper Bound on the Total Cost
Proof. Here, we give the full proof showing that the sum of the agents individual cost according to
their virtual price signals is an upper bound on the total central cost at market prices. We prove this,
by showing with Equations 1 and 7 that for every time slot j the difference between the total cost
for the aggregated demand and the sum of the agents individual cost is lower or equal to 0. Since it
is true for every time slot, it is also true for the sum over all time slots.
The central cost for the aggregated demand is given in Equation 1 as
N
X

 0
pj 0j rij
, j

i=1

+

0
= pH
0j  hj + pL
+ pL
j
j j  hj
j hj
(

H
0
L
0
pj j  hj + pj hj j > hj

=
0
L
0j  hj
pL
j j  hj + pj hj
(P
 PN

N
0
pH
rij
 hij + i=1 pL
j
j hij

P
= Pi=1
N
N
L
0
+ i=1 pL
j hij
i=1 pj rij  hij
918

0j > hj
0j  hj

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

The sum of the agents individual cost is given in Equation 7 as
N
X

 0
0
sij rij
rij , j

i=1

=

N h
X

0
pH
rij
 hij
j

+

0
+ pL
j rij  hij



+ pL
j hij

i

i=1

For the difference of the two costs we get:
N
X

N
X
 0
 0
0
pj 0j rij

sij rij
rij , j

i=1

i=1
P
i
h

+

N
L
0
L
0
H
L
0
H

h

h

p
r

h

p
r
h

p

h
+
p
r
p
ij
ij
ij
j ij
ij
j
ij
j
j ij
ij
j
i=1
i
= PN h

+

L
0
L
H
0
L
0
L

rij  hij  pj rij  hij  pj hij
i=1 pj rij  hij + pj hij  pj

Since

PN 
i=1

0j  hj


PN  L 
pL
h
and

ij
j
i=1 pj hij equal each other out they and can be removed.

P
h


 i
 N pH r0  hij  pH r0  hij +  pL r0  hij 
j
ij
j
ij
j
ij
i=1
= PN h

+
 i
L
0
H
0
L
0

p
r

h

p
r

h

p
r
ij
ij
j
ij
j
ij
j
ij  hij
i=1

Lets now write
P
analog as N
i: r0

0j > hj
0j  hj



+


PN L  0
PN
H r0  h
H r0  h
p
r

h
p
p
and
as
0
ij
ij
ij
ij
ij
ij
i=1 j
i=1 j
i: rij >hij j


PN
PN
PN
H
0
rij  hij . Lets also write i=1 as i: r0 >hij + i: r0 hij .
hij pj

PN

ij

=

0j > hj

ij

ij

 N
N


 H 0


P  H 0
P

H
0
L
0


i: r0 >h pj rij  hij  pj rij  hij + i: r0 h pj rij  hij  pj rij  hij
ij






i:

ij

N
P
0 >h
rij
ij

ij

i:

For 0j > hj the terms

PN
i:

0 h
rij
pH
ij
j

=

i: r 0 h
PN ij ij
0 >h
i: rij
ij



0
L
0
pL
j rij  hij  pj rij  hij



other out and can be removed. For 0j  hj remove
(PN



0 h
rij
ij



0 >h
rij
ij

ij

N
P

 L 0


0
pj rij  hij  pH
rij
 hij +
j

0
i: rij

PN

0 >h
rij
ij

 H 0


0
pj rij  hij  pL
j rij  hij


 L 0
0
pj rij  hij  pH
rij
 hij
j

0j > hj
0j  hj

0j > hj


H
0
pL
rij
 hij
0j  hj
j  pj
(P
 0

N
H
L
0j > hj
i=1 pj  pj  rij  hij 
= PN
+
L
H
0
rij
 hij
0j  hj
i=1 pj  pj
h P
i



N
H
L
0

rij
 hij
 0 0j > hj
i=1 pj  pj
h
i
=


P
+
N
L
H
0

rij
 hij
 0 0j  hj
i=1 pj  pj
=

i: r 0 h
PN ij ij
0 >h
i: rij
ij

L
pH
j  pj



919

0j  hj



H r0  h
p
ij equal each
ij
j
i:


L
0
hij pj rij  hij respectively.

and 
PN





L and pL  pH out.
Factor pH

p
j
j
j
j
(PN

0j > hj

0
rij
 hij



fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

Since this inequality is true for every time slot, it also holds for the sum over all time slots. It follows
that the sum of the agents individual cost according to their virtual price signals is an upper bound
P
0
on the total central cost at market prices, C (R0 )  N
i=1 Ci (ri ):
N X
M
X

N X
M
X
 0
 0
0
pj 0j rij
+ gi (r0i ) 
sij rij
rij + gi (r0i )

i=1 j=1

i=1 j=1

Appendix B. Proof by Counterexample: Basic Algorithm Can Converge to a
Suboptimal Solution in Case of gi () 6= 0
We present a counterexample to prove that the basic algorithm can converge to a suboptimal solution
in general settings with gi () 6= 0. Consider a population of 2 agents, N = 2, and a planning
horizon of 2 time slots, M = 2. The agents constraints are such that in the converged solution, the
aggregated demand is equal to the threshold in one time slot. Let the price function be given as:
H
L H
(pL
1 , p1 ) = (3, 8), h1 = 9 ; (p2 , p2 ) = (3, 8), h2 = 11

The individual constraints on the agents demand are:
r11  [1, 3], r12  [4, 6], r11 + r12 = 1 = 7
r21  [4, 6], r22  [4, 6], r21 + r22 = 2 = 10

The agents individual cost associated with the demand schedule is given as:

g1 (r1 ) = r1

5
1




, g2 (r2 ) = r2

6
3



(1)

At the beginning, the agents compute their initial demand profiles based on the market prices r1 =

(1)
(1, 6), r2 = (4, 6). The cost based on these demand profiles is C R(1) = 109.
(T )

(T )

At convergence, the final profiles of the twoagents are r1 = (1.5, 5.5), r2 = (4.5, 5.5). The
cost based on these demand profiles is C R(T ) = 107.5.
However a different demand profile of the agents exists with r01 = (1, 6), r02 = (5, 5). This
profile is feasible and leads to lower total cost, i.e., C (R0 ) = 107. It follows that the algorithm has
stopped in a suboptimal solution.

References
Albadi, M., & El-Saadany, E. (2007). Demand response in electricity markets: An overview. In
Power Engineering Society General Meeting, 2007. IEEE, pp. 15. IEEE.
Atzeni, I., Ordonez, L., Scutari, G., Palomar, D., & Fonollosa, J. (2013). Demand-side management
via distributed energy generation and storage optimization. IEEE Transactions on Smart Grid,
4(2), 866876.
Ausubel, L. M., & Milgrom, P. (2006). The lovely but lonely vickrey auction. Combinatorial
auctions, 1740.
Barbose, G., Goldman, C., & Neenan, B. (2004). A survey of utility experience with real time
pricing. Tech. rep., Ernest Orlando Lawrence Berkeley National Laboratory, Berkeley, CA,
USA.
920

fiM ULTIAGENT C OORDINATION FOR D EMAND S IDE E NERGY M ANAGEMENT

Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and distributed computation: numerical methods. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.
Chu, C., & Jong, T. (2008). A novel direct air-conditioning load control method. IEEE Transactions
on Power Systems, 23(3), 13561363.
Clement-Nyns, K., Haesen, E., & Driesen, J. (2010). The impact of charging plug-in hybrid electric
vehicles on a residential distribution grid. IEEE Transactions on Power Systems, 25(1), 371
380.
Dietrich, K., Latorre, J., Olmos, L., & Ramos, A. (2012). Demand response in an isolated system
with high wind integration. IEEE Transactions on Power Systems, 27(1), 2029.
Green, J., & Laffont, J.-J. (1977). Characterization of satisfactory mechanisms for the revelation of
preferences for public goods. Econometrica: Journal of the Econometric Society, 427438.
Hurwicz, L. (1975). On the existence of allocation systems whose manipulative nash equilibria are
pareto-optimal. In 3rd World Congress of the Econometric Society.
Jellings, C. W., & Chamberlin, J. H. (1993). Demand Side Management: Concepts and Methods.
PennWell Books, Tulsa, OK, USA.
Kirschen, D. (2003). Demand-side view of electricity markets. IEEE Transactions on Power Systems, 18(2), 520527.
Luo, L., Chakraborty, N., & Sycara, K. (2013). Distributed algorithm design for multi-robot generalized task assignment. In Proceedings of IEEE International Conference on Intelligent
Robots and Systems.
Medina, J., Muller, N., & Roytelman, I. (2010). Demand response and distribution grid operations:
Opportunities and challenges. IEEE Transactions on Smart Grid, 1(2), 193198.
Mohsenian-Rad, A., Wong, V., Jatskevich, J., Schober, R., & Leon-Garcia, A. (2010). Autonomous
demand-side management based on game-theoretic energy consumption scheduling for the
future smart grid. IEEE Transactions on Smart Grid, 1(3), 320331.
Mohsenian-Rad, A., & Leon-Garcia, A. (2010). Optimal residential load control with price prediction in real-time electricity pricing environments. IEEE Transactions on Smart Grid, 1(2),
120133.
Nguyen, H. K., Song, J. B., & Han, Z. (2012). Demand side management to reduce peak-to-average
ratio using game theory in smart grid. In IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS), pp. 9196. IEEE.
Palensky, P., & Dietrich, D. (2011). Demand side management: Demand response, intelligent energy
systems, and smart loads. IEEE Transactions on Industrial Informatics, 7(3), 381388.
Pedrasa, M., Spooner, T., & MacGill, I. (2010). Coordinated scheduling of residential distributed
energy resources to optimize smart home energy services. IEEE Transactions on Smart Grid,
1(2), 134143.
Philpott, A., & Pettersen, E. (2006). Optimizing demand-side bids in day-ahead electricity markets.
IEEE Transactions on Power Systems, 21(2), 488498.
Rahimi, F., & Ipakchi, A. (2010). Demand response as a market resource under the smart grid
paradigm. IEEE Transactions on Smart Grid, 1(1), 8288.
921

fiV EIT, X U , Z HENG , C HAKRABORTY, & S YCARA

Ramchurn, S., Vytelingum, P., Rogers, A., & Jennings, N. (2012). Putting the smarts into the
smart grid: A grand challenge for artificial intelligence. Communications of the ACM, 55(4),
8697.
Ramchurn, S. D., Vytelingum, P., Rogers, A., & Jennings, N. (2011). Agent-based control for decentralised demand side management in the smart grid. In The 10th International Conference
on Autonomous Agents and Multiagent Systems-Volume 1, pp. 512.
Samadi, P., Mohsenian-Rad, H., Wong, V., & Schober, R. (2013). Tackling the load uncertainty
challenges for energy consumption scheduling in smart grid. IEEE Transactions on Smart
Grid, 4(2), 10071016.
Tanaka, K., Uchida, K., Ogimi, K., Goya, T., Yona, A., Senjy, T., Funabashi, T., & Kim, C. (2011).
Optimal operation by controllable loads based on smart grid topology considering insolation
forecasted error. IEEE Transactions on Smart Grid, 2(3), 438444.
Veit, A., Xu, Y., Zheng, R., Chakraborty, N., & Sycara, K. (2013). Multiagent coordination for energy consumption scheduling in consumer cooperatives. In Proceedings of 27th AAAI Conference on Artificial Intelligence, pp. 13621368.
Voice, T., Vytelingum, P., Ramchurn, S., Rogers, A., & Jennings, N. (2011). Decentralised control
of micro-storage in the smart grid. In Proceedings of 25th AAAI Conference on Artificial
Intelligence, pp. 14211427.
Vytelingum, P., Ramchurn, S., Rogers, A., & Jennings, N. (2010). Agent-based homeostatic control
for green energy in the smart grid. In First International Workshop on Agent Technologies for
Energy Systems (ATES 2010).
Vytelingum, P., Voice, T., Ramchurn, S., Rogers, A., & Jennings, N. (2011). Theoretical and practical foundations of large-scale agent-based micro-storage in the smart grid. Journal of Artificial Intelligence Research, 42(1), 765813.
Wood, A., & Wollenberg, B. (1996). Power Generation Operation and Control. John Wiley & Sons.
Wu, C., Mohsenian-Rad, H., & Huang, J. (2012). Wind power integration via aggregator-consumer
coordination: A game theoretic approach. In Innovative Smart Grid Technologies (ISGT),
2012 IEEE PES, pp. 16. IEEE.

922

fiJournal of Artificial Intelligence Research 50 (2014) 763-803

Submitted 1/14; published 8/14

Policy Iteration Based on Stochastic Factorization
Andre M. S. Barreto

AMSB @ LNCC . BR

Laboratorio Nacional de Computacao Cientfica
Petropolis, Brazil

Joelle Pineau
Doina Precup

JPINEAU @ CS . MCGILL . CA
DPRECUP @ CS . MCGILL . CA

School of Computer Science
McGill University
Montreal, Canada

Abstract
When a transition probability matrix is represented as the product of two stochastic matrices,
one can swap the factors of the multiplication to obtain another transition matrix that retains some
fundamental characteristics of the original. Since the derived matrix can be much smaller than
its precursor, this property can be exploited to create a compact version of a Markov decision
process (MDP), and hence to reduce the computational cost of dynamic programming. Building
on this idea, this paper presents an approximate policy iteration algorithm called policy iteration
based on stochastic factorization, or PISF for short. In terms of computational complexity, PISF
replaces standard policy iterations cubic dependence on the size of the MDP with a function that
grows only linearly with the number of states in the model. The proposed algorithm also enjoys
nice theoretical properties: it always terminates after a finite number of iterations and returns a
decision policy whose performance only depends on the quality of the stochastic factorization. In
particular, if the approximation error in the factorization is sufficiently small, PISF computes the
optimal value function of the MDP. The paper also discusses practical ways of factoring an MDP
and illustrates the usefulness of the proposed algorithm with an application involving a large-scale
decision problem of real economical interest.

1. Introduction
Decisions rarely come up alone in real situations: usually, the outcome of a decision has an effect
over the next one, which in turn impacts on the next, and so on. Thus, a choice that seems beneficial
from a short-sighted perspective may reveal itself to be disastrous in the long run. When dealing
with a succession of interrelated choices, one must weigh the immediate effects of a decision against
its long-term consequences in order to achieve good overall performance. Formally, tasks involving
this trade-off between short- and long-term benefits are called sequential decision-making problems.
This work focuses on a particular decision-making model known as a Markov decision process
(MDP, Puterman, 1994). An MDP is a simple yet important mathematical model that describes a
sequential decision task in terms of transition probabilities and rewards. The transition probabilities
represent the dynamics of the process, while the rewards provide evaluative feedback for the decisions made. Given an MDP, one is usually interested in finding an optimal decision policy, which
maximizes the expected total reward the decision maker will receive in the long run. The natural
c
2014
AI Access Foundation. All rights reserved.

fiBARRETO , P INEAU , & P RECUP

way to perform such a search is to resort to dynamic programming, a class of methods for solving
sequential decision problems developed concomitantly with the MDP model (Bellman, 1957).
Since the publication of Bellmans (1957) seminal book, dynamic programming has been studied for more than 50 years, and is now supported by a strong and well understood theoretical basis.
Besides, it has long ago transcended the limits of academia to be tested in real situations (White,
1985, 1988, 1993). Despite the success of dynamic programming in several applications, there is a
serious obstacle that hinders its widespread use: the computational cost of dynamic programming
algorithms grows fast with the number of states of a problem, which precludes their use in many
domains. This limitation was noted by Bellman (1961), who also pointed out that the number of
states of a decision process grows exponentially with the number of dimensions of its state spacea
problem that came to be known as dynamic programmings curse of dimensionality.
Nowadays there is a consensus that, in order to solve large-scale sequential decision problems,
one must exploit special structure in the corresponding model or resort to some form of approximation (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Powell, 2007). One way of incorporating
approximation into the dynamic programming framework is to create a compact version of an MDP
that retains as much as possible of the information contained in the original model. The approach
presented in this paper is based on this idea. Specifically, it builds on the following insight: when
a transition probability matrix is approximated by the product of two stochastic matrices, one can
swap the factors of the multiplication to obtain another transition matrix, possibly much smaller than
the original, which is related to its precursor. This property, called here the stochastic-factorization
trick, can be exploited to create a compact version of an MDP, and hence to reduce the computational demands of dynamic programming. The main contribution of the paper is an approximate
policy iteration algorithm named policy iteration based on stochastic factorization (PISF). As will
be shown, the performance of the decision policy computed by PISF only depends on the quality of
the stochastic factorization; in particular, an exact factorization leads to an optimal policy. Moreover, the computational complexity of each iteration of the proposed algorithm is only linear in the
number of states of the MDP.
The stochastic factorization will be presented in detail in Section 2.4. Although simple, the
presentation depends on a few basic concepts, which will be introduced in Sections 2.2 and 2.3.
Section 3 discusses the use of stochastic factorization to approximate an MDP. This section also introduces and analyzes the PISF algorithm, the main contribution of the paper. Section 4 investigates
the computational issues surrounding the use of PISF in practice and presents possible solutions to
efficiently compute the factorization of an MDP. In Section 5 some of the proposed solutions are put
to the test in a large-scale decision problem involving the maintenance of an asset with components
that deteriorate over time. Section 6 outlines the relationship between the stochastic factorization
and other approaches described in the literature. The paper ends in Section 7, where a brief summary
is presented along with suggestions for future research.

2. Background
This section introduces the notation adopted and briefly reviews some concepts that will be used
throughout the paper.
764

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

2.1 Notation
Boldface letters will be used to denote matrices and vectors. Given a matrix A, the symbol ai is used
to represent its ith row; ai j denotes the jth element of vector ai . Inequalities should be interpreted
element-wise; thus A  B means that ai j  bi j for all i and all j. The operators max and argmax
are applied row-by-row, that is, given A  R pq , max A is a vector b  R p such that bi = max j ai j
for all i. Finally, the symbols bmax and bmin are used as a shorthand for maxi bi and mini bi .
2.2 Markov Decision Processes
In the sequential decision-making model considered here decisions are made at discrete time steps.
At each instant t the decision maker occupies a state si  S and must choose an action a from a set
A. The sets S and A are called the state and action spaces, respectively. In this paper it is assumed
that both S and A are finite (though possibly large). The execution of action a in state si moves the
decision maker to a new state s j , where a new action must be selected, and so on. Each transition
a
si 
 s j has a certain probability of occurrence and is associated with a reward r  R. The goal
of the decision maker is to find a policy  : S 7 A, that is, a mapping from states to actions, that
maximizes the expected return associated with every state in S.1 The return is defined as follows:
T

R (si ) = rt+1 +  rt+2 +  2 rt+3 + ... +  T 1 rt+T = k=1  k1 rt+k ,

(1)

where rt+k  R is the reward received on the kth transition starting from state si at time step t. The
parameter   [0, 1) is the discount factor, which determines the relative importance of individual
rewards depending on how far in the future they are received. The sequential decision process may
last forever (T = ) or until the decision maker reaches a terminal state (T < ).
The decision-making process described above can be formalized as a Markov decision process,
or MDP for short. An MDP is a tuple M  (S, A, P, R,  ) (Puterman, 1994). The element P is a
family of transition probability functions, one for each action a  A. The function Pa : S  S 7 [0, 1]
gives the transition probabilities associated with action a; Pa (s j |si ) is the probability of a transition
|S|
to state s j when action a is executed in state si . Note that  j=1 Pa (s j |si ) = 1, for all a  A and
all si  S (in this paper |  | is used to denote both the cardinality of a set and the absolute value
of a scalar; the distinction should be clear by the context). The remaining component of an MDP,
a
R, is fidefined analogously
to P: the reward received at transition si 
 s j is given by Ra (si , s j ),
fi
with fiRa (si , s j )fi  Rmax < . Usually one is interested in the expected reward resulting from the
|S|

execution of action a in state si , that is, ra (si ) =  j=1 Ra (si , s j )Pa (s j |si ). A policy  defined over an
MDP induces a Markov process M  . The dynamics of M  are given by P (si ) (|si ), where  (si ) is
the action selected by  in state si . Likewise, the expected reward to be collected by  in state si is
given by r (si ) (si ).
When both the state space S and the action space A are finite, the MDP can be represented
in matrix form. Each function Pa becomes a matrix Pa  R|S||S| , with paij = Pa (s j |si ). Since
the elements in each row of Pa are nonnegative and sum to one, this is a stochastic matrix (see
Definition 1). Stochastic matrices will play an important role in the rest of the paper. In matrix

1. More generally, decision policies are rules associating states to actions, and can range in generality from randomized
history-dependent to stationary deterministic (Puterman, 1994, Section 2.1.5). Since in discounted MDPs with finite
state and action spaces there always exists a stationary deterministic policy that performs optimally, this paper will
focus on this class of decision policies (Puterman, 1994, Thm. 6.2.7).

765

fiBARRETO , P INEAU , & P RECUP

form, each function ra is a vector ra  R|S| , where ria = ra (si ). Thus, a finite MDP M can be
represented by |A| matrices Pa and the same number of vectors ra : M  (S, A, Pa , ra ,  ). A decision
policy defined over a finite MDP is a vector   A|S| whose element i is the action selected by 
in si . The Markov process induced by  can be represented by a matrix P  R|S||S| and a vector
r  R|S| . The ith row of P corresponds to the row with the same index in the matrix Pi , that is,
pi = pi i . The entries of r are given by ri = rii .
In this paper it is assumed that A is a totally ordered set, and the symbol a is used to refer to
both the action a itself and its index in A. This slight abuse of notation simplifies the presentation
considerably, and the distinction should be clear from the context.
2.3 Dynamic Programming
All dynamic programming theory is built upon the concept of a value function. The value of a state
si under a policy  , denoted by V  (si ), is the expected return the decision maker will receive from si
when following  . Using (1), one can write V  (si ) = E  {R (si )}. In the case of a finite state space,
the value function is a vector v  R|S| . The vector v makes it possible to impose a partial ordering
over decision policies. In particular, a policy   is considered to be at least as good as another policy

 if v  v . The goal of the sequential decision problem is to find an optimal policy   such
that v  v for all  . It is well known that there always exists at least one such policy for a given
MDP (Bertsekas, 1987; Puterman, 1994). When there is more than one optimal policy, they all
share the same value function v .
What makes the search for an optimal policy feasible is the Bellman equation, a recursive relation between state values that lies at the core of all dynamic programming algorithms. The Bellman
equation of a decision policy  is given by v = r +  P v . It is possible to use this equation
to compute the value function of policy  . One way to do so is to simply convert it into the socalled Bellman operator of  , T  v = r +  P v. It is known that (T  )t v  v as t   for any
vector v  R|S| (Bertsekas, 1987; Puterman, 1994). A more direct approach to compute v is to
interpret the Bellman equation as a system of linear equations and compute the value function as
v = (I   P )1 r , where I is the identity matrix of dimension |S|.
Given v , it is possible to generate a decision policy whose performance is at least as good as
that of the original policy  . Let  : R|S| 7 R|S||A| be a mapping associated with a given MDP M
such that if v = Q, then the ath column of Q is
qa = ra +  Pa v.

(2)

It should be clear that (T  v)i = (v)ia , with a = i . If instead a = argmax j (v)i j , one has the
Bellman operator of the MDPthat is, T v = max v. Alternatively, T v can be viewed as a single

application of T  , with   = argmax v. The driving force of dynamic programming is the fact
that if   is derived from v it cannot perform worse than  . Therefore, all dynamic programming
algorithms are variations of the same basic scheme: starting from an arbitrary v  R|S| , compute
policy  = argmax v and apply the update rule v (T  )t v for some t > 0. Then, based on v ,

compute a new decision policy   , apply T  for t steps, and so on. It can be shown that, regardless
of the value of t, this process will eventually converge to an optimal policy   (Bertsekas, 1987;
Puterman, 1994).
When the scheme described above is adopted with t = 1, the process reduces to successive
applications of the Bellman operator T , and the resulting method is the popular value iteration
766

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

algorithm. This paper will focus on the other extreme of the spectrum, when t = . In this case one
has the policy iteration method (Howard, 1960). Algorithm 1 shows a step-by-step description of
the computations performed by policy iteration (see also Appendix A.1).
Algorithm 1 Policy iteration
Require: MDP M: Pa  R|S||S| and ra  R|S| for each a  A,   [0, 1)
Ensure:  
1:    random vector in A|S|
2: repeat
  
3:
4:
for i1, 2, ..., |S| do pi pii and ri rii
v (I   P )1 r
6:
   argmax v
7: until  =  

5:

 Ties are broken randomly

2.4 Stochastic-Factorization Trick
This section presents the stochastic-factorization trick, a mathematical concept recently introduced
by Barreto and Fragoso (2011) that will serve as a cornerstone for the subsequent developments.
The trick builds on the following definitions:
Definition 1. A matrix P  Rnz is called stochastic if and only if pi j  0 for all i, j and zj=1 pi j = 1
for all i. A square stochastic matrix is called a transition matrix.
Definition 2. Given a stochastic matrix P  Rnz , the relation P = DK is called a stochastic factorization of P if D  Rnm and K  Rmz are also stochastic matrices. The integer m > 0 is the order
of the factorization.
The relation P = DK and the fact that D is stochastic imply that every row of P can be obtained
as a convex linear combination of the rows of K. In other words, all n stochastic vectors pi  R1z
lie within the convex hull defined by the set of m stochastic vectors ki  R1z . Obviously, when
m  n it is always possible to find such a hull, that is, it is always possible to compute a stochastic
factorization of P. When m < n, however, an exact factorization might not be possible. This leads
to the following definition:
Definition 3. The stochastic rank of a stochastic matrix P  Rnz , denoted by srk(P), is the smallest
possible order of the stochastic factorization P = DK.
A matrix is called nonnegative if all its elements are greater than or equal to zero. That said,
the definitions of nonnegative factorization and nonnegative rank follow analogously to that of
their stochastic counterparts. Cohen and Rothblum (1991) have shown that it is always possible to
derive a stochastic factorization from a nonnegative factorization of a stochastic matrix (see their
Theorem 3.2). Since any stochastic factorization is also a nonnegative factorization, it follows that
the nonnegative and stochastic ranks of a stochastic matrix coincide. It is easy to show that if P has
only one nonzero element per row, the stochastic rank of this matrix coincides with its conventional
rank, that is, srk(P) = rk(P). In the general case, however, the only thing that can be said is that
rk(P)  srk(P)  min(n, z) (Cohen & Rothblum, 1991).
767

fiBARRETO , P INEAU , & P RECUP




0.10 0.90 0.00
P =  0.28 0.63 0.09 
0.70 0.00 0.30




1.0 0.0
D =  0.7 0.3 
0.0 1.0

K=



0.1 0.9 0.0
0.7 0.0 0.3



P =



0.73 0.27
0.70 0.30



Figure 1: Reducing the dimension of a Markov process from n = 3 states to m = 2 artificial states.
The original states are represented as white circles; black circles depict artificial states. These
figures have appeared before in the article by Barreto and Fragoso (2011).
The stochastic factorization has appeared before in the literature, either as defined above (Cohen
& Rothblum, 1991; Ho & van Dooren, 2007) or in slightly modified versions (Cutler & Breiman,
1994; Ding et al., 2010). However, this paper will focus on a useful property of this type of factorization that has only recently been noted (Barreto & Fragoso, 2011).
Let P be a transition matrix of dimension n representing the dynamics of a Markov process. The
stochastic factorization P = DK admits an interesting interpretation in this case. Suppose m < n,
where m is the order of the factorization. The elements in each row of D can be seen as transition
probabilities from the states of the original Markov process to a set of m artificial states. Similarly,
the rows of K may be interpreted as probabilities of transitions in the opposite direction. With this
interpretation in mind, it is interesting to ask why the product DK restitutes the dynamics of the
original process. To answer this question, it suffices to see each element pi j = m
l=1 dil kl j as the sum
of the probabilities associated with m two-step transitions: from si to each artificial state and from
these back to s j . In other words, pi j is the accumulated probability of all possible paths from si to
s j with a stopover in one of the artificial states. Following similar reasoning, it is not difficult to see
that by swapping the factors of the stochastic factorization, that is, by switching from DK to KD,
one obtains the transition probabilities between the artificial states. This makes it possible to define
a new Markov process, composed of m artificial states, whose dynamics are given by P = KD.
Figure 1 illustrates this idea for the case in which a Markov process with three states is reduced to a
compact model containing only two artificial states.
By simply swapping the factors of a stochastic factorization, it is possible to derive a new matrix P that retains information about the dynamics of the original Markov process in a compact
way. The stochasticity of P follows immediately from the same property of D and K. What is
perhaps more surprising is the fact that this matrix shares some fundamental characteristics with the
original matrix P. Specifically, it is possible to show that: (i) for each recurrent class in P there
is a corresponding class in P with the same period and, given some simple assumptions about the
factorization, (ii) P is irreducible if and only if P is irreducible and (iii) P is regular if and only if P
is regular (see Barreto & Fragoso, 2011, for details and formal definitions). This property is called
here the stochastic-factorization trick:

Given a stochastic factorization of a square matrix, P = DK, swapping the factors of the factorization yields another transition matrix P = KD, potentially much smaller than the original, which
retains the basic topology and properties of P.
768

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

3. Policy Iteration Based on Stochastic Factorization
This section introduces the main contribution of the paper, an approximate policy-iteration algorithm built upon the stochastic-factorization trick. The section starts with a description of how the
stochastic-factorization trick can be used to reduce the computational cost of evaluating a Markov
process, and then generalizes the idea to an MDP.
3.1 Approximating a Markov Process
Suppose that during the search for an optimal policy of a given MDP one has to determine the value
function of the decision policy  . Instead of computing it directly, one can generate a compact
version of the Markov process induced by  and use its value function to recover the value function
of the original process. The following proposition provides the mathematical foundation for the
implementation of the strategy above.
Proposition 1. Let M  (S, A, Pa , ra ,  ) be an MDP, with 0   < 1. Given a policy   A|S| , let
P  R|S||S| and r  R|S| be the transition probability matrix and the expected reward vector of
the Markov process M  induced by this policy in M. Let D  R|S|m be a nonnegative matrix, let
K  Rm|S| be a stochastic matrix, and let r be a vector in Rm such that
DK = P and Dr = r .

(3)

Then,
(i) P = KD and r define a Markov process M  with m states.
Let v  Rm be the value function of M  computed with discount factor  . Then,
(ii) v = Dv is the value function of M  .
Proof. Since K is a stochastic matrix and D is nonnegative, the equality DK = P implies that
D is also stochastic: 1 =  j pi j =  j l dil kl j = l dil  j kl j = l dil . The fact that D and K are
nonnegative implies that P is nonnegative; since  j pi j =  j l kil dl j = l kil  j dl j = l kil = 1,
P is a transition matrix (see Definition 1). This proves (i). In order to prove (ii), recall that v , the
value function of M  , can be written as
v = r +  P v

(4)

(the existence and uniqueness of a solution for (4) are guaranteed by the stochastic property of P
and the fact that 0   < 1see, for example, Lemma 2.3.3 of Golub & Loan, 1996 or Proposition 2.6 of Bertsekas & Tsitsiklis, 1996). Multiplying both sides of (4) by D, one has
Dv = Dr +  DP v = r +  DKDv = r +  P Dv .

(5)

Expression (5) is the Bellman equation associated with the value function of M  ; since this equation
has a single fixed point, (ii) must be true (Bertsekas, 1987; Puterman, 1994).
The computation of a decision policys value function involves O(|S|3 ) arithmetic operations
(Littman et al., 1995). In theory, Proposition 1 makes it possible to reduce the computational complexity of such a procedure to O(m3 ) (also see Appendices A.1 and A.2). Practically speaking,
769

fiBARRETO , P INEAU , & P RECUP

however, the application of Proposition 1 raises some difficulties. First, one must determine a reasonable value for m, the number of artificial states in the compact model. Obviously, one wants
this value to be as small as possible, but it is not trivial to find the smallest m that allows for the
application of the proposition. Even if the stochastic rank of P is known, it might not be possible
to simultaneously satisfy both equalities in (3) with m = srk(P ). Moreover, the computation of
D, K and r requires a number of arithmetic operations that can easily exceed the number of operations involved in the original calculation of v .2 For all these reasons, one may have to resort to an
approximate factorization of the Markov process.
In the approximate version of the stochastic factorization problem, one is interested in finding
stochastic matrices D and K that represent P as well as possible, i.e., that minimize a measure of
the dissimilarity between DK and P . In order to apply Proposition 1, one must also search for a
vector r  Rm that makes Dr as similar as possible to r . Once D, K, and r have been determined,
one can swap the factors of the stochastic factorization to define a Markov process with m artificial
states. As described in Proposition 1, the value function of the resulting Markov process, v , can be
used to restore the value function of the original model. Obviously, when DK  P and Dr  r ,
Dv will be, too, only an approximation of v . An important issue in this case is to quantify the
impact that errors in the approximation of P and r might have on the computation of the value
function. This section provides such an analysis for a specific dissimilarity measure. In particular,
it presents an upper bound for k v  Dv k based on k P  DK k and k r  Dr k . Here,
k  k denotes the maximum norm, which induces the following norm over the space of matrices:
k A k = maxi k ai k1 = maxi  j |ai j |.
Proposition 2. Let P  R|S||S| and r  R|S| be the transition probability matrix and the expectedreward vector describing the Markov process M  induced by decision policy  . Let D  R|S|m and
K  Rm|S| be stochastic matrices, and let r be a vector in Rm . Finally, let M  be the Markov
process described by P = KD and r. Then,



1






k P  DK k  ,
k v  Dv k     k r  Dr k +
(6)

2(1   )
where v and v are the value functions of M  and M  , both computed with the same   [0, 1),
 , r

  = 1   1  12 k P  DK k , and  = max(rmax
max )  min(rmin , rmin ).

Proof. Let M  be the Markov process with transition matrix DK and reward vector Dr. From
Proposition 1 it follows that the value function of M  is given by v = Dv . Recall that a Markov
process can be seen as an MDP in which |A| = 1. Then, applying Whitts (1978) Theorem 6.2 (b)
to M  and M  , with all mappings between the two models taken to be identities, one concludes that
vi  vi +   for all i. Since the mappings between M  and M  are the identity function, one can
apply Theorem 6.2 (b) again, exchanging the roles of the two models, to obtain vi  vi +   for all
i. The upper bound in (6) results from the combination of the two inequalities above.
According to Whitt (1978), it is possible to construct examples showing that (6) is a tight bound.
Proposition 2 makes it clear that the approximation of v depends on both the characteristics of M 
2. Vavasis (2009) has shown that the determination of the nonnegative rank of a stochastic matrix is an NP-hard problem.
This implies that no polynomial-time algorithm for computing D and K is currently known; if this were the case, one
could compute one factorization for each value of m = |S|, |S|  1, ..., 1, stopping when an exact factorization is no
longer possible, thus determining the matrixs rank in polynomial time.

770

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

and the quality of the stochastic factorization. First, the right-hand side of (6) increases as   1
and   . This is expected, since both the magnitude of the individual rewards and the rate at
which they accumulate over time tend to increase the states values. More important, the difference
between v and Dv directly depends on the stochastic factorization, and as DK  P and Dr  r
the approximation Dv gets closer to the real value function v . In the limit, when k r  Dr k = 0
and k P  DK k = 0, one recovers Proposition 1.
It is interesting to point out an intriguing property of Proposition 2. Observe that an increase
in the approximation error k P  DK k has two opposite effects on the variables involved in the
derived bound: if on the one hand it increases the coefficient multiplying  , as expected, on the
other hand it also decreases the factor 1/  scaling the entire right-hand of (6). This is precisely
what makes the bound tight, since the latter effect tends to alleviate the first. Needless to say, the
bound is still a monotonically increasing function of k P  DK k , as one can easily verify by
computing the appropriate partial derivative.
3.2 Approximating a Markov Decision Process
The most straightforward way to use the stochastic-factorization trick in the search for a decision
policy is to factor one by one the Markov processes that come up in the search. To be more specific,
let  be an arbitrary decision policy defined over a finite MDP M and let P and r describe the
Markov process induced by this policy. Given approximations DK  P and Dr  r , one can
define a new Markov process with transition matrix P = KD and reward vector r. This auxiliary
model potentially has many fewer states than the original Markov process. After determining the
value function of the compact model, v , one can compute an approximation of the original value
function by making v = Dv . Finally, a new policy   = argmax v can be derived, restarting
the usual dynamic programming loop. Notice that if DK = P and Dr = r for every policy  that
comes up during the search, this process will eventually converge to an optimal decision policy.
This is a direct consequence of Proposition 1.
Although feasible, the above strategy presents an obvious drawback: a new factorization must
be computed for each Markov process encountered in the search for a decision policy. Another possibility is to factor the entire MDP at once. In this case, a possible approach is to find approximate
factorizations for each Markov process M a , Da Ka  Pa and Da ra  ra , with a  A, and solve the
reduced MDP M whose transition matrices are Ka Da and whose expected reward vectors are ra .
However, when the MDP M is directly solved, the quality of the solution found does not depend on
the stochastic factorization only, and even an exact factorization of the MDP can lead to suboptimal
decision policies (for example, as shown in Barreto, Precup, & Pineau, 2011, Prop. 1, in the particular case in which a single matrix D is used to factor all the Markov processes, the approximation
error also depends on the level of stochasticity of D, measured by maxi (1  max j di j )). The remaining of this section discusses an alternative way of factoring an MDP in which the performance
of the final decision policy depends exclusively on the quality of the stochastic factorization.
Let M  (S, A, Pa , ra ,  ) be a finite MDP. Let Da  R|S|m and K  Rm|S| be stochastic matrices
such that Da K  Pa for all a  A, and let r  Rm be a vector such that Da r  ra , again with a  A. Let
  A|S| be a policy defined on M and let M  be the corresponding Markov process. As discussed
in Section 2.3, the ith row of P  R|S||S| , the transition matrix of M  , is the ith row of Pi , where
i is the action selected by  in si (see line 4 of Algorithm 1). From the approximations Da K  Pa ,
one can see that the ith row of Pi can be approximated as di i K (recall that di i is the ith row of Di ).
771

fiBARRETO , P INEAU , & P RECUP

Thus, in order to build an approximation of P , it suffices to construct matrix D  R|S|m whose
rows are given by di = di i . Once D has been determined, the transition matrix associated with
 can be approximated as P  D K. Analogously, the vector r  R|S| can be approximated as
r  D r.
Using the strategy above, it is straightforward to extend the ideas of Proposition 1 to a factorization of the entire MDP. Given a decision policy  , one must first compute the matrix D as described
and then define a reduced Markov process M  with transition matrix P = KD and reward vector
r. The corresponding value function v can then be used to compute an approximation of the value
function of  , which will serve as a reference for the derivation of a new decision policy   , and
so on. Algorithm 2 shows how these ideas can be embedded into policy iteration, giving rise to the
policy iteration based on stochastic factorization algorithm, or simply PISF.

Algorithm 2 Policy iteration based on stochastic factorization (PISF)
Require: Da  R|S|m for all a  A, K  Rm|S| , r  Rm , and   [0, 1)
Ensure:    
1:    random vector in A|S|
2: repeat
3:
  
4:
for i1, 2, ..., |S| do di dii
5:
P KD
6:
v (I   P )1 r
7:
Let Q  R|S||A|
8:
for a1, 2, ..., |A| do q ,a Da v
 q ,a is the ath column of Q
9:
   argmax Q
 Ties are broken randomly
10: until  =  

In the standard policy iteration algorithm, the computation of a decision policys value function
takes O(|S|3 ) arithmetic operations, while the derivation of a new policy is O(|S|2 |A|) (Littman
et al., 1995). In contrast, PISF only needs O(|S|m|A|) operations to derive a new policyas shown
in lines 8 and 9 of Algorithm 2and breaks the value function computation into several steps
whose overall complexity is O(|S|m2 ). The process of computing v is as follows. First, one has
to determine matrix D , which involves O(|S|) operations (line 4 of Algorithm 2). Then, it is
necessary to perform O(m2 |S|) operations to compute the transition matrix P (line 5). Finally,
one must calculate v , which is O(m3 ) (line 6). As one can see, the computational complexity of
one iteration of PISF is only linear in the number of states |S|. Thus, when m  |S|, the number of
arithmetic operations performed per iteration by PISF is much smaller than the number of operations
that would be executed by the conventional policy iteration algorithm.
Regarding the space complexity of PISF, storing Da , K and r requires O(|S||A|m) bits. Note
though that in an actual implementation the matrices Da do not need to be stored in the main memory. Thus, if one is willing to trade space for timesince in this case D would have to be computed
or loaded on demand, the actual memory usage of the algorithm drops to O(|S|m) only.
772

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

3.3 Convergence and Error Bound
The PISF algorithm rests on the approximations Da K  Pa and Da r  ra . If these approximations
happen to be exact, Proposition 1 is directly applicable to every decision policy generated by this algorithm, which implies that it will converge to an optimal policy   . Nevertheless, in order for PISF
to be considered a stable method, it is necessary to show that errors in the above approximations
will not cause the algorithm to behave in a completely unpredictable way. The following proposition
shows that PISF is a well-behaved algorithm, in the sense that it always terminates in a finite number
of iterations and the quality of the decision policy returned improves when kDa KPa k  0 and
kDa rra k  0 for all a  A.
Proposition 3. Let M  (S, A, Pa , ra ,  ) be an MDP with   [0, 1). Let Da  R|S|m and K  Rm|S|
be stochastic matrices, with a  A, and let r  R|S| . Then, if PISF is executed with Da , K, r, and  ,
(i) It will terminate after a finite number of iterations.
Let v be the value function of the policy returned by PISF and let ra = Da r for all a  A. Then,
(ii)
k

v  v k

2

1



maxa k

ra  ra



a
a
maxa k P  D K k  ,
k +
2(1   )

(7)

a  min ra .
where v is the optimal value function of M and  = maxa rmax
a min

Proof. Let M  (S, A, Da K, Da r,  ), that is, M is the MDP whose transition matrices are Da K and
whose expected reward vectors are Da r, for all a  A. The strategy of the proof will be to show that
executing PISF with Da , K, r, and  is equivalent to running standard policy iteration in M.
Let  , v and Q be the policy, value function, and matrix computed by PISF at the ith iteration
(lines 3, 6, and 8 of Algorithm 2, respectively). From the definition of Q , one can write
q ,a = Da v = Da (r +  P v ) = Da (r +  KD v ) = Da r +  Da KD v ,

(8)

where D is also a matrix constructed by PISF at the ith iteration (line 4 of Algorithm 2). Comparing (2) and (8), it is clear that Q = D v . From Proposition 1, it follows that D v is the value
function of the Markov process described by D K and D r. But this is exactly the Markov process
induced by  in M (see line 4 of Algorithm 1). Therefore, the policy computed in one iteration of
PISF starting with  ,   = argmax Q = argmax D v , is the same policy that would be computed
in one iteration of standard policy iteration applied to M and also starting with  . This implies that
PISF will converge to the optimal policy of M in a finite number of iterations, and hence (i) holds
(see, for example, Puterman, 1994, Thm. 6.4.2).
In order to show (ii), it suffices to resort to Whitts (1978) results comparing dynamic programs, a generalization of MDPs proposed by Denardo (1967). Specifically, if one applies the
corollary of Whitts Lemma 3.1 to M and M, with all mappings between the two MDPs taken to be
identities, it follows that
2
maxi j |hi j |,
(9)
k v  v k 
1
where hi j are the elements of matrix H = v  v and v is the optimal value function of M (since
the policy computed by PISF is optimal in M, the last term appearing in Whitts bound vanishes).
773

fiBARRETO , P INEAU , & P RECUP

Based on Corollary (b) of Whitts Theorem 6.1, one can write:
maxi j |hi j |  maxa k ra  ra k +


maxa k Pa  Da K k .
2(1   )

(10)

Substituting the right-hand side of (10) in (9) one gets the desired bound.3
Proposition 3 states that PISF will converge to a decision policy after a finite number of iterations. In fact, as the proof of the proposition shows, the solution returned by PISF is one of the
optimal policies of the MDP M  (S, A, Da K, Da r,  ). Thus, one can look at PISF as a fast specialized method to solve MDPs that have a specific type of structurenamely, MDPs that allow for an
exact factorization with a single K and a single r. Of course, PISF will converge to a decision policy even if the factorization is not exact; in this case the algorithm becomes an approximate policy
iteration method.
The error bound provided in Proposition 3 is not tight in general. This can be seen by noting
that when |A| = 1 the bound in (7) may be looser than its counterpart in (6). Also, the bound is
too pessimistic to be of practical value in many situations. Even so, Proposition 3 is of conceptual
importance because it establishes the soundness of PISF. In particular, it states that the performance
of the policy returned by this algorithm gets closer to optimal as the error in the MDP approximation
decreases. In fact, it is not difficult to show that, if the errors in the approximations Da K  Pa and
Da r  ra are below a certain threshold, the policy returned by PISF performs optimally in M. In
order to do that, first note that ra = Da r implies that rmin  ria  rmax for all a, and thus one can
a , max ra ] and still get an exact factorization.
restrict the elements of r to the interval [mina rmin
a max
Assuming this is the case, the term  appearing in the right-hand side of (7) can be replaced by
a  min ra . This means that max k Pa  Da K k and max k ra  Da r k are the only
maxa rmax

a min
a

a
terms in (7) that vary with Da , K and r, and hence one can make the bound arbitrarily small by
driving the approximation errors to zero. Let   A|S| be the set of non-optimal policies of M and
let  = min  k v  v k . Since v is the value function of a specific policy, it follows that, if the
right-hand side of (7) is smaller than  , then v = v . Therefore, there exists a scalar   such that if
maxa k Pa  Da K k <   and maxa k ra  Da r k <   the policy returned by PISF is optimal.

4. Computing the Stochastic Factorization
As shown in the previous section, PISFs performance depends crucially on the approximations
Da K  Pa and Da r  ra , with a  A. This section discusses how to compute Da , K, and r. It starts
with a generic presentation of the problem and then gradually focuses on more specific formulations
that can be solved much more efficiently.
4.1 The Optimization Problem
In order to facilitate the exposition, it will be assumed that the vectors ra and matrices Pa have been
concatenated and stacked to obtain a single matrix M  R|S||A||S|+1 representing an entire MDP, as
3. Ravindran and Barto (2004) follow similar strategy to bound the approximation loss resulting from an approximate
homomorphism.

774

fiP OLICY I TERATION BASED

follows:










M=







r11
..
.
1
r|S|
..
.
|A|
r1
..
.
|A|
r|S|

ON

p111
..
.
p1|S|1
..
.
|A|
p11
..
.
|A|
p|S|1

S TOCHASTIC FACTORIZATION

p112
..
.
p1|S|2
..
.
|A|
p12
..
.
|A|
p|S|2

p11|S|
..
.
p1|S||S|
..
.
|A|
p1|S|
..
.
|A|
p|S||S|


..
.

..
.

..
.











.







(11)

With the representation above, the objective of the factorization problem reduces to finding matrices
D and W such that DW  M. More formally, the approximate factorization of an MDP M can be
formulated as a constrained nonlinear optimization problem in the following way:
Problem 1. Given a matrix M  R|S||A||S|+1 representing an MDP, find D  R|S||A|m and W 
Rm|S|+1 in order to minimize a dissimilarity measure (M, DW), subjected to the following constraints:
di j  0 for i = 1, 2, ...|S||A| and j = 1, 2, ..., m,

(12)

wi j  0 for i = 1, 2, ...m and j = 2, 3, ..., |S| + 1,

(13)

m
d = 1 for i =
j=1 i j
|S|+1
wi j = 1 for
j=2




1, 2, ...|S||A|,

(14)

i = 1, 2, ...m.

(15)

Note that the above problem formulation assumes that the first column of M is reserved for the
rewards, as in (11). This is why the elements in the first column of W are not subjected to the
stochasticity constraints (13) and (15). When |A| = 1 the model M is a Markov process. Also, the
problem statement can be easily modified to reflect the case in which M represents a Markov chain
(that is, when there are no rewards in the Markov processsee Barreto & Fragoso, 2011). This
modification does not have any major impact on the discussion to follow.
It is not hard to see that the feasible region defined by the constraints of the above optimization
problem is a convex set. Thus, if  is continuously differentiable, one can resort to one of the several
methods presented by Bertsekas (1999) to solve a constrained nonlinear optimization problem with
a convex feasible region. Among them, the most obvious choice is probably a gradient projection
method, in which a candidate solution is iteratively refined by an update rule that keeps it within
the problems feasible region. Another approach that seems promising in this context is the block
coordinate descent method (Bertsekas, 1999). In this case, only a subset of the variables is updated
at a time while the remaining are kept fixed. For the optimization problem at hand, there are two
blocks of variables corresponding to the elements of matrices D and W. Thus, at each iteration of
the algorithm one applies the following update rules:
D argmin (M, DW)

and

DD

W argmin (M, D W),
WW

(16)

where D  R|S||A|m and W  Rm|S|+1 are the feasible regions of D and W, respectively. Depending
on the characteristics of the dissimilarity measure adopted, the repeated application of the above
775

fiBARRETO , P INEAU , & P RECUP

update rules will eventually converge to a stationary point of  (Grippo & Sciandrone, 2000). This
is true, for example, when (M, DW) =k M  DW kF , where k  kF is the Frobenius norm (Cutler &
Breiman, 1994; Lin, 2007b). When k  kF is used, one can compute D and W through a constrained
least-squares algorithm (Cutler, 1993).
The solution of the two subproblems in (16) may require a large number of arithmetic operations. Therefore, instead of searching for exact minima, one can take a small step per iteration
towards the solution of each subproblem, much like in conventional iterative descent methods (Lee
& Seung, 2000). In this case, it is relatively simple to enforce the stochasticity constraints by projecting the partial solutions onto the feasible region or by incorporating a penalty term into the
dissimilarity measure  (Lee & Seung, 1997). It should be noted, however, that when an iterative
update rule is used in place of (16), the guarantee of convergence to a solution may be lost (Lin,
2007a).
The ideas above have been extensively exploited in the study of a related optimization problem
known as nonnegative matrix factorization. As the name suggests, in this version of the problem
only constraints (12) and (13) are normally imposed (Paatero & Tapper, 1994; Lee & Seung, 1999).
There are many works available discussing theoretical and practical aspects of nonnegative matrix
factorization (for a survey, see Berry et al., 2007). Most of the ideas discussed in these works also
apply to the problem considered here. In particular, since one can derive a stochastic factorization
from a nonnegative factorization of a stochastic matrix, any algorithm designed for the latter can
also be used to compute the former (Cohen & Rothblum, 1991).
Instead of addressing Problem 1 as a generic nonnegative factorization, one can exploit the
particular structure of matrices D and W. As discussed in Section 2.4, the fact that D is stochastic
implies that an exact factorization DW = M is only possible when the rows of M are inside the
convex hull defined by the rows of W. So, one can try to solve the problem by (approximately)
computing a convex hull that contains the rows of M and then determining the coefficients that
recover mi as a convex combination of the vertices of the hullwhich will naturally give rise to a
stochastic D. This approach is closely related to Cutler and Breimans (1994) archetypal analysis
and Ding et al.s (2010) convex nonnegative factorization. There is also an interesting connection
with the problem known as spectral unmixing in the field of imaging spectroscopy, in which the
determination of matrix W is usually referred to as the task of extracting end-members (Keshava
& Mustard, 2002; Keshava, 2003).
Yet another way of approaching Problem 1 is to impose additional constraints and resort to
specialized methods. Consider for example the case in which D has only one nonzero element
per row. In this case, the factorization can be seen as a specific instance of the well-known dataclustering problem: the vectors mi must be grouped into m clusters C j whose centers are the
vectors wj the most common choice to define the centers is to have wj = 1/|C j | {i|mi C j } mi =
1/|C j | {i|di j =1} mi (Hartigan, 1975). The goal of the clustering problem is to determine an assignment of vectors mi to clusters C j in order to minimize (M, DW). Since W is automatically
defined by a given assignment, the problem reduces to computing matrix D. An advantage of interpreting the factorization as a clustering problem is the availability of a large number of algorithms
specifically designed to solve this type of optimization (Kaufman & Rousseeuw, 1990; Gan et al.,
2007). Conveniently, depending on how  is defined, the cluster centers wj will naturally satisfy
the stochasticity constraints (12), (13), (14), and (15). This is the case when the Frobenius norm is
adopted as the objective function.
776

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

4.2 Reducing the Computational Cost of the Factorization
This work presents the stochastic factorization as a general approach to reduce the number of states
in an MDP. Of course, the main reason one would be interested in such a reduction is to save
computational resources. One potential gain is in the amount of memory required to represent the
model. Nevertheless, this paper is mainly concerned with the use of stochastic factorization as a
strategy to reduce the time complexity of dynamic programming algorithms, that is, the number of
arithmetic operations performed in the search for a decision policy. In this case, the computational
cost of the factorization process itself is a concern. In particular, it only makes sense to resort to
stochastic factorization if the number of operations involved in the factorization process is much
smaller than the number of operations saved by replacing the original model with a compact one.
As discussed, the stochastic factorization problem is equivalent to nonnegative matrix factorization. Nonnegative matrix factorization has been a popular research topic in the last few years,
and as a result the efficiency of the algorithms has been increasing steadilyin fact, nowadays it is
possible to compute nonnegative factorizations of very large matrices in a matter of minutes (Esser
et al., 2012; Bittorf et al., 2012). Recently, Thurau et al. (2011, 2012) have proposed efficient algorithms to approximately compute a convex hull that contains the rows of M. They show that, by
exploiting the extra structure in Problem 1, one can compute the approximation DW  M much
faster than algorithms that treat the problem as a conventional nonnegative factorization. There are
also methods for the spectral unmixing problem that were specifically designed to be computationally efficient (Nascimento & Dias, 2004; Chang et al., 2006). Finally, if one interprets Problem 1
as a clustering task, it is possible to approximate very large matrices M by using some recently
proposed methods (Boutsidis et al., 2010; Shindler et al., 2011).
In principle, any of the methods above can be used to compute the approximation DW  M
required by PISF. Some of them are able to solve the problem in time linear in the number of rows
of M (Shindler et al., 2011; Thurau et al., 2012). Unfortunately, when it comes to Problem 1, this
does not mean that the factorization will depend linearly on |S|. The problem is that the vectors mi
live in R|S|+1 . This implies that the number of states in the MDP M defines not only the number of
vectors mi (the number of rows of M), but also their dimension (the number of columns of M). As
a consequence, even the linear methods will run in O(|S|2 ) time.
Depending on the size of the MDP and on the computational resources available, a quadratic dependency on |S| may be acceptable. There are many algorithms available in this case. For example,
Shindler et al.s (2011) clustering method can deliver an approximation DW  M in O(|S|2 |A|m)
time, which corresponds to m iterations of the value iteration algorithm (Littman et al., 1995). There
are also situations in which the transition matrices Pa are sparse, meaning that the execution of action a in state si can lead to a number of states z  |S|. In this case the factorization problem can be
solved in time linear in |S|. The remaining of this section focuses on the worst case scenario, that
is, the case in which M is not sparse and a quadratic dependency on |S| is not acceptable.
4.2.1 B REAKING

THE

D OUBLE D EPENDENCY

ON THE

S IZE

OF THE

MDP

In order to make the stochastic factorization problem tractable for large MDPs, one needs to circumvent the fact that |S| defines both the number of rows and the number of columns of M. The
strategy proposed in this section is to rewrite the problem in terms of a dissimilarity measure whose
computation does not depend on the number of states in the MDP.
777

fiBARRETO , P INEAU , & P RECUP

Let  be a dissimilarity measure defined in R|S|+1  R|S|+1 (such as a distance function, for
example). This section will assume that the measure  previously introduced is induced by  . For
a few examples, let A and B be two arbitrary matrices of the same dimension. Then one can have,
for instance,
(A, B)  i  (ai , bi ) =k A  B kF ,

 (ai , bi ) k ai  bi kF and
 (ai , bi )  k ai  bi k1 and

(A, B)  maxi  (ai , bi ) = k A  B k , (17)
q
 (ai , bi ) k ai  bi k2 and (A, B)  max i [(ai  bi )x]2 = max k (A  B)x k2 , (18)
x,kxk2 =1

x,kxk2 =1

where k  k2 is the Euclidean norm. Based on the expressions above, it is clear that in order to minimize (DW, M) one can focus instead on minimizing  ( j di j wj , mi ) for all i. One way of looking
at  j di j wj  mi is to think of the rows wj as a set of prototypical vectors that are representative
of the dynamics of the MDP M. Recalling that each row of D forms a convex combination, the element di j can be seen as the weight of representative vector wj in the approximation of mi (Cutler &
Breiman, 1994; Keshava & Mustard, 2002). The core assumption of this section is that, in general,
di j should decrease with  (mi , wj ). Although this is not necessarily true in an exact factorization,
many approximation schemes rely implicitly or explicitly on such a premise (Hastie et al., 2002).
One example is local kernel smoothing techniques such as the Nadaraya-Watson kernel-weighted
estimator (Hastie et al., 2002, Chap. 6). In this case, the elements of D would be computed as:
di j =

exp( (mi , wj )/ )
,
k exp( (mi , wk )/ )

(19)

where  controls the relative magnitude of the elements in one row of this matrix. More generally,
di j should be computed based on a function  that is non-increasing with respect to  (mi , wj ).
The assumption that di j decreases with  (mi , wj ) makes it possible to compute D based exclusively on  , as in the example given in (19). It is also possible to compute W using only this
dissimilarity measure. For example, Thurau et al. (2012) argue that, if one restricts the rows of W
to be a subset of the rows of M, the minimization of  as defined in (18) can be accomplished
through the maximization of the volume of the simplex defined by the rows of W. Based on concepts from distance geometry, the authors show that it is possible to efficiently compute the volume
of the simplex defined by a candidate W using  only. There are also several clustering algorithms
that only require a distance matrix to work, never accessing the vectors mi directly (Kaufman &
Rousseeuw, 1990). Finally, one can derive simple heuristics that use  to compute a matrix W that
covers as well as possible the convex set defined by M, ensuring that every row mi is close to
at least one representative vector wj . For example, one can adopt a constructive method that goes
through all vectors mi and successively adds new rows to the matrix W in order to guarantee that
min j  (mi , wj ) <  for all i, where  is a predefined threshold. Notice that in this case the number
of representative rows wj will be automatically determined by the value of  . This idea will be
further explored in Section 4.2.2.
The obvious advantage of computing DW  M based solely on  is that the problem of reducing
the computational cost of the factorization comes down to finding efficient ways of computing
 . Specifically, one can replace  with an approximation  whose computation requires fewer
arithmetic operations. One way to define  is to restrict the computation of  (mi , m j ) to a properly
defined subset of the elements of mi and m j which corresponds to enforcing some degree of
778

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

sparsity in M (see Mahoney, 2011). This section goes in another direction, though: it exploits the
fact that each vector mi is associated with a specific state-action pair which can often be represented
by a feature vector whose dimension is much smaller than |S|.
From dynamic programmings perspective, a state sk is nothing but an index k  {1, 2, ..., |S|}.
However, in an MDP describing a real decision problem, each state has a clear semantic interpretation. In general, a given state of the MDP can be represented by a set of features that are descriptive
of the real state of the problem. Based on the state features, it is usually easy to define feature
vectors that represent state-action pairs. Therefore, S  A can be thought of as a finite subset of a
vector space; with a slight abuse of notation (si , a) will be used to refer to the vectors representing
state-action pairs and dim(S  A) will denote the number of components of (si , a).
In many cases, it is reasonable to assume that state-action features provide some information
regarding the dynamics of the MDP. To be more precise, let  be a dissimilarity measure defined in
S  A. The assumption is that, if state-action pair (sk , a) is similar to state-action pair (sl , b), then the
associated transition probabilities and rewards should also be similarthat is, the distance between
the corresponding vectors mi and m j should be small. Thus, one can use  as a surrogate for  .
Note that this direct relationship between  and  will naturally happen when the MDP M is the
result of the discretization of a model with continuous state space, as long as the functions defining
the MDPs dynamics are reasonably smooth and the resolution of the discretization is sufficiently
fine. Moreover, often only the relative magnitude of the distance matters for the proper functioning
of the algorithms (Kaufman & Rousseeuw, 1990; Thurau et al., 2012). Therefore, an approximation
 ((sk , a), (sl , b))  C (mi , m j ), with C > 0, should suffice in many cases.
The strategy of replacing  with a dissimilarity measure  defined in the state-action space can
result in significant computational savings when dim(S  A)  |S| + 1. This is akin to the wellknown kernel trick, in which an algorithm is rewritten in terms of inner products which are then
replaced by an appropriately defined kernel (Scholkopf & Smola, 2002). The kernel trick allows
one to work in very high-dimensional feature spaces without ever explicitly computing the features.
Similarly, by adopting an algorithm based exclusively on  , and then replacing the latter by  , one
can compute the approximation DW  M without ever manipulating the vectors mi directly. This
strategy can be used with any algorithm that computes D and W based on  . As an illustration, the
next section describes a concrete algorithm that does just that.
4.2.2 A N A LGORITHM

TO

C OMPUTE

THE

FACTORIZATION

OF AN

MDP

IN

L INEAR T IME

The previous sections discussed several ways to address the stochastic factorization problem (Problem 1). Each approach has its advantages and drawbacks, and the decision about which method to
adopt should take into account factors like the size of the problem, the level of accuracy required
for the solution, and the computational resources available. This section describes in more detail a
specific algorithm to compute the approximation DW  M. The objective is to provide an illustration of how some of the ideas described above can be implemented, and also make the discussion
regarding computational and theoretical aspects of the factorization problem more concrete.
The proposed method, described in Algorithm 3, builds on the ideas discussed in Section 4.2.1.
The strategy is to select a subset of the rows of M to form matrix W, such that each row mi has
a representative vector wj within a predefined neighborhood. Such a neighborhood is induced by
a dissimilarity measure  defined in S  A. The mechanics of the method are very simple. It goes
over each state-action pair of the MDP and computes their distance to the  -closest neighbors in
779

fiBARRETO , P INEAU , & P RECUP

the set E of state-action pairs already selected to be part of the model (line 7 of Algorithm 3). If the
distance to the closest neighbor is above a predefined threshold  , the corresponding row of M is
added to W (lines 9 to 11). If not, the number of representative state-action pairs remains the same.
Regardless of whether the model has grown or not, the  -closest neighbors are used to compute
the elements in the zth row of D, where z is the index of the current state-action pair in M (lines 13
and 14). The elements di j can be computed by any function  that does not increase with  (see
discussion in Section 4.2.1 and equation (19) for an example).
Algorithm 3 Stochastic-factorization computation
MDP M
M  R|S||A||S|+1 and the corresponding features (si , a)
 : (S  A)  (S  A) 7 R similarity function
Require:  : R 7 R
non-decreasing function
  R+
neighborhood radius
  N
number of neighbors in the approximation
Ensure: Factorization DW  M
1: E{(s1 , 0)}
 E are the representative state-action pairs (|E| = m)
2: D0  R|S||A|1 ; Wm1  R1|S|+1
3: for i 1, 2, ..., |S| do
4:
for a 1, 2, ..., |A| do
5:
z(a  1)  |S| + i
 z is the row index of (si , a) in M (see (11))
6:
h min( , |E|)
7:
find the h nearest elements to (si , a) in E according to  ; call the jth closest pair (s, b) j
8:
if  ((s, b)1 , (si , a)) >  then
 A new representative state-action pair must be added
9:
EE+ {(s
,
a)}
i

W
10:
W m
 Add one row to W
z
11:
D[ D 0 ]
 Add one column to D
12:
for j1, 2, ..., h do
13:
k index of (s, b) j in W
14:
dzk  ( ((s, b) j , (si , a)))
15:

|E|

for j1, 2, ..., |E| do dz j dz j / l=1 dzl

 Make sure that D is stochastic

The parameter  has a strong effect on the output of Algorithm 3: decreasing its value usually
leads to a more accurate approximation DW  M, but it also increases the number m of representative state-action pairs (or, equivalently, representative vectors wj ). The algorithm can go through
the state-action pairs of the MDP in any order, and in the end every pair will have at least one representative counterpart within a distance of  in the space (S  A,  ). However, since Algorithm 3
is a greedy method, the set of state-action pairs selected to be part of the model may change depending on the order in which the pairs are visited. The parameter  determines the number of
representative vectors wj used to approximate each mi , and can be seen as a device to control how
local the approximation should be (this is akin to setting the parameter k of a k-nearest neighbor
approximation; see Hastie et al., 2002, Chap. 2, for an intuitive discussion). The parameter  also
has a direct effect on the computational cost of the algorithm, as discussed next.
The most demanding operation in each iteration of Algorithm 3 is the computation of the  nearest neighbors of the current state-action pair. There are several efficient algorithms available
780

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

to perform this search, either exactly or approximately (Liu et al., 2005). The most popular exact
method is to use a KD-tree, which takes O(dim(S  A)m log m) operations to be constructed and
allows the search to be performed in O( log m) time, on average (Friedman et al., 1977). Since
Algorithm 3 performs |S||A| iterations, its overall complexity is linear in |S|. Therefore, using
this algorithm to build the approximation DW  M, and assuming that the number of iterations
performed by PISF is much smaller than |S|, the cost of the entire process of computing a decision
policy depends only linearly on the number of states in the MDP. This is the best one can do without
assuming any extra structure in the model.
As for the space complexity of Algorithm 3, note that in an actual implementation the matrix M
is not really necessary, and neither W nor D must be explicitly stored: W can be represented by an
m-dimensional vector containing the indices of the representative state-action pairs and D can be a
data structure with the |S||A|   nonzero elements di j .
As mentioned, when Algorithm 3 terminates all state-action pairs of the MDP have at least one
representative state-action pair within a neighborhood of radius  defined in the space (S  A,  ).
In order to extrapolate this guarantee to the performance of PISF, it is necessary to relate  to  . Let
(sk , a) and (sl , b) be two state-action pairs associated with vectors mi and m j , respectively. Then, if

 ((sk , a), (sl , b)) <  =  (mi , m j ) <  ,

(20)

it should be relatively straightforward to provide guarantees regarding PISFs solution. For example,
if (17) is adopted and  = 1, one can resort to Proposition 3 to obtain such guarantees. There are
specific scenarios where it should be easy to ensure that assumptions like (20) hold, such as for
example when the MDP M results from the discretization of a continuous model. It may also be
possible to derive guarantees similar to (20) based on knowledge about the problem, for example by
looking at the transition equations describing the dynamics of the MDP. In general, though, it may
be difficult to relate  to  . Note that it is trivial to modify Algorithm 3 to reflect the case in which
 is computed based on a subset of the elements of mi and m j . In this case deriving guarantees
analogous to (20) is considerably easier (see for example Mahoney, 2011).
Algorithm 3 relies on two premises: (i) given M and W, making the elements di j inversely
proportional to  (mi , wj ) results in a good approximation DW  M; (ii) it is possible to define
a dissimilarity measure  : (S  A)  (S  A) 7 R such that  ((sk , a), (sl , b))  C (mi , m j ), with
C > 0, where mi and m j are the rows of M associated with (sk , a) and (sl , b), respectively. It is
important to point out that building an algorithm based on such assumptions is only one possible
artifice to circumvent the computational cost of factoring an MDP. Other strategies may be possible,
such as resorting to domain knowledge or developing methods that exploit some structural regularity
of the MDP (see Section 5.3.3). In fact, there are scenarios where an exact factorization is readily
available without the need for any computation (see Barreto, 2014, for an example). In any case, the
next section shows empirically how Algorithm 3 can generate very good decision policies in some
problems.

5. Computational Experiment
This section illustrates how PISF can be useful in practice with a real-world application of significant
economical interest.
781

fiBARRETO , P INEAU , & P RECUP

5.1 The Multicomponent-Replacement Decision Task
One of the big challenges faced by industry is the maintenance of assets over a long period of time.
For example, a commercial airline or a cargo company must have an operational fleet, while a power
company needs to maintain its electric power grid functioning at all times (Powell, 2007). In many
cases, the maintenance of expensive equipments involves sums of money counted in the millions
of dollars. In such situations, the decisions made during the maintenance activities may have an
enormous economical impact.
Usually, an equipment such as a jet engine or an electric generator is composed of several components that degrade over time. If a critical component breaks, it has to be replaced immediately. It
is often the case that such a maintenance operation has an associated setup cost which is independent of the number of components being replaced. This may be financial losses caused by the down
time or costs associated with activities such as the disassembling of the equipment, delivery of new
components, and displacement of specialists. Thus, in some circumstances, it may be advantageous
to perform opportunistic replacements of functioning components to avoid future setup costs. As
discussed, this trade-off between immediate and future costs is typical of decision making tasks.
The importance of the decision problem described above is attested by a huge body of literature
originated in the 1960s and spanning the subsequent four decades (Barlow & Proschan, 1965;
McCall, 1965; Pierskalla & Voelker, 1976; Sherif & Smith, 1981; Cho & Parlar, 1991; Dekker
et al., 1997; Wang, 2002). The problem can be formalized as follows. Suppose that the asset of
interest has nc components and let l j  N+ denote the expected lifetime of component c j measured
in some discrete time unit. Then, each state si is a vector si  Nnc whose jth entry si j represents the
remaining lifetime of component c j . At each time step the remaining lifetime of c j is decreased by
one, and si j = 0 indicates that this component is no longer operational. Even if c j has not reached
the end of its useful life, it may fail with a given probability, which is a function of si j and possibly
of the other components remaining lifetimes siu . An inactive component c j causes the entire asset
to stop working, and if c j is not replaced immediately a penalty of  takes place in the next
transition. To avoid that, one must replace c j , which incurs a cost of r j dollars. On top of that,
every replacement activityjoint or nothas an associated setup cost. The setup cost is composed
of two terms, a fixed amount of Rs dollars plus an extra fee of R f dollars which is only charged
if a component has failed before reaching its expected lifetime. The extra fee covers the expenses
of last-minute measures required by unexpected failures. Let action a be represented by a binary
vector ah  {0, 1}nc where ah j = 1 indicates that component c j should be replaced. The goal of
the decision maker is to select an action ah at each time step in order to minimize the expected
discounted future cost.
As one can see, this problem suffers from a particularly severe version of the curse of dimensionality: not only does its state space grow fast with nc , with |S| = i (li + 1), but also the cardinality
of the action space is an exponential function of this variable, since |A| = 2nc . Thus, even instances
of the problem with only a small number of components already represent a difficult challenge for
dynamic programming.
Given this scalability issue, researchers usually focus on particular cases of the multicomponentreplacement task whose structure can be exploited somehow. For example, it has been noted in the
literature that when the failure cost R f is not considered the state space of the multicomponentreplacement problem can be reduced in more than 50% by simply eliminating the states of S in
which all components are functional (i.e., si j > 0 for all j). Since one knows that the optimal action
782

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

in the excluded states is to replace zero components, this modification does not change the optimal
policy of the problem (see Sun et al., 2007; Arruda & Fragoso, 2011). Note though that when
R f > 0 it may be advantageous to carry out replacements even if no components have expired; as
will be seen, in this case the reduction strategies cited above may fail.
Researchers and practitioners have also explored other constrained versions of the multicomponent-replacement task. For example, if the replacement costs r j and the lifetimes l j are the same for
all components c j , all permutations of a given vector si can be considered as being the same state,
which reduces the state space considerably (Haurie & LEcuyer, 1982). Another simplification
of the problem is to assume that the probability of component c j failing is independent of the
other components remaining lifetimes siu . In this case, the components are related only through
the economical dependence represented by the setup cost (Cho & Parlar, 1991). The resulting
model, a factored MDP, can be solved orders of magnitude faster than a conventional MDP (see
Section 5.3.3). It is also possible to reduce the action space of the multicomponent maintenance
task by making some assumptions regarding the problems dynamics (Xia et al., 2008).
Although the methods above can be very effective in particular instances of the maintenance
task, they are not directly applicable to the most general version of the problem, in which components have different characteristics and depend on each other both economically and structurally.
Hence, in practice industry often relies on simple threshold policies that replace all components
with remaining lifetime above a given value (Haurie & LEcuyer, 1982; van der Duyn Schouten
& Vanneste, 1990). Unfortunately, it is known that, in general, the optimal policy for the multicomponent-replacement problem does not lie in the space of threshold decision policies (Ozekici,
1988; Xia et al., 2008).
5.2 Experimental Setup
The experiments described in this section were carried out assuming a general version of the maintenance task in which components interact with each other. The failure probability of component c j
was modeled as a linear function of the assets general condition. Suppose the asset is in state si and
the decision maker decides to perform action ah . Then, denoting the next state by sk , the probability
of component c j failing is:
P(sk j = 0 | si j > 1, ah j = 0) = f 

( f  fmin )(si j  1)  u6= j (lu  siu )
,
+f
lj  1
u6= j lu

(21)

where f , fmin and f are parameters of the model and it is assumed that l j > 1 for all j. The motivation
for equation (21) is that the probability of a given component failing should depend not only on the
condition of the component itself, but also on the condition of all other components of the asset.
This is in fact a generalization of a commonly used model of the problem; when f = fmin and f = 0,
equation (21) reduces to the fixed failure probability often assumed in the literature (Sun et al.,
2007; Arruda & Fragoso, 2011). In the experiments, equation (21) was considered with f = 0.1,
fmin = 0.01, and f = 0.1. Thus, if many components in the asset are about to expire, the probability
of component c j failing can more than double.
The formulation of the problem considered here is also general with respect to the characteristics of the individual components of the asset. Instead of assuming that all components have the
same lifetime and cost, which seems unrealistic, the variables l j and r j were drawn from normal
distributions with means l = 10 and r = 10 and common standard deviation lr = 3 (the values
783

fiBARRETO , P INEAU , & P RECUP

sampled for l j were rounded to the closest natural number and sampled again in case the result was
smaller than 2). With this configuration, the expected value of the models parameters coincide with
the ones adopted by Sun et al. (2007). The constant term of the setup cost was fixed at Rs = 10
and the failure cost was set as R f = 5nc . As discussed above, when R f > 0 it might happen that
  (si ) 6= 0 even if si j > 0 for all j. This increases the effective size of the state space, since one
cannot simply discard states without expired components.
The multicomponent-replacement task was modeled as a discounted problem with  = 0.999
(as discussed in Puterman, 1994, in this case  can be seen as a way of emulating the devaluation of
money). For each value of nc  {2, 3, ..., 7}, 100 instances of the problem were randomly generated.
The policy iteration algorithm was then used to find an optimal policy for the resulting MDPs (see
Appendix A.3). As a means of comparison, policy iteration was also applied to the reduced version
of the problem, in which a state si is removed from the MDP if and only if si j > 0 for all j. Note that
such a modification requires the recalculation of the transition probabilities between the states that
remain in the model (Arruda & Fragoso, 2011). Here, in order to accomplish this reduction, actions
were replaced with their temporally extended counterparts, options, giving rise to a semi-MDP
(options are closed-loop policies with a well-defined termination condition and an initiation set; see
Sutton et al., 1999, for details) .
In order to evaluate the heuristics usually adopted by industry, threshold policies using values
ranging from 1 to 10 were considered. For each value of nc , these policies were compared and
the one providing the lowest expected cost was used in the comparisons with the other algorithms.
Notice that a threshold policy using a value of 0 corresponds to the naive strategy of only replacing non-operational components. The performance of such a policy was used as a baseline in the
comparisons.
As discussed in Section 4, PISFs configuration comes down to the definition of matrices D 
R|S||A|m and W  Rm|S|+1 . In the experiments of this section these matrices were computed by
Algorithm 3. The following function was used as a similarity measure between state-action pairs:

 ((si , ah ), (sk , ag )) =



 if ah 6= ag ,
nc
(1  ah j )|r j |(si j  sk j )2 otherwise.
 j=1

(22)

The intuition behind (22) is straightforward: two state-action pairs should be considered similar
if, after the application of the actions to the corresponding states, the remaining lifetimes of the
components are approximately the same (except for eventual failures). Note that the difference
between the remaining lifetimes si j and sk j is weighed by the magnitude of the cost r j of replacing
the jth component. The other parameters of Algorithm 3 were defined as follows. The number of
neighbors  used in the approximation was set to nc , the function  was defined as the constant
1/ , and the neighborhood radius  was varied in {200, 400, 600}. Since  was the only parameter
that varied across the experiments, the specific instances of PISF will be referred to as PISF- (see
Appendix A.3 for more details regarding the implementation of PISF).
5.3 Results
Throughout this section, the following measure will be used to evaluate the algorithms:

 (v , v | S) =

vi  vi
1
,

|S| {i | s S} |vi |
i

784

(23)

fiP OLICY I TERATION BASED

S TOCHASTIC FACTORIZATION

0.15

0.16

0.17

PI
PIRED
BEST THR
PISF200
PISF400
PISF600

0.12

0.13

0.14

(v, vN)

ON

2

3

4
5
6
Number of components (nc)

7

Figure 2: Expected gain on the multicomponent-replacement problem with respect to the naive
decision policy. Error bars represent one standard error over 100 runs.
where v is the value function of the policy  returned by the algorithm under evaluation, v is a
reference value function, and S  S is the set of test states used in the evaluation (note that  can
take on negative values).
5.3.1 S TANDARD S OLUTIONS
The two most natural ways of addressing the multicomponent-replacement problem is to use dynamic programming or to resort to the threshold policies normally adopted in practice. This section
compares these methods with PISF. The performance measure used to evaluate the algorithms is
the relative gain one should expect when using them instead of naively replacing a component only
when it fails. Hence, the reference function v in (23) is the value function of the naive policy, vN .
When nc  5, it is possible to compute the value functions v and vN exactly, and thus in this
case the set S appearing in (23) is the entire state space S. However, for nc > 5 computing v and
vN becomes infeasible in the computers used for the experiments. In this case S was composed of
10, 000 test states si sampled uniformly at random from S, and the corresponding values vi and vi N
were approximated through Monte Carlo roll-outs of length 5, 000.4
Figure 2 compares the results of policy iteration (PI), policy iteration applied to the reduced
version of the problem (PI-RED), the best threshold policy (BEST THR), and PISF using  = 200,
 = 400, and  = 600. One thing that immediately stands out in the figure is the fact that the performance of the optimal policies improves as the number of components nc increases. This indicates
that the financial losses of a company that does not make opportunistic replacements increase with
the number of components in the asset.
4. The roll-outs were truncated at a point in which the value of the rewards (in the case of the current application,
dollars) has already decreased in more than 99% (that is,  5000 < 0.01).

785

fiBARRETO , P INEAU , & P RECUP

THR policies perform better than the naive policy in general. Notice though that the results
shown in Figure 2 correspond to the performance of the best THR policy selected independently for
each value of nc . So, for example, while the average gain provided by the best THR policy when
nc = 5 is 13.36%, the average gain of the worst policy of this type is only 3.77%. This means that,
in order to achieve the level of performance shown in Figure 2 in a real application, one cannot
arbitrarily pick one specific THR policy. Rather, it is necessary to have a model of the problem
and be willing to systematically compare all possible threshold values. Even in this case, the resulting policy will be suboptimal. For example, when nc = 5, making optimal decisions increases
the expected profit  of the best THR policy in 27.8%, on average (see Figure 2). Considering the
amounts of money often involved in maintenance activities, such a difference can represent significant monetary gains. This is a good illustration of the impact that dynamic programming may have
in practice.
Unfortunately, dynamic programming does not scale well with the number of components in
the multicomponent-replacement task. As mentioned above, in the experiments described here the
optimal policy could not be computed for instances of the problem in which nc > 5. Since in
standard dynamic programming algorithms there is no easy way to control the amount of memory
used, one is left with very few alternatives. In the case of the multicomponent-replacement task, one
possibility is to eliminate states si > 0 and apply dynamic programing to the reduced MDP. Such a
strategy reduces the state space considerably, which makes it possible to solve MDPs one order of
magnitude bigger (see Figure 2). Although the technique works well with the version of the problem
considered by Arruda and Fragoso (2011), it may fail on the version of the problem considered here,
in which the optimal policy may carry out opportunistic replacements in states without expired
components. This is illustrated by the PI-RED curve in Figure 2, which clearly shows that the
optimal policies of the reduced MDPs do not perform optimally in the original models.
PISF represents an alternative between the two extremes of computing an optimal policy and
resorting to simple heuristics such as the threshold policies. In contrast with conventional dynamic
programming algorithms, PISF provides a practical mechanism to control the trade-off between the
computational resources used and the quality of the resulting decision policy (the order m of the
stochastic factorization, here indirectly defined by the neighborhood radius  ). This point is better
illustrated when Figure 2 is analyzed in conjunction with Figure 3, which shows the size of the
models generated by each algorithm and the associated time needed to solve them.
The dimension of the matrices processed by the algorithms during the value function computation is defined by the number of states in the corresponding Markov process. Thus, this number is a
good measure of the algorithms time and space complexities. In the case of policy iteration, what
defines the size of the Markov processes is the number of states in the MDP; for PISF, such a number is m, the order of the stochastic factorization. The size of the Markov processes generated by
each method is shown in Figure 3a. The figure makes it clear how the amount of memory required
by the methods restricts their application. For example, if PI-RED was not run in the MDPs with
nc = 7 components, it is because the resulting Markov processes would be bigger than the largest
model shown.
As expected, the size of the Markov processes generated by PISF decreases with  . Take the
MDPs with nc = 5, for instance. In this case the average reduction on the original models provided
by PISF was 60% when  = 200, 88% when  = 400, and 95% when  = 600. When analyzing
these numbers, one should keep in mind that the computational cost of evaluating a decision policy
is cubic in the size of the Markov process. Thus, if the value functions are computed exactly, a
786

fi1e+06

P OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

15
5
0

2e+05
0e+00

PI
PIRED
PISF200
PISF400
PISF600

Hours
10

Size
4e+05 6e+05

8e+05

PI
PIRED
PISF200
PISF400
PISF600

2

3
4
5
6
Number of components (nc)

7

2

(a) Size of the Markov processes

3
4
5
6
Number of components (nc)

7

(b) Run time (PISFs values include the time to compute the MDP factorization through Algorithm 3)

Figure 3: Size of the models generated by the algorithms and the associated time needed to compute
a solution. Error bars represent one standard error over 100 runs.
reduction of 88% in the models size translates to a decrease of 99, 82% on the number of arithmetic
operations performed at each value-function computation.
Of course, in a real application the value function is seldom computed exactly. Besides, in
order to use PISF one has to compute the factorization of the MDP. Even considering these factors,
replacing PI with PISF can result in significant time savings. This is illustrated in Figure 3b, which
shows the actual run times of the algorithms. For a concrete example, take again the MDPs with
nc = 5 components. In this case, adopting PISF-600 instead of PI results in a 97% reduction of
computing timewhich is equivalent to saying that the former algorithm is 37 times faster than
the latter. To put this number in perspective, if it were possible to run PI in the MDPs with nc = 7
components, finding a decision policy would take over 8 days. PISF-600 was able to compute an
approximation in less than 5 hours.
But the reduction on the computational cost and memory usage provided by PISF are meaningless if the resulting decision policies perform poorly. Comparing Figures 2 and 3, one can clearly
see that this is not the case in the multicomponent-replacement problem. For example, by replacing PI with PISF-400, the average reduction on computing time is over 90% for all values of nc .
In contrast, the expected decrease on the profit  is below 6.2%. If one adopts PISF-600 instead
of PISF-400, the reduction on the computational cost is at least 94%, for all values of nc , while
the associated losses are below 9.5%. This illustrates how PISFs performance degrades gracefully with the quality of the MDPs factorization, as indicated by the theoretical results presented in
Section 3.3.
There is however a clear decrease on the expected gain provided by PISF when nc increases from
6 to 7, as shown in Figure 2. One possible explanation for this is the fact that in the multicomponentreplacement task increasing the number of components in the asset increases not only the size of
the state space S, but also the number of actions in A. This means that both the dimension and
787

fiBARRETO , P INEAU , & P RECUP

the number of matrices Pa increase exponentially with nc , making it harder to summarize all
the information in a single matrix K. On top of that, and perhaps more important, by keeping the
neighborhood radius  fixed, the relative size of the models generated by PISF actually decreases.
For example, for PISF-400, the average ratio m/|S| is equal to 0.23 when nc = 3, 0.12 when nc = 5,
and only 0.06 when nc = 7. Thus, in order to keep the relative size of m fixed,  must increase with
nc . In any case, even in the MDPs with nc = 7 components PISF clearly outperforms the best THR
policy, which is the only feasible alternative among the methods considered in this section. The next
sections investigate other approximate solutions for the multicomponent-replacement problem.
5.3.2 U PPER C ONFIDENCE B OUNDS

FOR

T REES (UCT)

It is only possible to compute an optimal policy for the multicomponent-replacement problem if
the number of components in the asset is below a certain threshold defined by the computational
resources available. Above this threshold, the standard solution adopted by industry is to resort to
simple heuristics. The previous section presented PISF as an alternative solution that allows for
some control on the trade-off between the computational resources used and the quality of the resulting policy. It is only natural to ask whether other approximate methods would also be applicable
in this case.
Two popular approaches for approximately solving an MDP are value-function approximation
and state aggregation. These techniques are closely related to the stochastic-factorization trick, and
will be discussed in Section 6. This section will focus on a set of methods collectively known as
Monte-Carlo tree search (MCTS). MCTS methods are capable of computing approximate solutions
for very large MDPs by combining tree search and random sampling (Browne et al., 2012). These
methods have been receiving a lot of attention recently due to their enormous success in several
applications, most notably in the game of Go (Bouzy & Helmstetter, 2003). MCTS algorithms use
Monte Carlo roll-outs to estimate the return associated with the actions available at the current state.
In order to do so, they build a tree, rooted at the current state, whose structure reflects the transitions
performed during the roll-outs.
The main feature that distinguishes the different MCTS algorithms is the strategy used to select
actions during the roll-outs. The most popular MCTS algorithm is Kocsis and Szepesvaris (2006)
upper confidence bounds for trees (UCT). In UCT the action selection process is interpreted as
a multi-armed bandit problem in which each arm corresponds to an action (Auer et al., 2002).
Intuitively, UCT works by adding an exploration bonus to the values of actions that have been
tried less often. One of the main parameters of the algorithm is the exploration constant C p used to
weigh the bonus against the value of actions (Kocsis & Szepesvari, 2006).
UCT has nice theoretical properties: given some  > 0, it is guaranteed to return an  -optimal
action if the depth of the tree and the number of roll-outs are large enough (Kocsis & Szepesvari,
2006). Therefore, given enough computation, UCT will always be able to perform at the same level
as PISF (and eventually better, if PISFs policy is not optimal). An interesting question in this case
is how much computation is needed in practice for that to happen.
In order to answer this question, an experiment was carried out in which UCT and PISF were
compared in the multicomponent-replacement problem. The algorithms were evaluated on the 100
MDPs with nc = 5 components described in Section 5.2. As before, the performance of the resulting
policies was contrasted with that of the naive policy N . The comparisons were based on a single
roll-out starting from a state selected uniformly at random and independently for each MDP. More
788

fiON

S TOCHASTIC FACTORIZATION

0.2

P OLICY I TERATION BASED

0.2
0.4

UCT

0.6

(v, vN)

0.0

PISF600

1

3

5
7
9
11
Seconds per step (tmax)

13

15

Figure 4: Expected performance on the multicomponent-replacement problem with respect to the
naive decision policy. Error bars and shadow represent three standard errors over 100 runs.
specifically, the set of test states S appearing in (23) was composed of a single state si , and, as
before, the corresponding values vi and viN were approximated through a Monte Carlo roll-out of
length 5, 000.5
As described above, before each action selection UCT builds a tree based on Monte-Carlo rollouts. As the number of roll-outs grows, the quality of the value function approximation increases,
but so does the computational cost of the algorithm. Thus, one way of evaluating UCT is to impose a
time limit tmax on each iteration of the algorithm and check the performance of the resulting policy
as a function of tmax . Note that, given a fixed time budget tmax , there is a trade-off between the
number of roll-outs carried out and their length, and finding the right compromise between these
two corresponds to finding the right balance between the bias and the variance of the value function
approximation (Hastie et al., 2002). Here this issue was resolved by changing the maximum depth
of the tree, hmax , in the set {5, 50, 500, 5000}.6 Also, UCTs exploration parameter C p was varied
in {100 , 101 , 102 , 103 }. Therefore, for each MDP and each value of tmax , UCT was run 16 times
corresponding to all possible combinations of values for hmax and C p , and then the best result was
selected and compared to that of the naive policy through (23). Figure 4 shows the average value of
 (v , vN ) as a function of tmax . As a reference for comparison, the average result of PISF-600 on
the same MDPs is also shown.
As shown in Figure 4, UCT performs worse than the naive policy N for tmax  15. Assuming the
logarithmic trend shown in the figure will persist for tmax > 15, UCT would need over 46 seconds
per step to reach the level of performance of N . In order to find solutions comparable to those
found by PISF-600, UCT would require around 115 seconds per step, or approximately 1.74 times
the time needed by the former algorithm to compute a decision policy for the entire state space.
5. The policies were evaluated from a single test state only because the nature of the UCT algorithm makes it computationally impractical to carry out many roll-outs of length 5, 000. In any case, the small variance of (23) across the
MDPs indicates that this decision did not have a strong effect on the comparisons.
|S|
6. Since |(vmax  vmin )/v | < 0.002 for all MDPs, where v = 1/|S| i=1 vi , the value of leaf nodes was set to zero.

789

fiBARRETO , P INEAU , & P RECUP

Note that, even though a time limit of tmax = 15 seconds per step may not seem like much at first,
the run time of UCT quickly escalates with the horizon of the problem. This becomes clear when
one observes that a trajectory of 5, 000 steps with UCT using tmax = 15 takes more than 20 hours,
while computing the optimal decision policy for the same problem takes an average of 34 minutes.
It should be mentioned that the most basic version of UCT was used in the experiments of this
section. Nowadays there are several extensions available in the literature (see for example Chaslot
et al., 2008; Gelly et al., 2012; Keller & Eyerich, 2012). These strategies, when built on top of
UCT, tend to improve its performance. That said, the fact that UCT needs orders of magnitude
more computation than PISF to reach the same level of performance is not surprising, since the
former only uses the MDP as a generative modelthat is, the algorithm only samples from the
conditional distributions represented by the transition matriceswhile the latter fully exploits all
the information available in the model.
5.3.3 FACTORED MDP S
The previous section illustrated how exploiting all the information available about a problem can
be important for computational efficiency. This section shifts the focus of the discussion to another
issue, namely that of the structure of the model. In particular, it describes a specific structured model
known as factored MDPs.
In a factored MDP the transition dynamics can be described by a dynamic Bayesian network
(DBN, Dean & Kanazawa, 1989; Boutilier et al., 1995). Let s(t) and s(t+1) denote, respectively, the
state at the current time and at the next step. The transition graph of a DBN is a two-layer directed
(t+1)
(t+1)
(t)
(t+1)
(t)
acyclic graph whose nodes are the variables in the set S = {s(t)
1 , s2 , ..., sn , s1 , s2 , ..., sn }. An
(t)
(t+1)
arc from s j to si indicates that the value of the ith variable at time t + 1 depends on the value of
the jth variable at time t. Let  (si(t+1) ) denote the set of all nodes with an arc to si(t+1) . The transition
model of a factored MDP is given by P(s(t+1) |s(t) ) = i P(si(t+1) | (si(t+1) )). What allows a compact
representationand efficient solutionof a factored MDP is the assumption that the DBN is sparse,
that is,  (si(t+1) ) is restricted to a small subset of S for all i. In addition, it is assumed that the reward
is given by a summation of functions that depend on the status of only a few variables. When these
assumptions hold, that is, when the MDP is truly factored, it is possible to represent the models dynamics very compactly using structures like decision trees or algebraic decision diagrams (Boutilier
et al., 1995; Hoey et al., 1999). Therefore, the number of states in the MDP remains the same; the
computational gain comes from the fact that the state space is never explicitly enumerated.
At first, one might expect that the structure of a factored MDP would induce a value function
with similar structure. If this were indeed the case, it would be possible to compute an optimal policy
for a factored MDP much faster than dynamic programming algorithms that ignore the models
special structure. Unfortunately, Koller and Parr (1999) have shown that, in general, the value
function of a factored MDP is not factored. Therefore, practical algorithms developed for factored
MDPs only compute an approximate solution (see Guestrin et al., 2003, and references therein).
The factored MDP model captures the dynamics of many real-world decision-making tasks (Powell, 2007). In fact, it is safe to say that some MDPs solved using this technique are among the largest
solved to date (Guestrin et al., 2003, see also Section 5.4). However, there are decision problems
of great interest whose dynamics do not satisfy the assumptions of such models. The version of the
multicomponent-maintenance problem addressed here is a good example: since the probability of
a given component failing depends on the condition of all other components, the resulting DBN is
790

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

not sparse (cf. equation (21)). But what happens if one ignores the fact that an MDP is not truly
factored and applies the algorithms developed under such an assumption anyway?
To help answering this question, an experiment was performed in the following way. Let M
be one of the MDPs with nc = 5 components described in Section 5.2. For each component c j
and each value of z  {1, 2, 3, 4}, a set z (c j )  {1, 2, 3, 4, 5}  { j} containing z distinct elements
was generated uniformly at random. Then, for each value of z, the transition matrices of M were
regenerated with (21) replaced by
( f  fmin )(si j  1)  u/ z (c j ) (lu  siu ) + uz (c j ) (lu  lu /2)
,
+f
lj  1
u6= j lu
(24)
giving rise to the MDP Mz . Using (24) corresponds to enforcing the sparsity of the DBN describing
the components failure probabilities. Note that, instead of completely ignoring the influence of the
components cu with u  z (c j ), their remaining lifetimes was replaced by lu /2, the middle point
of the range of possible values. This is a way of making the factored model closer to the original
one without incurring in a denser DBN. After the transition matrices of Mz were generated, policy
iteration was used to compute an optimal policy, z (note that this computation is exact). Finally,
the performance of z in the original MDP M was compared to that of   , an optimal policy of M
(thus, in this experiment the reference value function v appearing in (23) is v , and S = S). This
entire process was repeated for each one of the 100 MDPs with nc = 5 components.
Figure 5a compares the performance of z , the policies returned by policy iteration on the
artificially-factored MDPs (PI-FAC), with that of PISF-200, PISF-400, and PISF-600. Observe how
the performance of PI-FAC degenerates as the level of sparsity z of the artificially-factored MDPs
increases. This is expected, since the derived models deviate from the original ones as z approaches
nc . Since the computational cost of the algorithms developed for factored MDPs decreases with
z, one has to trade performance for speed when choosing which interactions between variables to
ignore. But the important point to note here is the fact that PI-FAC is outperformed by all instances
of PISF when z > 1. Since z is an optimal policy of the factored MDPs, its performance is the
best one can reasonably hope for when using algorithms that approximate it. In other words, a
factored MDP algorithm using (24) will outperform PISF on the multicomponent-replacement task
only if the approximation it computes luckily induces a policy that performs better than z in the
original, non-factored, MDP.
This experiment shows that, by pretending that a non-factored MDP is factored, one may
severely restrict the quality of the resulting policy, regardless of the specific algorithm chosen to
solve the factored MDP. There is no reason to believe that this phenomenon is restricted to the
multicomponent-replacement domain. Thus, there is a niche of problems that are too big to be
solved by standard dynamic programming and cannot be reliably solved by algorithms developed
for factored MDPs. For these problems, it might be a good alternative to resort to algorithms that
exploit other types of structure, such as PISF.
This is not to say that the stochastic-factorization trick should be seen as an alternative to factored MDPs. In principle, the properties of an MDP being factored or factorizable are orthogonal to each other, and therefore both structures can potentially be exploited in conjunction. To
illustrate this point, an experiment was carried out in which PISF and the THR policies were run in
the factored MDPs Mz . Note the difference with respect to the previous experiment: while there the
models Mz were considered as approximations of the MDPs M, here it is assumed that they are the
true models. Thus, both PISF and the THR policies were compared to the optimal policies z . The
P(sk j = 0|si j > 1, ah j = 0) = f 

791

fiBARRETO , P INEAU , & P RECUP

PISF400

2
3
Level of sparsity (z)

0.14

0.05

PIFAC

1

0.06
0.10

0.03

(v, vz*)

0.02

PISF600

0.04

(v, v*)

0.02

0.01

PISF200

4

(a) Artificially-factored MDPs

BEST THR

PISF 200
PISF 400
PISF 600
1

2
3
Level of sparsity (z)

4

(b) Truly-factored MDPs

Figure 5: Expected loss on the multicomponent-replacement problem with respect to the optimal
decision policy. Error bars and shadows represent one standard error over 100 runs.

idea is to investigate how the structure of a transition matrix induced by a sparse DBN affects PISFs
performance. Figure 5b shows the performance of PISF and the best THR policy on the factored
MDPs Mz as a function of the level of sparsity z (cf. equation (24)). Although the performance of
PISF degenerates slightly when z increases, the relative performance with respect to the best THR
policy generally improves as the model gets more sparse.
This experiment illustrates how the stochastic-factorization trick can potentially be useful in
the computation of a policy for a factored MDP, which may lead to the solution of very large
sequential decision problems. Of course, in order to simultaneously exploit the factored and
factorizable structures of a model, one has to apply the trick without explicitly manipulating the
matrices involved. This constitutes an interesting extension of the current research, and is left as a
suggestion for future work.
5.4 Discussion
This section described the application of PISF to a large problem of real interest. It was shown that,
by using Algorithm 3 to compute D and W, it is possible to handle MDPs whose exponentially large
state and action spaces preclude the use of standard dynamic programming. The largest instance
of the problem solved by PISF had 39, 916, 800 states and 128 actions, or 359 times the size of the
largest MDP solved by policy iteration. Note that in the maintenance problem studied here it is
trivial to define heuristics that perform reasonably well, but in other applications this may not be
true. For example, Dekker et al. (1996) point out that the multicomponent-replacement problem
has a structure similar to that of other important decision-making tasks arising in production and
inventory control. In such scenarios PISF can be an even more valuable tool.
792

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

It should be mentioned that the experiments of this section considered the classical scenario of
dynamic programming, in which one seeks a decision policy defined over the entire state space S.
Other formulations of the decision problem are possible. For example, in the field of automated
planning it is often assumed that the decision maker must perform well from a small number of
initial states only, and that at any state si  S only a small set of actions A(s)  A is available (Ghallab
et al., 2004; Geffner & Bonet, 2013). On top of that, it is sometimes assumed that the dynamics
of the MDP are truly factored (Sanner, 2010). When all these assumptions hold, it is possible for
the decision maker to perform well visiting only a small fraction of the state space, and, since the
MDPs dynamics are factored, sometimes not even this small subset must be explicitly enumerated.
Therefore, algorithms developed for this scenario can be applied to very large MDPs (Keller &
Eyerich, 2012; Kolobov et al., 2012).
PISF is not a direct competitor of planning algorithms developed for the scenario above because
it was not designed with such assumptions in mind. Since PISF is an offline method that computes
a policy for the entire state space, it will never scale to MDPs as large as those handled by on-line
methods that compute a policy on demand, such as UCT. Similarly, an MDP with truly factored
dynamics should favor methods that exploit this structure. On the other hand, as the experiments of
this section show, when the assumptions that underlie other planning algorithms do not hold, they
can be largely outperformed by PISF.
In the end, what determines the success of a given algorithm is the suitability of its underlying
assumptions to the scenario of interest. PISF has been developed for MDPs that are factorizable or
nearly soa structural regularity not exploited by any other dynamic programming algorithm. How
often this structure arises in problems of interest and how well it can be exploited together with other
assumptions, such as those usually considered in automated planning, are interesting questions to
be addressed in future research.

6. Related Work
This section reviews some of the approaches that have been proposed to circumvent dynamic programmings curse of dimensionality, and discusses how these methods relate to the stochasticfactorization trick.
6.1 Value-Function Approximation
Perhaps the most straightforward approach to deal with a large MDP is to use a compact parametric
representation of its value function. This is not a new idea; in fact, Bellman and Dreyfus explored
the use of polynomials to approximate the value function in as early as 1959. Since then the theory
has evolved a lot, and nowadays it is possible to find books that cover the subject in detail (Bertsekas
& Tsitsiklis, 1996; Powell, 2007).
The main issue regarding the use of a parametric representation of the value function is the damaging effect it may have on dynamic programming algorithms. In particular, it is well known that the
use of general approximators may cause instabilities or even the divergence of the algorithms (Boyan
& Moore, 1995; Baird, 1995; Tsitsiklis & Roy, 1996, 1997). The most common strategy to deal
with this problem is to restrict the structure of the approximator to compact representations with a
linear dependence on the parameters (Tsitsiklis & Roy, 1997; Tadic, 2001; Schoknecht & Merke,
2003). Indeed, the use of linear approximators has led to a number of successful algorithms with
793

fiBARRETO , P INEAU , & P RECUP

good convergence properties (Tsitsiklis & Roy, 1996; Bradtke & Barto, 1996; Perkins & Precup,
2003; Lagoudakis & Parr, 2003; Sutton et al., 2008).
There is some evidence in the literature that the instability caused by some function approximators is related to their tendency to exaggerate the difference between two successive estimates of the
value function (Thrun & Schwartz, 1993; Gordon, 1995; Ormoneit & Sen, 2002). For this reason,
many researches have advocated the use of conservative function approximators that compute the
value of a state as a weighted average of other states values (Gordon, 1995; Tsitsiklis & Roy, 1996;
Rust, 1997; Munos & Moore, 1999; Ormoneit & Sen, 2002; Szepesvari & Smart, 2004). Examples
of such approximators include kernel averaging, linear interpolation, k-nearest neighbor, and some
types of splines. In general, the combination of these approximators with dynamic programming
leads to convergent algorithms (Gordon, 1995; Tsitsiklis & Roy, 1996).
Conservative function approximators are similar in nature to the stochastic-factorization trick.
To see why this is so, consider the class of function approximators which Gordon (1995) called
averagers. An averager approximates the value of a state by a convex combination of other states
values and possibly some predetermined constants. Given an MDP M  (S, A, P, R,  ), let S be a
subset of the state space S, with |S| = m, and let v  Rm represent the values of the states in this
subset. An averager would compute an approximation of the value function as
m

vi = di0 ci +  j=1 di j v j ,

(25)

with ci  R, di j  R+ and mj=0 di j = 1. Since in the approximation scheme above the values of all
states can be determined from the values of the states in S, only the latter must be updated during
dynamic programmings iterative process.
For the sake of simplicity, suppose that there is an averager for which dio = 0 for all i 
{1, 2, ..., |S|}. In this case, approximate dynamic programming using (25) corresponds to its exact
|S|
version in a reduced MDP M  (S, A, P, R,  ) where Pa (s j |si ) = k=1 paik dk j and ra (si ) = ra (si ) (Gordon, 1995, Thm. 4.1). This model represents a particular case of the stochastic-factorization trick
in which there is a single matrix D  R|S|m and one matrix Ka  Rm|S| for each a  A (Barreto
et al., 2011). The dynamics of the reduced model are given by Pa = Ka D, where Ka is the matrix
composed of the rows of the original transition matrix Pa  R|S||S| associated with the states si  S.
By interpreting conservative approximators as a particular case of the stochastic-factorization trick,
schemes similar to (25) can be thought of as approximating the MDP itself. Thus, both the definition
of the approximators architecture and the configuration of its parameters are converted into a well
defined optimization problem, in which the objective is to find matrices D and Wa that minimize
(Ma , DWa ) for all a  A.
6.2 Model Reduction
Another way of handling large-scale decision-making problems is to find a compact representation
of the associated MDP. In this case, the simplest idea is to aggregate states that share a common
characteristic. There are many proposals of this type in the literature, and what distinguishes them
is the criterion used to group states. For a detailed account of model approximation techniques, the
reader is referred to the review provided by Li et al. (2006).
One of the earliest works on model approximation was that of Bertsekas and Castanon (1989),
who propose aggregating and disaggregating states dynamically, during the value function computation, according to the residual left after a few applications of the Bellman operator. An alternative
794

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

approach is to group states based on their associated transition probabilities and rewards. Following
this line, Givan et al. (2003) suggest the notion of stochastic bisimulation as a criterion to aggregate
states. Roughly speaking, two states are bisimilar if they have the same expected reward associated
with each action, and the same transition probabilities to all groups of bisimilar states. This notion
is closely related to Ravindrans (2004) concept of homomorphism between MDPs. Both bisimulation and homomorphism are principled criteria for state aggregation, and as such they guarantee the
optimality of the decision policy lifted from the compact model. However, they are too restrictive
to be applied in many real situations. Realizing that, several authors have proposed relaxed versions
of these criteria (Dean et al., 1997; Ferns et al., 2006; Ravindran, 2004; Sorg & Singh, 2009).
Regardless of the specific criterion used to group states, state aggregation can be very naturally
represented as a stochastic factorization. In this case, the rows of matrix W represent the dynamics
associated with each group and matrix D has one nonzero element per row indicating the group to
which each state-action pair belongs (note the flexibility of aggregating state-action pairs instead of
states only). In this context, applying the stochastic-factorization trick corresponds to computing
the probabilities of transitions between groups. A slightly more general approach for model reduction is to assume a soft aggregation of states, in which each state belongs to a group with given
probability (Singh et al., 1995; Sorg & Singh, 2009). Soft aggregation is also naturally represented
as a stochastic factorization by letting D have more than one nonzero element per row. In fact, it can
be shown that Sorg and Singhs (2009) concept of soft homomorphism is equivalent to the particular
case of the stochastic-factorization trick discussed in Section 6.1 (Barreto et al., 2011).
As one can see, the stochastic-factorization trick can serve as a useful formalism for thinking
about state aggregation. For example, a hard aggregation of states corresponds to a matrix D in
which each row contains a single 1 and all rows associated with a given state have the nonzero element in the same column. In a soft aggregation of state-action pairs both restrictions are removed.
Also, a single application of the trick leads to a (soft) homomorphism, while successive applications
lead to an aggregation/disaggregation scheme similar to Bertsekas and Castanons (1989) approach.
Perhaps more important, as with conservative approximators, the stochastic factorization turns the
aggregation problem into a well defined optimization problem. This can provide a unifying framework for the analysis, comparison, and solution of the different versions of the aggregation problem.

7. Conclusion
The approach presented in this paper builds on a simple idea, called here the stochastic-factorization
trick: given a stochastic factorization of a transition probability matrix, one can swap the factors of
the multiplication to obtain another transition matrix, possibly much smaller than the original. This
property can be exploited to reduce the number of states of a Markov process. Intuitively, the
stochastic-factorization trick corresponds to creating a small number of representative states (or
state-action pairs) and redirecting the transitions of the original model according to some similarity
measure. Formally, this process can be posed as a well defined optimization problem.
The stochastic-factorization trick can be extended to an MDP in at least two ways: one can
factor each Markov process that comes up in the search for a decision policy or factor the Markov
processes associated with the actions of the problem before the search begins. If a single matrix K
and a single vector r are used in the latter, the PISF algorithm can be used to compute a decision
policy for the problem at hand. PISF reduces the computational complexity of standard policy
iteration from a cubic dependence on |S| to a function that grows only linearly with the size of
795

fiBARRETO , P INEAU , & P RECUP

the MDP. PISF also enjoys nice theoretical guarantees, since it always converges to a decision
policy whose performance improves with the quality of the MDPs factorization. As in general the
factorization improves with its order, m, one can use this parameter to control the trade-off between
the use of computational resources and the performance of the resulting policy.
In order to apply PISF to a decision-making problem, one must find an approximate factorization
of the corresponding MDP, M  DW. One way to compute such a factorization is to see the rows
of W as prototypical vectors that represent the dynamics of M and interpret the elements of D
as a similarity measure between these vectors and the rows of M. This way, D and W can be
computed based on a dissimilarity measure defined in the problems state-action space S  A. Such
a technique allows the approximation M  DW to be computed in time linear in |S|. Therefore, the
entire process of computing a decision policy with PISF will depend only linearly on the number of
states of the MDP. Exploiting this fact, PISF was able to find approximate solutions for instances
of an important decision task with more than 5 billion state-action pairs. The solutions found were
considerably better than those found by a heuristic commonly adopted in practice.
Evidently, this paper does not exhaust the discussion on the stochastic-factorization trick and
PISF. One subject that calls for further investigation is the development of alternative methods to
efficiently compute matrices D and W (or the identification of scenarios where a factorization is
available or easy to compute). Another promising research topic is the application of the stochasticfactorization trick to factored MDPs or other types of structured models. Finally, PISF may also
be useful in the context of model-based reinforcement learning. In this case, instead of collecting
sample transitions in order to estimate all parameters of an MDP M, one can leverage the use of data
by focusing on the prototypical state-action pairs represented in W. After W has been determined,
the elements of D can be computed based on some measure of similarity defined in S  A, as done
in the experiment presented in this paper. These topics constitute interesting directions for future
research.

Appendix A. Modified Policy Iteration and Modified PISF
Throughout the paper, it is assumed that the value function v is computed exactly at each step
of policy iteration. This facilitates the analysis of the theoretical properties and computational
complexity of the algorithms. However, most of the ideas discussed extend naturally to the case in
which v is only approximated.
A.1 Modified Policy Iteration
In Puterman and Shins (1978) modified policy iteration, v is estimated through t applications of
T  . Thus, in this case each value function computation involves O(|S|2 t) operations. Decreasing t
reduces the computational cost of evaluating a decision policy, but in general it also increases the
number of policies that must be evaluated until convergence (Puterman, 1994). Note that when
t = 1 one recovers the value iteration algorithm. Similarly, for t  |S| one can simply compute v
exactly by solving the associated linear system, which comes down to conventional policy iteration.
Therefore, both value iteration and policy iteration can be seen as special cases of modified policy
iteration.
The amount of memory used by modified policy iteration depends on the specific implementation adopted. In general, the memory usage is inversely proportional to the algorithms effective run
time. One extreme implementation strategy is to only store one vector corresponding to the current
796

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

estimate of the value function, v . Note though that in this case each multiplication P v requires
loading the rows of P from secondary memory (or computing them on demand), thus significantly
increasing the algorithms run time. One can speed up the algorithm by keeping the entire MDP
loaded in memory at all times, therefore increasing the use of memory from O(|S|) to O(|S|2 |A|).
An intermediate solution between these two extremes is to load (or compute) P before each value
function computation, which leads to O(|S|2 ) memory usage.
A.2 Modified PISF
The definition of modified PISF is straightforward: the only change needed is to replace the exact
computation of v in line 6 of Algorithm 2 with t applications of T  v = r +  P v. In this case, using
PISF instead of modified policy iteration reduces the computational cost of each value function
computation from O(|S|2 t) to O(m2t). The reduction on memory usage depends on the strategy
used to represent the models, as discussed in Appendix A.1.
It should be clear that using modified PISF with t = 1 corresponds to combining the stochasticfactorization trick with value iteration. Note though that, in terms of computational cost, this may
not be the best alternative. As discussed in Appendix A.1, decreasing t tends to increase the number
of iterations performed by PISF. Each iteration of PISF involves building matrix D and computing
the multiplication KD (lines 4 and 5 of Algorithm 2, respectively), which can be seen as constructing the operator T  for the current  . Since the construction of T  takes O(|S|2 m) operations
and its application is only O(m2 ), one may be wasting computational effort by only applying this
operator once.
A.3 Implementation Details
The experiments of Section 5 were performed using modified policy iteration and modified PISF
(Appendices A.1 and A.2, respectively). Instead of fixing a value for t, the iterative value function
computation was interrupted according to the stop criterion described in Putermans (1994) Proposition 6.6.5, with  = 106 . Policy iteration and PISF were run until two successive policies were
identical, as shown in Algorithms 1 and 2. The matrices P of modified policy iteration were loaded
before each value function computation, since this represents a compromise between memory usage
and computational cost (see Appendix A.1). In the case of PISF only matrix K was kept in memory
at all times; the matrices D were computed on demand at each iteration of the algorithm. All the
statements in Section 5 regarding the algorithms computational requirements refer to this specific
implementation.

Acknowledgements
Part of this work was done while Andre Barreto was a postdoctoral fellow in the School of Computer
Science at McGill University. The authors would like to thank Amir-massoud Farahmand for valid
discussions, and also the anonymous reviewers for their suggestions to improve the paper. The
experiments were run using computational resources made available by Compute Canada and Calcul
Quebec. Funding for this research was provided by Coordenacao de Aperfeicoamento de Pessoal
de Nvel Superior (CAPES), the National Institutes of Health (grant R21 DA019800), the NSERC
Discovery Grant program, and Projets de Recherche en Equipe (FQRNT).
797

fiBARRETO , P INEAU , & P RECUP

References
Arruda, E., & Fragoso, M. D. (2011). Time aggregated Markov decision processes via standard
dynamic programming. Operations Research Letters, 39(3), 25762580.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47(2-3), 235256.
Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In
Proceedings of the International Conference on Machine Learning (ICML), pp. 3037.
Barlow, R. E., & Proschan, F. (1965). Mathematical Theory of Reliability. Wiley.
Barreto, A. M. S. (2014). Tree-based on-line reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence.
Barreto, A. M. S., & Fragoso, M. D. (2011). Computing the stationary distribution of a finite Markov
chain through stochastic factorization. SIAM Journal on Matrix Analysis and Applications,
32, 15131523.
Barreto, A. M. S., Precup, D., & Pineau, J. (2011). Reinforcement learning using kernel-based
stochastic factorization. In Advances in Neural Information Processing Systems (NIPS), pp.
720728.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bellman, R. E. (1961). Adaptive Control Processes. Princeton University Press.
Bellman, R. E., & Dreyfus, S. (1959). Functional approximations and dynamic programming. Mathematical Tables and Other Aids to Computation, 13(68), 247251.
Berry, M. W., Browne, M., Langville, A. N., Pauca, V. P., & Plemmons, R. J. (2007). Algorithms
and applications for approximate nonnegative matrix factorization. Computational Statistics
and Data Analysis, 52(1), 155173.
Bertsekas, D. P. (1987). Dynamic programming: deterministic and stochastic models. Prentice-Hall.
Bertsekas, D. P. (1999). Nonlinear Programming (2nd edition). Athena Scientific.
Bertsekas, D. P., & Castanon, D. A. (1989). Adaptive aggregation methods for infinite horizon
dynamic programming. IEEE Transactions on Automatic Control, 34(6), 589598.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bittorf, V., Recht, B., Re, C., & Tropp, J. A. (2012). Factoring nonnegative matrices with linear
programs. In Advances in Neural Information Processing Systems (NIPS), pp. 12141222.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction.
In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI), pp.
11041113.
Boutsidis, C., Zouzias, A., & Drineas, P. (2010). Random projections for k-means clustering. In
Advances in Neural Information Processing Systems (NIPS), pp. 298306.
Bouzy, B., & Helmstetter, B. (2003). Monte-Carlo Go developments. In Advances in Computer
Games, pp. 159174.
798

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

Boyan, J. A., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximating the value function. In Advances in Neural Information Processing Systems (NIPS),
pp. 369376.
Bradtke, S. J., & Barto, A. G. (1996). Linear least-squares algorithms for temporal difference
learning. Machine Learning, 22(1/2/3), 3357.
Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener, S.,
Perez, D., Samothrakis, S., & Colton, S. (2012). A survey of Monte Carlo tree search methods.
IEEE Transactions on Computational Intelligence and AI in Games, 4, 143.
Chang, C.-I., Wu, C.-C., Liu, W., & Ouyang, Y. C. (2006). A new growing method for simplex-based
endmember extraction algorithm. IEEE Transactions on Geoscience and Remote Sensing,
44(10), 28042819.
Chaslot, G. M. J.-B., Winands, M. H. M., van den Herik, H. J., Uiterwijk, J. W. H. M., & Bouzy, B.
(2008). Progressive strategies for Monte-Carlo tree search. New Mathematics and Natural
Computation, 4, 343357.
Cho, D. I., & Parlar, M. (1991). A survey of maintenance models for multi-unit systems. European
Journal of Operational Research, 51(1), 123.
Cohen, J. E., & Rothblum, U. G. (1991). Nonnegative ranks, decompositions and factorizations of
nonnegative matrices. Linear Algebra and its Applications, 190, 149168.
Cutler, A. (1993). A branch and bound algorithm for constrained least squares. Communications in
StatisticsSimulation and Computation, 22(2), 395321.
Cutler, A., & Breiman, L. (1994). Archetypal analysis. Technometrics, 36(4), 338347.
Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques for computing approximately
optimal solutions for Markov decision processes. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), pp. 124131.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation. Computational Intelligence, 5(2), 142150.
Dekker, R., Wildeman, R. E., & van Egmond, R. (1996). Joint replacement in an operational planning phase. European Journal of Operational Research, 91(1), 7488.
Dekker, R., Wildeman, R. E., & van der Duyn Schouten, F. A. (1997). A review of multi-component
maintenance models with economic dependence. Mathematical Methods of Operations Research, 45, 411435.
Denardo, E. V. (1967). Contraction mappings in the theory underlying dynamic programming.
SIAM Review, 9(2), 165177.
Ding, C. H. Q., Li, T., & Jordan, M. I. (2010). Convex and semi-nonnegative matrix factorizations.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(1), 4555.
Esser, E., Mller, M., Osher, S., Sapiro, G., & Xin, J. (2012). A convex model for nonnegative matrix
factorization and dimensionality reduction on physical space. IEEE Transactions on Image
Processing, 21(7), 32393252.
Ferns, N., Castro, P. S., Precup, D., & Panangaden, P. (2006). Methods for computing state similarity
in Markov decision processes. In Proceedings of the Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 174181.
799

fiBARRETO , P INEAU , & P RECUP

Friedman, J. H., Bentley, J. L., & Finkel, R. A. (1977). An algorithm for finding best matches in
logarithmic expected time. ACM Transactions on Mathematical Software, 3(3), 209226.
Gan, G., Ma, C., & Wu, J. (2007). Data Clustering: Theory, Algorithms, and Applications. ASASIAM Series on Statistics and Applied Probability. SIAM.
Geffner, H., & Bonet, B. (2013). A Concise Introduction to Models and Methods for Automated
Planning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &
Claypool Publishers.
Gelly, S., Kocsis, L., Schoenauer, M., Sebag, M., Silver, D., Szepesvari, C., & Teytaud, O. (2012).
The grand challenge of computer Go: Monte Carlo tree search and extensions. Communications of the ACM, 55(3), 106113.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning: Theory & Practice. Morgan
Kaufmann Publishers Inc.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in Markov
decision processes. Artificial Intelligence, 147(1-2), 163223.
Golub, G. H., & Loan, C. F. V. (1996). Matrix Computations (3rd edition). The Johns Hopkins
University Press.
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In Proceedings of
the International Conference on Machine Learning (ICML), pp. 261268.
Grippo, L., & Sciandrone, M. (2000). On the convergence of the block nonlinear Gauss-Seidel
method under convex constraints. Operations Research Letters, 26, 127136.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms for
factored MDPs. Journal of Artificial Intelligence Research, 19, 399468.
Hartigan, J. A. (1975). Clustering Algorithms. John Wiley and Sons.
Hastie, T., Tibshirani, R., & Friedman, J. (2002). The Elements of Statistical Learning: Data Mining,
Inference, and Prediction. Springer.
Haurie, A., & LEcuyer, P. (1982). A stochastic control approach to group preventive replacement
in a multicomponent system. IEEE Transactions on Automatic Control, 27, 387393.
Ho, N.-D., & van Dooren, P. (2007). Non-negative matrix factorization with fixed row and column
sums. Linear Algebra and Its Applications, 429(56), 10201025.
Hoey, J., St-Aubin, R., Hu, A. J., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision
diagrams. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 279288.
Howard, R. (1960). Dynamic Programming and Markov Processes. MIT Press.
Kaufman, L., & Rousseeuw, P. J. (1990). Finding Groups in Data: an Introduction to Cluster
Analysis. John Wiley and Sons.
Keller, T., & Eyerich, P. (2012). PROST: Probabilistic planning based on UCT. In Proceedings of
the International Conference on Automated Planning and Scheduling.
Keshava, N. (2003). A Survey of Spectral Unmixing Algorithms. Lincoln Laboratory Journal,
14(1), 5578.
800

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

Keshava, N., & Mustard, J. (2002). Spectral unmixing. Signal Processing Magazine, 19, 4457.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. In Proceedings of the
European Conference on Machine Learning (ECML), pp. 282293.
Koller, D., & Parr, R. (1999). Computing factored value functions for policies in structured MDPs.
In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp.
13321339.
Kolobov, A., Mausam, & Weld, D. S. (2012). LRTDP versus UCT for online probabilistic planning.
In Proceedings of the AAAI Conference on Artificial Intelligence.
Lagoudakis, M. G., & Parr, R. (2003). Least-squares policy iteration. Journal of Machine Learning
Research, 4, 11071149.
Lee, D. D., & Seung, H. S. (1997). Unsupervised learning by convex and conic coding. In Advances
in Neural Information Processing Systems (NIPS), pp. 515521.
Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401, 788791.
Lee, D. D., & Seung, H. S. (2000). Algorithms for nonnegative matrix factorization. In Advances
in Neural Information Processing Systems (NIPS), pp. 556562.
Li, L., Walsh, T. J., & Littman, M. L. (2006). Towards a unified theory of state abstraction for MDPs.
In Proceedings of the International Symposium on Artificial Intelligence and Mathematics,
pp. 531539.
Lin, C.-J. (2007a). On the convergence of multiplicative update algorithms for nonnegative matrix
factorization. IEEE Transactions on Neural Networks, 18, 1589  1596.
Lin, C.-J. (2007b). Projected gradient methods for nonnegative matrix factorization. Neural Computation, 19(10), 27562779.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). On the complexity of solving Markov
decision problems. In Proceedings of the Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 394402.
Liu, T., Moore, A. W., Gray, A., & Yang, K. (2005). An investigation of practical approximate
nearest neighbor algorithms. In Advances in Neural Information Processing Systems (NIPS),
pp. 825832.
Mahoney, M. W. (2011). Randomized algorithms for matrices and data. Foundations and Trends in
Machine Learning, 3(2), 123224.
McCall, J. J. (1965). Maintenance policies for stochastically failing equipment: A survey. Management Science, 11, 493524.
Munos, R., & Moore, A. (1999). Barycentric interpolators for continuous space & time reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pp. 1024
1030.
Nascimento, J. M. P., & Dias, J. M. B. (2004). Vertex component analysis: A fast algorithm to unmix
hyperspectral data. IEEE Transactions on Geoscience and Remote Sensing, 43, 898910.
Ormoneit, D., & Sen, S. (2002). Kernel-based reinforcement learning. Machine Learning, 49 (23),
161178.
801

fiBARRETO , P INEAU , & P RECUP

Ozekici, S. (1988). Optimal periodic replacement of multicomponent reliability systems. Operations Research, 36, 542552.
Paatero, P., & Tapper, U. (1994). Positive matrix factorization: A non-negative factor model with
optimal utilization of error estimates of data values. Environmetrics, 5, 111126.
Perkins, T. J., & Precup, D. (2003). A convergent form of approximate policy iteration. In Advances
in Neural Information Processing Systems (NIPS), pp. 15951602.
Pierskalla, W. P., & Voelker, J. A. (1976). A survey of maintenance models: The control and surveillance of deteriorating systems. Naval Research Logistics Quarterly, 23(3), 353388.
Powell, W. B. (2007). Approximate Dynamic ProgrammingSolving the Curses of Dimensionality.
John Wiley & Sons, Inc.
Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc.
Puterman, M. L., & Shin, M. (1978). Modified policy iteration algorithms for discounted Markov
decision problems. Management Science, 24(11), 11271137.
Ravindran, B. (2004). An Algebraic Approach to Abstraction in Reinforcement Learning. Ph.D.
thesis, University of Massachusetts, Amherst, MA.
Ravindran, B., & Barto, A. G. (2004). Approximate homomorphisms: A framework for non-exact
minimization in Markov decision processes. In Proceedings of the International Conference
on Knowledge Based Computer Systems.
Rust, J. (1997). Using randomization to break the curse of dimensionality. Econometrica, 65(3),
487516.
Sanner, S. (2010). Relational dynamic influence diagram language (RDDL): Language description.
Schoknecht, R., & Merke, A. (2003). Convergent combinations of reinforcement learning with linear function approximation. In Advances in Neural Information Processing Systems (NIPS),
pp. 15791586.
Scholkopf, B., & Smola, A. (2002). Learning with Kernels. MIT Press.
Sherif, Y. S., & Smith, M. L. (1981). Optimal maintenance models for systems subject to failurea
review. Naval Research Logistics Quarterly, 28(1), 4774.
Shindler, M., Wong, A., & Meyerson, A. W. (2011). Fast and accurate k-means for large datasets.
In Advances in Neural Information Processing Systems (NIPS), pp. 23752383.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1995). Reinforcement learning with soft state aggregation. In Advances in Neural Information Processing Systems (NIPS), pp. 361368.
Sorg, J., & Singh, S. (2009). Transfer via soft homomorphisms. In Autonomous Agents & Multiagent
Systems/Agent Theories, Architectures, and Languages, pp. 741748.
Sun, T., Zhao, Q., & Luh, P. (2007). Incremental value iteration for time-aggregated Markovdecision processes. IEEE Transactions on Automatic Control, 52, 21772182.
Sutton, R. S., Szepesvari, C., & Maei, H. R. (2008). A convergent O(n) algorithm for off-policy
temporal-difference learning with linear function approximation. In Advances in Neural Information Processing Systems (NIPS), pp. 16091616.
802

fiP OLICY I TERATION BASED

ON

S TOCHASTIC FACTORIZATION

Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: a framework for
temporal abstraction in reinforcement learning. Artificial Intelligence, 112, 181211.
Szepesvari, C., & Smart, W. D. (2004). Interpolation-based Q-learning. In Proceedings of the
International Conference on Machine Learning (ICML), pp. 791798.
Tadic, V. (2001). On the convergence of temporal-difference learning with linear function approximation. Machine Learning, 42(3), 241267.
Thrun, S., & Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In Proceedings of the Fourth Connectionist Models Summer School, pp. 255263.
Thurau, C., Kersting, K., Wahabzada, M., & Bauckhage, C. (2011). Convex non-negative matrix
factorization for massive datasets. Knowledge and Information Systems, 29, 457478.
Thurau, C., Kersting, K., Wahabzada, M., & Bauckhage, C. (2012). Descriptive matrix factorization
for sustainability adopting the principle of opposites. Data Mining and Knowledge Discovery,
24(2), 325354.
Tsitsiklis, J. N., & Roy, B. V. (1996). Feature-based methods for large scale dynamic programming.
Machine Learning, 22, 5994.
Tsitsiklis, J. N., & Roy, B. V. (1997). An analysis of temporal-difference learning with function
approximation. IEEE Transactions on Automatic Control, 42, 674690.
van der Duyn Schouten, F. A., & Vanneste, S. G. (1990). Analysis and computation of (n, n)strategies for maintenance of a two-component system. European Journal of Operational
Research, 48(2), 260274.
Vavasis, S. A. (2009). On the complexity of nonnegative matrix factorization. SIAM Journal on
Optimization, 20, 13641377.
Wang, H. (2002). A survey of maintenance policies of deteriorating systems. European Journal of
Operational Research, 139(3), 469  489.
White, D. J. (1985). Real applications of Markov decision processes. Interfaces, 15, 7383.
White, D. J. (1988). Further real applications of Markov decision processes. Interfaces, 18, 5561.
White, D. J. (1993). A survey of applications of Markov decision processes. The Journal of the
Operational Research Society, 44(11), 10731096.
Whitt, W. (1978). Approximations of dynamic programs, I. Mathematics of Operations Research,
3(3), 231243.
Xia, L., Zhao, Q., & Jia, Q.-S. (2008). A structure property of optimal policies for maintenance
problems with safety-critical components. IEEE Transactions Automation Science and Engineering, 5(3), 519531.

803

fiJournal of Artificial Intelligence Research 50 (2014) 697-722

Submitted 10/13; published 07/14

MDD Propagation for Sequence Constraints
David Bergman

david.bergman@business.uconn.edu

School of Business, University of Connecticut
2100 Hillside Road, Unit 1041, Storrs, CT 06260

Andre A. Cire
Willem-Jan van Hoeve

acire@andrew.cmu.edu
vanhoeve@andrew.cmu.edu

Tepper School of Business, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213 USA

Abstract
We study propagation for the Sequence constraint in the context of constraint programming based on limited-width MDDs. Our first contribution is proving that establishing
MDD-consistency for Sequence is NP-hard. Yet, we also show that this task is fixed parameter tractable with respect to the length of the sub-sequences. In addition, we propose
a partial filtering algorithm that relies on a specific decomposition of the constraint and a
novel extension of MDD filtering to node domains. We experimentally evaluate the performance of our proposed filtering algorithm, and demonstrate that the strength of the
MDD propagation increases as the maximum width is increased. In particular, MDD propagation can outperform conventional domain propagation for Sequence by reducing the
search tree size and solving time by several orders of magnitude. Similar improvements are
observed with respect to the current best MDD approach that applies the decomposition
of Sequence into Among constraints.

1. Introduction
The central inference process of constraint programming is constraint propagation (Rossi,
van Beek, & Walsh, 2006; Dechter, 2003; Apt, 2003). While traditional constraint processing
techniques were designed for explicitly defined relations of small arity, state-of-the-art constraint programming solvers apply specialized constraint propagation algorithms for global
constraints of any arity, often based on efficient combinatorial methods such as network
flows (van Hoeve & Katriel, 2006; Regin, 2011).
Conventional constraint propagation algorithms (or domain filtering algorithms) operate on individual constraints of a given problem. Their role is to identify and remove
values in the variable domains that are inconsistent with respect to the constraint under
consideration. Whenever the domain of a variable is updated (i.e., a value is removed),
the constraints in which this variable appears can be reconsidered for inspection. This
cascading process of propagating the changes in variable domains through the constraints
continues until a fixed point is reached. Most constraint programming solvers assume that
the variable domains are finite, which ensures termination of the constraint propagation
process. Note that constraint propagation in itself may not be sufficient to determine the
resolution of a given problem. Therefore, constraint propagation is normally applied at each
search state in a systematic search process.
c
2014
AI Access Foundation. All rights reserved.

fiBergman, Cire & van Hoeve

A major benefit of propagating variable domains is that it can be implemented efficiently
in many cases. However, an inherent weakness of domain propagation is that it implicitly
represents the Cartesian product of the variable domains as potential solution space. By
communicating only domain changes, this limits the amount of information shared between
constraints.
To address this shortcoming of domain propagation, Andersen, Hadzic, Hooker, and
Tiedemann (2007) proposed the use of multi-valued decision diagrams (MDDs) as an alternative to variable domains in the context of constraint propagation. MDDs are directed
acyclic layered graphs that can, in principle, compactly represent all solutions to a combinatorial problem (Wegener, 2000). Andersen et al. (2007) showed that MDDs of limited
width can provide a much stronger relaxation of the solution space than the traditional
Cartesian product of the variable domains, and as a consequence MDDs allow to represent
and communicate more refined information between constraints. By propagating MDDs
rather than variable domains, huge reductions in search tree size and computation time
can be realized (Andersen et al., 2007; Hadzic, Hooker, OSullivan, & Tiedemann, 2008a;
Hadzic, Hooker, & Tiedemann, 2008b; Hadzic, OMahony, OSullivan, & Sellmann, 2009;
Hoda, van Hoeve, & Hooker, 2010; Cire & van Hoeve, 2012, 2013).
MDDs can be used to represent individual (global) constraints, subsets of constraints,
or all constraints in a given problem. When representing individual constraints, as in the
work of Hawkins, Lagoon, and Stuckey (2005) and Cheng and Yap (2008), the higher-level
information carried by the MDD is lost when projecting this down to the variable domains
for the traditional domain propagation. The highest potential for MDD propagation instead
appears to be in representing specific subsets of constraints within the same MDD. That is,
for a given set of constraints, we create and maintain one single limited-width MDD, which
is then propagated through this constraint set. Since an MDD is defined with respect to a
fixed variable ordering, it is most useful to select a subset of constraints compatible with this
ordering. When applied in this way, MDD propagation can be implemented in parallel to the
existing domain propagation in constraint programming systems, thus complementing and
potentially strengthening the domain propagation process. For example, Cire and van Hoeve
(2013) introduced MDD propagation for a subset of constraints representing disjunctive
scheduling problems. They embedded this as a custom global constraint in the ILOG CP
Optimizer constraint programming solver, which greatly improved the performance.
1.1 Methodology
Constraint propagation based on limited-width MDDs amounts to MDD filtering and MDD
refinement. The role of an MDD filtering algorithm is to remove provably inconsistent arcs
from the MDD (Hadzic et al., 2008b; Hoda et al., 2010). An MDD refinement algorithm on
the other hand, aims at splitting nodes in the MDD to more accurately reflect the solution
space (Hadzic et al., 2008a). In order to make this approach scalable and efficient, refinement
algorithms must ensure that the MDD remains within a given maximum size (typically by
restricting its maximum widththe number of nodes on any layer). By increasing this
maximum width, the MDD relaxation can be strengthened to any desired level. That
is, a maximum width of 1 would correspond to the traditional Cartesian product of the
variable domains, while an infinite maximum width would correspond to an exact MDD
698

fiMDD Propagation for Sequence Constraints

representing all solutions. However, increasing the size of the MDD immediately impacts
the computation time, and one typically needs to balance the trade-off between the strength
of the MDD and the associated computation time.
In order to characterize the outcome of an MDD filtering algorithm, the notion of MDD
consistency was introduced by Andersen et al. (2007), similar to domain consistency in
finite-domain constraint programming: Given an MDD, a constraint is MDD consistent if
all arcs in the MDD belong to at least one solution to the constraint. As a consequence
of the richer data structure that an MDD represents, establishing MDD consistency may
be more difficult than establishing domain consistency. For example, Andersen et al. show
that establishing MDD consistency on the Alldifferent constraint is NP-hard, while
establishing traditional domain consistency can be done in polynomial time (Regin, 1994).
1.2 Contributions
The main focus of this paper is the Sequence constraint, that is defined as a specific conjunction of Among constraints, where an Among constraint restricts the occurrence of a
set of values for a sequence of variables to be within a lower and upper bound (Beldiceanu
& Contejean, 1994). The Sequence constraint finds applications in, e.g., car sequencing
and employee scheduling problems (Regin & Puget, 1997; van Hoeve, Pesant, Rousseau, &
Sabharwal, 2009). It is known that classical domain consistency can be established for Sequence in polynomial time (van Hoeve, Pesant, Rousseau, & Sabharwal, 2006; van Hoeve
et al., 2009; Brand, Narodytska, Quimper, Stuckey, & Walsh, 2007; Maher, Narodytska,
Quimper, & Walsh, 2008; Downing, Feydy, & Stuckey, 2012). Furthermore, Hoda et al.
(2010) present an MDD filtering algorithm for Among constraints establishing MDD consistency in polynomial time. However, it remained an open question whether or not MDD
consistency for Sequence can be established in polynomial time as well.
In this work, we answer that question negatively and our first contribution is showing
that establishing MDD consistency on the Sequence constraint is NP-hard. This is an
important result from the perspective of MDD-based constraint programming. Namely, of
all global constraints, the Sequence constraint has perhaps the most suitable combinatorial
structure for an MDD approach; it has a prescribed variable ordering, it combines subconstraints on contiguous variables, and existing approaches can handle this constraint
fully by using bounds reasoning only.
As our second contribution, we show that establishing MDD consistency on the Sequence constraint is fixed parameter tractable with respect to the lengths of the subsequences (the Among constraints), provided that the MDD follows the order of the Sequence constraint. The proof is constructive, and follows from a generic algorithm to filter
one MDD with another.
The third contribution is a partial MDD propagation algorithm for Sequence, that does
not necessarily establish MDD consistency. It relies on the decomposition of Sequence
into cumulative sums, and a new extension of MDD filtering to the information that is
stored at its nodes.
Our last contribution is an experimental evaluation of our proposed partial MDD propagation algorithm. We evaluate the strength of our algorithm for MDDs of various maximum
widths, and compare the performance with existing domain propagators for Sequence. We
699

fiBergman, Cire & van Hoeve

also compare our algorithm with the currently best known MDD approach that uses the
natural decomposition of Sequence into Among constraints (Hoda et al., 2010). Our
experiments demonstrate that MDD propagation can outperform domain propagation for
Sequence by reducing the search tree size, and solving time, by several orders of magnitude. Similar results are observed with respect to MDD propagation of Among constraints.
Our results thus provide further evidence for the power of MDD propagation in the context
of constraint programming.
The remainder of this paper is structured as follows. In Section 2, we provide the necessary definitions of MDD-based constraint programming and the Sequence constraint. In
Section 3, we present the proof that establishing MDD consistency on Sequence is NPhard. Section 4 describes that establishing MDD consistency is fixed parameter tractable.
In Section 5, the partial MDD filtering algorithm is presented. Section 6 shows the experimental results. We present final conclusions in Section 7.

2. Definitions
We first recall some basic definitions of MDD-based constraint programming, following the
work of Andersen et al. (2007) and Hoda et al. (2010). In this work, an ordered Multivalued
Decision Diagram (MDD) is a directed acyclic graph whose nodes are partitioned into n + 1
(possibly empty) subsets or layers L1 , . . . , Ln+1 , where the layers L1 , . . . , Ln correspond
respectively to variables x1 , . . . , xn . L1 contains a single root node r, and Ln+1 contains
a single terminal node t. For a node u in the MDD, we let L (u) denote the index of its
layer. For an MDD M , the width w(M ) is the maximum number of nodes in a layer, or
maxni=1 {|Li |}. In MDD-based CP, the MDDs typically have a given fixed maximum width.
All arcs of the MDD are directed from an upper to a lower layer; that is, from a node
in some Li to a node in some Lj with i < j. For our purposes it is convenient to assume
(without loss of generality) that each arc connects two adjacent layers. Each arc out of
layer Li is labeled with an element of the domain D(xi ) of xi . For an arc a, we refer to
the label it represents as `(a). For notational convenience, we also write `(u, v) instead of
`((u, v)) for an arc (u, v). An element in D(xi ) appears at most once as a label on the
arcs out of a given node u  Li . The set A(u, v) of arcs from node u to node v may
contain multiple arcs, and we denote each with its label. Let Ain (u) denote the set of arcs
comingfi into node u. We define the size of anfi MDD M by the number of its arcs, i.e.,
|M | = fi{a | a  Ain (u), u  Li , i = 2, . . . , n + 1}fi.
An arc with label v leaving a node in layer i represents an assignment xi = v. Each
path in the MDD from r to t can be denoted by the arc labels v1 , . . . , vn on the path and
is identified with the solution (x1 , . . . , xn ) = (v1 , . . . , vn ). A path v1 , . . . , vn is feasible for a
given constraint C if setting (x1 , . . . , xn ) = (v1 , . . . , vn ) satisfies C. Constraint C is feasible
on an MDD if the MDD contains a feasible path for C.
A constraint C is called MDD consistent on a given MDD if every arc of the MDD
lies on some feasible path. Thus MDD consistency is achieved when all redundant arcs
(i.e., arcs on no feasible path) have been removed. We also say that such MDD is MDD
consistent with respect to C. Domain consistency for C is equivalent to MDD consistency
on an MDD of width one that represents the variable domains. That is, it is equivalent
700

fiMDD Propagation for Sequence Constraints

to MDD consistency on an MDD in which each layer Li contains a single node si , and
A(si , si+1 ) = D(xi ) for i = 1, . . . , n.
Lastly, we formally recall the definitions of Among (Beldiceanu & Contejean, 1994),
Sequence (Beldiceanu & Contejean, 1994), and Gen-Sequence (van Hoeve et al., 2009)
constraints. The Among constraint counts the number of variables that are assigned to a
value in a given set S, and ensures that this number is between a given lower and upper
bound:
Definition 1 Let X be a set of variables, l, u integer numbers such that 0  l  u  |X|,
and S  xX D(x) a subset of domain values. Then we define Among(X, l, u, S) as
X
l
(x  S)  u.
xX

Note that the expression (x  S) is evaluated as a binary value, i.e., resulting in 1 if x  S
and 0 if x 
/ S. The Sequence constraint is the conjunction of a given Among constraint
applied to every sub-sequence of length q over a sequence of n variables:
Definition 2 Let X be an ordered set of n variables, q, l, u integer numbers such that
0  q  n, 0  l  u  q, and S  xX D(x) a subset of domain values. Then
Sequence(X, q, l, u, S) =

nq+1
^

Among(si , l, u, S),

i=1

where si represents the sub-sequence xi , . . . , xi+q1 .
Finally, the generalized Sequence constraint extends the Sequence constraint by allowing
the Among constraints to be specified with different lower and upper bounds, and subsequence length:
Definition 3 Let X be an ordered set of n variables, k a natural number, ~s, ~l, ~u vectors of
length k such that si is a sub-sequence of X, li , ui  N, 0  li  ui  n for i = 1, 2, . . . , k,
and S  xX D(x) a subset of domain values. Then
Gen-Sequence(X, ~s, ~l, ~u, S) =

k
^

Among(si , li , ui , S).

i=1

3. MDD Consistency for Sequence is NP-Hard
As stated before, the only known non-trivial NP-hardness result for a global constraint in
the context of MDD-based constraint programming is that of Andersen et al. (2007) for
the Alldifferent constraint. A challenge in determining whether a global constraint
can be made MDD consistent in polynomial time is that this must be guaranteed for any
given MDD. That is, in addition to the combinatorics of the global constraint itself, the
shape of the MDD adds another layer of complexity to establishing MDD consistency. For
proving NP-hardness, a particular difficulty is making sure that in the reduction, the MDD
remains of polynomial size. For Sequence constraints, so far it was unknown whether a
polynomial-time MDD consistency algorithm exists. In this section we answer that question
negatively and prove the following result.
701

fiBergman, Cire & van Hoeve

Theorem 1 Establishing MDD consistency for Sequence on an arbitrary MDD is NPhard even if the MDD follows the variable ordering of the Sequence constraint.
Proof. The proof is by reduction from 3-SAT, a classical NP-complete problem (Garey
& Johnson, 1979). We will show that an instance of 3-SAT is satisfied if and only if a
particular Sequence constraint on a particular MDD M of polynomial size has a solution.
Therefore, establishing MDD consistency for Sequence on an arbitrary MDD is at least
as hard as 3-SAT.
Consider a 3-SAT instance on n variables x1 , . . . , xn , consisting of m clauses c1 , . . . , cm .
We first construct an MDD that represents the basic structure of the 3-SAT formula (see
Example 1 after this proof for an illustration). We introduce binary variables yi,j and y i,j
representing the literals xj and xj per clause ci , for i = 1, . . . , m and j = 1, . . . , n (xj and xj
may or may not exist in ci ). We order these variables as a sequence Y , first by the index of
the clauses, then by the index of the variables, and then by yi,j , y i,j for clause ci and variable
xj . That is, we have Y = y1,1 , y 1,1 , y1,2 , y 1,2 ,. . . ,y1,n , y 1,n , . . . , ym,1 , y m,1 , . . . ,ym,n , y m,n . We
construct an MDD M as a layered graph, where the k-th layer corresponds to the k-th
variable in the sequence Y .
A clause ci is represented by 2n consecutive layers corresponding to yi,1 , . . . , y i,n . In
such part of the MDD, we identify precisely those paths that lead to a solution satisfying
the clause. The basis for this is a diamond structure for each pair of literals (yi,j , y i,j ),
that assigns either (0, 1) or (1, 0) to this pair. If a variable does not appear in a clause, we
represent it using such a diamond in the part of the MDD representing that clause, thus
ensuring that the variable can take any assignment with respect to this clause. For the
variables that do appear in the clause, we will explicitly list out all allowed combinations.
More precisely, for clause ci , we first define a local root node ri representing layer L (yi,1 ),
and we set tag(ri ) = unsat. For each node u in layer L (yi,j ) (for j = 1, . . . , n), we do the
following. If variable xj does not appear in ci , or if tag(u) is sat, we create two nodes v, v 0
in L y i,j , one single node w in L (yi,j+1 ), and arcs (u, v) with label 1, (u, v 0 ) with label 0,
(v, w) with label 0, and (v 0 , w) with label 1. This corresponds to the diamond structure.
We set tag(w) = tag(u). Otherwise (i.e., tag(u) is unsat and yi,j appears in ci ), we create
two nodes v, v 0 in L y i,j , two nodes w, w0 in L (yi,j+1 ), and arcs (u, v) with label 1, (u, v 0 )
with label 0, (v, w) with label 0, and (v 0 , w0 ) with label 1. If ci contains as literal yi,j , we set
tag(w) = sat and tag(w0 ) = unsat. Otherwise (ci contains y i,j ), we set tag(w) = unsat
and tag(w0 ) = sat.
This procedure will be initialized by a single root node r representing L (y11 ). We
iteratively append the MDDs of two consecutive clauses ci and ci+1 by merging the nodes
in the last layer of ci that are marked sat into a single node, and let this node be the
local root for ci+1 . We finalize the procedure by merging all nodes in the last layer that
are marked sat into the single terminal node t. By construction, we ensure that only one
of yij and y ij can be set to 1. Furthermore, the variable assignment corresponding to each
path between layers L (yi,1 ) and L (yi+1,1 ) will satisfy clause ci , and exactly n literals are
chosen accordingly on each such path.
We next need to ensure that for a feasible path in the MDD, each variable xj will
correspond to the same literal yi,j or y i,j in each clause ci . To this end, we impose the
702

fiMDD Propagation for Sequence Constraints

r
c1

:0
:1

y1,1

y1,1
y1,2
y1,2
y1,3
y1,3
y1,4
y1,4
c2

y2,1

y2,1
y2,2
y2,2
y2,3
y2,3
y2,4
y2,4
t

Figure 1: The MDD corresponding to Example 1.
constraint
Sequence(Y, q = 2n, l = n, u = n, S = {1})

(1)

on the MDD M described above. If the sub-sequence of length 2n starts from a positive
literal yi,j , by definition there are exactly n variables that take value 1. If the sub-sequence
starts from a negative literal y i,j instead, the last variable in the sequence corresponds to
the value xj in the next clause ci+1 , i.e., yi+1,j . Observe that all variables except for the
first and the last in this sequence will take value 1 already n  1 times. Therefore, of the
first and the last variable in the sequence (which represent xj and its complement xj in any
order), only one can take the value 1. That is, xj must take the same value in clause ci and
ci+1 . Since this holds for all sub-sequences, all variables xj must take the same value in all
clauses.
The MDD M contains 2mn + 1 layers, while each layer contains at most six nodes.
Therefore, it is of polynomial size (in the size of the 3-SAT instance), and the overall construction needs polynomial time.

703

fiBergman, Cire & van Hoeve

:0
:1

x1
0

1

x2
00

01

10

11

00

01

10

11

00

01

10

11

00

01

10

11

x3

x4

x5

x6

Figure 2: The exact MDD for the Sequence constraint of Example 2.

Example 1 Consider the 3-SAT instance on four Boolean variables x1 , x2 , x3 , x4 with clauses
c1 = (x1  x3  x4 ) and c2 = (x2  x3  x4 ). The corresponding MDD used in the reduction
is given in Figure 1.

4. MDD Consistency for Sequence is Fixed Parameter Tractable
In this section we show that establishing MDD consistency for Sequence on an arbitrary
MDD is fixed parameter tractable, with respect to the length of the sub-sequences q. It
was already shown by van Hoeve et al. (2006, 2009) that an exact MDD for the Sequence
constraint exists with O(n2q ) nodes (i.e., the unfolded automaton of the Regular constraint), as illustrated in the next example.
Example 2 Consider the constraint Sequence(X, q = 3, l = 1, u = 2, S = {1}) where
X = {x1 , x2 , . . . , x6 } is an ordered set of binary variables. The corresponding exact MDD,
following the order of X, is presented in Figure 2. For convenience, each node in the MDD
is labeled with the last q  1 labels that represent the sub-sequence up to that node (starting
q  1 layers up). For example, the second node in the third layer represents decisions x1 = 0
and x2 = 1, corresponding to sub-sequence 01. To construct the next layer, we either append
a 0 or a 1 to this sub-sequence (and remove the first symbol), leading to nodes labeled 10 and
11, respectively. Note that from nodes labeled 00 we must take an arc with label 1, because
l = 1. Similarly for nodes labeled 11 we must take an arc with label 0, because u = 2. After q
704

fiMDD Propagation for Sequence Constraints

layers, all possible sub-sequences have been created (maximally O(2q1 )), which thus defines
the width of the subsequent layers.
However, since we are given an arbitrary MDD, and not necessarily an exact MDD, we need
some additional steps to exploit this connection. For this we apply a generic approach that
will not only show fixed parameter tractability for Sequence, but in fact can be applied
to determine whether MDD consistency is tractable for any constraint.
Our goal is to establish MDD consistency on a given MDD M with respect to another
MDD M 0 on the same set of variables. This is compatible with our earlier definitions since
M 0 can be interpreted to define a constraint. That is, M is MDD consistent with respect to
M 0 if every arc in M belongs to a path (solution) that also exists in M 0 . For our purposes,
we assume that M and M 0 follow the same variable ordering.
We can establish MDD consistency by first taking the intersection of M and M 0 , and
then removing all arcs from M that are not compatible with the intersection. Computing the
intersection of two MDDs is well-studied, and we present a top-down intersection algorithm
that follows our definitions in Algorithm 1. This description is adapted from the melding
procedure presented by Knuth (2009).
The intersection MDD, denoted by I, represents all possible paths (solutions) that are
present both in M and M 0 . Each partial path in I from the root rI to a node u thus
will exist in M and M 0 , with respective endpoints v, v 0 . This information is captured by
associating with each node u in I a state s(u) = (v, v 0 ) representing those nodes v  M
and v 0  M 0 . The root of I is initialized as rI with s(rI ) := (r, r0 ) where r and r0 are the
respective roots of M and M 0 (lines 1-2). The algorithm then, in a top-down traversal,
considers a layer LIi in I, and augments a node u  LIi with s(u) = (v, v 0 ) with an arc
only if both M and M 0 have an arc with the same label out of v and v 0 respectively (lines
5-7). If the next layer already contains a node u with the same state we re-use that node.
Otherwise we add a new node u to LIi+1 and add the arc (u, u) to I. Note that the last layer
of I contains a single terminal tI with state s(tI ) = (t, t0 ), provided that I is not empty. In
the last step (line 14) we clean up I by removing all arcs and nodes that do not belong to a
feasible path. This can be done in a bottom-up traversal of I. Observe that this algorithm
does not necessarily create a reduced MDD.
Algorithm 2 presents an algorithm to establish MDD-consistency on M with respect to
0
M . We first compute the intersection I of M and M 0 (line 1). We then traverse M in a
top-down traversal, and for each layer LM
i we identify and remove infeasible arcs. For this,
we define a Boolean array Support[u, l] (initialized to 0) that represents whether an arc out
of node u  M with label l has support in I (line 3). In line 4, we consider all arcs out
of layer LIi in I. If an arc a = (v, v) exists in LIi with label l and s(v) = (u, u0 ), we mark
the associated arc out of u as supported by setting Support[u, l] := 1 (lines 4-6). We then
remove all arcs out of LM
i that have no support (lines 7-9). Lastly, we again clean up M
by removing all arcs and nodes that do not belong to a feasible path (line 11).
Theorem 2 Algorithm 2 establishes MDD-consistency on M with respect to M 0 in O(|M | 
w(M 0 ) time and space.
Proof. The correctness of Algorithm 1 follows by induction on the number of layers. To
prove that Algorithm 2 establishes MDD-consistency, consider an arc a = (u, u) in M after
705

fiBergman, Cire & van Hoeve

Algorithm 1 Intersection(M ,M 0 )
Input: MDD M with root r, MDD M 0 with root r0 . M and M 0 are defined on the same
ordered sequence of n variables.
Output: MDD I with layers LI1 , . . . , LIn+1 and arc set AI . Each node u in I has an
associated state s(u).
1: create node r I with state s(r I ) := (r, r 0 )
2: LI1 := {r I }
3: for i = 1 to n do
4:
LIi+1 := {}
5:
for all u  LIi with s(u) = (v, v 0 ) do
6:
for all a = (v, v)  M and a0 = (v 0 , v 0 )  M 0 such that `(a) = `(a0 ) do
7:
create node u with state s(u) := (v, v 0 )
8:
if  w  LIj+1 with s(w) = s(u) then u := w
9:
else LIi+1 += u end if
10:
add arc (u, u) with label `(a) to arc set AI
11:
end for
12:
end for
13: end for
14: remove all arcs and nodes from I that are not on a path from r I to tI  LIn+1
15: return I

Algorithm 2 MDD-Consistency(M ,M 0 )
Input: MDD M with root r, MDD M 0 with root r0 . M and M 0 are defined on the same
ordered sequence of n variables.
Output: M that is MDD-consistent with respect to M 0
1: create I := Intersection(M ,M 0 )
2: for i = 1 to n do
3:
create array Support[u, l] := 0 for all u  LM
i and arcs out of u with label l
4:
for all arcs a = (v, v) in AI with s(v) = (u, u0 ) such that v  LIi do
5:
Support[u, `(a)] := 1
6:
end for
7:
for all arcs a = (u, u) in M such that u  LM
i do
8:
if Support[u, `(a)] = 0 then remove a from M end if
9:
end for
10: end for
11: remove all arcs and nodes from M that are not on a path from r to t  LM
n+1
12: return M

706

fiMDD Propagation for Sequence Constraints

applying the algorithm. There exists a node v  I with s(v) = (u, u0 ) such that solutions
represented by the paths from r to u in M and from r0 to u0 in M 0 are equivalent. There
also exists an arc aI = (v, v)  AI with the same label as a. Consider s(v) = (w, w0 ). Since
M and I are decision diagrams, a label appears at most once on an arc out of a node.
Therefore, w = u. Since aI belongs to I, there exist paths from w (or u) to t in M and
from w0 to t0 in M 0 that are equivalent. Hence, a belongs to a feasible path in M (from r
to u, then along a into u and terminating in t) for which an equivalent path exists in M 0
(from r0 to u0 , then into w0 and terminating in t0 ).
Regarding the time complexity for computing the intersection, a coarse upper bound
multiplies n (line 3), w(M )  w(M 0 ) (line 5), and d2max (line 6), where dmax represents the
maximum degree out of a node, or maxxX |D(x)|. We can amortize these steps since the forloops in lines 3 and 6 consider each arc in M once for comparison with arcs in M 0 . Each arc
is compared with at most w(M 0 ) arcs (line 6); here we assume that we can check in constant
time whether a node has an outgoing arc with a given label (using an arc-label list). This
gives a total time complexity of O(|M |  w(M 0 )). The memory requirements are bounded by
the size of the intersection, which is at most O(n  w(M )  w(M 0 )  dmax ) = O(|M |  w(M 0 )).
This dominates the complexity of Algorithm 2, since lines 2-12 can be performed in linear
time and space (in the size of M ).

Observe that Algorithm 2 no longer ensures that each solution in M is represented by
some path in M 0 , as is the case for the intersection. MDD-consistency merely establishes
that each arc in M belongs to some solution that is also in M 0 . Although MDD intersections
are stronger than MDD consistency, their limitation is that the width of the intersection
MDD may be as large as the product of the widths of M and M 0 . Therefore intersecting M
with multiple MDDs will, in general, increase the size of the resulting MDD exponentially.
We next apply Theorem 2 to the Sequence constraint.
Corollary 1 Let X be an ordered sequence of variables, C = Sequence(X, q, l, u, S) a
sequence constraint, and M an arbitrary MDD following the variable ordering of X. Establishing MDD consistency for C on M is fixed parameter tractable with respect to parameter q.
Proof. We know that there exists an exact MDD M 0 of size O(n2q1 ) that represents C
(van Hoeve et al., 2006, 2009). Applying Theorem 2 gives an MDD-consistency algorithm
with time and space complexity O(|M | 2q1 ), and the result follows.

We note that Theorem 2 can also be applied to obtain the tractability of establishing
MDD consistency on other constraints. Consider for example the constraint Among(x1 , x2 ,
. . . , xn , l, u, S). For any variable ordering, we can construct an exact MDD in a top-down
procedure by associating with each node v the number of variables taking a value in S along
the path from r to v, representing the length of that path. Nodes with the same length are
equivalent and can be merged. Because the largest layer has at most u + 1 different path
lengths, the exact MDD has size O(nu), and by Theorem 2 establishing MDD consistency
is tractable for Among. Indeed, Hoda et al. (2010) also showed that MDD consistency can
be established for this constraint, with quadratic time complexity.
707

fiBergman, Cire & van Hoeve

The converse of Theorem 2 does not hold: There exist constraints for which MDD
consistency can be established in polynomial time on any given MDD, while a minimal
reduced exact MDD hasP
exponential size. As a specific example, consider linear inequality
constraints of the form ni=1 ai xi  b where xi is an integer variable, ai is a constant, for
i = 1, . . . , n, and b is a constant. MDD consistency can be established for such constraints in
linear time, for any given MDD, by computing for each arc the longest r-t path (relative to
the coefficients ai ) that uses that arc (Andersen et al., 2007). However, Hosaka, Takenaga,
Kaneda, and Yajima (1997)
provide the following explicit linear inequality. For k even
P
and n = k 2 , consider 1i,jk aij xij  k(22k  1)/2, where xij is a binary variable, and
aij = 2i1 + 2k+j1 , for 1  i, j  k. They show that, for any variable order,
the size of the

n/2
reduced ordered BDD for this inequality is bounded from below by (2
).

5. Partial MDD Filtering for Sequence
In many practical situations the value of q will lead to prohibitively large exact MDDs for
establishing MDD consistency, which limits the applicability of Corollary 1. Therefore we
next explore a more practical partial filtering algorithm that is polynomial also in q.
One immediate approach is to propagate the Sequence constraint in MDDs through
its natural decomposition into Among constraints, and apply the MDD filtering algorithms
for Among proposed by Hoda et al. (2010). However, it is well-known that for classical
constraint propagation based on variable domains, the Among decomposition can be substantially improved by a dedicated domain filtering algorithm for Sequence (van Hoeve
et al., 2006, 2009; Brand et al., 2007; Maher et al., 2008). Therefore, our goal in this section is to provide MDD filtering for Sequence that can be stronger in practice than MDD
filtering for the Among decomposition, and stronger than domain filtering for Sequence.
In what follows, we assume that the MDD at hand respects the ordering of the variables in
the Sequence constraint.
5.1 Cumulative Sums Encoding
Our proposed algorithm extends the original domain consistency filtering algorithm for
Sequence by van Hoeve et al. (2006) to MDDs, following the cumulative sums encoding as proposed by Brand et al. (2007). This representation takes the following form.
For a sequence of variables X = x1 , x2 , . . . , xn , and a constraint Sequence(X, q, l, u, S),
we first introduce variables y0 , y1 , . . . , yn , with respective initial domains D(yi ) = [0, i]
for
Pi i = 1, . . . , n. These variables represent the cumulative sums of X, i.e., yi represents
j=1 (xj  S) for i = 1, . . . , n. We now rewrite the Sequence constraint as the following
system of constraints:
i  {1, . . . , n},

(2)

yi+q  yi  l

i  {0, . . . , n  q},

(3)

yi+q  yi  u

i  {0, . . . , n  q},

(4)

yi = yi1 + S (xi )

where S : X  {0, 1} is the indicator function for the set S, i.e., S (x) = 1 if x  S and
S (x) = 0 if x 
/ S. Brand et al. show that establishing singleton bounds consistency on
this system suffices to establish domain consistency for the original Sequence constraint.
708

fiMDD Propagation for Sequence Constraints

In order to apply similar reasoning in the context of MDDs, the crucial observation is
that the domains of the variables y0 , . . . , yn can be naturally represented at the nodes of the
MDD. In other words, a node v in layer Li represents the domain of yi1 , restricted to the
solution space formed by all r-t paths containing v. Let us denote this information for each
node v explicitly as the interval [lb(v), ub(v)], and we will refer to it as the node domain of
v. Following the approach of Hoda et al. (2010), we can compute this information in linear
time by one top-down pass, by using equation (2), as follows:
lb(v) = min(u,v)Ain (v) {lb(u) + S (`(u, v))} ,
ub(v) = max(u,v)Ain (v) {ub(u) + S (`(u, v))} ,

(5)

for all nodes v 6= r, while [lb(r), ub(r)] = [0, 0].
As the individual Among constraints are now posted as yi+q  yi  l and yi+q  yi  u,
we also need to compute for a node v in layer Li+1 all its ancestors from layer Li . This can
be done by maintaining a vector Av of length q + 1 for each node v, where Av [i] represents
the set of ancestor nodes of v at the i-th layer above v, for i = 0, . . . , q. We initialize
Ar = [{r}, , . . . , ], and apply the recursion
Av [i] = (u,v)Ain (v) Au [i  1]

for i = 1, 2, . . . , q,

Av [0] = {v}.
The resulting top-down pass itself takes linear time (in the size of the MDD), while a direct
implementation of the recursive step for each node takes O(q  (w(M ))2 ) operations for an
MDD M . Now, the relevant ancestor nodes for a node v in layer Li+q are stored in Av [q],
a subset of layer Li . We similarly compute all descendant nodes of v in a vector Dv of
length q + 1, such that Dv [i] contains all descendants of v in the i-th layer below v, for
i = 0, 1, . . . , q. We initialize Dt = [{t}, , . . . , ].
However, for our purposes we only need to maintain the minimum and maximum value
of the union of the domains of Av , resp., Dv , because constraints (3) and (4) are inequalities;
see the application of Av and Dv in rules (8) below. This makes the recursive step more
efficient, now taking O(qw(M )) operations per node.
Alternatively, we can approximate this information by only maintaining a minimum
and maximum node domain value for each layer, instead of a list of ancestor layers. This
will compromise the filtering, but may be more efficient in practice, as it only requires to
maintain two integers per layer.
5.2 Processing the Constraints
We next process each of the constraints (2), (3), and (4) in turn to remove provably inconsistent arcs, while at the same time we filter the node information.
Starting with the ternary constraints of type (2), we remove an arc (u, v) if lb(u) +
S (`(u, v)) > ub(v). Updating [lb(v), ub(v)] for a node v is done similar to the rules (5)
above:

	
lb(v) = max lb(v), min(u,v)Ain (v) {lb(u) + S (`(u, v))} ,
(6)

	
ub(v) = min ub(v), min(u,v)Ain (v) {ub(u) + S (`(u, v))} ,
709

fiBergman, Cire & van Hoeve

:0
:1

y0

[0,0

[0,0]

x1

x1
[0,0]

[0,0]

[1,1]

y1

[1,1]

x2

x2
[0,0]

[2,2]

[1,1]

[0,0]

[2,2]

[1,1]

y2
x3

x3
[1,1]

[0,2]

[2,3]

[1,1]

[2,2]

[2,2]

y3
x4

x4
[1,1]

[0,2]

[1,4]

[1,1]

[2,2]

[3,3]

y4
x5

x5
[2,4]

[0,5]

a. Initial MDD

b. Node domains

y5

c. MDD after filtering

Figure 3: MDD propagation for the constraint Sequence(X, q = 3, l = 1, u = 2, S = {1})
of Example 3.

In fact, the resulting algorithm is a special case of the MDD consistency equality propagator of Hadzic et al. (2008a), and we thus inherit the MDD consistency for our ternary
constraints.
Next, we process the constraints (3) and (4) for a node v in layer Li+1 (i = 0, . . . , n).
Recall that the relevant ancestors from Li+1q are Av [q], while its relevant descendants
from Li+1+q are Dv [q]. The variable corresponding to node v is yi , and it participates in
four constraints:
yi  l + yiq ,
yi  u + yiq ,
(7)
yi  yi+q  l,
yi  yi+q  u.
Observe that we can apply these constraints to filter only the node domain [lb(v), ub(v)]
corresponding to yi . Namely, the node domains corresponding to the other variables yiq
and yi+q may find support from nodes in layer Li+1 other than v. We update lb(v) and
ub(v) according to equations (7):
lb(v) = max{ lb(v),

l + min lb(u),
uAv [q]

ub(v) = min{ ub(v), u + max ub(u),
uAv [q]

min lb(w)  u },
wDv [q]

max ub(w)  l }.

(8)

wDv [q]

The resulting algorithm is a specific instance of the generic MDD consistent binary
constraint propagator presented by Hoda et al. (2010), and again we inherit the MDD
consistency for these constraints. We can process the constraints in linear time (in the size
of the MDD) by a top-down and bottom-up pass through the MDD.
710

fiMDD Propagation for Sequence Constraints

Example 3 Consider the constraint Sequence(X, q = 3, l = 1, u = 2, S = {1}) with the
ordered sequence of binary variables X = {x1 , x2 , x3 , x4 , x5 }. Assume we are given the
MDD in Figure 3.a. In Figure 3.b. we show the node domains that result from processing
rules (5). Figure 3.c. shows the resulting MDD after processing the constraints via the
rules (6) and (8). For example, consider the middle node in the fourth layer, corresponding
to variable y3 . Let this node be v. It has initial domain [0, 2], and Av [q] only contains the
root node, which has domain [0, 0]. Since l = 1, we can reduce the domain of v to [1, 2]. We
can next consider the arcs into v, and conclude that value 1 in its domain is not supported.
This further reduces the domain of v to [2, 2], and allows us to eliminate one incoming arc
(from the first node of the previous layer).
The resulting MDD in Figure 3.c. reflects all possible deductions that can be made by
our partial algorithm. We have not established MDD consistency however, as witnessed by
the infeasible path (1, 1, 0, 0, 0).
Observe that our proposed algorithm can be applied immediately to the more general
Gen-Sequence constraints in which each Among constraint has its individual l, u and q.
The cumulative sums encoding can be adjusted in a straightforward manner to represent
these different values.
5.3 Formal Analysis
We next formally compare the outcome of our partial MDD filtering algorithm with MDD
propagation for the Among encoding and domain propagation for Sequence. First, we
recall the following theorem.
Theorem 3 (Brand et al., 2007, Thm. 4) Bounds consistency on the cumulative sums
encoding is incomparable to bounds consistency on the Among encoding of Sequence.
Note that since all variable domains in the Among and cumulative sums encoding are
ranges (intervals of integer values), bounds consistency is equivalent to domain consistency.
Corollary 2 MDD consistency on the cumulative sums encoding is incomparable to MDD
consistency on the Among encoding of Sequence.
Proof. We apply the examples from the proof of Theorem 4 in the work of Brand et al..
Consider the constraint Sequence(X, q = 2, l = 1, u = 2, S = {1}) with the ordered
sequence of binary variables X = {x1 , x2 , x3 , x4 } having domains D(xi ) = {0, 1} for i =
1, 2, 4, and D(x3 ) = {0}. We apply the trivial MDD of width 1 representing the Cartesian
product of the variable domains. Establishing MDD consistency on the cumulative sums
encoding yields
y0  [0, 0], y1  [0, 1], y2  [1, 2], y3  [1, 2], y4  [2, 3],
x1  {0, 1}, x2  {0, 1}, x3  {0}, x4  {0, 1}.
Establishing MDD consistency on the Among encoding, however, yields
x1  {0, 1}, x2  {1}, x3  {0}, x4  {1}.
711

fiBergman, Cire & van Hoeve

Consider the constraint Sequence(X, q = 3, l = 1, u = 1, S = {1}) with the ordered
sequence of binary variables X = {x1 , x2 , x3 , x4 } having domains D(xi ) = {0, 1} for i =
2, 3, 4, and D(x1 ) = {0}. Again, we apply the MDD of width 1 representing the Cartesian
product of the variable domains. Establishing MDD consistency on the cumulative sums
encoding yields
y0  [0, 0], y1  [0, 0], y2  [0, 1], y3  [1, 1], y4  [1, 1],
x1  {0}, x2  {0, 1}, x3  {0, 1}, x4  {0},
while establishing MDD consistency on the Among encoding does not prune any value. 
As an additional illustration of Corollary 2, consider again Example 3 and Figure 3. MDD
propagation for the Among encoding will eliminate the value x4 = 0 from the infeasible
path (1, 1, 0, 0, 0), whereas our example showed that MDD propagation for cumulative sums
does not detect this.
Theorem 4 MDD consistency on the cumulative sums encoding of Sequence is incomparable to domain consistency on Sequence.
Proof. The first example in the proof of Corollary 2 also shows that domain consistency
on Sequence can be stronger than MDD consistency on the cumulative sums encoding.
To show the opposite, consider a constraint Sequence(X, q, l, u, S = {1}) with a set
of binary variables of arbitrary size, arbitrary values q, l, and u = |X|  1. Let M be the
MDD defined over X consisting of two disjoint paths from r to t: the arcs on one path
all have label 0, while the arcs on the other all have value 1. Since the projection onto
the variable domains gives x  {0, 1} for all x  X, domain consistency will not deduce
infeasibility. However, establishing MDD consistency with respect to M on the cumulative
sums encoding will detect this.

Even though formally our MDD propagation based on cumulative sums is incomparable to
domain propagation of Sequence and MDD propagation of Among constraints, in the
next section we will show that in practice our algorithm can reduce the search space by
orders of magnitude compared to these other methods.

6. Computational Results
The purpose of our computational results is to evaluate empirically the strength of the partial MDD propagator described in Section 5. We perform three main comparisons. First,
we want to assess the impact of increasing the maximum width of the MDD on the filtering.
Second, we want to compare the MDD propagation with the classical domain propagation
for Sequence. In particular, we wish to evaluate the computational overhead of MDD
propagation relative to domain propagation, and to what extent MDD propagation can
outperform domain propagation. Third, we compare the filtering strength of our MDD
propagator for Sequence to the filtering strength of the MDD propagators for the individual Among constraints, being the best MDD approach for Sequence so far (Hoda et al.,
2010).
712

fiMDD Propagation for Sequence Constraints

We have implemented our MDD propagator for Sequence as a custom global constraint
in IBM ILOG CPLEX CP Optimizer 12.4, using the C++ interface. Recall from Section 5
that for applying rules (8) we can either maintain a minimum and maximum value for the q
previous ancestors and descendants of each node, or approximate this by maintaining these
values simply for each layer. We evaluated both strategies and found that the latter did
reduce the amount of filtering, but nonetheless resulted in much more efficient performance
(about twice as fast on average). Hence, the reported results use that implementation.
For the MDD propagator for Among, we apply the code of (Hoda et al., 2010). For the
domain propagation, we applied three models. The first uses the domain consistent propagator for Sequence by van Hoeve et al. (2009), running in O(n3 ) time. The second uses
the domain consistent propagator for Sequence based on a network flow representation
by Maher et al. (2008), which runs in O(n2 ) time.1 As third model, we applied the decomposition into cumulative sums, which uses no explicit global constraint for Sequence.
Propagating this decomposition also takes O(n2 ) in the worst case, as it considers O(n) variables and constraints while the variable domains contain up to n elements. We note that
for almost all test instances, the cumulative sums encoding established domain consistency
on Sequence. As an additional advantage, the cumulative sums encoding permits a more
insightful comparison with our MDD propagator, since both are based on the cumulative
sums decomposition.
We note that Brand et al. (2007) introduce the multiple-Sequence constraint that
represents the conjunction of multiple Sequence constraints on the same set of ordered
variables (as in our experimental setup). Narodytska (2011) shows that establishing bounds
consistency on such system is already NP-hard, and presents a domain consistent propagator
that encodes the system as an automaton for the Regular constraint. The algorithm runs
in O(nmq ) time, where n represents the number of variables, m the number of Sequence
constraints, and q the length of the largest subsequence.
In order to compare our algorithms with the multiple-Sequence constraint, we conducted experiments to identify a suitable testbed. We found that instances for which the
multiple-Sequence constraint would not run out of memory could be solved instantly by
using any domain propagator for the individual Sequence constraints, while creating the
data structures for the multiple-Sequence constraint took substantially more time on average. For instances that were more challenging (as described in the next sections), the
multiple-Sequence constraint could not be applied due to memory issues. We therefore
excluded this algorithm from the comparisons in the sections below.
Because single Sequence constraints can be solved in polynomial time, we consider
instances with multiple Sequence constraints in our experiments. We assume that these
are defined on the same ordered set of variables. To measure the impact of the different
propagation methods correctly, all approaches apply the same fixed search strategy, i.e.,
following the given ordering of the variables, with a lexicographic value ordering heuristic.
For each method, we measure the number of backtracks from a failed search state as well
as the solving time. All experiments are performed using a 2.33GHz Intel Xeon machine.
1. We thank Nina Narodytska for sharing the implementation with us.

713

fiBergman, Cire & van Hoeve

6.1 Systems of Sequence Constraints
We first consider systems of multiple Sequence constraints that are defined on the same set
of variables. We generate instances with n = 50 variables each having domain {0, 1, . . . , 10},
and 5 Sequence constraints. For each Sequence constraint, we set the length of subsequence uniform randomly between [5, n/2) as
q = (rand()%((n/2)  5)) + 5.
Here, rand() refers to the standard C++ random number generator, i.e., rand()%k selects
a number in the range [0, k  1]. Without the minimum length of 5, many of the instances
would be very easy to solve by either method. We next define the difference between l and
u as  := (rand()%q), and set
l := (rand()%(q  )),
u := l + .
Lastly, we define the set of values S by first defining its cardinality as (rand()%11) + 1, and
then selecting that many values uniformly at random from {0, 1, . . . , 10}. We generated 250
such instances in total.2
We solve each instance using the domain consistency propagator for Sequence, the
cumulative sums encoding (domain propagation), and the MDD propagator with maximum
widths 2, 4, 8, 16, 32, 64, 128. Each method is given a maximum time limit of 1,800 seconds
per instance.
We compare the performance of domain propagation and MDD propagation in Figure 4. In this figure, we report for each given time point how many instances could be
solved within that time by a specific method. The three domain propagation methods are
represented by Cumulative Sums (the cumulative sums decomposition), Sequence - HPRS
(the Sequence propagator in van Hoeve et al., 2006, 2009), and Sequence - Flow (the
flow-based propagator in Maher et al., 2008). Observe that the cumulative sums domain
propagation, although not guaranteed to establish domain consistency, outperforms both
domain consistent Sequence propagators. Also, MDD propagation with maximum width
2 can already substantially outperform domain propagation. We can further observe that
larger maximum widths require more time for the MDDs to be processed, but in the end
it does allow to solve more instances: maximum MDD width 128 permits to solve all 250
instances within the given time limit, whereas domain propagation can respectively solve
220 (Sequence - Flow), 230 (Sequence - HPRS), and 232 (Cumulative Sums) instances.
To illustrate the difference between domain and MDD propagation in more detail, Figure 5 presents scatter plots comparing domain propagation (cumulative sums) with MDD
propagation (maximum width 32). This comparison is particularly meaningful because
both propagation methods rely on the cumulative sums representation. For each instance,
Figure 5.a depicts the number of backtracks while Figure 5.b depicts the solving time of
both methods. The instances that were not solved within the time limit are collected under
TO (time out) for that method. Figure 5.a demonstrates that MDD propagation can lead
to dramatic search tree reductions, by several orders of magnitude. Naturally, the MDD
2. All instances are available at http://www.andrew.cmu.edu/user/vanhoeve/mdd/.

714

fi200
150
100

MDD Width 128
MDD Width 32
MDD Width 2
Domain (Cumulative Sums)
Domain (Sequence  HPRS)
Domain (Sequence  Flow)

0

50

Number of instances solved

250

MDD Propagation for Sequence Constraints

102

101

100

101

102

103

Time(s)

Figure 4: Performance comparison of domain and MDD propagators for the Sequence
constraint. Each data point reflects the total number of instances that are solved
by a particular method within the corresponding time limit.

propagation comes with a computational cost, but Figure 5.b shows that for almost all instances (especially the harder ones), the search tree reductions correspond to faster solving
times, again often several orders of magnitude.
We next evaluate the impact of increasing maximum widths of the MDD propagator.
In Figure 6, we present for each method the survival function with respect to the number
of backtracks (a.) and solving time (b.). Formally, when applied to combinatorial backtrack search algorithms, the survival function represents the probability of a run taking
more than x backtracks (Gomes, Fernandez, Selman, & Bessiere, 2005). In our case, we
approximate this function by taking the proportion of instances that need at least x backtracks (Figure 6.a), respectively seconds (Figure 6.b). Observe that these are log-log plots.
With respect to the search tree size, Figure 6.a clearly shows the strengthening of the MDD
propagation when the maximum width is increased. In particular, the domain propagation
reflects the linear behavior over several orders of magnitude that is typical for heavy-tailed
runtime distributions. Naturally, similar behavior is present for the MDD propagation, but
in a much weaker form for increasing maximum MDD widths. The associated solving times
are presented in Figure 6.b. It reflects similar behavior, but also takes into account the
initial computational overhead of MDD propagation.
715

fi102
101
100
102

101

MDD Propagator (Width 32)  Time (s)

106
104
102
100

MDD Propagator (Width 32)  Backtracks

TO

103 TO

Bergman, Cire & van Hoeve

100

102

104

106

102

TO

101

100

101

102

103 TO

Domain Propagator (Cumulative Sums)  Time (s)

Domain Propagator (Cumulative Sums)  Backtracks

b. Solving time

a. Number of backtracks

Figure 5: Comparing domain and MDD propagation for Sequence constraints. Each data
point reflects the number of backtracks (a.) resp. solving time in seconds (b.)
for a specific instance, when solved with the best domain propagator (cumulative
sums encoding) and the MDD propagator with maximum width 32. Instances for
which either method needed 0 backtracks (a.) or less than 0.01 seconds (b.) are
excluded. Here, TO stands for timeout and represents that the specific instance
could not be solved within 1,800s (Fig. b.). In Figure a., these instances are
labeled separately by TO (at tick-mark 108 ); note that the reported number of
backtracks after 1,800 seconds may be much less than 108 for these instances. All
reported instances with fewer than 108 backtracks were solved within the time
limit.

6.2 Nurse Rostering Instances
We next consider a more structured problem class inspired by nurse rostering problems.
The problem is to design a work schedule for a nurse over a given horizon of n days. On
each day, a nurse can either work a day shift (D), evening shift (E), night shift (N), or
have a day off (O). We introduce a variable xi for each day i = 1, . . . , n, with domain
D(xi ) = {O, D, E, N } representing the shift. We impose the eight Sequence constraints
modeling the requirements listed in Table 1.
By the combinatorial nature of this problem, the size of the CP search tree turns out to
be largely independent on the length of the time horizon, when a lexicographic search (by
increasing day i) is applied. We however do consider instances with various time horizons
(n = 40, 60, 80, 100), to address potential scaling issues.
The results are presented in Table 2. The columns for Domain Sequence show the total
number of backtracks (BT) and solving time in seconds (CPU) for the domain consistent
Sequence propagator. Similarly, the columns for Domain Cumul. Sums show this infor716

fiMDD Propagation for Sequence Constraints

1.0
0.5
0.1
0.05

Survival function

0.1

Domain Consistency
MDD Width 2
MDD Width 4
MDD Width 8
MDD Width 16
MDD Width 32
MDD Width 64
MDD Width 128

0.005 0.01

0.05
0.005 0.01

Survival function

0.5

1.0

Domain Consistency
MDD Width 2
MDD Width 4
MDD Width 8
MDD Width 16
MDD Width 32
MDD Width 64
MDD Width 128

100

101

102

103

104

105

106

107

102

Backtracks

101

100

101

102

103

Time (s)

a. Survival function with respect to backtracks

b. Survival function with respect to solving time

Figure 6: Evaluating the impact of increased width for MDD propagation via survival function plots with respect to search backtracks (a.) and solving time (b.). Both plots
are in log-log scale. Each data point reflects the percentage of instances that require at least that many backtracks (a.) resp. seconds (b.) to be solved by a
particular method.

Requirement

Sequence(X, q, l, u, S)

At least 20 work shifts every 28 days:
At least 4 off-days every 14 days:
Between 1 and 4 night shifts every 14 days:
Between 4 and 8 evening shifts every 14 days:
Nights shifts cannot appear on consecutive days:
Between 2 and 4 evening/night shifts every 7 days:
At most 6 work shifts every 7 days:

Sequence(X, 28, 20, 28, {D, E, N })
Sequence(X, 14, 4, 14, {O})
Sequence(X, 14, 1, 4, {N })
Sequence(X, 14, 4, 8, {E})
Sequence(X, 2, 0, 1, {N })
Sequence(X, 7, 2, 4, {E, N })
Sequence(X, 7, 0, 6, {D, E, N })

Table 1: Nurse rostering problem specification. Variable set X represents the shifts to be
assigned over a sequence of days. The possible shifts are day (D), evening (E),
night (N), and day off (O).

mation for the cumulative sums domain propagation. The subsequent columns show these
numbers for the MDD propagator, for MDDs of maximum width 1, 2, 4, and 8. Note that
propagating an MDD of width 1 corresponds to domain propagation, and indeed the associated number of backtracks is equivalent to the domain propagator of the cumulative sums.
As a first observation, a maximum width of 2 already reduces the number of backtracks
by a factor 8.3. For maximum width of 8 the MDD propagation even allows to solve the
717

fiBergman, Cire & van Hoeve

n
40
60
80
100

Domain
Sequence
BT
CPU
438,059 43.83
438,059 78.26
438,059 124.81
438,059 157.75

Domain
Cumul. Sums
BT
CPU
438,059
438,059
438,059
438,059

32.26
53.40
71.33
96.27

MDD
Width 1
BT
CPU
438,059 54.27
438,059 80.36
438,059 106.81
438,059 135.37

MDD
Width 2
BT
CPU
52,443
52,443
52,443
52,443

12.92
18.36
28.58
37.76

MDD
Width 4
BT CPU
439
439
439
439

0.44
0.68
0.94
1.22

MDD
Width 8
BT CPU
0
0
0
0

0.02
0.04
0.06
0.10

Table 2: Comparing domain propagation and the MDD propagation for Sequence on nurse
rostering instances. Here, n stands for the number of variables, BT for the number
of backtracks, and CPU for solving time in seconds.

problem without search. The computation times are correspondingly reduced, e.g., from
157s (resp. 96s) for the domain propagators to 0.10s for the MDD propagator (width 8) for
the instance with n = 100. Lastly, we can observe that in this case MDD propagation does
not suffer from scaling issues when compared to domain propagation.
As a final remark, we also attempted to solve these nurse rostering instances using the
Sequence domain propagator of CP Optimizer (IloSequence). It was able to solve the
instance with n = 40 in 1,150 seconds, but none of the others instances were solved within
the time limit of 1,800 seconds.
6.3 Comparing MDD Filtering for Sequence and Among
In our last experiment, we compare our Sequence MDD propagator to the MDD propagator for Among constraints by Hoda et al. (2010). Our main goal is to determine whether a
large MDD is by itself sufficient to solve these problem (irrespective of propagating Among
or a cumulative sums decomposition), or whether the additional information obtained by
our Sequence propagator makes the difference.
We apply both methods, MDD propagation for Sequence and MDD propagation for
Among, to the data set of Section 6.1 containing 250 instances. The time limit is again
1,800 seconds, and we run the propagators with maximum MDD widths 2, 8, 32, and 128.
We first compare the performance of the MDD propagators for Among and Sequence
in Figure 7. The figure depicts the number of instances that can be solved within a given
time limit for the various methods. The plot indicates that the Among propagators are
much weaker than the Sequence propagator, and moreover that larger maximum widths
alone do not suffice: using the Sequence propagator with maximum width 2 outperforms
the Among propagators for all maximum widths up to 128.
The scatter plot in Figure 8 compares the MDD propagators for Among and Sequence
in more detail, for widths 2, 8, 32, and 128 (instances that take 0 backtracks, resp. less
than 0.01 seconds, for either method are discarded from Figure 8.a, resp. 8.b). For smaller
widths, there are several instances that the Among propagator can solve faster, but the
relative strength of the Sequence propagator increases with larger widths. For width
128, the Sequence propagator can achieve orders of magnitude smaller search trees and
718

fi200
150
100

Sequence  Width 128
Sequence  Width 32
Sequence  Width 8
Sequence  Width 2
Among  Width 128
Among  Width 32
Among  Width 8
Among  Width 2

0

50

Number of instances solved

250

MDD Propagation for Sequence Constraints

102

101

100

101

102

103

Time(s)

103 TO
102
101
100
101

Sequence MDD Propagator  Time (s)

Width 2
Width 8
Width 32
Width 128

102

102

104

106

Width 2
Width 8
Width 32
Width 128

100

Sequence MDD Propagator  Backtracks

TO

Figure 7: Performance comparison of MDD propagation for Sequence and Among for
various maximum widths. Each data point reflects the total number of instances
that are solved by a particular method within the corresponding time limit.

100

102

104

106

102

TO

101

100

101

102

103 TO

Among MDD Propagator  Time (s)

Among MDD Propagator  Backtracks

b. Solving time

a. Number of backtracks

Figure 8: Evaluating MDD propagation for Sequence and Among for various maximum
widths via scatter plots with respect to search backtracks (a.) and solving time
(b.). Both plots are in log-log scale and follow the same format as Figure 5.

719

fiBergman, Cire & van Hoeve

solving time than the Among propagators, which again demonstrates the advantage of
MDD propagation for Sequence when compared to the Among decomposition.

7. Conclusion
Constraint propagation with limited-width MDDs has recently been shown to be a powerful
alternative to the conventional propagation of variable domains in constraint programming.
In this work, we have studied MDD propagation for the Sequence constraint, which appears in, e.g., rostering and scheduling applications. We have first proved that establishing
MDD consistency for Sequence is NP-hard. However, we have also shown that this task
is fixed parameter tractable with respect to the length of the sub-sequences defined by the
constraint, provided that the MDD follows the variable ordering specified by the constraint.
We then proposed a practical MDD propagation algorithm for Sequence that is also polynomial in the length of the sub-sequences, which is based on a cumulative decomposition.
We provided extensive experimental results comparing our MDD propagator for Sequence
to domain propagators for Sequence as well as an existing MDD propagator for Among.
Our computational experiments have shown that our MDD propagator for Sequence can
outperform domain propagators by orders by magnitude in terms of search tree size and
solving time. Similar results were obtained when compared to the existing MDD propagator for Among, which demonstrates that in practice a large MDD alone is not sufficient to
solve these problems; specific MDD propagators for global constraints such as Sequence
can lead to orders of magnitude speedups.

Acknowledgments
This material is based upon work supported by the National Science Foundation under
Grant No. CMMI-1130012, and a Google Research Award. We also thank the reviewers
whose comments helped improve the paper.

References
Andersen, H. R., Hadzic, T., Hooker, J. N., & Tiedemann, P. (2007). A Constraint Store
Based on Multivalued Decision Diagrams. In Proceedings of CP, Vol. 4741 of LNCS,
pp. 118132. Springer.
Apt, K. R. (2003). Principles of Constraint Programming. Cambridge University Press.
Beldiceanu, N., & Contejean, E. (1994). Introducing global constraints in CHIP. Journal
of Mathematical and Computer Modelling, 20 (12), 97123.
Brand, S., Narodytska, N., Quimper, C., Stuckey, P., & Walsh, T. (2007). Encodings of the
Sequence Constraint. In Proceedings of CP, Vol. 4741 of LNCS, pp. 210224. Springer.
Cheng, K., & Yap, R. (2008). Maintaining Generalized Arc Consistency on Ad Hoc r-Ary
Constraints. In Proceedings of CP, Vol. 5202 of LNCS, pp. 509523. Springer.
Cire, A. A., & van Hoeve, W.-J. (2012). MDD Propagation for Disjunctive Scheduling. In
Proceedings of ICAPS, pp. 1119. AAAI Press.
720

fiMDD Propagation for Sequence Constraints

Cire, A. A., & van Hoeve, W.-J. (2013). Multivalued Decision Diagrams for Sequencing
Problems. Operations Research, 61 (6), 14111428.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Downing, N., Feydy, T., & Stuckey, P. (2012). Explaining Flow-Based Propagation. In
Proceedings of CPAIOR, Vol. 7298 of LNCS, pp. 146162. Springer.
Garey, M., & Johnson, D. (1979). Computers and Intractability - A Guide to the Theory of
NP-Completeness. Freeman.
Gomes, C. P., Fernandez, C., Selman, B., & Bessiere, C. (2005). Statistical Regimes Across
Constrainedness Regions. Constraints, 10 (4), 317337.
Hadzic, T., Hooker, J. N., OSullivan, B., & Tiedemann, P. (2008a). Approximate Compilation of Constraints into Multivalued Decision Diagrams. In Proceedings of CP, Vol.
5202 of LNCS, pp. 448462. Springer.
Hadzic, T., Hooker, J. N., & Tiedemann, P. (2008b). Propagating Separable Equalities in
an MDD Store. In Proceedings of CPAIOR, Vol. 5015 of LNCS, pp. 318322. Springer.
Hadzic, T., OMahony, E., OSullivan, B., & Sellmann, M. (2009). Enhanced Inference for
the Market Split Problem. In Proceedings of ICTAI, pp. 716723. IEEE.
Hawkins, P., Lagoon, V., & Stuckey, P. (2005). Solving Set Constraint Satisfaction Problems
Using ROBDDs. JAIR, 24 (1), 109156.
Hoda, S., van Hoeve, W.-J., & Hooker, J. N. (2010). A Systematic Approach to MDD-Based
Constraint Programming. In Proceedings of CP, Vol. 6308 of LNCS, pp. 266280.
Springer.
Hosaka, K., Takenaga, Y., Kaneda, T., & Yajima, S. (1997). Size of ordered binary decision
diagrams representing threshold functions. Theoretical Computer Science, 180, 4760.
Knuth, D. E. (2009). The Art of Computer Programming, Volume 4, Fascicle 1: Bitwise
Tricks & Techniques; Binary Decision Diagrams. Addison-Wesley Professional.
Maher, M., Narodytska, N., Quimper, C.-G., & Walsh, T. (2008). Flow-Based Propagators
for the SEQUENCE and Related Global Constraints. In Proceedings of CP, Vol. 5202
of LNCS, pp. 159174. Springer.
Narodytska, N. (2011). Reformulation of Global Constraints. Ph.D. thesis, University of
New South Wales.
Regin, J.-C. (1994). A Filtering Algorithm for Constraints of Difference in CSPs. In
Proceedings of AAAI, Vol. 1, pp. 362367. AAAI Press.
Regin, J.-C. (2011). Global Constraints: A Survey. In Van Hentenryck, P., & Milano, M.
(Eds.), Hybrid Optimization, pp. 63134. Springer.
Regin, J.-C., & Puget, J.-F. (1997). A Filtering Algorithm for Global Sequencing Constraints. In Proceedings of CP, Vol. 1330 of LNCS, pp. 3246. Springer.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook of Constraint Programming.
Elsevier.
van Hoeve, W.-J., & Katriel, I. (2006). Global Constraints. In Rossi, F. van Beek, P., &
Walsh, T. (Eds.), Handbook of Constraint Programming, chap. 6. Elsevier.
721

fiBergman, Cire & van Hoeve

van Hoeve, W.-J., Pesant, G., Rousseau, L.-M., & Sabharwal, A. (2006). Revisiting the
Sequence Constraint. In Proceedings of CP, Vol. 4204 of LNCS, pp. 620634. Springer.
van Hoeve, W.-J., Pesant, G., Rousseau, L.-M., & Sabharwal, A. (2009). New Filtering
Algorithms for Combinations of Among Constraints. Constraints, 14, 273292.
Wegener, I. (2000). Branching Programs and Binary Decision Diagrams: Theory and Applications. SIAM monographs on discrete mathematics and applications. Society for
Industrial and Applied Mathematics.

722

fiJournal of Artificial Intelligence Research 50 (2014) 847-884

Submitted 11/13; published 8/14

Arbitration and Stability in
Cooperative Games with Overlapping Coalitions
Yair Zick

YAIRZICK @ CMU . EDU

School of Computer Science
Carnegie Mellon University, United States

Evangelos Markakis

MARKAKIS @ GMAIL . COM

Department of Informatics
Athens University of Economics and Business, Greece

Edith Elkind

ELKIND @ CS . OX . AC . UK

Department of Computer Science
University of Oxford, United Kingdom

Abstract
Overlapping Coalition Formation (OCF) games, introduced by Chalkiadakis, Elkind, Markakis,
Polukarov and Jennings in 2010, are cooperative games where players can simultaneously participate in several coalitions. Capturing the notion of stability in OCF games is a difficult task: deviating players may continue to contribute resources to joint projects with non-deviators, and the
crucial question is what payoffs the deviators expect to receive from such projects. Chalkiadakis
et al. introduce three stability concepts for OCF gamesthe conservative core, the refined core,
and the optimistic corethat are based on different answers to this question. In this paper, we
propose a unified framework for the study of stability in the OCF setting, which encompasses the
stability concepts considered by Chalkiadakis et al. as well as a wide variety of alternative stability
concepts. Our approach is based on the notion of arbitration functions, which determine the payoff
obtained by the deviators, given their deviation and the current allocation of resources. We provide
a characterization of stable outcomes under arbitration. We then conduct an in-depth study of four
types of arbitration functions, which correspond to four notions of the core; these include the three
notions of the core considered by Chalkiadakis et al. Our results complement those of Chalkiadakis
et al. and answer questions left open by their work. In particular, we show that OCF games with
the conservative arbitration function are essentially equivalent to non-OCF games, by relating the
conservative core of an OCF game to the core of a non-overlapping cooperative game, and use this
result to obtain a strictly weaker sufficient condition for conservative core non-emptiness than the
one given by Chalkiadakis et al.

1. Introduction
Consider a market exchange involving several agents, each owning a certain amount of some goods
(say, oil, sugar and flour). The agents trade with each other, and each vendor may offer different
prices to different buyers. Suppose that one of the vendors has 3 tons of sugar and has agreed to sell
them to buyers 1 and 2 so that buyer 1 receives 1 ton of sugar and pays 500 dollars, whereas buyer 2
receives 2 tons of sugar and pays 900 dollars. The vendor then discovers that he can sell one ton of
sugar to a third buyer for 700 dollars. He may decide that he is unhappy with the amount of money
he receives from the transaction with buyer 1, and cancel the deal in favor of buyer 3. However,
buyer 2, upon hearing that the transaction with buyer 1 has been canceled, may no longer wish
to work with this vendor, and cancel her agreement with him as well. Therefore, when deciding
c
2014
AI Access Foundation. All rights reserved.

fiZ ICK , M ARKAKIS , & E LKIND

whether a deviation from an agreement is profitable, the vendor needs to predict how other parties
he is trading with would react to his actions. If it is safe to assume that buyer 2 is willing to upkeep
the interaction, then the deviation is profitable; if this is not true, then the seller has to forgo the
better deal with buyer 3, or lose more than what the deviation is worth.
This setting possesses several interesting characteristics. First, a vendor may allocate his goods
to several buyers, and a buyer may also purchase goods from several vendors. In other words,
agents may allocate fractions of their resources to several profit-generating tasks. Second, agents
may withdraw some of their resources from some agreements, while keeping others unchanged. For
example, an oil vendor may wish to sell less oil to some customer, but not change her interactions
with other parties; similarly, a buyer may want to pay less to some vendors, but maintain the same
payments to others. Finally, when trying to strategically change an agreement, agents must take
into account the impact of their actions on the contracts they still maintain with other (possibly
unaffected) parties.
These features are typical of many multi-agent settings, where agents collaborate by allocating
parts of their resources to working together in order to generate revenue, and then share the resulting
profits. Profit sharing can be done directly, if the exchange of goods itself results in a profit, e.g., if
the agents can make a new good using their resources and sell it; profit sharing can also be indirect,
e.g., via setting a price for the good sold. In such settings, even if there are no external constraints
on how the profits should be distributed, agents should account for individuals or groups of agents
who are underpaid. A group of agents that can get more money by deviating from the proposed deal
may destabilize the entire agreement, causing a cascade of deviations that results in a less desirable
state. However, what constitutes a profitable deviation greatly depends on how the non-deviators
respond to the deviating set.
Modeling this system of incentives and reactions is a challenge in itself. Recently this challenge
was addressed by Chalkiadakis, Elkind, Markakis, Polukarov, and Jennings (2010), who propose a
novel approach to modeling scenarios where agents can divide resources among several coalitions.
They introduce overlapping coalition formation (OCF) games, which generalize the classic model
of cooperative games (Peleg & Sudholter, 2007).
In the classic cooperative game theory model, there is a set of agents N ; each subset S of N
can form a team and generate profit for its members. In transferable utility (TU) games, each subset
of agents S  N is identified with a value v(S). This value can be thought of as the monetary
payoff to the set S should its members agree to work together, and can be freely divided among the
members of S. As argued above, a desirable property of a payoff distribution scheme is its stability
against individual and group deviations, which is captured by the notion of the core (Gillies, 1953):
a payoff division is said to be in the core if every subset of agents S  N receives a total payoff
that is at least its value v(S).
Classic cooperative game theory assumes that each agent may belong to at most one coalition at
any given time. That is, agents form coalition structures by splitting into disjoint groups, with each
group working on an individual task. Consequently, the classic approach is not well-suited to handle
overlapping coalitions, and the intricacies of deviation in settings where each agent participates in
several agreements. Indeed, in the classic cooperative game setting, a set of agents assesses the
desirability of a payoff division against what it can make on its own: is the total payoff allotted
to S at least v(S)? This is because the deviating agents break all ties with the non-deviators,
so they cannot expect the non-deviators to collaborate with them in any way. In contrast, when
agents are allowed to split their resources among multiple coalitions, the very notion of a profitable
848

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

deviation must be reexamined, since partial coalitions allow for partial deviations, which means
that even though a set of agents decides to deviate from the group agreement, it remains involved in
collaborative projects with non-deviators. These non-deviators then decide how much the deviators
should receive from such projects. Hence, the deviators decision whether to withdraw from some
tasks (and to what extent) depends on what they expect to receive from their collaboration with
non-deviators.
Chalkiadakis et al. (2010) propose a model where agents can split their resources among several
coalitions, and study the effect of joint projects with non-deviators on the desirability of deviation
in the overlapping coalition setting. They propose and discuss three notions of reaction to deviation.
The first reaction, which they term conservative, gives the deviators no share of the profits from any
coalitions they maintain with non-deviators. The second reaction, termed refined, allows the deviators to keep their payoffs from coalitions that were unaffected by the deviation. Finally, under the
optimistic reaction the deviators may be able to receive some revenue from each coalition they are
involved in, as long as the non-deviators in that coalition are guaranteed their pre-deviation payoffs.
These reactions to deviation correspond to three notions of stability for OCF games, namely, the
conservative core, the refined core, and the optimistic core.
The non-deviators reaction to deviation can extend beyond these three notions. Many legal
contracts (e.g., ones involving a service provider and its clients, or vendors and suppliers, or several
companies working on a joint venture) are designed with the explicit aim of detailing the consequences of deviation to all parties. Typically, such contracts impose fines on parties that break the
agreement. However, there are also other forms of punishment: for instance, a company that fails
to uphold its contractual obligations may be blacklisted by some of its clients, or have its reputation
tarnished by bad press. Alternatively, one may be actively rewarded for deviating, e.g., when
an external party wishes to break a certain status-quo, or to encourage project diversification. By
adopting the notion of partial deviation and the possible reactions to deviation, we can capture far
more nuanced scenarios than those described by classic TU games.
1.1 Our Contribution
Our main contribution is a broad generalization of the overlapping coalition formation (OCF) model
proposed by Chalkiadakis et al. (2010). While Chalkiadakis et al. describe three ways in which nondeviators may react to a deviation, we propose a general framework for modeling such reactions.
Our framework is based on the notion of arbitration function (Section 3). This is a function that,
given a deviating group of players and its deviation, outputs, for every coalition containing the
deviators, the amount that the deviators can expect to receive from this coalition post-deviation.
Using arbitration functions, we present a class of solution concepts for OCF games that we call
arbitrated cores (Section 4). We show that the three concepts of the core described by Chalkiadakis
et al. (2010) are special cases of our model, and propose a new concept of the core, which we call
the sensitive core. We propose a criterion for checking whether an outcome is in the arbitrated
core, and characterize core outcomes under some important arbitration functions. We then focus
on identifying sufficient and necessary conditions for the arbitrated core to be non-empty (Section 5). Building on the work of Chalkiadakis et al., who provide an LP-based criterion for the
non-emptiness of the conservative core, we derive criteria for the sensitive and refined core to be
non-empty. We use this result to identify an interesting class of OCF games whose refined core is
guaranteed to be non-empty. Finally, we show that OCF games with the conservative arbitration
849

fiZ ICK , M ARKAKIS , & E LKIND

function are essentially equivalent to non-OCF games, by relating the conservative core of an OCF
game G to the core of its discrete superadditive cover, which is a non-OCF game that is constructed
from G in a natural way. In particular, this means that the conservative core of an OCF game G is
non-empty when the discrete superadditive cover of G is supermodular. We demonstrate that this
condition is strictly weaker than the convexity-based condition for conservative core non-emptiness
given by Chalkiadakis et al.
1.2 Related Work
The most direct precursor to our work is the work of Chalkiadakis et al. (2010): we generalize their
model to capture a broader class of possible reactions to deviation, and solve some of the problem
left open by their work.
Incentives and optimization in collaborative multi-agent environments have received plenty of
attention from the multiagent research community (Sims, Corkill, & Lesser, 2008; Airiau & Sen,
2009; Rahwan & Jennings, 2008; Dang, Dash, Rogers, & Jennings, 2006; Shehory & Kraus, 1996;
Lin & Hu, 2007; Zhang, Jiang, Su, Qi, & Fang, 2010); see also the PhD thesis of Rahwan (2007)
and the recent book of Chalkiadakis, Elkind, and Wooldridge (2011) for an overview. These studies
often use cooperative game theory as the modeling framework.
Several authors have considered the problem of optimal coalition structure generation in cooperative settings with overlapping coalitions. Shehory and Kraus (1996) have initiated this line of
research; in their work, they describe a distributed coalition formation algorithm where agents may
split their resources among several tasks in order to achieve better outcomes. Dang et al. (2006)
consider overlapping coalitions in sensor networks, where the agents are sensors that are tasked
with tracking objects. Lin and Hu (2007) provide a parallel algorithm for overlapping coalition formation. In all these papers, the underlying assumption is that agents are fully cooperative, i.e., they
seek to maximize the social welfare rather than their own gains. In contrast, in our work we focus
on game-theoretic aspects of strategic behavior in cooperative settings, taking agent incentives into
account.
Our model applies to settings where rational agents distribute their resources among several
projects, receiving profits from each. This scenario occurs in a variety of applications. For example,
multi-commodity network flows can be easily modeled within the overlapping coalition formation
framework (for the analysis of the underlying non-OCF game, see Markakis & Saberi, 2005). This
is also the case for several other fractional optimization problems (Deng, Ibaraki, & Nagamochi,
1999). For example, a fractional matching problem can be thought of as an OCF game, where each
partial coalition between a pair of agents describes the amount of resources that each of these agents
assigns to the respective task.
Another class of settings where overlapping coalitions arise are collaboration networks. Consider a social network where each agent is associated with a weighted node and derives some value
from assigning a certain portion of its weight to collaborating with its neighbors. Such settings
have received much attention in recent years (for a survey, see Jackson, 2003). For instance, Anshelevich and Hoefer (2010) discuss strategic aspects of network formation in settings where agents
may participate in more than one coalition; however, in contrast with our work, the main focus of
their analysis is individual rationality and pairwise equilibria in such networks, rather than group
stability. Fractional stable matchings were discussed by Aharoni and Fleiner (2003), in a nontransferable utility model. More recently, Ackerman and Branzei (2014) have analyzed research
850

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

collaboration using a graphical model, where authors devote some of their efforts to collaborative
projects, and receive some credit for their work based on the authorship order model. Ackerman and
Branzei are mainly interested in individually rational and pairwise stable outcomes, much like Anshelevich and Hoefer. The OCF framework has recently been used to model collaborative wireless
networks (Wang, Song, Han, & Saad, 2013; Zhang, Song, Han, Saad, & Lu, 2013).
Overlapping coalition formation games, as proposed by Chalkiadakis et al. (2010), share some
features with fuzzy games (Aubin, 1981). However, there are fundamental differences between the
two models. Arguably, the most important difference is that Chalkiadakis et al. assume that several
partial coalitions may form, whereas Aubin assumes that all agents pool their resources to work on
a single task, and only considers partial coalitions in the context of potential deviations. Assuming
the formation of a single coalition eliminates many of the subtleties that games with overlapping
coalitions can capture, as was emphasized by Chalkiadakis et al.
While the work presented in this paper is not computational in nature, algorithmic issues that
arise in cooperative games with overlapping coalitions have been discussed by Chalkiadakis et al.
(2010) and subsequently by Zick, Chalkiadakis, and Elkind (2012); we refer the reader to the companion paper (Zick, Chalkiadakis, Elkind, & Markakis, 2014) for in-depth analysis.

2. Preliminaries
Throughout the paper, we consider games with a finite set of players N = {1, . . . , n}. We use
boldface lowercase letters to denote vectors and uppercase letters to denote sets. Given a set S  N ,
we write eS to denote the indicator vector
Furthermore, given a vector x = (x1 , . . . , xn )  Rn
P of S.
and a subset S  N , we set x(S) = iS xi .
2.1 Classic TU Cooperative Games
A transferable utility (TU) cooperative game is given by a set of players, or agents, N = {1, . . . , n}
and a characteristic function u : 2N  R with u() = 0; we write G = hN, ui. A payoff vector
for a game G = hN, ui is a vector x  Rn such that x(N ) = u(N ). A payoff vector x for a game
G = hN, ui is said to be stable if x(S)  u(S) for each S  N . The set of all stable payoff vectors
for G is called the core of G.
It is sometimes assumed that agents are allowed to form coalition structures; a coalition structure CS is simply a partition of N , and its value u(CS ) is the sum of the values of its constituent
coalitions. If a coalition S is a part of some coalition structure CS , then the value of S, u(S), can
be freely divided among all members of S, but transfers to agents that are not members of S are not
allowed, i.e., x is a payoff vector for a coalition structure CS if and only if x(S) = u(S) for each
S  CS . The core of a game G = hN, ui with coalition structures is the set of all pairs (CS , x),
where CS is a coalition structure, x is a payoff vector for CS , and x(S)  u(S) for each S  N .
Cooperative games with coalition structures have been first studied by Aumann and Dreze (1974),
and have been the focus of a number of recent papers by the multiagent research community (see
Chalkiadakis et al., 2011).
2.2 OCF Games
Given a set of agents N = {1 . . . n}, a partial coalition of players in N is a vector c = (c1 , . . . , cn )
in [0, 1]n . The i-th coordinate of c indicates the fraction of player is resources contributed to c.
851

fiZ ICK , M ARKAKIS , & E LKIND

In what follows, we will omit the word partial, and refer to vectors in [0, 1]n as coalitions. Of
particular interest are the players who actively participate in c. These players may rightfully claim
a share of cs profits, and they are the ones who may potentially get hurt if some agent changes its
contribution to c. Formally, the support of the coalition c is the set supp(c) = {i  N | ci > 0}.
We now recall the formal definition of OCF games, given by Chalkiadakis et al. (2010).
Definition 2.1. An OCF game G = hN, vi is given by a set of players N = {1, . . . , n} and a
characteristic function v : [0, 1]n  R+ , which assigns a non-negative real value to each partial
coalition; we require that v(0n ) = 0.
The characteristic function of an OCF game can be quite general; we do not even require v to be
monotone. The reasoning behind this apparent leniency is that, given v, the agents in N may form
several coalitions in order to optimize their revenue, a process which results in a coalition structure.
There is, however, one additional assumption on v that we would like to impose; to introduce it, we
first need to formally define the concept of a coalition structure in an OCF game.
P
A coalition structure over N is a finite list of coalitions CS = (c1 , . . . , cm ) such that m
j=1 cj 
1n . The coalition structure CS can be thought of as an n  m matrix whose rows sum to at most 1.
The requirement that the rows sum to no more than 1 means that CS is a valid division of players
resources, i.e., each player gives at most 100% of its resources to CS . For readability, we use set
notation when referring to coalition structures: e.g., when referring to a coalition c that appears in
CS , we write c  CS , and when referring to a coalition structure CS 0 that is a sublist of CS , we
write CS 0  CS .
Recall that a coalition structure in a classic cooperative game is a partition of the agent set, i.e., a
collection of disjoint subsets of N whose union is N . In particular, no set of agents can appear twice
in the same coalition structure. In contrast, in OCF games, it is possible for a coalition structure CS
to have the same coalition appearing more than once. This simply means that the respective agents
are completing two separate tasks, which require the same resources and generate the same revenue.
Example 2.2. Consider a setting where two researchers collaborate. They can write a single long
research paper, which will require all of their time, and will generate a revenue of $100000 (in, say,
grant money or performance payment). Alternatively, if they split their time equally between two
research projects, they will produce two shorter papers, which generate $70000 each. In this setting,
it is in the agents best interest to split into two identical coalitions of the form ( .5
.5 ) rather than form
a single coalition.
P
The total revenue generated by the coalition structure CS is simply cCS v(c), and is denoted
by v(CS ).
Given a set of agents S  N , we say that a coalition structure CS is over S if supp(c)  S
for all c  CS . We denote the set of all coalition
structures over S by CS(S). The weight vector
P
of a coalition structure CS is w(CS ) = cCS c. The vector w(CS ) indicates the total amount
of resources that each player in N invests in the coalition structure CS . Note that w(CS )  [0, 1]n ,
and if CS  CS(S), then w(CS )  eS ; we say that a coalition structure CS over S is efficient
if w(CS ) = eS . We also write wS (CS ) to denote the total weight of S in CS ; namely, the i-th
coordinate of wS (CS ) equals the i-th coordinate of w(CS ) if i is in S, and is 0 otherwise.
As shown in Example 2.2, agents may form coalition structures in order to increase their revenue. Such considerations naturally give rise to the following definition. The superadditive cover
852

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

of v is defined as
v  (c) = sup{v(CS ) | CS  CS(N ), w(CS )  c}.
The value v  (c) is the most that the players can make using the resources of c. Note that a similar
notion of a superadditive cover exists for classic coalitional games (Aumann & Dreze, 1974): given
a function u : 2N  R, its superadditive cover is the function u : 2N  R defined as
X
u (S) = max{
v(T ) | P is a partition of S}.
T P

Recall that a function f : Rn  R is called superadditive if for all c, d  Rn we have
f (c) + f (d)  f (c + d);

(1)

if f is defined on a subdomain U of Rn , condition (1) is only imposed on vectors c, d such that
c + d is still in U . This definition has the following discrete analogue: a function u : 2N  R is
called superadditive if for all sets S, T  N such that S  T =  we have
u(S) + u(T )  u(S  T ).

(2)

These definitions explain the term superadditive cover: it is easy to see that v  is the minimal
superadditive function from [0, 1]n to R such that v  (c)  v(c) for every c  [0, 1]n , and, similarly,
u is the minimal superadditive function from 2N to R such that u (S)  u(S) for every S  N .
We are now ready to formulate the additional requirement that we wish to impose on v.
Definition 2.3. A characteristic function v has the efficient coalition structure property if for every
c  [0, 1]n there exists a CS  CS(N ) such that w(CS ) = c and v  (c) = v(CS ).
For characteristic functions that have the efficient coalition structure property, the sup in the definition of v  can be replaced with max. In what follows, we limit our attention to OCF games whose
characteristic functions enjoy this property. To see why the efficient coalition structure property is
desirable in the OCF setting, consider the function f given by f (x) =  for all x  [0, 1]n \ {0n },
where  is some positive constant. This function does not have the efficient coalition structure
property and its superadditive cover is f  (x) =  for all x  [0, 1]n \ {0n }. In particular, this
means that no coalition structure is optimal for f : it is always possible to increase the social welfare
by splitting some coalition into two non-zero coalitions. We remark that Chalkiadakis et al. (2010)
impose a number of conditions on the characteristic function of an OCF game that, taken together,
imply the efficient coalition structure property; we find it more convenient to state this property
directly.
To provide some intuition for the concepts introduced so far, we now describe a simple class
of OCF games. It is often useful to think of the agents as using their resources to complete a
given set of tasks. Chalkiadakis et al. (2010) describe a class of OCF games, which they call
Threshold Task Games (TTGs), that are based on this idea. A TTG is specified by a finite list of
tasks, T = {t1 , ..., tk }, where each task t` requires some weight (t` )  0 for its completion and
gives a certain payoff p(t` )  0, and a list of players weights, denoted (w1 , . . . , wn ). Each player
can allocate any fraction of its weight to the completion of any task. The worth of a coalition is
v(c) = max{p(t` ) | (t` ) 

n
X
i=1

853

ci wi };

fiZ ICK , M ARKAKIS , & E LKIND

that is, the value of a coalition is simply the value of the highest-paying task that the agents can
complete using their combined weight. Threshold Task Games are a natural extension of Weighted
Voting Games (WVGs) (Chalkiadakis et al., 2011). Indeed, in a weighted voting game, each agent
i  N has a non-negative weight wi , and the value of a set of players S  N is 1 if the combined
weight of the members of S is at least some given non-negative quota q, and 0 otherwise. Thus,
weighted voting games are simply threshold task games where there is a single task with weight q
and payoff 1, and overlapping coalitions are not allowed.
2.3 Payoff Division
Having split into coalitions and generated revenue, the agents have to decide how to divide this
revenue amongst themselves in an agreeable manner. We will now formally describe payoff division
in OCF games.
An imputation for a coalition structure CS = (c1 , ..., cm )  CS(N ) is a list of vectors x =
(x1 , ..., xm ); for each j = 1, . . . , m, xj is a vector in Rn that describes how much each agent in N
receives from cj . Given a coalition c  CS and an imputation x for CS , we refer to the way the
value of c is divided as x(c); thus, the payoff to agent i from coalition c  CS under x is x(c)i .
In order for x to be a valid division of payoffs among the agents, it should satisfy the following
conditions.
P
i
 {i}
Individual Rationality:
cCS x(c)  v (e ) for all i  N .
P
Payoff Distribution: for all c  CS it holds that iN x(c)i  v(c), and if ci = 0 then x(c)i = 0.
We denote the set of all imputations for CS by I(CS ). Observe that our definition does not allow
inter-coalitional transfers. That is, if an agent i  N is not in the support of c, then i cannot
expect to receive any payoff from c. We call a tuple (CS , x), where CS is a coalition structure and
x  I(CS ), a feasible outcome. Given a set S  N , we denote by F(S) the set of all feasible
outcomes (CS , x) with CS  CS(S). Thus, F(N ) refers to all possible ways that agents can form
coalition structures and divide payoffs.
Given a feasible
outcome (CS , x)  F(N ), we define the payoff to an agent i  N as
P
i
p (CS , x) = cCS x(c)i . This is the total payoff of i from all coalitions in CS . Observe that
pi (CS , x) is uniquely defined: for any given outcome (CS , x), the total payoff to player i depends
solely on the amount of payoff received from the coalitions in CS , which
by x, and
P is determined
S
i
x alone. Similarly, the total payoff to a set S is given by p (CS , x) = iS p (CS , x).
Definition 2.4. Given a set S  N and a coalition structure CS  CS (N ), the coalition structure
CS reduced to S is defined as
CS |S = (c  CS | supp(c)  S) .
These are all coalitions that are comprised solely of members of S.
The coalitions in CS |S are fully controlled by S. Hence, a deviation by the members of S would
only affect non-deviators if S changes its contribution to coalitions outside of CS |S .
We are now ready to formally define deviations in OCF games. Loosely speaking, a deviation
by a set S  N from a coalition structure CS  CS(N ) specifies the amount of resources that S
will withdraw from coalitions not in CS |S .
854

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Definition 2.5. Suppose that CS \CS |S = (cj1 , . . . , cjk ). A coalition structure CS 0 = (d1 , . . . , dk )
is a deviation of S from CS if for all ` = 1, . . . , k it holds that d`  cj` and d`  eS .
The deviation CS 0 describes how a set S withdraws resources from coalitions in CS \ CS |S .
The requirement that d`  cj` captures the fact that S cannot withdraw more resources from the
coalition cj` than it has invested in it. We also require that d`  eS since the deviation CS 0 can
involve members of S only.
It will sometimes be convenient to think of a deviation as a function that, for each coalition c
in CS \ CS |S , outputs how much agents in S withdraw from c. That is, given a coalition structure
CS  CS(N ), a set S  N , a deviation CS 0 of S from CS , and a coalition cj`  CS \ CS |S ,
we write dCS 0 (cj` ) to refer to the coalition d` in CS 0 that corresponds to the deviation of S from
cj` . Under this notation, (dCS 0 (cj` ))i specifies the amount of resources that agent i withdraws from
the coalition cj` under CS 0 . When the deviation CS 0 is understood from the context, we omit the
subscript CS 0 from dCS 0 (c) and refer to the deviation of S from c as d(c).
Note that if S withdraws all of its resources from CS \ CS |S , then the total weight available to
S is w(CS |S ) + wS (CS \ CS |S ) = wS (CS ).
Example 2.6. Consider a three-player game described as follows. There are several types of tasks:
 t12 requires 50% of player 1s resources and all of player 2s resources, and its payoff is 16;
 t012 requires 40% of player 1s resources and all of player 2s resources, and its payoff is 10;
 t13 requires 50% of player 1s resources and all of player 3s resources, and its payoff is 20;
 t1 requires 10% of player 1s resources and its payoff is 3.
The optimal coalition structure in this case is

 0.5 
 0.5 
CS = c1 = 1 , c2 = 0
.
0

1

The coalitions c1 and c2 complete tasks t12 and t13 , respectively; thus, v(c1 ) = 16 and v(c2 ) = 20.
A deviation of player 1 from CS would be a coalition structure
     
CS 1 =
0 , 0
0

0

such that   0.5 and   0.5. If  = 0, then player 2 is unaffected by the deviation; if  = 0 then
player 3 is unaffected by the deviation.
It is possible that player 1 deviates from c1 while not changing its contribution to c2 , or vice
versa. Moreover, it is possible that a coalition from which resources were withdrawn can still
generate some revenue. For instance, if player 1 withdraws 10% of its resources from c1 , what
remains of c1 still generates a revenue of 10. This opens opportunities for more nuanced postdeviation behavior: player 3 may decide not to break its agreement with player 1 if  = 0, and if
 = .1 and player 2s payment in the original coalition structure was less than 10, player 1 can hope
to get some payment from its reduced collaboration with player 2.
As Example 2.6 illustrates, deviations in OCF games may leave some of the non-deviators unaffected by the deviation. Moreover, it may be possible for deviators to ensure that the non-deviators
still receive the payoff they were allocated prior to the deviation. If deviators agree to assume the
marginal loss of their deviation, it is possible that the non-deviators will agree to maintain coalitions
with them. These notions are formalized in Section 3.
855

fiZ ICK , M ARKAKIS , & E LKIND

3. The Arbitration Function
In classic cooperative game theory, a set S that is unhappy with its payoffs and wants to deviate
compares its current payoff with what it can earn on its own. However, as Example 2.6 shows,
the complex structure of deviations in OCF games can lead to a variety of ways in which agents
may react to a deviation. If a deviating set still keeps some resources invested in coalitions with
non-deviators, it may be the case that it will continue to receive payments from these coalitions.
To discuss stability in OCF games, we need to describe how the agents react if some set S  N
deviates from (CS , x). One can think of this process in the following manner: given an outcome
(CS , x) and a deviation CS 0 of S  N from CS , the set S may use all available resources in CS |S
and CS 0 in order to generate revenue for itself. Moreover, each coalition c in CS \ CS |S needs
to decide how much payoff it gives to the deviators; this behavior is specified by the arbitration
function.
Definition 3.1. Given an outcome (CS , x), a set S  N and a deviation CS 0 of S from CS ,
the arbitration function A is a mapping that assigns a real value c (CS , x, S, CS 0 ) to each c 
CS \ CS |S .
The value c (CS , x, S, CS 0 ) represents the amount that the coalition c will pay to S, given the
current outcome (CS , x), the identity of the deviators, and the nature of their deviation.
3.1 Properties of Arbitration Functions
Clearly, we need to impose some restrictions on the amount that the deviators may expect to get
from the non-deviators. For example, it would be unreasonable for a deviating set to expect a payoff
that exceeds the post-deviation value of the respective coalition. Another natural restriction is that,
for deviators to get any payoff from a given coalition, every non-deviator in that coalition should
receive the same payoff as before the deviation, i.e., the non-deviators will never agree to pay the
deviating set S at the expense of the non-deviating members.
Formally, given an outcome (CS , x), a set S  N , a deviation CS 0 of S from CS , and an arbitration function A whose output on (CS , x, S, CS 0 ) is described by the list of values (c )cCS \CS |S ,
we require A to have the following properties:
P
Accountability: c (CS , x, S, CS 0 )  max{v(c  d(c))  iN \S x(c)i , 0}.
Deviation-Monotonicity: For every pair of subsets S, T  N such that S  T and their respective
deviations CS 0 and CS 00 from CS such that dCS 0 (c)  dCS 00 (c) for each c  CS \ CS |S , it
holds that c (CS , x, S, CS 0 )  c (CS , x, T, CS 00 ).
The first condition states a general upper bound on the amount that a deviating set can expect to
receive from a coalition: after the deviation, the coalition c can generate a profit of v(c  d(c)), so
the most that a deviating set can expect to receive is v(c  d(c)), minus the original payments given
to non-deviators under (CS , x). The second condition simply states that the punishment imposed
by an arbitration function should be monotone in the size of the violation: if the deviators withdraw
a smaller amount of resources from each coalition, they should receive at least as much payoff as
they received under their original deviation. The rationale behind the first condition is strategic in
nature; it stems from the assumption that a set of agents engaged in a task will not agree to pay
deviating members if the deviators cannot ensure that each non-deviator is paid the same amount as
856

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

prior to the deviation. We do stress however, that accountability is not a necessary component in our
proofs: our results still hold even if one does not assume accountability. The second condition, while
sensible as well, is instrumental in proving Theorem 3.4, and is a necessary assumption. Finally,
note that it is possible that a coalition imposes a fine on deviating members, that is, the value of c
need not be positive.
The total revenue of a deviating set S given its deviation CS 0 from CS can be written as
A(CS , x, S, CS 0 ) = v  (w(CS |S ) + w(CS 0 )) +

X

c (CS , x, S, CS ).

cCS \CS |S

Given an outcome (CS , x) and a set S  N , the most that S can get by deviating from (CS , x) is
A (CS , x, S) = sup{A(CS , x, S, CS 0 ) | CS 0 is a deviation of S from CS }.
Now, in order for S to deem a deviation profitable, each of its members should stand to gain
from it. The formal definition that captures this idea is somewhat complicated, as it has to describe
how the deviators share the profit/loss allocated to them by the arbitration function.
Definition 3.2. Given an outcome (CS , x) and a deviation CS 0 of S  N from CS , we say that
CS 0 is an A-profitable deviation if we can construct
1. a coalition structure CS d such that w(CS d ) = w(CS |S ) + w(CS 0 ) together with an imputation xd  I(CS d ), and
2. a collection of vectors y = (yc )cCS \CS |S , such that for every c  CS \ CS |S it holds that
(a) yc  (R+ )n if c (CS , x, S, CS 0 )  0 and yc  (R )n otherwise,
(b) (yc )i = 0 for i 6 S  supp(c  d(c)), and
Pn
0
i
(c)
i=1 (yc ) = c (CS , x, S, CS ),
P
so that for every i  S it holds that pi (CS d , xd ) + cCS \CS |S (yc )i > pi (CS , x).
Simply put, given Ss deviation from (CS , x) and the arbitration function A, the agents in S
have to agree on three things. First, they have to decide on a coalition structure to form with their
post-deviation resources. Second, they have to come up with a way of dividing profits from that
coalition structure. Third, they have to agree on a way of dividing the payoffs (or fines) assigned
to them by the arbitration function. For the deviation to be profitable, these agreements should be
such that all agents in S receive strictly more than what they had received under (CS , x). If no
such agreement exists, S cannot A-profitably deviate. Given (CS d , xd ) and y, the total payoff of
an agent i  S from deviating is denoted q i (xd , y).
A priori, it seems possible that, even when the total payoff to a deviating set is more than
S
p (CS , x), the members of S may be unable to divide the revenue from the deviation in a way
that benefits them all due to coalitional restrictions. Chalkiadakis et al. (2010) show that for the
conservative arbitration function (see Section 3.2) this is not the case: if there is a way for agents to
jointly gain more by deviating, then they can divide the profits in a way that benefits them all. In
Theorem 3.4, we will show how to extend this result to general arbitration functions.
857

fiZ ICK , M ARKAKIS , & E LKIND

Remark 3.3. It is possible to incorporate a freely divisible value into the arbitration function. This
value is a payoff (or fine) assigned to the deviators by an external entity. Just as the payments from
the coalitions in CS \ CS |S , this payment may depend on the original outcome, the identity of
the deviating set, and the nature of its deviation. All results in this paper carry through with the
addition of a freely divisible value. Intuitively, this value allows us to extend notions such as the
-core (Maschler, Peleg, & Shapley, 1979) to OCF games.
3.2 Some Arbitration Functions
We now present some arbitration functions and briefly discuss their properties. Three of these
arbitration functions correspond to the three notions of the core introduced by Chalkiadakis et al.
(2010) (the conservative core, the refined core and the optimistic core), and the names we use for
these arbitration functions reflect this; in contrast, the sensitive arbitration function is based on a
new idea.
3.2.1 T HE C ONSERVATIVE A RBITRATION F UNCTION
The simplest assumption that one can make with respect to reaction to deviation is that the nondeviators will react by voiding all agreements with the deviators; that is, the deviators do not expect
any payment from any of their coalitions with the non-deviators, regardless of their contribution.
This reasoning gives rise to the conservative arbitration function; formally, it is defined by setting
c  0 for any given input. Under the conservative arbitration function, a deviating set is not
rewarded in any way for continued interaction with the non-deviators; thus, the profit it can hope
to gain by deviating is exactly what it can make on its own. Therefore, under the conservative
arbitration function, denoted by Ac , we have Ac (CS , x, S) = v  (eS ) for every outcome (CS , x).
3.2.2 T HE S ENSITIVE A RBITRATION F UNCTION
When reasoning about agents reaction to deviation, a natural assumption to make is that the nondeviators do not care what the deviating set S does, as long as the deviation does not affect them.
This approach is captured by the sensitive arbitration function. Under this function the payment
that the deviators obtain from a coalition c  CS \ CS |S is 0 if there is some c0  CS \ CS |S such
that d(c0 ) 6= 0 and supp(c)  supp(c0 )  N \ S 6= ; otherwise, the payment to S is the total payoff
that the members of S were receiving from c prior to the deviation.
Under the sensitive arbitration function, denoted by As , the most that a deviating set S can get
can be computed as follows. First, it is clear that S does not benefit from investing resources into
joint projects with non-deviating agents that will be hurt in any way by the deviation. Thus, S only
needs to decide which agents it will keep collaborating with; as for all other agents, it can break all
agreements with them. In order to describe payments under the sensitive arbitration function, we
will employ the following notation. Given a set T  N , we let CS T be the set of all coalitions in
CS that involve members of T ; that is
CS T = (c  CS | supp(c)  T 6= ).
Now, if a set S  N chooses to break agreements that involve members of T , it forgoes all payments
from coalitions in CS T , but can use the resources it has invested in CS T in order to maximize its
own profits. That is, we obtain


	
As (CS , x, S) = max v  w(CS |S ) + wS (CS T ) + pS (CS \ (CS |S  CS T ), x) | T  N \ S .
858

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

This definition can be extended as follows: instead of being completely self-interested, each
agent i  N has a set of players Si  N that it cares about. If any of the agents in Si are hurt by
a deviation, agent i will not allow the deviating set to have any payoff from the coalitions that it is
involved in.
3.2.3 T HE R EFINED A RBITRATION F UNCTION
The sensitive arbitration function focuses on the impact of the deviation on individual agents; in
contrast, the refined arbitration function makes its decisions based on the deviations impact on
each coalition. Specifically, under this arbitration function a coalition c allows the deviators to keep
their payoffs from c if and only if S has not withdrawn any resources from c. Formally, given a
deviation CS 0 of S  N from CS in the outcome (CS , x), the refined arbitration function, denoted
Ar , will let S keep its payoff from a coalition c  CS \ CS |S if d(c) = 0, and allocate S no payoff
from c otherwise. Note that the refined arbitration function is more generous to the deviators than
the sensitive arbitration function: if a deviator i and a non-deviator j are both involved in coalitions
c and c0 , and c0 is affected by the deviation while c is not, under the refined arbitration function i
will receive its share of cs payoffs, whereas under the sensitive arbitration function it will not.
The most that a set S can expect to gain by deviating under the refined arbitration function can
be computed as follows. First, note that given a coalition c in CS \ CS |S , S should either withdraw
all resources from c or none at all. Thus, S needs to find the best set of coalitions in CS from which
to fully withdraw resources. Formally, we have
c )) + pS (CS \ CS
c , x) | CS |S  CS
c  CS }.
Ar (CS , x, S) = max{v  (wS (CS
3.2.4 T HE O PTIMISTIC A RBITRATION F UNCTION
A yet more lenient reaction to deviation is captured by the optimistic arbitration function, denoted
Ao . Under this arbitration function a coalition is willing to pay the deviating set as long as the
non-deviators receive their pre-deviation payoffs. Since we require arbitration functions to have the
accountability property, this means that the optimistic arbitration function gives a deviating set the
highest possible payoff it can receive under any arbitration function. Formally, given a set S, an
outcome (CS , x) and aP
deviation CS 0 of S from CS , the payoff that S gets from a coalition c is
i
max{0, v(c  d(c))  iS
/ x(c) }.
In order to compute the most that a set S can get by deviating under the optimistic arbitration
function, we need to determine the amount of resources that S is going to withdraw from every
coalition it is involved in. We obtain
X
c ) + w(CS 0 )) +
c , x)},
Ao (CS , x, S) = sup{v  (wS (CS
v(c  d(c))  pN \S (CS \ CS
c
cCS \CS

c such that CS |S  CS
c  CS (intuwhere the supremum is taken over all coalition structures CS
itively, these are the coalitions of CS from which the members of S withdraw all of their resources)
c.
and over all deviations CS 0 of S from CS \ CS
3.3 Redefining A-Profitable Deviations
The definition of an A-profitable deviation is somewhat difficult to work with. To remedy this,
we will now provide a simple sufficient condition for the existence of an A-profitable deviation.
859

fiZ ICK , M ARKAKIS , & E LKIND

Chalkiadakis et al. (2010) prove a similar result for the conservative arbitration function, and we
use some of their techniques (most notably, a coloring argument) in our proof.
Theorem 3.4. Consider an OCF game G = hN, vi, an outcome (CS , x) of G and an arbitration
function A. If A (CS , x, S) > pS (CS , x) for some S  N , then there is a subset of S that can
A-profitably deviate from (CS , x).
Proof. Consider a set S  N such that A (CS , x, S) > pS (CS , x). First, note that the space
of possible deviations of S from the coalition structure CS can be viewed as a subset of RD (for
an appropriate value of D) that is bounded and can be described by finitely many non-strict linear
inequalities. Therefore this space is compact. Thus, there exists a deviation of S from (CS , x)
maximizing the total payoff that S can derive from deviating; let CS 0 be some such deviation.
Since v has the efficient coalition structure property, there exists a coalition structure CS d such
that v(CS d ) = v  (w(CS |S ) + w(CS 0 )). For every c  CS \ CS |S , let c be the amount that
S  supp(c  d(c)) receives from c after the deviation. Our objective is to find an imputation
xd  I(CS d ) along with payoff divisions of (c )cCS \CS |S that would witness that a subset of S
has an A-profitable deviation from (CS , x).
Given a coalition c  CS \ CS |S , let Ic be the set of all possible ways that c can be divided
among the members of S  supp(c  d(c)). Formally, if c  0, then
Ic = {yc  (R+ )n | supp(yc )  S  supp(c  d(c)) and

n
X

(yc )i = c };

i=1
n
if c < 0, then IQ
c is defined similarly, with all yc  Ic being vectors in (R ) . Given xd 
I(CS d ) and y  cCS \CS |S Ic , recall that q i (xd , y) is the payoff of agent i from the deviation
when the payoffs from CS d and the arbitration functionQ
payoffs are divided according to xd and y,
respectively. We now define a function TL : I(CS d )  cCS \CS |S Ic  R as follows:

TL(xd , y) =

X


	
min pi (CS , x)  q i (xd , y), 0 .

iS

The function TL measures the total loss incurred by the members of S who are hurt by the deviation;
these are the members of S who, despite joining the deviators,
Q do not enjoy a profit from deviating.
Observe that TL is continuous, and that both I(CS d ) and cCS \CS |S Ic are compact sets; thus,
TL attains its minimum value on its domain. Among all minimizers of this function, pick one
for which the number of agents i with pi (CS , x) < q i (xd , y) is the largest; denote this point by
(x0 , y0 ).
Now, let us color the agents in S as follows: an agent i  S is green if pi (CS , x) < q i (x0 , y0 );
red if pi (CS , x) > q i (x0 , y0 ) and white if pi (CS , x) = q i (x0 , y0 ). Green agents derive a strictly
positive benefit from the proposed deviation, red agents suffer a loss, and white agents break even.
Now, consider a green agent g, and suppose that g can legally transfer payoff to some agent i
under x0 and y0 , i.e., (a) both g and i are in the support of a coalition c in CS d and g receives
a positive payoff from c, or (b) both g and i are in the support of a coalition c  d(c) such that
c  CS \ CS |S , c > 0, and g receives a positive payoff from c, or (c) both g and i are in the
support of a coalition c  d(c) such that c  CS \ CS |S , c < 0, and i receives a negative payoff
from c. If i is red, g can transfer a small positive payoff to i while remaining green, and the resulting
payoff division would have a strictly lower value of TL than (x0 , y0 ), a contradiction. Similarly,
860

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

if i is white, g can transfer a small amount to i, making it green as well, without changing the
value of T L; this is a contradiction, since we chose (x0 , y0 ) so as to maximize the number of green
agents. Thus, if both g and i are in the support of some coalition after the deviation (i.e., either a
coalition in CS d or a surviving coalition in CS \ CS |S ), then g cannot receive a positive payoff
from that coalition. By the same argument, if a coalition in CS \ CS |S assigns a negative payoff to
the deviating agents under A, then only the green agents incur a loss from that coalition.
Let Sg be the set of green agents in S. We have argued that the agents in Sg receive no payoff
from coalitions they form with non-green agents in S. It follows that Sg can A-profitably deviate
from (CS , x). Indeed, suppose that the agents in S \ Sg do not deviate, and the agents in Sg deviate
by only withdrawing resources they need for forming the coalitions among themselves in CS d ;
the resources that Sg withdrew from CS in the deviation CS 0 in order to invest in coalitions with
members of S \ Sg in CS d are kept as is. Since A is deviation-monotone, the payment that Sg
receives from the arbitration function under this new deviation is weakly higher than under CS 0 .
Furthermore, suppose that Sg forms the same coalitions as in CS d |Sg , divides payoffs from these
coalitions according to x0 , and distributes the arbitration function payoffs according to y0 , with any
surplus payments from the arbitration function shared arbitrarily. Then the payoffs of all members
of Sg are at least as high as under CS 0 , x0 and y0 ; since these agents were green, each of them
benefits from this new deviation. Finally, note that Sg 6= , since pS (CS , x) < A (CS , x, S). This
concludes the proof.
Simply put, Theorem 3.4 states that if the most that a set S can get by deviating from (CS , x)
is strictly greater than its payoff from (CS , x), then there is a non-empty subset of S that can
A-profitably deviate. Note that the distinction between coalitional value and the profitability of
deviating exists in cooperative games with coalition structures as well.
Example 3.5. Consider a modified version of the three player 2-majority game: the agent set is
N = {1, 2, 3}, the value of any coalition S  N of size at least 2 is 6, and the value of each
singleton is 1. Suppose that agents form the coalition N , and divide payoffs so that each agent
receives 2. The set N can earn more by forming the coalition structure ({1, 2}, {3}), whose value
is 7. However, there is no way to divide payoffs from this coalition structure so that every agent
receives more than 2: agent 3 can only receive 1, so it is not better off. Thus, N cannot deviate as
a whole, but the coalition {1, 2} or the singleton {3} (or both, depending on how the value of the
grand coalition is distributed) have a profitable deviation.
Theorem 3.4 provides us with a justification for using A (CS , x, S) as a measure of the deviators satisfaction with the outcome. Indeed, if S can A-profitably deviate from (CS , x) via some
deviation CS 0 , then clearly A (CS , x, S) > pS (CS , x). On the other hand, if A (CS , x, S) >
pS (CS , x), then there is some T  S that can A-profitably deviate from (CS , x).

4. The Arbitrated Core
Given an OCF game G = hN, vi and an arbitration function A, we say that an outcome (CS , x)
is A-stable if there is no set S  N that can A-profitably deviate from (CS , x). The arbitrated
core of G with respect to an arbitration function A, or the A-core of G (denoted Core(G, A)), is the
set of all A-stable outcomes of G. Using Theorem 3.4, we obtain the following characterization of
A-stable outcomes.
861

fiZ ICK , M ARKAKIS , & E LKIND

Theorem 4.1. Let G = hN, vi be an OCF game. An outcome (CS , x) is in the A-core of G if and
only if for every S  N we have pS (CS , x)  A (CS , x, S).
Proof. First, suppose that for every S  N we have pS (CS , x)  A (CS , x, S). Let CS 0 be
an arbitrary deviation of S from CS . We have pS (CS , x)  A(CS , x, S, CS 0 ). Thus, no matter
how S divides the deviation payoffs, there would be at least one agent i  S who gets no more
than pi (CS , x). As this is true for an arbitrary deviation CS 0 , it follows that S cannot A-profitably
deviate.
Conversely, suppose that pS (CS , x) < A (CS , x, S). By Theorem 3.4, there is a subset of S
that can A-profitably deviate from (CS , x), thus (CS , x) is not A-stable.
Let A and A be two arbitration functions. Intuitively, it is clear that if A is more lenient than
A, then the A-core is contained in the A-core. Indeed, if an outcome (CS , x) is stable with respect
to A and A always pays more to the deviators than A (thus making the deviation more tempting),
then (CS , x) should be A-stable as well. To formalize this intuition, we use the following notation.
Given two arbitration functions A and A, we write A  A if for every set S  N and every
outcome (CS , x) it holds that A (CS , x, S)  A (CS , x, S). We can now state our observation as
follows.
Corollary 4.2. If A  A then Core(G, A)  Core(G, A).
Proof. Suppose that (CS , x) is in Core(G, A). This means that for all S  N we have pS (CS , x) 
A (CS , x, S). Since A  A, it follows that pS (CS , x)  A (CS , x) for all S  N as well, which
implies (CS , x)  Core(G, A).
The conservative, sensitive, refined and optimistic arbitration functions described in Section 3.2,
denoted Ac , As , Ar and Ao , respectively, satisfy Ao  Ar  As  Ac . Thus, Corollary 4.2
implies that the conservative core contains the sensitive core, which contains the refined core, which
contains the optimistic core. These observations (with respect to the conservative, refined, and
optimistic core only) have been made by Chalkiadakis et al. (2010) as well; in fact, Chalkiadakis
et al. show that these containments can be strict, i.e., there are games for which the conservative core
strictly contains the refined core, and there are games for which the refined core strictly contains the
optimistic core. A similar separation can be shown for the sensitive core.
4.1 Some Arbitrated Cores
Using the characterization result of Theorem 4.1, we now proceed to describe some arbitrated cores,
namely, those corresponding to the arbitration functions introduced in Section 3.2.
4.1.1 T HE C ONSERVATIVE C ORE
As argued in Section 3.2, for every outcome (CS , x) and every set S  N , the most that S can get
under the conservative arbitration function is v  (eS ). Thus, the conservative core is the set of all
outcomes (CS , x) such that for every set S  N it holds that
pS (CS , x)  v  (eS ).
862

(3)

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

4.1.2 T HE S ENSITIVE C ORE
In order for an outcome (CS , x) to be stable under the sensitive arbitration function, for every set
S  N and every T  N \ S it should hold that
pS (CS , x)  v  (w(CS |S ) + wS (CS T )) + pS (CS \ (CS |S  CS T ), x);
recall that CS T is the set of all coalitions in CS that involve members of T . Subtracting payments
associated with coalitions in CS \ (CS |S  CS T ) from both sides of this inequality, we get
pS (CS |S , x) + pS (CS T , x)  v  (w(CS |S ) + wS (CS T )).
Chalkiadakis et al. (2010) show that if CS is not an optimal coalition structure then (CS , x) is
not in the conservative core and consequently it is not in the sensitive core either. Thus, we can
assume that CS is an optimal coalition structure, and hence CS |S is optimal for any S  N , i.e.,
v(CS |S ) = v  (w(CS |S )). We also have pS (CS |S , x) = v(CS |S ). This implies that, in order for
(CS , x) to be in the sensitive core, it must be the case that CS is an optimal coalition structure and
for every S  N and every T  N \ S it holds that
pS (CS T , x)  v  (w(CS |S ) + wS (CS T ))  v  (w(CS |S )),

(4)

i.e., the total payoff to S from CS T must be at least the marginal benefit of using the resources S
has invested in CS T . Note that if we take T = N \ S, we obtain the conservative core condition.
4.1.3 T HE R EFINED C ORE
For the refined core, we have
c )) + pS (CS \ CS
c , x) | CS |S  CS
c  CS }.
Ar (CS , x, S) = max{v  (wS (CS
Thus, an outcome (CS , x) is in the refined core if and only if for every S  N and for every
c containing CS |S it holds that
coalition structure CS
c )) + pS (CS \ CS
c , x).
pS (CS , x)  v  (wS (CS
c it holds that
Consequently, (CS , x) is in the refined core if and only if for every such S and CS
c , x)  v  (wS (CS
c )).
pS (CS

(5)

c containing CS |S should be at
In other words, the payoff to S from every coalition structure CS
c . Note that if we
least as large as the most that S can get from the resources it has invested in CS
c
c
limit ourselves to coalition structures CS such that CS = CS |S  CS T for some T  N \ S, we
obtain the sensitive core condition.
4.1.4 T HE O PTIMISTIC C ORE
For the optimistic arbitration function, Ao , we have shown that
X
c ) + w(CS 0 )) +
c , x)},
Ao (CS , x, S) = sup{v  (wS (CS
v(c  d(c))  pN \S (CS \ CS
c
cCS \CS

863

fiZ ICK , M ARKAKIS , & E LKIND

c such that CS |S  CS
c  CS and over
where the supremum is taken over all coalition structures CS
c . Thus, in order for an outcome (CS , x) to be in the optimistic
all deviations CS 0 of S from CS \ CS
c  CS containing
core, it must be the case that for every set S  N , every coalition structure CS
0
c
CS |S and every deviation CS of S from CS \ CS it holds that
X
c , x). (6)
c ) + w(CS 0 )) +
v(c  d(c))  pN \S (CS \ CS
pS (CS , x)  v  (wS (CS
c
cCS \CS

c , CS 0 ) denote the coalition structure formed by what remains of CS \ CS
c after
Let (CS \ CS
P
0
S has withdrawn resources from CS according to CS . We obtain cCS \CS
c v(c  d(c)) =
0
S
c , CS )). Thus, subtracting p (CS \ CS
c , x) from both sides of inequality (6) and
v((CS \ CS
N
c
c
using the fact that p (CS \ CS , x) = v(CS \ CS ), we can rewrite inequality (6) as


c , x)  v  (wS (CS
c ) + w(CS 0 ))  v(CS \ CS
c )  v((CS \ CS
c , CS 0 )) .
pS (CS

(7)

The optimistic core stability condition is thus similar to the refined core stability condition; however,
c if it is willing to assume the
S is also allowed to withdraw resources from coalitions outside of CS
marginal costs of this withdrawal.
Example 4.3. Consider the following example. We have three players, N = {1, 2, 3}, and the
following characteristic function:
 
 
0
0
1
1
1
1
v 4 = 1,
v 2 = 40,
v 41 = 20,
0

0

2

and v(c) = 0 for every other coalition c  [0, 1]3 . First, we observe that the only optimal coalition
structure (up to the order of coalitions) is CS = (c1 , c2 , c3 ), where
0
0
 
1
1
1
1
c2 = 41 ,
c3 = 41 .
c1 = 2 ,
0

2

2

In words, players 1 and 2 collaborate on one project (which requires 100% of player 1s resources,
and 50% of player 2s resources), and generate a revenue of 40; players 2 and 3 work on two
identical projects (each requiring 25% of player 2s resources and 50% of player 3s resources),
generating a revenue of 40 in total as well. Now, suppose that payoffs are divided according to
x = (x1 , x2 , x3 ), where
0
0
 40 
x2 = 0 ,
x3 = 4 .
x1 = 0 ,
0

20

16

Under the conservative arbitration function, this payoff division is stable: no subset of players
receives less than the most it can make on its own. However, under the sensitive arbitration function,
this is no longer the case: player 2 must receive a payoff of at least 2 from c1 , and a combined payoff
of at least 2 from c2 and c3 .
Now, let us suppose that the following payoff division is proposed instead: y = (y1 , y2 , y3 ),
where
 38 
0
0
y2 = 0 ,
y3 = 2 .
y1 = 2 ,
0

20

864

18

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

The outcome (CS , y) is stable with respect to the sensitive arbitration function; however, it is not
stable with respect to the refined arbitration function: player 2 can withdraw its contribution to c2
and earn strictly more. Thus, in order for an imputation to be r-stable, player 2 must receive at least
2 from c1 , and at least 1 from each of c2 and c3 .
Example 4.3 illustrates an interesting phenomenon: more lenient arbitration functions force
payments to be more spread out: player 2 receives the same payoff, but the payoff must be received
in a more distributed manner when more lenient reactions to deviations are assumed.

5. Non-emptiness of Arbitrated Cores
Core outcomes are highly desirable both in classic cooperative games and in OCF games. Thus, it
is important to find conditions on the characteristic function of an OCF game that would guarantee
non-emptiness of its arbitrated core, as well as to identify classes of OCF games whose arbitrated
core is always non-empty, for natural arbitration functions. We note that both of these questions can
beand have beenlooked at through the lens of computational complexity. That is, one can ask
whether there exist polynomial-time algorithms that decide whether the arbitrated core is non-empty
or find an outcome in the arbitrated core, either for the general model or for specific classes of OCF
games. While this question has been explored by Chalkiadakis et al. (2010) and Zick et al. (2012), in
this paper we do not consider computational complexity issues and focus instead on characterization
results (see, however, the companion paper by Zick et al., 2014).
Our characterization results are similar in spirit to the celebrated BondarevaShapley condition
for core non-emptiness in classic cooperative games (Bondareva, 1963; Shapley, 1967). We will
now briefly state this condition. Given a set P
N = {1, . . . , n}, a collection of weights (S )SN is
called balanced if S  0 for all S  N and S:iS S = 1 for all i  N . We can view a balanced
collection of weights as a way for an agent i to partially participate in all sets that contain i, with its
contribution specified by the respective weight.
Given a cooperative game G = hN, ui where u : 2N 
PR+ , we say that G is balanced if for
every balanced collection of weights (S )SN it holds that SN S u(S)  u(N ).
Theorem 5.1 (BondarevaShapley Theorem). A cooperative game G = hN, ui with u : 2N  R+
has a non-empty core if and only if it is balanced.
An important feature of this characterization is that it is stated in terms of the characteristic
function itself, and does not explicitly refer to outcomes of the game. In what follows, we describe
similar characterizations for OCF games, for the conservative, sensitive and refined arbitration functions.1 Specifically, for each of these arbitration functions, given an OCF game G = hN, vi and a
coalition structure CS  CS(N ), we characterize the set of imputations x  I(CS ) such that
(CS , x) is in the respective arbitrated core of G. We limit our attention to optimal coalition structures, i.e., we assume that v(CS ) = v  (eN ): if CS is not optimal, then no outcome of the form
(CS , x) can be in the arbitrated core of G for any of our arbitration functions, since the grand
coalition itself can profitably deviate from (CS , x) (Chalkiadakis et al., 2010).
1. The case of the conservative arbitration function has been considered by Chalkiadakis et al. (2010); we reproduce
their results for completeness and to facilitate the comparison with the other two cases.

865

fiZ ICK , M ARKAKIS , & E LKIND

5.1 The Conservative Core
Given an OCF game G = hN, vi and a coalition structure CS = (c1 , . . . , cm ), we say that a
collection of non-negative weights {(rj )m
j=1 ; (S )SN } is c-balanced with
P respect to CS if for every
i  N and every coalition cj such that i  supp(cj ) it holds that rj + S:iS S = 1. Chalkiadakis
et al. (2010) show the following result.
Theorem 5.2 (Chalkiadakis et al., 2010, Thm. 2). Given an OCF game G = hN, vi and an optimal coalition structure CS  CS(N ), there exists an outcome x  I(CS ) such that (CS , x) is in the
conservative core of G if and only if for every collection of non-negative weights {(rj )m
j=1 ; (S )SN }
that is c-balanced with respect to CS it holds that
m
X

rj v(cj ) +

j=1

X

S v  (eS )  v  (eN ).

SN

Our first observation is that the proof of Theorem 5.2, as given in the work of Chalkiadakis et al.
(2010), has a gap. Appendix A contains a correction to the proof.
We will now provide an alternative characterization of OCF games with non-empty conservative
core, by establishing a connection between the conservative core of an OCF game and the core of a
related classic cooperative game.
Given an OCF game G = hN, vi, its discrete superadditive cover is a classic cooperative game
G = hN, Uv i, where Uv (S) = v  (eS ). Simply put, the value of a set S  N in G is the most
that it can make under the function v by forming a coalition structure. We will now show that the
conservative core of G is non-empty if and only if the core of G is non-empty.
Theorem 5.3. The conservative core of an OCF-game G = hN, vi is non-empty if and only if the
core of its discrete superadditive cover G = hN, Uv i is non-empty.
Proof. We have argued that if the conservative core of G is non-empty, then there exists an outcome
(CS , x) such that for all S  N we have pS (CS , x)  v  (eS ). Note that CS is an optimal coalition
structure for G, since v(CS ) = pN (CS , x)  v  (eN ). Consider the payoff vector p = (p1 , . . . , pn )
where pi = pi (CS , x)
P for all i  N . It follows that p is in the core of G. Indeed, since CS
is optimal, we have ni=1 pi = v(CS ) = v  (eN ) = Uv (N ), and for every S  N we have
p(S) = pS (CS , x)  v  (eS ) = Uv (S).
Conversely, let p = (p1 , . . . , pn ) be a payoff vector in the core of G. Fix a coalition structure
CS = (c1 , . . . , cm ) such that v(CS ) = v  (eN ). We will now use p to construct an imputation
x  I(CS ) such that (CS , x) is in the conservative core of G.
Given an imputation z  I(CS ), we color the agents in N as follows: i is green if pi (CS , z) >
i
p ; red if pi (CS , z) < pi and white if pi (CS , z) = pi . Green agents are better off under (CS , z)
than under p, red agents are worse off under (CS , z) than under p, and white agents are indifferent.
Now, suppose that there is some z  I(CS ) such that no agent in the respective coloring is
green. Since CS is optimal, this means that no agent is red either. Thus, all agents are white. This
in turn implies that for all S  N we have pS (CS , z) = p(S)  Uv (S) = v  (eS ), so (CS , z) is in
the conservative core. Thus, all we need to show is that there exists some z  I(CS ) such that no
agent in the respective coloring is green.
Let F : I(CS )  R be defined as follows:
F (z) =

n
X

max{0, pi  pi (CS , z)}.

i=1

866

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

The function F measures the agents total unhappiness with the outcome (CS , z) as compared to
p. F is a continuous function (it is the sum of the maxima of functions that are continuous in z),
and I(CS ) is a compact set. Thus, there exists an imputation x  I(CS ) that minimizes the value
of F . We pick x  arg min F (z) so as to minimize the number of white agents in the respective
coloring of N .
Now suppose that there exist a green agent g, a non-green agent i and a coalition c  CS such
that g, i  supp(c) and x(c)g > 0. Then there is some  > 0 such that setting x(c)g = x(c)g  
and x(c)i = x(c)i +  (i.e., transferring an amount of  from g to i within the coalition c) results
in a valid payoff division for CS and keeps g green. If i was white prior to this transfer, it now
becomes green (and the value of F does not change). This is a contradiction to the fact that x
minimizes the number of white agents. On the other hand, if i was red, the transfer decreases the
contribution of i to F (while gs contribution to F remains 0), which is a contradiction to the fact
that x  arg min F (z). This means that green agents get zero payoff from coalitions with non-green
agents. Let us denote the set of green agents by Sg . We have
pSg (CS , x) = pSg (CS |Sg , x)  v  (eSg ).
On the other hand, if Sg 6= , since for each i  Sg we have pi (CS , x) > pi , we obtain
pSg (CS , x) > p(Sg )  Uv (Sg ) = v  (eSg ).
This contradiction shows that Sg = , and we have argued that in this case all agents are white.
This completes the proof.
The following corollary is immediately implied by the proof of Theorem 5.3.
Corollary 5.4. Consider an OCF game G = hN, vi. For every payoff vector p = (p1 , . . . , pn ) in
the core of G and every optimal coalition structure CS  CS(N ), there exists an x  I(CS ) such
that pi = pi (CS , x) for all i  N .
Corollary 5.4 enables us to generalize the following well-known result for classic cooperative
games. Aumann and Dreze (1974) show that if CS and CS 0 are optimal (non-overlapping) coalition
structures for G = hN, ui and (CS , p) is in the core of G then (CS 0 , p) is also in the core of G.
In other words, if CS opt is the set of all optimal coalition structures for G, and Istab is the set
of all stable payoff divisions for G, then the set of core outcomes for G is CS opt  Istab . Using
Corollary 5.4, we can extend this result to OCF games under the conservative arbitration function.
Corollary 5.5. Consider an OCF game G = hN, vi. For every pair of optimal coalition structures
CS , CS 0  CS(N ) and every imputation x  I(CS ) such that (CS , x) is in the conservative core,
there exists an imputation x0  I(CS 0 ) such that (CS 0 , x0 ) is in the conservative core of G and
pi (CS , x) = pi (CS 0 , x0 ) for all i  N .
5.1.1 C ONVEXITY IN OCF G AMES R EVISITED
For classic cooperative games, convexity, or supermodularity, of the characteristic function is wellknown to be a sufficient condition for non-emptiness of the core (Shapley, 1971). In more detail,
recall that a cooperative game G = hN, vi is said to be supermodular if for every pair of sets S, T
such that S  T  N and every R  N \ T it holds that
v(T  R)  v(T )  v(S  R)  v(S).
867

fiZ ICK , M ARKAKIS , & E LKIND

Supermodular games are often referred to as convex games in the literature; however, to avoid confusion with other notions of convexity considered in this paper, we use the term supermodularity.
Shapley (1971) proves the following result.
Theorem 5.6. If a cooperative game G is supermodular, then its core is non-empty.
In order to extend Theorem 5.6 to OCF games, Chalkiadakis et al. (2010) propose the following
notion of convexity for OCF games.
Definition 5.7 (OCF convexity, Chalkiadakis et al., 2010). An OCF game is OCF-convex if for
every pair of sets S, T such that S  T  N , every R  N \ T , every outcome (CS S , xS )  F(S),
every outcome (CS T , xT )  F(T ), and every outcome (CS SR , xSR )  F(S  R) the following
condition holds: if pi (CS S , xS )  pi (CS SR , xSR ) for all i  S then there is an outcome
(CS T R , xT R )  F(T  R) such that
(1) pi (CS T , xT )  pi (CS T R , xT R ) for all i  T ;
(2) pi (CS SR , xSR )  pi (CS T R , xT R ) for all i  S  R.
Intuitively, Definition 5.7 simply means that larger coalitions can offer more lucrative options
to players who join them, compared to smaller coalitions.
Chalkiadakis et al. (2010) then show that OCF convexity is a sufficient condition for nonemptiness of the conservative core.
Theorem 5.8 (Chalkiadakis et al., 2010, Thm. 3). If an OCF game G is OCF-convex, then its
conservative core is non-empty.
On the other hand, combining Theorem 5.3 with Theorem 5.6, we obtain the following sufficient
condition for conservative core non-emptiness.
Proposition 5.9. Consider an OCF game G. If G = hN, Uv i is supermodular, then the conservative
core of G is non-empty.
We will now argue that Proposition 5.9 is strictly stronger than Theorem 5.8, by showing that
supermodularity of G is a strictly weaker condition than OCF-convexity of G. We will first show
that OCF-convexity of G implies supermodularity of G. We then present an example of a game
G = hN, vi such that G is supermodular, but G is not OCF-convex (Example 5.13).
To implement the first step of this plan, we first establish the following crucial proposition,
which may seem counterintuitive at first sight. Its proof is based on coloring arguments similar to
the ones used in the proofs of Theorems 3.4 and 5.3.
Proposition 5.10. For every pair of sets S, T such that S  T  N and every pair of optimal
coalition structures CS S  CS(S), CS T  CS(T ), there exist imputations x  I(CS S ) and
y  I(CS T ) such that pi (CS S , x) = pi (CS T , y) for all i  S.
Proof. Fix the sets S and T and optimal coalition structures CS S  CS(S), CS T  CS(T ). We will
first establish that we can find imputations x0  I(CS S ) and y0  I(CS T ) with pi (CS S , x0 ) 
pi (CS T , y0 ) for all i  S. We will then show how to use this fact to prove our original claim.
Lemma 5.11. There exist imputations x0  I(CS S ) and y0  I(CS T ) such that pi (CS S , x0 ) 
pi (CS T , y0 ) for all i  S.
868

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Proof. Given a pair of imputations x  I(CS S ), y  I(CS T ), we color the agents in T as follows:
an agent i is green if i  S and pi (CS T , y) > pi (CS S , x) or if i  T \ S and pi (CS T , y) > 0; red
if i  S and pi (CS T , y) < pi (CS S , x); and white otherwise.
We define a mapping D : I(CS S )  I(CS T )  R by setting
X
D(x, y) =
max{0, pi (CS S , x)  pi (CS T , y)}.
iS

The function D is continuous over a compact set, and thus attains its minimum value at some point
(x, y)  I(CS S )  I(CS T ). Among all such points, pick one with the smallest number of white
agents and denote it by (x0 , y0 ). Let q i = pi (CS T , y0 ) for i  T , pi = pi (CS S , x0 ) for i  S. We
will now show that q i  pi for all i  S.
Assume for the sake of contradiction that this is not the case. Then D(x0 , y0 ) > 0 and there is
at least one red agent in T . Since v  (T )  v  (S), there is also a green agent in T .
Now, consider a green agent g and a non-green agent i. Suppose that
(a) there exists a coalition c  CS T such that g, i  supp(c) and y0 (c)g > 0, or
(b) there exists a coalition c  CS S such that g, i  supp(c) and x0 (c)i > 0.
In case (a) we can modify y0 by making g transfer a small amount of payoff to i, and in case (b)
we can modify x0 by making i transfer a small amount of payoff to g; we choose the transfer to
be small enough that g remains green. If i was white, it now becomes green, and if i was red, this
lowers its contribution to D. In both cases, we get a contradiction with our choice of (x0 , y0 ). Thus,
no such coalition c exists.
Let Sg denote the set of all green agents in S, and let Sng = S \ Sg . Under x0 , every i  Sng
gets no payoff from coalitions in CS S whose support contains members of Sg . Similarly, under y0
green agents in T get no payoff from coalitions in CS T that contain members of Sng .
Now, suppose that we modify CS T by having the agents in Sng abandon their existing coalitions
and form the coalitions they form among themselves in CS S instead. Denote the resulting coalition
structure by CS 0 . We define an imputation z0 for CS 0 as follows. The imputation z0 coincides with
y0 for each c with supp(c)  Sng = , and it coincides with x0 for each coalition c0 that is formed
by the members of Sng . The remaining coalitions in CS 0 involve both agents in Sng and agents
in T \ Sng , and may have suffered a reduction in their value relative to CS T after Sng withdrew
resources from them; for such coalitions z0 distributes their value arbitrarily among the members of
Sng .
As argued above, in (CS S , x0 ) the members of Sng only receive payoffs from coalitions that
they fully control, so pi (CS 0 , z0 )  pi (CS S , x0 ) for all i  Sng . Further, since at least one
agent in S is red, we have pSng (CS S , x0 ) > pSng (CS T , y0 ). On the other hand, in (CS T , y0 )
the members of T \ Sng receive no payoff from coalitions with members of Sng , so letting Sng
receive all of the payoffs from those coalitions does not affect the payoff that the members of
T \ Sng receive, relative to what they received in (CS T , y0 ). Thus, for all i in T \ Sng it holds that
pi (CS 0 , z0 ) = pi (CS T , y0 ). Hence, we obtain
v(CS 0 ) = pT (CS 0 , z0 )
T \Sng

= p
> p

T \Sng

(8)

0

(CS , z0 ) + p

Sng

(CS T , y0 ) + p


T

= v(CS T ) = v (e ).
869

0

(CS , z0 )

Sng

(CS T , y0 )

fiZ ICK , M ARKAKIS , & E LKIND

This contradiction shows that the set of red agents is empty and hence pi (CS T , y0 )  pi (CS S , x0 )
for all i  S.
We will now use Lemma 5.11 to complete the proof of Proposition 5.10. Define
P (CS S , CS T ) = {(x, y)  I(CS S )  I(CS T ) | pi (CS T , y)  pi (CS S , x) for all i  S}.
Lemma 5.11 implies that P (CS S , CS T ) is not empty. Moreover, P (CS S , CS T ) is compact; thus,
it contains a point (x, y) that minimizes the value pS (CS T , y) over P (CS S , CS T ).
Let M (CS S , CS T ) be the set of points (x, y) in P (CS S , CS T ) that minimize pS (CS T , y); to
complete the proof, we will identify a point (x1 , y1 ) in M (CS S , CS T ) such that pS (CS T , y1 ) =
pS (CS S , x1 ) = v  (eS ).
Given a point (x, y)  M (CS S , CS T ), we color the agents in T as follows: i is green if i  S
and pi (CS T , y) > pi (CS S , x), and white otherwise. Note that, since (x, y)  P (CS S , CS T ),
if S contains a white agent i, we have pi (CS T , y) = pi (CS S , x). Let (x1 , y1 ) be a point in
M (CS S , CS T ) that maximizes the number of green agents in T .
Now, consider a green agent g and a white agent i. Suppose that
(a) g  S, i  T , and there is a coalition c  CS T such that g, i  supp(c) and y(c)g > 0, or
(b) g, i  S, and there is a coalition c  CS S such that g, i  supp(c) and x(c)i > 0.
In case (a) we could transfer a small amount of payoff from g to i in CS T so as to either make i
green and thereby increase the number of green vertices (if i  S) or lower the total payoff of S in
CS T (if i  T \ S), and in case (b) we could transfer a small amount of payoff from i to g in CS S
so as to make i green (again, we choose the transfer to be small enough that g remains green). In
both cases, we get a contradiction with our choice of (x1 , y1 ).
Let Sg be the set of all green agents in S, and let Sw = S \ Sg be the set of all white agents
in S. We have shown that in (CS S , x1 ) the members of Sw do not receive payoffs from their
joint coalitions with members of Sg , and in (CS T , y1 ) the members of Sg do not receive payoffs from their joint coalitions with members of Sw , so we have pSw (CS S , x1 )  v  (eSw ) and
pSg (CS T , y1 )  v  (eSg ). Thus, if Sg 6= , we have
v  (eS ) = pS (CS S , x1 )

(9)

Sg

Sw

Sg



= p (CS S , x1 ) + p

(CS S , x1 )

< p (CS T , y1 ) + v (eSw )
 v  (eSg ) + v  (eSw ).
Thus, if Sg 6= , then v  (eS ) < v  (eSg ) + v  (eSw ), a contradiction with the superadditivity of v  .
We conclude that Sg = , which implies that pi (CS S , x1 ) = pi (CS T , y1 ) for all i  S.
Armed with Proposition 5.10, we are now ready to prove the following theorem.
Theorem 5.12. If G = hN, vi is OCF-convex then G = hN, Uv i is supermodular.
Proof. Consider sets S, T, R  N such that S  T and R  N \ T ; we will demonstrate that
Uv (S  R)  Uv (S)  Uv (T  R)  Uv (T ). Set S 0 = S  R, T 0 = T  R, and consider coalition
0
structures CS S  CS(S), CS S 0  CS(S 0 ) such that v(CS S ) = v  (eS ), v(CS S 0 ) = v  (eS ). By
870

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Proposition 5.10, there exist imputations xS and xS 0 such that pi (CS S , xS ) = pi (CS S 0 , xS 0 ) for all
i  S; thus, the total payoff to R from (CS S 0 , xS 0 ) is v  (eSR )  v  (eS ).
Consider an outcome (CS T , xT )  FT such that v(CS T ) = v  (eT ); since G is OCF-convex,
there is an outcome (CS T 0 , xT 0 ) that is better for all members of S 0 than (CS S 0 , xS 0 ), and also pays
T a total of at least v  (eT ). The payoff to R under (CS T 0 , xT 0 ) is v(CS T 0 )  pT (CS T 0 , xT 0 ). We
have v(CS T 0 )  v  (eT R ) and pT (CS T 0 , xT 0 )  v  (eT ), so the payoff to R in (CS T 0 , xT 0 ) is at
most v  (eT R )  v  (eT ). Finally, OCF convexity of G implies that the payoff to R in (CS T 0 , xT 0 )
is at least as large as its payoff in (CS S 0 , xS 0 ). Combining these inequalities, we get
Uv (S  R)  Uv (S) = v  (eSR )  v  (eS )
= pR (CS S 0 , xS 0 )
 pR (CS T 0 , xT 0 )
 v  (eT R )  v  (eT )
= Uv (T  R)  Uv (T ),
which concludes the proof.
Theorem 5.12 shows that if an OCF game is OCF-convex, then its discrete superadditive cover
is supermodular. We will now show that the converse does not always hold, i.e., supermodularity is
a strictly weaker property than OCF convexity.
Example 5.13. Consider a game G = hN, vi where N = {1, 2, 3} and v is defined as follows:
1
0
 v 0 = 1, v 1 = 1;
0

0

 v

1

= 6, v

 v

1

= 4, v

.5
0
0
1

0
.5
1

0
1
1

= 3;

= 4;

 v(c) = 0 for any other partial coalition c  [0, 1]3 .
We have
 Uv ({1}) = Uv ({2}) = 1, Uv ({3}) = 0;
 Uv ({1, 2}) = 6, Uv ({1, 3}) = Uv ({2, 3}) = 4;
 Uv (N ) = 9.
One can check that Uv is indeed supermodular.
However, G
is not
Indeed,
OCF-convex.
 set S =
 0 
0
1
0
0
1
{3}, T = {1, 3}, R = {2}, with CS S =
, CS T =
, CS SR =
. Assume
1
1
1
that the players in T and S  R share the payoffs according to xT = ((0, 0, 4)) and xSR =
((0, 4, 0)), respectively. For G to be OCF-convex, there has to exist a coalition structure CS where
player 3 earns at least 4 and player 2 earns
at least 4. However,
this is impossible: if player 3 earns
1
0
0
at least 4, then CS contains either c = 0 or c = 1 . If c is formed, then player 2 can get at
1

1

most 1; if c0 is formed, then players 2 and 3 together can get at most 4. Thus, there is no way to
satisfy all of the agents demands.
871

fiZ ICK , M ARKAKIS , & E LKIND

5.2 The Sensitive Core
Recall that under the sensitive arbitration function, an agent will withhold payments from the deviators if any of the coalitions it participates in are hurt by the deviation. In Section 4.1, we obtained
the following characterization of the sensitive core: an outcome (CS , x) is in the sensitive core if
and only if CS is an optimal coalition structure, and for each set S  N it holds that for each
T  N \ S the total payoff that S receives from investing its resources in coalitions involving T is
at least as large as the marginal returns of investing the same resources in working on its own (see
formula (4)).
Let CS = (c1 , . . . , cm ) be an optimal coalition structure. Using the characterization above,
we conclude that deciding whether the sensitive core contains an outcome of the form (CS , x) is
equivalent to determining whether the value of the following linear program equals v  (eN ).
m
P

min

P

j=1 isupp(cj )

(10)

xij  v(cj )

P

s.t.

xij
cj  CS

isupp(cj )

pS (CS T , x)  v  (wS (CS |S ) + wS (CS T ))  v  (wS (CS |S )) S  N, T  N \ S
We note that in the linear program (10) we do not require that xij  0 for each cj  CS and each
i  supp(cj ). This is because the argument presented in Appendix A shows that these constraints
can be safely omitted. We will revisit this point in the proof of Theorem 5.15 in the context of the
refined core.
Consider the dual of this linear program.
max

m
P

rj v(cj ) +

j=1

s.t.

P

P

S,T (v  (wS (CS |S ) +wS (CS T ))  v  (wS (CS |S ))) (11)

SN T N \S

rj +

P

P

S:iS

T N \S:
supp(cj )T 6=

S,T = 1
rj  0
S,T  0

cj  CS , i  supp(cj )
cj  CS
S  N, T  N \ S

m
We say that a collection of non-negative
P weights
P {(rj )j=1 ; (S,T )SN ;T N \S } is s-balanced with
respect to CS = (c1 , . . . , cm ) if rj + S:iS T N \S:supp(cj )T 6= S,T = 1 for all cj  CS and
all i  supp(cj ). Applying linear programming duality to linear programs (10) and (11), we obtain
the following theorem.

Theorem 5.14. The sensitive core of a game G = hN, vi is not empty if and only if there exists
a coalition structure CS = (c1 , . . . , cm ) with v(CS ) = v  (eN ) such that for every collection of
non-negative weights {(rj )m
j=1 ; (S,T )SN ;T N \S } that is s-balanced with respect to CS it holds
that
m
X
j=1

rj v(cj ) +

X

X

S,T (v  (wS (CS |S ) + wS (CS T ))  v  (wS (CS |S )))  v  (eN ).

SN T N \S

872

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Theorem 5.14 is similar to Theorem 5.2, which presents a criterion for non-emptiness of the
conservative core. The main difference is that Theorem 5.2 considers collections of weights that
contain a single weight S for each set S  N , whereas for the sensitive core we consider collections
of weights that contain a weight S,T for each pair of disjoint sets S, T  N .
5.3 The Refined Core
Under the sensitive arbitration function, the deviating players only need to decide which of the nondeviating players they do not wish to work with any more. In contrast, under the refined arbitration
function, the deviators have to look at the coalitions with the non-deviators one by one, and decide
which of them are worth keeping. We will now provide a characterization of games with nonempty refined core that is similar to the characterizations of games with non-empty conservative and
sensitive cores (Theorem 5.2 and Theorem 5.14, respectively). Employing this characterization, we
then describe a class of OCF games with a non-empty refined core.
In Section 4.1, we obtained the following characterization of outcomes in the refined core: given
a coalition structure CS , there is an imputation x  I(CS ) such that (CS , x) is in the refined core if
and only if pS (CS 0 , x)  v  (wS (CS 0 )) for every S  N and every coalition structure CS 0  CS
containing CS |S .
In what follows, given a subset of agents S  N and a coalition structure CS , we write [CS ]S =
{CS 0  CS | CS |S  CS 0 }. Also, given a coalition structure CS = (c1 , . . . , cm ), we say that a
collection of non-negative weights {(rj )m
j=1 ; (S,CS 0 )SN ;CS 0 [CS ]S } is r-balanced with respect to
CS if for each cj  CS and each i  supp(cj ) it holds that
X
X
rj +
S,CS 0 = 1.
S:iS CS 0 [CS ]S :
cj CS 0

We are now ready to present our characterization.
Theorem 5.15. The refined core of an OCF game G = hN, vi is non-empty if and only if there
exists a coalition structure CS = (c1 , . . . , cm ) with v(CS ) = v  (eN ) such that for every collection
of weights {(rj )m
j=1 ; (S,CS 0 )SN ;CS 0 [CS ]S } that is r-balanced with respect to CS it holds that
m
X

rj v(cj ) +

X

X

S,CS 0 v  (wS (CS 0 ))  v  (eN ).

SN CS 0 [CS ]S

j=1

Proof. Fix a coalition structure CS = (c1 , . . . , cm ) with v(CS ) = v  (eN ), and consider the following linear program.
m
P

min

P

j=1 isupp(cj )

s.t.

P

xij

(12)

xij  v(cj )

cj  CS

isupp(cj )

pS (CS 0 , x)  v  (wS (CS 0 )) S  N, CS 0  [CS ]S
We claim that the refined core of G contains an outcome of the form (CS , x) if and only if the value
of this linear program is v  (eN ).
873

fiZ ICK , M ARKAKIS , & E LKIND

To see why this is the case, it suffices to observe that, by the argument in Appendix A, we can
omit constraints of the form xij  0 for each cj  CS and each i  supp(cj ). In fact, it turns out
that for the refined core, a much simpler explanation holds: Non-negativity of xij is implied by the
stability constraints. To see this, consider a coalition cj such that i  supp(cj ). If supp(cj ) = {i},
xij = v(cj )  0 and we are done. Otherwise, consider the coalition structure CS 0 = (CS |{i} , cj ).
The total payoff to i from CS 0 is pi (CS 0 , x) = pi (CS |{i} , x)+xij = v(CS |{i} )+xij . The constraint
that corresponds to S = {i} and CS 0 states that this payoff must be at least v  (w{i} (CS 0 )), which
is at least v(CS |{i} ), and hence xij  0.
Consider now the dual of linear program (12).
max

m
P

P

rj v(cj ) +

j=1

P

rj +

s.t.

S,CS 0 v  (wS (CS 0 ))

(13)

SN
CS 0 [CS ]S

S,CS 0 = 1

cj  CS , i  supp(cj )

S,CS 0  0

S  N, CS 0  [CS ]S

S:iS
CS 0 [CS ]S :cj CS 0

rj  0

cj  CS

Observe that the dual constraints in (13) are equalities since xij are unconstrained in (12). Note
also that every feasible solution of (13) corresponds to a collection of non-negative weights that is
r-balanced with respect to CS . Our claim now follows by the standard linear programming duality
argument.
Theorem 5.15 enables us to identify an interesting class of OCF games with non-empty refined
core. Recall that a function f : Rn  R is homogeneous of degree k, or k-homogeneous, if
f (x) = k f (x) for all   R+ . Intuitively, this means that f scales in a consistent manner: if all
players invest twice as much resources in a coalition, they will get 2k times the profit. The returns
to scale are positive if k  1 and negative if k < 1.
Corollary 5.16. Consider an OCF game G = hN, vi. If v  is homogeneous of degree k  1, then
the refined core of G is non-empty.
Proof. Consider a coalition structure CS = (c1 , . . . , cm ) with v(CS ) = v  (eN ) and a collection
of non-negative weights {(rj )m
j=1 ; (S,CS 0 )SN ;CS 0 [CS ]S } that is r-balanced with respect to CS .
According to Theorem 5.15, it suffices to show that
m
X
j=1

rj v(cj ) +

X

S,CS 0 v  (wS (CS 0 ))  v  (eN ).

(14)

SN
CS 0 [CS ]S

First, since v  is k-homogeneous, we have

X
SN
CS 0 [CS ]S


S,CS 0 v  (wS (CS 0 ))  v  


874


X
SN
CS 0 [CS ]S


S,CS 0 wS (CS 0 )
.

(15)

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

S,CS 0 wS (CS 0 ) by  i . We have

P

Denote the i-th coordinate of

SN
CS 0 [CS ]S

i =

X

X

S:iS CS 0 [CS ]S

=

m
X

cij

j=1

X
S:iS

=

cij

cj CS 0

X

S,CS 0

CS 0 [CS ]
cj CS 0

m
X

X

S,CS 0

S:

cij (1  rj ),

j=1

P
where the first transition is derived from the fact that wi (CS 0 ) = cj CS 0 cij if i  S, and the
second transition uses the fact that {(rj )m
j=1 ; (S,CS 0 )SN ;CS 0 [CS ]S } is r-balanced with respect to
CS . We conclude that the right-hand side of equation (15) is upper-bounded by
v(

m
X
(1  rj )cj ).
j=1

We now consider the first summand in the left-hand side of (14). By k-homogeneity of v  , we have
m
X

rj v(cj ) 

j=1

m
X

m
X
rj v  (cj )  v  (
rj cj ).

j=1

j=1

Using the superadditivity of v  , we obtain that the left-hand side of (14) can be upper-bounded by
m
m
m
X
X
X
v(
rj cj +
(1  rj )cj ) = v  (
cj ) = v  (eN ).
j=1

j=1

j=1

This concludes the proof.
In fact, the proof of Corollary 5.16 shows a stronger claim: if v  is k-homogeneous, then every
optimal coalition structure CS admits an imputation x  I(CS ) such that (CS , x) is in the refined
core. This is not the case in general; there exist games where some optimal coalition structures
cannot be stabilized under the refined arbitration function, while others can. This is illustrated by
the following example.
Example 5.17. Consider the following three-player game. There are four types of tasks: A task of
type t1 can be completed by player 1 alone, requires all of its resources, and is worth 5. A task of
type t12 requires 50% of both player 2 and player 1s resources and is worth 10. A task of type T12
requires all of the resources of players 1 and 2 and is worth 20. Finally, a task of type t23 requires
all of player 3s resources and 50% of player 2s resources, and is worth 9. Consider coalition
structures CS = (c1 , c2 ) and CS 0 = (c01 ), where
 .5 
 .5 
1
c1 = .5 , c2 = .5 , c01 = 1 .
0

0

875

0

fiZ ICK , M ARKAKIS , & E LKIND

It is easy to see that both CS and CS 0 are optimal for this game. Simply put, it is best for players 1
and 2 to work together and earn a total of 20 (by completing t12 twice or T12 once), while player 3
makes no profit.
First, we claim that CS cannot be stabilized with respect to the refined arbitration function. The
reason is that for an outcome (CS , x) to be in the refined core, it must be the case that player 2 gets
at least 9 from each of the coalitions c1 and c2 . However, this means that player 1 gets at most 2
from working with player 2, while he can get 5 by working alone. On the other hand, let y be the
imputation for CS 0 that splits the payoff from T12 evenly between players 1 and 2. Then (CS 0 , y)
is in the refined core.
Finally, we note that one cannot immediately employ an LP duality argument for characterizing
OCF games with non-empty optimistic core. The difficulty is that under the optimistic arbitration
function the number of possible deviations is infinite: a deviating set needs to specify the amount of
resources it withdraws from each coalition with non-deviators. In other words, the linear program
that describes the optimistic core has infinitely many constraints, so its dual has infinitely many
variables.

6. Conclusions and Future Work
The main contribution of our work is the concept of arbitration functions, and the analysis of the
impact of arbitration on the stability of cooperative games with overlapping coalitions. This concept
allows us to put the three notions of stability proposed by Chalkiadakis et al. (2010) in a broader
context and to derive new notions of stability such as the sensitive core.
Perhaps the most interesting of our results is the connection between OCF games with the conservative arbitration function and their discrete superadditive covers. In a sense, it shows that if
the players are so pessimistic as to assume that their post-deviation payoffs will be given by the
conservative arbitration function, there is little benefit to employing the OCF framework. In other
words, the main value of the OCF approach lies in its ability to model non-trivial post-deviation
interactions. Thus, the OCF framework is eminently suitable for settings where agents have a reason to expect that deviating from some of their collaborative projects will not cause other agents to
boycott them. We believe that such settings are becoming increasingly more common in the modern
interconnected society, where multitasking is the norm and collaboration is necessary to succeed.
The reaction of non-deviators to deviation plays a decisive role in the analysis of many strategic
interactions. For example, Ackerman and Branzei (2014) use the concept of arbitration function
in the analysis of Nash equilibrium and pairwise equilibrium. Also, Branzei, Michalak, Rahwan,
Larson, and Jennings (2013) study matchings with externalities; their model can be viewed as an
application of the idea of arbitration functions to matching problems. In fact, in any strategic setting
where a deviating set must still interact with the non-deviators one has to reason about the nondeviators behavior after a deviation occurs. While it is common to make the worst-case assumption,
this is not always appropriate or realistic. Thus, a notion similar to arbitration functions may be
useful beyond the setting of cooperative games. For instance, when players interact in a market,
their behavior is governed by contracts (think of a wireless service provider and a consumer buying
a new data plan), which clearly specify penalties for failing to fulfill ones obligations (in the case
of wireless service providers, it is typically the case that a withdrawal from an agreed-upon contract
entails a monetary penalty, i.e. a worse-than-conservative reaction to deviation). Such contracts
876

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

can be viewed as arbitration functions. Thus, the concept of arbitration function may be useful for
modeling market interactions under contracts.
6.1 Future Work
In Section 5, we have provided criteria for core non-emptiness under several arbitration functions.
For the conservative and refined core, we have also identified simple sufficient conditions for the
core to be non-empty (supermodularity of the discrete superadditive cover and first-order homogeneity of v  , respectively). It would be useful to obtain similar results for other natural notions of
arbitration, such as the sensitive arbitration function and the optimistic arbitration function.
Another interesting research direction is to consider the inverse of this problem: given an
OCF game, what is the most generous arbitration function for which the arbitrated core of that
game is not empty? More formally, given a game G, let us call an arbitration function A maxstable for G if Core(G, A) 6= , and for all A0 such that A0  A we have Core(G, A0 ) = .
Simply put, the arbitration function A is max-stable if it provides the largest possible payoffs to the
deviators without destabilizing G. It would be interesting to prove that some arbitration function is
max-stable for a class of OCF games, or, more ambitiously, to completely characterize the set of
max-stable arbitration functions. This line of enquiry is similar in spirit to the concepts of the least
core (Maschler et al., 1979), the cost of stability (Bejan & Gomez, 2009; Bachrach, Elkind, Meir,
Pasechnik, Zuckerman, Rothe, & Rosenschein, 2009), and taxation (Zick, Polukarov, & Jennings,
2013).
We note that in our model deviators are allowed to withdraw resources from coalitions with nondeviators, but not to add resources to such coalitions. The implications of a more general definition,
where deviators can both add and withdraw resources to their coalitions with non-deviators, are
worth exploring as well.
Our analysis focuses on the core of cooperative games with overlapping coalitions. This is, to
an extent, a reflection of the state of the art in the study of classic cooperative games: the core and
related notions (i.e., the -core and the least core) have been studied in greater detail than other
cooperative solution concepts (barring, perhaps, the Shapley value). Further analysis of the various
solution concepts for OCF games, such as the nucleolus, the kernel, and the bargaining set, and
their relations to one another, would greatly improve our understanding of overlapping coalition
formation. While some of these solution concepts for OCF games have been briefly discussed
by Zick and Elkind (2011), their analysis is far from complete.
Finally, it would be interesting to study models of dynamic coalition formation in cooperative
games with overlapping coalitions. While such models have been explored for classic cooperative
games (see, e.g., Arnold & Schwalbe, 2002; Lehrer & Scarsini, 2013), or for games with overlapping coalitions and fully cooperative agents (Shehory & Kraus, 1996), it is not immediately clear
how to extend prior work to arbitrated OCF games. In such settings, arbitration functions can be
used as a method of controlling the coalition formation process: If an outcome at some stage of the
coalition formation process is stable with respect to the currently used arbitration function, more
permissive reaction to deviation is allowed in future rounds; if the outcome is unstable, players can
be assumed to be less tolerant towards deviators. Thus, by controlling the extent to which players
are allowed to deviate, one can trade off the speed of the coalition formation process and the quality
of the resulting outcome.
877

fiZ ICK , M ARKAKIS , & E LKIND

Acknowledgments
Part of this work was done while Y. Zick and E. Elkind were affiliated with Nanyang Technological University, Singapore. Y. Zick was supported by the Singapore International Graduate Award
(SINGA), provided by the Agency for Science, Technology and Research (A*STAR). E. Markakis
was supported by the European Union (European Social Fund--ESF) and Greek national funds
through the Operational Program Education and Lifelong Learning of the National Strategic Reference Framework (NSRF)Research Funding Program: THALES. Investing in knowledge society
through the European Social Fund. E. Elkind was supported by the National Research Foundation
(Singapore) under grant NRF RF2009-08. The authors would like to thank the anonymous JAIR
reviewers as well as the anonymous reviewers of earlier versions of this work (Zick & Elkind, 2011;
Zick, Markakis, & Elkind, 2012). Y. Zick would also like to thank the anonymous reviewers of his
Ph.D thesis (Zick, 2014), parts of which are incorporated into this paper.

Appendix A. A Note on the Characterization of the Conservative Core
The purpose of this appendix is to fill a gap in the proof of the characterization of the conservative core that was given by Chalkiadakis et al. (2010) (see Theorem 5.2). Our argument also has
implications for the characterization of the sensitive core (Section 5.2).
The proof of Theorem 5.2 proceeds as follows. Given an optimal coalition structure CS =
(c1 , . . . , cm ), consider the following linear program:
Pm P

min

i
isupp(cj ) xj
i
isupp(cj ) xj  v(cj )
j=1

P

s.t.

P

iS

Pm

j  {1, . . . , m}

(16)

 v  (eS ) S  N

i
j=1 xj

The dual of LP (16) is
max
s.t.

Pm

j=1 rj v(cj )

rj +

+
P

P

SN

S:iS S

S v  (eS )

=1

j, i  supp(cj )

rj  0

j  {1, . . . , m}

S  0

S  N

(17)

Chalkiadakis et al. (2010)
P argue that LP (16) describes the constraints of the conservative core,
with the exception that isupp(cj ) xij is required to be at least v(cj ) rather than equal to v(cj ).
Thus,
solution to LP (16), then (CS , x) is in the conservative core if and only
P if x
Pis an optimal
i = v(CS ) = v  (eN ). LP duality then implies that the value of LP (17)
if m
x
j=1
isupp(cj ) j
is at most v  (eN ) if and only if CS can be stabilized with respect to the conservative arbitration
function.
i
The problem with this argument is that
PLP (16) does not require the variables
P xj to be nonnegative, which allows us to have rj + S:iS S = 1 rather than rj + S:iS S  1 in
the dual LP. However, if we allow the variables xij to take negative values, we are effectively
breaking the payoff distribution requirement for imputations. We mention that, in contrast, in the
BondarevaShapley theorem it is acceptable not to impose non-negativity constraints explicitly,
since stability constraints imply pi  u({i})  0. However, for the conservative core we only get
878

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

 v  (e{i} ) for all i  N , which does not imply that individual xij are non-negative.
In other words, Chalkiadakis et al. (2010) show that every c-balanced OCF game and every optimal
coalition structure CS = (c1 , . . . , cm ) for this game admit a pre-imputation x such that (CS , x) is
in the conservative core (recall that x = (x1 , . . . , xm ) is a pre-imputation
CS = (c1 , . . . , cm )
Pfor
n
n
if for each j = 1, . . . , m it holds that xj is a vector in R such that i=1 xij = v(cj ) and if
cij = 0 then xij = 0, i.e., a preimputation satisfies the same conditions as an imputation, except for
non-negativity). We will now argue that one can transform this pre-imputation into an imputation.
i
j:isupp(cj ) xj

P

Theorem A.1. If there exists a pre-imputation x that satisfies the constraints of LP (16), then there
exists an imputation x  I(CS ) such that pi (CS , x) = pi (CS , x) for all i  N .
Proof. Let Ipre (CS ) denote the set of all pre-imputations for a coalition structure CS . Given a
point x  Ipre (CS ), we define a graph (x) = hV, Ei as follows: the vertices of (x) are
V = {(i, j) | cj  CS , i  supp(cj )} ,
and the set of edges is E = E1  E2 , where


	
E1 = (i, j), (i, j 0 ) | i  supp(cj )  supp(cj 0 ), j 6= j 0 ,
and
E2 =




	
(i, j), (i0 , j) | i, i0  supp(cj ), xij > 0 .

That is, E1 consists of all pairs (i, j), (i, j 0 ) such that i is in both supp(cj ) and supp(cj 0 ), and E2
consists of all pairs (i, j), (i0 , j) such that i, i0  supp(cj ) for some cj  CS and xij > 0. An edge
in E2 means that i can transfer a small amount of payoff to i0 in the coalition cj , and still maintain
a positive payoff from cj .
Further, given a point x  Ipre (CS ), set
TN (x) =

m X
n
X

min{0, xij }.

j=1 i=1

Fix an arbitrary point x  Ipre (CS ), and consider the set
Ipre (CS , x) = {y  Ipre (CS ) | pi (CS , y) = pi (CS , x) for all i  N and TN (y)  TN (x)}.
The set Ipre (CS , x) is compact and TN is continuous, so there exists a point x  Ipre (CS , x) that
maximizes TN over Ipre (CS , x).

Suppose that there exists a coalition cj  and an agent i  supp(cj  ) such that xij  < 0. Note
that in (x) there must be an edge in E1 that leaves (i , j  ): otherwise, i is only being paid by cj  ,
in which case we would have
p (CS , x) =
i

m
X







xij = xij  < 0  v  (e{i } ),

j=1

violating the respective constraint of LP (16).
Let C be a minimum-length directed cycle in (x) that contains an edge ((i , j  ), (i , j0 )) for
some cj0  CS as well as some edges from E2 . Note that C contains no path of the form
(i, j1 )  (i, j2 )  (i, j3 );
879

fiZ ICK , M ARKAKIS , & E LKIND

this is because the edge ((i, j1 ), (i, j3 )) is in E1 , a contradiction to C being of minimum length.
Moreover, C cannot contain a path of the form
(i1 , j)  (i2 , j)  (i3 , j);
indeed, if ((i1 , j), (i2 , j)) is in E2 , then agent i1 receives a positive payoff from cj , and hence the
edge ((i1 , j), (i3 , j)) is also in E2 , a contradiction with our choice of C.
We conclude that a minimum-length cycle that passes through (i , j  ) and intersects E2 must
be of the form
(i , j  )  (i , j0 )  (i1 , j0 )  (i1 , j1 )  (i2 , j1 )      (it , j  )  (i , j  ).
This means that i receives a positive payoff from cj0 , i1 receives a positive payoff from cj1 , i2
receives a positive payoff from cj2 and, in general, i` receives a positive payoff from the coalition
cj` for all 1  `  t  1. Finally, the player it receives a positive payoff from cj  .
Now, pick an  satsifying


i

0 <  < min{xij0 , xij11 , . . . , xjt1
, xijt },
t1
and let y = (y1 , . . . , ym ) be the pre-imputation obtained from x by transferring a payoff of  from


i to i1 in cj0 , from i` to i`+1 in cj` for ` =
Pfrom it to ii in cj . We have
P1, . . . , t  i1, and
x
for
all
cj  CS .
pi (CS , y) = pi (CS , x) for all i  N , and isupp(cj ) yj =
isupp(cj ) j
Therefore, y is an optimal solution to LP (16). However, TN (y) > TN (x), a contradiction to our
choice of x. We conclude that (x) contains no cycles that pass through (i , j  ) and intersect E2 .
Let us define
Np = {i  N | there is a path from (i , j  ) to (i, j) for some cj }.
Then for every i  Np and every i0  N \Np , our graph contains no edge of the form ((i, j), (i0 , j)).
Hence, if i, i0  supp(cj ) then xij  0. That is, in every coalition that involves both agents in Np
and agents in N \ Np the payoffs are split so that the share of each agent in Np is non-positive.
Hence, pNp (CS , x)  pNp (CS |Np , x).
Moreover, we will now argue that for cj  the total share of the agents in Np is, in fact, negative,

and therefore pNp (CS , x) < pNp (CS |Np , x). Indeed, recall that xij  < 0. Suppose that xij  > 0 for
some i  Np ; note that this implies that the edge e = ((i, j  ), (i , j  )) is in E2 . Consider a coalition
cj such that the graph contains a path P from (i , j  ) to (i, j). If j = j  , by adding e to P we obtain
a cycle that passes through (i , j  ) and intersects E2 . Otherwise, the edge e0 = ((i, j), (i, j  )) is
in E1 , and adding e0 and e to P results in a cycle that passes through (i , j  ) and intersects E2 . In
both cases, we get a contradiction, as we have argued that no such cycle exists. Thus, all players
in Np get a non-positive payoff from cj  , and i gets a strictly negative payoff from this coalition.
Consequently, we have pNp (CS , x) < pNp (CS |Np , x). On the other hand, we have
pNp (CS |Np , x) = v(CS |Np )  v  (eNp );
combining the inequalities we obtain that pNp (CS , x) < v  (eNp ), which is a contradiction to
(CS , x) satisfying the inequalities in LP (16).
We conclude that x is an imputation of CS such that pi (CS , x) = pi (CS , x) for all i  N and
xij  0 for all cj  CS and all i  supp(cj ), which concludes the proof.
880

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

1,1

1,2

1,3

2,1

2,2

2,3

3,1

3,3

Figure 1: The graph (x) formed for the coalition structure and the imputation described in Example A.2.

Example A.2. Suppose the players form the coalition structure CS = (c1 , c2 , c3 ) given by
 0.6 
 0.3 
 0.1 
c1 = 0.3 ,
c2 = 0.6 ,
c3 = 0.1 ,
0.3

0

and use the imputation x = (x1 , x2 , x3 ), where
0
2
x1 = 2 ,
x2 = 2 ,
3

0

0.7

x3 =



3
0
1



.

The resulting graph is shown in Figure 1, and the shortest cycle starting from (3, 3) including edges
in E2 is
(3, 3)  (3, 1)  (1, 1)  (1, 3)  (3, 3).
That is, the negative payoff to player 3 from c3 can be reduced if player 3 transfers a small amount
of payoff to player 1 under c1 , and player 1 transfers the same amount of payoff to player 3 under
c3 ; for instance, moving 1 unit of payoff in this manner would result in a valid imputation.
Theorem A.1 implies that even if we assume that xij are unconstrained, if there is a solution to
LP (16) with a value of v  (eN ), then there is a solution with no negative payoffs from coalitions
and the same value, i.e., an optimal solution that is in I(CS ). In particular, this means that there is
no loss of generality in assuming that the variables xij in LP (16) are unconstrained, and therefore
Theorem 5.2 holds.

References
Ackerman, M., & Branzei, S. (2014). Research quality, fairness, and authorship order. In Proceedings of the 13th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS-14), pp. 14871488.
881

fiZ ICK , M ARKAKIS , & E LKIND

Aharoni, R., & Fleiner, T. (2003). On a lemma of Scarf. Journal of Combinatorial Theory, Series
B, 87, 7280.
Airiau, S., & Sen, S. (2009). A fair payoff distribution for myopic rational agents. In Proceedings
of the 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS09), pp. 13051306.
Anshelevich, E., & Hoefer, M. (2010). Contribution games in social networks. In Proceedings of
the 18th European Symposium on Algorithms (ESA-10), pp. 158169.
Arnold, T., & Schwalbe, U. (2002). Dynamic coalition formation and the core. Journal of Economic
Behavior & Organization, 49(3), 363380.
Aubin, J. (1981). Cooperative fuzzy games. Mathematics of Operations Research, 6(1), 113.
Aumann, R., & Dreze, J. (1974). Cooperative games with coalition structures. International Journal
of Game Theory, 3, 217237.
Bachrach, Y., Elkind, E., Meir, R., Pasechnik, D., Zuckerman, M., Rothe, J., & Rosenschein, J.
(2009). The cost of stability in coalitional games. In Proceedings of the 2nd International
Symposium on Algorithmic Game Theory (SAGT-09), pp. 122134.
Bejan, C., & Gomez, J. C. (2009). Core extensions for non-balanced TU-games. International
Journal of Game Theory, 38(1), 316.
Bondareva, O. (1963). Some applications of linear programming methods to the theory of cooperative games. Problemy kibernetiki, 10, 119139.
Branzei, S., Michalak, T., Rahwan, T., Larson, K., & Jennings, N. R. (2013). Matchings with externalities and attitudes. In Proceedings of the 12th International Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS-13), pp. 295302.
Chalkiadakis, G., Elkind, E., Markakis, E., Polukarov, M., & Jennings, N. (2010). Cooperative
games with overlapping coalitions. Journal of Artificial Intelligence Research, 39, 179216.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational Aspects of Cooperative
Game Theory. Morgan and Claypool.
Dang, V. D., Dash, R. K., Rogers, A., & Jennings, N. R. (2006). Overlapping coalition formation for
efficient data fusion in multi-sensor networks. In Proceedings of the 21st AAAI Conference
on AI (AAAI-06), pp. 635640.
Deng, X., Ibaraki, T., & Nagamochi, H. (1999). Algorithmic aspects of the core of combinatorial
optimization games. Mathematics of Operations Research, 24(3), 751766.
Gillies, D. (1953). Some Theorems on n-Person Games. Ph.D. thesis, Princeton University.
Jackson, M. O. (2003). A survey of models of network formationstability and efficiency. In
Demange, G., & Wooders, M. (Eds.), Group Formation in Economics: Networks, Clubs and
Coalitions, chap. 1. Cambridge University Press.
Lehrer, E., & Scarsini, M. (2013). On the core of dynamic cooperative games. Dynamic Games and
Applications, 3(3), 359373.
Lin, C., & Hu, S. (2007). Multi-task overlapping coalition parallel formation algorithm. In Proceedings of the 6th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS-07), p. 211.
882

fiA RBITRATION AND S TABILITY IN C OOPERATIVE G AMES WITH OVERLAPPING C OALITIONS

Markakis, E., & Saberi, A. (2005). On the core of the multicommodity flow game. Decision Support
Systems, 39(1), 310.
Maschler, M., Peleg, B., & Shapley, L. S. (1979). Geometric properties of the kernel, nucleolus,
and related solution concepts. Mathematics of Operations Research, 4(4), 303338.
Peleg, B., & Sudholter, P. (2007). Introduction to the Theory of Cooperative Games (Second edition)., Vol. 34 of Theory and Decision Library. Series C: Game Theory, Mathematical Programming and Operations Research. Springer, Berlin.
Rahwan, T. (2007). Algorithms for Coalition Formation in Multi-Agent Systems. Ph.D. thesis,
University of Southampton.
Rahwan, T., & Jennings, N. (2008). An improved dynamic programming algorithm for coalition
structure generation. In Proceedings of the 7th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS-08), pp. 14171420.
Shapley, L. S. (1967). On balanced sets and cores. Naval Research Logistics Quarterly, 14(4),
453460.
Shapley, L. S. (1971). Cores of convex games. International Journal of Game Theory, 1, 1126.
Shehory, O., & Kraus, S. (1996). Formation of overlapping coalitions for precedence-ordered taskexecution among autonomous agents. In Proceedings of the 2nd International Conference on
Multi-Agent Systems (ICMAS-96), pp. 330337.
Sims, M., Corkill, D., & Lesser, V. (2008). Automated organization design for multi-agent systems.
Autonomous Agents and Multi-Agent Systems, 16, 151185.
Wang, T., Song, L., Han, Z., & Saad, W. (2013). Overlapping coalitional games for collaborative
sensing in cognitive radio networks. In Proceedings of 2013 Wireless Communications and
Networking Conference (WCNC-13), pp. 41184123.
Zhang, G., Jiang, J., Su, Z., Qi, M., & Fang, H. (2010). Searching for overlapping coalitions in
multiple virtual organizations. Information Sciences, 180, 31403156.
Zhang, Z., Song, L., Han, Z., Saad, W., & Lu, Z. (2013). Overlapping coalition formation games for
cooperative interference management in small cell networks. In Proceedings of 2013 Wireless
Communications and Networking Conference (WCNC-13), pp. 643648.
Zick, Y. (2014). Arbitration, Fairness and Stability: Revenue Division in Collaborative Settings.
Ph.D. thesis, Nanyang Technological University.
Zick, Y., Chalkiadakis, G., & Elkind, E. (2012). Overlapping coalition formation games: Charting
the tractability frontier. In Proceedings of the 11th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS-12), pp. 787794.
Zick, Y., Chalkiadakis, G., Elkind, E., & Markakis, E. (2014). Cooperative games with overlapping
coalitions: Charting the tractability frontier. arXiv 1407:0420.
Zick, Y., & Elkind, E. (2011). Arbitrators in overlapping coalition formation games. In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS-11), pp. 5562.
Zick, Y., Markakis, E., & Elkind, E. (2012). Stability via convexity and LP-duality in OCF games.
In Proceedings of the 26th AAAI Conference on AI (AAAI-12).
883

fiZ ICK , M ARKAKIS , & E LKIND

Zick, Y., Polukarov, M., & Jennings, N. R. (2013). Taxation and stability in cooperative games.
In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS-13), pp. 523530.

884

fiJournal of Artificial Intelligence Research 50 (2014) 805-845

Submitted 02/14; published 08/14

Speeding Up Iterative Ontology Alignment using
Block-Coordinate Descent
Uthayasanker Thayasivam
Prashant Doshi

UTHAYASA @ CS . UGA . EDU
PDOSHI @ CS . UGA . EDU

THINC Lab, Department of Computer Science,
University of Georgia, Athens, GA 30602, USA

Abstract
In domains such as biomedicine, ontologies are prominently utilized for annotating data. Consequently, aligning ontologies facilitates integrating data. Several algorithms exist for automatically aligning ontologies with diverse levels of performance. As alignment applications evolve and
exhibit online run time constraints, performing the alignment in a reasonable amount of time without compromising the quality of the alignment is a crucial challenge. A large class of alignment
algorithms is iterative and often consumes more time than others in delivering solutions of high
quality. We present a novel and general approach for speeding up the multivariable optimization
process utilized by these algorithms. Specifically, we use the technique of block-coordinate descent
(BCD), which exploits the subdimensions of the alignment problem identified using a partitioning
scheme. We integrate this approach into multiple well-known alignment algorithms and show that
the enhanced algorithms generate similar or improved alignments in significantly less time on a
comprehensive testbed of ontology pairs. Because BCD does not overly constrain how we partition
or order the parts, we vary the partitioning and ordering schemes in order to empirically determine
the best schemes for each of the selected algorithms. As biomedicine represents a key application
domain for ontologies, we introduce a comprehensive biomedical ontology testbed for the community in order to evaluate alignment algorithms. Because biomedical ontologies tend to be large,
default iterative techniques find it difficult to produce a good quality alignment within a reasonable
amount of time. We align a significant number of ontology pairs from this testbed using BCDenhanced algorithms. Our contributions represent an important step toward making a significant
class of alignment techniques computationally feasible.

1. Introduction
Recent advances in Web-based ontologies provide a needed alternative to conventional schemas
allowing descriptive annotations of data sets. As an example, the National Center for Biomedical
Ontology (NCBO) hosts more than 370 curated biomedical ontologies in its BioPortal including
those in high use such as SNOMED-CT, and whose concepts participate in more than 2 billion data
annotations (Musen et al., 2012). Therefore, the present day challenge toward data integration and
to manage the multitude of ontologies is to build bridges between ontologies that have overlapping
scope  a problem often referred to as that of ontology matching which produces an alignment (Euzenat & Shvaiko, 2007). We illustrate a partial alignment between biomedical ontologies in Fig. 1.
Consequently, several algorithms exist for automatically aligning ontologies using various techniques (Euzenat, Loup, Touzani, & Valtchev, 2004; Jian, Hu, Cheng, & Qu, 2005; Li, Li, & Tang,
2007; Jean-Mary, Shironoshita, & Kabuka, 2009; Doshi, Kolli, & Thomas, 2009; Wang & Xu,
2009; Hanif & Aono, 2009; Bock & Hettenhausen, 2010; Jimenez-Ruiz & Grau, 2011; Shvaiko &
c
2014
AI Access Foundation. All rights reserved.

fiT HAYASIVAM & D OSHI

data
starting material,
intermediate material, end
products of a scientific
experiment and parameters

process
the occurrent entities
that affect individuals

sample
specimen

agent

researcher

individual involved in the
experimental processes

role

Entity

continuant entities that
causally affect the
individuals of process

the role of a person,
chemical compund,etc...

worker_role
Processual_Entity Public
sector workers in
exists in time by occurring or
happening, has temporal parts
and always involves and depends
on some entity.

reagent_role

states

drug

Buffer, dye, a catalyst, a
solvating agent.

drug_role

Any chemical substance that,
when absorbed into a cell,
alters normal cell function

drug

region

region

Specimen

region

sample

region

(a)

(b)

Figure 1: Biomedicine is an important application domain for ontologies. Alignment (shown in
dashed red) between portions of, (a) the Parasite Experiment Ontology (PEO) and, (b) the
Ontology of Biomedical Investigations (OBI) as discovered by an automated algorithm
called AgreementMaker (Cruz et al., 2012). Both these ontologies are available at NCBO.
Each identified map in the alignment signifies an equivalence between the concepts.

Euzenat, 2013), with mixed levels of performance. Crucial challenges for these algorithms involve
scaling to large ontologies and performing the alignment in a reasonable amount of time without
compromising on the quality of the alignment. As a case in point, only 6 alignment algorithms (not
including their variants) out of the 21 that participated in the 2012 and 2013 instances of the annual
ontology alignment evaluation initiative (OAEI) competition (Shvaiko et al., 2012, 2013) generated
results in an acceptable amount of time for aligning large biomedical ontologies.
Although ontology alignment is traditionally perceived as an offline and one-time task, the second challenge is gaining importance. In particular, as Hughes and Ashpole (2004) note, continuously evolving ontologies and applications involving real-time ontology alignment such as semantic
search and Web service composition stress the importance of computational complexity considerations. Recently, established competitions such as OAEI (Shvaiko et al., 2011) began reporting the
execution times of the participating alignment algorithms as well. As ontologies become larger,
efficiency and scalability become key properties of alignment algorithms.
A large class of algorithms that performs automated alignment is iterative in nature (Melnik,
Garcia-molina, & Rahm, 2002; Euzenat et al., 2004; Jian et al., 2005; Li et al., 2007; Doshi et al.,
2009; Wang & Xu, 2009; Hanif & Aono, 2009; Bock & Hettenhausen, 2010). These algorithms
repeatedly improve on the previous preliminary solution by optimizing a measure of the solution
quality. Often, this is carried out as a guided search through the alignment space using techniques
such as gradient descent or expectation-maximization. These algorithms may run until convergence,
which means that the solution cannot be improved further because it is a, possibly local, optimum.
However, in practice, the runs are often terminated after an ad hoc number of iterations. Through
repeated improvements, the computed alignment is usually of high quality but these approaches
also consume more time in general than their non-iterative counterparts. For example, algorithms
performing among the top three in OAEI 2012 in terms of alignment quality such as YAM++ (Ngo
& Bellahsene, 2012), which ranked first in the conference track, Optima+, ranked third in the conference track, and GOMMA (Kirsten et al., 2011), which ranked first in anatomy and library tracks,
806

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

are iterative. 1 On the other hand, YAM++ consumed an excessive amount of time in completing the
conference track (greater than 5 hours) and Optima+ consumed comparatively more time as well.
Furthermore, iterative techniques tend to be anytime algorithms, which deliver an alignment
even if the algorithm is interrupted before its convergence. While considerations of computational
complexity have delivered ways of scaling the algorithms to larger ontologies, such as through
ontology partitioning (Hu, Zhao, & Qu, 2006; Seddiqui & Aono, 2009; Stoutenburg, Kalita, Ewing,
& Hines, 2010; Rahm, 2011) and the use of inverted indices (Jimenez-Ruiz & Grau, 2011), we seek
to speed up the alignment process of multiple algorithms. We think that considerations of space and
time go hand in hand in the context of usability.
Our primary contribution in this article is a general approach and its comprehensive evaluation
for significantly speeding up the convergence of iterative ontology alignment techniques. Thayasivam and Doshi (2012a) provide a preliminary introduction to this approach. Objective functions
that measure the solution quality are typically multidimensional. Instead of the traditional approach
of modifying the values of a large number of variables in each iteration, we decompose the problem into optimization subproblems in which the objective is optimized with respect to a single or a
small subset, also called a block, of variables while holding the other variables fixed. This approach
of block-coordinate descent (BCD) is theoretically shown to converge faster under considerably
relaxed conditions on the objective function such as pseudoconvexity  and even the lack of it in
certain cases  or the existence of optima in each variable (coordinate) block (Tseng, 2001). While it
forms a standard candidate tool for multidimensional optimization in statistics, and has been applied
in contexts such as image reconstruction (Pinter, 2000; Fessler & Kim, 2011) and channel capacity
computation (Blahut, 1972; Arimoto, 1972), this article presents its use in ontology alignment.
We extensively evaluate this approach by integrating it into multiple ontology alignment algorithms. We selected Falcon-AO (Jian et al., 2005), MapPSO (Bock & Hettenhausen, 2010),
OLA (Euzenat & Valtchev, 2004) and Optima (Doshi et al., 2009) as representative algorithms.
These algorithms have participated in OAEI competitions in the past, and some of them have ranked
in the top tier. Consequently, these algorithms in their default forms exhibit favorable alignment performance. Furthermore, their implementations and source codes that are needed for our approach
are freely accessible.
Using a comprehensive testbed of several ontology pairs  some of which are large  spanning
multiple domains, we show a significant reduction in the execution times of the alignment processes
thereby converging faster. Corresponding alignment quality continues to remain the same as before
or is improved by a small amount in some cases. This enables the application of these algorithms
toward aligning more ontology pairs in a given amount of time, or to more subsets in large ontology
partitions. Also, it allows these techniques to run until convergence if possible in contrast to a
predefined ad hoc number of iterations, which possibly leads to the similar or improved alignments.
This is useful in the context of techniques that are guaranteed to converge.
BCD does not constrain how the alignment variables are divided into blocks except for the rule
that each block be chosen at least once in a cycle through all blocks. Furthermore, we may order the
blocks for consideration in any manner within a cycle. Consequently, our second contribution is an
empirical study of the impact of different ordering and partitioning schemes on the improvement that
BCD brings to the alignment. In addition to the default ordering scheme based on increasing height
of grouped entities, we consider reversing this ordering, and a third approach in which we sample the
1. GOMMA utilizes multiple matching strategies some of which may not be iterative, and these partly contributed
toward its performance in OAEI as well.

807

fiT HAYASIVAM & D OSHI

blocks based on a probability distribution that represents the estimated likelihood of finding a large
alignment in a block. In the context of partitioning, we additionally consider grouping alignment
variables such that the entities are divided in a breadth-first search based partition. While our default
approach partitions one of the ontologies in a pair, we also consider the impact of partitioning both.
Performances of the iterative algorithms are impacted differently by various ways of formulating
the blocks and ordering them. Notably, the quality of the alignment may be adversely impacted.
Surprisingly, the algorithms differ in which ordering and partitioning scheme optimizes their
alignment performance. In order to comprehensively evaluate the efficiency of the BCD-enhanced
and optimized algorithms, we contribute a novel biomedical ontology alignment testbed. In addition
to being an important application domain, aligning biomedical ontologies has its own unique challenges. We selected biomedical ontologies published in NCBO for our testbed, which also provides
a primarily UMLS-sourced but incomplete reference alignment. Thirty-two different biomedical
ontologies form the 50 pairs in our testbed with about half of these having 3,000+ named classes.
The rest of this article is organized as follows. In the next section, we briefly explain iterative ontology alignment and introduce the four representative iterative algorithms. Additionally, we
briefly review the technical approach of BCD. We show how BCD may be integrated into iterative
ontology alignment algorithms in Section 3. In Section 4, we empirically evaluate the performances
of the BCD enhanced algorithms using a comprehensive data set. Then, in Section 5, we explore
other ways of ordering the blocks and partitioning the alignment variables. Thereafter, in Section 6,
we detail a new biomedical ontology benchmark and report the performances of the BCD enhanced
and optimized iterative techniques on this benchmark. We discuss the impact of BCD along with
its limitations in Section 7, and conclude this article in Section 8. Appendix A outlines the representative algorithms and their modifications to utilize BCD, followed by details on the biomedical
ontology alignment testbed in Appendix B.

2. Background
We provide a brief overview of the ontology alignment problem in the next subsection. This is
followed by brief descriptions of the four algorithms that are representative of iterative alignment
approaches. Finally, we describe the technique of BCD in general.
2.1 Overview of Ontology Alignment
An ontology is a specification of knowledge pertaining to a domain of interest formalized into
entities and relationships between the entities. Contemporary ontologies utilize description logics (Baader, Horrocks, & Sattler, 2003) such as the Web Ontology Language (OWL) (McGuinness
& Harmelen, 2004) in order to facilitate publication on the Web. OWL allows the use of classes to
represent entities, different types of properties to represent relationships, and individuals to include
instances.
The ontology alignment problem is to find a set of correspondences between two ontologies, O1
and O2 . Though OWL is based on description logic, several alignment algorithms model ontologies
as labeled graphs (with some possible loss of information) due to the presence of a class hierarchy
and properties that relate classes, in order to facilitate alignment. For example, Falcon-AO and
Optima transform OWL ontologies into a bipartite graph (Hayes & Gutierrez, 2004) and OLA
utilizes an OL-graph (Euzenat et al., 2004). Consequently, the alignment problem is often cast as a
matching problem between such graphs. An ontology graph, O, is defined as, O = hV, E, Li where,
808

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

V is the set of uniquely labeled vertices representing the entities, E is the set of edges representing
the relations, which is a set of ordered 2-subsets of V , and L is a mapping from each edge to its
label. A correspondence, ma , between two entities, xa  O1 and y  O2 , consists of the relation,
r  {=, , }, and confidence, c  R. However, the alignment algorithms that we use focus on the
possible presence of = relation (also called equivalentClass in OWL) between entities only. In this
case, an alignment may be represented as a |V1 |  |V2 | matrix that represents the correspondence
between the two ontologies, O1 = hV1 , E1 , L1 i and O2 = hV2 , E2 , L2 i:


m11
m12    m1|V2 |
 m21
m22    m2|V2 | 




.
.



.

M =


.
.

.




.
.

.
m|V1 |1 m|V1 |2    m|V1 ||V2 |

Note that if the ontologies are not modeled as graphs, the rows and columns of M are the concepts in
O1 and O2 defined in the description logic. Each assignment variable, ma in M , is the confidence
of the correspondence between entities, xa  V1 and y  V2 . Consequently, M could be a realvalued matrix, commonly known as the similarity matrix between the two ontologies. However, the
confidence may also be binary with 1 indicating a correspondence, otherwise 0, due to which the
match matrix M becomes a binary matrix representing the alignment. Two of the algorithms that
we use maintain a binary M while the others use a real M .
An alignment is not limited to correspondences between entities alone, and may include correspondences between the relationship labels as well. In order to facilitate matching relationships,
alignment techniques, including some that we use transform the edge-labeled graphs into unlabeled
bipartite ones by elevating the edge labels to first-class citizens of the graph. This process involves
treating the relationships as resources thereby adding them as nodes to the graph.
2.2 Iterative Ontology Alignment
A large class of alignment algorithms is iterative in nature (Melnik et al., 2002; Euzenat et al., 2004;
Jian et al., 2005; Li et al., 2007; Doshi et al., 2009; Wang & Xu, 2009; Hanif & Aono, 2009;
Bock & Hettenhausen, 2010; Ngo & Bellahsene, 2012). Iterative algorithms utilize a seed matrix,
M 0 , which is iteratively improved until it converges. The seed matrix is either input by the user or
generated automatically often using fast string matching and other lexical matching.
Two types of iterative techniques are predominant. These differ in how the next match matrix,
M , is obtained from the previous iterations match matrix at each step. The first type of iterative
algorithms improve the real-valued similarity matrix from the previous iteration, M i1 , by directly
updating it:
M i = U (M i1 )
(1)
where, U is a function that updates the similarities. This type of algorithms often converges to a
fixed point, M , such that, M = U (M ). 2
The second type of iterative algorithms repeatedly and explicitly search over the space of match
matrices, denoted as M. The goal is to find the alignment that optimizes an objective function,
2. Convergence is predicated on U , and a fixed point may not exist for some techniques. However, convergence is a
desirable property for iterative alignment algorithms; in its absence the stop criteria is often ad hoc.

809

fiAlignment Quality

T HAYASIVAM & D OSHI

 

Space of Alignments

Figure 2: Both iterative update and search jump from one alignment to another improving on the
previous one. The two differ in how they obtain the next alignment in each iteration and
in the qualitative metric used for assessing it. An alignment that cannot be improved
further signifies convergence.

which gives a measure of the quality of the alignment in the context of the alignment from the
previous iteration. This approach is appropriate when the search space is bounded such as when
the match matrix is binary. Nevertheless, with a cardinality of 2|V1 ||V2 | this space could get very
large. Some of the algorithms sample this space to reduce the effective search space though scaling
to large ontologies continues to remain challenging. Formally,
Mi = arg max Q(M, Mi1 )

(2)

M M

where, Mi is the alignment that optimizes the Q function in iteration i given the best alignment
from previous iteration, Mi1 . Convergence of these algorithms occurs when iterations reach a
point, M , which cannot be improved on searching for an alignment matrix, M  M, such that
Q(M, M ) > Q(M , M ). Equations 1 and 2 help solve a multidimensional optimization problem
iteratively with ma in M as the variables. We abstractly illustrate the iterative approaches in Fig. 2.
In Fig. 3, we show the abstract algorithms for the two types of iterative approaches. In the
iterative update of Fig. 3(a), we may settle for a near fixed point by calculating the distance between
a pair of alignment matrices (line 8) and terminating the iterations when the distance is within a
parameter, . As   0 we get closer to the fixed point and obtain the fixed point in the limit.
Iterative search in Fig. 3(b) often requires a seed map (line 3) to obtain M 0 , which is typically
generated using fast lexical matching.
Next, we briefly review four ontology alignment algorithms that optimize iteratively. The selection of these algorithms is based on their accessibility and competitive performance in previous
OAEI competitions, and is meant to be representative of iteration-based alignment algorithms. 3
2.2.1 FALCON -AO
Falcon-AO (Jian et al., 2005) is a well-known automated ontology alignment system combining
output from multiple components including a linguistic matcher, an iterative structural graph matching algorithm called GMO (Hu, Jian, Qu, & Wang, 2005), and a method for partitioning large
ontologies and focusing on some of the parts.
3. We sought to include YAM++ as well in our evaluation, which was the top performer in the conference track of OAEI
2012 and 2013. However, its source code is not freely available and we could not access it.

810

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

I TERATIVE U PDATE (O1 , O2 , )

I TERATIVE S EARCH (O1 , O2 )

Initialize:
1. Iteration counter i  0
2. Calculate similarity between the
entities in O1 and O2 using a measure
3. Populate the real-valued matrix, M 0 ,
with initial similarity values
4. M  M 0

Initialize:
1. Iteration counter i  0
2. Generate seed map between
O1 and O2
3. Populate binary matrix, M 0 , with
seed correspondences
4. M  M 0

Iterate:
5. Do
6.
ii+1
7.
M i = U (M i1 )
8.
  Dist(M i , M )
9.
M  M i
10. While   
11. Extract an alignment from M

Iterate:
5. Do
6.
ii+1
7.
Search M i  arg max Q(M, M i1 )
M M

8.
M  M i
9. While M 6= M i1
10. Extract an alignment from M

(a)

(b)

Figure 3: General algorithms for iterative (a) update, and (b) search approaches toward aligning
ontologies. The distance function, Dist, in line 8 of (a) is a measure of the difference
between two real-valued matrices.

GMO measures the structural similarity between the ontologies that are modeled as bipartite
graphs (Hayes & Gutierrez, 2004). Matrix M in GMO is real-valued and this similarity matrix
is iteratively updated (Eq. 1) by updating each variable, ma , with the average of its neighborhood
similarities until M stops changing significantly. GMO takes external input, typically obtained from
lexical matching, as the seed. Equation 1 manifests in GMO as a series of matrix operations:
M i = G1 M i1 GT2 + GT1 M i1 G2

(3)

Here, G1 and G2 are the adjacency matrices of the bipartite models of the two ontologies O1 and O2 ,
respectively. In the first term of the summation, the outbound neighborhood of entities in O1 and
O2 is considered, while the second term considers the inbound neighborhood. Iterations terminate
when the cosine similarity between successive matrices, M i and M i1 , is less than a parameter, .
The iterative update algorithm manifests in Falcon-AO as shown in Fig. 17(a) in Appendix A.
2.2.2 M AP PSO
MapPSO (Bock & Hettenhausen, 2010) utilizes discrete particle swarms to perform the optimization. Each of K particles in a swarm represents a valid candidate alignment, which is updated
iteratively. In each iteration, given the particle(s) representing the best alignment(s) in the swarm,
alignments in other particles are adjusted as influenced by the best particle.
Equation 2 manifests in MapPSO as a two-step process consisting of retaining the best particle(s) (alignment(s)) and replacing all others with improved ones influenced by the best alignment
in the previous iteration. The measure of the quality of an alignment in the k th particle is determined
811

fiT HAYASIVAM & D OSHI

by the mean of the measures of its correspondences as shown below:

Q(Mki )

=

|V
P1 | |V
P2 |

ma  f (xa , y )

a=1 =1

|V1 ||V2 |

(4)

where, ma is a correspondence in Mki and f represents a weighted combination of multiple syntactic and possibly semantic similarity measures between the entities in the two ontologies. Improved
particles are generated by keeping aside a random number of best correspondences according to f
in a particle, and replacing others based on the correspondences in the previous best particle. Iterations terminate when the increment in Q due to a new alignment matrix is lower than a parameter,
. Iterative search of Eq. 2 manifests in MapPSO as shown in the algorithm in Fig. 18(a).
2.2.3 OWL-L ITE A LIGNMENT
OWL-Lite alignment (OLA) (Euzenat et al., 2004) is limited to aligning ontologies expressed in
OWL with an emphasis on its most restricted dialect called OWL-Lite. OLA adopts a bipartite
graph model of an ontology, and distinguishes between 8 types of nodes such as classes, objects,
properties, restrictions and others; and between 5 types of edges: rdfs:subClassOf, rdf:type, between
classes and properties, objects and property instances, owl:Restriction, and properties in individuals.
OLA computes the similarity between a pair of entities from two ontologies as a weighted
aggregation of the similarities between respective neighborhood entities. Due to its consideration
of multiple types of edges, cycles are common. Consequently, it computes the similarities between
entities as the solution of a large system of linear equations, solved iteratively for the fixed point.
Let F(xa ) be the set of all nodes in O1 , which are connected to the node xa via an edge type,
F. Formally, similarity Sim(xa , y ), between vertex, xa  O1 , and vertex, y  O2 , is defined as,
X
a
Sim(xa , y ) =
wF
SetSim(F(xa ), F(y ))
(5)
F N (xa ,y )

a , for an
where, N (xa , y ) is the set of all edge types in which xa , P
y participate. Weight, wF
a
wF = 1. Function, SetSim,
entity pair, xa , y , and edge type, F, is normalized, i.e.,
F N (xa ,y )

evaluates the similarity between sets, F(xa ) and F(y ), as the average of maximal pairing.
OLA initializes a real-valued similarity matrix, M 0 , with values based on lexical attributes
only, while the iterations update each variable, ma , in the matrix using the structure of the two
ontologies. In particular, if two entities, xa and y are of the same type, then ma is updated using
Eq. 5, otherwise the value is 0. Iterative update of Eq. 1 is realized by OLA as in Fig. 19(a) in
Appendix A.
2.2.4 O PTIMA
Optima (Doshi et al., 2009) formulates ontology alignment as a maximum likelihood problem, and
searches for the match matrix, M , which gives the maximum conditional probability of observing
the ontology O1 , given the other ontology, O2 , under the match matrix M .
It employs generalized expectation-maximization to solve this optimization problem in which,
it iteratively evaluates the expected log likelihood of each candidate alignment and picks the one
which maximizes it. It implements Eq. 2 as a two-step process of computing expectation followed
812

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

by maximization, which is iterated until convergence. The expectation step consists of evaluating
the expected log likelihood of the candidate alignment given the previous iterations alignment:
Q(M i |M i1 ) =

|V1 | |V2 |
X
X

P r(y |xa , M i1 )  logP r(xa |y , M i )i

(6)

a=1 =1

where, xa and y are entities in ontologies O1 and O2 respectively, and i is the prior probability
of y . P r(xa |y , M i ) is the probability that node xa is in correspondence with node y given the
match matrix M i . The prior probability is computed as,
|V1 |

i =

1 X
P r(y |xa , M i1 )
|V1 |
a=1

The generalized maximization involves finding a matrix, Mi , that improves on the previous one:
Mi = M i  M : Q(M i |Mi1 )  Q(Mi1 |Mi1 )

(7)

We show the iterative alignment algorithm of Optima in Fig. 20(a).
Altogether, the four alignment algorithms that we describe in this subsection represent a broad
variety of iterative update and search techniques, realized in different ways. This facilitates a broad
evaluation of the usefulness of BCD. Over the years, algorithms such as Falcon-AO, OLA and Optima have performed satisfactorily in the annual OAEI competitions, with Falcon-AO and Optima
demonstrating strong performances with respect to the comparative quality of the generated alignment. For example, Falcon-AO often ranked in the top 3 systems when it participated in OAEI
competitions between 2005 and 2010, and its performance continues to remain a benchmark for
other algorithms. Optima enhanced with BCD (called Optima+) ranked second in the conference
track (F2-measure and recall) in the 2012 edition of the OAEI competition (Thayasivam & Doshi,
2012b). Consequently, these representative algorithms exhibit strong alignment performances. On
the other hand, MapPSOs performance is comparatively poor but its particle-swarm based iterative approach motivates its selection in our representative set.
2.3 Block-Coordinate Descent
Large-scale multidimensional optimization problems maximize or minimize a real-valued continuously differentiable objective function, Q, of N real variables. Block-coordinate descent (BCD)
(Tseng, 2001) is an established iterative technique to gain faster convergence in the context of such
large-scale N -dimensional optimization problems. In this technique, within each iteration, a set of
the variables referred to as coordinates are chosen and the objective function, Q, is optimized with
respect to one of the coordinate blocks while the other coordinates are held fixed. In our application
setting, recall that the coordinates are the alignment variables in the match matrix, M .
Let S denote a block of coordinates, which is a non-empty subset of {1, 2, . . . , N }. Define a set
of such blocks as, B = {S0 , S1 , . . . , SC }, which is a set of subsets each representing a coordinate
block with the constraint that, S0  S1  . . .  SC = {1, 2, . . . , N }. B could have a single block or
be a partition of the coordinates although this is not required and the blocks may intersect. We also
define the complement of a coordinate block, Sc , where c  {0, 1, . . . , C}, as, Sc = B  Sc . To
813

fiT HAYASIVAM & D OSHI

illustrate, let the domain of a real-valued, continuously differentiable, multidimensional function, Q,
with N = 10 be, M = {m1 , m2 , m3 , . . . , m10 }, where each element is a variable. We may partition
this set of coordinates into two blocks, C = 2, so that, B = {S0 , S1 }. Let S0 = {m2 , m5 , m8 } and
S1 = {m1 , m3 , m4 , m6 , m7 , m9 , m10 }. Finally, S0 denotes the block, S1 .
BCD converges to a fixed point such as a local or a global optimum of the objective function
under relaxed conditions such as pseudoconvexity of the function and requires the function to have
bounded level sets (Tseng, 2001). While pseudoconvex functions continue to have fixed points, they
may have non-unique optima along different coordinate directions. In the absence of pseudoconvexity, BCD may oscillate without approaching any fixed point of the function. Nevertheless, BCD
still converges if the function has unique optima in each of the coordinate blocks.
In order to converge using BCD, we must satisfy the following rule, which ensures that each
coordinate is chosen sufficiently often (Tseng, 2001).
Definition 1 (Cyclic rule) There exists a constant, T  C and C > 0, such that every block, Sc , is
chosen at least once between the ith iteration and the (i + T  1)th iteration, for all i.
In the context of the cyclic rule, BCD does not mandate a specific partitioning or an ordering scheme
for the blocks. A simple way to meet this rule is by sequentially iterating through each block
although we must continue iterating until each block converges to the fixed point.
Recently, Saha and Tewari (2013) show that the nonasymptotic convergence rate 4 of BCD
under the cyclic rule is faster than that of gradient descent (GC) if they both start from the same
point, under the conditions that the objective function, Q, has a Lipschitz continuous gradient (it
is differentiable everywhere and has a bounded derivative) or is strongly convex, and I  Q
L is
isotonic, where I is the identity function and L is the Lipschitz constant. Starting from the same
0
0 , let M i
i
initial map, MBCD
= MGC
BCD and MGC denote the alignment at iteration i by BCD with
cyclic rule and GC, respectively. Under the condition that the objective function, Q, which must be
i
i ). The nonasymptotic
say, minimized, is continuous and isotonic, i  1, Q(MBCD
)  Q(MGC
convergence rate of BCD under the cyclic rule for objective functions with the previous properties
is, O(1/i), where i is the iteration count.

3. Integrating BCD into Iterative Alignment
As we mentioned previously, ontology alignment may be approached as a principled multivariable
optimization of an objective function, where the variables are the correspondences between the
entities of the two ontologies. Different algorithms formulate the objective function differently. As
the objective functions are often complex and difficult to differentiate, numerical iterative techniques
are appropriate but these tend to progress slowly. In this context, we may speed up the convergence
rate using BCD as we describe below.
3.1 General Approach
In Section 2.2, we identified two types of iterative ontology alignment algorithms. BCD may be
integrated into both these types. In order to integrate BCD into the iterations, the match matrix,
M , must be first suitably partitioned into blocks. Of course, existing algorithms may be viewed as
having a single block of variables and therefore trivially utilizing BCD.
4. This is the rate of convergence effective from the first iteration itself.

814

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

Though a matrix may be partitioned using one of several ways, we adopt an approach that is
well supported in the context of ontology alignment. An important heuristic, which has proved
highly successful in both ontology and schema alignment, matches parent entities in two ontologies
if their respective child entities were previously matched (Doan, Madhavan, Domingos, & Halevy,
2003). This motivates grouping together those variables, ma in M , into a coordinate block such
that the xa participating in the correspondence belong to the same height leading to a partition of
M . The height of an ontology node is the length of the shortest path from a leaf node. Subsequently,
the alignment in blocks with less height (containing the child entities) is optimized first followed by
those with increasing height (containing the parent entities). In determining this height, we utilize
the tree or graph model of the ontology that is built internally by the respective ontology alignment
algorithm. These include property nodes and may differ between algorithms.
Let the partition of M into coordinate blocks be {MS0 , MS1 , . . . , MSC }, where C is the height
of the largest class hierarchy in ontology O1 . Thus, each block is a submatrix with as many rows as
the number of entities of O1 at a height and number of columns equal to the number of all entities in
O2 . For example, correspondences between the leaf entities of O1 and all entities of O2 will form
the block, MS0 . In the context of a bipartite graph model as utilized by Falcon-AO and Optima,
which represents properties in an ontology as vertices as well and are therefore part of M , these
would be included in the coordinate blocks.
Iterative ontology alignment integrated with BCD optimizes with respect to a single block, MSc ,
at an iteration while keeping the remaining blocks fixed. In order to meet the cyclic rule, we choose
a block, MSc , at iterations, i = c + qC where q  {0, 1, 2, . . .}. We point out that BCD is applicable
to both types of iterative alignment techniques outlined in Section 2.2. Alignment algorithms which
update the similarity matrix iteratively as in Eq. 1 will now update only the current block of interest,
MSc , and the remaining blocks are carried forward as is, as shown below:
MSi c = USc (M i1 )
MSi = MSi1
S  Sc

(8)

where Sc is the complement of Sc in B. Note that MSi c combined with MSi for all S  Sc forms
M i . Update function, USc , modifies U in Eq. 1 to update just a block of the coordinates.
Analogously, iterative alignment which searches for the candidate alignment that maximizes
the objective function as in Eq. 2, will now choose a block, MSc , at each iteration. It will search
over the reduced search space pertaining to the subset of all variables included in MSc , for the best
candidate coordinate block. Formally,

MSi c , = arg max QS MSc , Mi1
MSc MSc
(9)
i = M i1
MS,

S

S
c
S,
where, MSc is the space of alignments limited to block, Sc . The original objective function, Q, is
modified to QS such that it provides a measure of the quality of the block, MSc , given the previous
best match matrix. Note that the previous iterations matrix, Mi1 , contains the best block that was
of interest in that iteration.
Performing the update, USc , or evaluating the objective function, QS , while focusing on a coordinate block may be performed in significantly reduced time as compared to performing these
operations on the entire alignment matrix. While we may perform more iterations as we cycle
815

fiT HAYASIVAM & D OSHI

2.2e+07

Q-Value

2.1e+07
2e+07
1.9e+07
1.8e+07
1.7e+07

Optima with BCD
Optima

1.6e+07
0

10

20

30

40

50

60

time (s)

Figure 4: BCD facilitates faster convergence in aligning ontologies iasted and sigkdd both related
to conference organization.

through the blocks, the use of partially updated matrices from the previous iteration in evaluating
the next block facilitates faster convergence. We illustrate the impact of BCD on iterative search
as performed by Optima on an example ontology pair in Fig. 4. Alignment using BCD shows the
faster convergence rate.
I TERATIVE U PDATE WITH BCD (O1 , O2 , )

I TERATIVE S EARCH WITH BCD (O1 , O2 )

Initialize:
1. Iteration counter i  0
2. Calculate similarity between the
entities in O1 and O2 using a measure
3. Populate the real-valued matrix, M 0 ,
with initial similarity values
4. Create a partition of M :
{MS0 , MS1 , . . . , MSC }
5. M  M 0

Initialize:
1. Iteration counter i  0
2. Generate seed map between
O1 and O2
3. Populate binary matrix, M 0 ,
with seed correspondences
4. Create a partition of M :
{MS0 , MS1 , . . . , MSC }
5. M  M 0

Iterate:
6. Do
7.
c  i % (C + 1), i  i + 1
8.
MSi c  USc (M i1 )
9.
MSi  MSi1 S  Sc
10. If c = C then
11.
  Dist(M i , M )
else
12.
 is a high value
13. M  M i
14. While   
15. Extract an alignment from M

Iterate:
6. Do
7.
c  i % (C + 1), i  i + 1

8.
Search MSi c ,  arg max QS MSc , Mi1
MSc MSc

i1
i
MS,
 MS,
S  Sc
If c = C then
changed  Mi 6= Mi1 ?
else
12.
changed  true
13. While changed
14. Extract an alignment from Mi

9.
10.
11.

(a)

(b)

Figure 5: General iterative algorithms of Fig. 3 are modified to obtain, (a) iterative update enhanced
with BCD, and (b) iterative search enhanced with BCD. The update or search steps in line
numbers 8 and 9 are modified to update only the current block of interest.

816

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

Algorithms in Fig. 5 revise the iterative update and search algorithms of Fig. 3 in order to
integrate BCD. The primary differences in both involve creating a partition of the alignment matrix,
M (line 4), and iterations that sequentially process each coordinate block only while keeping the
others fixed (lines 7-9). On completing a cycle through all coordinate blocks as determined by the
check in line 10, we evaluate whether the new alignment matrix differs from the one in the previous
iteration, and continue the iterations if it does (lines 11-13). Observe that the regular iterations
improving the full match matrix are now replaced with mini-iterations updating the blocks.
Given the general modifications brought about by BCD, we describe how these manifest in the
four iterative alignment systems that form our representative set. The modifications are based on
the type of iterative technique and are uniform within each group. They do not change the core
alignment approach of each algorithm given the input as we see next.
3.2 BCD Enhanced Falcon-AO
We enhance Falcon-AO by modifying GMO to utilize BCD as it iterates. As depicted in Fig. 17(b),
we begin by partitioning the similarity matrix used by GMO into C + 1 blocks based on the height
of the entities in O1 that are part of the correspondences, as mentioned previously. GMO is then
modified so that at each iteration, a block of the similarity matrix is updated while the other blocks
remain unchanged. If block, Sc , is updated at iteration i, then Eq. 3 becomes:
MSi c = G1,Sc M i1 GT2 + GT1,Sc M i1 G2
MSi = MSi1 S  Sc

(10)

Here, G1,Sc focuses on that portion of the adjacency matrix of O1 that corresponds to the outbound
neighborhood of entities participating in correspondences of block Sc , while GT1,Sc focuses on the
inbound neighborhood of entities in Sc . Adjacency matrix, G2 , is utilized as before. The outcome
of the matrix operations is a similarity matrix, with as many rows as the variables in Sc and columns
corresponding to all the entities in O2 . The complete similarity matrix is obtained at iteration, i, by
carrying forward the remaining blocks unchanged, which is then utilized in the next iteration. The
general iterative update modified to perform BCD of Fig. 5(a) may be realized in Falcon-AO as in
the algorithm of Fig. 17(b) in Appendix A.
3.3 BCD Enhanced MapPSO
We may integrate BCD into MapPSO by ordering the particles in a swarm based on a measure
of the quality of a coordinate block, Sc , in each particle in an iteration. Equation 4 is modified to
measure the quality of the correspondences in just the coordinate block Sc , in the k th particle by
taking the average:
|VP
1,c | |V
P2 |
ma  f (xa , y )
a=1 =1
i
QS (Mk ) =
(11)
|V1,c ||V2 |
where, V1,c denotes the set of entities of ontology, O1 , of identical height participating in the correspondences included in block Sc . As before, we retain the best particle(s) based on this measure
i
, in the remaining particles using the best
and improve on the alignment in a coordinate block, Mk,S
c
particle in the previous iteration. The remaining coordinates are held unchanged. Iterative search of
MapPSO modified using BCD is shown in the algorithm of Fig. 18(b).
817

fiT HAYASIVAM & D OSHI

3.4 BCD Enhanced OLA
As explained earlier, OLA evolves its similarity matrix M by similarity exchange between pairs of
neighboring entities. In each iteration, it performs an element-wise matrix update operation. OLA is
enhanced with BCD by adopting Eq. 8. Specifically, the similarity values of the coordinates of the
chosen block, Sc , will be updated using the similarity computations (Eq. 5). The remaining blocks,
MSi , are kept unchanged.
c

mia

=



Sim(xa , y ) if types of xa and y are the same
, mia M i
Sc
0
otherwise

(12)

MSi = MSi1 S  Sc
3.5 BCD Enhanced Optima
As we mentioned previously, Optima utilizes generalized expectation-maximization to iteratively
improve the likelihood of candidate alignments. Jeffery and Alfred (1994) discuss a BCD inspired expectation-maximization scheme and call it the space alternating generalized expectationmaximization (SAGE). Intuitively, SAGE maximizes the expected log likelihood of a block of coordinates thereby limiting the hidden space, instead of maximizing the likelihood of the complete
alignment. The sequence of block updates in SAGE monotonically improves the objective likelihood. For a regular objective function, the monotonicity property ensures that the sequence will not
diverge, but it does not guarantee convergence. However, proper initialization lets SAGE converge
locally. 5 In each iteration, Optima enhanced using SAGE chooses a block of the match matrix,
MSi c , and its expected log likelihood is estimated. As in previous techniques, we choose the blocks
in a sequential manner such that all the blocks are iterated in order.
Equation 6 changes to estimate the expected log likelihood of a block of a candidate alignment:
|V1,c | |V2 |

QS (MSi c |M i1 ) =

XX

i
P r(y |xa , M i1 )  logP r(xa |y , MSi c ) ,c

(13)

a=1 =1

Recall that V1,c denotes the set of entities of ontology, O1 , participating in the correspondences
i , is modified as well to utilize just V
included in Sc . Notice that the prior probability, ,c
1,c in its
calculations.
The generalized maximization step now involves finding a match matrix block, MSi c , , that
improves on the previous one:
|Mi1 )
MSi c , = MSi c  MSc : QS (MSi c , |Mi1 )  QS (MSi1
c ,

(14)

Here, MSi1
is a part of Mi1 .
c ,
At iteration i, the best alignment matrix Mi , is formed by combining the block MSi c , , which
i1
improves QS as defined in Eq. 14 with the remaining blocks from the previous iteration, MS,
, in
the complement of Sc , unchanged.
5. Furthermore, the convergence rate may be improved by choosing the hidden space with less Fisher information (Hero
& Fessler, 1993).

818

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

The algorithm in Fig. 20(b) shows how Optima may be enhanced with BCD. We expect significant savings in time because of the search over a reduced space of alignments focused on a block,
MSc , in each iteration. Additionally, both the objective function, QS , and the prior operate on a
single coordinate block in reduced time. Finally, using aligned blocks in the next iteration improves
the convergence rate.

4. Empirical Analysis
While the use of BCD is expected to make the iterative approaches exhibit a greater rate of improvement, and if the approach converges, reach the fixed point faster, we seek to empirically determine:
1. The amount of speed up obtained for the various alignment algorithms by integrating BCD;
and
2. Changes in the quality of the final alignment, if any, due to BCD. This may happen because
the iterations converge to a different local optimum.
Ontology

Named Classes
Conference domain
ekaw
74
sigkdd
49
iasted
150
cmt
36
edas
104
confOf
38
conference
60
Life Sciences
mouse anatomy
2,744
human anatomy
3,304

Properties
33
28
41
59
50
36
64
2
3

Table 1: Ontologies from OAEI 2012 used in our evaluation. We show the number of named classes
and properties in each as an estimate of their size. Notice that our evaluation includes
some large ontologies from different domains as well. Additionally, Thayasivam and
Doshi (2012a) present evaluations on the four pairs in the 300 range of the bibliography
benchmark competition.
We use a comprehensive testbed of several ontology pairs  some of which are large  spanning
two domains. We used ontology pairs from the OAEI competition in its 2012 version as the testbed
for our evaluation (Shvaiko et al., 2012). Among the OAEI tracks, we focus on the test cases
that involve real-world ontologies for which the reference (true) alignment was provided by OAEI.
These ontologies were either acquired from the Web or created independently of each other and
based on real-world resources. This includes all pairs of the expressive ontologies in the conference
track all of which structure knowledge related to conference organization, and the anatomy track,
which consists of a pair of mid-sized ontologies from the life sciences describing the anatomy
of an adult mouse and human. We list the ontologies from OAEI participating in our evaluation
in Table 1 and provide an indication of their sizes. Additionally, Thayasivam and Doshi (2012a)
evaluate Falcon-AO, MapPSO and Optima with BCD on the four pairs in the 300 range of the
819

fiT HAYASIVAM & D OSHI

bibliography benchmark competition. Ontology pairs in the 100 and 200 ranges of the bibliography
benchmark were not utilized as the participating ontologies are very small with just 33 classes and
64 properties. Subsequently, our representative iterative techniques align these very quickly in the
order of milliseconds leaving no significant room for improvement.
We align ontology pairs using the four representative algorithms, in their original forms and with
BCD using the same seed alignment, M 0 , if applicable. The iterations were run until the algorithm
converged and we measured the total execution time, final recall, precision and F-measure, and the
number of iterations performed until convergence. Recall measures the fraction of correspondences
in the reference alignment that were found by an algorithm while precision measures the fraction of
all the found correspondences that were in the reference alignment thereby indicating the fraction
of false positives. F-measure represents a harmonic mean of recall and precision.
We averaged results of 5 runs on every ontology pair using both the original and the BCD
enhanced version of each algorithm. Because of the large number of total runs, we ran the tests on
two different computing platforms while ensuring comparability. One of these is a Red Hat machine
with Intel Xeon Core 2, processor speed of about 3 GHz with 8GB of memory (anatomy ontology
pair) and the other one is a Windows 7 machine with Intel Core i7, 1.6 GHz processor and 4GB of
memory (benchmark and conference ontology pairs). While comparing the performance metrics for
statistical significance, we tested the data for normality and used Students paired t-test if it exhibits
normality. Otherwise, we employed the Wilcoxon signed-rank test. We utilized the 1% level (p 
0.01) to deem significance.
As Thayasivam and Doshi (2012a) did not previously evaluate OLA on the bibliography domain
ontology pairs, we discuss its performance in this article for completeness. Similar to the other algorithms, the introduction of BCD in OLA reduced its execution time on all four pairs by a total of 1.3
seconds compared to the original time of 27.3 seconds. OLAs precision and recall reduced slightly
causing its F-measure to reduce by 1% for the ontology pair (302,101), while the the alignments for
the other pairs remained the same.
The ontologies in the conference domain vary widely in their size and structure. As shown
in Fig. 6, the introduction of BCD to the four iterative techniques clearly improves their speed of
convergence and the differences for each algorithm are significant (Students paired t-test, p 
0.01). In particular, we observed an order of magnitude reduction in time for aligning relatively
larger ontologies such as iasted and edas. For example, pairs (conference, iasted) on MapPSO and
(edas, iasted) on Optima showed such reductions. Overall, we observed a total reduction of 50
seconds for Falcon-AO to 3 minutes, 1 minute and 37 seconds for MapPSO, 31 seconds for OLA
to a total of 1 minute and 37 seconds, and by 29 minutes and 20 seconds for Optima to 4 minutes
and 53 seconds.
Falcon-AO shows no change due to BCD in its alignment, holding its precision at 25% and
recall at 66%. Optima shows a 4% improvement in average precision from 56% to 60% but average
recall reduced from 70% to 68%. Nevertheless, this causes a 2% improvement in average F-measure
to 64%. MapPSO with BCD resulted in a significant improvement in final precision from 9% to
43% on average, although the difference in recall was not significant. The precision and recall for
OLA remained unchanged.
The mid-sized anatomy ontologies for mouse and human were not successfully aligned by
MapPSO and OLA despite the use of BCD. However, BCD reduced Falcon-AOs average execution time for aligning this single ontology pair by 6.2 seconds to 2.6 minutes, and drastically
reduced Optimas average execution time to 4.4 minutes from 62.7 minutes. The alignment gen820

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

60

20
Falcon-AO
Falcon-AO with BCD

16
14

Time (sec)

40

Time (sec)

MapPSO
MapPSO with BCD

18

50

30
20

12
10
8
6

10

4
2

0
e

er

f
on

m

(c

t,C

s
da

e
nc

re

fe

on

(C

(c

,i
as

(e

)

f)

aw

fO

,ek

on

t,c

m

d

o

n

re

fe

on

(C

O
nf

d
te

as

f,e

sig

ce

)

)

s
da

d
kd

,e

ig

m

(c

d
kd

t,s

)

)

)

)

e
nc

(c

re

fe

on

(a)

,i
ce

d)

,s
ce

(c

(c

w)

ka

f,e

f,e

O
nf

o

n

re

fe

on

(c

s)

da

kd

ig

n

re

fe

on

(c

d)

te

as

e
nc

O
nf

o

(c

(b)

10
OLA
OLA with BCD

Optima
Optima with BCD

1000

8

Time (sec)

Time (sec)

100
6

4

10

1

2

0

0.1
s)

d)

da

kd

t

m

(c

g
,si

a
e,i

nc

en

r
fe

on

(C

f

e
er

on

(C

)

ed

st

,e
ce

d)

te

s
-ia

s

da

(e

ka

(e

)

d)

ed

st

ia
w,

e)

kd

g
,si

t,

m

(c

(c)

n
Co

nc

re

fe

aw

k
(e

t

m

(c

d)

f)

fO

kd

n
,co

t

m

(c

a
s,i

da

(e

(d)

)

ed

st

g
,si

ka

(e

d)

)

kd

ed

st

ia
w,

sig

,
ed

st

(ia

Figure 6: Average execution time consumed by, (a) Falcon-AO, (b) MapPSO, (c) OLA, and (d)
Optima in their original form and with BCD, for 6 of the 21 ontology pairs from conference domain. We ran the algorithms for all the pairs, and selected ontology pairs which
exhibited the three highest and the three lowest differences in average execution times
for clarity. Note that the time axis of (d) is in log scale. Notice the improvements in
execution time for the larger pairs. Specifically, about a 50% reduction in average execution time for the ontology pair (edas, iasted) by Falcon-AO and an order of magnitude
reductions in average run time for ontology pairs (conference, iasted) in MapPSO and
(edas, iasted) in Optima, were observed.

erated by Falcon-AO with BCD remained unchanged at 76.1% precision and 80% recall while
the alignment from Optima with BCD improved to a precision of 96% and recall of 74.2%. Both
Falcon-AO and Optima automatically utilized their ontology partitioning methods in order to align
these mid-sized pairs.
In summary, the introduction of BCD led to significant reductions in convergence time for all
four iterative algorithms on several ontology pairs, some extending to an order of magnitude. Simultaneously, the quality of the final alignments as indicated by F-measure improved for a few pairs,
with one pair showing a reduction in the context of Optima. However, we did not observe a change
in the F-measure for many of the pairs. Therefore, our empirical observations indicate that BCD
does not have a significant adverse impact on the quality of the alignment.
821

fiT HAYASIVAM & D OSHI

5. Optimizing BCD using Ordering and Partitioning Schemes
As we mentioned previously, BCD does not overly constrain the formation of the coordinate blocks
and neither does it impose an ordering on the consideration of the blocks, other than satisfying
the cyclic rule. Consequently, we explore other ways of ordering the blocks and partitioning the
alignment variables in the context of the representative algorithms. These include:
1. Ordered from roots to leaves: Cycle over blocks of decreasing height starting with the block
containing entities with the largest height.
2. Ordered by similarity distribution: Obtain an aggregate measure of the lexical similarity between the ontology entities participating in each block. The normalized distribution of similarities provides the likelihood of picking the next block.
3. Both ontologies partitioned: A block contains participating entities from each ontology that
are at the same height.
4. Subtree-based partitioning: Transform the ontology into a tree and form a block of variables
such that the participating entities are a part of a subtree of a predefined size.
5. Random partitioning: Form a block by randomly selecting alignment variables for inclusion.
While the partitioning and ordering utilized in the previous section are intuitive, our objective is to
discover if other ways may further improve the run time performances of the algorithms. In subsequent experimentation, we exclude MapPSO from our representative set due to the randomness in
its algorithm, which leads to comparatively high variability in its run times.
5.1 Ordering The Blocks
The order in which the blocks are processed may affect performance. This is because updated
correspondences from the previous blocks are used in generating the alignment for the current block.
Initially, blocks with participating entities of increasing height beginning with the leaves were used
as illustrated in Fig. 7. Other ordering schemes could improve performance:
 We may reverse the previous ordering by cycling over blocks of decreasing height, beginning
with the block that contains entities with the largest height. This leads to processing parent
entities first followed by the children.
 We may obtain a quick and approximate estimate of the amount of alignment in a block of
variables. One way to do this is to compute an aggregate measure of the lexical similarity
between the entities of the two ontologies participating in the block. Assuming the similarity to be an estimate of the amount of alignment in a block, we may convert the estimates
into a probability distribution that gives the likelihood of finding multiple correspondences
in a block. The block to process next is then sampled from this distribution. This approach
requires a relaxation of the cyclic rule because a particular block is not guaranteed to be selected. In this regard, an expectation of selecting each block is sufficient to obtain asymptotic
convergence of BCD (Nesterov, 2012).
822

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

role
agent

data

worker_role
sample

reagent_role

researcher

drug

drug_role
Specimen
O2
drug

m11

m12

m13

m14

m15

agent

m21

m22

m23

m24

m25

sample

m31

m32

m33

m34

m35

researcher

m41

m42

m43

m44

m45

m51

m52

m53

m54

m55

O1

data

Figure 7: Presence or absence of correspondences between entities of two ontologies is represented
in the match matrix. Concepts drug, sample, and researcher are all leaves and correspondences with these may be grouped into a block (highlighted). We may process this block
first followed by the block containing data and agent. Alternately, we may reverse this
ordering for optimizing blocks.

We compare the performances of the alternate ordering schemes with the initial on the 21 ontology pairs in the conference domain. The results of reversing the order of the original scheme
are shown in Fig. 8. Clearly, the original ordering allows all three BCD-enhanced approaches to
converge faster in general. While Optimas average recall across all pairs improved slightly from
68% to 70%, average precision reduced by 4% to a final of 56%. Falcon-AOs average F-measure
improved insignificantly at the overall expense of 40 seconds in run time. Reversing the order has
no impact on the precision and recall of OLA. These results are insightful in that they reinforce the
usefulness of the alignment heuristic motivating the original ordering scheme.
Our second alternate ordering scheme involves determining the aggregate lexical similarity between the entities participating in a block. The distribution of similarities is normalized and the next
block to consider is sampled from this distribution. Notice from Fig. 9 that Falcon-AO and OLA
demonstrate significant increases in convergence time (p  0.01) compared to utilizing BCD with
the initial ordering scheme; on the other hand, the overall time reduces for Optima and by orders
of magnitude for some of the pairs containing the larger ontologies such as edas and iasted. We
select 6 pairs, which exhibit the highest and lowest differences in average execution times to show
in Fig. 9. Falcon-AOs precision and recall show no significant change and its F-measure remains
unchanged. OLA loses both precision and recall with the similarity distribution scheme. The precision across all pairs went down to 13% from 37% along with a 24% drop in recall from 58% leading
to a drop in F-measure to 19%. However, Optimas F-measure remains largely unaffected.
Recall that both Falcon-AO and OLA perform iterative updates while Optima conducts an
iterative search. While all sampled blocks undergo updates by the iterative update algorithms, search
algorithms may not improve the blocks having low similarity. Consequently, blocks with high
similarity that are sampled more often are repeatedly improved. This results in quicker convergence
823

fiT HAYASIVAM & D OSHI

Falcon-AO with BCD
Falcon-AO with BCD (ordered from roots to leaves)

50

OLA with BCD
OLA with BCD (ordered from roots to leaves)

14
12

40

Time (sec)

Time (sec)

10
30

20

8
6
4

10
2
0

0

)

(

e
nc

re

fe

on

(C

f
on

on

e
nc

O

s
,ia

nc

(c

(

n
Co

t,

m

(c

(a)
1000

e
er

f

s

a
ed

s)

e)

d)

te

d
f,e

f
on

re

fe

on

(C

k
sig

,

c

n
re

)

as

dd

d
e,e

fe

(C

)

)

as

,c

t,

cm

)

Of

dd

k
sig

da

)
OF

f

n
,co

t

m

(c

,e
ce

en

on

r
fe

(C

on

(C

(b)

d)

kd

ig

en

r
fe

d)

te

as

,i
ce

(c

f
on

,s
Of

)

ed

st

,ia

s
da

(e

Optima with BCD
Optima with BCD (ordered from roots to leaves)

Time (sec)

100

10

1

)

aw

,ek

re

fe

on

(C

e
nc

n

re

fe

n
Co

d)

te

as

,i
ce

f

on

(c

(

d)

te

as

,i
Of

ka

(e

)

dd

si
d,

e

st

(ia

dd

gk

gk

si
w,

ka

(e

)

)

ed

st

ia
w,

(c)

Figure 8: Average execution times of, (a) Falcon-AO, (b) OLA, and (c) Optima, with BCD that
uses the initial ordering scheme and with BCD ordering the blocks from root(s) to leaves,
for 6 of the 21 ontology pairs from the conference domain. While we ran the algorithms
for all the pairs, we selected ontology pairs which exhibited the highest and lowest differences in average execution times. This alternate ordering increases the run times to
convergence and we did not observe significant improvements in the F-measures.

to a different and peculiar local optima where the blocks with high similarity have converged while
the others predominantly remain unchanged. Thus, the alignment quality remains largely unaffected
while the convergence time is reduced, as we see in the context of Optima.
5.2 Partitioning the Alignment Variables
Because BCD does not impose a particular way of grouping variables, other well-founded partitioning schemes may yield significant improvements:
 An extension of the initial scheme (Fig. 10(a)) would be to group variables representing
correspondences such that the participating entities from each of O1 and O2 are at the same
height in relation to a leaf entity in the ontology, as we illustrate in Fig. 10(b). Note that
824

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

20

Falcon-AO with BCD
Falcon-AO with BCD (ordered by similarity distribution)

60
50

15

Time (sec)

40

Time (sec)

OLA with BCD
OLA with BCD (ordered by similarity distribution)

30

10

20
5
10
0

0

)

)
re

fe

on

(C

fO

on

(c

aw

k
sig

f,

nc

)

dd

d
e,e

t,

m

(c

)

as

dd

k
sig

da

(e

d)

s
,ia

as

k
sig

(e

e
nf

o

t,C

(a)
1000

as

OF

nf

o
t,c

m

(c

m

(c

)

)

nc

re

w,

ka

d

(e

e)

)

dd

te

k
s,e

,ed

e
nc

re

fe

on

(

(b)

)

fO

on

(c

dd

gk

i
f,s

n

re

fe

n
Co

(C

d)

te

as

,i
ce

d)

te

as

,i
as

d

(e

Optima with BCD
Optima with BCD (ordered by similarity distribution)

Time (sec)

100

10

1

0.1

(C

on

r
fe

d)

,e
ce

kd

ig
f,s

en

gk

,si

t

m

(c

s)

da

)
dd

on

(c

da

(e

(c)

)

)

)

ed

st

a
s,i

fO

aw

da

(e

dd

gk

k
s,e

si
w,

ka

(e

Figure 9: Average execution time consumed by, (a) Falcon-AO, (b) OLA and (c) Optima with
BCD utilizing the previous ordering scheme and with BCD ordering the blocks by similarity distribution, for 6 of the 21 ontology pairs from conference domain. Although we
ran the algorithms for all the pairs, we show ontology pairs which exhibited the highest and lowest differences in average execution times. The new ordering helped Optima
further cut down the total execution time by 262 seconds while finding 1 more correct
correspondence and 6 false positives across all pairs.

the entity heights may differ between the two ontologies. This is based on the observation
that the generalization-specialization hierarchy of concepts pertaining to a subtopic is usually
invariant across ontologies.
 A more sophisticated scheme founded on the same observation is to temporarily transform
each ontology, which is modeled as a labeled graph, into a tree. We may utilize any graph
search technique that handles repeated nodes, such as breadth-first search for graphs (Russell
& Norvig, 2010), to obtain the tree. If the ontology has isolated graphs leading to separate
trees, we use the owl:thing node to combine them into a single tree. Subsequently, we group
those variables such that participating entities from each ontology are part of a subtree of a
825

fiT HAYASIVAM & D OSHI

O

O

m m
 mff m	 mfi

m m
 mff m	 mfi



O



m
 m

 m
ff m
	 m
fi

O





m
 m

 m
ff m
	 m
fi

mff mff
 mffff mff	 mfffi

mff mff
 mffff mff	 mfffi

m	 m	
 m	ff m		 m	fi

m	 m	
 m	ff m		 m	fi





O



O



m m
 mff m	 mfi
m
 m

 m
ff m
	 m
fi
mff mff
 mffff mff	 mfffi
m	 m	
 m	ff m		 m	fi


Figure 10: Matrices representing an intermediate alignment between entities of O1 and O2 . (a)
Identically shaded rows form a block of variables because the corresponding entities
of O1 are at the same height. (b) Identically shaded rows and columns correspond to
entities at the same heights in O1 and O2 , respectively. Variables in overlapping regions
form a block. (c) Entities corresponding to identically shaded rows or columns form
subtrees. A fourth approach is to randomly select variables for inclusion into a block.

predefined size (Fig. 10(c)). We may discard the ontology trees after forming the blocks.
While the previous schemes form blocks of differing numbers of variables, this scheme forms
all but one block with the same number of variables by limiting the subtree size.
 A simple point for comparison would be a scheme that randomly selects alignment variables
for inclusion in a block. With no clear way to determine how many variables to include in a
block, we randomly inserted variables into 5 blocks.
Based on the findings in the previous subsection, the blocks are ordered based on height of the
participating entities or the subtrees root nodes for Falcon-AO and OLA. We begin with the blocks
of smaller height and proceed to those with increasing height. For Optima, we sample the blocks
using a distribution based on the lexical similarity between participating entities.
As illustrated in Fig. 11, partitioning both the ontologies helped Optima the most and significantly saves on its execution times (p  0.01). For the pairs involving some of the larger ontologies,
it reduced by more than an order of magnitude. Furthermore, Optima gains in precision over all
pairs by 6% with a 1% reduction in recall resulting in a 3% gain in F-measure to 67%. OLA saves
on execution time as well  relatively less than Optima  with a slight improvement in its alignment
quality. On the other hand, Falcon-AO experienced an increase in its total execution time over all
the pairs. Optimas improved performance is attributed to blocks that are now smaller allowing a
more comprehensive coverage of the search space in less time. On the other hand, iterative update
techniques such as Falcon-AO do not show any improvement because the smaller blocks may be a
sign of overpartitioning.
Figure 12 illustrates the impact of subtree-based partitioning in all three algorithms. Falcon-AO
exhibited a significant reduction in execution times (p < 0.01) simultaneously with an improvement
in precision and F-measure over all the pairs by 3%. Similar to the previous optimization, OLAs
execution time reduces significantly as well (p < 0.01) while keeping its output unchanged. On
826

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

12

Falcon-AO with BCD
Falcon-AO with BCD (both the ontologies partitioned)

50

OLA with BCD
OLA with BCD (both the ontologies partitioned)

10
40

Time (sec)

Time (sec)

8
30

20

6

4

10

2

0

0

as

d
e,e

i

t,s

cm

(

c
en

r

fe

on

(C

d)

)

d)

d
gk

,si

f
fO

on

(c

w)

d
gk

(

t
ias

d)

s
da

e)

d
gk

si

,

s

a
ed

)

ed

a
,ek

,
aw

k

(e

(e

1000

kd

e,s

c
en

r

fe

on

(C

kd

ig

ig

t,s

m
(c

(a)

d)

d)

da

t,e

m

(c

t,

m

(c

s)

nc

re

fe

n
Co

(

d)

d)

te

as

s,i

a
ed

kd

sig

w,

a
ek

(

(b)

Optima with BCD
Optima with BCD (both the ontologies partitioned)

Time (sec)

100

10

1

0.1
f)

)

(

aw

,ed

fe

n
re

,ek

(c

f
on

a
ed

d)

te

as

s,i

Of

ce

on

(C

)

as

fO

on

t,c

cm

(

(

d)

d)

te

ias

w,

a
ek

kd

ig

,s

d
te

s

(ia

(c)

Figure 11: Execution times consumed by, (a) Falcon-AO, (b) OLA, and (c) Optima with BCD
that uses blocks obtained by partitioning a single ontology and with BCD that utilizes
partitions of both the ontologies, for 6 of the 21 ontology pairs from conference domain.
Although we ran the algorithms for all the pairs, we selected ontology pairs which exhibited the highest and lowest differences in execution times. Optimas total execution
time over all pairs reduced by 274 seconds. False positive correspondences reduced by
37 at the expense of 3 correct correspondences. OLA cut down 10 seconds of the total
execution time and 2 incorrect correspondences.

the other hand, this partitioning technique reduces the efficiency of Optima with a small reduction
in alignment quality as well. Falcon-AOs GMO employs an approach that relies on inbound and
outbound neighbors, which is benefited by using blocks whose participating entities form subtrees.
As structure-based matching in Optima is limited to looking at the correspondences between the
immediate children, including larger subtrees in blocks may not be of benefit to Optima.
Finally, in Fig. 13 we explore the impact of randomly partitioning the variables into blocks on
all three alignment algorithms. Both Falcon-AO and OLA showed significant increases in execution time (p < 0.01) on the conference pairs. While Falcon-AOs precision improved by less than
1%, its recall dropped by 2% with an overall reduction in F-measure of 1%. OLA exhibited a minor increase in precision of 0.2% while the recall remained unchanged resulting in an increase of
F-measure by 0.2%. Optima demonstrated mixed results as shown in Fig. 13(c) with the execution
827

fiT HAYASIVAM & D OSHI

35

12

Falcon-AO with BCD
Falcon-AO with BCD (subtree based partitioning)

30

OLA with BCD
OLA with BCD (subtree based partitioning)

10

25

Time (sec)

Time (sec)

8
20
15

6

4
10
2

5
0

0
f)

f)

fO

fO

m

t

(c

as
e,i

ce

n

re

fe

on

(C

nc

re

fe

on

(C

d)

)

aw

te

n
,co

n
,co

da

(e

d)

te

k
s,e

as
s,i

da

F)

d)

te

s
,ia

(e

f,e

t,c

m

k

(e

(c

d)

w)

ka

fO

on

aw

O
nf

f

o

(c

on

(c

(a)

)

aw

kd

,ek

ig

,s
Of

s
da

(e

(e

)

d)

ed

st

,ia

s
da

kd

ig

,s
as

d

(e

(b)
1000

Optima with BCD
Optima with BCD (subtree based partitioning)

Time (sec)

100

10

1

m

(c

f)

d)

fO

te

as

t,i

n

re

fe

on

(C

on

n

re

fe

on

(C

,i
ce

d)

te

as

,c
ce

d

(e

d)

d)

te

as

,i
as

kd

d

(e

,
aw

d)

te

ias

ig

,s
as

k

(e

(c)

Figure 12: Execution times consumed by, (a) Falcon-AO, (b) OLA, and (c) Optima, with BCD
that uses the default partitioning approach and with BCD that uses subtree-based partitioning, for 6 of the 21 ontology pairs from conference domain. We ran the algorithms
for all the pairs of which we selected ontology pairs that exhibited the highest and lowest
differences in execution times. The total execution time of Falcon-AO for the complete
conference track reduces by 8 sec along with a reduction of 71 false positives. OLA
saves 1.5 sec in total execution time while keeping the output alignments unchanged.
However, Optima consumes 192 seconds more.

time increasing for some pairs while reducing for others. On the whole, we did not observe a statistically significant difference in execution times. Furthermore, BCD due to random partitioning did
not improve beyond the seed alignment for many of the pairs, with an overall decrease in F-measure
of 1% across all pairs.
In summary, a side-by-side comparison of the various block ordering and partitioning techniques
discussed previously is presented in Fig. 14 for all three alignment algorithms on a single ontology
pair, (edas, iasted). We do not include the random partitioning as its alignment performance in terms
of recall and precision was poor on many of the ontology pairs making it illsuited as a candidate.
Differences in run time performance of the algorithms on (edas, iasted) is representative of their
828

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

40

30
Falcon-AO with BCD
Falcon-AO with BCD (randomly partitioned)

35

OLA with BCD
OLA with BCD (randomly partitioned)
25
20

Time (sec)

Time (sec)

30
25
20
15

15
10

10
5

5
0
nc

re

fe

on

(C

0

)

Of

nf

o
e,c

)

as

d
e,e

re

fe

on

(C

)

w)

nc

a
,ek

da

g
,si

s

da

(e

(e

n
,co

t

m

(c

w

ka

(e

(a)

g
,si

d)

w)

fO

kd

kd

ias

s,

s

da

(e

d
te

f)

d)

d)

t

m

(c

kd

ig

a
,ek

nf

,s
Of

o

(c

d

(e

d)

d)

te

as

,i
as

kd

d

(e

,
aw

d)

te

ias

ig

,s
as

k

(e

(b)

10

Time (sec)

Optima with BCD
Optima with BCD (randomly partitioned)

1

0.1

f)

en

r
fe

on

(C

da

,e
ce

en

r
fe

(C

on

d)

s)

fO

on

,c
ce

kd

ig

,s
ce

en

r
fe

on

(C

on

(c

(c)

)

ed

st

,ia

f
fO

(e

)

ed

st

,ia

s
da

)

ed

st

,ia

w
ka

(e

Figure 13: Execution times consumed by, (a) Falcon-AO, (b) OLA, and (c) Optima, with BCD
that uses the default partitioning approach and with BCD that uses random partitioning.
We show 6 of the 21 ontology pairs from conference domain. We ran the algorithms for
all the pairs of which we selected ontology pairs that exhibited the highest and lowest
differences in execution times. The total execution time of Falcon-AO for the complete
conference track increases by 19.5 secs due to the random partitioning. OLA takes an
additional 28 secs in total execution time while Optima saves 8.5 seconds over all the
pairs at the expense of alignment quality.

performances on the larger data set in general. In particular, Falcon-AOs run time reduces on
using subtree-based partitioning to obtain the blocks. OLAs run time reduces the most when both
ontologies in the pair are partitioned using entity height, while Optima benefits from ordering
blocks based on a preliminary measure of the similarity of the participating entities and forming
blocks by partitioning both ontologies.

6. Aligning Large Biomedical Ontologies
Ontologies are becoming increasingly critical in the life sciences (Bodenreider & Stevens, 2006;
Lambrix, Tan, Jakoniene, & Stromback, 2007) with multiple repositories such as Bio2RDF (Belleau et al., 2008), OBO Foundry (Smith et al., 2007) and NCBOs BioPortal (Musen et al., 2012)
829

fiT HAYASIVAM & D OSHI

Time (sec)

100

default
ordered from roots to leaves
ordered by similarity distribution
both the ontologies partitioned
subtree based partitioning

10

1

Falcon-AO with BCD

OLA with BCD

Optima with BCD

Figure 14: A side-by-side comparison of the performances of the three iterative algorithms using
various block ordering and formation techniques. A single moderately large ontology
pair, (edas, iasted), is aligned. The default represents the iterative alignment algorithm with BCD where the blocks are ordered based on the height of the participating
entities from the leaves to the root and a single ontology is partitioned to form the blocks.
Differences in run times are indicative of the performance in general.

publishing a growing number of biomedical ontologies from different domains such as anatomy
and molecular biology. For example, BioPortal hosts more than 370 ontologies whose domains fall
within the life sciences. These ontologies are primarily being used to annotate biomedical data and
literature in order to facilitate improved information exchange. With the growth in ontology usage,
reconciliation between those that overlap in scope gains importance.
Evaluation of general ontology alignment algorithms has benefited immensely from the standardsetting benchmark  OAEI (Shvaiko et al., 2012). In addition to multiple tracks with real-world test
cases, the competition emphasizes on benchmark comparison tracks that use test pairs that are modifications of a single ontology pair in order to systematically identify the strengths and weaknesses
of the alignment algorithms. One of the tracks on real-world ontology pairs involves aligning the
ontology on the adult mouse anatomy with the human anatomy portion of NCI thesaurus (Golbeck
et al., 2003), while another seeks to align the foundational model of anatomy (FMA), SNOMED CT
and the national cancer institute thesaurus (NCI). However, aligning biomedical ontologies poses
its own unique challenges. In particular,
1. Entity names are often identification numbers instead of descriptive names. Hence, the alignment algorithm must rely more on the labels and descriptions associated with the entities,
which are expressed differently using different formats.
2. Although annotations using entities of some ontologies such as the gene ontology (Ashburner
et al., 2000) are growing rapidly, for other ontologies they continue to remain sparse. Consequently, we may not overly rely on the entity instances while aligning biomedical ontologies.
3. Finally, biomedical ontologies tend to be large with many including over a thousand entities.
This motivates the alignment approaches to depend less on brute-force steps, and compels
assigning high importance to issues related to efficiency and scalability.
Given these specific challenges, we combed through more than 370 ontologies hosted at NCBO
(Musen et al., 2012) and OBO Foundry (Smith et al., 2007), and isolated a community benchmark of
830

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

50 different biomedical ontology pairs. Thirty-two ontologies with sizes ranging from a few hundred
to tens of thousands of entities constitute the pairs. We provide the list of ontologies participating
in the benchmark and the ontology pairs in Appendix B. This new benchmark guides comparative
evaluation of alignment algorithms to the context of a key application domain of biomedicine.
Our primary criteria for including a pair in the benchmark was an expectation of a sufficient
amount of correspondences between the ontologies in the pair, as determined from NCBOs BioPortal. In particular, we calculated the ratio of the correspondences posted in BioPortal for each
ontology pair to the largest number of possible correspondences that could exist. We selected the
50 pairs with the largest such ratio. Existing correspondences will serve in the reference alignment.
These include maps from the UMLS Metathesaurus and those that are crowd sourced. Nevertheless, our analysis reveals that the existing correspondences constitute just a small fraction of the
total alignment that is possible between two ontologies.
We sought to align the pairs in our new biomedical ontology alignment testbed using the BCDenhanced representative algorithms. The obtained alignments are evaluated using the existing correspondences previously present in BioPortal; the reference alignments between the pairs are likely
incomplete. A secondary objective is to discover new correspondences between the ontologies and
submit them to NCBOs BioPortal for curation.
Informed by the experimentation described in Section 5, blocks for the BCD in Falcon-AO were
formed using subtree-based partitioning of one ontology and ordered as they were created. Blocks
in OLA were formed similarly though both ontologies were partitioned while blocks in Optima
were formed by partitioning both ontologies on the basis of the height of the entities and ordered
from leaves to root. The execution times and F-measure for all the pairs successfully aligned within
an arbitrary window of 5 hours per pair by the BCD-enhanced algorithms are shown in Figs. 15
and 16. We point out that BCD speeds up the algorithms but does not explicitly promote scalability.
In other words, while it reduces the time to convergence it does not provide a way to manage the
memory in order to align large ontologies.
OLA with BCD failed to align a single pair within our time window. Both Falcon-AO enhanced
with BCD and without aligned 47 pairs within the time window. Falcon-AO was unable to parse
one or both the ontologies in the remaining 3 pairs due to which no results are available for these.
Falcon-AO with BCD aligned the pairs taking 3.7 hours less time in total than the original which
consumed about 7.5 hours for all the pairs. We show the time for each pair in Fig. 15(a). A closer
look reveals that Falcon-AO with BCD exhibited time greater than the default on 9 of the 47 pairs.
Time on these few pairs did not exceed by more than 16 seconds and is due to performing the
subtree-based partitioning of the variables for forming the blocks in BCD. The corresponding Fmeasure did not change significantly due to the use of BCD over all the pairs with the F-measure
over all the pairs being 54.7%.
Optima enhanced with BCD aligned 42 pairs each within the time window compared to 30 pairs
without BCD. Optima was unable to parse one or both ontologies in the remaining 8 pairs due to
which no results are available for these. Focusing on the 30 pairs that were aligned by both within
the time window (Fig. 16), Optima with BCD aligned these pairs in 2.3 hours taking 11.4 hours
less time compared to the original algorithm. Simultaneously, it found an additional 269 correct
correspondences across all the pairs with an increase in F-measure of about 2%.
LogMap, a fast non-iterative algorithm that targets biomedical ontologies returned alignments
for all 50 pairs in 20 minutes of total time. It produced a precision and recall of 23.5% and 39.5%
(F-measure = 29.5%), respectively over all the pairs. These are significantly less than those of
831

fiT HAYASIVAM & D OSHI

10000

Falcon-AO
Falcon-AO with BCD

Time (sec)

1000

100

10

1

)
N)
)
N)
N) )
L) )
N) )
)
O
A) GA
N) )
)
)
) AA A)
)
T) A) A) O) RO MA A)
I) DA EL DA
)
)
)
BI DA P )
)
)
DA ER
)
A) )
)
RO
)
A) )
)
RO DA
G
A)
RO O
)
DA O) ,OB EH IFC ,EH V) HD FA AO AO BE V) HD FA AO BE EH O) HO HD HD B-B HD GM AR BE N, HD ,MA ,EV ,EH ,UB E) V) BE ,BT ,EV ,PO ,PO VM HE ,EH O-C DS EV HD
EH ,TA SP S,
,N O O,E O,E O,Z O,X O,T O,U ,E ,E ,Z
,T
,U O, ,BT ,V A,E A,E O,F O,E O,T O,C O,U ERO ,E G
G
G
G ,PA ,E ,U BT BT CV CV O,E D,C RO ,GR ,PS A, A,E
,
O
P
O
O
O
O
O
T
O
O
O
O
O
D
D
O
O
O
F
O
F
S
A
A
A
A
A
A
IL
IL
A
A
A
A
A
B
E
T
T
A
A
A
A
A
A
A
A
O
H
H
H
H
O
O
O
H
FA FA B
A
A
B
B
B
B
(Z (Z (F (T (S (B (A (A (A (A (A (A (X (X (X (X (X (P (P (T (B (B (H (H (H (H (H (U (A (V (V (V (V (P (B (B (F (F (F (F (M (M (C (P (P (E (M

Ontology pair

(a)
1

Falcon-AO
Falcon-AO with BCD

F-measure

0.8

0.6

0.4

0.2

0

)
N)
) )
)
N)
L) )
N) )
)
O
A) GA
N) )
)
)
) AA A) T) A) A) O) RON MA ) )
)
I) DA EL DA
)
)
BI DA P )
)
)
DA ER
A) )
)
RO
)
A) )
)
RO DA
G
A
A)
RO O )
)
DA O) ,OB EH IFC EH V) HD FA AO AO BE V) HD FA AO BE EH O) HO HD HD B-B HD GM AR BE N, HD MA EV EH UB E) V) BE BT EV ,PO ,PO VM HE EH O-C DS EV HD
EH ,TA SP S, ,N O, ,E ,E ,Z ,X ,T ,U ,E ,E ,Z ,T ,U O, ,BT ,V A,E A,E ,F ,E ,T ,C ,U RO ,E G, G, G, G, ,PA ,E ,U BT, BT, CV CV ,E D,C O, ,GR ,PS A, ,E
,
A FA B- AD AO SP AO AO AO AO AO AO AO AO AO AO AO AT O AO IL IL AO AO AO AO AO BE EO HO HO HO HO O TO TO B- B- B- B- FO O AR O O HD FA
F
(Z (Z (F (T (S (B (A (A (A (A (A (A (X (X (X (X (X (P (P (T (B (B (H (H (H (H (H (U (A (V (V (V (V (P (B (B (F (F (F (F (M (M (C (P (P (E (M

Ontology pair

(b)

Figure 15: (a) Time consumed, and (b) F-measure attained by the original Falcon-AO and that
with optimized BCD for 47 pairs of our large biomedical ontology testbed, respectively.
Note that the time axis is in log scale. Ontology names are NCBO abbreviations. The
alignment was performed on a Red Hat machine with Intel Xeon Core 2, processor speed
of about 3 GHz with 8GB of memory.

Falcon-AO, which exhibited a precision and recall of 80.9% and 41.3% respectively, for the pairs
it aligned. Optima with BCD exhibited a precision of 76.1% and a recall of 35.8% with an overall
F-measure of 48.7%. While the recall is less than LogMap, the F-measure is significantly better
due to the improved precision.
Finally, we submitted 15 new correspondences between entities in the pairs of the testbed to
NCBO for curation and publication. These are nontrivial correspondences identified by both algorithms, not present in the reference alignments and appropriately validated by us.
832

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

100000

Optima
Optima with BCD

Time (sec)

10000

1000

100

10

N)
N)
N) A)
L)
N) )
N) )
)
)
)
A)
)
)
)
I)
)
A)
RO
)
I)
EL DA
)
)
)
)
A) )
)
RO
A) )
)
RO DA
G DA DA -BT DA) MA RO ERO ,M DA) A) V)
RO TO V)
DA
EB
O PO
HD UBE )
DA O) ,OB EH IFC ,EH V)
N H
H
A
B
HD FA AO AO BE V)
H
VM H
HD FA AO BE EH O)
E
B
G
BE ,B
HO H
V)
,P
,
,E
,M
,E
,E
,
O
EH ,TA SP
S, O,N PO O,E O,E O,Z O,X O,T O,U O,E O,E O,Z O,T O,U TO, ,BT O,V A,E A,E O,F O,E O,T O,C O,U ER O,E OG OG OG OG ,PA O,E O,U -BT -BT -CV -CV O,E D,C
,
D
FA ZFA FB TA SA BS AA AA AA AA AA AA XA XA XA XA XA PA PO TA BIL BIL HA HA HA HA HA UB AE VH VH VH VH PO BT BT FB FB FB FB MF MO
(Z
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(

Ontology pair

(a)
Optima
Optima with BCD

1

F-measure

0.8

0.6

0.4

0.2

0

N)
) )
)
N)
)
O
L) )
N) )
N) )
)
)
) AA A) T) A) A) O) RON MA ) )
)
I) DA EL DA
)
)
BI
)
)
DA ER
A) )
RO
)
)
A) )
RO DA
A
)
G
RO O )
DA O) ,OB EH IFC ,EH V) HD FA AO AO BE V) HD FA AO BE EH O) HO HD HD B-B HD GM AR BE N, HD ,MA ,EV ,EH ,UB E) V) BE ,BT ,EV ,PO ,PO VM HE
H
O
A
,
,
E
E
E
E
Z
X
T
U
F
E
T
C
U
N
E
E
Z
T
U
T
E
A
V
,E A,T -SP DS O, PO O, O, O, O, O, O, O, O, O, O, O, TO ,B O, A, A, O, O, O, O, O, ER O, OG OG OG OG ,P O,E O,U -BT -BT -CV -CV O,E D,C
A
L
L
F
O
S
A A A A A A A A A A A A O A
I
I
A A A A A B E H H H H O T
T B B B B
A
F
F
B A
(Z (Z (F (T (S (B (A (A (A (A (A (A (X (X (X (X (X (P (P (T (B (B (H (H (H (H (H (U (A (V (V (V (V (P (B (B (F (F (F (F (M (M

Ontology pair

(b)

Figure 16: (a) Time consumed, and (b) F-measure attained by the original Optima and that with
optimized BCD, for 42 pairs of our biomedical ontology testbed, respectively. Note that
the time axis is in log scale.

7. Discussion
Performances of the iterative update and search techniques are impacted differently by various ways
of formulating the blocks and the order of processing them. Importantly, the quality of the alignment
may be adversely impacted. Nevertheless, the approach of grouping alignment variables into blocks
based on the height of the participating entities in the ontologies is motivated by a recognized
heuristic and leads to competitive performance with no observed negative impact on the precision
and recall of the alignments. However, different ontology pairs may lead to a differing number of
blocks of various sizes: in particular, tall ontologies that exhibit a deep class hierarchy result in
more blocks than short ontologies.
833

fiT HAYASIVAM & D OSHI

Given the BCD-based enhancement and optimization, how well do these algorithms compare
in terms of execution time and alignment quality with the state of the art? In order to answer this
question, we compare with the performances of 18 algorithms that participated in the conference
track of OAEI 2012 (Shvaiko et al., 2012). Among these, an iterative alignment algorithm, YAM++,
produced the best F-measure for the 21 pairs followed by LogMap  which does not utilize optimization  CODI, and Optima+, which is Optima augmented with BCD. These latter approaches
all produced F-measures that were tied or within 2% of each other. Optima+ ranked second after
YAM++ when the alignment is evaluated using F2 measure due to its comparatively high recall.
OAEI reports run time on a larger task of aligning 120 conference ontology pairs. On this task,
while YAM++ consumed more than 5 hours for all the pairs, LogMap took slightly less than 4
minutes and Optima+ consumed 22 minutes. Because Falcon-AO and OLA did not participate in
OAEI 2012, we ran them separately on the 120 pairs on our machines, whose configurations are
comparable to that utilized by OAEI. Falcon-AO and OLA enhanced with BCD consumed 11 and
5 minutes respectively although their alignment quality is lower than that of Optima+. This would
place all three representative algorithms in the top two-thirds among the 18 that participated in the
conference track of OAEI in terms of run time with OLA in the top half, and Optima+ and OLA in
group 1 with respect to alignment quality. 6 While 7 competing algorithms completed the evaluation
faster, 5 of these exhibit alignment quality that is substantially worse than that of the representative
algorithms. In the absence of BCD, the representative algorithms would have ranked among the
bottom third or exceeded the 5 hour cut off. Performance on the anatomy pair due to BCD would
place both Falcon-AO and Optima+ in the top half of the 14 algorithms that participated in terms
of run time and F-measure. Previously, Optima without BCD ranked in the bottom quarter.
The reductions in convergence time and the observed increases in precision of the alignment
due to BCD is, in part, because of the optimized correspondences found for the previous coordinate block, which influence the selection of the correspondences for the current coordinate block.
Furthermore, as we mentioned previously, limiting the randomly generated correspondences in
MapPSO to the block instead of the whole ontology makes the search more guided. This is representative of the effect that BCD has on iterative search in general. Focusing on a single block
significantly reduces the space of alignments over which iterative techniques must search thereby
arriving at an optimum quicker. However, a greater number of these smaller optimization subproblems must be solved but as our results imply the smaller optimization problem offsets this expense.
Given that on integrating BCD the iterative algorithms converged to different values of the Q
function during iterative search or different match matrices, M , during iterative update, which
often produced better quality alignments, we infer that the original algorithms were converging to
local optima instead of the global optima, and that using BCD has likely resulted in convergence to
(better) local optima as well. While this insight is not new (Euzenat et al., 2004), it is significant
because it further reinforces the presence of local optima in the alignment space of these algorithms.
This may limit the efficacy of iterative alignment techniques.
Falcon-AO and Optima+s comparatively better performance measured using F-measure against
the fast, non-iterative algorithm, LogMap, on the biomedical ontology alignment testbed indicates
that iterative techniques continue to be among the best in the quality of the obtained alignment including on large ontology pairs. This motivates ways of making them efficient, such as BCD, and
more scalable.
6. Note that MapPSO with BCD would have placed in the bottom third.

834

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

8. Conclusion and Future Work
While techniques for scaling automated alignment to large ontologies have been previously proposed, we presented a novel approach based on BCD to speed up the alignment process of an
important class of algorithms. These algorithms are iterative and anytime demonstrating high quality alignments while often consuming more time than non-iterative algorithms. We demonstrated
this technique in the context of four different iterative algorithms and evaluated its impact on both
the total time of execution and the final alignments precision and recall. We reported significant
reductions in the total execution times of the algorithms enhanced using BCD. These reductions
were most noticeable for larger ontology pairs. Often the algorithms converged in a lesser number
of iterations. Simultaneously, utilizing our default scheme of grouping those alignment variables
such that the participating entities from one ontology in a block have the same height and optimizing the blocks in the order of increasing height, we observe an improvement in the precision of the
alignments generated by some of the algorithms with no significant change in the recall.
While it is possible to improve on the run time performance of the default partitioning and
ordering scheme by utilizing other schemes, we note that some of these may negatively impact the
alignment quality. Subsequently, the default scheme is generally recommended for existing and new
iterative alignment techniques that seek to utilize BCD.
The ability to improve quickly allows an iterative alignment algorithm to run until convergence if
possible, in contrast to the common practice of terminating the alignment process after an arbitrary
number of iterations. As predefining a common bound for the number of iterations is difficult,
speeding up the convergence becomes vital. We observe that BCD does not promote scalability to
large ontologies.
Finally, we demonstrated the benefits of BCD toward aligning pairs in a new biomedical ontology testbed. Due to the large number of ontologies in biomedicine, there is a critical need for
ontology alignment in this vast domain. Our future work is to continue to focus on methods that
would allow general and principled alignment approaches such as Falcon-AO and Optima to perform better on this testbed by producing better quality alignment for more pairs in less time, and on
aligning other large biomedical ontologies that are in popular use such as SNOMED-CT and NCI.
Consequently, we believe that our community benchmark could potentially drive future research
toward pragmatic ontology alignment.

9. Acknowledgment
This research is supported in part by grant number R01HL087795 from the National Heart, Lung,
And Blood Institute. The content is solely the responsibility of the authors and does not necessarily
represent the official views of the National Heart, Lung, And Blood Institute or the National Institutes of Health. The authors thank Todd Minning and Rick Tarleton from the Center for Tropical
and Emerging Diseases at the University of Georgia and Amit Sheth at Wright State University for
useful discussions. The authors also thank the anonymous reviewers for feedback that benefited the
article greatly.

835

fiT HAYASIVAM & D OSHI

Appendix A. Representative Iterative Algorithms Enhanced with BCD
We chose four representative iterative alignment algorithms, Falcon-AO, MapPSO, OLA and Optima in order to illustrate how iterative algorithms could be enhanced with BCD. In this section, we
present each alignment algorithm in its original form and enhanced with BCD, to facilitate a direct
comparison and a quick identification of the needed modifications.
FALCON -AO/GMO-BCD (O1 , O2 , )

FALCON -AO/GMO (O1 , O2 , )
Initialize:
1. Iteration counter i  0
2. G1  AdjacencyMatrix (O1 )
3. G2  AdjacencyMatrix (O2 )
4. For each ma  M 0 do
5.
ma  1
6. M  M 0
Iterate:
7. Do
8.
ii+1
9.
M i  G1 M i1 GT2 + GT1 M i1 G2
10.   CosineSim(M i , M )
11. M  M i
12. While   
13. Extract an alignment from M

Initialize:
1. Iteration counter i  0
2. G1  AdjacencyMatrix (O1 )
3. G2  AdjacencyMatrix (O2 )
4. For each ma  M 0 do
5.
ma  1
6. Create a partition of M :
{MS0 , MS1 , . . . , MSC }
7. M  M 0
Iterate:
8. Do
9.
c  i % (C + 1), i  i + 1
10. MSi c  G1,Sc M i1 GT2 + GT1,Sc M i1 G2
11. MSi  MSi1 S  Sc
12. If c = C then
13.
  CosineSim(M i , M )
else
14.
 is a very high value
15. M  M i
16. While   
17. Extract an alignment from M

Figure 17: (a) Iterative update in the structural matcher, GMO, in Falcon-AO. (b) Iterative update
in GMO modified to perform BCD.

In Fig. 17, we show the iterative algorithm of the GMO component of Falcon-AO and its
enhancement due to the use of BCD. AdjacencyMatrix (O1 ) (line 2 in Fig. 17(a)) produces a binary
matrix, G1 , of size |V1 |  |V1 |, where a value of 1 in the ith row and j th column represents an
edge from the vertex indexed by i to the vertex indexed by j in the bipartite graph model of O1 ;
analogously for AdjacencyMatrix (O2 ). The update and distance functions are implemented as
shown in lines 9 and 10, respectively, of the algorithm. In particular, the cosine similarity computes
the cosine of the two matrices from consecutive iterations serialized as vectors. Notice that in each
iteration of Fig. 17(b), just a block of variables, MSi c , are updated using Eq. 10 while holding the
remaining blocks fixed (lines 10 and 11). This yields a partially updated but complete alignment
matrix in reduced time, which is utilized in the next iteration.
MapPSOs iterative search algorithm that performs particle swarm optimization and its modification due to BCD are shown in Fig. 18. The algorithm takes as input the number of particles, K,
836

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

M AP PSO-BCD (O1 , O2 , K, )
Initialize:
1. Iteration counter i  0
2. Generate seed map between
O1 and O2
3. Populate binary matrix, M 0 , with
seed correspondences
4. Generate K particles using the
0
seed M 0 : P = {M10 , M20 , . . . , MK
}
5. Create a partition of M :
{MS0 , MS1 , . . . , MSC }
6. Search M0  arg max Q(Mk0 )

M AP PSO (O1 , O2 , K, )
Initialize:
1. Iteration counter i  0
2. Generate seed map between O1 and O2
3. Populate binary matrix, M 0 , with
seed correspondences
4. Generate K particles using the
0
seed M 0 : P = {M10 , M20 , . . . , MK
}
0
0
5. Search M  arg max Q(Mk )

Mk0 P

Mk0 P

Iterate:
7. Do
8.
c  i % (C + 1), i  i + 1
9.
For k  1, 2, . . . , K do
i
i
, Mi1 )
 U pdateBlock(Mk,S
10.
Mk,S
c
c
i1
i
11.
Mk,
 Mk,S S  Sc
S
12. Search Mi  arg max QS (Mki )

Iterate:
6. Do
7.
ii+1
8.
For k  1, 2, . . . , K do
9.
Mki  U pdateP article(Mki , Mi1 )
10. Search Mi  arg max Q(Mki )
Mki P

Mki P

11. While |Q(Mi )  Q(Mi1 )|  
12. Extract an alignment from Mi

If c = C then
changed  |Q(Mi )  Q(Mi1 )|  ?
else
15.
changed  true
16. While changed
17. Extract an alignment from Mi

13.
14.

(a)

(b)

Figure 18: (a) Iterative search in MapPSO. Objective function, Q, is as given in Eq. 4. (b)
MapPSOs particle swarm based iterative algorithm enhanced with BCD.

and the threshold, , in addition to the two ontologies to be aligned. It iteratively searches for an
alignment until it is unable to find one that improves on the previous best alignment by more than
or equal to . The objective function, Q, is modified to QS in Fig. 18(b), such that it is calculated
for the coordinate block of interest. A coordinate block in each particle, k, is updated while keeping
the remaining blocks unchanged (lines 10 and 11), followed by searching for the best particle based
on a measure of the alignment in the block (line 12). Both these steps may be performed in reduced
time. Additionally, the randomly generated mappings in MapPSO are limited to the block instead
of the whole ontology, due to which the search becomes more guided.
OLAs iterative algorithm is shown in Fig. 19 (a), and its enhancement due to the use of BCD in
Fig. 19(b). The distance function of line 11 measures the similarity between the updated alignment
matrix with that from the previous iteration. The iterations terminate when the distance falls below
the parameter, . Observe that we cycle through the blocks in the BCD enhanced algorithm in
Fig. 19 (b) and only the coordinates belonging to the current block, MSi c , are updated in lines 8-11.
837

fiT HAYASIVAM & D OSHI

OLA-BCD (O1 , O2 , )

OLA (O1 , O2 , )
Initialize:
1. Iteration counter i  0
2. Fill real-valued matrix, M 0 , with lexical similarity
3. M  M 0
Iterate:
4. Do
5. i  i + 1
6. for each ma  M i
7. if the types of x
a and y are the same then
P
a
wF
SetSim(F(xa ), F(y ))
8.
ma 
F N (xa ,y )

9.
10.
11.
12.
13.
14.

else
ma  0
  Dist(M i , M )
M  M i
While   
Extract an alignment from M

Initialize:
1. Iteration counter i  0
2. Populate the real-valued matrix M 0
with lexical similarity values
3. Create a partition of M :
{MS0 , MS1 , . . . , MSC }
4. M  M 0
Iterate:
5. Do
6. c  i % (C + 1), i  i + 1
7. for each ma  MSi c
8. if the types of xa and y are the same
then
P
a
wF
SetSim(F(a), F())
9.
ma 
F N (a,)

10. else
11. ma  0
12. MSi = MSi1 S  Sc
13. If c = C then
14.
  Dist(M i , M )
else
15.
 is a high value
16. M  M i
17. While   
18. Extract an alignment from M

(a)

(b)

Figure 19: (a) OLA iteratively updates the alignment matrix using a combination of neighboring
similarity values. (b) OLAs BCD-integrated iterative ontology alignment algorithm.

Finally, in Fig. 20, we outline the iterative search undertaken by Optima and its modification
due to BCD. Optimas expectation-maximization based iterative search uses binary matrix, M i , to
represent an alignment. Objective function, Q, is defined in Eq. 6. The search for an improved
alignment in line 8 is implemented using the two steps of expectation and maximization. Iterations
terminate when no sample M i  M, which improves the objective function, Q further, is available.
The search is modified in Fig. 20 (b) to explore a reduced search space, MSc , as we cycle through
the blocks. Both the objective function, QS , and the prior operate on a single coordinate block.

Appendix B. Biomedical Ontology Alignment Benchmark
Biomedical ontologies bring unique challenges to the ontology alignment problem. Moreover, there
is an explicit interest for ontologies and ontology alignment in the domain of biomedicine. Consequently, we present a new biomedical ontology alignment testbed, which provides an important
application context to the alignment research community. Due to the large sizes of biomedical
838

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

O PTIMA -BCD (O1 , O2 )
Initialize:
1. Iteration counter i  0
2. For all   {1, 2, . . . , |V2 |} do
3.
0  |V12 |
4. Generate seed map between
O1 and O2
5. Populate binary matrix, M0 ,
with seed correspondences
6. Create a partition of M :
{MS0 , MS1 , . . . , MSC }

O PTIMA (O1 , O2 )
Initialize:
1. Iteration counter i  0
2. For all   {1, 2, . . . , |V2 |} do
3.
0  |V12 |
4. Generate seed map between
O1 and O2
5. Populate binary matrix, M0 ,
with seed correspondences

Iterate:
7. Do
8.
c  i % (C + 1), i  i + 1
9.
Search MSi c ,  arg max QS (MSi c |Mi1 )

Iterate:
6. Do
7.
ii+1
8.
Search Mi  arg max Q(M |Mi1 )
M
P|V1M
|
P r(y |xa , Mi1 )
9.
i  |V11 | a=1
10. While Mi 6= Mi1
11. Extract an alignment from Mi

10.
11.
12.
13.

MSc MSc
i1
i
MS,  MS, S  Sc
P|V1,c |
1
i1
i
)
,c
 |V1,c
a=1 P r(y |xa , M
|

If c = C then
changed  Mi 6= Mi1 ?
else
14.
changed  true
15. While changed
16. Extract an alignment from Mi

(a)

(b)

Figure 20: (a) Optimas expectation-maximization based iterative search algorithm.
(b)
Expectation-maximization based iterative ontology alignment of Optima with BCD.

ontologies, the testbed could serve as a comprehensive large ontology benchmark. Existing correspondences in NCBO may serve as the reference alignments for the pairs, although our analysis
reveals that these maps represent just a small fraction of the total alignment that is possible between
two ontologies. Consequently, new correspondences that are discovered during benchmarking may
be submitted to NCBO for curation and publication.
In order to create the testbed, we combed through more than 370 ontologies hosted at NCBO
and OBO Foundry, and isolated a benchmark of 50 different biomedical ontology pairs. Thirty-two
ontologies with sizes ranging from a few hundred to tens of thousands of entities constitute the
pairs, and are listed in Table 2. We provide a snapshot of the full benchmark in Table 3. The testbed
with reference alignments is available for download at http://tinyurl.com/n4t2ns3. Our
primary criteria for including a pair in the benchmark was the presence of a sufficient amount of
correspondences between the ontologies in the pair, as determined from NCBOs BioPortal. We
briefly describe the steps in creating the testbed:
1. We selected ontologies, which exist in either OWL or RDF models.
839

fiT HAYASIVAM & D OSHI

2. We paired the ontologies and ordered the pairs by the percentage of available correspondences. This is the ratio of correspondences that exist in BioPortal for the pair of ontologies
under consideration divided by the product of the number of entities in both the ontologies.
3. Top 100 ontology pairs based on the above ratio are selected, followed by ordering the pairs
based on their joint sizes.
4. We created 5 bins of equal sizes and randomly sampled each bin with a uniform distribution
to obtain the final 50 pairs.
Named
Data
Classes
Properties
114
0
Bilateria anatomy (BILA)
50
0
Common Anatomy Reference Ontology (CARO)
282
2
Plant Growth and Development Stage (PO PSDA)
821
0
FlyBase Controlled Vocabulary (FBcv)
129
0
Spatial Ontology (BSPO)
1603
0
Amphibian gross anatomy (AAO)
238
0
Anatomical Entity Ontology (AEO)
1270
7
Cereal plant gross anatomy (GR CPGA)
1,270
6
Plant Anatomy (PO PAE)
821
0
Subcellular Anatomy Ontology (SAO)
1,041
0
Xenopus anatomy and development (XAO)
1,184
0
vertebrate Homologous Organ Groups (sHOG)
1,930
4
Hymenoptera Anatomy Ontology (HAO)
3,039
0
Teleost Anatomy Ontology (TAO)
628
0
Tick gross anatomy (TADS)
2,788
5
Zebrafish anatomy and development (ZFA)
4,358
0
Medaka fish anatomy and development (MFO)
5,139
4
BRENDA tissue / enzyme source (BTO)
2274
0
Expressed Sequence Annotation for Humans (eVOC)
7,797
0
Drosophila gross anatomy (FBbt)
2,281
24
Phenotypic quality (PATO)
7,294
112
Uber anatomy ontology (UBERON)
6,599
0
Fly taxonomy (FBsp)
1,338
4
Protein modification (MOD)
2,314
0
Human developmental anatomy (EHDAA)
8,340
0
Human developmental anatomy timed version (EHDA)
1,585
7
Plant Ontology (PO)
2,703
73
NIF Cell (NIF Cell)
2,982
1
Mouse adult gross anatomy (MA)
1,864
3
Mosquito gross anatomy (TGMA)
3,537
102
Ontology for Biomedical Investigations (OBI)
31,470
9
Chemical entities of biological interest (CHEBI)
Table 2: Selected ontologies from NCBO in the biomedical ontology
alignment testbed and the number of named classes and properties in
each. Notice that this data set includes large ontologies. NCBO abbreviations for these ontologies are also provided.
Ontology

840

Object
Properties
9
9
0
10
9
9
6
0
0
85
10
7
4
9
0
0
6
9
7
10
0
0
0
0
7
7
0
5
6
0
6
0

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

Biomedical ontology alignment testbed
Ontology O1
Ontology O2
|V1 |  |V2 |
Common Anatomy Reference Ontology
Human developmental anatomy (EHDAA)
115,700
(CARO)
Bilateria anatomy (BILA)
Human developmental anatomy (EHDAA)
263,796
Bilateria anatomy (BILA)
Human developmental anatomy (EHDAA)
263,796
Spatial Ontology (BSPO)
Human developmental anatomy (EHDAA)
298,506
Plant Growth and Development Stage
Plant Ontology (PO)
446,970
(PO PSDA)
Anatomical Entity Ontology (AEO)
Human developmental anatomy (EHDAA)
550,732
FlyBase Controlled Vocabulary (FBcv)
Cereal plant gross anatomy (GR CPGA)
1,042,670
FlyBase Controlled Vocabulary (FBcv)
Plant Ontology (PO)
1,301,285
Tick gross anatomy (TADS)
Human developmental anatomy (EHDAA)
1,453,192
Amphibian gross anatomy (AAO)
Xenopus anatomy and development (XAO)
1,668,723
Cereal plant gross anatomy (GR CPGA)
Plant Ontology (PO)
2,012,950
Plant Anatomy (PO PAE)
Plant Ontology (PO)
2,012,950
Subcellular Anatomy Ontology (SAO)
NIF Cell (NIF Cell)
2,219,163
Expressed Sequence Annotation for Humans
Xenopus anatomy and development (XAO)
2,367,234
(eVOC)
Xenopus anatomy and development (XAO)
Human developmental anatomy (EHDAA)
2,408,874
vertebrate Homologous Organ Groups Expressed Sequence Annotation for Humans
2,692,416
(sHOG)
(eVOC)
vertebrate Homologous Organ Groups
Human developmental anatomy (EHDAA)
2,739,776
(sHOG)
Xenopus anatomy and development (XAO)
Zebrafish anatomy and development (ZFA)
2,902,308
Xenopus anatomy and development (XAO)
Teleost Anatomy Ontology (TAO)
3,163,599
vertebrate Homologous Organ Groups
Mouse adult gross anatomy (MA)
3,530,688
(sHOG)
Hymenoptera Anatomy Ontology (HAO)
Mosquito gross anatomy (TGMA)
3,597,520
vertebrate Homologous Organ Groups
Teleost Anatomy Ontology (TAO)
3,598,176
(sHOG)
Expressed Sequence Annotation for Humans
Amphibian gross anatomy (AAO)
3,645,222
(eVOC)
Amphibian gross anatomy (AAO)
Human developmental anatomy (EHDAA)
3,709,342
Hymenoptera Anatomy Ontology (HAO)
Human developmental anatomy (EHDAA)
4,466,020
Amphibian gross anatomy (AAO)
Zebrafish anatomy and development (ZFA)
4,469,164
Amphibian gross anatomy (AAO)
Teleost Anatomy Ontology (TAO)
4,871,517
Expressed Sequence Annotation for Humans
Human developmental anatomy (EHDAA)
5,262,036
(eVOC)
Phenotypic quality (PATO)
Human developmental anatomy (EHDAA)
5,278,234
Zebrafish anatomy and development (ZFA)
Human developmental anatomy (EHDAA)
6,451,432
Plant Anatomy (PO PAE)
BRENDA tissue / enzyme source (BTO)
6,526,530
Teleost Anatomy Ontology (TAO)
Human developmental anatomy (EHDAA)
7,032,246
Xenopus anatomy and development (XAO)
Uber anatomy ontology (UBERON)
7,593,054
Zebrafish anatomy and development (ZFA)
Teleost Anatomy Ontology (TAO)
8,472,732
Continued on next page

841

fiT HAYASIVAM & D OSHI

Ontology 1
vertebrate Homologous Organ Groups
(sHOG)
Medaka fish anatomy and development
(MFO)
Medaka fish anatomy and development
(MFO)
BRENDA tissue / enzyme source (BTO)
Amphibian gross anatomy (AAO)
BRENDA tissue / enzyme source (BTO)
Hymenoptera Anatomy Ontology (HAO)
Hymenoptera Anatomy Ontology (HAO)
Expressed Sequence Annotation for Humans
(eVOC)

Ontology 2

|V1 |  |V2 |

Uber anatomy ontology (UBERON)

8,636,096

Expressed Sequence Annotation for Humans
(eVOC)

9,910,092

Human developmental anatomy (EHDAA)
Expressed Sequence Annotation for Humans
(eVOC)
Uber anatomy ontology (UBERON)
Human developmental anatomy (EHDAA)
Uber anatomy ontology (UBERON)
Drosophila gross anatomy (FBbt)
Uber anatomy ontology (UBERON)

Expressed Sequence Annotation for Humans
(eVOC)
Zebrafish anatomy and development (ZFA)
Uber anatomy ontology (UBERON)
Uber anatomy ontology (UBERON)
Mouse adult gross anatomy (MA)
Ontology for Biomedical Investigations
Fly taxonomy (FBsp)
(OBI)
BRENDA tissue / enzyme source (BTO)
Uber anatomy ontology (UBERON)
Drosophila gross anatomy (FBbt)
BRENDA tissue / enzyme source (BTO)
Chemical entities of biological interest
Protein modification (MOD)
(CHEBI)
Table 3: The biomedical ontology pairs in our testbed sorted in terms of
|V1 |  |V2 |. This metric is illustrative of the complexity of aligning the
pair.
Drosophila gross anatomy (FBbt)

10,084,412
11,686,086
11,692,282
11,891,646
14,077,420
15,048,210
16,586,556
17,730,378
20,335,672
21,750,708
23,340,663
37,483,866
40,068,783
42,106,860

References
Arimoto, S. (1972). An algorithm for computing the capacity of arbitrary discrete memoryless
channels. IEEE Transactions on Information Theory, 18(1), 1420.
Ashburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., Davis, A. P., Dolinski, K., Dwight, S. S., Eppig, J. T., Harris, M. A., Hill, D. P., Issel-Tarver, L., Kasarskis,
A., Lewis, S., Matese, J. C., Richardson, J. E., Ringwald, M., Rubin, G. M., & Sherlock, G.
(2000). Gene ontology: tool for the unification of biology. the gene ontology consortium..
Nature genetics, 25(1), 2529.
Baader, F., Horrocks, I., & Sattler, U. (2003). Description logics as ontology languages for the
semantic web. In Lecture Notes in Artificial Intelligence, pp. 228248. Springer-Verlag.
Belleau, F., Nolin, M.-A., Tourigny, N., Rigault, P., & Morissette, J. (2008). (bio2rdf): Towards
a mashup to build bioinformatics knowledge systems. Journal of Biomedical Informatics,
41(5), 706716.
842

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

Blahut, R. E. (1972). Computation of channel capacity and rate-distortion functions. IEEE Transactions on Information Theory, 18, 460473.
Bock, J., & Hettenhausen, J. (2010). Discrete particle swarm optimisation for ontology alignment.
Information Sciences, 192, 122.
Bodenreider, O., & Stevens, R. (2006). Bio-ontologies: current trends and future directions. Brief
Bioinform, 7, 256274.
Cruz, I. F., Stroe, C., & Palmonari, M. (2012). Interactive user feedback in ontology matching
using signature vectors. In IEEE 28th International Conference on Data Engineering, pp.
13211324. IEEE Computer Society.
Doan, A., Madhavan, J., Domingos, P., & Halevy, A. (2003). Ontology matching: A machine learning approach. In Handbook on Ontologies in Information Systems, pp. 397416. Springer.
Doshi, P., Kolli, R., & Thomas, C. (2009). Inexact matching of ontology graphs using expectationmaximization. Web Semantics: Science, Services and Agents on the World Wide Web, 7(2),
90106.
Euzenat, J., Loup, D., Touzani, M., & Valtchev, P. (2004). Ontology alignment with OLA. In
In Proceedings of the 3rd EON Workshop, 3rd International Semantic Web Conference, pp.
5968. CEUR-WS.
Euzenat, J., & Valtchev, P. (2004). Similarity-based ontology alignment in OWL-lite. In European
Conference on Artificial Intelligence (ECAI), pp. 333337.
Euzenat, J., & Shvaiko, P. (2007). Ontology Matching. Springer.
Fessler, J. A., & Hero, A. O. (1994). Space-alternating generalized expectation-maximization algorithm. IEEE Transactions on Signal Processing, 42, 26642677.
Fessler, J. A., & Kim, D. (2011). Axial block coordinate descent (abcd) algorithm for X-ray CT
image reconstruction. In Proceedings of Fully 3D Image Reconstruction in Radiology and
Nuclear Medicine, pp. 262265.
Golbeck, J., Fragoso, G., Hartel, F., Hendler, J., Oberthaler, J., & Parsia, B. (2003). The national
cancer institutes thesaurus and ontology. Journal of web semantics, 1(1), 7580.
Hanif, M. S., & Aono, M. (2009). Anchor-flood: results for OAEI 2009. In Proceedings of the
Workshop on Ontology Matching at 8th International Semantic Web Conference, pp. 127
134.
Hayes, J., & Gutierrez, C. (2004). Bipartite graphs as intermediate model for RDF. In Proceedings of the 3rd International Semantic Web Conference (ISWC), Lecture Notes in Computer
Science, pp. 4761. Springer Berlin / Heidelberg.
Hero, A. O., & Fessler, J. A. (1993). Asymptotic convergence properties of (em)-type algorithms.
Tech. rep., Department of EECS, Univ. of Michigan, Ann Arbor, MI.
Hu, W., Jian, N., Qu, Y., & Wang, Y. (2005). GMO: A graph matching for ontologies. In K-Cap
Workshop on Integrating Ontologies, pp. 4350.
Hu, W., Zhao, Y., & Qu, Y. (2006). Partition-based block matching of large class hierarchies. In
Proceedings of the 1st Asian Semantic Web Conference (ASWC), pp. 7283.
843

fiT HAYASIVAM & D OSHI

Hughes, T. C., & Ashpole, B. C. (2004). The semantics of ontology alignment. In Information
Interpretation and Integration Conference (I3CON).
Jean-Mary, Y. R., Shironoshita, E. P., & Kabuka, M. R. (2009). Ontology matching with semantic
verification. Web Semantics: Science, Services and Agents on the World Wide Web, 7(3),
235251.
Jian, N., Hu, W., Cheng, G., & Qu, Y. (2005). Falcon-AO: Aligning ontologies with Falcon. In
K-Cap Workshop on Integrating Ontologies, pp. 8793.
Jimenez-Ruiz, E., & Grau, B. C. (2011). LogMap: Logic-based and scalable ontology matching. In
International Semantic Web Conference, pp. 273288.
Kirsten, T., Gross, A., Hartung, M., & Rahm, E. (2011). GOMMA: a component-based infrastructure for managing and analyzing life science ontologies and their evolution. Journal of
Biomedical Semantics, 2, 6.
Lambrix, P., Tan, H., Jakoniene, V., & Stromback, L. (2007). Biological ontologies In: Semantic
Web: Revolutionizing Knowledge Discovery in the Life Sciences, pp. 8599. Springer.
Li, Y., Li, J., & Tang, J. (2007).
RiMOM: Ontology alignment with strategy selection. In Proceedings of the 6th International and 2nd Asian Semantic Web Conference
(ISWC2007+ASWC2007), pp. 5152.
McGuinness, D., & Harmelen, F. (2004). Owl web ontology language overview. Tech. rep., W3C.
Melnik, S., Garcia-molina, H., & Rahm, E. (2002). Similarity flooding: A versatile graph matching
algorithm. In ICDE: Int. Conference on Data Engineering, pp. 117128.
Musen, M. A., Noy, N. F., Shah, N. H., Whetzel, P. L., Chute, C. G., Storey, M.-A. D., & Smith, B.
(2012). The national center for biomedical ontology. JAMIA, 19(2), 190195.
Nesterov, Y. (2012). Efficiency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2), 341362.
Ngo, D., & Bellahsene, Z. (2012). YAM++ : A multi-strategy based approach for ontology matching
task. In International Conference on Knowledge Engineering and Knowledge Management,
pp. 421425.
Pinter, J. D. (2000). Yair censor and stavros a. zenios, parallel optimization  theory, algorithms,
and applications. Journal of Global Optimization, 16, 107108.
Rahm, E. (2011). Towards large-scale schema and ontology matching. In Bellahsene, Z., Bonifati,
A., & Rahm, E. (Eds.), Schema Matching and Mapping, pp. 327. Springer.
Russell, S. J., & Norvig, P. (2010). Artificial Intelligence - A Modern Approach (3rd edition).
Pearson Education.
Saha, A., & Tewari, A. (2013). On the non-asymptotic convergence of cyclic coordinate descent
methods. SIAM Journal on Optimization, 23(1), 576601.
Seddiqui, M. H., & Aono, M. (2009). An efficient and scalable algorithm for segmented alignment
of ontologies of arbitrary size. Web Semantics: Science, Services and Agents on the World
Wide Web, 7, 344356.
Shvaiko, P., & Euzenat, J. (2013). Ontology matching: State of the art and future challenges. IEEE
Transactions on Knowledge and Data Engineering, 25(1), 158176.
844

fiS PEEDING U P I TERATIVE O NTOLOGY A LIGNMENT USING BCD

Shvaiko, P., Euzenat, J., Heath, T., Quix, C., Mao, M., & Cruz, I. F. (Eds.). (2011). Proceedings
of the 6th International Workshop on Ontology Matching, Vol. 814 of CEUR Workshop Proceedings. CEUR-WS.org.
Shvaiko, P., Euzenat, J., Kementsietsidis, A., Mao, M., Noy, N., & Stuckenschmidt, H. (Eds.).
(2012). Results of the Ontology Alignment Evaluation Initiative (OAEI) 2012, Vol. 946 of
CEUR Workshop Proceedings. CEUR-WS.org.
Shvaiko, P., Euzenat, J., Srinivas, K., Mao, M., & Jimenez-Ruiz, E. (Eds.). (2013). Preliminary
Results of the Ontology Alignment Evaluation Initiative (OAEI) 2013, Vol. 1111 of CEUR
Workshop Proceedings. CEUR-WS.org.
Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters, W., Goldberg, L. J., Eilbeck,
K., Ireland, A., Mungall, C. J., Leontis, N., Rocca-Serra, P., Ruttenberg, A., Sansone, S.-A.,
Scheuermann, R. H., Shah, N., Whetzel, P. L., & Lewis, S. (2007). The OBO foundry: coordinated evolution of ontologies to support biomedical data integration. Nature Biotechnology,
25(11), 12511255.
Stoutenburg, S. K., Kalita, J., Ewing, K., & Hines, L. M. (2010). Scaling alignment of large ontologies. International Journal of Bioinformatics Research and Applications, 6, 384401.
Thayasivam, U., & Doshi, P. (2012a). Improved convergence of iterative ontology alignment using
block-coordinate descent. In Twenty-Sixth Conference on Artificial Intelligence (AAAI), pp.
150156.
Thayasivam, U., & Doshi, P. (2012b). Optima+ results for OAEI 2012. In Workshop on Ontology Matching at 11th International Semantic Web Conference (ISWC). Vol. 946 of CEURWS.org.
Tseng, P. (2001). Convergence of block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109, 475494.
Wang, P., & Xu, B. (2009). Lily: Ontology alignment results for OAEI 2008. In Proceedings of the
Workshop on Ontology Matching at 7th International Semantic Web Conference (ISWC).

845

fiJournal of Artificial Intelligence Research 50 (2014) 321-367

Submitted 1/14; published 6/14

Game-Theoretic Security Patrolling with Dynamic Execution
Uncertainty and a Case Study on a Real Transit System
Francesco M. Delle Fave
Albert Xin Jiang
Zhengyu Yin
Chao Zhang
Milind Tambe

dellefav@usc.edu
jiangx@usc.edu
zhengyuy@usc.edu
zhan661@usc.edu
tambe@usc.edu

University of Southern California,
Los Angeles, CA 90089 USA

Sarit Kraus

sarit@cs.biu.ac.il

Bar Ilan University,
Ramat Gan 52900, Israel

John P. Sullivan

jpsulliv@lasd.org

Los Angeles County Sheriff s Department
Los Angeles, CA 90059

Abstract
Attacker-Defender Stackelberg security games (SSGs) have emerged as an important
research area in multi-agent systems. However, existing SSGs models yield fixed, static,
schedules which fail in dynamic domains where defenders face execution uncertainty, i.e., in
domains where defenders may face unanticipated disruptions of their schedules. A concrete
example is an application involving checking fares on trains, where a defenders schedule is
frequently interrupted by fare evaders, making static schedules useless.
To address this shortcoming, this paper provides four main contributions. First, we
present a novel general Bayesian Stackelberg game model for security resource allocation
in dynamic uncertain domains. In this new model, execution uncertainty is handled by
using a Markov decision process (MDP) for generating defender policies. Second, we study
the problem of computing a Stackelberg equilibrium for this game and exploit problem
structure to reduce it to a polynomial-sized optimization problem. Shifting to evaluation,
our third contribution shows, in simulation, that our MDP-based policies overcome the
failures of previous SSG algorithms. In so doing, we can now build a complete system, that
enables handling of schedule interruptions and, consequently, to conduct some of the first
controlled experiments on SSGs in the field. Hence, as our final contribution, we present
results from a real-world experiment on Metro trains in Los Angeles validating our MDPbased model, and most importantly, concretely measuring the benefits of SSGs for security
resource allocation.

1. Introduction
In recent years, research in algorithmic game theory has started to show a significant interest
in security resource optimization problems. This research has led to decision aids for realworld security agencies which need to deploy patrols and checkpoints to protect targets
from terrorists and criminals (Tambe, 2011). Stackelberg security games (SSGs) have been
advocated as a powerful tool to model these problems (Gatti, 2008; Conitzer, 2012; Basilico,
c
2014
AI Access Foundation. All rights reserved.

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

Gatti, & Amigoni, 2009a; Vorobeychik & Singh, 2012; Vanek, Jakob, Lisy, Bosansky, &
Pechoucek, 2011; Pita, Jain, Western, Portway, Tambe, Ordonez, Kraus, & Paruchuri, 2008;
Tambe, 2011). An SSG is a two-player game between a defender (the security agency) and
an adversary (a terrorist or a criminal). The defender commits to a mixed strategy a
randomized resource allocation specified by a probability distribution over deterministic
schedules which takes into the account the adversarys best response to his observation of
the mixed strategy1 . Several decision-support systems based on SSGs have been successfully
deployed in real world domains for assisting the security of ports, airports, ferries and
transit systems. Examples include ARMOR and GUARDS for airport security (Pita et al.,
2008; Pita, Tambe, Kiekintveld, Cullen, & Steigerwald, 2011), IRIS for allocating security
personnel to international flights of US Carriers (Tsai, Rathi, Kiekintveld, Ordonez, &
Tambe, 2009), PROTECT for randomized patrols for security of ports and passenger ferries
in ports such as New York, Boston and Los Angeles (Shieh, An, Yang, Tambe, Baldwin,
DiRenzo, Maule, & Meyer, 2012; Fang, Jiang, & Tambe, 2013) and TRUSTS for patrolling
Metro trains in Los Angeles (Yin, Jiang, Johnson, Tambe, Kiekintveld, Leyton-Brown,
Sandholm, & Sullivan, 2012).
Some of the domains discussed above involve patrolling a transportation system such
as a train-line, a ferry or a flight system. In such settings, schedules are typically timecritical because they depend on the time table of the vehicles (trains, ferries or flights).
However, interruptions are frequent while patrolling key transportation systems because
the officer might have to respond to an emergency, provide assistance to a passenger or
need to arrest someone. For example, when patrolling trains, whenever an officer is delayed
midway, it might become impossible for the officer to complete his patrol schedule. Hence,
fixed schedules that cannot be updated after an interruption will be hard to follow after an
officer is delayed. Unfortunately, previous work has often provided static or fixed patrolling
schedules that face problems in the presence of unanticipated disruptions.
In general, such execution uncertainty is endemic in transportation domains and it will
affect the defender units ability to carry out their planned schedules in later time steps. One
motivating example, which will be used throughout this work, is the TRUSTS system for
scheduling fare inspections in the Los Angeles metro rail system (LA Metro). TRUSTS (Yin
et al., 2012), currently being evaluated by the Los Angeles sheriffs department (LASD),
provides a game-theoretic solution to scheduling randomized patrols for fare inspections on
trains and at stations. As we will see later in this paper, in real world trials carried out
by the LASD, a significant fraction of the executions of the pre-generated schedules got
interrupted for a variety of reasons such as writing citations, felony arrests, and handling
emergencies. Such interruptions caused the officers to miss the train that they were supposed
to take as part of their patrol schedule. On such occasions, the solution of TRUSTS did
not provide instructions on what to do after the interruption making the schedules useless
to the officers.
Previous work has addressed some aspects of execution uncertainty. In particular, Yin,
Jain, Tambe, and Ordonez (2011), Yin and Tambe (2012) present two different approaches,
one based on robust optimization and another on a Bayesian method, whereby the defender
optimizes her mixed strategy taking into account that some (small) fraction of it will be
1. By convention in security games literature, the defender is referred to as she and the adversary as
he.

322

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

incorrectly executed. Unfortunately, as discussed above, in the domains of interest in this
work, including TRUSTS, a significant fraction of the schedules are interrupted. Most
importantly, in such cases, this previous work does not suggest any alternatives whenever
the defenders on patrol are interruptedthus it fails to optimally use the patrol time. This
clearly indicates that a key challenge still needs to be addressed in SSGs: a new framework
is needed, that can generate patrol schedules that are robust against execution uncertainty
and that can provide contingency plans whenever disruptions occur.
To provide such a framework, this paper presents four main contributions. The first
contribution consists of a general Bayesian Stackelberg game model for security patrolling
with execution uncertainty. In this model, execution uncertainty is handled via a Markov
decision process (MDP). The second contribution is a detailed study of the problem of
computing a Stackelberg equilibrium (SSE) for this game. Computing such SSE in a timely
fashion presents significant computational challenges because the defenders strategy space,
already exponential in most real-world applications (Jain, Kardes, Kiekintveld, Tambe, &
Ordonez, 2010; Conitzer, 2012), only grows in complexity given that it must now address all
of the contingencies during execution. To address this shortcoming, we show that when the
games utility functions have a specific separable structure, the defenders strategy space
can be compactly represented. By using this structure, we can then reduce the problem to
a polynomial-sized optimization problem, which can be solved by existing approaches for
solving Bayesian Stackelberg games without execution uncertainty, e.g., DOBSS (Paruchuri,
Pearce, Marecki, Tambe, Ordonez, & Kraus., 2008b). However, the randomized patrol
schedules that we obtain, are now well-defined MDP-policies, i.e., plans, which take into
account contingencies for unexpected events. As we will show in the remainder of this work,
such policies can always be generated with a polynomially-sized support. In addition, these
policies can be loaded into a smart-phone application carried by patrol units during a shift.
The next two contributions focus on the application of the former approach to generate
patrol schedules for fare inspection on the LA Metro. In more detail, the third contribution
shows in simulation that, by modeling execution uncertainty as an MDP, we are able to
generate policies that overcome the failures of existing SSG algorithms which do not take
such uncertainty into account. In addition, results of numerical experiments show that
execution uncertainty has a significant impact on the defenders expected utility.
A key question raised for deployed applications of SSGs is the evaluation of their performance in the field. Whereas many different evaluation metrics have been offered, it
is difficult to evaluate SSGs approaches and the resource allocation that they generate in
actual domains of airport or port security (Tambe, 2011). Fortunately, the MDP policies
from our new game model, once loaded onto smartphonesan application we discuss later
in this paperenables us to test the use of SSGs in the field against alternatives. This is
a fundamentally new test in the real world to validate not only the new game model, but
more generally, algorithmic game theory in the field. Therefore, the fourth contribution
is a real-world experiment that aims to evaluate the comprehensive game-theoretic system
in the field. Specifically, we ran a 21-day experiment, where we compared schedules generated using our approach against competing schedules comprised of a random scheduler
augmented with officers providing real-time knowledge of the current situation. The results
provided evidence in support of our MDP-based modelthe contingency plans provided by
the MDP were actually used with significant frequency in the real world. More importantly,
323

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

these results showed that game-theoretic schedules led to statistically significant improvements over the competing schedules, despite the fact that the latter were improved with
real-time knowledge. These results constitute the first example of head-to-head comparison
of SSGs with competing approaches in the field. In fact, they constitute some of the first
data obtained about deploying algorithmic game theory in the real-world.
In summary, this paper makes the following contributions:
 We present a novel Bayesian Stackelberg game model that accounts for execution
uncertainty in security patrolling using an MDP.
 We study the problem of computing a Stackelberg equilibrium (SSE) for this game.
Specifically, we derive conditions under which the game can be represented in a compact form, which can be solved in polynomial time. The resulting strategies are,
however, MDP-policies, i.e., plans, which take into account contingencies for unexpected events.
 We present an extensive empirical evaluation whereby we analyze the impact of execution uncertainty on the new game model and on the expected utility for the defender.
 We present a real-world experiment where we compared schedules generated using our
approach against competing schedules comprised of a random scheduler. The results
showed that game-theoretic schedules outperformed the competing schedules in terms
of the number of fare evaders captured. In so doing, they provide evidence about the
benefits of deploying algorithmic game theory in the real-world.
The remainder of this paper is organized as follows: Section 2 presents related work
on SSGs and how they handle uncertainty. Section 3 discusses the motivating problem of
patrolling the LA Metro system and presents the formal model of the problem as a Bayesian
Stackelberg game. Section 4 discusses the solution method. Section 5 discusses the way we
apply the model defined in Section 4 to the LA Metro problem. Section 6 discusses our
evaluation consisting of both simulations and real world experiments and, finally Section 7
concludes and discusses future work.

2. Related Work
Stackelberg security games (SSGs) have gathered significant attention in literature (Basilico
et al., 2009a; Dickerson, Simari, Subrahmanian, & Kraus, 2010; Letchford, MacDermed,
Conitzer, Parr, & Isbell, 2012; Letchford & Conitzer, 2013; Letchford & Vorobeychik, 2013;
Korzhyk, Conitzer, & Parr, 2011a, 2011b). Indeed, as stated earlier, SSGs models and
algorithms have been used to build decision aids including ARMOR (Pita et al., 2008), IRIS
(Tsai et al., 2009), GUARDS (Pita et al., 2011) and PROTECT (Shieh et al., 2012). Most
importantly, two systems, namely TRUSTS (Yin et al., 2012) and RaPtoR (Varakantham,
Lau, & Yuan, 2013), have been used to generate schedules for patrolling public transit
systems such as the LA Metro and the Singapore Metro system. Unfortunately, all these
deployed applications did not take execution uncertainty into account. As a consequence,
they are not useful in settings of interest in this paper, such as ones involving patrolling a
transportation system where disruptions may occur frequently.
324

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

Nonetheless, tackling uncertainty has become one of the principal challenges in SSGs
research. In particular, previous work has focused on different types of uncertainties: uncertainty in adversary response due to bounded rationality (Yang, Kiekintveld, Ordonez,
Tambe, & John, 2011; Nguyen, Yang, Azaria, Kraus, & Tambe, 2013), uncertainty in adversary surveillance (An, Tambe, Ordonez, Shieh, & Kiekintveld, 2011), uncertainty in adversary capability and uncertainty in defender execution of strategies (Yin et al., 2011; Yin &
Tambe, 2012)2 . With respect to bounded rationality, previous approaches have focused on
different models of bounded rationality, such as logit quantal response and subjective utility
quantal response (Yang et al., 2011; Nguyen et al., 2013). However, both these frameworks
do not address execution uncertainty and, as a consequence, do not address the challenge
studied in this work. With respect to adversary surveillance, previous approaches have focused on modeling the fact that in many domains the adversary can only partially observe
the defenders mixed strategy (An, Kempe, Kiekintveld, Shieh, Singh, & Tambe, 2012).
Similarly, with respect to uncertainty in the adversarys surveillance capability and in the
defenders execution of strategy, previous approaches have focused on modeling uncertainty
using a Bayesian game (Yin & Tambe, 2012) and on using robust strategy computation,
including robust optimization, to provide safe quality guarantees for the obtained defenders
strategy (Yin et al., 2011). Unfortunately, as discussed in Section 1, these approaches do
not suggest any alternative whenever the defenders on patrol are interrupted. Thus they do
not address the challenge in our setting, because they would generate schedules that would
become useless anytime a defender is interrupted.
From a game theoretic perspective then, the game model in this paper can be considered
as an extensive-form Stackelberg games with chance nodes (Letchford & Conitzer, 2010),
or as a special case of a stochastic Stackelberg game where the follower can only choose
one action in the initial state and stick to that action in all future states (Letchford et al.,
2012). The general cases of both games were shown to be NP-hard. Vorobeychik and
Singh provided mixed integer linear programs for finding optimal and approximate Markov
stationary strategy in general-sum stochastic Stackelberg games (Vorobeychik & Singh,
2012). However, their approach does not handle multiple adversary types and their MILP
formulation lacks the scalability to a large number of states such as the LA Metro problems.
Another related line of research is on equilibrium refinement for dynamic games, such as
trembling hand perfect equilibrium (Aoyagi, 1996), which considers the possibility that a
strategy can be imperfectly executed. However such research is mainly interested in the
limit as uncertainty goes to zero, while in our real world settings the probability of imperfect
execution really is non-zero.
Other types of SSGs include multi-robot adversarial patrolling games (MAPG). A
MAPG is a special restricted type of SSG which considers the problem of multi-robot
patrol around a closed area with the existence of an adversary attempting to penetrate
into the area (Agmon, Kraus, & Kaminka, 2008a; Agmon, Kaminka, & Kraus, 2011). The
penetration requires time and the defender should identify the attacker during her attempt.
Most literature about uncertainty in MAPGs studied uncertainty related to the type and
2. Execution uncertainty has also been studied in the context of finding Nash-equilibrium in standard
multi-player simultaneous move games (Bowling & Veloso, 2004; Archibald & Shoham, 2011). Despite
addressing a similar topic, however, this literature is out of the scope of our work, which is centered on
modeling execution uncertainty in SSGs.

325

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

the knowledge of the attacker (Agmon, Sadov, Kaminka, & Kraus, 2008b; Basilico, Gatti,
Rossi, Ceppi, & Amigoni, 2009b; Basilico, Gatti, & Villa, 2010). Furthermore, it is assumed
that if an adversary is detected the robots continue to patrol around the area looking for
additional attackers without the need to modify their strategy. Thus, execution uncertainty,
as discussed in this work, is not addressed.
One exception, which is closer to our work, is the work by Sless, Agmon, and Kraus
(2014) that requires the robots to physically inspect penetration attempts for a given time
period. This, as in our work, can have far reaching consequences on the performance of
the patrol algorithm. Specifically, it creates vulnerability points along the patrol path that
can be taken advantage of by a knowledgeable adversary. In particular, Sless et al. (2014)
investigate the problem of coordinated attacks, in which the adversary initiates two attacks
in order to maximize its chances of successful penetration, assuming a robot from the team
will be sent to examine a penetration attempt. They suggest an algorithm for computing the
robots strategy for handling such coordinated attacks, and show that despite its exponential
time complexity, practical run time of the algorithm can be significantly reduced without
harming the optimality of the strategy. Unfortunately, whereas this work assumes that the
contribution of multiple patrol units covering the same edge is additive, thus enabling to
formulate the problem as a linear programming problem, in Sless et al. settings this does
not hold because multiple robot covering the same segment contribute the same as a single
robot.
Finally, since a significant portion of this work deals with deploying game-theoretic
schedules in the field, it is relevant to discuss existing literature that has addressed a similar
challenge. As will be discussed in Section 6, we will deploy game-theoretic schedules to deter
fare evasion in the LA metro system. In so doing, our work is similar to a number of studies
on fare-evasion prevention conducted in the systems of London and Alberta (Clarke, 1993;
Weidner, 1996; Clarke, Contre, & Petrossian, 2010). These studies focused on understanding
the impact of introducing automatic gates, turn-styles and ticket prices, on the fare evasion
rate. In our work, we are interested in a different aspect: understanding how game-theoretic
scheduling will affect the performance of the security resources responsible for patrolling a
transit system every day.
Given this focus on validating game-theoretic scheduling in the real world, our work
shares many ideas with literature on game theory in the field. This line of research has
focused on showing equilibrium concepts in the human and animal activities (Ostling, Wang,
Tao-yi, Chou, & Camerer, 2011; Brown, Camerer, & Lovallo, 2012). Our work shares their
enthusiasm of taking game theory to the field, but fundamentally focuses on algorithmic
deployments and the impact of such algorithms.

3. Problem Statement
This section discusses, first, the Los Angeles Metro domain, the key domain which is used
as a motivation for our work. Second, it presents the Stackelberg game model which we
define to formalize the problem.
326

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

3.1 Motivating Example: LA Metro System
While our model is quite general for modeling time-sensitive patrols in security domains
with execution uncertainty, the study in this paper is substantially motivated by TRUSTS,
an application for scheduling fare inspections in the Los Angeles Metro Rail system (Yin
et al., 2012). The LA Metro Rail system, similar to other proof-of-payment transit systems
worldwide, is a barrier-free transit system where passengers are legally required to purchase tickets before boarding, but are not physically blocked by gates or turnstiles. Instead,
security personnel are dynamically deployed throughout the transit system, randomly inspecting passenger tickets. With approximately 300,000 daily riders, the revenue loss due
to fare evasion can be significantthis cost has been estimated at $5.6 million (Booz Allen
Report, 2007). The Los Angeles Sheriffs Department (LASD) deploys uniformed patrols
onboard trains and at stations for fare-checking (and for other purposes such as crime suppression), in order to discourage fare evasion. With limited resources to devote to patrols,
it is impossible to cover all locations at all times.
TRUSTS, currently being evaluated at LASD, provides a game-theoretic solution to
scheduling randomized patrols for fare evasion deterrence. For a given day, TRUSTS generates one patrol schedule for each fare inspection team according to a pre-computed probability distribution over a large set of possible patrol candidates. A patrol schedule generated
is a sequence of fare-check operations, alternating between in-station and on-train operations. Each operation indicates specifically where and when a patrol unit should check fares.
Unfortunately, the security personnel may deviate from the given schedule for a variety of
reasons, such as writing citations, felony arrests, handling emergencies, etc. Indeed, in 5 real
world trials carried out by the LASD, 4 times the pre-generated schedules got interrupted.
Often the entire schedule got abandoned after the interruption if the operations specified
afterwards became irrelevant. For example, an officer following a pre-generated schedule
had to write a citation to a rider not carrying a valid ticket, preventing her from carrying
out the rest of the schedule.
3.2 Formal Model

Figure 1: Example of a schedule.
As the first contribution of this work, we present a formal game-theoretic model for
patrolling with dynamic execution uncertainty. A patrolling game with execution uncertainty
is a two-player Bayesian Stackelberg game, between a leader (the defender) and a follower
(the adversary). The leader has  patrol units, and commits to a randomized daily patrol
schedule for each unit. A patrol schedule consists of a list of commands to be carried out
in sequence. Each command is of the form: at time  , the unit should be at location l,
327

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

and should execute patrol task a. The patrol action a of the current command, if executed
successfully, will take the unit to the location and time of the next command. A graphical
representation of a schedule is shown in Figure 1. Each unit faces uncertainty in the
execution of each command: delays, or being called to deal with emergencies (possibly at
another location). As a result the unit may end up at a location and a time that is different
from the intended outcome of the action, and thus the rest of the naive patrol schedule
cannot be executed.
We use Markov Decision Processes (MDPs) as a compact representation to model each
individual defender units patrol actions. We emphasize that these MDPs are not the whole
game: they only model the defenders patrol actions with the environment when executing
patrols; we will later describe the interaction between the defender and the adversary.
Formally, for each defender unit i  {1, . . . , } we define an MDP (Si , Ai , Ti , Ri ), where
 Si is a finite set of states. Each state si  Si is a tuple (l,  ) of the current location of
the unit and the current discretized time. We denote by l(si ) and  (si ) the location and
time of si , respectively.
 Ai is a finite set of actions. Let Ai (si )  Ai be the set of actions available at state si .
 For each si  Si and each action ai  Ai (s), the default next state n(si , ai )  Si is the
intended next state when executing action ai at si . We call a transition (si , ai , si ) a default
transition if si = n(si , ai ) and a non-default transition otherwise.
 Ti (si , ai , si ) is the probability of next state being si if the current state is si and the
action taken is ai .
 Ri (si , ai , si ) is the immediate reward for the defender from the transition (si , ai , si ). For
example, being available for emergencies (such as helping a lost child) is an important
function of the police, and we can take this into account in our optimization formulation
by using Ri to give positive rewards for such events.
We assume that the MDP is acyclic: Ti (si , ai , si ) is positive only when  (si ) >  (si ),
i.e., all transitions go forward in time. Si+  Si is the subset of states where a patrol
could start. A patrol could end at any state. For convenience, we add a dummy source
state s+
i  Si that has actions with deterministic transitions going into each of the states
+
in Si , and analogously a dummy sink state s
i  Si . Thus each patrol of defender i

starts at s+
and
ends
at
s
.
A
patrol
execution
of i is specified by its complete trajectory
i
i
+ 1 1 2

ti = (s+
,
a
,
s
,
a
,
s
,
.
.
.
,
s
),
which
records
the
sequence of states visited and actions
i
i i
i
i
i
performed. A joint complete trajectory, denoted by t = (t1 , . . . , t ), is a tuple of complete
trajectories of all units. Let X be the finite space of joint complete trajectories.
The immediate rewards Ri are not all the utility received by the defender. The defender
also receives rewards from interactions with the adversary. The adversary can be of a set
 of possible types and has a finite set of actions A. The types are drawn from a known
distribution, with p the probability of type   . The defender does not know the
instantiated type of the adversary, while the adversary does and can condition his decision
on his type.
In this general game model, the utilities resulting from defender-adversary interaction
could depend arbitrarily on the complete trajectories of the defender units. Formally, for a
joint complete trajectory t, the realized adversary type   , and an action of the adversary
  A, the defender receives utility ud (t, , ), while the adversary receives ua (t, , ).
328

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

We are interested in finding the Strong Stackelberg Equilibrium (SSE) of this game, in
which the defender commits to a randomized policy which we define next, and the adversary
plays a best response to this randomized policy. It is sufficient to consider only pure
strategies for the adversary (Conitzer & Sandholm, 2006). Finding one SSE is equivalent
to the following optimization problem:
max


X

p Et [ud (t, ,  ) +

X

Ri (ti )]

(1)

i



s.t.   arg max Et [ua (t, ,  )],   


(2)

where Ri (ti ) is the total immediate reward from the trajectory ti , and Et [] denotes the
expectation over joint complete trajectories induced by defenders randomized policy .
Whereas MDPs always have Markovian and deterministic optimal policies, in our game
the defenders optimal strategy may be non-Markovian because the utilities depend on
trajectories, and may be randomized because of interactions with the adversary. We consider
two cases: coupled execution and decoupled execution. In coupled execution, patrol units
can coordinate with each other; that is, the behavior of unit i at si could depend on the
earlier joint trajectory of all units. Formally, let Ti be the set of unit isQpartial Q
trajectories
+ 1 1
 ). A coupled randomized policy is a function  :
T

(s+
,
a
,
s
,
a
,
.
.
.
,
s
i
i
i
i Ai  R
i i
i
i
that specifies a probability distribution over joint actions of units for each joint partial
trajectory. Let (t; )  R be the probability that joint complete trajectory t  X is
instantiated under policy . In decoupled execution, patrol units do not communicate with
each other. Formally, a decoupled randomized policy  = (1 , . . . ,  ) where for each unit
i, i : Ti  Ai  R specifies a probability distribution over is actions given each partial
trajectory of i. Thus a decoupled randomized policy (1Q
, . . . ,  ) can be thought of as a
coupled randomized policy   where   (t, (a1 , . . . , a )) = i i (ti , ai ).
Coupled execution potentially yields higher expected utility than decoupled execution.
Suppose the defender wants to protect an important target with at least one unit, and unit
1 is assigned that task. Then if she knows unit 1 is dealing with an emergency and unable to
reach that target, she can reroute unit 2 to cover the target. However, coordinating among
units presents significant logistical and (as we will see in this paper) computational burden.

4. Approach
The defenders optimal strategy may be coupled and non-Markovian, i.e., the policy at s
could depend on the entire earlier trajectories of all units rather than the current state
s. This makes solving the game computationally difficultthe dimension of the space of
mixed strategies is exponential in the number of states.
Nevertheless, in many domains, the utilities have additional structure. There has been
extensive research on efficient computation of SSE for massive games with structured utility
functions (Tambe, 2011), including for the LA Metro domain (Yin et al., 2012), but these
works cannot deal with the type of execution uncertainty studied in this paper. In Section 4.1 we show that under the assumption that the utilities have separable structure, it is
possible to efficiently compute an SSE of patrolling games with execution uncertainty. In
Section 4.2 we discuss generating patrol schedules from solutions described in Section 4.1.
329

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

In Section 4.3 we present a procedure to extract an optimal mixed strategy but with a
polynomial-sized support. In Section 4.4 we consider a more general case with partially
separable utilities.
4.1 Efficient Computation via Compact Representation of Strategies
Consider a coupled strategy . Denote by xi (si , ai , si ) the marginal probability of defender
unit i reaching state si , executing action ai , and ending up at next state si . Formally,
xi (si , ai , si ) =

X

(t; )(ti , si , ai , si ),

(3)

tX

where the value of the membership function (ti , si , ai , si ) is equal to 1 if trajectory ti
contains transition (si , ai , si ) and is equal to 0 otherwise. As defined in Section 3.2, each
state is a tuple (l,  ) of the current location of the unit and the current discretized time.
Hence, each marginal probability xi (si , ai , si ) takes into account not only the location, but
also the time when a specific patrol action is taken. As we will see in the remainder of
this section, both time and location affect the expected utility of both players. Hence, this
design choice improves the accuracy of the model compared to the real
P world problem. Let
x  RM be the vector of these marginal probabilities, where M = i |Si |2 |Ai |. Similarly,
let wi (s
Pi , ai ) be the marginal probability of unit i reaching si and taking action ai . Let
w  R i |Si ||Ai | be the vector of these marginal probabilities.
Our goal is to compactly represent the SSE problem in terms of w and x, which have
dimensions polynomial in the sizes of the MDPs. We first show that w and x satisfy the
linear constraints:
xi (si , ai , si ) = wi (si , ai )Ti (si , ai , si ), si , ai , si
X
X
xi (si , ai , si ) =
wi (si , ai ), si

ai

(5)

ai

si ,ai

X

(4)

wi (s+
i , ai ) =

X

xi (si , ai , s
i ) = 1,

(6)

si ,ai

wi (si , ai )  0, si , ai

(7)

Lemma 1. For all coupled randomized policy , the resulting marginal probabilities wi (si , ai )
and xi (si , ai , si ) satisfy constraints (4), (5), (6), (7).
Proof. Constraint (4) holds by the definition of transition probabilities of MDPs. Constraint
(5) holds because both lhs and rhs equal the marginal probability of reaching state s.
Constraint (6) holds because by construction, the marginal probability of reaching s+
i is 1,

and so is the marginal probability of reaching si . Constraint (7) holds because wi (si , ai ) is
a probability.
Similar formulations for marginal probabilities of MDPs are known (e.g., Filar & Vrieze,
1996). However, unlike in MDPs, in general our utility functions can depend on the defenders complete trajectory and the adversarys type and action, and as a result w and x
are not sufficient to determine the expected utilities of the game. Thus, in order to make
330

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

use of this compact representation, we need to make further restrictions to the structure
of the utility functions. It turns out that we can formulate expected utilities in terms of
w and x if the games utilities are separable, which intuitively means that given the adversarys strategy, the utilities of both players are sums of contributions from individual units
individual transitions:
Definition 1. A patrolling game with execution uncertainty as defined in Section 3.2 has
separable utilities if there exist utilities Ud (si , ai , si , ) and Ua (si , ai , si , ) for each unit
i, transition (si , ai , si ),   ,   A, such that for all t  X ,   ,   A, the defenders
and the adversarys utilities can be expressed as
X X
ud (t, , ) =
(ti , si , ai , si )Ud (si , ai , si , )
i

and
ua (t, , ) =

si ,ai ,si

X X
i

(ti , si , ai , si )Ua (si , ai , si , ),

si ,ai ,si

respectively.
Let Ud , Ua  RM |A| be the corresponding matrices. Then Ud , Ua completely specifies
the utility functions ud and ua .
Recall that (ti , si , ai , si ) is equal to 1 if trajectory ti contains transition (si , ai , si ) and is
equal to 0 otherwise. So the above definition is saying that in a separable game, each players
utility when the trajectory is t can be decomposed to contributions from each transition
of each units trajectory ti . This is a natural extension of the additive reward model of
MDPs to the multi-player setting. Separable games can represent common attacker-defender
patrolling scenarios, as illustrated in the following example.
0
L1

L1, 0

1
1.0

Stay

L1, 1

2

To L2

0.9

To L1

To L1
0.1

0.1
L2, 0

Stay

0.9
0.9

0.9

L2

L1, 2

0.1

0.1
To L2

1.0

Stay

1.0

L2, 1

Stay

1.0

L2, 2

Figure 2: Example game with separable utilities.
Example 1. Consider a simple example game with one defender unit, whose MDP is
illustrated in Figure 2. There are six states, shown as circles in the figure, over two locations L1 , L2 and three time points 0 , 1 , 2 . From states at 0 and 1 , the unit has
two actions: to stay at the current location, which always succeeds, and to try to go to
331

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

the other location, which with probability 0.9 succeeds and with probability 0.1 fails (in
which case it stays at the current location). There are 12 transitions in total, which is
fewer than the number of complete trajectories (18). There is a single type of adversary who chooses one location between L1 and L2 and one time point between 1 and 2
to attack (0 cannot be chosen). If the defender is at that location at that time, the attack fails and both players get zero utility. Otherwise, the attack succeeds, and the adversary gets utility 1 while the defender gets 1. In other words, the attack succeeds if
and only if it avoids the defender units trajectory. This game has separable utilities: for
any transition (si , ai , si ) in the MDP, let Ua (si , ai , si , ) be 0 if  coincides with si and
1 otherwise. Then each players utility given a trajectory t can be expressed as a sum
of contributions from transitions, exactly as in Definition 1. For example, the utility expression for the adversary given trajectory ((L1 , 0 ), To L2 , (L1 , 1 ), To L2 , (L2 , 2 )) is
Ua ((L1 , 0 ), To L2 , (L1 , 1 ), ) + Ua ((L1 , 1 ), To L2 , (L2 , 2 ), ), which gives the correct
utility value for the adversary: 0 if  equals (L1 , 1 ) or (L2 , 2 ) and 1 otherwise.
It is straightforward to show the following.
Lemma 2. Consider a game with separable utilities. Suppose x is the vector of marginal
probabilities induced by the defenders randomized policy . Let y  R|A| be a vector
describing the mixed strategy of the adversary of type , with y () denoting the probability
of choosing action
and the adversarys expected utilities from their
P
P . Then the defenders
interactions are  p xT Ud y and  p xT Ua y , respectively.
In other words, given the adversarys strategy, the expected utilities of both players
are linear in the marginal probabilities xi (si , ai , si ). Lemma 2 also applies when (as in
an SSE) the adversary is playing a pure strategy, in which case y is a 0-1 integer vector
with y () = 1 if  is the action chosen. We can thus use this compact representation of
defender strategies to rewrite the formulation for SSE (1) as a polynomial-sized optimization
problem.
max

w,x,y

X

p xT Ud y +

 X
X

xi (si , ai , si )Ri (si , ai , si )

(8)

i=1si ,ai ,si



s.t. constraints (4), (5), (6), (7)
X
y () = 1,   

(9)



y ()  {0, 1},
x
y  arg max

y

  ,   A
T

Ua y

(10)
(11)

As we will show in Section 4.2, given a solution w, x to (8), we can calculate a decoupled
policy that matches the marginals w, x. Compared to (1), the optimization problem (8) has
exponentially fewer dimensions; in particular the numbers of variables and constraints are
polynomial in the sizes of the MDPs. Furthermore, existing methods for solving Bayesian
Stackelberg games, such as mixed-integer linear program formulation (Paruchuri, Pearce,
Marecki, Tambe, Ordonez, & Kraus, 2008a) or branch-and-bound approach (Yin & Tambe,
2012), can be adapted to solve (8). For example, Paruchuri et al. (2008a) formulated
332

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

the problem as a mixed-integer quadratic program, then transformed it to an equivalent
mixed-integer linear program, which can be solved using standard optimization solvers like
CPLEX. While Paruchuri et al. (2008a) assumed that the defenders strategy space is the
standard simplex, we can replace the simplex constraints with the flow constraints (4), (5),
(6), (7), and apply the same techniques described by Paruchuri et al. (2008a) to derive a
mixed-integer linear program.
For the special case of Ud + Ua = 0 for all , i.e., when the interaction between defender
and adversary is zero-sum, the SSE problem can be formulated as the following linear
program (LP):
X
X X
max
p u  +
xi (si , ai , si )Ri (si , ai , si )
(12)
w,x,u



i

si ,ai ,si

s.t. constraints (4), (5), (6), (7)
u  xT Ud e ,

  ,   A,

(13)

where e is the basis vector corresponding to adversary action . This LP is similar to the
maximin LP for
zero-sum game with the utilities given by Ud and Ua , except that an adP aP
ditional term i si ,ai ,s xi (si , ai , si )Ri (si , ai , si ) representing defenders expected utilities
i
from immediate rewards is added to the objective. One potential issue arises: because of
the extra defender utilities from immediate rewards, the entire game is no longer zero-sum.
Is it still valid to use the above maximin LP formulation? It turns out that the LP is indeed
valid, as the immediate rewards do not depend on the adversarys strategy.
Proposition 1. If the game has separable utilities and Ud + Ua = 0 for all , then a
solution of the LP (12) is an SSE.
Proof. We can transform this game to an equivalent zero-sum Bayesian game whose LP
formulation is equivalent to (12). Specifically, given the non-zero-sum Bayesian game 
specified above, consider the Bayesian game  with the following meta type distribution
for the second player: for all    of  there is a corresponding type    in  , with
probability p = 0.5p , with the familiar utility functions; and there is a special type   
with probability p = 0.5, whose action does not affect
P either
P players utility. Specifically
the utilities under the special type  are ud (t, , ) = i si ,ai ,s (ti , si , ai , si )Ri (si , ai , si )
i
P P
and ua (t, , ) =  i si ,ai ,s (ti , si , ai , si )Ri (si , ai , si ). The resulting game  is zeroi
sum, with the defenders utility exactly half the objective of (12). Since for zero-sum games
maximin strategies and SSE coincide, a solution of the LP (12) is an optimal SSE marginal
vector for the defender of  . On the other hand, if we compare the induced
P normal forms of

 and  , the only difference is that for the adversary the utility 0.5 eE  Ue xe is added,
which does not depend on the adversarys strategy. Therefore  and  have the same set
of SSE, which implies that a solution of the LP is an SSE of .
4.2 Generating Patrol Schedules
The solution of (8) does not yet provide a complete specification of what to do. We ultimately want an explicit procedure for generating the patrol schedules. We define a Markov
strategy  to be a decoupled strategy (1 , . . . ,  ), i : Si  Ai  R, where the distribution
333

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

over next actions depends only on the current state. Proposition 2 below shows that given
w, x, there is a simple procedure to calculate a Markov strategy that matches the marginal
probabilities. This implies that if w, x is the optimal solution of (8), then the corresponding
Markov strategy  achieves the same expected utility. We have thus shown that for games
with separable utilities it is sufficient to consider Markov strategies.
Proposition 2. Given w, x satisfying constraints (4) to (7), construct a Markov strategy 
P
(si ,ai )

as follows: for each si  Si , for each ai  Ai (si ), i (si , ai ) = Pwiw
 ) . If
a wi (si , ai ) =
(s
,a
i i
a

i

i

i

0 we set i (si , ) to be an arbitrary distribution. Suppose the defender plays , then for
all unit i and transition (si , ai , si ), the probability that (si , ai , si ) is reached by i equals
xi (si , ai , si ).
Proof. Such a Markov strategy  induces a Markov chain over the states Si for each unit
i. We claim that the resulting marginal probability vector of this Markov chain matches x.
We can show this by induction from the starting state s+
i to successor states. The marginal
+
probability Pr(s+
,
a
)
of
reaching
state
s
and
taking
action
ai is equal to i (s+
i
i
i
i , ai ), since
+
s+
1 is always reached. But i (si , ai ) =

P

wi (s+
i ,ai )

w
(s+

i
a
i ,ai )

= wi (s+
i , ai ) by constraint (6), So the

i

marginals are matched at s+
i . For the inductive step, the marginal probability Pr(si , ai ) of
reaching si and taking action ai is equal to Pr(si )i (si , ai ), where Pr(si ) is the probability
of reaching si . By the induction hypothesis,
P Pr(si ) can be computed from the marginals
x, w on the previous states, as Pr(si ) = s ,a xi (si , ai , si ). Then
i

Pr(si , ai ) = i (si , ai )

i

X

wi (si , ai ) X
xi (si , ai , si ) = P
xi (si , ai , si ) = wi (si , ai )
)
w
(s
,
a

i
i
a
i  


si ,ai

i

si ,ai

by constraint (5). Thus the marginals are matched at all states.
In practice, directly implementing a Markov strategy  requires each unit i to draw an
action according to the probability distribution i (si , ) at each state si . This is possible
when each unit can consult a random-number generator, or can communicate with a central
command. However, in certain domains such requirement on computation or communication
at each time step places additional logistical burden on the patrol unit. To avoid unnecessary
computation or communication at every time step, it is desirable to let each unit execute a
deterministic schedule (i.e., a pure strategy). To guarantee the optimal expected utility, we
want the deterministic schedule to be drawn from a distribution that has the same marginals
as the optimal solution of (8). We say a procedure that generates patrols is correct if it
has this property. With no execution uncertainty, a pure strategy can be specified by the
complete trajectory for each unit. However, this no longer works in the case with execution
uncertainty, as interruptions will lead to states outside the trajectory.
We thus begin by defining a Markov pure strategy, which specifies a deterministic choice
at each state.
Definition 2. A Markov pure strategy q is a tuple (q1 , . . . , q ) where for each unit i,
qi : Si  Ai .
334

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

We note that the set of Markov pure strategies is a subset of all pure strategies, which
can generally condition their choices on earlier histories in addition to the current states.
Nevertheless, we will show that Markov pure strategies are useful as part of an efficient
procedure for sampling from the solution of (8).
Given a Markov strategy , we sample a Markov pure strategy q as follows:
Procedure 1. Given , for each unit i and state si  Si , sample an action ai according to
i , and set qi (si ) to be ai . Output the Markov pure strategy q.
This procedure is correct since each state in is MDP is visited at most once and thus
qi exactly simulates a walk from s+
i on the Markov chain induced by i .
To directly implement a Markov pure strategy, the unit needs to either remember the
entire mapping q or to receive the action from a central command at each time step. An
alternative scheme that requires a small amount of storage and a minimal amount of communication is the following: the central command sends the unit a trajectory assuming
perfect execution, and only after a non-default transition happened does the unit communicates with the central command to get a new trajectory starting from the current state.
Formally, given si  Si and qi , we define the optimistic trajectory from si induced by qi to
be (si , qi (si ), n(si , qi (si )), . . . s ), i.e, the trajectory assuming it always reaches its default
next state. Given a Markov pure strategy q, the following procedure exactly simulates q:
Procedure 2. For each unit i: (i) central command gives unit i the optimistic trajectory
from s+ induced by qi ; (ii) unit i follows the trajectory until the terminal state s is reached
or some unexpected event happens and takes i to state si ; (iii) in the latter case, central
command sends unit i the new optimistic trajectory from si induced by qi and repeat from
step (ii).
4.3 Extracting a Mixed Strategy with Small Support
So far the procedures for generating patrol schedules we described are different implementations of the Markov strategy  from Proposition 2. This corresponds to a mixed strategy
over the set of Markov pure strategies: each Markov pure strategy q is played with probability equal to the probability that it is sampled by the sampling procedure. The support
of this mixed strategy, i.e., the set of pure strategies with non-zero probability, is in general
an exponential-sized set.
In practice, it is sometimes desirable to have a mixed strategy with polynomial-sized
support. For example, the security agency may need to carry out training exercises for each
of the pure strategies in the support (e.g., Fang et al., 2013). In such cases, we would like a
polynomial-support mixed strategy that achieves the same expected utility as the optimal
solution of (8). The following proposition shows that there always exists a polynomialsupport mixed strategy that matches the given marginals w, x, and therefore achieves the
optimal expected utility when w, x are the solution of (8).
Proposition 3. Given w, x satisfying constraints (4) to (7), there exists a mixed strategy
with polynomial-sized support that matches the marginals.
Proof. Since (w, x) is in the polytope defined by constraints (4) to (7), and extreme points
of the polytope correspond to pure strategies, Caratheodorys Theorem3 implies that (w, x)
3. See Chapter 3 from (Gruber, 2007)

335

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

can be written as a convex combination of at most dim(w)+dim(x)+1 = O(
pure strategies.

P

i |Si |

2 |A |)
i

However, the above proof is not constructive. Grotschel, Lovasz, and Schrijver (1981)
provided a polynomial-time procedure that given a point inside a polytope, decomposes it
into a convex combination of a polynomial number of the extreme points of the polytope;
however their method requires the application of ellipsoid method, which tends to be slow in
practice. In this section, we provide an efficient procedure for generating a mixed strategy
with polynomial-sized support that matches the marginals w, x.
Procedure 3. Given (w, x), an optimal solution of (8), we initialize the mixed strategy to
have an empty support set.
1. Compute the Markov strategy  as i (si , ai ) =

Pwi (si ,ai )  .
a wi (si ,ai )
i

If

P

ai

wi (si , ai ) = 0 we

set i (si , ) to be an arbitrary distribution.
2. Select a Markov pure strategy q that is played with positive probability in . One
possible way to construct q is as follows: at each state si , set qi (si ) to be an action
ai that is played with positive probability at si in .
3. Compute the marginals wq , xq corresponding to pure strategy q. This can be done
from s+
i to successor states.
4. Let q be the maximum  such that w  wq  0 and x  xq  0.
5. Set w := w  q wq , x := x  q xq . Add pure strategy q to the support, played with
probability q .
6. If w = 0, Stop. Otherwise, go to Step 1.
Proposition 4. Procedure 3 outputs in polynomial time a mixed strategy with polynomialsized support, matching the marginals.
Proof. Since wq , xq are valid marginal vectors satisfying constraints (4) to (7), the updated
marginals (w, x) after Step 5 still satisfies the flow conservation constraints (4), (5), and
the nonnegativity constraint (7), while the total flow (corresponding to (6)) is reduced by
q . This implies that the steps of the procedure are well-defined; furthermore once the
procedure stops, the output s sum to 1, and the corresponding mixed strategy matches
the marginals wq , xq .
It remains to show that the procedure stops after a polynomial number of iterations.
To see this, note that by construction, each execution of Step 5 will reduce at least one
component of (w, x) to zero. This is because otherwise we could increase qP
, contradicting
Step 4. Therefore the procedure stops after at most dim(w) + dim(x) = O( i |Si |2  |Ai |)
iterations; and since each iteration adds one pure strategy to the support, the resulting
mixed strategy has a polynomial-sized support.
336

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

4.4 Coupled Execution: Cartesian Product MDP
Without the assumption of separable utilities, it is no longer sufficient to consider decoupled
Markov strategies of individual units MDPs. We create a new MDP that captures the joint
execution of patrols by all units. For simplicity of exposition we look at the case with two
defender units. Then a state in the new MDP corresponds to the tuple (location of unit 1,
location of unit 2, time). An action in the new MDP corresponds to a tuple (action of unit
1, action of unit 2). Formally, if unit 1 has an action a1 at state s1 = (l1 ,  ) that takes her to
s1 = (l1 ,   ) with probability T1 (s1 , a1 , s1 ), and unit 2 has an action a2 at state s2 = (l2 ,  )
that takes her to s2 = (l2 ,   ) with probability T2 (s2 , a2 , s2 ), we create in the new MDP
an action a = (a1 , a2 ) from state s = (l1 , l2 ,  ) that transitions to s = (l1 , l2 ,   ) with
probability T (s , a , s ) = T1 (s1 , a1 , s1 )T2 (s2 , a2 , s2 ). The immediate rewards R of the
MDP are defined analogously. We call the resulting MDP (S , A , T , R ) the Cartesian
Product MDP.
An issue arises when at state s the individual units have transitions of different time
durations. For example, unit 1 rides a train that takes 2 time steps to reach the next station
while unit 2 stays at a station for 1 time step. During these intermediate time steps only
unit 2 has a free choice. How do we model this on the Cartesian Product MDP? One
approach is to create new states for the intermediate time steps. For example, suppose at
location LA at time 1 a non-default transition takes unit 1 to location LA at time 3. We
modify unit 1s MDP so that this transition ends at a new state (L1A , 2)  S1 , where L1A
is a special location specifying that the unit will become alive again at location LA in
one more time step. There is only one action from (L1A , 2), with only one possible next
state (LA , 3). Once we have modified the individual units MDPs so that all transitions
take exactly one time step, we can create the Cartesian Product MDP as described in the
previous paragraph.
Like the units MDPs, the Cartesian Product MDP is also acyclic. Therefore we can
analogously define marginal probabilities w (s , a ) and x (s , a , s ) on the Cartesian
2
Product MDP. Let w  R|S ||A | and x  R|S | |A | be the corresponding vectors.
Utilities generally cannot be expressed in terms of w and x . We consider a special case
in which utilities are partially separable:
Definition 3. A patrolling game with execution uncertainty has partially separable utilities if there exist Ud (s , a , s , ) and Ua (s , a , s , ) for each transition (s , a , s ),
  ,   A, such that for all t  X ,  P
 ,   A, the defenders and the adversarys
utilities can be expressed as ud (t, , ) = s ,a ,s  (t, s , a , s )Ud (s , a , s , ) and

P
ua (t, , ) = s ,a ,s  (t, s , a , s )Ua (s , a , s , ), respectively.


Partially separable utilities is a weaker condition than separable utilities, as now the
expected utilities may not be sums of contributions from individual units. When utilities
are partially separable, we can express expected utilities in terms of w and x and find
 , we can
an SSE by solving an optimization problem analogous to (8). From the optimal w
 (s , a ) =
get a Markov strategy 
 

w (s ,a )
P  

 ,
a w (s,a )

which is provably the optimal coupled



strategy.
This approach cannot scale up to a large number of defender units, as the size of S and
A grow exponentially in the number of units. In particular the dimension of the Markov
337

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

policy  is already exponential in the number of units. To overcome this we will need
a more compact representation of defender strategies. One approach is to use decoupled
strategies. The resulting defender strategy model resembles a transition independent DECMDP (Becker, Zilberstein, Lesser, & Goldman, 2004). However, due to strategic interaction
against adversaries, existing methods for DEC-MDPwhich compute pure strategies
cannot be directly applied. Efficient computation in these settings remains an open problem.

5. Case Study on the LA Metro System
This section discusses how we apply the model presented in Section 3.2 and the approach
presented in Section 4 to the problem of patrolling the LA Metro system for fare evasion. In
particular, Section 5.1 discusses how the game model is adapted to the LA metro patrolling
problem and Section 5.2 discusses how we use this framework to build a scheduling system
whereby schedules are generated by a central planner and are visualized by a smartphone
application running on android phones.
5.1 Application to LA Metro Domain
We now explain how our proposed techniques can be used in the LA Metro domain. This
involves defining a number of parameters specific to the LA Metro domain which will be
initialized for our real-world experiment in Section 6. In addition, as we will see, although
the utilities in this domain are not separable, we are able to provide an upper bound to the
defender utilities by separable utilities, allowing efficient computation.
Similar to the work by Yin et al. (2012), a state here comprises the current station and
time of a unit, as well as necessary history information such as starting time4 . At any state,
a unit may stay at her current station to conduct an in-station operation for some time or
she can ride a train to conduct an on-train operation when her current time coincides with
the train schedule. Due to execution uncertainty, a unit may end up at a state other than
the intended outcome of the action. For ease of analysis, we assume throughout the rest of
this paper a single type of unexpected event which delays a patrol unit for some time beyond
the intended execution time. Specifically, we assume for any fare check operation taken,
there is a probability  that the operation will be delayed, i.e., staying at the same station
(for in-station operations) or on the same train (for on-train operations) involuntarily for
some time. Furthermore, we assume that units will be involved with events unrelated to
fare enforcement and thus will not check fares during any delayed period of an operation.
Intuitively, a higher chance of delay leads to less time spent on fare inspection. As we will
see in Section 6, initializing this probability for conducting real-world experiments required
adopting a robust approach based on cross-validation (Kohavi, 1995; Jaulmes, Pineau, &
Precup, 2007) to address the uncertainty related to the duration of a delay.
The adversary faced here are the riders in the system. Specifically, we model the most
common type of riders which use the metro line every day to go to their work and come
back home. Each of them takes a fixed route: it starts at a specific station A (at a specific
time) and ends at a new station B (at a new time). Since there exists multiple stations and
time units, there also exist multiple routes to considered. To address this issue, we define
4. Interested readers are encouraged to read the work by Yin et al. (2012) for more details

338

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

multiple riders each representing a specific route. A rider observes the likelihood of being
checked and makes a binary decision between buying and not buying the ticket. If the rider
of type  buys the ticket, he pays a fixed ticket price  . Otherwise, he rides the train for
free but risks the chance of being caught and paying a fine of  >  . The LASDs objective
is set to maximize the overall revenue of the whole system including ticket sales and fine
collected, essentially forming a zero-sum game. This revenue depends on the number of
resources available to the LASD, i.e., the ones that can be deployed for each patrol5 . Given
a number of available officers then, the costs associated with their deployment (e.g., number
of officers and working hours) can be incorporated into the objective function by defining
negative rewards in Equation 8.
Since the fare check operation performed is determined by the actual transition rather
than the action taken, we define the effectiveness of a transition (s, a, s ) against a rider type
, f (s, a, s ), as the percentage of riders of type  checked by transition (s, a, s ). Following
the same argument by Yin et al. (2012), we assume the probability that a joint complete
trajectory t detects evader  as the sum of f over all transitions in t = (t1 , . . . , t ) capped
at one:
 X
X
Pr(t, ) = min{
f (si , ai , si )(ti , si , ai , si ), 1}.
(14)
i=1 si ,ai ,si

For type  and joint trajectory t, the LASD receives  if the rider buys the ticket and
  Pr(t, ) otherwise. The utilities in this domain are indeed not separable  even though
multiple units (or even a single unit) may detect a fare evader multiple times, the evader
can only be fined once. As a result, neither players utilities can be computed directly using
marginal probabilities x and w. Instead, we provide an upper bound to the defender utility
by overestimating the detection probability as the following:
\) =
Pr(t,

 X
X

f (si , ai , si )(ti , si , ai , si )  Pr(t, ).

(15)

i=1 si ,ai ,si

\),
Then the defender utility if the rider does not buy the ticket is upper-bounded by  Pr(t,
which is separable. Given a marginal vector x, the detection probability is then upperbounded by

X
X
\
Pr(x, ) =
f (si , ai , si )xi (si , ai , si ).
(16)
i=1 si ,ai ,si

Equation (16) leads to the following upper bound LP for the LA Metro problem:
max

x,w,u

s.t.

X


p u  +


X
X

Ri (si , ai , si )

(17)

i=1 si ,ai ,si

constraints (4), (5), (6), (7)
\)}, for all   
u  min{ ,   Pr(x,

(18)

5. The number of resources required to maximize the revenue for the LASD is equal to the number of number
of edges of the MDP defined in Section 4. Unfortunately, in the real world the available resources are
much less than this number. Therefore, the idea is to maximize the revenue for the LASD given the
resources available.

339

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

We prove the claims above by the following two propositions.
\) is an upper bound of the true detection probability of any coupled
Proposition 5. Pr(x,
strategy with marginals x.
Proof. Consider a coupled strategy . Recall that (t; )  R is the probability that
joint trajectory
P t  X is instantiated. For rider type , the true detection probability is
Pr(, ) = tX (t; )Pr(t, ). Applying Equations (14) and (3) we have,
Pr(, ) 

X

(t; )


X
X

f (si , ai , si )

i=1 si ,ai ,si

=


X

f (si , ai , si )(ti , si , ai , si )

i=1 si ,ai ,si

tX

=


X
X

X

X

(t; )(ti , si , ai , si )

tX

\).
f (si , ai , si )xi (si , ai , si ) = Pr(x,

i=1 si ,ai ,si

Proposition 6. LP (17) provides an upper bound of the optimal coupled strategy.
Sketch. Let x and w be the marginal coverage and u be the value of the patroller
against rider type  in the optimal coupled strategy   . It suffices to show that (x , w ,
u ) is a feasible point of the LP. From Lemma 1, we already know x and w must satisfy
constraints (4) to (7). Furthermore, we have u   since the rider pays at most the ticket
\) since Pr(x,
\) is an overestimate of the true detection
price. Finally, u    Pr(x,


probability.
Intuitively, LP (17) relaxes the utility functions by allowing an evader to be fined multiple times during a single trip. The relaxed utilities are separable and thus the relaxed
problem can be efficiently solved. Since the solution returned x and w satisfy constraints
(4) to (7), we can construct a Markov strategy from w as described in Section 4.2. The
Markov strategy provides an approximate solution to the original problem, whose actual
value can be evaluated using Monte Carlo simulation. This strategy is also sampled to
produce a patrol schedule which is then uploaded on the smartphone application which is
discussed next.
5.2 The Smartphone Application
The smartphone app is a software agent carried by each patrol officer that visualizes the
patrol schedules generates using the approach discussed in the previous section. Shown in
Figure 3, the app provides three principal features: (i) a patrol schedule for the current
shift; (ii) a system for reporting passenger violations and (iii) a shift statistics summary
report. At the beginning of the shift, a patrol schedule is loaded into the app either by
hand or using a database. The app then displays a schedule of the current and upcoming
patrol actions in the schedule view, shown in Figure 3(a). Implementing recovery from
340

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

(a) Schedule View

(b) Reporting View

(c) Summary View

Figure 3: The smartphone user interface

real-world unexpected events that interrupt the officers schedule, the schedule view also
allows the officer to manually set their current location, triggering the app to select a new
patrol schedule based on the officers current location and time. To do so, a number of
patrol schedules are sampled from the Markov strategy (see Section 4.2) and are uploaded
in the app before deployment. In the remainder of this work, we will refer to this action as
requesting an update. The app also allows patrol officers to record passenger violations,
such as fare evasion, for the current patrol action using Reporting View, shown in Figure
3(b). Officers can also view and edit the passenger violations reported for past actions
in Summary View, shown in Figure 3(c). Upon shift completion, the officer can also use
Summary View to submit the app-generated shift statistics summary report, including all
unexpected events and violations reported throughout the shift, to a database.
This app presents two key advantages for security agencies. First, it allows for the
collection of patrol data, which could then be used to analyze the behavior of adversaries
such as fare evaders and criminals. Second, this app-collected data could also benefit transit
system security departments that manually record violations data or conduct their own
analysis on patrol strategy performance.

6. Evaluation
This section describes our experiments. In Section 6.1, we describe the datasets and how
we instantiated some of the model parameters for our experiments. Section 6.2, discusses
our simulations in which we studied the key properties of the approach discussed in Section
5.1. Finally, Section 6.3 discusses the real-world experiment where we ran a head-to-head
comparison between our approach and a uniform random approach, the automated approach
that most security agencies use when not using a game-theoretic approach (Tambe, 2011).
341

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

6.1 Data Sets
The experiments presented in this work, either in simulation or in the real-world, are based
on four data sets, each based on a different Los Angeles Metro Rail line: Red (including
Purple), Blue, Gold, and Green. In these data sets, the train schedules were obtained
from http://www.metro.net and the ridership distributions for each line were estimated
from hourly boarding and alighting counts provided by the LASD. We allowed any ontrain operation, i.e., train checks could be between two or more stations and defined instation operations, i.e., station-checks, to be fixed intervals between 10 and 20 minutes. As
recommended by the LASD, this duration is the one that security officers typically adopt
to conduct fare inspections at a train station. The effectiveness of each fare check operation
was adjusted based on the volume of riders during the period with an assumption that
a unit would check three riders per minute. The ticket fare was set to $1.5 (the actual
current value), while the fine was set to $100 (fare evaders in Los Angeles can be fined $200,
however, they also may be issed warnings). A rider can always pay the ticket price for $1.5
and will only evade the ticket when the expected fine is lower. The immediate rewards Ri
are all set to zero. Table 1 summarizes the detailed statistics for the four Metro lines.
Line
Red
Blue
Gold
Green

Stops

Trains

Daily Riders

Types

16
22
19
14

433
287
280
217

149991.5
76906.2
30940.0
38442.6

26033
46630
41910
19559

Table 1: Statistics of Los Angeles Metro lines.

6.2 Simulations
The simulations are aimed to analyze the performance of the Markov strategies calculated
solving the LP defined in Section 5.1. More specifically, we aim to analyze the key features
of our approach by systematically manipulating those parameters that are more likely to
vary significantly in the real world and, as consequence, affect the revenue for the defender
and the rate of fare evaders captured. These parameters include the delay length, the train
lines, the levels of execution uncertainty and the number of available resources. We also
tested the runtime of our LP algorithm to verify whether it was capable of generating an
output in a timely manner. Most result are presented for the Red line only. Results for the
Blue, the Gold and the Green line are presented in the Appendix A.
In all such simulations, we found that the Markov strategy was close to optimal with
revenue always above 99% of the LP upper bound. Figure 4 shows such a result, assuming
6 resources patrolling the Red line for 3 hours and varying uncertainty probability (from 0%
to 25%). Similar results were found for the Blue, Green and Gold lines and are reported in
Figure 14 in the Appendix A. This result indicated that the relaxed detection probability
given in Equation (16) provided a good estimation of the true probability, implying that
riders were unlikely to be checked more than once by joint execution trajectories produced
by the Markov strategy in all of our data sets. For this reason, in the remainder of this
342

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

Figure 4: the Markov Strategy (Equation (17)) vs. the true LP (Equation (8)).

section we will report values of the Markov strategy without mentioning the LP upper
bound.
In our experiments, we compared, under execution uncertainty, the performance of our
Markov strategy (obtained by solving LP (17)) against two types of benchmarks: gametheoretic, deterministic, policies assuming no execution uncertainty and Markov policies
that take execution uncertainty into account, but that assign actions based on a uniform
random probability. The pre-generated schedules are calculated using TRUSTS (Yin et al.,
2012), the original system developed for the patrolling train lines, based on a deterministic
model assuming no execution uncertainty. However, since actions to take after deviations
from the original plan are not well-defined in TRUSTS schedules, we augmented these pregenerated schedules with two naive contingency plans indicating the actions to follow after
a unit deviates from the original plan. The first plan, Abort, is to simply abandon the
entire schedule and return to the base. The second plan, Arbitrary, is to pick an action
randomly from all available actions at any decision point after the deviation. The uniform
random Markov strategy (Markov UR) then assigns, given a state s  Si of the MDP defined
in Section 3.2, a uniform probability to all the actions taken in s leading to another state
s  Si . In essence, this strategy is similar to the Arbitrary policy but it assumes that
resources always pick a random action and not only after they are deviated. It was chosen
because this is the approach that security agencies adopt when not using a game-theoretic
approach for randomization6 .
Each of these strategies was generated using CPLEX 12.2 with the barrier method on
standard 2.8GHz machines with 4GB memory. Then, to generate significant datapoints,
each strategy was evaluated using Monte Carlo simulations with 100000 samples. In such
simulations, the riders were assumed to choose a best response based on the frequency of
being checked over these samples. The discussion of our results follows.

6. See the work of Tambe (2011) for more detail.

343

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

(a) Varying uncertainty

(b) Varying delay

Figure 5: Defenders revenue per rider on the Red line: Markov vs. TRUSTS pre-generated
strategies and Markov UR

6.2.1 Expected Revenue against Varying Delay Probability and Time
To run this experiment, we fixed the number of units to 6 and the patrol length to 3 hours
and varied the delay probability and the delay time. The results presented below are based
on simulations on the Red line. Figure 15 in the Appendix A shows the results for the Blue,
Green and Gold lines.
First, we fixed the delay time to 10 minutes and varied the delay probability  from 0%
to 25%. As we can see in Figure 5(a), Abort, Arbitrary and Markov-UR performed
poorly in the presence of execution uncertainty. With increasing values of , the revenue
of Abort and Arbitrary decayed much faster than the Markov strategy. In fact, by
increasing the delay probability, the number of interruptions is also increased. In such
situations, both the Abort and the Arbitrary strategies will perform sub-optimal actions
(dropping the schedule or selecting a random action) which will yield a poor performance.
344

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

For example, when  was increased from 0% to 25%, the revenue of Abort and Arbitrary
decreased 75.4% and 37.0% respectively while that of the Markov strategy only decreased
3.6%. In contrast, the performance ofMarkov-UR remained constantly around the same
value of, approximately 0.4. This is expected, since the strategy constantly performs random
actions, therefore its performance will be independent from the delay probability.
An important observation here is that the revenue of Abort decayed extremely fast
with increasing   even with a 5% probability of delay, the revenue of Abort was only
73.5% of that of the Markov strategy. With a conservative estimate of 6% potential fare
evaders (?) and 300, 000 daily riders in the LA Metro Rail system, the 26.5% difference
implies a daily revenue loss of $6, 500 or $2.4 million annually.
Second, we fixed  to 10% and varied the delay time from 5 to 25 minutes. The results, in
Figure 5(b), present similar trends to the ones in Figure 5(a). The three strategies Abort,
Arbitrary and Markov-UR performed worse than the Markov strategy. With increasing
delay time, the revenue of Arbitrary, decayed in a faster rate than the Markov strategy.
Similar to what discussed earlier, after a delay, the strategy would start to generate suboptimal actions which would result in a low expected revenue. In contrast, both the revenue
of Abort and of Markov-UR remained the approximately the same. The time of the
delay does not matter for the Abort strategy if a resource abandon the schedule after the
first unexpected event. Similarly, the time of the delay does not matter for Markov-UR
if a resource only performs random actions. When the delay time was increased from 5 to
25 minutes, the revenue of Abort and Markov UR remained the same, 0.4 and 0.75
respectively, while that of Arbitrary and the Markov strategy decreased 14.4% and 3.6%
respectively.

Figure 6: Markov vs. deterministic strategies: evasion rate

6.2.2 Fare Evasion Rate against Varying Delay Probability and Time
The settings for this experiment were the same as the ones for the experiment discussed
above. Here, we present the results for the Red line only. The results for the Blue, the
Green and the Gold line, present similar trends and are shown in Figure 16 in the Appendix
A. As discussed in Section 6.1, we considered a rider to prefer fare evasion if and only if
345

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

his expected penalty from fare evasion is lower than $1.5, the ticket price. The results are
shown in Figure 6, which shows the fare evasion rate of the four policies with increasing .
As we can see, Abort, Arbitrary and Markov UR showed extremely poor performance
in evasion deterrence with even a tiny probability of execution error. Similar to the first
experiment, increasing the number of interruptions led the two deterministic strategies to
produce sub-optimal actions yielding a fewer number of fare evaders captured. In particular,
when the delay probability  was increased from 0% to 5%, the evasion rate of the Markov
strategy barely increased while that of Abort and Arbitrary increased from 11.2% both
to 74.3% and 43.9% respectively. In contrast, the Markov UR strategy remained stable
around a fare evasion rate of 80%. This result confirms the trend for the Markov UR
strategy depicted in Figure 5(a). The delay probability does not affect the performance of
the strategy because it consists of computing random actions.

20%
Fare evasion rate

Revenue per rider

1.5
1.45
1.4
1.35
Blue

Gold

Green

Red

1.3
0 0.05 0.1 0.15 0.2 0.25
Probability of unexpected event

Blue

Gold

Green

Red

15%
10%
5%
0%
0 0.05 0.1 0.15 0.2 0.25
Probability of unexpected event

(a) Revenue per rider of Markov strategy

(b) Evasion rate of Markov strategy

Figure 7: Markov strategy over different lines.

6.2.3 Robustness of the Approach Against increasing Delay Probability
To run this experiment, we fixed the number of units to 6 and the patrol length to 3 hours,
but varied the delay probability  from 0% to 25%. Figure 7(a) and Figure 7(b) show the
expected revenue per rider and the evasion rate of the four lines respectively7 . As we can
see, the revenue decreased and the evasion rate increased with increasing . However, the
Markov strategy was able to effectively allocate resources to counter the effect of increasing
 in terms of both revenue maximization and evasion deterrence. For example, the ratio of
the revenue of  = 25% to that of  = 0% was 97.2%, 99.1%, 99.9%, 95.3% in the Blue,
Gold, Green and Red line respectively. Similarly, when  was increased from 0% to 25%,
the evasion rate of the Blue, Gold, Green and Red line was increased by 4.6, 1.9, 0.1, 5.2
percentage points respectively. Thus, the Markov strategy performance degraded gracefully
as uncertainty increased for each of the four lines.
7. The revenue of the Red line was significantly lower than the other lines because fare check effectiveness
f defined in Section 5.1 was set inversely proportional to the ridership volume.

346

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

Revenue per rider

1.5
1.3
Low

Medium

High

1.1
0.9
0.7
0 0.05 0.1 0.15 0.2 0.25
Probability of unexpected event

Figure 8: Revenue decay with varying coverage levels

6.2.4 Revenue per Rider while Increasing Delay Probability
In this experiment, we considered 3, 6 and 9 patrol units, representing three levels of fare
enforcement: low, medium and high, respectively, and evaluated the revenue per rider with
increasing . The results for the red line are depicted in Figure 8. Results for the blue,
the green and the gold line present similar trends and are depicted in Figure 17 in the
Appendix A. As shown in Figure 8, the rate of revenue decay with respect to  decreased
as we increased the level of fare enforcement from low to high. Intuitively, with more
resources, the defender could better afford the time spent on handling unexpected events
without sacrificing the overall revenue. For example, when  was increased from 0% to 25%,
the revenue drop in the low, medium and high enforcement setting was 13.2%, 4.7%, and
0.4% respectively.

Revenue per rider

1.4
1.2
1

 = 0%
 = 10%
 = 20%

0.8
0.6
2

3
4
5
Number of patrol units

6

Figure 9: Revenue per rider with increasing coverage

347

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

6.2.5 Revenue per Rider while Increasing the Number of Resources

Runtime(in minutes)

In this experiment, we fixed the patrol length to 3 hours, we then considered three delay
probabilities  = 0%, 10%, and 20%, each representing an increasing level of uncertainty.
To measure the impact of the number of units, we increased the number of units from 2 to
6. The results for the Red line are shown in Figure 9, Figure 18 in the Appendix A shows
the results for the Blue, Green and Gold lines. Both figures show the revenue per rider
for the defender for increasing number of units. As depicted in the figure, as we increased
such number, the revenue increased towards the maximal achievable value of $1.5 (ticket
price). As the number resources increases, the algorithm produces Markov strategies which
distribute the resources so as to check more rider types. For example, as shown in the
figure, when  = 10%, the revenue per rider was $0.65, $1.12, and $1.37 for 2, 4, and to 6
patrol units respectively. In addition, the figure shows that as the uncertainty increases, the
revenue per rider slightly decays. For example, considering 4 units, the revenue per rider is
1.09, 1.13 and 1.18 for  = 0%, 10% and 20% respectively.

60
Blue
54
48
Gold
42
Green
36
Red
30
24
18
12
6
0
0 0.05 0.1 0.15 0.2 0.25
Probability of unexpected event
Figure 10: Worst-case LP runtime

6.2.6 Runtime of the LP Algorithm
To confirm this hypothesis, we ran an experiment considering the worst-case runtime (over
10 runs) of the LP with increasing  for the four metro lines. The number of units was
fixed to 3 and the patrol length per unit was fixed to 3 hours. To verify whether the delay
probability had any impact on the runtime, we ran simulations varying  from 0% to 25%.
The results are depicted in Figure 10. As we can see, the algorithm could solve all of the
problems within an hour. For example, when  = 10%, the runtime for the Blue, Gold,
Green, and Red line was 14.0, 24.3, 2.4, and 4.3 minutes respectively.
In addition, these results present a number of additional features that can be analyzed.
The runtime varied among the four Metro lines, related to their number of states and
types. In other words, the number of stations and daily trains (i.e. the density of the train
schedule) affected the runtime of the algorithm. For example, since the Green line has
significantly fewer types and states, solving the LP was easier than the other three lines.
348

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

A surprising result is the fact that we found no correlation between the runtime and
delay probability . Our results showed that, for all of the four lines, stochastic models with
 > 0% took less time to solve than deterministic models ( = 0%). More precisely, with
increasing  beyond 0%, the runtime of all of the four lines fluctuated and showed an upwards trend, yet the correlation between runtime and delay probability was not statistically
significant.
6.3 Real-World Experiment
The results of the simulations presented above showed that deterministic approaches that
do not take execution uncertainty into account perform poorly. Given the large number of
interruptions, the officers were rarely able to complete a schedule provided by deterministic
strategies. As discussed in Section 1, these results motivated this work and led to our new
solution concept based on an MDP. In addition, considering the limited time that we were
given by the LASD to run our experiment, we decided to use only the Markov UR strategy
as a benchmark in the real world experiment. Real-world failure was not acceptable for the
LASD. Therefore, they recommended not testing deterministic schedules any further. In
addition, despite performing poorly in simulation, not only is Markov UR the strategy
used by most security agencies to automatically allocate their resources, but schedules can
be updated whenever an interruption occurs.
The real world experiment took place over 21 days during the months of July and
August 2013. The organization of the experiment (e.g., train the security officers, design
and organize the experiment in collaboration with the LASD) required approximately two
weeks. This experiment had two key purposes. The first was to validate the MDP model
in the real world. The second was to run a head-to-head comparison between our gametheoretic approach and the Markov-UR approach. This section discusses the setup of the
experiment and the results that we obtained.
6.3.1 Experiment Setup
The fare evasion experiment took place on the Blue line of the LA Metro system (see Figure
11 for the map of the metro line). Other lines could not be tested, because the LASD only
allowed us to use our strategies on the Blue line during our real-world test. This line
consists of 22 different stations and is one of the biggest lines in the LA Metro system. It
was selected by the LASD, which helped to organize the experiment (e.g., assign security
officers and patrol times).
Each day, a team of two security officers (see Figure 12), was randomly selected by the
LASD, to patrol the line for a duration of at most 120 minutes. Patrols were run during
both the morning and the afternoon. Some days the tests ended early due to the officers
being reassigned. One of the two officers acted as the leader of the team: he was given
the smartphone, he had to read the schedule to the other officers, collect the data and
eventually update it whenever a delay occurred. An update could be made either during
a station-check as described in Section 6.1) or during a train-check. In the latter case, the
officers were required to leave the train at the next station to request an update. This was
required because, as discussed in Section 5.1, the Markov strategy was defined over each
349

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

Figure 11: The map of the blue line of the LA Metro

Figure 12: Two security officers performing fare checks on a train.
state of the MDP (i.e., station, time). Thus any new strategy has to be sampled from a
specific state. Every week the team was provided with one of two types of schedules:
Game-theoretic schedules (GT): This type of schedule was generated according to the
LP in Equation (17) presented in Section 5.1.
Markov UR schedules (UR): This type of schedule was generated by modeling the
problem as an MDP, as discussed in Section 3.2. However, the corresponding Markov
strategy si ,ai , for each state si and action ai was calculated assuming a uniform
probability distribution.
The officers were not told which schedule they were using as not to bias their performance. Before the experiment, we anticipated that the officers might view some of the
350

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

schedules as leading to low performance in terms of catching very few fare evaders. In such
situation, the officers, in order to avoid poor performance, might end up voluntarily deviating from their given schedules to reach a better location because they were unsatisfied
with the current one. In anticipation of such voluntary deviations, we augmented both the
game-theoretic and UR schedules with the ability to perform updates. More specifically, we
allowed the officers to request VOLUNTARY or INVOLUNTARY updates. VOLUNTARY
updates consisted of the officers updating the current schedule because in their opinion, the
current specified action was not fruitful as a venue to check fares. Officers were allowed
to choose a new location that they considered more fruitful for catching fare evaders and
request a new schedule from there. INVOLUNTARY updates consisted of the officers requesting a new schedule because they were delayed (e.g., from issuing citations or arresting
a suspect) and were unable to perform the next action on their schedule. This type of
update could be requested anytime an officer was delayed. As we will see below the officers
used VOLUNTARY updates almost every day with the UR schedules, but never in the GT
schedules.
Finally, it is important to notice that given the duration of our experiment, the gametheoretic schedules are essentially testing a maximin strategy. As discussed in Section
5.1, the LP computes a Stackelberg strategy, a strategy based on the assumption that
the riders will conduct surveillance and observe the defenders mixed strategy. However,
considering only 21 days of patrol whereby the officers could only patrol less than few
hours per day, either in the morning or the afternoon, we cannot assume that the riders
had sufficient time to conduct accurate surveillance, observe the mixed strategy and best
respond to it. Nonetheless, the LP in Section 5.1 solves a zero-sum game for which a
Stackelberg equilibrium and the maximin strategy are known to be equivalent (Yin et al.,
2012). Thus, since the maximin strategy provides a guaranteed level of defender utility
without making any assumption on the adversarys surveillance of the defenders mixed
strategy, these experiments compare the benefit of using a maximin strategy against other
(non-game-theoretic) approaches for generating patrol schedules.
6.3.2 Estimating the Uncertainty Parameter
Given the unpredictability of the real-world, two key parameters for instantiating the
MDPthe length of a delay and, most importantly, the probability that such a delay
could happen could not be defined before-hand, as was done in Section 6.1 with the remaining problem parameters. In such a setting, adopting a continuous-time MDP could be
a possible alternative. However, continuous time MDP algorithms, which involve techniques
such as forward search (Marecki & Tambe, 2008; Mausam, Benazera, Brafman, Meuleau,
& Hansen, 2005), would appear difficult to implement and would add significant computational complexity. Another alternative, the one adopted in this work, is to adopt a cross
validation approach, a well-known technique used in machine learning (Jaulmes et al., 2007)
and statistics (Kohavi, 1995). The idea is to select a policy robust against uncertainty, i.e.,
a policy which guarantees the highest expected revenue in the worst case setting, when
uncertainty will minimize the defenders expected utility. To achieve this, we discretized
the delays and defined an MDP model which assumed multiple delays, each with a specific
probability. We divided this approach into two steps. First, we randomly generated N
351

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

GT
UR

D1
60
60

D2
60
60

D3
90
60

D4
60
60

D5
90
60

D6
10
75

D7
90
100

D8
110
100

D9
90
100

D10
90
90

D11
105

total
855
765

Table 2: Patrol duration over each of the 21 days.
MDPs. Each MDP is generated assuming that a delay will happen within a time window
of 30 minutes. In other words, we assume that resource i can experience delays up to 30
minutes (any delay longer than 30 minutes is considered to be beyond repair and a new
schedule is generated). This time window is then divided into five different time intervals:
[0, 6], [6, 12], [12, 18], [18, 24] and [24, 30] minutes and one delay is sampled for each interval. In essence, this process discretizes the unknown delay length into 5 possible delays
distributed within a 30 minutes time window.
Second, we solve each MDP-based patrolling game using the LP in Section 5.1. In so
doing, we obtain N Markov policies ik corresponding to each M DP k . Next, we crossvalidate each policy ik against each M DPk with k 6= k  , i.e., we calculate the expected

revenue that each policy ik generates when evaluated against M DP k . This expected
revenue is calculated by running 100000 Monte Carlo simulations. Each simulation consists
of sampling one policy for the defender and calculate the resulting expected utility against
all N MDPs. At the end of the simulations, we obtain a N  N matrix where the rows
represented the policies ik and the columns represent the N MDPs. Each cell (k, k  ) of the

matrix contains the expected revenue obtained by evaluating policy ik against M DP k .
Then, the policy to deploy was selected using a maximin strategy. In more detail, we
chose the policy which was maximizing the expected utility in the worst case scenario, i.e.
considering the MDP yielding the lowest expected utility among all the different MDPs.
From a practical perspective, the policy obtained earlier might not yield a schedule
which will represent exactly all the delays that might happen during a patrol. However,
by modelling five different delays, the schedules are now able to cover a larger range of
delays. Hence, whenever an officer is interrupted and requests an update, the smartphone
application will simply search the schedule for the state that best matches the officer current
location and time and will present the new list of actions starting from there.
6.3.3 Results
During the 21 weekdays of our experiments, we were able to run GT schedules for 11 days
of testing while UR schedules were deployed for 10 days, resulting in 855 and 765 patrol
minutes, respectively. The schedules were compared using two different metrics. First, we
counted the number of passengers checked and the number of captures at the end of each
patrol. The captures were defined as the sum of the number of warnings, citations, and
arrests. Passengers without a valid ticket could be given a warning or cited for a violation
on the discretion of the officer. This metric was chosen because it would allow us to measure
the performance of each schedule in the real world. Second, we counted the number of times
that the update function was used voluntarily and involuntarily. While involuntary updates
helped determine the value of using MDPs as discussed below, voluntary updates measured
352

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

GT
UR

D1
0
0

D2
1
2

D3
3
1

D4
1
1

D5
1
1

D6
0
2

D7
2
2

D8
2
2

D9
4
3

D10
2
2

D11
1

total
18
16

Table 3: Number of INVOLUNTARY (delays) deviations for each day of patrol

GT
UR

D1
0
1

D2
0
0

D3
0
1

D4
0
1

D5
0
1

D6
0
0

D7
0
1

D8
0
1

D9
0
1

D10
0
1

D11
0

total
0
8

Table 4: Number of VOLUNTARY (updates) deviations for each day of patrol
the human (officer) perception of quality of the schedules  the more such voluntary updates,
the more the officers were dissatisfied with their given action. Table 2 shows the duration
of each day of patrol for both GT and UR schedules8 .
As shown in the table, the actual duration of a daily patrol was often different over the
21 days of the experiment, for both GT and UR schedules. For this reason, providing a
comparison normalized over the days of the experiment was impossible. However, most of
the days, we were able to collect data for multiples of 30 minutes (e.g., 60, 90 minutes).
Hence, to properly compare our results, we divided our data in 30 minutes segments. More
specifically, we considered all the train and station checks within a time window of 30
minutes and collected the data resulting from these actions9 . Having defined the data
points, we can now proceed to analyze our results.
Validation of the MDP model: As discussed at the beginning of this section Both GT
and UR schedules were calculated by solving an MDP. For this reason both schedules could
be updated to request a new schedule. Tables 3 and 4 then show, for each day of patrol,
the number of VOLUNTARY and INVOLUNTARY deviations requested by the officers.
In total, GT schedules were updated 18 times, all of which were involuntary deviations,
i.e., delays. All these update requests confirm that the MDP model was able to provide
schedules that could be updated whenever necessary.
All INVOLUNTARY deviations were due to the officers writing citations or helping
people. The average delay length was of 12 minutes (the largest delay was of 20 minutes).
In each case, as discussed at the beginning of this section, a new schedule was provided
starting at the officers current location and closest time. Finally, Table 4 shows that
voluntary deviations were used only with UR schedules. This result strongly suggests that
the officers were mostly satisfied with GT schedules. In addition, it means that GT schedules
8. As shown in Table 2, each day of patrol correspond to a 2-day test where GT schedules were tested on
the first day and UR schedules were tested on the second, both at identical times.
9. In so doing, the segments are also statistically independent. Within each segment the officers will check
different people who are unable to affect each other. Each segment corresponds to a sample of different
train riders taken at different times and locations. Not only do the officers never check the same rider
twice but most importantly, during 30 minutes, they will visit different locations by riding the trains
(roughly, one train every 6 minutes in the blue line) and inspecting the stations (on-station operations
last no longer than 20 minutes).

353

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

(a) Captures

(b) Warnings

(c) Violations

(d) Passengers

Figure 13: Results of the Fare Evasion tests
did not really compete against UR schedules only. Rather, the comparison was between UR
schedules which were augmented with real-time human intelligence for most of the time (8
out of 10 days). We discuss the results of such comparison next.
Game-Theory vs. Uniform Random: The results that we obtained are shown
in Figure 13 and in Table 5. Figure 13 shows eight boxplots depicting the data that
we collected during each patrol, using both GT and UR schedules. Respectively, the four
figures present data collected on captures (Figure 13(a)), warnings (Figure 13(b)), violations
(Figure 13(c)), and passengers checked (Figure 13(d)) per 30 minutes of patrolling10 . For
each boxplot, the top and bottom of the box represent the 75th and 25th percentiles,
respectively, while the middle line indicates the median of the collected data. The +
data points indicate statistical outliers, while the whiskers show the most extreme nonoutlier data points. Each of the four figures (captures, warnings, violations and passengers
checked) shows that the data collected using GT schedules had higher values than the
data collected using UR schedules. As shown in Table 5, on average, GT schedules led
to, respectively 15.52 captures, 10.42 warnings and 5.03 violations issued every 30 minutes
against, respectively against 9.55 captures, 6.48 warnings and 3.07 violations obtained using
UR schedules. To confirm the statistical significance of these results, we ran a number of
weighted unpaired student t-tests (p = 0.05) (Goldberg, Kercheval, & Lee, 2005; Bland &
Kerry, 1998) and verified, for each metric, that the difference in the results was statistically
significant. We used a weighted t-test because some data segments had a duration shorter
than 30 minutes and we wanted to use all the available data for our analysis. As shown
in Table 2, not all the patrol durations could be properly divided into a finite number of
30 minutes segments (e.g., UR: D6, D7, D8, D9 and GT: D6, D8, D11). Therefore, we
10. GT schedules also led to two arrests on day 6. This is why the patrol only lasted 10 minutes.

354

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

GT
UR

Days
11
10

avg. C
15.52
9.55

avg. W
10.42
6.48

avg. V
5.03
3.07

avg. P
96.77
60.85

Table 5: Average captures (C), warnings (W), violations (V) and passengers (P) based on
the results obtained in Figure 13

calculated a weighted average for each of the metric defined above, whereby each segment
was given a weight which was defined based on the segments duration (longer segments
corresponded to higher weights).
From a practical perspective, the magnitude of the difference between the two approaches is significant: cumulatively over a period of 21 days GT would capture a much
larger total number of fare evaders. This result can be emphasized even further if we correlate it with the results shown in Tables 4 and 3. While running UR schedules the officers
were requesting INVOLUNTARY deviations essentially every day, whereas no such deviations were requested while running GT schedules. In other words, they were using real-time
situation awareness to augment the quality of the schedules, thus making the UR schedule
more compelling.
The results in Table 5 also indicate that GT schedules led to 96.77 passengers checked
every 30 minutes against 60.85 passengers checked by using UR schedules. As discussed in
Section 5.1, GT schedules are generated by leveraging all the possible sequences of train
and station checks and by taking into account all the key dimensions discussed in Section
6.1, including the train schedules, the officers effectiveness and, most importantly the daily
ridership statistics. This means that stations or trains with a higher presence of riders will
be given a higher coverage probability since they are more likely to contain fare evaders.
Hence, these results confirm the accuracy of the model as both Figure 13(d) and Table 5
show that GT schedules led the officers to check more passengers than UR schedules.
This raises the question of whether a static type of schedule, which only deploys the
officers at the most crowded locations, would lead to similar or even better results than
those obtained with GT. Given the limited amount of time that we had to conduct our
experiments, we were unable to compare GT schedules against a static deployment  where
the key weakness is predictability in the longer term. Indeed, effective randomization was
one of the main reasons for LASD to collaborate on these experiments  security agencies
know that static schedules become predictable in the long term11 . After a certain amount
of time, the passengers would know where the officers are located and could exploit this
information to avoid paying the fare.

7. Summary and Future Work
This paper addressed dynamic execution uncertainty in security resources allocation. More
specifically, this paper presented four main contributions. First, we proposed a general
Bayesian Stackelberg game model for security patrolling whereby execution uncertainty is
11. Tambe (2011) discusses the benefits of randomization in detail.

355

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

handled via a Markov decision process (MDP). Second, we studied the problem of computing
a Stackelberg equilibrium (SSE) for this game and showed that by exploiting some structure
in the games utility functions, we could represent the defenders strategy space in a compact
form which could be efficiently solved using canonical algorithms for solving Bayesian SSGs.
In addition, we showed that it is always possible to generate a mixed strategy with a
polynomially-sized support. Third, we ran a number of simulations whereby we tested our
framework within various different settings. The key result is that by modeling execution
uncertainty as an MDP, we were able to generate policies that overcame the failures of
existing SSG algorithms which do not take such uncertainty into account. Finally, for
our fourth contribution, we ran a real-world experiment whereby we compared schedules
generated using our approach against competing schedules comprised of a random scheduler
augmented with officers providing real-time knowledge of the current situation. Our results
supported our MDP-based model because we were actually able to show the use of the
contingency plans provided by the MDP in the real-world. In addition, our results showed
that game-theoretic schedules outperformed the competing schedules, despite the fact that
the latter were improved with real-time knowledge. In so doing, these results constitute
one of the first examples of the potential of employing algorithmic game theory to solve
real-world security allocation problems.
In terms of future work, there exist a number of challenges left to address. One interesting technical challenge is that of addressing adjustable autonomy (Huber, 1999; Scerri,
Pynadath, & Tambe, 2002) or mixed initiative planning (Ferguson, Allen, & Miller, 1996)
in the context of game theoretic scheduling. Our experiments showed that the officers might
deviate from a schedule if they perceive that it might lead to a poor performance in terms of
fare evaders captured. Hence, it would be interesting to augment our schedules to take this
possibility into account. More specifically, we could extend the game theoretic model described in Section 3 to account for the possibility that the officers would eventually deviate
from the schedules and execute some action based on real-time situational awareness.
One interesting empirical challenge would be to run a long-term controlled experiment
(e.g., one year) complementary to the one we present in this paper. The idea would be to
measure how the riders will react to game-theoretic scheduling. As discussed in Section 6.3,
given the practical difficulties related to running real-world security experiments, security
agencies such as the LASD typically prefer to avoid running long term experiments because
they would interfere with the every-day security of the transit system. Nonetheless, if this
could be done, such an experiment would provide some very valuable insight on the effects
of SSGs in the real-world.

Acknowledgements
This article is the product of the joint work of Francesco M. Delle Fave and Albert X. Jiang.
Both of them are first authors of this work.
In terms of contributions, this article extends the paper by Jiang, Yin, Zhang, Tambe,
and Kraus (2013). In this work, we extend this initial version with the following contributions: (i) we present a new theoretical result, whereby we show how we can always calculate
an optimal mixed strategy for the defender with a polynomially-sized support; (ii) we extend the simulations presented by Jiang et al. (2013) by evaluating our approach against
356

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

a uniform random scheduler; (iii) we present results of a large scale real-world experiment
whereby we validate the MDP-based approach defined by Jiang et al. (2013) in the field; (iv)
in the same experiment, we compare the actual outcome of executing schedules generated
by the MDP-based approach against ones generated using a competing uniform random
scheduler  presenting some of the first results of algorithmic game theory in the field; (v)
to run these real-world experiments, we describe the development of a smartphone application to load and visualize game-theoretic schedules and a sampling technique to instantiate
key parameters of the MDP; (vi) we discuss significant new related work and future work.
We thank the Los Angeles Sheriffs Department for their exceptional collaboration. This
research is supported by TSA grant HSHQDC-10-A-BOA19, MURI grant W911NF-11-10332 and MOST 3-6797.

Appendix A
This appendix complements the simulations results discussed in Section 6.2, by presenting
the results obtained on the Blue, the Green and the Gold line. Figure 14 compares, for the
former three lines, the defenders revenue per rider obtained by the LP (Equation (8)), i.e.,
the upper bound, and the true value obtained by running 100000 Monte Carlo simulations
over the Markov strategy. The experiment was run assuming the same setting as discussed
at the beginning of Section 6.2: 3 hours of patrolling and 6 resources.
Figure 15 shows the simulation results complementing the ones presented in Section 6.2
for Hypothesis 1. The setting is the same as described in Section 6.2: 6 resources patrolling
the Blue, the Green and the Gold lines for 3 hours with  varying from 0% to 25% and the
delay time from 5 to 25 minutes.
Figure 16 shows the results complementing the ones presented in Section 6.2 for validating hypothesis 2. In this simulation, we considered 6 resources patrolling for 3 hours over
each of the four lines and varied uncertainty from 0% to 25%.
Figure 17 shows the results complementing the ones presented in Section 6.2 for validating hypothesis 4. In this simulation, we considered 3, 6 and 9 resources representing 3
levels of coverage, low, medium and high respectively. We then evaluated the revenue per
rider varying the delay probability from 0% to 25%.
Figure 18 shows the results complementing the ones presented in Section 6.2 for validating hypothesis 5. In this simulation, we considered different delay probabilities (0%, 10%
and 20%) and evaluated the revenue per rider varying the number of patrol units from 2 to
6.

357

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

(a) Blue line

(b) Green line

(c) Gold line

Figure 14: the Markov Strategy (Equation (17)) vs. the true LP (Equation (8)) for the
Blue, Green and Gold lines

358

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

(a) Blue line

(b) Green line

(c) Gold line

Figure 15: Defenders revenue per rider for the Blue, Green and Gold line, for varying
uncertainty and delay time.

359

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

(a) Blue line

(b) Green line

(c) Gold line

Figure 16: Fare evasion rate over the Blue, Green and Gold lines for varying delay probability

360

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

(a) Blue line

(b) Green line

(c) Gold line

Figure 17: Revenue per rider over the Blue, Green and Gold lines for different allocation of
resources

361

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

(a) Blue line

(b) Green line

(c) Gold line

Figure 18: Revenue per rider over the Blue, Green and Gold lines for three levels of delay
probability

362

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

References
Agmon, N., Kaminka, G. A., & Kraus, S. (2011). Multi-robot adversarial patrolling: facing
a full-knowledge opponent. Journal of Artificial Intelligence Research (JAIR), 42 (1),
887916.
Agmon, N., Kraus, S., & Kaminka, G. A. (2008a). Multi-robot perimeter patrol in adversarial settings. In Proceedings of the International Conference on Robotics and
Automation (ICRA), pp. 23392345. IEEE.
Agmon, N., Sadov, V., Kaminka, G., & Kraus, S. (2008b). The impact of adversarial
knowledge on adversarial planning in perimeter patrol. In Proceedings of the Seventh International Joint Conference on Autonomous Agents and Multiagent Systems
(AAMAS), pp. 5562.
An, B., Kempe, D., Kiekintveld, C., Shieh, E., Singh, S., & Tambe, M. (2012). Security
games with limited surveillance. In Proceedings of the AAAI Conference on Artificial
Intelligence (AAAI), pp. 12411247.
An, B., Tambe, M., Ordonez, F., Shieh, E., & Kiekintveld, C. (2011). Refinement of strong
Stackelberg equilibria in security games. In Proceedings of the Twenty-Fifth Conference for the Advancement of Artificial Intelligence (AAAI), pp. 587593.
Aoyagi, M. (1996). Reputation and dynamic Stackelberg leadership in infinitely repeated
games. Journal of Economic Theory, 71 (2), 378  393.
Archibald, C., & Shoham, Y. (2011). Hustling in repeated zero-sum games with imperfect
execution. In Proceedings of the Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI), pp. 3136.
Basilico, N., Gatti, N., & Amigoni, F. (2009a). Leader-follower strategies for robotic patrolling in environments with arbitrary topologies. In Proceedings of the Eight International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp.
5764.
Basilico, N., Gatti, N., Rossi, T., Ceppi, S., & Amigoni, F. (2009b). Extending algorithms
for mobile robot patrolling in the presence of adversaries to more realistic settings. In
Proceeding of the Conference of Intelligence Agent Technology (IAT), pp. 557564.
Basilico, N., Gatti, N., & Villa, F. (2010). Asynchronous multi-robot patrolling against intrusions in arbitrary topologies. In Proceeding of the Conference for the Advancement
of Artificial Intelligence (AAAI), pp. 345350.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving Transition Independent Decentralized Markov Decision Processes. JAIR.
Bland, M. J., & Kerry, S. M. (1998). Weighted comparison of means. BMJ: British Medical
Journal, 316, 125129.
Booz Allen Report (2007). Faregating analysis. Tech. rep., Booz Allen Hamilton Company. Report commissioned by the LA Metro, http://boardarchives.metro.net/
Items/2007/11_November/20071115EMACItem27.pdf.
Bowling, M., & Veloso, M. (2004). Existence of multiagent equilibria with limited agents.
Journal of Artificial Intelligence Research (JAIR), 22, 353384.
363

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

Brown, A., Camerer, C. F., & Lovallo, D. (2012). To review or not to review? limited strategic thinking at the movie box office. American Economic Journal: Microeconomics,
4 (2), 126.
Clarke, R. V. (1993). Fare evadion and automatic ticket collection on the london underground. Crime Prevention Studies, 1, 135146.
Clarke, R. V., Contre, S., & Petrossian, G. (2010). Deterrence and Fare Evasion: Results
of a Natural Experiment. Security Journal.
Conitzer, V. (2012). Computing game-theoretic solutions and applications to security. In
Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 21062112.
Conitzer, V., & Sandholm, T. (2006). Computing the optimal strategy to commit to. In
EC: Proceedings of the ACM Conference on Electronic Commerce.
Dickerson, J. P., Simari, G. I., Subrahmanian, V. S., & Kraus, S. (2010). A graph-theoretic
approach to protect static and moving targets from adversaries. In Proceedings of the
Ninth International Joint Conference on Autonomous Agents and Multiagent Systems,
pp. 299306.
Fang, F., Jiang, A., & Tambe, M. (2013). Protecting moving targets with multiple mobile
resources. Journal of Artificial Intelligence Research (JAIR), 48, 583634.
Ferguson, G., Allen, J., & Miller, B. (1996). Trains-95: Towards a mixed-initiative planning
assistant. In Proceedings of the 3rd Conference on Artificial Intelligence Planning
Systems (AIPS), pp. 7077.
Filar, J., & Vrieze, K. (1996). Competitive Markov Decision Processes. Springer.
Gatti, N. (2008). Game theoretical insights in strategic patrolling: Model and algorithm
in normal form. In Proceedings of the European Conference on Artificial Intelligence
(ECAI), pp. 403407.
Goldberg, L. R., Kercheval, A. N., & Lee, K. (2005). t-statistics for weighted means in
credit risk modeling. Journal of Risk Finance, 6 (4), 349365.
Grotschel, M., Lovasz, L., & Schrijver, A. (1981). The ellipsoid method and its consequences
in combinatorial optimization. Combinatorica, 1 (2), 169197.
Gruber, P. M. (2007). Convex and Discrete Geometry. Springer.
Huber, M. J. (1999). Considerations for flexible autonomy within bdi intelligent agent architectures. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
pp. 431438.
Jain, M., Kardes, E., Kiekintveld, C., Tambe, M., & Ordonez, F. (2010). Security games
with arbitrary schedules: A branch and price approach. In AAAI.
Jaulmes, R., Pineau, J., & Precup, D. (2007). A formal framework for robot learning and
control under model uncertainty. In Proceedings of the International Conference on
Robotics and Automation (ICRA), pp. 21042110. IEEE.
Jiang, A. X., Yin, Z., Zhang, C., Tambe, M., & Kraus, S. (2013). Game-theoretic randomization for security patrolling with dynamic execution uncertainty. In Proceedings of
the Twelfth International Conference on Autonomous Agents and Multiagent Systems,
pp. 207214.
364

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and
model selection. In Proceedings of the Fourteenth International Joint Conference on
Artificial Intelligence (IJCAI), pp. 11371143.
Korzhyk, D., Conitzer, V., & Parr, R. (2011a). Security games with multiple attacker
resources. In Proceedings of the Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI), pp. 273279.
Korzhyk, D., Conitzer, V., & Parr, R. (2011b). Solving stackelberg games with uncertain
observability. In Proceedings of the Tenth International Conference on Agents and
Multi-agent Systems (AAMAS), pp. 10131020.
Letchford, J., & Conitzer, V. (2013). Solving security games on graphs via marginal probabilities. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
pp. 591597.
Letchford, J., & Vorobeychik (2013). Optimal interdiction of attack plans. In Proceedings of
the Twelfth International Conference of Autonomous Agents and Multi-agent Systems
(AAMAS)., pp. 199206.
Letchford, J., & Conitzer, V. (2010). Computing optimal strategies to commit to in
extensive-form games. In Proceedings of the ACM Conference on Electronic Commerce (EC), pp. 8392.
Letchford, J., MacDermed, L., Conitzer, V., Parr, R., & Isbell, C. L. (2012). Computing
optimal strategies to commit to in stochastic games. In Proceedings of the AAAI
Conference on Artificial Intelligence.
Marecki, J., & Tambe, M. (2008). Towards faster planning with continuous resources in
stochastic domains. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), No. 10491055.
Mausam, Benazera, E., Brafman, R. I., Meuleau, N., & Hansen, E. A. (2005). Planning
with continuous resources in stochastic domains. In Proceedings of the Nineteenth
International Joint Conference on Artificial Intelligence, pp. 12441250.
Nguyen, T. H., Yang, R., Azaria, A., Kraus, S., & Tambe, M. (2013). Analyzing the
effectiveness of adversary modeling in security games. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI), pp. 718724.
Ostling, R., Wang, J., Tao-yi, J., Chou, E. Y., & Camerer, C. F. (2011). Testing game theory
in the field: Swedish lupi lottery games. American Economic Journal: Microeconomics,
3 (3), 133.
Paruchuri, P., Pearce, J. P., Marecki, J., Tambe, M., Ordonez, F., & Kraus, S. (2008a).
Playing games with security: An efficient exact algorithm for Bayesian Stackelberg
games. In AAMAS.
Paruchuri, P., Pearce, J. P., Marecki, J., Tambe, M., Ordonez, F., & Kraus., S. (2008b).
Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games. In Proceedings of the Seventh International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), pp. 539547.
365

fiDelle Fave, Jiang, Yin, Zhang, Kraus, & Tambe

Pita, J., Jain, M., Western, C., Portway, C., Tambe, M., Ordonez, F., Kraus, S., & Paruchuri,
P. (2008). Deployed ARMOR protection: The application of a game theroetic model
for security at the los angeles international airport. In Proceedings of the Seventh
Internation Conference on Autonomous Agents and Multi-Agent Systems (AAMAS).
Pita, J., Tambe, M., Kiekintveld, C., Cullen, S., & Steigerwald, E. (2011). GUARDS game theoretic security allocation on a national scale. In Proceedings of the Tenth
International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),
pp. 3744.
Scerri, P., Pynadath, D. V., & Tambe, M. (2002). Towards adjustable autonomy for the
real-world. Journal of Artificial Intelligence Research (JAIR), 17, 171228.
Shieh, E., An, B., Yang, R., Tambe, M., Baldwin, C., DiRenzo, J., Maule, B., & Meyer, G.
(2012). Protect: A deployed game theoretic system to protect the ports of the united
states. In AAMAS.
Sless, E., Agmon, N., & Kraus, S. (2014). The impact of adversarial knowledge on adversarial planning in perimeter patrol. In Proceedings of the Thirteenth International
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), p. In press.
Tambe, M. (2011). Security and Game Theory: Algorithms, Deployed Systems, Lessons
Learned. Cambridge University Press.
Tsai, J., Rathi, S., Kiekintveld, C., Ordonez, F., & Tambe, M. (2009). IRIS - a tool for
strategic security allocation in transportation networks. In Proceedings of the Eight
International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),
pp. 831839.
Vanek, O., Jakob, M., Lisy, V., Bosansky, B., & Pechoucek, M. (2011). Iterative gametheoretic route selection for hostile area transit and patrolling. In Proceedings of
the Tenth International Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS), pp. 12731274.
Varakantham, P., Lau, H. C., & Yuan, Z. (2013). Scalable randomized patrolling for securing
rapid transit networks. In Proceedings of the Conference for Innovative Applications
for Artificial Intelligence (IAAI), pp. 15631568.
Vorobeychik, Y., & Singh, S. (2012). Computing stackelberg equilibria in discounted
stochastic games. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), pp. 14781484.
Weidner, R. R. (1996). Target-hardening at a new york city subway station: Decreased fare
evasion at what price?. Crime Prevention Studies, 6, 117132.
Yang, R., Kiekintveld, C., Ordonez, F., Tambe, M., & John, R. (2011). Improving resource
allocation strategy against human adversaries in security games. In Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI), pp. 458464.
Yin, Z., Jiang, A., Johnson, M., Tambe, M., Kiekintveld, C., Leyton-Brown, K., Sandholm,
T., & Sullivan, J. (2012). Trusts: Scheduling randomized patrols for fare inspection
in transit systems. In Proceedings of the Conference on Innovative Applications for
Artificial Intelligence (IAAI), pp. 5972.
366

fiGame-Theoretic Security Patrolling with Dynamic Execution Uncertainty

Yin, Z., Jain, M., Tambe, M., & Ordonez, F. (2011). Risk-averse strategies for security
games with execution and observational uncertainty. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI), pp. 758763.
Yin, Z., & Tambe, M. (2012). A unified method for handling discrete and continuous uncertainty in Bayesian stackelberg games. In Proceedings of the Eleventh International
Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 234242.

367

fiJournal of Artificial Intelligence Research 50 (2014)

Submitted 10/13; published 05/14

A Decision-Theoretic Model of Assistance
Alan Fern

AFERN @ EECS . OREGONSTATE . EDU

School of EECS, Oregon State University, Corvallis, OR USA

Sriraam Natarajan

NATARASR @ INDIANA . EDU

SoIC, Indiana University, Bloomington, IN USA

Kshitij Judah

JUDAHK @ EECS . OREGONSTATE . EDU

School of EECS, Oregon State University, Corvallis, OR USA

Prasad Tadepalli

TADEPALL @ EECS . OREGONSTATE . EDU

School of EECS, Oregon State University, Corvallis, OR USA

Abstract
There is a growing interest in intelligent assistants for a variety of applications from sorting
email to helping people with disabilities to do their daily chores. In this paper, we formulate the
problem of intelligent assistance in a decision-theoretic framework, and present both theoretical
and empirical results. We first introduce a class of POMDPs called hidden-goal MDPs (HGMDPs),
which formalizes the problem of interactively assisting an agent whose goal is hidden and whose
actions are observable. In spite of its restricted nature, we show that optimal action selection
for HGMDPs is PSPACE-complete even for deterministic dynamics. We then introduce a more
restricted model called helper action MDPs (HAMDPs), which are sufficient for modeling many
real-world problems. We show classes of HAMDPs for which efficient algorithms are possible.
More interestingly, for general HAMDPs we show that a simple myopic policy achieves a near
optimal regret, compared to an oracle assistant that knows the agents goal. We then introduce
more sophisticated versions of this policy for the general case of HGMDPs that we combine with
a novel approach for quickly learning about the agent being assisted. We evaluate our approach
in two game-like computer environments where human subjects perform tasks, and in a real-world
domain of providing assistance during folder navigation in a computer desktop environment. The
results show that in all three domains the framework results in an assistant that substantially reduces
user effort with only modest computation.

1. Introduction
Personalized AI systems that interactively assist their human users have received significant attention in recent years (Yorke-Smith, Saadati, Myers, & Morley, 2012; Lieberman, 2009; Myers, Berry,
Blythe, Conleyn, Gervasio, McGuinness, Morley, Pfeffer, Pollack, & Tambe, 2007). However, an
overarching formal framework of interactive assistance that captures all these different systems and
provides a theoretical foundation is largely missing. In this paper we address this lacuna by introducing a general framework of decision-theoretic assistance, analyzing the problem complexity under
different assumptons, proposing different heuristic solutions, and evaluating their effectiveness.
We consider a model where the assistant observes a goal-oriented agent and must select assistive
actions in order to best help the agent achieve its goals. In real applications, this requires that the
assistant be able to handle uncertainty about the environment and the agent, to reason about varying
c
2014
AI Access Foundation. All rights reserved.

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

action costs, to handle unforeseen situations, and to adapt to the agent over time. Here we consider
a decision-theoretic model, based on partially observable Markov decision processes (POMDPs),
which naturally handles these features, providing a formal basis for designing intelligent assistants.
The first contribution of this work is to formulate the problem of selecting assistive actions as
a class of partially observable Markov decision processes (POMDPs) called Hidden Goal MDPs
(HGMDPs), which jointly models the application environment along with the agents policy and
hidden goals. A key feature of this approach is that it explicitly reasons about the environment and
the agent, which provides the potential flexibility for assisting in ways unforeseen by the developer
as new situations are encountered. Thus, the developer need not design a hand-coded assistive
policy for each preconceived application scenario. Instead, when using our framework, the burden
on the developer is to provide a model of the application domain and the agent, or alternatively a
mechanism for learning one or both of these models from experience. Our framework then uses
those models in an attempt to compute, in any situation, whether assistance could be beneficial and
if so what assistive action to select.
The second contribution of this work is to analyze the properties of this formulation. Despite
the restricted nature of HGMDPs, the complexity of determining if an HGMDP has a finite-horizon
policy of a given value is PSPACE-complete even in deterministic environments. This motivates a
more restricted model called Helper Action MDP (HAMDP), where the assistant executes a helper
action at each step. The agent is obliged to accept the helper action if it is helpful for its goal and
receives a reward bonus (or cost reduction) by doing so. Otherwise, the agent can continue with
its own preferred action without any reward or penalty to the assistant. We show classes of this
problem that are complete for PSPACE and NP. We also show that for the class of HAMDPs with
deterministic agents there are polynomial time algorithms for minimizing the expected and worstcase regret relative to an oracle assistant that knows the goal of the agent. Further, we show that the
optimal worst case regret can be characterized by a graph theoretic property called the tree rank of
the corresponding all-goals policy tree and can be computed in linear time.
In principle, given a HGMDP, one could apply a POMDP solver in order to arrive at an optimal
assistant policy. Unfortunately, the relatively poor scalability of POMDP solvers will often force us
to utilize approximate/heuristic solutions. This is particularly true when the assistant is continually
learning updated models of the agent and/or environment, which results in a sequence of more
accurate HGMDPs, each of which needs to be solved. A third contribution of our work is a set
of myopic action selection mecahnisms that approximate the optimal policy. For HAMDPs, we
analyze a myopic heuristic and show that it has a regret which is upper bounded by the entropy
of the goal distribution for HAMDPs. Furthermore we give a variant of this policy that is able
to achieve worst-case and expected regret that is logarithmic in the number of goals without any
prior knowledge of the goal distribution. We also describe two other approaches that are based
on a combination of explicit goal estimation, myopic heuristics, and bounded search and are more
generally applicable to HGMDPs.
In order for the above approach to be useful, the HGMDP must incorporate a reasonably accurate model of the agent being assisted. A fourth contribution of our work is to describe a novel
model-based bootstrapping mechanism for quickly learning the agent policy, which is important
for the usability of an assistant early in its lifetime. The main idea is to assume that that agent is
close to rational in the decision-theoretic sense, which motivates defining a prior on agent policies
that places higher probability on policies that are closer to optimal. This prior in combination with

72

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

Bayesian updates allows for the agent model to be learned quickly when the rationality assumption
is approximately satisfied.
The final contribution of our work is to evaluate our framework in three domains. First we
consider two game-like computer environments with human subjects. The results in these domains
show that the assistants resulting from our framework substantially reduce the amount of work
performed by the human subjects. We also consider a more realistic domain, the folder navigator
(Bao, Herlocker, & Dietterich, 2006) of the Task Tracer project. In this domain, the user navigates
the directory structure searching for a particular location to open or save a file, which is unknown to
the assistant. The job of the assistant is to predict the users destination folder and take actions that
provide short cuts to reach it. The results show that our generic assistant framework compares
favorably to the hand-coded solution of Bao et al.
The remainder of this paper is organized as follows. In the next section, we introduce our formal problem setup in terms of HGMDPs, followed by an analysis of computational complexity and
guarantees for myopic heuristics in the case of HAMDPs. Next, we present our approximate solution approach for general HGMDPs based on goal estimation and online action selection. Finally
we give an empirical evaluation of the approach in three domains and conclude with a discussion of
related and future work.

2. Hidden Goal Markov Decision Processes
Throughout the paper we will refer to the entity that we are attempting to assist as the agent and the
assisting entity as the assistant. We consider an episodic problem setting where at the beginning of
each episode the agent begins in some world state and selects a goal from a finite set of possible
goals. The goal set, for example, might contain all possible dishes that the agent might be interested
in cooking, or all the possible destination folders that the agent may possibly navigate to. Importantly, the assistant can fully observe the world state and the agents actions, but cannot observe the
goal of the agent. We model the interaction between the agent and assistant as sequential where
the agent and assistant alternate turns, taking a single action per turn (possibly noop).1 The episode
ends when either the agents or the assistants action leads to a goal state. An immediate reward
is accumulated after each action during the episode. The total reward of an episode is equal to the
sum of the rewards obtained during the episode. Note that the available actions for the agent and
assistant need not be the same and may have varying rewards. Since the assistant and the agent
share the same objective, all rewards are viewed from the perspective of the agent. The objective of
the assistant is to behave in a way that maximizes the expected total reward of an episode.
More formally, we model the above interaction via Hidden Goal Markov Decision Processes
(HGMDPs). An HGMDP is an MDP in which the goal of the user is not observed while the rest
of the environment is completely observed. An HGMDP is a tuple hS, G, A, A0 , T, R, , IS , IG i
where S is a set of states, G is a finite set of possible agent goals, A is the set of agent actions, and
A0 is the set of assistant actions. Typically A0 will include a noop action, which allows the assistant
to decide not to provide any assistance at a particular decision epoch. T is the transition function
where T (s, g, a, s0 ) is the probability of a transition to state s0 from s after taking action a  A  A0
when the agents goal is g. R is the reward function which maps S  G  (A  A0 ) to real values. 
1. We consider this strictly alternating turn model for simplicity. However, it is straightforward to use this model to
capture interactions that are not strictly alternating, e.g. allowing the assistant or agent to take multiple actions in a
row.

73

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

is the agents policy that maps S  G to distributions over A and need not be optimal in any sense.
IS (IG ) is an initial state (goal) distribution.
An assistant policy for an HGMDP defines a distribution over actions given the sequence of
preceding observations, i.e., the sequence of state action pairs. It is important that the assistant policy depends on the history rather than only the current state since the entire history can potentially
provide evidence about the goal of the agent, which may be necessary for selecting an appropriate
action. We must now define an objective function that will be used to evaluate the value of a particular assistant policy. We consider a finite-horizon episodic problem setting, where each HGMDP
episode begins by drawing an initial state s  IS and a goal g  IG . The process then alternates
between the agent and the assistant executing actions (including noops) in the environment until the
horizon or a terminal state is reached. The agent is assumed to select actions according to . In
many domains, a terminal goal state will be reached within the horizon, though in general, goals
can have arbitrary impact on the reward function. The reward for the episode is equal to the sum
of the rewards of the actions executed by the agent and assistant during the episode. The objective
of the assistant is to reason about the HGMDP and observed state-action history in order to select
actions that maximize the expected (or worst-case) total reward of an episode.
Before proceeding further it is worth reviewing some of the assumptions of the above formulation and the potential implications.
 Partial Observability: The definition of the HGMDP is very similar to the definition of a
Partially Observable Markov Decision Process (POMDP). In fact, the HGMDP is a special
case of the POMDP where the unobserved part of the state space consists of a single component which corresponds to the goal of the user. For simplicity we have assumed that the
world state is fully observable. This choice is not fundamental to our framework and one can
imagine relatively straightforward extensions of our techniques that model the environment
as a partially observable MDP (POMDP) where the world states are not fully observable. In
Section 3, we shed some light on the hardness of HGMDPs and describe some specialized
heuristic solutions with performance guarantees.
 Agent Policy: We have also assumed that the agent is modeled as a memoryless/reactive
policy that gives a distribution over actions conditioned on only the current world state and
goal. This assumption is also not fundamental to our framework and one can also extend
it to include more complex models of the user, for example, that include hierarchical goal
structures. Such an extension has been explored previously (Natarajan, Tadepalli, & Fern,
2007).
 Sequential Interaction: We have also assumed for simplicity an interaction model between
the assistant and agent that involves interleaved, sequential actions rather than parallel actions.
This, for example, precludes the assistant from taking actions in parallel with the agent. While
parallel assistive actions are useful in many cases, there are many domains where sequential
actions are the norm. We are especially motivated by domains such as intelligent desktop
assistants that help store and retrieve files, filter spam, sort email, etc., and smart homes
that open doors, switch on appliances, and so on. Many opportunities for assistance in these
domains are of the sequential variety. In many cases, tasks that appear to require parallel
activity can often be formulated as a set of threads where each thread is sequential and hence

74

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

can be formulated as a separate assistant. Extending our framework to handle general parallel
assistance is an interesting future direction.
 Goal-Dependent Transitions: The dependence of the reward and policy on the goal allows
the model to capture the agents desires and behavior under each goal. The dependence of T
on the goal is less intuitive and in many cases there will be no dependence when T is used
only to model the dynamics of the environment. However, we allow goal dependence of T
for generality of modeling. For example, it can be convenient to model basic communication
actions of the agent as changing aspects of the state, and the result of such actions will often
be goal dependent.
There are two main obstacles to solving the problem of intelligent assistance in our framework.
First, in many scenarios, initially the HGMDP will not be directly at our disposal since we will lack
accurate information about the agent policy  and/or the goal distribution IG . This is often due to
the fact that the assistant will be deployed for a variety of initially unknown agents. Rather, the assistant will find itself in an environment and be only given the possible set of goals. As described in
Section 5, our approach to this difficulty is to learn an approximate HGMDP by estimating the agent
policy  and the goal distribution IG . Furthermore, we will also describe a bootstrapping mechanism for learning these approximations quickly. The second obstacle to solving the HGMDP is the
generally high computational complexity of solving HGMDPs. To deal with this issue Section 4
considers various approximate techniques for efficiently solving the HGMDPs.
It should be mentioned that it is possible to provide assistance using simpler domain-specific
engineered solutions. In particular, the domains we consider later could be solved using several less
expensive solutions and do not require the machinery of HGMDPs. In fact, in one of our domains,
we compare our model against an existing supervised learning method. The goal of our work is
to provide a domain-independent framework that can potentially encapsulate several such assistant
systems with the hope that it gives rise to a robust understanding and methodology of building such
systems with much less human effort in the future.

3. Theoretical Analysis
In this section, we will analyze the hardness of solving HGMDPs, and show that despite being a
special case of POMDPs, they are just as hard. This motivates a new model called the Helper-Action
MDP (HAMDP) which is more restricted and is more amenable to approximate solutions. We will
then introduce a myopic heuristic to solve HAMDPs and analyze its performance. We will also
analyze some special cases of HAMDPs which permit more efficient solutions.
3.1 Complexity Results on Hidden Goal MDPs
Given knowledge of the agents goal g in an HGMDP, the assistants problem reduces to solving an
MDP over assistant actions. The MDP transition function captures both the state change due to the
assistant action and also the ensuing state change due to the agent action selected according to 
given g. Likewise the reward function on a transition captures the reward due to the assistant action
and the ensuing agent action conditioned on g. The optimal policy for this MDP corresponds to an
optimal assistant policy for g. However, since the real assistant will often have uncertainty about
the agents goal, it is unlikely that this optimal performance will be achieved.

75

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

We can view an HGMDP as a collection of |G| MDPs that share the same state space, where
the assistant is placed in one of the MDPs at the beginning of each episode, but cannot observe
which one. Each MDP is the result of fixing the goal component of the HGMDP definition to one
of the goals. This collection can be easily modeled as a restricted type of partially observable MDP
(POMDP) with a state space S  G. The S component is completely observable, while the G component is unobservable but only changes at the beginning of each episode (according to IG ) and
remains constant throughout an episode. Furthermore, each POMDP transition provides observations of the agent action, which gives direct evidence about the unchanging G component. From
this perspective HGMDPs appear to be a significant restriction over general POMDPs. However,
our first result shows that despite this restriction the worst-case complexity is not reduced even for
deterministic dynamics.
Given an HGMDP M , a horizon m = O(|M |), and a reward target r , the short-term reward
maximization problem asks whether there exists a history-dependent assistant policy that achieves
an expected finite horizon reward of at least r . For general POMDPs this problem is PSPACEcomplete (Papadimitriou & Tsitsiklis, 1987; Mundhenk, 2001), and for POMDPs with deterministic
dynamics, it is NP-complete (Littman, 1996). However, we have the following result.
Theorem 1. Short-term reward maximization for HGMDPs with deterministic dynamics is PSPACEcomplete.
Proof. Membership in PSPACE follows from the fact that any HGMDP can be polynomially encoded as a POMDP for which policy existence is in PSPACE. To show PSPACE-hardness, we
reduce the PSPACE-complete problem TQBF (truth of quantified Boolean formula) to the problem
of the existence of a history-dependent assistant policy of expected reward  r.
Let  be a quantified Boolean formula in the form x1 x2 x3 . . . xn {C1 (x1 , . . . , xn )  . . .
 Cm (x1 , . . . , xn )}, where each Ci is a disjunctive clause. For us, each goal gi is a clause, The
agent chooses a goal uniformly randomly from the set of goals formed from  and hides it from the
assistant. The states consist of pairs of the form (v, i), where v  {0, 1} is the current value of the
goal clause, and i is the next variable to set. The actions of the assistant are to set the existentially
quantified variables. The agent simulates setting the universally quantified variables by choosing
actions from the set {0, 1} with equal probability. The episode terminates when all the variables are
set, and the assistant gets a reward of 1 if the value of the clause is 1 at the end and a reward of 0
otherwise.
Note that the assistant does not get any useful informtion about the goal until the termination
of the episode. If  is satisfiable, then there is an assistant policy that leads to a reward of 1
over all goals and all choices of agent actions, and hence has an expected value of 1 over the goal
distribution. If not, then at least one of the goals will not be satisfied for some setting of the universal
quantifiers, leading to an expected value < 1. Hence the TQBF problem reduces to deciding if a
HGMDP has a policy of expected reward  1.
This result shows that any POMDP can be encoded as an HGMDP with deterministic dynamics, where the stochastic dynamics of the POMDP are captured via the stochastic agent policy in the
HGMDP. However, the HGMDPs resulting from the PSPACE-hardness reduction are quite pathological compared to those that are likely to arise in realistic assistant domains. Most importantly, the
agents actions provide practically no information about the agents goal until the end of an episode,
when it is too late to exploit the knowledge. This suggests that we search for restricted classes of
HGMDPs that will allow for efficient solutions with performance guarantees.
76

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

3.2 Helper Action MDPs
The motivation for Helper Action MDPs (HAMDPs) is to place restrictions on the agent and assistant that avoid the following three complexities that arise in general HGMDPs: 1) the agent can
behave arbitrarily poorly if left unassisted and as such the agent actions may not provide significant evidence about the goal; 2) the agent is free to effectively ignore the assistants help and not
exploit the results of assistive action, even when doing so would be beneficial; and 3) the assistant
actions have the possibility of negatively impacting the agent compared to not having an assistant.
HAMDPs will address the first issue by assuming that the agent is competent at (approximately)
maximizing reward without the assistant. The second and the third issues will be addressed by assuming that the agent will always detect and exploit helpful actions and that the assistant actions
never hurt the agent even when they are unhelpful.
Informally, the HAMDP provides the assistant with a helper action for each of the agents actions. Whenever a helper action h is executed directly before the corresponding agent action a,
the agent receives a bonus reward of 1. However, the agent will only accept the helper action h
(by taking a) and hence receive the bonus, if a is an action that the agent considers to be good for
achieving the goal without the assistant. Thus, the primary objective of the assistant in an HAMDP
is to maximize the number of helper actions that get accepted by the agent.2
While simple, this model captures much of the essence of assistive domains where assistant
actions cause minimal harm and the agent is able to detect and accept good assistance when it
arises.
An HAMDP is an HGMDP hS, G, A, A0 , T, R, , IS , IG i with the following constraints:
 The agent and assistant actions sets are A = {a1 , . . . , an } and A0 = {h1 , . . . , hn }, so that for
each ai there is a corresponding helper action hi .
 The state space is S = W  (W  A0 ), where W is a set of world states. States in W  A0
encode the current world state and the previous assistant action.
 The reward function R is 0 for all assistant actions. For agent actions the reward is zero
unless the agent selects the action ai in state (s, hi ) which gives a reward of 1. That is, the
agent receives a bonus of 1 whenever it takes an action directly after the corresponding helper
action.
 The assistant always acts from states in W , and T is such that taking hi in s deterministically
transitions to (s, hi ).
 The agent always acts from states in S  A0 , resulting in states in S according to a transition function that does not depend on hi , i.e. T ((s, hi ), g, ai , s0 ) = T 0 (s, g, ai , s0 ) for some
transition function T 0 .
 Finally, for the agent policy, let (s, g) be a function that returns a set of actions and P (s, g)
be a distribution over those actions. We will view (s, g) as the set of actions that the agent
2. Note that here we have assumed all bonus rewards are 1 for simplicity. Most of our results below are easily extended
to non-uniform positive bonus rewards. In particular for our main result concerning bounding the regret of the
myopic policy, the analogous result simply includes a constant factor equal to the maximum possible reward bonus.
The exceptions are Theorems 7 and 8, which we do not currently have analogous results for non-uniform reward
bonuses.

77

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

considers acceptable (or equally good) in state s for goal g. The agent policy  always selects
ai after its helper action hi whenever ai is acceptable. That is, ((s, hi ), g) = ai whenever
ai  (s, g). Otherwise the agent draws an action from P (s, g).
In a HAMDP, the primary impact of an assistant action is to influence the reward of the following
agent action. Also notice that HAMDPs do not have rewards that are inherent to the underlying
environment. Rather, the only rewards are the bonuses received whenever the agent accepts a helper
action. While we could have defined the model to include environmental reward in addition to
helper bonuses, this unnecessarily complicates the model (as the following hardness result shows).
Instead, we assume that the inherent environmental reward is already captured by the agent policy
via (s, g), which is considered to contain actions that approximately optimize this reward.
As an example, the HAMDP model captures both the doorman domain, and the desktop domain in our experiments. In the doorman domain, the helper actions correspond to opening doors
for the agent, which reduces the cost of navigating from one room to another. In the desktop domain, the helper actions correspond to offering shortcuts to a users destination folders. Importantly
opening an incorrect door or offering an incorrect shortcut does not increase the (physical) cost to
the agent over having no assistant at all, which is a key property of HAMDPs.
Despite the apparent simplification of HAMDPs over HGMDPs, it turns out that somewhat
surprisingly the worst case computational complexity is not reduced.
Theorem 2. Short-term reward maximization for HAMDPs is PSPACE-complete.
Proof. Membership in PSPACE follows easily since HAMDPs are a specialization of HGMDPs.
The proof of PSPACE-hardness is identical to that of Theorem 1 except that here, instead of the
agents actions, the stochastic environment models the universal quantifiers. The agent accepts
all actions until the last one and sets the variable as suggested by the assistant. After each of the
assistants actions, the environment chooses a value for the universally quantified variable with equal
probability. The last action is accepted by the agent if the goal clause evaluates to 1, otherwise not.
There is a history-dependent policy whose expected reward is greater than or equal to the number
of existential variables if and only if the quantified Boolean formula is satisfiable.
Unlike the case of HGMDPs, the stochastic dynamics are essential for PSPACE-hardness as will be
shown in a later section. Despite this negative result, the following sections demonstrate the utility of the HAMDP restriction by giving performance guarantees for simple policies and improved
complexity results. So far, there are no analogous results for general HGMDPs.
3.3 Myopic Heuristic Analysis
HAMDPs are closely related to the sequential prediction framework of Littlestone (1988). In this
framework, in each round the learner is shown a new instance for which it predicts a binary label. If
the prediction is incorrect, a mistake is made, and the learner is given the true label. In the realizable
setting, the true labels are determined by a hypothesis in a target class. An optimal prediction
algorithm minimizes the upper bound on the number of mistakes over all possible hypotheses. We
can view the helper action in HAMDP as a prediction of the user action. Maximizing the bonus
reward in HAMDP is equivalent to minimizing the number of mistakes in sequential prediction.
Unlike sequential prediction, here the predictions (actions) need not be binary. While in sequential
prediction the sequence of states are arbitrarily chosen, here we assume that they are generated by
78

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

a Markov process. In spite of these differences, some of the results of sequential prediction can be
adapted to HAMDPs albeit using a different terminology. However, we derive all our results from
first principles for consistency.
Given an assistant policy  0 , the regret of a particular episode is the extra reward that an oracle
assistant with knowledge of the goal would achieve over  0 . For HAMDPs the oracle assistant can
always achieve a reward equal to the finite horizon m, because it can always select a helper action
that will be accepted by the agent. Thus, the regret of an execution of  0 in a HAMDP is equal to
the number of helper actions that are not accepted by the agent, which we will call mispredictions.
From above we know that optimizing regret is PSPACE-hard and thus here we focus on bounding
the expected and worst-case regret of the assistant. We now introduce our first myopic heuristic and
show that it is able to achieve regret bounds that are logarithmic in the number of goals.
Coarsened Posterior Heuristic. Intuitively, our myopic heuristic will select an action that has
the highest probability of being accepted with respect to a coarsened version of the posterior
distribution over goals. The myopic policy in state s given history H is based on the consistent goal
set C(H), which is the set of goals that have non-zero probability with respect to history H. It is
straightforward to maintain C(H) after each observation (observations include the world state and
the agents actions). The myopic policy is defined as:
(s, H) = arg max IG (C(H)  G(s, a))
a

where G(s, a) = {g | a  (s, g)} is the set of goals for which the agent considers a to be an
acceptable action in state s. The expression IG (C(H)  G(s, a)) can be viewed as the probability
mass of G(s, a) under a coarsened goal posterior which assigns goals outside of C(H) probability
zero and otherwise weighs them proportional to the prior.
Theorem 3. For any HAMDP the expected regret of the coarsened posterior heuristic is bounded
above by the entropy of the goal distribution H(IG ).
Proof. The main idea of the proof is to show that after each misprediction of the myopic policy (i.e.
the selected helper action is not accepted by the agent) the uncertainty about the goal is reduced by
a constant factor, which will allow us to bound the total number of mispredictions on any trajectory.
Consider a misprediction step where the coarsened posterior heuristic selects helper action hi in
state s given history H, but the agent does not accept the action and instead selects a 6= ai . By the
definition of the myopic policy we know that IG (C(H)  G(s, ai ))  IG (C(H)  G(s, a )), since
otherwise the assistant would not have chosen hi . From this fact we now argue that IG (C(H 0 )) 
IG (C(H))/2 where H 0 is the history after the misprediction. That is, the probability mass under
IG of the consistent goal set after the misprediction is less than half that of the consistent goal set
before the misprediction. To show this we will consider two cases: 1) IG (C(H)  G(s, ai )) <
IG (C(H))/2, and 2) IG (C(H)  G(s, ai ))  IG (C(H))/2. In the first case, we immediately get
that IG (C(H)  G(s, a )) < IG (C(H))/2. Combining this with the fact that C(H 0 )  C(H) 
G(s, a ) we get the desired result that IG (C(H 0 ))  IG (C(H))/2. In the second case, note that
C(H 0 )  C(H)  (G(s, a )  G(s, ai ))
 C(H)  (C(H)  G(s, ai ))
Combining this with our assumption for the second case immediately implies that IG (C(H 0 )) 
IG (C(H))/2.
79

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

The above shows that if a misprediction is made between histories H and H 0 then IG (C(H 0 )) 
IG (C(H))/2. This implies that for any episode, after n mispredictions resulting in a history Hn ,
IG (C(Hn ))  2n . Now consider an arbitrary episode where the true goal is g. We know that
IG (g) is a lower bound on IG (C(Hn )), which implies that IG (g)  2n or equivalently that n 
 log(IG (g)). Thus for any episode with goal g the maximum number of mistakes is bounded by
 log(IG (g)). Using this fact we get that thePexpected number of mispredictions during an episode
with respect to IG is bounded above by  g IG (g) log(IG (g)) = H(IG ), which completes the
proof.
Since H(IG )  log(|G|), this result implies that for HAMDPs the expected regret of the myopic
policy is no more than logarithmic in the number of goals. Furthermore, as the uncertainty about
the goal decreases (decreasing H(IG )) the regret bound improves until we get a regret of 0 when IG
puts all mass on a single goal. It turns out that this logarithmic bound is asymptotically tight in the
worst case.
Theorem 4. There exists a HAMDP such that for any assistant policy the expected regret is at least
log(|G|)/2.
Proof. Consider a deterministic HAMDP such that the environment is structured as a binary tree
of depth log(|G|), where each leaf corresponds to one of the |G| goals. By considering a uniform
goal distribution it is easy to verify that at any node in the tree there is an equal chance that the true
goal is in the left or right sub-tree during any episode. Thus, any policy will have a 0.5 chance of
committing a misprediction at each step of an episode. Since each episode is of length log(|G|), the
expected regret of an episode for any policy is log(|G|)/2.
Resolving the gap between the myopic policy bound and this regret lower bound is an open problem.
3.3.1 A PPROXIMATE G OAL D ISTRIBUTIONS .
0 instead of the true underlying
Suppose that the assistant uses an approximate goal distribution IG
goal distribution IG when computing the myopic policy. That is, the assistant selects actions that
0 (C(H)  G(s, a)), which we will refer to as the myopic policy relative to I 0 . The
maximize IG
G
0 instead of I can be bounded in terms of the KullbackLeibler (KL) diverextra regret for using IG
G
0 ), which is zero when I 0
gence (Kullback & Leibler, 1951) between these distributions KL(IG k IG
G
equals IG .

Theorem 5. For any HAMDP with goal distribution IG , the expected regret of the myopic policy
0 is bounded above by H(I ) + KL(I k I 0 ).
with respect to distribution IG
G
G
G
Proof. The proof is similar to that of Theorem 3, except that since the myopic policy is with respect
0 rather than I , we derive that, on any episode, the maximum number of mispredictions n is
to IG
G

80

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

0 (g)). Using this fact, the average number of mispredictions is given by:
bounded above by  log(IG

P

g

P

g

P

g

=

1
)=
IG (g) log( 0
IG (g)


1
IG (g) log( 0
) + log(IG (g))  log(IG (g)) =
IG (g)
X
IG (g)
IG (g) log(IG (g))
IG (g) log( 0
)
IG (g)
g
0
H(IG ) + KL(IG k IG
).

Note that for any random variable X with distribution P over a finite domain of size N , we
have KL(P k U ) = log(N )  H(P ), where U is the uniform distribution. Thus, a consequence of
Theorem 5 is that the myopic policy with respect to the uniform goal distribution has expected regret
bounded by log(|G|) for any HAMDP, showing that logarithmic regret can be achieved without
knowledge of IG . This can be strengthened to hold for worst case regret.
Theorem 6. For any HAMDP, the worst case and hence expected regret of the myopic policy with
with respect to the uniform goal distribution is bounded above by log(|G|).
Proof. The proof of Theorem 5 shows that the number of mispredictions on any episode is bounded
0 ). In our case I 0 = 1/|G| which shows a worst case regret bound of log(|G|).
above by  log(IG
G
This immediately implies that the expected regret bound of the uniform myopic policy is bounded
by log(|G|).
3.4 Deterministic Agent Policies
We now consider several special cases of HAMDPs. First, we restrict the agents policy to be
deterministic for each goal, i.e. (s, g) has at most a single action for each state-goal pair (s, g).
Theorem 7. The myopic policy achieves the optimal expected reward for HAMDPs with deterministic agent policies.
The proof is given in the Appendix. It is sometimes desirable to minimize the worst possible regret
compared to an oracle assistant who knows the agents goal. As we show below, this can be captured
by a graph-theoretic notion of tree rank that generalizes the rank of decision trees (Ehrenfeucht &
Haussler, 1989).
Definition 1. The rank of a rooted tree is the rank of its root node. If a node is a leaf node then
rank(node) = 0, else if a node has at least two distinct children c1 and c2 with equal highest ranks
among all children, then rank(node) = 1+ rank(c1 ). Otherwise rank(node) = the rank of the highest
ranked child.
The optimal trajectory tree (OTT) of a HAMDP in deterministic environments is a tree where
the nodes represent the states of the HAMDP reached by the prefixes of optimal action sequences
for different goals starting from the initial state.3 Each node in the tree represents a state and a set of
3. If there are multiple initial states, we build an OTT for each initial state. Then the rank would be the maximum of the
ranks of all trees.

81

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

goals for which it is on the optimal path from the initial state. Since the agent policy is deterministic,
there is at most one trajectory per goal in the tree. Hence the size of the optimal trajectory tree is
bounded by the number of goals times the maximum length of any trajectory, which is at most the
size of the state space in deterministic domains. The following Lemma follows by induction on the
depth of the optimal trajectory tree. The proof is in the Appendix.
Lemma 1. The minimum worst-case regret of any policy for an HAMDP for deterministic environments and deterministic agent policies is equal to the tree rank of its optimal trajectory tree.
This leads to the following.
Theorem 8. If the agent policy is deterministic, the problem of minimizing the maximum regret in
HAMDPs in deterministic environments is in P.
Proof. We first construct the optimal trajectory tree. We then compute its rank in linear time while
simultaneously computing the optimal minimax policy using the recursive definition of tree rank.
The result then follows from Lemma 1.
3.5 Bounded Branching Factor Policies
The assumption of a deterministic agent policy may be too restrictive in many domains. We now
consider agent policies which may have a constant number of possible actions in (s, g) for each
state-goal pair as defined below.
Definition 2. The branching factor of a HAMDP is the largest number of possible actions in (s, g)
by the agent in any state for any goal and any assistants action.
The Doorman domain of Section 6.1 has a branching factor of 2 since there are at most two optimal
actions to reach any goal from any state.
Theorem 9. Minimizing the worst-case regret in finite horizon HAMDPS in deterministic environments with a constant branching factor k is NP-complete.
The proof is in the appendix. We can also show that minimizing the expected regret for a bounded
k is NP-hard. We conjecture that this problem is also in NP, but this question remains open.

4. Solving Practical HGMDPs
Although HAMDPs offer a theoretically elegant framework, the requirements of practical assistant
systems are not easily satisfed by its assumptions. In this section, we consider the more general
problem of solving HGMDPs and offer some practical heuristic solutions that are inspired by our
theoretical analysis.
In principle we could use a general purpose POMDP solver to solve HGMDPs. While the
POMDP solvers based on point-based methods and search-based methods have become more efficient over the years, they are still too inefficient to be used in an interactive setting, where the
parameters of the POMDP are continually being updated. Moreover, as the analysis in the previous
section suggests, simple myopic heuristics based on the knowledge of the goal distribution and the
optimal policies given the goals appear promising to yield respectable performance. For this reason,
we adopt an approach based on Bayesian goal estimation followed by heuristic action selection, and
evaluate it in three different domains. Below, we first give an overview of our solution algorithm
and then describe each of the components in more detail.
82

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

4.1 Overview
In this section, we will assume that we are given an HGMDP M , delegating the problem of learning
M to Section 5. Let Ot = o1 , ..., ot be an observation sequence observed by the assistant from the
beginning of the current trajectory until time t. Each observation is a tuple of a world state and the
previously selected action (by either the assistant or agent). Given Ot and M our goal is to compute
an assistant action whose value is (close to) optimal.
To motivate the approach, it is useful to consider some special characteristics of the HGMDP.
Most importantly, the belief state corresponds to a distribution over the agents goal. Since the agent
is assumed to be goal directed, the observed agent actions provide substantial evidence about what
the goal might and might not be. In fact, even if the assistant does nothing, the agents goals will
often be rapidly revealed by analyzing the relevance of the agents initial actions to the possible
goals. In such cases, this suggests that the state/goal estimation problem for the HGMDP may be
solved quite effectively by just observing how the agents actions relate to the various possible goals,
rather than requiring the assistant to select actions explicitly for the purpose of information gathering
about the agents goals. In other words, in such cases, we can expect purely (or nearly) myopic
action selection strategies, which avoid reasoning about information gathering, will be effective.
Reasoning about information gathering is one of the key complexities involved in solving POMDPs
compared to MDPs. Here we leverage the intuitive properties of the HGMDP to gain tractability
by limiting or completely avoiding such reasoning. Of course, as shown by our PSPACE-hardness
results, goals will not always be rapidly revealed and non-myopic reasoning will be essential.
We note that in some cases, the assistant will have pure information-gathering actions at its
disposal, e.g. asking the agent a question. While we do not consider such actions in our experiments,
we believe that such actions can be handled naturally in this framework by incorporating only a
small amount of look-ahead search.
With the above motivation, our assistant architecture, depicted in Figure 1, alternates between
goal estimation and action selection as follows:
1. After observing the agents next action, we update the goal distribution based on the HGMDP
model.
2. Based on the updated distribution we evaluate the effectiveness of assistant actions (including
noop) by building a sparse-sampling look-ahead tree of bounded depth (perhaps just depth
one), where leaves are evaluated via a myopic heuristic.
The key element of the architecture is the computation of the myopic heuristics. On top of this
heuristic, we can optionally obtain non-myopic behavior via search by building a look-ahead sparsesampling tree. Our experiments show that such search can improve performance by a small margin
at a significant computational cost. We note that the idea of utilizing myopic heuristics to select
actions in POMDPs is not new, see for example (Cassandra, 1998; Geffner & Bonet, 1998), and
similar methods have been used previously with success in applications such as computer bridge
(Ginsberg, 1999). The main contribution here is to show that this approach is particularly well
suited to our setting and to evaluate some efficiently computable heuristics specifically designed for
solving HGMDPs. Below we describe the goal estimation and action selection operations in more
detail.

83

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Goal Estimation

P(G)

Action Selection

Assistant
Ot

At

Wt
Environment

User
Ut

Figure 1: Depiction of the assistant architecture. The agent has a hidden goal and selects actions
Ut that cause the environment to change world state Wt , typically moving closer to the
goal. The assistant (upper rectangle) is able to observe the world state along with the
observations generated by the environment, which in our setting contain the user/agent
actions along with the world state. The assistant is divided into two components. First,
the goal estimation component computes a posterior over agent goals P (G) given the observations. Second, the action selection component uses the goal distribution to compute
the best assistive action At via a combination of bounded search and myopic heuristic
computation. The best action might be noop in cases where none of the other assistive
actions has higher utility for the user.

84

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

4.2 Goal Estimation
Given an HGMDP with agent policy  and initial goal distribution IG , our objective is to maintain
the posterior goal distribution P (g|Ot ), which gives the probability of the agent having goal g
conditioned on observation sequence Ot . Note that since we have assumed that the assistant cannot
affect the agents goal, only observations related to the agents actions are relevant to the posterior.
Given the agent policy , it is straightforward to incrementally update the posterior P (g|Ot ) upon
each of the agents actions.
At the beginning of each episode we initialize the goal distribution P (g|O0 ) to IG . On timestep t
of the episode, if Ot does not involve an agent action, then we leave the distribution unchanged. Otherwise, when the agent selects action a in state s, we update the posterior according to P (g|Ot ) =
(1/Z)  P (g|Ot1 )  (a|s, g), where Z is a normalizing constant. That is, the distribution is adjusted to place more weight on goals that are more likely to cause the agent to execute action a in s.
The accuracy of goal estimation relies on how well the policy  learned by the assistant reflects the
true agent policy. As described in Section 5.2, we use a model-based bootstrapping approach for
estimating  and update this estimate at the end of each episode. Provided that the agent is close to
optimal, as in our experimental domains, this approach can lead to rapid goal estimation, even early
in the lifetime of the assistant.
We have assumed for simplicity that the actions of the agent are directly observable. In some
domains, it is more natural to assume that only the state of the world is observable, rather than the
actual action identities. In these cases, after observing the agent transitioning from s to s0 we can
use the MDP transition function T to marginalize over possible agent actions yielding the update,
P (g|Ot ) = (1/Z)  P (g|Ot1 ) 

X

(a|s, g)T (s, a, s0 ).

aA

4.3 Action Selection
Given the HGMDP M and a distribution over goals P (g|Ot ), we now address the problem of
selecting an assistive action. Our mechanisms utilize a combination of bounded look-ahead search
and myopic heuristic computations. By increasing the amount of look-ahead search the actions
returned will be closer to optimal at the cost of more computation. Fortunately, for many HGMDPs,
useful assistant actions can be computed with relatively little or no search. We first describe several
myopic heuristics that can be used either for greedy action selection or in combination with search.
Next, we review how to utilize sparse sampling to obtain non-myopic action selection.
4.3.1 ACTION S ELECTION H EURISTICS
To explain the action selection procedure, we introduce the idea of an assistant MDP relative to a
goal g and M , which we will denote by M (g). The MDP M (g) is identical to M except that we
change the initial goal distribution such that P (G0 = g) = 1. That is, the goal is always fixed to g in
each episode. Since the only hidden component of M s state space was the goal, fixing the goal in
M (g) makes the state fully observable, yielding an MDP. Each episode in M (g) evolves by drawing
an initial world state and then selecting assistant actions until the goal g is achieved. Note that the
state transition after an assistant action a0 in state s is the result of successive state transitions, first
due to the assistant action and then due to the ensuing agent action, which is selected based on the
agent policy and goal g. An optimal policy for M (g) gives the optimal assistive action assuming
85

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

that the agent is acting to achieve goal g. We will denote the Q-function of M (g) by Qg (s, a),
which is the expected cost of executing action a in state s and then following the optimal policy.
We now consider a second heuristic for action selection, which accounts for non-uniform rewards and the true goal posterior, unlike the coarsened posterior heuristic introduced in Section 3.3.
It is simply the expected Q-value of an action over assistant MDPs, and has also been called the QMDP
method by Cassandra (1998). The heuristic value for assistant action a in state s given observations
Ot is
X
Qg (s, a)  P (g|Ot ).
H(s, a, Ot ) =
g

Intuitively H(s, a, Ot ) measures the utility of taking an action under the assumption that all
goal ambiguity is resolved in one step. Thus, this heuristic will not value the information-gathering
utility of an action. Rather, the heuristic will favor assistant actions that make progress toward goals
with high posterior probability. When the goal posterior is highly ambiguous this will often lead
the assistant to prefer noop, which at least does not hurt progress toward the goal. Note that this
heuristic, as well as the others below, can be used to evaluate the utility of a state s, rather than a
state-action pair, by maximizing over all actions maxa H(s, a, Ot ).
The primary computational complexity of computing H is to solve the assistant MDPs for each
goal in order to obtain the Q-functions. Technically, since the transition functions of the assistant
MDPs depend on the approximate agent policy , we must re-solve each MDP after updating the
 estimate at the end of each episode (see Section 5.2 for policy learning). However, using incremental dynamic programming methods such as prioritized sweeping (Moore & Atkeson, 1993) can
alleviate much of the computational cost. In particular, before deploying the assistant we can solve
each MDP offline based on the default agent policy given by the Boltzmann bootstrapping distribution describe in Section 5.2. After deployment, prioritized sweeping can be used to incrementally
update the Q-values based on the learned refinements we make to .
When it is not practical to exactly solve the assistant MDPs, we may resort to various approximations. We consider two approximations in our experiments. One is to replace the users policy
to be used in computing the assistant MDP with a fixed default user policy, eliminating the need to
compute the assistant MDP at every step. We denote this approximation by Hd . Another approximation uses the simulation technique of policy rollout (Bertsekas & Tsitsiklis, 1996) to approximate
Qg (s, a) in the expression for H. This is done by first simulating the effect of taking action a in
state s and then using  to estimate the expected cost for the agent to achieve g from the resulting
state. That is, we approximate Qg (s, a) by assuming that the assistant will only select a single
initial action followed by only agent actions. More formally, let Cn (, s, g) be a function that simulates n trajectories of  achieving the goal from state s and then averaging the trajectory costs.
The
to H(s, a, Ot ) except that we replace Qg (s, a) with the expectation
P heuristic H0r is identical
0
s0 S T (s, a, s )  C(, s , g). We can also combine both of these heuristics, using a fixed default
user policy and policy rollouts, which we denote by Hd,r .
4.3.2 S PARSE S AMPLING
All of the above heuristics are somewhat myopic in the sense that they do not take into account
potentially persistent ambiguity about the agents goal and do not consider the use of information
gathering actions to resolve the ambiguity. In cases where it is beneficial to consider some nonmyopic reasoning, one can combine these heuristics with shallow search in the belief space of

86

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

the assistant MDP. For this purpose we utilize depth d bounded sparse sampling trees (Kearns,
Mansour, & Ng, 1999) to compute an approximation to the Q-function for a given belief state
(st , Ot ), denoted by Qd (st , a, Ot ). Given a particular belief state, the assistant will then select the
action that maximizes Qd . Note that for convenience we represent the belief state as a pair of the
current state st and observation history Ot . This is a lossless representation of the belief state since
the posterior goal distribution can be computed exactly from Ot and the goal is the only hidden
portion of the POMDP state.
The base case Q0 (st , a, Ot ) will be equal to one of our myopic heuristics described above.
Increasing the depth d will result in looking ahead d state transitions and then evaluating one of
our heuristics. By looking ahead it is possible to track the potential changes to the belief state after
taking certain actions and then determine whether those changes in belief would be beneficial with
respect to providing better assistance. Sparse sampling does such look-ahead by approximately
computing:
Qd (s, a, O) = E[R(s, g, a) + V d1 (s0 , O0 )]
d

d

V (s, O) = max Q (s, a, O)
a

(1)
(2)

where g is a random variable distributed according to the goal posterior P (g|O) and (s0 , O0 ) is
a random variable that represents the belief state after taking action a in belief state (s, O). In
particular, s0 is the world state arrived at and O0 is simply the observation sequence O extended
with the observation obtained during the state transition. The first term in the above expectation
represents the immediate reward of the assistant action a when the goal is g.
Sparse sampling approximates the above expectation by averaging a set of b samples of successor belief states. The sparse-sampling pseudo-code is presented in Table 4.3.2. Given an input belief
state (s, O), assistant action a, heuristic H, depth bound d, and sampling width b the algorithm returns (an approximation of) Qd (s, a, O). First, if the depth bound is equal to zero the heuristic
value is returned. Otherwise b samples of observations resulting from taking action a in belief state
(s, O) are generated. The observations will be of the form oi = (s0i , ai , si ), where s0i is the state
resulting from taking action a in state s, ai is the ensuing agent action selected in s0i based on a goal
drawn from the goal posterior, and si is the result of taking action ai in state s0i . Each observation oi
corresponds to a new belief state (si , [O; oi ]) where [O; oi ] is simply the concatenation of oi to O.
The code then recursively computes a value for each of these belief states by maximizing Qd over
all actions and then averages the results.
As b and d become large, sparse sampling will produce an arbitrarily close approximation to the
true Q-function of the belief state MDP. The computational complexity of sparse sampling is linear
in b and exponential in d. Thus the depth must be kept small for real-time operation.

5. Learning HGMDPs
In this section, we tackle the problem of learning the HGMDP while interacting with the environment to assist the agent. We assume that the set of goals G is known to the agent. The primary
role for learning is to acquire the agents policy and goal distribution. This assumption is natural
in situations where the assistant is being applied many times in the same environment, for possibly
different agents. For example, in a desktop environment, the environment MDP corresponds to a
description of the various desktop functionalities, which remains fixed across users. If one is not
87

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

 Given: heuristic function H, belief state (s, O), action a, depth bound d, sampling width b
 Return: an approximation Qd (s, a, O) of the value of a in belief state (s, O)
1. If d = 0 then return H(s, a, O)
2. Sample a set of b observations {o1 , . . . , ob } resulting from taking action a in
belief state (s, O) as follows:
(a) Sample s0i from the environment MDP transition function T (s, a, )
(b) Sample a goal gi from P (gi |O)
(c) Sample an agent action ai from the agent policy (|s0i , gi )
(d) oi = (s0i , ai , si ), where si is sample from the environment MDP transition
function T (s0i , ai , )
3. For each oi = (s0i , gi , ai , si ) compute Vi = maxa0 Qd1 (si , a0 , [O; oi ])
P
4. Return Qd (s, a, O) = 1b i R(s, gi , a) + Vi

Table 1: Pseudo-code for Sparse Sampling in the HGMDP
provided with a description of the MDP then it is typically straightforward to learn this model with
the primary cost being a longer warming up period for the assistant.
Relaxing the assumption that we are provided with the set of possible goals is more problematic
in our current framework. As we saw in Section 4, our solution methods depend on knowing this set
of goals and it is not clear how to learn these from observations, since the goals, unlike states and
actions, are not directly observable to the assistant. Extending our framework so that the assistant
can automatically infer the set of possible user goals, or allow the user to define their own goals, is
an interesting future direction. We note, however, it is often possible for a designer to enumerate a
set of user goals before deployment that while perhaps not complete, allows for useful assistance to
be provided.
5.1 Maximum Likelihood Estimates
It is straightforward to estimate the goal distribution G0 and agent policy  by simply observing the
agents actions, possibly while being assisted, and to compute empirical estimates of the relevant
quantities. This can be done by storing the goal achieved at the end of each episode along with
the set of world state-action pairs observed for the agent during the episode. The estimate of IG
can then be based on observed frequency of each goal (usually with Laplace correction to avoid
extreme values of the probabilities). Likewise, the estimate of (a|s, g) is simply the frequency for
which action a was taken by the agent when in state s and having goal g. While in the limit these
maximum likelihood estimates will converge to the correct values of the true HGMDP, in practice
convergence can be slow. This slow convergence can lead to poor performance in the early stages
of the assistants lifetime. To alleviate this problem we propose an approach for bootstrapping the
learning of the agent policy .
88

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

5.2 Model-Based Bootstrapping
We will leverage our environment MDP model in order to bootstrap the learning of the agent policy.
In particular, we assume that the agent is near optimal in the sense that, for a particular goal and
world state, he is more likely to select actions that are close to optimal. This is not unrealistic
in many application domains that might benefit from intelligent assistants. In particular, there are
many tasks, that are conceptually simple for humans, yet are quite tedious, e.g., navigating through
the directory structure of a computer desktop. Performing optimally in such tasks is not difficult for
humans.
Given the near rationality assumption, we initialize the estimate of the agents policy to a
prior that is biased toward optimal agent actions. To do this we will consider the environment MDP
with the assistant actions removed and solve for the Q-function Q(a, s, g) using MDP planning
techniques. The Q-function gives the expected cost of executing agent action a in world state s and
then acting optimally to achieve goal g using only agent actions. In a world without an assistant,
a rational agent would always select actions that maximize the Q-function for any state and goal.
Furthermore, a close-to-rational agent would prefer actions that achieve higher Q-values to highly
suboptimal actions. We first define the Boltzmann distribution, which will be used to define our
prior,
1
(a|s, g) =
exp(K  Q(a, s, g))
(3)
Z(s, g)
where Z(s, g) is a normalizing constant, and K is a temperature constant. Using larger values
of K skews the distribution more heavily toward optimal actions. Given this definition, our prior
distribution over (|w, g) is taken to be a Dirichlet with parameters (1 , . . . , |A| ), where i =
0  (ai |s, g). Here 0 is a parameter that controls the strength of the prior. Intuitively 0 can be
thought of as the number of pseudo-actions represented by the prior, with each i representing the
number of those pseudo-actions that involved agent action ai . Since the Dirichlet is conjugate to
the multinomial distribution, which is the form of (|s, g), it is easy to update the posterior over
(|s, g) after each observation. One can then take the mode or mean of this posterior to be the point
estimate of the agent policy used to define the HGMDP.
In our experiments, we found that this prior provides a good initial proxy for the actual agent
policy, allowing for the assistant to be immediately useful. Further updating of the posterior tunes
the assistant better to the peculiarities of a given agent. For example, in many cases there are
multiple optimal actions and the posterior will come to reflect any systematic bias among equally
good actions that an agent has. Computationally the main obstacle to this approach is computing the
Q-function, which needs to be done only once for a given application domain since the environment
MDP is constant. Using dynamic programming this can be accomplished in polynomial time in the
number of states and goals. When this is not practical, a number of alternatives exist including the
use of factored MDP algorithms (Boutilier et al., 1999), approximate solution methods (Boutilier
et al., 1999; Guestrin et al., 2003), or developing domain specific solutions.
Finally, in this work, we utilize an uninformative prior over the goal distribution. An interesting
future direction would be to bootstrap the goal distribution estimate based on observations from a
population of agents.

89

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

6. Experimental Results
In this section, we present the results of conducting user studies and simulations in three domains:
two game-like environments and a folder predictor domain for an intelligent desktop assistant. In
the user studies in the two game-like domains, for each episode, the users and the assistants actions
were recorded. These user studies were performed using 12 human subjects (graduate students in CS
department at Oregon State University) over a single session. The ratio of the cost of achieving the
goal with the assistants help to the optimal cost without the assistant was calculated and averaged
over the multiple trials for each user. We present similar results for the simulations as well. The third
domain is a folder predictor domain, where we simulated the user and used one of our heuristics to
generate the top 3 recommended folders for the user. We present the number of clicks required on an
average for the user to reach her desired folder. Two of these three domains, namely, the doorman
domain and the folder predictor domain, fall under the category of HAMDPs since the assistive
actions can be merely viewed as helper actions that the agent can ignore. The kitchen domain on
the other hand needs a slightly more general formulation since the agent and the assistant do not
strictly alternate, and the assistants actions cannot be ignored by the agent.
6.1 Doorman Domain
In the doorman domain, there is an agent and a set of possible goals such as collect wood, food and
gold. Some of the grid cells are blocked. Each cell has four doors and the agent has to open the
door to move to the next cell (see Figure 2). The door closes after one time-step so that at any time
only one door is open. The goal of the assistant is to help the user reach his goal faster by opening
the correct doors.
A state is a tuple hs, di, where s stands for the the agents cell and d is the door that is open. The
total number of states is 245 (49 squares with 5 possibilities for the door). The actions of the agent
are to open door and to move in each of the 4 directions or to pickup whatever is in the cell, for a
total of 9 actions. The assistant can open the doors or perform a noop (5 actions. The agents and
the assistants actions strictly alternate in this domain, satisfying the definition of HAMDPs. There
is a reward of 1 (or a cost of 1) if the user has to open the door and no reward to the assistants
action. The trial ends when the agent picks up the desired object. Note that while we have included
a noop action for the assistant, in this domain the action is never selected, since the cost of opening
a wrong door and noop are the same, while there is no potential benefit of selecting noop.
In this experiment, we evaluated two heuristics: one where we fixed the user policy to the default
policy in the HGMDP creation (Hd ) avoiding the need for repeated computation of the HGMDP at
every step and the second where we use the policy rollout to calculate the Q-values (Hr ). In each
trial, the system chooses a goal and one of the two heuristics at random. The user is shown the
goal and he tries to achieve it, always starting from the center square. After every users action, the
assistant opens a door or does nothing. The user may pass through the door or open a different door.
After the user achieves the goal, the trial ends, and a new one begins. The assistant then uses the
users trajectory to update the agents policy.
The results of the user studies for the doorman domain are presented in Tabe 2. The first two
rows give cumulative results for the user study when actions are selected greedily according to Hr
and Hd respectively. Rather than reporting the negtive rewards, the table shows the total number of

90

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

Figure 2: Doorman Domain. The agents goal is to fetch a resource. The grid cells are separated by
doors that must be opened before passing through.

actions for all trials across all users without the assistant N, and the total number of actions with the
assistant U, and the average of percentage savings (1-(U/N)) over all trials and over all the users.4
As can be seen, both the methods reduce the number of actions by more than 50%. Note that
an assistant that selects among the four doors at random would reduce the number of actions by
only 25% in comparison. An omniscient assistant who knows the users goal reduces the number
of actions by 78%. This is not 100% because the first door is always opened by the user. In our
experiments, if we do not count the users first action, the number of actions reduces by 65%.5 It
can be observed that Hr appears to have a slight edge over Hd . One possible reason for this is that
while using Hd , we do not re-solve the MDP after updating the user policy, while Hr is always
using the updated user policy. Thus, rollout is reasoning with a more accurate model of the user.
Heuristic
Hr
Hd
Hr
d = 2, b = 1
d = 2, b = 2
d = 3, b = 1
d = 3, b = 2

Total
Actions
N
750
882
1550
1337
1304
1167
1113

User
Actions
U
339
435
751
570
521
467
422

Fractional Savings
1  (U/N )
0.55 0.055
0.51  0.05
0.543  0.17
0.588  0.17
0.597  0.17
0.6  0.15
0.623  0.15

Time
per
action (in secs
0.0562
0.0021
0.031
0.097
0.35
0.384
2.61

Table 2: Results of experiments in the Doorman Domain. The first two rows of the table present the
results of the user studies while the rest of the table presents the results of the simulation.

4. This gives a pessimistic estimate of the usefulness of the assistant assuming an optimal user and is a measure of utility
normalized by the optimal utility without the aid of the assistant.
5. Note that this first action requirement is easily aviodable. It is simply the equivalent of an on switch to indicate that
the user is ready to move about in the grid. We can also replace this requirement by explicitly adding an on button to
the interface to start a new episode.

91

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Another interesting observation is that there are individual differences among the users. Some
users always prefer a fixed path to the goal regardless of the assistants actions. Some users are
more flexible. From the survey we conducted at the end of the experiment, we learned that one of
the features that the users liked was that the system was tolerant to their choice of suboptimal paths.
The data reveals that the system was able to reduce the costs by approximately 50% even when the
users chose suboptimal trajectories.
We also conducted experiments using sparse sampling with non-zero depths. We considered
depths of d = 1 and d = 2 while using sampling widths of b = 1 or b = 2. The leaves of the sparse
sampling tree are evaluated using Hr which simply applies rollout to the user policy. Hence sparse
sampling of d = 0 and b = 1, would correspond to the heuristic Hr . For these experiments, we did
not conduct user studies, due to the high cost and effort required for humans in such studies, but
simulated the human users by choosing actions according to policies learned from their observed
actions from the previous user study. The results are presented in the last 5 rows of Table 2. Note that
the absolute numbers of actions in the user studies and the simulations are not comparable as they
are based on different numbers of trajectories. The human users were tested on fewer trajectories
to minimize their fatigue. We see that sparse sampling increased the average run time (last column)
by an order of magnitude, but is able to produce a reduction in average cost for the user. This result
is not surprising in hindsight, for in the simulated experiments, sparse sampling is able to sample
from the exact user policy (i.e. it is sampling from the learned policy, which is also being used
for simulations). These results suggest that a small amount of non-myopic reasoning can have a
positive benefit with a substantial computation cost. Note, however, that the bulk of the benefit
realized by the assistant can be obtained without such reasoning, showing that the myopic heuristics
are well-suited to this domain.
6.2 Kitchen Domain
In the kitchen domain, the goals of the agent are to cook various dishes. There are 2 shelves with
3 ingredients each. Each dish has a recipe, represented as a partially ordered plan. The ingredients
can be fetched in any order, but should be mixed before they are heated. The shelves have doors
that must be opened before fetching ingredients and only one door can be open at a time.
There are 8 different recipes. The state consists of the location of each of the ingredients
(bowl/shelf/table), the mixing state and temperature state of the ingredient (if it is in the bowl)
and the door that is open. The state also includes the action history to preserve the ordering of the
plans for the recipes. The users actions are: open the doors, fetch the ingredients, pour them into
the bowl, mix, heat and bake the contents of the bowl, or replace an ingredient back to the shelf. The
assistant can perform all user actions except for pouring the ingredients or replacing an ingredient
back to the shelf. We restricted the assistant from pouring ingredients as it is an irreversible action. The reward for all non-pour actions is -1. Experiments were conducted on 12 human subjects
who are computer science graduate students. Unlike in the doorman domain, here we allowed the
the assistant to take multiple consecutive actions. The turn switches to the user when the assistant
executes the noop action.
This domain has a large state space and hence it is not possible to update the user policy after
every trajectory. Hence, the two heuristics that we compare both use the default user policy. The
second heuristic in addition uses policy rollout to compare the actions. In other words, we compare
Hd and Hd,r . The results of the user studies are shown in top part of the Table 3. As in the doorman

92

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

Figure 3: The kitchen domain. The user is to prepare the dishes described in the recipes on the
right. The assistants actions are shown in the bottom frame.

domain, the total number of agent actions with and without the assistant, and the percentage reduction due to the assistant are presented. The number of user actions was summed over 12 users and
the cumulative results are presented. It can be observed that Hd,r performs better than Hd . It was
observed from the experiments that the Hd,r technique was more aggressive in choosing non-noop
actions than Hd , which would wait until the goal distribution is highly skewed toward a particular
goal.
Heuristic
Hd,r
Hd
Hd,r
d = 2, b = 1
d = 2, b = 2
d = 3, b = 1
d = 3, b = 2

Total
Actions
N
3188
3175
6498
6532
6477
6536
6585

User
Actions
U
1175
1458
2332
2427
2293
2458
2408

Fractional Savings
1  (U/N )
0.6361 0.15
0.5371 0.10
0.6379  0.14
0.6277  0.14
0.646  0.14
0.6263 0.15
0.645  0.14

Time
per
action (secs)
0.013
0.013
0.013
0.054
0.190
0.170
0.995

Table 3: Results of experiments in the Kitchen Domain. The first two rows of the table present the
results of the user studies while the last 5 rows present the results of the simulation.

We compared the use of sparse sampling and our heuristic on simulated user trajectories for
this domain as well (see the last 5 rows of Table 3). Again, the absolute numbers of actions of the
user studies are not comparable to that of simuations due to different numbers of trajectories in each
case. Since sparse sampling considers a larger number of trajectories than the other methods, the
policies learned are sometimes better than those learned from other heuristics, although they took
more time to execute. However, there is no significant difference between the solution quality of
rollouts and sparse sampling on simulations, showing that our myopic heuristics are performing as
93

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

well as sparse sampling with much less computation. Sparse sampling with higher depths requires
an order of magnitude more computation time when compared to the rollout.
6.3 Folder Predictor
In this section, we present the evaluation of our framework on a real-world domain. As a part of
the Task Tracer project (Dragunov, Dietterich, Johnsrude, McLaughlin, Li, & Herlocker, 2005),
researchers developed a file location system called folder predictor (Bao et al., 2006). The idea
behind the folder predictor is that by learning about the users file access patterns, the assistant can
help the user with his file accesses by predicting the folder in which the file has to be accessed or
saved.
In this setting, the goal of the folder predictor is to minimize the number of clicks of the user.
The predictor would choose the top three folders that would minimize the cost and then append them
to the UI (shown in ovals in Figure 4). Also, the user is taken to the first recommended folder. So if
the users target folder is the first recommended folder, the user would reach the folder in zero clicks
and reach the second or the third recommended folder in one click. The user can either choose one
of the recommendations or navigate through the windows folder hierarchy if the recommendations
are not relevant.

Figure 4: Folder predictor (Bao et al., 2006).
Bao et al. considered the problem as a supervised learning problem and implemented a costsensitive algorithm for the predictions with the cost being the number of clicks of the user (Bao
et al., 2006). But, their algorithm does not take into account the response of the user to their
predictions. For instance, if the user chooses to ignore the recommended folders and navigates the
folder hierarchy, they do not make any re-predictions. This is due to the fact that their model is
a one-time prediction and does not consider the user responses. Also, their algorithm considers
a restricted set of previously accessed folders and their ancestors as possible destinations. This
precludes handling the possibility of user accessing a new folder.
Our decision-theoretic model naturally handles the case of re-predictions by changing the recommendations in response to the user actions. As a first step, we used the data collected from their
user interface and used our model to make predictions. We use the users response to our predic94

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

tions to make further predictions. Also, to handle the possibility of a new folder, we consider all
the folders in the folder hierarchies for each prediction. We used a mixture density to obtain the
probability distribution over the folders.
P (f ) = 0 P0 (f ) + (1  0 )Pl (f )
Here P0 is the probability according to Bao et.als algorithm (2006), Pl is the uniform probability distribution over the set of folders and 0 is ratio of the number of times a previously accessed
folder has been accessed to the total number of folder accesses.
The idea behind using the above density function is that during the early stages of a task, the user
will be accessing new folders while in later stages the user will access the folders of a particular task
hierarchy. Hence as the number of folder accesses increases the value of 0 increases and would
eventually converge to 1, and hence the resulting distribution would converge to P0 . The data set
consists of a collection of requests to open a file (Open) and save a file (saveAs), ordered by time.
Each request contains information such as, the type of request (open or saveAs), the current task,
the destination folder, etc. The data set consists of a total of 810 open/saveAs requests. The folder
hierarchy consists of 226 folders.
The state space consists of 4 parts: the current folder that the user is accessing and the three
recommendations two of which are unordered. This would correspond to a state space of size
226  225  224
2 . The action of the user is either to choose a recommended folder or select
a different folder. The action of the assistant corresponds to choosing the top 3 folders and the
action space is of size 225  224
2 . The reward in our case was the negative of the number of user
clicks. In this domain, the assistant and the users actions strictly alternate as the assistant revises its
predictions after every user action. The prior distribution was initialized using the rewards computed
by the model developed by Bao et al. (2006).
We applied the decision theoretic model to the data set. For each request, our assistant would
make the prediction using the Hd,r heuristic (which uses the default user policy and the rollout
method) and then the user is simulated. The user would accept the recommendation if it shortens
his path to the goal, and otherwise would act according to his optimal policy. The user here is
considered close to optimal, which is not unrealistic in the real world. To compare our results, we
also used the model developed by Bao et al. in the data set and present the results in Table 4.
Restricted folder set
All Folders

One-time Prediction
1.3724
1.319

With Repredictions
1.34
1.2344

Table 4: Results of the experiments in the folder predictor domain. The numbers indicate the average number of clicks required by the agent to reach his/her correct folder. The entry in
the top left hand cell is the performance of the current Task Tracer, while the one in the
bottom right hand cell is the performance of the decision-theoretic assistant.

The table shows the average cost of folder navigation for 4 different cases: Bao et.als original
algorithm, their algorithm modified to include mixture distributions and our model with and without
mixture distributions. It can be seen that our model with the use of mixture distributions has the
least user cost for navigation and hence is the most effective. Bao et. al have shown that their
95

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

algorithm performs significantly better than the windows default prediction which has an average
of 2.6 clicks per folder navigation. This improvement can be attributed to the two modifications
mentioned earlier. First, the use of re-predictions in our model which is natural to the decisiontheoretic framework while their model makes a one-time prediction and hence cannot make use
of the users response to the recommendations. Secondly, considering all folders in the hierarchy
for prediction including the possibility of the user accessing a new folder is found to be useful. It
can be observed that either of the modifications yields a lower cost than the original algorithm, but
combining the two changes is significantly more effective.

7. Discussion and Related Work
Our work is inspired by the growing interest and success in building useful software assistants
(Yorke-Smith et al., 2012; Lieberman, 2009; Myers et al., 2007). Some of this effort is focused
on building desktop assistants that help with tasks such as calendar scheduling (Refanidis, Alexiadis, & Yorke-Smith, 2011), email filtering (Cohen, Carvalho, & Mitchell, 2004), on-line diagnostics (Skaanning, Jensen, & Kjaerulff, 2000), and travel planning (Ambite, Barish, Knoblock,
Muslea, Oh, & Minton, 2002). Each of these tasks typically requires designing a software system
around specialized technologies and algorithms. For example, email filtering is typically posed as a
supervised learning problem (Cohen et al., 2004), travel planning combines information gathering
with search and constraint propagation (Ambite et al., 2002), and printer diagnostics is formulated
as Bayesian network inference (Skaanning et al., 2000). Other approaches focus on socially assistive robots setting where a robot is designed to aid human agents in achieving their goals (Johnson,
Cuijpers, Juol, Torta, Simonov, Frisiello, Bazzani, Yan, Weber, Wermter, et al., 2013). Unfortunately the plethora of systems and approaches lacks an overarching conceptual framework, which
makes it difficult to build on each others work. In this paper, we argue that a decision-theoretic
approach provides such a common framework and allows the design of systems that respond to
novel situations in a flexible manner reducing the need for pre-programmed behaviors. We formulate a general version of the assistantship problem that involves inferring the users goals and taking
actions to minimize the expected costs.
Earlier work on learning apprentice systems focused on learning from the users by observation (Mahadevan, Mitchell, Mostow, Steinberg, & Tadepalli, 1993; Mitchell, Caruana, Freitag,
J.McDermott, & Zabowski, 1994). This work is also closely related to learning from demonstration
or programming by demonstration (Johnson, 2014; Konidaris, Kuindersma, Grupen, & Barto, 2012;
Atkeson & Schaal, 1997; Cypher, 1993; Lau, Wolfman, Domingos, & Weld, 2003). The emphasis
in these systems is to provide an interface where the computer system can unobtrusively observe the
human user doing a task and learn to do it by itself. The human acts both as a user and as a teacher.
The performance of the system is measured by how quickly the system learns to imitate the user,
i.e., in the supervised learning setting. Note that imitation and assistance are two different things in
general. While we expect our secretaries to learn about us, they are not typically expected to replace
us. In our setting, the assistants goal is to reduce the expected cost of users problem solving. If the
user and the assistant are capable of exactly the same set of actions, and if the assistants actions cost
nothing compared to the users, then it makes sense for the assistant to try to completely replace the
human. Even in this case, the assistantship framework is different from learning from demonstration
in that it still requires the assistant to infer the users goal from his actions before trying to achieve
it. Moreover, the assistant might learn to solve the goal by itself by reasoning about its action set

96

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

rather than by being shown examples of how to do it by the user. In general, however, the action
set of the user and the assistant may be different, and supervised learning is not appropriate. For
example, this is the case in our folder predictor. The system needs to decide which set of folders
to present to the user, and the user needs to decide which of those to choose. It is awkward if not
impossible to formulate this problem as supervised learning or programming by demonstration.
Taking the decision-theoretic view helps us approach the assistantship problem in a principled
manner taking into account the uncertainty in the users goals and the costs of taking different
actions. The assistant chooses an action whose expected cost is the lowest. The framework naturally
prevents the assistant from taking actions (other than noop) when there is no assistive action which
is expected to reduce the overall cost for the user. Rather than learning from the user how to behave,
in our framework the assistant learns the users policy. This is again similar to a secretary who learns
the habits of his boss, not so much to imitate her, but to help in the most effective way. In this work
we assumed that the user MDP is small enough that it can be solved exactly given the users goals.
This assumption may not always be valid, and it makes sense in those cases to learn from the user
how to behave. It is most natural to treat this as a case where the users actions provide exploratory
guidance to the system (Clouse & Utgoff, 1992; Driessens, 2002). This gives an opportunity for the
system to imitate the user when it knows nothing better and improve upon the users policy when it
can.
There have been other personal assistant systems that are based on POMDP models. However,
these systems are formulated as domain-specific POMDPs and are solved offline. For instance,
the COACH system helped people suffering from dementia by giving them appropriate prompts as
needed in their daily activities (Boger, Poupart, Hoey, Boutilier, Fernie, & Mihailidis, 2005). They
use a plan graph to keep track of the users progress and then estimate the users responsiveness to
determine the best prompting strategy. A distinct difference from our approach is that there is only
a single fixed goal of washing hands, and the only hidden variable is the user responsiveness which
is either low or high. Rather, in our formulation the goal is a random variable that is hidden to the
assistant. Since their state-action space is significantly smaller (1280 as against 2  108 states in our
folder predictor domain), it is possible for them to solve the POMDP exactly. Given that we need
to re-solve the POMDP after every user action, it becomes prohibitively expensive. Yet another
difference is that the length of the trajectory to the goal is small in their case and hence a plan graph
would suffice to capture the user policy. In our model, we do not restrict to a plan graph and instead
solve the user MDP to bootstrap the policy. They have mentioned learning the user policy as a
future direction. In our work, even though we start with an initial estimate of the user policy, we
update it after every goal is achieved. This can be considered as online learning of user policy with
a reasonably good prior. We note that a combination of these two frameworks (one for modeling
users responsiveness and the other for modeling users goal) would be useful, where the assistant
infers both the agent goals and other relevant hidden properties of the user, such as responsiveness.
In Electric Elves, the assistant takes on many of the mundane responsibilities of the human
agent including rescheduling a meeting should it appear that the user is likely to miss it. Again a
domain-specific POMDP is formulated and solved offline using a variety of techniques. In one such
approach, since the system monitors users in short regular intervals, radical changes in the belief
states are usually not possible and are pruned from the search space (Varakantham, Maheswaran,
& Tambe, 2005). Neither exact nor approximate POMDP solvers are feasible in our online setting,
where the POMDP is changing as we learn about the user, and must be repeatedly solved. They are
either too costly to run (Boger et al., 2005), or too complex to implement as a baseline, e.g., Electric
97

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Elves (Varakantham et al., 2005). Our experiments demonstrate that simple methods such as onestep look-ahead followed by rollouts would work well in many domains where the POMDPs are
solved online. In a distinct but related work (Doshi & Gmytrasiewicz, 2004), the authors introduce
the setting of interactive POMDPs, where each agent models the other agents beliefs. Clearly, this
is more general and more complex than ordinary POMDPs. Our model is simpler and assumes that
the agent is oblivious to the presence and beliefs of the assistant. While the simplified model suffices
in many domains, relaxing this assumption without sacrificing tractability would be interesting.
There have been several dialogue systems proposed and many of them are based on decisiontheoretic principles (Walker, 2000; Singh, Litman, Kearns, & Walker, 2002). For instance, the
NJFun system is designed as an MDP to provide assistance to the user by interacting with the user
and providing the answer to the users questions. It uses an automatic speech recognizer (ASR) to
interpret the human dialogues and uses a dialogue policy to choose the best action (the response).
The goals of the user could be a set of standard queries such as locations of restaurants, wineries,
shopping centers etc. The state space would be the dialogue states, i.e., the current state of the
dialogue between the user and assistant (such as greeting, choice state etc). The observations are
the interpretations of the dialogues of the human by the ASR. The NJFun system can be usefully
modeled as an HGMDP, where the goal of the assistant is to infer the users query given the observations and provide appropriate response. The initial assistant policy can be learned from training
data, in a manner similar to the dialogue policy of the NJFun system.
Our work is also related to on-line plan recognition and can be naturally extended to include
hierarchies as in the hierarchical versions of HMMs (Bui, Venkatesh, & West, 2002) and PCFGs
(Pynadath & Wellman, 2000). Blaylock and Allen describe a statistical approach to goal recognition
that uses maximum likelihood estimates of goal schemas and parameters (Blaylock & Allen, 2004).
These approaches do not have the notion of cost or reward. By incorporating plan recognition in the
decision-theoretic context, we obtain a natural notion of optimal assistance, namely maximizing the
expected utility.
There has been substantial research in the area of user modeling. Horvitz et al. took a Bayesian
approach to model whether a user needs assistance based on his actions and attributes and provided
assistance as needed in a spreadsheet application (Horvitz et al., 1998). Hui and Boutilier used a
similar idea for assistance with text editing (Hui & Boutilier, 2006). They use DBNs with handcoded
parameters to infer the type of the user and compute the expected utility of assisting the user. It
would be interesting to explore the use of ideas from plan recognition (Charniak & Goldman, 2013;
Gal, Reddy, Shieber, Rubin, & Grosz, 2012; Chu, Song, Kautz, & Levinson, 2011) in our system
to take into account the users intentions and attitudes while computing the optimal policy for the
assistant.
Recently, there have been methods proposed for solving POMDPs called point based methods (Pineau, Gordon, & Thrun, 2003; Porta, Vlassis, Spaan, & Poupart, 2006; Kurniawati, Hsu,
& Lee, 2008; Shani, Pineau, & Kaplow, 2013). An example of this method is point based value
iteration (PBVI) (Pineau et al., 2003; Porta et al., 2006) that takes a set of belief points B as input
and maintains a set of POMDP -vectors at each iteration. Each iteration produces a new set of vectors that are optimal for each belief point with respect to the -vectors in the previous iteration.
The approximation made by PBVI when compared to value iteration is that there is no guarantee
that the set of -vectors is optimal for the entire belief space. By omitting some -vectors, PBVI
maintains a constant run time per iteration. Application of efficient point based methods such as

98

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

PBVI to the decision-theoretic assistance problem and evaluation of their performance compared to
the policy rollout and sparse sampling methods remains a promising research direction.

8. Summary and Future Work
We introduced a decision-theoretic framework for assistant systems and described the HGMDP
as an appropriate model for selecting assistive actions. The computational complexity of HGMDPs
motivated the definition of a simpler model called HAMDP, which allows efficient myopic heurstics
and more tractable special cases.
We also described an approximate solution approach based on iteratively estimating the agents
goal and selecting actions using myopic heuristics. Our evaluation using human subjects in two
game-like domains show that the approach can significantly help the user. We also demonstrated in
a real world folder predictor that the decision-theoretic framework was more effective than the state
of the art techniques for folder prediction.
One future direction is to consider more complex domains where the assistant is able to do a series of activities in parallel with the agent. Another possible direction is to assume hierarchical goal
structure for the user and do goal estimation in that context. Recently, the assistantship model was
extended to hierarchical and relational settings (Natarajan et al., 2007) by including parameterized
task hierarchies and conditional relational influences as prior knowledge of the assistant. This prior
knowledge would relax the assumption that the user MDP can be solved tractably. This knowledge
was compiled into an underlying Dynamic Bayesian network, and Bayesian network inference algorithms were used to infer a distribution of users goals given a sequence of her atomic actions.
The parameters for the users policy were estimated by observing the users actions.
Our framework can be naturally extended to the case where the environment is partially observable to the agent and/or to the assistant. This requires recognizing actions taken to gather
information, e.g., opening the fridge to decide what to make based on what is available. Incorporating more sophisticated user modeling that includes users forgetting their goals, not paying attention
to an important detail, and/or changing their intentions would be extremely important for building
practical systems. The assistive technology can also be very useful if the assistant can quickly learn
new tasks from expert users and transfer the knowledge to novice users during training.

Acknowledgements
This material is based upon work supported by the Defense Advanced Research Projects Agency
(DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of DARPA. Alan
Fern and Prasad Tadepalli gratefully acknowledge the following grants: NSF IIS-0964705 and ONR
N00014-11-1-0106. Sriraam Natarajan thanks Army Research Office grant number W911NF-13-10432 under the Young Investigator Program.

Appendix A. Proof of Theorem 7
According to the theory of POMDPs, the optimal action in a POMDP maximizes the sum of the
immediate expected reward and the value of the resulting belief state (of the assistant) (Kaelbling,

99

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Littman, & Cassandra, 1998). When the agent policy is deterministic, the initial goal distribution
IG and the history of agent actions and states H fully captures the belief state of the agent. Let
V (IG , H) represent the value of the current belief state. Then the value function of the belief state
is given by by the following Bellman equation, where H 0 stands for the history after the assistants
action hi and the agents action aj .
V (IG , H) = max E(R((s, hi ), g, aj )) + V (IG , H 0 )
hi

H0

(4)

Since there is only one agents action in (s, g), the agent action aj , the subsequent state s0 in
and its value do not depend on hi . Hence the best helper action h of the assistant is given by:
h (IG , H) = arg max E(R((s, hi ), g, a (s, g)))
hi
X
= arg max
IG (g)I(ai  (s, g))
hi

gC(H)

= arg max IG (C(H)  G(s, ai ))
hi

where C(H) is the set of goals consistent with the current history H, and G(s, ai ) is the set of goals
for which ai is good in state s. I(ai  (s, g)) is an indicator function which is = 1 if ai  (s, g).
Note that h is exactly the myopic policy. 2

Appendix B. Proof of Lemma 1
The worst-case regret of any pair (s, G) in the HAMDP is given by the following Bellman equation.
assuming that G is the set of possible goals of the agent and the current state is s. Regret(s, G) = 0
if s is a terminal state or all goals in G are satisfied in s. Otherwise, Regret(s, G) = mini maxj6=i
{Regret(si , Gi ), 1 + Regret(sj , Gj ))} where (si , Gi ) and (sj , Gj ) are in Children((s, G)).
Here the outer min is due to the assistant picking a helper action for a goal in Gi to maximize
the reward and the inner max is due to the agent either accepting it, or picking a different goal to
minimize the reward. The proof is by induction. Each node in the trajectory tree represents a state
and a set of goals G for which that state is on the optimal path.
Basis: If (s, G) is a leaf node, it is either a terminal state or all goals in G are satisfied in s. Hence
its rank equals its reward which is 0.
Inductive step: Suppose that the induction is true for all children of (s, G). We will consider two
cases.
Case 1. There is a unique child of (s, G) representing (s1 , G1 ) which has the highest regret among
all its children. By inductive hypothesis, rank((s1 , G1 )) = regret(s1 , G1 ). If the assistant chooses
the helper action that corresponds to (s1 , G1 ), the agent can only choose actions that yield lower
regret in the worst case. Choosing any other helper action would increase the regret, since the
agent could then choose a1 and add 1 to the regret. So we have, regret(s, G)= regret(s1 , G1 ) =
rank((s1 , G1 )) = rank((s, G)).
Case 2. There are at least two children (s1 , G1 ) and (s2 , G2 ) of (s, G) which have the highest rank
among all its children. By inductive hypothesis, rank((s1 , G1 )) = rank((s2 , G2 )) = regret(s1 , G1 )
= regret(s2 , G2 ). Here the agent can increase the regret by 1 more by choosing a goal in G2 if the
assistant chooses G1 and vice versa. Hence, regret(s, G)= 1+regret(s1 , G1 ) = 1+rank((s1 , G1 )) =
100

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

rank((s, G)).
Hence in both cases, we have shown that regret(s, G) is rank((s, G)). 2

Appendix C. Proof of Theorem 9
We first show that the problem is in NP. We build a tree representation of a history-dependent policy
for each initial state. Every node in the tree is represeted by a triple (s, i, G), where s is a state,
G is a set of goals for which it is on a good path, and i is the index of the helper action chosen
by the policy at that node. The root node corresponds to a possible initial state and the initial goal
set IG . The children of a node in the tree represent possible successor nodes (sj , j, Gj ) reached by
the agents response to hi , whether by accepting hi and executing ai or by executing other actions.
The children resulting from ai are called accepted, and the latter are called rejected. Note that
multiple children can result from the same action because the dynamics are a function of the agents
goal.
We now guess a policy tree and check that its maximum regret, i.e. the maximum number of
rejected children in any path from the root to a leaf, is within bounds. To verify that the optimal
policy tree is of polynomial size we note that the number of leaf nodes is upper bounded by |G| 
maxg N (g), where N (g) is the number of leaf nodes generated by the goal g. To estimate N (g),
we start from the root and navigate downwards. For any node that contains g in its goal set, if some
accepted child contains g, then it will be the only child that will be reached for g. If not, there is a
misprediction and there are at most k children reached. Hence, the number of nodes reached for g
grows geometrically with the number of mispredictions. From Theorem 6, since there are at most
log |G| mispredictions in any such path, N (g)  k log2 |G| = k logk |G| log2 k = |G|log2 k . Hence the
total number of all leaf nodes of the tree is bounded by |G|1+log k , and the total number of nodes
in the tree is bounded by m|G|1+log k , where m is the number of steps to the horizon. Since this is
polynomial in the problem parameters, the problem is in NP.
To show NP-hardness, we reduce 3-SAT to the given problem. We consider each 3-literal clause
Ci of a propositional formula  as a possible goal. The rest of the proof is identical to that of
Theorem 1 except that all variables are set by the assistant since there are no universal quantifiers.
The agent only rejects the setting of the last variable in the clause if the clause evaluates to 0. The
worst regret on any goal is 0 iff the 3-SAT problem has a satisfying assignment. 2

References
Ambite, J. L., Barish, G., Knoblock, C. A., Muslea, M., Oh, J., & Minton, S. (2002). Getting from
here to there: Interactive planning and agent execution for optimizing travel. In Proceedings
of the Fourteenth Conference on Innovative Applications of Artificial Intelligence, pp. 862
869.
Atkeson, C. G., & Schaal, S. (1997). Learning tasks from a single demonstration. In Proceedings
of IEEE International Conference on Robotics and Automation, pp. 17061712.
Bao, X., Herlocker, J. L., & Dietterich, T. G. (2006). Fewer clicks and less frustration: reducing the
cost of reaching the right folder. In Proceedings of the Eleventh International Conference on
Intelligent User Interfaces, pp. 178185.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.

101

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Blaylock, N., & Allen, J. F. (2004). Statistical goal parameter recognition. In Proceedings of the
Fourteenth International Conference on Automated Planning and Scheduling, pp. 297305.
Boger, J., Poupart, P., Hoey, J., Boutilier, C., Fernie, G., & Mihailidis, A. (2005). A decisiontheoretic approach to task assistance for persons with dementia. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence, pp. 12931299.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
and computational leverage. Journal of Artificial Intelligence Research, 11, 194.
Bui, H., Venkatesh, S., & West, G. (2002). Policy recognition in the abstract hidden markov models.
Journal of Artificial Intelligence Research, 17, 451499.
Cassandra, A. R. (1998). Exact and approximate algorithms for partially observable Markov decision processes. Ph.D. thesis, Brown University.
Charniak, E., & Goldman, R. (2013). Plan recognition in stories and in life. CoRR, abs/1304.1497.
Chu, Y., Song, Y., Kautz, H., & Levinson, R. (2011). When did you start doing that thing that you
do? interactive activity recognition and prompting. In Proceedings of the Twenty-Fifth AAAI
Conference Workshop on Artificial Intelligence and Smarter Living, pp. 1521.
Clouse, J. A., & Utgoff, P. E. (1992). A teaching method for reinforcement learning. In Proceedings
of the Ninth International Workshop on Machine Learning, pp. 92110.
Cohen, W. W., Carvalho, V. R., & Mitchell, T. M. (2004). Learning to classify email into speech acts.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp.
309316.
Cypher, A. (1993). Watch What I Do: Programming by Demonstration. MIT Press.
Doshi, P., & Gmytrasiewicz, P. (2004). A particle filtering algorithm for interactive POMDPs. In
Proceedings of the Workshop on Modeling Other Agents from Observations, pp. 8793.
Dragunov, A. N., Dietterich, T. G., Johnsrude, K., McLaughlin, M., Li, L., & Herlocker, J. L. (2005).
Tasktracer: A desktop environment to support multi-tasking knowledge workers. In Proceedings of the Tenth International Conference on Intelligent User Interfaces, pp. 7582.
Driessens, K. (2002). Adding guidance to relational reinforcement learning. In Third FreiburgLeuven Workshop on Machine Learning.
Ehrenfeucht, A., & Haussler, D. (1989). Learning decision trees from random examples. Information and Computation, 82(3), 231246.
Gal, Y., Reddy, S., Shieber, S., Rubin, A., & Grosz, B. (2012). Plan recognition in exploratory
domains. Artificial Intelligence, 176(1), 22702290.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. In
Proceedings of AAAI Fall Symposium on POMPDs.
Ginsberg, M. L. (1999). GIB: Steps Toward an Expert-Level Bridge-Playing Program. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pp. 584589.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms for
factored MDPs. Journal of Artificial Intelligence Research, 19, 399468.

102

fiA D ECISION -T HEORETIC M ODEL OF A SSISTANCE

Horvitz, E., Breese, J., Heckerman, D., Hovel, D., & Rommelse, K. (1998). The lumiere project:
Bayesian user modeling for inferring the goals and needs of software users. In Proceedings
of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 256265.
Hui, B., & Boutilier, C. (2006). Whos asking for help?: a Bayesian approach to intelligent assistance. In Proceedings of the Eleventh International Conference on Intelligent User Interfaces,
pp. 186193.
Johnson, D., Cuijpers, R., Juol, J., Torta, E., Simonov, M., Frisiello, A., Bazzani, M., Yan, W.,
Weber, C., Wermter, S., et al. (2013). Socially assistive robots: A comprehensive approach to
extending independent living. International Journal of Social Robotics, 6(2), 195211.
Johnson, M. (2014). Inverse optimal control for deterministic continuous-time nonlinear systems.
Ph.D. thesis, University of Illinois at Urbana-Champaign.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and acting in partially bservable
stochastic domains. Artificial Intelligence, 101(1-2), 99134.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for near-optimal
planning in large markov decision processes. In Proceedings of the Sixteenth International
Joint Conference on Artificial Intelligence, pp. 13241331.
Konidaris, G., Kuindersma, S., Grupen, R., & Barto, A. (2012). Robot learning from demonstration
by constructing skill trees. The International Journal of Robotics Research, 31(3), 360375.
Kullback, S., & Leibler, R. (1951). On information and sufficiency. The Annals of Mathematical
Statistics, 22(1), 7986.
Kurniawati, H., Hsu, D., & Lee, W. (2008). Sarsop: Efficient point-based POMDP planning by
approximating optimally reachable belief spaces. In Proceedings of Robotics: Science and
Systems IV.
Lau, T., Wolfman, S., Domingos, P., & Weld, D. (2003). Programming by demonstration using
version space algebra. Machine Learning, 53(1-2), 111156.
Lieberman, H. (2009). User interface goals, AI opportunities. AI Magazine, 30(3), 1622.
Littlestone, N. (1988). Learning quickly when irrelevant attributes abound: A new linear-threshold
algorithm. Machine Learning, 2(4), 285318.
Littman, M. L. . (1996). Algorithms for Sequential Decision Making. Ph.D. thesis, Brown University.
Mahadevan, S., Mitchell, T. M., Mostow, J., Steinberg, L. I., & Tadepalli, P. (1993). An apprenticebased approach to knowledge acquisition.. Artificial Intelligence, 64(1), 152.
Mitchell, T. M., Caruana, R., Freitag, D., J.McDermott, & Zabowski, D. (1994). Experience with a
learning personal assistant. Communications of the ACM, 37(7), 8091.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less
data and less time. Machine Learning, 13, 103130.
Mundhenk, M. (2001). The complexity of planning with partially-observable Markov Decision
Processes. Ph.D. thesis, Friedrich-Schiller-Universitdt.

103

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Myers, K., Berry, P., Blythe, J., Conleyn, K., Gervasio, M., McGuinness, D., Morley, D., Pfeffer,
A., Pollack, M., & Tambe, M. (2007). An intelligent personal assistant for task and time
management. In AI Magazine, Vol. 28, pp. 4761.
Natarajan, S., Tadepalli, P., & Fern, A. (2007). A relational hierarchical model for decision-theoretic
assistance. In Proceedings of the Seventeenth Annual International Conference on Inductive
Logic Programming, pp. 175190.
Papadimitriou, C., & Tsitsiklis, J. (1987). The complexity of Markov Decision Processes. Mathematics of Operations Research, 12(3), 441450.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: An anytime algorithm
for POMDPs. In Proceedings of the Eighteenth International Joint Conference on Artificial
Intelligence, pp. 1025  1030.
Porta, J., Vlassis, N., Spaan, M., & Poupart, P. (2006). Point-based value iteration for continuous
POMDPs. Journal of Machine Learning Research, 7, 23292367.
Pynadath, D. V., & Wellman, M. P. (2000). Probabilistic state-dependent grammars for plan recognition. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence,
pp. 507514.
Refanidis, I., Alexiadis, A., & Yorke-Smith, N. (2011). Beyond calendar mashups: Intelligent calendaring. In Proceedings of the Twenty-First International Conference on Automated Planning
and Scheduling System Demonstrations.
Shani, G., Pineau, J., & Kaplow, R. (2013). A survey of point-based POMDP solvers. Autonomous
Agents and Multi-Agent Systems, 27(1), 151.
Singh, S. P., Litman, D. J., Kearns, M. J., & Walker, M. A. (2002). Optimizing dialogue management with reinforcement learning: Experiments with the njfun system.. Journal of Artificial
Intelligence Research, 16, 105133.
Skaanning, C., Jensen, F. V., & Kjaerulff, U. (2000). Printer troubleshooting using bayesian networks. In Proceedings of the Thirteenth International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, pp. 367379.
Varakantham, P., Maheswaran, R. T., & Tambe, M. (2005). Exploiting belief bounds: practical
POMDPs for personal assistant agents. In Proceedings of the Fourth Internation Conference
on Autonomous Agents and Multiagent Systems, pp. 978985.
Walker, M. A. (2000). An application of reinforcement learning to dialogue strategy selection in a
spoken dialogue system for email. Journal of Artificial Intelligence Research, 12, 387416.
Yorke-Smith, N., Saadati, S., Myers, K., & Morley, D. (2012). The design of a proactive personal
agent for task management. International Journal on Artificial Intelligence Tools, 21(1), 90
119.

104

fiJournal of Artificial Intelligence Research 50 (2014) 923970

Submitted 06/14; published 08/14

Belief Tracking for Planning with Sensing: Width,
Complexity and Approximations
Blai Bonet

bonet@ldc.usb.ve

Departamento de Computacion
Universidad Simon Bolvar
Caracas, Venezuela

Hector Geffner

hector.geffner@upf.edu

ICREA & Universitat Pompeu Fabra
Roc Boronat 138
08018 Barcelona, Spain

Abstract
We consider the problem of belief tracking in a planning setting where states are valuations over a set of variables that are partially observable, and beliefs stand for the sets
of states that are possible. While the problem is intractable in the worst case, it has been
recently shown that in deterministic conformant and contingent problems, belief tracking is exponential in a width parameter that is often bounded and small. In this work,
we extend these results in two ways. First, we introduce a width notion that applies to
non-deterministic problems as well, develop a factored belief tracking algorithm that is exponential in the problem width, and show how it applies to existing benchmarks. Second,
we introduce a meaningful, powerful, and sound approximation scheme, beam tracking, that
is exponential in a smaller parameter, the problem causal width, and has much broader applicability. We illustrate the value of this algorithm over large instances of problems such
as Battleship, Minesweeper, and Wumpus, where it yields state-of-the-art performance in
real-time.

1. Introduction
Planning with incomplete information can be formulated as a search problem in belief space
where two issues need to be addressed: keeping track of beliefs, and searching for a goal
belief (Bonet & Geffner, 2000). While the two tasks are intractable in the worst case
over compact representations, this is the approach adopted in most recent conformant and
contingent planners where beliefs are handled using SAT, regression techniques, or logical
normal forms such as CNF, DNF, and OBDDs, and the search for goal beliefs is guided
by domain-independent heuristics (Bertoli, Cimatti, Roveri, & Traverso, 2001; Hoffmann &
Brafman, 2006; Bryce, Kambhampati, & Smith, 2006; To, Pontelli, & Son, 2011; Shani &
Brafman, 2011; Brafman & Shani, 2012).
Recently, the complexity of belief tracking in deterministic conformant and contingent
planning has been shown to be exponential in a problem width parameter that is often
bounded and small (Palacios & Geffner, 2009; Albore, Palacios, & Geffner, 2009). The
bound follows from a family of translations developed for compiling planning problems over
beliefs into planning problems over states. The translations are exponential in the problem
c
2014
AI Access Foundation. All rights reserved.

fiBonet & Geffner

width, and for deterministic conformant problems result in problems that can be solved by
classical planners.
The difficulty in extending the results of Palacios, Albore, and Geffner to the nondeterministic setting is a consequence of the special role played by the initial situation in
deterministic problems. In such a case, all uncertainty, and in particular, the uncertainty
about observations, action preconditions, and goals, which is the one that matters in a
complete planner, is the result of the uncertainty about the initial situation. In the nondeterministic setting, on the other hand, uncertainty is produced dynamically as a result of
the application of non-deterministic actions. Moreover, while an uncertain initial situation
can always be modeled by a fully known initial situation and a dummy non-deterministic
action, the opposite transformation is not as simple. Indeed, non-deterministic effects can
be compiled into deterministic effects that are conditional on the value of hidden variables,
but the number of hidden variables required must grow then with the planning horizon
(Weld, Anderson, & Smith, 1998; Albore, Ramirez, & Geffner, 2010).
The aim of this work is the study of the computational complexity of belief tracking
in terms of novel width parameters that apply to both deterministic and non-deterministic
planning problems, and the formulation of practical approximate belief tracking algorithms
that can be efficient and effective even for problems with large width. We will achieve this
by considering two decomposition schemes for belief tracking, and three algorithms based
on these decompositions. More precisely, we introduce:
1. A width notion for planning that is in close correspondence with the notion introduced
by Palacios, Albore, and Geffner but which applies to non-deterministic problems as
well.
2. A first belief tracking algorithm, factored belief tracking, that is sound and complete
for both deterministic and non-deterministic problems P , and runs in time and space
exponential in the problem width w(P ). The algorithm is based on a decomposition
of the problem P into projected subproblems PX , one for every goal and precondition
variable X, each one including the variables that are relevant to X.
3. A second belief tracking algorithm, causal belief tracking, that is based on an alternative
decomposition scheme, where subproblems PX are defined for every goal, precondition,
and observable variable X, each one including the variables that are causally relevant to
X. The algorithm is sound and complete for a large and meaningful class of problems,
and while it is still time exponential in the problem width, it is space exponential in the
causal width of the problem that is often much smaller.
4. A final belief tracking algorithm, beam tracking that is a sound but incomplete approximation of causal belief tracking, and is often practical enough, even in problems with
large widths, as it runs in time and space that are exponential in the problem causal
width.
The power of the last algorithm, beam tracking, will be shown empirically over large
instances of problems such as Minesweeper, Battleship, and Wumpus, where state-of-the924

fiBelief Tracking for Planning with Sensing

art performance is obtained in real-time by combining the belief tracking algorithm with
simple heuristics for action selection.1
The organization of the paper follows this structure, preceded by an overview of the
relevant notation and background, and followed by a description of the experiments, a
discussion of related work, and a summary. The paper integrates results from two conference
papers (Bonet & Geffner, 2012b, 2013), providing proofs and additional details. The work
is related to other proposals for tractable forms of belief tracking in logical and probabilistic
frameworks (Doucet, Freitas, Murphy, & Russell, 2000; Amir & Russell, 2003), yet there are
two key differences. One is that we start with an exact account that is used to determine with
certainty whether the goal has been achieved or an action is applicable. The second is that
belief tracking accounts in planning do not have to be complete over all formulas. In order to
have a sound and complete planner, only the beliefs over observations, action preconditions,
and goals are required. This is important because observations, action preconditions, and
goals are given, and the structure of the actions, sensors, and goals can be exploited to
track those beliefs more efficiently. This observation is implicit in lazy belief tracking
schemes for planning with incomplete information that appeal to SAT-solvers (Hoffmann &
Brafman, 2006) or regression (Shani & Brafman, 2011). Well say more about related work
in Section 12.

2. Model
The model for planning with sensing is a simple extension of the model for conformant
planning where a goal is to be achieved with certainty in spite of uncertainty in the initial
situation or action effects (Goldman & Boddy, 1996; Smith & Weld, 1998). The model for
conformant planning is characterized by a tuple S = hS, S0 , SG , A, F i where
 S is a finite state space,
 S0 is a non-empty set of possible initial states, S0  S,
 SG is a non-empty set of goal states, SG  S,
 A is a set of actions with A(s) denoting the sets of actions applicable in s  S, and
 F is a non-deterministic state-transition function such that F (a, s) denotes the nonempty set of possible successor states that follow action a in s, for a  A(s).
A solution to a conformant model is an action sequence that maps each possible initial state
into a goal state. More precisely,  = ha0 , . . . , an1 i is a conformant plan if for each possible
sequence of states s0 , s1 , . . . , sn such that s0  S0 and si+1  F (ai , si ), i = 0, . . . , n  1,
action ai is applicable in si and sn is a goal state.
Conformant planning can be cast as a path finding problem over beliefs, defined as the
sets of states that are deemed possible at any time point (Bonet & Geffner, 2000). The
initial belief b0 is S0 , and the belief ba that results from an action a in a belief state b is:
ba = {s0 | there is a s  b such that s0  F (a, s)} ,

(1)

1. A real-time animation of the algorithm for several instances of Minesweeper can be seen in https:
//www.youtube.com/watch?v=U98ow4n87RA, while all the source code and graphical interfaces can be
obtained at http://code.google.com/p/belief-tracking.

925

fiBonet & Geffner

where the action a is applicable in b if it is applicable in each state s in b. In this formulation,
a conformant plan is an action sequence that maps the initial belief b0 into a goal belief bG ;
i.e., a set of goal states.
Contingent planning or planning with sensing is planning with both uncertainty and
feedback. The model for contingent planning is the model for conformant planning extended
with a sensor model. A sensor model is a function O(s, a) mapping state-action pairs
into observations tokens o. The expression o  O(s, a) means that token o is a possible
observation when s is the true state of the system and a is the last action done. The
observed token o provides partial information about the true but possibly hidden system
state as the same token may be possible in different states. If two different tokens o1
and o2 belong to O(s, a), it means that either one can be observed in s when a is the
last action. Sensing is deterministic or noiseless when O(s, a) contains one token, else it
is non-deterministic or noisy. The contingent model is similar to POMDPs (Kaelbling,
Littman, & Cassandra, 1999) but with uncertainty encoded through sets of states rather
than probability distributions.
Executions in the contingent setting are sequences ha0 , o0 , a1 , o1 , . . .i of pairs of actions
ai and observations oi . If b = bi is the belief state when the action ai is applied and oi is
the token that is observed, then the belief ba after the action a = ai is given by (1), and
the belief bi+1 = boa that follows from observing the token o is:
boa = {s | s  ba and o  O(s, a)} .

(2)

An execution ha0 , o0 , a1 , o1 , . . .i is possible if starting from the initial belief b0 , each action
ai is applicable in the belief bi (i.e., ai  A(s) for all s  bi ), for i  0, and each belief bi is
not empty.
In off-line contingent planning, an action selection strategy is sought that ensures that
all possible executions end up in a goal belief. In on-line contingent planning, an action
selection strategy is sought that ensures that the single execution that results from the
interaction with the real system or simulator, ends up in a goal belief. In both cases, the
action selection strategy can be expressed as a partial function  over beliefs, called a policy,
such that (b) is the action to do in belief b. The function is partial because it has to be
defined only over the initial belief b0 and some non-goal beliefs b; namely, those that can be
reached with  from b0 in off-line planning, and those that have been reached with  from
b0 in on-line planning.

3. Language
Syntactically, conformant problems can be expressed in compact form through a set of
state variables, which for convenience we assume to be multi-valued.2 More precisely, a
conformant planning problem is a tuple P = hV, I, A, Gi where V stands for the problem
variables X, each one with a finite and discrete domain DX , I is a set of clauses over the
V -literals defining the initial situation, A is a set of actions, and G is a set of V -literals
defining the goal. Every action a has a precondition P re(a) given by a set of V -literals, and
2. Multi-valued variables can be compiled into boolean variables but the compilation affects the syntactic
structure of the problem. In principle, such a structure could be recovered from boolean encodings but
this would result in a more complex formulation.

926

fiBelief Tracking for Planning with Sensing

a set of conditional effects C  E1 | . . . |En where C and each Ei are sets (conjunctions) of
V -literals. The conditional effect is non-deterministic if n > 1; else n = 1 and the effect is
deterministic.
A problem P = hV, I, A, Gi defines a conformant model S(P ) = hS, S0 , SG , A, F i, where
S is the set of possible valuations over the variables in V , S0 and SG are the sets of valuations
that satisfy I and G respectively, A(s) is the set of operators whose preconditions are true
in s, and F (a, s) is the non-deterministic transition function that results from collecting
the successor states that may follow from a by selecting one head Ei from each conditional
effect C  E1 | . . . |En whose body C is true in s.3
Contingent problems can be described by extending the syntactic description of conformant problems with a compact encoding of the sensor model. For this, we assume a set
V 0 of observable multi-valued variables Y , not necessarily disjoint with the state variables
V (i.e., some state variables may be observable), and formulas Wa (Y = y) over the state
variables, for each action a and each possible value y of each observable variable Y . The
formula Wa (Y = y) implicitly encodes the states over which the observation literal Y = y
is possible when a is the last action executed. The formulas Wa (Y = y) for the different y
values in DY must be logically exhaustive, as every state-action pair must give rise to some
observation Y = y. If in addition, the formulas Wa (Y = y) for the different y values are
logically exclusive, then every state-action pair gives rise to a single observation Y = y and
the sensing over Y is deterministic. If a state variable X is observable, then Wa (X = x) is
just the formula X = x.
A contingent problem P is a tuple P = hV, I, A, G, V 0 , W i that defines a contingent
model which is made of the conformant model hS, S0 , SG , A, F i determined by the first four
components in P , and the sensor model O(a, s) determined by the last two components,
where o  O(a, s) iff o is a valuation over the observable variables Y  V 0 such that Y = y
is true in o only if the formula Wa (Y = y) in W is true in s for y  DY .
This is a standard language for representing contingent problems in compact form featuring both incomplete information, and non-deterministic actions and sensors. Its two
distinctive features in relation to similar languages are the use of multi-valued variables,
and the distinction between state and observable variables.
As an illustration, if X encodes the position of an agent, and Y encodes the position of
an object that can be seen by the agent when X = Y , we can have an observable variable
Z  {Y es, N o} encoding whether
the object can be seen by the agent or
W
W not, defined by the
formulas Wa (Z = Y es) = lD (X = lY = l), and Wa (Z = N o) =  lD (X = lY = l),
where D is the set of possible locations and a is any action. This will be a deterministic
sensor. A non-deterministic sensor could be used if, for example, the agent cannot detect
0
the presence
W of the object at certain locations l  D . For this, it suffices to push the
disjunct lD0 (X = l) into the formulas characterizing Wa (Z = Y es) and Wa (Z = N o), so
that the two observations Z = Y es and Z = N o would be possible when the agent is in a
position l  D0 .
Since a conformant problem hV, I, A, Gi can be expressed as a contingent problem
hV, I, A, G, V 0 , W i with just one (dummy) observable variable Z, with Z 
/ V and domain
3. These conditional effects must be consistent in the sense that is explained below.

927

fiBonet & Geffner

DZ = {>}, and observation model Wa (Z = >) = true for every action a, we will focus
from now on on the more general contingent problem.
Likewise, for convenience, if a variable Y is boolean, we often represent the literals
Y = true and Y = f alse as Y and Y . Similarly, if variable Y is observable, unless stated
otherwise, we assume that the observation model for Y is deterministic so that the formula
Wa (Y = f alse) becomes the complement of the formula Wa (Y = true).

4. Belief Tracking Problem and Flat Belief Tracking Algorithm
An execution over the problem P = hV, I, A, G, V 0 , W i is a sequence ha0 , o0 , a1 , o1 , . . .i of
actions ai and observations oi such that each ai is in A and each observation oi is a full
valuation over observation variables in V 0 . An execution ha0 , o0 , . . . , an , on i is possible over
a problem P with a non-empty belief state b0 , if it generates a sequence of beliefs b0 , . . . , bn
such that the preconditions of the action ai are true in the belief bi , and the belief states
bi are not empty. The problem of belief tracking in contingent planning is the problem of
determining if an execution is possible and if the final belief state achieves the goal:
Definition 1. Belief tracking in planning (BTP) is the problem of determining whether an
execution ha0 , o0 , a1 , o1 , . . .i over a planning problem P = hV, I, A, G, V 0 , W i is possible, and
if so, whether the resulting belief state makes the goal G true.
A complete planner needs to solve this problem for determining which actions are applicable after a given execution, what observations may result, and whether the goal has
been achieved. The machinery that we will develop is aimed at the slightly more general
belief tracking problem over generalized executions: these are executions ha0 , o0 , a1 , o1 , . . .i
where the observations oi are partial rather than full valuations over the observable variables. Moreover, it suffices to consider generalized executions where the observations are
valuations over a single observable variable. Such observations oi can be represented by
observation literals `i :
Definition 2. Generalized belief tracking in planning (GBTP) is the problem of determining whether a generalized execution ha0 , `0 , a1 , `1 , . . .i over a planning problem P =
hV, I, A, G, V 0 , W i is possible, and if so, whether it achieves a given goal, precondition, or
observation literal.
Given a procedure for deciding GBTP, it is simple to decide BTP over an execution 
by calling the procedure for deciding GBTP over the generalized execution  0 that replaces
each observation oi by a sequence of the observation literals that are true in oi separated
by NO-OP actions (actions with no effects).
Proposition 3. BTP is polynomial-time reducible to GBTP.
While our interest is in belief tracking for planning, we will find it convenient to focus on
the generalized problem, as none of the belief update equations or algorithms is sensitive to
this distinction. For simplicity, however, we will just talk about belief tracking, and make
explicit the distinctions between BTP and GBTP, and between normal and generalized
executions, when needed.
928

fiBelief Tracking for Planning with Sensing

The plain solution to the belief tracking problem is given by the updates expressed in
Eqs. 1 and 2, where belief states are explicitly represented as sets of states, states are full
valuations over the state variables, and the actions, transition function, and observations
are obtained from the syntactic representation of the problem:
Definition 4. The flat belief tracking algorithm over an execution ha0 , o0 , a1 , o1 , . . .i and
problem P , starts with the belief b0 that contains the states that satisfy the initial situation,
setting the next belief state bi+1 to boa using (1) and (2) with b = bi , a = ai , and o = oi .
The complexity of flat belief tracking is exponential in the number of state variables. Yet,
often some state variables do not add up to the complexity of tracking beliefs. Syntactically,
this happens when a state variable X is initially known, all variables Y that are causally
relevant to X (see below) are initially known as well, and neither X nor any variable Y
causally relevant to X appears in the head of a non-deterministic effect. We say that these
variables are determined as their value in every reachable belief is known, and can be fully
predicted from the preceding actions and their preceding values. For example, the variable
that encodes the position of the agent in the Wumpus game is determined, as its initial
value is known and the effect of the actions on the variable is deterministic and depends
only on its previous value.
Formally, we define the set of variables that are determined in a problem to be the
largest set of state variables X in the problem that are initially known such that every
state variable X 0 that is causally relevant to X belongs to the set. This set of variables
is easily identifiable in low polynomial time. The complexity of flat belief tracking can be
then expressed as follows:
Theorem 5. Flat belief tracking is exponential in |VU |, where VU = V \ VK and VK is the
set of state variables that are determined in the problem.
Given this result, the first question that arises is how bad is the naive approach of flat belief
tracking. Interestingly, the following result for the decision problem shows that flat belief
tracking is not bad in the worst case:
Theorem 6. BTP and GBTP are Turing complete for the class PNP .
That is, BTP and GBTP can be decided in polynomial time using an oracle for NP (SAT,
for example), and every decision problem that can be decided in polynomial time with
such an oracle, can be decided in polynomial time with an oracle for BTP or GBTP. The
complexity class PNP includes the classes NP and coNP, and it is contained in PSPACE
(Sipser, 2006).

5. Structure and Width
It is possible to improve on the complexity of flat belief tracking over a specific problem
by exploiting the structure of the problem. Before introducing the graph that captures
this structure, it will be convenient to make explicit some assumptions that do not restrict
the generality of the approach but make the definitions simpler. First, we assume that the
formula I encoding the initial situation contains just positive or negative literals; i.e., unit
clauses only. This is not a restrictive assumption since any set of clauses can be encoded
929

fiBonet & Geffner

with the help of dummy observations. Second, we assume that non-deterministic effects
involve just one variable in their heads. Again, this can always be achieved by adding extra
variables and effects. For example, the non-deterministic effect X  Y  Z | Y  Z of an
action can be replaced by the deterministic effects X W  Y Z and X W  Y Z,
along with the non-deterministic effect true  W | W , where W is a new random boolean
variable that is initially unknown and changes randomly. Third, we assume that the problem
is consistent, meaning that the initial situation I is logically consistent so that the initial
belief state b0 is not empty, and that the effects of any action a are consistent so that the
heads of the deterministic conditional effects that are applicable in a reachable state s, along
with any choice of heads of the non-deterministic conditional effects that are applicable in
s, are jointly consistent.4 Last, we assume that every observable variable is relevant to a
variable appearing in some precondition or goal, with the notion of relevance to be spelled
below. Observable variables that dont comply with this condition can be eliminated from
the problem with no relevant information loss.
5.1 Relevance and Width
For a variable X, whether a state variable, an observable variable, or both, the immediate
causes of X are defined as follows:
Definition 7. A variable X is an immediate cause of a variable Y in a problem P , written
X  Ca(Y ), iff X 6= Y , and either X occurs in the body C of a conditional effect C 
E1 |    |En and Y occurs in a head Ei , 1  i  n, or Y is an observable variable and X
occurs in a formula Wa (Y = y) for some y  DY and some action a.
Basically, X is an immediate cause of Y when uncertainty about X may affect the
uncertainty about Y directly, not through other variables. X is not necessarily an immediate
cause of Y when X appears as the precondition of an action that affects Y , as preconditions
must be known with certainty, and hence, do not propagate uncertainty. The notion of
causal relevance is given by the transitive closure of the immediate cause relation:
Definition 8. X is causally relevant to Y in P if X = Y , X  Ca(Y ), or X is causally
relevant to a variable Z that is causally relevant to Y .
In order to test whether a given literal Z = z is known after a certain execution
ha0 , a1 , . . . , ai i of actions in the conformant setting, it is possible to show that one can
just progress the state over the variables X that are causally relevant to Z:
Proposition 9. Belief tracking in the deterministic or non-deterministic conformant setting is exponential in the maximum number of non-determined variables that are all causally
relevant to a variable appearing in an action precondition or goal.
This bound is closely related to the bound obtained by Palacios and Geffner in the
deterministic setting. Indeed, if we refer to the number of non-determined state variables
4. From a semantic point of view, this means that a state s0 is a possible successor of state s after an
action a applicable in s, i.e. s0  F (a, s), iff for every literal X = x true in s0 , X = x is in the head of a
deterministic or non-deterministic conditional effect of the action a whose body is true in s, or if X = x
is true in s, and there is no effect of the action a with X = x0 in the head, with x0 6= x, whose body is
true in s.

930

fiBelief Tracking for Planning with Sensing

that are causally relevant to X, as the conformant width of X, and set the width of P as
the maximum conformant width over the variables X that appear in action preconditions
or goals, Proposition 9 simply says that belief tracking for a non-deterministic conformant
problem is exponential in the problem width. This width notion, however, is not exactly
equivalent to the notion of Palacios and Geffner when used in the deterministic setting as
it is defined over variables rather than literals. We will say more about this distinction
below. In general, however, the two accounts yield similar widths over most deterministic
benchmarks.
In the contingent setting, there are variables whose uncertainty may affect a variable
Z but which are not causally relevant to Z. The situation is similar to the one arising in
Bayesian networks (Pearl, 1988), where relevance flows both causally, in the direction of
the arrows, and evidentially, from the observations against the direction of the arrows.
Definition 10. X is evidentially relevant to Y in P if X is an observable variable and Y
is causally relevant to X.
The notion of relevance captures the transitive closure of the (directional) causal and evidential relations:
Definition 11. X is relevant to Y if X is causally or evidentially relevant to Y , or X is
relevant to a variable Z that is relevant to Y .
Thus, a variable X = W1 is relevant to a variable Y = Wn iff there is a chain of variables
Wi , 1  i  n  1, such each variable Wi is causally or evidentially relevant to the next
variable Wi+1 in the chain. For example, if X is causally relevant to Y and Z, and Y is an
observable variable, then Y will be relevant to Z as Y is evidentially relevant to X and X
is causally relevant to Z.
Like in Bayesian networks, the relevance relations can be understood graph-theoretically.
Thus, if the directed edge Z  Y stands for Z being an immediate cause of Y , then X is
causally relevant to X 0 when there is a directed path from X to X 0 , and X is evidentially
relevant to X 0 when X is an observable variable, and there is a directed path from X 0
to X. In terms of Bayesian networks, the relevance relation takes the transitive closure
of the causal and evidential relationships, and encodes potential dependency given what
may be observed, using the information that certain variables will not be observed (are
not observable). Unlike Bayesian networks, this means however that the relevance relation
is not symmetric. Namely, a cause X of Y is relevant to Y , but Y is not automatically
relevant to X if it is not causally relevant to an observable variable Z, which may be Y
itself. The context of a variable is the set of variables in the problem that are relevant to
X:
Definition 12. The context of variable X, Ctx(X), denotes the set of state variables in
the problem that are relevant to X.
The width of a variable is defined as the number of state variables in its context that are
not determined:
Definition 13. The width of a variable X, w(X), is |Ctx(X)  VU |, where VU = V \ VK
and VK is the set of state variables that are determined.
931

fiBonet & Geffner

The width of a problem is then:
Definition 14. The width w(P ) of a conformant or contingent problem P , whether deterministic or not, is maxX w(X) where X ranges over the variables that appear in a goal or
action precondition in P .
The relation between width and complexity can be expressed as:
Theorem 15. Belief tracking in P is exponential in w(P ).
The proof for this theorem follows from the results below where an algorithm that
achieves this complexity bound is presented. The significance of the theorem is that belief
tracking over planning domains with width bounded by a constant becomes polynomial in the
number of problem variables. We will see examples of this below. This complexity bound is
similar to the ones obtained for deterministic conformant and contingent problems (Palacios
& Geffner, 2009; Albore et al., 2009). The main difference is that the new account applies
to non-deterministic problems as well. The new account is simpler and more general, but
as we will see, it is also slightly less tight on some deterministic domains.

6. Examples
We illustrate the definitions above with some benchmark domains, starting with DET-Ring
(Cimatti, Roveri, & Bertoli, 2004). In this domain, there is a ring of n rooms and an
agent that can move forward or backward along the ring. Each room has a window which
can be opened, closed, or locked when closed. Initially, the status of the windows is not
known and the agent does not know his initial location. In this domain the agent has no
means for obtaining information about the status of the windows or its position, and its
goal is to have all windows locked. A plan for this deterministic conformant problem is to
repeat n times the actions (close, lock, f wd), skipping the last f wd action. Alternatively,
the action f wd can be replaced by the action bwd throughout the plan. The state variables
for the problem encode the agent location Loc  {1, . . . , n}, and the status of each window,
W (i)  {open, closed, locked}, i = 1, . . . , n. The location variable Loc is (causally) relevant
to each window variable W (i), but no window variable W (i) is relevant to Loc or W (k)
for k 6= i, as W (i) is not causally relevant to an observable variable. None of the variables
is determined and the largest contexts are for the window variables W (i) that include two
variables, W (i) itself and Loc. As a result the width of the domain is 2, which is independent
of the number of state variables W (i) that grows with the number of rooms n. The causal
graph of the problem, where a directed edge X  Y means that X is an immediate cause
of Y is shown in Figure 1a.
NON-DET-Ring is a variation of the domain where the actions f wd and bwd of the
agent have a non-deterministic effect on the status of all windows that are not locked,
capturing the possibility of external events that can open or close unlocked windows. This
non-determinism has no effect on the causal graph over the variables. As a result, the
change has no effect on the contexts or domain width that remains bounded and equal to
2 for any number of rooms n.
The last version of the domain considered by Cimatti et al. is NON-DET-Ring-Key,
where a key is required to lock the windows. The initial position of the key is not known,
932

fiBelief Tracking for Planning with Sensing

Loc

Loc

W (1)

W (2)



W (n)

W (1)

(a) DET-Ring

W (2)

KLoc



W (n)

H

(b) CONT-NON-DET-Ring-Key

Figure 1: Causal graphs for the problems DET-Ring (left) and CONT-NON-DET-Ring-Key
(right). In the latter, the variable H is observable and tells us whether the key is being held
or not. An arc X  Y denotes that X is an immediate cause of Y . In these graphs, variables
in preconditions or goals are underlined and yellow colored, while observable variables are
enclosed in a blue circle.
yet if the agent tries to collect the key from a room and the key is there, the agent will
have the key. A conformant plan for this problem is to repeat the actions pick and f wd, n
times, skipping the last f wd action, following then the plan for DET-Ring. In NON-DETRing-Key, there is an additional state variable, KLoc  {1, . . . , n, hand}, that represents the
key location. The agent location Loc is relevant to KLoc which is relevant to each window
variable W (i). As a result, both the size of the contexts Ctx(W (i)) and the problem width
increase by 1. The width however remains bounded with a value of 3 independently of the
number of rooms n.5
In the presence of partial observability, the analysis is similar but it is necessary to
consider the relevance relationships that arise due to the presence of observable variables.
For example, one can express that the agent can always observe whether it is holding the
key or not, by having a boolean observable variable H with (deterministic) observation
model Wa (H = true) given by KLoc = hand, for all actions a. The only new relevance
relation among state variables that arises from adding this observable variable is between
Loc and KLoc, as both are causally relevant to H. Before, Loc was relevant to KLoc but
not the other way around. Yet this does not affect the domain width that remains 3 for
any n. The causal graph of the resulting domain is shown in Figure 1b.

7. Factored Belief Tracking
Belief tracking over a problem P is exponential in the width w(P ) of P . The algorithm
that achieves this bound exploits the relevance relations encoded in the variable contexts
for decomposing beliefs. In particular, if no variable is relevant to any other variable,
the problem width is 1, and beliefs over each variable can be maintained separately. The
belief decomposition is obtained from projecting the problem P into smaller problems PS
where S is a set of state variables in P . Semantically, the projected problems PS capture the
dynamics of the problem P when expressed over a subset S of state variables. Syntactically,
the projected problems PS are defined by means of the logical notion of projection. The
5. The problem can also be encoded by making holding key a precondition rather than a condition for
locking the windows. In such an encoding, the variable KLoc is no longer relevant to the window
variables W (i) according to the definitions, as then KLoc = hand must be known with certainty, and
hence uncertainty about the windows variables W (i) is not affected by uncertainty about KLoc. The
result is that in such an encoding, the domain width reduces to 2.

933

fiBonet & Geffner

logical projection of a formula F over a subset S of its variables refers to the formula F 0
defined over the variables in S, such that the valuations that satisfy F 0 are exactly those that
can be extended into valuations that satisfy F (Darwiche & Marquis, 2002). Likewise, the
projection of a conditional effect C  E 1 |    |E n is the conditional effect CS  ES1 |    |ESn
where the body C and the effects E i are replaced by their logical projections CS and ESi
respectively.
Definition 16. The projection of problem P = hV, I, A, G, V 0 , W i over a set of variables
S  V is the problem PS = hVS , IS , AS , GS , VS0 , WS i where VS is S, IS and GS are the
initial and goal formulas I and G logically projected on the variables in S, AS is A but with
the preconditions and conditional effects projected over S, VS0 is V 0 , and WS is the set of
formulas Wa (Y = y) in W logically projected on the variables in S.
The notion of a projected planning problem has been used before in the setting of
classical planning for introducing a class of admissible heuristics known as pattern databases
(Edelkamp, 2001). Here we use it in the richer contingent setting for decomposing the belief
tracking problem over P into the belief tracking problem over smaller problems PS obtained
from P by projecting away some of the state variables in P .
Before defining the target subproblems PS of the decomposition, notice that variables
Y that are both state and observable variables in P but are not in S, will belong to VS0 but
not to VS , meaning that they will be just observable variables in the projected problem PS .
Moreover, the formulas for such variables Y in WS will become Wa (Y = y) = true for all
y  DY , meaning that in the problem PS , the observations Y = y will be possible for any
y, regardless of the state and last action done. Such observations will thus be completely
irrelevant in PS and will have no effect. In any case, P and PS share the same set of actions
and the same set of observations even if some of the actions and observations in PS may be
defined over a smaller set of state variables.
The target subproblems PS are defined in terms of the set of state variables that are
relevant to precondition and goal variables. Recall that we assume that each observable
variable in the problem is relevant to some action precondition or goal, as else the variable
could be safely removed.
Definition 17. The projection of a problem P over a variable X, denoted as PX , is the
projection PS of P over the set of variables S = Ctx(X), where Ctx(X) is the context for
X in P ; i.e., the set of state variables in P that are relevant to X.
Two basic properties of the projected problems PX are:
Proposition 18. If variable X appears in a goal or precondition, then the number of state
variables in PX that are not determined is bounded by w(P ).
Proposition 19. If an execution ha0 , o0 , a1 , o1 , . . .i is possible in P , then it is also possible
in PX for any state variable X in P .
If b is the belief that results after an execution in P , we will call bX the belief that
results after the same execution in the projected problem PX . The completeness of the
decomposition of the global belief b over P is expressed in terms of the local beliefs bX
over the subproblems PX . We treat the beliefs b and bX as relations in a database where
934

fiBelief Tracking for Planning with Sensing

the state variables in these beliefs are the columns and the possible combination of values
(states and local states) are the rows. The projection Y b, for a set of variables Y thus
represents the combination of values of the variables in Y that are possible in b, while the
join bX o
nbY represents the combination of values x and y over the sets of variables in the
two beliefs bX and bY such that x and y coincide over the variables that are in both X and
Y . For example, if b contains the valuations (states) X = 1, Y = 1 and X = 2, Y = 2, the
projection {X} b will contain the valuations X = 1 and X = 2. Likewise, if b0 contains
Y = 1, Z = 1 and Y = 1, Z = 2, the join b o
n b0 will contain X = 1, Y = 1, Z = 1 and
X = 1, Y = 1, Z = 2.
Theorem 20. For a state variable X, let b and bX be the beliefs that result from an execution
that is possible over both P and PX . Then,
X bX = X b .

(3)

Equation 3 states that a literal X = x is possible in the true global belief b iff it is
possible in the belief bX that results from the same execution in the projected problem
PX . This is exactly the type of completeness that is needed in planning for any variable
X involved in an action precondition or goal. The stronger form of completeness over all
formulas, that can be expressed as

o
nX bX = b ,

(4)

where o
n stands for the join operation and X ranges over all precondition and goal variables
in the problem, is not needed, and it is actually not necessarily true, even when all state
variables appear in some context Ctx(X). For example, if the value of the boolean variable
Z is initially unknown, and variables X and Y are initially false, an action a with conditional
effects Z  X  Y and Z  Z results in a belief b with two states, corresponding to the
terms Z  X  Y and Z  X  Y . If X and Y are precondition or goal variables such that
they are not relevant to each other, the projected problem PX will contain the variables
X and Z, and the projected problem PY will contain the variables Y and Z. The belief
bX resulting from the execution of the action a in PX will include then the local states
corresponding to the terms Z  X and Z  X, while the belief bY over PY will include the
local states corresponding to the terms Z  Y and Z  Y . Clearly, the projection of b and
bX (bY ) over the variable X (Y ) coincide as dictated by (3), but the join of the two local
beliefs bX and bY does not yield the global belief b as would correspond to (4); indeed, a
formula like X  Y is false in the latter but not in the former. From (3), we can prove
inductively on the size of any execution that:
Theorem 21. 1) An execution is possible in P iff it is possible over each of the subproblems
PX for X being a precondition or goal variable in P . 2) For an execution  and precondition
or goal variable X, X = x (resp. X 6= x) is true in b iff X = x (resp. X 6= x) is true in
bX , where b and bX are the beliefs that result of executing  in P and PX respectively.
Since plain belief tracking over each projected problem PX is exponential in the size of
PX , which is bounded by w(P ) once the determined variables are excluded, it follows that:
935

fiBonet & Geffner

Theorem 22. Flat belief tracking over each of the projected problems PX for X being a
precondition or goal variable in P , provides a sound and complete factored algorithm for
belief tracking over P that is time and space exponential in the width of P .
We call this algorithm, factored belief tracking. In order to check whether a precondition
or goal literal X = x is true after an execution, factored belief tracking checks whether
X = x is true in the belief bX that results from the execution over the subproblem PX . An
execution is not possible if an action precondition X = x is not true in bX or if it results in
an empty belief over some subproblem. Theorem 22 thus says that factored belief tracking is
a sound and complete algorithm for BTP with time and space complexity exponential in the
problem width. Indeed, since every observable variable Y is relevant to some precondition
or goal variable X by assumption, then every direct cause Z of Y is relevant to X because
Y is evidentially relevant to X. Thus, any formula Wa (Y = y) can be evaluated in bX to
determine whether the observation Y = y is necessary, possible or impossible after applying
the action a. Thus, factored belief tracking also solves the generalized BTP problem.
As an illustration of Theorem 22, let us go back to the DET-Ring problems P whose
structure was analyzed before. The theorem implies that in order to check whether a given
possible execution achieves the goal in P , it is sufficient to check whether each goal literal
W (i) = locked, for 1  i  n, is achieved by the execution over the subproblem PW (i) . Thus,
factored belief tracking over P can be done in O(n2 ) time since there are n subproblems
PW (i) , each one involving 2 variables: W (i) with a constant-size domain and Loc with a
domain of size n.
The exact same situation arises in the non-deterministic conformant problem NON-DET
Ring whose causal graph is the same as the one for DET-Ring. On the other hand, for NONDET-Ring-Key, all the subproblems must keep track of the KLoc variable encoding the key
location, and thus a belief update operation requires O(n3 ) time, which is still much better
than flat belief tracking over P which requires time exponential in n. The same complexity
results applies when the problem is no longer conformant and the agent can observe whether
it is holding the key or not.
Some experimental figures for these domains are shown in Table 1, where factored belief
tracking was used in combination with simple heuristics. The experiments were run on a
Xeon Woodcrest 5140 CPU running at 2.33 GHz and with 8 GB of RAM. The planner
KACMBP by Cimatti et al. uses an OBDD-based belief representation and cardinality
heuristics, and can solve problems with up to n = 20 rooms, producing plans with 206
steps in slightly more than 1,000 seconds in NON-DET-Ring-Key. Conformant planners
such as T0 (Palacios & Geffner, 2009) cannot be used as the problem is non-deterministic.
Tables 1a and 1b show the scalability of the factored belief tracking algorithm in the context
of a greedy best-first search with a P
heuristic h(b), similar to the one used by Albore, Ramirez,
and Geffner (2011), where h(b) = ni=1 h(bi ), with bi being the belief factor in the projected
problem for the goal variable W (i) representing the status of the ith window, and h(bi )
representing the fraction of states in bi where the goal W (i) = locked is false. As displayed
in the tables, the resulting planner scales up polynomially, and for NON-DET-Ring-Key
with 100 rooms, produces a plan with 1, 111 actions in 783.1 seconds. For the contingent
version of the problem in which the agent detects when the key is in the room, CONT-DETRing-Key, a policy greedy in the cardinality heuristic h(b) = maxni=1 |bi | is used instead, with
936

fiBelief Tracking for Planning with Sensing

n

steps

exp.

time

n

steps

exp.

time

n

avg. steps

avg. time

10
20
30
40
50
60
70
80
90
100

68
138
208
277
345
415
476
545
610
679

355
705
1,055
1,400
1,740
2,090
2,395
2,740
3,065
3,410

< 0.1
0.1
0.9
3.1
8.3
18.6
34.5
62.8
106.4
171.0

10
20
30
40
50
60
70
80
90
100

118
198
278
488
438
468
543
616
682
1,111

770
1,220
1,670
3,210
2,570
2,660
3,080
3,480
3,880
7,220

< 0.1
0.8
4.2
15.2
34.4
52.2
100.6
172.9
285.6
783.1

10
20
30
40
50
60
70
80
90
100

326.8  4.3
1, 036.0  13.5
2, 068.0  26.5
3, 462.9  47.2
5, 130.7  71.0
7, 070.9  100.9
9, 334.1  127.6
11, 724.0  162.2
14, 617.4  204.6
17, 891.2  252.3

0.0
0.1
0.5
1.8
4.4
9.3
17.5
30.6
50.0
79.0

(a) DET-Ring-Key

(b) NON-DET-Ring-Key

(c) CONT-DET-Ring-Key

Table 1: Results for conformant and contingent Ring problems obtained by combining factored belief tracking with simple heuristics. Each data point in panel (c) for the contingent
problem is the average (and sample standard deviation) over 1,000 random instances. Times
are in seconds. The column exp. contains number of expansions.

ties broken randomly, where bi is the belief factor for the goal variable W (i). As it can be
seen in Table 1c, the resulting planner runs in polynomial time and can solve problems with
up to 100 rooms. Thus, while the heuristic and policy are weak, and long executions result,
belief tracking in this problem is efficient and scales up well.

8. Causal Belief Tracking
Factored belief tracking is exponential in the problem width. In many problems, however,
the width may be too high for the method to be usable in practice. As an illustration,
consider a problem P with state variables X1 , . . . , Xn+1 , and observable variables O1 , . . . , On
such that Oi is true iff Xi = Xi+1 . The sensors are thus Wa (Oi = true) = (Xi = Xi+1 )
and Wa (Oi = f alse) = (Xi 6= Xi+1 ) for all actions a and 1  i  n. Let us also assume
that the actions in the problem may affect each of the Xi variables but do not introduce
causal relations among them, and that all the state variables appear in preconditions or
goals. The causal graph of the problem is shown in Figure 2. Its width is n + 1 as all the
state variables interact. Indeed, each variable Xi is relevant to each variable Xk , with the
relevance flowing from Xi to Xi+1 , and vice versa, as both variables are causally relevant
to the observable variable Oi which is evidentially relevant to both. The result is that the
problem P and the projected problems PXi all coincide and denote the same problem, as
the contexts for each of the state variables include all state variables.
We now focus on a different decomposition for belief tracking that maps a problem P
into smaller subproblems PXc whose size is bounded by the number of state variables that are
all causally relevant to a given precondition, goal, or observation variable. This new width
measure will be called the causal width of the problem. The problem shown in Figure 2 has
width n + 1 but causal width 2. We will then explore belief tracking algorithms that are
exponential in the problem causal width and analyze the conditions under which they are
937

fiBonet & Geffner

X1

X2

X3



Xn

O1

O2

O3



On

Xn+1

Figure 2: Causal graph for the 2-layer network example with state variables X1 , . . . , Xn+1
and observable variables O1 , . . . , On+1 . The immediate causes of each observable Oi are
the variables Xi and Xi+1 . Precondition or goal variables appear as underlined and in a
yellow box, while observable variables appear within a blue circle. Since all Xi variables
are relevant to each other, the width of the problem is n + 1. On the other hand, since at
most two variables are causally relevant to a precondition, goal, or observable variable, the
causal width of the problem is 2.
complete. For this, we first generalize and make explicit the decomposition underlying the
factored belief tracking algorithm:
Definition 23. A decomposition of a problem P is a pair D = hT, Bi, where T is a set of
variables X appearing in P , called the target variables of the decomposition, and B is the
collection of beams B(X) associated with each such target variable which are made up of
state variables from P .
A decomposition D = hT, Bi maps P into a set of subproblems PXD , one for each variable
X in T , that corresponds to the projections of P over the state variables in the beam B(X).
The decomposition that underlies factored belief tracking is:
Definition 24. The factored decomposition F = hTF , BF i of P is the decomposition with
target variables TF given by the state variables X appearing in action preconditions or goals,
and beams BF (X) given by the state variables Y that are relevant to X.
Factored belief tracking is flat belief tracking applied to the subproblems determined by
the factored decomposition. The algorithms that we introduce next are based on a different
decomposition:
Definition 25. The causal decomposition C = hTC , BC i of P is the decomposition with
target variables TC given by the observable variables and the state variables appearing in
action precondition or goals, with beams BC (X) given by the state variables Y that are
causally relevant to X.
The causal decomposition determines a larger number of subproblems, as subproblems
are also generated for the observable variables, but these subproblems have smaller beams
BC (X), as they only contain the state variables that are causally relevant to X as opposed
to the variables that are relevant to X. The causal width of a problem is given by the size of
the largest beam in the causal decomposition, discounting the variables that are determined
in the problem:
Definition 26. The causal width of a variable X in a problem P , wc (X), is the number of
state variables that are causally relevant to X and are not determined. The causal width
938

fiBelief Tracking for Planning with Sensing

of P is maxX wc (X), where X ranges over the target variables in the causal decomposition
of P .
The first and simplest belief tracking algorithm defined over the causal decomposition
is what we call Decoupled Causal Belief Tracking, which runs in time and space that are
exponential in the problem causal width:
Definition 27. Decoupled causal belief tracking (Decoupled CBT) is flat belief tracking
applied independently to each of the problems PXC determined by the causal decomposition
C = hTC , BC i of P . The subproblem PXC is the problem P projected on the variables in
BC (X) for X  TC ; i.e., PXC = PBC (X) .
Since causal width is never greater than width and is often much smaller, Decoupled
CBT runs much faster than factored belief tracking in general. This, however, comes at a
price that we express using the expression S b for denoting the projection of (the states in
the) belief b over the variables in S.
Theorem 28. Decoupled CBT runs in time and space that are exponential in wc (P ), and
it is sound but not complete. That is, for any target variable X in the causal decomposition,
if b and bX are the beliefs resulting from an execution on P and PXC respectively, then
bX  BC (X) b is necessarily true, but bX  BC (X) b is not.
One reason for the incompleteness is that the beliefs bX associated with different target
variables X are assumed to be independent in Decoupled CBT while this may not be
true. Indeed, the causal decomposition of a problem may give rise to a beam BC (Y )
involving variable X, and a second beam BC (Z) involving the same variable X and another
variable X 0 . If variable Y is then observed, X = x may become false, which from a further
observation on Z may lead to X 0 = x0 becoming false as well. Yet, in Decoupled CBT, this
inference cannot be captured as there is no information flow across beams. In the factored
decomposition a situation like this cannot happen as variable X 0 will be relevant to variable
X and hence beams that contain X will necessarily contain X 0 (X 0 is relevant to X because
its causally relevant to Z which is evidentially relevant to X).
In the causal decomposition, beams are kept small by not closing them with the relevance
relation, but as a result, the beliefs over such beams are no longer independent. However,
regarding the beliefs as tables or relations, a consistency relation among the local beliefs
in the causal decomposition can be enforced by means of the join operation. The resulting
algorithm is Coupled Causal Belief Tracking, abbreviated simply as Causal Belief Tracking:
Definition 29. Causal Belief Tracking (CBT) is the belief tracking algorithm that operates
on the causal decomposition C = hTC , BC i by setting the beliefs b0X at time 0 for each beam
BC (X) to the projection on BC (X) of the initial belief, X  TC , and the successive beliefs
bi+1
X as:
bi+1
n{(biY )oa : Y  TC and Y is relevant to X}
(5)
X = BC (X) o
where a = ai and o = oi are the action and observation at time i in the execution, and
(biY )oa is boa from Eqs. 1-2 with b = biY .
In CBT, the beliefs are not tracked independently over each of the subproblems PXC of
the causal decomposition; rather, the beliefs are first progressed and filtered independently,
939

fiBonet & Geffner

but are then merged and projected back onto the beams, making them consistent with each
other. The progression and filtering of the local beliefs in the causal decomposition is performed in time and space exponential in the problem causal width, but the full consistency
operation captured by the join-project operation in (5) requires time that in the worst case
is exponential in the problem width:
Theorem 30. CBT is space exponential in the causal width of the problem, and time
exponential in its width.
CBT is sound but incomplete. However, the range of problem for which CBT is complete,
unlike Decoupled CBT, is large and meaningful enough, and it includes for example three
of the domains to be considered in the experiments below: Battleship, Minesweeper and
Wumpus. We express the completeness conditions for CBT by introducing the notion of
memory variables:
Definition 31. A state variable X is a memory variable in problem P when the value X k of
the variable X at time point k in an execution is determined uniquely from an observation
of the value X i of X at any time point i, i  k, the actions in the execution, and the initial
belief state of the problem.
For example, static variables are memory variables as they do not change and thus
knowing their value at any time point determines their value at any other point. Determined
variables (Section 4) are also memory variables since the value X k of such variables is
determined by the initial belief and the actions done up to time k. Likewise, variables in
permutation domains where actions permute the values of the variables (Amir & Russell,
2003), are also memory variables. These are three sufficient conditions for a state variable
to be a memory variable that are all easy to check. A problem is said then to be causally
decomposable when the following condition holds:
Definition 32. A problem P is causally decomposable when for every pair of beams BC (X)
and BC (X 0 ) in the causal decomposition of P with a non-empty intersection, where X 0 is
an observation variable, either 1) the variables in the intersection are all memory variables,
or 2) there is a variable W in the causal decomposition that is relevant to X or X 0 and
whose causal beam BC (W ) contains both BC (X) and BC (X 0 ).
If the problem is causally decomposable, the filtering implemented by the updates in
CBT using Equation 5 suffices for completeness:
Theorem 33. Causal belief tracking is always sound and it is complete for causally decomposable problems.
The importance of this result is that there are many meaningful domains whose problem
instances are causally decomposable; in particular, domains where all variables that appear
in two different beams are static (this include Minesweeper), domains where all variables
that appear in two different beams are either static or determined (this includes Wumpus,
where the non-static variable for the agent location is determined), domains where the
hidden non-static state variables only appear in one beam (this includes Battleship where
the hidden non-static variables do not appear in intersection of beams), and other cases
as well. In Sect. 11.4, we present a variation of Wumpus in which the monster moves
non-deterministically in the grid and that it is also an instance of a causally-decomposable
problem.
940

fiBelief Tracking for Planning with Sensing

9. Approximation: Beam Tracking
The causal belief tracking algorithm shows that it is possible to track beliefs for planning in a
sound and complete manner for a large and meaningful class of problems, while considering
the beliefs over subproblems that are smaller than those in the factored decomposition.
The algorithm, however, while space exponential in the causal width of the problem, it is
time exponential in the problem width. This is because of the global consistency operation
enforced by (5). Beam tracking is the final belief tracking algorithm that we consider:
it replaces this global consistency operation by a local consistency operation that can be
performed in polynomial time. Beam tracking is thus an approximation of causal belief
tracking which is aimed at being efficient and effective rather than complete.
Definition 34. Beam tracking is the belief tracking algorithm that operates on the causal
decomposition C = hTC , BC i, setting the beliefs b0X at time 0 to the projection of the initial
belief over the beam for X  TC , and setting the successive beliefs bi+1
X in two steps. First,
i
o
they are set to the progressed and filtered belief ba for b = bX , a = ai and o = oi , where
ai and oi are the action and observation at time i in the execution. Then, a local form
of consistency is enforced upon these beliefs by means of the following updates until a fixed
point is reached:
i+1
(6)
bi+1
nbi+1
Y )
X = BC (X) (bX o
where Y refers to any other target variable in the causal decomposition such that BC (Y ) 
BC (X) is non-empty.
The filtering represented by the iterative update in Eq. 6 defines a form of relational
arc consistency (Dechter & Beek, 1997) where equality constraints among beams sharing
common variables is enforced in polynomial time and space in the size of the beams. Beam
tracking remains sound but is not complete. In causally decomposable problems, however,
the incompleteness is the sole result of replacing global by local consistency.

10. Extensions, Modeling, and Width
Before testing the beam tracking algorithm empirically, we present two simple extensions
to the language of contingent planning that are useful for modeling, and briefly discuss
modeling choices that affect the causal width of a problem. The first extension allows the
use of defined variables in preconditions and goals; the second extension allows the use of
state constraints for restricting the possible value combination of subsets of variables.
10.1 Defined Variables
A variable Z with domain DZ can be defined as a function of a subset of state variables in the
problem, or as a function of the belief over such variables. For example, a boolean variable
Z can be defined as true when two variables X and Y are equal, or when a third variable W
is known to be true. Defined variables Z that are a function of a set SZ of state variables
or a function of the belief over such variables, can then be handled in action preconditions
and goals by introducing a beam in the decomposition that includes the variables in SZ
along with the variables that are relevant or causally relevant to them, according to whether
the decomposition is factored or causal. The width and causal width of the problem follow
941

fiBonet & Geffner

then, as before, as the size of the largest beam in the factored and causal decompositions
with the determined variables excluded.
10.2 State Constraints
State constraints are used to restrict the value combinations of given subsets of state variables. The game of Battleship, for example, can be modeled with state variables associated
with each of the cells in a grid for representing whether the cell is part of a ship, the size of
the ship to which the cell belongs (if any), the relative position of the cell within the ship
to which the cell belongs (if any), and whether such a ship is placed vertically or horizontally. These state variables, however, are not independent, and indeed, if a ship of size 10
is horizontally placed at cell (0, 0), the cells (0, i), for i  {0, 1, . . . , 9} must belong to (the
same) ship.
Formally, a state constraint represented by a formula C over the state variables can
be encoded by means of a dummy observable variable Y that is always observed to be
true, and that can be observed to be true only in states where C holds; i.e., with model
Wa (Y = true) = C for every action a. For the implementation, however, it pays off to treat
such constraints C as relations (the set of valuations that satisfy C), and to include them
in all the joins over the beliefs that include the variables in C. In causal belief tracking
this has no effect on the completeness or complexity of the algorithm, but in beam tracking,
changing the update in (6) to
i+1
n C1 o
n  o
n Cn )
n bi+1
bi+1
Y o
X = BC (X) (bX o

(7)

where C1 , . . . , Cn are the state constraints whose variables are included in BC (X)  BC (Y ),
makes local consistency stronger with no effect on the complexity of the algorithm. Moreover, when there is one such pair of beams for each state constraint, the state constraints
can increase the causal width of the problem by a constant factor of 2 at most, yet the
effective causal width of the problem does not change, as the beams associated with the
dummy observables introduced for such constraints are redundant and can then be ignored.
In this later case, when using beam tracking, the constraints Ci do not need to be stored
in extensional form as relations but can be handled intentionally as boolean functions that
test whether an assignment in the join of two beams satisfies the constraint.
10.3 Modeling and Width
The complexity of the belief tracking algorithms is a function of the width or causal width
of the problem, which in turns depends on the way the problem is encoded. Often small
changes in the encoding can have a drastic effect on the resulting widths. For example, in
the Wumpus problem (Russell & Norvig, 2009), it is natural to define the conditions under
which the stench signal can be received by setting its observation model to:

W
W
Wa (stench = true) = c (pos = c)  c0 wumpc0
where pos encodes the agent position, c ranges over the possible cells, c0 ranges over the cells
that are adjacent to c, and wumpc0 denotes the presence of a wumpus at c0 . This encoding,
however, results in a beam for the observable variable stench that includes all the wumpc
942

fiBelief Tracking for Planning with Sensing

szx,y

hitx,y

waterx,y

nhitsx,y

ancx,y

hzx,y

Figure 3: Causal graph fragment for Battleship. Circled variables are observable while the
others are state variables. The problem has one type of variable for each cell (x, y) on the
grid. Causal width for the problem is 5.
variables, and hence whose size grows with the grid size. A better alternative that results
in beams of bounded causal width is to exploit the fact that the position of the agent pos
is determined. Taking advantage of this, the observable variable stench can be replaced by
observable variables stenchc , one for each cell in the grid, with sensors characterized by the
model:
Wa (stenchc = true) = (pos = c) 

W

c0

wumpc0 .

The beams for the stenchc variables contain at most four wumpc0 variables, one for each cell
c0 adjacent to c. In this way, the causal width of the Wumpus problem becomes bounded
and independent of the grid size, and of the number of wumpus and pits (see below).
The idea
W can be generalized and automated. Any observation model of the form Wa (Z =
z) = x (x)  (x), where (x) is a formula constructed from determined variables,
can be replaced by observation models Wa (Zx = z) = (x)  (x) by expanding the
number of observable variables. Likewise, multiple observation models Wai (Z = z) = i
for one observable variable Z and different actions {ai }iR can be conveniently replaced by
observation models Wai (Zi = z) = i , i  R for different observable variables Zi , when the
different i formulas involve different variables. These alternatives in the domain encoding
can be the difference between bounded and unbounded causal width, and hence, on whether
the complexity of beam tracking will grow polynomially or exponentially.

11. Experiments
We have tested beam tracking over large instances of Battleship, Minesweeper, and Wumpus, in combination with simple heuristics for action selection that make use of the computed beliefs. The width of these problems is not bounded, and hence, neither factored
or causal belief tracking can be used except over small instances. On the other hand, all
these domains have small and bounded causal widths in the encodings provided, and hence
beam tracking runs efficiently in both time and space. Exact belief tracking in some of
these domains is difficult (Kaye, 2000; Scott, Stege, & Rooij, 2011), and the sizes of the
instances considered are much larger than those used in contingent planning. Moreover,
some of these domains do not have full contingent solutions. We thus compare our on-line
planner that relies on handcrafted heuristics with two reported solvers that rely on belief
tracking algorithms tailored to the domains. We also consider a non-deterministic version
of the Wumpus domain. The results have been obtained on a Xeon Woodcrest 5140 CPU
running at 2.33 GHz with 8GB of RAM.
943

fiBonet & Geffner

11.1 Battleship
Battleship is a popular two-player guessing game. The standard version consists of four
ships of length 2, 3, 4 and 5 units that are secretly placed on a 10-by-10 grid, with no ship
adjacent or diagonally adjacent to another. The task is to sink the ships by firing torpedos
at specific cells. For each fired torpedo, we are told whether the torpedo hits water or ship.
A ship is sunk when all its cells are hit. The problem is encoded with 6 state variables
per cell (x, y):6 hitx,y tells if a torpedo has been fired at the cell, szx,y tells the size of the
ship occupying the cell (0 if no such a ship), hzx,y tells if the ship is placed horizontally or
vertically (true if no such ship), nhitsx,y tells the number of hits on the ship (0 if no such
ship), and ancx,y tells the relative position of the ship on the cell (0 if no such ship). There
is a single observable boolean variable water with a deterministic sensor model given by
Wf ire(x,y) (waterx,y = true) = (szx,y = 0). The action model is more complex because firing
a torpedo at (x, y) may cause a change in the variables associated to other cells (x0 , y 0 ).
Indeed, if d denotes the maximum size of a ship (5 in the standard game), then f ire(x, y)
includes conditional effects for variables referring to cells (x0 , y 0 ) that are at a vertical or
horizontal distance of at most d units. The goal of the problem is to achieve the equality
nhitsx,y = szx,y over the cells that may contain a ship. State constraints are used for
constraining sets of state variables as described above. In this encoding, the causal beams
never contain more than 5 variables, even though the problem width is not bounded and
grows with the grid size. Figure 3 shows a fragment of the causal graph for Battleship.
Table 2 shows results for two policies: a random policy that fires at a non-fired cell at
random, and a greedy policy that fires at the non-fired cell most likely to contain a ship.
Approximations of these probabilities are obtained from the beliefs maintained by beam
tracking.7 The difference in performance between the two policies shows that the beliefs
are very informative. Moreover, for the 10  10 game, the agent fires 40.0  6.9 torpedos
in average, matching quite closely the average results of Silver and Veness (2010) that are
obtained with a combination of UCT (Kocsis & Szepesvari, 2006) for action selection, and
a particle filter (Doucet et al., 2000) hand-tuned to the domain for belief tracking. Their
approach, however, involves 65,000 simulation per action that result in the order of 2 seconds
per game over 10  10 instances, while our greedy approach takes 0.0096 seconds per game.
11.2 Minesweeper
The objective in Minesweeper is to clear a rectangular minefield without detonating a mine.
Each play either opens or flags a cell. In the first case, if the cell contains a mine, the game
is terminated; otherwise an integer counting the number of mines surrounding the cell is
revealed. An initial configuration for minesweeper consists of a m  n minefield with k
randomly-placed mines. There are three standard difficulty levels for the game that are
made up of 8  8, 16  16 and 16  30 boards with 10, 40 and 99 mines respectively.
6. This is a rich encoding that allows to accommodate the observation that a ship has been fully sunk. In
the experiments, however, this observation is not used in order to compare with the results reported by
Silver and Veness (2010).
7. Probabilities for events defined by the variables in a beam are obtained by the ratio of number of states
in the beam that satisfy the event to the total number of states in the beam.

944

fiBelief Tracking for Planning with Sensing

avg. time per
dim

policy

#ships

#torpedos

decision

game

10  10
20  20
30  30
40  40

greedy
greedy
greedy
greedy

4
8
12
16

40.0  6.9
163.1  32.1
389.4  73.4
723.8  129.2

2.4e-4
6.6e-4
1.2e-3
2.1e-3

9.6e-3
1.0e-1
4.9e-1
1.5

10  10
20  20
30  30
40  40

random
random
random
random

4
8
12
16

94.2  5.9
387.1  13.6
879.5  22.3
1,572.8  31.3

5.7e-5
7.4e-5
8.5e-5
9.5e-5

5.3e-3
2.8e-2
7.4e-2
1.4e-1

Table 2: Results for Battleship. The table contains results for the greedy and random
policies described in the text. For the 10  10 board, there are 4 ships of sizes 2, 3, 4 and 5.
As the size of the board is increased with n, the number of ships of each size gets multiplied
by n. Average and sample standard deviation for the number of torpedos required to sunk
all ships, calculated over 10,000 random instances for each board, are shown. Average times
are in seconds.

The problem is encoded with 3mn boolean state variables minex,y , openedx,y and
f laggedx,y that denote the presence/absence of a mine at cell (x, y) and whether the cell has
been opened or flagged, and mn observable variables obsx,y with domain D = {0, . . . , 9}.
There are two type of actions open(x, y) and f lag(x, y) where the first has no precondition and effect f laggedx,y  openedx,y , while the second has precondition minex,y and
effect f laggedx,y . The sensor model is given by formulas that specify the integer that the
agent receives when opening a cell in terms of the status of the minex0 ,y0 variables over the
surrounding cells. These formulas are:
Wopen(x,y) (obsx,y = 9) = minex,y ,
Wopen(x,y) (obsx,y = k) = minex,y 

W

tN (x,y,k) t ,

for 0  k < 9 ,

Wopen(x,y) (obsx0 ,y0 = k) = true ,

for (x0 , y 0 ) 6= (x, y) and 0  k  9 ,

Wf lag(x,y) (obsx0 ,y0 = k) = true ,

for each (x0 , y 0 ) and 0  k  9 ,

where N (x, y, k) are the terms over the 8 cell variables minex0 ,y0 surrounding the cell (x, y)
that make exactly k literals true. In the initial situation, the variables openedx,y and
f laggedx,y are false and minex,y is unknown. The goal of the problem is to get the disjunction f laggedx,y  openedx,y for each cell (x, y) without triggering an explosion.
The beams that result from the factored decomposition contain all the 3mn state variables, making all beams identical and resulting in an unbounded width of 3mn. The causal
width, on the other hand, is 9 as the causal beams for openedx,y and f laggedx,y are identical
and contain just 3 variables, while the beams for obsx,y contain the 9 minex0 ,y0 variables
for the cells (x0 , y 0 ) that surround the cell (x, y) along with the variable minex,y . Figure 4
contains a fragment of the causal graph for Minesweeper.
945

fiBonet & Geffner

minex0 ,y0

minex,y

f laggedx,y

openedx,y

obsx,y
Figure 4: Sketch of the causal graph for Minesweeper. There are observable variables obsx,y
and state variables minex,y , f laggedx,y and openedx,y for each cell (x, y). The cell (x0 , y 0 )
represents one of the adjacent cells to (x, y). Since there are 8 such cells, the causal width
of the problem is 9.
avg. time per
dim

#mines

density

%win

#guess

decision

game

88
16  16
16  30
32  64

10
40
99
320

15.6%
15.6%
20.6%
15.6%

83.4
79.8
35.9
80.3

606
670
2,476
672

8.3e-3
1.2e-2
1.1e-2
1.3e-2

0.21
1.42
2.86
2.89

Table 3: Results for Minesweeper. The table contains results for the three standard levels
of the game plus a larger instance. Average results over 1,000 runs are shown. Average
times are in seconds.

Table 3 shows results for the three standard levels of the game and for a much larger
instance. As in Battleship, the greedy policy used for action selection makes use of the beliefs
computed by beam tracking, flagging or opening a cell when certain about its content, else
selecting the cell with the lowest probability of containing a mine and opening it, with the
probabilities approximated from the beliefs over the beams as indicated before. Despite
the complexity of the game, NP-complete for checking consistency (Kaye, 2000) and coNPcomplete for inference (Scott et al., 2011), beam tracking scales well and solves difficult
games quickly. Moreover, the results shown in the table are competitive with those recently
reported by Lin, Buffet, Lee, and Teytaud (2012), which are obtained with a combination of
UCT for action selection, and a domain-specific CSP solver for tracking beliefs. The success
ratios that they report are: 80.2  0.48% for the 8  8 instances with 10 mines, 74.4  0.5%
for the 16  16 instances with 40 mines, and 38.7  1.8% for the 16  30 instances with 99
mines. The authors do not report times.
11.3 Wumpus
The Wumpus game (Russell & Norvig, 2009) consists of a maze in which there is an agent
that moves around looking for the gold while avoiding hidden pits and wumpus monsters.
Initially, the agent does not know the positions of the gold, pits or wumpuses, but it senses
glitter when at the same cell as the gold, and senses a stench or a breeze when at an adjacent
cell to a wumpus or a pit respectively. An m  n instance is described with known state
variables for the position and orientation of the agent, and hidden boolean variables for
each cell that tell whether there is a pit, a wumpus, or nothing at the cell. One more
946

fiBelief Tracking for Planning with Sensing

heading
gold-pos

pos

pitx0 ,y0

wumpx0 ,y0

glitter

deadx0 ,y0

breezex,y

stenchx,y

Figure 5: Fragment of the causal graph for Wumpus. There are observable variables
breezex,y , stenchx,y and deadx,y , and state variables heading, pos, pitx,y and wumpx,y ,
for (x, y) ranging over the grid cells. Cells (x0 , y 0 ) stand for cells adjacent to (x, y). The
causal width of the problem is 4 as there are 4 such cells, while the state variables heading
and pos are determined.
hidden state variable stores the position of the gold. The observable variables are boolean:
glitter, breezex,y , stenchx,y and deadx,y , with (x, y) ranging over the different cells. The
actions are move forward, rotate right or left, and grab the gold. The causal width for the
encoding is 4 while the problem width grows with m and n. Figure 5 shows a fragment
of the causal graph for Wumpus. The size of the causal beams for the breeze and stench
variables is bounded by 4 because each cell has at most 4 neighbors and the heading and
position variables for the agent are determined.
Table 4 shows results for different grid sizes and number of pits and wumpus, for an
agent that selects actions with a greedy policy based on a heuristic that returns the length
of a minimum-length safe path to the nearest cell that may contain the gold. The beliefs
computed by beam tracking are used to determine which cells are safe (known to contain
no wumpus or pit) and may contain the gold. We are not aware of any other tested and
scalable solver for Wumpus for making a comparison, with the exception of our own recent
LW1 planner that has built on this work (Bonet & Geffner, 2014). The figures in the table
show clearly that beam tracking computes beliefs effectively and efficiently in this domain.
For instance, the 30  30 instances with 32 pits and 32 wumpus are solved successfully 89%
of the time, in less than 4.4 seconds on average. Moreover, all the unsolved instances were
actually shown to be unsolvable in the sense that the agent could not reach an unvisited
cell in a safe manner. This was proved for each unsolved instance by calling a SAT solver
on a propositional theory that encodes the game and the literals learned by the agent after
the execution.
11.4 Non-Deterministic Moving Wumpus
In order to evaluate beam tracking in a more complex non-deterministic domain (the NONDET-Ring-Key domain in Section 7 has small width), we designed a non-deterministic
variant of the Wumpus domain. In Moving Wumpus there is just one wumpus in the grid
but this wumpus moves around non-deterministically everytime that the agent moves. The
grid still contains the hidden pits and the hidden gold, but in order to make the game safer
for the agent, the wumpus sensor is enhanced to detect the position of the wumpus when at
a (euclidean) distance less than 3 from the agent (else there is no safe strategy for escaping
death in general).
947

fiBonet & Geffner

avg. time per
dim

#pits/#wumpus

%density

#decisions

%win

decision

game

55
10  10
15  15
20  20
25  25
30  30
35  35
40  40
45  45
50  50

1/1
2/2
4/4
8/8
16 / 16
32 / 32
64 / 64
128 / 128
256 / 256
512 / 512

8.0
4.0
3.5
4.0
5.1
7.1
10.4
16.0
25.2
40.9

22,863
75,507
165,263
295,305
559,595
937,674
2,206,905
4,471,168
6,026,625
7,492,503

93.6
98.3
97.9
97.8
94.0
89.0
54.3
7.3
0.8
0.1

3.8e-4
9.6e-4
1.6e-3
2.4e-3
3.8e-3
4.7e-3
3.7e-3
2.8e-3
8.6e-3
1.3e-2

8.7e-3
7.2e-2
2.6e-1
7.2e-1
2.1
4.4
8.2
12.7
51.8
100.4

Table 4: Results for Wumpus. For each size, we performed 1,000 runs. The table shows
the total number and density of pits and wumpus in the grid, the total number of decisions
across all the runs, the percentage of runs in which the agent found the gold, and the average
time in seconds per decision and game.

Moving Wumpus is causally decomposable and thus the incompleteness of beam tracking
in this domain is only due to the replacement of the full consistency among beams done
by CBT by the weaker but efficient (relational) arc consistency done by beam tracking. To
see this, observe that the only variable that is not a memory variable is the position of
the wumpus WLoc. However, there are only two beams in the causal decomposition that
contain this variable: the beam for WLoc and the beam for the observable variable that
tells the position of the wumpus, and the former beam is contained in the latter beam.8
Experimental results for beam tracking over this domain are presented in Table 5 for a
policy obtained using the AOT lookahead algorithm based on AO* (Bonet & Geffner, 2012a)
that builds a lookahead tree of depth 10 using 50 expansions, and a heuristic function that
measures the distance between the agent position and the closest unvisited cell.
The algorithm was evaluated on different instances with grids nn for n = 4, 6, 8, . . . , 20,
each with a number of pits equal to (n  4)/2. For each grid size, we performed 1,000
evaluations for different initial configurations where the wumpus, pits and gold are randomly
placed. An instance of this game may turn unsolvable because the gold is isolated from the
agent by pits, because the agent finds itself in a position where there is no safe movement,
or because the agent exceeded the maximum number of actions (set to 3 times the number
of cells in the grid).

8. Indeed, a more general version of this problem involves m wumpuses that move non-deterministically
in the grid. This version is also causally decomposable as the beams for the positions of the wumpuses
(one for each wumpus) are all contained in the beam for the observable variable. In such general case,
the problem would have causal width equal to m.

948

fiBelief Tracking for Planning with Sensing

avg. time per
dim

#pits

%density

#decisions

%win

decision

game

44
66
88
10  10
12  12
14  14
16  16
18  18
20  20

0
1
2
3
4
5
6
7
8

0.0
2.7
3.1
3.0
2.7
2.5
2.3
2.1
2.0

13,770
30,666
54,528
85,635
123,921
159,977
231,307
309,919
362,816

97.6
95.0
94.8
93.0
93.6
93.4
91.7
90.0
90.8

3.5e-2
1.6e-1
4.1e-1
8.5e-1
1.3
2.2
3.1
4.1
5.3

4.9e-1
5.1
22.7
73.5
173.4
352.4
722.0
1,282.3
1,942.8

Table 5: Results for the Non-Deterministic Moving Wumpus domain. For each grid size,
averages over 1,000 runs shown. The table shows the total number and density of pits in
the grid, the total number of decisions across all the runs, the percentage of runs in which
the agent found the gold, and the average time in seconds per decision and game.

12. Related Work
The formulation in the paper is closely related to recent translation-based approaches to
conformant and contingent planning that compile beliefs away (Palacios & Geffner, 2009;
Albore et al., 2009). These translations, however, assume that the problems are deterministic.
Our account yields similar widths on most deterministic benchmarks, but is
simpler, because it is defined over multi-valued variables, and is more general, because it
handles non-deterministic actions. Yet our account is also less tight on some deterministic problems. As an illustration, if I = {x1      xn } and the actions are ai , each with
conditional effect xi  G, i = 1, . . . , n, the conformant problem with goal G has width
1 in Palacios and Geffners account, but width n in ours. The relevance account based
on literals is indeed finer than the one based on variables but it is also more difficult to
generalize to non-deterministic settings. This difference does not seem to have practical
effects over most benchmarks where disjunctions in the initial situation are exclusive and
implicitly encode the possible values of a set of multi-valued variables. Another important
difference with these approaches is that complete translations are always exponential in the
problem width, while our complexity bound is worst case; i.e., if the variables in contexts
are highly correlated, the actual complexity of factored belief tracking can be much lower.
The notion of width appears also in Bayesian networks where inference is exponential
in the width of the network (Pearl, 1988). Three differences that can be pointed out in
relation to our notion of width are that 1) we exploit the knowledge that certain variables
are not observable, 2) we can determine and use the knowledge that certain variables are
determined, and 3) we make use of the distinction between action conditions and preconditions in planning. As an example, a problem where an agent has to go through n doors
whose status, open or closed, can only be observed when the agent is near the door, will
have width no smaller than n when modeled as a dynamic Bayesian network, as all the door
variables affect the agent location variable. In our setting, however, the problem has width
949

fiBonet & Geffner

1 because the status of a door need to be known to the agent before it can open, close or
walk through the door.
The causal decomposition and the resulting causal belief tracking algorithms are similarly related to the ideas of variable splitting or renaming in graphical models, where a
variable X appearing in different factors fi is replaced by different variables Xi , one per
factor fi (Choi & Darwiche, 2006; Ramirez & Geffner, 2007), so that the problem width
can be reduced. Then, equality constraints relating the Xi variables must be enforced.
Approximate belief tracking algorithms for dynamic bayesian networks and POMDPs have
also appealed to the idea of decomposing global beliefs over all the variables into local beliefs over subsets of variables (Boyen & Koller, 1998; Shani, Poupart, Brafman, & Shimony,
2008). A key difference with the causal belief tracking algorithm is that we provide the
conditions under which this type of decomposition remains sound and complete. On the
other hand, we only deal with uncertainty represented by sets of states, not probability
distributions.
A number of logical schemes for representing and tracking beliefs have been used and
developed in contingent planning, appealing to OBDDs, CNF, and DNF representations
(Bertoli et al., 2001; Bryce et al., 2006; To et al., 2011), relevance considerations (Tran,
Nguyen, Son, & Pontelli, 2013), and lazy SAT and regression techniques (Hoffmann & Brafman, 2005; Rintanen, 2008; Shani & Brafman, 2011). None of these approaches, however,
has been tried on the domains considered in this paper or over instances of similar size.
Indeed, while the causal width of these domains bounds the complexity of beam tracking, no similar bound is known for these schemes that unlike beam tracking are complete.
Moreover, while in principle some of these schemes handle non-determinism naturally, other
methods like those based on SAT do not. The K-replanner (Bonet & Geffner, 2011) is also
based on a very efficient and effective belief tracking method that is polynomial but not
fully general and cannot deal with non-deterministic actions. The follow up LW1 planner
(Bonet & Geffner, 2014) shares the features of the K-replanner and is complete for width-1
problems.
From an experimental perspective, several comments and questions are in order on the
relation between beam tracking and the algorithms used for belief tracking in contingent
planners over the existing benchmarks. First of all, practically all of the benchmarks used
so far in contingent planning are easy from a belief tracking point of view. Indeed, the
quadratic and linear time representation of beliefs in CLG and LW1 respectively, have been
shown to be adequate for all such problems, including the Wumpus problems above. The
exception to this is Minesweeper, where belief tracking is provably NP-hard and where
the linear approximation in LW1 turns out to be much weaker than beam tracking, failing
to solve without guessing most of the instances that beam tracking can solve in this
way (Bonet & Geffner, 2014). This means that, whether the width of these problems
is low or high, their effective width is 1, and in such cases, beam tracking cannot help
computationally, and actually, may degrade performance (except in Minesweeper), as beam
tracking is exponential in the problem causal width, which while lower than width in general
is usually higher than 1. The effective width of a problem P is the minimum non-negative
integer value i such that the contingent translation Xi (P ) (Palacios & Geffner, 2009; Albore
et al., 2009) has a solution. The effective width of a problem is never greater than its width
but can be much smaller than both its width and than its causal width. For example, a
950

fiBelief Tracking for Planning with Sensing

avg. time per
dim

#mines

%density

%succ

%failure

%aborted

decision

game

88
16  16
30  16

10
40
99

15.6
15.6
20.6

93.0
94.0
65.0

7.0
6.0
6.0

0.0
0.0
29.0

0.8
4.9
12.4

56.3
1,268.4
5,998.6

Table 6: Comparison with the SDR on-line planner over Minesweeper instances. SDR is fed
with random hidden states and solutions (action sequences) computed by beam tracking
with no guessing. The planner task is then to check the applicability of actions in the given
solution and whether the goal holds. For each instance size, SDR is tested over 100 different
random problems. The column failure indicates the number of times that SDR was not able
to verify a correct solution, while the column aborted indicates the number of times that
SDR terminated early due to a bug. Times are in seconds. Beam tracking takes a few
seconds for solving these instances (see Table 3).

problem with actions ai with conditional effects that map valuations vi of a set of variables
X1 , . . . , Xn into the goal literal Y = y, will have a width and a causal width not smaller
than n as all the variables Xi are causally relevant to Y . Yet the effective width of such
a problem may be 1 if the values of each the Xi variables can be observed directly or
inferred from the observations, or also, if the goal can be achieved without using any of
these actions at all. In this sense, while the notion of effective width provides a lower bound
on the number of state variables whose uncertainty must be tracked jointly in order to make
the problem solvable, the notions of width, characterized syntactically, provides an upper
bound on the number of state variables whose uncertainty must be tracked jointly so that
no solution would be missed. The gap between these two bounds can be large indeed, and
obtaining syntactic characterizations of the former is an open problem.
A related question is how the various belief tracking algorithms used in contingent planning such as regression, OBDDs, CNF, and DNF, scale up over these domains. While a
general comparison of these complete but exponential algorithms with incomplete and polynomial algorithms like beam tracking (over domains with bounded causal width) would not
be fair, it would still be interesting to find out in which of the easy cases these algorithms scale up polynomially and in which exponentially. Performing these tests, however,
is not simple, as it requires getting into the code of the planners so that they would all
follow a fixed common policy in each instance, thus leaving the planning component aside.
Moreover, even fixing a policy for each instance, is not enough, as some of the planners are
off-line and hence track beliefs over many possible executions and not just one, as in the
case of on-line planners.
Just for the purpose of an illustration we performed this test in one of the difficult
domains, Minesweeper, by supplying the on-line planner SDR (Shani & Brafman, 2011)
the execution computed from beam tracking along with the hidden initial state for such an
execution. In this setting, the on-line planner SDR is not doing planning, but rather it is
tracking the beliefs over the problem to verify goal achievement and the preconditions of
the given applicable action at each time point. These are all Minesweeper instances solved
951

fiBonet & Geffner

by beam tracking without guessing; i.e., by pure inference after a first fixed choice. Table 6
shows the results for SDR that tracks beliefs using a form of regression (Rintanen, 2008;
Shani & Brafman, 2011). Two observations can be made by comparing the results in this
table with those in Table 3 for beam tracking. First, while SDR takes 56.3, 1,268.4 and
5,998.6 seconds on average for verifying solutions for 8  8, 16  16 and 30  16 instances
respectively, beam tracking takes 0.21, 1.42 and 2.86 seconds for finding these solutions by
following a greedy policy. Since finding solutions is more expensive than verifying them
 one must at least identify all applicable actions at each time point  the difference in
performance turns out to be of several orders of magnitude, growing with the grid size. In
addition, the regression mechanism in SDR fails to verify correct solutions in several cases
and aborts with failure in a large number of cases for the large instances. In any case,
the performance gap is not surprising: belief tracking in Minesweeper is NP-hard, thus
complete algorithms like regression will run in exponential time in the worst case, while
beam tracking remains polynomial as the causal width of the domain is bounded. In other
challenging problems, the gap in performance between beam tracking and complete belief
tracking algorithms will be similar. Beam tracking will be useful then if the causal width
of the problem is bounded and not too large, trading off in a principled way completeness
by tractability.

13. Summary
Effective belief tracking is crucial for planning with incomplete information and sensing.
While the problem is intractable in general, it has been shown elsewhere that belief tracking over deterministic problems is exponential in a width parameter that is often bounded
and small. In this work, we have introduced a related formulation that applies to nondeterministic problems as well. The factored belief tracking algorithm results from a set of
projected problems whose size is bounded by the problem width. The beliefs over goals
and preconditions are then obtained directly from the beliefs over these projected problems
that can be maintained independently. We have then developed a different decomposition
scheme and belief tracking algorithm that maintains beliefs over smaller projections, and
have provided the conditions under which the algorithm is complete. Causal belief tracking
is space exponential in the problem causal width but remains time exponential in the problem width, as the global consistency of the beliefs over the smaller projections need to be
enforced. Finally, beam tracking is a sound but incomplete approximation of causal belief
tracking where global consistency is replaced by a local but powerful form of consistency.
Beam tracking runs in time and space that are exponential in the problem causal width that
is often much smaller than the problem width. We have tested beam tracking over large
instances of Battleship, Minesweeper, and Wumpus, in combination with simple heuristics
for action selection, where performance compares well with state-of-the-art solvers while
using orders-of-magnitude less time. In the future, we would like to explore extensions of
the proposed framework for belief tracking in POMDPs, where belief states are not sets
of states but probability distributions, and particle-based algorithms provide a common
approximation (Doucet et al., 2000).
952

fiBelief Tracking for Planning with Sensing

Acknowledgments
We thank Gabriel Detoni for his Java Tewnta framework (http://code.google.com/p/
tewnta) for implementing client/server games with a graphical interface on which we developed graphical interfaces for Battleship, Minesweeper and Wumpus. Thanks also to
James Biagioni for his wumpuslite JAVA simulator (http://www.cs.uic.edu/~jbiagion/
wumpuslite.html) that we adapted to run the experiments for Wumpus, and to Guy Shani
for help running SDR. Hector Geffner is partially supported by EU FP7 Grant# 270019
(Spacebook) and MICINN CSD2010-00034 (Simulpast).

Appendix A. Proofs
Formal results that are needed but which are not stated as propositions or theorems in the
main text of the article appear here in the form of lemmas.
A.1 Complexity of Flat Belief Tracking
Let us first formally define the decision problems BTP and GBTP. BTP is the language
BTP = {hP,  i : P is a contingent problem,  is a possible execution, and b |= G}
where P = hV, I, A, G, V 0 , W i,  = ha0 , o0 , . . . , an , on i is an execution, and b is the belief
that results of the execution of  on the initial belief state. GBTP is like BTP except that it
consists of triplets hP, , `i such that P is a contingent problem,  is a possible generalized
execution, ` is a goal, precondition or observation literal, and b |= `.
Observe that BTP and GBTP respectively include the tuples hP,  i and hP, , `i such
that the problem has an empty initial belief state, due to two complementary literals appearing as unit clauses in I, since in such case every execution is trivially possible and b
trivially entails any literal `.
Proposition 3. BTP is polynomial-time reducible to GBTP.
Proof. The idea is to map a normal execution  into a generalized execution m that results
of replacing each pair ha, oi in  by the sequence ha, `1 , noopa , `2 , . . . , noopa , `|V 0 | i where
`1 , . . . , `|V 0 | are the observation literals made true by o, one for each observable variable
in V 0 , and noopa is the action that requires nothing and does nothing and whose sensor
model is Wnoopa (`) = Wa (`) for each observation literal `.
Formally, given an instance hP,  i for BTP, the reduction must generate in polynomial
time an instance hP 0 ,  0 , `i for GBTP such that hP,  i  BTP iff hP 0 ,  0 , `i  GBTP.
The problem P 0 is the problem P extended with the actions noopa , a new boolean
variable Xgoal that denotes the achievement of the goal G in P , a new action agoal with
precondition G and effect Xgoal , and a new dummy observable variable Y with domain
{>} and models Wa (Y = >) = true for all actions a. On the other hand, the generalized
execution  0 is hm , agoal , Y = >i and ` = Xgoal . Clearly, the reduction works in polynomial
time and hP,  i  BTP iff hP 0 ,  0 , `i  GBTP.
Theorem 5. Flat belief tracking is exponential in |VU |, where VU = V \ VK and VK is the
set of state variables in V that are determined.
953

fiBonet & Geffner

Proof. As described in Definition 4, flat belief tracking consists of an explicit representation
of beliefs as set of states, but some savings in space and time can be obtained by noting
that the variables in VK are determined.
With an explicit representation of beliefs, the belief tracking problem gets trivially solved
because checking whether an execution  = ha0 , o0 , . . . , an , on i is possible and a literal ` is
true after  reduces to computing the belief bn+1 that results from  and checking whether
bn+1 is empty and whether every state in it satisfies `. The time complexity of this algorithm
is the time needed to compute the initial belief b0 plus (n + 1) multiplied by the time needed
to compute bi+1 from bi plus the time needed to check the validity of `. Among these times,
the last is the easiest to calculate as it is linear in the size of bn+1 . We thus need to bound
the first two times. We begin the proof by showing that flat belief tracking can be done in
time exponential in |V | and then reduce the exponential dependency from |V | to |VU |.
For computing b0 it is enough to generate all possible states (valuations of variables)
and filter out those that do not satisfy the clauses in I. The total time thus spent is
|V |  |I|  2O(|V |) since there are 2O(|V |) valuations, |I| clauses, and each clause has at most
|V | literals.9
The time to compute bi+1 from bi consists of the time to check that the preconditions
of a hold at b, and the times to compute ba from b and boa from ba when b = bi , a = ai and
o = oi . The preconditions are easily verified by iterating over all states in b. The time for
this is bounded by |V |  2O(|V |) since a contains at most |V | preconditions and b contains
at most 2O(|V |) states. If some precondition is not satisfied at some state in b, the execution
is not possible.
The belief ba can be computed from b by iterating over each state s in b, and each
possible state s0 for ba , and then and checking whether s0  F (a, s). The two nested
iterations require time 2O(|V |)  2O(|V |) = 2O(|V |) . The test s0  F (a, s) can be performed
in time that is exponential in |V | as follows. Let Ci  E1i |    |Eni i , for 1  i  m, be the
collection of conditional effects for the action a that trigger at the state s. If s0  F (a, s),
then s0 is the result of applying one head from each such conditional effect on s. Since the
problem has |V | variables, then among the m heads there are at most |V | heads that map
s into s0 while the rest (if any) are subsumed by the first. All subsets of heads of size at
most |V | can be enumerated in 2O(|V |) time, for each such subset checking whether s gets
mapped into s0 requires O(|V |) time. Therefore, checking s0  F (a, s) requires 2O(|V |) time
as well as computing ba from b.
Once ba is obtained, boa is calculated by removing (filtering) from ba all states that do
not comply with the observation o. For each state s in ba and each observation literal
` compatible with o, the state s belongs to boa iff s |= Wa (`). This latter test can be
performed in time linear in |Wa (`)|, the size of the formula Wa (`). Hence, since there are
|V 0 | observation literals compatible with o, boa can be computed from ba in time O(|ba | 
|V 0 |  |Wa |) where |Wa | = max` |Wa (`)| and the max ranges over all observation literals `.
If boa is empty and ba is non-empty, the execution is not possible.
9. In this calculation, we implicitly assume that the variable domains are of constant size. Otherwise, if the
domains have size n that is linear in the input size, the number of valuations is bounded by 2O(|V | log n)
instead of 2O(|V |) . In either case, the number of valuations is still exponential in the number of variables
as well as the resulting complexity of flat belief tracking.

954

fiBelief Tracking for Planning with Sensing

Once all times are weighed in, we see that flat belief tracking can be done in time that
is exponential in |V |.
We now reduce the exponent from |V | to |VU |. This is direct since any determined
variable has the same valuation across all states in a reachable belief. Hence, such variables
do not contribute to increase the number of states in reachable beliefs. Likewise, only
subsets of heads of size |VU | need to be considered when computing the belief ba from b.
Hence, all computations can be done in time and space that is exponential only in |VU |.
Theorem 6. BTP and GBTP are Turing complete for the class PNP .
Proof. By Proposition 3, BTP is polynomial-time reducible to GBTP, and thus it is enough
to show the hardness for BTP and the inclusion for GBTP.
The class PNP is the set of all decisions problems that can be decided in (deterministic)
polynomial time using an oracle for SAT. To show that BTP is hard for this class, it is
enough to show that UNSAT can be reduced in polynomial time to BTP since then every
call to the NP oracle can be replaced by a call to the BTP oracle. On the other hand, to
show that GBTP belongs to PNP , it is enough to show that there is an algorithm for the
complement of GBTP (since PNP is closed under complementation) that runs in polynomial
time and that makes calls to an oracle for SAT.
Hardness. Let  = {C1 , . . . , Cm } be a CNF theory over boolean variables X1 , . . . , Xn .
We need to construct in polynomial time a contingent problem P = hV, I, A, G, V 0 , W i
and an execution  such that hP,  i  BTP iff  is unsatisfiable. The variables in the
problem P are all boolean given by V = {X1 , . . . , Xn , Q} and V 0 = {Z1 , . . . , Zm }. I is
the empty set of clauses and G = {Q = true}. The actions are a1 , . . . , am , all with empty
preconditions and no conditional effects, but with sensor model Wai (Zi = true) = Ci  Q
and Wai (Zj = true) = f alse for j 6= i. Finally, the execution is  = ha1 , o1 , . . . , am , om i
where oi is the V 0 -valuation that makes Zi true and Zj false for j 6= i.
Note that the initial belief contains all the 2n+1 V -valuations, half of them satisfying
Q and the other half Q. After the first observation o1 is received, only the valuations
that satisfy the clause C1 or Q are preserved. Thus, inductively, after observation oi is
received, only the valuations that satisfy the clauses in {C1 , C2 , . . . , Ci } or Q are preserved.
Therefore, b is the set of valuations that satisfy  or Q and hence it is non-empty (i.e.,  is
a possible execution). Thus, b |= G iff all valuations for Q are gone iff  is unsatisfiable.
Inclusion. The complement of GBTP consists of all tuples hP, , `i such that b0 is non-empty
and either  is non-executable or b 6|= `. Since I consists only of unit clauses, b0 6=  iff
I contains no pair of complementary literals. Assume that  = ha0 , `0 , a1 , `1 , . . . , an , `n i,
where the literals `i are observation literals, and let bi be the belief before the action ai
is applied; i.e., bi = boa for b = bi1 , a = ai1 and o = `i1 . Then,  is possible iff each
bi is non-empty and each action ai is applicable at bi . Assume that we have established
that the prefix i = ha0 , `0 , . . . , ai1 , `i1 i is possible, then checking whether i+1 is possible
involves two operations: 1) checking that each precondition literal of ai holds at bi1 and
2) checking whether there is at least one state in bai that complies with `i , for b = bi1 .
The first check can be done by calling the SAT oracle over the CNF theory i1 , over
time-indexed propositions for state-variable literals and actions, that encodes all possible
state trajectories for a fixed valuation of the actions. The time horizon for the theory is
955

fiBonet & Geffner

T = 0, . . . , i  1, and the theory is built in such way that it is satisfiable iff there is a state s
at time i  i (i.e., s in bi1 ) that does not satisfy at least one precondition of ai . This theory
is of polynomial size and can be built in polynomial time. Likewise, the second check can
be performed by calling the SAT oracle over the CNF theory i1 with the property that
it is satisfiable iff there is a state in bi1 that complies with the observation `i .
Hence, the algorithm A that decides the complement of GBTP works by building theories
t and t for t = 0, . . . , n. At each stage t, A ACCEPTs the input if t is satisfiable or t is
unsatisfiable. If, at the end, the algorithm has not accepted yet, then it builds another theory
n+1 , that is like i but instead of checking whether a precondition of action ai doesnt
hold, checks whether the input literal ` doesnt hold. If n+1 is satisfiable, A ACCEPTs
since the belief b does not satisfy `, else A REJECTs because hP, , `i  GBTP.
A.2 Factored Belief Tracking
In the following, for a state s (valuation of variables) and a subset S of variables, we write
s|S to denote the valuation s restricted to the variables in S, also called the projection of
s on S. In general, we use the symbols s, t and their primed versions to denote states,
and the symbols u, v and their primed versions to denote projected states (restrictions or
partial valuations).
Proposition 9. Belief tracking in the deterministic or non-deterministic conformant setting is exponential in the maximum number of non-determined variables that are all causally
relevant to a variable appearing in an action precondition or goal.
Proof. This proposition is a special case of Theorem 15 (and also of Theorem 33).
Theorem 15. Belief tracking in P is exponential in w(P ).
Proof. In the conformant setting, there are no observable variables and hence the evidential
relevance relation is empty and the relevant relation equals the causally relevant relation.
Therefore, the context of a variable X equals the set of variables that are causally relevant
to X, and this theorem establish Proposition 9 in the conformant setting.
In the general setting, the theorem is shown by constructing an algorithm for belief
tracking whose time complexity is only exponential in w(P ). The definition and analysis of
the algorithm is done through a series of claims that terminate at Theorem 22 below.
Proposition 18. If variable X appears in a goal or precondition, then the number of state
variables in PX that are not determined is bounded by w(P ).
Proof. The number of state variables in PX is |Ctx(X)| and the number of state variables
that are not determined in PX is |Ctx(X)  VU |. By definition of width, this quantity is
less than or equal to w(P ) when X is a goal or precondition variable.
We now establish two fundamental lemmas about the progression of actions and projection of observable models. In the following, we say that a subset S of variable is causally
closed if for any variable X  S and any variable Y that is causally relevant to X, Y  S.
Likewise, the causal closure of a variable Z is the minimum (with respect to set inclusion)
subset S of variables that is causally closed and includes Z.
956

fiBelief Tracking for Planning with Sensing

Lemma 1 (Factored Progression). Consider a consistent problem P . Let s be a state, and
a be an action that is applicable at s. Then, for any causally-closed subset S of variables:
1) for every u0 , if u0  FS (a, s|S ) then there is s0  F (a, s) such that u0 = s0 |S , and
2) for every s0 , if s0  F (a, s) then s0 |S  FS (a, s|S )
where FS is the transition function on the projected problem PS . Therefore, S F (a, s) =
FS (a, s|S ) for every state s on which a is applicable, and S F (a, U ) = FS (a, S U ) for every
set U of valuations on which a is applicable.
Proof. Part 1. Let u0 be an element in FS (a, s|S ) and let HS = {ESi }m
i=1 be the collection
of heads of the conditional effects CSi  ESi for a that trigger at s|S and result in u0 . For
fixed i  {1, . . . , m}, we know that s|S |= CSi . If ESi 6=  then, by definition of the causally
relevant relation, V ars(C i )  S and thus CSi = C i . Therefore, s |= C i and this effect also
triggers when a is applied at s. We now show that no other effect that affects a variable
in S triggers when a is applied at s. Indeed, if a conditional effect C  F that affects a
variable in S triggers, then s |= C and thus s|S |= CS and FS  HS . Finally, the effects
that trigger and affect the variables in S are the same in both problems P and PS . Since P
is consistent, the set of effects {E i }m
i=1 is contained in a set H of heads for the effects that
trigger when a is applied at s. Therefore, u0 is the projection over S of the state s0 that
results of applying the effects in H at s; i.e., u0 = s0 |S .
Part 2. Let s0 be an element in F (a, s) and let H = {E i }m
i=1 be the collection of heads of the
conditional effects C i  E i for a that trigger at s and result in s0 . For fixed i  {1, . . . , m},
we know that s |= C i and thus s|S |= C i |S . Therefore, the effects CSi  ESi also trigger
when a is applied at s|S in PS . We now show that no other effect that affects a variable in S
triggers when a is applied at s|S in PS . Indeed, let us suppose that a projected conditional
effect CS  FS that affects a variable in S triggers at s|S . Then, V ars(F )  S and thus
V ars(C)  S, since S is causally closed, and CS = C. Therefore, s |= C and this effect
also triggers when a is applied at s. Finally, since the effects that trigger and affect the
variables in S are the same in both problems P and PS , then s0 |S is the result of applying
0
the projected effects {ESi }m
i=1 at s|S ; i.e., s |S  FS (a, s|S ).
Lemma 2 (Observational Closure). For every variable X, action a, and observation literal
` = Z = z, Wa (`)S is either Wa (`) or true, where S = Ctx(X).
Proof. Let {X1 , . . . , Xn } be the variables in Wa (`). By the definition of the relevant relation,
Z is relevant to each Xi and vice versa. Hence, if Z or some Xi belongs to S, then Z and
all Xi belongs to S as well. Therefore, Wa (`)S is either Wa (`) or true.
The following results are obtained by induction on the length of the executions. As
noted before, there is no loss of generality if we consider generalized executions instead of
executions. However, it is easier to consider even more general executions that correspond
to finite sequences over the alphabet A  {ha, `i : a  A, `  Lits(V 0 )} where A is the
set of actions and Lits(V 0 ) is the set of observation literals. This type of executions are
more general because they do not require the interleaving of actions and observations; i.e.,
such an execution may contain multiple actions or observations in sequence. For example,
957

fiBonet & Geffner

an execution like ha0 , a1 , ha2 , `2 i, ha3 , `3 i, . . .i indicates that the initial belief needs to be
progressed with the actions a0 and a1 , then filtered with the formula Wa2 (`2 ), filtered again
with the formula Wa3 (`3 ), and so on. As in the case of normal and generalized executions,
there is a direct mapping between generalized executions and this new type of executions.
If  is one such execution, then b denotes the belief that results of applying  at the initial
belief, while if b = b , then ba and ba,` denote the beliefs that result from the executions
 0 = h, ai and  0 = h, ha, `ii respectively. Therefore, when making induction on the
length of executions to prove a claim, we need to show the claim for the initial belief that
corresponds to the empty execution, and then for beliefs of the form ba and ba,` for b = b .
The next definition and lemma make precise the notion of decomposable belief that
plays a fundamental role in our results. Intuitively, a belief b is decomposable when for
every pair of states s, t  b, there is a state w  b that agrees with s on the variables in
a subset S and agrees with t on the variables in a subset T (where S and T are certain
subsets of variables); in symbols, there is w  b such that w|S = s|S and w|T = t|T .
Definition and Lemma 3 (Decomposability). A belief state b is decomposable iff for
every variable X, observation literal ` = Z = z, action a, and subset T  V ars(Wa (`))
such that T is causally closed and T  Ctx(X) = , it holds:


s, t s, t  b = w w  b  w|Ctx(X) = s|Ctx(X)  w|T = t|T .
It turns out that every reachable belief is decomposable.
Proof. Let b be a reachable belief. Then, there is an execution  such that b = b . The
proof is by induction on the length of  . If  is empty, the claim holds since I contains only
unit clauses and T  Ctx(X) = .
Assume now that all beliefs reachable through executions of length less than or equal
to n are decomposable, and consider an execution  0 of length n + 1 that augments an
execution  of length n. In the following, b denotes the belief b , and res(a, s) denotes the
state that results of applying a deterministic action a on state s.
Case:  0 = h, a0 i. Let X, `, a and T be as in the statement of the lemma, S = Ctx(X),
and let s0 , t0 be two states in ba0 . Therefore, there are two determinizations a1 and a2 of
a0 such that s0 = res(a1 , s) and t0 = res(a2 , t) for some s, t  b. Apply inductive hypothesis
to obtain w  b such that w|S = s|S and w|T = t|T . Since S and T are disjoint and
causally closed, there is a determinization10 a3 of a0 such that res(a1 , s)|S = res(a3 , w)|S
and res(a2 , t)|T = res(a3 , w)|T . The sought w0  ba0 is thus w0 = res(a3 , w).
Case:  0 = h, ha0 , `0 ii. As before, let X, `, a and T be as in the statement of the lemma,
0 0
let `0 = Z 0 = z 0 , and let s0 , t0 be two states in ba ,` . We consider the two subcases whether
V ars(Wa0 (`0 ))  S or not, for S = Ctx(X):
Subcase: V ars(Wa0 (`0 ))  S. Since, s0 , t0  b, apply the inductive hypothesis to get w0  b
0 0
such that w0 |S = s0 |S and w0 |T = t0 |T . Then, w0  ba ,` because w0 |S = s0 |S |= Wa0 (`0 )S =
Wa0 (`0 ).
10. The existence of this determinization is granted by the second assumption on the planning problem and
the fact that the sets S and T of variables are disjoint.

958

fiBelief Tracking for Planning with Sensing

Subcase: V ars(Wa0 (`0 )) * S. By Lemma 2, V ars(Wa0 (`0 ))  S = . Let T 0 be the minimal
causally-closed subset of variables that includes T and V ars(Wa0 (`0 )). Observe that T 0 S =
 since if Y belongs to the intersection, then Z 0 is relevant to Y , Y is relevant to X, and thus
Z 0 is relevant to X contradicting V ars(Wa0 (`0 ))  S = . Apply the inductive hypothesis
using T 0 to get w0  b such that w0 |S = s0 |S and w0 |T 0 = t0 |T 0 . This is the w0 that
0 0
we are looking for because T  T 0 and thus w0 |T = t0 |T , and because w0  ba ,` since
w0 |T 0 = t0 |T 0 |= Wa0 (`0 )T 0 = Wa0 (`0 ).
A last technical lemma, before giving the proofs of Theorems 20 to 22, that establish
the existence of partial valuations in the projection of filtered beliefs is the following:
Lemma 4 (Factored Filtering). Let X be a variable, S = Ctx(X), b be a reachable belief,
a be an action, and ` = Z=z be an observation literal. If ba,` is non-empty and u is such
that u  S b and u |= Wa (`)S , then u  S ba,` .
Proof. Assume that ba,` is non-empty and let u be an S-valuation that satisfies the antecedent in the lemma. If Wa (`)S = Wa (`), then u |= Wa (`) and u  S ba,` .
If Wa (`)S 6= Wa (`) then by Lemma 2, V ars(Wa (`))  S = . Let T be the minimal
causally-closed subset of variables that includes V ars(Wa (`)). Note that if Y  S  T then
Z is evidentially relevant to Y which is relevant to X, and thus Z is relevant to X and
V ars(Wa (`))  S. Therefore, S  T = . Let t  ba,`  b and apply Lemma 3 to get
w  b such that w|S = u and w|T = t|T . Hence, w|T |= Wa (`)T = Wa (`), w  ba,` and
u  S ba,` .
Theorem 20. For a state variable X, let b and bX be the beliefs that result from an execution
that is possible over both P and PX . Then, X bX = X b.
Proof. Let  be an execution that is possible over both P and PX . We prove the more
general result that bX = S b for S = Ctx(X); it is more general because X  S and
X bX = X S b = X b. The proof is by induction on the length of  . If  is the empty
execution, then result follows readily since I contains only unit clauses. Assume that the
claim holds for executions of length n, and let  0 be an execution of length n + 1 that
augments an execution  of length n and that is possible over P and PX . Further, let b
and bX be the beliefs that result from  in P and PX respectively. Then, by inductive
hypothesis bX = S b.
Case:  0 = h, ai. We need to show that bX,a = S ba . In the following, FS denotes the
transition function in PX . The forward inclusion is given by


1
u0  bX,a = u u  bX  u0  FS (a, u)


2
= us u  bX  u0  FS (a, u)  s  b  s|S = u


3
= uss0 u  bX  u0  FS (a, u)  s  b  s|S = u  s0  F (a, s)  s0 |S = u0


4
= ss0 s  b  s0  F (a, s)  s0 |S = u0

 6
5
= s0 s0  ba  s0 |S = u0 = u0  S ba
959

fiBonet & Geffner

where 1 is by the definition of bX,a , 2 by inductive hypothesis, 3 by Lemma 1, and 5
and 6 by the definitions of ba and S ba respectively. The backward inclusion is

 2


1
s0 |S  S ba = s s  b  s0  F (a, s) = s s  b  s0  F (a, s)  s0 |S  FS (a, s|S )

 4
3
= s s|S  bX  s0 |S  FS (a, s|S ) = s0 |S  bX,a
where 1 is by the definition of ba , 2 by Lemma 1, 3 by inductive hypothesis, and 4 by
the definition of bX,a . Therefore, bX,a = S ba .
a,`
Case:  0 = h, ha, `ii. We need to show that ba,`
X = S b . The forward inclusion is
1

2

3

a,`
u  ba,`
X = u  bX  u |= Wa (`)S = u  S b  u |= Wa (`)S = u  S b

where 1 is by the definition of ba,`
X , 2 by inductive hypothesis, and 3 by Lemma 4. The
backward inclusion is
1

2

3

s|S  S ba,` = s  b  s |= Wa (`) = s|S  bX  s|S |= Wa (`)S = s|S  ba,`
X .
where 1 is by the definition of ba,` , 2 by inductive hypothesis, and 3 by the definition of
a,`
a,`
ba,`
X . Therefore, bX = S b .
Theorem 21. 1) An execution is possible in P iff it is possible over each of the subproblems
PX for X being a precondition or goal variable in P . 2) For an execution  and precondition
or goal variable X, X = x (resp. X 6= x) is true in b iff X = x (resp. X 6= x) is true in
bX , where b and bX are the beliefs that result of executing  in P and PX respectively.
Proof. Part 1. The proof is by induction on the length of the executions. The base case for
the induction is for the empty execution which is possible in P and in each PX . Assume
that the claim holds for executions  of length n, and let b and bX be the beliefs that result
from  in P and in each subproblem PX respectively. Let  0 be an execution of length
n + 1 that augments  . In the following, F denotes the collection of precondition and goal
variables in P , and S = Ctx(X) for X  F.
Case:  0 = h, ai. First, assume that  0 is possible in P . We need to show that  0 is possible
on each PX for X  F. By assumption, for each literal `  P re(a) and each s  b, s |= `.
Let ` be a literal in P re(a)S and u  bX for X  F. Then, V ars(`)  S and, by inductive
hypothesis and Theorem 20 (since  is applicable at P and PX ) applied to  , u = s|S for
some s  b. Therefore, u |= ` and a is applicable at bX .
Now, assume that  0 is possible in PX for each X  F. We need to show that  0 is
possible in P . If ` = X = x is a precondition of a, then `  P re(a)S and ` holds at each
state u  bX . If s  b then, by inductive hypothesis and Theorem 20 applied to  , there is
u  bX such that s|S = u. Thus, s |= ` and a is applicable at b.
Case:  0 = h, ha, `ii. First, assume that  0 is possible in P ; i.e., ba,` is non-empty. We need
to show that ba,`
X is non-empty as well for each X  F. We have
1

2

3

s  ba,` = s  b  s |= Wa (`) = s|S  bX  s|S |= Wa (`)S = s|S  ba,`
X
960

fiBelief Tracking for Planning with Sensing

where 1 is by the definition of ba,` , 2 by inductive hypothesis and Theorem 20, and 3 by
a,`
the definition of ba,`
X . Hence, bX is non-empty.
Finally, assume that  0 is possible in each PX ; i.e., ba,`
X is non-empty for each X  F.
a,`
We need to show that b is non-empty. Let X  F be such that Wa (`)S = Wa (`) for
S = Ctx(X); it exists because of the fourth assumption on the problem P and Lemma 2.
We have


1
2
u  ba,`
X = u  bX  u |= Wa (`)S = s u  bX  u |= Wa (`)S  s  b  u = s|S


3
= s u  bX  s |= Wa (`)  s  b  u = s|S

 5


4
= s s |= Wa (`)  s  b = s s  ba,`
where 1 is by the definition of ba,`
X , 2 by inductive hypothesis and Theorem 20, 3 by
Wa (`)S = Wa (`), and 5 by the definition of ba,` . Hence, ba,` is non-empty.
Part 2. Let  be a possible execution in P , and hence, by part 1, also possible in PX .
Let b and bX be the beliefs that result from  in P and PX respectively. By Theorem 20,
X bX = X b. Therefore, X = x (or X 6= x) holds at bX iff it holds at b.
Theorem 22. Flat belief tracking over each of the projected problems PX for X being a
precondition or goal variable in P , provides a sound and complete factored algorithm for
belief tracking over P that is time and space exponential in the width of P .
Proof. This is direct from Theorem 21. Let  be an execution and b and bX, be the
beliefs that result of executing  in P and PX respectively. Then,  is possible in P iff it is
possible at each PX . Therefore, flat belief tracking for the subproblems PX tells whether 
is possible or not in P . Furthermore, for each precondition or goal variable X, X = x holds
at b iff it holds at bX, . Thus, flat belief tracking for the subproblems PX is sufficient to
determine when an action is applicable or a goal belief has been reached.
By Theorem 5, flat belief tracking for subproblem PX is exponential in |Ctx(X)  VU |.
Therefore, flat belief tracking for all subproblems PX (simultaneously) is exponential only
in maxX |Ctx(X)  VU | where the max ranges over the precondition or goal variables X.
This latter expression is the one that defines w(P ).
Proposition 19. If an execution ha0 , o0 , a1 , o1 , . . .i is possible in P , then it is also possible
in PX for any state variable X in P .
Proof. If X is a precondition or goal variable, then the claim follows from Theorem 21. So,
assume that X is a state variable that does not appear as a precondition or goal. We show
using induction on the length of the (generalized) execution  that if  is possible in P
then it is also possible in PX . The base case for empty executions is direct. Consider an
execution  0 of length n + 1 that extends an execution  of length n. Let b and bX be the
result of applying the execution  in P and PX respectively, and let S = Ctx(X).
Case:  0 = h, ai. Let ` = Y = y be a precondition in P re(a)S and T = Ctx(Y ). Then,
Y  S and Ctx(Y )  Ctx(X) because Y is relevant to X. By Lemma 5 (below), bY  T bX .
961

fiBonet & Geffner

On the other hand, by Theorem 21, s |= ` for every s  bY . Therefore, ` holds at each state
s in bX , a is applicable at bX , and  0 is possible at PX .
Case:  0 = h, ha, `ii. If Wa (`)S = true, then ba,`
X = bX which is non-empty by inductive
hypothesis and thus  0 is possible at PX . If Wa (`)S 6= true and ` = Z = z, then Z is
relevant to X. Since by assumption there is a precondition or goal variable Y such that Z
is relevant to Y , it is not difficult to show that X is relevant to Y . Thus, Ctx(X)  Ctx(Y )
and bX  S bY by Lemma 5. Since  0 is possible in P and ba,`
Y is non-empty by Theorem 21,
0 is possible in P .
then ba,`
is
non-empty
and

X
X
A.3 Causal Belief Tracking
Lemma 5 (Soundness of Causally-Closed Decompositions). Let D = hT, Bi be a decomposition whose beams are causally closed, and let PXD be the subproblem corresponding to the
projection of P on the variables in B(X) for X  T . For any target variable X  T , if b and
bX are the beliefs resulting from an execution on P and PXD respectively, then bX  B(X) b.
Proof. The proof is by induction on the length of the executions. For the empty execution,
the claims holds since I contains only unit clauses. Let  0 be an execution of length  0
that augments  . In the following, b and bX denote the beliefs in P and PXD resulting after
execution  , and S denotes B(X).
Case:  0 = h, ai. Let u0  S ba . Then, there is s  b such that u0  S F (a, s). By
Lemma 1, u0  FS (a, s|S ). Thus, since s|S  bX by inductive hypothesis, u0  bX,a .
Case:  0 = h, ha, `ii. Let s|S  S ba,` . We have s  b and s |= Wa (`). By inductive
hypothesis, s|S |= Wa (`)S and s|S  bX . Thus, s|S  ba,`
X .
Theorem 28. Decoupled CBT runs in time and space that are exponential in wc (P ), and
it is sound but not complete. That is, for any target variable X in the causal decomposition,
if b and bX are the beliefs resulting from an execution on P and PXC respectively, then
bX  BC (X) b is necessarily true, but bX  BC (X) b is not.
Proof. Soundness follows directly from Lemma 5. The bounds on time and space are also
direct because the size of each beam BC (X) is bounded by the causal width wc (P ).
Theorem 30. CBT is space exponential in the causal width of the problem, and time
exponential in its width.
Proof. CBT maintains beliefs over the beams of the causal decomposition whose size are
bounded by the causal width of the problem. The join-project operation in CBT can be
performed across time, by considering one valuation at a time, without the need to first
compute and store the full joint. This is done by recursively iterating over all the beliefs
(bY )oa that participate in the join in (5), combining partial valuations from each belief, and
then storing its projection on the resulting belief bi+1
X . The number of valuations in the join
O(w(P
))
in (5) is bounded by 2
as any variable Z  BC (Y ), for Y relevant to X, is relevant
to X and thus Z  Ctx(X).
962

fiBelief Tracking for Planning with Sensing

It only remains to show Theorem 33 (stated below). The proof is not straightforward
so we split it in two parts. The first part reformulates CBT into an algorithm called Wide
(Causal) Belief Tracking (WBT), that is like CBT but performs the join operation over the
beliefs for all the variables in the problem and not only for the variables that are relevant to
X, and shows the soundness and completeness of WBT. In the second part, we show that
CBT is simply WBT applied to the subproblem PCtx(X) associated to the variable X in the
factored decomposition F , and then use the soundness and completeness of the factored
decomposition to finish the proof. The first part of the proof consists of Lemmas 68, while
the second part consists of Lemma 9 and Theorem 33.
WBT works on the causal decomposition C = hTC , BC i like CBT. The beliefs at time
0 for WBT are the same as for CBT: they are the initial belief projected into the causal
beams BC (X) for X  TC . Beliefs at later times are associated with executions  0 that
augment executions  . If we denote the belief for variable X  TC and execution  with
bX, , then the update equations for WBT are:
bX,h,ai = BC (X) o
n{FT (a, bY, ) : Y  TC } ,
bX,h,ha,`ii = BC (X) o
n{F ilter(Wa(`)T , bY, ) : Y  TC }

(8)
(9)

where T = BC (Y ) is the beam for Y , FT (a, U ) is the set uU FT (a, u), and F ilter(, U )
is the set {u  U : u |= }. These equations are essentially the equation (5) for CBT,
where progression and filtering had been separated, except that the join is performed over
all target variables instead of joining only the target variables that are relevant to X.
The following basic facts about joins, projections and filtering are easily shown and will
be used in the proofs. (We do not include their proofs here.) In their statements, the sets
U and Ui refer to sets of valuations, S refers to a collection of subset of variables, S refers
to a subset of variables and Si = V ars(Ui ), and  refers to a logical formula. The facts are:
BF1. U  o
n{S U : S  S},
BF2. For collection {Ui }iI ,

o
n{Si o
n{Ui : i  I} : i  I} = o
n{Ui : i  I},

BF3. S F ilter(, U )  F ilter(S , S U ).
Definition and Lemma 6. A decomposition D = hT, Bi factors a set U of V -valuations
iff U = o
n{B(X) U : X  T }.
A decomposition D = hT, Bi preserves transitions in a set U of V -valuations iff for
each pair of variables X, Y  T , and Z  B(X)  B(Y ), either i) Z is known in U (i.e.,
u[Z] = u0 [Z] for each u, u0  U ), ii) B(X)  B(Y )  B(W ) for some variable W  T , or
iii) for every action a, the transition function FS (a, ) is 1-1 for variable Z in U , where S
is the causal closure of Z.
Let D = hT, Bi be a decomposition such that V = XT B(X) and B(X) is causally
closed for each X  T , U be a set of V -valuations, and  be a V -formula. The following
claims hold:
1. If D factors U , then
F (a, U )  o
n{B(X)F (a, U ) : X  T } = o
n{FB(X)(a, B(X)U ) : X  T } .
963

fiBonet & Geffner

2. If D factors and preserves transitions in U , then
F (a, U ) = o
n{B(X)F (a, U ) : X  T } = o
n{FB(X)(a, B(X)U ) : X  T } .
3. If D factors U and there is X  T such that B(X) = , then
F ilter(, U ) = o
n{B(X)F ilter(, U ) : X  T } = o
n{F ilter(B(X), B(X)U ) : X  T } .
Proof. Part 1. The containment is direct by BF1, while the equality follows directly from
B(X) F (a, U ) = FB(X) (a, B(X) U ) by Lemma 1.
Part 2. The second equality and the forward inclusion for the first equality are the same
as in Part 1. We thus only need to show F (a, U )  o
n{B(X) F (a, U ) : X  T }. Let
u0 be an element in the right-hand side of this expression and X  T . Then, u0 |B(X) 
B(X) F (a, U ) and so (by Lemma 1) there is uX  B(X) U such that u0 |B(X)  FB(X) (a, uX ).
We claim that {uX }XT is a consistent collection of valuations. Indeed, if it is not, there
are valuations uX , uY and variable Z such that uX [Z] 6= uY [Z]. Clearly, Z is not known
in U . If B(X)  B(Y )  B(W ) for some W  T , then we can exchange uX and uY by
uW |B(X) and uW |B(Y ) respectively. Otherwise, we see that the function FS (a, ), where S is
the causal closure for Z, is not 1-1 for Z, contradicting the assumptions. Therefore, there is
a valuation u such that u|B(X) = uX for all X  T (i.e., u  o
n{B(X) U : X  T }) and thus,
by the assumption, u  U . Finally, since B(X) F (a, u) = FB(X) (a, u|B(X) ) = FB(X) (a, uX )
by Lemma 1, we have u0 |B(X)  B(X) F (a, u) and u0  F (a, U ).
Part 3. First, observe that BF1 and BF3 imply the chain of containments
F ilter(, U )  o
n{B(X)F ilter(, U ) : X  T }  o
n{F ilter(B(X), B(X)U ) : X  T } .
We finish by showing that equality holds through by proving that the last subset is contained
in the first. Let u0 be an element in the last subset. This u0 belongs to o
n{B(X) U : X  T }
and also to U since D factors U . We thus only need to show that u0 |= . This is direct
since by assumption there is X  T with B(X) = , and thus u0 |B(X) |= B(X) = .
Lemma 7 (Soundness of WBT). WBT is sound. That is, if C = hTC , BC i is the causal
decomposition of problem P , {bX }XTC are the local beliefs at time i, and b is the global
belief at time i, then b  o
n{bX : X  TC } and BC (X) b  bX for X  TC .
Proof. We really only need to proof the first claim b  o
n{bX : X  TC } because the second
follows directly from it by observing that bX is a belief over the variables in BC (X).
The proof of the first claim is by induction on the length of the executions. The base
case for the empty execution is easily verified. Assume that the claims hold for executions
of length n and let  0 be an execution of length n + 1 that augments an execution  of
length n. Observe that C factors U = o
n{bY, : Y  TC } by BF2, and b  U by inductive
hypothesis.
Case:  0 = h, ai.

o
n{bX,

1

0

: X  TC } = o
n{BC (X) o
n{FBC (Y )(a, bY, ) : Y  TC } : X  TC }
964

fiBelief Tracking for Planning with Sensing

2

o
n{BC (X) o
n{FBC (Y )(a, BC (Y )U ) : Y  TC } : X  TC }
3

4

 F (a, U )  F (a, b ) = b 0
where 1 is by Eq. 8, 2 because bY,  BC (Y ) U , 3 by part 1 of Lemma 6, and 4 by
inductive hypothesis.
Case:  0 = h, ha, `ii.

o
n{bX,

9

0

: X  TC } = o
n{BC (X) o
n{F ilter(Wa(`)BC (Y ), bY, ) : Y  TC } : X  TC }
10



o
n{B

C (X)

o
n{F ilter(Wa(`)B

C (Y

) , BC (Y ) U )

: Y  TC } : X  TC }

12

11

= F ilter(Wa (`), U )  F ilter(Wa (`), b ) = b 0
where 9 is by Eq. 9,
inductive hypothesis.

10

because bY,  BC (Y ) U ,

11

by part 3 of Lemma 6, and

12

by

Lemma 8 (Completeness of WBT). Let C = hTC , BC i be the causal decomposition of
problem P . If C preserves transitions in every reachable belief state, then WBT is complete.
That is, if {bX }XTC are the local beliefs at time i, and b is the global belief at time i, then
b=o
n{bX : X  TC } and BC (X) b = bX for X  TC .
Proof. The proof is by induction on the length of the executions. The base case for the
empty execution is easily verified since I contains only unit clauses. Assume that the claims
hold for executions of length n and let  0 be an execution of length n + 1 that augments
an execution  of length n. Observe that C factors U = o
n{bY, : Y  TC } by BF2, and
the inductive hypothesis implies b = U and bY, = BC (Y ) U . The proof of the first claim
b=o
n{bX : X  TC } is exactly like the proof of Lemma 7 except that all the containments
are replaced by equalities, by either using the part 2 of Lemma 6 or the inductive hypothesis.
For the second claim, we make a similar induction (in tandem with the first induction).
Again, the base case of the induction is easily verified. For the inductive step,
Case:  0 = h, ai.
1

2

bX, 0 = BC (X) o
n{FBC (Y )(a, bY, ) : Y  TC } = BC (X) o
n{FBC (Y )(a, BC (Y )U ) : Y  TC }
3

4

= BC (X) F (a, U ) = BC (X) F (a, b ) = BC (X) b 0
where 1 is by Eq. 8, 2 and 4 by inductive hypothesis, and 3 by part 2 of Lemma 6.
Case:  0 = h, ha, `ii.
5

bX, 0 = BC (X) o
n{F ilter(Wa(`)BC (Y ), bY, ) : Y  TC }
6

= BC (X) o
n{F ilter(Wa(`)BC (Y ), BC (Y )U ) : Y  TC }
7

8

= BC (X) F ilter(Wa (`), U ) = BC (X) F ilter(Wa (`), b ) = BC (X) b 0
where 5 is by Eq. 9, 6 and 8 by inductive hypothesis, and 7 by part 3 of Lemma 6.
965

fiBonet & Geffner

The following lemma shows that the tracking that CBT does on a variable X is equivalent
to the tracking that WBT does on the subproblem PX of the factored decomposition (i.e.,
PX = PBF (X) for the factored decomposition F = hTF , BF i).
Lemma 9. Let F = hTF , BF i and C = hTC , BC i be the factored and causal decompositions
W
for problem P . If  is an execution and X  TC is a state variable, then bC
X = bX where
C
bX denotes the local belief after  for variable X that is computed by CBT on the problem
P , and bW
X denotes the local belief after  for variable X that is computed by WBT on the
subproblem PBF (X) .
Proof sketch. This is a simple but tedious proof, so we just provide the sketch. Let CX =
hTX , BX i be the causal decomposition of the subproblem PBF (X) for X  TF (i.e., the
causal decomposition of the subproblem associated with variable X  TF in the factored
decomposition). The beams that participate in the join in CBT are the beams for the
variables Y  TC that are relevant to X; all such variables appear in TX as well. TX has
other variables however: observable variables Y that are not relevant to X. Yet, since all
the state variables in PBF (X) are relevant to X, the projected formulas Wa (Y = y)BF (X)
for such variables Y are all equal to true. Hence, the beams for such variables are over an
empty set of variables and just contain the empty valuation. Therefore, such beams can be
removed from the join that defines WBT on the problem PBF (X) without altering its value.
The resulting join for WBT on PBF (X) just contain the beams for variables Y  TC that
are relevant to X.
Once this fact is observed, the proof consists of a simple induction on the length of the
executions. An induction that is left as an exercise.
Theorem 33. Causal belief tracking is always sound. It is complete for causally decomposable problems.
Proof. Let F = hTF , BF i and C = hTC , BC i be the factored and causal decompositions for
problem P , and CX = hTX , BX i be the causal decomposition of the subproblem PBF (X) for
X  TF (notice that X is a state variable as TF is only comprised of such). Further, let 
W
be an execution, let bC
X and bX be the local beliefs after  for variable X that are computed
by CBT on problem P and WBT on problem PBF (X) respectively, let bFX be the local belief
after  for variable X that is computed by factored belief tracking on problem P , and let b
be the (global) belief after  on problem P .
If no observable variable is relevant to X, then BC (X) = BF (X) and CBT on X is equal
to factored belief tracking on X which is sound and complete by Theorem 20. If there are
observable variables that are relevant to X, then first notice that
W
F
bC
X = bX  BC (X) bX = BC (X) b

(10)

because Lemma 9, the soundness of WBT (cf. Lemma 7), and the soundness and completeness of FBT (cf. Theorem 20). Therefore, CBT is sound.
If the causal decomposition CX preserves transitions for every reachable belief state in
problem PBF (X) , then the containment in (10) is an equality and CBT is complete as well.
We thus finish the proof by showing that the decomposition CX for causally-decomposable
problems is a decomposition that preserves transitions for each reachable belief in PBF (X) .
966

fiBelief Tracking for Planning with Sensing

Let X  TC be a variable, let bX be a reachable belief in problem PBF (X) , let X 0 and
be two variables in TX (the target variables for the causal decomposition of problem
PBF (X) ), and let Z be a variable in BC (X 0 )  BC (X 00 ). We will show that either 1) Z is
known in bX , 2) BC (X 0 )  BC (X 00 )  BC (W ) for some variable W  TX , or 3) for every
action a, the transition function FS (a, ) is 1-1 for variable Z in bX where S is the causal
closure of Z. In such a case, the causal decomposition CX preserves transitions for every
reachable belief in problem PBF (X) .
X 00

We consider two cases:
Case: X 0 or X 00 is observable. First, apply the causal-decomposability of P to conclude that
either there is a variable W  TC that is relevant to X 0 or X 00 with BC (W )  BC (X 0 ) 
BC (X 00 ), or that Z is a memory variable. In the former case, W is relevant to X and thus
belongs to TX . In the latter case, we will show that either Z is known in bX or the transition
function FS (a, ) is 1-1 for Z in bX , where S is the causal closure of Z and a is any action
applicable at bX .
Indeed, for a proof by contradiction let us suppose that Z is not known in bX and that
the transition function is not 1-1. Then, there are two valuations s1 , s2  bX and two
progressions s01  FX (a, s1 ) and s02  FX (a, s2 ) such that s1 [Z] 6= s2 [Z] and s01 [Z] = s02 [Z].
Therefore, from observing the value of Z at state s01 and knowing the initial belief and the
actions in the execution h, ai (where  is the execution that leads to bX ), one cannot infer
the value of Z at bX because there are two different such values that are compatible with
the observation, namely s1 [Z] and s2 [Z]. Hence, Z is not a memory variable contradicting
the assumed causal-decomposability of P .
Case: X 0 and X 00 are not observables. We further divide this case in two subcases on
whether both variables X 0 and X 00 are causal ancestors of X or not. In the affirmative
subcase, BC (X 0 )  BC (X 00 )  BC (X). In the negative subcase, assume without loss of
generality that X 0 is not a causal ancestor of X. Then, there is an observable variable Y
such that X 0 is a causal ancestor of Y and Y is relevant to X. Hence, BC (Y )  BC (X 0 )
which implies Z  BC (Y )  BC (X 00 ) and this case is reduced to the previous case.

References
Albore, A., Palacios, H., & Geffner, H. (2009). A translation-based approach to contingent
planning. In Proc. 21st Int. Joint Conf. on Artificial Intelligence, pp. 16231628,
Pasadena, California.
Albore, A., Ramirez, M., & Geffner, H. (2010). Compiling uncertainty away in nondeterministic conformant planning. In Proc. 19th European Conf. on Artificial Intelligence, pp. 465470, Lisbon, Portugal.
Albore, A., Ramirez, M., & Geffner, H. (2011). Effective heuristics and belief tracking
for planning with incomplete information. In Proc. 21st Int. Conf. on Automated
Planning and Scheduling, pp. 29, Freiburg, Germany.
Amir, E., & Russell, S. (2003). Logical filtering. In Proc. 18th Int. Joint Conf. on Artificial
Intelligence, pp. 7582, Acapulco, Mexico.
967

fiBonet & Geffner

Bertoli, P., Cimatti, A., Roveri, M., & Traverso, P. (2001). Planning in nondeterministic domains under partial observability via symbolic model checking. In Nebel, B.
(Ed.), Proc. 17th Int. Joint Conf. on Artificial Intelligence, pp. 473478, Seattle, WA.
Morgan Kaufmann.
Bonet, B., & Geffner, H. (2000). Planning with incomplete information as heuristic search
in belief space. In Chien, S., Kambhampati, S., & Knoblock, C. (Eds.), Proc. 5th
Int. Conf. on Artificial Intelligence Planning Systems, pp. 5261, Breckenridge, CO.
AAAI Press.
Bonet, B., & Geffner, H. (2011). Planning under partial observability by classical replanning:
Theory and experiments. In Proc. 22nd Int. Joint Conf. on Artificial Intelligence, pp.
19361941, Barcelona, Spain.
Bonet, B., & Geffner, H. (2012a). Action selection for MDPs: Anytime AO* vs. UCT. In
Proc. 26th AAAI Conf. on Artificial Intelligence, pp. 17491755, Toronto, Canada.
Bonet, B., & Geffner, H. (2012b). Width and complexity of belief tracking in nondeterministic conformant and contingent planning. In Proc. 26th AAAI Conf. on
Artificial Intelligence, pp. 17561762, Toronto, Canada.
Bonet, B., & Geffner, H. (2013). Causal belief decomposition for planning with sensing:
Completeness results and practical approximation. In Proc. 23rd Int. Joint Conf. on
Artificial Intelligence, pp. 22752281, Beijing, China.
Bonet, B., & Geffner, H. (2014). Flexible and scalable partially observable planning with
linear translations. In Proc. 28th AAAI Conf. on Artificial Intelligence, pp. 22352241,
Quebec City, Canada.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
Cooper, G., & Moral, S. (Eds.), Proc. 14th Conf. on Uncertainty in Artificial Intelligence, pp. 3342, Madison, WI. Morgan Kaufmann.
Brafman, R. I., & Shani, G. (2012). Replanning in domains with partial information and
sensing actions. Journal of Artificial Intelligence Research, 1 (45), 565600.
Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics for belief
space search. Journal of Artificial Intelligence Research, 26, 3599.
Choi, A., & Darwiche, A. (2006). An edge deletion semantics for belief propagation and
its practical impact on approximation quality. In Proc. 21st Nat. Conf. on Artificial
Intelligence, pp. 11071114.
Cimatti, A., Roveri, M., & Bertoli, P. (2004). Conformant planning via symbolic model
checking and heuristic search. Artificial Intelligence, 159, 127206.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial
Intelligence Research, 17, 229264.
Dechter, R., & Beek, P. V. (1997). Local and global relational consistency. Theoretical
Computer Science, 173 (1), 283308.
Doucet, A., Freitas, N. D., Murphy, K., & Russell, S. (2000). Rao-blackwellised particle filtering for dynamic bayesian networks. In Proc. 16th Conf. on Uncertainty in Artificial
Intelligence, pp. 176183.
968

fiBelief Tracking for Planning with Sensing

Edelkamp, S. (2001). Planning with pattern databases. In Cesta, A. (Ed.), Proc. 6th
European Conf. on Planning, pp. 1324, Toledo, Spain. Springer: LNCS.
Goldman, R. P., & Boddy, M. S. (1996). Expressive planning and explicit knowledge. In
Drabble, B. (Ed.), Proc. 3rd Int. Conf. on Artificial Intelligence Planning Systems,
pp. 110117, Edinburgh, Scotland. AAAI Press.
Hoffmann, J., & Brafman, R. I. (2005). Contingent planning via heuristic forward search
with implicit belief states. In Biundo, S., Myers, K., & Rajan, K. (Eds.), Proc. 15th
Int. Conf. on Automated Planning and Scheduling, pp. 7180, Monterey, CA. Morgan
Kaufmann.
Hoffmann, J., & Brafman, R. I. (2006). Conformant planning via heuristic forward search:
A new approach. Artificial Intelligence, 170, 507541.
Kaelbling, L. P., Littman, M., & Cassandra, A. R. (1999). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101, 99134.
Kaye, R. (2000). Minesweeper is NP-Complete. Mathematical Intelligencer, 22 (2), 915.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. In Proc. 17th
European Conf. on Machine Learning, pp. 282293. Springer.
Lin, W., Buffet, O., Lee, C., & Teytaud, O. (2012). Optimistic heuristics for Minesweeper.
In Proc. of the Int. Computer Symposium (ICS-12). At http://hal.inria.fr/docs/
00/75/05/77/PDF/mines3.pdf.
Palacios, H., & Geffner, H. (2009). Compiling uncertainty away in conformant planning
problems with bounded width. Journal of Artificial Intelligence Research, 35, 623
675.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Ramirez, M., & Geffner, H. (2007). Structural relaxations by variable renaming and their
compilation for solving MinCostSAT. In Proc. 13th Int. Conf. on Principles and
Practice of Constraint Programming, pp. 605619. Springer.
Rintanen, J. (2008). Regression for classical and nondeterministic planning. In Ghallab,
M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.), Proc. 18th European
Conf. on Artificial Intelligence, pp. 568572, Patras, Greece.
Russell, S., & Norvig, P. (2009). Artificial Intelligence: A Modern Approach (3rd edition).
Prentice Hall.
Scott, A., Stege, U., & Rooij, I. V. (2011). Minesweeper may not be NP-Complete but is
Hard nonetheless. Science+Business Media, LLC, 33 (4), 517.
Shani, G., & Brafman, R. I. (2011). Replanning in domains with partial information and
sensing actions. In Proc. 22nd Int. Joint Conf. on Artificial Intelligence, pp. 2021
2026, Barcelona, Spain.
Shani, G., Poupart, P., Brafman, R. I., & Shimony, S. (2008). Efficient ADD operations
for point-based algorithms. In Rintanen, J., Nebel, B., & J. C. Beck, E. A. H. (Eds.),
Proc. 18th Int. Conf. on Automated Planning and Scheduling, pp. 330337, Sydney,
Australia.
969

fiBonet & Geffner

Silver, D., & Veness, J. (2010). Monte-Carlo planning in large POMDPs. In Proc. 24th
Annual Conf. on Advances in Neural Information Processing Systems, pp. 21642172.
Sipser, M. (2006). Introduction to Theory of Computation (2nd edition). Thomson Course
Technology, Boston, MA.
Smith, D., & Weld, D. (1998). Conformant graphplan. In Mostow, J., & Rich, C. (Eds.),
Proc. 15th Nat. Conf. on Artificial Intelligence, pp. 889896, Madison, WI. AAAI
Press / MIT Press.
To, S. T., Pontelli, E., & Son, T. C. (2011). On the effectiveness of CNF and DNF representations in contingent planning. In Proc. 22nd Int. Joint Conf. on Artificial Intelligence,
pp. 20332038, Barcelona, Spain.
Tran, V., Nguyen, K., Son, T. C., & Pontelli, E. (2013). A conformant planner based on
approximation: CpA(H). ACM Trans. on Intelligent Systems and Technology, 4 (2),
36.
Weld, D., Anderson, C., & Smith, D. (1998). Extending Graphplan to handle uncertainty
and sensing actions. In Proc. 15th Nat. Conf. on Artificial Intelligence, pp. 897904.
AAAI Press.

970

fiJournal of Artificial Intelligence Research 50 (2014) 189-233

Submitted 03/14; published 05/14

An Efficient Algorithm for Estimating State Sequences
in Imprecise Hidden Markov Models
Jasper De Bock
Gert de Cooman

JASPER . DEBOCK @ UGENT. BE
GERT. DECOOMAN @ UGENT. BE

Ghent University, SYSTeMS Research Group
TechnologieparkZwijnaarde 914
9052 Zwijnaarde, Belgium

Abstract
We present an efficient exact algorithm for estimating state sequences from outputs or observations in imprecise hidden Markov models (iHMMs). The uncertainty linking one state to the next,
and that linking a state to its output, is represented by a set of probability mass functions instead
of a single such mass function. We consider as best estimates for state sequences the maximal sequences for the posterior joint state model conditioned on the observed output sequence, associated
with a gain function that is the indicator of the state sequence. This corresponds to and generalises
finding the state sequence with the highest posterior probability in (precise-probabilistic) HMMs,
thereby making our algorithm a generalisation of the one by Viterbi. We argue that the computational complexity of our algorithm is at worst quadratic in the length of the iHMM, cubic in the
number of states, and essentially linear in the number of maximal state sequences. An important
feature of our imprecise approach is that there may be more than one maximal sequence, typically
in those instances where its precise-probabilistic counterpart is sensitive to the choice of prior. For
binary iHMMs, we investigate experimentally how the number of maximal state sequences depends on the model parameters. We also present an application in optical character recognition,
demonstrating that our algorithm can be usefully applied to robustify the inferences made by its
precise-probabilistic counterpart.

1. Introduction
In the field of Artificial Intelligence, probabilistic graphical models have become a powerful tool,
especially in domains where reasoning under uncertainty is needed (Koller & Friedman, 2009;
Pearl, 1988). Usually, this uncertainty is expressed by probabilities, which are estimated from data
or elicited from domain experts. However, the assumption that such probabilities can be obtained,
or for that matter, that they exist, is not always realistic. This can for example happen when multiple
experts disagree, when rounding errors occur, or when the available data is limited; the latter can
either be inherent to the problem or a consequence of economic and temporal constraints.
In order to relax this assumption, one can use the theory of imprecise probabilities. The basic
idea is to allow for sets of probability distributions rather than requiring the specification of a single
one. In this way, partial probabilistic information can be expressed easily, for example, by means
of linear constraints on probability distributions. This theory of imprecise probability encompasses
a number of different, but closely related frameworks; coherent lower previsions (Walley, 1991),
interval probabilities (Weichselberger, 2000) and belief functions (Dempster, 1967; Shafer, 1976)
are well-known examples.
c
2014
AI Access Foundation. All rights reserved.

fiD E B OCK & D E C OOMAN

In the context of graphical models, these imprecise-probabilistic ideas have been used to develop
the notion of a credal network (Cozman, 2000, 2005). It is similar to a Bayesian network, but more
general in the sense that it allows for local uncertainty models that are imprecisely specified, such
as sets of probability distributions. This gain in generality, however, comes at the price of added
computational complexity and most existing algorithms are either approximative or cannot handle
large networks. In fact, inferences in credal networks are proven to be NP-hard even for singly
connected networks with ternary variables (Mau, de Campos, Benavoli, & Antonucci, 2013).
A notable exception to the intractability of inference problems in credal networks occurs when
we drop the so-called strong independence assumption that is usually associated with credal networks and replace it by an assessment of epistemic irrelevance. Strong independence requires that
the credal network is a convex hull of (precise) Bayesian networks, whereas epistemic irrelevance
is a less restrictive property, which is imposed on the imprecise model itself instead of on the individual precise models it consists of; for more information about the difference between these two
approaches, see for example the pioneering work of Cozman (2000). Recent work (De Cooman,
Hermans, Antonucci, & Zaffalon, 2010) has shown that the use of epistemic irrelevance guarantees that there is an efficient algorithm for updating beliefs about a single target node of a credal
tree, that is essentially linear in the number of nodes in the tree. For imprecise-probabilistic hidden Markov models (iHMMs), which are the credal network equivalent of hidden Markov models
(HMMs), this efficiency for single target node inferences has been succesfully exploited to develop
an imprecise-probabilistic counterpart to the Kalman filter (Benavoli, Zaffalon, & Miranda, 2011).
In this paper, we tackle the imprecise-probabilistic counterpart of another important application
of HMMs: finding the sequence of hidden states that has the highest posterior probability conditional on an observed sequence of outputs (Rabiner, 1989). For HMMs with precise local transition
and emission probabilities, an efficient dynamic programming algorithm for performing this task
was developed by Viterbi (1967). For imprecise-probabilistic HMMs however, we know of no algorithm in the literature for which the computational complexity comes even close to that of Viterbis.
We remedy this situation by developing an efficient exact algorithm, called EstiHMM1 , that solves
the following imprecise-probabilistic generalisation of the state estimation problem: given an observed sequence of outputs, which are the maximal (Troffaes, 2007; Walley, 1991) state sequences
for the posterior joint model?
An important difference between our imprecise approach and the more conventional preciseprobabilistic approach is that the EstiHMM algorithm may sometimes return more than one solution,
whereas the Viterbi algorithm will always produce only a single one. The more imprecise the iHMM
is, the more maximal state sequences there will be. For precise HMMs, the EstiHMM and Viterbi
algorithms produce identical results. The advantage of this behaviour is that the EstiHMM algorithm
will typically return more than one maximal sequence only in those instances where the precise
approach is sensitive to the choice of prior. In those cases, the set-valued solution of the EstiHMM
algorithm is more likely to contain the correct hidden sequence. Our application in optical character
recognition (see Section 9) illustrates this advantage convincingly.
From a credal network point of view, the main contribution of this paper is the EstiHMM algorithm itself. What is especially surprising about this algorithm, is that it provides an efficient
solution to an inference problem that deals with multiple target nodes at once, a situation which, in
general, is very difficult to handle for current state of the art algorithms in the field. We think that
1. EstiHMM: Estimation in imprecise Hidden Markov Models

190

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

the promising results in this paper motivate the study of similar problems for network topologies
that go beyond HMMs.
The importance of our results to the HMM community and, by extension, to the field of AI
in general, is that they illustrate that model uncertaintynot to be confused with the probabilistic
uncertainty that is intrinsic to the model itselfcan be dealt with efficiently and can, at the same
time, lead to informative, set-valued estimates (sets of maximal state sequences) that can be usefully
applied in real-life problems. We believe that model uncertainty is relevant in all subfields of AI
where it is difficultif not impossibleto accurately pinpoint a single probability distribution.
Such model uncertainty might have a severe impact on the resulting inferences and, if so, this
should be taken into account when basing decisions on these inferences.
We start of in Section 3 by describing imprecise hidden Markov models as a special case of
credal trees under epistemic irrelevance. We show in particular how we can use the ideas underlying
the MePiCTIr2 algorithm (De Cooman et al., 2010) to construct a most conservative joint model
from imprecise local transition and emission models. We also derive a number of interesting and
useful formulas from that construction. The results in this section assume basic knowledge of the
theory of coherent lower previsions. We include a short introduction to this theory in Section 2.
In Section 4, we explain the maximality criterion and show how it leads to a set of optimal
estimates for the hidden state sequence. Finding all the maximal state sequences seems a daunting
task at first: it has a search space that grows exponentially in the length of the Markov chain.
However, as shown in Section 5, we can use the basic formulas of Section 3 to derive an appropriate
version of Bellmans Principle of Optimality (Bellman, 1957), resulting in an exponential reduction
of the search space. By using a number of additional tricks, including a clever reformulation of
the maximality criterion, this enables us in Section 6 to devise the EstiHMM algorithm, which
efficiently constructs the set of all maximal state sequences.
Section 7 discusses the computational complexity of this EstiHMM algorithm. We show that it
is essentially linear in the number of maximal sequences, quadratic in the length of the chain, and
cubic in the number of states. We perceive this complexity to be comparable to that of the Viterbi
algorithm, especially after realising that the latter makes the simplifying step of resolving ties more
or less arbitrarily in order to produce only a single optimal state sequence.
In Section 8, we consider the special case of binary iHMMs, and investigate experimentally how
the number of maximal state sequences depends on the model parameters. We comment on the interesting structures that emerge, and provide an heuristic explanation for them. We also demonstrate
the algorithms efficiency by calculating the maximal sequences for an iHMM of length 100.
Finally, in Section 9, we present an application in optical character recognition. It clearly
demonstrates the advantages of our algorithm and gives a clear indication that the EstiHMM algorithm is able to robustify the results of the existing Viterbi algorithm in an intelligent manner.
We conclude the paper in Section 10 and discuss a number of possible avenues for future research. In order to make our main argumentation as readable as possible, all technical proofs are
relegated to an appendix.

2. Freshening Up on Coherent Lower Previsions
We begin with some basic theory of coherent lower previsions; for more information, we refer to
Walleys book (1991) and the more recent survey by Miranda (2008).
2. MePiCTIr: Message Passing in Credal Trees under Irrelevance.

191

fiD E B OCK & D E C OOMAN

Coherent lower previsions are a special type of imprecise probability model. Roughly speaking,
whereas classical probability theory assumes that a subjects uncertainty can be represented by a
single probability mass function, the theory of imprecise probabilities effectively works with sets
of possible probability mass functions, and thereby allows for imprecision as well as indecision to
be modelled and represented. For people who are unfamiliar with the theory, looking at it as a way
of robustifying the classical theory is perhaps the easiest way to understand and interpret it, and we
will use this approach here.
2.1 Unconditional Lower Previsions
Let X be any non-empty, finite3 set of possible states. We call a real-valued function f on X a
gamble and denote the set of all gambles on X as G (X). Consider now a set M of probability mass
functions on X. Then with each mass function p  M , we can associate a linear previsionor expectation functionalPp , defined on G (X). For every gamble f  G (X) , Pp ( f ) := xX p(x) f (x)
is the expected value of f , associated with the probability mass function p. We now define the lower
previsionor lower expectation functionalPM that corresponds with the set M as the following
lower envelope of linear previsions:
PM ( f ) := inf {Pp ( f ) : p  M } for all f  G (X).

(1)

Similarly, we define the upper previsionor upper expectation functionalPM as
PM ( f ) := sup {Pp ( f ) : p  M } =  inf {Pp ( f ) : p  M } = PM ( f ) for all f  G (X). (2)
We will mostly talk about lower previsions, since it follows from the conjugacy relation (2) that the
two models are mathematically equivalent.
An event A is a subset of the set of possible values X: A  X. With such an event, we can
associate an indicator IA , which is the gamble on X that assumes the value 1 on A, and 0 outside A.
We call


PM (A) := PM (IA ) = inf  p(x) : p  M
xA

the lower probability of the event A, and similarly PM (A) := PM (IA ) its upper probability.
It can be shown (Walley, 1991) that the functional PM satisfies the following set of interesting
mathematical properties, which define a coherent lower prevision:
C1. PM ( f )  min f for all f  G (X),
C2. PM ( f ) =  PM ( f ) for all f  G (X) and real   0,
C3. PM ( f + g)  PM ( f ) + PM (g) for all f , g  G (X).

[non-negative homogeneity]
[superadditivity]

Every set of mass functions M uniquely defines a coherent lower prevision PM , but in general
the converse does not hold. However, if we limit ourselves to sets of mass functions M that are
closed and convexwhich makes them credal setsthey are in a one-to-one correspondence with
coherent lower previsions (Walley, 1991). This implies that we can use the theory of coherent
3. The theory of coherent lower previsions is applicable to non-finite sets as well, at the expense of some complications.
However, for our present purposes, it suffices to consider the finitary case only.

192

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

lower previsions as a tool for reasoning with closed convex sets of probability mass functions.
From now on, we will no longer explicitly refer to credal sets M , but we will simply talk about
coherent lower previsions P. It is useful to keep in mind that there always is a unique credal set that
corresponds with such a coherent lower prevision: P = PM for some unique credal set M , given
by M = {p : ( f  G (X))Pp ( f )  P( f )}.
A special kind of imprecise model on X is the vacuous lower prevision. It is a model that
represents complete ignorance and therefore has the set of all possible mass functions on X as its
credal set M . It can be shown easily that for every f  G (X), the corresponding lower prevision is
given by P( f ) = min f .
2.2 Conditional Lower Previsions
Conditional lower and upper previsions, which are extensions of the classical conditional expectation functionals, can be defined in a similar, intuitively obvious way: as lower envelopes associated
with sets of conditional mass functions.
Consider a variable X in X and a variable Y in Y . A conditional lower prevision P(|Y ) on the
set G (X) of all gambles on X is a two-place real-valued function. For any gamble f on X, P( f |Y )
is a gamble on Y , whose value P( f |y) in y  Y is the lower prevision of f , conditional on the event
Y = y. If for any y  Y , the lower prevision P(|y) is coherentsatisfies conditions C1C3then
we call the conditional lower prevision P(|Y ) separately coherent. It will sometimes be useful to
extend the domain of the conditional lower prevision P(|y) from G (X) to G (X  Y ) by letting
P( f |y) := P( f (, y)|y) for all gambles f on X  Y .
If we have a number of conditional lower previsions involving a number of variables, then each
of them must be separately coherent, but we also have to make sure that they satisfy a more stringent
joint coherence requirement. Explaining this in detail would take us too far; Walley (1991) provides
a detailed discussion with motivation. For our present purposes, it suffices to say that joint coherence
is very closely related to making sure that these conditional lower previsions are lower envelopes
associated with conditional mass functions that satisfy Bayess Rule.
For a given lower prevision P on G (X  Y ), there may be more than one corresponding conditional lower prevision P(|Y ) that is jointly coherent with P. Depending on the updating method
that is used, one obtains a different model.
If we use natural
coherent lower prevision P(|Y ) is defined
 extension, then the conditional
	
by P( f |y) := max   R : P(I{y} [ f  ])  0 if P({y}) > 0 and is vacuous and thus given by
P( f |y) := min f if P({y}) = 0. This is the smallest, most conservative coherent way of conditioning
a lower prevision. If P({y}) > 0, it corresponds to conditioning every probability mass function
in the credal set of P on the observation that Y = y and taking the lower envelope of all these
conditioned mass functions.
If we use regular extension, then P(|Y ) is defined by P( f |y) := max {  R : P(Iy [ f  ])  0}
if P({y}) > 0 and is vacuous if P({y}) = 0. If P({y}) > 0, then regular extension (a) gives us the
greatestmost informativeconditional lower prevision that is jointly coherent with the original
unconditional lower prevision and (b) corresponds to taking all mass functions p in the credal set
of P for which p(y) 6= 0, conditioning them on the observation that Y = y and taking their lower
envelope.
Natural and regular extension coincide if P({y}) > 0 or P({y}) = 0 but they may differ if
P({y}) > P({y}) = 0. In the latter case, natural extension is vacuous, but regular extension usu193

fiD E B OCK & D E C OOMAN

ally remains more informative. Furthermore, if P({y}) > 0, then every coherent updating method
yields a conditional lower prevision that lies in between those obtained by natural and regular extension (Walley, 1991; Miranda, 2009).
2.3 Different Interpretations for Lower Previsions
As we have just seen, a coherent lower prevision P serves as an alternative representation for a
closed and convex set M of probability mass functions. Often, this credal set M is interpreted
as a set of candidates for the one true but unknown probability mass function. This interpretation is particularly intuitive for people that are used to working with classical probability theory.
Walley (1991, Section 2.10.4) calls this the sensitivity analysis interpretation. For the sake of completeness, we mention here that coherent lower previsions can also be given a behavioural interpretation, without using the notion of a probability mass function. The lower prevision P( f ) of
a gamble f  G (X) is then interpreted as the supremum acceptable buying price that a subject is
willing to pay in order to gain thepossibly negativereward f (x) after the outcome x  X of the
experiment has been determined. Walley discusses this alternative interpretation extensively.

3. Basic Notions
An imprecise hidden Markov model can be depicted using the following probabilistic graphical
model:
Q1 ()

Q2 (|X1 )

Qk (|Xk1 )

Qn (|Xn1 )

State sequence:

X1

X2

Xk

Xn

Output sequence:

O1

O2

Ok

On

S1 (|X1 )

S2 (|X2 )

Sk (|Xk )

Sn (|Xn )

Figure 1: Tree representation of a hidden Markov model
Here n is some natural number. The state variables X1 , . . . , Xn assume values in the respective finite
sets X1 , . . . , Xn , and the output variables O1 , . . . , On assume values in the respective finite sets O1 ,
. . . , On . We denote generic values of Xk by xk , xk or zk , and generic values of Ok by ok .
3.1 Local Uncertainty Models
We assume that we have the following local uncertainty models for these variables. For X1 , we have
a marginal lower prevision Q1 , defined on the set G (X1 ) of all real-valued mapsor gambles
on X1 . For the subsequent states Xk , with k  {2, . . . , n}, we have a conditional lower prevision
Qk (|Xk1 ) defined on G (Xk ), called a transition model. In order to maintain uniformity of notation,
we will also denote the marginal lower prevision Q1 as a conditional lower prevision Q1 (|X0 ),
where X0 denotes a variable that may only assume a single value x0  X0 := {x0 }, and whose
194

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

value is therefore certain. For any gamble fk in G (Xk ), Qk ( fk |Xk1 ) is interpreted as a gamble on
Xk1 , whose value Qk ( fk |xk1 ) in any xk1  Xk1 is the lower prevision of the gamble fk (Xk ),
conditional on Xk1 = xk1 .
In addition, for each output Ok , with k  {1, . . . , n}, we have a conditional lower prevision
Sk (|Xk ) defined on G (Ok ), called an emission model. For any gamble gk in G (Ok ), Sk (gk |Xk ) is
interpreted as a gamble on Xk , whose value Sk (gk |xk ) in any xk  Xk is the lower prevision of the
gamble gk (Ok ), conditional on Xk = xk .
We take all these localmarginal, transition and emissionuncertainty models to be separately
coherent. Recall that this simply means that for any k  {1, . . . , n}, the lower prevision Qk (|xk1 )
should be coherentas an unconditional lower previsionfor all xk1  Xk1 and Sk (|xk ) should
be coherent for all xk  Xk .
3.2 Interpretation of the Graphical Structure
We will assume that the graphical representation in Figure 1 represents the following irrelevance
assessments: conditional on its mother variable, the non-parent non-descendants of any variable
in the tree are epistemically irrelevant to this variable and its descendants. We say that a variable
X is epistemically irrelevant to a variable Y if observing X does not affect our beliefs about Y .
Mathematically stated in terms of lower previsions: P( f (Y )) = P( f (Y )|x) for all f  G (Y ) and all
x  X.
Before we go on, it will be useful to introduce some mathematical short-hand notation for
describing joint variables in the tree of Figure 1. For any 1  k  `  n, we denote the tuple
(Xk , Xk+1 , . . . , X` ) by Xk:` , and the tuple (Ok , Ok+1 , . . . , O` ) by Ok:` . Xk:` is a variable that can assume
all values in the set Xk:` := `r=k Xr , and Ok:` is a variable that can assume all values in the set
Ok:` := `r=k Or . Generic values of Xk:` are denoted by xk:` , xk:` or zk:` , and generic values of Ok:` by
ok:` .
Example 1. Consider the variable Xk with mother variable Xk1 in Figure 1. The variables X1:k2
and O1:k1 are its non-parent non-descendants, and the variables Xk+1:n and Ok:n its descendants.
Our interpretation of the graphical structure of Figure 1 implies that once we knowconditional
onthe value xk1 of Xk1 , additionally learning the values of any of the variables X1 , . . . , Xk2 and
O1 , . . . , Ok1 will not change our beliefs about Xk:n and Ok:n .

3.3 Constructing a Global Uncertainty Model
Using the local uncertainty models, we now want to construct a global model: a joint lower prevision
P on G (X1:n  O1:n ) for all the variables (X1:n , O1:n ) in the tree. This joint lower prevision should
(i) be jointly coherent with all the local models; (ii) encode all epistemic irrelevance assessments
encoded in the tree; and (iii) be as small, or conservative,4 as possible. This is a special case of
a more general problem for credal trees, discussed and solved in great detail by De Cooman et
al. (2010). In this section, we summarise the solution for iHMMs and give an heuristic justification
for it; De Cooman et al. prove that the joint model that is presented below is indeed the most
conservative lower prevision that is coherent with all the local models and captures all epistemic
irrelevance assessments encoded in the tree.
4. Recall that pointwise smaller lower previsions correspond to larger credal sets.

195

fiD E B OCK & D E C OOMAN

We proceed in a recursive manner. For any k  {1, . . . , n} and any xk1  Xk1 , we consider the
smallest coherent joint lower prevision Pk (|xk1 ) on G (Xk:n  Ok:n ) for the variables (Xk:n , Ok:n ) in
the iHMM depicted in Figure 2, representing a subtree of the tree represented in Figure 1, with the
lower prevision Qk (|xk1 ) acting as the marginal model for the first state variable Xk . Note that,
due to the notational trick that was introduced in Section 3.1, the global model P can be identified
with the conditional lower prevision P1 (|x0 ).

Qk (|xk1 )

Qk+1 (|Xk )

Xk

Xk+1

Pk (|Xk1 )
E k (|Xk )

Ok

Ok+1

Sk (|Xk )

Sk+1 (|Xk+1 )

Pk+1 (|Xk )

Figure 2: Subtree of the iHMM involving the variables (Xk:n , Ok:n )
Our aim is now to develop recursive expressions that enable us to construct Pk (|xk1 ) out of
Pk+1 (|Xk ), Sk (|Xk ) and Qk (|xk1 ). Using these expressions over and over again will eventually
yield the global model P = P1 (|x0 ).
As a first step, we consider any xk  Xk and combine the joint model Pk+1 (|xk ) for the variables
(Xk+1:n , Ok+1:n ), defined on G (Xk+1:n  Ok+1:n )see the thick dotted lines in Figure 2,with the
local model Sk (|xk ) for the variable Ok , defined on G (Ok ). This will lead to a joint model E k (|xk )
for the variables (Xk+1:n , Ok:n ), defined on G (Xk+1:n  Ok:n )see the semi-thick dotted lines in
Figure 2. This is trivial for k = n, since we must have that E n (|xn ) = Sn (|xn ).
For k 6= n, the solution is less obvious. A joint model can be constructed in many different ways,
so we will have to impose some conditions. A first condition is that E k (|xk ) should be a coherent
lower prevision that is jointly coherent with the marginal models Pk+1 (|xk ) and Sk (|xk ). A second, rather obvious, condition is that E k (|xk ) should coincide with Pk+1 (|xk ) and Sk (|xk ) on their
respective domains. A third condition is that the model should capture the epistemic irrelevance
assessments encoded in the tree. In particular these state that, conditional on Xk = xk , the two variables (Xk+1:n , Ok+1:n ) and Ok should be epistemically independent, or in other words, epistemically
irrelevant to one another.
Any model that meets all these conditions is called an independent product (De Cooman, Miranda, & Zaffalon, 2011) of Pk+1 (|xk ) and Sk (|xk ). Generally speaking, such an independent product is not unique. We call the pointwise smallest, most conservative, of all possible independent
196

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

products, which always exists, the independent natural extension (Walley, 1991; De Cooman et al.,
2011) of Pk+1 (|xk ) and Sk (|xk ), and we denote it as Pk+1 (|xk )  Sk (|xk ).
Summarising, E k (|xk ) is given by
(
Sn (|xn )
if k = n
E k (|xk ) :=
Sk (|xk )  Pk+1 (|xk ) if k = n  1, . . . , 1.

(3)

The conditionally independent natural extension and its properties were studied in great detail
by De Cooman et al. (2011). For the purposes of this paper, it will suffice to recall from that
study thatvery much like independent products of precise probability modelssuch independent
natural extensions are factorising, which implies in particular that
E k ( f g|xk ) = E k (gE k ( f |xk )|xk ) = Sk (gPk+1 ( f |xk )|xk )
(
Sk (g|xk )Pk+1 ( f |xk )
=
Sk (g|xk )Pk+1 ( f |xk )

if Pk+1 ( f |xk )  0
if Pk+1 ( f |xk )  0

= Sk (g|xk ) fi Pk+1 ( f |xk ),

(4)

for all f  G (Xk+1:n  Ok+1:n ) and all non-negative g  G (Ok )we call a gamble non-negative if
all its values are. In this expression, the first equality is the actual factorisation property. The second
equality holds because E k (|xk ) coincides with Pk+1 (|xk ) and Sk (|xk ) on their respective domains.
The third equality follows from the conjugacy relationEquation (2)and coherence condition
C2, and for the fourth we have used the shorthand notation m fi x := m max{0, x} + m min{0, x}.
Further on, we will also use the analogous notation m n fi x := m n max{0, x} + m n min{0, x}.
In a second and final step, we combine the joint model E k (|Xk ) for the variables (Xk+1:n , Ok:n ),
defined on G (Xk+1:n Ok:n ), with the local model Qk (|xk1 ) for the variable Xk , defined on G (Xk ),
into the joint model Pk (|xk1 ) for the variables (Xk:n , Ok:n ), defined on G (Xk:n  Ok:n ). It has been
shown elsewhere (Miranda & de Cooman, 2007; Walley, 1991) that the most conservative coherent
way of doing this, is by means of marginal extension, also known as the law of iterated lower
expectations. This leads to Pk (|xk1 ) := Qk (E k (|Xk )|xk1 ), or, if we now allow xk1 to range over
Xk1 :
Pk (|Xk1 ) := Qk (E k (|Xk )|Xk1 ).

(5)

For practical purposes, it is useful to see that this is equivalent with

Pk ( f |Xk1 ) = Qk



xk Xk


fi
fi
I{xk } E k ( f (xk , Xk+1:n , Ok:n )|xk )fiXk1

for all f  G (Xk:n  Ok:n ). Recall that in this expression, the indicator I{xk } is a gamble on Xk that
assumes the value 1 if Xk = xk and 0 if Xk 6= xk .
197

fiD E B OCK & D E C OOMAN

3.4 Interesting Lower and Upper Probabilities
Without too much trouble,5 we can use Equations (3)(5) to derive the following expressions for a
number of interesting lower and upper probabilities:
n

Pk ({ok:n }  {xk:n }|xk1 ) =  Si ({oi }|xi )Qi ({xi }|xi1 )

(6)

Pk ({ok:n }  {xk:n }|xk1 ) =  Si ({oi }|xi )Qi ({xi }|xi1 )

(7)

i=k
n
i=k

for all xk1  Xk1 , xk:n  Xk:n , ok:n  Ok:n and k  {1, . . . , n}, and
n

E k ({ok:n }  {xk+1:n }|xk ) = Sk ({ok }|xk )



Si ({oi }|xi )Qi ({xi }|xi1 )

(8)



Si ({oi }|xi )Qi ({xi }|xi1 ).

(9)

i=k+1
n

E k ({ok:n }  {xk+1:n }|xk ) = Sk ({ok }|xk )

i=k+1

for all xk  Xk , xk+1:n  Xk+1:n , ok:n  Ok:n and k  {1, . . . , n}. Recall that we equate events with
their indicators, and that the lower and upper prevision of these indicators correspond to the lower
and upper probability of that event; see Section 2. For example, in Equation (6), Pk ({ok:n } 
{xk:n }|xk1 ) := Pk (I{ok:n } I{xk:n } |xk1 ) is the lower probability that, conditional on Xk1 = xk1 , the
rest of the hidden sequence has the value xk:n , with corresponding observations ok:n . This joint lower
probablity is obtained simply by multiplying the relevant local lower (transition and emission) probabilities.
We will assume throughout that
P({x1:n }  {o1:n }) > 0 for all x1:n  X1:n and o1:n  O1:n
or, equivalentlyby Equation (7), for k = 1, that all local upper probabilities are positive, in the
sense that (De Cooman et al., 2010):
Qk ({xk }|xk1 ) > 0 and Sk ({ok }|xk ) > 0
for all k  {1, . . . , n}, xk1  Xk1 , xk  Xk and ok  Ok . (10)
This assumption is very weak and not at all restrictive for practical purposes. The impreciseprobabilistic local models are often constructed by adding some margin of error around a precise
model, thereby making all upper transition probabilities positive by construction. We will however allow lower transition probabilities to be zero, which is something that does happen often in
practical problems.
Proposition 1. If all local upper probabilities are positiveEquation (10), then we have for all
k  {1, . . . , n}, xk  Xk , xk1  Xk1 and ok:n  Ok:n that Pk ({ok:n }|xk1 ) > 0 and E k ({ok:n }|xk ) > 0.
5. As an example, we derive Equations (6) and (7) in Appendix A.

198

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

4. Estimating States from Outputs
In a hidden Markov model, the states are not directly observable, but the outputs are, and the general
aim is to use the outputs to estimate the states. We concentrate on the following problem: Suppose
we have observed the output sequence o1:n , estimate the state sequence x1:n . We will use an essentially Bayesian approach to do so, but need to allow for the fact that we are working with imprecise
rather than precise probability models. We consider as optimal estimates all state sequences that are
maximal, a criterion which we introduce in Section 4.2; see Section 4.3 for two alternative criteria,
which we will not consider further in the context of this paper. The main contribution of this section
is a formulation of maximality that is stated directly in terms of the unconditional global model P,
instead of the conditional model P(|o1:n ) that is conventionally used for this purpose. Furthermore,
and rather surprisingly, this alternative formulation is valid regardless of whether we use regular or
natural extension to derive P(|o1:n ) from P.
4.1 Updating the iHMM
The first step in our approach consists in updating (or conditioning) the joint model P := P1 (|x0 )
on the observed outputs O1:n = o1:n . As mentioned in Section 2, there is no unique coherent way
to perform this updating. However, for the particular problem we are solving in this paper, it so
happens that it makes no difference which updating method is used, as long as it is coherent. For
the time being, we use regular extension, but later on in Section 4.2, we will show that any other
coherent updating method yields the same results.
Since it follows from the positivity assumption (10) and Proposition 1 that P({o1:n }) > 0, regular
extension leads us to consider the updated lower prevision P(|o1:n ) on G (X1:n ), given by:

	
P( f |o1:n ) := max   R : P(I{o1:n } [ f  ])  0 for all gambles f on X1:n .
(11)
Using the coherence of the joint lower prevision P, it is not hard to prove that when P({o1:n }) > 0,
P(I{o1:n } [ f  ]) is a strictly decreasing and continuous function of , which therefore has a unique
zerosee Lemma 7(i)&(iii) in Appendix A. As a consequence, we have for any f  G (X1:n ) that
P( f |o1:n )  0  ( > 0)P(I{o1:n } [ f  ]) < 0  P(I{o1:n } f )  0.

(12)

In fact, it is not hard to infer from the strictly decreasing and continuous character of P(I{o1:n } [ f  ])
that P( f |o1:n ) and P(I{o1:n } f ) have the same sign. They are either both negative, both positive or
both equal to zero; see also Figure 3.

P(I{o1:n } f )

P(I{o1:n } [ f  ])
P( f |o1:n )
Figure 3: Conditional versus unconditional lower prevision
199

fiD E B OCK & D E C OOMAN

Equation (12) will be of crucial importance further on. However, in general, we want to allow
P({o1:n }) to be zerosince this may happen if you allow lower transition probabilities to be zero,
while requiring that P({o1:n }) > 0because this follows from the positivity assumption (10) and
Proposition 1. This will, generally speaking, invalidate the second equivalence in Equation (12): it
turns into an implication only. But, if we limit ourselves to the specific type of gambles on X1:n of
the form f = I{x1:n }  I{x1:n } , we can still prove the following important theorem.
Theorem 2. If all local upper probabilities are positiveEquation (10), then for fixed values of
x1:n , x1:n  X1:n and o1:n  O1:n , we have that P(I{o1:n } [I{x1:n }  I{x1:n } ]) and P(I{x1:n }  I{x1:n } |o1:n )
have the same sign. They are both positive, both negative or both zero.
4.2 Maximal State Sequences
The next step now consists in using the posterior model P(|o1:n ) to find best estimates for the state
sequence x1:n . On the Bayesian approach, this is usually done by solving a decision-making, or
optimisation problem: we associate a gain function I{x1:n } with every candidate state sequence x1:n ,
and select as best estimates those state sequences x1:n that maximise the posterior expected gain,
resulting in state sequences with maximal posterior probability.
Here we generalise this decision-making approach towards working with imprecise probability
models. The criterion we use to decide which estimates are optimal for the given gain functions is
that of (WalleySen) maximality (Troffaes, 2007; Walley, 1991). Maximality has a number of very
desirable properties that make sure it works well in optimisation contexts (De Cooman & Troffaes,
2005; Huntley & Troffaes, 2010), and it is well-justified from a behavioural point of view, as well
as in a robustness approach, as we shall see presently.
We can express a strict preference  between two state sequence estimates x1:n and x1:n as
follows:
x1:n  x1:n  P(I{x1:n }  I{x1:n } |o1:n ) > 0.
On a behavioural interpretation, this expresses that a subject with lower prevision P(|o1:n ) is disposed to pay some strictly positive amount of utility to replace the gain associated with the estimate
x1:n with the gain associated with the estimate x1:n ; Walley (1991, Section 3.9) provides additional
information. Alternatively, from a robustness point of view, this expresses that for each conditional
mass function p(|o1:n ) in the credal set associated with the updated lower prevision P(|o1:n ), the
state sequence x1:n has a posterior probability p(x1:n |o1:n ) that is strictly higher than the posterior
probability p(x1:n |o1:n ) of the state sequence x1:n .
The binary relation  thus defined is a strict partial orderan irreflexive and transitive binary
relationon the set of state sequences X1:n , and we consider an estimate x1:n to be optimal when it
is undominated, or maximal, in this strict partial order:
x1:n  opt (X1:n |o1:n )  (x1:n  X1:n )x1:n 6 x1:n
 (x1:n  X1:n )P(I{x1:n }  I{x1:n } |o1:n )  0
 (x1:n  X1:n )P(I{o1:n } [I{x1:n }  I{x1:n } ])  0,

(13)

where the very useful last equivalence follows from Theorem 2. In summary then, the aim of this
paper is to develop an efficient algorithm for finding the set of maximal estimates opt (X1:n |o1:n ).
Our statement in Section 4.1, that any coherent updating method would yield the same results
as regular extension, can now be justified. Since coherent updating is unique if P({o1:n }) > 0, and
200

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

since the case P({o1:n }) = 0 is excluded by Proposition 1 and our positivity assumption (10), we
only need to motivate our statement in the special case that P({o1:n }) = 0 and P({o1:n }) > 0.
If we use regular extension to update our model, then the optimal estimates are given by Equation (13). For the special case P({o1:n }) = 0, we find for all x1:n  X1:n and x1:n  X1:n that
P(I{o1:n } [I{x1:n }  I{x1:n } ])  P(I{o1:n } ) = P({o1:n }) = 0,
where the first inequality follows from the monotonicity of coherent lower previsions (as a consequence of C1 and C2). Therefore, we find that if P({o1:n }) = 0, then all sequences are optimal,
resulting in opt (X1:n |o1:n ) = X1:n .
If we use natural extension to update our joint model, then the optimal state sequences are
still given by Equation (13), but the final equivalence no longer holds because it uses Theorem 2,
which assumes the use of regular extension to perform updating of the joint model. However, for
the special case of P({o1:n }) = 0, natural extension by definition leads to the updated model being
equal to the vacuous one. Therefore, we find for all x1:n  X1:n and x1:n  X1:n that
P(I{x1:n }  I{x1:n } |o1:n ) = min(I{x1:n }  I{x1:n } )  0.
This implies that for the special case where P({o1:n }) = 0 and P({o1:n }) > 0identical to what we
found for regular extensionnatural extension also results in all sequences being optimal, meaning
that opt (X1:n |o1:n ) = X1:n .
We have thus shown that, even in the special case that P({o1:n }) = 0 and P({o1:n }) > 0, the
set of optimal sequences is the same, regardless of whether we use natural or regular extension
to update our joint model. Since in that special case, every other coherent updating method lies
in between these two methods, all of them are bound to yield the same opt (X1:n |o1:n ). We can
therefore conclude that the results in this paper do not depend on the particular updating method
that is chosen, as long as it is coherent.
4.3 Other Decision Criteria
Instead of looking for maximal state sequences, one could use other decision criteria as well (Troffaes, 2007), two of which we discuss in the present section.
A first approach that we will not consider further on, consists in trying to find the so-called
-maximin state sequences x1:n , which maximise the posterior lower probability:
x1:n  argmax P({x1:n }|o1:n ).
x1:n X1:n

This approach basically optimises the worst-case scenariothe lower probabilityand can therefore be regarded as a risk averse choice. From a computational point of view, finding these maximin sequences is a rather complicated affair. Not only do we need to optimise over an exponential number of sequences, but on top of that, every single lower probability P({x1:n }|o1:n ) in this
optimisation problem is hard to compute. On the positive side, we have recently discovered thatin
the case of epistemic irrelevanceit is possible to calculate these lower probabilities efficiently in
a recursive manner. However, these results are not published yet and fall beyond the scope of the
current paper. We know of no other algorithm that can calculate these lower probabilities efficiently.
In any case, the issue still remains that we need to optimise over the exponentially large set X1:n .
201

fiD E B OCK & D E C OOMAN

A second approach that will not be considered further on consists in working with the so-called
E-admissible sequences, which are those sequences that maximise the expected gain for at least one
conditional mass function p(|o1:n ) in the credal set associated with the updated lower prevision
P(|o1:n ). If one interprets an imprecise model as a collectiona credal setof precise models,
one of which is the unknown true model, then one of these E-admissible solutions is the unknown
true solution. E-admissible state sequences are very difficult to compute. The intuitive reason
is that we need to solve the precise problem for every p(|o1:n ) in the credal set associated with
P(|o1:n ), of which there are infinitely many. State of the art algorithms (Kikuti, Cozman, & de Campos, 2005; Utkin & Augustin, 2005) avoid this issue, but are still quadratic in the search space. This
makes them intractable for the present problem because our search space X1:n is exponential in the
length of the iHMM.
Besides the computational difficulties with the other approaches, there are a number of additional reasons why, in this paper, we focus on maximal state sequences rather than -maximin or
E-admissible ones. The first and most important reason is that we were able to develop an algorithm
that can determine them efficiently; see Sections 6 and 7. Secondly, and this is a common advantage
of maximality and E-admissibility: the higher the imprecision of the model, the more solutions are
returned. In contrast, even for high imprecision, in most cases, there will be only one -maximin
sequence (except if two or more sequences have the same highest conditional lower probability).
Our application in Section 9 clearly illustrates that emitting more than a single solution can indeed
be useful. Thirdly, in those cases where other decision criteria are preferred, maximal state sequences can still be of use because every -maximin and E-admissible state sequence is guaranteed
to be maximal as well (Troffaes, 2007). If our algorithm yields only a single maximal solution,
this is also the unique -maximin and E-admissible solution. If more than one maximal sequence
is returned, this can be regarded as preprocessing step. For example, once we know all maximal
solutions, finding the -maximin solutions amounts to comparing the posterior lower probabilities
of these maximal sequences only, instead of all sequences in X1:n .
4.4 Maximal Subsequences
We shall see below that in order to find the set of maximal estimates, it is useful to consider more
general sets of so-called maximal subsequences: for any k  {1, . . . , n} and xk1  Xk1 , we define
opt (Xk:n |xk1 , ok:n ):
xk:n  opt (Xk:n |xk1 , ok:n )  (xk:n  Xk:n ) Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 )  0.

(14)

The interpretation of these sets is immediate if we consider the part of the original iHMM that is
depicted in Figure 4, where we take Qk (|xk1 ) as the marginal model for the first state Xk . Then,
as explained in Section 3.3, the corresponding joint lower prevision on G (Xk:n  Ok:n ) is precisely
Pk (|xk1 ), and if we have a sequence of outputs ok:n , then opt (Xk:n |xk1 , ok:n ) is the set of state
sequence estimates that are undominated by any other estimate in Xk:n . It should be clear that the
set opt (X1:n |o1:n ) we are eventually looking for, can also be written as opt (X1:n |x0 , o1:n ).
4.5 Useful Recursion Equations
Fix any k in {1, . . . , n}. If we look at Equation (14), we see that it will be useful to derive a manageable expression for the lower prevision Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 ). This can be easily donesee
202

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

Qk (|xk1 )

Qr (|Xr1 )

Qn (|Xn1 )

State subsequence:

Xk

Xr

Xn

Output subsequence:

Ok

Or

On

Sk (|Xk )

Sr (|Xr )

Sn (|Xn )

Figure 4: Tree representation of a part of the original iHMM

Appendix Aby using Equations (3)(7) and a few algebraic manipulations. We consider three different cases. If xk = xk and k  {1, . . . , n  1} then, using the notation introduced in Section 3.3:
Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 )
= Qk ({xk }|xk1 )Sk ({ok }|xk ) fi Pk+1 (I{ok+1:n } [I{xk+1:n }  I{xk+1:n } ]|xk ). (15)
If xn = xn then
Pn (I{on } [I{xn }  I{xn } ]|xn1 ) = 0.

(16)

Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 ) = Qk (I{xk }  (xk:n )  I{xk } (xk:n )|xk1 ),

(17)

If xk 6= xk and k  {1, . . . , n} then

where we define, for any xk:n  Xk:n :
n

 (xk:n ) := E k (I{ok:n } I{xk+1:n } |xk ) = Sk ({ok }|xk )



Si ({oi }|xi )Qi ({xi }|xi1 )

(18)



Si ({oi }|xi )Qi ({xi }|xi1 ).

(19)

i=k+1
n

(xk:n ) := E k (I{ok:n } I{xk+1:n } |xk ) = Sk ({ok }|xk )

i=k+1

It is useful to realise that  (xk:n ) and (xk:n ) are just shorthand notations for the lower and upper
probabilities in Equations (8) and (9), for a fixed sequence of observations. For any given sequence
of states xk:n  Xk:n , the (xk:n ) and  (xk:n ) can be found by simple backward recursion:
(xk:n ) := (xk+1:n )Sk ({ok }|xk )Qk+1 ({xk+1 }|xk )
 (xk:n ) :=  (xk+1:n )Sk ({ok }|xk )Qk+1 ({xk+1 }|xk ),

(20)
(21)

for k  {1, . . . , n  1}, and starting from:
(xn:n ) = (xn ) := Sn ({on }|xn ) and  (xn:n ) =  (xn ) := Sn ({on }|xn ).
203

(22)

fiD E B OCK & D E C OOMAN

5. The Principle of Optimality
Determining the state sequences in opt (X1:n |o1:n ) directly using Equation (13) clearly has a complexity that is exponential in the length of the chain. We are now going to take a dynamic programming approach (Bellman, 1957) to reducing this complexity by deriving a recursion equation for the
sets of optimal (sub)sequences opt (Xk:n |xk1 , ok:n ).
Theorem 3 (Principle of Optimality). For k  {1, . . . , n  1}, all xk1  Xk1 and all xk:n  Xk:n :
if Qk ({xk }|xk1 ) > 0 and Sk ({ok }|xk ) > 0, then
xk:n  opt (Xk:n |xk1 , ok:n )  xk+1:n  opt (Xk+1:n |xk , ok+1:n ) .
As an immediate consequence, we find that
opt (Xk:n |xk1 , ok:n )  cand (Xk:n |xk1 , ok:n ) ,

(23)

where the set cand (Xk:n |xk1 , ok:n ) consists of all the sequences in Xk:n that can still be an element
of opt (Xk:n |xk1 , ok:n ) according to Theorem 3:
cand (Xk:n |xk1 , ok:n )

:=

[

 
xk  opt (Xk+1:n |xk , ok+1:n ) 

[


xk  Xk+1:n . (24)

xk Pos
/
k (xk1 )

xk Posk (xk1 )

Here  denotes concatenation of state sequences and the set of states Posk (xk1 )  Xk is defined as
xk  Posk (xk1 )  Qk ({xk }|xk1 ) > 0 and Sk ({ok }|xk ) > 0.

(25)

Equation (24) simplifies to
cand (Xk:n |xk1 , ok:n ) =

[

xk  opt (Xk+1:n |xk , ok+1:n )

(26)

xk Xk

if all local lower probabilities are positive, but this is not generally true in the more general case we
are considering here, where only the upper probabilities are required to be positive.

6. An Algorithm for Finding all Maximal State Sequences
We now use Equation (23) to devise an algorithm for constructing the set opt (X1:n |o1:n ) of maximal
state sequences in a recursive manner.
6.1 Initial Set-up Using Backward Recursion
We begin by defining a few auxiliary notions. First of all, we consider the following thresholds:
n
o
k (xk , xk |xk1 ) := min a  0 : Qk (I{xk }  aI{xk } |xk1 )  0
(27)
for all k  {1, . . . , n}, xk1  Xk1 and x1 , x1  X1 such that x1 6= x1 .
204

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

Next, we define
kmax (xk ) := max (zk:n ) and kmax (xk ) := max  (zk:n )
zk:n Xk:n
zk =xk

zk:n Xk:n
zk =xk

(28)

for all k  {1, . . . , n} and xk  Xk . Using Equations (20)(21), these can be calculated efficiently
using the following backward recursive (dynamic programming) procedure:
kmax (xk ) =

max
max k+1
(xk+1 )Sk ({ok }|xk )Qk+1 ({xk+1 }|xk )

xk+1 Xk+1

max
= Sk ({ok }|xk ) max k+1
(xk+1 )Qk+1 ({xk+1 }|xk ),
xk+1 Xk+1

(29)

and
kmax (xk ) =

max
max k+1
(xk+1 )Sk ({ok }|xk )Qk+1 ({xk+1 }|xk )

xk+1 Xk+1

max
= Sk ({ok }|xk ) max k+1
(xk+1 )Qk+1 ({xk+1 }|xk ),
xk+1 Xk+1

(30)

for k  {1, . . . , n  1}, starting from
nmax (xn ) = (xn ) = Sn ({on }|xn ) and nmax (xn ) =  (xn ) = Sn ({on }|xn ).

(31)

Finally, we let
opt

k (xk |xk1 ) := max kmax (xk )k (xk , xk |xk1 ),
xk Xk
xk 6=xk

(32)

for all k  {1, . . . , n}, xk1  Xk1 and xk  Xk .
6.2 A Recursive Solution Method
opt

It turns out that the k (xk |xk1 ), calculated by Equation (32), are extremely useful. As proved in
Appendix A, they allow us to significantly simplify Equation (14) as follows:
n
o
opt
opt (Xk:n |xk1 , ok:n ) = xk:n  cand (Xk:n |xk1 , ok:n ) : (xk:n )  k (xk |xk1 ) ,

(33)

which, for k = n, reduces to

	
opt (Xn |xn1 , on ) = xn  Xn : (xn )  nopt (xn |xn1 ) .

(34)

Since opt (X1:n |x0 , o1:n ) = opt (X1:n |o1:n ), this suggest the following algorithm for constructing the
set of all maximal state sequences.
205

fiD E B OCK & D E C OOMAN

Algorithm 1: ConstructMaximals
opt

Data: the local lower and upper probabilities and the parameters kmax and k
(calculated as in Section 6.1)
Result: the set of all maximal state sequences: opt (X1:n |o1:n )
1
2
3
4
5
6
7
8
9
10

for xn1  Xn1 do
opt (Xn |xn1 , on )  0/
for xn  Xn do
opt
if (xn )  n (xn |xn1 ) then add xn to opt (Xn |xn1 , on )
for k  n  1 to 1 do
for xk1  Xk1 do
opt (Xk:n |xk1 , ok:n )  0/
for xk:n  cand (Xk:n |xk1 , ok:n ) do
opt
if (xk:n )  k (xk |xk1 ) then add xk:n to opt (Xk:n |xk1 , ok:n )
return opt (X1:n |x0 , o1:n )

While Algorithm 1 is already much more efficient than a straightforward implementation of
Equation (13), there is still room for improvement. If Posk (xk1 ) 6= Xk , then by Equation (24), we
know that cand (Xk:n |xk1 , ok:n ) has a number of elements that is exponential in the length of the
considered sequences, making it very inefficient to execute the steps in Lines 8 and 9 of Algorithm 1.
In order to circumvent this problem, we propose a method that does not require an explicit check of
the inequality in Criterion (33) for all elements of cand (Xk:n |xk1 , ok:n ). The approach is identical
to that of Algorithm 1, except for Lines 8 and 9, which are replaced by Lines 8 and 9, as given in
Algorithm 2.
Algorithm 2: An efficient alternative to Lines 8 and 9 of Algorithm 1
...
...
...
for xk  Xk do
opt
if kmax (xk )  k (xk |xk1 ) then Recur(xk , k)

8
9

...

In order to be able to define the recursive procedure Recur that is used in Line 9 of Algorithm 2,
we need some additional notation. First of all, for all k  {1, . . . , n}, s  {k, . . . , n}, xk1  Xk1 ,
xk:s  Xk:s and ok:n  Ok:n , we define
candxk:s (Xk:n |xk1 , ok:n ) := {xk:n  cand (Xk:n |xk1 , ok:n ) : xk:s = xk:s } .

(35)
opt

Secondly, for all k  {1, . . . , n}, s  {k, . . . , n}, xk1  Xk1 and xk:s  Xk:s , we define k (xk:s |xk1 )
opt
opt
as follows. For s = k, we let k (xk:k |xk1 ) := k (xk |xk1 ), as given by Equation (32). For
206

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

opt

s  {k + 1, . . . , n}, k (xk:s |xk1 ) is then recursively defined by
opt

opt
k (xk:s |xk1 )

k (xk:s1 |xk1 )
=
.
Ss1 ({os1 }|xs1 )Qs ({xs }|xs1 )

(36)

Procedure Recur(xk:s , s)
1
2
3
4
5
6

if s = n then
add xk:n to opt (Xk:n |xk1 , ok:n )
else
for xs+1  Xs+1 do
if candxk:s xs+1 (Xk:n |xk1 , ok:n ) 6= 0/ then
opt
max (x
if s+1
s+1 )  k (xk:s  xs+1 |xk1 ) then Recur(xk:s  xs+1 , s + 1)

The following result establishes that Lines 8 and 9 of Algorithm 2 are indeed a valid alternative
for Lines 8 and 9 of Algorithm 1.
Theorem 4. The set opt (Xk:n |xk1 , ok:n ) that is obtained by executing Algorithm 2 is correct, in the
sense that it satisfies Equation (33).
As we will show in Section 7, Algorithm 2 is surprisingly efficient. One of the reasons for this
efficiency is that checking the if-conditions in Lines 5 and 6 of the Procedure Recur is really easy,
perhaps in contrast to what one might think at first sight. For the condition in Line 6, this is because
opt
opt
one can use Equation (36) to derive k (xk:s  xs+1 |xk1 ) from k (xk:s |xk1 ), the latter of which
opt
is either available from the previous call to the Procedure Recur or, if s = k, equal to k (xk |xk1 ),
which has already been calculated during the initial set-up phase (see Section 6.1). Before we can
explain why checking the condition in Line 5 is easy as well, we first need to introduce the data
structure that we use to store the sets opt (Xk:n |xk1 , ok:n ) of optimal sequences.
For k = n, opt (Xn |xn1 , on ) is simply a list of states xn  Xn . For k < n, we could also just store
the optimal sequences xk:n in opt (Xk:n |xk1 , ok:n ) as a simple list, but this would imply storing the
same information multiple times, since the initial part of some of those sequences will be identical.
Furthermore, it would make checking the condition in Line 5 of the Procedure Recur very elaborate.
We therefore choose to represent the set opt (Xk:n |xk1 , ok:n ) as a collection of trees. Each xk  Xk
that satisfies the inequality in Line 9 corresponds to a root of a tree. The paths of these trees
correspond to elements of opt (Xk:n |xk1 , ok:n ).
Example 2. We consider a simple binary HMM with, for all i  {1, . . . , n}, Xi = {0, 1}. Then for
k = n  7, we could for example find that
opt (Xk:n | 0, ok:n ) = {00001000, 00001010, 00001110, 00011110, 10001010, 10001110}.
That same set of optimal sequences can also be represented as a collection of trees, which is depicted
in Figure 5.

Representing opt (Xk:n |xk1 , ok:n ) as a collection of trees has two important advantages. The
first advantage is that such a collection of trees can be constructed step by step while running Algorithm 2. In Line 9 of that algorithm, with every call to the Procedure Recur, we add the current
207

fiD E B OCK & D E C OOMAN

0

0

0

1

0

0

1

1

1

0

0

0

0

1

1

1

1

0

1

0

0

0

1

0

1

0

1

1

0

Figure 5: Tree representation of opt (Xk:n | 0, ok:n ), for k = n  7
state xk as a root node of a new tree. With every subsequent recursive call to the Procedure Recur
(in Line 6 of that same procedure), we add a new child xs+1 to an already existing node xs , where xs
is the last state of the presently considered sequence xk:s .
In order for such a step by step construction to lead to a representation for opt (Xk:n |xk1 , ok:n ),
each path of the resulting set of trees must have length n  k + 1. In other words, it is necessary
that every node in this representation has at least one child, except for nodes that form the end of
a path that has length n  k + 1. Equivalently, and more technically, it is necessary that with every
execution of Line 4 of the Procedure Recur, at least one xs+1  Xs+1 satisfies both of the subsequent
if-conditions (in Lines 5 and 6). The following result establishes that this condition is always met.
Theorem 5. Fix k  {1, ..., n  1} and s  {k, ..., n  1} and consider any execution of the Procedure Recur(xk:s , s) while running Algorithm 2. Then there will be at least one xs+1  Xs+1
for which we obtain that both candxk:s xs+1 (Xk:n |xk1 , ok:n ) 6= 0/ [the if-condition in Line 5] and
opt
max (x
s+1
s+1 )  k (xk:s  xs+1 |xk1 ) [the if-condition in Line 6].
All that is now left to explain is how the if-condition in Line 5 of the Procedure Recur can
be checked efficiently. We consider two distinct cases: xk  Posk (xk1 ) and xk 
/ Posk (xk1 ). If
xk 
/ Posk (xk1 ), then by Equations (24) and (35), we find that
candxk:s xs+1 (Xk:n |xk1 , ok:n ) = xk:s  xs+1  Xs+2:n 6= 0,
/
which makes the if-condition in Line 5 trivially true. If xk  Posk (xk1 ), then again by Equations (24) and (35), candxk:s xs+1 (Xk:n |xk1 , ok:n ) 6= 0/ if and only if opt (Xk+1:n |xk , ok+1:n ) contains
a sequence that starts with xk+1:s  xs+1 . If we represent opt (Xk+1:n |xk , ok+1:n ) as a collection of
trees, this is equivalent to checking whether xs+1 is a child of the node that corresponds to the last
state in the sequence xk:s .
This brings us to the second advantage of representing sets of optimal sequences as a collection of trees: it makes checking the if-condition in Line 5 of the Procedure Recur both elegant
and efficient. If in Line 8 of Algorithm 2, xk 
/ Posk (xk1 ), then in all of the subsequent calls
to the Procedure Recur, Line 5 can simply be ignored. If xk  Posk (xk1 ), then in all the subsequent calls to the Procedure Recur, Lines 5 and 6 can be condensed into a single for-loop that
runs over the children of the node that corresponds to xs . Hence, for xk  Posk (xk1 ), executing
Line 8 of Algorithm 2including all the subsequent recursive calls to the Procedure Recurcan
208

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

be done efficiently by traversing depth-first through the trees that make up the representation of
opt (Xk+1:n |xk , ok+1:n ), copying them as we go, except for those nodes for which the condition in
Line 6 of the Procedure Recur is not satisfied.
Example 3. We continue with Example 2. This time, we let k = n  8, which implies that Figure 5
is now a representation for opt (Xk+1:n | 0, ok+1:n ). For the sake of this example, let us assume that
0  Posk (0) and 1 
/ Posk (0). Then we know from Equations (23) and (24) that every sequence
xk:n in opt (Xk:n | 0, ok:n ) is an element of either 0  opt (Xk+1:n | 0, ok+1:n ) or 1  Xk+1:n . Hence,
opt (Xk:n | 0, ok:n ) might for example have a representation that looks like the one depicted in Figure 6.
0

1

0

0

0

0

0

0

1

1

1

0

1

1

0

0

1

1

1

1

0

1

1

1

1

1

0
1

Figure 6: Tree representation of opt (Xk:n |0, ok:n ), for k = n  8
Figure 7 should clarify how Algorithm 2 constructs this representation. The two sequences in
opt (Xk:n | 0, ok:n ) that start with 0 correspond to the greengrey in monochrome versions of this
paperbranches in the topmost part of Figure 7. The first node of these two sequences was added
in Line 9 of Algorithm 2 (in this case, the if-condition in that line turned out to be true). The
other green nodes in the topmost part of Figure 7 were added during the subsequent calls to the
Procedure Recur. Checking the if-condition in Line 5 of those procedures was done by traversing
depth-first through the nodes of the representation of opt (Xk+1:n | 0, ok+1:n ). The redgrey with
a thicker outline in monochrome versions of this papernodes correspond to states for which the
if-condition in Line 6 was not satisfied. The (white) descendants of these red nodes were never
visited by the algorithm; we depict them only to allow for an easy comparison with Figure 5. The
tree representation for the three sequences in opt (Xk:n | 0, ok:n ) that start with 1 were constructed
in a similar manner, which is illustrated in the bottommost part of Figure 7. The main difference
is that for those sequences, since 1 
/ Posk (0), the if-condition in Line 5 of the Procedure Recur
is trivially true, implying that the algorithm does not need to traverse through a tree representation
of opt (Xk+1:n | 1, ok+1:n ), but rather trough the complete set Xk+1:n . Again, we stop whenever the
if-condition on Line 6 is not satisfied, as symbolised by the red nodes.

6.3 Additional Comment
It might happen that the available information consists of assessments for the lower and upper
transition and emission probabilities only:
Qk ({xk }|xk1 ), Qk ({xk }|xk1 ), Sk ({ok }|xk ) and Sk ({ok }|xk )
209

fiD E B OCK & D E C OOMAN

opt (Xk+1:n | 0, ok+1:n )

0

0

0

1

0

0

1

1

1

0

0

0

0

1

1

1

1

0

1

0

0

0

1

0

1

0

1

1

0

0

0

0

0

0

0

1
1

0

0

1

1

1

1

0

1

1

0

0

0

0

1

1

1

1

1

0
1

Figure 7: Clarification of the construction of opt (Xk:n |0, ok:n ), for k = n  8

for all k  {1, . . . , n}, xk1  Xk1 , xk  Xk and ok  Ok . In that case, one can use the following method to construct, for all k  {1, . . . , n}, xk1  Xk1 and xk , xk  Xk such that xk 6= xk , a
conservative value for the threshold k (xk , xk |xk1 ).
The most conservative coherent models Qk (|Xk1 ) that correspond to assessments of lower and
upper probabilities of singletons are 2-monotone (de Campos, Huete, & Moral, 1994). Due to their
comonotone additivity (De Cooman, Troffaes, & Miranda, 2008), this implies that:
Qk (I{xk }  aI{xk } |xk1 ) = Qk ({xk }|xk1 )  aQk ({xk }|xk1 )
for all a  0, and therefore Equation (27) leads to

k (xk , xk |xk1 ) =

Qk ({xk }|xk1 )
Qk ({xk }|xk1 )

.

The right-hand side is the smallest possible value of the threshold k (xk , xk |xk1 ) corresponding to
the assessments Qk ({xk }|xk1 ) and Qk ({xk }|xk1 ), leading to the most conservative inferences and
therefore the largest possible sets of maximal sequences that correspond to these assessments.
210

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

7. Discussion of the Algorithms Complexity
This section discusses the computational compexity of the different steps of the EstiHMM algorithm, as developed in the previous section. In the end, we find that the total complexity of the
EstiHMM algorithm is polynomial in the size of the inputquadratic in the length of the iHMM
and cubic in the number of states, as well as linear in the size of the outputthe number of maximal sequences in opt (X1:n |o1:n ). The linearity in the size of the output is especially interesting; we
discuss this in Section 7.4.
7.1 Preparatory Calculations
We begin with the preparatory calculations of the quantities in Equations (27)(32). For the thresholds k (xk , xk |zk1 ) in Equation (27), the computational complexity is clearly cubic in the number
of states, andexcept for stationary iHMMslinear in the length of the iHMM. Calculating the
kmax (xk ) and kmax (xk ) in Equations (29) and (30) is linear in the length of the iHMMeven for staopt
tionary iHMMsand quadratic in the number of states. The complexity of finding the k (xk |xk1 )
in Equation (32) is thereforein the worst, non-stationary caselinear in the lenght of the iHMM
and cubic in the number of states.
7.2 Algorithm 2
The computational complexity of Algorithm 2 is less trivial. Let us start by noting that this construction essentially consists in repeating the same small step over and over again, namely executing
the Procedure Recur. As we explained in the previous section, our data structurea collection of
treesenables us to do this very efficiently. Each of the three if-conditions in the Procedure Recur
can be checked in constant time. Therefore, taking into account the for-loop in Line 4, we find that
the computational complexity of a single execution of the Procedure Recur is linear in the number
of states.
Next, notice that every optimal sequence xk:n that is obtained by running Algorithm 2 is constructed by adding extra states xs+1 to an already constructed sequence xk:s , repeating this for s going
from k to n  1. Adding such a state means executing the Procedure Recur once, and is therefore
linear in the number of states. Similarly, creating the first state xk is at most linear in the number of
states as welldue to the for-loop in Line 8 of Algorithm 2. Hence, constructing a single optimal
sequence xk:n is linear in the length of this sequence, as well as linear in the number of states. By
Theorem 5, we also know that every execution of the Procedure Recur is guaranteed to be part
of the construction of at least one optimal sequence. Therefore, we find that constructing a single
set opt (Xk:n |xk1 , ok:n )executing Algorithm 2is linear in the number of optimal sequences it
consists of, linear in the length of those sequences and linear in the number of states.
7.3 Algorithm 1
What Algorithm 1 basically does to obtain the set opt (X1:n |o1:n ) is to construct all of the the sets
opt (Xk:n |xk1 , ok:n ), for every xk1  Xk1 , letting k run from n to 1. For k = n and a fixed
xn1  Xn1 , this is linear in the number of statessee Lines 3 and 4 of Algorithm 1. For k < n
and a fixed xk1  Xk1 , this comes down to executing Algorithm 2. As shown in the previous
section, Algorithm 2 is linear in the number of optimal sequences in opt (Xk:n |xk1 , ok:n ), linear in
the length of those sequences (n  k + 1) and linear in the number of states. Hence, we conclude
211

fiD E B OCK & D E C OOMAN

that complexity of Algorithm 1 is quadratic in the length of the iHMM, quadratic in the number of
states and roughly speaking6 linear in the number of maximal sequences.
7.4 The Total Complexity
The complete EstiHMM algorithm consists of the preparatory calculations in Section 6.1 and a single execution of Algorithm 1, where, in the latter, Lines 8 and 9 are replaced by their more efficient
versions in Algorithm 2. We conclude from the previous sections that the total computational complexity of all of this isat worstquadratic in the length of the iHMM, cubic in the number of
states, and roughly speaking linear in the number of maximal sequences.
This linearity in the number of maximal sequences is clearly the remaining bottleneck of the
algorithm, since there may be exponentially many such sequences. However, this should not lead
the reader to conclude that the EstiHMM algorithm has exponential complexity, meaning that it is
exponential in the size of the inputthe length of the iHMM and the number of states. It is crucial to
realise that the complexity is linear in the size of the outputthe number of maximal sequences,
which in turn may be exponential in the input. However, as long as the size of the output is bounded,
the algorithm is guaranteed to have a computational complexity that is polynomial in the size of the
input. No such guarantee can be given for algorithms whose complexity is linear in the size of the
inputfor example a naive implementation of Algorithm 1 that does not replace Lines 8 and 9 by
their more efficient counterparts in Algorithm 2.
Although linearity in the size of the output might seem rather bad, it is in fact all we can hope for.
Even simply printing the outputall maximal sequencesalready has a computational complexity
that is linear in its size as well as linear in the length of the iHMM. Linearity in the size of the output
is inherent to all problems that do not necessarily lead to a single solution, but allow for set-valued
solutions as well. If the size of the output is too large, then no algorithm, however cleverly designed,
can overcome this hurdle.
In order for the EstiHMM algorithm not to choke when the number of maximal sequences is
very large, one can keep trackfor every set opt (Xk:n |xk1 , ok:n )of how many times Line 2 of the
Procedure Recur has been executed so far, aborting the algorithm whenever some preset treshold
has been exceeded. It is however not possible to return only the k best solutions, simply because
there is no such thing as a better or worse maximal sequence; they are all incomparable. The only
way in which the number of maximal sequences can be reduced is by decreasing the imprecision of
the model: to gather extra data or expert knowledge, leading to smaller local credal sets, pointwise
larger local lower previsions and therefore fewer maximal sequences. Alternatively, one could also
consider using E-admissable sequencesof which there may be multiple as well, but not as many as
maximal onesor -maximin sequencesof which, in most instances, there is only one. However,
we know of no algorithm that can calculate the E-admissable or -maximin sequences in an efficient
manner, let alone one that is linear in the output; see Section 4.3.
7.5 Comparison with Viterbis Algorithm
For precise HMMs, the state sequence estimation problem can be solved very efficiently by the
Viterbi algorithm (Rabiner, 1989; Viterbi, 1967), whose complexity is linear in the length of the
HMM, and quadratic in the number of states. However, this algorithm only emits a single optimal
6. For every k and xk1  Xk1 , constructing the set opt (Xk:n |xk1 , ok:n ) has linear complexity in the number of optimal
sequences at that stage.

212

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

most probablestate sequence, even in cases where there are multipleequally probableoptimal
solutions: this of course simplifies the problem. If we would content ourselves with giving only a
single maximal solution, the ensuing version of our algorithm would have a complexity that is
similar to Viterbis.
So, to allow for a fair comparison between Viterbis algorithm and ours, we would need to alter
Viterbis algorithm in such a way that it no longer resolves ties arbitrarily, and emits allequally
probableoptimal state sequences. This new version will remain linear in the length of the HMM,
and quadratic in the number of states, but will also have added complexity. As discussed in the
previous section, even printing all optimal sequences is linear in the number of them and therefore
possibly exponential, for example if all possible solutions are equally probableimagine a precise
HMM of which all local probability mass functions are uniform.
For the complexity of the most time-consuming part of our algorithmAlgorithm 1, the only
difference is this: Viterbis approach is linear and ours is quadratic in the length of the HMM. Where
does this difference come from? In imprecise HMMs we have mutually incomparable solutions,
whereas in precise HMMs the optimal solutions are indifferent, or equally probable. This makes
sure that the algorithm for precise HMMs requires no forward loops, as is the case in the EstiHMM
algorithm, every time we run Algorithm 2. We believe that this added complexity is a reasonable
price to pay for the robustness that working with imprecise-probabilistic models offers.

8. Some Experiments
Since the complexity of the EstiHMM algorithm depends so crucially on the number of maximal
sequences it emits, the present section will study this number in more detail. We do so by taking
a closer look at how it depends on the transition probabilities of the model, and how it evolves
when we let the imprecision of the local models grow. We shall see that the number of maximal
sequences displays very interesting behaviour that can be explained, and even predicted to some
extent. To allow for easy visualisation, we limit this discussion to stationary binary iHMMs, where
both the state and output variables can assume only two possible values, say 0 and 1.
8.1 Describing a Stationary Binary iHMM
The precise transition probabilities for going from one state to the next are completely determined
by numbers in the unit interval: the probability p to go from state 0 to state 0, and the probability
q to go from state 1 to state 0. To further pin down the HMM we also need to specify the marginal
probability m for the first state to be 0, and the two emission probabilities: the probability r of
emitting output 0 from state 0 and the probability s of emitting output 0 from state 1.
In this binary case, all coherent imprecise-probabilistic models can be found by contamination:
taking convex mixtures of precise models, with mixture coefficient 1  , and the vacuous model,
with mixture coefficient , leading to a so-called linear-vacuous model (Walley, 1991), often referred
to as an -contaminated model as well. To simplify the analysis, we let the emission model remain
precise, and use the same mixture coefficient  for the marginal and the transition models. As 
ranges from zero to one, we then evolve from a precise HMM towards an iHMM with vacuous
marginal and transition models (and precise emission models).
213

fiD E B OCK & D E C OOMAN

8.2 An iHMM of Length Two
We now examine the behaviour of an iHMM of length two, with the following precise probabilities
fixed:
m = 0.1, r = 0.8 and s = 0.3.
Fixing an output sequence and a value for , we can use our algorithm to calculate the corresponding
numbers of maximal state sequences as p and q range over the unit interval. The results can be
represented conveniently in the form of a heat plot. The plots in Figure 8 correspond to the output
sequence o1:2 = 01.
1

1

q

q

 = 2%
0

0

p

 = 5%
1

1

0

0

1

1

q

q

 = 10%
0

p

0

p

 = 15%
1

0

0

p

1

Figure 8: Heat plots for o1:2 = 01
The number of maximal state sequences clearly depends on the transition probabilities p and
q. In the rather large parts of probability space that are coloured white, we get a single maximal
sequenceas we would for HMMs, but there are continuous regions where we see a higher number appear. In the present examplea binary chain of length two, the highest possible number
of maximal sequences is of course four. In the dark grey area, there are three maximal sequences,
and two in the light grey regions. The plots show what happens when we let  increase: the grey
areas expand and the number of maximal sequences increases. For  = 15%, we even find a small
areacoloured blackwhere all four possible state sequences are maximal: locally, due to the relatively high imprecision of our local models, we cannot provide any useful robust estimate for the
state sequence producing the output sequence o1:2 = 01.
214

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

For small , the areas with more than one maximal state sequence are quite small and seem to
resemble strips that narrow down to lines as  tends to zero. This suggests that we should be able to
explain at least qualitatively where these areas come from by looking at compatible precise models:
the regions where an iHMM produces different maximal (mutually incomparable) sequences, are
widened versions of loci of indifference for precise HMMs.
By a locus of indifference, we mean the set of (p, q) that correspond to two given state sequences
x1:2 and x1:2 having equal posterior probability:
p(x1:2 |o1:2 ) = p(x1:2 |o1:2 ),
or, provided that p(o1:2 ) > 0,
p(x1:2 , o1:2 ) = p(x1:2 , o1:2 ).
In our example, where o1:2 = 01, we find the following expressions for each of the four possible
state sequences:
p(00, 01) = mr(1  r)p;

p(10, 01) = (1  m)s(1  r)q;

p(01, 01) = mr(1  s)(1  p);

p(11, 01) = (1  m)s(1  s)(1  q).

1
00  11

01

10
01 

11

10  11


10

00  01

01

q

11

00 

0

0

10

p

1

Figure 9: Loci of indifference for o1:2 = 01
By equating any two of these expressions, we express that the corresponding two state sequences
have an equal posterior probability. Since the resulting equations are a function of p and q only, each
of these six possible combinations defines a locus of indifference. All of them are depicted as lines
in Figure 9.
215

fiD E B OCK & D E C OOMAN

111000001101000010001000011111111110111101000010110110110000 . . .
111000001101000010001000011111111110111101000010110110110000
110000001101000010001000011111111110111101000010110110110000
111000000101000010001000011111111110111101000010110110110000
111000001100000010001000011111111110111101000010110110110000
111000001101000000001000011111111110111101000010110110110000

...
...
...
...
...

Figure 10: Maximal sequences for an iHMM of length 100
Parts of these loci, depicted in bluedarker and bolder in monochrome versions of this paper,
demarcate the three regions where the state sequences 01, 10 and 11 are optimalhave the highest
posterior probability.
What happens when the transition models become imprecise? Roughly speaking, nearby values
of the original p and q enter the picture, effectively turning the locilinesof indifference into
bands of incomparability: the emergence of regions with two and more maximal sequences can be
seen to originate from the loci of indifference; compare Figure 9 with Figure 8.
8.3 An iHMM of Length 100
In order to demonstrate that our algorithm is indeed efficient, we let it determine the maximal
sequences for a random output sequence of length 100.
We consider the same stationary binary HMM as before, but with the following precise marginal
and emission probabilities:
m = 0.1, r = 0.98, and s = 0.01.
In practical applications, the probability for an output variable to have the same value as the corresponding hidden state variable is usually quite high, which explains why we have chosen r and s
to be close to 1 and to 0, respectively. In contrast with the previous experiments, we do not let the
transition probabilities vary, but fix them to the following values:
p = 0.6 and q = 0.5.
The local models of the iHMM that we use to determine the maximal sequences are now generated by -contaminating these precise local models. We use the same mixture coefficient  for
the marginal, transition and emission models. In Figure 10, we show the five maximal sequences
that correspond to the highlighted output sequence, with  = 2%. Due to space constraints, we
display only the first 60 digits of these sequences. Since the emission probabilities were chosen to
be quite accurate, it is no surprise that the output sequence itself is one of the maximal sequences.
In addition, we have indicated in bold face the state values that differ from the outputs in the output
sequence; in the 40 digits that are not displayed, no such differences occured. We see that the model
represents more indecision about the values of the state variables as we move further away from the
end of the sequence. This is a result of a phenomenon called dilation, whichas has been noted in
another paper (De Cooman et al., 2010)tends to occur when inferences in a credal tree proceed
from the leaves towards the root.
As for the efficiency of our algorithm: it took about 0.2 seconds to calculate these 5 maximal
sequences.7 The reason why this could be done so fast is that the algorithm is more or less linear
7. Running a Python program on a 2012 MacBook Pro.

216

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

in the number of solutions (see Section 7), which in this case is only 5. If we let  grow to for
example 5%, the number of maximal sequences for the same output sequence is 764 and these
can be determined in about 32 seconds. This demonstrates that the complexity is indeed more or
less proportional toand therefore linear inthe number of solutions and that the algorithm can
efficiently calculate the set of maximal sequences, even for long output sequences. For larger values
of , say 10%, it took more than 30 minutes to determine all maximal sequences, leading us to abort
the algorithm. This should not lead the reader to conclude that for large , the EstiHMM algorithm
is no longer linear in the number of maximal sequences. No, it simply means thatat least for long
iHMMsthis number of maximal sequences can increase quickly as soon as  passes some critical
boundary.

9. An Application in Optical Character Recognition
As a first application, we use the EstiHMM algorithm to detect and correct mistakes in words. The
hidden sequence x1:n corresponds to the original, correct version of a word, of which the output
sequence o1:n is an artificially corrupted version. In this way, we simulate observational processes
that are not perfectly reliable, such as the output of an Optical Character Recognition (OCR) device.
This leads to observed output sequences that may contain errors, which we will try to detect and
correct. The original words were taken from Dantes Divina Commedia, of which the 1018 words
of the second canto were used as a training set and the initial 200 words of the first canto as a test
set. By comparing the results of the EstiHMM algorithm with those of the Viterbi algorithm, we are
able to illustrate some of the advantages of the former.
9.1 Learning the Local Models
In order to apply our algorithm, we must identify a local uncertainty model for each original and
observed letter: a marginal model Q1 for the first letter X1 of the original word, a transition model
Qk (|Xk1 ) for the subsequent letters Xk , with k  {2, . . . , n}, and an emission model Sk (|Xk ) for the
observed letters Ok , with k  {1, . . . , n}. We use the same state space X = O for all these variables,
consisting of the 21 letters of the Italian alphabet. For the sake of simplicity, we assume stationarity,
making the transition and emission models independent of k.
For the identification of the local models of the iHMM, we use the imprecise Dirichlet model
(IDM) (Walley, 1996). This corresponds to considering the set of all Dirichlet priors with some fixed
strength s > 0, using the lower and upper bounds of the inferences obtained by each of these priors
as our model. For example, for the marginal model Q1 , applying the IDM leads to the following
lower and upper probabilities:
Q1 ({x}) =

nx
s + nx
and Q1 ({x}) =
for all x  X,
s + zX nz
s + zX nz

where, for all z  X, nz is the number of words in the training text for which the first letter X1 is
equal to z. The hyperparameter s can be regarded as a degree of caution that is taken into account
in the inferences. We use s = 2; Walley (1996, Section 2.5) provides a number of arguments in
favour of this choice. For the transition and emission models, we can proceed similarly, by counting
the transitions of one letter to another, respectively in the original word or during the observation
process. In this way, we obtain lower and upper transition and emission probabilities for singletons,
which, as pointed out in Section 6.3, suffice to run the algorithm. In fact, since the IDM leads
217

fiD E B OCK & D E C OOMAN

to local models that are linear-vacuous and hence completely and therefore also 2-monotone, the
approach described in Section 6.3 actually leads to exact values for the parameters k (xk , xk |xk1 )
instead of a conservative approximation.
For the identification of the local models of the precise HMM, we use a similar but now precise
Dirichlet model approach, with a Perkss prior that has the same prior strength s = 2. As an example,
for the precise marginal model Q1 , this leads to the following simple identification:
Q1 ({x}) =

s/|X | + nx

s + zX nz

,

where |X | is the number of states.
The difference between the precise and imprecise models that are constructed in the way described above is relatively small. For example, using our training set of 1018 words, 67 of which
start with the letter A, we obtained the following (lower, upper and precise) probability that the first
letter of a word is A:
Q1 ({A}) = 0.06569, Q1 ({A}) = 0.06578 and Q1 ({A}) = 0.06765.
Nevertheless, as illustrated in the next section, the imprecise model can lead to results that are rather
diferent from those obtained by the precise model.
9.2 Results
Let us first discuss an example of the difference between the results obtained by the Viterbi and the
EstiHMM algorithm, in order to illustrate an important advantage of the latter. OCR software has
mistakenly read the Italian word QUANTO as OUANTO. Using a precise model, the Viterbi algorithm does not correct this mistake, as it suggests that the original correct word is DUANTO. The
EstiHMM algorithm on the other hand, using an imprecise model, returns CUANTO, DUANTO,
FUANTO and QUANTO as maximal, undominated solutions, including the correct one. Of course
we would still have to pick the correct solution out of this set of suggestionsfor example by using a dictionary or a human opinion, but by using the EstiHMM algorithm, we have managed to
reduce the search space from all possible five letter words to the much smaller set of four words
given above. Notice that the solution of the Viterbi algorithm is included in the maximal solutions
EstiHMM returns. One can easily prove that this will always be the case.
We applied our method to the first 200 words of the first canto of Dantes Divina Commedia,
137 of which where correctly read by our artifical OCR device and 63 of which contained errors.
We tried to correct these errors using both the EstiHMM and the Viterbi algorithm, and compare
both approaches. The results are summarised in Table 1.
For the Viterbi algorithm, the main conclusion is that applying it to the output of the OCR device
results in a decreased number of incorrect words. The number of correct words rises from 68.5% to
78.5%. However, the Viterbi algorithm also introduces new errors for 5 correctly read words.
The EstiHMM algorithm manages to suggest the original correct word as one of her solutions
in 86% of the cases. Assuming we are able to detect this correct word, the percentage of correct
words rises from 68.5% to 86% by applying the EstiHMM algorithm, thereby outperforming the
Viterbi algorithm by almost 10%. Secondly, we also notice that the EstiHMM algorithm has never
introduced new errors in words that were already correct.
218

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

total number
Viterbi
correct solution
wrong solution
EstiHMM
correct solution included
correct solution not included

total number
200 (100%)

correct after OCR
137 (68.5%)

wrong after OCR
63 (31.5%)

157 (78.5%)
43 (21.5%)

132
5

25
38

172 (86%)
28 (14%)

137
0

35
28

Table 1: Summary of the results of the EstiHMM and Viterbi algorithm

Of course, since the EstiHMM algorithm allows for multiple solutions, instead of a single one,
it is no surprise that we manage to increase the amount of times we suggest the correct solution.
This would happen even if we added random extra solutions to the solution of the Viterbi algorithm.
Giving extra solutions can only be seen as an improvement if this is done smartly. To investigate
this, we distinguish between the cases where the EstiHMM algorithm returns a single solution, and
those where it returns multiple solutions; and look at how the Viterbi and EstiHMM algorithms
compare in those two cases.
The EstiHMM algorithm returned a single solution for 155 of the 200 words. As we have
already mentioned above, this single solution will always coincide with the one given by the Viterbi
algorithm. The results for the EstiHMM and Viterbi algorithms are summarised in Table 2.
EstiHMM (single solutions)
total number
single correct solution
single wrong solution

total number
155 (100%)
134 (86.5%)
21 (13.5%)

correct after OCR
129 (83.2%)
129
0

wrong after OCR
26 (16.8%)
5
21

Table 2: The instances where EstiHMM produces a single estimate
The percentage of words correctly read by the OCR software is now 83.2% instead of the global
68.5%. When the result of the EstiHMM algorithm is a single solution, this serves as an indication
that the word we are trying to correct has a fairly high probability of already being correct. We also
see that the eventual percentage of correct words is 86.5%, which is only a slight improvement over
the 83.2% that were already correct before applying the algorithms.
Next, we look at the remaining 45 words, for which the EstiHMM algorithm returns more than
one maximal element. In this case, we do see a significant difference between the results of the
Viterbi and the EstiHMM algorithm because the Viterbi algorithm always returns only a single
solution. The results for both algorithms are listed in Table 3.
A first and very important conclusion to be drawn from this table is that if the EstiHMM algorithm is indecisive, this serves as a rather strong indication that the word we are applying the
algorithm to does indeed contain errors: when the EstiHMM algorithm returns multiple solutions,
the original word has been incorrectly read by the OCR software in 82.2% of cases.
219

fiD E B OCK & D E C OOMAN

total number
EstiHMM (multiple solutions)
correct solution included
correct solution not included
Viterbi
correct solution
wrong solution

total number
45 (100%)

correct after OCR
8 (17.8%)

wrong after OCR
37 (82.2%)

38 (84.4%)
7 (15.6%)

8
0

30
7

23 (51.1%)
22 (48.9%)

3
5

20
17

Table 3: The instances where EstiHMM produces a set-valued estimate

A second conclusion, related to the first, is that if the EstiHMM algorithm is indecisive, this
also serves as an indication that the result returned by the Viterbi algorithm is less reliable: the
percentage of correct words after applying the Viterbi algorithm has dropped to 51.1%, in contrast
with the global percentage of 78.5%. The EstiHMM algorithm, however, still gives the correct
word as one of its solutions in 84.4% of cases, which is almost as high as its global percentage of
86%. If the set given by the EstiHMM algorithm contains the correct solution, the Viterbi algorithm
manages to pick this correct solution out of the set in 60.5% of cases. We see that the EstiHMM
algorithm seems to notice that we are dealing with more difficult words and therefore gives us
multiple solutions, between which it cannot decide.
9.3 Advantages of the Imprecise Approach
We learn from our experiments that the EstiHMM algorithm can be usefully applied to make the
results of the Viterbi algorithm more robust, and to gain an appreciation of where it is likely to go
wrong. If the EstiHMM algorithm is indeterminate, this serves as an indication of robustness issues
that would occur if we solved the same problem with the Viterbi algorithm. In those instances,
the EstiHMM algorithm returns multiple solutions, between which it cannot decide, whereas the
Viterbi algorithm will pick one out of this set in a fairly arbitrary waydepending on the choice of
the prior, thereby increasing the amount of errors made.
This leads us to conclude that the imprecise approach of the EstiHMM algorithm has two main
advantages. The first advantage is that it can easily detect when the precise approach becomes
sensitive to the adopted prior: this kind of sensitivity occurs exactly in those instances where the
EstiHMM algorithm returns an indeterminate result. The second advantage is that, instead of simply
detecting this sensitivity to the choice of prior, the EstiHMM algorithm also offers an alternative
solution that does not suffer from such issues, in the form of a set of maximal sequencesa set
of suggestions for the correct hidden word. As illustrated by our experiments, this set will often
contain the actual correct word.
Future work could try to exploit these set-valued solutions by trying to pick the correct word
out of the given set of options in some non-arbitrary way. This could for example be done by
comparing the options with the entries of a dictionary. Alternatively, one could consider asking the
user for feedback, asking him to choose among the options. In this way, additional data is gathered
that can be used to build a better model that is less sensitive to the choice of the prior.
220

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

10. Conclusions
Interpreting the graphical structure of an imprecise hidden Markov model as a credal network under
epistemic irrelevance leads to an efficient algorithm for finding the maximal, undominated hidden
state sequences for a given observed sequence. An interesting feature of this algorithm is that it has
a computational complexity that is linear in the size of the outputthe number of maximal state
sequences. Preliminary simulations show that, even for long iHMMs of which the transition models
have non-negligible imprecision, this number of maximal state sequences is often reasonably low.
It remains to be seen whether this observation can be corroborated by a deeper theoretical analysis.
Our application in OCR clearly shows that the EstiHMM algorithm is able to robustify the
results of the Viterbi algorithm. Not only does it reduce the amount of wrong conclusions by
providing extra possible solutions, it does so in an intelligent manner. It adds extra solutions in the
specific cases where the Viterbi algorithm is likely to be wrong, thereby also serving as an indicator
of the reliability of the result given by the Viterbi algorithm. Since these set-valued solutions often
contain the correct hidden state sequence, they can be usefully applied in a postprocessing phase,
for example by offering the set to the user, asking him for feedback.
A first important avenue of future research would be to compare the EstiHMM algorithm with
other methods that also try to robustify the Viterbi algorithm by producing set-valued solutions. We
distinguish between two different approaches.
On the one hand, we have imprecise methods such as the one adopted by us. They combine an
imprecise model with an imprecise-probabilistic decision criterion. In this paper, we have chosen to
use maximality as a decision criterion. However, other decision criteria can be adopted as well; see
Section 4.3. Some of these other criteria, such as E-admissibility, also lead to set-valued estimates.
A common feature of all of these methods is that they take into account model uncertainty: what
happens with inferences when the model is imperfect? What happens if instead of a single probability mass function, there are a set of possible candidates? In many instances, the resulting inferences
will still be determinate. Set-valued solutions are typically obtained only for those instances where
the precise-probabilistic approach is more likely to be wrong.
On the other hand, precise models may lead to set-valued solutions as well. In the context of
HMMs, the most important example seems to be the k-best Viterbi algorithm (Brown & Golod,
2010). Instead of returning only the a posteriori most probable hidden state sequence, the k-best
Viterbi algorithm returns the k most probable hidden state sequences. There are two important
differences with the imprecise approaches described above. First of all, the k-best approach has
nothing to do with model uncertainty. Instead, it deals with the probabilistic uncertainty that is
inherent to the model itself, while assuming that this model is perfectly known. If the model is
indeed correct, then by returning the k most probable sequences, the probability of the correct
estimate to be included in this set-valued solution increases, at the expense of losing determinacy.
Secondly, and related to the previous difference, the k-best method will always return k sequences,
regardless of the accuracy of the 1-best approach. In contrast, imprecise approaches are typically
able to distinguish between easy and hard cases, producing determinate answers for the former and
set-valued answers for the latter. Nevertheless, despite these differences, one gets the impression
that the k-best method can be used to tackle similar applications as the EstiHMM algorithm. It would
be interesting to check whether this is indeed the case, and to compare their respective results. We
leave this as a topic for future research.
221

fiD E B OCK & D E C OOMAN

Another, more theoretical avenue of future research is to investigate the extent to which the
ideas presented in this paper can be applied to credal networks other than iHMMs under epistemic
irrelevance. There are two specific instances where we have concrete ideas on how to proceed. First
of all, we have strong reasons to believe that it is possible to derive a similarly efficient algorithm
for iHMMs whose graphical structure is interpreted as a credal network under strong independence
rather then epistemic irrelevance. This could be interesting and relevant, as this more stringent
independence condition leads to joint models that are less imprecise, and therefore produce fewer
maximal state sequencesalthough they will be included in our solutions. Secondly, the EstiHMM
algorithm demonstrates that efficient inference in credal trees under epistemic irrelevance is not
necessarily limited to queries with a single target node only. In fact, we believe that it is possible
to develop polyonomial time algorithms, capable of solving wide classes of inference problems in
credal trees under epistemic irrelevance, thereby extending the results of De Cooman et al. (2010).

Acknowledgments
Jasper De Bock is a Ph.D. Fellow of the Research Foundation - Flanders (FWO) at Ghent University, and has developed the algorithm described here in the context of his Masters thesis, in
close cooperation with Gert de Cooman, who acted as his thesis supervisor. The present article
describes the main results of this Masters thesis. Research by De Cooman has been supported by
SBO project 060043 of the IWT-Vlaanderen.
The authors would like to thank the anonimous referees of this paper and a previous conference version for their useful, constructive comments. They led to a significant improvement of the
current version, most notably regarding its presentation. This paper has also benefitted from discussions with Marco Zaffalon, Alessandro Antonucci, Alessio Benavoli, Cassio de Campos, Erik
Quaeghebeur and Filip Hermans. We are grateful to Marco Zaffalon for providing travel funds,
which allowed us to visit IDSIA and discuss practical applications.

Appendix A. Proofs of Main Results
In this appendix, we justify the formulas (6), (7), (15), (16), (17), (33) and (34) and we give proofs
for Proposition 1 and Theorems 25. We will frequently use terms such as positive, negative,
decreasing and increasing. We therefore start by clarifying what we mean by them. For x  R, we
say that x is positive if x > 0, negative if x < 0, non-negative if x  0 and non-positive if x  0. We
call a real-valued function f defined on R:
(i) increasing if (x, y  R)(x > y  f (x) > f (y));
(ii) decreasing if (x, y  R)(x > y  f (x) < f (y));
(iii) non-decreasing if (x, y  R)(x > y  f (x)  f (y));
(iv) non-increasing if (x, y  R)(x > y  f (x)  f (y)).
222

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

Proof of Equation (6). For all k  {1, . . . , n}, xk1  Xk1 , xk:n  Xk:n and ok:n  Ok:n we infer from
Equation (5) that
Pk (I{xk:n } I{ok:n } |xk1 ) = Qk (E k (I{xk:n } I{ok:n } |Xk )|xk1 )


fi
fi
= Qk  I{zk } E k (I{xk } (zk )I{xk+1:n } I{ok:n } |zk )fixk1
zk Xk

= Qk (I{xk } E k (I{xk+1:n } I{ok:n } |xk )|xk1 ).
Since E k (I{xk+1:n } I{ok:n } |xk )  0 by C1, we see that C2 transforms the above into
= Qk (I{xk } |xk1 )E k (I{xk+1:n } I{ok:n } |xk ),
which can be reformulated as
= Qk (I{xk } |xk1 )Sk (I{ok } |xk )Pk+1 (I{xk+1:n } I{ok+1:n } |xk )
= Qk ({xk }|xk1 )Sk ({ok }|xk )Pk+1 (I{xk+1:n } I{ok+1:n } |xk ),
if we take into account Equation (4), since Pk+1 (I{xk+1:n } I{ok+1:n } |xk )  0 by C1.
Repeating these steps again and again eventually yields Equation (6):
n

Pk (I{xk:n } I{ok:n } |xk1 ) =  Qi ({xi }|xi1 )Si ({oi }|xi ).
i=k

In the last step, for k = n, we have used the equality E n ({on }|xn ) = Sn ({on }|xn ), which follows from
Equation (3).
Proof of Equation (7). For all k  {1, . . . , n}, xk1  Xk1 , xk:n  Xk:n and ok:n  Ok:n we infer from
conjugacy and Equation (5) that
Pk (I{xk:n } I{ok:n } |xk1 ) = Pk (I{xk:n } I{ok:n } |xk1 )
= Qk (E k (I{xk:n } I{ok:n } |Xk )|xk1 )


fi
fi
= Qk  I{zk } E k (I{xk } (zk )I{xk+1:n } I{ok:n } |zk )fixk1
zk Xk

= Qk (I{xk } E k (I{xk+1:n } I{ok:n } |xk )|xk1 )
= Qk (I{xk } (E k (I{xk+1:n } I{ok:n } |xk )))|xk1 ).
Since E k (I{xk+1:n } I{ok:n } |xk ) = E k (I{xk+1:n } I{ok:n } |xk )  0 by conjugacy and Lemma 6, we see that
C2 and Equation (2) transform the above into

=   E k (I{xk+1:n } I{ok:n } |xk ) Qk (I{xk } |xk1 )
= Qk (I{xk } |xk1 )E k (I{xk+1:n } I{ok:n } |xk ),
which can be reformulated as
= Qk (I{xk } |xk1 )Sk (I{ok } |xk )Pk+1 (I{xk+1:n } I{ok+1:n } |xk )
= Qk (I{xk } |xk1 )Sk (I{ok } |xk )Pk+1 (I{xk+1:n } I{ok+1:n } |xk )
= Qk ({xk }|xk1 )Sk ({ok }|xk )Pk+1 (I{xk+1:n } I{ok+1:n } |xk ),
223

fiD E B OCK & D E C OOMAN

using conjugacy and Equation (4), since Pk+1 (I{xk+1:n } I{ok+1:n } |xk )  0. This last inequality is true
because we know that Pk+1 (I{xk+1:n } I{ok+1:n } |xk ) = Pk+1 (I{xk+1:n } I{ok+1:n } |xk ) by conjugacy and
that Pk+1 (I{xk+1:n } I{ok+1:n } |xk )  0 by Lemma 6.
Repeating the steps above again and again, eventually yields Equation (7):
n

Pk (I{xk:n } I{ok:n } |xk1 ) =  Qi ({xi }|xi1 )Si ({oi }|xi ).
i=k

In the last step, for k = n, we have used the equality E n ({on }|xn ) = Sn ({on }|xn ), which follows from
Equation (3) and conjugacy.
Lemma 6. Consider a coherent lower prevision P on G (X). Then, for all f  G (X), we have that
min f  P( f )  P( f )  max f and, for all   R, that P( f ) = P() = .
Proof. We prove the inequalities in min f  P( f )  P( f )  max f one by one. The first one is
the same as C1. It follows by C3 that P( f  f )  P( f ) + P( f ) and therefore, since we know by
C2 that P(0) = 0P(0) = 0, this implies that P( f )  P( f ) = P( f ), using conjugacy for the last
equality. For the gamble  f , C1 yields that min  f  P( f ) which in turn implies that max f =
 min  f  P( f ) = P( f ).
To conclude, P( f ) = P() =  follows by applying these inequalities for f = .
Proof of Proposition 1. Observe that

Pk (I{ok:n } |xk1 ) = Pk I{ok:n }



zk:n Xk:n




fi
fi
fi
fi
I{zk:n } fixk1  Pk I{ok:n } I{zk:n } fixk1 > 0,

where zk:n is any element of Xk:n . The equality follows from zk:n Xk:n I{zk:n } = 1, the first inequality
from Lemma 8(ii), and the second one from the positivity assumption (10) and Equation (7).
In the same way, we can easily prove that


fi 
fi 
fi
fi

E k ({ok:n }|xk ) = E k I{ok:n }
 I{zk+1:n } fixk  E k I{ok:n } I{zk+1:n } fixk > 0.
zk+1:n Xk+1:n

This time, we have used the positivity assumption (10) and Equation (9) for the last inequality.
Proof of Theorem 2. Consider the real-valued function , defined by
() := P(I{o1:n } [I{x1:n }  I{x1:n }  ]) for all   R.
It follows from Equation (11) that P(I{x1:n }  I{x1:n } |o1:n ) is s rightmost zero, and we also know
that (0) = P(I{o1:n } [I{x1:n }  I{x1:n } ]). Furthermore,  is non-increasing and continuous by Lemma
7(i), and has at least one zero by Lemma 7(ii). Hence, if (0) > 0, then  has at least one positive
zero and P(I{x1:n }  I{x1:n } |o1:n ) > 0. If (0) < 0, then  has only negative zeroes and we then find
that P(I{x1:n }  I{x1:n } |o1:n ) < 0. Hence, proving the theorem comes down to proving that (0) = 0
implies that () < 0 for all  > 0, since this in turn implies that P(I{x1:n }  I{x1:n } |o1:n ) = 0. We
now prove this implication. We consider two different cases.
224

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

The case x1 = x1 . For any real  > 0:
() = P(I{o1:n } [I{x1:n }  I{x1:n }  ])
= Q1 (E 1 (I{o1:n } [I{x1:n }  I{x1:n }  ]|X1 ))

= Q1 I{x1 } E 1 (I{o1:n } [I{x2:n }  I{x2:n }  ]|x1 ) +




I{z1 } E 1 (I{o1:n } |z1 ) .

(37)

z1 6=x1

The coefficients E 1 (I{o1:n } |z1 ) can be written as E 1 ({o1:n }|z1 ) by conjugacy and C2, which
makes them negative, decreasing functions of , since E 1 ({o1:n }|z1 ) > 0 by the positivity assumption (10) and Proposition 1.
For the coefficient E 1 (I{o1:n } [I{x2:n }  I{x2:n }  ]|x1 ), we consider two possible cases.
If E 1 (I{o1:n } [I{x2:n }  I{x2:n } ]|x1 ) > 0, we know that E 1 (I{o1:n } [I{x2:n }  I{x2:n }  ]|x1 ) is a decreasing function of  by Lemma 7(vi). Therefore, the argument of Q1 in Equation (37) decreases
pointwise in , which by Lemma 8(i) implies that () is a decreasing function of  and therefore
() < (0) = 0.
If, on the other hand, E 1 (I{o1:n } [I{x2:n }  I{x2:n } ]|x1 )  0, then we know by Lemma 8(ii) that
E 1 (I{o1:n } [I{x2:n }  I{x2:n }  ]|x1 )  0, implying that

()  Q1




I{z1 } E 1 (I{o1:n } |z1 )

z1 6=x1


 Q1 I{z1 } E 1 (I{o1:n } |z1 ) = E 1 ({o1:n }|z1 )Q1 {z1 } < 0.
In this expression, z1 is an arbitrary z1 6= x1 . The first two inequalities are due to Lemma 8(ii).
Conjugacy and C2 yield the equality and the last inequality is a consequence of the positivity assumption (10) and Proposition 1. Also in this case, therefore, we find that () < 0.
The case x1 6= x1 . For any real  > 0:
() = P(I{o1:n } [I{x1:n }  I{x1:n }  ])
= Q1 (E 1 (I{o1:n } [I{x1:n }  I{x1:n }  ]|X1 ))

= Q1 I{x1 } E 1 (I{o1:n } [I{x2:n }  ]|x1 ) + I{x1 } E 1 (I{o1:n } [I{x2:n }  ]|x1 )
+




I{z1 } E 1 (I{o1:n } |z1 )

(38)

z1 6=x1 ,x1

In the proof for the case x1 = x1 , we have already shown that the coefficients E 1 (I{o1:n } |z1 )
are negative, decreasing functions of . Together with Lemma 8(ii), this allows us to infer that
E 1 (I{o1:n } [I{x2:n }  ]|x1 )  E 1 (I{o1:n } |x1 ) < 0, which in turn by Lemma 7(vii) implies that
E 1 (I{o1:n } [I{x2:n }  ]|x1 ) is a decreasing function of . All that is left to consider is the coefficient E 1 (I{o1:n } [I{x2:n }  ]|x1 ). There are two possibilities.
If E 1 (I{o1:n } I{x2:n } |x1 ) > 0, then Lemma 7(vi) implies that E 1 (I{o1:n } [I{x2:n } ]|x1 ) is a decreasing
function of . Therefore, the argument of Q1 in Equation (38) decreases pointwise in , which by
Lemma 8(i) implies that () is a decreasing function of  and therefore () < (0) = 0.
225

fiD E B OCK & D E C OOMAN

If, on the other hand, E 1 (I{o1:n } I{x2:n } |x1 ) = 0, then by Lemma 8(ii), E 1 (I{o1:n } [I{x2:n } ]|x1 )  0,
implying that
()  Q1 (I{x1 } E 1 (I{o1:n } [I{x2:n }  ]|x1 ))
 Q1 (I{x1 } E 1 (I{o1:n } |x1 )) = E 1 ({o1:n }|x1 )Q1 ({x1 }) < 0.
The first two inequalities follow from Lemma 8(ii). Conjugacy and C2 yield the equality, and the
last inequality is a consequence of the positivity assumption (10) and Proposition 1. Also in this
case, then, we find that () < 0.
Lemma 7. Let P be a coherent lower prevision on G (X). For any f  G (X) and y  Y , consider
the real-valued map  defined on R by () := P(I{y} [ f  ]) for all real . Then the following
statements hold:
(i)  is non-increasing, concave and continuous.
(ii)  has at least one zero.
(iii) If P({y}) > 0, then  is decreasing and has a unique zero.
(iv) If P({y}) = 0, then  is identically zero.
(v) If P({y}) = 0 and P({y}) > 0, then  is zero on (, P( f |y)], and negative and decreasing
on (P( f |y), +).
(vi) If (a) > 0 for some a, then  is decreasing and has a unique zero.
(vii) If  is negative on an interval (a, b), then it is also decreasing on (a, b).
Proof. We start by proving (i). It follows directly from Lemma 8(ii) that  is non-increasing in .
Now consider 1 and 2 in R and 0    1.  is concave because
( 1 + (1   )2 ) = P(I{y} [ f  ( 1 + (1   )2 )])
= P( I{y} [ f  1 ] + (1   )I{y} [ f  2 ])
 P( I{y} [ f  1 ]) + P((1   )I{y} [ f  2 ])
=  P(I{y} [ f  1 ]) + (1   )P(I{y} [ f  2 ])
=  (1 ) + (1   )(2 ),
where the inequality follows from C3 and the subsequent step is due to C2. To prove that () is
continuous, consider any 1 and 2 in R, then we see that
(2 ) = P(I{y} [ f  2 ]) = P(I{y} [ f  1 + (1  2 )])
= P(I{y} [ f  1 ] + I{y} (1  2 ))  P(I{y} [ f  1 ]) + P(I{y} (1  2 ))
= (1 )  P({y}) fi (2  1 ),
where the inequality follows from C3, and the last equality is due to conjugacy and C2. Hence
|(1 )  (2 )|  |2  1 |P({y}), which proves that  is Lipschitz continuous, and therefore also
continuous.
226

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

To prove (ii), first notice that (min f ) = P(I{y} [ f  min f ])  P(I{y} [min f  min f ]) = 0 and
(max f ) = P(I{y} [ f  max f ])E  P(I{y} [max f  max f ]) = 0. The inequalities are a consequence
of Lemma 8(ii), and the last equalities follow from Lemma 6. Since () is continuous, this implies
the existence of a zero between min f and max f .
Property (iii) can be proved by considering 1 and 2 in R with 2 > 1 . If P({y}) > 0, we see
that  is decreasing, since
(1 ) = P(I{y} [ f  1 ]) = P(I{y} [ f  2 + (2  1 )])
= P(I{y} [ f  2 ] + I{y} (2  1 ))  P(I{y} [ f  2 ]) + P(I{y} (2  1 ))
= (2 ) + (2  1 )P({y}) > (2 ),
where the first inequality follows from C3 and the last equality from C2. We know by (ii) that  has
at least one zero, which must be unique because  is decreasing.
To prove (iv), first note that P({y}) = 0 also implies P({y}) = 0, because of Lemma 6. Now fix
 in R and choose a and b in R such that
a < min{0, min{ f  }}  max{0, max{ f  }} < b.
Then we find that () = P(I{y} [ f  ])  P(I{y} a) = aP({y}) = 0 and () = P(I{y} [ f  ]) 
P(I{y} b) = bP({y}) = 0, using Lemma 8(ii), C2 and conjugacy. We conclude that () = 0 for any
 in R.
The proof of (v) starts by noticing that ()  0 for   (, P( f |y)] and () < 0 for
  (P( f |y), +), due to the definition of P( f |y) (see Equation (11)), and the fact that  is nonincreasing by (i). In the proof of (iv), we have already shown that  is non-positive if P({y}) = 0,
which allows us to conclude that () = 0 for   (, P( f |y)]. We are left to prove that 
is decreasing on the interval (P( f |y), +). We will do so by contradiction. Suppose that  is
not decreasing on that interval, then there are 1 and 2 in this interval, such that 2 > 1 and
0 > (2 )  (1 ). Since  is zero on (, P( f |y)), we can also choose 0 < 1 such that
(0 ) = 0. The existence of such 0 , 1 and 2 contradicts the concavity of , established by (i).
To prove (vi), observe that P({y})  P({y})  0 by Lemma 6. This implies that the three cases
considered in (iii), (iv) and (v) are exhaustive and mutually exclusive. If there is an a for which
(a) > 0, we can only have the case considered in (iii), which implies that  is decreasing and has
a unique zero.
It now only remains to prove (vii). By repeating the argument in the proof of (vi), we see that 
is negative on an interval (a, b), only the cases considered in (iii) and (v) can obtain. For (iii),  is
decreasing on its entire domain. For (v),  is definitely decreasing on (a, b).
Lemma 8. Consider a coherent lower prevision P on G (X) and two gambles f , g  G (X).
(i) If f (x) > g(x) for all x  X, then P( f ) > P(g).
(ii) If f (x)  g(x) for all x  X, then P( f )  P(g).
Proof. We start with (i). Since f g is pointwise positive, we have that min( f g) > 0 and therefore
that P( f  g)  min ( f  g) > 0, using C1 for the first inequality. It now follows from C3 that
P( f ) = P(( f  g) + g)  P( f  g) + P(g), and therefore that P( f )  P(g)  P( f  g) > 0, whence
indeed P( f ) > P(g). The proof for (ii) is analogous; this time, we have that min( f  g)  0 and
therefore that P( f )  P(g)  P( f  g)  min ( f  g)  0.
227

fiD E B OCK & D E C OOMAN

Proof of Equation (15). Let [xk:n , xk:n ] := I{ok:n } [I{xk:n }  I{xk:n } ]. Since we are considering the case
k  {1, . . . , n  1} and xk = xk , we find that
[xk:n , xk:n ] = I{ok:n } [I{xk:n }  I{xk:n } ] = I{ok } I{xk } I{ok+1:n } [I{xk+1:n }  I{xk+1:n } ]
= I{ok } I{xk } [xk+1:n , xk+1:n ],
which in turn implies that
Pk ([xk:n , xk:n ]|xk1 ) = Qk (E k (I{ok } I{xk } [xk+1:n , xk+1:n ]|Xk )|xk1 )
= Qk (I{xk } E k (I{ok } [xk+1:n , xk+1:n ]|xk )|xk1 )
= Qk ({xk }|xk1 ) fi E k (I{ok } [xk+1:n , xk+1:n ]|xk )
= Qk ({xk }|xk1 )Sk ({ok }|xk ) fi Pk+1 ([xk+1:n , xk+1:n ]|xk ),
proving Equation (15). The first equality follows from Equation (5). The second equality holds
because I{xk } (zk ) = 0 for all zk 6= xk , implying that
E k (I{ok } I{xk } [xk+1:n , xk+1:n ]|Xk ) = I{xk } E k (I{ok } [xk+1:n , xk+1:n ]|xk ).
The third equality is follows from conjugacy and C2, and the last one follows from Equation (4).
Proof of Equation (16). Since xn = xn , Lemma 6 yields:
Pn (I{on } [I{xn }  I{xn } |xn1 ) = Pn (I{on } [I{xn }  I{xn } |xn1 ) = Pn (0|xn1 ) = 0.
Proof of Equation (17). If k  {1, . . . , n} and xk 6= xk , then
Pk (I{ok:n } [I{xk:n }  I{xk:n } |xk1 ) = Qk (E k (I{ok:n } [I{xk:n }  I{xk:n } ]|Xk )|xk1 )
= Qk (I{xk } E k (I{ok:n } I{xk+1:n } |xk ) + I{xk } E k (I{ok:n } I{xk+1:n } |xk )|xk1 )
= Qk (I{xk } E k (I{ok:n } I{xk+1:n } |xk )  I{xk } E k (I{ok:n } I{xk+1:n } |xk )|xk1 )
= Qk (I{xk }  (xk:n )  I{xk } (xk:n )|xk1 ),
proving Equation (17). The reasons why all these equalities hold, are analogous to the ones given
in the proof of Equation (15).
Proof of Theorem 3. Fix k  {1, . . . , n  1}, xk1  Xk1 and xk:n  Xk:n . We now assume that
xk+1:n 
/ opt (Xk+1:n |xk , ok+1:n ) and show that xk:n 
/ opt (Xk:n |xk1 , ok:n ). It follows from the assumption that Pk+1 (I{ok+1:n } [I{xk+1:n }  I{xk+1:n } |xk ) > 0 for some xk+1:n  Xk+1 . Now prefix this state
sequence xk+1:n with the state xk to form the state sequence xk:n , implying that xk = xk . We then
infer from Equation (15) that
Pk (I{ok:n } [I{xk:n }  I{xk:n } |xk1 ) = Qk ({xk }|xk1 )Sk ({ok }|xk )Pk+1 (I{ok+1:n } [I{xk+1:n }  I{xk+1:n } |xk ) > 0,
which tells us that indeed xk:n 
/ opt (Xk:n |xk1 , ok:n ).
228

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

Proof of Equations (33) and (34). First, we consider k = n. For every xn1  Xn1 , we determine
opt (Xn |xn1 , on ) as the set of those elements xn of Xn for which
(xn  Xn \ {xn })Qn (I{xn } nmax (xn )  I{xn } (xn )|xn1 )  0,
as this condition is equivalent to the optimality condition (14) for k = n, taking into account Equations (16), (17) and (31). We now show that this condition is also equivalent to
(xn  Xn \ {xn })(xn )  nmax (xn )n (xn , xn |xn1 ),

(39)

To see this, we consider two different cases. For those xn for which nmax (xn ) = 0, the inequalities Qn (I{xn } nmax (xn )  I{xn } (xn )|xn1 )  0 and (xn )  nmax (xn )n (xn , xn |xn1 ) are both trivially
satisfied since (xn ) = Sn ({on }|xn ) > 0 by the positivity assumption (10). If nmax (xn ) > 0, both
inequalities are equivalent because of C2 and Equation (27):


(xn ) fifi
max
Qn (I{xn } n (xn )  I{xn } (xn )|xn1 )  0  Qn I{xn }  I{xn } max
fixn1  0
n (xn )
(xn )
 n (xn , xn |xn1 )
 max
n (xn )
 (xn )  nmax (xn )n (xn , xn |xn1 ).
opt

Using Equation (32), Equation (39) can now be reformulated as (xn )  n (xn |xn1 ), which completes the proof of the equivalence.
Next, consider any k  {1, . . . , n1} and xk1  Xk1 . We must determine opt (Xk:n |xk1 , ok:n ).
We know from the Principle of Optimality (23) that we can limit the candidate optimal sequences
xk:n to the set cand (Xk:n |xk1 , ok:n ). Consider any such xk:n , then we must check for any xk:n  Xk:n
whether Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 )  0; see Equation (14).
If xk:n is such that xk = xk , this inequality is always satisfied. Indeed, if xk 
/ Posk (xk1 ), then we
infer from Equation (25) that Qk ({xk }|xk1 ) = 0 or Sk ({ok }|xk ) = 0, and then Equation (15) tells
us that Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 ) = 0. If xk  Posk (xk1 ), we know from Equation (24) that
xk+1:n  opt (Xk+1:n |xk , ok+1:n ), which in turn implies that Pk+1 (I{ok+1:n } [I{xk+1:n }  I{xk+1:n } ]|xk )  0.
Hence Pk (I{ok:n } [I{xk:n }  I{xk:n } ]|xk1 )  0, again by Equation (15).
This means we can limit ourselves to checking the inequality for those xk:n for which xk 6= xk .
So fix any xk 6= xk , then we must check whether
(xk+1:n  Xk+1:n )Qk (I{xk }  (xk:n )  I{xk } (xk:n )|xk1 )  0;
see Equation (17). By Equation (28) and Lemma 8, this is equivalent to
Qk (I{xk } kmax (xk )  I{xk } (xk:n )|xk1 )  0,
which can in turn be seen to be equivalent to (xk:n )  kmax (xk )k (xk , xk |xk1 ), using a course of
reasoning completely analogous to the one used above for the case k = n. Since this inequality must
opt
hold for every xk 6= xk , we infer from Equation (32) that we must have that (xk:n )  k (xk |xk1 ).
So we must check this condition for all the candidate sequences xk:n in cand (Xk:n |xk1 , ok:n ), which
proves Equation (33).
229

fiD E B OCK & D E C OOMAN

Proof of Theorem 4. We start by proving that every sequence xk:n that is added in Line 2 of the
Procedure Recur(xk:n , n) is indeed an element of opt (Xk:n |xk1 , ok:n ). If Line 2 of the Procedure Recur(xk:n , n) is executed, this means that the Procedure Recur(xk:n1 , n  1) was executed
in the previous step, and that at that point, the if-conditions in Lines 5 and 6 were satisfied. Due to
the first if-condition, we know that xk:n  candxk:n (Xk:n |xk1 , ok:n ) and therefore, by Equation (35),
also that xk:n  cand (Xk:n |xk1 , ok:n ). From the second if-condition, we infer that nmax (xn ) 
opt
opt
k (xk:n |xk1 ), which can be seen to be equivalent with (xk:n )  k (xk |xk1 ), by Equation (31)
and the repeated use of Equations (36) and (20). It now follows from Equation (33) that xk:n is an
element of opt (Xk:n |xk1 , ok:n ).
To conclude the proof, we show that a sequence xk:n that has not been added during the course
of the algorithm cannot be an element of opt (Xk:n |xk1 , ok:n ). If a sequence xk:n has not been added,
this either implies that it is not an element of cand (Xk:n |xk1 , ok:n ) [the if-condition on Line 5 of
the Procedure Recur was not satisfied], or that there is some i  {k, . . . , n} for which imax (xi ) <
opt
k (xk:i |xk1 ) [the if-condition on Line 9 of Algorithm 2 or Line 5 of the Procedure Recur was
not satisfied]. In the first case, it follows directly from Equation (33) that xk:n cannot be an element
opt
of opt (Xk:n |xk1 , ok:n ). In the second case, we find that imax (xi ) < k (xk:i |xk1 ) implies that
opt
opt
(xk:n ) < k (xk |xk1 ), which can be seen to be equivalent with (xk:n ) < k (xk |xk1 ) by the
repeated use of Equations (36) and (20). It then follows from Equation (33) that xk:n cannot be an
element of opt (Xk:n |xk1 , ok:n ).

Proof of Theorem 5. Equation (28) implies that there is at least one sequence xs+1:n
 Xs+1:n for

max

which (xs  xs+1:n ) = s (xs ). We prove that the first state xs+1 of this sequence meets both
criteria of the theorem.
opt
We know that candxk:s (Xk:n |xk1 , ok:n ) 6= 0/ and smax (xs )  k (xk:s |xk1 ) because both conditions are necessary in order for the Procedure Recur(xk:s , s) to be executed while running Algorithm 2. For s = k, the condition candxk:s (Xk:n |xk1 , ok:n ) 6= 0/ is not explicitely checked by Algorithm 2, but nevertheless also true because of Equations (24) and (35) and because we know
that opt (Xk+1:n |xk , ok+1:n ) 6= 0/ [because every finite partially ordered set has at least one maximal
element].
opt

Since (xs  xs+1:n
) = smax (xs )  k (xk:s |xk1 ), we know from Equations (20) and (36) that
opt

 |x
max 
(xs+1:n
)  k (xk:s xs+1
k1 ). By combining this with Equation (28), we find that s+1 (xs+1 ) 
opt


k (xk:s  xs+1 |xk1 ), meaning that xs+1 satisfies the if-condition in Line 6.

Due to Lemma 9, we can infer from candxk:s (Xk:n |xk1 , ok:n ) 6= 0/ and (xs  xs+1:n
) = smax (xs )


that candxk:s xs+1
(Xk:n |xk1 , ok:n ) 6= 0,
/ meaning that xs+1 satisfies the if-condition in Line 5 as well.


Lemma 9. Consider any k  {1, . . . , n  1}, s  {k, . . . , n  1}, xk1  Xk1 , xk:s  Xk:s and xs+1:n


max
Xs+1:n . Then if candxk:s (Xk:n |xk1 , ok:n ) 6= 0/ and (xs  xs+1:n ) = s (xs ), we also have that

candxk:s xs+1
(Xk:n |xk1 , ok:n ) 6= 0.
/

Proof. Let zs+1:n be any sequence in Xs+1:n for which xk:s  zs+1:n  cand (Xk:n |xk1 , ok:n ); this is
possible because, by assumption, candxk:s (Xk:n |xk1 , ok:n ) 6= 0.
/
If there is some q  {k, . . . , s  1} for which xq 
/ Posq (xq1 ), then we denote the smallest such q

as q . In that case, by Equation (24), we find that xq :s  xs+1:n
and xq :s  zs+1:n are both elements of

cand (Xq :n |xq 1 , oq :n ). If no such q exists, we let q := s. In that case, since xq  Posq (xq1 ) for all
230

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

q  {k, . . . , s  1}, it follows from xk:s  zs+1:n  cand (Xk:n |xk1 , ok:n ) and the repeated use of Equa
tions (24) and (23) that xs  zs+1:n belongs to cand (Xs:n |xs1 , os:n ). Since (xs  xs+1:n
) = smax (xs ),

we can infer from Lemma 10 that xs+1:n
 opt (Xs+1:n |xs , os+1:n ) and therefore, by Equation (24),

that xs  xs+1:n  cand (Xs:n |xs1 , os:n ).

In any case, we now have a q  {k, . . . , s} for which xq :s  xs+1:n
and xq :s  zs+1:n belong to

cand (Xq :n |xq 1 , oq :n ) and for which, for all q  {k, . . . , q  1}, xq  Posq (xq1 ). If q = k, this
concludes the proof. Therefore, we will from now on consider the case q  {k + 1, . . . , s}.
We first recall that cand (Xk:n |xk1 , ok:n ) can be constructed by applying Equations (33) and (24)
repeatedly. Therefore, since we know that xq :s  zs+1:n  cand (Xq :n |xq 1 , oq :n ) and xk:s  zs+1:n 
cand (Xk:n |xk1 , ok:n ), it must be that
(xq:s  zs+1:n )  qopt (xq |xq1 ) for all q  {k + 1, . . . , q }.

(40)



Furthermore, since (xs  xs+1:n
) = smax (xs ), we infer from Equation (28) that (xs  xs+1:n
)
(xs  zs+1:n ) and therefore, by Equation (20), we find hat

(xq:s  xs+1:n
)  (xq:s  zs+1:n ) for all q  {k + 1, . . . , s}.

Hence, by Equation (40):

(xq:s  xs+1:n
)  qopt (xq |xq1 ) for all q  {k + 1, . . . , q }.

(41)

Since cand (Xk:n |xk1 , ok:n ) can be constructed by repeatedly applying Equations (33) and (24) and


because xq :s xs+1:n
 cand (Xq :n |xq 1 , oq :n ), we now infer from Equation (41) that xk:s xs+1:n

cand (Xk:n |xk1 , ok:n ).

Lemma 10. Consider any s  {1, . . . , n  1}, xs  Xs and xs+1:n
 Xs+1:n . Then


(xs  xs+1:n
) = smax (xs ) = xs+1:n
 opt (Xs+1:n |xs , os+1:n ) .

Proof. Assume that (xs  xs+1:n
) = smax (xs ) and consider any zs+1:n  Xs+1:n . Then we know by

Equation (28) that (xs  xs+1:n
)  (xs  zs+1:n ) and therefore, by Equation (19) and (7), that

Ss ({os }|xs )Ps+1 (I{xs+1:n
} I{os+1:n } |xs )  Ss ({os }|xs )Ps+1 (I{zs+1:n } I{os+1:n } |xs ).

Together with the positivity assumption (10), this implies that

Ps+1 (I{xs+1:n
} I{os+1:n } |xs )  Ps+1 (I{zs+1:n } I{os+1:n } |xs ).

(42)

By C3, we also know that


Ps+1 (I{xs+1:n
} I{os+1:n } |xs )  Ps+1 (I{os+1:n } (I{zs+1:n }  I{xs+1:n
} )|xs ) + Ps+1 (I{zs+1:n } I{os+1:n } |xs )

which, by conjugacy, implies that


Ps+1 (I{os+1:n } (I{zs+1:n }  I{xs+1:n
} )|xs )  Ps+1 (I{zs+1:n } I{os+1:n } |xs )  Ps+1 (I{xs+1:n
} I{os+1:n } |xs ).

Using Equation (42), we see that Ps+1 (I{os+1:n } (I{zs+1:n }  I{xs+1:n
} )|xs )  0. Since this holds for all

zs+1:n  Xs+1:n , we infer from Equation (14) that xs+1:n
 opt (Xs+1:n |xs , os+1:n ).

231

fiD E B OCK & D E C OOMAN

References
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton.
Benavoli, A., Zaffalon, M., & Miranda, E. (2011). Robust filtering through coherent lower previsions. Automatic Control, IEEE Transactions on, 56(7), 15671581.
Brown, D. G., & Golod, D. (2010). Decoding HMMs using the k best paths: algorithms and applications.. BMC Bioinformatics, 11(S-1), 28.
Cozman, F. G. (2000). Credal networks. Artificial Intelligence, 120, 199233.
Cozman, F. G. (2005). Graphical models for imprecise probabilities. International Journal of
Approximate Reasoning, 39(2-3), 167184.
de Campos, L. M., Huete, J. F., & Moral, S. (1994). Probability intervals: a tool for uncertain
reasoning. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,
2, 167196.
De Cooman, G., Miranda, E., & Zaffalon, M. (2011). Independent natural extension. Artificial
Intelligence, 175, 19111950.
De Cooman, G., Hermans, F., Antonucci, A., & Zaffalon, M. (2010). Epistemic irrelevance in credal
nets: the case of imprecise Markov trees. International Journal of Approximate Reasoning,
51, 10291052.
De Cooman, G., & Troffaes, M. C. M. (2005). Dynamic programming for deterministic discretetime systems with uncertain gain. International Journal of Approximate Reasoning, 39, 257
278.
De Cooman, G., Troffaes, M. C. M., & Miranda, E. (2008). n-Monotone exact functionals. Journal
of Mathematical Analysis and Applications, 347, 143156.
Dempster, A. P. (1967). Upper and lower probabilities induced by a multivalued mapping. Annals
of Mathematical Statistics, 38, 325339.
Huntley, N., & Troffaes, M. C. M. (2010). Normal form backward induction for decision trees with
coherent lower previsions. Annals of Operations Research. Submitted for publication.
Kikuti, D., Cozman, F., & de Campos, C. (2005). Partially ordered preferences in decision trees:
computing strategies with imprecision in probabilities. In IJCAI Workshop About Advances
on Preference Handling, pp. 13131318.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques Adaptive Computation and Machine Learning. The MIT Press.
Mau, D., de Campos, C., Benavoli, A., & Antonucci, A. (2013). On the complexity of strong and
epistemic credal networks. In Proceedings of the 29th Conference on Uncertainty in Artificial
Intelligence, pp. 391400. AUAI Press.
Miranda, E. (2008). A survey of the theory of coherent lower previsions. International Journal of
Approximate Reasoning, 48(2), 628658.
Miranda, E. (2009). Updating coherent lower previsions on finite spaces. Fuzzy Sets and Systems,
160(9), 12861307.
232

fiE STIMATING S TATE S EQUENCES IN I MPRECISE H IDDEN M ARKOV M ODELS

Miranda, E., & de Cooman, G. (2007). Marginal extension in the theory of coherent lower previsions. International Journal of Approximate Reasoning, 46(1), 188225.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2), 257286.
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton University Press, Princeton, NJ.
Troffaes, M. C. M. (2007). Decision making under uncertainty using imprecise probabilities. International Journal of Approximate Reasoning, 45(1), 1729.
Utkin, L. V., & Augustin, T. (2005). Powerful algorithms for decision making under partial prior information and general ambiguity attitudes. In in: Proceedings of the 3th International Symposium on Imprecise Probability: Theories and Applications (ISIPTA), Prague,Czech Republic,
pp. 349358.
Viterbi, A. J. (1967). Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory, 13(2), 260269.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities. Chapman and Hall, London.
Walley, P. (1996). Inferences from multinomial data: learning about a bag of marbles. Journal of
the Royal Statistical Society, Series B, 58, 357. With discussion.
Weichselberger, K. (2000). The theory of interval probability as a unifying concept for uncertainty.
International Journal of Approximate Reasoning, 24(23), 149170.

233

fiJournal of Artificial Intelligence Research 50 (2014) 447-485

Submitted 2/14; published 6/14

Monotone Temporal Planning:
Tractability, Extensions and Applications
Martin C. Cooper
Frdric Maris
Pierre Rgnier

COOPER@IRIT.FR
MARIS@IRIT.FR
REGNIER@IRIT.FR

IRIT, Paul Sabatier University
118 Route de Narbonne
31062 Toulouse, France

Abstract
This paper describes a polynomially-solvable class of temporal planning problems. Polynomiality follows from two assumptions. Firstly, by supposing that each sub-goal fluent can be established
by at most one action, we can quickly determine which actions are necessary in any plan. Secondly,
the monotonicity of sub-goal fluents allows us to express planning as an instance of STP (Simple
Temporal Problem with difference constraints). This class includes temporally-expressive problems
requiring the concurrent execution of actions, with potential applications in the chemical, pharmaceutical and construction industries.
We also show that any (temporal) planning problem has a monotone relaxation which can lead to
the polynomial-time detection of its unsolvability in certain cases. Indeed we show that our relaxation is orthogonal to relaxations based on the ignore-deletes approach used in classical planning
since it preserves deletes and can also exploit temporal information.

1. Introduction
Planning is a field of AI which is intractable in the general case (Erol, Nau & Subrahmanian,
1995). In particular, propositional planning is PSPACE-Complete (Bylander, 1994). Identifying tractable classes of planning is important for at least two reasons. Firstly, real-world applications may fall
into such classes. Secondly, relaxing an arbitrary instance I so that it falls in the tractable class can
provide useful information concerning I in polynomial time.
Temporal planning is an important extension of classical planning in which actions are durative
and may overlap. An important aspect of temporal planning is that, unlike classical planning, it permits us to model problems in which the execution of two or more actions in parallel is essential in order to solve the problem (Cushing, Kambhampati, Mausam & Weld, 2007). Although planning has
been studied since the beginnings of research in Artificial Intelligence, temporal planning is a relatively new field of research. No tractable classes had specifically been defined in the temporal framework
before the research described in this paper. We present a class of temporal planning problems that can
be solved in polynomial time. In particular, we considerably extend the theoretical results given in
conference papers (Cooper, Maris & Rgnier, 2012, 2013b) by considering plans with optimal
makespan, by relaxing the assumption that two instances of the same action do not overlap and by
introducing the notion of unitary actions. We also give previously unpublished results of experimental
trials on benchmark problems. But first, we review previous work in the identification of tractable
classes of classical planning problems.
 2014 AI Access Foundation. All rights reserved.

fiCOOPER, MARIS, & REGNIER

A lot of work has been done on the computational complexity of non-optimal and optimal planning
for classical benchmark domains. In the non-optimal case, Helmert (2003, 2006) proved that most of
these benchmarks can be solved by simple procedures running in low-order polynomial time. In the
optimal case, finding an optimal plan for the famous blocksworld domain is NP-hard (Gupta & Nau,
1992) but Slaney and Thibaux (2001) proved that this domain is tractable when searching for a nonoptimal plan.
Moreover, some planners empirically showed that the number of benchmark problems that can be
solved without search may be even larger than the number of tractable problems that have been identified theoretically. The FF planner (Hoffmann, 2005) demonstrated that domains with constantbounded heuristic plateaus can theoretically be solved in polynomial time using the h+ heuristic. The
eCPT planner (Vidal & Geffner, 2005) can solve, by use of inference, many instances of benchmark
domains without having to backtrack.
Since the work of Bckstrm and Klein (1991a) on the SAS formulation of planning, several studies have also been performed to define tractable classes of planning problems. Many of these results
(Bylander, 1994; Bckstrm & Nebel, 1995; Erol, Nau & Subrahmanian, 1995; Jonsson & Bckstrm, 1998) are based on syntactic restrictions on the set of operators. For example, operators having
a single effect, no two operators having the same effect, etc.
Another important body of work focused on the underlying structure of planning problems which
can be highlighted using the causal graph, a directed graph that describes variable dependencies
(Knoblock, 1994). Jonsson and Bckstrm (1995) presented a class of planning problems with an acyclic causal graph and unary operators. In this "3S" class, variables are either Static, Symmetrically
reversible, or Splitting; plan existence can be determined in polynomial time while plan generation is
provably intractable. Gimnez and Jonsson (2008) designed an algorithm that solves these problems
in polynomial time while producing a compact macro plan in place of the explicit exponential solution. They also proved that the problem of plan existence for planning problems with multi-valued
variables and chain causal graphs is NP-hard. Plan existence for planning problems with binary state
variables and polytree causal graphs was also proven to be NP-complete.
Jonsson and Bckstrm (1994, 1998) considered optimal and non-optimal plan generation and presented an exhaustive map of complexity results based on syntactic restrictions (using the SAS+ formulation of planning) together with restrictions on the causal graph structure (interference-safe, acyclic, prevail-order-preserving). They present a planning algorithm which is correct and runs in polynomial time under these restrictions. Williams and Nayak (1997) designed a polynomial-time algorithm for solving planning problems with acyclic causal graphs and reversible actions. Domshlak and
Dinitz (2001) investigated connections between the structure of the causal graph and the complexity
of the corresponding problems in the case of coordination problems for dependent agents with independent goals acting in the same environment. This general problem is shown to be intractable, but
some significant subclasses are in NP and even polynomial.
Brafman and Domshlak (2003, 2006) studied the complexity of planning in the propositional
STRIPS formalism under the restrictions of unary operators and acyclic graphs. They give a polynomial planning algorithm for domains whose causal graph induces a polytree of bounded indegree.
However, they also demonstrated that for singly connected causal graphs the problem is NP-complete.
Gimnez and Jonsson (2012) gave a polynomial algorithm for the class P(k) of k-dependent planning
problems with binary variables and polytree causal graphs for any fixed value of k. They also showed
448

fiMONOTONE TEMPORAL PLANNING

that if, in addition, the causal graph has bounded depth, plan generation is linear in the size of the input. Haslum (2008) defines planning problems in terms of graph grammars. This method reduces the
original problem to that of graph parsing, which can be solved in polynomial time under certain restrictions on the grammar. Haslum thus explores novel classes of restrictions that are distinct from
previously known tractable classes. Katz and Domshlak (2008) showed that planning problems whose
causal graphs are inverted forks are tractable if the root variable has a domain of fixed size. Jonsson
(2007, 2009) introduced the class IR of inverted tree reducible planning problems and gave an algorithm that uses macros to solve problems from this class. Its complexity depends on the size of the
domain transition graph and it runs in polynomial time for several subclasses of IR. Chen and Gimnez (2008) gave a unified framework to classify the complexity of planning under causal graph restrictions. They give a complete complexity classification of all sets of causal graphs for reversible
planning problems. The graph property that determines tractability is the existence of a constant bound
on the size of strongly connected components.
However, in real application domains, the sequential nature of classical plans is often too restrictive
and a temporal plan is required consisting of a set of instances of durative actions which may overlap.
Whereas classical planning consists in scheduling action-instances, temporal planning can be seen as
scheduling the events (such as the establishment or destruction of fluents) of action-instances subject
to temporal constraints capturing the internal structure of actions. A temporal planning framework
must therefore be used to formalize temporal relations between events within the same or different
actions-instances. In the PDDL 2.1 temporal framework (McDermott, 1998; Fox & Long, 2003), the
PSPACE-complete complexity of classical planning can be preserved only when different instances of
the same action cannot overlap. If they do overlap, testing the existence of a valid plan becomes an
EXPSPACE-complete problem (Rintanen, 2007).
In this paper we present a polynomially-solvable sub-problem of temporal planning. To our
knowledge no previous work has specifically addressed this issue. Polynomiality follows from the
double assumption that each sub-goal fluent can be established by at most one action and also satisfies a monotonicity condition. This allows us to express temporal planning as an instance of the
polynomial-time solvable problem STP (Simple Temporal Problem with difference constraints).
An STP instance consists of a set of real-valued variables and a set of constraints of the three following forms xy < c, xy  c or xy  c, where x,y are any variables and c is any constant. Our
tractable class includes temporally-expressive problems requiring the concurrent execution of actions, and has potential industrial applications. We also show how to derive, from an arbitrary (temporal) planning problem, a relaxed version belonging to this tractable class. This can lead to the
polynomial-time detection of unsolvability in certain cases. It also provides a polynomial-time heuristic for detecting actions or fluents satisfying certain properties.
The article is organized as follows: Section 2 reviews existing temporal planners and their use of
temporal constraints. Section 3 presents our temporal framework. Section 4 introduces the notion of
monotonicity of fluents. Section 5 shows how the notion of monotonicity can be extended to monotonicity* in order to define a larger tractable class and presents the main theorem. Section 6 demonstrates how to build a tractable relaxation of any temporal planning problem (or classical planning
problem) based on simple temporal problems. Section 7 shows how to determine whether fluents are
monotone* using this relaxation and describes a tractable class of temporal planning problems. Section 8 describes experimental trials to validate and identify the limits of this temporal relaxation. Section 9 gives examples of temporal planning problems that can be solved in polynomial time, including
449

fiCOOPER, MARIS, & REGNIER

a detailed example involving concrete mixing, delivery and use. It is worth noting that all solutions to
the examples discussed in Section 9 require concurrent actions. Sections 10 and 11 conclude and discuss avenues of future research.
2. Temporal Constraint Solving in Temporal Planning
After the first temporal planner DEVISER (Vere, 1983), planners such as FORBIN (Dean, Firby &
Miller, 1988) quite rapidly used an independent module, called Time-Map Manager (Dean &
McDermott, 1987), to handle temporal constraints. The HTN (Hierarchical Tasks Network) planners
IxTeT (Ghallab & Alaoui, 1989; Laborie & Ghallab, 1995), TRIPTIC (Rutten & Hertzberg, 1993) and
TEST (Reichgelt & Shadbolt, 1990) kept this idea of an Independent module to manage temporal
data.
Todays temporal planners are essentially based on one of three types of algorithms: plan-space
search, state-space search and GRAPHPLAN (Blum & Furst, 1995).
The plan-space planners HTN and POP (Partially Ordered Planning) were the first to be extended
to the temporal framework. In general, they use temporal intervals for the representation of actions
and propositions, the causality relation between actions being replaced by a temporal order in partial
plans. Conflict handling is then performed by a system inspired by Time-Map Manager. For example,
the VHPOP planner (Younes & Simmons, 2003) uses a system of simple temporal constraints (STP:
Simple Temporal Problem) (Dechter, Meiri & Pearl, 1991), whereas DT-POP (Schwartz & Pollack,
2004) is based on a system of disjunctive temporal constraints (DTP: Disjunctive Temporal Problem)
(Stergiou & Koubarakis, 2000). The advantage of STPs is that they can be solved in polynomial time.
DTPs cannot be solved in polynomial time, but allow the user to express temporal constraints such as
"A appears before or after B", which lightens the work of the planner.
State-space search planners associate a start instant with each world state. Search can be based first
on the instants when an event can occur: each decision of the form "when to perform an action" is then
taken before all decisions of the form "which action is to be performed". This approach is called Decision Epoch Planning. Search can also be based first on finding which actions to use before scheduling
these actions in time: all decisions of the form "when to perform an action" are taken only after all
decisions of the form "which action is to be performed" have been taken. This approach is called
Temporally Lifted Progression Planning.
GRAPHPLAN has also been extended to temporal domains through the use of solvers, in the planners LPGP (Long & Fox, 2003), TM-LPSAT (Shin & Davis, 2004) and TLP-GP (Maris & Rgnier,
2008).
As we have seen, many temporal planners use the resolution of a system of temporal constraints.
However, even when this system of constraints can be solved in polynomial time, as is the case for
simple temporal constraints, the PSPACE complexity of classical planning remains. Indeed, certain
planners even solve a system of disjunctive temporal constraints, which is known to be NP-hard. The
tractable classes of classical planning (discussed in Section 1) have not been explicitly extended to
temporal planning. In this paper we present what is to our knowledge the first tractable class of temporal planning problems. Its solution algorithm is based on solving a system of simple temporal constraints.
450

fiMONOTONE TEMPORAL PLANNING

3. Definitions
We study temporal propositional planning in a language based on the temporal aspects of
PDDL2.1 (Fox & Long, 2003). A fluent is a positive or negative atomic proposition. As in PDDL2.1,
we consider that changes of the values of fluents are instantaneous but that conditions on the value of
fluents may be imposed over an interval. An action a is a quadruple <Cond(a), Add(a), Del(a), Constr(a)>, where the set of conditions Cond(a) is the set of fluents which are required to be true for a to
be executed, the set of additions Add(a) is the set of fluents which are established by a, the set of deletions Del(a) is the set of fluents which are destroyed by a, and the set of constraints Constr(a) is a set
of constraints between the relative times of events which occur during the execution of a. An event
corresponds to one of four possibilities: the establishment or destruction of a fluent by an action a, or
the beginning or end of an interval over which a fluent is required by an action a. In PDDL2.1, events
can only occur at the beginning or end of actions, but we relax this assumption so that events can occur at any time provided the constraints Constr(a) are satisfied. Note that Add(a)  Del(a) may be
non-empty. Indeed, it is not unusual for a durative action to establish a fluent at the beginning of the
action and destroy it at its end. We can also observe that the duration of an action, the time between
the first and last events of the action, does not need to be explicitly stored.
We represent non-instantaneous actions by a rectangle. The duration of an action is given in square
brackets after the name of the action. Conditions are written above an action, and effects below. The
action LOAD(m,c) shown in Figure 1 represents loading a batch of concrete c in a mixer m. We have
Cond(LOAD(m,c)) = {Fluid(c), Empty(m), At-factory(m)}. We can see from the figure that the mixer
must be empty at the start of the loading, whereas the concrete must be fluid and the mixer at the factory during the whole duration of the loading. We have Del(LOAD(m,c)) = {Empty(m)} and
Add(LOAD(m,c) = {On(m,c)}. We can see from the figure that as soon as loading starts, the mixer is
no longer empty and at the end of loading the mixer contains the concrete.
Fluid(c)
Empty(m)

At-factory(m)
LOAD(m,c)[5]
On(m,c)

Empty(m)

Figure 1: An example of the representation of a durative action

We use the notation a  f to denote the event that action a establishes fluent f, a  f to denote
the event that a destroys f, and f | a and f | a, respectively, to denote the beginning and end of the
interval over which a requires the condition f. If f is already true (respectively, false) when the event
a  f (a  f ) occurs, we still consider that a establishes (destroys) f. A temporal plan may contain
several instances of the same action, but since most of the temporal plans studied in this paper contain
at most one instance of each action, for notational simplicity, we only make the distinction between
actions and action-instances if this is absolutely necessary. We use the notation (e) to represent the
time in a plan at which an event e occurs.
451

fiCOOPER, MARIS, & REGNIER

For a given action (or action-instance) a, let Events(a) represent the different events which constitute its definition, namely (a  f ) for all f in Add(a), (a  f ) for all f in Del(a), ( f | a) and
( f | a) for all f in Cond(a). The definition of an action a includes constraints Constr(a) on the relative times of events in Events(a). For example, the internal structure of the fixed-length action
LOAD(m,c) shown in Figure 1 is defined by constraints such as
(Fluid(c) | LOAD(m,c))  (Fluid(c) | LOAD(m,c)) = 5
(LOAD(m,c)  On(m,c))  (Fluid(c) | LOAD(m,c)) = 0
As in PDDL2.1, we consider that the length of time between events in Events(a) is not necessarily
fixed and that Constr(a) is a set of interval constraints on pairs of events, such as
( f | a)  ( f | a)  [, ] for some constants ,. We use [a(e1, e2), a(e1, e2)] to denote the interval of possible values for the relative distance between events e1, e2 in action a. A fixed length of time
between events e1, e2  Events(a) can, of course, be modelled by setting a(e1, e2) = a(e1, e2). Similarly, the absence of any constraint can be modelled by the interval [, +]. We now introduce two
basic constraints that all temporal plans must satisfy.
inherent constraints on the set of action-instances A: for all aA, a satisfies Constr(a), i.e. for all
pairs of events e1, e2  Events(a), (e1)  (e2)  [a(e1, e2), a(e1, e2)].
contradictory-effects constraints on the set of action-instances A: for all ai, aj A, for all positive
fluents f  Del(ai)  Add(aj), (ai  f )  (aj  f ).
The inherent constraints define the internal structure of each action-instance, whereas the contradictory-effects constraints ensure that the truth-value of each fluent never becomes undefined during the
execution of a temporal plan. For example, if a plan contains an instance a of the action LOAD(m,c)
shown in Figure 1 and an instance b of another action CLEAN(m) with Empty(m) 
Add(CLEAN(m)), then the temporal plan must satisfy the contradictory-effects constraint
(a  EMPTY(m))  (b  EMPTY(m)).
Definition 3.1. A temporal planning problem <I,A,G> consists of a set of actions A, an initial state I
and a goal G, where I and G are sets of fluents.
Notation: If A is a set of action-instances, then Events(A) is the union of the sets Events(a) (for all
action-instances a  A).
Definition 3.2. P = <A,>, where A is a finite set of action-instances {a1,..., an} and  is a real-valued
function on Events(A), is a temporal plan for the problem <I, A, G> if
(1) A  A, and
(2) P satisfies the inherent and contradictory-effect constraints on A;
and when P is executed (i.e. fluents are established or destroyed at the times given by ) starting from
the initial state I:
(3) for all ai  A, each f  Cond(ai) is true when it is required, and
(4) all goal fluents g  G are true at the end of the execution of P.
(5) P is robust under infinitesimal shifts in the starting times of actions.
452

fiMONOTONE TEMPORAL PLANNING

Events are instantaneous, whereas actions are not only durative but may also be of variable length.
Thus a temporal plan P does not schedule its action-instances directly but schedules all the events in
its action-instances.
Condition (5) in Definition 3.2 means that we disallow plans which require perfect synchronisation
between different actions. Fox, Long and Halsey (2004) show how this condition can be imposed
within PDDL2.1. We require that in all plans fluents are established strictly before the beginning of
the interval over which they are required. The only exception to this rule is when a fluent f is established and required by the same action a. We allow the possibility of perfect synchronization within an
action, which means that we can have (a  f ) = ( f | a). Similarly, fluents can only be destroyed
strictly after the end of the interval over which they are required. The only exception to this rule is
when a fluent f is required and destroyed by an action a, in which case we can have ( f | a) =
(a  f ). For example, the fluent Empty(m) is simultaneously required and destroyed by the action
LOAD(m,c) shown in Figure 1.
Since a set of actions can be viewed as a set of action-instances in which each action occurs exactly
once, we can apply constraints, such as the inherent and contradictory-effects constraints, to a set of
actions rather then a set of action-instances. We now look in more detail at the type of constraints that
we impose on the relative times of events within an action-instance.
Definition 3.3. An interval constraint C(x,y) on real-valued variables x,y is a binary constraint of the
form xy  [a,b] where a,b are real constants.
Definition 3.4. (Jeavons & Cooper, 1995) A binary constraint C(x,y) is min-closed if for all pairs of
values (x1,y1), (x2,y2) which satisfy C, (min(x1,x2),min(y1,y2)) also satisfies C. A binary constraint
C(x,y) is max-closed if for all pairs of values (x1,y1), (x2,y2) which satisfy C, (max(x1,x2),max(y1,y2))
also satisfies C.
Lemma 3.5. Let A = {a1,..., an} be a set of actions and A a set of action-instances in which each action
ai (i =1,..., n) occurs ti1 times. Let  be a real-valued function on the set of events in A. For each
e  Events(ai), let e[ j] ( j =1,...,ti) represent the occurrence of event e within instance number j of ai.
For i  {1,...,n}, define the real-valued functions min, max on the set of events in the set of actions A by
min(e) = min{(e[ j]) | j =1,..., ti} and max(e) = max{(e[ j]) | j =1,..., ti}. If  satisfies the inherent constraints on A, then both min and max satisfy the inherent constraints on A.
Proof: All interval constraints are both min-closed and max-closed (Jeavons & Cooper, 1995). By
applying the definition of min-closedness (respectively, max-closedness) ti 1 times, for each action ai,
we can deduce that if  satisfies an interval constraint on each of the ti instances of ai, then min (max)
satisfies this constraint on the action ai. In other words, for all pairs of events e1, e2 in Events(ai), if
(e1[ j])  (e2[ j])  [a(e1, e2), a(e1, e2)] for j=1,...,ti, then min(e1)  min(e2)  [a(e1, e2), a(e1, e2)] and
max(e1)  max(e2)  [a(e1, e2), a(e1, e2)]. Hence if  satisfies the inherent constraints on A, then min
and max satisfy the inherent constraints on A.

Definition 3.6. A temporal planning problem <I,A,G> is positive if there are no negative fluents in the
conditions of actions nor in the goal G.
453

fiCOOPER, MARIS, & REGNIER

In this paper, we will only consider positive temporal planning problems <I,A,G>. It is well known
that any planning problem can be transformed into an equivalent positive problem in linear time by
the introduction, for each positive fluent f, of a new fluent notf to replace occurrences of f in conditions of actions (Ghallab, Nau & Traverso, 2004). It is important to note, however, that this transformation may not conserve other properties of the instance. By the assumption that all problems are positive, G and Cond(a) (for any action a) are composed of positive fluents. By convention, Add(a) and
Del(a) are also composed exclusively of positive fluents. The initial state I, however, may contain
negative fluents.
For simplicity of presentation, we assume throughout this paper that the set of actions A has undergone the filtering operation consisting of eliminating those actions a from A which cannot possibly be
executed since Cond(a) is not a subset of I  Add(A).
We will need the following notion of establisher-uniqueness in order to define our tractable class
of temporal planning problems. This is equivalent to post-uniqueness in SAS+ planning (Jonsson &
Bckstrm, 1998) restricted to Boolean variables but specialised so that it applies to a specific subset
of the positive fluents. In the next section, we apply it to the subset of positive fluents which may be
required for the realisation of the goal.
Definition 3.7. A set of actions A = {a1,...,an} is establisher-unique (EU) relative to a set of positive
fluents S if for all i  j, Add(ai)  Add(aj)  S = , i.e. no fluent of S can be established by two distinct actions of A.
If a set of actions is establisher-unique relative to the set of sub-goals of a problem, then we can determine in polynomial time the set of actions which are necessarily present in a temporal plan. There
remains the problem of determining how many times each action must occur and then scheduling these action-instances in order to produce a valid temporal plan. Establisher-uniqueness alone cannot
prevent minimal plans from being of exponential size (Bckstrm & Klein, 1991b).
4. Monotone Planning
In this section, we introduce the notion of monotonicity of fluents. Together with establisheruniqueness, the monotonicity of fluents is a sufficient condition for the existence of a polynomial-time
algorithm for temporal planning.
Definition 4.1. A fluent f is monotone (relative to a positive temporal planning problem <I,A,G>) if,
after being destroyed f is never re-established in any temporal plan for <I,A,G>. A fluent f is
+monotone (relative to <I,A,G>) if, after having been established f is never destroyed in any temporal
plan for <I,A,G>. A fluent is monotone (relative to <I,A,G>) if it is either + or monotone (relative to
<I,A,G>).
Example 4.2: Consider the two actions shown in Figure 2: LIGHT-MATCH and LIGHT-CANDLE.
The action LIGHT-MATCH requires that the match be live, in order to light it. The match remains lit
until it is blown out at the end of the action. A constraint in Constr(LIGHT-MATCH) imposes that the
duration of the action, i.e. (LIGHT-MATCH  Match-lit)  (LIGHT-MATCH  Match-lit), is
between 1 and 10 time units. The second action LIGHT-CANDLE requires that the match be lit dur454

fiMONOTONE TEMPORAL PLANNING

ing two time units for the candle to be lit. For an initial state I = {live, Match-lit} and a set of goals
G = {Candle-lit}, it is clear that all temporal plans for this problem involve executing the two actions
in parallel with the start (respectively, end) of LIGHT-MATCH being strictly before (after) the start
(end) of LIGHT-CANDLE. There is only one match available, which means that LIGHT-MATCH
can be executed at most once. This means that the fluent Match-lit is monotone since it cannot be
established after being destroyed. This same fluent Match-lit is not +monotone since it is destroyed
after being established.
Live

Match-lit
LIGHT-MATCH

Live

LIGHT-CANDLE[2]
Candle-lit

Match-lit

Match-lit

Figure 2: An example of a set of actions which allows us to light a candle using a single match.

Notation: If A is a set of actions, we use the notation Del(A) to represent the union of the sets Del(a)
(for all actions a  A). Add(A), Cond(A), Constr(A) are defined similarly.
The following lemma follows trivially from Definition 4.1.
Lemma 4.3. If f  Add(A)  Del(A), then f is both monotone and +monotone relative to the positive
temporal planning problem <I,A,G>.
Certain physical actions or chemical reactions are irreversible. Examples include bursting a balloon, killing a fly, adding milk to a cup of coffee or burning fuel. Since there can be no action to destroy the corresponding fluents Burst, Fly-dead, Milk-added, Fuel-burnt, these fluents are necessarily
both monotone and +monotone by Lemma 4.3. A similar remark holds for fluents that may be true in
the initial state but for which there is no action which establishes them, such as Fly-alive, for example.
In Example 4.2, the fluent Live is both monotone and +monotone since there is no action to establish it, and the fluent Candle-lit is monotone and +monotone since there is no action to destroy it.
We now introduce three other sets of constraints, the authorisation constraints being applied to
monotone fluents f and the +authorisation constraints to +monotone fluents. The causality constraints
on fluent f are only valid if there is a unique action-instance which establishes f.

authorisation constraints on the positive fluent f and the set of action-instances A: for all
ai  aj A, if f  Del(aj)  Cond(ai), then ( f | ai) < (aj  f ); for all ai  A, if f  Del(ai) 
Cond(ai), then ( f | ai)  (ai  f ).
+authorisation constraints on the positive fluent f and the set of action-instances A: for all ai,aj  A,
if f  Del(aj)  Add(ai)  (Cond(A)  G), then (aj  f ) < (ai  f ).
causality constraints on the positive fluent f and the set of action-instances A: for all ai  aj  A, if
f(Cond(aj)  Add(ai))\I, then (ai  f ) < ( f | aj); for all ai  A, if f  (Cond(ai)  Add(ai))\ I then
(ai  f )  ( f | ai).
455

fiCOOPER, MARIS, & REGNIER

Within the same action-instance ai, perfect synchronisation is possible between the events f | ai
and ai  f. Indeed, one way of ensuring that an action a is executed at most once in any temporal
plan is to create a fluent fa  Cond(a)  Del(a)  I which is simultaneously required and deleted at
the start of a and which is established by no action. For example, when a is the action LIGHTMATCH in Example 4.2, fa is the fluent live. On the other hand, by condition (5) of Definition 3.2 of a
temporal plan, we cannot have perfect synchronisation between events in distinct action-instances.
This explains why the authorisation constraints impose the strict inequality ( f | ai) < (aj  f )
when ai  aj but only the non-strict inequality ( f | ai)  (aj  f ) when ai = aj. A similar remark
holds for the perfect synchronisation of the events ai  f and f | aj which is only permitted by the
causality constraints when ai = aj.
Definition 4.4. A temporal plan <A,> for a positive temporal planning problem <I,A,G> is monotone
if each pair of action-instances (in A) satisfies the +authorisation constraints for all +monotone fluents
and satisfies the authorisation constraints for all monotone fluents.
Definition 4.5. Given a temporal planning problem <I,A,G>, the set of sub-goals is the minimum subset SG of Cond(A)  G satisfying
1. G  SG
2. for all a  A, if Add(a)  (SG \ I)  , then Cond(a)  SG.
The reduced set of actions is Ar = {a  A | Add(a)  (SG \ I)  }.
We can determine SG and then Ar in polynomial time and the result is unique. To see this consider
the simple algorithm which initialises SG to G and then repeatedly adds to SG the set of fluents F
which is the union of (Cond(a) \ SG) over all actions a  A such that Add(a)  (SG \ I)  , until
F=. This simple algorithm has worst-case time complexity O(n3), where n is the total number of
events in the actions of A, and produces a unique result which is clearly the minimum set of fluents
satisfying the two conditions of Definition 4.5. Note that this algorithm is similar to the standard
method of relevance detection used in GRAPHPLAN (Blum & Furst, 1995).
In order to state our theorem, we require a more relaxed definition of the set of sub-goals and the
reduced set of actions to take into account the case in which fluents in the initial state are destroyed
and re-established. Let SG p (the set of possible sub-goals) denote the minimal set of fluents satisfying
1. G  SG p
2. for all actions a  A, if Add(a)  SG p   then Cond(a)  SG p.
Let Ap be the set of actions { a  A | Add(a)  SG p   }. The difference between Ar and Ap is that
A is the set of actions which could occur in a minimal temporal plan in which fluents in the initial
state can be destroyed and re-established. As with SG and Ar, SG p and Ap are unique and can be determined in O(n3) time.
p

If each fluent in Cond(Ar)  G is monotone, we say that a plan P for the temporal planning problem <I,A,G> satisfies the authorisation constraints if each monotone fluent satisfies the authorisation constraints and each +monotone fluent satisfies the +authorisation constraints (it is assumed that
we know, for each fluent f  Cond(Ar)  G, whether f is + or monotone).
456

fiMONOTONE TEMPORAL PLANNING

The following theorem contains minor improvements and corrections compared to the conference
version of the present paper (Cooper, Maris & Rgnier, 2012). Since it is a corollary of Theorem 5.6
(proved in the following section), we omit its proof.
Theorem 4.6. Given a positive temporal planning problem <I,A,G>, let SG and Ar be, respectively,
the set of sub-goals and the reduced set of actions, with Ap defined as above. Suppose that Constr(Ar)
are interval constraints, the set of actions Ar is establisher-unique relative to SG \ I, each fluent in
Cond(Ar)  G is monotone relative to <I,Ar,G> and each fluent in I  (Cond(Ar)  G) is monotone
relative to <I,Ap,G>. Then <I,A,G> has a temporal plan P if and only if
(1) G  (I \ Del(Ar))  Add(Ar)
(2) Cond(Ar)  I  Add(Ar)
(3) all fluents g  G  Del(Ar)  Add(Ar) are +monotone relative to <I,Ar,G>
(4) the set of authorisation, inherent, contradictory-effects and causality constraints has a solution
over the set of actions Ar.
5. Extending Monotonicity of Fluents
In this section we introduce the notion of monotonicity*, thus allowing us to define a larger tractable class of temporal planning problems.
Definition 5.1. A plan is minimal if removing any non-empty subset of action-instances produces an
invalid plan. A fluent f is monotone* (relative to a positive temporal planning problem <I,A,G>) if,
after being destroyed f is never re-established in any minimal temporal plan for <I,A,G>. A fluent f is
+monotone* (relative to <I,A,G>) if, after having been established f is never destroyed in any minimal
temporal plan for <I,A,G>. A fluent is monotone* (relative to <I,A,G>) if it is either + or monotone*
(relative to <I,A,G>).
Example 5.2. To give an example of a monotone* fluent which is not monotone, consider the following planning problem in which all actions are instantaneous:
Start_vehicle: k  o
Drive: o  d, o
Unload: d  p
with I = {k}, G = {p}. The fluents represent that I have they ignition key (k), the engine is on (o), the
destination has been reached (d) and that the package has been delivered (p). There is only one minimal plan, namely Start_vehicle, Drive, Unload, but there is also the non-minimal plan Start_vehicle,
Drive, Start_vehicle, Unload in which the fluent o is established, destroyed and then re-established.
Hence o is monotone* but not monotone.
A +monotone (monotone) fluent is clearly +monotone* (monotone*) since in any plan, including minimal plans, after having been established (destroyed) it is never destroyed (re-established). In
order to prove the equivalent of Theorem 4.6 for monotone* fluents, we first require another definition
and two minor technical results.
457

fiCOOPER, MARIS, & REGNIER

Definition 5.3. A minimal temporal plan <A,> for a positive temporal planning problem <I,A,G> is
monotone* if each pair of action-instances in A satisfies the +authorisation constraints for all
+monotone* fluents and satisfies the authorisation constraints for all monotone* fluents.
The following lemma follows directly from Definition 5.1 of the monotonicity* of a fluent along
with the fact that a fluent cannot be simultaneously established and destroyed in a temporal plan.
Lemma 5.4. Suppose that the positive fluent f is monotone* relative to a positive temporal planning
problem <I,A,G>. Let <A,> be a minimal temporal plan for <I,A,G> with actions ai, aj  A such that
f  Add(ai)  Del(aj). If f is +monotone* relative to this problem, then (aj  f ) < (ai  f ). If f is
monotone* relative to this problem, then (ai  f ) < (aj  f ).
Proposition 5.5. If each fluent in Cond(A) is monotone* relative to a positive temporal planning problem <I,A,G>, then all minimal temporal plans for <I,A,G> are monotone*.
Proof: Let P be a minimal temporal plan. Consider firstly a positive monotone* fluent f. We have to
show that the authorisation constraints are satisfied for f in P, i.e. that f is not destroyed before (or at
the same time as) it is required in P. But this must be the case since P is a plan and f cannot be reestablished once it is destroyed. Consider secondly a positive +monotone* fluent f. By Lemma 5.4, the
+authorisation constraint is satisfied for f in P.

We can now give our main theorem which generalizes Theorem 4.6 to monotone* fluents.
Theorem 5.6. Given a positive temporal planning problem <I,A,G>, let SG and Ar be, respectively,
the set of sub-goals and the reduced set of actions. Suppose that all constraints in Constr(Ar) are interval constraints, the set of actions Ar is establisher-unique relative to SG \ I, each fluent in Cond(Ar)  G
is monotone* relative to <I,Ar,G> and each fluent in I  (Cond(Ar)  G) is monotone* relative to
<I,Ap,G>. Then <I,A,G> has a temporal plan P if and only if
(1) G  (I \ Del(Ar))  Add(Ar)
(2) Cond(Ar)  I  Add(Ar)
(3) all fluents g  G  Del(Ar)  Add(Ar) are +monotone* relative to <I,Ar,G>
(4) the set of authorisation, inherent, contradictory-effects and causality constraints (given in Sections 3 and 4) has a solution  over the set of actions Ar (where the +authorisation constraints apply to
each +monotone* fluent and the authorisation constraints apply to each monotone* fluent).
Proof: () If <I,A,G> has a temporal plan, then it clearly has a minimal plan P. Ar is the set of those
actions which establish sub-goals f  SG \ I. By definition, SG = Cond(Ar)  G. Since Ar is establisherunique relative to SG \ I, each sub-goal f  SG \ I has a unique action which establishes it. Hence each
action in Ar must occur in the plan P. Furthermore, (Add(A) \ Add(Ar))  (Cond(Ar) \ I) =  by Definition 4.5. It follows that (2) is a necessary condition for a temporal plan P to exist.
Let P p be a version of P in which we only keep actions in Ap. P p is a valid temporal plan since,
by definition of Ap, no fluent in (Cond(Ap)  G) can be established by actions in A \ Ap. Indeed, since P
was assumed to be minimal, we must have P p =P. Now let P be a version of P in which we only keep
actions in Ar. By Definition 4.5, no conditions of actions in P and no goals in G are established by any
458

fiMONOTONE TEMPORAL PLANNING

of the actions eliminated from P, except possibly if they are also in I. Thus to show that P is also a
valid temporal plan we only need to show that any establishment of a fluent f  I  (Cond(Ar)  G) in
P by an action a  Ap \ Ar is unnecessary. By hypothesis, f is monotone* relative to <I,Ap,G> and
hence f cannot be established in P after having been destroyed. Since f  I, this means that the establishment of f in P was unnecessary. Hence P is a valid temporal plan. Indeed, since P was assumed to
be minimal, we must have P=P.
We have seen that P contains exactly the actions in Ar. Hence, all fluents gG (which are necessarily positive by our hypothesis of a positive planning problem) that are either not present in the initial state I or are deleted by an action in Ar must be established by an action in Ar. It follows that (1) is
a necessary condition for P to be a valid temporal plan. Consider g  G  Del(Ar)  Add(Ar). From
Lemma 5.4, we can deduce that g cannot be monotone*, since g is true at the end of the execution of
P. Thus (3) holds. Let Pmin=<Ar,min> be the version of the temporal plan P=<Ar,> in which we only
keep one instance of each action ai  Ar (and no instances of the actions in A \ Ar) and min is defined
from  by taking the first instance of each event in Events(ai), for each action ai  Ar, as described in
the statement of Lemma 3.5. We will show that Pmin satisfies the authorisation, inherent, contradictory-effects and causality constraints.
We know that P is a temporal plan for the problem <I,A,G>. Hence it is also a temporal plan for
the problem <I,Ar,G>, since it uses only actions from Ar. By hypothesis, all fluents in Cond(Ar) are
monotone relative to <I,Ar,G>. Therefore, by Proposition 5.5, the temporal plan P is monotone*. Since
P is monotone* and by the definition of a temporal plan, the authorisation constraints are all satisfied.
P must also, by definition of a temporal plan, satisfy the inherent and contradictory-effects constraints.
It follows from Lemma 3.5 that Pmin also satisfies the inherent constraints. Since the events in Pmin are
simply a subset of the events in P, Pmin necessarily satisfies both the authorisation constraints and the
contradictory-effects constraints.
Consider a positive fluent f  (Cond(aj)  Add(ai)) \ I, where ai, aj  Ar. Since aj  Ar, we know
that Add(aj)  (SG \ I)   and hence that Cond(aj)  SG, by the definition of the set of sub-goals SG.
Since f  Cond(aj) we can deduce that f  SG. In fact, f  SG \ I since we assume that f  I. It follows
that if f  Add(a) for some a  A, then a  Ar. But we know that Ar is establisher-unique (relative to
SG). Hence, since f  Cond(aj)  Cond(Ar) and f  Add(ai), f can be established by the single action
a=ai in A. Since f  I, the first establishment of f by an instance of ai must occur in P before f is first
required by any instance of aj. It follows that the causality constraint must be satisfied by f in Pmin.
() Suppose that conditions (1)-(4) are satisfied by Ar. Let P be a solution to the set of authorisation,
inherent, contradictory-effects and causality constraints over Ar. A solution to these constraints uses
each action in Ar (in fact, it uses each action exactly once since it assigns one start time to each action
in Ar). Consider any g  G. By (1), g  (I \ Del(Ar))  Add(Ar). If g  Del(Ar), then g is necessarily
true at the end of the execution of P. On the other hand, if g  Del(aj) for some action aj  Ar, then by
(1) there is necessarily some action ai  Ar which establishes g. Then, by (3) g is +monotone*. Since P
satisfies the +authorisation constraint for g, ai establishes g after all deletions of g. It follows that g is
true at the end of the execution of P.
Consider some monotone* f  Cond(aj) where aj  Ar. Since the authorisation constraint is
satisfied for f in P, f can only be deleted in P after it is required by aj. Therefore, it only remains to
459

fiCOOPER, MARIS, & REGNIER

show that f was either true in the initial state I or it was established some time before it is required by
aj. By (2), f  I  Add(Ar), so we only need to consider the case in which f  I but f  Add(ai) for
some action ai  Ar. Since P satisfies the causality constraint, (ai  f ) < ( f | aj) and hence, during
the execution of P, f is true when it is required by action aj.
Consider some f  Cond(aj), where aj  Ar, such that f is not monotone*. By the assumptions of the theorem, f is necessarily +monotone* and f  I. First, consider the case f  Del(Ar) 
Add(Ar). By Lemma 4.3, f is monotone (and hence monotone*) which contradicts our assumption.
Therefore f  Del(ak)  Add(ai), for some ai, ak  Ar, and recall that f  I. Since the +authorisation
constraint is satisfied for f in P, any destruction of f occurs before f is established by ai. It then follows
from the causality constraint that the condition f will be true when required by aj during the execution
of P.


The makespan of a temporal plan P = <A,> is the time interval between the first and last events of
P, i.e. max{(e) | eEvents(A)}  min{(e) | eEvents(A)}. The problem of finding a plan with minimum makespan is polytime approximable if there is a polynomial-time algorithm which, given a
temporal planning problem <I,A,G> and any  > 0, finds a temporal plan whose makespan is no more
than Mopt + , where Mopt is the minimum makespan of all temporal plans for <I,A,G>.
If the constraints in Constr(a) impose that the time interval between each pair of events in
Events(a) is fixed, then we say that action a is rigid.
We express complexities in terms of the total number n of events in the actions in A. Without loss
of generality, we assume that all actions in A contain at least one event and all fluents occur in at least
one event. Hence the number of actions and fluents are both bounded above by n.
Theorem 5.7: Let EUM* be the class of positive temporal planning problems <I,A,G> in which A is
establisher-unique relative to Cond(A)  G, all fluents in Cond(A)  G are monotone* relative to
<I,A,G> and all fluents in I  (Cond(A)  G) are monotone* relative to <I,A,G>. Then EUM* can be
solved in time O(n3) and space O(n2), where n is the total number of events in the actions in A. Indeed,
we can even find a temporal plan with the minimum number of action-instances or of minimal cost, if
each action has an associated non-negative cost, in the same complexity. Furthermore, if all actions in
A are rigid then the problem of finding a plan with minimum makespan is polytime approximable.
Proof: The fact that EUM* can be solved in time O(n3) and space O(n2) follows almost directly from
Theorem 5.6 and the fact that the set of authorisation, inherent, contradictory-effects and causality
constraints form an STP, a simple temporal problem with difference constraints (Koubarakis, 1992).
An instance of STP can be solved in O(n3+k) time and O(n2+k) space (Gerevini & Cristani, 1997),
where n is the number of variables and k the number of difference constraints (i.e. constraints of the
form xj  xi  d). Here, the only difference constraints are the contradictory-effects constraints of
which there are at most n2, so k=O(n2). Furthermore, as pointed out in Section 4, the calculation of SG
and Ar is O(n3).
Establisher-uniqueness tells us exactly which actions must belong to minimal temporal plans. Then,
as we have seen in the proof of Theorem 5.6, the monotonicity* assumptions imply that we only need
one instance of each of these actions. It then trivially follows that we solve the optimal version of the
temporal planning problem, in which the aim is to find a temporal plan with the minimum number of
460

fiMONOTONE TEMPORAL PLANNING

action-instances or of minimal cost, if each action has an associated cost, by solving the set of authorisation, inherent, contradictory-effects and causality constraints.
Now suppose that all actions in A are rigid. We will express the problem of minimising makespan
while ignoring the contradictory-effects constraints as a linear program. We will then show that it is
always possible to satisfy the contradictory-effects constraints (without violating the other constraints)
by making arbitrarily small perturbations to the start times of actions. We assume that the events in a
single action-instance satisfy the contradictory-effects and authorisation constraints; since actions are
rigid this can be checked independently for each action in A in polynomial time. Let P be a temporal
plan for <I,A,G> which has minimum makespan. We showed in the proof of Theorem 5.6 that Pmin,
obtained from P by keeping only one instance of each action in P, is also a valid temporal plan. Since
makespan cannot be increased by eliminating action-instances from a temporal plan, Pmin also minimises makespan. Let C be the set of inherent, authorisation and causality constraints that Pmin must
satisfy. C contains equality constraints of the form xj  xi = d, where d is a constant and xj, xi are times
of events in the same action, and constraints of the form xj  xi < d, where d is a constant and xj, xi are
times of events in different actions. We introduce two other variables begin, end and we denote by Copt
the set of constraints C together with the constraints begin  (e) and (e)  end for all eEvents(Ar).
The linear program which minimises end  begin subject to the constraints Copt minimises makespan
but does not take into account the contradictory-effects constraints C that must be satisfied by a valid
temporal plan. Let PLP be a solution to this linear program. Its makespan is clearly no greater than
Mopt, the minimum makespan of all temporal plans (since valid temporal plans must satisfy the constraints C  C ). Let  be the minimum difference d  (xj  xi) in PLP over all constraints of the form
xj  xi < d in C. Let  be the minimum non-zero difference | d  (xj  xi) | in PLP over all contradictoryeffects constraints xj  xi  d in C . Suppose that Ar = {a1,...,am}. Let P be identical to PLP except that
we add i = min{,,}(i1)/m to (e) for all eEvents(ai). By construction, all contradictory-effects
constraints which were violated in PLP are satisfied in P, all contradictory-effects constraints which
were satisfied in PLP are still satisfied in P, and all strict inequalities which were satisfied in PLP are
still satisfied in P. The inequalities begin  (e) are still satisfied in P. Finally, in order to guarantee
satisfying the inequalities (e)  end, it suffices to add  to end. The resulting solution P corresponds to
a valid temporal plan whose makespan is no more than Mopt + . The result then follows from the fact
that linear programming is solvable in O(n3.5L) time by Karmarkars interior-point algorithm, where n
is the number of variables and L the number of bits required to encode the problem (Karmarkar,
1984).







f

f

a1

a1

a1
g

a2

g

h

h

a2

f

a2

f

(a)

g

f

f

(b)

g

(c)

Figure 3: (a) all instances of action a1 occur strictly before all instances of action a2, (b) all instances of
a2 are contained in all instances of a1 (c) all instances of a1 overlap all instances of a2.
461

fiCOOPER, MARIS, & REGNIER

In the class EUM*, all sub-goal fluents f  SG are true over a single interval Int(f, P) during the execution of a temporal plan P. If f is +monotone*, then Int(f, P) is necessarily of the form [t1,) where t1
is the moment when f is first established. If f is monotone*, then Int(f, P) is necessarily of the form
[t1, t2] where t1 is again the moment when f is first established (or 0 if f  I) and t2 is the first moment
after t1 when f is destroyed (or [t1,) if f is never destroyed). The class EUM* is solvable in polynomial
time due to the fact that establisher-uniqueness ensures that there is no choice concerning which actions to include in the plan and monotonicity* ensures that the only choice concerning the time of
events is within an interval. Given these two restrictions it is quite surprising that a large range of industrial planning problems fall in this class (Cooper, Maris & Rgnier, 2012, 2013b). EU monotone
planning is a sufficiently powerful modelling language to allow us to impose constraints such as an
action occurs at most once in a plan or that all instances of event e1 occur before all instances of event
e2. To illustrate this, Figure 3 shows how we can impose precedence, containment or overlapping constraints between actions a1 and a2 by the introduction of, respectively, one, two or three fluents
f, g, h  I which occur only in the events shown in Figure 3. By Lemma 4.3, these fluents f, g, h are all
necessarily both monotone and +monotone in all temporal plans.
6. Temporal Relaxation
Relaxation is ubiquitous in Artificial Intelligence. A valid relaxation of an instance I has a solution
if I has a solution. Hence when the relaxation has no solution, this implies the unsolvability of the
original instance I. A tractable relaxation can be built and solved in polynomial time.
The traditional relaxation of propositional non-temporal planning problems consisting of ignoring
deletes has two drawbacks. Firstly, it is traditionally used with a forward search, which is not valid in
temporal planning unless some specific transformation has been applied beforehand to the set of actions (Cooper, Maris & Rgnier, 2013a). Secondly, it does not use information which may be essential
for the detection of unsolvability of the original instance, namely the destruction of fluents and temporal information such as the relative duration of actions. In this section we present a valid tractable
relaxation inspired by EU monotone temporal planning. In the following section we show how to use
our temporal relaxation to detect monotonicity* of fluents. There are other possible applications, such
as the detection of action landmarks (actions which occur in each solution plan) (Karpas & Domshlak,
2009), which immediately leads to a lower bound on the cost of a plan when each action has an associated cost (Cooper, de Roquemaurel & Rgnier, 2011).
The traditional relaxation of classical non-temporal planning problems consists of ignoring deletes
of actions. By finding the cost of an optimal relaxed plan, this relaxation can be used to calculate the
admissible h+ heuristic. As shown by Betz and Helmert (2009), h+ is very informative but unfortunately NP-hard to compute (Bylander, 1994) and also hard to approximate (Betz & Helmert, 2009).
As this relaxation does not use information which may be essential for the detection of un-solvability
of the original instance (namely the destruction of fluents), a lot of research has been carried out to
take some deletes into account (Fox & Long, 2001; Gerevini, Saetti & Serina, 2003; Helmert, 2004;
Helmert & Geffner, 2008; Keyder & Geffner, 2008; Cai, Hoffmann & Helmert, 2009). Another recent
approach (Haslum, Slaney & Thibaux, 2012; Keyder, Hoffmann & Haslum, 2012) consists in enriching the classical relaxation with a set of fact conjunctions. Finally the red-black relaxation (Katz,
Hoffmann & Domshlak, 2013a, 2013b) generalizes delete-relaxed planning by relaxing only a subset
of the state variables.
462

fiMONOTONE TEMPORAL PLANNING

Unfortunately, these relaxations do not directly generalize to temporal planning, since techniques
based on a combination of ignoring deletes and forward search are not valid in temporal planning unless some specific transformation has been applied beforehand to the set of actions. An important aspect of temporal planning, which is absent from non-temporal planning, is that certain temporal planning problems, known as temporally-expressive problems, require concurrency of actions in order to
be solved (Cushing, Kambhampati, Mausam & Weld, 2007). A typical example of a temporallyexpressive problem is cooking: several ingredients must be cooked simultaneously in order to be
ready at the same moment. In a previous paper (Cooper, Maris & Rgnier, 2010), we identified a subclass of temporally expressive problems, known as temporally-cyclic, which require cyclicallydependent sets of actions in order to be solved.
Wages-paid

PAY(1)

WORK(10)
Job-started

Job-started

Job-finished

Wages-paid

Figure 4: An example of a temporally cyclic temporal planning problem.
A simple temporally-cyclic problem is shown in Figure 4, where I =  and G = {Job-finished}. A
condition for a workman to start work is that he is paid (at the end of the job) whereas his employer
will only pay him after he has started work. In a valid temporal plan for this problem the actions PAY
and WORK must be executed in parallel with the execution of action PAY contained within the interval over which action WORK is executed. After applying the traditional ignore-deletes relaxation,
forward chaining from the initial state would not be able to start either of the two actions since both of
them have a missing condition. Thus, certain proposed techniques, although very useful in guiding
heuristic search (Eyerich, Mattmller & Rger, 2009; Do & Kambhampati, 2003), are not valid for
temporally cyclic problems. Different solutions exist to get round the problem of temporal cycles. For
example, we gave a polynomial-time algorithm to transform a temporally-cyclic problem into an
equivalent acyclic one (Cooper, Maris & Rgnier, 2013a). Other transformations have been proposed
in the literature (Long & Fox, 2003; Coles, Fox, Long & Smith, 2008) which also eliminate the possibility of temporal cycles, although this was not an explicitly-stated aim in the descriptions of these
transformations: temporal cycles are avoided by decomposing durative actions into instantaneous actions denoting the start and end of the action. Intermediate conditions can also be managed by splitting
actions into component actions enclosed within an envelope action (Smith, 2003). In each case, ignoring deletes in the transformed problem followed by forward search provides a valid relaxation. If
the original problem is not temporarily cyclic, then ignoring deletes followed by forward search is a
valid relaxation.
In this section, we present an alternative form of relaxation, which we call TR (for Temporal Relaxation), inspired by EU monotone planning, comprising an STP instance which has a solution only
if the original temporal planning instance has a solution. It is incomparable with the relaxation based
on ignoring deletes, as we will show through temporal and non-temporal examples, in the sense that
there are instances that can be detected as unsolvable using EU monotone relaxation but not by ignoring deletes (and vice versa).
463

fiCOOPER, MARIS, & REGNIER

By applying the following simple rule until convergence we can transform (in polynomial time)
any temporal planning problem P into a relaxed version P which is EU relative to the set of sub-goals
SG: if a sub-goal fluent f is established by two distinct actions, then delete f from the goal G and from
Cond(a) for all actions a. As a consequence, f is no longer a sub-goal and SG has to be recalculated.
Clearly, P is a valid relaxation of P. From now on we assume the temporal planning problem is EU
relative to SG.
We denote by ALM the set of action landmarks that have been detected (Karpas & Domshlak,
2009). Action landmarks are also known as indispensable actions (Cooper, de Roquemaurel & Rgnier, 2011). Establisher-uniqueness implies that we can easily identify many such actions, in particular the set of actions Ar which establish sub-goals not present in the initial state I.
We cannot assume in the STP that a single instance of each action will be sufficient. For each action landmark a and for each event e  Events(a), we introduce two variables first(e), last(e) representing the times of the first and last occurrences of event e in the plan. The constraints of our temporal
relaxation TR include versions of the internal, contradictory-effects, authorization and causality constraints (which we give below) together with the following obvious constraint:
intrinsic TR-constraints: aALM, for all events e  Events(a), first(e)  last(e).
In the conference version of this paper in which we described a preliminary version of TR (Cooper,
Maris & Rgnier 2013b), we made the assumption that no two instances of the same action can overlap. Under this assumption, for e1, e2  Events(a), the first occurrences of e1, e2 in a plan correspond to
the same instance of action a. A similar remark holds for the last occurrences of e1, e2. It turns out that
we do not need to make this assumption in order to apply in TR each inherent constraint in Constr(a)
independently to the values of first(e) and last(e) (for each e  Events(a)). Indeed, according to Lemma 3.5, assuming that Constr(Ar) are interval constraints, if each instance of action a satisfies its inherent constraints, then both first and last satisfy the inherent constraints on events in action a:
inherent TR-constraints: aALM,  e1,e2  Events(a), first(e1)  first(e2)  [a(e1,e2), a(e1,e2)]
and last(e1)  last(e2)  [a(e1,e2), a(e1,e2)].
The contradictory-effects constraints in TR are as follows:
contradictory-effects TR-constraints: ai, aj  ALM, for all positive fluents f  Del(ai)  Add(aj),
L1,L2  {first,last}, L1(ai  f )  L2(aj  f ).
For each positive fluent f which is known to be monotone*, we apply in TR the following modified version of the authorisation constraints on f:

authorisation TR-constraints:  ai  aj  ALM, if f  Del(aj)  Cond(ai), then last( f | ai) <
first(aj  f ); for all ai  ALM, if f  Del(ai)  Cond(ai), then last( f | ai)  first(ai  f ).
For each positive fluent f which is known to be +monotone*, we apply in TR the following modified
version of the +authorisation constraints on f:
+authorisation TR-constraints: ai, aj  ALM, if f  Del(aj)  Add(ai), then last(aj  f ) <
first(ai  f).
We check that every condition and every goal can be established, i.e. Cond(ALM)  I  Add(A) and
G  (I \ Del(ALM))  Add(A). If not, we consider that the relaxation TR has no solution.
464

fiMONOTONE TEMPORAL PLANNING

We also apply in TR the following causality constraints for each positive fluent f:
causality TR-constraints:  ai  aj  ALM, if f  (Cond(aj)  Add(ai)) \ I then first(ai  f ) <
first(f | aj); for all ai  ALM, if f  (Cond(ai)  Add(ai)) \ I then first(ai  f )  first( f | ai).
We also apply the following goal constraints for each g  G:
goal TR-constraints: ai, aj  ALM, if g  Del(aj)  Add(ai), then last(aj  g) < last(ai  g).
Of course, these causality and goal constraints are necessary conditions for the existence of a plan only if ALM is EU relative to (Cond(ALM) \ I)  (G  Del(A)).
Definition 6.1: An action aA is unitary for a temporal planning problem <I,A,G> if each minimal
temporal plan for the <I,A,G> contains at most one instance of a.
If an action aALM is known to be unitary, then in TR, for each event e  Events(a), we replace the
two variables first(e), last(e) in the above constraints by a unique variable (e).
Thus TR consists in first eliminating from G and Cond(a) (for all a  A) all fluents established by
two distinct actions, then, after checking that Cond(ALM)  I  Add(A) and G  (I \ Del(ALM)) 
Add(A), solving the STP consisting of the intrinsic, inherent, contradictory-effects, authorisation,
+authorisation, causality and goal TR-constraints, given above.
TR is a valid relaxation since the constraints of TR must clearly be satisfied by any temporal plan.
Furthermore, if a plan exists then a minimal plan necessarily exists and in minimal plans there is at
most one instance of each unitary action. Under assumptions of establisher-uniqueness and monotonicity*, TR is in fact a solution procedure for the tractable class described in Theorem 5.7.
The temporal relaxation TR can be significantly strengthened by the prior identification of unitary
actions. We therefore present some lemmas which cover several simple but common cases in which
actions can be identified as unitary. We first give a lemma which allows us to simplify certain actions.
Lemma 6.2: Suppose that f  I and that f is monotone* in the positive temporal planning problem
<I,A,G>. Let A be identical to the set of actions A except that f has been deleted from Add(a) for each
action a. Then all minimal temporal plans for <I,A,G> are minimal temporal plans for <I,A,G> and
vice versa.
Proof: If P is a minimal temporal plan for <I,A,G>, then, since f is monotone*, it cannot be established after having been destroyed in P. It follows that all establishments of f in P are unnecessary
since they can only occur when f  I was already true. Hence, P is also a plan for <I,A,G>. It is also
minimal for <I,A,G> since all conditions and goals are identical in both problems. A minimal temporal plan for <I,A,G> is necessarily a temporal plan for <I,A,G>, since all conditions and goals are
positive, and is again necessarily minimal since all conditions and goals are identical in both problems.


We assume in the rest of the paper that in TR the set of actions A has been simplified as indicated in
Lemma 6.2.

465

fiCOOPER, MARIS, & REGNIER

Lemma 6.3: If f is monotone*, f  Cond(a), f  Del(a) and a simultaneously requires and destroys f
(i.e. Constr(a) contains the constraints ( f | a) = ( f | a) = (a  f )), then a is unitary.
Proof: Let P be a minimal temporal plan containing a and consider the instance of a in P which
first destroys f. By condition (5) of Definition 3.2 of a temporal plan, no two instances of a can be
synchronised so that they destroy f simultaneously, so this instance is unique. By monotonicity*,
f cannot later be established in P. Hence no other instance of a can require (and destroy) f after
this instant. It follows that the minimal temporal plan P can contain at most one instance of a.
Hence a is unitary.

Lemma 6.4: Let <I,A,G> be a positive temporal planning problem and aA an action such that a
is rigid or no two instances of a can overlap in a temporal plan for <I,A,G>. In each of the three
following cases a is unitary in <I,A,G>: (1) all fluents in Add(a) are monotone*, (2) Add(a) 
G\Cond(A), or (3) Add(a) = {h} for some fluent hG, where there is a unique action b such that h
 Cond(b), and furthermore b is unitary.
Proof: Let P be a minimal temporal plan for <I,A,G> containing action a. First, consider case (1)
in which all fluents in Add(a) are monotone*. Monotone* fluents never need to be established
more than once in a minimal plan. This is because in minimal plans, once a +monotone* fluent
has been established, it cannot be destroyed and, once a monotone* fluent has been established,
it can be destroyed but not established again. It follows that all but the first establishment of each
fluent in Add(a) by a is unnecessary in P. Whether a is rigid or no two instances of a can overlap
in P, all the first establishments of each of the fluents in Add(a) correspond to the same instance
of a. All other instances of a can thus be deleted from P without destroying its validity. Hence a
is unitary.
Now consider case (2), i.e. Add(a)  G\Cond(A). All but the last establishment of each fluent
in Add(a) by a is unnecessary in P, since no fluents in Add(a) are conditions of actions in A.
Whether a is rigid or no two instances of a can overlap in P, all the last establishments of each of
the fluents in Add(a) correspond to the same instance of a. All other instances of a can thus be
deleted from P without destroying its validity. Hence a is unitary.
Now consider case (3). Since b is unitary, the minimal plan P contains at most one instance of
b. Only the instance of a which last establishes h before it is required by the unique instance of b
can actually be necessary. All other instances of a can be deleted from P without destroying its
validity. Hence a is unitary.


It is often the case that no two instances of an action a can be executed in parallel, for example due
to limited resources. It is therefore quite common when modelling a temporal planning problem to
forbid that two instances of the same action a overlap. This can be achieved by introducing a fluent
f  Cond(A\{a})  Add(A\{a})  Del(A\{a})  G, adding f to I and placing the events f | a,
f | a and a  f at the beginning of a and the event a  f at the end of a. Alternatively, we can
place the event a  f at the beginning of a and the events f | a, f | a and a  f at the end of a,
in which case we do not need to have fI. In either case, we say that f is a non-overlap fluent for a.
We can now state a more general version of Lemma 6.4.
466

fiMONOTONE TEMPORAL PLANNING

Lemma 6.5: Let <I,A,G> be a positive temporal planning problem and aA an action such that f
is a non-overlap fluent for a. In each of the three following cases a is unitary in <I,A,G>: (1) all
fluents in Add(a)\{f} are monotone*, (2) Add(a)\{f}  G\Cond(A), or (3) Add(a)\{f} = {h} for
some fluent hG, where there is a unique action b such that h  Cond(b), and furthermore b is
unitary.
Proof: Let P be a minimal temporal plan for <I,A,G> containing action a. No two instances of a can
overlap in P. As in the proof of Lemma 6.4, we only need to keep a single instance of a: in case (1)
this is the first instance of a, in case (2) the last instance of a, and in case (3) the last instance of a before h is required by b. Hence a is unitary.

The temporal relaxation TR uses two types of information not used by the ignore-deletes relaxation:
the destruction of fluents and temporal information. We give two very simple examples to illustrate
this.
Example 6.6: The simplest possible example showing that TR can detect the unsolvability of a planning problem that cannot be detected by the ignore-deletes relaxation consists of an initial state
I = {f}, a goal G = {f, g} and a single action which simultaneously establishes g and destroys f. Unsolvability is detected by TR since the condition G  I \ Del(ALM)  Add(A) is not satisfied.
Example 6.7: Consider again the problem of lighting a candle using a single match described in Example 4.2. Suppose now that the match is very short and will only burn for at most two time units.
The problem is clearly establisher-unique. Furthermore, both actions belong to Ar and are hence landmarks. We can deduce that LIGHT-MATCH is unitary by Lemma 6.3 and that LIGHT-CANDLE is
unitary by Lemma 6.4 (case (2)). Thus, in TR there is a single variable (e) for each event
eEvents(A). As we have already seen, Match-lit is monotone, so TR contains the authorisation
constraint (match-lit | LIGHT-CANDLE) < (LIGHT-MATCH  match-lit). It also contains
the causality constraint (LIGHT-MATCH  match-lit) < (match-lit | LIGHT-CANDLE). The
two inherent constraints (LIGHT-MATCH  match-lit)  (LIGHT-MATCH  match-lit)  2
and (match-lit | LIGHT-CANDLE)  (match-lit | LIGHT-CANDLE) = 2 then provide a contradiction. No form of relaxation which does not take into account the duration of actions can detect
the unsolvability of this problem, since the identical problem with different durations given in Example 4.2 has a solution.
Example 6.8: We now give a generic example involving the choice between two alternatives in
which the temporal relaxation TR can detect unsolvable problems that cannot be detected by ignoring
all deletes. We can illustrate our generic example by a simple non-temporal planning problem P with
initial state I = {f}, goal G = {g,h} and the following two actions:
B: f  f, g
C: f  f, h
The fluents have many possible interpretations, including: f = I have a packet, g = I have sent the
packet to Destination1, h = I have sent the packet to Destination2. Clearly this problem has no solution, but this is not discovered by the ignore-deletes relaxation (which cannot take into account the fact
that I no longer have the packet once I have sent it somewhere).
467

fiCOOPER, MARIS, & REGNIER

To show that TR has no solution we give a proof for the general case in which ALM is EU, actions
B,C  ALM are instantaneous, f  (Cond(B)  Del(B)  Cond(C)  Del(C)  I)\Add(A), g  Add(B)
 (G\I) and h  Add(C)  (G\I). The fluent f is monotone by Lemma 4.3 since there is no action to
establish it. TR has no solution since we obtain the following contradiction by a sequence of authorisation, inherent, intrinsic, authorisation, inherent and intrinsic (respectively) constraints: last(f | B)
< first(C  f) = first(f | C)  last(f | C) < first(B  f) = first(f | B)  last(f | B).
We should point out that improved versions of the ignore-deletes relaxation which retain some information concerning deletes would also be able to detect the unsolvability of this simple problem.
For example, the transformation of Keyder, Hoffmann and Haslum (2012) can detect unsolvability by
introducing a special fluent representing the conjunction of g and h .
Example 6.9: We now show that the temporal relaxation TR can detect unsolvable problems which
are not necessarily establisher-unique. In this example, all actions are instantaneous and hence we present it in the form of a non-temporal planning problem P with initial state I = {j,m,d}, goal G = {g}
and the following three actions:
Buy: j, m  h, d, m
Sell: h  m, h
Mort2: d, h  m, d, g
We can interpret the fluents as follows: j = I have a job, m = I have money, d = I am debt-free, h = I
own a house, g = I have taken out a second mortgage. For example, the action Buy is possible only if I
have a job and money to put down a deposit on a house; the result is that I own a house but I am in
debt and no longer have money. The goal is to take out a second mortgage via the action Mort2.
This problem has no solution, but this fact is not detected by the standard relaxation consisting of
ignoring destructions of fluents. To set up TR, we first determine the action landmarks ALM = {Buy,
Mort2} easily identified as landmarks by the rules given by Cooper, de Roquemaurel and Rgnier
(2011) since they establish the sub-goals h and g, respectively, not present in the initial state. Observe
that ALM is EU relative to the set of sub-goals {g,h} and retains some destructions of fluents. By applying Lemma 6.3 to the fluent d (which is the monotone by Lemma 4.3), we can deduce that Mort2 is
unitary. Then we can deduce from Lemma 6.4 (case (3)) that Buy is unitary, since Add(Buy) = {h}
and Mort2 is the only action requiring h. Thus, in TR there is a single variable (e) for each event
eEvents(ALM). TR contains the following constraints: (d | Mort2) = (h | Mort2) by an internal
TR-constraint in Mort2; (Buy  h) = (Buy  d) by an internal TR-constraint in Buy; (Buy  h)
< (h | Mort2) by the causality TR-constraint on h; (d | Mort2) < (Buy  d) by the 
authorisation TR-constraint, since d is monotone. This set of four constraints has no solution, from
which we can deduce that P has no solution. This example shows that temporal relaxation can be useful even in non-temporal planning problems which are not establisher-unique.
The above examples show that EU monotone relaxation TR can be stronger than the relaxation
based on ignoring deletes for two reasons: TR uses temporal information, for example concerning the
duration of actions, and retains destructions of fluents. To see that ignoring deletes can be stronger
than EU monotone relaxation, consider a problem in which the unique goal g is produced by a unique
action a such that Cond(a) = {f} where the fluent f is produced by two distinct actions b and c. In the
EU monotone relaxation, the fluent f is deleted from Cond(a), since it is established by two distinct
actions, and the relaxed version of the problem is immediately solvable by a plan containing the single
468

fiMONOTONE TEMPORAL PLANNING

action a. Ignoring deletes, on the other hand, can detect the unsolvability of the original problem in
certain cases, for example, if actions b and c are instantaneous, b is the only action that establishes
some fluent p  Cond(c) \ I and c is the only action that establishes some fluent q  Cond(b) \ I.
An obvious application of temporal relaxation is the detection of action landmarks by the following
classic technique which applies to any valid relaxation (Hoffmann, Porteous & Sebastia, 2004;
Cooper, de Roquemaurel & Rgnier, 2011). Let P [-a] represent the planning problem P without a
particular action a. If the temporal relaxation of P [-a] has no solution, then we can conclude that a is
an action landmark for P.
In the following sections we investigate other applications of temporal relaxation concerning the
detection of different forms of monotonicity. The basic idea is that if H is a hypothesis to be tested and
H can be expressed as the conjunction of STP constraints, then we can add H to the constraints of the
temporal relaxation TR. We thus obtain an STP instance which we denote by TR[H]: if TR[H] has no
solution then H cannot be true in any solution to the planning problem. In each case, the complexity of
solving TR[H] is O(n3) time and O(n2) space, where n is the total number of events in the actions in A
(as we have already seen in the proof of Theorem 5.7).
7. Detecting Monotonicity* Using Temporal Relaxation
A subclass  of instances of an NP-hard problem is generally considered tractable if it satisfies two
conditions: (1) there is a polynomial-time algorithm to solve , and (2) there is a polynomial-time
algorithm to recognize . It is clearly polynomial-time to detect whether all actions are establisherunique. On the other hand, our very general definition of monotonicity of fluents implies that this is
not the case for determining whether fluents are monotone. In this section we show how our temporal
relaxation can be used to detect monotonicity* of certain fluents. Unfortunately, the following theorem shows that, in general, detection of monotonicity* is as difficult as temporal planning.
Theorem 7.1. Determining whether a fluent of a temporal planning problem <I,A,G> is monotone (or
monotone*) is PSPACE-hard if overlapping instances of the same action are not allowed in plans and
EXPSPACE-complete if overlapping instances of the same action are allowed.
Proof: Notice that if <I,A,G> has no solution, then all fluents are trivially monotone (and hence monotone*) by Definition 4.1, since they are neither established nor destroyed in any plan. It is sufficient to
add two new goal fluents f1, f2 and two new instantaneous actions to A, a1 which simply adds f1 and a2
which has f1 as a condition, adds f2 and deletes f1 (a1 and a2 being independent of all other fluents) to
any problem <I,A,G>: f1 is monotone (monotone*) if and only if the resulting problem has no temporal plan. The theorem then follows from the fact that testing the existence of a temporal plan for a
temporal planning problem <I,A,G> is PSPACE-hard if overlapping instances of the same action are
not allowed in plans and EXPSPACE-complete if overlapping instances of the same action are allowed (Rintanen, 2007).

We can nevertheless detect the monotonicity* of certain fluents in polynomial time. In this section
we give rules which can be applied in polynomial time. Given Theorem 7.1, we clearly do not claim
to be able to detect all monotone* fluents with these rules. The set of temporal planning problems
whose fluents can be proved +monotone* or monotone* by the rules given in this section, as re469

fiCOOPER, MARIS, & REGNIER

quired by the conditions of Theorem 5.7, represents a tractable class, since it can be both recognized
and solved in polynomial time.
To detect the +monotonicity (+monotonicity*) of a fluent f it suffices to give a proof that f cannot
be destroyed in a (minimal) plan after being established. In the conference version of this paper we
gave rules to provide such a proof, based on knowledge of the monotonicity of other fluents (Cooper,
Maris & Rgnier, 2012). It turns out that there is a simpler and more general proof rule (although
computationally more expensive) which involves solving an STP for each pair of actions a, b such
that f  Add(a)  Del(b). If the set of actions is establisher-unique, then there is at most one such action a. To try to prove that b cannot destroy f after a establishes f, we set up a relaxation TR[Before(a,
f, b)] consisting of the temporal relaxation TR of the planning problem together with a single hypothesis constraint: Before(a, f, b) = { first(a  f ) <  last(b  f )}. We can consider the case in which such
a pair of actions a,b does not exist (see Lemma 4.3) as simply a special case of this rule. Note, however, that the fact that TR[Before(a, f, b)] has a solution is a necessary but not a sufficient condition for
the existence of a valid temporal plan in which b destroys f after a establishes f. Indeed, Theorem 7.1
tells us that it is highly unlikely that a polynomial-time algorithm exists for determining whether a
fluent is monotone*.
To detect the monotonicity* of a fluent f we need to prove that f cannot be established in a minimal plan after being destroyed. In the corresponding STP TR[After(a, f, b)], the hypothesis is: After(a, f, b) = {first(b  f ) < last(a  f )}. We assume that when setting up the temporal relaxation
TR[Before(a, f, b)] or TR[After(a, f, b)] we apply the rules given in the previous section for the identification of unitary actions. This implies that implicitly we are only considering minimal plans and
hence that we detect monotonicity* rather than monotonicity.
Lemma 7.2. Suppose that the set of actions A is EU. If TR[Before(a, f, b)] has no solution for any pair
of actions a,b  A such that f  Add(a)  Del(b), then f is +monotone* relative to <I,A,G>. If
TR[After(a, f, b)] has no solution for any pair of actions a, b  A such that f  Add(a)  Del(b), then f
is monotone* relative to <I,A,G>.
In order to apply Theorem 5.6, we also have to prove that all fluents in I  (Cond(Ar)  G) are
monotone* relative to <I,Ap,G>. A plan for the problem <I,Ap,G> necessarily includes all actions
from ALM, but may or may not include actions from Ap \ ALM. As a consequence of this, we can only
impose constraints on actions in ALM. Indeed, under the extra hypothesis that the set of actions Ap is
establisher-unique, the corresponding STP is identical to TR[After(a, f, b)].
Lemma 7.3. If the set of actions Ap is establisher-unique and the temporal relaxation TR[After(a, f, b)]
has no solution for any pair of actions a, b  Ap such that f  Add(a)  Del(b), then f is monotone*
relative to <I,Ap,G>.
We now give a simple lemma to detect certain +monotone* fluents based on the notion of unitary
action. We assume that Lemmas 6.3, 6.4 and 6.5 are used to detect unitary actions.
Lemma 7.4. If A is establisher-unique and action aA is unitary, then all fluents f  Add(a) 
(G \(I \Del(ALM))) are +monotone* relative to the temporal planning problem <I,A,G>.

470

fiMONOTONE TEMPORAL PLANNING

Proof: Let f  Add(a)  (G \ (I \ Del(ALM))) and let P be a minimal plan for <I,A,G>. All fluents in
G\(I \ Del(ALM)) must be established in P. Thus P must contain an instance of action a, since A is establisher-unique. Indeed, since a is unitary, P contains exactly one instance of a. Therefore, f is established exactly once in P, and furthermore cannot later be destroyed in P since f is a goal fluent. It follows that f is +monotone*.

Example 7.5. Consider the following simple example of a planning problem with instantaneous actions:
Wash_hair:  d, c
Dry_clean_hair: c  d
where d means dry hair and c means clean hair, I =  and G = {d,c}. Note that we impose the condition that hair must be clean before it can be dried. The fluent d is not monotone since there is a solution plan Wash_hair, Dry_clean_hair, Wash_hair, Dry_clean_hair (in which the last two actions are
clearly redundant) which destroys, establishes, destroys and re-establishes d, but this plan is clearly
not minimal. We can deduce from Lemma 6.4 (case(2)) that Dry_clean_hair is unitary. Lemma 7.4
then tells us that d is +monotone* since Add(Dry_clean_hair)  (G \(I \ Del(ALM))) = {d}.
The following theorem now follows from Theorem 5.7 together with the fact that each STP can be
solved in polynomial time. Recall that a class is tractable if it can be recognised and solved in polynomial time.
Theorem 7.6. Let 1 be the class of positive temporal planning problems <I,A,G> in which Constr(Ar) are interval constraints, Ar is establisher-unique relative to SG, all fluents in Cond(Ar)  G are
monotone* relative to <I,Ar,G> and all fluents in I  (Cond(Ar)  G) are monotone* relative to
<I,Ap,G>, where monotonicity* of all fluents can be detected by applying Lemmas 7.3 and 7.4. Then
1 is tractable.
We have already seen in the proof of Theorem 5.7 that each temporal relaxation can be solved in
O(n3) time and O(n2) space, where n is the total number of events in the actions in A. The number of
temporal relaxations to solve, in order to prove that a temporal planning problem belongs to 1, is
proportional to the number of triples (a, f, b) such that a, b  Ap and f  Add(a)  Del(b). The number
of pairs ( f, b) such that b  Ap and f  Del(b) is bounded above by n. If Ap is establisher-unique, then
there is at most one action that a  Ap such that f  Add(a). Therefore, the complexity of recognizing
1 is O(n4) time and O(n2) space. In the conference version of this paper (Cooper, Maris & Rgnier,
2012) we gave simple rules that can be used to recognize a subclass of 1 in O(n2) time and O(n)
space.
We now discuss further rules for the detection of monotone* fluents. We will show that more monotone* fluents can be detected in polynomial time but at the cost of greater computational complexity.
We say that an action-instance a usefully produces a fluent h during the execution of a plan if h was
false just before being established by a. We say that a usefully produces the required fluent h if a usefully produces h and either h  G or the fluent h is the condition of some action c in the plan such that
(a  h) < (h | c). We can now state the following general proposition.
471

fiCOOPER, MARIS, & REGNIER

Proposition 7.7. Suppose that the set of actions A is EU relative to the set of sub-goals and let a  A
be the unique action that establishes sub-goal f. (a) If b  A such that f  Del(b), there is no minimal
plan in which some instance of b destroys f after some instance of a establishes f, and such that the
instance of a which first establishes f and the last instance of b which last destroys f both usefully produce required fluents, then f is +monotone*. (b) If b  A such that f  Del(b), there is no minimal
plan in which some instance of a establishes f after some instance of b destroys f, and such that the
instance of a which last establishes f and the instance of b which first destroys f both usefully produce
required fluents, then f is monotone*.
Proof: (a) Let P be a minimal plan in which some instance of b destroys f after some instance of a
establishes f. Then, by the hypothesis of the proposition, either the instance of a which first establishes
f in P or the last instance of b which last destroys f in P does not usefully produce a required fluent.
Hence P cannot be minimal, since we could delete either this instance of b or this instance of a from P
to leave another valid plan. This contradiction shows that f is +monotone*. The proof of case (b) is
similar.

We now give a lemma which allows us to deduce one of the hypotheses of Proposition 7.7 and
hence to deduce that a fluent f is +monotone* or that it is monotone*. To simplify the expression of
the lemma, we suppose that there is a goal-achieving action aG that must be executed at the end of all
plans and such that Cond(aG) = G. This simply means that goal fluents h do not need to be treated as
special cases.
Lemma 7.8. Suppose that A is EU relative to the set of sub-goals SG and let a  A be the unique action that establishes fluent f  SG. Let b  A be such that f  Del(b).
(a) Let h  SG  Add(a) and h  SG  Add(b). If any of the following conditions hold, then there is
no minimal plan P in which the last destruction of f by an instance of b occurs after the first establishment of f by an instance of a, and in which this instance of b usefully produces the required fluent h
and this instance of a usefully produces the required fluent h:
(1) for all actions c,c such that h  Cond(c), h  Cond(c), TR[Before(a,f,b)  For(a,first,h,c) 
For(b,last,h,c)] has no solution, where For(x,L,h,c) = {L(x h) < last(h | c)}.
(2) Constr(b) imposes a fixed interval between the destruction of f and the establishment of h by b,
h is monotone* and for all actions c,c such that h  Cond(c) and h  Cond(c), TR[Before(a,f,b)
 Once(b)  For(a,first,h,c)  For(b,last,h,c)] has no solution, where Once(x) = {first(E) = last(E)
| E  Events(x)}.
(b) Let h  SG  Add(a) and h  SG  Add(b). If any of the following conditions hold, then there is
no minimal plan P in which the last establishment of f by an instance of a occurs after the first destruction of f by an instance of b, and in which this instance of a usefully produces the required fluent h and
this instance of b usefully produces the required fluent h:
(1) for all actions c,c such that h  Cond(c), h  Cond(c), TR[After(a,f,b)  For(a,last,h,c) 
For(b,first,h,c)] has no solution.
(2) Constr(b) imposes a fixed interval between the destruction of f and the establishment of h by b,
h is monotone* and for all actions c,c such that h  Cond(c) and h  Cond(c), TR[After(a,f,b) 
Once(a)  For(a,last,h,c)  For(b,first,h,c)] has no solution.
472

fiMONOTONE TEMPORAL PLANNING

Proof: (a) We suppose that A is EU relative to SG, f  SG  Add(a)  Del(b), h  SG  Add(a) and
h  SG  Add(b).
(1) If TR[Before(a,f,b)  For(a,first,h,c)  For(b,last,h,c)] has no solution for all actions c,c such
that h  Cond(c), h  Cond(c), then it cannot be the case that in a minimal plan P the first establishment of f by a occurs before the last destruction of f by an instance of b, and the instance of a
which first establishes f usefully produces the required fluent h in P, and the instance of b which
last destroys f usefully produces the required fluent h in P.
(2) If h is monotone*, then only the instance of b which first establishes h can usefully produce h
in P. Since there is a fixed interval between the destruction of f and the establishment of h by b, it
is necessarily the same instance of b which first destroys f. Since it is the instance of b which last
destroys f which is assumed to usefully produce h, we can deduce that all instances of b are synchronised to destroy f at exactly the same moment. But then this contradicts Condition (5) of the
Definition 3.2 of a temporal plan, unless there is only one instance of b in P. The result follows
from the same argument as in case (2) with the extra constraint Once(b) that there is only one instance of b in P.

The proof of part (b) of the lemma is similar.



Example 7.9. Consider the following EU temporal planning problem in which all actions are instantaneous:
Check: p  g, o
Drive: p, o  a, g
Take: g  p
and I = {g} and G = {a}. One interpretation of these actions and fluents is: Have_Engine_checked
(Check), Drive_to_destination (Drive), Take_Petrol (Take), Have_petrol (p), At_garage (g), Engine_OK (o), Arrived (a). The fluent g is not monotone since there is a plan Take, Check, Drive,
Check (in which the last action is clearly redundant) which establishes, destroys, and establishes g.
However, g is monotone* since in a minimal plan action Check cannot usefully produce a fluent
h  Add(Check) = {g, o} after action Drive has destroyed g. In the case h=g, this is by Lemma
7.8(b)(1): Take is the only action such that g  Cond(Take), and TR[After(Check,g,Drive) 
For(Check,last,g, Take)] has no solution. In the case h = o, this is by Lemma 7.8(b)(2) since o is
monotone* (by Lemma 4.3) and TR[After(Check,g,Drive)  Once(Check)] has no solution. Note that
since a, p are monotone by Lemma 4.3, we can deduce from Lemma 6.4 (case (1)) that actions Drive
and Take are unitary. It then follows from Lemma 6.4 (case (3)) that action Check is also unitary. We
therefore impose in TR that the actions Check, Drive and Take occur only once; it follows that we
could have deduced that g is monotone* directly from Lemma 7.2 without having to use Lemma 7.8.
Combining Proposition 7.7 and Lemma 7.8 allows us to define a tractable class of temporal planning problems which is larger than the class described in Theorem 7.6.
Theorem 7.10. Let 2 be the class of positive temporal planning problems <I,A,G> in which Constr(Ar) are interval constraints, Ar is establisher-unique relative to SG, all fluents in Cond(Ar)  G are
monotone* relative to <I,Ar,G> and all fluents in I  (Cond(Ar)  G) are monotone* relative to
473

fiCOOPER, MARIS, & REGNIER

<I,Ap,G>, where monotonicity* of all fluents can be deduced from Proposition 7.7, Lemmas 7.4 and
7.8. Then 2 is tractable.
The number of temporal relaxations to solve, in order to prove that a temporal planning problem
belongs to 2, is proportional to the number of septuples (a,f,b,c,h,c,h) such that a,b,c,c  Ap,
f  Add(a)  Del(b), h  Add(a)  Cond(c) and h  Add(b)  Cond(c). We have seen in Section 5
that, assuming A is establisher-unique, the number of triples (a,f,b) satisfying a,b  Ap and
f  Add(a)  Del(b) is bounded above by n, the total number of events in the actions in A. The number of pairs (c,h) such that c  Ap and h  Cond(c) is again bounded above by n. Therefore, the number of relaxations to be solved is O(n3). We have seen in the proof of Lemma 4.3 that each temporal
relaxation can be solved in O(n3) time and O(n2) space. It follows that the complexity of recognizing
2 is O(n6) time and O(n2) space.
8. Experiments on IPC-2011 Benchmarks
We conducted experiments on the benchmark problems from the temporal deterministic track of
the 7th international planning competition IPC-2011, in order to test the applicability of our proposed
temporal relaxation TR as well as the relative utility of our various lemmas for the detection of monotonicity*. The main drawback of our temporal relaxation is that it only concerns EU fluents (i.e. fluents f for which there is a single action a with f  Add(a)). Indeed, the first step in setting up TR is to
remove all fluents which are not EU from the goal. All goal fluents are removed for all problems in
the following domains: floortile, matchcellar, parking, pegsol, openstacks, sokoban, storage,
turnandopen. We therefore concentrated our experiments on the three domains crewplanning,
parcprinter, tms. For each problem in each domain, let SGp denote the set of possible sub-goals in
the original unrelaxed problem. For each of the 20 problems in each of these domains, we determined
the set EUSGp of possible sub-goals SGp which are EU. Not all of these EU possible sub-goals of the
unrelaxed problem remain possible sub-goals in TR, since in TR we remove all fluents which are not
EU from conditions of all actions. We calculated the set of possible sub-goals in the relaxed problem,
which we call SGp(rel) to distinguish this set from the set of possible sub-goals SGp in the unrelaxed
problem. Moreover, we calculated the set of actions which produce possible sub-goals in the relaxed
problem, which we call Ap(rel) to distinguish this set from Ap in the unrelaxed problem. The first three
columns of Table 8.1 show the minimum, mean and maximum percentages of possible sub-goals
which remain possible sub-goals in TR (i.e. the ratio | SGp(rel) | / | SGp | ) over the 20 problems. We
then used the fluents in EUSGp and, in particular, the ones from SGp(rel) to test the relative frequency
of monotonicity*. The results for SGp(rel) are shown in the next three columns of Table 8.1. In the
parcprinter domain, all EUSGp fluents were detected to be monotone*. For each problem in the
crewplanning domain, almost all fluents in EUSGp were detected to be monotone*. In each problem
in the tms domain, 37% of the fluents in EUSGp were detected to be monotone*, and 50% of SGp(rel).
These results indicate that in certain temporal planning problems EU monotone* fluents are quite
common, but that in others TR can provide no useful information since no goals are EU.

474

fiMONOTONE TEMPORAL PLANNING

IPC 2011
Domain

SGp(rel)

Monotone* SGp(rel)

Unitary Ap(rel)

MIN MEAN MAX MIN MEAN MAX MIN MEAN MAX

Crewplanning

21%

36%

94%

87%

98% 39%

56%

71%

Parcprinter

4%

8%

15% 100% 100% 100% 56%

72%

94%

Tms

60%

60%

60%

54%

54%

50%

95%
50%

50% 54%

Table 8.1. Results of experiments on three domains from IPC 2011.
The last three columns of Table 8.1 give the number of actions in Ap(rel) which we detected as unitary in TR. The identification of unitary actions by Lemmas 6.3, 6.4 and 6.5 can be achieved in linear
time and provides useful information which is used by TR in the detection of monotonicity*. On average, over half the actions in Ap(rel) were found to be unitary.
We detected monotonicity* by applying our lemmas in increasing order of computational complexity: Lemma 4.3, Lemma 7.4, Lemma 7.2, and then Lemma 7.8. In all domains, the majority of
fluents which are recognised as monotone* are recognised as such by applying Lemma 4.3, some fluents are recognised as monotone* by applying Lemma 7.4 and all other monotone* fluents are recognised by Lemma 7.2 (which uses the temporal relaxation TR). We found no monotone* fluents which
required Lemma 7.8 to be detected. The significantly greater complexity of applying Lemma 7.8
compared with Lemma 7.2 means that it is not worth applying systematically to all problems. On the
other hand, our experiments have confirmed that Lemmas 4.3, 7.4 and 7.2 are all effective for the detection of monotonicity*. Table 8.2 shows for each of the three domains, the minimum, mean and
maximum percentage of fluents in SGp which are detected by each of these three lemmas.

IPC 2011
Domain
MIN
Crewplanning MEAN
MAX
MIN
Parcprinter
MEAN
MAX
MIN
Tms
MEAN
MAX

Lemma 4.3
80%
87%
95%
88%
95%
100%
29%
29%
29%

Monotone* SGp(rel)
Lemma 7.4 Lemma 7.2
0%
0%
0%
7%
0%
13%
0%
0%
0%
5%
0%
12%
21%
0%
21%
0%
21%
0%

ALL
87%
95%
98%
100%
100%
100%
50%
50%
50%

Table 8.2. Percentages of fluents detected as monotone* by three different lemmas.
To illustrate the degree of variation between different problems within the same domain, in Figures
8.3, 8.4 and 8.5 we show the details of each of the 20 problems in the three different domains. For
each problem we show the number of monotone* fluents in SGp(rel) detected by different lemmas.
475

fiCOOPER, MARIS, & REGNIER

Figure 8.3. The number of fluents in SGp(rel) detected as monotone* by different lemmas in the
crewplanning domain.

Figure 8.4. The number of fluents in SGp(rel) detected as monotone* by different lemmas in the
parcprinter domain.

476

fiMONOTONE TEMPORAL PLANNING

Figure 8.5. The number of fluents in SGp(rel) detected as monotone* by different lemmas in the
tms domain.
As a general conclusion of our experimental trials, we have seen that in many problems TR provides no useful information since all goal fluents are removed. Nevertheless, we have identified various benchmark domains in which it can be applied. The fact that a large percentage of fluents were
found to be monotone* and a large percentage of actions were found to be unitary demonstrates the
potential importance of these notions beyond their use in the temporal relaxation TR. These experimental trials together with our investigation of specific examples (such as Example 7.9 or the Temporal Cement Factory domain described in the following section) seem to indicate that integrating the
detection of unitary actions into TR provides as much information as the more computationally expensive approach of Lemma 7.8.
9. Examples of Applications of EU Monotone Planning
We have previously shown that EU monotone planning has potential applications in various industrial settings, such as the construction or the chemical and pharmaceutical industries (Cooper, Maris &
Rgnier, 2012, 2013b). For example, the Temporal Chemical Process domain, described in detail by
Cooper, Maris and Rgnier (2013b), involves different kinds of operations on chemicals that are performed in the industrial production of compounds. For each raw material, there is an operator that can
activate its source. Then, this raw material can be catalysed in different ways to synthesize different
products. These products can be mixed and reacted using the raw material once again to produce the
desired compound. For example, acetylene is a raw material derived from calcium carbide using water. Then, a vinyl chloride monomer is produced from acetylene and hydrogen chloride using mercuric
chloride as a catalyst. PVC is then produced by polymerization. Other examples occur in the pharmaceutical industry in the production of drugs (such as paracetamol or ibuprofen) and, in general, in
many processes requiring the production and combination of several molecules, given that there is a
unique way to obtain them (which is often the case due to industrial constraints).
477

fiCOOPER, MARIS, & REGNIER

We now give in detail an example from the construction industry to show how the detection of unitary actions can greatly speed up the recognition of such problems. The Temporal Cement Factory
planning domain (Cooper, Maris & Rgnier, 2013b) allows us to plan concrete mixing, delivery and
use. An action of duration 30 time units makes and times a batch of concrete which is fluid from time
unit 3 to 30 (after which it sets). At the same time, a concrete-mixer must be cleaned, in order for the
concrete to be loaded, then driven to a building site, where it is unloaded. The concrete must then be
used while it is still fluid. This process is illustrated by the temporal plan given in Figure 5. This set of
actions A (illustrated in the temporal plan shown in Figure 5) are all landmarks. The initial state I and
the goal G are given by
I = {At-factory(m), Available(c)}
G = {Delivered(m,c,s), Used(c)}
Given the temporal planning problem <I,A,G>, where A is the set of all actions from the Temporal
Cement Factory domain, the set of sub-goals SG and the reduced set of actions Ar are:
SG = {Delivered(m,c,s), Used(c), Fluid(c), At(m,s), Available(c),
On(m,c), At-factory(m), Empty(m)}

Ar = {USE(c), UNLOAD(m,c,s), DRIVE(m,s), LOAD(m,c), CLEAN(m), MAKEAND-TIME-CONCRETE(c)}

For all ai  aj  A, we have Add(ai)  Add(aj)  SG = . Hence, by Definition 3.3, the set of actions A is EU relative to SG. We can immediately remark that no actions delete the fluents Used(c),
Delivered(m,c,s) and At(m,s), and no actions add the fluents Available(c) and Atfactory(m). Thus, by Lemma 4.3, each of these fluents are both monotone and +monotone relative to <I,A,G>. By Lemma 7.2, we can deduce that On(m,c) is monotone since the temporal relaxation TR[After(LOAD(m,c),On(m,c),UNLOAD(m,c,s))] has no solution. By a similar argument,
Fluid(c) is also monotone by Lemma 7.2.
Available(c)
MAKE-AND-TIME-CONCRETE(c)[30]
Available(c)

Fluid(c)

Fluid(c)
At-factory(m)

At-factory(m)

DRIVE(m,s)[6]

CLEAN(m)[4]

At(m,s)

At-factory(m)

Empty(m)

Fluid(c)

Empty(m)

At-factory(m)

At(m,s)
On(m,c)

LOAD(m,c)[5]
Empty(m)

Delivered(m,c,s)
Fluid(c)

Fluid(c)

USE(c)[4]

UNLOAD(m,c,s)[7]

On(m,c)
On(m,c)

Used(c)

Delivered(m,c,s)

Figure 5: Ready-mix Concrete Delivery Temporal Plan
We can then detect unitary actions. From Lemma 6.4 (case (1)), we can deduce that
UNLOAD(m,c,s) is unitary. Then, applying Lemma 6.4 (case (3)), with h = On(m,c) and b =
478

fiMONOTONE TEMPORAL PLANNING

UNLOAD(m,c,s), tells us that LOAD(m,c) is unitary. Finally, applying Lemma 6.4 (case (3)), with h
= Empty(m) and b = LOAD(m,c), tells us that CLEAN(m) is unitary. Since CLEAN(m) is unitary, we
effectively add the constraint Once(CLEAN(m)) to TR and with this constraint Lemma 7.2 is now sufficient to detect that Empty(m) is monotone*. Thus, using the new notion of unitary action and the

linear-time rules to detect such actions given in Section 6, we can prove monotonicity* of all fluents
without needing to use the computationally expensive Lemma 7.8 as we previously proposed (Cooper,
Maris & Rgnier, 2013b).
It is now possible to apply Theorem 5.6, since A is EU, all fluents are monotone* and all fluents in
I are monotone*. It follows that TR is a solution procedure for this problem. The problem <I,A,G>
has a solution-plan, found by TR, shown in Figure 5. We represent non-instantaneous actions by a
rectangle. Conditions are written above an action, and effects below; causality constraints are represented by bold arrows, and authorisation constraints by dotted arrows.
This example can be extended to the generic case in which there are several sites, several batches
of concrete and several mixers. It is monotone and remains EU provided that the goals (via the fluents
Delivered(m,c,s)) specify which mixer m is to deliver which batch c to which building site s. All
such instances can be solved in polynomial time by Theorem 7.10.
10. Discussion
The results in this paper can also be applied to non-temporal planning since, for example, a classical STRIPS planning problem can be modelled as a temporal planning problem in which all actions
are instantaneous. It is worth pointing out that the tractable class of classical planning problems in
which all actions are establisher-unique and all fluents are detectable as (both + and ) monotone by
applying only Lemma 4.3, is covered by the PA tractable class of Jonsson and Bckstrm (1998).
An obvious question is whether both establisher-uniqueness and monotonicity are necessary to obtain tractability. An affirmative answer to this question follows from intractability results in nontemporal planning: Bckstrm and Klein (1991b) showed that establisher-uniqueness alone cannot
prevent minimal plans being of exponential size, and Jonsson and Bckstrm (1998) showed that under conditions implying monotonicity of all fluents (the class BA in their terminology), planning is
NP-hard.
+

For simplicity of presentation and for conformity with PDDL2.1, we have considered that inherent
constraints between the times of the events within the same action-instance are all interval constraints.
We can, however, remark that Theorem 5.6 still holds if the inherent constraints are arbitrary minclosed constraints, since this was the only property required of the constraints in the proof of Theorem
5.6. An example of such a constraint C(x,y) is a binary interval constraint with variable bounds:
yx  [f(x,y),g(x,y)], which is min-closed provided that f(x,y) is a monotone increasing function of x
and g(x,y) is a monotone decreasing function of y. The shift-monotonic constraints used by Pralet and
Verfaillie (2012) in the scheduling of agile satellites are a subclass of such constraints since in shiftmonotonic constraints both f(x,y) and g(x,y) are monotone increasing functions of x and monotone
decreasing functions of y. The consistency of a set of shift-monotonic constraints can be tested in time
O(n3).

479

fiCOOPER, MARIS, & REGNIER

An important aspect of temporal planning, which is absent from non-temporal planning, is that certain temporal planning problems, known as temporally-expressive problems, require concurrency of
actions in order to be solved (Cushing, Kambhampati, Mausam & Weld, 2007). The cement factory
planning problem given in Section 9 is an example of a temporally-expressive problem, since concurrency of actions is required in any solution. Indeed, in industrial environments, concurrency of actions
is often used to keep storage space and turn-around times within given limits. In a previous paper
(Cooper, Maris & Rgnier, 2013a), we identified a subclass of temporally expressive problems,
known as temporally-cyclic, which require cyclically-dependent sets of actions in order to be solved.
A simple but commonly occurring example was given in Figure 4, concerning an agreement between
an employer and employee. The tractable class of temporal planning problems described in Theorem
7.6 contains both temporally-expressive and temporally-cyclic problems. For example, the temporally-cyclic problem given in Figure 4 is establisher-unique and all fluents are both + and monotone
(this follows from Lemma 4.3 since no fluents are destroyed by either action).
Most temporal planning problems will not fall into the tractable class of EU monotone problems.
Even so, it may be that certain sub-problems do fall into this class. Given a temporal planning problem
<I,A,G>, we can test in polynomial time, for each fluent f, whether the sub-problem <I,A,{f}> satisfies
the conditions of Theorem 7.6 (i.e. EU monotone, with monotonicity detectable using the temporal
relaxation TR). If this is the case, then we can find in polynomial time a plan Pf which establishes the
fluent f. This plan Pf can then be considered as an action which could be added to the set of actions A
in order to facilitate the solution of the original problem <I,A,G>.
Our work is related to the literature regarding landmarks. Porteous, Sebastia and Hoffmann (2001)
and Keyder, Richter and Helmert (2010) define a landmark as a fact that must be true at some point in
every valid solution-plan. Landmarks have been used in planning in two main ways. The first one is
the conception of heuristic functions to guide search algorithms (Richter, Helmert & Westphal, 2008;
Richter & Westphal, 2010; Helmert & Domshlak, 2009). Another use of landmarks is to partition the
problem into easier subproblems whose goals are disjunctions of landmarks (Hoffmann, Porteous &
Sebastia, 2004; Sebastia, Onaindia & Marzal, 2006). More recently, Vernhes, Infantes and Vidal
(2013) define a landmark-based meta best-first search algorithm.
Landmarks have also been used for the detection of unsolvable temporal planning problems (Marzal, Sebastia & Onaindia, 2008). A graph is built by adding causal relationships between the extracted
landmarks. Then temporal intervals are associated with each landmark and these intervals, together
with the causal relationships, define a set of constraints. Finally, a CSP solver checks the consistency
of this set and indicates that the problem has no solution when an inconsistency is found. Unlike the
set of constraints in our temporal relaxation TR, this set of constraints does not fall into a tractable
class. Further research is required to determine whether certain of these constraints could be usefully
combined with the STP constraints of our temporal relaxation TR to obtain an even stronger tractable
relaxation.
In the general case, verifying that a fact is a landmark is PSPACE-complete (Hoffmann, Porteous
& Sebastia, 2004). However, some landmarks can be found more efficiently by using various techniques: Porteous and Cresswell (2002) and Hoffmann, Porteous and Sebastia (2004) present methods
for detecting landmarks and relations between landmarks based on backchaining from the goals in the
relaxed planning graph, whereas Zhu and Givan (2003) use forward propagation in the same graph
and Richter, Helmert and Westphal (2008) use the domain transition graph, a graph whose nodes
480

fiMONOTONE TEMPORAL PLANNING

represent the possible values of the variable and edges represent the possible transitions between values induced by actions.
The notion of monotonicity* introduced in this paper depends on the relative order of the establishment and the destruction of the same fluent within a minimal plan. Our experiments have demonstrated that many fluents in benchmark problems are indeed monotone*. An interesting avenue of future research would be to investigate, both theoretically and empirically, the relative order of the establishment and the destruction of different fluents within a minimal temporal plan. This is again
closely related to research on landmarks. Different orderings between landmarks have been studied in
non-temporal planning. Some of these orderings are guaranteed to hold in every solution-plan and do
not prune the solution space (they are sound): "Natural" (Koehler & Hoffmann, 2000), "Necessary"
and "Greedy-necessary" (Hoffmann, Porteous & Sebastia, 2004). The natural ordering is the most
general and then greedy-necessary ordering and necessary ordering. Others orderings such as "Reasonable", "Obedient-Reasonable" (Koehler & Hoffmann, 2000) are not sound (it is possible that no
solution-plan respects these orderings) but they may prune the solution space. All these orderings between landmarks are defined assuming instantaneous actions and would need to be redefined in the
temporal framework. Further research is required to determine whether landmark orderings could usefully be extended to incorporate orderings between all types of events in temporal plans (the establishment or destruction of a fluent by an action landmark, or the beginning or end of an interval over
which a fluent is required by an action landmark).
11. Conclusion
We have presented a class of temporal planning problems which can be solved in polynomial time
and which has a number of possible applications, notably in the chemical, pharmaceutical and construction industries. The notion of monotonicity in temporal planning is an essential part of the definition of this class. We extended our basic notion of monotonicity to monotonicity* by considering only
minimal plans.
We have also shown that all planning problems have a relaxation based on EU monotone planning
which is an interesting alternative to the standard relaxation produced by ignoring deletes. It also provides a means of detecting action landmarks and monotone* fluents.
Further research is required to discover other possible application areas and, on a practical level, to
develop tools to help users find a model of a problem involving only monotone* fluents when such a
model exists. On a theoretical level, an interesting avenue of future research is the extension of the
tractable classes presented in this paper by relaxing the condition of establisher-uniqueness so that a
fluent can be established by more than one action provided that there is only one action that can establish it at any given moment.
Acknowledgements
This research was supported by ANR Project ANR-10-BLAN-0210. We gratefully acknowledge
the help of the reviewers whose constructive suggestions led to significant improvements in the
presentation of this paper.
481

fiCOOPER, MARIS, & REGNIER

References
Bckstrm C. & Klein I. (1991a) Parallel non-binary planning in polynomial time. Proceedings
IJCAI1991, 268-273.
Bckstrm C. & Klein I. (1991b) Planning in polynomial time: the SAS-PUBS class,
Computational Intelligence 7 (3), 181-197.
Bckstrm C. & Nebel B. (1995) Complexity results for SAS+ planning. Computational
Intelligence 11(4), 625-655.
Baier J. A. & Botea A. (2009). Improving planning performance using low-conflict relaxed plans.
Proc. 19th International Conference on Automated Planning and Scheduling (ICAPS2009).
Betz C. & Helmert M. (2009). Planning with h+ in Theory and Practice. KI 2009: Advances in
Artificial Intelligence, LNCS Vol. 5803, 9-16.
Blum A.L. & Furst M.L. (1995) A.L. Blum, M.L. Furst, Fast planning through planning-graphs
analysis, in: Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-95), Montral, Qubec, Canada, 1636-1642.
Bonet B., Loerincs G. & Geffner H. (1997) A Robust and Fast Action Selection Mechanism for
Planning, Proceedings AAAI-97/IAAI-97, 714-719.
Brafman R.I. & Domshlak C. (2003) Structure and Complexity in Planning with Unary
Operators. Journal of Artificial Intelligence Research 18, 315-349.
Brafman R.I. & Domshlak C. (2006) Factored Planning: How, When, and When Not. Proc. 21st
National Conference on Artificial Intelligence, 809-814.
Bylander T. (1994) The Computational Complexity of Propositional STRIPS Planning. Artificial
Intelligence 69(1-2), 165-204.
Cai D., Hoffmann J. & Helmert M. (2009). Enhancing the context-enhanced additive heuristic
with precedence constraints. Proc. 19th International Conference on Automated Planning and
Scheduling (ICAPS2009), 5057.
Chen H. & Gimnez O. (2008) Causal Graphs and Structurally Restricted Planning. Proc. 18th
International Conference on Automated Planning and Scheduling (ICAPS2008), 36-43.
Coles A., Fox M., Long D. & Smith A. (2008) Planning with Problems Requiring Temporal
Coordination, Proc. of AAAI 2008, 892-897.
Cooper M.C., de Roquemaurel M. & Rgnier, P. (2011) A weighted CSP approach to costoptimal planning, Artificial Intelligence Communications 24(1), 1-29.
Cooper M.C., Maris F. & Rgnier P. (2010) Solving temporally cyclic planning problems,
International Symposium on Temporal Representation and Reasoning (TIME), 113-120.
Cooper M.C., Maris F. & Rgnier, P. (2012) Tractable monotone temporal planning, Proceedings
ICAPS 2012, 20-28.
Cooper M.C., Maris F. & Rgnier, P. (2013a) Managing Temporal Cycles in Planning Problems
Requiring Concurrency, Computational Intelligence 29(1), 111-128.
Cooper M.C., Maris F. & Rgnier P. (2013b) Relaxation of Temporal Planning Problems,
International Symposium on Temporal Representation and Reasoning (TIME), 37-44.
Cushing W., Kambhampati S., Mausam & Weld D.S. (2007) When is Temporal Planning Really
Temporal? Proceedings of 20th International Joint Conference on Artificial Intelligence,
IJCAI2007, 1852-1859.
Dean T., Firby J. & Miller D. (1988) Hierarchical Planning involving deadlines, travel time and
ressources. Computational Intelligence 6(1), 381-398.
482

fiMONOTONE TEMPORAL PLANNING

Dean T. & McDermott D.V. (1987) Temporal Data Base Management. Artificial Intelligence
32(1), 1-55.
Dechter R., Meiri I. & Pearl J. (1991) Temporal Constraint Networks, Artificial Intelligence 49(13), 61-95.
Do M.B. & Kambhampati S. (2003) Sapa: A Multi-objective Metric Temporal Planner, Journal
of Artificial Intelligence Research 20, 155-194.
Domshlak C. & Dinitz Y. (2001) Multi-agent off-line coordination: Structure and complexity.
Proceedings of 6th European Conference on Planning, ECP2001, 277-288.
Erol K., Nau D.S. & Subrahmanian V.S. (1995) Complexity, decidability and undecidability
results for domain-independent planning. Artificial Intelligence 76(1-2), 75-88.
Eyerich P., Mattmller R. & Rger G. (2009) Using the Context-enhanced Additive Heuristic for
Temporal and Numeric Planning, Proceedings ICAPS 2009, 130-137.
Fox, M. & Long, D. (2001). Stan4: A hybrid planning strategy based on subproblem abstraction.
The AI Magazine 22(3), 8184.
Fox M. & Long D. (2003) PDDL2.1: An Extension to PDDL for Expressing Temporal Planning
Domains, Journal of Artificial Intelligence Research 20, 61-124.
Fox M., Long D. & Halsey K. (2004). An Investigation into the Expressive Power of PDDL2.1,
Proc. 16th European Conference on Artificial Intelligence, 328-342.
Gerevini A. & Cristani M. (1997) On Finding a Solution in Temporal Constraint Satisfaction
Problems. Proc. 15th International Joint Conference on Artificial Intelligence, 1460-1465.
Gerevini, A., Saetti, A. & Serina, I. (2003). Planning through stochastic local search and temporal
action graphs. Journal of Artificial Intelligence Research 20, 239290.
Ghallab M. & Alaoui A.M. (1989) Managing Efficiently Temporal Relations Through Indexed
Spanning Trees. Proc. 11th Int. Joint Conference on Artificial Intelligence, 1297-1303.
Ghallab M., Nau D.S. & Traverso P. (2004) Automated Planning: Theory and Practice, Morgan
Kaufmann.
Gimnez O. & Jonsson A. (2008) The complexity of planning problems with simple causal
graphs. Journal of Artificial Intelligence Research 31, 319-351.
Gimnez O. & Jonsson A. (2012). The influence of k-dependence on the complexity of planning.
Artificial Intelligence 177-179, 25-45.
Haslum P. (2008) A New Approach To Tractable Planning. Proceedings of ICAPS2008, 132139.
Haslum P., Slaney J. & Thibaux S. (2012). Incremental lower bounds for additive cost planning
problems. Proc. 22nd International Conference on Automated Planning and Scheduling
(ICAPS2012), 7482.
Helmert M. (2003) Complexity results for standard benchmark domains in planning. Artificial
Intelligence 143 (2), 219-262.
Helmert M. (2004). A planning heuristic based on causal graph analysis. Proc. 14th International
Conference on Automated Planning and Scheduling (ICAPS2004), 161170.
Helmert M. (2006) New Complexity Results for Classical Planning Benchmarks. Proc. 16th
International Conference on Automated Planning and Scheduling (ICAPS2006), 52-61.
Helmert M. & Geffner H. (2008). Unifying the causal graph and additive heuristics. Proc. 18th
International Conference on Automated Planning and Scheduling (ICAPS2008), 140147.
Helmert M. & Domshlak C. (2009) Landmarks, Critical Paths and Abstractions: Whats the
Difference Anyway? Proc. International Conference on Automated Planning and Scheduling
(ICAPS 2009), 162-169.
483

fiCOOPER, MARIS, & REGNIER

Hoffmann J. (2005) Where Ignoring Delete Lists Works, Local Search Topology in Planning
Benchmarks. Journal of Artificial Intelligence Research 24, 685-758.
Hoffmann J., Porteous J. & Sebastia L. (2004) Ordered Landmarks in Planning. Journal of
Artificial Intelligence Research 22, 215278.
Jeavons P. & Cooper M.C. (1995) Tractable constraints on ordered domains, Artificial
Intelligence 79, 327-339.
Jonsson A. (2007) The Role of Macros in Tractable Planning Over Causal Graphs. Proc. 20th
International Joint Conference on Artificial Intelligence (IJCAI2007), 1936-1941.
Jonsson A. (2009) The Role of Macros in Tractable Planning. Journal of Artificial Intelligence
Research 36, 471-511.
Jonsson P. & Bckstrm C. (1994) Tractable planning with state variables by exploiting structural
restrictions. Proc. AAAI1994, 998-1003.
Jonsson P. & Bckstrm C. (1995) Incremental Planning. In New Directions in AI Planning: 3rd
European Workshop on Planning, EWSP1995, 79-90.
Jonsson P. & Bckstrm C. (1998) State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence 100(1-2), 125-176.
Karmarkar N. (1984) A new polynomial time algorithm for linear programming. Combinatorica 4
(4) 373395.
Karpas E. & Domshlak C. (2009) Cost-optimal planning with landmarks, International Joint
Conference on Artificial Intelligence (IJCAI2009), 17281733.
Katz M. & Domshlak C. (2008) New Islands of Tractability of Cost-Optimal Planning. Journal of
Artificial Intelligence Research 32, 203-288.
Katz M., Hoffmann J. & Domshlak C. (2013a). Who said we need to relax All variables? Proc.
23rd International Conference on Automated Planning and Scheduling, (ICAPS2013).
Katz M., Hoffmann J. & Domshlak C. (2013b). Red-Black Relaxed Plan Heuristics. Proc. 27th
AAAI Conference on Artificial Intelligence (AAAI2013).
Keyder E. & Geffner H. (2008). Heuristics for planning with action costs revisited. Proc. 18th
European Conference on Artificial Intelligence (ECAI2008), 588592.
Keyder E., Hoffmann J. & Haslum P. (2012) Semi-Relaxed Plan Heuristics, Proc. 22nd
International Conference on Automated Planning and Scheduling, ICAPS2012, 128-136.
Keyder E., Richter S. & Helmert M. (2010) Sound and Complete Landmarks for And/Or Graphs.
Proceedings of the European Conference on Artificial Intelligence (ECAI), 335-340.
Knoblock C.A. (1994) Automatically Generating Abstractions for Planning. Artificial Intelligence
68(2), 243-302.
Koehler J. & Hoffmann J. (2000) On reasonable and forced goal orderings and their use in an
agenda-driven planning algorithm. Journal of Artificial Intelligence Research 12, 338386.
Koubarakis M. (1992) Dense Time and Temporal Constraints with . Proc. 3rd International
Conference on Principles of Knowledge Representation and Reasoning (KR1992), 24-35.
Laborie P. & Ghallab M. (1995) Planning with Sharable Resource Constraints. Proc. 14th
International Joint Conference on Artificial Intelligence, 1643-1651.
Long D. & Fox M. (2003) Exploiting a graphplan framework in temporal planning, Proc. 13th
International Conference on Automatic Planning and Scheduling, 52-61.
Maris F. & Rgnier P. (2008) TLP-GP: Solving Temporally-Expressive Planning Problems,
TIME 2008, 137-144.

484

fiMONOTONE TEMPORAL PLANNING

Marzal E., Sebastia L. & Onaindia E. (2008) Detection of unsolvable temporal planning problems
through the use of landmarks. Proceedings of ECAI2008. 919-920.
McDermott D. (1998) PDDL, The Planning Domain Definition Language. Technical Report,
http://cs-www.cs.yale.edu/ homes/dvm/.
Porteous J. & Cresswell S. (2002) Extending landmarks analysis to reason about resources and
repetition. Proceedings of PLANSIG2002, 4554.
Porteous J., Sebastia L. & Hoffmann J. (2001) On the Extraction, Ordering, and Usage of
Landmarks in Planning. Recent Advances in AI Planning. European Conference on Planning
(ECP 2001), 3748.
Pralet C. & Verfaillie G (2012) Time-Dependent Simple Temporal Networks, Proc. 18th
International Conference on Principles and Practice of Constraint Programming, 608-623.
Reichgelt H. & Shadbolt N. (1990). A Specification Tool for Planning Systems. Proc. 9th
European Conference on Artificial Intelligence, 541-546.
Richter S., Helmert M. & Westphal M. (2008) Landmarks revisited. Proc. 23rd AAAI Conference
on Artificial Intelligence (AAAI'08). 975-982.
Richter S. & Westphal M. (2010) The LAMA Planner: Guiding Cost-Based Anytime Planning
with Landmarks. Journal of Artificial Intelligence Research (JAIR) 39: 127-177.
Rintanen J. (2007) Complexity of Concurrent Temporal Planning. Proc. 17th International
Conference on Automated Planning and Scheduling (ICAPS2007), 280-287.
Rutten E. & Hertzberg J. (1993) Temporal Planner = Nonlinear Planner + Time Map Manager.
Artificial Intelligence Communications 6(1), 18-26.
Schwartz P. & Pollack M.E. (2004) Planning with Disjunctive Temporal Constraints. Proc.
ICAPS'04 Workshop on Integrating Planning into Scheduling, 67-74.
Sebastia L., Onaindia E. & Marzal E., (2006) Decomposition of planning problems. AI
Communications 19:4981.
Shin J. & Davis E. (2004) Continuous Time in a SAT-Based Planner. Proc. 19th National
Conference on Artificial Intelligence (AAAI'04), 531-536.
Slaney J. & Thibaux S. (2001) Blocks World revisited. Artificial Intelligence 125, 119-153.
Smith D.E. (2003) The Case for Durative Actions: A Commentary on PDDL2.1, Journal of
Artificial Intelligence Research 20, 149-154.
Stergiou K. & Koubarakis M. (2000) Backtracking algorithms for disjunctions of temporal
constraints. Artificial Intelligence 120(1):81117.
Vere S. (1983) Planning in Time: Windows and Durations for Activities and Goals. IEEE Trans.
on Pattern Analysis and Machine Intelligence 5, 246-267.
Vernhes S., Infantes G. & V. Vidal V. (2013) Problem Splitting using Heuristic Search in
Landmark Orderings. Proc. 23rd International Joint Conference on Artificial Intelligence
(IJCAI2013), 2401-2407.
Vidal V. & Geffner H. (2005) Solving Simple Planning Problems with More Inference and No
Search. Proc. 11th International Conference on Principles and Practice of Constraint
Programming, CP'05, 682-696.
Williams B.C. & Nayak P. (1997) A reactive planner for a model-based executive. Proc. 15th
International Joint Conference on Artificial Intelligence, 1178-1185.
Younes H.L.S. & Simmons R.G. (2003) VHPOP: Versatile Heuristic Partial Order Planner.
Journal of Artificial Intelligence Research 20, 405-430.
Zhu L. & Givan R. (2003) Landmark extraction via planning graph propagation. ICAPS2003
Doctoral Consortium, 156160.
485

fiJournal of Artificial Intelligence Research 50 (2014) 265-319

Submitted 10/13; published 06/14

Property Directed Reachability for Automated Planning
Martin Suda

suda@mpi-inf.mpg.de

Max-Planck-Institut fur Informatik,
Saarbrucken, Germany
Charles University, Prague, Czech Republic

Abstract
Property Directed Reachability (PDR) is a very promising recent method for deciding
reachability in symbolically represented transition systems. While originally conceived as
a model checking algorithm for hardware circuits, it has already been successfully applied
in several other areas. This paper is the first investigation of PDR from the perspective of
automated planning.
Similarly to the planning as satisfiability paradigm, PDR draws its strength from internally employing an efficient SAT-solver. We show that most standard encoding schemes
of planning into SAT can be directly used to turn PDR into a planning algorithm. As a
non-obvious alternative, we propose to replace the SAT-solver inside PDR by a planningspecific procedure implementing the same interface. This SAT-solver free variant is not only
more efficient, but offers additional insights and opportunities for further improvements.
An experimental comparison to the state of the art planners finds it highly competitive,
solving most problems on several domains.

1. Introduction
Property Directed Reachability (PDR), also known as IC3, is a recently proposed algorithm
for deciding reachability in symbolically represented transition systems (Bradley, 2011; Een,
Mishchenko, & Brayton, 2011).1 Since its discovery in 2010, it has already established
itself as one of the strongest model checking algorithms used in hardware verification. The
original and inspiring way in which PDR harnesses the power of a modern SAT-solver
gives the algorithm a unique ability to discover long counterexample paths combined with
a remarkable performance in proving unreachability. Other interesting traits include a
typically small memory footprint and a good potential for parallelization.
With awareness of the well-known equivalence between model checking and automated
planning, the aim of this work is to investigate PDR from the planning perspective. Our
main goal is to establish whether the practical success of the algorithm can be repeated
on planning benchmarks. Moreover, we are also interested in the relation of PDR to the
currently used planning techniques. It is illustrative for highlighting some of the interesting
features of the algorithm to start with a preliminary comparison right away.
The fact that PDR builds upon the SAT-solving technology makes it obviously related
to the planning as satisfiability approach (Kautz & Selman, 1996). While in planning as
satisfiability the underlying SAT-solver receives formulas of increasing size as the method
progresses to check for existence of increasingly longer plans, in PDR each SAT-solver call
1. IC3 is the name Aaron Bradley, the originator of the algorithm, gave to the first implementation (Bradley,
2011). The more descriptive name Property Directed Reachability was coined by Een et al. (2011).
c
2014
AI Access Foundation. All rights reserved.

fiSuda

corresponds only to a single step in the transition system. That is why the algorithm is
sometimes said to operate without unrolling. Dealing with formulas over a fixed signature
lifts some of the computational burden from the SAT-solver, making it respond in a more
reliable way.
Similarly to planning as satisfiability, PDR proceeds iteratively, gradually disproving
existence of plans of length 0, 1, 2,. . . . Due to a so-called obligation rescheduling technique,
however, PDR can discover a plan of length l already during iteration k < l, that is, while
the existence of shorter plans has not necessarily been ruled out yet. This typically leads
to improved performance, as it allows the algorithm to avoid completing the potentially
expensive non-existence proofs. A similar effect can be achieved in the planning as satisfiability approach by running several SAT-solvers in parallel or interleaved (Rintanen, 2004).
Such a modification, however, requires a non-trivial engineering effort and the resulting
system contains parameters that need to be tuned for the problem at hand. In contrast,
rescheduling in PDR amounts literally to a one-line change in the algorithm. When this
line is disabled, PDR resorts to the more expensive search for an optimal length plan.
Surprisingly, PDR can also be naturally compared to the explicit heuristic search planning (Bonet & Geffner, 2001). Indeed, PDR is probably best understood as a hybrid
between explicit and symbolic approaches. Because the satisfying assignment in PDR is
built systematically, one transition at a time, the finished part of it corresponds to an explicit path through the transition system and its last piece to a state to be expanded next.
At the same time, PDR maintains symbolic reachability information in a form of a sequence
of sets of clauses. The k-th set in the sequence over-approximates the k-fold preimage of
the set of goal states. These clause sets play a role similar to an admissible heuristic. They
represent a lower bound estimate for the distance of a state to the goal and thus provide a
means to guide the search towards it. However, while a heuristic value of a particular state
is normally computed only once and it remains constant during the search for a plan, the
clause sets in PDR are refined continually. The refinement happens on demand, driven by
the states encountered during the search.
1.1 Paper Overview
In order to apply PDR to a planning problem, the problem has to be processed into a
suitable form. In Section 2 we introduce a symbolic transition system, a description of a
reachability task based on the clausal language of propositional logic, which serves as a
generic input for PDR. We observe that most of the standard encoding schemes of planning
into SAT provide us with such a description. This means that a general implementation of
the algorithm, which we describe in detail in Section 3, combined with any such encoding
already yields a stand-alone planner. This is, however, not the most efficient path to take.
The main contribution of this paper is presented in Section 4. We show that instead of
relying on an encoding and a general purpose SAT-solver, we can, at least in the case of the
sequential plan semantics, delegate the single step reachability queries to a planning-specific
procedure. Not only do we gain a polynomial time guarantee for answering the individual
queries, but by decoupling PDR from the underlying SAT-solver we also gain additional
insights and ideas for further improvements.
266

fiProperty Directed Reachability for Automated Planning

We implemented the proposed idea in a new planner PDRplan. In Section 5 we experimentally confirm that it is more efficient than the standard PDR combined with encodings.
We also evaluate the practical impact of various improvements, and compare the most successful configuration of PDRplan to the state of the art planners with encouraging results.
Section 6 returns to related work and uncovers a perhaps surprising connection between
PDR and the Graphplan algorithm of Blum and Furst (1997). Finally, Section 7 uses
examples of the behavior of PDR on two classical planning domains to discuss possibilities
for future extensions of the algorithm and Section 8 concludes.

2. Preliminaries
In this section we build the necessary background for explaining PDR. After recalling the
basic notions of propositional logic and fixing the notation, we introduce symbolic transition
systems, which serve as a canonical input for the algorithm. We observe that the encoding
part of the well-known planning as satisfiability paradigm can be seen to translate a given
planning problem into a symbolic transition system. This means that by combining an
encoding with PDR one already obtains a standalone planner.
2.1 Propositional Logic
A signature  is a finite set of propositional variables. Formulas are built from variables
using the propositional connectives negation , conjunction , disjunction , and implication . Given a formula F over  we denote by Vars(F ) the set of variables actually
occurring in F (i.e. Vars(F )  ). A literal l is either a variable p   or its negation p.
In the first case, the literal is called positive. We define the complement l of a literal l to
be p if l = p, and to be p if l = p. Also in other contexts, double negations are silently
reduced away.
A consistent conjunction of literals is referred to as a cube and a disjunction as a clause.
A cube r is full if Vars(r) = . When describing algorithms, it is advantageous to treat
both cubes and clauses simply as sets of literals and leave the interpretation to follow from
the context. Then, by complementing a cube r we obtain a clause r = {l | l  r} and
also vice versa. We call a clause positive if all its literals are positive. A clause c is said to
subsume another clause d if c  d. As usual, sets of clauses stand for their conjunction.
The semantics of propositional logic is built around the notion of an assignment, which
is a mapping s :   {0, 1} from the signature  to the truth values {0, 1}. We write
s |= F if an assignment s satisfies a formula F . A formula F is called satisfiable if there is
an assignment that satisfies it, and it is called valid if F is not satisfiable. Assignments
naturally correspond to full cubes: Given an assignment s we define a full cube
Lits(s) = {p | p   and s(p) = 1}  {p | p   and s(p) = 0}.
There is exactly one assignment, namely s, which satisfies Lits(s).
2.2 Encoding Discrete Time
When reasoning about systems which evolve in time, we use the basic signature  =
{p, q, . . .} to describe the current state of the system and introduce a disjoint copy of 
267

fiSuda

denoted 0 = {p0 , q 0 , . . .} to represent the state of the system after one step. Similarly,
further copies 00 , 000 , . . . (also written as (2) , (3) , . . .) stand for the states further in the
future. The priming notation is extended to formulas and assignments in the following way.
By F 0 we denote the formula obtained from a formula F by priming every variable occurring
in F . For an assignment s :   {0, 1}, we denote by s0 : 0  {0, 1} the assignment
that behaves on primed symbols as s does on unprimed ones, i.e., s0 (p0 ) = s(p) for every
p  . If s and t are two assignments from , we let (s, t0 ) denote the joint assignment
(s  t0 ) :   0  {0, 1}. This means that
(
s(p)
(s, t0 )(x) =
t(p)

if x = p  ,
if x = p0  0 .

Such an assignment gives a truth value to formulas over the joint signature   0 .
2.3 Symbolic Transition Systems
A symbolic transition system (STS) is a tuple S = (, I, G, T ), where  is a signature, I,
called the initial formula, and G, the goal formula, are sets of clauses over , and T , the
transition formula, is a set of clauses over   0 . An STS S symbolically represents an
explicit transition system TS = (S, SI , SG , RT ), which we describe next. Notice that the
symbolic representation can be exponentially more succinct than the explicit system. The
explicit transition system TS consists of
 the set of states S, identified with the set of all assignments from :
S = {s | s :   {0, 1}},
 a subset SI  S of the initial states, which are states that satisfy the initial formula:
SI = {s  S | s |= I},
 a subset SG  S of the goal states, which are states that satisfy the goal formula:
SG = {s  S | s |= G},
 and the transition relation RT  S S of pairs of states (also called transitions) which
jointly satisfy the transition formula:
RT = {(s, t) | s, t  S and (s, t0 ) |= T }.
A path in TS is a finite sequence s0 , . . . , sk of states such that (sj , sj+1 )  RT for every
j = 0, . . . k  1. We will be interested in the existence of paths connecting an initial state
with a goal state. We say that an STS S is satisfiable if there is a path s0 , . . . , sk in TS such
that s0  SI and sk  SG . For simplicity, we call such a path a witnessing path for S.
268

fiProperty Directed Reachability for Automated Planning

SI

SG
b

b

b

b

s01 = {p 7 0, q 7 1}

s00 = {p 7 0, q 7 0}

s11 = {p 7 1, q 7 1}
s10 = {p 7 1, q 7 0}

Figure 1: The explicit transition system TS represented by the STS S from Example 1. Its
four states correspond to the four assignments over the signature  = {p, q}.
Example 1. Consider an STS S = (, I, G, T ), where  = {p, q}, I = {p}, G = {p, q}, and
T = {p  p0  q 0 , p  q  p0 , p  q  q 0 , p  q  p0 , q  p0  q 0 }.

Notice that we prefer the formula notation (as opposed to the set notation) for concrete
clauses. This means that I consist of one and G of two unit clauses, i.e. clauses with just
a single literal. The corresponding explicit transition system TS is shown in Figure 1. The
path s00 , s01 , s10 , s11 is an example of a witnessing path for S. The STS S is satisfiable.
It is useful to notice that the definition of an STS is symmetrical in the following sense.
Given an STS S = (, I, G, T ) an inverted STS is defined as S 1 = (, G, I, T 1 ), where
T 1 is obtained from T by simultaneously removing primes from all the occurrences of
primed variables and adding primes to all the occurrences of originally unprimed variables.
This corresponds, on the explicit side, to exchanging the initial and goal states and inverting
the direction of all the transitions. Therefore, an STS S is satisfiable if and only S 1 is.
Moreover, a witnessing path for S can be recovered from a witnessing path for S 1 (also
vice versa) by reading the respective sequence backwards.
2.4 Propositional STRIPS Planning
In this paper we work with planning problems described in the STRIPS planning formalism. Similarly to states in transition systems, states of the world in STRIPS planning are
identified with propositional assignments. The propositional variables encoding the state
are in this context called state variables and we denote their set by X.
An action a is determined by a tuple a = (pre a , eff a ), where pre a , called the precondition
list, and eff a , the effect list, are cubes over X, i.e. consistent conjunctive sets of literals. An
action a is applicable in a state s if s |= pre a . If this is the case then applying the action
a in s results in a successor state t = apply(s, a), which is the unique state that satisfies
eff a and for every p  X not occurring in eff a it has t(p) = s(p). A degenerate action with
empty precondition and effect lists is called the noop action. It is applicable in any state s
and the corresponding successor is identical to the original state: apply(s, noop) = s.
A STRIPS planning problem is a tuple P = (X, sI , g, A), where X is the set of state
variables, sI the initial state, g the goal condition in the form of a cube over X, and A a
set of actions. A plan for P is a finite sequence a1 , . . . , ak of actions from A such that there
are states s0 , . . . , sk satisfying the following conditions:
269

fiSuda

 s0 = sI ,
 aj is applicable in sj1 for j = 1, . . . , k,
 sj = apply(sj1 , aj ) for j = 1, . . . , k,
 and sk |= g.

Notice that the empty sequence  is a plan for P if and only if sI |= g.
2.5 Planning as Satisfiability
The basic idea behind the planning as satisfiability paradigm (Kautz & Selman, 1992, 1996)
is as follows. Given a planning problem P we define a sequence of propositional formulas
F0 , F1 , . . . such that there is a plan for P if and only if a formula Fi is satisfiable for some
i. The individual formulas Fi are then iteratively checked using a SAT-solver and when a
satisfiable Fi is found, a plan is recovered from the corresponding satisfying assignment.
The concrete form of the formulas in the sequence is dictated by an encoding scheme
(see, e.g., Kautz, McAllester, & Selman, 1996; Rintanen, Heljanko, & Niemela, 2006; Huang,
Chen, & Zhang, 2012). Most encoding schemes have a simple structure that can be captured
by an STS S = (, I, G, T ), from which the individual formulas Fi are then obtained as
Fi = I  T (0)  T (1)  . . .  T (i1)  G(i) .

(1)

Note that we use the priming notation as described in Section 2.2 and thus T (0) stands for
the same formula as T , T (1) S
for the same formula as T 0 , etc. The resulting formula (1) for
Fi , which is over signature j=0,...,i (i) , expresses the existence of a witnessing path for
S of length i. If the encoding scheme uses a so called sequential plan semantics, such a
witnessing path also directly corresponds to a plan of length i. This is equivalent to saying
that the transition relation encoded by T allows for application of a single action in one
step. Parallel plan semantics allow multiple actions to be applied in one time step. This
leads to a more compact representation and potentially faster discovery of plans. Additional
conditions on the parallel actions need to be imposed, however, to guarantee that a true
sequential plan can be recovered in the end (see Rintanen et al., 2006, for more details).
2.6 Two Simple Encodings
We close this section by introducing two example encodings of a STRIPS planning problem
P = (X, sI , g, A) into an STS. They are perhaps the simplest representatives of encoding
schemes with the sequential and parallel plan semantics, respectively. We will later refer to
them in our theoretical considerations.
The transition systems SPseq and SPpar corresponding to the two encodings share several
building blocks. Let the signature  consist of the state variables X in union with a set
of fresh auxiliary variables A = {pa | a  A} used for encoding applied actions. Further,
let us identify the initial formula I with the cube Lits(sI ) and define the goal formula G
by reinterpreting the goal condition g, which is formally a cube, as a set of unit clauses
G = {{l} | l  g}. The action mechanics is in both encodings captured by the following
action precondition axioms AP and action effect axioms AE :
AE = {pa  l0 | a  A, l  eff a }.

AP = {pa  l | a  A, l  pre a },
270

fiProperty Directed Reachability for Automated Planning

The encodings differ in how they formalize the preserving part of actions semantics.
The sequential encoding SPseq relies on the so called classical frame axioms
CF (McCarthy
W
& Hayes, 1969) complemented by the single at-least-one axiom alo = aA pa :
CF = {pa  l  l0 | a  A, l literal over X such that l 6 eff a and l 6 eff a }.
Putting these together, we obtain SPseq = (, I, G, T seq ), where T seq = AP  AE  CF  alo.
Note that the at-least-one axiom is needed, because without it a transition into an arbitrary
state would be possible from a state where no action is applied, i.e. a state in which pa is
false for every a  A. On the other hand, the classical frame axioms ensure that if two
actions are applied together in a state their effects must be identical. Thus when extracting
a (sequential) plan from a witnessing path for SPseq we can arbitrarily choose in each step
any action a  A such that pa is true in the corresponding state.
The parallel encoding SPpar uses the following explanatory frame axioms EF (Haas, 1987)
EF = {l  l0 

W

aA leff a pa

| l literal over X},

in combination with the so called conflict exclusion axioms CE
CE = {pa  pb | a, b  A, a 6= b, and the actions a and b are conflicting},
where two actions are considered conflicting if ones precondition is inconsistent with the
others effect, i.e. if there is a literal l over X such that
either l  pre a and l  eff b , or l  pre b and l  eff a .
In sum, we define SPpar = (, I, G, T par ) where T par = AP  AE  EF  CE . In this encoding
two actions can be applied in parallel if they have consistent effects (action effect axioms)
and one does not destroy a precondition of the other (conflict exclusion axioms). When
recovering a sequential plan, such parallel actions can be serialized in any order.
Please consult the work of Ghallab, Nau, and Traverso (2004, ch. 7.4) for further details.

3. Property Directed Reachability
In this section we present PDR as an algorithm for deciding satisfiability of symbolic transition systems. The algorithm is probably best understood as an explicit search through the
given transition system complemented by symbolic reachability analysis. On the explicit
side, it constructs a path starting from an initial state and extending it step by step towards
the goal.2 At the same time, it maintains symbolic stepwise approximating reachability information, which is locally refined whenever the current path cannot be extended further.
The reachability information guides the path construction and is also bound to eventually
converge to a certificate of non-reachability, if no witnessing path exists.
2. In the standard formulation, PDR actually builds the path the other way round, from a goal state
backwards towards an initial state. This is only a small detail from the theory point of view, since the
definition of an STS is symmetrical. On the other hand, as we later show, the direction we adopt here
gives rise to a much more successful algorithm on typical planning benchmarks.

271

fiSuda

3.1 Extension Query and Reason Computation
Let us assume an STS S = (, I, G, T ) is given. While the basic building block for the
constructed path is a state, the reachability information is composed of sets of clauses. The
core operation, around which the algorithm is built, is extending the current path by one
step. Given a state s and a set of clauses L, we ask whether there is a state t, a successor
of s with respect to T , satisfying the clauses of L. Such a question can be delegated to a
SAT-solver by posing the following query:
SAT ?[ Lits(s)  T  (L)0 ].

(2)

If the answer is positive, we can extract a successor state t from the satisfying assignment,
which is necessarily of the form (s, t0 ), and extend the current path. In the unsatisfiable
case, we compute a reason for why s does not have a successor with the property L. A
reason is a cube r  Lits(s) such that already the formula r  T  (L)0 is unsatisfiable. PDR
then removes s from the path and learns the clause c = r to prevent the same situation
from happening in the future. The clause c is a property of the preimage of L with respect
to T which the state s fails to satisfy.
It is important for the efficiency of the algorithm to always compute a reason which is as
small as possible. This is because a reason with fewer literals gives rise to a shorter clause,
which better generalizes from the current situation. After such a clause is learned not only
the state s but also many similar states will be known to have no successor satisfying L.
There are several techniques for computing small reasons. Below we describe two of
them: SAT-solving under assumptions and explicit minimization. We postpone discussing
a third one, inductive minimization, till Section 3.5. It is important for the correctness of
PDR that the final reason is not consistent with the goal formula G. We close this section
by explaining how this property can be achieved.
3.1.1 SAT-solving under Assumptions
Many modern SAT-solvers do not simply return UNSAT, but are able to identify which
of the input clauses were actually used in the derivation of unsatisfiability. Solving under
assumptions is a particular, simple and efficient form of such unsatisfiable core extraction
technique, first introduced by Een and Sorensson (2003) in their SAT-solver Minisat. By
assumptions we in this technique understand designated unit (single literal) clauses passed
along with the rest of the input to the solver. In case the input is unsatisfiable, the solver
is able to report which of these units were actually used in the proof.
Solving under assumptions provides us with essentially free mechanism for computing
small reasons. We simply designate the literals of Lits(s) to be treated as unit assumptions
in the query (2) above. In the unsatisfiable case, we obtain a reason r  Lits(s) as required.
3.1.2 Explicit Minimization
The subset r of the assumption literals Lits(s) returned by the solver can typically be
further reduced. We can explicitly minimize it by trying to remove literals one by one.
If the respective query remains unsatisfiable we leave the literal out. Otherwise we put it
back. In a number of steps proportional to |r| we obtain a final reason set r  r minimal
272

fiProperty Directed Reachability for Automated Planning

with respect to subset relation such that the query
SAT ?[ r  T  (L)0 ],
is unsatisfiable. Note that the order in which the literals are tried out influences the final
result and may be subject to heuristical tuning. Although reason minimization is an expensive operation (we need one extra SAT-solver call per literal), experiments show that it
is an important ingredient for solving hard problems.
3.1.3 Keeping the Reason Disjoint from G
As will become clear later, to ensure correctness of PDR we require that the computed
reason r is not consistent with the goal formula G. In other words, the formula r  G must
be unsatisfiable. This can be always achieved, because the algorithm never attempts to
extend a goal state and thus we only start minimizing with unsatisfiable Lits(s)  G.
A particularly simple strategy, which works whenever the goal formula G is in the form
of a set of unit clauses (as is the case in planning), minimizes the reason as much as possible,
but afterwards puts a single literal back to r provided the required unsatisfiability condition
would be otherwise compromised. We look for a literal l to be added to r (unless found to
be already present) such that {l}  G. This condition is, under our assumptions, both
sufficient and necessary to ensure that r  G is unsatisfiable and can be established without
additional calls to a SAT-solver.
Another option is to make sure beforehand that the set of goal states is included in its
own preimage with respect to the transition relation T . For instance, making the transition
relation reflexive by adding self-loops to every state achieves this without actually affecting
the existence or the length of the shortest witnessing path. In planning, we can simply
include the noop action into the action set.
3.2 Data Structures and Main Invariants
We continue our exposition of PDR by describing its main data structures. The clause sets
representing the reachability information are organized in a sequence3 L0 , L1 , . . . and we
refer to them as layers. At any moment during the run of the algorithm the layers satisfy
the following three invariants:
1) L0 is equivalent to G,
2) Lj+1  Lj and thus Lj  Lj+1 for any j  0,
3) (Lj )0  T  Lj+1 for any j  0, i.e., Lj+1 over-approximates the preimage of Lj .

When the algorithm starts, the layer L0 is initialized to be equal to the set G and all
the remaining layers are empty. Thus, initially, the invariants 1), 2), and 3) are trivially
satisfied. We will return to the invariants at an appropriate place below to argue that they
are indeed maintained by the algorithm.
The constructed paths  there can actually be more than one  are represented via
so called obligations.4 Formally, an obligation is a pair (s, i) consisting of a state s and
3. At any point in time only finitely many layers are non-empty and need to be represented in memory.
4. The full term is proof obligation. It comes from the verification perspective, where an obligation must be
proven unreachable, otherwise the property of the system does not hold and a counter-example is found.

273

fiSuda

an index i. The index is a natural number denoting the position of s with respect to the
layers. It may be seen to stand for a lower bound estimate on the distance of s towards the
goal. In a practical implementation an obligation also stores a link to its parent, i.e. to the
obligation from which it was derived, and we use such links to recover the full witnessing
path once the goal has been reached.
3.3 The Algorithm
We are now ready to have a look at the overall structure of PDR (see Pseudocode 1). After
initializing the layers (line 1), the algorithm proceeds in iterations, counted by a variable
k. The main part of each iteration is a path construction phase during which the algorithm
attempts to build a path step by step and terminates if a full witnessing path is actually
discovered. If iteration k finishes without completing the path, PDR has established that
there is no witnessing path for the transition system of length k or less.
As we describe in detail below, path construction can be enhanced by the obligation
rescheduling technique, which allows the algorithm during iteration k to consider paths
potentially longer than k. Path construction is in each iteration complemented by a clause
propagation phase, which attempts to push clauses from low index layers to high index
ones and checks for a global convergence within the layers, the occurrence of which implies
that no witnessing path (of any length) is possible. Neither obligation rescheduling nor
clause pushing are needed for ensuring correctness of the algorithm, but typically greatly
improve its performance.
3.3.1 Path Construction
The path construction phase of iteration k starts by using a SAT-solver to pick an initial
state s satisfying Lk (lines 4 and 5). It manipulates a set Q, working as a priority queue,
for storing obligations. The set Q is initialized (line 6) with the obligation (s, k). The
inner loop (starting at line 7) processes individual obligations, selecting first those that are
estimated to be closer to the goal (line 8). Let (s, i) be a selected obligation. When i = 0,
this means that a full witnessing path has been constructed and the algorithm terminates
(line 10). The path extension query described previously is executed next (line 11). We look
for a successor of s which would satisfy the clauses of Li1 . Since s was originally obtained
satisfying Li , this represents an attempt to extend the current path one step closer towards
the goal. If the extension is successful both the new obligation (t, i1) to be worked on next
and the current (s, i) are stored in Q (lines 12, 13). In the opposite case, a new clause is
derived from the reason of the failure and used to strengthen the layers L0 , . . . , Li (lines 15,
16). Notice that after the strengthening the state s no longer satisfies Li . This means the
approximation has become strictly more precise, which ensures progress.
3.3.2 Obligation Rescheduling
The unsatisfiable branch of the extension attempt continues with two more lines (19, 20),
which are not necessary for correctness of PDR. Without this obligation rescheduling technique, the algorithm would now forget the obligation (s, i) and would return to work on its
parent. This would correspond to a strict backtracking behavior, with the set Q functioning
as a stack. Instead, we try to reuse the obligation and reschedule it one step further from
274

fiProperty Directed Reachability for Automated Planning

Pseudocode 1 Algorithm PDR(, I, G, T ):
Input:
A symbolic transition system S = (, I, G, T )
Output:
A witnessing path for S or a guarantee that no path exists
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:

L0  G; foreach j > 0 : Lj   /* Initialize the layers */
for k = 0, 1, . . . do
/* Path construction: */
while SAT ?[ I  Lk ] do
extract state s from the model
Q  {(s, k)}
while Q not empty do
pop some (s, i) from Q with minimal i
if i = 0 then
return WITNESSING PATH FOUND
if SAT ?[ Lits(s)  T  (Li1 )0 ] then
extract a successor state t from the model
Q  Q  {(s, i), (t, i  1)}
else
compute a reason r  Lits(s) such that r  G is unsatisfiable
foreach 0  j  i : Lj  Lj  {r}
/* Obligation rescheduling: */
if i < k then
Q  Q  {(s, i + 1)}
/* Clause propagation: */
for i = 1, . . . , k + 1 do
foreach c  Li1 \ Li do
/* Clause push check */
if not SAT ?[ c  T  (Li1 )0 ] then
Li  Li  {c}
/* Convergence check */
if Li1 = Li then
return NO PATH POSSIBLE

275

fiSuda

b

(t, 1)

b

(s, 2)
I

L2

G
?
L0

G
b

r
b

(s, 2)
I

L1

b

b

(t, 2)

L2

L0
L1

Figure 2: Layers, obligations and rescheduling.
the goal. This typically boosts the performance as it allows PDR to discover paths longer
than the current iteration number k, before the existence of paths of length k is ruled out.
Without obligation rescheduling, PDR only looks for optimal length paths.
The obligation rescheduling technique as well as the general interaction between layers,
obligations, and reasons are illustrated in Figure 2. There, PDR is in the middle of the path
construction phase of iteration 2. The algorithm is attempting to extend the obligation (t, 1)
and to reach a goal state in one step (left). When the attempt fails (right), PDR generalizes
from t, obtains a reason r, and learns a new clause c = r to strengthen the layers L1 and
L0 . Notice how the obligation is rescheduled to (t, 2) and PDR can now attempt to extend
it and satisfy the new L1 in one step. Without rescheduling, PDR would forget t and would
go back to extending (s, 2) instead.
3.3.3 Clause Propagation
Let us proceed to the clause propagation phase. It checks for every clause c lying in layer
Li1 but not in Li (line 24) whether it could be pushed forward and added to strengthen
the layer Li . This is done when the query on line 26 returns UNSAT, which means that
(Li1 )0  T  c,
and adding the clause to Li will preserve invariant 3). The motivation for clause propagation
is that stronger layers will provide for a better guidance in the following path construction
phase. Indeed, notice that some clauses may even enter the till now empty layer Lk+1 . More
importantly, however, clause propagation is an opportunity to check for equality between
two neighboring layers (line 29).5 As we explain later, the equality implies that there is no
witnessing path for the transition system of any length and so PDR terminates (line 30).
3.3.4 Example Execution
Let us recall the STS S = (, I, G, T ) from Example 1, defined over a two variable signature
 = {p, q} with the initial, goal, and transition formulas I = {p}, G = {p, q}, and
T = {p  p0  q 0 , p  q  p0 , p  q  q 0 , p  q  p0 , q  p0  q 0 },
5. It is not necessary to perform the relatively expensive clause pushing (which requires one SAT-solver call
per clause) before checking for layer equivalence. Experience from model-checking suggests, however,
that pushing substantially helps to speed up the convergence and clause propagation is one of the key
ingredients for efficiently detecting unsatisfiable problems.

276

fiProperty Directed Reachability for Automated Planning

step
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

event / query

effect / explanation

Initialization
Path construction (iteration 0)
SAT ?[ I  L0 ] false
Clause propagation
SAT ?[ p  T  (L0 )0 ] false
push p and add it to L1
SAT ?[ q  T  (L0 )0 ] true
Path construction (iteration 1)
SAT ?[ I  L1 ] false
Clause propagation
SAT ?[ q  T  (L0 )0 ] true
SAT ?[ p  T  (L1 )0 ] true
Path construction (iteration 2)
SAT ?[ I  L2 ] true
extract a state and initialize Q
SAT ?[ Lits(s)  T  (L1 )0 ] false
compute a reason and learn a clause
SAT ?[ I  L2 ] true
extract a state and initialize Q
SAT ?[ Lits(t)  T  (L1 )0 ] true
extract a state and store it in Q
SAT ?[ Lits(u)  T  (L0 )0 ] true
extract a state . . .
. . . and store it in Q
witnessing path found

L0 = {p, q}; Li =  for i > 0
k=0
p  p unsat.
p  (p  p0  q 0 )  (p  q)0 unsat.
L1 = {p}
q  T  (p  q)0 sat.
k=1
p  p unsat.
q  T  (p  q)0 sat.
p  T  p0 sat.
k=2
p sat.
s = {p 7 0, q 7 0}; Q = {(s, 2)}
p  q  (p  q  p0 )  (p)0 unsat.
r = p  q; L2 = {p  q}, L1 = . . .
p  (p  q) sat.
t = {p 7 0, q 7 1}; Q = {(t, 2)}
p  q  T  (p)0 sat.
u = {p 7 1, q 7 0}; Q = {(t, 2), (u, 1)}
p  q  T  (p  q)0 sat.
v = {p 7 1, q 7 1};
Q = {(t, 2), (u, 1), (v, 0)}
return t, u, v

Table 1: Execution trace of PDR on the STS from Example 1.

respectively. Table 1 showcases an execution trace of PDR on this STS. It lists all the
SAT-solver queries in order of execution, provides explanation for the results in the form
of unsatisfiable cores, and tracks changes to the global variables. Notice that in step 6 the
clause p is successfully pushed from layer L0 to L1 . In step 17 the new clause p  q is
formally added to L2 , L1 , and L0 . However, it only properly strengthens the layer L2 .

3.4 Correctness
To show correctness of PDR we first review invariants 1)3) and demonstrate that they
are indeed preserved during the run of the algorithm. We than add a fourth observation,
invariant 4), important for showing correctness in the unsatisfiable case. The invariants are
then used to prove an independent lemma and, finally, also the main correctness theorem.
277

fiSuda

3.4.1 Four Invariants
Invariant 1) states that the layer L0 is equivalent to G. This could be only violated when
a new clause c is added to L0 (line 16). But since c = r for some reason r and we assume
that r G is unsatisfiable (recall Section 3.1.3), we have G  c and invariant 1) is preserved.
Invariant 2) asserts that Lj+1  Lj for any index j  0. This is trivially maintained both
when a new clause is added to the layers (line 16) and during clause pushing (line 27).
Invariant 3) is the statement that (Lj )0 T  Lj+1 for any j  0. When a new clause c is
added to the layers L0 , . . . , Li (line 16) there has been an unsuccessful extension of obligation
(s, i), which means that c = r for some reason r  Lits(s). By the definition of a reason
we have that the formula r T (Li1 )0 is unsatisfiable and, therefore, (Li1 )0 T  c. This
guarantees that invariant 3) will hold for j = i  1 after clause c is added to Li . However,
because the layers of index j < i  1 are even stronger than Li1 by invariant 2), invariant
3) will also hold for j < i  1. Finally, the case of j > i  1 is trivial. As already discussed
(recall Section 3.3.3) invariant 3) is also preserved during clause propagation.
There is one more observation that we will need in order to show correctness of PDR.
Let us call it invariant 4). Invariant 4) states that when the path construction of iteration
k finishes there is no initial state satisfying Lk . This follows from the fact that the query
on line 4 must be unsatisfiable for the path construction to finish.
3.4.2 Correctness and Termination
Lemma 1. When PDR creates (either on line 6 or on line 13) a new obligation (s, i) then
s |= Li . Moreover, s 6|= Lj for any j < i. This latter property is maintained throughout the
run of the algorithm and, in particular, holds also after rescheduling (line 20).
Proof. First note that it is sufficient to show the second part only for j = i  1 and then
use invariant 2). Also note that during the run of PDR clauses are only added to and never
removed from the layers. This means it is sufficient to focus on the moments when a new
obligation is created: if s 6|= Lj when the obligation (s, i) is created, this must also hold
later, after the layer Lj has been strengthened by addition of new clauses.
Let us now consider iteration k. When creating a new obligation (s, k) on line 6, we have
s |= Lk by construction and s 6|= Lk1 by invariant 4). When creating a new obligation
(t, i  1) on line 13, we assume that its parent (s, i) already satisfies our lemma and, in
particular, that s 6|= Li1 . We now have t |= Li1 , again by construction, and if i > 1 we
infer t 6|= Li2 from our assumption about s and from invariant 3). Finally, an obligation
(s, i) is only rescheduled to (s, i + 1) after the addition of a clause c = r into Li for some
r  Lits(s). This means that s 6|= Li at the time of the rescheduling.
Lemma 1 captures the intuition that a state s of an obligation (s, i) is always at least
i steps from reaching the goal. It follows from this lemma that PDR never attempts to
extend a goal state, which is the assumption we relied on in Section 3.1.3 to show that we
can always keep reasons disjoint from the goal formula G.
Theorem 1 (Bradley, 2011). Given an STS S = (, I, G, T ) the algorithm terminates and
returns a witnessing path for S if and only if S is satisfiable.
278

fiProperty Directed Reachability for Automated Planning

Proof. It is easy to see that if PDR returns a path (line 10)6 it is a witnessing path for S.
Indeed, for every considered obligation (s, i) the state s is reachable from an initial state
and when i = 0 the state s satisfies L0 , which is equivalent to G by invariant 1).
If PDR terminates claiming that no witnessing path exists (line 30) the path construction
phase of iteration k has finished and there is an index 0  j  k such that Lj = Lj+1 . By
combining invariants 1)3) and the detected equality we obtain G  Lj and (Lj )0 T  Lj .
This together with invariant 4) rules out the existence of a witnessing path of any length.7
To address termination we first show that the path construction phase of iteration k
cannot run indefinitely. Recall that PDR always selects for extension an obligation with
minimal index i (line 8). Thus it follows from Lemma 1 that after a successful extension
of obligation (s, i) the new extracted state t is not equal to any other state previously
considered during iteration k (t is currently the only state that satisfies Li1 ). On the other
hand, after an unsuccessful extension of obligation (s, i) the addition of the new clause c
to the layer Li ensures that s 6|= Li anymore (recall that c = r for some r  Lits(s)).
This means there cannot be more than k  2|| repetitions of the Q-processing while-loop
(line 7) and more than 2|| repetitions of the outer while-loop of path construction (line 4).
We are left to bound the maximal number of iterations of PDR. By invariant 2) the sets
of states represented by the individual layers are ordered by inclusion and after the clause
propagation phase of iteration k finishes the first k + 1 of these sets are necessarily distinct.
Thus there cannot be more than 2|| iterations before PDR terminates.
3.5 Inductive Reason Minimization
Inductive reason minimization is a technique for obtaining small reasons from unsuccessful
extensions. We postponed discussing the technique after the presentation of PDR, because
it relies in an non-obvious way on the overall architecture of the algorithm. Moreover,
although inductiveness was one of the main initial ideas behind PDR (Bradley, 2011),
our experiments suggest that the practical value of this relatively advanced technique for
automated planning may be limited.
To demonstrate inductive minimization let us once more recall the situation of an unsuccessful extension of obligation (s, i). We want to compute a reason r, which is a subset
of Lits(s), ideally as small as possible, such that the formula
r  T  (Li1 )0

(3)

is unsatisfiable. Now, the crucial observation is that since we are in the next step going to
strengthen the layers L0 , . . . , Li (and, in particular, the layer Li1 ) with the clause c = r,
we may already assume c on the primed side of (3) when minimizing r. This means, we
can use the stronger query
r  T  (Li1  r)0 .
Having r on both sides of the transition breaks monotonicity: as r gets weaker, r gets
stronger. Satisfiable query may become unsatisfiable again when more literals are removed
6. Line 10 of Pseudocode 1 only reports the existence of a path. A true witnessing path can, however, be
easily recovered by following the parent pointers (see Section 3.2) from the last obligation (s, 0).
7. The layer Lj can be understood as the promised certificate of non-reachability. It is a property of the
goal states incompatible with the initial formula that is preserved by traversing the transitions backwards.

279

fiSuda

from r. This makes the task of finding subset-minimal inductive reason computationally
difficult (Bradley & Manna, 2007).
Pseudocode 2 Inductive Reason Minimization:
Input:
A set of clauses L and a cube r such that
the formulas r  T  (L)0 and r  G are unsatisfiable
Output:
Minimized inductive reason r  r, i.e.,
the formulas r  T  (L  r )0 and r  G are unsatisfiable
1:
2:
3:
4:
5:
6:
7:
8:

repeat
r0  r
foreach l  r do /* Check each literal of r0 once */
if there is l0  (r0 \ {l}) such that {l0 }  G then /* Can try removing l */
r0  (r0 \ {l})
if SAT ?[ r0  T  (Li1  r0 )0 ] then
r0  (r0  {l}) /* Put the literal back */
until r = r0 /* No removal in the last iteration */

9:
10:

return r

In Pseudocode 2 we present a simple version of inductive reason minimization with no
minimality guarantee, which was, however, successfully applied in hardware model checking
(Een et al., 2011). The procedure is meant to improve on and replace the explicit reason
minimization described in Section 3.1.2. It assumes that the goal formula G is in the form
of a set of unit clauses to keep the reason disjoint from G (see Section 3.1.3). Notice that in
the non-monotone setting of inductive minimization it makes sense to retry all the literals
once a single literal has been successfully removed. That is why the procedure employes
the outer loop to continue minimizing till a true fixed point is reached.
3.6 Notes on Implementation
There are several important points relevant for a practical implementation of PDR which
did not fit the level of detail of the presented pseudocode. We moved them to this section.
3.6.1 Representation of the Layers
Because the individual clause sets Li are ordered by inclusion it is advantageous to store
each clause only once, namely at the position where it appears last. This convention has
been named delta encoding by Een et al. (2011). It is defined by setting
i = Li \ Li+1

and

Li =

S

ji j .

With delta encoded layers clause propagation moves clauses around instead of copying
them and the equality check between neighboring layers becomes an emptiness check of the
respective delta.
280

fiProperty Directed Reachability for Automated Planning

3.6.2 Clause Subsumption
It has been observed that PDR often derives a clause c into a layer Li while a weaker clause
is already present. It pays off to remove from i (so effectively from L0 , . . . , Li ) all those
clauses which are subsumed by this new clause c. Keeping the layers small this way (while
preserving their semantic strength) helps to speed up the algorithm in several places. It
reduces the load on the SAT-solver (provided we can retract the subsumed clause from it)
and it also means fewer clauses need to be checked for pushing.
3.6.3 Obligation Subsumption
Another place where subsumption can (and should) be employed is when dealing with
obligations. It may happen that before an obligation (s, i) is handled, the layer Li , where
the obligation belongs, gets in the meantime strengthened in such a way that s no longer
satisfies Li . At this point we already know that the obligation cannot be extended, so we
can save one SAT-solver call and directly reschedule the obligation. Such a situation can
be detected by subsumption: a state s does not satisfy a clause set L if and only if there
is a clause c  L such that c  Lits(s). We insert the test at the point in the algorithm
where a new clause c is derived into Li . We then check for subsumption all the obligations
of the form (s, i) that are currently in the set Q.
3.6.4 Breaking Ties when Popping from Q
When popping obligations from the set Q (line 8) we make sure we select among those
estimated closest to the goal. This is necessary for ensuring termination of the algorithm.
Otherwise, however, we are free to choose any obligation with the minimal index i. Two
prominent strategies for resolving this dont-care non-determinism are
 to select the most recently added obligation first, which we call the stack strategy,
 to select the least recently added obligation first, the queue strategy.
The stack strategy prefers exploring longer paths before short ones, while the queue strategy
does the opposite. Een et al. (2011) report a small performance gain with the stack strategy
on hardware model checking benchmarks. We used the stack strategy as the default in our
experiments and observed its superiority over the queue strategy in satisficing planning, but
also its slightly unfavorable effect on plan quality (see Section 5.3.3).

4. PDR without a SAT-solver
Although it is possible to encode a STRIPS planning problem into an STS and use a general
implementation of PDR to solve it, a more efficient approach can be adopted. The approach
relies on an observation that the work normally delegated in PDR to the SAT-solver can
in the case of planning with the sequential plan semantics be instead implemented directly
by a planning-specific procedure. Not only do we gain with this procedure a polynomial
time guarantee for the response of each extension query, but the ensuing perspective also
enables us to devise new improvements of the overall algorithm.
The SAT-solver is employed in several places within PDR. We will start by focusing on
its primary role which lies in extending the current path by one step. In Section 4.1 we
281

fiSuda

develop procedure extend to replace the SAT-solver in path extension queries. A separate
section is then devoted to discussing inductive reason minimization in the planning context.
In Section 4.3 we deal with replacing the remaining SAT-solver calls. We show how to
efficiently implement clause pushing for positive STRIPS planning problems, a subclass
of STRIPS problems that is typically used in practice. We then discuss the possibility of
reversing the default search direction of PDR in Section 4.4 and, finally, we propose several
improvements of the algorithm in Section 4.5.
4.1 Planning-Specific Path Extensions
Let us recall the interface for path extensions, which is normally implemented in PDR by
a call to a SAT-solver (Section 3.1). Given a state s and a set of clauses L decide whether
there exists a state t, a successor of s with respect to the transition relation T , such that t
satisfies L. In the positive case, which we refer to as a successful extension, return such a t.
In the negative case, when no such a successor exists, compute a reason r for the failure in
the form of a preferably small subset of the literals defining s, such that no state satisfying
r has a successor that would satisfy L.
Let us assume a STRIPS planning problem P = (X, sI , g, A) is given. We now gradually
work towards a planning-specific implementation of the above interface within the procedure
extend(s, L). Our central idea is to emulate the mechanics of the sequential encoding SPseq
(see Section 2.6). This makes the implementation particularly straightforward from the
perspective of the positive part of the interface. Given a state s, we can simply iterate over
all the actions a  A, generate a successor ta = apply(s, a) whenever a is applicable in s,
and check for each ta whether it satisfies the clauses of L. If such a successor is found,
it is returned and the procedure terminates. Such an iteration is clearly affordable from
the complexity point of view. In fact, it is very similar in spirit to what all explicit state
planners need to do: they enumerate successor states and evaluate their heuristic value.
The non-trivial part of the extend procedure deals with computing a small reason in the
case of an unsuccessful extension. We conceptually simplify the problem by first separately
collecting a set of reasons Ra for every action a  A and then computing the overall reason
r as a union
[
r=
ra
(4)
aA

of reason contributions ra  Ra selected in a way that minimizes the size of the union. The
idea is that each ra  Ra is a distinct reason for why the action a cannot be applied in s to
produce a successor state t that would satisfy all the clauses from L. The union (4) then
justifies why there is no such a successor state via any action a  A whatsoever.
In the rest of this section, we first explain how the individual reasons ra  Ra for an
action a  A are derived from the actions failed preconditions and from those clauses of L
which the respective successor state fails to satisfy. We then show how this reason collecting
process can be in practice sped up by employing certain subsumption concepts. Finally, we
present our approach to obtaining a small overall reason r, along with a detailed pseudocode
of the extend procedure and a proof of its correctness. To satisfy the requirement of PDR
that the final reason be disjoint from the set of goal states, we here adopt the solution of
formally adding the noop action to the action set (see Section 3.1.3).
282

fiProperty Directed Reachability for Automated Planning

4.1.1 Reasons for Individual Actions
We construct the set of reasons Ra for a particular action a as follows. First we check
whether the action a is applicable in the given state s. If not then there is a precondition
literal l  pre a false in s. The negation of each such literal represents a singleton reason
{l}  Lits(s) which we add to Ra . Clearly, as long as a state satisfies l there is no way
a can be used to produce a successor state, let alone one that would satisfy L.
Next, we compute the successor state ta = apply(s, a). Strictly speaking, ta cannot be
regarded as a true successor if a is not applicable in s. Nevertheless, ta is useful even then
for computing further reasons, namely reasons corresponding to clauses of L that are false
in ta . These are either clauses that were already false in s and a failed to make them true
or clauses that became false due to an effect of a. For each such clause c we add to Ra a
reason rc consisting of negations of literals l  c. As an optimization, we only include those
negated literals which were not made false by an effect of a. Since the other literals will
always be false after a is applied due to its effects, as long as s satisfies rc , the successor ta
cannot satisfy c. Summarizing formally, this is the final set of reasons we obtain:
Ra = {{l} | l  pre a and s 6|= l}  {rc | c  L and ta 6|= c},
where rc = {l | l  c and l 6 eff a }. It is easy to check that rc  Lits(s) as required.
Notice that the set Ra is empty if an only if the action a is applicable in s and the successor
ta satisfies all the clauses from L.
Example 2. Starting from a state s = {o 7 0, p 7 0, q 7 0, r 7 0}, let us compute the
reasons for an action a = (pre a , eff a ) with pre a = {p, q} and eff a = {o, r} with respect
to the clause set L = {o  q, p  r}. Because the precondition q is not satisfied in s, one
reason is {q}. Next we compute ta = apply(s, a) = {o 7 1, p 7 0, q 7 0, r 7 0}. The
first clause, o  q is satisfied in ta and so does not give rise to a reason. The second clause,
p  r, however, is false in ta . The reason corresponding to the second clause is {p}. The
other negated literal, r, is not part of the reason, because it was explicitly set to false by
an effect of a. The final reason set Ra we obtain is thus {{p}, {q}}. Notice that both
the computed reasons are subsets of Lits(s) = {o, p, q, r}.
Correctness of the reason set construction is captured by the following lemma.

Lemma 2. Let ra  Ra be any reason for an action a  A  {noop} as defined above. Then
ra  AP  AE  CF  (L)0 |= pa ,
where AP , AE and CF are, respectively, the action precondition, the action effect and the
classical frame axioms used in the transition formula T seq of the sequential encoding SPseq .
Proof. Let us first assume that ra = {l} is a reason derived from a failed precondition
literal l  pre a . There is an action precondition axiom pa  l  AP from which the
conclusion pa follows by a single resolution inference with the unit assumption l.
The other possibility is that ra = {l | l  c and l 6 eff a } for some clause c  L false
in the successor state ta . There must be an action effect axiom pa  l0  AE for every
literal l  c such that l  eff a and also a classical frame axiom pa  l  l0  CF for every
literal l  c such that l 6 eff a (if the literal l was in eff a the clause c would be satisfied in
283

fiSuda

ta ). By resolving these axioms on the respective primed
literals l0 with the primed version
W
0
0
(c)  (L) of the clause c we obtain a clause pa  lra l from which the final unit clause
pa can be derived by resolution with the available assumptions from ra .
4.1.2 Reason Subsumption
Before we describe how to compute the overall reason r from the actions contributions
Ra , let us note that there are two useful notions of subsumption both between individual
reasons and between reason sets, which can be used to simplify the reason sets before the
computation is started. The subsumption between individual reasons inside one particular
Ra is simply the subset relation. It does not make sense to keep both r1 and r2 inside Ra if
r1  r2 . Keeping the smaller r1 is sufficient, because whenever we would decide to pick r2
as the reason for a inside the union (4), switching to r1 instead could only make the result
smaller. In practice, we only check for this kind of subsumption between the unit reasons
of failed preconditions and the reasons from the false clauses.8 This can be implemented by
simply ignoring those false clauses that would have been true if the action was applicable.
Dually to the above, we can discard the whole reason set Ra of an action a if there is
another action b with reason set Rb such that
rb  Rb ra  Ra ra  rb .
Here we remove the reason set Ra , which is in some sense more lean, because for any
contribution rb  Rb there is a choice for ra  Ra which would be dominated by rb in
the final union r. For efficiency reasons, we only exploit this trick in our implementation
with respect to one particular action in the role of the subsuming action b, namely the
noop action. As mentioned before, we include the noop action to the action set to ensure
PDRs correctness. Its reason set Rnoop consists of reasons corresponding to those clauses
from L which are false in s.9 If an action a does not make any of these clauses true in its
corresponding successor state, its reason set Ra will be subsumed in the described sense by
Rnoop and can be skipped.
4.1.3 Computing the Overall Reason
Computing the overall reason r amounts to selecting for every a a particular reason ra  Ra
such that union (4) is as small as possible. Stated in this general form we are facing an
optimization version of an NP-complete problem. In fact, it is easily seen to be a dual of the
Maximum Subset Intersection problem shown NP-complete by Clifford and Popa (2011).
We therefore do not attempt to find an optimal solution for it and contend ourselves with
a reasonable approximation instead.
We sort the reason sets Ra according to their size |Ra | and traverse them from smaller
to larger ones. The idea is to deal with the more constrained cases first before moving on
to those where we have more freedom. During the traversal, we maintain an unfinished
union r0 which is initialized as the empty set . Then each reason set Ra is considered in
turn and we pick from each a reason ra  Ra that minimizes the size of r0  ra and update
8. This is sufficient, because PDR keeps layers L subsumption reduced and so the reasons for false clauses
are subsumption reduced for free.
9. PDR only calls extend(s, L) when s 6|= L, so there is always at least one such clause.

284

fiProperty Directed Reachability for Automated Planning

the set r0 accordingly to describe the union of those reasons selected so far. Although this
greedy pass through the action sets does not guarantee that the final value of r0 is minimal,
it already gives satisfactory results.
To improve the quality of the reason set even further, we then minimize r0 with respect
to the subset relation by explicitly trying to remove individual literals and checking whether
the result is still a valid overall reason. This is a direct adaptation of the explicit reason
minimization procedure employed in the original PDR (recall Section 3.1.2). In detail, we
iteratively pick a literal l  r0 and check for every action a whether there is a reason ra  Ra
such that ra  (r0 \ {l}). If this is indeed the case, r0 can be shrunk to (r0 \ {l}), otherwise
we continue with the old r0 and try another literal instead. When all the literals have been
tried out, we obtain the final result r.
4.1.4 Pseudocode and Correctness
The code of the procedure extend(s, L) is detailed in Pseudocode 3. The corresponding
reason construction proceeds in three stages. In the first stage we collect reasons from the
individual actions, constructing the sets Ra . This is performed during the same iteration
through the action set which establishes whether a successor state t satisfying L exists. It
either terminates by discovering such a t or computes a non-empty set Ra for every action
a. The first stage also includes the subsumption-based filtering of reasons, both within a
particular actions reason set and between the reason sets of the noop action and one other
action. In the second stage, the above described simple greedy pass through the sets Ra
computes an initial overall reason, which is then explicitly minimized in stage three.
Correctness of the extend procedure in the positive case as well as the fact that for any
returned reason r we have r  Lits(s) are easy to establish. The remaining argument is
captured by the following lemma.
Lemma 3. Let r be a cube returned by the procedure extend(s, L). Then the formula
r  T seq  (L)0

(5)

is unsatisfiable, where T seq is the transition formula of the sequential encoding SPseq .
Proof. WeSfirst observe that for every action a  A  {noop} there is a reason ra such
that r = aA{noop} ra . For those actions a for which Ra  R this reason is initially
picked during stage two (line 22) and possibly later changed to the reason ra for which
ra  (r \ {l}) during stage three (line 26). For those actions whose reason set is subsumed
by Rnoop (line 14) we can formally pick the same reason as for noop.
Since ra  r for every action a  A  {noop}, we can use Lemma 2 to infer that formula
(5) entails the unit clause pa for every a WA  {noop}. But because formula (5) also
trivially entails the at-least-one axiom alo = aA{noop} pa , it must be unsatisfiable.
It is easy to see that the procedure extend(s, L) runs in time polynomial in |X|, the
number of state variables, |A|, the number of actions of the planning problem, and |L|,
the size of the given clause set. This is mainly enabled by the fact that extend emulates
285

fiSuda

Pseudocode 3 Procedure extend(s, L):
Input:
State s; a set of clauses L such that s 6|= L
Output:
Either state t, a successor of s such that t |= L
or a reason r  Lits(s) such that no state satisfying r has a successor satisfying L
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

/* Stage one: look for the successor state and prepare the reason sets */
Ls  {c  L | s 6|= c} /* Clauses false in s */
Rnoop  {c | c  Ls } /* The reasons for the noop action */
assert Rnoop 6=  /* Follows from the contract with the caller */
R  {Rnoop } /* The set of reason sets collected so far */
foreach a  A do
pre sa  {l  pre a | s 6|= l} /* Preconditions false in s */
t  apply(s, (, eff a )) /* Ignore the preconditions and apply the effects of a */
Lt  {c  L | t 6|= c} /* Clauses false in t */
if pre sa =  and Lt =  then
return t /* The positive part: returning a successor */
else if Ls  Lt then
pass /* Do nothing: the reason set would be subsumed by Rnoop */
else
Lt0  {c  Lt | c  pre sa = } /* False clauses with a non-subsumed reason */
Ra  {{l} | l  pre sa }  {{l | l  c and l 6 eff a } | c  Lt0 }
R  R  {Ra } /* Record the reason set */

/* Stage two: compute an overall reason */
r
21: foreach Ra  R ordered by |Ra | from small to large do
22:
pick ra  Ra such that |r  ra | is minimal
23:
r  r  ra
19:
20:

/* Stage three (optional): minimize the reason */
foreach l  r do
26:
if for every Ra  R there is ra  Ra such that ra  (r \ {l}) then
27:
r  (r \ {l})
24:
25:

28:
29:

return r /* The negative part: returning a (subset minimal) reason cube */

286

fiProperty Directed Reachability for Automated Planning

the sequential encoding SPseq and the individual actions can be in the first stage considered
independently.10
A similar complexity guarantee seems to be achievable within a general-purpose SATsolver when supplied with the same encoding and configured to prefer branching on the
action variables A and setting them first to true. However, the inherent overhead connected
with explicitly generating all the corresponding axioms and storing them in memory will be
probably noticeable in practice. Moreover, the reason set subsumption optimization does
not have a counterpart in a general-purpose solver.
4.2 Inductive Reason Minimization in Procedure extend
Inductive minimization is based on the idea that when checking whether a particular literal l
can be removed from the final reason r we can assume that the clause c = r0 corresponding
to the reduced reason r0 = (r\{l}) is already present in the set of clauses L (see Section 3.5).
We can perform inductive minimization within the extend procedure by speculating for each
action a whether we would be able to satisfy the additional clause c by applying a. Only if
the answer is positive do we need to look for a proper reason ra  Ra .
Pseudocode 4 Stage three of extend(s, L); inductive version:
1: repeat
2:
r0  r
3:
foreach l  r do /* Check each literal of r0 once */
4:
if there is l0  (r0 \ {l}) such that l0  g then /* Can attempt to remove l */
5:
r0  (r0 \ {l})
6:
foreach a  A do
7:
if for every l0  eff a : l0 6 r0 then
8:
continue /* a passed by the inductive argument */
9:
if Ra  R and there is ra  R such that ra  r0 then
10:
continue /* a passed; it has its own small reason */
11:
if Ra 6 R and there is ra  Rnoop such that ra  r0 then
12:
continue /* Ra was subsumed by Rnoop which contains a small reason */
13:

/* Action a says: Literal l cannot be removed */
15:
r0  (r0  {l}) /* Put the literal back */
16:
break
17: until r = r0 /* No removal in the last iteration */
14:

18:
19:

return r

The idea is demonstrated in Pseudocode 4, which should be regarded as a replacement
for stage three of the original extend procedure. Notice that we no longer consider the
noop action to be part of the action set11 and thus we need to explicitly check that there
10. When devising an analogous procedure for a parallel plan semantics, one would in general need to
consider every subset of actions that can be applied together. This seems to make a polynomial time
solution much more difficult, if not hopeless. However, see Section 6 for an interesting connection.
11. The noop action trivially passes the inductiveness check, because it can never make any clause true.

287

fiSuda

remains at least one literal incompatible with the goal condition g (line 4). There can still
be actions, however, whose reason set has been subsumed by Rnoop and for these we look
for a reason in Rnoop (line 11) whenever they fail to pass the inductiveness check (line 7).
To avoid confusion we remark that the continue and break commands refer to the innermost cycle, which iterates over actions (line 6). Finally, we note that the presence of a small
reason in Rnoop depends only on the current value of r0 and so the corresponding check
could be precomputed outside the inner cycle.
Example 3. Recall Example 2, in which the action a = ({p, q}, {o, r}) failed in the state
s = {o 7 0, p 7 0, q 7 0, r 7 0} to provide a successor state that would satisfy the clauses
from L = {o  q, p  r} and so we computed a reason set Ra = {{p}, {q}}. Assume that
apart from a the action set A contains just one other action, namely b = ({r}, {p}),
for which we obtain a reason set Rb = {{o, q}}. The overall reason after stage two is
thus necessarily r = {o, q}. Assuming that the goal condition of the given problem is
g = {o, p, q, r}, inductive minimization of the reason r could proceed as follows.
First we try the reason r0 = {o}. Since o  eff a we cannot use the inductive argument
for the action a and also no proper reason ra  Ra has the property that ra  r0 . Thus
the literal q cannot be removed from r. Next we try the reason r0 = {q}. Since neither
the action a nor b contain the literal q in their effect lists, the smaller reason is justified
inductively for both actions and the overall reason r is reduced to {q}. We cannot minimize
r further, because there has to remain at least one literal l0  r such that l  g.
Looking from the perspective of the final learned clause c = r we observe that inductive
minimization allows us (as in the example above) to remove from c every literal that cannot
be made true by any action of the action set A. Although this may seem like a powerful
(global) criterion, it is effectively made redundant in practice by the so called relaxed reachability analysis (see Hoffmann & Nebel, 2001, Section 4.3), a standard preprocessing step
which, before the actual search is started, removes from the problem all such unattainable
variables as well as all actions that mention them in their precondition lists. Non-trivial
invocations of inductive minimization were actually quite rare in our experiments.
4.3 Replacing the Remaining SAT-solver Calls
Beside the query for extending states, there are two other points in the formulation of PDR
(recall Pseudocode 1 on page 275) where a SAT-solver call is employed. It is used to pick
initial states at the beginning of the path construction phase (line 4) and is also central to
verifying the condition for pushing clauses during the clause propagation phase (line 26). In
planning, we can easily do without a SAT-solver in the first case, because there is only one
initial state to be picked, namely the state sI , and we just need to verify that sI satisfies
the clauses of Lk before the path construction phase of iteration k can be started.
We have basically two options how to deal with the second case. Since clause pushing
is not need for ensuring correctness of PDR, we can simply leave the operation out. As we
later show in our experiments, this does not significantly affect the performance on planning
benchmarks, which are typically satisfiable. As a second option, we propose to restrict the
planning formalism such that the query corresponding to a push check of a clause c, i.e.,
SAT ?[ c  T  (L)0 ],
288

(6)

fiProperty Directed Reachability for Automated Planning

can be decided in polynomial time.12 We say that a STRIPS planning problem is positive
if the precondition list of every action and the goal condition of the problem consist of
positive literals only.13 It is easy to see that when running on a positive STRIPS problem,
PDR only deals with positive clauses. The unit clauses of layer L0 , which describe the goal,
are positive by assumption and all the learned clauses are transitively built only from the
goal literals and from the action precondition literals. This observation allows us to reduce
query (6) to the evaluation of the positive part of the interface for path extensions.
Lemma 4. Let P = (X, sI , g, A) be a positive STRIPS planning problem and T seq the
transition formula of the sequential encoding SPseq . Further, let L be a set of positive clauses
over X, c a positive clause over X, and sc : X  {0, 1} a state defined for every p  X by
(
0 if p  c,
sc (p) =
1 otherwise.
Then the following formula
Fc = c  T seq  (L)0
is satisfiable if and only if there is an action a  A such that sc |= pre a and apply(sc , a) |= L.
Proof. Let us first assume that there is an action a  A applicable in sc such that the
successor state t = apply(sc , a) satisfies the clauses from L. Notice that Vars(Fc ) = X 
A  X 0 , where A = {pa | a  A} is the set of variables used for encoding applied actions.
We define the following assignment a : A  {0, 1}:
a = {pa 7 1}  {pb 7 0 | b  A, b 6= a}.
It is easy to verify that the joint assignment (sc  a  t0 ) satisfies Fc .
For the opposite direction, let us assume that an assignment V : X  A  X 0  {0, 1}
satisfies the formula Fc . We fix an action a  A such that
must
W V (pa ) = 1. Such an action
seq
exist, because V satisfies the at-least-one axiom alo = aA pa , which is part of T . By
restricting V , first, to the state variables X, and, second, to the primed variables X 0 , we
extract, respectively, a state s = V  X and a state t such that t0 = V  X 0 . The axioms of
T seq ensure that the action a is applicable in s and that t = apply(s, a).
We now notice that s |= c, which means that s(p) = 0 for every p  c. Thus if there is a
difference between the states s and sc it is only because of variables p 6 c for which s(p) = 0
and sc (p) = 1. But this means, for one thing, that since the action a is applicable in s, it
must also be applicable in sc (preconditions are positive) and, for the other, since t |= L,
the successor state tc = apply(sc , a) corresponding to sc must also satisfy the clauses from L
(the implication p  X : s(p) = 1  sc (p) = 1 is preserved by the transition and becomes
p  X : t(p) = 1  tc (p) = 1 and the clauses from L are positive by assumption).
12. In our current setting, there does not seem to be a general polynomial solution. In fact, even in the
degenerate case of T encoding a transition by the single noop action and c being the empty clause, the
query (6) boils down to satisfiability of L and its evaluation is thus an NP-complete problem.
13. Most of the standard planning benchmarks are positive STRIPS. Moreover, there is a well-known reduction (Gazen & Knoblock, 1997) that turns a general STRIPS problem into a positive one. The reduction
introduces a new variable p for every variable p that occurs negatively in a precondition or in the goal
and updates the actions to always force p to have the opposite value to that of p.

289

fiSuda

Pseudocode 5 Algorithm PDRplan(X, sI , g, A):
Input:
A positive STRIPS planning problem P = (X, sI , g, A)
Output:
A plan for P or a guarantee that no plan exists
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:

L0  {{p} | p  g} /* The goal cube treated as a set of unit clauses */
foreach j > 0 : Lj  
for k = 0, 1, . . . do
/* Path construction: */
if sI |= Lk then
Q  {(sI , k)}
while Q not empty do
pop some (s, i) from Q with minimal i
if i = 0 then
return PLAN FOUND
if extend(s, Li1 ) returns a successor state t then
Q  Q  {(t, i  1), (s, i)}
else
extend returned a reason r  Lits(s)
foreach 0  j  i : Lj  Lj  {r}
/* Obligation rescheduling: */
if i < k then
Q  Q  {(s, i + 1)}
/* Clause propagation: */
for i = 1, . . . , k + 1 do
foreach c  Li1 \ Li do
/* Clause push check */
sc  {p 7 0 | p  c}  {p 7 1 | p  (X \ c)}
if for every a  A : sc 6|= pre a or apply(sc , a) 6|= Li1 then
Li  Li  {c}
/* Convergence check */
if Li1 = Li then
return NO PLAN POSSIBLE

290

fiProperty Directed Reachability for Automated Planning

A version of PDR specialized to positive STRIPS planning is shown in Pseudocode 5.
The calls to a SAT-solver of the original formulation were replaced, respectively, by a simple
entailment check (line 5), a call to the extend procedure (line 11), and by an enumeration
of the successor states of the state sc as defined in Lemma 4 (line 26).
4.4 Reversing the Search Direction
It has been mentioned that the original formulation of PDR is based on the opposite search
direction than the one described in this paper and extends the paths from a goal state
backwards towards the initial state. We would like to test the algorithm with both directions
to see which one is more favorable in practice.
One possibility to achieve this is to provide PDR with an inverted version of the input,
where the initial and goal states have been swapped and the transition relation turned
around. This is straightforward to do when the input is an STS (recall Section 2.3), as is
the case with the general version of PDR. The situation is more complicated with the SATsolver free version, which directly takes a STRIPS planning problem as an input. Indeed,
it seems the extend procedure substantially relies on the forward direction.
Interestingly, there exists a transformation for inverting STRIPS planning problems. It
was first described by Massey (1999) in his dissertation. We present here a more streamlined version due to Pettersson (2005) which relies on the problem being positive. Let us
start by introducing an alternative representation of positive STRIPS planning problems,
which makes the description of the transformation particularly straightforward. A positive
STRIPS planning problem in the subset representation is given by a tuple P = (X, i, g, A),
where i, g  X are the initial and goal conditions, respectively, and every action a  A is
encoded by a triple a = (pre a , add a , del a ), consisting of a precondition list, an add list and
a delete list, which are subsets of X such that pre a  add a =  and add a  del a = . The
subset representation differs from the one presented in Section 2 by encoding the initial
state by the set of those variables that are true in it:
sI (p) = 1 if and only if p  i,
and by splitting actions effects into positive and negative ones:
eff a = add a  {p | p  del a }.
The goal condition g and precondition lists pre a remain intact, but now may be understood as subsets of X since the problem is positive. It should be clear that the subset
representation and the one of Section 2 are equivalent.
Now, for an action a = (pre a , add a , del a ) an inverted action a1 is formed by exchanging
the precondition and delete list: a1 = (del a , add a , pre a ). For a set of actions A the set of
inverted actions is A1 = {a1 | a  A}. Given a planning problem P = (X, i, g, A) in the
subset representation, the inverted problem P 1 is obtained by exchanging the initial and
goal conditions while taking their complements with respect to X and using the inverted
action set:
P 1 = (X, (X \ g), (X \ i), A1 ).
The original problem and its inverted version are related in the following sense:
291

fiSuda

Theorem 2. The sequence of actions a0 , a1 , . . . , ak is a plan for the planning problem P if
1
1
1 .
and only if the sequence a1
k , ak1 , . . . , a0 is a plan for P
This means that performing forward search (also called progression) in P is equivalent to
performing backward search (regression) in P 1 and vice versa. Notice that, a priori, there
is no computational overhead incurred by the transformation: the inverted problem has the
same number of actions as well as the same set of state variables X and so the representation
of states is of the same size. A proof of Theorem 2 along with further intuition behind the
transformation and its theoretical and practical implications are described by Suda (2013b).
4.5 Further Improvements
We describe three additional modifications of PDR that aim to make the algorithm more efficient in practice. While the first is a planning-specific improvement of the extend procedure,
the other two focus on how obligations are handled by the overall algorithm. In Section 5
we experimentally evaluate the effect of these modifications on solving planning problems.
The interested reader can find the pseudocode of the modifications in Appendix A.
4.5.1 Lazy False Clause Computation
One way to speed up the extend procedure in practice is a technique we named lazy false
clause computation. It is based on the following two observations:
 Ls , the set of clauses false in the state s, is typically only a small subset of L, the set
of all the clauses the successor state should satisfy,
 only a small fraction of the available actions makes any of the clauses of Ls true in
their respective successor.
The idea is to avoid the relatively expensive computation of the set of clauses false in
the successor t, i.e. the computation of the set Lt on line 10 (Pseudocode 3), and instead
first only look at the truth value of the clauses from Ls . (Notice that Ls is precomputed
before we start iterating over the actions.) Only if we find an action a such that all its
preconditions are satisfied in s and it makes all the clauses from Ls true in the respective
successor t, we classify the action as promising and go back to computing the full Lt . Thus,
with non-promising actions we save computational time. We may pay for it a little on the
side of the quality of the reason set, because for them we only use Ls,t = {c  Ls | t 6|= c}
instead of the full Lt for computing the reasons. On the other hand, with promising actions
a complete test is necessary to distinguish a true successor t satisfying all of L from an
action that repairs everything which was false in s, but breaks something else instead.
4.5.2 Sidestepping
Sidestepping is a technique we propose to make PDR more active in early exploration of
promising paths. It partially circumvents the limitation stemming from the fact the extend
procedure emulates the sequential encoding SPseq .
Imagine we want to extend an obligation (s, i), i.e. to find a successor of s that would
satisfy Li1 , and there are two clauses c1 , c2  Li1 false in s. Let us think of the two clauses
292

fiProperty Directed Reachability for Automated Planning

as of two independent subgoals to be achieved. There are two actions a1 and a2 applicable
in s. Action a1 makes c1 true in the successor state and a2 makes c2 true, but no action can
make both the clauses true in one step. This means the extension cannot be successful and
PDR will learn a new clause c = c1  c2 (or a superset thereof). The clause c expresses the
fact that in order to reach a state satisfying Li1 in one step, at least one of the two clauses
c1 , c2 must be satisfied beforehand. This could be an important ingredient to showing that
no path of length k can reach the goal, helping the algorithm eventually advance to the next
iteration. However, because in planning we are usually more interested in actually finding
plans than showing their non-existence, deriving the clause c could represent unnecessary
extra work. The idea behind sidestepping is to make the extend procedure succeed more
often, even if that does not mean directly advancing into the next layer. In our example,
we return the successor state t = apply(s, a1 ) with an additional flag informing the caller
that the new obligation should have index i and not the usual i  1. We are effectively
sidestepping from (s, i) to (t, i). In the next round the obligation (t, i) will be picked and
successfully (provided the actions a1 and a2 do not interfere) extended into (u, i  1) via
the action a2 . This way we end up with a state satisfying Li1 almost as if we executed the
two actions a1 and a2 in parallel.
Let us now present the sidestepping technique in more detail. In order for an action a
and its respective successor ta = apply(s, a) to qualify for a sidestep during extension of an
obligation (s, i) the following conditions must to be met:
1) a is applicable in s,
2) ta improves over s with respect to the set of satisfied Li1 clauses:
a
Lti1
= {c  Li1 | ta 6|= c}  Lsi1 = {c  Li1 | s 6|= c},

3) ta satisfies all the Li clauses.
Notice that we require the improvement to be strict (condition 2). This ensures that
sidestepping does not compromise termination. We also make sure that the new state
stays within Li (condition 3)  an improvement in one respect should not be payed for by
an overall deterioration.
If there is no action that qualifies for a sidestep, we compute and return a reason set
a
as usual. Otherwise, we choose among them an action a for which the size of Lti1
is the
ta
smallest. The case when |Li1 | = 0 corresponds to a regular successful extension and a new
a
obligation (ta , i  1) will be stored in the set Q. If |Lti1
| > 0, we store (ta , i) instead, which
means that we perform a sidestep.
After sidestepping both the old obligation (s, i) and the new (ta , i) occupy the same
index in Q. It is important that we prioritize the latter over the former for picking (e.g.,
even if we otherwise want to use queue tie-breaking strategy; see Section 3.6.4), by which
we prevent the algorithm from sidestepping in the same way more than once.14
Notice that sidestepping is an extension of PDR that relies on a modification of the
planning-specific extend procedure. As such it does not have an immediate counterpart in
the original algorithm, where path extensions are delegated to a SAT-solver.
14. By the time (s, i) is reconsidered we must have had an unsuccessful extension of (ta , i), which means Li
got in the meantime strengthened and ta no longer satisfies it.

293

fiSuda

4.5.3 Keeping Obligations between Iterations
Let us return to obligation rescheduling (lines 18 and 19 of Pseudocode 5) to discuss one
additional aspect of this feature. Notice that we reschedule an obligation (s, i) only if i < k
so that the new obligation (s, i + 1) is never positioned further from the goal than k steps
during iteration k. Obligations of the form (s, k) are simply forgotten which ensures that
the path construction phase eventually terminates. A viable alternative to this strategy is to
reschedule these obligations on the queue Q with index k + 1, but set them into a dormant
state and return to them only during the next iteration. This can be understood as
effectively enlarging the set of initial states for the next iteration so that it includes all the
states reached so far.
Although this modification is quite simple to implement and seems to go well with the
spirit of obligation rescheduling itself, it has not been publicly described yet. A potential
disadvantage could be the increased memory consumption, since all the states ever encountered during the run must be stored by the algorithm. The utility of this modification may,
therefore, depend on the application domain.

5. Experiments
In this section we report on a series of experiments aimed to establish the practical relevance
of PDR for automated planning. We first compare the standard version of the algorithm
combined with encodings to the SAT-solver free variant of PDR proposed in this paper.
The latter is implemented in our new planner PDRplan. Next, we measure the influence of
the several design choices mentioned in Section 3 as well as of the various improvements
proposed in Section 4.5 on the performance of PDRplan. The most successful configuration
of PDRplan is then compared to other planners, including state of the art representatives of
the heuristic search planning and planning as satisfiability paradigms. Finally, we also assess
PDRplan from the perspective of plan quality, finding optimal length plans and detecting
unsatisfiable problems.
5.1 The Setup
We performed the experiments on machines with 3.16 GHz Intel Xeon CPU, 16 GB RAM,
running Debian 6.0. Although multiple cores are available on each machine, all the planners
used only one core and we made sure that there was no other busy process running concurrently that would compete with a planner for memory, etc. The main measured resource
was computation time. We used a time limit of 180 seconds per problem instance for most
of the runs, but increased it to 1800 seconds for the main comparison.
To increase the level of confidence towards the correctness of our implementation all
the generated plans were subsequently checked by the latest version of plan validator VAL
(Howey, Long, & Fox, 2004). No discrepancies were found during the experiments reported
in this paper.
We tested the planners on the STRIPS15 benchmarks of the International Planning
Competition (IPC, 2014). So far, there were altogether seven repetitions of the competition
happening biennially from 1998 to 2008 and once in 2011. Each time the planners competed
15. The richer ADL formalism is currently not supported by PDRplan.

294

fiProperty Directed Reachability for Automated Planning

over several benchmark domains of various planning scenarios. We used all the available
STRIPS domains except the following:
 1998-MOVIE, where it turned out to be technically difficult to validate the plans.16
Note that the domain is, in fact, trivial to solve.
 2000-SCHEDULE, which is originally an ADL domain. The competition archive contains also a STRIPS version, but accompanied by a note saying that this version later
proved to be problematic and was dropped from the competition.
 2002-ROVERS, the problems of which are included in the set 2006-ROVERS.
 2002-SATELLITE and 2011-TIDYBOT, which make use of actions with negative preconditions, a feature not supported by our parser.
Altogether, we collected 1561 problems in 49 domains (see Table 3 on page 304 for a detailed
list). The 2008 and 2011 competition benchmarks specify action costs. We modified the
respective files to remove this feature, which is not supported by PDRplan.
We implemented PDR with the extend procedure as described in Section 4 in the
PDRplan system. The code of PDRplan (approximately 2K lines of C++) is built on top of
a PDDL parser and a grounder adopted from SatPlan 2006 (Kautz, Selman, & Hoffmann,
2006). We modified the parser to successfully process the large problems of the more recent
IPC domains. The source code of PDRplan is publicly available on our web page (Suda,
2014), which also contains all the other material relevant for reproducing the experiments.
5.2 PDRplan v.s. Standard PDR plus Encodings
The main purpose of the first experiment was to compare PDRplan and its planning-specific
implementation of the extend procedure to a composition of the general PDR, which uses a
SAT-solver to answer the one-step reachability queries, with various encodings of planning
into an STS. We also wanted to establish which of the two possible search directions in
PDR is more favorable for discovering plans.
We took our implementation of PDR called minireachIC3, originally developed as a
model checking tool for hardware circuits. It internally relies on the SAT-solver Minisat
(Een & Sorensson, 2003) version 2.2. We extended minireachIC3 to be able to read a
description of an STS. We designed a new input format for that purpose, which we call
DIMSPEC (Suda, 2013a). It is a simple modification of the well-known DIMACS CNF
format used by most SAT-solvers extended to define the individual clause sets of an STS.
We coupled minireachIC3 with four encoders of planning into an STS. The first two
encoders, seq and par, are our implementations of the two simple encoding SPseq and SPpar ,
respectively (recall Section 2.6). The third encoder is a version of the planner Mp (Rintanen,
2012) modified to output the encoded instance in the form of an STS and quit before starting
the actual solving process. Mp uses the -step parallel encoding scheme of Rintanen et al.
(2006). Finally, the fourth encoder implements the SASE encoding scheme introduced by
16. The parser we adopted for PDRplan removes vacuous arguments of operators from the resulting actions
names. The validator VAL then complains about the resulting plans.

295

fiSuda

Huang et al. (2012). The particular implementation we used derives from the FreeLunch
planning library (Balyo et al., 2012).
In order to obtain a fair comparison we used a basic version of PDRplan configured in
a way that most resembles the workings of minireachIC3. The configuration follows the
planning-specific version of the overall algorithm (Pseudocode 5) and relies on the extend
procedure (Pseudocode 3) with the minimization phase of the reason computation (stage
three) enhanced by induction (Pseudocode 4). The additional improvements of Section 4.5
were disabled for this experiment.
We compared the systems in both search directions. By the forward direction, we mean
the one preferred in this paper, where PDR constructs the path from the initial state towards
the goal. The opposite, backward direction is preferred by the original exposition of PDR
used in model-checking. To start minireachIC3 in the backward direction we inverted the
encoded STS, to reverse the search direction of PDRplan we inverted the planning problem
(as explained in Section 4.4).17
5.2.1 Adding Invariants
An invariant of a transition system is a property of the initial state preserved by all transitions. In planning, one typically considers invariants in the form of binary clauses (Rintanen,
1998), which can be computed by a simple fixpoint algorithm (Rintanen, 2008b). Adding
the invariant clauses into an encoding is known to speed up plan search in the planning as
satisfiability paradigm.
We noticed that the performance of PDRplan in the backward direction can also be
enhanced with the help of invariants. When PDR is run in the backward direction, it is
sound to strengthen every layer by the binary clauses of a precomputed invariant. These
clauses then help to guide the path construction towards the initial state. Adding invariants
in the forward direction does not make sense for PDRplan, because all the generated states
are reachable from the initial state and, therefore, satisfy the invariant automatically.18
We used the same invariant generation algorithm as in PDRplan to also enhance the encodings seq and par for the run of minireachIC3. It turned out that in case minireachIC3
invariants slightly help even in the forward direction.19 We note that binary clause invariants are also explicitly included in the Mp encoding and implicitly present in the SASE
encoding, which relies on the SAS+ planning formalism (Backstrom & Nebel, 1995) to
which a STRIPS problem is converted with the help of invariants (Helmert, 2009).
5.2.2 Detecting Auxiliary Transition Variables
It is essential for a good performance of minireachIC3 combined with encodings that the
algorithm does not make decisions prematurely.
17. One could also experiment with encodings of the inverted problems. We leave this for future work.
18. In theory, there is a corresponding notion of a backward invariant, a property of the goal states preserved
by traversing the transitions backwards. Symmetrically, backward invariants could be used to enhance
the performance of forward PDR. In practice, however, while standard invariants are typically very
useful, there is rarely a non-trivial binary clause backward invariant in the planning benchmarks.
19. This can be explained by observing that the SAT-solver does not necessarily construct the successor state
by first choosing an action (or a set of actions, in the case of par), which would then fully determine the
successor. When it starts by deciding on the state variables of the successor, invariants become useful.

296

fiProperty Directed Reachability for Automated Planning

Example 4. Consider a run of the algorithm in the forward direction with the encoding SPseq .
Because in this encoding the action variables A occur in the unprimed part of the transition
clauses T seq , any given state s, being an assignment over  = X  A, already stores the
information about the action that will be applied next and, therefore, fully determines the
value of the state variables X 0 of its successor. As a result, contrary to the intuition, the
evaluation of the extension query
SAT ?[ Lits(s)  T seq  (L)0 ]
does not boil down to choosing an action applicable in the state (s  X) of the original
planning task, such that the successor state would satisfy the clauses of L, but instead
involves choosing an action to be applied in the already determined successor such that the
successor (as a valuation over X 0 ) and the chosen action (as a valuation over A0 ) together
satisfy the clauses from (L)0 , which, in general, span the whole signature 0 . We can see
that, in some sense, all the decisions are made one step too early.
We observed a marked improvement in the performance of minireachIC3 combined
with encodings when we extended the tool with a preprocessing step that detects auxiliary
transition variables in the unprimed part of the transition clauses and re-encodes them into
the primed part in order to avoid committing to decisions prematurely as demonstrated in
the example above. Formally, given an STS S = (, I, G, T ), auxiliary transition variables
Aux are those variables of  that do not appear in I or G and, when primed, are not shared
by T and T 0 . This means that
Aux 0 = 0 \ (Vars(I 0 )  Vars(G0 )  (Vars(T )  Vars(T 0 ))).
The action variables A of the SPseq encoding are an example of auxiliary transition variables.
For every transition clause c  T , the preprocessing step identifies literals l  c such that
Vars(l)  Aux and turns each such l into l0 . The soundness of the transformation is easy
to establish.
5.2.3 Results of the Experiment
The results of the first experiment can be found in Figure 3. There are several observations to be made. As foretold, the forward direction is generally more successful than the
backward. Within the time limit of 180 seconds, each of the five systems solves more problems in the forward direction than in the backward direction. We see that in the backward
direction, invariants help to improve the performance of PDRplan. Nevertheless, within
that direction minireachIC3 combined with the Mp encoding is more successful. The most
successful system is PDRplan in the forward direction. It solves 8.6 percent more problems
than the second best system, minireachIC3 combined with the Mp encoding in the forward
direction. Although we do not consider these results as a definitive answer to the PDRplan
vs. encodings question,20 we decided to only focus on PDRplan in the forward direction for
(most of) the subsequent experiments.
The overall trends captured by Figure 3 are most of the time respected when the comparison is performed on level of individual problem domains (comparing the number of
20. For instance, replacing Minisat in minireachIC3 by a more recent and more efficient SAT-solver could
change the picture to a certain degree.

297

fiSuda

1400

1000

minireachIC3(seq)
minireachIC3(par)
minireachIC3(SASE)
minireachIC3(Mp)
PDRplan

1200

problems solved

1200

problems solved

1400

minireachIC3(seq)
minireachIC3(par)
minireachIC3(Mp)
minireachIC3(SASE)
PDRplan+i
PDRplan

800
600

1000
800
600

400

400

200

200

0

0
1

10
time (seconds)

100

1

10
time (seconds)

100

Figure 3: Comparing PDRplan to minireachIC3 combined with encodings. Number of
problems solved within the given time limit is shown, separately for the backward
direction (left) and the forward direction (right).

problems solved in 180 seconds), nevertheless there are some notable exceptions worth
mentioning.
 On the LOGISTICS domain PDRplan behaves better in the backward direction and
without invariants. The best system on this domain, however, is minireachIC3 with
Mp encoding in the forward direction.
 The relatively difficult 2011-BARMAN domain is almost fully solved (19 out of 20
problems) by PDRplan in the backward direction with invariants. The second best
system on this domain is minireachIC3 with par encoding in the backward direction
with only 7 problems solved.
 The following are among the domains where PDRplan is not the best system: 1998MYSTERY (minireachIC3 with SASE and Mp encodings in the forward direction
both solve 5 problems more), 2004-PHILOSOPHERS (18 more problems solved by
minireachIC3 both with par and Mp encodings in the forward direction), and 2011VISITALL (minireachIC3 with Mp encoding solves 4 more problems).
 There are several domains where minireachIC3 with Mp encoding is better in the
backward direction than in the forward direction. The difference is most pronounced
on 2006-OPENSTACKS, and 2011-FLOORTILE.
The observation of the last point is in accord with how the Mp encoding is used with
the Mp planner itself. What Rintanen (2012) describes is effectively a depth-first backward
chaining planning algorithm inside the SAT-solving framework. This can be seen to be very
298

fi1200

1200

1100

1100
problems solved

problems solved

Property Directed Reachability for Automated Planning

1000
900
800
ind_off
min_off
default

700
600
1

10
time (seconds)

1000
900
800
700
cp_off
queue
default

600
500
100

1

10
time (seconds)

100

Figure 4: Tuning PDRplan. The effect of explicit (inductive) reason minimization (left),
and clause propagation and the queue tie-breaking strategy (right).

close to backward PDR when coupled with the same encoding. We hypothesize that the
suitability of the Mp encoding for the backward direction of search emerges also with PDR.
5.3 Tuning PDRplan
In the second experiment (see Figure 4) we focused on several features of the standard
PDR and tried to established their importance for solving planning problems. We used
PDRplan in the forward direction and 180 seconds time limit. We measured the effect of
each feature separately with the reference configuration denoted as default. This is the same
configuration as the one used in the previous experiment.
5.3.1 Explicit (Inductive) Reason Minimization
By explicit minimization we mean the optional stage three of the reason computation in
the extend procedure, which can be enhanced by induction as described in Section 4.2. In
Figure 4 (left) we compare the performance of the default configuration, which relies on the
inductive version of reason minimization (Pseudocode 4), to configuration ind off, which
does not use induction and implements minimization as described in Pseudocode 3, and to
configuration min off, which skips the optional stage three altogether.
We can see that while the positive effect of explicit minimization is slight but consistent
along the time axis, induction only starts to pay off on the global scale when the time limit
exceeds 100 seconds. At the 180 seconds mark the ind off configuration solved 1.0 percent
fewer and the min off configuration 2.4 percent fewer problems than default.
Per domain view reveals that induction is especially important for the success on 2000BLOCKS, 2004-PHILOSOPHERS, and 2008-CYBER-SECURITY. On domains such as
2002-ZENOTRAVEL or 2008-TRANSPORT it is better to turn minimization off completely,
and there are also domains, such as 1998-MYSTERY or 2006-TRUCKS, where it pays off to
minimize, but not inductively. In the last two categories, however, the difference is never by
299

fiSuda

more than a problem or two per domain and thus it could be potentially equalized within
a higher time limit.
Interestingly, out of the total of 1561 problems, the execution of default and ind off
diverged only on 145 problems.21 This means that on most of the problems induction does
not help to minimize reasons beyond what can be achieved with non-inductive minimization.
To give another statistics, we note that over the whole problem set during a call to the
extend procedure inductive minimization removes 1.49 literals and computes a reason with
50.60 literals on average. The non-inductive minimization in min off removes 1.44 literals
and generates a reason with 51.22 literals on average.
5.3.2 Clause Propagation
In Figure 4 (right) we can compare the default configuration to a configuration in which
clause propagation has been turned off (cp off). We see that clause propagation slows PDRplan slightly down without any clear benefit before the 180 seconds mark. Although a later
independent experiment with a 1800 second time limit showed that clause propagation can
be useful on planning problems, it is questionable whether the effect justifies the relatively
high effort connected with implementing the technique.22
A closer look reveals that only on 28 percent of the tested problems a clause was successfully pushed forward during the 180 seconds bounded runs. This may seem to be in
contrast with the experience from hardware model checking where clause propagation plays
a key role. Its main effect there, however, lies in speeding up convergence of PDR on the
unsatisfiable problems. Since more than 99 percent of our planning benchmarks are satisfiable, this role of clause propagation cannot be demonstrated. In fact, we also independently
confirmed in an experiment with minireachIC3 on satisfiable hardware benchmarks23 that
clause propagation is not beneficial in the forward direction.
5.3.3 Stack vs. Queue Tie-breaking
Here we evaluate the effect of the strategy for breaking ties during popping obligations
from the set Q (see Section 3.6.4). The stack strategy used in the default configuration
is compared to the curve of the queue strategy in Figure 4 (right). The queue strategy
solves about 5.9 percent fewer problems in total. However, there are 18 problems solved
by the queue strategy only (and 85 problems solved only by the stack strategy). The most
interesting observations on the per domain scope are probably
 59 problems (out of 60) from the 2000-BLOCKS domain solved by the stack strategy
compared to only 36 solved by the queue strategy, and
 2 problems (out of 20) from the 2011-BARMAN domain solved by the queue strategy
compared to 0 problems solved by the stack strategy.
21. By either generating a different number of obligations before a successful termination or differing in
whether they successfully terminated at all before the 180 seconds mark.
22. In the final comparison to other planners (see Section 5.5) clause propagation is responsible for 6 additional problems scored by PDRplan.
23. On benchmarks from the Hardware Model Checking Competitions 20072012 (Biere, Heljanko, Seidl, &
Wieringa, 2012) with a time limit of 100 seconds.

300

fiProperty Directed Reachability for Automated Planning

1300
1 = default
2 = 1 + lfcc
3 = 2 + side
4 = 3 + keep

1200

problems solved

1100
1000
900
800
700
600
1

10
time (seconds)

100

Figure 5: Improving PDRplan. The default configuration is progressively extended by turning on three different techniques.

Preferring to explore longer paths before short ones has the unpleasant side effect that
also the plans discovered by the stack strategy tend to be longer. Measured over the 1055
problems solved by both strategies, the plans generated by the stack strategy are on average
24 percent longer.
A more detailed discussion on the topic of plan quality is postponed till Section 5.6.
5.4 Improving PDRplan
The purpose of the third experiment was to evaluate the three improvements proposed
in Section 4.5. These were successively: 1) lazy false clause computation (lfcc), 2) the
sidestepping technique (side), and 3) keeping obligations between iterations (keep). Figure 5
displays the effect of progressively enabling the three techniques in the presented order. We
see that to varying degrees each technique represents an improvement and each successive
configuration solves more problems.
A different perspective is provided by Table 2 which also reveals how many problems
were uniquely solved by only one of the two successive configurations. It shows that none
of the improvements are unambiguous across the whole problem set and that there are
exceptions to the prevailing trends.
These can be best highlighted on the level of individual domains. For instance, the
number of solved problems drops on 2000-BLOCKS and 2008-CYBER-SECURITY with
lazy false clause computation (configuration 2), but it is improved again by the subsequently
enabled techniques. Sidestepping (configuration 3) makes the performance worse on 2002DRIVERLOG, 2004-SATELLITE, or 2008-CYBER-SECURITY. On the other hand, the
technique represents a huge improvement on the 2004-OPTICAL-TELEGRAPH domain
301

fiSuda

configuration
1 = default
2 = 1 + lfcc
3 = 2 + side
4 = 3 + keep

total
1145
1180
1195
1212

delta

35
15
17

gained

55
67
42

lost

20
52
25

Table 2: Number of problems solved within 180 seconds (total). The difference (delta) between two successive configurations decomposed into additionally solved problems
(gained) and problems only solved without the improvement (lost).

1500
1400

problems solved

1300
1200
1100
1000
900
800
700

FF
LAMA-2011
Mp
PDRplan1.1

600
500
1

10

100
time (seconds)

1000

Figure 6: Comparing the final version of PDRplan to other planners. Showing the number
of problems solved within the given time limit.

(from 2 to all 14 problems solved) and on 2004-PHILOSOPHERS (from 11 to all 29 problems
solved). Finally, keeping the obligations (configuration 4) is detrimental to the performance
on the 2011-FLOORTILE domain (the number of solved problems drops from 19 to 13),
but the technique, for example, helps to recover the 2008-CYBER-SECURITY problems
that were lost due to sidestepping.
5.5 Comparing to other Planners
We compared the improved PDRplan  the configuration 4 from the previous experiment
denoted here PDRplan1.1  to the following planners:
 The planner FF (Hoffmann & Nebel, 2001) as a baseline representative of heuristic
search (Bonet & Geffner, 2001) planners. We used version 2.3, but enhanced its input
302

fiProperty Directed Reachability for Automated Planning

module to make it cope with the large problems of the more recent IPC domains. The
default parameters for FF have been used.
 The planner Fast Downward (Helmert, 2006), the current state of the art heuristic
search planner. We used the configuration LAMA-2011 (Richter & Westphal, 2010),
the winner of the satisficing track of IPC 2011.
 The Mp planner (Rintanen, 2012), probably the current best representative of the
planning as satisfiability approach (Kautz & Selman, 1996). We used version 0.99999
with default parameters.
For this experiment the time limit was increased to 1800 seconds.
The overall performance of the planners can be seen in Figure 6. The planner FF has
a very fast startup and solves the most problems (952) within one second. However, FF is
the worst of the planners to make use of the additional time and solves the fewest problems
(1247) in total. On the opposite side stands LAMA-2011 with the slowest startup (566
problems within one second), but with the best total (1437). PDRplan1.1 and Mp are close
to each other in performance both at the beginning  PDRplan1.1 solves 741 and Mp 790
problems within one second  and at the end  in total PDRplan1.1 solves 1333 problems
gaining a slight edge over Mp with 1310 problems solved.
Table 3 shows a domain-by-domain decomposition of the results. We can see that
there are several domains where PDRplan1.1 solved the most problems of the four planners, namely the 2000-BLOCKS, 2002-FREECELL, 2004-PIPESWORLD-NOTANKAGE,
and 2006-TRUCKS domains. The domains 2004-PHILOSOPHERS, 2006-PATHWAYS, and
2006-STORAGE were completely solved by only PDRplan1.1 and Mp. On the hand, a comparatively poor performance of PDRplan1.1 can be observed on the 1998-LOGISTICS and
1998-MPRIME domains, and also on the 2011-PARKING (shared with FF) and 2008+2011SOKOBAN (shared with Mp) domains.
5.6 Plan Quality
IPC 2008 (Helmert, Do, & Refanidis, 2008) introduced a criterion for measuring planner
performance which takes into account the quality of the obtained plans. For every problem
solved, a planner aggregates a score computed as the ratio c /c, where c is the cost24 of the
returned plan and c the cost of the best known plan (either a plan computed beforehand
by the competition organizers or the best plan found by any of the participating systems).
When viewing the results of the previous experiment through the lenses of this criterion,
one discovers that PDRplan1.1 drops from the second place to the last.
We reviewed all the previously discussed features and improvements and discovered
that the configuration of PDRplan1.1 is not the best possible with respect to plan quality.
In particular, by switching to the queue tie-breaking strategy (we denote the respective
configuration PDRplan1.1+queue) the aggregated score of the planner improves. A slight
24. As mentioned before, we do not consider action costs in this paper, so, in our setting, a cost of a plan is
simply equal to its length.

303

fiSuda

1998-GRID
1998-GRIPPER
1998-LOGISTICS
1998-MPRIME
1998-MYSTERY
2000-BLOCKS
2000-ELEVATOR
2000-LOGISTICS
2000-FREECELL
2002-DEPOTS
2002-DRIVERLOG
2002-ZENOTRAVEL
2002-FREECELL
2004-AIRPORT
2004-PIPESWORLD-NOTANKAGE
2004-PIPESWORLD-TANKAGE
2004-OPTICAL-TELEGRAPH
2004-PHILOSOPHERS
2004-PSR
2004-SATELLITE
2006-OPENSTACKS
2006-PATHWAYS
2006-PIPESWORLD
2006-ROVERS
2006-STORAGE
2006-TPP
2006-TRUCKS
2008-CYBER-SECURITY
2011-BARMAN
2008+2011-ELEVATORS
2011-FLOORTILE
2011-NOMYSTERY
2008+2011-OPENSTACKS
2008+2011-PARCPRINTER
2011-PARKING
2008+2011-PEGSOL
2008+2011-SCANALYZER
2008+2011-SOKOBAN
2008+2011-TRANSPORT
2011-VISITALL
2008+2011-WOODWORKING
TOTAL

size
5
20
35
35
30
60
150
36
60
22
20
19
20
50
50
50
14
29
50
36
30
30
50
40
30
30
30
30
20
50
20
20
50
50
20
50
50
50
50
20
50
1561

PDRplan1.1
5
20
18
25
19
60
150
36
57
21
18
19
20
40
45
37
14
29
50
28
30
30
32
39
30
30
27
30
6
40
14
14
49
50
8
50
46
11
27
9
50
1333

FF
5
20
35
34
18
48
150
36
60
21
18
19
19
38
32
17
14
14
42
34
30
20
21
40
18
28
12
4
0
50
10
7
50
50
7
50
44
40
38
4
50
1247

LAMA-2011
5
20
35
35
23
55
150
36
60
22
20
19
19
33
44
42
14
13
50
36
30
29
40
40
19
30
15
30
20
50
6
10
50
50
20
50
50
48
49
20
50
1437

Mp
5
20
32
34
19
46
150
36
44
22
20
19
15
49
42
38
14
29
50
35
19
30
25
40
30
30
19
30
8
50
20
19
18
50
20
50
48
9
26
0
50
1310

Table 3: Number of problems solved within 1800 seconds, grouped by domain. We highlighted those entries where PDRplan1.1 solves the most problems or shares the
first place with one other planner. To save space the entries of IPC 2008 domains
recurring later in IPC 2011 were merged with the respective entries of IPC 2011.

304

fiProperty Directed Reachability for Automated Planning

1300
1200

aggregated score

1100
1000
900
800
700

FF
LAMA-2011
Mp
PDRplan1.1
PDRplan1.1+queue

600
500
1

10

100
time (seconds)

1000

Figure 7: Comparing the planners with respect to plan quality. Showing the score aggregated by each planner within the given time limit.

improvement can also be observed when the lazy false clause computation is turned off in
PDRplan1.1. Interestingly, doing both changes at once does not bring a combined benefit.25
Figure 7 shows the aggregated scores for the runs of the previous experiment together
with a run of PDRplan1.1+queue.26 Although PDRplan1.1+queue solves only 1263 problems
in 1800 seconds (compared to 1333 solved by PDRplan1.1), it aggregates a score of 1141.1
points while PDRplan1.1 only reaches 1041.4. This means the former configuration catches
up with Mp, which aggregates 1102.7 points.
We note that these statistics should be taken with a grain of salt, because they only
provide a plan quality view on the satisficing runs of the planners. None of the systems
was explicitly attempting to find short plans or making use of the fact that the time limit is
1800 seconds. Moreover, even in such a setting the plan quality can typically be improved
afterwards by a post-processing of the discovered plans (Balyo & Chrpa, 2014). We later
incorporated the polynomial Action Elimination algorithm (Nakhost & Muller, 2010) as a
plan post-processor into PDRplan1.1 and we were able to improve its aggregated score by
7.0 percent. A more thorough investigation of the quality of plans produced by PDRplan,
as well as by PDR in general, is left for future work.

25. It seems that the already carefully advancing PDRplan1.1+queue benefits from the speed provided
by lazy false clause computation, whereas with the stack strategy it helps to wait for the more precise
reasons (having lfcc turned off) that will not allow the planner to search too deep too often.
26. The reference values for the best known cost c were collected just from the runs in the figure.

305

fiSuda

5.7 Anytime PDR and Optimal Planning
Recall that PDR can be adjusted to perform optimal planning by turning off the obligation
rescheduling technique (and sidestepping).27 Alternatively, we can modify PDR to continue
the computation after a first plan is found, but afterwards only reschedule obligations that
can be part of an improving plan.28 This anytime version of PDR progressively reports on
better and better solutions until finally terminating with a guarantee that the last reported
plan is an optimal one. This happens when it reaches an iteration i equaling the length of
the best discovered plan.
In this experiment we focused on optimal planning with respect to the sequential plan
semantics.29 We compared the performance of the anytime version of PDRplan1.1 (counting
only solutions provably shown to be optimal) to BJOLP (Domshlak et al., 2011), an optimizing version of Fast Downward, and to an optimizing configuration of Mp.30 Ordering
the planners by the number of problems optimally solved within 1800 seconds we obtain:
1. BJOLP with 668 problems solved,
2. PDRplan1.1-anytime with 360 problems solved, and
3. optimizing Mp with 325 problems solved.
This order is preserved on the level of individual domains, except for several domains where
Mp does not end up last. Mp solves optimally the most problems from 1998-MYSTERY,
2000-BLOCKS, 2008-CYBER-SECURITY, and also from the 1998-MPRIME domain. The
margin is exceptionally pronounced on the last domain, where Mp solves 32 out of 35
problems, while BJOLP solves 21 and PDRplan1.1-anytime only 20 problems.
5.8 Detecting Unsatisfiable Problems
Although the main focus of the planning community, as reflected by the International
Planning Competition, has traditionally been on satisfiable problems only, more recently,
the importance of detecting unsatisfiable instances is getting recognized and addressed
(Backstrom, Jonsson, & Stahlberg, 2013; Hoffmann, Kissmann, & Alvaro Torralba, 2014).
According to the experience from hardware model checking, PDR should be particularly
strong at detecting unsatisfiable instances. In our last experiment, we tried to established
whether this also holds in planning.
As the test problems, we used a collection by Hoffmann et al. (2014) consisting of 8
domains and a total of 183 unsatisfiable benchmarks. Table 4 shows domain-by-domain
coverage results (for a time limit of 1800 seconds) of the following configurations of PDR:
27. PDR then looks for minimal length witnessing paths with respect to the encoded transition relation T .
Using an encoding with sequential plan semantics (as implicitly done by PDRplan) ensures optimizing
the number of actions of the resulting plan.
28. Formally, we keep an obligation (s, i) if the length of the path from the initial state sI to s plus the value
of the index i does not exceed the length of the best plan found so far.
29. This choice ruled out systems like SatPlan (Kautz et al., 2006) or SASE (Huang et al., 2012) from the
comparison, because they only optimize with respect to the parallel plan semantics.
30. It uses a sequential encoding (option -P 0), does not skip any horizon length (option -S 1), and evaluates
a single horizon length at a time (option -A 1).

306

fiProperty Directed Reachability for Automated Planning

size
3UNSAT
Bottleneck
Mystery
UnsNoMystery
UnsPegsol
UnsRovers
UnsTiles
UnsTPP
Total

25
30
9
25
24
25
20
25
183

PDRplan
fwd bwd
10
10
19
24
4
9
12
11
14
8
11
11
0
0
5
6
75
79

minireachIC3 with par
fwd bwd noind nocp
11
15
11
5
25
23
20
22
9
9
9
9
3
13
13
6
14
8
8
4
11
20
15
12
0
0
0
0
4
6
3
3
66
94
79
61

blind
15
10
2
0
24
0
10
5
66

M&S
cf1 cf2
15
15
10
21
9
6
25
25
24
24
17
9
10
10
9
9
119 119

Table 4: Unsatisfiable benchmarks. Number of problems solved within 1800 seconds,
grouped by domain. Best scores per domain are typeset in bold.

 PDRplan, in the same configuration as in the first experiment (Section 5.2), both in
the forward (fwd) and backward (bwd) direction.
 minireachIC3 combined with the par encoding (with invariants), also in both directions (fwd, bwd), and, in the backward direction, also with the inductive minimization
replaced by the non-inductive version (noind), and, independently, with the clause
propagation turned off (nocp).
In addition, Table 4 also contains entries adopted from the work of Hoffmann et al. (2014).
These belong to the Fast Downward planner equipped with three different heuristics:
 Configuration blind uses a heuristic which returns 0 on goal states and 1 elsewhere
 it essentially proves unsatisfiability by enumerating all the reachable states.
 Configuration cf1 and cf2 each use a version of a merge-and-shrink (M&S) heuristic
(Helmert, Haslum, & Hoffmann, 2007), specifically adapted for detecting unsatisfiable
problems (Hoffmann et al., 2014). These were the two best performing configurations
in the experiment of Hoffmann et al. (2014).
We note that Hoffmann et al. (2014) also used a time limit of 1800 seconds, but ran their
experiment on 2.20 GHz Intel E5-2660 machines with a 4 GB memory limit. This means
that the last three configurations could be potentially solve more problems in our setup.
5.8.1 Results of the Experiment
When comparing the various configurations of PDR, we can see that the backward direction is generally more successful than the forward, although not consistently across all the
domains. Interestingly, minireachIC3 with the par encoding in the backward direction
solves more problems than PDRplan. In fact, a preliminary test with a lower time limit
showed that on these benchmarks this configuration is the strongest of all those considered
in our first experiment (Section 5.2). Finally, we can also see that both induction and clause
propagation are consistently helpful for solving unsatisfiable problems.
307

fiSuda

PDR does not come out as a winner from the comparison to the heuristic approach of
Hoffmann et al. (2014), although it is able to solve the most problems on four domains. On
two other domains, however, PDR is even dominated by blind search, i.e. by a simple state
space enumeration. It seems that more benchmarks will be needed to establish which of the
two approaches is generally more successful at detecting unsatisfiable planning problems.
5.8.2 Performance on UnsTiles
PDR is particularly bad at enumerating states when there is little possibility to generalize
from the encountered ones. This is manifested most clearly on the UnsTiles domain, from
which PDR could not solve a single problem within the given time limit. The domain
represents the well known sliding puzzle and contains 10 problems with 8 tiles in a 3  3
grid and 10 problems with 11 tiles in a 3  4 rectangular grid.31 We ran PDRplan in the
forward direction to the end on the one of the smaller, 3  3 instances. It took about a day
to complete, processed 701704 obligations and terminated when all the clauses from layer
11, in total 181440 clauses, where pushed to layer 12 during the clause propagation phase
of iteration 11.
Notice that 181440 = 9!/2 is half the size of the state space. By the classical result of
Johnson and Story (1879), the state space of the sliding puzzle decomposes into exactly
two connected components depending on the value of a certain parity function defined on
the states. Unsatisfiable instances are those where the parity of the initial state and the
goal state are different. Because the state space consists of just of two components, on a
unsatisfiable instance PDR must converge (with the repeating layer) to a CNF description
of the component containing the goal state. As we can see, this description is as large (in
the number of clauses) as the component itself (in the number of states), and thus on the
sliding puzzle PDR does not benefit at all from the symbolic representation via CNF.
5.9 Summary
Let us summarize the empirical findings obtained in this section. We state them as general
claims while keeping in mind that they are, in fact, derived from the performance on two
particular benchmark sets: the main set of 1561 mostly satisfiable IPC problems and the
set of 183 unsatisfiable problems used in the last experiment.
 When planning with PDR it pays off to look for a plan from the initial state towards
the goal and not vice versa. In other words, progression is preferable to regression
in PDR. This holds even when invariants are employed, which help to improve the
performance of regression considerably.
Unsatisfiable instances, however, are typically better detected via regression.
 On satisfiable problems the SAT-solver free variant of PDR with planning-specific
extend procedure (as described in Section 4) is generally more successful than the
standard version of the algorithm combined with various encodings.
31. Most famous is the 15 tiles puzzle on a 4  4 grid (Wikipedia, 2014).

308

fiProperty Directed Reachability for Automated Planning

 Neither clause propagation nor inductive minimization, two techniques which are normally deemed essential for the performance of PDR, are very helpful on satisfiable
planning problems. The techniques are, however, useful for detecting unsatisfiability.
 There are various ways of tuning PDR and improving its performance for planning.
We tried to identify a configuration of the algorithm that would be most successful in
our setup and later used it in PDRplan for a comparison with other planners. For all
the techniques that turned out to be an improvement on average, however, there were
exceptions in the form of individual problems or domains were performance degraded.
These represent an interesting opportunity for future investigations (see Section 7).32
 When compared to other planners PDRplan shows respectable performance. In fact,
its performance is comparable to or even slightly better than that of the planner Mp, a
state of the art representative of the planning as satisfiability approach. It also solves
the most problems of all the tested planners on several domains. Although PDRplan
does not reach the score of LAMA-2011, the presented results are quite encouraging,
especially given that PDR is a relatively young algorithm with a potential for further
improvements.
 When plan quality is more important than just the number of problems solved, it
pays off to switch from the stack to the queue tie-breaking strategy in PDR. Such a
configuration is then able to keep up with and improve upon the performance of Mp
with respect to the plan quality metric based on aggregated score.
Another option for improving plan quality is to employ a post-processing step which
attempts to remove redundant actions from the generated plan (Balyo & Chrpa, 2014).
 PDR can be easily modified to look for increasingly better solutions when given sufficient time and to eventually terminate with an optimality guarantee (with respect to
plan length). Although LAMA-2011 is much more successful in finding optimal plans,
the fact the PDRplans natural encoding follows the sequential plan semantics could
be the reason why PDRplan scores higher than Mp in this respect.

6. Related Work: Graphplan
We have shown in this paper that PDR is an algorithm closely related to the planning as
satisfiability approach, although with the planning-specific implementation of the extend
procedure no explicit encoding is present. We also highlighted the connection to heuristic
search planning, with the direct correspondence on the side of explicitly explored reachable
states and a little more subtle one on the side of the guiding layers, which can be seen as
a continually refined admissible heuristic estimator. What we would like to discuss here is
a perhaps surprising relation of PDR to the well-known Graphplan planning algorithm by
Blum and Furst (1997).
32. On the one hand, by looking at the problems where a particular technique leads to a poor performance,
we can identify its weak points and attempt to improve the technique. On the other hand, instead of
relying on an overall best configuration we can also try to decide prior to running the algorithm itself
on a promising set of enabled features for a given problem based on the problems characteristics.

309

fiSuda

The main data structure of Graphplan is a planning graph, a layered structure for compressed representation of reachability information about the given problem. The individual
layers of the graph over-approximate the set of states reachable by a given number of sets
of parallel actions and are computed incrementally by propagation of so called exclusion
relations between actions and state variables. The planning graph is searched for a plan by
a backward-chaining strategy, starting from the goal set and regressing it, in the sense of
the parallel plan semantics, to subgoals that do not violate the exclusions of the respective
layer. Candidate (sub)goal sets shown not to lead to a plan within a specific number of
steps are memoized to avoid repeating the same work in the future.
As already shown by Rintanen (2008a) the exclusion relations of the planning graph are
equivalent to binary clause representation of k-step reachability information. This means
they could be represented inside PDR as binary clauses in the respective layers. We claim
additionally that also the memoized goal sets could be stored as layer clauses at the respective position: the clause being simply the negation of the conjunctive description of the
goal set. With these two observations in mind, we can state that
Graphplan is essentially a version of PDR with a specific implementation of the
extend procedure based on the parallel plan semantics.
This correspondence allows us to highlight some other differences between the two algorithms beyond the preferred semantics of the emulated encoding.
 While the planning graph is built systematically by Graphplan and search for a plan
is only started (resumed) when a full new layer has been computed, in PDR the layer
construction is lazy, being triggered by unsuccessful path extensions.
Goal set memoization in Graphplan, however, follows the same lazy pattern.
 Graphplan does not attempt to reduce the size of a memoized goal set, so, apart from
the binary clauses, it only deals with long clauses representing the negation of the
goal set. Notice that this would in PDR correspond to returning the full reason set
Lits(s) after an unsuccessful extension of the state s.
A subset memoization has later been proposed by Long and Fox (1999), which corresponds to finding smaller reason sets.
 Graphplan searches for a plan in the backward direction. In PDR, the direction can
be changed, but forward is more successful.33
 There is no equivalent to obligation rescheduling in Graphplan and so the algorithm
always searches for optimal plans (with respect to the parallel plan semantics).
The wavefront heuristic described by Long and Fox (1999) in their enhancement of
Graphplan, however, seems to overcome this limitation, similarly to rescheduling.
The realization that PDR is related to Graphplan made us curious about the differences
of the two algorithms in practice. We set up a small experiment where we compared PDR
33. Changing the search direction in Graphplan by running in on an inverted problem (see Section 4.4)
is possible, but would likely lead to fewer problems solved. This is related to the already mentioned
observation that there are very few problems with non-trivial backward invariants in the benchmark set.

310

fiProperty Directed Reachability for Automated Planning

to a mature implementation of Graphplan within the planner IPP (Koehler, 1999). In
order to bring PDR as close as possible to what Graphplan does, we represented it by
minireachIC3 combined with the simple parallel encoding SPpar (see Section 2.6) enhanced
by the binary clause invariant (as explained in Section 5.2). We ran minireachIC3 in the
backward direction and with obligation rescheduling turned off, so, similarly to IPP, it
was looking for optimal plans. When measuring the number of problems solved (out of the
main problem set described in Section 5) within 180 seconds, we obtained 466 solved by IPP
and 484 by our configuration of minireachIC3. It should be noted that IPP erroneously
reports UNSAT for most of the problems from the PARCPRINTER and WOODWORKING
domains and we counted this as failures. Because minireachIC3, on the other hand, solves
most of the problems from these domains, its score should be lowered by 94 problems to
obtain a fair comparison on the problem set which excludes these two domains.
Notice that the performance of IPP with 466 solved problems is quite low compared
to the best configuration of PDRplan1.1, which solves 1212 problems within 180 seconds.
This raises the question whether Graphplan could be improved by enhancing it with the
obligation rescheduling trick. We were able to confirm this experimentally. A relatively
straightforward modification of IPP which retries a candidate goal set at time t + 1 after
it has failed at time t was able to solve 676 problems.34 Thus obligation rescheduling can
be seen as an answer to the long standing question posed in the last remark of the original
Graphplan paper by Blum and Furst (1997), i.e., as a way to trade plan quality for speed.

7. Discussion: a Closer Look at Two Domains
The fact that PDR maintains its reachability information organized in layers and uses the
simple language of propositional clauses (CNF) to express the corresponding constraints
often allows us to obtain additional insights on how the algorithm traverses the search
space by inspecting the layers generated for concrete problems. This is especially rewarding
in cases where PDR seems to be struggling with a relatively simple problem, as it often
leads to a discovery of ideas for future improvements. In this section we have a closer look
at the behavior of PDR on two simple domains. We conjecture that the algorithm could be
improved by employing a more expressive constraint formalism than CNF.
7.1 1998-LOGISTICS
The task in the LOGISTICS domain is to transport packages between locations. Locations
belong to cities and within a city trucks may be used to move packages with the help of the
load-truck, drive-truck, and unload-truck actions. Additionally, some of the locations are
designated as airports and airplanes may be used to transport packages between airports
possibly across cities via the load-airplane, fly-airplane, and unload-airplane actions.
Although the LOGISTICS domain is generally considered to be a simple one, Table 3
(page 304) reveals relatively poor performance of PDRplan on LOGISTICS problems. Here
are two of our initial findings from the inspection of the layer clauses generated by PDR,
which shed some light on what is going on under the hood.
34. Also the performance of the corresponding configuration of minireachIC3 goes up from the mentioned
484 to 733 solved problems within 180 seconds when obligation rescheduling is turned on.

311

fiSuda

 PDR often generates very long clauses.

Because there are typically many distinct (although similar) ways to achieve a subgoal
and all of them need to be taken into account, large reason sets are computed and
subsequently long clauses derived. For example, if a package needs to be transported
from one city to another, any of the available airplanes can potentially be used for
that purpose. We often encounter derived clauses like
subg  at(apn 1 , apt)  at(apn 2 , apt)  . . .  at(apn n , apt)

(7)

expressing that if the subgoal subg has not been derived yet, at least one of the
available airplanes apni need to be present at the airport apt.
 PDR generates many similar clauses.

Even if an action has more than one precondition false in the current state, at most one
of these preconditions is reflected in the computed reason of an unsuccessful extension.
Thus with many actions available for achieving a subgoal, sometimes many clauses are
needed as PDR tries to find the right achieving action and satisfy all its preconditions.
In addition to the above clause (7) we could see PDR subsequently derive the following
clauses in the same layer:
subg  in(obj , apn 1 )  at(apn 2 , apt)  . . .  at(apn n , apt),

subg  at(apn 1 , apt)  in(obj , apn 2 )  . . .  at(apn n , apt),
...

(8)

subg  at(apn 1 , apt)  at(apn 2 , apt)  . . .  in(obj , apn n ).
Note that although the pattern indicates n different clauses, there are in the worst
case 2n clauses potentially derivable with the arbitrary choice between at(apn i , apt)
and in(obj , apn i ) for every i.
Although we have so far described PDR as an algorithm based on propositional logic,
we believe it could be generalized to take advantage of first order constraints. Consider the
clause (7) above. An equivalent first order version (aware of the type airplane) would read
subg  Apn  airplane . at(Apn, apt),
which is much more succinct.35 Moreover, it could potentially be derived by just analyzing
the action schemes unload -airplane(Obj , Apn, Loc), . . . , etc., instead of iterating through
the much larger set of instantiated actions. Working out the missing details is an interesting
direction for future research. An inspiration could be found in the work of Ranise (2013),
whose setting of security policy analysis is very close to automated planning.
Another independent direction for enhancing the expressive power of the used constraints
could be the introduction of conjunctive literals. Notice that the set of clauses (8) is, in
fact, subsumed by a single generalized clause
subg 

n
_
i=1

in(obj , apn i )  at(apn i , apt),

35. Symbols starting with an uppercase letter, like Apn, stand for first order variables.

312

fiProperty Directed Reachability for Automated Planning

where we allow conjunctions in place of single literals. In this envisioned generalization of
PDR, such conjunctions would naturally come from the precondition lists of actions, and
their use could help with solving, e.g., the LOGISTICS problems more efficiently. Of course,
there are again details that would need to be worked out.
7.2 1998-GRIPPER
GRIPPER is a very simple domain which models a robot with two grippers trying to move
balls from one room to another. This domain is fully solved by PDRplan in the default
configuration. In fact, although the individual problems differ in size, PDRplan is able
(thanks to obligation rescheduling) to solve all of them during iteration 3 of the main loop.36
The reason for this seems to be the virtual independence of the individual goals, which can
be considered one be one by PDR. We conjecture that the algorithm solves problems from
the GRIPPER domain in polynomial time.
Despite the simplicity, GRIPPER is known to be difficult to solve optimally by heuristic
search planners (see Helmert & Roger, 2008). This also holds for PDR, which exhibits
exponential behavior when attempting to find a minimal length plan, i.e., when run with
obligation rescheduling (and sidestepping) turned off. To demonstrate the reason, let us
abstract and simplify GRIPPER a bit more and consider a domain in which the task is to
achieve n independent goals from the set {g1 , . . . , gn }, such that achieving a particular goal
is trivial, but the individual goals can only be achieved one by one.
On such a domain, PDR will eventually need to express via layer Li that at least (n  i)
goals should already be achieved. Such a counting constraint has inherently large clausal
description. Namely, the set Li takes the form
^
gj0  . . .  gji ,
where the conjunction ranges over all (i + 1)-element subsets {j0 , . . . , ji } of {1, .. . , n}. The
n
size of the layer Li is, therefore, proportional to the binomial coefficient i+1
, which, in
particular, means that the size of the layer Lbn/2c grows exponentially with n.
As already suggested by Helmert and Roger (2008) this phenomenon could be overcome
by exploiting symmetries (Fox & Long, 1999) inherently present in the problem. This could
be particularly rewarding in PDR, where the layer clauses (although derived as a response
to unsuccessful extensions of arbitrary reachable states) logically depend only on the goal
condition G, where the symmetries typically reside. Thus unlike Fox and Long (1999),
who define symmetric objects to be those which are indistinguishable from one another in
terms of their initial and goal configuration, one could with PDR use a stronger notion of
symmetry derived from the goal condition only.

8. Conclusion
In this paper we have examined PDR, a novel algorithm for analyzing reachability in symbolic transition systems, from the perspective of automated planning. Our main contribution lies in recognizing that a part of the algorithms work normally delegated to a
36. Other domains fully solved by PDRplan during a particular fixed iteration are 2002-ZENOTRAVEL
(iteration 3), 2004-PHILOSOPHERS (iteration 6), and 2006,2008,2011-OPENSTACKS (iteration 4).

313

fiSuda

SAT-solver can, in the context of planning, be implemented directly by a polynomial time
procedure. We have experimentally confirmed that this modification, as well as several
other proposed improvements, boost the performance of PDR on planning benchmarks.
Our implementation of the algorithm called PDRplan was able to compete respectably with
state of the art planners, solving most problems on several domains.
Despite the already promising results, there is still room for further development. One
direction is work on extending PDRplan towards richer planning formalisms. For example,
we believe the extend procedure can be enhanced to cope with conditional effects of actions
in a straightforward way. Efficiently dealing with action costs or domain axioms could turn
out to be more difficult. Another promising direction is the idea to generalize PDR to a
more expressive constraint language than CNF. While it is clear that stronger constraints
imply better guidance towards the goal, devising an efficient method for combining new
constraints from old ones is obviously a challenging task. It seems, however, that this
departure beyond the clausal level could have a simpler solution inside the planningspecific framework of the extend procedure than it, perhaps, has within general purpose
constraint solvers.

Acknowledgments
We thank Jussi Rintanen for useful comments and remarks, as well as for the help with
the Mp encoder. We also thank Tomas Balyo for providing us with the SASE encoding
tool. Finally, we want to thank the anonymous reviewers for their insightful suggestions
and Malte Helmert for the help with the preparation of the final version of this text.
This research was partially supported by the Czech Science Foundation under the project
P103-10-1287.

Appendix A. Pseudocode of the Improvements of Section 4.5
Pseudocode 6 displays stage one of procedure extend+ , an enhancement of the extend
procedure by the lazy false clause computation technique (Section 4.5.1) and with a support
for sidestepping (Section 4.5.2). Stage two of extend+ is meant to be supplemented from
the same stage of the original extend procedure (Pseudocode 3) and stage three employes
inductive minimization as described in Section 4.2 (Pseudocode 4).
Pseudocode 7 details the workings of PDRplan1.1. With the help of the extend+ procedure it realizes sidestepping (Section 4.5.2). Moreover, it incorporates the technique for
keeping obligations between iterations (Section 4.5.3). The clause propagation phase is
identical to the one already presented in Pseudocode 5.

314

fiProperty Directed Reachability for Automated Planning

Pseudocode 6 Stage one of extend+ (s, i):
Input:
Obligation (s, i), i.e. a state s and an index i, such that s 6|= Li1
Output:
Either an obligation (t, i  1) where t is a successor of s and t |= Li1 , or
an obligation (t, i) where t is a successor of s, t |= Li and
t satisfies strictly more clauses from Li1 than s, or
an inductive reason r  Lits(s)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Ls  {c  Li1 | s 6|= c} /* Clauses false in s */
Rnoop  {c | c  Ls } /* The reasons for the noop action */
assert Rnoop 6=  /* Follows from the contract with the caller */
R  {Rnoop } /* The set of reason sets collected so far */
aside  noop /* Current best candidate for sidestepping (noop as a dummy value) */
xside  |Ls | /* Score of the current best candidate */
foreach a  A do
pre sa  {l  pre a | s 6|= l} /* Preconditions false in s */
t  apply(s, (, eff a )) /* Ignore the preconditions and apply the effects of a */
Ls,t  {c  Ls | t 6|= c} /* The lazy approach: clauses false both in s and t */
if Ls,t = Ls then /* No improvement over s */
continue /* Do nothing: the reason set would be subsumed by Rnoop */

14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:

if pre sa =  and |Ls,t | < xside then /* The action is promising . . . */
Lt  {c  Li1 | t 6|= c} /* . . . we must compute the full Lt */
if Lt =  then
return (t, i  1) /* The positive part: returning a true successor */
if Lt = Ls,t and t |= Li then /* No false clauses besides those from Ls,t */
aside  a
xside  |Ls,t |
else
Lt  Ls,t /* Save time by using Ls,t instead of the full Lt below */
Lt0  {c  Lt | c  pre sa = } /* False clauses with a non-subsumed reason */
Ra  {{l} | l  pre sa }  {{l | l  c and l 6 eff a } | c  Lt0 }
R  R  {Ra } /* Record the reason set */

if xside < |Ls | then
30:
assert aside 6= noop
31:
return (apply(s, aside ), i) /* Successfully sidestepping with the best candidate */
29:

32:

/* Continue with stage two as in Pseudocode 3 and stage three as in Pseudocode 4 */

315

fiSuda

Pseudocode 7 Algorithm PDRplan1.1(X, sI , G, A):
Input:
A positive STRIPS planning problem P = (X, sI , g, A)
Output:
A plan for P or a guarantee that no plan exists
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:

L0  {{p} | p  g} /* The goal cube treated as a set of unit clauses */
foreach j > 0 : Lj  
Q  {(sI , 0)}
for k = 0, 1, . . . do
/* Path construction: */
while there is (s, i) in Q with i  k do
pop some (s, i) from Q with minimal i
if s 6|= Li then
Q  Q  (s, i + 1)
else if i = 0 then
return PLAN FOUND
else if extend+ (s, i) returns an obligation (t, j) then
assert j = i  1 or j = i /* Either a regular extension or a sidestep */
Q  Q  {(s, i), (t, j)}
else
extend+ returned a reason r  Lits(s)
foreach 0  j  i : Lj  Lj  {r}
/* Obligation rescheduling: */
Q  Q  {(s, i + 1)} /* Keep obligations with i + 1 > k till the next iteration */
/* Clause propagation: */
for i = 1, . . . , k + 1 do
foreach c  Li1 \ Li do
/* Clause push check */
sc  {p 7 0 | p  c}  {p 7 1 | p  (X \ c)}
if for every a  A : sc 6|= pre a or apply(sc , a) 6|= Li1 then
Li  Li  {c}
/* Convergence check */
if Li1 = Li then
return NO PLAN POSSIBLE

316

fiProperty Directed Reachability for Automated Planning

References
Backstrom, C., Jonsson, P., & Stahlberg, S. (2013). Fast detection of unsolvable planning
instances using local consistency. In Helmert, M., & Roger, G. (Eds.), SOCS. AAAI
Press.
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11, 625656.
Balyo, T., Bardiovsky, V., Dvorak, F., & Toropila, D. (2012). Freelunch planning library.
Available at http://ktiml.mff.cuni.cz/freelunch/.
Balyo, T., & Chrpa, L. (2014). Eliminating all redundant actions from plans using SAT
and MaxSAT. In ICAPS 2014 Workshop on Knowledge Engineering for Planning and
Scheduling (KEPS). To appear.
Biere, A., Heljanko, K., Seidl, M., & Wieringa, S. (2012). Hardware model checking competition 2012. Web site, http://fmv.jku.at/hwmcc12/.
Blum, A., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artif.
Intell., 90 (12), 281300.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artif. Intell., 129 (12), 533.
Bradley, A. R. (2011). SAT-based model checking without unrolling. In Jhala, R., &
Schmidt, D. A. (Eds.), VMCAI, Vol. 6538 of Lecture Notes in Computer Science, pp.
7087. Springer.
Bradley, A. R., & Manna, Z. (2007). Checking safety by inductive generalization of counterexamples to induction. In FMCAD, pp. 173180. IEEE Computer Society.
Clifford, R., & Popa, A. (2011). Maximum subset intersection. Inf. Process. Lett., 111 (7),
323325.
Domshlak, C., Helmert, M., Karpas, E., Keyder, E., Richter, S., Roger, G., Seipp, J., &
Westphal, M. (2011). BJOLP: The big joint optimal landmarks planner. In Seventh
International Planning Competition (IPC 2011), Deterministic Part, pp. 9195.
Een, N., Mishchenko, A., & Brayton, R. K. (2011). Efficient implementation of property
directed reachability. In Bjesse, P., & Slobodova, A. (Eds.), FMCAD, pp. 125134.
FMCAD Inc.
Een, N., & Sorensson, N. (2003). An extensible SAT-solver. In Giunchiglia, E., & Tacchella, A. (Eds.), SAT, Vol. 2919 of Lecture Notes in Computer Science, pp. 502518.
Springer.
Fox, M., & Long, D. (1999). The detection and exploitation of symmetry in planning
problems. In Dean, T. (Ed.), IJCAI, pp. 956961. Morgan Kaufmann.
Gazen, B. C., & Knoblock, C. A. (1997). Combining the expressivity of UCPOP with the
efficiency of Graphplan. In Steel, S., & Alami, R. (Eds.), ECP, Vol. 1348 of Lecture
Notes in Computer Science, pp. 221233. Springer.
Ghallab, M., Nau, D. S., & Traverso, P. (2004). Automated planning  theory and practice.
Elsevier.
317

fiSuda

Haas, A. R. (1987). The case for domain-specific frame axioms. In The Frame Problem in
Artificial Intelligence, Proceedings of the 1987 Workshop on Reasoning about Action.
Morgan Kaufmann.
Helmert, M. (2006). The Fast Downward planning system. J. Artif. Intell. Res. (JAIR),
26, 191246.
Helmert, M. (2009). Concise finite-domain representations for PDDL planning tasks. Artif.
Intell., 173 (56), 503535.
Helmert, M., Do, M., & Refanidis, I. (2008). IPC 2008, deterministic part. Web site,
http://ipc.informatik.uni-freiburg.de.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics for optimal
sequential planning. In Boddy, M. S., Fox, M., & Thiebaux, S. (Eds.), ICAPS, pp.
176183. AAAI.
Helmert, M., & Roger, G. (2008). How good is almost perfect?. In Fox, D., & Gomes, C. P.
(Eds.), AAAI, pp. 944949. AAAI Press.
Hoffmann, J., Kissmann, P., & Alvaro Torralba (2014). Distance? Who cares? Tailoring
Merge-and-Shrink heuristics to detect unsolvability. In ICAPS 2014 Workshop on
Heuristics and Search for Domain-independent Planning (HSDIP). To appear.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. J. Artif. Intell. Res. (JAIR), 14, 253302.
Howey, R., Long, D., & Fox, M. (2004). VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. In ICTAI, pp. 294301.
IEEE Computer Society. Software available at http://www.plg.inf.uc3m.es/
ipc2011-deterministic/Resources.
Huang, R., Chen, Y., & Zhang, W. (2012). SAS+ planning as satisfiability. J. Artif. Intell.
Res. (JAIR), 43, 293328.
IPC

(2014).
International planning competition.
icaps-conference.org/, accessed 19/05/2014.

Web

site,

http://ipc.

Johnson, W. W., & Story, W. E. (1879). Notes on the 15 puzzle. American Journal of
Mathematics, 2 (4), 397404.
Kautz, H., Selman, B., & Hoffmann, J. (2006). SatPlan: Planning as satisfiability. In
Working Notes of the 5th International Planning Competition, Cumbria, UK. Software
available at http://www.cs.rochester.edu/~kautz/satplan/.
Kautz, H. A., McAllester, D. A., & Selman, B. (1996). Encoding plans in propositional
logic. In Aiello, L. C., Doyle, J., & Shapiro, S. C. (Eds.), KR, pp. 374384. Morgan
Kaufmann.
Kautz, H. A., & Selman, B. (1992). Planning as satisfiability. In ECAI, pp. 359363.
Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic
and stochastic search. In Clancey, W. J., & Weld, D. S. (Eds.), AAAI/IAAI, Vol. 2,
pp. 11941201. AAAI Press / The MIT Press.
318

fiProperty Directed Reachability for Automated Planning

Koehler, J. (1999). IPP  A Planning System for ADL and Resource-Constrained Planning
Problems. Habiliation thesis, University of Freiburg.
Long, D., & Fox, M. (1999). Efficient implementation of the plan graph in STAN. J. Artif.
Intell. Res. (JAIR), 10, 87115.
Massey, B. (1999). Directions In Planning: Understanding The Flow Of Time In Planning.
Ph.D. thesis, University of Oregon.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint of
artificial intelligence. In Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp.
463502. Edinburgh University Press.
Nakhost, H., & Muller, M. (2010). Action elimination and plan neighborhood graph search:
Two algorithms for plan improvement. In Brafman, R. I., Geffner, H., Hoffmann, J.,
& Kautz, H. A. (Eds.), ICAPS, pp. 121128. AAAI.
Pettersson, M. P. (2005). Reversed planning graphs for relevance heuristics in AI planning.
In Planning, Scheduling and Constraint Satisfaction: From Theory to Practice, Vol.
117 of Frontiers in Artificial Intelligence and Applications, pp. 2938. IOS Press.
Ranise, S. (2013). Symbolic backward reachability with effectively propositional logic 
applications to security policy analysis. Formal Methods in System Design, 42 (1),
2445.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. J. Artif. Intell. Res. (JAIR), 39, 127177.
Rintanen, J. (1998). A planning algorithm not based on directional search. In Cohn, A. G.,
Schubert, L. K., & Shapiro, S. C. (Eds.), KR, pp. 617625. Morgan Kaufmann.
Rintanen, J. (2004). Evaluation strategies for planning as satisfiability. In de Mantaras,
R. L., & Saitta, L. (Eds.), ECAI, pp. 682687. IOS Press.
Rintanen, J. (2008a). Planning graphs and propositional clause-learning. In Brewka, G., &
Lang, J. (Eds.), KR, pp. 535543. AAAI Press.
Rintanen, J. (2008b). Regression for classical and nondeterministic planning. In Ghallab,
M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.), ECAI, Vol. 178 of
Frontiers in Artificial Intelligence and Applications, pp. 568572. IOS Press.
Rintanen, J. (2012). Planning as satisfiability: Heuristics. Artif. Intell., 193, 4586.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning as satisfiability: parallel plans
and algorithms for plan search. Artif. Intell., 170 (1213), 10311080.
Suda, M. (2013a). DIMSPEC, a format for specifying symbolic transition systems. Web
site, http://www.mpi-inf.mpg.de/~suda/DIMSPEC.html.
Suda, M. (2013b). Duality in STRIPS planning. CoRR, abs/1304.0897.
Suda, M. (2014). Property directed reachability for automated planning. Web site, http:
//www.mpi-inf.mpg.de/~suda/PDRplan.html.
Wikipedia (2014). 15 puzzle  wikipedia, the free encyclopedia. Web site, http://en.
wikipedia.org/wiki/15_puzzle, accessed 19/05/2014.

319

fiJournal of Artificial Intelligence Research 50 (2014) 105-140

Submitted 07/13; published 05/14

Finding Optimal Solutions for Voting Game Design Problems
Bart de Keijzer

dekeijzer@dis.uniroma1.it

Web Algorithmics and Data Mining,
Sapienza  Universita di Roma,
Rome, Italy

Tomas B. Klos

t.b.klos@tudelft.nl

Algorithmics; Delft University of Technology,
Delft, The Netherlands

Yingqian Zhang

yqzhang@ese.eur.nl

Department of Econometrics; Erasmus University Rotterdam,
Rotterdam, The Netherlands

Abstract
In many circumstances where multiple agents need to make a joint decision, voting is
used to aggregate the agents preferences. Each agents vote carries a weight, and if the
sum of the weights of the agents in favor of some outcome is larger than or equal to a
given quota, then this outcome is decided upon. The distribution of weights leads to a
certain distribution of power. Several power indices have been proposed to measure such
power. In the so-called inverse problem, we are given a target distribution of power, and
are asked to come up with a gamein the form of a quota, plus an assignment of weights
to the playerswhose power distribution is as close as possible to the target distribution
(according to some specified distance measure).
Here we study solution approaches for the larger class of voting game design (VGD)
problems, one of which is the inverse problem. In the general VGD problem, the goal is
to find a voting game (with a given number of players) that optimizes some function over
these games. In the inverse problem, for example, we look for a weighted voting game that
minimizes the distance between the distribution of power among the players and a given
target distribution of power (according to a given distance measure).
Our goal is to find algorithms that solve voting game design problems exactly, and we
approach this goal by enumerating all games in the class of games of interest. We first
present a doubly exponential algorithm for enumerating the set of simple games. We then
improve on this algorithm for the class of weighted voting games and obtain a quadratic
2
exponential (i.e., 2O(n ) ) algorithm for enumerating them. We show that this improved
algorithm runs in output-polynomial time, making it the fastest possible enumeration algorithm up to a polynomial factor. Finally, we propose an exact anytime-algorithm that
runs in exponential time for the power index weighted voting game design problem (the
inverse problem).
We implement this algorithm to find a weighted voting game with a normalized Banzhaf
power distribution closest to a target power index, and perform experiments to obtain some
insights about the set of weighted voting games. We remark that our algorithm is applicable
to optimizing any exponential-time computable function, the distance of the normalized
Banzhaf index to a target power index is merely taken as an example.

c
2014
AI Access Foundation. All rights reserved.

fiDe Keijzer, Klos, & Zhang

1. Introduction
In many real-world settings involving multiple players who have to come up with a joint
decision, for instance elections, there is a need for fair decision-making protocols in which
different players have different amounts of influence on the outcome of a decision. A weighted
voting game (WVG) is often used as such a decision-making protocol. In a weighted voting
game, a quota is given, and each player (or agent) in the game has a certain weight. If the
total weight of a coalition of agents is not smaller than the quota, then that coalition is said
to be winning, and otherwise, losing.
Weighted voting games arise in various practical settings, such as in political decision
making (decision making among larger and smaller political parties), stockholder companies
(where number of shares determines amount of influence), and elections (e.g., in the US
presidential election, where each state can be regarded as a player who has a weight equal
to its number of electors).
The weight that a player has is not equal to his actual influence on the outcome of the
decisions that are made using the weighted voting game. Consider for example a weighted
voting game in which the quota is equal to the sum of the weights of all players. In such a
game, a players influence is equal to the influence of any other player regardless of the weight
he has. Various power indices have been proposed in the literature, as ways to measure a
players (a priori) power in influencing the outcome of a voting game (see Banzhaf III, 1965;
Dubey & Shapley, 1979; Shapley & Shubik, 1954). However, computing a power index has
turned out to be a challenge in many cases: see the work of Algaba, Bilbao, Garca, and
Lopez (2003), Deng and Papadimitriou (1994), Matsui and Matsui (2001), Prasad and Kelly
(1990), and De Keijzer (2009b) for a recent survey.
In this paper, instead of analyzing the power of each agent in a voting game, we investigate the problem referred to as the inverse problem (see Alon & Edelman, 2010)
or the generalized apportionment problem (see Leech, 2003). We will call this problem
the weighted voting game design problem. In the power index voting game design problem,
we are given a target power index for each of the agents, and we study how to design a
weighted voting game for which the power of each agent is as close as possible to the given
target power index.
The motivation behind our work is a practical one: It is desirable to have an algorithm
that can quickly compute a fair voting protocol, given that we want each agent to have some
specified amount of influence in the outcome. When new decision making bodies must be
formed, or when changes occur in the formation of these bodies, such an algorithm may be
used to design a voting method that is as fair as possible.
The most intuitive approach for solving the weighted voting game design problem would
be to simply enumerate all possible weighted voting games of n players. However, enumerating all weighted voting games efficiently is not quite as straightforward as it seems. Because
even for a single weighted voting game there is an infinite number of weighted representations (the representation of a game as a listing of a quota and a weight for each player),
enumerating games in this weighted representation is not an option. To address this problem, we prove the existence of, and exploit, a new partial order on the class of weighted
voting games that allows us to efficiently enumerate all weighted voting games. This enumeration method leads to an exact algorithm that can be used to solve any weighted voting

106

fiFinding Optimal Solutions for Voting Game Design Problems

game design problem. Although this algorithm runs in exponential time, asymptotically it
is a big improvement over the naive approach that runs in doubly exponential time, as we
will show. Therefore, besides the fact that we broaden our understanding of the problem by
this improvement to exponential time, the algorithm allows us to solve the problem to optimality for small numbers of players, and to find approximate solutions for larger numbers
of players, thanks to its anytime property.1 We implement our algorithm for the power
index voting game design problem where our power index of choice is the (normalized)
Banzhaf index. We emphasize that this choice of power index is quite arbitrary: we could
pick any other power index as well, and the normalized Banzhaf index is merely taken as
an example. We use our implementation to illustrate the applicability of our approach and
to obtain some relevant statistics on the set of weighted voting games.
In the next section we define relevant concepts and set up some notation. Section 3
then formally defines the problem we address, and gives an overview of related work. In
Section 4, we analyze the problem and present our solution. First we address the design
of (monotonic) simple games in Section 4.1. Then, in the most important section (4.2), we
focus on weighted voting game design. Section 4.3 concludes this part of the paper with
some improvements that we wish to discuss separately from the main ideas. In Section 5, we
report on some experiments we performed with an implementation of our main algorithm,
and Section 6 concludes the paper.

2. Preliminaries
In this section, we will discuss some required preliminary definitions and results related to
the theory of cooperative simple games.2
A cooperative game is a pair (N, v), where N is a finite set of players; subsets of N are
called coalitions, and v : 2N  R0 is the characteristic function or gain function, mapping
coalitions to non-negative real numbers. Intuitively, v describes how much collective payoff
a coalition of players can gain when they cooperate. The set N is also called the grand
coalition. A simple game is a cooperative game (N, v) where the codomain of v is restricted
to {0, 1}.3 In this context, a coalition S is called a winning coalition if v(S) = 1, and a losing
coalition otherwise. A cooperative game (N, v) is monotonic if and only if v(S)  v(T ) for
all pairs of coalitions (S, T )  2N  2N that satisfy S  T . In other words: in a monotonic
game, the value of a coalition can not decrease when players are added to the coalition.
In this paper, we are concerned with the class of monotonic simple games, which we
denote as Gmon . In general, if G is a class of games, we use G(n) to denote that class of
1. An algorithm is said to have the anytime property if: (i) it is able to provide a solution anytime during
execution, (ii) the solution quality improves the longer the algorithm runs, and (iii) the algorithm is
guaranteed to output the optimal solution eventually.
2. Much of the information in this section can be found in an introductory text on cooperative game theory
(e.g., Peleg & Sudholter, 2003) or on simple games (e.g., Taylor & Zwicker, 1999). Throughout this
paper, we assume familiarity with big-O notation and analysis of algorithms, as well as knowledge of
some basic order-theoretic notions related to graded partial orders. In some parts of this paper, some
basic knowledge of computational complexity theory is assumed as well, although these parts are not
crucial for understanding the main results presented. We will not cover these topics in this section.
3. We purposefully do not exclude the games with only losing and only winning coalitions, as is customary.
The reason is that including these games will make it more convenient later to show that a particular
structure exists in a subclass of the simple games.

107

fiDe Keijzer, Klos, & Zhang

games, restricted to the set of players {1, . . . , n}. So Gmon (3) is the class of monotonic simple
games with 3 players.
There are various important ways to represent (classes of) simple games. If G = (N, v) is
a simple game, we let WG and LG be Gs sets of winning and losing coalitions, respectively,
so that WG  LG = 2N and WG  LG = . The set Wmin,G  WG of Gs minimal winning
coalitions (MWC) contains every winning coalition from which we cannot remove a player
without making it a losing coalition. The set Lmax,G  LG of Gs maximal losing coalitions
(MLC) is defined analogously: A coalition is maximal losing if its losing and no player
can be added to it without making it winning. We can now describe a simple game in the
following forms:
Winning coalition form: (N, WG ) is called the winning coalition form of G.
Losing coalition form: (N, LG ) is called the losing coalition form of G.
Minimal winning coalition form: (N, Wmin,G ) is the minimal winning coalition form of
G. Observe that Wmin,G fully describes v if and only if G is monotonic.
Maximal losing coalition form: (N, Lmax,G ) is the maximal losing coalition form of G.
Again, Lmax,G fully describes v if and only if G is monotonic.
For some, but not all simple games, the exists another representation.
Weighted form: For a simple game G = (N, v), if there exists a quota q  R0 and a
weight wi  R0 P
for each player i  N , such that for each coalition S  2N it holds
that v(S) = 1  iS wi  q, then we say that G is weighted or has a weighted form,
and the vector w = (q, w1 , . . . , wn ), also written as [q; w1 , . . . , P
wn ], is called a weighted
representation of G. (We will write w(S) as a shorthand for iS wi .) Observe that
every game that has a weighted form is also monotonic. The converse is not true in
general,4 but we are interested only in those monotonic games that do have a weighted
form.
Games that have a weighted form, or weighted voting games, are our main interest.
We denote the class of all weighted voting games by Gwvg . A weighted voting game is an
important type of simple game because it has a compact representation, and because it is
used in many practical situations, such as elections, (European Union) political decision
making, and stockholder meetings. An important property of weighted voting games that
we will use is that a weighted representation of such a game is invariant to scaling: if
we multiply the quota and each of the weights in the weighted form of a WVG G with a
constant c  R+ , then the resulting weighted form [cq; cw1 , . . . , cwn ] represents the same
game G, that is, it has the same winning and losing coalitions.
We next turn our attention to the topic of influence and power in monotonic simple
games. For a monotonic simple game, it is possible to define a relation called the desirability
relation among the players (see Isbell, 1958):
4. To see this, consider the four-player monotonic game with MWCs {1, 2} and {3, 4}. Suppose, for the sake
of contradiction, that there exist weights w1 , . . . , w4 and a quota q that form a weighted representation
of this game. Then w1 + w2  q and w3 + w4  q, so w1 + w2 + w3 + w4  2q. But coalitions {1, 3}
and {2, 4} are losing, since they are not (supersets of) MWCs, so w1 + w3 < q and w2 + w4 < q, which
means that w1 + w2 + w3 + w4 < 2q, yielding a contradiction.

108

fiFinding Optimal Solutions for Voting Game Design Problems

Definition 1 (Desirability relation). For a monotonic simple game (N, v), the desirability
relation v is defined as follows: For any (i, j)  N 2 :
 if S  N \ {i, j} : v(S  {i})  v(S  {j}), then i v j. We say that i is more
desirable than j.
 if S  N \ {i, j} : v(S  {i}) = v(S  {j}), then i v j. We say that i and j are
equally desirable.
 if S  N \ {i, j} : v(S  {i})  v(S  {j}), then j v i (also written as i v j). We
say that i is less desirable than j.
 if i v j and not i v j, then i v j. We say that i is strictly more desirable than j.
 if i v j and not i v j, then i v j. We say that i is strictly less desirable than j.
Moreover, if neither i v j nor j v i holds for some i, j  N , then we call i and j
incomparable.
Using this notion of desirability, we can now define the class of linear games.
Definition 2 (Linear game). A simple game (N, v) is a linear game if and only if it is
monotonic, and no pair of players in N is incomparable with respect to v . Thus, for a
linear game (N, v), v is a total preorder on N . We denote the class of linear games by
Glin .
All weighted voting games are linear. If (N, v) is a weighted voting game with weighted
form [q; w1 , . . . , wn ], then i v j when wi  wj . So, every pair of players is comparable
with respect to v . In fact, the following sequence of strict containments holds: Gwvg 
Glin  Gmon . The following are definitions of two special classes of games used in subsequent
sections.
Definition 3 (Canonical weighted voting games and canonical linear games). A linear
game (N, v) is a canonical linear game whenever N = {1, . . . , n} for some n  N>0 , and
the desirability relation  satisfies 1  2      n. When G is also weighted, then G is a
canonical weighted voting game (CWVG). The class of canonical linear games is denoted
by Gclin , and the class of CWVGs is denoted by Gcwvg . Note that a CWVG always has a
weighted representation that is nonincreasing.
There are two special ways of representing canonical linear games. To introduce these,
we need the notions of left-shift and right-shift.
Definition 4 (Left-shift and right-shift). Let N be the set of players {1, . . . , n} and let S
be a nonempty subset of N . A coalition S 0  N is a direct left-shift of S whenever there
exists an i  S and an i  1 6 S with 2  i  n such that S 0 = (S \ {i})  {i  1}.
A coalition S 0  N is a left-shift of S whenever for some k  1 there exists a sequence
(S1 , . . . , Sk )  (2N )k , such that:
 S1 = S,
 Sk = S 0 ,
109

fiDe Keijzer, Klos, & Zhang

 for all i with 1  i < k, we have that Si+1 is a direct left-shift of Si .
We say that a coalition S 0 is a strict left-shift of S when S 0 is a left-shift of S and S 0 6= S.
The definitions of direct right-shift and (strict) right-shift are obtained when we replace in
the above definition i  1 with i + 1 and i + 1 with i  1.
For example, coalition {1, 3, 5} is a direct left-shift of coalition {1, 4, 5} because in the
former coalition, player 4 is replaced by player 4  1 = 3, and coalition {1, 2, 4} is a left-shift
of {1, 4, 5}, because the former can be obtained from the latter by a sequence of (three)
such direct left shifts.
The notions of left-shift and right-shift make sense for canonical linear games and canonical weighted voting games. Due to the specific desirability order that holds in canonical
linear games, a left-shift of a winning coalition is always winning in such a game, because
in the left-shift, some player from the winning coalition is replaced by a lower-numbered,
and thus more desirable player. Similarly, a right-shift of a losing coalition is always losing
in such a game. This allows us to represent a canonical linear game in one of the following
two forms.
Definition 5 (Roof/ceiling coalition/form). Let G = (N, v) be a canonical linear game.
Also, let Wmin,G be Gs set of minimal winning coalitions and let Lmax,G be Gs set of
maximal losing coalitions. A minimal winning coalition S  Wmin,G is a roof coalition
whenever every right-shift of S is losing. Let Wroof,G denote the set of Gs roof coalitions.
The pair (N, Wroof,G ) is called the roof form of G. A maximal losing coalition S  Lmax,G
is a ceiling coalition whenever every left-shift of S is winning. Let Wceil,G denote the set of
Gs ceiling coalitions. The pair (N, Wceil,G ) is called the ceiling form of G.5
2.1 Power Indices
If we consider the weighted voting game with weighted form [100; 98, 1, 1], we see that the
weight of a player is not necessarily directly proportional to the influence he has in the game.
In this game, only the grand coalition is winning. Since all players need to be present in
this coalition for it to be winning, they can all be said to have the same influence, despite
the fact that there is a huge difference between the weights of the first versus the other
players.
A variety of power indices have been proposed to measure players influence (instead of
weight) in a monotonic simple game. Power indices measure players a priori power in a
voting game. For this reason, power indices do not rely on (statistical) information about
which coalitions are likely to actually form due to the preferences of the players. In this
paper, we focus on the normalized Banzhaf index (also called simply the Banzhaf index,
see Banzhaf III, 1965), inasmuch as it is used in the experiments in Section 5. However, for
the theoretical part of our work, the particular choice of power index is irrelevant.
Definition 6 (normalized Banzhaf index & raw Banzhaf index). The (normalized) Banzhaf
index of a monotonic simple game (N = {1, . . . , n}, v) is defined as  = (1 , . . . , n ), where
5. The terminology roof and ceiling is taken from Peled and Simeone (1985), while Taylor and Zwicker
(1999) call these coalitions shift-minimal winning coalitions and shift-maximal losing coalitions.

110

fiFinding Optimal Solutions for Voting Game Design Problems

for 1  i  n,
0
i = Pn i

0
j=1 j

,

and

i0 = |{S  N \ {i} : v(S) = 0  v(S  {i}) = 1}|.

(1)

Here, i0 is called the raw Banzhaf index of player i, and it counts the number of losing
coalitions of other agents that player i can turn into winning by joining them.
The problem of computing power indices and its associated computational complexity
has been widely studied. For a survey of complexity results, and exact and approximation
algorithms for computing power indices, see the work of De Keijzer (2009b). Prasad and
Kelly (1990) prove computation of the raw Banzhaf index to be #P-complete,6 and the
fastest known exponential time algorithm for computing
the Banzhaf index is due to Klinz

and Woeginger (2005), achieving a runtime in O(( 2)n  n2 ).

3. Problem Statement and Related Work
In its most general statement, the voting game design (VGD) problem is the problem of
finding a simple game that optimizes a given requirement. One obtains different variants
of the problem by specifying (i) the type of simple game one is interested in, and (ii) the
requirement to be optimized. In this paper, we focus on finding a weighted voting game (in
weighted form), and on the requirement that the normalized Banzhaf power index of the
game is as close as possible to a given target power index. We call this variant the power
index weighted voting game design (PIWVGD) problem. Please note that the approach
we propose is not specific to minimizing the distance with a target power index; other
optimization criteria can be addressed as well. In fact, most of what follows pertains just to
enumerating the games in given classes, and not to the optimization part of the problem, let
alone the focus on the (normalized Banzhaf) power index. Only in (some of) our experiments
do we focus on the normalized Banzhaf index. In those experiments, we choose to measure
closeness in terms of the Euclidean distance in Rn between the normalized Banzhaf power
index of the game and the target power index.
To illustrate the problem, we visualize the 3 player-instance of the problem, because
this can be done nicely in two dimensions. The normalized Banzhaf index of each weighted
voting game is a vector in the 2-dimensional unit simplex.7 In our analysis in Section 4, it
will turn out to be convenient to restrict ourselves to canonical WVGs (see Definition 3).
We can do this without loss of generality, because for every WVG there exists an canonical
WVG that can be obtained by ordering the players. For three players, these games have
normalized Banzhaf power indices only in the shaded area in Figure 1, where the vertices
of the simplex are labeled with the players numbers (1, 2 and 3). corresponding power
indices are listed on the left.
The four dark dots in Figure 1 represent the normalized Banzhaf power indices corresponding to all ten existing three-player canonical WVGs. Two of these ten games are
6. #P is the complexity class that contains the counting versions of problems in NP. Problems complete
for this class are believed to be hard to solve, and a polynomial-time algorithm for one such problem
implies P = NP.

	
P
7. Recall that the n-dimensional unit simplex is defined as x  (R0 )n+1 : n+1
i=1 xi = 1 , so it contains
the (n + 1)-dimensional vectors of non-negative real numbers for which the elements sum to 1.

111

fiDe Keijzer, Klos, & Zhang

(1, 0, 0)
(, , )

1

[999: 1000, 0, 0]
[999: 1000, 500, 500]
[999: 998, 2, 2]
[999: 1000, 1000, 0]

(, , 0)

[999: 500, 500, 0]
[999: 1000, 1000, 1000]

(, , )

[999: 500, 500, 500]
[999: 333, 333, 333]

2

3

Figure 1: The games with three players and their power indices.
degenerate, namely, the two games with no winning and no losing coalitions, respectively.
Weighted form representations for the other eight games are given on the right of the figure, which also shows that different games may have the same distribution of power. The
PIWVGD problem (for n = 3) is now: given a target power index (a point) somewhere in
the shaded area of the figure (and in the corresponding part of the (n  1)-dimensional unit
simplex for general n), return a weighted representation of a game that is closest to this
target (in terms of Euclidean distance, for example).
We know of only a couple of studies that propose algorithms for the inverse problem.
Fatima, Wooldridge, and Jennings (2008) and Aziz, Paterson, and Leech (2007) present
similar algorithms for the inverse problem for a target Shapley-Shubik index (Shapley &
Shubik, 1954) and Banzhaf index (Banzhaf III, 1965), respectively. Both algorithms iteratively update a weight vector by using update rules based on the distance from the current
weight vectors power index and the target power index. Fatimal et al. use two update rules
for which they prove that by applying them, the Shapley-Shubik index of each player cannot
get further away from the target. Hence, the proposed algorithm is an anytime algorithm.
Aziz et al. do not give such an analysis. Leech (2002a, 2003) proposes an approach that
largely resembles the method of Aziz et al., with the exception that a different updating
rule is used. Neither algorithm comes with an approximation guarantee.
There are two recent interesting works on the voting game design problem. One is
by Kurz (2012b). Kurz proposes an exact method using integer linear programming, for
solving the weighted voting game design problem for both the Shapley-Shubik index and
the Banzhaf index. The set of linear games is taken as the search space, and branch-andbound techniques (along with various insights about the set of weighted voting games) are
used in order to find in this set a weighted voting game with a power index closest to the
target. Kurz does not provide a runtime analysis. The experiments performed show that the
algorithm works well for small numbers of players. Our work is independent of the work of
Kurz and differs from it in that we are interested in devising an algorithm with provable asgood-as-possible runtime guarantees. Moreover, the approach we take is different from that
of Kurz, and the theory necessary to develop our algorithm can be considered interesting
in itself.

112

fiFinding Optimal Solutions for Voting Game Design Problems

The other recent work is by De, Diakonikolas, and Servedio (2012b). This paper provides as its main result an algorithm for the inverse power index problem for the case of
the Shapley-Shubik index, and has a certain approximation guarantee: in addition to a
target power index, the algorithm takes a precision parameter  and guarantees to output
a weighted voting game of which the power index is -close to it, on the precondition that
there exists an -close weighted voting game with the property that the quota is not too
skewed, in a particular sense. This is, to our knowledge, the only polynomial time algorithm
for a power index voting game design problem that provides an approximation guarantee
in any sense.
Closely related to our work are two papers that deal with the Chow parameters problem
(ODonnell & Servedio, 2011; De, Diakonikolas, Feldman, & Servedio, 2012a). The results
in their paper are stated in terms of boolean function theory and learning theory, but when
translated to our setting, these papers can be seen to deal with approximation algorithms
for a type of value that can be considered a power index: The Chow parameters of a given
player in a given game is defined to be the total number of winning coalitions that the
player is in. The authors present in these papers, as a main result, a polynomial time
approximation scheme for computing the Chow parameters of a weighted voting game.
The problem of enumerating the set of weighted voting games with a fixed number of
players is, as we will see, closely related to the approach we take for solving the weighted
voting game design problem. This enumeration problem was studied by Kurz (2012a), who
uses integer programming techniques to enumerate all canonical weighted voting games up
to nine players. Kurz generates integer weighted representations for all of these games and
classifies the games that do not have a unique minimum-sum integer weighted representation.
Threshold functions (Hu, 1965; Muroga, 1971) are of fundamental research interest in
voting games, circuit complexity and neural networks. The problem of realizing Boolean
threshold functions by neural networks has been extensively studied (Parberry, 1994; Siu,
Roychowdhury, & Kailath, 1995; Freixas & Molinero, 2008), where upper and lower bounds
are derived on the synaptic weights for such realizations. The enumeration of threshold
functions is closely related to the enumeration of weighted voting games (see Appendix
A): Threshold functions are essentially weighted voting games where negative weights are
allowed. The enumeration of threshold functions up to six variables was done by Muroga,
Toda, and Kondo (1962). Subsequently, in the work of Winder (1965), and Muroga, Tsuboi,
and Baugh (1970), all threshold functions of respectively seven and eight variables were
enumerated. Krohn and Sudholter (1995) enumerated the canonical weighted voting games
up to eight players, as well as the class of canonical linear games. Kurz (2012a) was the first
to enumerate all nine player canonical weighted voting games, and Freixas and Molinero
(2010) were the first to enumerate all nine player canonical linear games. To the best of
our knowledge, enumerations for 10 players have not been carried out.
There exists some literature on enumeration of special subclasses of voting games as well:
see the work of Freixas, Molinero, and Roura (2012) for linear games with two desirability
classes; the work of Freixas and Kurz (2013a) for weighted voting games with one roof; and
the work of Freixas and Kurz (2013b) for linear games with certain special types of voters
and few desirability classes.

113

fiDe Keijzer, Klos, & Zhang

In the work of Krohn and Sudholters (1995), the enumeration of canonical linear games
and a subclass thereof is studied using various order theoretic concepts. It does not directly address the problem of enumerating weighted voting games, although it does discuss
a correspondence between the n-player proper weighted voting games and the (n + 1)-player
canonical decisive weighted voting games.8 Because the class of canonical linear games is
much bigger than the class of weighted voting games, their algorithms do not imply an efficient enumeration procedure for weighted voting games, as is one of our main contributions
in the present work. However, there are some connections between our work and Krohn
and Sudholter: their enumeration procedures work by exploiting graded posets, just like
ours; although their posets consist of subsets of winning coalitions together with the set
inclusion relation (for the case of decisive canonical linear games, they use a variant of this
poset), and not on the subsets of minimal winning coalitions as in our case. Although their
idea of using a graded poset corresponds with ours, it seems to us that our results cannot
be connected to theirs in any stronger sense. Moreover, the proofs of the properties that
we establish for the partially ordered set we propose here, use vastly different ideas, and
crucially exploit weightedness.
Alon and Edelman (2010) observe that we need to know a priori estimates of what power
indices are achievable in simple games to analyze the accuracy of these kinds of iterative
algorithms, i.e., there is a need for information about the distribution of power indices in
[0, 1]n . As a first step in solving this problem, they prove a specific result for the case of
the Banzhaf index for monotonic simple games.
In addition, some applied work has been done on the design of voting games. Laruelle
and Widgren (1998), and Sutter (2000) analyze and design the distribution of voting power
in the European Union using iterative methods that resemble the algorithm of Aziz et al.
(2007). Laruelle and Widgrens algorithm was systematically analyzed and improved by
De Nijs, Wilmer, and Klos (2012). Similar work was done by Leech for the EU (Leech,
2002b), and for the IMF (Leech, 2002c).
Finally, a research direction that is related to our problem is that of studying minimal
integer representations for weighted voting games: Bounds on the maximum weight in such
a representation provide us with a finite set of weighted representations to search through,
as a means of solving our design problem. We explain this in greater detail in the next
section. Some classical relevant bounds can be found in the work of Muroga (1971), Section
9.3. See the work of Freixas and Kurz (2011), Freixas and Molinero (2010) for some recent
work in this direction.

4. Solving the Power Index Voting Game Design Problem
The most natural representation for a weighted voting game is the weighted representation.
However, by the invariance to scaling of weighted representations, there exist an infinite
number of weighted representations for each individual weighted voting game, even though
for every n, the number of weighted voting games is finite: weighted voting games are simple
n
games, and there are 22 simple games for n players. This makes it hard to derive an exact
algorithm for the weighted voting game design problem that is based on working with
8. A game is called proper if the complement of any winning coalition is losing. A game is called decisive
if it is proper and the complement of any losing coalition is winning.

114

fiFinding Optimal Solutions for Voting Game Design Problems

weighted representations alone, since there is not immediately a clear finite set of weight
vectors that an algorithm can search through.9 We resort to working with alternative
representations.
We approach voting game design problems by devising an enumeration method that
generates every voting game relatively efficiently. First, we discuss a naive method that
enumerates all monotonic simple games on a given number of players in doubly exponential
time (Section 4.1). Subsequently, in Section 4.2, for the case of weighted voting games, we
improve on this runtime exponentially by showing how to enumerate all weighted voting
games on a given number of players within exponential time. Although the runtime of this
enumeration method is still exponential, we will see that the algorithm for the PIWVGD
problem that results from this enumeration method (trivially) has the anytime property:
Because we remember the best game found so far, the longer we run our algorithm, the
better the result becomes. In addition, we are guaranteed that the algorithm eventually
finds the optimal answer. The enumeration method exploits a specific (graded) partial order
that we prove to exist for the class of weighted voting games.
Because we will be dealing with algorithms that run in exponential time, we make use
of the O -notation: A function f : R  R is in O (g) for some g : R  R if and only if
there is a polynomial p : R  R such that f  O(g  p). This essentially means that we
make light of polynomial factors.
4.1 Monotonic Simple Game Design
In this section we consider briefly the power index voting game design problem for the class
of monotonic simple games.
A monotonic simple game is represented by either a set of minimal winning coalitions
or maximal losing coalitions, with each of those sets always forming an antichain under
the -relation on coalitions of players:10 No pair of coalitions in a set of minimal winning
(maximal losing) coalitions can be comparable with respect to , because then one of these
coalitions would not be minimal winning (maximal losing). An exact algorithm that solves
this problem must therefore search for the antichain that represents a game (as either a
list of MWCs or MLCs) that has a power index closest to the target power index. In
either case, a simple exact algorithm for this problem would be one that considers every
possible antichain, and computes for each antichain the power index for the game that the
antichain represents, and its distance to the target power index, to finally return a game
that minimizes this distance.
The number of antichains on a set of n elements is known as the nth Dedekind number Dn . Because the sequence of Dedekind numbers (Dn ) quickly grows very large, this
algorithm has high time complexity. Kleitman and Markowski (1975) prove the following
bounds on Dn :
0 log n
n/2 )E
n
2(1+c n )En  Dn  2(1+c2
,
(2)
9. However, the literature does provide us with bounds on the maximum weight necessary in an integer
representation of a weighted voting game, and we could utilize this in order to come up with an enumeration algorithm based on generating a finite set of integer weighted representations. We elaborate
on this idea in refwvgdesign.
10. A family of sets is an antichain with respect to some relation R, if and only if no pair of sets in the
family is comparable with respect to R.

115

fiDe Keijzer, Klos, & Zhang

where c0 and c are constants and En is the size of the largest antichain on an n-set.11
n
, a result known as Sperners theorem.
Sperner (1928) proves that En = bn/2c
From Sperners theorem and Stirlings approximation, we get
 n
2
En   
.
(3)
n
We conclude that Dn is doubly exponential in n. Therefore, any algorithm that solves the
voting game design problem for monotonic simple games in this way achieves a running
n
time in  (22  h(n)). The function h is exponential for all popular power indices, e.g., the
Shapley-Shubik index and the Banzhaf index (see Aziz, 2008; Deng & Papadimitriou, 1994;
Prasad & Kelly, 1990).
4.2 Enumerating Weighted Voting Games
As mentioned in Section 3, the literature on voting game design problems has focused on
the weighted voting game variant of the power index voting game design problem where
the power index of choice is either the Banzhaf index or the Shapley-Shubik index. Here,
we propose an exact algorithm for this problem that runs in exponential time. What will
turn out to make this algorithm interesting for practical purposes is that it is an anytime
algorithm (trivially): it is guaranteed to output an optimal solution eventually, but we can
stop execution of this algorithm at any time, and the answer output will be closer to the
optimum, the longer we run it. The advantage of this algorithm over the current local
search methods is obviously that we will not get stuck in local optima.
All of the existing local search methods that try to solve the weighted voting game
design problem use the weighted representation directly, but as mentioned before, there
exist infinitely many weighted representations for even a single weighted voting game, so
that such algorithms stall if they move to a different weighted representation that doesnt
represent a different game. As we mentioned before, using weighted representations as a
basis for an enumeration algorithm is a possibility as well, but it is not immediately clear
how to do this, as there is an infinite set of weighted representations for every weighted
voting game. Muroga (1971, Thm. 9.3.2.1) provides us with a solution to this problem, as
his theorem tells us that for every weighted voting game there exists an integer weighted
representation where none of the weights nor the quota exceeds 2n log n . This means that a
possible enumeration algorithm could work by iterating over all of the 2(n+1)n log n integer
weight vectors that have weights that fall within these bounds, and output a weight vector
in case it corresponds to a weighted voting game that has not been output before. This
yields an improvement over the enumeration algorithm outlined in Section 4.1 for the special
case of weighted voting games. But we still do not consider this a satisfactory enumeration
procedure because the runtime of this algorithm is still significantly larger than the known
upper bounds on the number of weighted voting games (see Appendix A). The enumeration
11. Korshunov (2003) devised an asymptotically equal expression:
n/2

2 n5

n4

+n 2
n2
Dn  2C(n) ec(n)2
,


n
n
with C(n) = bn/2c
and c(n) = bn/2c1
. He describes this expression as the number of monotonic
boolean functions, which is equal to the nth Dedekind number.

116

fiFinding Optimal Solutions for Voting Game Design Problems

algorithm that we propose below has a better runtime, and indeed has the property that it is
also efficient in the sense that it runs in time polynomial in the number of weighted voting
games it outputs. Our algorithm does not rely on weighted representations of weighted
voting games; instead it works by representing weighted voting games by their sets of
minimal winning coalitions.
4.2.1 A New Structural Property for the Class of Weighted Voting Games
The enumeration algorithm we propose enumerates canonical WVGs and exploits the fact
that the set of canonical weighted voting games on n players is partially ordered by a specific
relation on these games sets of minimal winning coalitions. In particular, we will show that
if we take any canonical weighted voting game on n players with at least one minimal
winning coalition (MWC), then there exists an MWC C in this games set of MWCs such
that, if we remove C, the resulting set of MWCs represents another canonical weighted
voting game on n players. In our analysis, we will also show how we can check whether the
set of MWCs of a given game can be extended with some coalition to form another games
set of MWCs. This way, we can start from the game with zero MWCs, and enumerate all
games from the bottom up, according to a specific partial order.
We proceed by developing some necessary theory behind the algorithm that we will
propose. We will focus only on the class of canonical weighted voting games since for each
noncanonical weighted voting game there is a canonical one that can be obtained by merely
permutating the players.
We use some order-theoretic notions in this section. These are given in the following
definition.
Definition 7 (Partial order, (graded) poset, cover, rank function, least element). For a set
S, a partial order  is a relation on S that is reflexive, so x  S : x  x; antisymmetric,
so x, y  S : ((x  y  y  x)  x = y); and transitive, so x, y, z  S : ((x  y  y 
z)  x  z). A partially ordered set or poset is a set S equipped with a partial order ,
i.e., a pair (S, ). A least element of a poset (S, ) is an element x  S such that x  y
for all y  S. A minimal element of (S, ) is an element x  S such that y  x implies
y = x for all y  S. We say that y  S covers x  S in (S, ) when x  y and there is
no z  S such that x  z  y. A poset (S, ) is graded when there exists a rank function
 : S  N such that: (i)  is constant on all minimal elements of (S, ), (ii) (x)  (y)
for all x, y  S for which x  y holds, and (iii) for any pair x, y  S it holds that if y covers
x in (S, ), then (y) = (x) + 1.
The algorithm we will propose is based on a new structural property that allows us
to enumerate the class of canonical weighted voting games efficiently. We define a relation
MWC and we will prove that for any number of players n, the class Gcwvg (n) forms a graded
poset with a least element under this relation.
Definition 8 (The relation MWC ). Let G, G0  Gcwvg (n) be any two canonical weighted
voting games. We define G MWC G0 to hold if and only if there exists for some k  N1
a sequence G1 , . . . , Gk of canonical weighted voting games of n players, such that G1 = G,
Gk = G0 , and for all 1  i < k it holds that Wmin,i  Wmin,i+1 and |Wmin,i | = |Wmin,i+1 |1,
where Wmin,i denotes the set of minimal winning coalitions of Gi .
117

fiDe Keijzer, Klos, & Zhang

The following theorem provides the foundation for our enumeration algorithm.
Theorem 1. For each n, (Gcwvg (n), MWC ) is a graded poset with rank function
 : Gcwvg (n) 
N
G
7 |Wmin,G |.
Moreover, this graded poset has a least element of rank 0.
Proof of Theorem 1. We need to prove (1) that the pair (Gcwvg (n), MWC ) is a poset, which
requires showing that the relation MWC is a partial order on Gcwvg (n), (2) that the partial
order is graded, which requires showing that the function  is a rank function, and (3) that
the graded poset has a least element of rank 0. As for (1), the relation MWC is reflexive,
because for every game G  Gcwvg (n), G MWC G holds: we can take k = 1 in Definition 8,
and the required sequence of games is just G. The relation is also antisymmetric, because if
G MWC G0 and G0 MWC G are true, then there exist two sequences of CWVGs G1 , . . . , Gm
(with G1 = G and Gm = G0 ), and G1 , . . . , Gn (with G1 = G0 and Gn = G), for which the
conditions in Definition 8 hold. Neither sequence can have a length > 1, because that would
mean that Wmin,G  Wmin,G0 (or vice versa), and then the inclusion wouldnt hold in the
other direction. This means we have m = n = 1, so that G = G0 . Finally, the relation is
transitive, because if G MWC G0 and G0 MWC G00 hold, there exists sequences of CWVGs
G1 , . . . , Gm (with G1 = G and Gm = G0 ) and G1 , . . . , Gn (with G1 = G0 and Gn = G00 ) for
which the conditions in Definition 8 hold. Concatenating these sequences establishes that
G MWC G00 holds.
In order to prove (2), that the poset is graded under the rank function  specified in
the theorem, we have to show that the three conditions (i) through (iii) from Definition 7
hold for the function  defined in Theorem 1. As for condition (i), the CWVG G with
Wmin,G =  is a minimal element: take an arbitrary CWVG G0 for which G0 MWC G
holds. We will show that G0 = G. Because G0 MWC G holds, there exists a sequence of
CWVGs G1 , . . . , Gm (with G1 = G0 and Gm = G) for which the conditions in Definition 8
hold. But because Wmin,G = , the sequence can not have a length > 1, so G0 = G. Because
G0 was arbitrary, this holds for all CWVGs, establishing that G is a minimal element. The
value of the rank function for this game G is 0. To show that the game G with Wmin,G = 
is the only minimal element, we will prove the following lemma constructively.
Lemma 1. For every game G  Gcwvg (n) with a nonempty set Wmin,G as its set of minimal
winning coalitions, there is a coalition C  Wmin,G and a game G0  Gcwvg (n) so that
Wmin,G \ {C} is the set of minimal winning coalitions of G0 .
This lemma implies that for every game G  Gcwvg , there exists a sequence of CWVGs
that starts at G and ends at the game with no minimal winning coalitions. Each game
has a set of MWCs that is one smaller than the previous game in the sequence. To prove
Lemma 1, we first prove two preliminary Lemmas (2 and 3).
Lemma 2. Let G = (N = {1, . . . , n}, v) be a weighted voting game, and let ` = [q; w1 , . . . , wn ]
be a weighted representation for G. For each player i there exists an  > 0 such that for all
positive 0 < , the vector `0 = [q; w1 , . . . , wi + 0 , wi+1 , . . . , wn ] is also a weighted representation for G.
118

fiFinding Optimal Solutions for Voting Game Design Problems

Informally, this lemma states that it is always possible to increase the weight of a player
by some amount without changing the game.
Proof. Recall that WG and LG are Gs sets of winning and losing coalitions, respectively.
Let wmax = maxCLG w(C) and wmin = minCWG w(C), so wmax < q is the largest weight of
a losing coalition, wmin  q is the smallest weight of a winning coalition, and wmax < wmin .
Take  = wmin  wmax and note that  > 0. Increasing any players weight by any positive
amount 0 that is less than  does not turn any losing coalition into a winning coalition.
Also, as 0 > 0, this change of weight does not turn any winning coalition into a losing
coalition, so the weighted representation `0 represents the same game G.
Lemma 3. Let G = (N = {1, . . . , n}, v) be a weighted voting game, and let w` (C) be the
weight of coalition C in the game represented by `. There exists a weighted representation
` for G such that for all C, C 0  2N , C 6= C 0 , for which v(C) = v(C 0 ) = 1, it holds that
w` (C) 6= w` (C 0 ).
Or, informally stated: for every weighted voting game, there exists a weighted representation such that all winning coalitions have a different weight.
Proof. Let `0 = [q; w1 , . . . , wn ] be some weighted representation for G. We will construct
the required weighted representation ` from `0 . First fix an arbitrary player i. By Lemma
2, there is an  > 0 such that increasing wi by any value 0  (0, ) will result in another
weighted representation for G. Let E be the set of choices for 0 such that increasing wi
by 0 yields a weighted representation ` where there are two coalitions C, C 0  2N , such
that i  C and i 6 C 0 , that have the same weight under `. There are finitely many such
pairs (C, C 0 ) so E is finite and therefore (0, ) \ E is non-empty. By picking for 0 any value
in (0, ) \ E, and increasing the weight of player i by 0 , we thus end up with a weighting
` in which there is no coalition C containing i such that w` (C) is equal to any coalition
not containing i. Furthermore, if C and C 0 are two arbitrary coalitions that have distinct
weights under `0 , then certainly they will have distinct weights under `.
By sequentially applying the above operation for all i  N , we end up with a weighting
` for which it holds for every player i that there is no coalition C containing i such that
w` (C) is equal to the weight of any coalition not containing i.
Let C, C 0  2N be arbitrary distinct coalitions. Assume without loss of generality that
C \ C 0 6=  (otherwise, we may swap the names of C and C 0 ) and let i  C \ C 0 . Because
C contains i and C 0 does not contain i, we have w` (C) 6= w` (C 0 ), and this completes the
proof.
Using Lemma 3, we can prove Lemma 1, which establishes Theorem 1.
Proof of Lemma 1. Let G = ({1, . . . , n}, v) be a canonical weighted voting game. Let
Wmin,G be its set of minimal winning coalitions and let ` = [q; w1 , . . . , wn ] be a weighted
representation for which it holds that all minimal winning coalitions have a different weight.
By Lemma 3, such a representation exists. We will construct an `00 from ` for which it holds
that it is a weighted representation of a canonical weighted voting game with Wmin,G \ {C}
as its list of minimal winning coalitions, for some C  Wmin,G .

119

fiDe Keijzer, Klos, & Zhang

Let i be the highest-numbered player that is in a coalition in Wmin,G , i.e., i is a least
desirable nondummy player. We may assume without loss of generality that wj = 0 for all
j > i (i.e., the dummy players), as after setting these weights to 0, it still holds that all
minimal winning coalitions have different weights. Let C  Wmin,G be the minimal winning
coalition containing i with the lowest weight among all MWCs in Wmin,G that contain i.
Next, define `0 as [q; w1 , . . . , wi  (w` (C)  q), . . . , wn ]. Note that under `0 the weights
of the players are still decreasing and nonnegative: Because w` (C \ {i}) < q (due to C \ {i}
being a losing coalition, because C is a MWC), w` (C \ {i}) + wi = w` (C) < q + wi , so
wi > w` (C)  q. So player is weight under `0 is indeed nonnegative. Player is weight is
decreased in `0 , but not by enough to turn even the lightest of all MWCs that contain i
into a losing coalition. So now G`0 = G` = G and w`0 (C) = q. Moreover, the weights of the
coalitions in Wmin,G that contain player i are still mutually distinct under `0 .
We now decrease player is weight further, by an amount that is so small that the
only minimal winning coalition that turns into a losing coalition is C. Note that under `0
(representing the same game G), minimal winning coalition C is still the lightest MWC
containing player i. Let C 0  Wmin,G be the second-lightest minimal winning coalition
containing i. Obtain `00 by decreasing is weight under `0 by a positive amount smaller
than w`0 (C 0 )  w`0 (C). Coalition C will become a losing coalition and all other minimal
winning coalitions will stay winning. No new minimal winning coalition is introduced in
this process: Suppose there would be such a new minimal winning coalition S in G`00 , then
S contains only players that are at least as desirable as i (the other players have weight
0). In case i 6 S, we have that S would also be a minimal winning coalition in the original
game G because w`00 (S) = w` (S)  q, which contradicts the fact that S is a new MWC in
G`00 . In case i  S, it must be that S \ {i} is winning in the original game G (because S is
winning in the original game and S is not a MWC in the original game, and i was picked
to be a least desirable nondummy player). Thus, w`00 (S \ {i}) = w` (S \ {i})  q, and this
is in contradiction with S being an MWC in G`00 .
G`00 is therefore an n-player canonical weighted voting game whose set of MWCs forms
a subset of the MWCs of G, and the cardinalities of these two sets differ by exactly 1. This
proves the claim.
Continuing the proof of Theorem 1, it follows from Lemma 1 that the game with no
minimal winning coalitions is the unique minimal element. The fact that properties (ii) and
(iii) hold for the rank function  follows immediately from the definitions of the relation
MWC and the function . Regarding claim (3), that the poset has a least element with rank
0, consider the game G with no minimal winning coalitions. For this game G, G MWC G0
holds for all games G0  Gcwvg (n), so G is a least element, and it has rank 0.
In Figure 2, (Gcwvg (4), MWC ) is depicted graphically. Note that for convenience of
display, this figure is not exactly the Hasse diagram of the poset (Gcwvg (4), MWC ). It is
clear from Figure 2 that (Gcwvg (4), MWC ) is not a tree since there is one game that covers
multiple games: it is the game labelled 0110 that has two edges coming into it, one of which
is dashed. From the set of minimal winning coalitions representing this game, there are two
minimal winning coalitions (1001 and 0110) for each of which it holds that if it is removed,
the remaining set of coalitions is the set of minimal winning coalitions representing another

120

fiFinding Optimal Solutions for Voting Game Design Problems



1000

1100

0100 0110 0111

1011

1101

0010 0011 0101 0110 0111 1001 0111

1011

1001

0001

0011

1010

1110

1111

0000

0110

0110

0111

0111

0101
0011

Figure 2: Graphical depiction of (Gcwvg (4), MWC ). Each node in this graph represents
a canonical weighted voting game of four players. The figure should be read as follows:
each node has the characteristic vector of a minimal winning coalition as a label; a binary
sequence indicating for each player whether it is (1) or is not (0) a member of the coalition.
The set of minimal winning coalitions of a game that corresponds to a certain node n in the
graph, has as its elements those coalitions that are described by the elements of the set Vn of
characteristic vectors on the path from the top node to n along the solid edges. The top node
corresponds to the canonical weighted voting game with zero minimal winning coalitions
(i.e., every coalition loses). The actual Hasse diagram of this poset can be obtained by
changing the label of each node n to Vn and including the solid edges as well as the dashed
edge in the diagram.

121

fiDe Keijzer, Klos, & Zhang

canonical weighted voting game. Since one could add any number of dummy players to this
game (i.e., players that do not occur in any winning coalition), we conclude the following.
Proposition 1. For every n  4, (Gcwvg (n), MWC ) is not a tree.
When we develop our algorithm in the next section, it will turn out that Proposition 1
makes things significantly more complicated.
4.2.2 The Algorithm
We use the results from the previous section to develop an exponential-time exact algorithm
for the power index voting game design problem. The way this algorithm works is straightforward. Just as in the naive algorithm suggested in Section 4.1, we enumerate the complete
class of games (weighted voting games in this case), and we compute for each game that is
output by the enumeration algorithm its power index and the distance of this power index
to the target power index. We keep track of the game that minimizes this distance.
The key to efficient enumeration is that by utilizing Theorem 1, it is possible to efficiently
generate the minimal winning coalition listings of the canonical weighted games of rank i
(according to the graded poset just defined) from the minimal winning coalition listings of
the canonical weighted voting games of rank i  1.
The following two theorems show us how to do this. Firstly, we need a result from the
work of Peled and Simeone (1985).
Theorem 2 (Peled & Simeone, 1985). There exists a polynomial time algorithm for testing
whether a game given as a list of minimal winning coalitions is a weighted voting game.
Moreover, if it is a weighted voting game, then its list of maximal losing coalitions and a
weighted representation can be found in polynomial time.
The algorithm proposed by Peled and Simeone (1985) that has the above-named characteristics is called the Hop-Skip-and-Jump algorithm. The algorithm was originally designed
for applications related to solving problems in the fields of threshold logic and set covering,
but it is only a matter of changing terminology in a straightforward way to see that the
Hop-Skip-and-Jump algorithm fulfills our purposes as well.
To state the next theorem, we first define the truncation operation.
Definition 9 (Right-truncation). Let S  N = {1, . . . , n} be a coalition of players. Using
P (S, i) to denote the ith highest numbered player among the players in S, the ith righttruncation of S, denoted rtrunc(S, i), is defined as


if i = 0,
S
rtrunc(S, i) = S \ {P (S, i), . . . , n} if 0 < i  |S|,


undefined
otherwise.
In canonical linear games, the ith right-truncation of a coalition S (for i  |S|) is the
coalition that remains when the i least desirable players are removed from S. For example,
the 2nd right-truncation of the coalition {1, 2, 4, 6, 8} is the coalition {1, 2, 4}.

122

fiFinding Optimal Solutions for Voting Game Design Problems

Theorem 3. For any n, let G, G0  Gclin (n) be a pair of canonical linear games with
respective sets of minimal winning coalitions Wmin,G and Wmin,G0 , such that Wmin,G 
Wmin,G0 and |Wmin,G0 | = |Wmin,G | + 1. Let Lmax,G and Lmax,G0 be the sets of maximal losing
coalitions of G and G0 , respectively. There is a C  Lmax,G and an i  N with 0  i  n
such that Wmin,G0 = Wmin,G  {rtrunc(C, i)}.
Proof. Let C 0 be the coalition such that Wmin,G0 = Wmin,G  {C 0 }. Coalition C 0 cannot
be a superset of a coalition in Wmin,G because then it would not be a minimal winning
coalition in G0 . Therefore, C 0 is a losing coalition in G, so it must be a subset of a coalition
in Lmax,G . Suppose for contradiction that C 0 is not a right-truncation of a maximal losing
coalition C  Lmax,G . So there is a C  Lmax,G such that C 0 is a subset of C, but not a
right-truncation of C. This means that in C 0 , some player j from C is not present, while at
least one less desirable player k > j from C is in C 0 . This implies that there is a left-shift C 00
of C 0 such that C 00 is a subset of a coalition in Lmax,G : C 00 is obtained from C 0 by replacing
j by k. Because C 00 is a subset of C, C 00 is still a losing coalition in G. So C 00 is not a
superset of any coalition in Wmin,G , and hence C 00 is also not a superset of any coalition in
Wmin,G0 . So C 00 is a losing coalition in G0 . But G0 is a canonical linear game, so by the
desirability relation 1      n, C 00 is a winning coalition in G0 because it is a left-shift of
the winning coalition C 0 . This is a contradiction.
From Theorems 2 and 3, it now becomes apparent how to use (Gcwvg (n), MWC ) for
enumerating the class of n-player canonical weighted voting games. We start by outputting
the n-player weighted voting game with zero minimal winning coalitions. After that, we
repeat the following process until we find no further games: We use Theorem 3 to generate
the minimal winning coalition lists of all canonical weighted voting games with i minimal
winning coalitions, using the set of canonical weighted voting games games with i1 minimal
winning coalitions (also represented as a list of minimal winning coalitions), starting at i = 1.
Once generated, we have the choice to output the games either as a list of minimal winning
coalitions or in a weighted representation, by using the Hop-Skip-and-Jump algorithm.
Generating the set of games of i minimal winning coalitions works as follows. For each
game G with i1 minimal winning coalitions, we obtain the set of maximal losing coalitions
by using the Hop-Skip-and-Jump algorithm. Next, we check for each right-truncation of
each maximal losing coalition C  Lmax,G whether, when we add it to Gs set of minimal
winning coalitions, the resulting set represents another weighted voting game. Again, testing whether a game is a weighted voting game is done by using the Hop-Skip-and-Jump
algorithm. If a game turns out to be weighted, we store and output it.
There is one remaining problem with this approach: it outputs duplicate games. If
(Gcwvg (n), MWC ) were a tree, then this would not be the case. However, by Proposition
1, (Gcwvg (n), MWC ) is not a tree for any n  4. Thus, we have to do a duplicates check
for each weighted voting game we find. In principle, this seems not to be so difficult. For
each game we find, we can sort its list of minimal winning coalitions, and check if this
list of coalitions already occurs in the array of listings of minimal winning coalitions that
correspond to games that we already found. The problem with this solution is that the list
can grow very large, making these checks very time- and space-consuming operations.
We will therefore use a different method for doing this duplicates check: Suppose we
have found an n-player canonical weighted voting game G of i minimal winning coalitions by
123

fiDe Keijzer, Klos, & Zhang

adding a coalition C to a minimal winning coalition listing of a canonical weighted voting
game that we have already found. We first sort Gs list of minimal winning coalitions
according to some fixed total order. Then we check for each coalition C 0 that occurs before
C in this sorted list, whether C 0 s removal from the list results in a list of minimal winning
coalitions of a canonical weighted voting game. If there is such a C 0 , then we discard G, and
otherwise, we keep it. This way, it is certain that each canonical weighted voting game will
be generated only once. In terms of orderly generation of combinatorial objects (McKay,
1998), our method thus provides a canonical construction path for CWVGs.
Algorithm 1 gives the pseudocode for this enumeration method. The array element
games[i] will be the list of canonical weighted voting games that have i minimal winning
n
coalitions; the rank-i games. The value of i can not exceed bn/2c
by Sperners Theorem.
The games are represented as lists of minimal winning coalitions. The algorithm iterates
from every new game found, starting with the game in games[0], with zero minimal winning
coalitions.
Algorithm 1 An enumeration algorithm for the class of n player canonical weighted voting
games. hopskipjump refers to the Hop-Skip-and-Jump algorithm.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

Output [1; 0, . . . , 0]
. Output the game with no MWCs
games[0]  {} 
. Add this game to the list of rank-0 games
n
for i = 1 to bn/2c
do
for all G  games[i  1] do
. Evaluate each game G of rank i  1
Lmax  hopskipjump(Wmin,G )
. Obtain game Gs set of MLCs
for all C  Lmax do
. Evaluate each MLC C
for j = 0 to n do
. Evaluate each of Cs right-truncations
G0  Wmin,G  rtrunc(C, j)
. Call the game to be evaluated G0
if isweighted(G0 ) then
. This requires solving an LP
if G0 passes the duplicates check then
Output a weighted representation of G0 .
Append G0 to games[i].
end if
end if
end for
end for
end for
end for

Correctness of the algorithm follows from our discussion above. Next, we analyze the
time-complexity of the algorithm.
2 +2n

Theorem 4. Algorithm 1 runs in O (2n

) time.

Proof. Lines 5 to 16 are executed at most once for every canonical weighted voting game.
From Sperners Theorem, we know that any list of minimal winning coalitions has fewer than
n
elements. So by the runtime of the Hop-Skip-and-Jump algorithm, line 5 runs in time
bn/2c

2


n
n
O n bn/2c
+ n3 bn/2c
= O(n2 n22n ). Within an iteration of the outer loop (line 4),
124

fiFinding Optimal Solutions for Voting Game Design Problems



n
= O( n2n ) times (because Lmax is also an anlines 9 to 14 are executed at most n bn/2c
tichain, Sperners Theorem also applies for maximal losing coalitions). The time-complexity
of one execution of lines 9 to 14 is as follows.


n
 At line 9, we must solve a linear program, taking time O n4.5 bn/2c
= O(n4 2n )
using Karmarkars interior point algorithm (Karmarkar, 1984).
 At line
 10, we must execute the duplicates check. This consists of checking for at most
n
bn/2c sets of minimal winning coalitions whether they are weighted. This involves
running the Hop-Skip-and-Jump algorithm, followed by solving a linear program. In

total, this takes O(n3 n22n ) time.
 Lines 11 and 12 take linear time.
Bringing everything together, we see that a single pass from lines 5 to 16 costs us O(n4 23n )
time. As mentioned earlier, these lines are executed at most |Gcwvg (n)| times. From Corol2
lary 1 in Appendix A we know that |Gwvg (n)|  O(2n n ), and of course |Gcwvg (n)| <
2
|Gwvg (n)|, hence lines 5 to 16 are executed at most O(2n n ) times, and therefore the run2
2
time of the algorithm is O(2n +2n n4 ) = O (2n +2n ).
Although the runtime analysis of Algorithm 1 is not very precise, we want to emphasize
that this method runs in exponential time instead of doubly exponential time. We can also
show that the runtime of this algorithm is only polynomially greater than the amount of
data output. This implies that Algorithm 1 is essentially the fastest possible enumeration
algorithm for canonical weighted voting games, up to a polynomial factor.
Theorem 5. Algorithm 1 runs in output-polynomial time, i.e., a polynomial in the number
of bits that Algorithm 1 outputs.
Proof. Lines 5 to 16 are executed less than |Gcwvg (n)| times. From (5), we have as a lower
n2 (1

10

)

log n /(n!2n )). One execution of lines 5 to 16 costs O(n4 23n )
bound that |Gcwvg (n)|  (2
time, and thus one iteration runs in
 2

10
n (1 log
)
n /(n!2n )
O(n4 23n )  O 2
 O(|Gcwvg (n)|)

time. We conclude that the algorithm runs in O(|Gcwvg (n)|2 ) time.
Remark 1. We cannot give a very sharp bound on the space complexity of Algorithm 1 because we do not know much about the maximum cardinality of an antichain in (Gcwvg (n), MWC
). (Nonetheless, we did obtain the maximum sizes of the antichains for n  8; see Figure 3
in the next section.) However, it can be seen that it is also possible to generate the games
in this poset in a depth-first manner, instead of in a breadth-first manner like we do now.
In that case, the amount of space that needs to be used is bounded by the maximum length
2n
of a chain in (Gcwvg (n), MWC ). This is a total amount of O( 
) space.
n
We will now briefly illustrate how the algorithm enumerates (four-player) CWVGs by
referring to Figure 2, and afterwards indicate the application to solving the power index
125

fiDe Keijzer, Klos, & Zhang

weighted voting game design problem. The algorithm starts in line 1 by outputting a
weighted representation of the game at the root node. This is the game that has no winning
coalitions. (By our definition of simple game, it exists.) This root games list of MWCs
is , and it is added to the array element games[0] in line 2. Now we loop through
 all
n
possible values i the rank function  defined in Theorem 1 can take: 1  i  bn/2c (see
Section 4.1). For four players, these values for 1  i  6 correspond to the six horizontal
rank-levels below the root node in the graph in Figure 2. For every game G in the set of
games one level higher in the graph (line 4), we consider the set of maximal losing coalitions
Lmax,G (line 6), and for each of those (line 7), we evaluate whether the game obtained by
adding each of its (at most) n right truncations (line 8) to Wmin,G yields a weighted voting
game (line 9). In the four player case, when i = 1, we look at the single root game. Since
this game has only losing coalitions, the grand coalition is the only element of this games set
of MLCs. Now we evaluate whether, for each of this grand coalitions five right truncations
(0  j  4), adding it to  yields a CWVG. It turns out that this is true in all five cases,
yielding the five CWVGs G with (G) = 1 that we see at rank-level 1 (the second level
from the top of the graph). These games are added to games[1]. Now we increase i by one,
and iteratively consider these five games in turn.
To apply this enumeration algorithm to the power index weighted voting game design
problem, we just compute the Banzhaf index for every generated game, and compute its
distance to the target power index (according to a chosen distance function). We store and
update the weighted representation for the game that minimizes this distance function, and
output that representation after all games have been generated.
4.3 Improvements and Optimizations
Algorithm 1 is in its current state not quite suitable for solving the weighted voting game
design problem in practice. In this section, we will describe several improvements to the
algorithm. These improvements will result in a version of the enumeration algorithm which
we expect to output canonical weighted voting games at a steady rate, and give us a
practically applicable anytime-algorithm for voting game design problems (defined on small
numbers of players).
The Hop-Skip-and-Jump algorithm of Peled and Simeone (1985) works by first generating the list of maximal losing coalitions of a game, given the list of minimal winning
coalitions of a game, and subsequently solving a linear program to check whether the game
is a weighted voting game. Peled and Simeone give a first improvement by showing several ways to improve and compactify the linear program in question. This smaller linear
program requires only the lists of roof coalitions and ceiling coalitions of the game.
In the next section, we present a strengthened version of Theorem 3. This strengthened
version implies that it is only necessary to know a games ceiling coalitions (instead of all
maximal losing coalitions) to generate the weighted voting games that cover it. In Section
4.3.2 we give an output-polynomial time algorithm for enumerating all ceiling coalitions,
given a set of roof coalitions.
By using these improvements, combined with the more compact linear program, we
eliminate the need to compute the complete set of maximal losing coalitions of the weighted
voting games that we enumerate. Instead, it suffices to keep track only of the sets of minimal

126

fiFinding Optimal Solutions for Voting Game Design Problems

winning coalitions and ceiling coalitions of the weighted voting games enumerated, and this
will speed up our algorithm significantly.12
4.3.1 A Better Way of Finding New Minimal Winning Coalitions
Theorem 3 allows us to find potential minimal winning coalitions with which we can extend
canonical weighted voting games to generate new ones. We will now see that we do not
really need to consider every right-truncation of every maximal losing coalition. In fact, we
only need to look at ceiling coalitions.
Theorem 6. For any n, let G, G0  Gclin (n) be a pair of canonical linear games such that
Wmin,G  Wmin,G0 and |Wmin,G0 | = |Wmin,G | + 1. Let Wmin,G and Wmin,G0 be the sets of
minimal winning coalitions of G and G0 , respectively, and let Lceil,G and Lceil,G0 be the sets
of ceiling coalitions of G and G0 , respectively. There is a C  Lceil,G and an i  N with
0  i  n such that Wmin,G0 = Wmin,G  rtrunc(C, i).
Proof. Let C 0 be the coalition such that Wmin,G0 = Wmin,G  {C 0 }. By Theorem 3, C 0 is
a right-truncation of a coalition S  Lmax,G . Suppose for contradiction that C 0 is not a
right-truncation of a ceiling in Lceil,G . Then there is a ceiling C 00  Lceil,G such that C 0 is a
subset of a strict right-shift of C 00 . (This is because S is by definition a subset of a right-shift
of some ceiling, and C 0 is a subset of S.) This in turn means that there is a strict left-shift
T of C 0 such that T is a subset of a right-shift of C 00 . Coalition T is not a superset of any
MWC in Wmin,G because T is losing in G, and T is not a superset of C 0 either, because
T is a strict left-shift of C 0 . So it follows that T is a losing coalition in G0 . But G0 is a
canonical linear game, so the desirability relation 1      n is satisfied. Because T is a
left-shift of C 0 , and C 0 is winning in G0 , it follows that T is a winning coalition in G0 . This
is a contradiction.
4.3.2 An Algorithm for Obtaining the Ceiling-List of a Canonical
Weighted Voting Game
Using the Hop-Skip-and-Jump algorithm, we can output in polynomial time the list of
maximal losing coalitions from the list of minimal winning coalitions, when given a weighted
voting game. Given this, an interesting question at this point is whether we can also output
in polynomial time the list of ceiling coalitions from the list of roof coalitions.
In Appendix B, we prove that this is impossible in general because the output may
be exponentially sized in the input. Nevertheless, it certainly still makes sense to try to
come up with as efficient as possible an algorithm for this problem, considering that such
an algorithm can be used in combination with the improvements of the previous section
for finding weight vectors for weighted voting games. Using such an algorithm is certainly
preferred to using the Hop-Skip-and-Jump algorithm, because in a canonical linear game
there are always fewer roof coalitions than minimal winning coalitions, and fewer ceiling
coalitions than maximal losing coalitions.
However, a recent construction by Polymeris and Riquelme (2013) shows that an outputpolynomial time algorithm for this problem would have sensational consequences, as it would
12. Related is Proposition 2.3 of Krohn and Sudholter (1995), which gives insight in the maximum number
of ceiling coalitions that a canonical linear game can have.

127

fiDe Keijzer, Klos, & Zhang

imply a polynomial time algorithm for the monotone boolean duality problem (Fredman &
Khachiyan, 1996): a well-known problem that can be solved in sub-exponential time, but
of which it is not known whether it admits a polynomial time algorithm.
Finding an output-polynomial algorithm for generating the ceiling set of a game from
its roof set, is thus a very interesting open problem, but due to its alleged difficulty13 we
instead resort to studying the problem of outputting the ceiling set of a game from its set
of minimal winning coalitions.
Of course, one could simply solve the latter problem by using the Hop-Skip-and-Jump
algorithm. This would provide us with a list of MLCs of the input game, after which we
could filter out the ceilings. This algorithm would run in O(nm2 + n3 m) time, where m
is the number of MWCs. Below, we will provide an alternative simpler algorithm for the
special case where we only need to output the ceilings of a given game. In the remainder
of this section, we will use the following definitions.
Definition 10. Let N be the set of players {1, . . . , n} and let S  N be a coalition. The
functions a and b and trunc are defined as follows.
 b(S) is the highest-numbered player in S.
 a(S) is the highest-numbered player j in N \ S such that j + 1  S (if such a j does
not exist, then we define a(S) = 0).
 The truncation of S, defined by trunc(S) = S \ {a(S) + 1, . . . , n}.
For example, if N = {1, 2, 3, 4, 5} and S = {1, 2, 4}, then the highest numbered player in
S is b(S) = 4, N \ S = {3, 5}, so the highest numbered player j  N \ S for which j + 1  S
is a(S) = 3, and trunc(S) = S \ {4, 5} = {1, 2}. If S = {1, 2, 3} then N \ S = {4, 5}, there
does not exists a j  N \ S for which j + 1  S, so a(S) = 0.
Theorem 7. Let G  Gclin (n) be a canonical linear game on players N = {1, . . . , n}, let
Wmin,G be Gs set of MWCs, let C be the set of ceilings of G, and let C  C such that
a(C) > 0. Then there exists an i  N0 , 1 < i  |C|  |trunc(C)|, such that trunc(C) 
{a(C)}  {a(C) + j : 2  j  i} is a minimal winning coalition.
Proof. The coalition C is a ceiling, so it is losing, which implies that trunc(C) is also losing,
and trunc(C)  {b(C)}  {b(C) + j : 2  j  |C|  |trunc(C)|} is a left-shift of C, and hence
winning. Therefore there exists an i  N0 , 2  i  |C|  |trunc(C)| such that
(C 0 := trunc(C)  {a(C)}  {a(C) + j : 0  j  i} is winning) AND
(C 00 := trunc(C)  {a(C)}  {a(C) + j : 2  j  i  1} is losing) OR (C 00 = C 0 ).
By canonicity, this means that C 0 is a minimal winning coalition.
From the above theorem it becomes clear how to efficiently generate the set of ceilings
from the set of minimal winning coalitions: For each MWC S, it suffices to check for all
k  N0 , k  n  b(S) whether:
13. We thank Andreas Polymeris and Fabian Riquelme for pointing out to us the connection to the monotone
boolean duality problem, as well as for pointing out an error in a preliminary version of the present paper.

128

fiFinding Optimal Solutions for Voting Game Design Problems

 (S \ {a(S)  1})  {a(S)}  {b(S) + j : 1  j  k} is a ceiling (in case a(S)  1  S),
 (S \ {b(S)})  {b(S) + j : 1  j  k} is a ceiling.
This would generate all ceilings C with the property that b(C) > 0. There are furthermore
at most n ceilings C for which it holds that b(C) = 0, and it is clear that such coalitions
can be generated and checked straightforwardly.
The runtime of this implied algorithm is theoretically no better than the Hop-Skip-andJump algorithm. However, due to the simplicity of this algorithm, and due to the fact that
this algorithm only finds the ceilings (of which there are in general far less than there are
MWCs), we expect this algorithm to run much faster in practice, in most cases.

5. Experiments
We have implemented Algorithm 1 together with all of the optimization strategies described
in Section 4.3, that allow us to work with just the ceiling coalitions, rather than all maximal losing coaltions. The programming language we used is C. Execution of the algorithm
involves solving a large number of linear programs. To to this, we made use of the GNU Linear Programming Toolkit (see Makhorin, 2004). Our implementation solves the normalized
Banzhaf index weighted voting game design problem. This means that for each weighted
voting game that is output by our enumeration algorithm, we must invoke a procedure for
computing the normalized Banzhaf index. The algorithm used for this is simply the naive
brute-force approach. (In our experiments, the runtime with Banzhaf computation (to three
decimal places) is only different from the runtime without it for four players or more. Up
to eight players, including computation of the Banzhaf indices never increases the runtime
by more than 0.6%.)
The purpose of the experiments is to gain insight on the average optimal attainable
error in random instances (for small n), as well as on the number of weighted voting games
as a function of the number of players and the number of minimal winning coalitions.
Experiment 1. We used the enumeration algorithm to compute for all n with 1  n  8
n
and all m with 0  m  bn/2c
, the exact number of canonical weighted voting games of
n players with m minimal winning coalitions. The results are displayed in Figure 3. Note
that on the vertical axis we have a log-scale. We see that for each of these choices of n, most
of the canonical weighted voting games have a relatively small number of minimal winning

n
coalitions relative to the maximum possible number of winning coalitions, which is bn/2c
.
Experiment 2. For n between 1 and 7, we computed for 1000 random instances the
average optimal error. That is, the average error that is attained out of 1000 random
instances (i.e., uniform random vectors in the (n  1)-dimensional unit-simplex restricted to
the non-increasingness constraint) when the algorithm is allowed to run to completion on
these instances. We also report the worst error that is attained among these 1000 instances.
The error function we used is the square root of the sum of squared errors. The reason for
using this specific error measure is because it has a nice geometric interpretation: it is
the Euclidean distance between the target (input) vector and the closest point in the unit
simplex that is a normalized Banzhaf index of a weighted voting game.

129

fiDe Keijzer, Klos, & Zhang

From Figure 4, we see that the errors decrease as n gets larger. This confirms the
observation by Alon and Edelman (2010) that when the number of voters is small, it is
clear that one can not closely approximate every power vector. The main result in that
paper states that if the target vector has a lot of its weight concentrated in a strict subset
of the players, there exists a game with a Banzhaf vector close to this target. Here we study
random vectors, but we still see the general phenomenon that as the number of players
increases, there exist more games and so the probability of one of those being close to the
target increases. We also see that the worst case optimal error can be much worse than the
average case. This is expected, as Alon and Edelman show that for all n, there exist vectors
in the standard n-simplex that can not be approximated well. Our results are computed
over only 1000 random instances. Therefore, these worst case optimal errors serve only as
a lower bound for the worst case optimal error over all possible instances.
It should be noted that in preliminary work (De Keijzer, Klos, & Zhang, 2012), and
even earlier preliminary work (De Keijzer, 2009a), additional experiments are reported:
Firstly, we used our implementation (respectively an early version of our implementation)
to find the number of weighted voting games on n players, for 1  n  8. However, in
independent work (Kurz, 2012b) that is by now already published, the authors compute the
same numbers for 1  n  9 (using incomparable techniques). Secondly, some experiments
were performed in order to study the time performance of the algorithm and the errorconvergence of the algorithm for larger numbers of players. We omit these experiments
here (see De Keijzer et al., 2012, for more results) and give a short summary instead: the
outcome of these experiments is not surprising and in line with the theoretical runtime
analysis of the previous section. The runtime graph indeed looks like a quadratic function,
when shown on a log-scale. As mentioned, computing the Banzhaf index constitutes only
a fraction of the total runtime. The error-convergence of the algorithm is slow already for
15 players, and therefore the algorithm is only practical when the number of players is not
too big, as expected.14

6. Conclusions and Future Work
In this paper, we developed an exact algorithm for solving power index weighted voting
game design problems. First, we derived a doubly exponential algorithm for the large class
of monotonic simple games. Subsequently, we showed that it is possible to obtain a (singly)
exponential algorithm for the important special case of weighted voting games.
At the core of these algorithms are methods for enumerating weighted voting games
based on a new partial order on the class of weighted voting games with some specific
interesting properties. The enumeration algorithm resulting from this, is to the best of our
knowledge the first enumeration algorithm for weighted voting games that provably runs in
output-polynomial time.
The algorithm works with families of minimal winning coalitions. We demonstrated how
it is possible to improve the runtime of this algorithm by showing that it suffices to work
only with a subset of these minimal winning coalitions: i.e., the roof coalitions. Using this
idea, we provided various techniques to improve our algorithm. Among these improvements
14. For a more detailed discussion of the results of these experiments, see the work of De Keijzer et al.
(2012).

130

fiFinding Optimal Solutions for Voting Game Design Problems

# of 1 player CWVGs with m MWCs
# of 2 player CWVGs with m MWCs
# of 3 player CWVGs with m MWCs
# of 4 player CWVGs with m MWCs
# of 5 player CWVGs with m MWCs
# of 6 player CWVGs with m MWCs
# of 7 player CWVGs with m MWCs
# of 8 player CWVGs with m MWCs

Number of canonical WVGs with m MWCs

100000

10000

1000

100

10

1
0

10

20

30

40

50

60

70

80

Number of minimal winning coalitions m

Figure 3: The number of canonical weighted voting games (y-axis) of n players, for 1  n 
8, with m minimal winning coalitions (x-axis).

0.35

Maximal Euclidean error out of 1000 random instances
Average Euclidean error out of 1000 random instances

0.3

Error (Euclidean distance)

0.25

0.2

0.15

0.1

0.05

0
1

2

3

4

5

6

7

Number of players

Figure 4: Optimal Euclidean error of 1000 random n player instances, for 1  n  7. The
error bars indicate one standard deviation.

131

fiDe Keijzer, Klos, & Zhang

is an output-polynomial time algorithm for outputting the list of ceiling coalitions of a linear
game, given the list of roof coalitions.
In addition, we implemented the aforementioned enumeration algorithm for weighted
voting games to measure its performance, obtained some interesting data about the class
of weighted voting games, and validated some theoretical results related to weighted voting
games.
This algorithm is based on an enumeration procedure for the class of weighted voting
games: it works by simply enumerating every game, and verifying for each game whether
it lies closer to the target power index than the games that we encountered up until that
point. For this reason, the algorithm has the anytime-property: as we run this algorithm
for a longer period of time, the algorithm enumerates more games and the quality of the
solution will improve, and the algorithm is guaranteed to output the optimal solution in
the long run.
The choice of the normalized Banzhaf index in our implementation in the previous
section is arbitrary: the algorithm works for any choice of power index. Moreover, due
to the genericity of enumeration, we can use our algorithm to solve not only power index
voting game design problems, but also any other voting game design problem. The only
thing we have to adapt is the error-function of the algorithm (i.e., the part of the algorithm
that checks the property in question for each of the games that the enumeration procedure
outputs); the enumeration procedure does not need to be changed.
As for the future work, note that in most real-life examples, the number of players in
a weighted voting game is rather small: usually 10 to 50 players are involved. Thus, one
of our goals is to get the proposed algorithm to yield good results within a reasonable
amount of time when the number of players is somewhere in this range. It would already
be interesting to be able to solve the problem for ten players, as we are not aware of any
enumerations of ten player canonical weighted voting games. However, we concluded that
the current implementation of the algorithm is not yet fast enough to be able to handle
ten players. Optimistic extrapolation tells us that this would take tens of years; pessimistic
extrapolation gives us thousands of years. However, the current implementation is not really
efficient, and we have hope that with some future insights, together with careful computer
programming, enumerating weighted voting games for ten players or more will be within
scope. One way to improve the current algorithm is to study in more depth the partial
order we introduced in this paper. One possible prospect is the following. With regard
to weighted voting game design problems, we suspect that it is possible to prune a lot of
areas in this partial order: Careful analysis of the partial order and its properties might
lead to results that allow us to construct an enumeration algorithm that a priori discards
certain (hopefully large) subsets of weighted voting games.
We are moreover interested to see how an algorithm performs that searches through
the partial order in a greedy manner, or what will happen if we use some other (possibly
heuristic) more intelligent methods to search through the partial order. First steps in this
direction are taken by Kleijn (2012). We wonder if it is possible to use such a search method
while still having an optimality guarantee or approximation guarantee on the quality of the
solution. In addition, we can also consider the ideas presented here as a postprocessing step
to existing algorithms. In other words, it might be a good idea to first run the algorithm
of Fatima et al. (2008) or Aziz et al. (2007) in order to obtain a good initial game. Subse132

fiFinding Optimal Solutions for Voting Game Design Problems

quently, we can try to search through the neighborhood of the game to find improvements,
according to the partial order introduced in this paper.
Lastly, some related questions for which it would be interesting to obtain an answer are
about the computational complexity of the power index voting game design problem, and
also about the polynomial-time-approximability of the problem. It is quite straightforward
to see that the decision version of this problem is in most cases in NP#P (and therefore
in PSPACE), as one could nondeterministically guess a weight vector, and subsequently
use a #P-oracle to obtain the particular power index of interest.15 On the other hand,
at the moment we do not have any ideas on how to prove hardness for this problem for
any complexity class whatsoever. It seems a challenge to come up with a polynomial-time
reduction from any known computational problem that is hard for any nontrivial complexity
class.

Acknowledgments
We thank Fabian Riquelme and Andreas Polymeris for pointing out a problem in a preliminary version of this paper (see Section 4.3.2). We thank the anonymous referees for
very extensive feedback, comments and suggestions. This is an extended version of a paper
published in the proceedings of the 2010 International Conference on Autonomous Agents
and Multi-Agent Systems (De Keijzer, Klos, & Zhang, 2010). This research is partially supported by the EU FET project MULTIPLEX 317532, the ERC StG Project PAAI 259515,
and the Google Research Award for Economics and Market Algorithms. The majority of
this research has been carried out while the first author was a masters student at Delft
University of Technology. A part of this research has been carried out while the first author
was a Ph.D. student at Centrum Wiskunde & Informatica (CWI), Amsterdam.

Appendix A. The Number of Weighted Voting Games
To our knowledge, the existing game theory literature does not provide us with any insights
in the number of weighted voting games with n players, beyond n = 5. Fortunately there
is a closely related field of research, called threshold logic (see for example Muroga, 1971),
that has some relevant results.
Definition 11 (Boolean threshold function, realization, LT). Let f be a boolean function
on n boolean variables. The function f is a (boolean) threshold function when there exists
a weight vector of real numbers r = (r0 , r1 , . . . rn )  Rn+1 such that r1 x1 +    + rn xn  r0
if and only if f (x1 , . . . , xn ) = 1. We say that r realizes f . We denote the set of threshold
functions of n variables {x1 , . . . , xn } by LT(n).16
Threshold functions resemble weighted voting games, except we talk about boolean variables instead of players now. Also, an important difference between threshold functions and
weighted voting games is that r0 , r1 , . . . , rn are allowed to be negative for threshold functions, whereas q, w1 , . . . , wn , must be non-negative in weighted voting games.
15. All power indices that have been proposed and that we have encountered are known to be in #P.
16. LT stands for Linear Threshold function.

133

fiDe Keijzer, Klos, & Zhang

(Zunic, 2004) presents an upper bound on the number of threshold functions of n vari2
ables: |LT(n)|  2n n+1 . Also, the following asymptotic lower bound is known (Zuev,
1989): For large enough n, we have
|LT(n)|  2

10
n2 (1 log
)
n

.

(4)

From these bounds, we can deduce some easy upper and lower bounds for |Gwvg |. First
we observe the following property of the set of threshold functions on n variables. Let
LT+ (n) be the set of non-negative threshold functions of variables (x1 , . . . , xn ), containing
those threshold functions f  LT(n) for which there exists a non-negative weight vector r
2
that realizes f . Then, it is clear that |Gwvg (n)| = |LT+ (n)|  2n n+1 for all n.
We will proceed by obtaining a lower bound on the number of weighted voting games.
Corollary 1. For large enough n, it holds that
|Gwvg (n)|  2

10
n2 (1 log
)n1
n

Proof. Let f be a non-negative threshold function and let r be a non-negative weight vector
that realizes f . There are 2n+1 possible ways to negate the elements of r, so there are at
most 2n+1  1 threshold functions f 0  LT(n) \ LT+ (n) such that f 0 has a realization that is
obtained by negating some of the elements of r. From this, it follows that |LT+ (n)|  |LT(n)|
,
2n+1
and thus also |Gwvg (n)|  | LT(n)
|. Now by using (4) we get |Gwvg (n)| 
2n+1
2

10
n2 (1 log
)n1
n

n2 (1 10 )
log n
2n+1

2

=

.

Our next question is: what about the canonical case, Gcwvg (n)? The set Gcwvg (n) is a
subset of Gwvg (n), and for each noncanonical weighted voting game there exists a permutation of the players that makes it a canonical one. Since there are n! possible permutations,
|G (n)|
it must be that |Gcwvg (n)|  wvgn! , and thus we obtain that
|Gcwvg (n)| 

2

10
n2 (1 log
)n1
n

n!

(5)

for large enough n.

Appendix B. Generating Roofs from Ceilings
In this section, we answer the question whether it is possible to generate in polynomial time
the list of ceiling coalitions of a linear game from its list of roof coalitions: This turns out
to not be the case. We give a family of examples of canonical linear games in which the
number of roof coalitions is exponential in n, while the number of ceiling coalitions is only
polynomial in n. As a consequence, any algorithm that generates the list of roofs from the
list of ceilings will run in exponential time in the worst case. By symmetry it also follows
that generating the list of roof coalitions from the list of ceiling coalitions is not possible in
polynomial time.
Let us first define the following specific type of coalition.

134

fiFinding Optimal Solutions for Voting Game Design Problems

Definition 12 ((k, i)-encoding coalition). Let N = {1, . . . , n} be a set of players such that
n = 4i for some i  N. For any k satisfying 0  k < 2i  1, the (k, i)-encoding coalition
Sk,i  N is then defined as
{4(j  1) + 2, 4(j  1) + 3 : the jth bit in the binary representation of k is 0} 
{4(j  1) + 1, 4(j  1) + 4 : the jth bit in the binary representation of k is 1}
For example, S2,2 = {1, 4, 6, 7}, and S5,3 = {1, 4, 6, 7, 9, 12}. We can then define canonical linear games in which the roof coalitions are (k, i)-encoding coalitions.
Definition 13 (i-bit roof game). Let N = {1, . . . , n} be a set of players such that n = 4i
for some i  N. The i-bit roof game with N , denoted Gibit , is the canonical linear game
such that the set of roof coalitions of G is {S0,i , . . . , S2i 1,i }.
For example, the 2-bit roof game, G2bit , consists of the roofs {{2, 3, 6, 7}, {2, 3, 5, 8},
{1, 4, 6, 7}, {1, 4, 5, 8}}. Gibit is well-defined for all i because the binary representations
of two arbitrary i-bit numbers k and k 0 differ in at least one bit. Therefore, Si,k is not a
superset of a left-shift of Si,k0 and hence the set of roofs that we have defined for Gibit
is indeed a valid set of roofs (i.e., there are no two roofs such that one is a left-shift of
another).
n
The game Gibit has 2i = 2 4 roofs, i.e., an exponential number in n. We will show that
the number of ceilings in Gibit is only polynomially bounded. First let us use the following
definitions for convenience.
Definition 14 (Accepting roof set). Let G  Gclin (n) be a canonical linear game with players
N = {1, . . . , n}. Let C  N be a coalition, let x be a natural number such that 1  x  |C|,
and let D(C, x) be the x-th most desirable player in C. The accepting set of roofs of the x-th
most desirable player in C, denoted A(C, x), is the set consisting of those roof coalitions R
for which either the x-th most desirable player in R is greater than or equal to D(c, x), or
|R| < x.
It is important to now observe that the following fact holds.
Proposition 2. In a canonical linear game, a coalition C is winning if and only if
T|C|
a=1 A(C, a) 6= .
Proof. This lemma is in fact an equivalent statement of the fact that C is winning in
a canonical linear game if and only if it is a superset of a left-shift of a roof. If R 
T|C|
a=1 A(C, a) then it means that replacing the a-th most desirable player in R by the a-th
most desirable player in C for all a,1  a  R would result in a left-shift of R that is a
subset of C, so C must be winning.
Conversely, suppose C is winning. Then there must be a roof R that is a right-shift of
a subset of C. By removing from C the players with a higher number than D(C, |R|), we
obtain a subset C 0 of C with |R| players. By replacing the a-th most desirable player of C
by the a-th most desirable player of R for 1  a  R, we obtain a right-shift of C that is
R. Because in this last step we replaced each player in C 0 by a higher-numbered player, we
T|R|
T|C|
get that R  a=1 A(C, a). R is also in a=|R|+1 A(C, a) by definition.
135

fiDe Keijzer, Klos, & Zhang

Using the notion of an accepting roof set, we can prove the following technical lemma.
The reader should recall the definition of a direct left-shift (Definition 4).
Lemma 4. Let C be a ceiling of Gibit with two or more distinct coalitions that are direct
left-shifts of C, and let p be an arbitrary player that we can apply the direct left-shift operation on, i.e., let p be a player such that C1 = C  {p  1} \ {p} is a direct left-shift of C.
Also, let a be the number such that p = D(C, a). Then p = 2a.
Proof. Observe that for all b it holds that every roof R of Gibit has either D(R, b) = 2b  1
or D(R, b) = 2b. By construction of Gibit , the number of roofs of Gibit that contain player
i
i
2b  1 is 22 , and the number of roofs that contain player 2b is also 22 .
C has at least two distinct direct left-shifts, so there must be another player p0 , p0 6= p,
such that C2 = C  {p0  1} \ {p0 } is a direct left-shift of C.
First we will show that p  2a. Assume therefore
that p > 2a. Now we have that
T
|A(C, a)| = 0, so then |A(C2 , a)| = 0 and hence a A(C2 , a) = . We see that C2 is losing,
but C2 is a direct left-shift of C, which is a ceiling, so C2 is winning. This is a contradiction,
so p  2a.
Now we will show that p  2a. Assume therefore that
T p < 2a. Now
T we have that
i , so then A(C , a) = 2i . Now it must be that
|A(C,
a)|
=
2
A(C
,
a)
=
1
1
a
a A(C, a). But
T
T
A(C,
a)
=

because
C
is
losing,
and
therefore
A(C
,
a)
=

so
C
is
losing. C1 is
1
1
a
a
also winning, because it is a left-shift of ceiling C. This is a contradiction, so p  2a.
Because p  2a and p  2a, we conclude that p = 2a.
Lemma 5. In Gibit , a ceiling does not have more than two direct left-shifts.
Proof. For contradiction, let C be a ceiling with more than two direct left-shifts. Let k be
the number of direct left-shifts of C, and let P = {p1 , . . . , pk } be the set containing the
players of C that we can apply the direct left-shift operation on (we say that we can apply
the direct left-shift operation on a player q when C  {q  1} \ {q} is a left-shift of C). Let
A = {a1 , . . . , ak } then be the numbers such that pj is the aj -th most desirable player in C,
for all i with 1  j  k. For any j  {1, . . . , i} and any b  {0, 1}, let R(j, b) denote the
following set of roofs of Gibit :
R(j, b) = {Sk,i : The j-th bit of the binary representation of k is b. }
Observe that by the previous lemma, there is a k-tuple of bits (b1 , . . . , bk )  {0, 1}k such
that for all j with 1  j  k:
A(C, aj ) = R(dpj /4e, kj ).
There are now two cases:
Case 1: All of the players {p1 , . . . , pk } are in different multiples of 4, i.e., dp1 /4e 6=
dp2 /4e =
6T    =
6 dpk /4e.TThen by the properties of the binary numbers, the intersection aA A(C, a) = pP R(dp/4e, b) is not empty, therefore C must be winning,
which is in contradiction with C being a ceiling. So this case is impossible.

136

fiFinding Optimal Solutions for Voting Game Design Problems

Case 2: There are two distinct players p and p0 , both in P , that are in the same multiple
of 4, i.e., dp/4e = dp0 /4e. Assume without loss of generality that p < p0 . Then
A(C, a)  A(C, a0 ) = . But then we would be able to apply a direct left-shift on
player p00 without turning C into a winning coalition, i.e., C  {p00  1} \ {p00 } is
winning. But C is a ceiling, so that is a contradiction.
From the previous lemma it follows that there can not be more than two players that
are the same multiple of 4, so the above two cases are indeed exhaustive. Both cases are
impossible, so we must reject the assumption that there exists a ceiling C with more than
two left-shifts.
It is easy to see that there exist no more than O(n5 ) coalitions with exactly two leftshifts, there are no more than O(n3 ) coalitions with one left-shift, and there are no more
than O(n) coalitions with no left-shifts. So we get the following corollary.
Corollary 2. The game Gibit (on n = 4i players) has O(n5 ) ceilings.
We can now conclude that {Gibit : i  N} is an infinite family of examples in which
there are exponentially many more roofs than ceilings. Hence, finally we obtain:
Corollary 3. There is no polynomial time algorithm that generates the list of ceilings of a
linear game from its list of roofs. There is no polynomial time algorithm that generates the
list of roofs of a linear game from its list of ceilings.

References
Algaba, E., Bilbao, J. M., Garca, J. R. F., & Lopez, J. J. (2003). Computing power indices
in weighted multiple majority games. Mathematical Social Sciences, 46, 6380.
Alon, N., & Edelman, P. H. (2010). The inverse Banzhaf problem. Social Choice and
Welfare, 34 (3), 371377.
Aziz, H. (2008). Complexity of comparison of influence of players in simple games. In Proceedings of the 2nd International Workshop on Computational Social Choice (COMSOC), pp. 6172.
Aziz, H., Paterson, M., & Leech, D. (2007). Efficient algorithm for designing weighted voting
games. In Proceedings IEEE International Multitopic Conference, pp. 16.
Banzhaf III, J. (1965). Weighted voting doesnt work: A mathematical analysis. Rutgers
Law Review, 19 (2), 317343.
De, A., Diakonikolas, I., Feldman, V., & Servedio, R. A. (2012a). Nearly optimal solutions
for the Chow parameters problem and low-weight approximation of halfspaces. In
Proceedings of the 44th Symposium on Theory of Computing, pp. 729746.
De, A., Diakonikolas, I., & Servedio, R. (2012b). The inverse Shapley value problem. In
Czumaj, A., Mehlhorn, K., Pitts, A., & Wattenhofer, R. (Eds.), Automata, Languages,
and Programming, Vol. 7391 of Lecture Notes in Computer Science, pp. 266277.
Springer.

137

fiDe Keijzer, Klos, & Zhang

Deng, X., & Papadimitriou, C. H. (1994). On the complexity of cooperative solution concepts. Mathematics of Operations Research, 19 (2), 257266.
Dubey, P., & Shapley, L. S. (1979). Mathematical properties of the Banzhaf power index.
Mathematics of Operations Research, 4 (2), 99131.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2008). An anytime approximation method
for the inverse Shapley value problem. In Proceedings of the Seventh International
Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp.
935942.
Fredman, M. L., & Khachiyan, L. (1996). On the complexity of dualization of monotone
disjunctive normal forms. Journal of Algorithms, 21 (3), 618628.
Freixas, J., & Kurz, S. (2011). On minimal integer representations of weighted games.
CoRR, abs/1303.0868.
Freixas, J., & Kurz, S. (2013a). Enumeration of weighted games with minimum and an
analysis of voting power for bipartite complete games with minimum. Annals of
Operations Research, Online first.
Freixas, J., & Kurz, S. (2013b). The golden number and Fibonacci sequences in the design
of voting structures. European Journal of Operational Research, 226 (2), 246257.
Freixas, J., & Molinero, X. (2008). The greatest allowed relative error in weights and
threshold of strict separating systems. IEEE Transactions on Neural Networks, 19 (5),
770781.
Freixas, J., & Molinero, X. (2010). Weighted games without a unique minimal representation
in integers. Optimization Methods and Software, 25 (2), 203215.
Freixas, J., Molinero, X., & Roura, S. (2012). Complete voting systems with two classes of
voters: weightedness and counting. Annals of Operations Research, 193 (1), 273289.
Hu, S. (1965). Threshold logic. University of California Press.
Isbell, J. R. (1958). A class of simple games. Duke Mathematical Journal, 25 (3), 423439.
Karmarkar, N. (1984). A new polynomial-time algorithm for linear programming. In Proceedings of the sixteenth annual ACM Symposium on Theory of computing (STOC),
pp. 302311.
De Keijzer, B. (2009a). On the design and synthesis of voting games: exact solutions for
the inverse problem. Masters thesis, Delft University of Technology.
De Keijzer, B. (2009b). A survey on the computation of power indices. Tech. rep., Delft
University of Technology.
De Keijzer, B., Klos, T., & Zhang, Y. (2010). Enumeration and exact design of weighted
voting games. In Proceedings of the 9th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), pp. 391398.
De Keijzer, B., Klos, T., & Zhang, Y. (2012). Solving weighted voting game design problems
optimally: Representations, synthesis, and enumeration. CoRR, abs/1204.5213.
Kleijn, A. (2012). Weighted voting game design heuristics. Masters thesis, Erasmus School
of Economics, Erasmus University Rotterdam.
138

fiFinding Optimal Solutions for Voting Game Design Problems

Kleitman, D., & Markowski, M. (1975). On Dedekinds problem: The number of isotone
boolean functions II. In Transactions of the American Mathematical Society, Vol. 213,
pp. 373390.
Klinz, B., & Woeginger, G. J. (2005). Faster algorithms for computing power indices in
weighted voting games. Mathematical Social Sciences, 49, 111116.
Korshunov, A. D. (2003). Monotone boolean functions. Russian Mathematical Surveys,
58 (5), 198162.
Krohn, I., & Sudholter, P. (1995). Directed and weighted majority games. Mathematical
Methods of Operations Research, 42 (2), 189216.
Kurz, S. (2012a). On minimum sum representations for weighted voting games. Annals of
Operations Research, 196 (1), 361369.
Kurz, S. (2012b). On the inverse power index problem. Optimization, 61 (8), 9891011.
Laruelle, A., & Widgren, M. (1998). Is the allocation of voting power among EU states
fair?. Public Choice, 94, 317339.
Leech, D. (2002a). Computation of power indices. Tech. rep. 664, Warwick Economic
Research Papers.
Leech, D. (2002b). Designing the voting system for the EU Council of Ministers. Public
Choice, 113 (34), 437464.
Leech, D. (2002c). Voting power in the governance of the International Monetary Fund.
Annals of Operations Research, 109, 373395.
Leech, D. (2003). Power indices as an aid to institutional design: the generalised apportionment problem. In Holler, M., Kliemt, H., Schmidtchen, D., & Streit, M. (Eds.),
European Governance, No. 22 in Jahrbuch fur Neue Politische Okonomie, pp. 107121.
Mohr Siebeck.
Makhorin, A. (2004). GNU linear programming toolkit..
Matsui, Y., & Matsui, T. (2001). NP-completeness for calculating power indices of weighted
majority games. Theoretical Computer Science, 263 (12), 305310.
McKay, B. D. (1998). Isomorph-free exhaustive generation. Journal of Algorithms, 26,
306324.
Muroga, S. (1971). Threshold logic and its applications. Wiley-Interscience.
Muroga, S., Toda, I., & Kondo, M. (1962). Majority functions of up to six variables.
Mathematics of Computation, 60 (80), 459472.
Muroga, S., Tsuboi, T., & Baugh, C. R. (1970). Enumeration of threshold functions of eight
variables. IEEE Transactions on Computers, C-19 (9), 818825.
De Nijs, F., Wilmer, D., & Klos, T. (2012). Evaluation and improvement of LaruelleWidgren inverse Banzhaf approximation. In Proceedings Benelux AI Conference
(BNAIC), pp. 194201.
ODonnell, R., & Servedio, R. A. (2011). The Chow parameters problem. SIAM Journal
on Computing, 40 (1), 165199.
139

fiDe Keijzer, Klos, & Zhang

Parberry, I. (1994). Circuit complexity and neural networks. Mit Press.
Peled, U. M., & Simeone, B. (1985). Polynomial-time algorithms for regular set-covering
and threshold synthesis. Discrete Applied Mathematics, 12, 5769.
Peleg, B., & Sudholter, P. (2003).
Springer.

Introduction to the Theory of Cooperative Games.

Polymeris, A., & Riquelme, F. (2013). On the complexity of the decisive problem in simple,
regular and weighted games. CoRR, abs/1303.7122.
Prasad, K., & Kelly, J. S. (1990). NP-completeness of some problems concerning voting
games. International Journal of Game Theory, 19 (1), 19.
Shapley, L. S., & Shubik, M. (1954). A method for evaluating the distribution of power in
a committee system. American Political Science Review, 48 (3), 787792.
Siu, K., Roychowdhury, V., & Kailath, T. (1995). Discrete Neural Computation: A Theoretical Foundation. Prentice Hall.
Sperner, E. (1928). Ein Satz uber Untermengen einer endlichen Menge. Mathematische
Zeitschrift, 27 (1), 544548.
Sutter, M. (2000). Fair allocation and re-weighting of votes and voting power in the EU
before and after the next enlargement. Journal of Theoretical Politics, 12, 433449.
Taylor, A. D., & Zwicker, W. S. (1999). Simple Games: Desirability Relations, Trading,
Pseudoweightings. Princeton University Press.
Winder, R. O. (1965). Enumeration of seven-argument threshold functions. IEEE Transactions on Electronic Computers, EC-14 (3), 315325.
Zuev, Y. A. (1989). Asymptotics of the logarithm of the number of threshold functions of
the algebra of logic. Soviet Math. Dokl., 39, 512513.
Zunic, J. (2004). On encoding and enumerating threshold functions. IEEE Transactions on
Neural Networks, 15 (2), 261267.

140

fiJournal of Artificial Intelligence Research 50 (2014) 31-70

Submitted 08/13; published 05/14

Knowledge Forgetting in Answer Set Programming
Yisong Wang

CSC . YSWANG @ GZU . EDU . CN

Department of Computer Science,
Guizhou University, Guiyang, China

Yan Zhang
Yi Zhou

YAN @ SCEM . UWS . EDU . AU
YZHOU @ SCEM . UWS . EDU . AU

Artificial Intelligence Research Group,
University of Western Sydney, Australia

Mingyi Zhang

ZHANGMINGYI 045@ GMAIL . COM

Guizhou Academy of Sciences, Guiyang, China

Abstract
The ability of discarding or hiding irrelevant information has been recognized as an important
feature for knowledge based systems, including answer set programming. The notion of strong
equivalence in answer set programming plays an important role for different problems as it gives
rise to a substitution principle and amounts to knowledge equivalence of logic programs. In this
paper, we uniformly propose a semantic knowledge forgetting, called HT- and FLP-forgetting, for
logic programs under stable model and FLP-stable model semantics, respectively. Our proposed
knowledge forgetting discards exactly the knowledge of a logic program which is relevant to forgotten variables. Thus it preserves strong equivalence in the sense that strongly equivalent logic
programs will remain strongly equivalent after forgetting the same variables. We show that this
semantic forgetting result is always expressible; and we prove a representation theorem stating that
the HT- and FLP-forgetting can be precisely characterized by Zhang-Zhous four forgetting postulates under the HT- and FLP-model semantics, respectively. We also reveal underlying connections
between the proposed forgetting and the forgetting of propositional logic, and provide complexity
results for decision problems in relation to the forgetting. An application of the proposed forgetting
is also considered in a conflict solving scenario.

1. Introduction
Motivated by Lin and Reiters seminal work (Lin & Reiter, 1994), the notion of forgetting in propositional and first-order logics  distilling from a knowledge base only the part that is relevant to
a subset of the alphabet  has attracted extensive interests in the KR community, (e.g., see Lang
& Marquis, 2010; Zhou & Zhang, 2011). In recent years, researchers have developed forgetting
notions and theories in other non-classical logic systems from various perspectives, such as forgetting in description logics (Kontchakov, Wolter, & Zakharyaschev, 2008; Wang, Wang, Topor, &
Pan, 2010; Lutz & Wolter, 2011; Packer, Gibbins, & Jennings, 2011), forgetting in logic programs
(Zhang & Foo, 2006; Eiter & Wang, 2008; Wong, 2009; Wang, Wang, & Zhang, 2013), and forgetting in modal logic (Zhang & Zhou, 2009; Su, Sattar, Lv, & Zhang, 2009; van Ditmarsch, Herzig,
Lang, & Marquis, 2009; Liu & Wen, 2011). As a logical notion, forgetting has also been studied
under some different terms such as variable elimination (Lang, Liberatore, & Marquis, 2003), irrelevance, independence, irredundancy, novelty, or separability (Bobrow, Subramanian, Greiner, &
c
2014
AI Access Foundation. All rights reserved.

fiWANG , Z HANG , Z HOU , & Z HANG

Pearl, 1997). It has been shown that in the study of modeling agents behaviors, forgetting plays an
important role in conflict resolution (Zhang & Foo, 2006; Lang & Marquis, 2010).
In propositional logic, the result of forgetting an atom p from a formula , written Forget(, {p}),
is the formula [p/]  [p/>], where [p/] and [p/>] is the formula obtained from  by replacing each occurrence of atom p with  (false) and > (true) respectively. Forgetting a set of atoms
from a formula  is defined as Forget(, V  {p}) = Forget(Forget(, {p}), V ) (Lin, 2001). It is
easy to see that the forgetting preserves logical equivalence. That is, logically equivalent formulas
(theories) will remain logically equivalent after forgetting the same atoms. It is well known that, if
 does not mention any atoms from V then
 |=  iff Forget(, V ) |= .
In this sense the forgetting in propositional logic, called propositional forgetting, is a knowledge
forgetting since Forget(, V ) exactly contains the logical content of  that is irrelevant to V .
For logic programs under stable model/answer set semantics (Gelfond & Lifschitz, 1988), the issue of logical equivalence is rather complicated due to its different notions of equivalence: (weak)
equivalence and strong equivalence. Two logic programs 1 and 2 are (weakly) equivalent if and
only if 1 and 2 have the same stable models; 1 and 2 are strongly equivalent if and only if
1   and 2   are equivalent for every logic program . It is well known that strong equivalence is an important concept in answer set programming (ASP), because it amounts to knowledge
equivalence which captures the logical content of a logic program (Osorio & Zacarias, 2004; Osorio
& Cuevas, 2007; Delgrande, Schaub, Tompits, & Woltran, 2013), and can be used for simplifying
logic programs where two strongly equivalent rules may be interchangeable without affecting the
original logic programs stable models (Pearce, Tompits, & Woltran, 2001; Ferraris, Lee, & Lifschitz, 2011; Lin & Chen, 2007; Lin & Zhou, 2011). The strong equivalence can be characterized
in the logic here-and-there (HT), viz, two logic programs are strongly equivalent if and only if they
have the same HT-models (Lifschitz, Pearce, & Valverde, 2001). For instance, a rule of the following form p  p   has the same HT-models as that of > (tautology), where  can be an arbitrary
formula. Thus it can be safely removed from every logic programs without changing their stable
models.
Besides the stable model/answer set semantics of logic programs (Gelfond & Lifschitz, 1988),
FLP-stable model semantics also steadily gains its importance (Faber, Pfeifer, & Leone, 2011;
Truszczynski, 2010). The notion of strong equivalence is similarly generalized to logic programs
under FLP-stable models semantics: two theories 1 and 2 are strongly FLP-equivalent if and only
if 1   and 2   have the same FLP-stable models for every logic program . It is shown that
this strong equivalence can be characterized in terms of FLP-models, viz, two logic programs are
strongly FLP-equivalent if and only if they have the same FLP-models (Truszczynski, 2010).
When we develop the notion of forgetting in logic programs, preserving strong equivalence is
important, like that the propositional forgetting preserves equivalence of propositional logic. Consider that two agents need to achieve an agreement for a certain goal, where each agents knowledge
base is represented by a logic program. Suppose that there are two consistent1 logic programs, but
their combination is inconsistent. To achieve a consistent combination, one may forget some atoms
from each of the logic programs, so that the combination of their forgetting results is consistent.
Then forgetting may be effectively used to solve the conflict between the two agents knowledge
1. A logic program is consistent if it has some stable models.

32

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

bases (Zhang & Foo, 2006; Eiter & Wang, 2008; Lang & Marquis, 2010). For the purpose of simplicity, on the other hand, agents may also replace their knowledge bases with strongly equivalent
but syntactically simpler ones.
Let us consider a simple Yale Shooting scenario where the logic program  consisting of the
following rules:2
shoot  not aux;

aux  not shoot;

 aux, shoot.

Here aux is used to generate possible occurrences of action shoot. One can be interested in which
logic program represents the same knowledge as that of  when the auxiliary atom aux is ignored.
This intuitively results in a logic program 0 consisting of the rule3 :
shoot  not not shoot,
which captures exactly the knowledge of  that is irrelevant to aux. We will see that 0 can be
obtained from  by HT-forgetting aux (cf. Example 5 with other atom names), while it cannot be
obtained in terms of previous forgetting approaches in logic programming (cf. Example 11).
It turns out that preserving strong equivalence in forgetting is challenging. There have been
several attempts to define the notion of forgetting in logic programs, but none of these approaches
is fully satisfactory. Zhang and Foo (2006) first defined syntax oriented weak and strong forgetting
notions for normal logic programs. But these forgetting notions preserve neither (weak) equivalence
nor strong equivalence. Eiter and Wang (2008) then proposed a semantic forgetting for consistent
disjunctive logic programs, which preserves equivalence but not strong equivalence. They specifically indicated the importance of preserving strong equivalence in logic programming forgetting
and raised this issue as a future work. Wong (2009) proposed two forgetting operators for disjunctive logic programs. Although the two operators indeed preserve strong equivalence, it may lose the
intuition of weakening under various circumstances (see Section 5 for details). A recently proposed
forgetting for logic programs may introduce extra knowledge (cf., see Wang et al., 2013, Ex. 2).
Thus it is not a knowledge forgetting.
Together with preserving strong equivalence, expressiveness is another desired criterion for
logic programming forgetting. Ideally we would expect that the result of forgetting some atoms
from a logic program is still expressible by a logic program. This is particularly necessary when we
view agents knowledge bases as logic programs and forgetting is employed as a means of conflict
solving among these agents knowledge bases (Zhang & Foo, 2006). While previous logic programming forgetting approaches all meet this criterion, as we will see in this paper, once we consider
forgetting in arbitrary logic programs, retaining expressibility is challenging objective to achieve for
a semantic forgetting notion.
Finally, we believe that as a way of weakening, knowledge forgetting in logic programs should
obey some common intuitions shared by forgetting in classical logics. For instance, forgetting
something from a logic program should lead to a weaker program in certain sense. On the other
hand, such weakening should only be associated to the relevant information that has been forgotten.
For this purpose, Zhang and Zhou (2009) proposed four forgetting postulates to formalize these
common intuitions and showed that forgetting in propositional logic and modal logic S5 can be
precisely captured by these postulates. Surprisingly, none of previous forgetting notions in logic
2. This is due to one of the anonymous reviewers.
3. The rule is strongly equivalent to the choice rule 0{shoot}1 but it is not a normal rule.

33

fiWANG , Z HANG , Z HOU , & Z HANG

programs actually satisfies Zhang-Zhous postulates. In this sense these previous forgetting notions
for logic programs are not knowledge forgetting operators.
In summary, we consider the following criteria that a knowledge forgetting notion in logic programs should meet:
 Expressibility. The result of forgetting in an arbitrary logic program should also be expressible via a logic program;
 Preserving strong equivalence. Two strongly equivalent logic programs should remain strongly
equivalent after forgetting the same variables;
 Satisfying common intuitions of forgetting. Preferably, forgetting in logic programs should
be semantically characterized by Zhang-Zhous four forgetting postulates.
In this paper we present a comprehensive study on knowledge forgetting in the context of arbitrary logic programs (propositional theories) under stable model and FLP-stable models semantics,
called HT- and FLP-forgetting respectively. We show that the HT- and FLP-forgetting meet all above
criteria, and hence have primary advantages when compared to previous logic program forgetting
notions.
The main contributions of the paper may be summarized as follows, where ?  {HT, FLP },
- As a starting point, we investigate the model theoretical characterization for strong equivalence of logic programs under stable model and FLP-stable model semantics, and explore their
strong equivalence by the equivalence in propositional logic.
- We propose a semantic ?-forgetting for logic programs under ?-stable model semantics respectively. Here HT-stable model means stable model. The ?-forgetting result is always
expressible via a logic program and it preserves strong equivalence under stable model and
FLP-stable model semantics.
- We investigate semantic properties of the ?-forgetting, and show that the ?-forgetting satisfies
Zhang-Zhous four postulates under the ?-model respectively. In particular, the forgetting
result consists of the logical content that is irrelevant to forgotten atoms.
- We establish the underlying connections between ?-forgetting and propositional forgetting,
based on which we provide complexity results for some decision problems in relation to ?forgetting. In particular, we show that resulting checking  deciding if a logic program is a
result of ?-forgetting a set of atoms from a logic program  is P2 -complete, while the related
inference problem in terms of ?-forgetting varies from co-NP-complete to P2 -complete.
The theoretical negative results confirm that it is not a easy task to simplify logic programs
by forgetting. But fortunately, this kind of simplification can be computed offline in general.
For instance, a problem domain description involves a lot of auxiliary propositional variables.
One can firstly simplify the description by forgetting (part of) the auxiliary propositional
variables, like a kind of compilation (Lang et al., 2003).
- Finally we consider an application of knowledge forgetting in the solving of conflicts in the
context of logic programming.
34

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

The rest of the paper is organized as follows. Section 2 briefly reviews necessary concepts and
notions of answer set programming. Section 3 presents the characterizations for strong equivalence
of logic programs. We firstly present a uniform definition of the knowledge forgetting for logic
programs in section 4, and then explore their expressibility, forgetting postulates, relationship with
propositional forgetting, computational complexity and an application of knowledge forgetting in
conflict solving. Section 5 discusses other forgetting approaches in logic programs, and finally,
Section 6 concludes the paper with some remarks. All the proofs in the paper are deferred to
Appendix for clarity.
This paper is the revised and extended version of a paper which appeared in Proceedings of KR
2012 (Wang, Zhang, Zhou, & Zhang, 2012).

2. Answer Set Programming
In this section we briefly recall the basic notions of logic programming under stable model semantics, including its syntax, reduction, stable model (Ferraris, 2005) and FLP-stable models (Truszczynski, 2010) and strong equivalence (Lifschitz et al., 2001; Truszczynski, 2010). In the paper a stable
model is called an HT-stable model for convenience, and we assume ?  {HT , FLP }.
We assume a propositional language LA over the finite set A of propositional atoms, which is
called the signature of the language LA .
2.1 Syntax
The formulas of LA are built from the signature4 A and the 0-place connective  (false) using
the binary connectives ,  and  as follows:
 ::=  | p |    |    |   

(1)

where p  A. > (true) is the shorthand of   ,  for   , and    for (  )  ( 
). A theory is a set of formulas.
An interpretation is a set I of atoms from A, where each atom of A is viewed to be true if it is in
I, and false otherwise. In propositional logic, the notions of model and satisfaction relation |= are
defined as usual. In the following we denote A \ X by X for X  A, Mod() for {M |M |= },
/ M} where
   for Mod() = Mod() (i.e.  is equivalent to ) and M for {I  A|I 
A
M  2 . A formula  is irrelevant to a set V of atoms, written IR(, V ), if there exists a formula
 mentioning no atoms from V such that   .
For convenience,
we
W
V also define the following notations. Let S be a finite set of formulas.
W
We denote SV(resp.
S) the disjunction (resp. conjunction) of all formulas in S, where 
denotes  and  denotes >, and |S| the cardinality of S. Similarly by S (resp. S) we mean
{ |   S} (resp. { |   S}).
2.2 Reduct and Stable Models
Let  be a formula and X  A. The ?-reduct of  w.r.t. X, written Red? (, X), is recursively and
uniformly defined as follows:
4. In the rest of this paper, whenever there is no confusion, we may not explicitly mention the signature when we talk
about formulas of LA .

35

fiWANG , Z HANG , Z HOU , & Z HANG

(?-R1) Red? (, X) = ;
(?-R2) Red? (p, X) = p if X |= p, and  otherwise;
(?-R3) Red? (1  2 , X) = Red? (1 , X)  Red? (2 , X) if X |= 1  2 where   {, }, and
 otherwise;
(HT-R4) RedHT (1  2 , X) = RedHT (1 , X)  RedHT (2 , X) if X |= 1  2 , and  otherwise;

 1  RedFLP (2 , X), if X |= 1  2 ;
(FLP-R4) RedFLP (1  2 , X) =
>,
if X 6|= 1 ;

,
otherwise (i.e. X 6|= 1  2 ).

Definition 1 A set X  A is a ?-stable model of a formula  if X is a minimal (under set inclusion)
model of Red? (, X). We denote the set of ?-stable models of  by SM ? ().
Please note that, traditionally, the HT-reduct is named reduct; Red HT (, X) is written as X ;
HT-stable model is called stable model (Ferraris, 2005); and RedFLP (, X) is written as X 
(Truszczynski, 2010).
It is known that, HT-stable models and FLP-stable models are not comparable in the sense that
some HT-stable models are not FLP-stable models, and some FLP-stable models are not HT-stable
models (cf., see Truszczynski, 2010, Exs. 1, 2, 4 and 5).
Example 1 Let us consider the following formulas:
 Let  = p  p  p. We have that
RedHT (, )  , RedHT (, {p})  >, RedFLP (, )  , RedHT (, {p})  p.
Thus SM HT () = , while SM FLP () = {{p}}.
 Let 1 = p  p and 2 = p  p. We have the following:
RedHT (i , )  > and RedHT (i , {p})  p, for i = 1, 2,
RedFLP (1 , )  >, Red FLP (1 , {p})  p, RedFLP (2 , )  >, RedFLP (2 , {p})  >.
Thus, while SM FLP (1 ) =

SM HT (1 )

= {, {p}}, SM FLP (2 ) = {}.

Definition 2 Two formulas 1 and 2 are ?-SM -equivalent (under ?-stable model semantics), written 1 ?SM 2 , iff they have the same ?-stable models.
Here the notion of HT-SM -equivalence is indeed the notion of equivalence in logic programs
under stable model semantics (cf., see Lifschitz et al., 2001, Thm. 1).
36

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

2.3 Strong Equivalence and Knowledge of Logic Programs
Unlike the equivalence in propositional logic, the equivalence of logic programs does not allow
equivalent replacement i.e.,   1 and   2 may have different stable models, even though 1
and 2 are equivalent.
Example 2 Let 1 = p  q and 2 = p  p. As SM ? (1 ) = SM ? (2 ) = {}, 1 and 2 are
?-SM-equivalent; however, p  1 has a ?-stable model {p, q} while the unique ?-stable model of
p  2 is {p}. Thus it does not allow replacing 1 by 2 in p  1 . It also indicates that 1 has
different knowledge from 2 under the ?-stable model semantics.
This motivates the notion of strong equivalence.
Definition 3 Two formulas 1 and 2 are strongly ?-equivalent (under ?-stable model semantics)
iff   1 ?SM   2 for every formula . In the case 1 and 2 are strongly ?-equivalent, they
are ?-knowledge equivalent.
It is known that the notion of strong ?-equivalence can be captured in terms of ?-models, where
a ?-interpretation is a pair hX, Y i such that X  Y  A. The ?-satisfiability (thus ?-models),
denoted by |=? , is recursively defined as follows:
(?-S1) hX, Y i 6|=? ;
(?-S2) hX, Y i |=? p if p  X;
(?-S3) hX, Y i |=? 1  2 if hX, Y i |=? 1 or hX, Y i |=? 2 ;
(?-S4) hX, Y i |=? 1  2 if hX, Y i |=? 1 and hX, Y i |=? 2 ;
(HT-S5) hX, Y i |=HT 1  2 if Y |= 1  2 ; and hX, Y i |=HT 1 implies hX, Y i |=HT 2 ;
(FLP-S5) hX, Y i |=FLP 1  2 if Y |= 1  2 ; and Y 6|= 1 or X 6|= 1 or hX, Y i |=FLP 2 .
By Mod? () we denote the set of all ?-models of formula . Please note here that, ? can be
either HT or FLP . In particular, ModHT () (resp. ModFLP ()) denotes the set of all HT-models (resp.
FLP-models) of . For the formulas 1 and 2 in Example 2, one can check that none of h, {p}i,
h{p}, {p}i or h{p}, {p, q}i is a ?-model of 1 , while every ?-interpretation is a ?-model of 2 .
Definition 4 A formula  is a logical ?-consequence of a formula , written  |=? , iff Mod? () 
Mod? (); two formulas  and  are ?-equivalent (under ?-model semantics), written  ? , iff
Mod? () = Mod? ().
In the following proposition, item (i) is proved by Lifschitz, Tang, and Turner (cf., see Lifschitz
et al., 1999, (iii) of Prop. 6).
Proposition 1 Let A, B, C, D be set of atoms. We have the following
V
W
V
W
(i) (A  B)  (D  C) HT (A  B  C)  D.
V
W
V
W
(ii) (A  B)  (D  C) |=FLP (A  B  C)  D.
37

fiWANG , Z HANG , Z HOU , & Z HANG

Please note here that the inverse of (ii) does not generally hold. For instance, p  p FLP >
while h, {p}i 6|=FLP p  p.
Given two formulas 1 and 2 , it is known that 1 and 2 are strongly HT-equivalent under
HT -stable model semantics if and only if they are HT -equivalent, viz. 1  HT 2 ; 1 and 2 are
strongly FLP -equivalent under FLP -stable model semantics if and only if they are FLP -equivalent,
viz. 1 FLP 2 (cf., see Truszczynski, 2010, Thm. 7). It is commonly recognized that strong
equivalence amounts to knowledge equivalence of formulas. That is, strong ?-equivalence captures
the logical content of a formula under ?-stable model semantics (Osorio & Zacarias, 2004; Osorio
& Cuevas, 2007; Delgrande et al., 2013). Now we formally define the knowledge of logic programs.
Definition 5 The ?-knowledge of a formula  under ?-stable model semantics, written Cn? (),
consists of the logical ?-consequence of , viz, Cn? () = { |  |=? }.
The ?-knowledge of a formula stands for the ?-logical content of the formula. For instance,
CnHT (>) = CnHT (p  p)  CnHT (p  q).
Recall that, under ?-model semantics, every formula can be transformed into a conjunction of
formulas in the following normal form:
^
_
(B  C)  (A  D)
(2)
where A, B, C, D are sets of atoms (cf., for ? = HT, see Cabalar & Ferraris, 2007, Thm. 2;
Truszczynski, 2010, Thm. 9 for ? = FLP ). That is, for every formula , there is a conjunction
of formulas in the form (2) which is strongly ?-equivalent to .
A formula of the form (2) is called a rule, which is also generally written as
a1 ; . . . ; al ; not d1 ; . . . ; not dn  b1 , . . . , bk , not c1 , . . . , not cm

(3)

where A = {ai |1  i  l}, B = {bi |1  i  k}, C = {ci |1  i  m} and D = {di |1  i  n}.
A logic program is a finite set of rules. Let r be a rule of the form (2). It is said to be
 disjunctive if D = ;
 positive if C = D = ;
 normal if |A|  1 and D = ; and
 Horn if |A|  1 and C = D = .
A logic program is disjunctive (resp. positive, normal, and Horn) iff it consists of disjunctive
(resp. positive, normal, Horn) rules. A logic program is ?-consistent (under ?-stable model semantics) if it has at least one ?-stable model.
It is known that every logic program has the same HT-models and FLP-models (cf., see Truszczynski, 2010, Prop. 8).
Proposition 2 Every logic program has the same HT- and

FLP-models.

3. Characterizations of Knowledge Equivalence
In the section, from the perspective of ?-models, we consider the characterization for knowledge
equivalence of various logic programs firstly, and relate the knowledge equivalence to the equivalence of propositional logic secondly.
38

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

3.1 Model Theoretical Characterization
We firstly recall some basic properties of the ?-satisfiability (Ferraris & Lifschitz, 2005; Ferraris,
2005; Truszczynski, 2010).
Proposition 3 Let  be a formula and X  Y  A.
(i) If hX, Y i |=?  then hY, Y i |=?  (i.e., Y |= ).
(ii) hX, Y i |=?  iff Y |= .
(iii) hX, Y i |=?  iff X |= Red? (, Y ).
A collection M of ?-interpretations is ?-expressible whenever there exists a formula  such that
Mod? () = M. A collection M of ?-interpretations may be not ?-expressible. For instance, there
is no formula whose ?-models are the ones in M = {h, {p}i}. The reason is that if there is a
formula  such that Mod? () = M then we have h{p}, {p}i |=?  by (i) of Proposition 3. This
requires h{p}, {p}i belonging to Mod? (), a contradiction.
Given a formula  and X  Y  A, hX, Y i is a ?-countermodel of  if hX, Y i 6|=?  and
hY, Y i |=? ; hY, Y i is a ?-countermodel of  if hY, Y i 6|=? . Let X  Y  A, we define the
following formulas:
_
(X  Y )  ((Y \ X)  (Y \ X)),
_
^
FLP (X, Y ) = (X  Y )  (X  Y ),
^
(Y, Y ) = (Y  Y )  ,
_
^
(X, Y ) = (X  Y )  (Y \ X).
HT (X, Y ) =

^

(4)
(5)
(6)
(7)

Here ? (X, Y ) and (Y, Y ) is to capture the ?-countermodel hX, Y i and hY, Y i respectively.
The following lemma shows that the ?-countermodel can be captured by a formula (cf., for
? = HT, see Cabalar & Ferraris, 2007, Prop. 1; Truszczynski, 2010, Props. 5 and 6 for ? = FLP ).
Lemma 1 Let X  Y  A and U  V  A.
(i) hU, V i is a ?-countermodel of ? (X, Y ) iff U = X and V = Y .
(ii) hU, V i is a ?-countermodel of (Y, Y ) iff V = Y .
Proposition 4 A collection M of ?-interpretations is ?-expressible iff
hX, Y i  M implies hY, Y i  M.
Actually, if M satisfy condition (8) then the following logic program
? = {? (X, Y )|hX, Y i 
/ M and hY, Y i  M}  {(Y, Y )|hY, Y i 
/ M}
captures M in the sense that Mod? (? ) = M.
39

(8)

fiWANG , Z HANG , Z HOU , & Z HANG

Note that Wong (2009) presented a model-theoretical characterization for the HT-models of
disjunctive logic programs (cf., see Wong, 2009, Thm. 2.7). Formally speaking, a collection M of
HT-interpretations is disjunctively HT-expressible, i.e., there is a disjunctive logic program  such
that ModHT () = M, iff the condition (8) and the following one hold:
if hX, Y i  M, Y  Y 0 and hY 0 , Y 0 i  M then hX, Y 0 i  M.

(9)

Together with Proposition 2, we have
Corollary 1 A collection M of
tions (8) and (9) hold.

FLP -interpretations

is disjunctively

FLP -expressible

iff the condi-

Actually, if M satisfies the conditions (8) and (9) then the following disjunctive logic program
captures M.
 = {(X, Y )|hX, Y i 
/ M and hY, Y i  M}  {(Y, Y )|hY, Y i 
/ M}.
Lemma
V
W2 Let A, B beVtwo sets
W of atoms, and X  Y  A. hX, Y i |=?
B  A and Y |= B  A.

V

B 

W

A iff X |=

Proposition 5 A set M of ?-interpretations is positively ?-expressible, i.e., there is a positive logic
program  s.t Mod? () = M, iff M satisfies the criteria:
hX, Y i  M iff X  Y, hX, Xi  M and hY, Y i  M.

(10)

As
Va matter
W of fact, in the case M satisfies the condition (10), the positive logic program  =
/ M} captures M.
{ X  X|hX, Xi 

Corollary 2 Two positive logic programs are strongly ?-equivalent if and only if they are equivalent
in propositional logic.

Eiter, Fink, Tompits, and Woltran (2004) have showed that a disjunctive logic program  is
strongly equivalent to a normal logic program if and only if  is closed under here-intersection, i.e.,
for every pair of HT-models hX, Y i and hX 0 , Y i of , hX  X 0 , Y i is also an HT-model of  (cf.,
see Eiter et al., 2004, Thms. 1 and 2). In terms of the characterization of disjunctive logic programs
and Proposition 2, we obtain a ?-model characterization for normal logic programs as follows.
Corollary 3 A set M of ?-interpretations is normally ?-expressible, i.e., there is a normal logic
program  such that Mod? () = M, iff M satisfies, in addition to (8) and (9), the following
criteria:
if hX, Y i  M and hX 0 , Y i  M then hX  X 0 , Y i  M.
(11)
Proposition 6 A collection M of ?-interpretations is Horn ?-expressible, i.e., there is a Horn logic
program  such that Mod? () = M, iff M satisfies, in addition to (10), the following criteria:
hX, Y i  M and hH, T i  M  hX  H, Y  T i  M.
40

(12)

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

3.2 Relating Knowledge Equivalence to Propositional Logic
It is proved that strong equivalence of logic programs under stable model semantics can be related
to the equivalence in propositional logic (Pearce et al., 2001; Lin, 2002). This holds for the strong
FLP-equivalence of logic programs as we will show in the following.
Firstly, we extend the language LA to LAA0 where A0 = {p0 |p  A} and p0 s are fresh atoms.
For each expression  of LA , by 0 we denote the result obtained from  by replacing each atom p
from A by the corresponding atom p0 in A0 . In the following we denote
(A) = {p  p0 | p  A}.

(13)

Please note that, for each model M of (A), M has a splitting MA and MA0 where MA = M  A
and MA0 = M  A0 and, for every p  MA , the atom p0 of A0 belongs to MA0 . For M  A0 we
denote by M  the set {p  A|p0  M }.
Definition 6 HT [.] and FLP [.] are recursively defined as follows:
(T1) ? [] = ;
(T2) ? [p] = p;
(T3) ? [1  2 ] = ? [1 ]  ? [2 ] where   {, };
(HT-T4) HT [1  2 ] = (01  02 )  (HT [1 ]  HT [2 ]);
(FLP-T4) FLP [1  2 ] = (01  02 )  (1  01  FLP [2 ]).
Please note that the translation HT is same to the translation  defined by Pearce, Tompits, and
Woltran (2001). One can verify that HT [] = 0  HT [], while FLP [] = 0 . Given a
theory  of LA , we define ? [] = {? [] |   }. It is evident that ? [] is in linear size of .
Example 3 Let  = p  p  p. We have that
HT [] = ((p0  p0 )  p0 )  ((p  p  p0 )  p)  p0 ,
FLP [] = ((p0  p0 )  p0 )  ((p  p)  (p0  p0 )  p)  p0  p.
The unique FLP-model (over the signature {p}) of  is h{p}, {p}i. However,  has two HT-models
h, {p}i and h{p}, {p}i. Over the signature {p, p0 }, one can easily check that {HT []}  (A) has
two models {p, p0 } and {p0 }, while {FLP []}  (A) has a unique model {p, p0 }.
V
W
Proposition 7 Let  = (B  C)  (A  D), where A, B, C, D are subsets of A. Then we
have (A) |= FLP []  HT [].
The following proposition connects the ?-equivalence with the equivalence in classical propositional logic (cf., for ? = HT, see Pearce et al., 2001, Lem. 2).
Proposition 8 Let  be a formula of LA and X  Y  A. Then hX, Y i is a ?-model of  iff
X  Y 0 is a model of (A)  {? []}.
41

fiWANG , Z HANG , Z HOU , & Z HANG

The following theorem shows that the strong ?-equivalence of logic programs under ?-stable
model semantics can be reduced to the equivalence in propositional logic (cf., for ? = HT, see
Ferraris et al., 2011, Thm. 9; or Lin & Zhou, 2011, (5) of Thm. 6).
Theorem 4 Two formulas  and  have the same ?-models (over A) iff (A){? []} and (A)
{? []} have the same models (over A  A0 ).
Based on the theorem, we obtain the following complexity result (cf., for ? =
Tompits, & Woltran, 2009, Thms. 8 and 11).

HT ,

see Pearce,

Proposition 9 (i) The problem of deciding if a formula is ?-satisfiable is NP-complete.
(ii) The problem of deciding if two formulas are ?-equivalent is co-NP-complete.

4. Knowledge Forgetting in Logic Programs
As mentioned in the introduction, we concentrate on the knowledge forgetting of logic programs
under stable model semantics. It is formally stated as following:
Definition 7 (Knowledge forgetting) Let  be a logic program and V  A. A logic program  is
a result of ?-knowledge forgetting V from , if and only if  consists of the ?-knowledge of  that
mentions no atom from V .
We will show that such a knowledge forgetting result always exists and it is unique up to strong
equivalence (cf. Theorem 6) after a semantic ?-forgetting is defined and explored.
Let V, X, Y be sets of atoms. The set Y is V -bisimilar to X, written Y V X, if Y \V = X \V .
It intuitively states that the interpretations X and Y agree with each other on those atoms not in V .
Two ?-interpretations hH, T i and hX, Y i are V -bisimilar, written hH, T i V hX, Y i, if H V X
and T V Y . Now, we are in the position to define the semantic knowledge forgetting in terms of
bisimulation.
Definition 8 (Semantic knowledge forgetting) Let  be a formula and V  A. A formula  is a
result of (semantic) ?-forgetting V from  whenever, for every ?-interpretation M ,
M  Mod? () iff M 0  Mod? () s.t M V M 0 .

(14)

According the definition, one can see that the ?-models of  can somehow exactly constructed from
those of . This motivates us to define the following notion of extension.
Let V, X, Y be sets of atoms. The V -extension of X, denoted by XV , is the collection of
interpretations that are V -bisimilar to X. The V -extension of a ?-interpretation hH, T i, denoted
by hH, T iV , is the collection of ?-interpretations that are V -similar to hH, T i. For instance, let
hH, T i = h{p, q}, {p, q}i and V = {q, r}. Then hH, T iV contains h{p}, {p}i, h{p}, {p, q}i,
h{p}, {p, q, r}i, h{p, q, r}, {p, q, r}i and so on. Intuitively speaking, the V -extension of an interpretation M is the collection of interpretations formed from M by freely adding or removing some
atoms
Sin V . The V -extension of a collection M of (?-)interpretations, written MV , is the collection M V .
In classical propositional logic if M corresponds to a formula , i.e. M = Mod(), then MV
corresponds to a formula whose truth value has nothing to do with the atoms in V . The intended
meaning in the case of ?-models is similar when MV corresponds to a formula under ?-model
42

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

semantics that is relevant to only the atoms not in V . In other words, suppose MV = Mod? ().
If hX, Y i |=?  then hH, T i |=?  where H (resp. T ) is obtained from X (resp. Y ) by freely
adding or removing any atoms in V whenever H  T . The following lemma shows an equivalent
condition for the semantic ?-knowledge forgetting.
Lemma 3 Let  be a formula and V  A. A formula  is a result of ?-forgetting V from , iff the
following condition holds:
Mod? () = Mod? ()V .
(15)
This condition of ?-forgetting is a generalization of the forgetting in propositional logic (Lin &
Reiter, 1994) in terms of the following corollary.
Corollary 5 A formula  is a result of forgetting a set V of atoms in a formula  iff Mod() =
Mod()V , where Mod(.) refers to classical propositional logic.
A syntactic counterpart of the forgetting in propositional logic is defined as follows (Lin, 2001;
Lang et al., 2003):
Forget(, {p}) = [p/]  [p/>],
Forget(, V  {p}) = Forget(Forget(, {p}), V )
where [p/>] (resp. [p/]) is the formula obtained from  by replacing every occurrence of the
atom p with > (resp. ).
As ?-interpretations are related to the given signature A, in what follows, we shall assume that
the signature of a formula/theory is implicitly given by the atoms occurring in the formula/theory,
unless explicitly stated otherwise. The example below illustrates how ?-forgetting results can be
computed.
Example 4 Let  be the following formula
(p  q)  (q  p)  (p  )  (q  ).
Over the signature {p, q}, we have Mod? () = {h, {p, q}i, h{p, q}, {p, q}i}. Please note here
that ? can be either HT or FLP. Then from Definition 8, we can verify that Mod? (){p} =
{h, {q}i, h{q}, {q}i}{p} . It corresponds to the formula  = (p  q  )  (p  q)   under
the ?-model semantics by Proposition 4. As a matter of fact, we have  ? q   ? q.
Note that Forget(, {p}) = [p/>]  [p/]  q and q 6? q. It shows that, unlike the
syntactic counterpart of the forgetting in classical propositional logic, the ?-forgetting results cannot
be computed via [p/>]  [p/] as Mod? (q) = {h, {q}i, h{q}, {q}i}, while Mod? (q) =
{h{q}, {q}i} (over the signature {q}).

4.1 Expressibility
Please note that Definition 8 does not guarantee the existence of the forgetting results, however the
next theorem shows that the ?-forgetting result always exists. It also implies that the ?-forgetting
result is unique (up to strong ?-equivalence).
43

fiWANG , Z HANG , Z HOU , & Z HANG

Theorem 6 (Expressibility theorem) Let  be a formula and V a set of atoms. There exists a
formula  such that Mod? () = Mod? ()V .
Here, the uniqueness up to strong ?-equivalence of the ?-forgetting result follows from the fact
that, if a formula  0 is a result ?-forgetting V from  as well then Mod? ( 0 ) = Mod? ()V =
Mod? (), which shows that  and  0 are strongly ?-equivalent under the ?-stable model semantics.
Based on the expressibility result and by abusing the denotation, we denote the forgetting result
by Forget? (, V ):
Definition 9 Let  be a formula and V  A. Forget? (, V ) is a formula  s.t Mod? () =
Mod? ()V , i.e., Forget? (, V ) is a result of ?-forgetting V from .
In this sense Forget? is an operator which maps a formula and a set of atoms to a formula. According
to Definition 8 and the expressibility theorem, the following corollary easily follows.
Corollary 7 Let ,  be formulas, V , V1 and V2 be sets of atoms.
(i) Forget? (Forget ? (, V1 ), V2 ) ? Forget? (Forget ? (, V2 ), V1 ).
(ii) If  ?  then Forget? (, V ) ? Forget? (, V ).
It firstly states that ?-forgetting is independent of the order of forgotten atoms, and secondly, the
?-forgetting preserves strong ?-equivalence of logic programs under ?-stable model semantics.
To further investigate the properties of the forgetting, we introduce a notion of irrelevance under
?-model semantics.
Definition 10 A formula  is ?-irrelevant to a set V of atoms, denoted as IR? (, V ), if there exists
a formula  mentioning no atoms from V and  ? .
Some basic properties on ?-forgetting are presented below.
Proposition 10 Let  and  be two formulas and V a set of atoms.
(i) IR? (Forget ? (, V ), V ).
(ii)  has a ?-model iff Forget? (, V ) has.
(iii)  |=? Forget? (, V ).
(iv) If  |=?  then Forget? (, V ) |=? Forget? (, V ).
(v) Forget? (  , V ) ? Forget? (, V )  Forget? (, V ).
(vi) Forget? (  , V ) |=? Forget? (, V )  Forget? (, V ).
(vii) Forget? (  , V ) ? Forget? (, V )   if IR? (, V ).
44

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

Intuitively, (i) of the Proposition says that the ?-forgetting result is irrelevant to atoms in V ,
i.e., those forgotten atoms. In this sense, the signature of ?-forgetting result can be constrained
to A \ V . The intended meaning of the others can be easily read out. E.g., item (iii) says that this
forgetting is a kind of weakening, while item (v) shows that the forgetting has a distributive property
for disjunction.
As mentioned earlier, disjunctive programs, positive programs, normal logic programs and Horn
programs are four types of special cases of (arbitrary) logic programs under our setting. Then it is
interesting to consider whether the expressibility result also holds for each of these special programs.
For instance, we would like to know whether the result of ?-forgetting in a disjunctive (positive,
normal, and Horn) logic program is still expressible by a disjunctive (resp. positive, normal, and
Horn) logic program.
As indicated by the following two examples, HT- and FLP-forgetting in disjunctive, positive and
normal logic programs is possibly not expressible in either disjunctive or positive logic programs.
For simplicity, we identify a singleton set {} as  when it is clear from its context, and thus we
denote Forget? (, {p}) as Forget? (, p), and IR? (, {p}) as IR? (, p), and M{p} as Mp etc..
Example 5 Consider the following normal logic program  over signature {p, q}:
(p  q)  (q  p)  (p  q  ).
We have that Mod? () = {h{p}, {p}i, h{q}, {q}i} and
Mod? ()p = {h, i, h{q}, {q}i}{p} .
Here h{p}, {p}i{p} = h, i{p} . It implies that Forget? (, p) ? q  q. It can be easily seen that
q  q cannot be expressed as a disjunctive logic program because Mod? ()p does not satisfy (9).
Hence Forget? (, p) cannot be expressed by a normal logic program.
Please note that q  q HT q  q. Thus q  q is also a result of HT-forgetting p from .
However, q  q is not a result of FLP-forgetting p from  as q  q FLP > 6FLP q  q. 
Example 6 Let  be a positive logic program over signature {p, q, r} as follows:
(p  q  r)  (p  q  r)  (p  r  q)  (q  r  p).
It is not difficult to verify that, over the signature {p, r}, Mod? (){q} consists of
h, i, h, {p, r}i, h{p}, {p}i, h{p}, {p, r}i, h{r}, {r}i, h{r}, {p, r}i, h{p, r}, {p, r}i.
Clearly it does not satisfy the condition (9). Hence it can not captured by a disjunctive logic program. As a matter of fact, we have the following
Forget HT (, q) HT HT (, {p})  HT (, {r}) = (r  p  p)  (p  r  r),
Forget FLP (, q) FLP FLP (, {p})  FLP (, {r}) = (r  p  r  p)  (p  p  r  r)
in terms of Proposition 4. Interestingly, this example also shows that, though a logic program may
have the same HT-models as FLP-models, its HT-forgetting result may be different from its FLPforgetting result.

45

fiWANG , Z HANG , Z HOU , & Z HANG

The HT- and FLP-forgetting in Horn logic programs is of special interest, because unlike disjunctive, positive and normal logic programs, the result of HT- and FLP-forgetting result in a Horn
logic program is always expressible by a Horn logic program, as we show below.
Theorem 8 (Horn expressibility) Let  be a Horn logic program and V  A. There is a Horn
logic program 0 such that Forget? (, V ) ? 0 .
Having obtained the model-theoretical characterization of the classes of disjunctive and normal
logic programs respectively, we can easily derive a sufficient and necessary condition for HT- and
FLP-forgetting results to remain in the same class, i.e., the result of HT- and FLP-forgetting a set of
atoms in a disjunctive (resp. normal) logic program is a disjunctive (resp. normal) logic program.
Proposition 11 Let  be a disjunctive logic program, V  A. We have that Forget? (, V ) is
expressible in disjunctive logic programs if and only if,
hH1 , T1 i |=? , hT2 , T2 i |=?  and T1  T2  hH3 , T3 i |=?  such that hH3 , T3 i V hH1 , T2 i.
Proposition 12 Let  be a normal logic program, V  A. Then Forget? (, V ) is expressible in
normal logic programs if and only if, in addition to condition (16), the following condition holds,
hH1 , T1 i |=? , hH2 , T2 i |=?  and T1 V T2
 hH3 , T3 i |=?  such that H3 V H1  H2 and (T3 V T1 or T3 V T2 ).

(16)

4.2 Forgetting Postulates
Zhang and Zhou (2009) proposed four forgetting postulates in their work of knowledge forgetting,
and showed that their knowledge forgetting can be precisely characterized by the four postulates.
They further argued that these postulates should be viewed as a general semantic characterization
for knowledge forgetting in other logics. Indeed, the classical propositional forgetting can be also
characterized by these postulates. In terms of forgetting in logic programs, as we addressed in the
introduction, imposing these postulates is not feasible for existing approaches. In the following,
we show that ?-forgetting is exactly captured by these postulates, which we think is one major
advantage over other logic program forgetting approaches.
The notion of forgetting is closely related to that of uniform interpolation property (Visser, 1996;
Goranko & Otto, 2007), for instance, the forgetting in description logics (Lutz & Wolter, 2011) and
the semantic forgetting in logic programs (Gabbay, Pearce, & Valverde, 2011). The following
corollary follows from Theorem 6, which actually implies the uniform interpolation property of the
logics under ?-model semantics. Namely, for any formulas  and  with  |=? , there exists a
formula  such that  |=? ,  |=?  and  contains only the atoms occurring in both  and . The
formula  is called a uniform interpolant of  and . This is stated as:
Corollary 9 Let  and  be two formulas, V a set of atoms and IR? (, V ).
 |=? 

iff

Forget? (, V ) |=? .

Let  and  be two formulas and V a set of atoms. The following are Zhang-Zhous four
postulates for logic programs under ?-model semantics.
46

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

(W) Weakening:  |=? .
(PP) Positive persistence: if IR? (, V ) and  |=?  then  |=? .
(NP) Negative persistence: if IR? (, V ) and  6|=?  then  6|=? .
(IR) Irrelevance: IR? (, V ).
By specifying  ? Forget? (, V ), (W), (PP), (NP) and (IR) are called postulates for knowledge
forgetting in logic programs under ?-stable model semantics. Viz,  is a result of ?-forgetting V
in . Based on the uniform interpolation property (cf. Corollary 9), we can show the following
representation theorem.
Theorem 10 (Representation theorem) Let  and  be two formulas and V a set of atoms. Then
the following statements are equivalent:
(i)  ? Forget? (, V ).
(ii)  ? {0 |  |=? 0 and IR? (0 , V )}.
(iii) Postulates (W), (PP), (NP) and (IR) hold.
This theorem justifies that the knowledge forgetting (cf. Definition 7) exists and is unique up to
strong equivalence.
An obvious consequence follows from the representation theorem is that
Forget? (, V ) ? { |  |=?  and IR? (, V )}.
It says that the result of ?-forgetting V from  consists of the ?-logical consequence of  that
is ?-irrelevant to V . For this reason the forgetting is a knowledge forgetting of logic programs
under stable models semantics. As we have mentioned in the introduction that none of the other
forgetting approaches in logic programs is a knowledge forgetting since it does not satisfy some of
the postulates (see Section 5 for details).
One should note that the representation theorem is applicable for the forgetting in classical
propositional logic, viz, Forget(, V )  { |  |=  and IR(, V )}.
4.3 Relating to Propositional Forgetting
It has been shown that strong equivalence of logic programs may be related to the equivalence of
propositional logic (Pearce et al., 2001; Lin, 2002). As the ?-forgetting preserves strong equivalence
of logic programs under ?-stable model semantics, it is worth exploring further connections between
?-forgetting and the forgetting in propositional logic. In this section, we undertake an in-depth
investigation on this aspect.
We first provide a direct connection between ?-forgetting and propositional forgetting via the
following proposition.
Proposition 13 Let , 0 ,  be formulas and V  A such that  ? Forget? (, V ) and 0 
Forget(, V ). Then
(i)   0 .
47

fiWANG , Z HANG , Z HOU , & Z HANG

(ii) 0 |=? .
The result (i) in Proposition 13 simply says that the result of ?-forgetting and classical propositional forgetting are equivalent in classical propositional logic. Thus the forgetting in classic propositional logic can be computed by a ?-forgetting in logic programs. However as we have seen in
Example 4, Forget? (, V ) is possibly not ?-equivalent to Forget(, V ). The reverse of (ii) does not
hold generally. For instance, Forget? (p, q) ? p, while Forget(p, q)  p, and evidently
p 6|=? p. From this result and Theorem 8, we immediately have the following corollary.
Corollary 11 Let  be a Horn logic program and V a set of atoms. Then Forget(, V ) is expressible by a Horn logic program.
The following result states that, for Horn logic programs, ?-forgetting and the forgetting of
propositional logic are strongly ?-equivalent. Thus it provides a method of computing ?-forgetting
results of Horn logic programs through the propositional forgetting.
Proposition 14 Let  and 0 be two Horn logic programs, and V a set of atoms such that 0 
Forget(, V ). Then 0 ? Forget? (, V ).
The following proposition states that the ?-forgetting of double negative formulas is closely
connected with the classical propositional forgetting, which will be used to prove some complexity
results later.
Proposition 15 Let  and  be two formulas and V a set of atoms.
(i)   Forget(, V ) iff  ? Forget? (, V ).
(ii) Forget(, V )  Forget(, V ) iff Forget? (, V ) ? Forget? (, V ).
As it is known that the strong equivalence of logic programs is closed related to the equivalence
in propositional logic by translating logic programs into propositional theories (Pearce et al., 2001;
Lin, 2002). This motivates us to investigate the connection between the forgettings in the view of
the translations. Now our main result of this section is stated as follows.
Theorem 12 (?-forgetting vs propositional forgetting) Let  and  be two formulas of LA and
V  A. Then
 ? Forget? (, V ) iff (A) |= ? []  Forget((A)  {? []}, V  V 0 ).
By Theorem 12, we know that to check whether a formula  is a result of ?-forgetting a set
V of atoms from a formula , it is equivalent to check whether ? [] is classically equivalent
to Forget((A)  {? []}, V  V 0 ) under the theory (A). The following example shows an
application of this theorem.
Example 7 [Example 5 continued] Recall that  is the following formula:
(p  q)  (q  p)  (p  q  )
48

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

and Forget? (, p) ? q  q. Over the signature {p, q}, (A) = (p  p0 )  (q  q 0 ) and, the
program translation yields:
? ()  (p0  q)  (p0  q 0 )  (q 0  p)  (q 0  p0 )  (p0  q 0 ).
Now we have that Forget(? []  (A), {p, p0 }) is equivalent to:
(q  q 0 )  (q  q 0 ),

i.e.

(q 0  q)  (q  q 0 )

which is equivalent to q 0  q under the theory ({q}) = {q  q 0 }. One can further check that
? [q  q] = q 0  q  q  q 0  q (under the theory ({q})). Thus the formula q  q is a result
of ?-forgetting p from  by Theorem 12.

The following example further shows that (A) occurring in Forget? ({ []}  (A), V  V 0 )
is necessary for Theorem 12.
Example 8 [Continued from Example 6] Recall that A = {p, q, r}, (A) = {p  p0 , q  q 0 , r 
r 0 } and  consists of
(p  q  r)  (p  q  r)  (p  r  q)  (q  r  p).
We have that,
HT []    ,
(A) |= ? []    ,
FLP []  (p  q  r)  (p  q  p0  q 0  r)  (p  r  p0  r 0  q)  (q  r  q 0  r 0  p)  
where  = (p0  q 0  r 0 )  (p0  r 0  q 0 )  (q 0  r 0  p0 ).
One can check that
Forget(HT [], {q, q 0 })  >,
(A) |= Forget(FLP [], {q, q 0 })  >.
Recall that the formula 1 = (r  p  p)  (p  r  r) is a result of HT-forgetting q from ;
and 2 = (r  p  r  p)  (p  p  r  r) is a result of FLP-forgetting q from . We have
that
HT [1 ]  01  (r  r 0  p  p  p0 )  (p  p0  r  r  r 0 ),
FLP [2 ]  02  (r  r 0  p  r  p0 )  (p  p0  p  r  r 0 ).
Under the theory (A), we have
(A) |= HT [1 ]  (p0  p  r 0 )  (r 0  r  p0 ),
(A) |= HT [1 ]  (p0  p  r 0 )  (r 0  r  p0 ).
One can verify further that the model {p0 } of (A) is not a model of HT [1 ], nor it is a model of
FLP [2 ], i.e. (A) 6|= HT [1 ]  > and (A) 6|= FLP [2 ]  >. Actually, we have that,
(A) |= Forget({? []}  (A), {q, q 0 })  ((p0  r 0 )  (p  r)  (p0  r 0 )).
One can check further that
(A) |= (p0  p  r 0 )  (r 0  r  p0 )  ((p0  r 0 )  (p  r)  (p0  r 0 )),
which shows that 1 (resp. 2 ) is a result of HT-forgetting (resp.
49

FLP-forgetting)

q from .



fiWANG , Z HANG , Z HOU , & Z HANG

The following result states that we can reduce checking whether the ?-forgetting results of two
formulas are strongly ?-equivalent to checking whether the propositional forgetting results of corresponding two formulas are equivalent.
Proposition 16 Let  and  be two formulas of LA and V a set of atoms. Then Forget? (, V ) ?
Forget? (, V ) iff the following condition holds:
Forget({? []}  (A), V  V 0 )  Forget({? []}  (A), V  V 0 ).
4.4 Computation and Complexity
Theorem 6 and Propositions 4 and 10 imply a naive approach to compute ?-forgetting results. Formally speaking, given a formula  over a signature A and a set V of atoms, Forget? (, V ) can be
computed as follows:
(Step 1) Evaluating all ?-models of , denoted by M.
(Step 2) Restrict M to A \ V , denoted by M|V , i.e.
M|V = {hH \ V, T \ V i|hH, T i  M}.
(Step 3) Enumerating the following formulas (over the signature A \ V ) from M|V :
 ? (X, Y ) if hX, Y i 
/ M|V but hY, Y i  M|V ,
 (Y, Y ) if hY, Y i 
/ M|V .
(Step 4) Finally, conjunct all the constructed formulas, denoted by .
Corollary 13 Let , V and  be given as above. Then  ? Forget? (, V ).
Alternatively, in terms of Theorem 10, we can compute Forget? (, V ) by enumerating the ?consequences of  that are ?-irrelevant to V . As there exist sound and complete axiomatic systems
for the HT-logic (Jongh & Hendriks, 2003), checking HT-consequence relation is axiomatically
doable. Though a sound and complete axiomatic system for FLP-logic is recently unknown, we still
can enumerate all the formulas of form (2) over the signature A \ V and check if they are FLPconsequence of . Nevertheless, it is also observed that from a computational viewpoint, like the
propositional forgetting, each of the above two approaches would be expensive. This appears to be
inevitable in terms of the following complexity results, unless the complexity hierarchy collapses.
Theorem 14 Let  and  be two formulas and V a set of atoms.
(i) The problem of deciding if  ? Forget? (, V ) is co-NP-complete.
(ii) The problem of deciding if Forget? (, V ) ? Forget? (, V ) is P2 -complete.
(iii) The problem of deciding if  ? Forget? (, V ) is P2 -complete.
50

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

According to our representation theorem (i.e. Theorem 10), the result (i) in Theorem 14 means
that checking if  is ?-irrelevant to V , i.e. IR? (, V ), is intractable. The result (ii) of Theorem 14,
on the other hand, presents the complexity of ?-forgetting equivalence checking, i.e., if two formulas
are strongly ?-equivalent when they are restricted to a common signatures. The last result (iii) of
Theorem 14 states that checking if a formula is a result of ?-forgetting is generally difficult.
Proposition 17 Let  and  be two formulas and V a set of atoms.
(i) The problem of deciding whether  |=? Forget? (, V ) is P2 -complete.
(ii) The problem of deciding whether Forget? (, V ) |=?  is co-NP-complete.
Theorem 14 and Proposition 17 tell us that for ?-forgetting, in general the complexity of resulting checking and inference problems is located at the same level of the complexity polynomial
hierarchy as the propositional forgetting.
4.5 Conflict Solving Based on Knowledge Forgetting
In the following, we consider the application of the proposed forgetting in conflict solving for logic
program contexts, that represent a knowledge system consisting of knowledge bases of multiple
agents.
Definition 11 A logic program context is an n-ary tuple  = (1 , . . . , n ) where i is a consistent
logic program.  is ?-conflict-free if 1      n is consistent under ?-stable model semantics.
Definition 12 Let  = (1 , . . . , n ) be a logic program context. A ?-solution of  is a minimal
subset S of A such that (Forget? (1 , S), . . . , Forget? (n , S)) is ?-conflict-free, where A is the
underlying signature.
It is obvious that  is a ?-solution of ?-conflict-free logic program context .
We consider the following simplified Zhang and Foos conflict solving scenario (cf., see Zhang
& Foo, 2006, Ex. 6).
Example 9 A couple John and Mary are discussing their family investment plan. There are four
different shares shareA, shareB, shareC and shareD, where shareA and shareB are of high risk
but also have high return; shareC and shareD are of low risk and may be suitable for a long term
investment. Johns and Marys investment preference over these shares are encoded as the following
logic programs J and m respectively:
J :

M :

r1 :sA  not sB,

r10 :sC ,

r2 :sC  not sD,

r20 :sD ,

r3 :sD  not sC,

r30 :sB  not sA, not sC,

r4 :  sC, sD,
r40 :  sA, sB,
where s# stands for share#. The intuitive meaning of these rules can be easily read out. E.g. rule r1
says that John wants to buy shareA if he dont buy shareB, while rules r2 , r3 and r4 mean that John
wants to buy shareC or shareD, but not both of them.
51

fiWANG , Z HANG , Z HOU , & Z HANG

As one can see that J  M has no ?-stable model due to the confliction between rule r4 and
r10 , r20 , the logic program context  = (J , M ) is not ?-conflict-free.
For S = {sD}, we have the following
Forget HT (J , S) HT {sA  not sB,
Forget HT (M , S) HT {sC ,

sC; not sC },

sB  not sA, not sC,

 sA, sB}.

One can check that Forget HT (J , S)  Forget HT (M , S) has a unique HT-stable model {sA, sC}.
Thus S is an HT-solution of . It can be said that John and Mary may have an agreement on
their investment plan about shares shareA, shareB and shareC if they agree to give up the belief
(knowledge) about shareD. It results in an investment to shares shareA and shareC, but not to
shareB.
One can further check that, under the FLP-stable model semantics, if John and Mary can give up
the belief about shareD then it results in the same investment plan to shares shareA and shareC, but
not to share shareB. The reason is that Forget FLP (J , S)Forget FLP (M , S) has a unique FLP-stable
model {sA, sC}.

5. Related Work
In this section we compare the ?-forgetting with weak and strong forgetting (Zhang & Foo, 2006),
semantic forgetting (Eiter & Wang, 2008) and the forgetting operators FS and FW (Wong, 2009).
5.1 Weak and Strong Forgetting
Let  be a normal logic program and p a propositional atom. The reduction of  with respect to p,
denoted by Red(, {p}), is the normal logic program obtained from  by
(1) for each rule r of  with p  Head(r), if there is a rule r 0 in  such that p  Body+ (r 0 ), then
replacing r 0 with
Head(r 0 )  Body(r), Body(r 0 ) \ {p}.
(2) if there is such a rule r 0 in  and it has been replaced by a new rule in the previous step, then
removing the rule r from the remaining normal logic program.
Let X be a set of propositional atoms. Then the reduction of  with respect to X is inductively
defined as follows:
Red(, ) = ,
Red(, X  {p}) = Red(Red(, {p}), X).
The strong forgetting p in a normal logic program  is the normal logic program SForget(, {p})
obtained from Red(, {p}) by removing each rule r if either r is valid 5 or p  Head(r) 
Body+ (r)  Body (r). The weak forgetting p in  is the normal logic program WForget(, {p})
obtained from Red(, {p}) by firstly removing each rule r if either r is valid, or p  Head(r) 
Body+ (r) and then removing not p from the remaining rules.
5. A rule r is valid if Head(r)  Body+ (r) 6=  or Body+ (r)  Body (r) 6= .

52

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

Let X be a set of atoms. The strong (and weak) forgetting X in  is recursively defined as
SForget(, ) = ;

WForget(, ) = ;

SForget(, X  {p}) = SForget(SForget(, {p}), X);
WForget(, X  {p}) = WForget(WForget(, {p}), X).
It is known that the two forgetting operators are independent of the ordering of forgotten atoms in
the sense of strong HT-equivalence of logic programs under HT-stable model semantics (cf., see
Zhang & Foo, 2006, Prop. 2).
Example 10 Consider the below two normal logic programs:
 = {p  q,

q  p,

r  not p},

 = {p  q,

q  p,

r  not q}.

One can check that  and  are strongly equivalent. We have that
SForget(, {p}) = ,

WForget(, {p}) = {r },

SForget(, {p}) = WForget(, {p}) = {r  not q}.
The example shows that neither weak forgetting preserves strong equivalence, nor is strong forgetting. One can further verify that  |=? q  r   and  6|=? r for ?  {HT, FLP }. Thus
the strong forgetting does not satisfy positive persistence, and the weak forgetting does not satisfy weakening and negative persistence. Actually, for HT- and FLP-forgetting, we have the
following
Forget HT (, p) HT Forget HT (, p) HT {q  r  },
Forget FLP (, p) FLP Forget FLP (, p) FLP {q  r  }.
Here  FLP  follows from the fact that  HT  and Proposition 2.



5.2 Semantic Forgetting
Having addressed certain issues of weak and strong forgetting, Eiter and Wang (2008) proposed
a semantic forgetting for consistent disjunctive logic programs. Formally speaking, let  be a
consistent disjunctive logic program and p an atom. A set M of atoms is a p-stable model of  iff
M is a stable model of  and there is no stable model M  of  such that M  \ {p}  M \ {p}. A
disjunctive logic program 0 represents the result of forgetting about p in , if
 0 does not mention the atom p, and
 a set M 0 of atoms is a stable model of 0 iff  has a p-stable model M such that M 0 p M .
In terms of the above definition, such forgetting results are not unique under strong equivalence.
This means, their forgetting does not preserve strong equivalence. To compute the result of forgetting an atom in a consistent disjunctive logic program, they proposed three algorithms forget1 ,
forget2 and forget3 (Eiter & Wang, 2008). The example below further demonstrates the difference
between this semantic forgetting and the ?-forgetting.
53

fiWANG , Z HANG , Z HOU , & Z HANG

Example 11 Let  = {p  q} be a program over signature A = {p, q, r}. Although program  has nothing to do with the atom r, we have that forgeti (, r) =  (i = 1, 2, 3), which
seems not intuitive as it loses some information irrelevant to what we want to forget. However
Forget? (, r) ? .

This example also shows that the semantic forgetting does not satisfy positive persistence
postulate as  |=? q  p, which is lost in the semantic forgetting result forgeti (, r) for i = 1, 2, 3.
5.3 Forgetting Operators FS and FW
Wong (2009) developed his forgetting for disjunctive logic programs. Differently from the work
of Zhang and Foo (2006), and Eiter and Wang (2008), Wongs forgetting is defined based on the
HT-logic. In this sense, his approach probably shares a common logic ground with HT-forgetting.
Wong also defined two forgetting operators FS and FW , which correspond to two series of program
transformations. See Appendix D for the detailed definitions.
The interesting feature of Wongs forgetting is that it preserves strong equivalence. However,
a major issue with this forgetting is that: on one hand, the forgetting FS may cause unnecessary
information loss; on the other hand, the forgetting FW may also introduce extra information that
one does not want, as illustrated by the following example.
Example 12 Let us consider the normal logic program  consisting of:
a  x,

y  a, not z,

q  not p,

p  not q,

 p, q.

Then we have:
FS (, {a, p}) HT {y  x, not z},
FW (, {a, p}) HT {y  x, not z,

 x,

Forget HT (, {a, p}) HT {y  x, not z,
Forget FLP (, {a, p}) FLP {y  x, not z,

q },
q  not not q},
q  not not q}.

Since  |=HT {q  not not q}, which is irrelevant to atoms a and p, it seems to us that forgetting
{a, p} from  should not affect this fact. But FS (, {a, p}) 6|=HT {q  not not q}. In this sense,
we see that FS has lost some information that we wish to keep. This shows that the operator FS does
not satisfy positive persistence postulate.
On the other hand, from the fact that  6|=HT q but FW (, {a, p}) |=HT q, it appears that FW may
introduce unnecessary information, which indeed conflicts our intuition of program weakening via
forgetting, i.e., it does not satisfy the weakening postulate.

As we mentioned in the introduction, the following example confirms that an expected result
can not be obtained from either one of the above three forgetting approaches.
Example 13 [Continued from Example 5] For the normal logic program :
(p  q)  (q  p)  (p  q  ),
54

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

we have the following:
SForget(, {p}) = forget1 (, {p}) = FS (, {p}) = ,
WForget(, {p}) = FW (, {p}) = {q}.
Here, the expected logic program that represents the same information of  when the auxiliary atom
p is ignored should be q  q.


6. Concluding Remarks
In this paper two semantic knowledge forgetting approaches, called HT- and FLP-forgetting respectively, were proposed for logic programs under stable model and FLP-stable model semantics respectively. These knowledge forgetting results can be captured by the corresponding logical consequence of forgotten logic programs that are irrelevant to forgotten atoms. It consequently preserves
strong equivalence of logic programs under HT- and FLP-stable model semantics respectively. This
is a major advantage when compared to other existing forgetting approaches in logic programming.
As a starting point, we investigated the model theoretical characterization of logic programs under HT- and FLP-stable model semantics, and studied their respective strong equivalence problems
using classical propositional logic equivalence. Many properties of forgetting have been explored,
such as existence of forgetting results, a representation theorem, and the complexity of some decision problems related to these forgettings. We also considered an application of knowledge forgetting in conflict solving.
Although we have presented abstract approaches to computing the forgetting results and we
showed the underlying difficulties of the computation, it is valuable to study practical algorithms
for different subclasses of logic programs. Another challenging future work is to extend the knowledge forgetting to other nonmonotonic systems, and in particular first-order logic programs (Ferraris
et al., 2011). As we have mentioned in the introduction that forgetting can be effectively used to
solve some confliction, e.g. the strong and weak forgetting (Zhang & Foo, 2006) and the propositional forgetting (Lang & Marquis, 2010), such an application of knowledge forgetting deserves
further studying.
As what we concentrate upon in this paper is knowledge forgetting in logic programs, which is
based on the notion of strong equivalence, an interesting work is to consider forgetting under the
stable model semantics of logic programs along the work (Wang et al., 2013). Last but not least,
logic programs under supported model semantics enjoys some similar properties as that of logic
programs under HT- and FLP-stable models semantics (Truszczynski, 2010), we will consider the
knowledge forgetting for logic programs under the supported model semantics in another paper.

Acknowledgments
We thank Mirek Truszczynski for encouraging us to consider knowledge forgetting for logic programs under the FLP-stable model semantics. We thank the anonymous reviewers for their insightful comments, and Robin Bianchi for his help on formatting the paper. Yisong Wang is partially
supported by the National Natural Science Foundation of China grant 61370161 and Stadholder
Foundation of Guizhou Province under grant (2012)62.
55

fiWANG , Z HANG , Z HOU , & Z HANG

Appendix A. Proofs for Section 2
Proposition 1 Let A, B, C, D be set of atoms. We have the following
V
W
V
W
(i) (A  B)  (D  C) HT (A  B  C)  D.
V
W
V
W
(ii) (A  B)  (D  C) |=FLP (A  B  C)  D.

V
W
Proof:
(ii)
Suppose
hX,
Y
i
is
an
FLP-model of (A  B)  (D  C) but not an FLP-model
V
W
of (A  B  C)  D. It follows that the following conditions hold:
V
V
(a) X |= (A  B  C), which implies X |= (A  B).
V
V
V
(b) Y |= (A  B  C), which implies Y |= (A  B)  C, and
W
W
(c) hX, Y i 6|=FLP D, i.e. X 6|= D.
W
W
W
The conditions (a) and (b) show that hX, Y i |=FLP (D  C), i.e. X |= D or Y |= C.
Together with the conditions (b) and (c), a contradiction follows.


Appendix B. Proofs for Section 3
Proposition 4 A collection M of ?-interpretations is ?-expressible iff
hX, Y i  M implies hY, Y i  M.

(17)

Actually, if M satisfy condition (17) then the following logic program
? = {? (X, Y )|hX, Y i 
/ M and hY, Y i  M}  {(Y, Y )|hY, Y i 
/ M}
captures M in the sense that Mod? (? ) = M.
Proof: The direction from left to right follows from (i) of Proposition 3. We prove the other
direction. Let ? be the propositional theory consisting of, for every X  Y  A,
 ? (X, Y ) if hX, Y i 
/ M and hY, Y i  M, and
 (Y, Y ) if hY, Y i 
/ M.
By Lemma 1, Mod? (? ) = M.



Lemma
V
W2 Let A, B beVtwo sets
W of atoms, and X  Y  A. hX, Y i |=?
B  A and Y |= B  A.

V

B 

W

A iff X |=

Proof: According to (iii) of Proposition 3 and Proposition 2, it is sufficient to show that, for the
case ? = HT,
^
_
^
_
^
_
X |= ( B 
A)Y iff X |=
B
A and Y |=
B
A.

V
W
V
W Y
Note that Y |= B  A and X |= ( B)Y implies X
V () W
V |= ( A) . Suppose X 6|=
B  A, i.e. B  X and A  X = . It follows that Y |= B due to B  Y , and then
56

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

W
V
V
V
W
Y |= A,Wi.e. A  Y 6= . Thus we have X |= ( B)Y since ( B)Y = B. By X |= ( A)Y
i.e. X |= A, we have X  A 6= , a contradiction.
V
W
V
W
()VWe need only to show
X |= ( B)Y  ( A)Y since Y |=
B 
A. Suppose
W
X |=V( B)YW and X 6|= ( A)Y . The former implies B  X  Y , thus X  A 6=  by
X |= B  A. The latter implies X  (A  Y ) = , which means X  A =  since X  Y , a
contradiction.

Proposition 5 A set M of ?-interpretations is positively ?-expressible, i.e., there is a positive logic
program  s.t Mod? () = M, iff M satisfies the criteria:
hX, Y i  M iff X  Y, hX, Xi  M and hY, Y i  M.

(18)

Actually, if M satisfy condition (18) then the following logic program
^
_
X|hX, Xi 
/ M}
? = { X 

captures M in the sense that Mod? (? ) = M.

Proof: It suffices to prove the case ? = HT by Proposition 2.
() Let  be a positive logic program whose HT-models are exact the ones in M. For every
HT-interpretation hX, Y i, by Lemma 2, hX, Y i |= HT  iff X  Y , X |=  i.e. hX, Xi |= HT ,
and hY, Y i |=HT  i.e. Y |=  since every rule of  is positive. The condition (18) follows.
() Let N = {X  A|hX, Xi  M}. We construct the propositional theory  consisting of
^
_
X
X

for every X  N (= 2A \ N ).
Firstly we show
V Mod()
W = N . Suppose X |=  and X 6 N . We have that X  N . It follows
that X 6|=  as X  X belongs to . On the
V other hand,
W suppose X  N and X 6|= . It
follows that there exists X 0  N such that X 6|= X 0  X 0 , i.e., X 0  X and X  X 0 = ,
from which we have X = X 0 thus X  N , a contradiction.
Secondly we show ModHT () = M. On the one hand, let hX, Y i |=HT . We have that X |= 
and Y |=  by Lemma 2. It follows X, Y  N , which implies hX, Xi  M and hY, Y i  M.
Thus hX, Y i  M by (18). On the other hand, let hX, Y i  M. In terms of (18), we have
hX, Xi  M and hY, Y i  M. Thus X  N and Y  N , i.e. X |=  and Y |= . Thus
hX, Y i |=HT  by Lemma 2.

Proposition 6 A collection M of ?-interpretations is Horn ?-expressible, i.e., there is a Horn logic
program  such that Mod? () = M, iff M satisfies, in addition to (10), the following criteria:
hX, Y i  M and hH, T i  M  hX  H, Y  T i  M.

(19)

Proof: It suffices to prove the case ? = HT by Proposition 2.
() Suppose  is a Horn logic program such that ModHT () = M. By Proposition 5,
ModHT () satisfies (18). Suppose hX, Y i and hH, T i are two HT-models of . It follows that
X, Y, H and T are models of  by Lemma 2. Thus X  H |=  and Y  T |= , by which
hX  H, Y  T i |=  due to X  H  Y  T .
57

fiWANG , Z HANG , Z HOU , & Z HANG

() Let N and  be the ones defined in the proof of Proposition 5. If X, Y  N then X  Y 
N according to (19). It follows that there exists a Horn logic program (a set of Horn clauses) whose
0
models are exactly the
Vones inWN . As a matter of fact, the Horn program  can be constructed from
 by replacing each X  Y with
^
^
X  p1 , . . . , X  pk
(20)

T
where X  Y =  and {Y 0 \ X|X  Y 0 and Y 0  N } = {p1 , . . . , pk }.
We firstly show   0 by proving


^
_ 
^
^
X
Y  X
 |=
pi 
1ik

V
W
where pi (1  i  k) are defined in (20). The direction from right to left is trivial as V X  W Y
belongs to V
. Let us consider the other direction. Suppose H |= , H is a model of X  Y
and H 6|= X  pi for some i (1  i  k). We have that X  H and H  Y 6= . It follows
that H is some element of {Y 0 \ X|X  Y 0 and Y 0  N } and then {p1 , . . . , pk }  H. It is a
contradiction.
Finally ModHT (0 ) = M follows from ModHT () = M and Proposition 5.

V
W
Proposition 7 Let  = (B  C)  (A  D), where A, B, C, D are subsets of A. Then we
have (A) |= FLP []  HT [].
Proof: Note that HT [p] = p  p0 and FLP [p] = p0 . We have
HT [] = 0 

^

^

B

(c  c0 ) 

cC

_

A

_

!

(d  d0 ) ,

dD

^

_
FLP [] = 0 
(B  C  B 0  C 0 )  (A  D 0 ) .

Since (A) |= p  p0  p0 , we have that
0

(A) |= HT []   

!
^
_
_
0
0
d ,
(B  C ) 
A
dD

0

(A) |= FLP []   
It completes the proof.

^


_
(B  C )  (A  D 0 ) .
0



Proposition 8 Let  be a formula of LA and X  Y  A. hX, Y i is a ?-model of  iff X  Y 0 is
a model of (A)  {? []}.
Proof: We prove the case ? =

FLP

by induction on the structures of . Let X  Y  A.

  = p or  = . It is trivial for  = . On the other hand, hX, Y i |=FLP p iff X |= p iff
X  Y 0 |= p.
58

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

  = 1  2 where   {, }. It follows from the inductive assumption.
  = 1  2 . We have FLP [1  2 ] = (01  02 )  (1  01  FLP [2 ]). Recall that
hX, Y i |=FLP 1  2 iff
 Y |= (1  2 ) and,
 either (a) X 6|= 1 , or (b) Y 6|= 1 , or (c) hX, Y i |=FLP 2 .
Note that
 Y |= (1  2 ) iff Y 0 |= 01  02 iff X  Y 0 |= 01  02 , and
 (a) X 6|= 1 iff X  Y 0 6|= 1 , (b) Y 6|= 1 iff Y 0 6|= 01 iff X  Y 0 6|= 01 , and (c)
hX, Y i |=FLP 2 iff X  Y 0 |= FLP [2 ] by the inductive assumption.
It follows that hX, Y i |=FLP 1  2 iff X  Y 0 |= FLP [1  2 ].


This completes the proof.

Theorem 4 Two formulas  and  have the same ?-models (over LA ) iff (A)  {? []} and
(A)  {? []} have the same models (over LAA0 ).
Proof: We prove the case ? = FLP .
() M |= (A)  {FLP []}
iff MA  MA0 |= (A)  {FLP []}
0

 i |=
iff hMA , MA
0
FLP  by Proposition 8, here MA0 = {p|p  MA0 }

iff hMA , MA0 i |=FLP  since  FLP 
iff MA  MA0 |= (A)  {FLP []} by Proposition 8
iff M |= (A)  {FLP []}.
() hX, Y i |=FLP 
iff X  Y 0 |= (A)  {FLP []} by Proposition 8, here Y 0 = {p0 |p  Y }
iff X  Y 0 |= (A)  {FLP []} since (A)  {FLP []}  (A)  {FLP []}
iff hX, Y i |=FLP  by Proposition 8.



Proposition 9 (i) The problem of deciding if a formula is ?-satisfiable is NP-complete.
(ii) The problem of deciding if two formulas are ?-equivalent is co-NP-complete.
Proof: (i) Membership. If a formula  is FLP-satisfiable then there exists an FLP-interpretation
hH, T i such that hH, T i |=FLP . It is feasible to guess such an FLP-interpretation and check the
condition hH, T i |=FLP . Thus the problem is in NP.
Hardness. It follows from the fact that  is FLP-satisfiable iff  is satisfiable, which is NPhard, by (ii) of Proposition 3. This shows that the problem is NP-hard.
(ii) Membership. If  6FLP  then there exists hH, T i such that, either
(a) hH, T i |=FLP  and hH, T i 6|=FLP , or
(b) hH, T i 6|=FLP  and hH, T i |=FLP .
59

fiWANG , Z HANG , Z HOU , & Z HANG

To guess such an FLP-interpretation hH, T i and to check the conditions (a) and (b) are feasible in
polynomial time in the size of  and . Thus the problem in co-NP.
Hardness. We have that  FLP 
iff  has no FLP-model
iff  has no model by (ii) of Proposition 3
iff  is valid, which is co-NP-hard. Thus the problem is co-NP-hard.


Appendix C. Proofs for Section 4
Lemma 3 Let  be a formula and V  A. A formula  is a result of ?-forgetting V from , iff the
following condition holds:
Mod? () = Mod? ()V .
Proof:  is a result of ?-knowledge forgetting V from 
iff, for every ?-interpretation M , M |=?  iff there exists M 0 |=?  s.t. M V M 0
iff Mod? () = {M is an ?-interpretation | M 0 |=?  and M V M 0 }
iff Mod? () = Mod()V .



Lemma 4 Let X, Y, H, T and V be subsets of A.
(i) If X V H and Y V T then X  Y V H  T and X  Y V H  T .
(ii) If X V H and Y 0 V 0 T 0 then H  T 0 V V 0 X  Y 0 .
Proof: (i) Note that (X  Y ) \ V
=(X \ V )  (Y \ V )
=(H \ V )  (T \ V ) due to X V H and Y V T
=(H  T ) \ V .
Thus X  Y V T  T . We can similarly prove X  Y V H  T .
(ii) Please note that Y 0 = {p0 |p  Y }, V 0 = {p0 |p  V } and T 0 = {p0 |p  V }. We have that
(H  T 0 ) \ (V  V 0 ).
= (H \ (V  V 0 ))  (T 0 \ (V  V 0 ))
= (H \ V )  (T 0 \ V 0 ) since H  V 0 =  and T 0  V = 
= (X \ V )  (Y 0 \ V 0 ) since H V H and T 0 V 0 Y 0
= (X \ (V  V 0 ))  (Y 0 \ (V  V 0 )) since X  V 0 =  and Y 0  V = 
= (X  Y 0 ) \ (V  V 0 ).

It follows that H  T 0 V V 0 X  Y 0 .
Theorem 6 (Expressibility theorem) Let  be a formula and V a set of atoms. There exists a
formula  such that Mod? () = Mod? ()V .
Proof: For every hX, Y i  Mod? ()V , there exists hH, T i |=?  such that hH, T i V hX, Y i,
i.e. X V H and Y V T . By (i) of Proposition 3, hT, T i |=? . Thus hY, Y i  Mod? ()V
due to hY, Y i V hT, T i. It follows that the collection Mod? ()V satisfies the condition (8), then
there is a formula  such that Mod? () = Mod? ()V by Proposition 4.

Lemma 5 A formula  is ?-irrelevant to a set V of atoms iff hH, T i |=?  implies hX, Y i |=? 
for every two ?-interpretations hX, Y i and hH, T i with hX, Y i V hH, T i
60

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

Proof:  is ?-irrelevant to V
iff there exists a formula  mentioning no atoms in V such that  ? 
iff there exists a formula  mentioning no atoms in V s.t Mod? () = Mod? ()
iff Mod? () = {hX, Y i|X  Y and hH, T i V hX, Y i s.t hH, T i |=? }
iff hH, T i |=?  implies hX, Y i |=?  for every two ?-interpretations hX, Y i and hH, T i such that
hX, Y i V hH, T i.

Proposition 10 Let  and  be two formulas and V a set of atoms.
(i) IR? (Forget ? (, V ), V ).
(ii)  has a ?-model iff Forget? (, V ) has.
(iii)  |=? Forget? (, V ).
(iv) If  |=?  then Forget? (, V ) |=? Forget? (, V ).
(v) Forget? (  , V ) ? Forget? (, V )  Forget? (, V ).
(vi) Forget? (  , V ) |=? Forget? (, V )  Forget? (, V ).
(vii) Forget? (  , V ) ? Forget? (, V )   if IR? (, V ).
Proof: (i) It immediately follows from Lemma 5.
(ii) It is evident that Mod? () 6=  iff Mod? ()V 6=  by Definition 8.
(iii) It is easy to see that Mod? ()  Mod? ()V by Definition 8.
(iv) Let  |=? , and hH, T i |=? Forget? (, V ), i.e. hH, T i  Mod? ()V . In terms of
Definition 8, there exists hH 0 , T 0 i |=?  such that hH, T i V hH 0 , T 0 i. It implies that hH 0 , T 0 i |=?
 since  |=? . Thus hH, T i  Mod? ()V , i.e. hH, T i |=? Forget? (, V ).
(v) hH, T i |=? Forget? (  , V )
iff hH, T i  Mod? (  )V
iff hH 0 , T 0 i |=?    such that hH, T i V hH 0 , T 0 i
iff hH 0 , T 0 i such that hH, T i V hH 0 , T 0 i and, either hH 0 , T 0 i |=?  or hH 0 , T 0 i |=? 
iff hH, T i  Mod? ()V or hH, T i  Mod? ()V
iff hH, T i |=? Forget? (, V ) or hH, T i |=? Forget? (, V )
iff hH, T i |=? Forget? (, V )  Forget? (, V ).
(vi) hH, T i |=? Forget? (  , V )
 hH, T i  Mod? (  )V
 hH 0 , T 0 i |=?    such that hH, T i V hH 0 , T 0 i
 hH 0 , T 0 i such that. hH, T i V hH 0 , T 0 i, hH 0 , T 0 i |=?  and hH 0 , T 0 i |=? 
 hH, T i  Mod? ()V and hH, T i  Mod? ()V
 hH, T i |=? Forget? (, V ) and hH, T i |=? Forget? (, V )
 hH, T i |=? Forget? (, V )  Forget? (, V ).
(vii) The direction from left to right follows from (vi) and the fact IR(, V ), i.e. Forget? (, V ) ?
. Let us consider the other direction.
hH, T i |=? Forget? (, V )  
 hH, T i |=? Forget? (, V ) and hH, T i |=? 
 hH 0 , T 0 i |=?  such that hH, T i V hH 0 , T 0 i, and hH, T i |=? 
61

fiWANG , Z HANG , Z HOU , & Z HANG

 hH, T i V hH 0 , T 0 i such that hH 0 , T 0 i |=?    by IR(, V ) and Lemma 5
 hH, T i  Mod? (  )V
 hH, T i |=? Forget? (  , V ).



Theorem 8 (Horn expressibility) Let  be a Horn logic program and V  A. There is a Horn
logic program 0 such that Forget? (, V ) ? 0 .
Proof: In terms of Proposition 2, it suffices to prove for ? = HT. Let M = ModHT ()V . By
Proposition 6, it is sufficient to show that M satisfies conditions (5) and (12).
We first prove that M satisfies (5). For each HT-interpretation hX, Y i  M, we have that
X  Y , and there exists hH, T i  ModHT () such that hX, Y i V hH, T i. Note that  is positive,
which shows that hH, Hi and hT, T i are HT-models of  by Lemma 2. Thus hX, Xi  M and
hY, Y i  M due to X V H and T V Y . On the other hand, suppose hX, Xi  M, hY, Y i  M
and X  Y . There exist two HT-models hH 0 , T 0 i and hH 00 , T 00 i of  such that hH 0 , T 0 i V hX, Xi
and hH 00 , T 00 i V hY, Y i. By Lemma 2, we have H 0 |= , T 0 |= , H 00 |=  and T 00 |= . Since
models of Horn theories are closed under set intersection (Alfred, 1951), H 0  H 00 |= . By
Lemma 2 again, we have hH 0  H 00 , T 00 i |=HT . By Lemma 4, H 0  H 00 V X  Y (= X). Thus
hH 0  H 00 , T 00 i V hX, Y i. It follows hX, Y i  M.
Now we show that M satisfies (12). Suppose hX, Y i and hH, T i are two HT-interpretations
in M. It follows that there are two HT-models hX 0 , Y 0 i and hH 0 , T 0 i of  such that hX 0 , Y 0 i V
hX, Y i and hH 0 , T 0 i V hH, T i. Since  is Horn, we have that hH 0  X 0 , T 0  Y 0 i |=HT  by
Proposition 6. By Lemma 4, we have H 0  X 0 V H  X and Y 0  T 0 V Y  T . It implies
hH 0  X 0 , T 0  Y 0 i V hX  H, Y  T i, thus hX  H, Y  T i  M.

Proposition 11 Let  be a disjunctive logic program, V  A. We have that Forget? (, V ) is
expressible in disjunctive logic programs if and only if,
hH1 , T1 i |=? , hT2 , T2 i |=?  and T1  T2  hH3 , T3 i |=?  such that hH3 , T3 i V hH1 , T2 i.
Proof: By Proposition 2, it suffices to prove ? = HT. Let 0 HT Forget HT (, V ). The direction
from left to right is obvious. We show the other direction.
Suppose that 0 is not expressible in disjunctive logic programs. There exists hX, Y i |=HT 0 ,
Y  Y 0 and hY 0 , Y 0 i |=HT 0 such that hX, Y 0 i 6|=HT 0 . It follows that, for each hH1 , T1 i |=HT 
and hT2 , T2 i |=HT  such that hH1 , T1 i V hX, Y i, T2 V Y 0 and T1  T2 , there exists no
hH3 , T3 i |=HT  such that hH3 , T3 i V hH1 , T2 i, viz. hH3 , T3 i V hX, Y 0 i by hX, Y 0 i V
hH1 , T2 i, a contradiction.

Proposition 12 Let  be a normal logic program, V  A. Then Forget? (, V ) is expressible in
normal logic programs if and only if, in addition to condition (21), the following condition holds,
hH1 , T1 i |=? , hH2 , T2 i |=?  and T1 V T2
 hH3 , T3 i |=?  such that H3 V H1  H2 and (T3 V T1 or T3 V T2 ).

(21)

Proof: By Proposition 2, it suffices to prove ? = HT. Let 0 HT Forget HT (, V ). The direction
from left to right is easy. We consider the other direction in what follows.
In terms of Proposition 11 and Corollary 3, it is sufficient to show that, for each hX, Y i |=HT 0
and hX 0 , Y i |=HT 0 , hX  X 0 , Y i |=HT 0 according to Corollary 3. Suppose that hX, Y i and
62

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

hX 0 , Y i are two HT-models of 0 . There are two HT-models hH1 , T1 i and hH2 , T2 i of  such
that hX, Y i V hH1 , T1 i and hX 0 , Y i V hH2 , T2 i. It follows that T1 V T2 and, by condition (21), there exists an HT-model hH3 , T3 i of  satisfying either hH3 , T3 i V hH1  H2 , T1 i or
hH3 , T3 i V hH1  H2 , T2 i, which shows hH3 , T3 i V hX  X 0 , Y i, hence hX  X 0 , Y i |=HT
0 .

Theorem 10 (Representation theorem) Let  and  be two formulas and V a set of atoms. Then
the following statements are equivalent:
(i)  ? Forget? (, V ).
(ii)  ? {0 |  |=? 0 and IR? (0 , V )}.
(iii) Postulates (W), (PP), (NP) and (IR) hold.
Proof: Let ? = { |  |=?  and IR? (, V )}. It is evident that IR? (? , V ).
The equivalence between (i) and (ii) follows from Corollary 9. (ii) obviously implies (iii). It
suffices to show (iii)  (ii).
By Positive Persistence, we have  |=?  for each   ? , from which follows Mod? () 
Mod? (? ). On the other hand, by ( W)  |=?  and (IR) IR? (, V ), it follows   ? . Thus
Mod? (? )  Mod? (). Thus  ? ? .

Proposition 13 Let , 0 ,  be formulas and V  A such that  ? Forget? (, V ) and 0 
Forget(, V ). Then
(i)   0 .
(ii) 0 |=? .
Proof: (i) T |= 
iff hT, T i |=?  by (i) of Proposition 3
iff hT, T i |=? Forget? (, V ) since  ? Forget? (, V )
iff hY, Y i |=?  such that hT, T i V hY, Y i by Definition 8
iff Y |=  such that T V Y by (i) of Proposition 3
iff T |= Forget(, V ) by Corollary 5
iff T |= 0 since 0  Forget(, V ).
(ii) hH, T i |=? 0
 T |= 0 by (i) of Proposition 3
 T |= Forget(, V ) since 0  Forget(, V )
 Y |=  such that Y V T by Corollary 5
 hH \ V, Y i |=?  such that Y V T by (ii) of Proposition 3
 hH, T i |=? Forget? (, V ) due to hH \ V, Y i V hH, T i and Definition 8
 hH, T i |=?  due to Forget? (, V ) ? .



Proposition 14 Let  and 0 be two Horn logic programs, and V a set of atoms such that 0 
Forget(, V ). Then 0 ? Forget? (, V ).
63

fiWANG , Z HANG , Z HOU , & Z HANG

Proof: By Proposition 2, it suffices to show ? = HT.
() hH 0 , T 0 i |=HT 0
 H 0 |= 0 and T 0 |= 0 by Lemma 2
 H, T such that H |= , T |= , H V H 0 and T V T 0 by 0  Forget(, V )
 H, T such that H  T |= , T |= , H  T V H 0 and T V T 0
 H, T such that hH  T, T i |=HT  and hH  T, T i V hH 0 , T 0 i
 hH 0 , T 0 i |=HT Forget HT (, V ).
() hH 0 , T 0 i |=HT Forget HT (, V )
 hH, T i |=HT  such that hH 0 , T 0 i V hH, T i
 H  T such that H |= , T |=  and hH 0 , T 0 i V hH, T i by Lemma 2
 H 0 |= Forget(, V ) and T 0 |= Forget(, V )
 H 0 |= 0 and T 0 |= 0 due to 0  Forget(, V )
 hH 0 , T 0 i |=HT 0 .
Proposition 15 Let  and  be two formulas and V a set of atoms.
(i)   Forget(, V ) iff  ? Forget? (, V ).
(ii) Forget(, V )  Forget(, V ) iff Forget? (, V ) ? Forget? (, V ).
Proof: (i) () hH, T i |=? 
iff T |= , i.e. T |=  by (ii) of Proposition 3
iff T |= Forget(, V ) since   Forget(, V )
iff Y |=  i.e. Y |=  such that Y V T by Corollary 5
iff hH \ V, Y i |=?  (H \ V  T \ V = Y \ V ) by (ii) of Proposition 3
iff hH, T i |=? Forget? (, V ) by Definition 8.
() T |=  i.e. T |= 
iff hH, T i |=?  by (ii) of Proposition 3
iff hH, T i |=? Forget? (, V ) for H  T since  ? Forget? (, V )
iff hX, Y i |=?  such that hH, T i V hX, Y i by Definition 8
iff Y |=  such that Y V T by (ii) or Proposition 3
iff T |= Forget(, V ) by Corollary 5.
(ii) () hH, T i |=? Forget? (, V )
iff hX, Y i |=?  such that hX, Y i V hH, T i by Definition 8
iff Y |=  i.e. Y |=  such that Y V T by (ii) of Proposition 3
iff T |= Forget(, V ) by Corollary 5
iff T |= Forget(, V ) since Forget(, V )  Forget(, V )
iff Y 0 |=  i.e. Y 0 |=  such that Y 0 V T by Definition 8
iff hX \ V, Y 0 i |=?  by (ii) of Proposition 3 (X \ V  Y \ V = Y 0 \ V )
iff hH, T i |=? Forget? (, V ) by hH, T i V hX \ V, Y 0 i and Definition 8.
() T |= Forget(, V )
iff Y |=  i.e. Y |=  such that Y V T by Corollary 5
iff hX, Y i |=?  such that Y V T by (ii) of Proposition 3
iff hX \ V, T i |=? Forget? (, V ) hX \ V, T i V hX, Y i and by Definition 8
iff hX \ V, T i |=? Forget? (, V ) since Forget? (, V ) ? Forget? (, V )
iff hX 0 , Y 0 i |=?  such that hX \ V, T i V hX 0 , Y 0 i by Definition 8
64



fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

iff Y 0 |=  i.e. Y 0 |=  such that T V Y 0 by (ii) of Proposition 3
iff T |= Forget(, V ) by Corollary 5.



Theorem 12 (?-forgetting vs propositional forgetting) Let  and  be two formulas of LA and
V  A. Then
 ? Forget? (, V ) iff (A) |= ? []  Forget((A)  {? []}, V  V 0 ).
 be a model of (A).
Proof: () Let M = MA  MA
0
M |= (A)  {? []}
 i |=  by Proposition 8
iff hMA , MA
0
?
 i |= Forget (, V ) since   Forget (, V )
iff hMA , MA
0
?
?
?
?
 i by Definition 8
iff hH, T i |=?  such that hH, T i V hMA , MA
0

iff hH, T i |=?  such that H V MA and T V MA
0
0
iff H  T |= (A)  {? []} and H V MA and T 0 V 0 MA0 by Proposition 8
iff H  T 0 |= (A)  {? []} and H  T 0 V V 0 MA  MA0 by Lemma 4
iff MA  MA0 |= Forget((A)  {? []}, V  V 0 ) by Definition 8
iff M |= Forget((A)  {? []}, V  V 0 ).
() hX, Y i |=? 
iff X  Y 0 |= (A)  {? []} by Proposition 8
iff X  Y 0 |= (A)  Forget((A)  {? []}, V  V 0 )
iff M |= (A)  {? []} such that M V V 0 X  Y 0
 i |=  such that M  M   X  Y by Proposition 8
iff hMA , MA
0
V
?
A
A0
 i by Definition 8.
iff hX, Y i |=? Forget? (, V ) due to hX, Y i V hMA , MA
0



Proposition 16 Let  and  be two formulas of LA and V a set of atoms. Then Forget? (, V ) ?
Forget? (, V ) iff the following condition holds:
Forget({? []}  (A), V  V 0 )  Forget({? []}  (A), V  V 0 ).
Proof: () We show Forget({? []  (A), V  V 0 ) |= Forget({? []  (A), V  V 0 ). The
other side can be similarly proved.
M |= Forget({? []}  (A), V  V 0 )
 N  A  A0 such that N V V 0 M and N |= {? []}  (A)
 hX, Y i |=?  with N = X  Y 0 by Proposition 8
 hX, Y i |=? Forget? (, V ) by (iii) of Proposition 10
 hX, Y i |=? Forget? (, V ) as Forget? (, V ) ? Forget? (, V )
 hH, T i |=?  such that hH, T i V hX, Y i by Definition 8
 H  T 0 |= ? []  (A) by Proposition 8
 X  Y 0 |= Forget({? []}  (A), V  V 0 ) as H  T 0 V V 0 X  Y 0
 M |= Forget({? []}  (A), V  V 0 ) by M V V 0 X  Y 0 (= N ).
() We show Forget? (, V ) |=? Forget? (, V ). The other side is similar.
hH, T i |=? Forget? (, V )
 hX, Y i |=?  such that hH, T i V hX, Y i) by Definition 8
 X  Y 0 |= {? []}  (A) by Proposition 8
 X  Y 0 |= Forget({? []}  (A), V  V 0 )
65

fiWANG , Z HANG , Z HOU , & Z HANG

 X  Y 0 |= Forget({? []}  (A), V  V 0 )
 H1  T10 |= {? []}  (A) such that H1  T10 V V 0 X  Y 0
 hH1 , T1 i |=?  by Proposition 8
 hX, Y i |=? Forget? (, V ) as hX, Y i V hH1 , T1 i by Definition 8
 hH, T i |=? Forget? (, V ) as hX, Y i V hH, T i.



Theorem 14 Let  and  be two formulas and V a set of atoms.
(i) The problem of deciding if  ? Forget? (, V ) is co-NP-complete.
(ii) The problem of deciding if Forget? (, V ) ? Forget? (, V ) is P2 -complete.
(iii) The problem of deciding if  ? Forget? (, V ) is P2 -complete.
Proof: (i) Membership. Recall that  |=? Forget? (, V ) by (iii) of Proposition 10. We have
 6? Forget? (, V )
iff Forget? (, V ) 6|=? 
iff hX, Y i |=? Forget? (, V ) and hX, Y i 6|=? 
iff hH, T i |=?  such that hH, T i V hX, Y i and hX, Y i 6|=? .
Since both guessing hH, T i, hX, Y i and checking the ?-satisfiability can be done in polynomial
time in the size of  and V . Thus the complement of  6? Forget? (, V ), i.e.  ? Forget? (, V ),
is in co-NP.
The hardness follows from the fact that, by (i) of Proposition 15,  ? Forget? (, V ) iff
  Forget(, V ), which is co-NP-complete (cf., see Lang et al., 2003, Prop. 10).
(ii) Membership. If Forget? (, V ) 6? Forget? (, V ) then there exists a ?-interpretation hH, T i
such that either
(a) hH, T i |=? Forget? (, V ) and hH, T i 6|=? Forget? (, V ), or
(b) hH, T i 6|=? Forget? (, V ) and hH, T i |=? Forget? (, V ).
On the one hand, to guess a ?-interpretation hH, T i is feasible by a nondeterministic Turing machine. On the other hand, checking if hH, T i |=?  is feasible by a deterministic Turing machine;
and hH, T i |=? Forget? (, V ) iff there exists hX, Y i |=?  such that hX, Y i V hH, T i. Thus
checking the conditions (a) and (b) can be done in polynomial time in the size of  and  by calling
a nondeterministic Turing machine. Thus the problem is in P2 .
Note that, by (ii) of Proposition 15, Forget? (, V ) ? Forget? (, V ) iff Forget(, V ) 
Forget(, V ), which is P2 -complete (cf., see Lang et al., 2003, Prop. 24). Thus the hardness
follows.
(iii) Membership. Note that  6? Forget? (, V ) iff there is a ?-interpretation hH, T i such that
 hH, T i |=?  and hH, T i 6|=? Forget? (, V ), or
 hH, T i 6|=?  and hH, T i |=? Forget? (, V ).
Similar to the case of (ii), the guessing and checking are in polynomial time in the size of ,  and
V by calling a nondeterministic Turing machine. Thus the problem is in P2 .
Note that  ? Forget? (, V ) iff  ? Forget? (, V ) and Forget? (, V ) ? Forget? (, V ),
the latter is P2 -hard by (ii). Then the hardness follows.

66

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

Proposition 17 Let  and  be two formulas and V a set of atoms.
(i) The problem of deciding whether  |=? Forget? (, V ) is P2 -complete.
(ii) The problem of deciding whether Forget? (, V ) |=?  is co-NP-complete.
Proof: (i) Membership. Recall that  6|=? Forget? (, V ) iff there exists a ?-model hH, T i of 
such that hH, T i 6|= Forget? (, V ). As hH, T i 6|= Forget? (, V ) iff hX, Y i 6|=  for every ?interpretation hX, Y i such that hX, Y i V hH, T i. Such hH, T i can be guessed in polynomial
time in the size of ,  and V . Checking hH, T i 6|= Forget? (, V ) is possible in polynomial time
in the size of ,  and V by calling a nondeterministic Turing machine. Thus the original problem
is in p2 .
Hardness. It follows from the following fact:
> |=? Forget? (, V )
iff > ? Forget? (, V )
iff >  Forget(, V ) by (i) of Proposition 15 (> ? >)
iff the QBF V V  is valid, which is P2 -complete (Papadimitriou, 1994).
(ii) Membership. Note that
Forget? (, V ) 6|=? 
iff hH, T i |=? Forget? (, V ) such that hH, T i 6|= 
iff hX, Y i |=?  such that hX, Y i V hH, T i and hH, T i 6|= .
Since the guessing and checking are both polynomial in the size of ,  and V , the original problem
is in co-NP.
Hardness follows from the fact that
Forget? (, V ) |=? 
iff  |=?  by (ii) of Proposition 10
iff  has no ?-model, which is co-NP-complete by Proposition 9.


Appendix D. Forgetting Operators FW and FS
Wong proposed six postulates and argued that the postulates should to respected by all forgetting
operators in disjunctive logic programs under strong equivalence:
(F-1) If  |=HT  then F (, a) |=HT F (, a);
(F-2) If a does not appear in , then F ({r}  , a) HT F ({r}, a)  ;
(F-3) F (, a) does not contain any atoms not in ;
(F-4) If F (, a) |=HT r then F ({s}, a) |=HT r for some s  Cn();
(F-5) If F (, a) |=HT (A  B, not C), then  |=HT (A  B, not C, not a);
(F-6) F (F (, a), b) HT F (F (, b), a)
where F is a forgetting operator, ,  and  are disjunctive logic programs, a and b are atoms, r is
a disjunctive rule, and
Cn() ={r| r is a disjunctive rule such that  |=HT r and var(r)  var()}.
67

fiWANG , Z HANG , Z HOU , & Z HANG

where var() is the set of atoms occurring in .
Accordingly, he proposed two forgetting operators FS and FW : the result of forgetting an atom
a from a disjunctive logic program  is defined by the below procedure:
(1) Let 1 = Cn().
(2) Form 1 , remove rules of the form (A  B, a, not C), replace each rule of the form (A 
{a}  B, not C, not a) with (A  B, not C, not a). Let the resulting logic program be 2 .
(3) Replace or remove each rule in 2 , of the form (A  B, not C, not a) or (A  {a} 
B, not C) according to the following table:
S
W

A  B, not C, not a
(remove)
A  B, not C

A  {a}  B, not C
(remove)
A  B, not C

Let 3 be the resulting logic program.
The logic program 3 is the result of forgetting p from .

References
Alfred, H. (1951). On sentences which are true of direct unions of algebras. The Journal of Symbolic
Logic, 16(1), 1421.
Bobrow, D. G., Subramanian, D., Greiner, R., & Pearl, J. (Eds.). (1997). Special issue on relevance
97 (1-2). Artificial Intelligence Journal.
Cabalar, P., & Ferraris, P. (2007). Propositional theories are strongly equivalent to logic programs.
Theory and Practice of Logic Programming, 7(6), 745759.
Delgrande, J. P., Schaub, T., Tompits, H., & Woltran, S. (2013). A model-theoretic approach to
belief change in answer set programming. ACM Transactions on Computational Logic, 14(2),
A:1A:42.
Eiter, T., Fink, M., Tompits, H., & Woltran, S. (2004). On eliminating disjunctions in stable logic
programming. In Principles of Knowledge Representation and Reasoning: Proceedings of
the Ninth International Conference (KR2004), pp. 447458, Whistler, Canada. AAAI Press.
Eiter, T., & Wang, K. (2008). Semantic forgetting in answer set programming. Artificial Intelligence,
172(14), 16441672.
Faber, W., Pfeifer, G., & Leone, N. (2011). Semantics and complexity of recursive aggregates in
answer set programming. Artificial Intelligence, 175(1), 278298.
Ferraris, P. (2005). Answer sets for propositional theories. In Logic Programming and Nonmonotonic Reasoning, 8th International Conference, Vol. 3662 of Lecture Notes in Computer Science, pp. 119131, Diamante, Italy. Springer.
Ferraris, P., Lee, J., & Lifschitz, V. (2011). Stable models and circumscription. Artificial Intelligence, 175(1), 236263.
Ferraris, P., & Lifschitz, V. (2005). Mathematical foundations of answer set programming. In
Artemov, S. N., Barringer, H., dAvila Garcez, A. S., Lamb, L. C., & Woods, J. (Eds.), We Will
Show Them! Essays in Honour of Dov Gabbay, Vol. 1, pp. 615664. College Publications.
68

fiK NOWLEDGE F ORGETTING

IN

A NSWER S ET P ROGRAMMING

Gabbay, D. M., Pearce, D., & Valverde, A. (2011). Interpolable formulas in equilibrium logic and
answer set programming. Journal of Artificial Intelligence Research, 42, 917943.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming. In Proceedings of the Fifth International Conference and Symposium on Logic Programming, pp.
10701080, Seattle, Washington. MIT Press.
Goranko, V., & Otto, M. (2007). Handbook of Modal Logic, Vol. 3, chap. 5 Model Theory Of Modal
Logic, pp. 249329. Elsevier.
Jongh, D. D., & Hendriks, L. (2003). Characterization of strongly equivalent logic programs in
intermediate logics. Theory and Practice of Logic Programming, 3(3), 259270.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). Can you tell the difference between dl-lite
ontologies?. In Principles of Knowledge Representation and Reasoning: Proceedings of the
Eleventh International Conference, KR 2008, pp. 285295, Sydney, Australia. AAAI Press.
Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formula-variable independence and forgetting. Journal of Artificial Intelligence Research, 18, 391443.
Lang, J., & Marquis, P. (2010). Reasoning under inconsistency: A forgetting-based approach. Artificial Intelligence, 174(12-13), 799823.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM Transactions on Computational Logic, 2(4), 526541.
Lifschitz, V., Tang, L. R., & Turner, H. (1999). Nested expressions in logic programs. Annals of
Mathematics and Artificial Intelligence, 25(3-4), 369389.
Lin, F. (2001). On strongest necessary and weakest sufficient conditions. Artificial Intelligence,
128(1-2), 143159.
Lin, F. (2002). Reducing strong equivalence of logic programs to entailment in classical propositional logic. In Proceedings of the Eights International Conference on Principles and Knowledge Representation and Reasoning (KR-02), pp. 170176, Toulouse, France. Morgan Kaufmann.
Lin, F., & Chen, Y. (2007). Discovering classes of strongly equivalent logic programs. Journal of
Artificial Intelligence Research, 28, 431451.
Lin, F., & Reiter, R. (1994). Forget it!. In In Proceedings of the AAAI Fall Symposium on Relevance,
pp. 154159.
Lin, F., & Zhou, Y. (2011). From answer set logic programming to circumscription via logic of GK.
Artificial Intelligence, 175(1), 264277.
Liu, Y., & Wen, X. (2011). On the progression of knowledge in the situation calculus. In IJCAI
2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, pp.
976982, Barcelona, Catalonia, Spain. IJCAI/AAAI.
Lutz, C., & Wolter, F. (2011). Foundations for uniform interpolation and forgetting in expressive
description logics. In IJCAI 2011, Proceedings of the 22nd International Joint Conference on
Artificial Intelligence, pp. 989995, Barcelona, Catalonia, Spain. IJCAI/AAAI.
Osorio, M., & Cuevas, V. (2007). Updates in answer set programming: An approach based on basic
structural properties. TPLP, 7(4), 451479.
69

fiWANG , Z HANG , Z HOU , & Z HANG

Osorio, M., & Zacarias, F. (2004). On updates of logic programs: A properties-based approach. In
Seipel, D., & Torres, J. M. T. (Eds.), FoIKS, Vol. 2942 of Lecture Notes in Computer Science,
pp. 231241. Springer.
Packer, H. S., Gibbins, N., & Jennings, N. R. (2011). An on-line algorithm for semantic forgetting. In IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial
Intelligence, pp. 27042709, Barcelona, Catalonia, Spain. IJCAI/AAAI.
Papadimitriou, C. H. (1994). Computational complexity. Addison Wesley.
Pearce, D., Tompits, H., & Woltran, S. (2001). Encodings for equilibrium logic and logic programs
with nested expressions. In Proceedings of the10th Portuguese Conference on Artificial Intelligence on Progress in Artificial Intelligence, Knowledge Extraction, Multi-agent Systems,
Logic Programming and Constraint Solving, pp. 306320, London, UK. Springer-Verlag.
Pearce, D., Tompits, H., & Woltran, S. (2009). Characterising equilibrium logic and nested logic
programs: Reductions and complexity. Theory and Practice of Logic Programming, 9(5),
565616.
Su, K., Sattar, A., Lv, G., & Zhang, Y. (2009). Variable forgetting in reasoning about knowledge.
Journal of Artificial Intelligence Research, 35, 677716.
Truszczynski, M. (2010). Reducts of propositional theories, satisfiability relations, and generalizations of semantics of logic programs. Artificial Intelligence, 174(16-17), 12851306.
van Ditmarsch, H. P., Herzig, A., Lang, J., & Marquis, P. (2009). Introspective forgetting. Synthese,
169(2), 405423.
Visser, A. (1996). Uniform interpolation and layered bisimulation. In Godel96, pp. 139164.
Wang, Y., Wang, K., & Zhang, M. (2013). Forgetting for answer set programs revisited. In IJCAI
2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pp.
11621168, Beijing, China. IJCAI/AAAI.
Wang, Y., Zhang, Y., Zhou, Y., & Zhang, M. (2012). Forgetting in logic programs under strong
equivalence. In Principles of Knowledge Representation and Reasoning: Proceedings of the
Thirteenth International Conference, pp. 643647, Rome, Italy. AAAI Press.
Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2010). Forgetting for knowledge bases in dl-lite.
Annuals of Mathematics and Artificial Intelligence, 58(1-2), 117151.
Wong, K.-S. (2009). Forgetting in Logic Programs. Ph.D. thesis, The University of New South
Wales.
Zhang, Y., & Foo, N. Y. (2006). Solving logic program conflict through strong and weak forgettings.
Artificial Intelligence, 170(8-9), 739778.
Zhang, Y., & Zhou, Y. (2009). Knowledge forgetting: Properties and applications. Artificial Intelligence, 173(16-17), 15251537.
Zhou, Y., & Zhang, Y. (2011). Bounded forgetting. In Proceedings of the Twenty-Fifth AAAI
Conference on Artificial Intelligence, AAAI 2011, pp. 280285, San Francisco, California,
USA. AAAI Press.

70

fi