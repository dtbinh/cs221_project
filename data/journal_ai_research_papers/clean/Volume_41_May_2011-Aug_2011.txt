Journal of Artificial Intelligence Research 41 (2011) 445475

Submitted 02/11; published 08/11

On the Intertranslatability of Argumentation Semantics
Wolfgang Dvorak
Stefan Woltran

dvorak@dbai.tuwien.ac.at
woltran@dbai.tuwien.ac.at

Technische Universitat Wien,
Institute of Information Systems 184/2
Favoritenstrasse 9-11, 1040 Vienna, Austria

Abstract
Translations between different nonmonotonic formalisms always have been an important topic in the field, in particular to understand the knowledge-representation capabilities
those formalisms offer. We provide such an investigation in terms of different semantics
proposed for abstract argumentation frameworks, a nonmonotonic yet simple formalism
which received increasing interest within the last decade. Although the properties of these
different semantics are nowadays well understood, there are no explicit results about intertranslatability. We provide such translations wrt. different properties and also give a few
novel complexity results which underlie some negative results.

1. Introduction
Studies on the intertranslatability of different approaches to nonmonotonic reasoning have
always been considered as an important contribution to the field in order to understand the
expressibility and representation capacity of the various formalisms. By intertranslatability
we understand a function Tr which maps theories from one formalism into another such
that intended models of a theory  from the source formalism are in a certain relation to the
intended models of Tr (). Several desired properties for such translation functions have
been identified, including to be polynomial (Tr () can be computed in polynomial time
wrt. the size of ) or to be modular (roughly speaking, that allows to transform parts of the
theory independently of each other). In particular, the relationship between (variants of)
default logic (Reiter, 1980) and nonmonotonic modal logics, e.g. autoepistemic logic (Moore,
1985), has always received a lot of attention, (see, e.g., Denecker, Marek, & Truszczynski,
2003; Konolige, 1988; Marek & Truszczynski, 1993). Perhaps most notably, Gottlob (1995)
showed that a modular translation from default logic to autoepistemic logic is impossible.
Other important contributions in this direction include translations between default logic
and circumscription (Imielinski, 1987), modal nonmonotonic logics and logic programs (see,
e.g., de Bruijn, Eiter, & Tompits, 2008 for an overview and recent applications) and the work
by Janhunen (1999). Let us also refer here to recent work by Pearce and Uridia (2011), who
show that translations of the aforementioned kind have already been known in the context
of non-classical logics and related results date back to the work of Godel.
In this work, we study translation functions within a particular formalism of nonmonotonic reasoning but wrt. to different semantics proposed for this formalism. In the area
of default logic, similar research was undertaken, for instance by Liberatore (2007) or Delgrande and Schaub (2005). Likewise, for work concerning the relationship between different
logic programming semantics we refer to the work of Janhunen, Niemela, Seipel, Simons,
c
2011
AI Access Foundation. All rights reserved.

fiDvorak & Woltran

and You (2006) and the references therein. The formalism we focus on in this paper are
Dungs argumentation frameworks (Dung, 1995) which received increasing interest within
the last decade. In a nutshell, such argumentation frameworks (AFs, for short) represent
abstract statements1 together with a relation denoting attacks between them. Different
semantics provide different ways to solve the inherent conflicts between statements by selecting acceptable subsets  usually called extensions  of them. Several such semantics
have already been proposed by Dung in his seminal paper (Dung, 1995), but also alternative
approaches play a major role nowadays (see, e.g., Baroni, Dunne, & Giacomin, 2011; Baroni, Giacomin, & Guida, 2005; Caminada, 2006; Dung, Mancarella, & Toni, 2007; Verheij,
1996). Compared to other nonmonotonic formalisms (which are built on top of classical
logical syntax), argumentation frameworks are a much simpler formalism (in the end, they
are just directed graphs). However, this simplicity made them an attractive modeling tool
in several diverse areas, like formalizations of legal reasoning (Bench-Capon & Dunne, 2005)
or multi-agent negotiation (Amgoud, Dimopoulos, & Moraitis, 2007).
In the field of argumentation, intertranslatability has mainly been studied in connection
with generalizations of Dungs argumentation frameworks. By generalization we mean here
the augmentation of simple frameworks by further concepts as priorities or additional relations between arguments. In this context, translations were used to show that proposed
semantics for such generalizations are in a close relation with the corresponding semantics of standard AFs. In other words, given such a generalized AF one is interested in
translating them to standard AFs while preserving semantics. Such translations have been
discussed, for instance, in terms of bipolar AFs (Cayrol & Lagasquie-Schiex, 2009), valuebased AFs (Bench-Capon & Atkinson, 2009), AFs with recursive attacks (Baroni, Cerutti,
Giacomin, & Guida, 2011), or abstract dialectical frameworks (Brewka, Dunne, & Woltran,
2011). A recent exception where intertranslatability within Dung AFs is discussed, is the
work by Baumann and Brewka (2010), who consider to enforce a desired extension in Dung
AFs by adding new arguments and switching semantics. From a slightly different perspective, also the work by Gabbay (2009) is related, since it investigates the substitution of an
argumentation framework as a node in another framework.
We focus here exclusively on standard argumentation frameworks and have the following
main objective: Given an AF F and argumentation semantics  and   , find a function Tr
such that the -extensions of F are in certain correspondence to the   -extensions of Tr (F ).
We believe that such results are important from different points of view.
Firstly, consider there is an advanced argumentation engine for a semantics   , but one
wants to evaluate an AF F wrt. to a different semantics . Then, it might be a good plan to
transform F in such a way into an AF F  such that evaluating F  wrt. semantics   allows
for an easy reconstruction of the -extensions of F . If the required transformations are efficiently computable, this leads to a potentially more successful approach than implementing
a distinguished algorithm for the -semantics from scratch. Figure 1 illustrates this idea.
The concept of a filter is required in case Tr (F ) introduces further arguments (which thus
might appear in the   -extensions of Tr (F )) in the course of the translation making a filter1. In general, arguments are not considered as simple statements but contain a number of reasons that
lead to a conclusion (see, e.g., Besnard & Hunter, 2001; Caminada & Amgoud, 2007). However, for
the purpose of this work, we will treat arguments as atomic entities thus abstracting from their internal
structure.

446

fiOn the Intertranslatability of Argumentation Semantics

Input AF: F

Translation
for    

Tr (F )

Solver
for  

  (Tr (F ))

(F )
Filter

Figure 1: Solver for semantics .

ing of these new arguments necessary to obtain the desired original extensions. However,
we will also consider translations for which such a back-translation is not necessary.
A second motivation of our work is concerned with meta(level) argumentation (see,
e.g., Modgil & Bench-Capon, 2011; Villata, 2010) which can be explained as follows: A
meta-level Dung argumentation framework is itself instantiated by arguments that make
statements about arguments, their interactions, and their evaluation in an object-level argumentation framework(Modgil & Bench-Capon, 2011). The translations we shall present
here exactly fit into this picture in the sense that we have to model certain features of a
semantics  within another semantics   , when giving a translation from  to   . To have
a more concrete example, let   be the complete semantics and  denote stable semantics
(we will provide the formal details about the different semantics in Section 2; for the sake of
this illustration the details are not required). Then, the transformation has to capture the
concept of admissibility (informally speaking, a set of arguments has to defend itself) which
is implicitly present in complete semantics by a suitable introduction of new arguments,
such that stable semantics can perform such a type of reasoning. In other words, translatability results between different semantics of AFs yield an understanding of how certain
properties, which are specified implicitly within one semantics, can be made (syntactically)
explicit within an AF in order to make these properties amenable to another semantics.
As a third important application of our work, we consider situations where different
semantics of argumentation have to be dealt with simultaneously. This might be the case
if different agents share their views about a certain situation (modeled as AFs) but these
agents use different semantics to reason over their frameworks. As well, the problem of
combining frameworks which have been constructed under the assumption that they will
be evaluated under different semantics falls into a possible application area of our work.
Finally, we emphasize that understanding which translations can be efficiently performed
wrt. different semantics complements the picture about expressibility of argumentation
semantics. For instance, if there exists an efficient translation from semantics  to semantics
  , but there is no such translation in the other direction,  could be understood as more
expressible than   , although complexity analysis for typical decision problems associated
to AFs does not show any difference between  and   . As an example consider semi-stable
and stage semantics. For both semantics the credulous acceptance problem is P2 -complete
and the skeptical acceptance problem is P2 -complete (Dvorak & Woltran, 2010). But when
considering what we call efficient exact translations one can map stage semantics to semistable semantics but not vice versa. Thus semi-stable semantics are more expressible than
stage semantics wrt. efficient exact translations. However we will argue that the notion of
exact translations may be too restrictive for comparing the expressibility of argumentation
semantics.
447

fiDvorak & Woltran

Beside these aspects of motivation, we would like to mention that positive results on
intertranslations indicate a certain form of independence of semantics in argumentation.
This is of particular importance, having in mind that the argumentation community nowadays is overwhelmed with different proposals of semantics. Thus understanding the basic
principles underlying different semantics is crucial, and we believe the results provided in
this paper contribute to this question.
The organization of the remainder of the paper and its main contributions are as follows:
 In Section 2, we introduce argumentation frameworks and the different semantics we
deal with in this paper. We also review known complexity results which we complement in the sense that we show some of the known tractable problems to be P -hard;
a fact we will use for some impossibility results in Section 5.
 Section 3 defines properties for translations basically along the lines of Janhunen
(1999). In particular, we consider here as desired properties efficiency (the translation
can be computed in logarithmic space wrt. the given AF), modularity (the translation
can be done independently for certain parts of the framework) and faithfulness (there
should be a clear correspondence between the extensions of the translated AF and the
original AF). However, we also consider some additional features which are needed to
deal with some of the argumentation semantics (for instance, the admissible semantics
always yields the empty set as one solution; thus filtering such an entire solution is
necessary).
 Section 4 contains our main results, in particular we provide translations between
Dungs original semantics (admissible, preferred, stable, complete, grounded), stage
semantics (Verheij, 1996) and semi-stable semantics (Caminada, 2006). We analyze
these translations wrt. the properties mentioned above using as minimal desiderata
efficiency and (a particular form of) faithfulness.
 As already mentioned, Section 5 then provides negative results, i.e. we show that
certain translations between semantics are not possible. Some of these impossibility
results make use of typical complexity-theoretic assumptions; others are genuine due
to the different properties of the compared semantics.
 Finally, in Section 6 we conclude the paper with a summary and discussion of the
presented results. As well, an outlook to potential future work is given there.

2. Argumentation Frameworks
In this section we introduce (abstract) argumentation frameworks (Dung, 1995) and recall
the semantics we study in this paper (see also Baroni & Giacomin, 2009, for an overview).
Moreover, we highlight and complement complexity results for typical decision problems
associated to such frameworks.
Definition 1. An argumentation framework (AF) is a pair F = (A, R) where A is a nonempty set of arguments 2 and R  A  A is the attack relation. For a given AF F = (A, R)
2. For technical reasons we only consider AFs with A 6= .

448

fiOn the Intertranslatability of Argumentation Semantics

we use AF to denote the set A of its arguments and RF to denote its attack relation R. The
pair (a, b)  R means that a attacks b.
We sometimes use the notation a R b instead of (a, b)  R. For S  A and a  A,
we also write S R a (resp. a R S) in case there exists an argument b  S, such that
b R a (resp. a R b). In case no ambiguity arises, we use  instead of R .
An AF can naturally be represented as a directed graph. Semantics for argumentation
frameworks are given via a function  which assigns to each AF F = (A, R) a set (F )  2A
of extensions. We shall consider here for  the functions stb, adm, prf , com, grd , stg,
and sem which stand for stable, admissible, preferred, complete, grounded, stage, and
respectively, semi-stable semantics. Before giving the actual definitions for these semantics,
we require a few more formal concepts.
Definition 2. Given an AF F = (A, R), an argument a  A is defended (in F ) by a set
S  A if for each b  A, such that b  a, also S  b holds. Moreover, for a set S  A,
+
we define the range of S, denoted as SR
, as the set S  {b | S  b}.
We continue with the definitions of the considered semantics. Observe that their common feature is the concept of conflict-freeness, i.e. arguments in an extension are not allowed
to attack each other.
Definition 3. Let F = (A, R) be an AF. A set S  A is conflict-free (in F ), if there are
no a, b  S, such that (a, b)  R. For such a conflict-free set S, it holds that
+
 S  stb(F ), if for each a  A \ S, S  a, i.e. SR
= A;

 S  adm(F ), if each a  S is defended by S;
 S  prf (F ), if S  adm(F ) and there is no T  adm(F ) with T  S;
 S  com(F ), if S  adm(F ) and for each a  A that is defended by S, a  S;
 S  grd (F ), if S  com(F ) and there is no T  com(F ) with T  S;
+
 S  stg(F ), if there is no conflict-free set T in F , such that TR+  SR
;
+
 S  sem(F ), if S  adm(F ) and there is no T  adm(F ) with TR+  SR
.

For all semantics , the sets defined above are the only ones in (F ).
We recall that for each AF F ,
stb(F )  sem(F )  prf (F )  com(F )  adm(F )
holds, and that for each of the considered semantics  except stable semantics, (F ) 6= 
holds. The grounded semantics always yields exactly one extension. Moreover if an AF has
at least one stable extension then its stable, semi-stable, and stage extensions coincide.
Example 1. Consider the AF F = (A, R), with A = {a, b, c, d, e} and R = {(a, b), (c, b),
(c, d), (d, c), (d, e), (e, e)}. The graph representation of F is given as follows.
a

b

c
449

d

e

fiDvorak & Woltran

x

xy z

yz x

x

y

z
t

Figure 2: Argumentation framework FT,z for T = { x, x  y  z, y  z  x}.
We have stb(F ) = stg(F ) = sem(F ) = {{a, d}}. Further we have as admissible sets of F
the collection {}, {a}, {c}, {d}, {a, c}, {a, d}, thus prf (F ) = {{a, c},{a, d}}. Finally the
complete extensions of F are {a}, {a, c} and {a, d}, with {a} being the grounded extension
of F .

We now turn to the complexity of reasoning in AFs. To this end, we define the following
decision problems for the semantics  introduced in Definition 3.
 Credulous Acceptance Cred : Given AF F = (A, R) and an argument a  A. Is a
contained in some S  (F )?
 Skeptical Acceptance Skept : Given AF F = (A, R) and an argument a  A. Is a
contained in each S  (F )?
 Verification of an extension Ver : Given AF F = (A, R) and a set of arguments
S  A. Is S  (F )?
 Existence of an extension Exists : Given AF F = (A, R). Is (F ) 6= ?
 Existence of a nonempty extension Exists
 : Given AF F = (A, R). Does there exist
a set S 6=  such that S  (F )?
Before giving an overview about known results, we provide a few lower bounds which,
to the best of our knowledge, have not been established yet.
Proposition 1. The problems Credgrd = Skeptgrd = Skeptcom as well as Vergrd are P-hard
(under L-reductions, i.e. reductions using logarithmic space).
Proof. We use a reduction from the P-hard problem to decide, given a propositional definite
Horn theory T and an atom x, whether x is true in the minimal model of T .
Let, for a definite Horn theory T = {rl : bl,1      bl,il  hl | 1  l  n} over atoms X
and an atom z  X, FT,z = (A, R) be an AF defined as follows:
A = T  X  {t}
R = {(x, x), (t, x) | x  X}  {(z, t)} 
{(rl , hl ), (bl,j , rl ) | rl  T, 1  j  il )}
where t is a fresh argument. See Figure 2 for an example. Clearly the AF FT,z can be
constructed using only logarithmic space in the size of T .
450

fiOn the Intertranslatability of Argumentation Semantics

In the following we show that z is in the minimal model of T iff t is in the grounded
extension of FT,z iff grd (FT,z ) = {T  {t}}.
First we attend that t is in the grounded extension E of FT,z iff E = {T {t}}. Obviously
the if-direction holds. Thus let us assume t  E, then each x  X is attacked by E and
thus each r  T is defended by E. Hence E = {T  {t}}.
It remains to show that z is in the minimal model of T iff t is in the grounded extension
E of FT,z . We recall the definition of the characteristic function FF of an AF F , defined as
FF (S) = {x  AF | x is defended by S}, and that the grounded extension of F is the least
fix-point of FF . To show the only-if part, let us assume that z is in the minimal model of
T . Thus there exists a finite sequence of rules (rli )1ik , such that (i) for each rule rli and
each atom bli ,s there exists a rule rlj , j < i with hlj = bli ,s and (ii) hlk = z. Clearly rl1 has
empty body and thus the corresponding argument has no attackers in FT,z , i.e. rl1  E.
We now claim that for each i, 1  i  k, rli  E holds as well and prove this by induction.
To this end, we assume the claim holds for all m < i, i.e. rlm  E, and thus E  hlm for
m < i holds. Using (i) we get that for each argument a  A with a  rli , it holds that
E  a. Hence rli  E. Now in particular rlk  E and by (ii) we have that E  z. As z is
the only argument attacking t we also have that t  E.
To show the if-part, let us assume that t is contained in the grounded extensions E of
FT,z . Then by construction E  z and thus there exists an integer k, such that FFk ()  z
and for each m < k : FFm () 6 z. We claim that for 1  m  k and x  X it holds
that if FFm ()  x then x is in the minimal model of T . The proof is by induction on
m. As induction base consider FF (). By construction FF () is the set of arguments that
correspond to rules in T having empty body. The arguments attacked by FF () are the
head atoms of these rules, which are clearly in the minimal model. For the induction step
assume that FFm1 () only attacks arguments corresponding to atoms in the minimal model.
As FFm1 () 6 z we have t 6 FFm1 (). Let x  X be an argument such that FFm ()  x,
but FFm1 () 6 z. Then there exists an ri  T such that hi = x and ri  FFm (). By
construction of FT,z we have that the argument ri is defended by FFm1 () iff each atom in
the body of ri is attacked by FFm1 (). Hence, by assumption each atom in the body of ri
is contained in the minimal model of T . But then the head hi of ri is in the minimal model
of T . Hence, as FFk ()  z, we get that z is in the minimal model of T .
Proposition 2. Verstg is coNP-hard.
Proof. We prove the assertion by reducing the (NP-hard) problem 3-SAT to the complementary problem of Verstg . We assume that a 3-CNF formula is given as a set C of clauses,
where each clause is a set over atoms and negated atoms (denoted by x). For such a CNF
 over variables X, define the AF F = (A, R) with
A = X  X  C  {s, t, b}
R = {(x, x), (x, x) | x  X}  {(l, c) | l  c, c  C} 
{(c, t) | c  C}  {(s, y), (y, s) | y  A \ {s, b}}  {(t, b), (b, b)}
where X = {x | x  X} and s, t, b are fresh arguments. See Figure 3 for an illustrating
example. We show that  is satisfiable iff {s} is not a stage extension of F . First let
us assume  is satisfiable and let T be any satisfying assignment of . Then the set
451

fiDvorak & Woltran

t
c1
x1

c2
x2

x1

b
c3
x3

x2

x4

x3

x4

s
Figure 3: AF F{c1 ,c2 ,c3 } with c1 = {x1 , x2 , x3 }, c2 = {x2 , x3 , x4 }, c3 = {x1 , x2 , x4 }.
E = {t}  {x | x  X, T (x) = true}  {x | x  X, T (x) = f alse} is a stable extension
+
of F , i.e. ER
= A, and since {s}+
R = A \ {b}, {s} is not a stage extension of F . Now
let us assume that {s} is a stage extension. By the same argumentation as above, i.e.
using {s}+
R  A, we get that F has no stable extension. But as we have seen before each
satisfying assignment of  corresponds to a stable extension of F . Thus we can conclude
that  is unsatisfiable.
Together with results from the literature (Coste-Marquis, Devred, & Marquis, 2005;
Dimopoulos & Torres, 1996; Dung, 1995; Dunne & Bench-Capon, 2002; Dunne & Caminada, 2008; Dvorak & Woltran, 2010), we obtain the complexity-landscape of abstract
argumentation as given in Table 1.

3. Properties for Translations
In what follows, we understand as a translation Tr a function which maps AFs to AFs. In
particular, we seek translations, such that for given semantics ,   , the extensions (F ) are
in a certain relation to extensions   (F ) for each AF F . To start with, we introduce a few
additional properties which seem desirable for such translations. To this end, we define, for
Cred

Skept

Ver

Exists

Exists


grd

P-c

P-c

P-c

trivial

in L

stb

NP-c

coNP-c

in L

NP-c

NP-c

adm

NP-c

trivial

in L

trivial

NP-c

com

NP-c

P-c

in L

trivial

NP-c

prf

NP-c

coNP-c

trivial

NP-c

sem

P2 -c
P2 -c

P2 -c
P2 -c
P2 -c

coNP-c

trivial

NP-c

coNP-c

trivial

in L



stg

Table 1: Complexity of abstract argumentation (C-c denotes completeness for class C).
452

fiOn the Intertranslatability of Argumentation Semantics

AFs F = (A, R), F  = (A , R ), the union of AFs as F  F  = (A  A , R  R ), and inclusion
as F  F  iff jointly A  A and R  R .
Definition 4. A translation Tr is called
 efficient if for every AF F , the AF Tr (F ) can be computed using logarithmic space
wrt. to |F |;
 covering if for every AF F , F  Tr (F );
 embedding if for every AF F , AF  ATr (F ) and RF = RTr (F )  (AF  AF );
 monotone if for any AFs F, F  , F  F  implies Tr (F )  Tr (F  );
 modular if for any AFs F, F  , Tr (F )  Tr (F  ) = Tr (F  F  ).
A translation should not reduce the expressiveness of a semantic using some expensive
computation. Thus the computational cost of a translation should be less than the computational cost of any semantic under our focus, i.e. less than P. Thus using the class of
logarithmic space computable functions is appropriate for our purposes. In addition, one
could seek translations which are minimal wrt. certain parameters (for instance, number
of additional arguments and attacks). However, we decided not to design our translations
towards such aims, since this would partly hide the main intuitions underlying the translations.
While the property of efficiency is clearly motivated, let us spend a few words on the
other properties. Covering holding ensures that the translation does not hide some original
arguments or conflicts. Being embedding, in addition, ensures that no additional attacks
between the original arguments are pretended. While efficiency is motivated by expressiveness and the possibility to reuse reasoning algorithms, the properties of covering and
embedding can be motivated by the meta-argumentation scenario. Translations which are
covering or embedding preserve the arguments and conflicts we (meta)-argue about, an
assumption one usually has in mind in the context of meta-argumentation. To put it in
other words, having an embedding translation, the original framework and the meta-level
part are clearly separated in the translated framework.
Monotonicity and modularity are crucial when extending the source AF after translation.
Let us first consider monotonicity. In multi-agent scenarios it may be impossible for one
agent to withdraw already interchanged arguments and attacks, as the other agents may not
agree to forget arguments and conflicts they already know about; hence, re-translating the
augmented source AF should respect the already existing translation. Now let us consider
modularity and adding only a few arguments/attacks to a huge AF. When updating the
translation it suffices to only consider the new arguments/attacks, instead of the whole
source AF, which indeed can be of computational value. In the field of meta-argumentation,
modular translations are in particular interesting as they are compatible with merging
AFs. Thus one can interchange merge- and translation-operations, i.e. it does not make a
difference if one first merges two AFs and then translates the union or first translates both
AFs and then merges the translations. Moreover, as it can be easily checked each modular
transformation is also monotone.
453

fiDvorak & Woltran

Next, we give two properties which refer to semantics. We note that our concept of
faithfulness follows the definition used by Janhunen (1999); while exactness is in the spirit
of bijective faithfulness wrt. equivalence as used by Liberatore (2007).
Definition 5. For semantics ,   we call a translation Tr
 exact for     if for every AF F , (F ) =   (Tr (F ));
 faithful for     if for every AF F , (F ) = {E  AF | E    (Tr (F ))} and
|(F )| = |  (Tr (F ))|.
However, due to the very nature of the different semantics we want to consider, we need
some less restricted notions. For instance, if we consider a translation from stable to some
other semantics, we have to face the fact that some AFs do not possess a stable extension,
while other semantics always yield at least one extension. The following definition takes
care of this issue.
Definition 6. For semantics ,   , we call a translation Tr
 weakly exact for     if there exists a collection S of sets of arguments, such that
for any AF F , (F ) =   (Tr (F )) \ S;
 weakly faithful for     if there exists a collection S of sets of arguments, such that
for any AF F , (F ) = {E  AF | E    (Tr (F )) \ S} and |(F )| = |  (Tr (F )) \ S|.
We sometimes refer to the elements from S as remainder sets. Note that S depends
only on the translation, but not on the input AF. Thus, by definition, each S  S only
contains arguments which never occur in AFs subject to translation. In other words, we
reserve certain arguments for introduction in weak translations.
Finally, we mention that the properties from Definition 4 as well as being exact, weakly
exact and faithful are transitive, i.e. for two transformations satisfying one of these properties, also the concatenation satisfies the respective property. However, transitivity is not
guaranteed for being weakly faithful.

4. Translations
In this section, we provide numerous faithful translations between the semantics introduced
in Definition 3. As minimal desiderata, we want the translations to be efficient, monotone,
and covering (see Definition 4). Thus, in this section when speaking about translations we
tacitly assume that they satisfy at least these three properties.
4.1 Exact Translations
We start with a rather simple such translation, which we will show to be exact for prf  sem
and adm  com.
Translation 1. The translation Tr 1 is defined as Tr 1 (F ) = (A , R ), where
A = AF  AF
R = RF  {(a, a ), (a , a), (a , a ) | a  AF },
with AF = {a | a  AF }.
454

fiOn the Intertranslatability of Argumentation Semantics

a

b

c

d

e

a

b

c

d

e

Figure 4: Tr 1 (F ) for the AF F from Example 1.
A few words about the intuition behind the above translation (for illustration see Figure 4 which depicts the translation of our example AF from Example 1): the new arguments
a  AF are all self-attacking and thus never appear in any extension of the resulting framework. However, each a attacks the original argument a (and a attacks a ), thus an argument
a is only defended by a set E in Tr 1 (F ) if a  E. Consequently, we have that in Tr 1 (F )
each admissible set is also a complete one.
Lemma 1. For an AF F and a set E of arguments, the following propositions are equivalent:
1. E  adm(F )
2. E  adm(Tr 1 (F ))
3. E  com(Tr 1 (F ))
Proof. As all arguments in AF are self-conflicting, every conflict-free set E of Tr 1 (F ) satisfies E  AF . Further, since Tr 1 is embedding, E is conflict-free in F iff E is conflict-free
in Tr 1 (F ). Moreover, since Tr 1 only adds symmetric attacks against arguments a  AF ,
we have that E defends its arguments in F iff E defends its arguments in Tr 1 (F ). Thus,
adm(F ) = adm(Tr 1 (F )) and (1)(2) follows. For (2)(3), let a  A be an arbitrary
argument and E  A. In Tr 1 (F ) the argument a is attacked by a and a is the only
attacker (except a itself) of a . Hence, for each a  A, E defends a only if a  E and
thus every admissible set of Tr 1 (F ) is also a complete one. Finally, (2)(3) holds since
com(F )  adm(F ) is true for any AF F .
Concerning Tr 1 we observe another side effect. As already mentioned a  A is the only
argument attacking a . Thus different preferred extensions of Tr 1 (F ) have incomparable
range (recall Definition 2), and therefore each preferred extension of Tr 1 (F ) is also a semistable extension of Tr 1 (F ).
Lemma 2. For an AF F and a set E of arguments, the following propositions are equivalent:
1. E  prf (F )
2. E  prf (Tr 1 (F ))
3. E  sem(Tr 1 (F ))
Proof. For (1)(2), it is sufficient to show that E  adm(F ) iff E  adm(Tr 1 (F )) holds
for each E. This is captured by Lemma 1. For (2)(3), let D, E  prf (Tr 1 (F )) and,
+
+
towards a contradiction, assume that DR
/ sem(Tr 1 (F )). As both D and
  ER , i.e. D 
455

fiDvorak & Woltran

a

b

c

d

e

a

b

c

d

e

Figure 5: Tr 2 (F ) for the AF F from Example 1.
E are preferred extensions, we have D 6 E. Thus, there exists an argument a  D \ E.
+
 / E + , a contradiction to D +  E + .
By construction of Tr 1 (F ), we get a  DR
 but a 
R
R
R
(2)(3) follows from the fact sem(F )  prf (F ) for any AF F .
Obviously Tr 1 is an embedding translation and as the introduction of a new argument
or attack in Tr 1 only depends on one original argument it is also modular. Together with
the results from Lemma 1 and 2 we thus get our first main result.
Theorem 1. Tr 1 is a modular, embedding, and exact translation for prf  sem and
adm  com.
Our next translation, Tr 2 , is concerned with stage and semi-stable semantics. In addition to Tr 1 , we make all attacks from the original AF symmetric (thus Tr 2 will not be
embedding) and add for each original attack (a, b) also an attack (a, b ).
Translation 2. The translation Tr 2 is defined as Tr 2 (F ) = (A , R ), where
A = AF  AF
R = RF  {(b, a), (a, b ) | (a, b)  RF }
 {(a, b) | a  AF , (b, b)  RF }
 {(a, a ), (a , a ) | a  AF }
The symmetric attacks in Tr 2 (F ) mirror the fact that we do not mind the orientation
of attacks when considering conflict-freeness. In other words, we exploit the well known
property that for symmetric frameworks conflict-free and admissible sets coincide. However,
making attacks symmetric destroys the original range of extensions. Thus we make use of
arguments a  AF in the sense that, for a given set E of arguments, an argument a is
+
+
contained in ER
Likewise, we have to add attacks into self iff a is contained in ER .
defeating arguments. The technical reason for this is that we require that each original
argument is attacked by a maximal conflict-free non-empty set in Tr 2 (F ) (see also the
proof of the forthcoming lemma). For illustration we refer to Figure 5.
Lemma 3. For an AF F and any set E of arguments, the following propositions are
equivalent:
1. E  stg(F )
2. E  stg(Tr 2 (F ))
3. E  sem(Tr 2 (F ))
456

fiOn the Intertranslatability of Argumentation Semantics

a

c

b

d

e

t
Figure 6: Tr 3 (F ) for the AF F from Example 1.
Proof. First, we mention that every stage extension of an AF F is also maximal (wrt.
) conflict-free in F . Let us now consider the case where   stg(F ). We then have that
stg(F ) = {} which is equivalent to, for each a  AF also (a, a)  RF . Then by construction
of Tr 2 for each a  A also (a, a)  R and therefore stg(Tr 2 (F )) = sem(Tr 2 (F )) = {}.
Hence the lemma holds for such AFs, and for the remainder of the proof we can assume
that  6 stg(F ).
For (1)(2), we again observe that a set E is conflict-free in F iff it is conflict-free in
+ 
+
Tr 2 (F ). In the following we use (ER
) as a short hand for {a  A | a  ER
}. Then
F
F
+ 
+


we have that (ERF )  ER , since for each (a, b)  RF , we have (a, b )  R . Furthermore,
+
for each maximal conflict-free set E in F (and thus in Tr 2 (F )), it holds that AF  ER
.
+
We show this by contradiction. To this end, let us assume that AF 6 ER
 , i.e. there
+
exists a  AF such that a 6 ER
 . As E 6=  we have that all self-attacking arguments are
+
+
R a and a 6R E, but
contained in ER , thus (a, a) 6 R . As a 6 ER
 we have that E 6
then the set E {a} is conflict-free in F and as E is maximal a  E; a contradiction. Hence,
for each maximal conflict-free set E  AF in F , i.e. the candidates for stage extensions, it
+
+ 
+
+
holds that ER
is maximal (wrt. subset inclusion) iff ER
 = AF  (ER ) and thus ER
 is
F
F
maximal.
For (2)(3), observe that each a  AF with (a, a) 6 R defends itself in Tr 2 (F ) and all
arguments a  AF are self-conflicting. Thus, admissible and conflict-free sets coincide in
Tr 2 (F ). Consequently, the stage and semi-stable extensions of Tr 2 (F ) coincide.
By definition the translation Tr 2 is covering, but not embedding. Moreover, as each selfattacking argument is attacked by all of the other arguments Tr 2 is not modular. Together
with the above lemma, we thus obtain the following result.
Theorem 2. Tr 2 is an exact translation for stg  sem.
The next translations consider the stable semantics as source formalism. Recall that
not all AFs possess a stable extension, while this holds for all other semantics (also recall
we excluded empty AFs for our considerations). Thus we have to use weak translations
as introduced in Definition 6. Our first such translation is weakly exact and uses a single
remainder set {t} (recall the definition of remainder sets as given in Definition 6).
Translation 3. The translation Tr 3 (F ) is defined as Tr 3 (F ) = (A , R ) where
A = AF  {t}
R = RF  {(t, a), (a, t) | a  AF }
457

fiDvorak & Woltran

Here the intuition is rather simple, see also Figure 6. In fact, the new argument t in
Tr 3 (F ) encodes that there might not exist a stable extension for F . Thus none of the
(other) arguments in Tr 3 (F ) is accepted, whenever t is accepted. Since the argument t
guards that there exists at least one stable extension of Tr 3 (F ) (for any AF F ), namely
{t}, we can make use of the fact that stable, semi-stable and stage semantics thus coincide
for Tr 3 (F ).
Lemma 4. Let F = (A, R) be an AF and E  A. Then the following statements are
equivalent:
1. E  stb(F )
2. E  stb(Tr 3 (F ))
3. E  sem(Tr 3 (F ))
4. E  stg(Tr 3 (F ))
Further for each E  (Tr 3 (F )) with   {stb, sem, stg} either E = {t} or t 6 E holds.
Proof. As the translation does not modify the original AF F , i.e. Tr 3 is embedding, we
have that for each E  AF , E is conflict-free in F iff E is conflict-free in Tr 3 (F ).
+
(1)(2): Each E  stb(F ) by definition is non-empty, conflict-free and satisfies ER
=
F

+
R

AF . By construction it also holds that E  t and thus ER = A , i.e. E  stb(Tr 3 (F )).
For (1)(2) consider E  stb(Tr 3 (F )), E  AF . Then by definition we have that E is
+

conflict-free in Tr 3 (F )) and thus in F ; moreover, ER
 = A and as Tr 3 is embedding also
+
ER
= AF . Hence E  stb(F ).
For (2)(3)(4), we mention that {t} is a stable extension of Tr 3 (F ) for any AF
F . Furthermore, we know that if there exists a stable extension for an AF, then stable,
semi-stable and stage extensions coincide.
Finally as the argument t is in conflict with all of the other arguments the only extension
E with t  E is the set {t}.
Adding argument t and the corresponding attacks to the source AF is a modular operation and as no further attacks are added Tr 3 is also embedding.
Theorem 3. Tr 3 is modular, embedding and weakly exact for stb  ,   {sem, stg}.
Proof. The result follows from Lemma 4, which states that sem(Tr 3 (F )) = stg(Tr 3 (F )) =
stb(F )  {{t}}. Thus by taking as remainder set S = {{t}}, Tr 3 is weakly exact.
We continue with a different translation from stable to other semantics.
Translation 4. Tr 4 is defined as Tr 4 (F ) = (A , R ) where
A = AF  AF
R = RF  {(b , a) | a, b  AF }
 {(a , a ), (a, a ) | a  AF }
 {(a, b ) | (a, b)  RF }.
458

fiOn the Intertranslatability of Argumentation Semantics

a

b

c

d

e

a

b

c

d

e

Figure 7: Tr 4 (F ) for the AF F from Example 1.
As before in translation Tr 2 , new arguments a  AF are used to encode the range of
an extension in the sense that a is attacked by a set E in Tr 4 (F ) only if a is in the range
of E in F . However, given the fact that each a  AF attacks back all original arguments
a  A, we can now accept an argument in a set E only if all arguments are in the range
of E. For illustration on our running example, see Figure 7. Observe that in our example
each of the arguments a , b , c , d , e attacks each of the arguments a, b, c, d, e.
Lemma 5. Let F = (A, R) be an AF and E  A with E 6= . Then, the following
statements are equivalent:
1. E  stb(F )
2. E  stb(Tr 4 (F ))
3. E  adm(Tr 4 (F ))
4. E  prf (Tr 4 (F ))
5. E  com(Tr 4 (F ))
6. E  sem(Tr 4 (F ))
Further for each conflict-free set E of Tr 4 (F ) it holds that E  A.
Proof. First, as all arguments a  A are self-attacking, for each conflict-free set E in
Tr 4 (F ) it holds that E  A. Since the translation is embedding, any set E is conflict-free
in F iff it is conflict-free in Tr 4 (F ). To show (1)(2), let E  stb(F ). Hence, for all
a  A \ E, E R a. We now claim that each argument in A \ E is attacked by E in
Tr 4 (F ). We distinguish between two cases for the different arguments in A \ E:
(i) a  A \ E: The construction of Tr 4 (F ) preserves all attacks in R. Thus as each

a  A \ E satisfies E R a, we obtain that E R a


(ii) a  A : In case a  E we have E R a , since (a, a )  R In case a  A \ E, by
the assumption E  stb(F ), there exists an argument b  E such that (b, a)  R. But

then by construction (b, a )  R and thus E R a .
Together with our observations about conflict-free sets, we get E  stb(Tr 4 (F )).

Vice versa, to show (1)(2) we get, for E  stb(Tr 4 (F )), E R a, for each a  A \ E,
and thus, in particular, for each a  A \ E. By definition of Tr 4 , we also have E R a for
each a  A \ E. Thus E  stb(F ) follows.
459

fiDvorak & Woltran

To show (2)(3), let E be a nonempty admissible extension of Tr 4 (F ) and a  E. By
construction, we have that a := {b  A | (b, a)  R }  A . As E  adm(Tr 4 (F )),


E R a for each a  A . But E  a only if either a  E or E R a. Thus for every

a  A it holds that either a  E or E R a; hence, E  stb(Tr 4 (F )).
The remaining implications follow by well-known relations between the semantics, i.e.
stb(G)  sem(G)  prf (G)  com(G)  adm(G), for each AF G. Hence, in particular, since for Tr 4 (F ), stable extensions and non-empty admissible sets coincide, the claim
follows.
Clearly Tr 4 is an embedding translation, but as for each new argument we add attacks
to all original arguments, Tr 4 is not modular.
Theorem 4. Tr 4 is an embedding and weakly exact translation for stb   with  
{adm, com, prf , sem}.
Proof. By Lemma 5, we in particular have that stb(F ) = (Tr 4 (F )) \ {}, for any AF
F . Thus taking  as a remainder set, we obtain that Tr 4 is weakly exact for the involved
semantics.
Thus we have that both Tr 3 and Tr 4 are weakly exact translations for stb  sem, of
course with different remainder sets. Due the to different properties of two translations it
depends on the concrete application which of them would be the better choice.
4.2 Faithful Translations
So far, we have only introduced exact and weakly exact translations. We now present
translations which relax this semantical property, i.e. we switch to faithful translations. As
a first example, we consider a translation for stg  sem which is faithful and embedding,
but not exact. This is in contrast to translation Tr 2 which is exact for stg  sem but
not embedding. As we will see in Section 5 it is impossible to give a translation that is
both embedding and exact for stg  sem, thus one has to decide which property is more
important for a concrete application scenario.
Translation 5. The translation Tr 5 (F ) is defined as Tr 5 (F ) = (A , R ) where
A = AF  AF  AF
R = RF  {(a, a), (a, a) | a  AF }
 {(a, a ), (a , a ) | a  AF }
 {(a, b ) | (a, b)  RF }
As in Tr 2 (F ) the arguments a  AF handle the range of the original extensions. But
instead of making original attacks symmetric (as in Tr 2 ) we add the arguments a  AF
to encode that an argument is not in the extension (also compare Figures 5 and 8). In
fact, such meta-arguments indicating that some a is out of an extension will be used in all
faithful translations presented in this subsection.
Lemma 6. Let F = (A, R) be an AF, E  A and E  = E  (A \ E). The following
statements are equivalent:
460

fiOn the Intertranslatability of Argumentation Semantics

a

b

c

d

e

a

b

c

d

e

a

b

c

d

e

Figure 8: Tr 5 (F ) for the AF F from Example 1.
1. E  stg(F )
2. E   stg(Tr 5 (F ))
3. E   sem(Tr 5 (F ))
Moreover for each S  sem(Tr 5 (F )) there exists a set E  A such that S = E  (A \ E).
Proof. First we prove that each S  stg(Tr 5 (F )) is of the form S = E  (A \ E). As S is
conflict-free we have that AF  S =  (each a  A is self-attacking) and for each a  A
that {a, a} 6 E  (as a attacks a and vice versa). Further as each stage extension is also a
-maximal conflict-free set we have that for each a  A either a  S or a  S. Hence there
exists an E  A such that S = E  (A \ E).
(1)(2): Let E  stg(F ). It is easy to see that E  is conflict-free in Tr 5 (F ). By
construction for each argument a  A either a  E  or a  E  holds and there are mutual
attacks between a and a, hence we have that A  A  (E  )+
R . Next we observe that each
  a . Further by the definition of
iff
E
a  A is self-attacking and thus a  (E  )+
R
Tr 5 (F ) each argument a is attacked by a and all arguments b such that (b, a)  R. That
+
is a  (E  )+
R iff either a  E or there exists a b  A such that (b, a)  R iff a  (E)R . By
+
assumption E is a stage extension of F and thus we have that (E)R is -maximal. Using
the above observation we have that also (E  )+
R is -maximal in Tr 5 (F ) and therefore
E   stg(Tr 5 (F )).
(1)(2): Let E   stg(Tr 5 (F )). We recall that E  is of the form S = E  (A \ E), for
some E  A. It can be easily checked that E is conflict-free in F . By the above observation
+
 +
that a  (E  )+
R iff a  (E)R and the fact that (E )R is -maximal in Tr 5 (F ) we get that
+
also ER
is -maximal in F . Hence, E  stg(F ).
(2)(3): Let us consider E   stg(Tr 5 (F )). As we have already observed, E  is of the
desired form and for each a  AF  AF either a  E  or E   a. Further by construction an
argument b  AF does not attack E  . We can conclude that each stage extension defends
itself against all attackers, i.e. is an admissible set. Hence, stage and semi-stable extensions
of Tr 5 (F ) coincide.
By above lemma and construction of Tr 5 , the following result is immediate.
Theorem 5. Tr 5 is a modular, embedding and faithful translation for stg  sem.
Next we give a faithful translation from admissible semantics to stable, semi-stable and
stage semantics.
461

fiDvorak & Woltran

a

b

c

d

e

a

b

c

d

e

(a, b)

(c, b)

(d, c)

(c, d)

(d, e)

(e, e)

Figure 9: Tr 6 (F ) for the AF F from Example 1.
Translation 6. The translation Tr 6 (F ) is defined as Tr 6 (F ) = (A , R ) where
A = AF  AF  RF
R = RF  {(a, a), (a, a) | a  AF }
 {(r, r) | r  RF }
 {(a, r) | r = (y, a)  RF }
 {(a, r) | r = (z, y)  RF , (a, z)  RF }
The main idea is to use additional arguments (a, b)  A which represent the attack
relations from the source framework in order to capture admissibility as follows: (a, b)
is attacked by an extension E  in Tr 6 (F ) if (a, b) is not critical wrt. the corresponding
extension E in F , meaning that either b 
/ E or there exists a c  E such (c, b)  RF , i.e.
a is defended by E. For instance, consider the argument (c, b) in the translation of our
example framework as depicted in Figure 9. Then, we have that (1) b attacks (c, b) since if b
is chosen to be out (i.e. b is chosen in), there is no need to defend b; (2) d attacks (c, b) since
if d is chosen in, d defends b against attacker c (recall that (d, c) is present in the source
AF). Thus, as long as (c, b) is attacked by some argument, b is treated corrected in terms
of admissibility (wrt. attacker c). Note that in our example b cannot be defended against
a, thus the only way to get (a, b) into the range is to select b to be out.
Lemma 7. Let F = (A, R) be an AF, E  A and E  = E  (A \ E). The following
statements are equivalent:
1. E  adm(F )
2. E   stb(Tr 6 (F ))
3. E   sem(Tr 6 (F ))
4. E   stg(Tr 6 (F ))
Moreover for each E   (Tr 6 (F )) (  {stb, sem, stg}) there exists a set E  A such that
E  = E  (A \ E).
Proof. (1)(2): Let E  adm(F ). It is easy to see that E  is conflict-free in Tr 6 (F ) and

further that A  A  (E  )+
R . It remains to show that each argument r  A for r  R is
462

fiOn the Intertranslatability of Argumentation Semantics



attacked by E  . Let (a, b) be such an argument r. If b 
/ E then b  E  and thus E  R r.

Otherwise, b  E (thus b  E ) and, by assumption, E defends b in F , i.e. (c, a)  R for

some c  E (thus c  E  ). By construction, (c, r)  R and E  R r.
(1)(2): Let E   stb(Tr 6 (F )). E  is conflict-free, thus R  E  =  and {a, a} 6 E 
for all a  A. By construction, E is conflict-free in F . It remains to show that E defends
all its arguments in F . Let b  A \ E such that b R a for some a  E. Then there exists
an argument (b, a) in Tr 6 (F ) attacked by E. As a  E we have that a 
/ E  and thus there
exists an argument c  E such that (c, b)  R.
(2)(3)(4): As the empty set is always admissible we have that A is always a stable
extension of Tr 6 (F ). Hence, stable, semi-stable and stage extensions coincide in Tr 6 (F ),
for any AF F .
Observe that in the construction of Tr 6 drawing attacks {(a, r) | r = (z, y)  RF , (a, z) 
RF } depends on two attacks and three arguments from the original framework. Hence Tr 6
is not modular. By Lemma 7 the next result follows quite easily.
Theorem 6. Translation Tr 6 is embedding and faithful for adm   (  {stb, sem, stg}).
In our faithful translation from complete to stable semantics which we present next, we
extend the given AF by arguments that represent whether an argument is attacked in the
corresponding extension or not. Further we add arguments that ensure admissibility and
completeness. The entire translation is thus slightly more complicated; see also Figure 10
which depicts the translated framework for our running example.
Translation 7. The translation Tr 7 (F ) is defined as Tr 7 (F ) = (A , R ) where
A = AF  AF  AF  AF  AF  RF
R = RF  {(x, x) | x  AF  RF }
 {(a, a), (a, a), (a , a ), (a, a ) | a  AF }
 {(a, b ), (a , b ) | (a, b)  RF }
 {(a, r ), (b , r ) | r = (b, a)  RF }
The intuition behind arguments AF , AF , and RF is similar as in previous translations.
An argument a  AF indicates that a is attacked by an extension E of F , while a  AF
says that a is not attacked by E.
Lemma 8. Let F = (A, R) be an AF, E  A and E  = E  (A \ E)  {a | E R a}  {a |
E 6R a}. Then the following statements are equivalent:
1. E  com(F )
2. E   stb(Tr 7 (F ))
3. E   sem(Tr 7 (F ))
4. E   stg(Tr 7 (F ))
Moreover for each E   (Tr 6 (F )) (  {stb, sem, stg}) there exists a set E  A such that
E  = E  (A \ E)  {a | E R a}  {a | E 6R a}.
463

fiDvorak & Woltran

a

b

c

d

e

a

b

c

d

e

a

b

c

d

e

a

b

c

d

e

a

b

c

d

e

(a, b)

(c, b)

(d, c)

(c, d)

(d, e)

(e, e)

Figure 10: Tr 7 (F ) for the AF F from Example 1.
Proof. To show (1)(2), let E  com(F ). Then by construction E  is conflict-free in

Tr 7 (F ) (for x, y  E we have x R y  x R y). Moreover, by definition of E  , it can

 +
be verified that A  A  A  A  (E  )+
R . Thus it remains to show that (i) A  (E )R
and (ii) R  (E  )+
R .
(i) Let a  A be an arbitrary argument of F . As E is a complete extension we have that
either a  E, and thus a  E  , or there exists an attack (b, a)  R with E 6R b, and

thus b  E  . As by construction (b , a )  R we thus have that E  R a .
(ii) Let r = (b, a)  R be an arbitrary attack of F . As E is admissible it holds that either

a
/ E, and thus a  E  , or E R b, and thus b  E  . In both cases E  R r.
Putting things together, we get that A  A  A  A  A  R = A  (E  )+
R which is
equivalent to E  being a stable extension of Tr 7 (F ).
To show (1)(2), let E   stb(Tr 7 (F )). First we prove that E  is of the desired form.
As E  is both conflict-free and -maximal we clearly have that E   (A  A) = E  A \ E
for some E  A. Let now a  A be an arbitrary argument. We have that a  E  iff
a 6 E  . But as E  is stable a 6 E  iff there exists an attack (b, a ) such that b  E  . By
construction of Tr 7 (F ) this is equivalent to b  E and therefore E R a. Thus E  is of
the desired form, it remains to show that E is complete. As mentioned before we have for

x, y  E : x R y  x R y and thus E is conflict-free in F . Thus it remains to show
that (i) E defends each of its arguments in F and (ii) E contains each argument defended
by E in F .
(i) Let us assume there exists an argument a  E not defended by E. Thus there exists
r = (b, a)  R, E 6 b. By construction we also have that a 
/ E  (as a  E) and


b 6 E (as E 6 b). But in Tr 7 (F ) the self-attacking argument r is only attacked by
the arguments a, b (and itself). Hence, this is in contradiction to E  being a stable
extension.
464

fiOn the Intertranslatability of Argumentation Semantics

(ii) Let a  A be an argument defended by E. Then for all arguments b R a we have
that E R b and thus b  E  and b 
/ E  . Recall that in Tr 7 (F ) the argument a is
self-attacking and thus does not belong to E  and is only attacked by the arguments
b and a. As E  is a stable extension and a 6 E  we have that a  E  and a  E.
(2)  (3)  (4): As there always exists a complete extension we know that any framework Tr 7 (F ) has a stable extension. But then stable, stage and semi-stable extensions
coincide.
Translation Tr 7 introduces a huge number of new arguments, despite this the introduction of a concrete argument or attack only depends on a single argument or attack. Hence
Tr 7 is modular. It is easily checked that Tr 7 is also embedding. Together with Lemma 8
we thus can state the following result for Tr 7 .
Theorem 7. Tr 7 is a modular, embedding and faithful translation for com   ( 
{stb, sem, stg}).
Finally we present a translation from grounded semantics to most of the other semantics
under our focus, i.e. to all semantics except admissible semantics. The main idea is to
simulate the computation of the least fixed-point of the characteristic function FF (S) =
{x  AF | x is defended by S} of an AF F within the target AF.
Translation 8. The translation Tr 8 (F ) is defined as Tr 8 (F ) = (A , R ) where
A = AF,1  AF,1      AF,l  AF,l
R = RF  {(ai , bi ) | (a, b)  R, i  [l]}
 {(ai , bi+1 ) | (a, b)  R, i  [l  1]}
with AF = AF,l and l =  |A2F | .
For illustration, we use here a slightly different example depicted in Figure 11(a). Observe that this AF has {a, c, d} as its grounded extension. The translated framework is
given in Figure 11(b).
The intuition behind arguments ai  AF,i is that a  FFi (), while the intuition of
(i1)
() 6 a. The integer l is an upper bound for the number of
ai  AF,i is that FF
iterations we need to reach the least fixed-point, i.e. the grounded extension.
Lemma 9. Let F = (A, R) be an AF and E  the grounded extension of Tr 8 (F ). Then
E   A is the grounded extension of F . We further have that on Tr 8 (F ) the grounded,
stable, complete, preferred, semi-stable and stage extensions coincide.
Proof. We recall the definition of the characteristic function FF of an AF F , defined as
FF (S) = {x  AF | x is defended by S}, and that the grounded extension of F is the least
fix-point of FF . Further we use as a shorthand F  = Tr 8 (F ). One can show that for
arbitrary a  A we have
(i) ai  E  iff a  FFi ();
(ii) ai  E  iff FFi1 () 6R a; and
465

fiDvorak & Woltran

a

c

b

d

e

a1

b1

c1

d1

e1

a1

b1

c1

d1

e1

a2

b2

c2

d2

e2

a2

b2

c2

d2

e2

a3

b3

c3

d3

e3

a

b

c

d

e

(a) AF F

(b) Tr 8 (F )

Figure 11: An example for Tr 8 .
(iii) AF,i  (E  )+
R .
(iv) AF,i  (E  )+
R .
We prove this by structural induction. As induction base we show (ii) and (iv) for the
arguments a1 . For a  A we have that a1  E  as they are not attacked by any argument.
This coincides with the fact that FF0 () =  doesnt attack any argument and thus (ii) and
(iv) holds.
We have two induction steps: (1) Showing that (i) and (iii) hold for arbitrary n iff (ii)
and (iv) hold for n; and (2) showing that (ii) and (iv) hold for arbitrary n iff (i) and (iii)
hold for n  1.
(1) We assume (ii) and (iv) hold for all an . By the definition of FF we have that a  FFn ()
iff all b  a = {b  A | b  a} are attacked by FFn1 (). Applying the induction
hypothesis (ii) to b  a we obtain that a  FFn () iff each bi  {bi | (b, a)  R} is
attacked by E  . Further, as by the construction of Tr 8 (F ) these are the only attackers
of a, this is equivalent to argument ai being defended by E  . Now recall that the each
argument defended by the grounded extension is indeed contained in the grounded
extension. Hence, a  FFn () iff ai  E  and (i) holds.
To show (iii) we consider ai  AF,i . If ai  E  then clearly ai  (E  )+
R . Thus let
us consider ai 
/ E  . Then, by the above observations, there exists a bi such that
bi  ai and E  6 bi . Using the latter and the induction hypothesis (iv) we obtain
that bi  E  . Now we have that E   ai , hence ai  (E  )+
R and we obtain (iii).
(2) Now let us assume that (i) and (iii) hold for all an1 . We have that FFn1 ()  a
iff there exists b  FFn1 ()  {b | (b, a)  R }. By induction hypothesis this holds iff
466

fiOn the Intertranslatability of Argumentation Semantics

there exists a bi1  E  such that (b, a)  R. In other words there exists bi1  E 

such that bi1 R ai , which implies that ai 6 E  . Moreover if there is no bi1  E 

such that bi1 R ai , by assumption (iii) we have that E  defends ai and thus
ai  E  . Hence (ii) and (iv) hold.
Furthermore when applying the FF operator we either add a new argument to the set
and attack an additional argument or we reach the fixed-point. So in each step we make a
decision about at least two arguments and thus FFl () = grd (F ). In combination with (i),
we get that al  E  iff a  grd (F ). Moreover by (iii) and (iv) it holds that E  is also a stable
extension and thus grd (F  ) = stb(F  ) = com(F  ) = prf (F  ) = sem(F  ) = stg(F  ).
As in Tr 8 the integer value l depends on the size S of the source AF, Tr 8 is not modular.
However, it can be verified that the computation of the translation only requires logarithmic
space wrt. S and that Tr 8 is embedding (the original AF is indeed contained in the resulting
AF; see also the bottom layer in Figure 11(b)). Our final result concerning translations thus
follows immediately from Lemma 9.
Theorem 8. Tr 8 is an embedding and faithful translation for grd   (  {stb, com,
prf , stg, sem}).

5. Negative Results
In this section, we present results fortifying that for several semantics there does not exist
any translation with the desired properties. The first result, which is rather straight forward,
relies on the fact that the grounded semantics is a unique-status semantics.
Proposition 3. There is no (weakly) faithful translation for   grd with   {sem,
stg, prf , com, stb, adm}.
Proof. For instance consider the AF F = ({a, b}, {(a, b), (b, a)}). We have that {{a}, {b}} 
(F ) for   {sem, stg, prf , com, stb, adm} but the grounded semantics always proposes a
unique extension.
We observe that in general it holds that if  is a multiple status semantics and   is a
unique status semantics then there is no (weakly) faithful translation for     .
Further results are based on complexity gaps between different semantics (see Table 1)
and the fact that certain translations preserve some decision problem. We start with cases
where it is impossible to find efficient faithful translations; even if we allow for weakly
faithful translations, cf. Definition 6. Afterwards, we give some negative results concerning
(weakly) exact translations.
The following theorem concerns the intertranslatability of preferred, semi-stable and
stage semantics, i.e. the semantics where skeptical acceptance is P2 -complete. The underlying reason for the impossibility result is the complexity gap for the credulous acceptance
problems.
Theorem 9. There is no efficient (weakly) faithful translation for sem  prf or stg  prf
unless P2 = NP.
467

fiDvorak & Woltran

Proof. Let Tr be an efficient (weakly) faithful translation from   {sem, stg} to prf . By
definition this translation is L-computable and as we show next reduces Cred to Credprf :
Let F = (A, R) be an arbitrary AF, x  A an argument. First let us assume that x is
credulously accepted wrt. to . Hence, there exists an E  (F ) with x  E. As Tr is
a weakly faithful translation, there is an E   prf (Tr (F )), such that E   A = E. Thus
x  E  , i.e. x is credulously accepted wrt. preferred semantics in Tr (F ).
So assume x is credulously accepted in Tr (F ) wrt. to prf , i.e. x  E  for some E  
prf (Tr (F )). By x  E   A we can conclude that E  is not a remainder set of Tr . As Tr is
a weakly faithful translation we have that E = E   A is in (F ), and thus x is credulously
accepted in F wrt. . Thus, Tr is a L-reduction from the P2 -hard problem Cred to the
NP-easy problem Credprf .
The following theorem makes use of complexity gaps for the skeptical acceptance.
Theorem 10. There is no efficient (weakly) faithful translation for     , where  
{sem, stg, prf } and    {com, stb, adm}, unless P2 = NP.
Proof. Given an efficient weakly faithful translation Tr with remainder set S for     we
have that Skept is translated to the problem Skept S , that is deciding whether an argument
is in each   -extension which is not in the set S. Next we show that the problem Skept S
remains in coNP. One can disprove Skept S , by guessing a set E  A, such that a 6 E
and verify that E    (F ) and E 6 S. As Ver   P and the set S is fixed, i.e. S does
not depend on the input, this is an NP-algorithm. Hence proving Skept S is in coNP. Thus
Tr would be an L-reduction from the P2 -hard problem Skept to the coNP-easy problem
Skept S , which implies P2 = NP.
One might prefer (weakly) exact over (weakly) faithful translations. As we have seen in
Section 4, several of our translations are not exact but only faithful. In these cases we are
interested in either finding an exact translation or an evidence that an exact translation is
not possible. The following theorems approve that it was appropriate to have given only a
(weakly) faithful translation in Section 4, as there cannot be any exact such translation.
Theorem 11. There is no (weakly) exact translation for     where   {adm, com}
and    {stb, prf , sem, stg}.
Proof. This is basically by the fact that admissible resp. complete extensions may be in a
-relation; consider e.g. F = ({a, b}, {(a, b), (b, a)}) with (F ) = {{a}, {b}, }. Let us now
assume there exists a (weakly) exact translation Tr for     . By definition, (F ) =
{{a}, {b}, }    (Tr (F )), but as   {a} this contradicts    {stb, prf , sem, stg}.
Theorem 12. There is no (weakly) exact translation for com  adm.
Proof. We observe that for every AF F it holds that   adm(F ), but there are AFs
where  
/ com(F ). Thus for a weakly exact translation Tr , with the collection S of
remainder sets, it holds that   S. But then, given an AF F with   com(F ), e.g.
F = ({a, b}, {(a, b), (b, a)}), we can conclude that   adm(Tr (F )) \ S, a contradiction.
Theorem 13. There is no efficient (weakly) exact translation for grd   where  
{stb, adm, com}, unless L = P.
468

fiOn the Intertranslatability of Argumentation Semantics

f

h
g1
a

g2

e
c

b

d

Figure 12: Counterexample for exact translations   stg (  {sem, prf }).
Proof. Let us, towards a contradiction, assume that there exists an efficient (weakly) exact
translation Tr for grd  . For a given AF F = (A, R) with a set E  A it holds that
E  grd (F ) iff E  (Tr (F )). Thus Tr would be an L-reduction from the P-hard problem
Ver grd (see Proposition 1) to Ver  (  {stb, adm, com}) which is in L.
In Section 4 we presented two translations for stg  sem: Tr 2 which is an exact
translation, but not embedding, and Tr 5 which is an embedding and faithful translation,
but not exact. Let us also mention at this point that Tr 2 was the only translation presented
in Section 4 that is not embedding. Hence a natural question that occurs is whether a
translation that is embedding and exact for stg  sem is possible. We give a negative
answer to this question.
Theorem 14. There is no embedding and (weakly) exact translation for stg  sem.
Proof. Let us assume there exists an embedding and (weakly) exact translation Tr for
stg  sem. Consider the AF F = ({a, b}, {(a, a), (a, b)}) with stg(F ) = {{b}}. As Tr is
a (weakly) exact translation we have that {b}  sem(Tr (F )) and thus {b}  adm(Tr (F )).
Further we have that (a, b)  RTr (F ) (Tr (F ) is embedding) and thus {b} must attack
a. But then we have (b, a)  RTr (F ) which is contradiction to Tr being an embedding
translation.
Finally we present an impossibility result for prf  stg and sem  stg.
Theorem 15. There is no (weakly) exact translation for   stg (  {sem, prf }).
Proof. Consider the AF F = ({a, b, c, d, e, f, g1 , g2 , h}, {(g1 , g1 ), (g2 , g2 ), (a, b), (b, a), (c, d),
(d, c), (a, g1 ), (b, e), (c, e), (d, g2 ), (e, f ), (f, h), (h, e)}) illustrated in Figure 12. We have that
sem(F ) = {{b, d, f }, {a, c, f }, {a, d}} and prf (F ) = sem(F )  {{b, c, f }}.
To prove that there is no weakly exact translation for   stg (  {sem, prf }), we will
show that there exists no AF F  with sem(F )  stg(F  ). To this end, let us assume that
F  = (A , R ) is such an AF with {{b, d, f }, {a, c, f }, {a, d}}  stg(F  ). Using the fact that
{b, d, f } is conflict-free in F  we obtain that (d, f ), (f, d) 6 R and similar by using that
{a, c, f } is conflict-free in F  we get that (a, f ), (f, a) 6 R . By assumption {a, d}  stg(F  )
and thus {a, d} is a maximal conflict-free set of F  , but by the above observations the set
{a, d, f } is also conflict-free in F  , a contradiction.

469

fiDvorak & Woltran

grd
adm
stb
com
prf
sem
stg
grd id Tr 4  Tr 8 / - Tr 8 / - Tr 8 / Tr 8 / ?
Tr 8 / ? Tr 8 / ?
adm 
id
Tr 6 / - Tr 1 Tr 4  Tr 6 / - Tr 6 / - Tr 6 / stb

Tr 4
id
Tr 4
Tr 4
Tr 3 , Tr 4 Tr 3
com  Tr 4  Tr 7 / - Tr 7 / id
Tr 4  Tr 7 / - Tr 7 / - Tr 7 / prf




id
Tr 1
? /sem 




id
? /stg





Tr 2
id
Table 2: Results about (weakly) faithful / exact translations.

6. Conclusion
In this work, we investigated intertranslations between different semantics for abstract argumentation. We focused on translations which are efficiently computable and faithful (with a
few relaxations due to certain differences implicit to the semantics). An overview of our results is given in Table 2.3 The entry in row  and column   is to read as follows:  states
that we have shown (Section 5) that no efficient faithful (even weakly faithful) translation
for     exists. If the entry refers to a translation (or a concatenation of translations), we
have found an efficient (weakly) exact translation for     . An entry which is split into
two parts, e.g. Tr 8 / -, means that we have found an efficient (weakly) faithful translation,
but there is no such exact translation. ? indicates an open problem. We mention that
all the concatenated translations are weakly faithful as they are built from a weakly exact
translation Tr 4 (which has as only remainder set the empty set) and a faithful translation
(either Tr 6 , Tr 7 , or Tr 8 ).
Figure 13 illustrates our intertranslatability results at one glance. Here, a solid arrow
expresses that there is an efficient faithful translation while a dotted arrow depicts that there
may exist such a translation, but so far we have neither found one nor have an argument
against its existence. Furthermore, if for two semantics ,   there is no path from  to  
then it is proven (partly under typical complexity theoretical assumptions) that there is no
efficient faithful translation for     . If we consider the relations between the semantics
wrt. exactness rather than just faithfulness, the overall picture changes; see Figure 14. Here,
we get a more detailed picture about the relations between stable, admissible, and complete
semantics. One conclusion, we can draw from these pictures is that semi-stable semantics
is the most expressive one, since each of the other investigated semantics can be efficiently
embedded. Moreover, we believe that our investigations complements recent results about
comparisons between the different semantics proposed for argumentation frameworks.
Let us at this point also mention that, instead of considering different properties for the
translations, we could also have used slightly revised semantics. The notion of remainder
sets (as given in Definition 6) can partly be circumvented by, for instance, using a quasiadmissible semantics instead of admissible semantics, where the quasi-admissible extensions
3. One may notice that Tr 5 does not appear in the table. Recall that Tr 5 was proposed as an alternative
to Tr 2 satisfying slightly different properties for stg  sem; see also the discussion before Theorem 14.

470

fiOn the Intertranslatability of Argumentation Semantics

of an AF are all non-empty admissible extensions (in case such ones exist), or is only the
empty set otherwise. Also it is obvious that the more restricted the properties for a translation are, the less such translations exist (compare Figures 13 and 14). Hence, we observe
a certain trade-off between translation criteria and comparability between semantics.
An alternative option to obtain translations would have been to exploit known relations
between argumentation semantics and logic-programming semantics (see, e.g., Dung, 1995;
Wu, Caminada, & Gabbay, 2009) and making use of known translatability results for the
latter. However, we refrained from such an approach here, since it might blur the minimal
requirements for the translations under consideration. In particular, from the point of
meta-argumentation, translations via logic-programming semantics might introduce new
arguments just for technical reasons due to the logic-programming syntax, but which have
no meaning on the level of AFs.
semi-stable

preferred

stage

admissible, complete, stable

grounded

Figure 13: Intertranslatability of argumentation semantics wrt. weakly faithful translations.

semi-stable

preferred

stage

stable

admissible

complete

grounded

Figure 14: Intertranslatability of argumentation semantics wrt. weakly exact translations.
471

fiDvorak & Woltran

For future work, we identify the following tasks: First, we want to solve the few open slots
in Table 2. Second, further properties for translations could be of interest. For instance,
one could even strengthen the property of being exact (which is defined in terms of the
extensions) to the requirement that the labelings (Caminada & Gabbay, 2009) of the source
and target framework coincide. Labelings provide additional information, in particular for
arguments not contained in an extension. Likewise, it would be interesting to investigate
intertranslatability in the more general approach of equational semantics for argumentation
frameworks (Gabbay, 2011). Further properties for translations could be given in terms of
graph properties. As an example, acyclic AFs should remain acyclic after the translations,
or parameters as tree-width should remain unchanged. Requirements of such a form are also
termed structural preservation (Janhunen et al., 2006). Such properties are of interest
from a computational point of view in the sense that, in case the source AF is easy to
evaluate (because of its structure), this advantage should not be lost during the translation;
recall here Figure 1 where we suggested to use our translations for a rapid prototyping
approach to compute the extensions of a semantics via an argumentation engine based on
a different semantics. Finally, we plan to extend our considerations to other important
semantics like the ideal semantics (Dung et al., 2007), cf2-semantics (which is proposed
among others in Baroni et al., 2005), or resolution-based semantics (Baroni, Dunne, &
Giacomin, 2011), among which the resolution-based grounded semantics is of particular
interest. As well studying translations between semantics for generalizations of Dung-style
AFs as EAFs (Modgil, 2009) or AFRAs (Baroni, Cerutti, et al., 2011) is an interesting
subject for future work.

Acknowledgments
This work was supported by the Vienna Science and Technology Fund (WWTF) under grant
ICT08-028. A preliminary version of this paper has been presented at the International
Conference 30 Years of Nonmonotonic Logic.
The authors are grateful to Christof Spanring for suggesting the counterexample used
in the proof of Theorem 15. Moreover, the authors want to thank Tomi Janhunen as well as
the anonymous referees from the 30 Years of Nonmonotonic Logic symposium and from
JAIR for valuable comments which helped to improve the paper.

References
Amgoud, L., Dimopoulos, Y., & Moraitis, P. (2007). A unified and general framework for
argumentation-based negotiation. In Durfee, E. H., Yokoo, M., Huhns, M. N., & Shehory, O. (Eds.), Proceedings of the 6th International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2007), pp. 963970. IFAAMAS.
Baroni, P., Dunne, P. E., & Giacomin, M. (2011). On the resolution-based family of abstract
argumentation semantics and its grounded instance. Artif. Intell., 175 (3-4), 791813.
Baroni, P., Cerutti, F., Giacomin, M., & Guida, G. (2011). AFRA: Argumentation framework with recursive attacks. Int. J. Approx. Reasoning, 52 (1), 1937.
472

fiOn the Intertranslatability of Argumentation Semantics

Baroni, P., & Giacomin, M. (2009). Semantics of abstract argument systems. In Rahwan,
I., & Simari, G. (Eds.), Argumentation in Artificial Intelligence, pp. 2544. Springer.
Baroni, P., Giacomin, M., & Guida, G. (2005). SCC-recursiveness: A general schema for
argumentation semantics. Artif. Intell., 168 (1-2), 162210.
Baumann, R., & Brewka, G. (2010). Expanding argumentation frameworks: Enforcing and
monotonicity results. In Baroni, P., Cerutti, F., Giacomin, M., & Simari, G. R. (Eds.),
Proceedings of the 3rd Conference on Computational Models of Argument (COMMA
2010), Vol. 216 of Frontiers in Artificial Intelligence and Applications, pp. 7586. IOS
Press.
Bench-Capon, T. J. M., & Atkinson, K. (2009). Abstract argumentation and values. In
Rahwan, I., & Simari, G. (Eds.), Argumentation in Artificial Intelligence, pp. 4564.
Springer.
Bench-Capon, T. J. M., & Dunne, P. E. (2005). Argumentation in AI and law: Editors
introduction. Artif. Intell. Law, 13 (1), 18.
Besnard, P., & Hunter, A. (2001). A logic-based theory of deductive arguments. Artif.
Intell., 128, 203235.
Brewka, G., Dunne, P. E., & Woltran, S. (2011). Relating the semantics of abstract dialectical frameworks and standard AFs. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI 2011), pp. 780785. AAAI Press.
Caminada, M. (2006). Semi-stable semantics. In Dunne, P. E., & Bench-Capon, T. J. M.
(Eds.), Proceedings of the 1st Conference on Computational Models of Argument
(COMMA 2006), Vol. 144 of Frontiers in Artificial Intelligence and Applications, pp.
121130. IOS Press.
Caminada, M., & Amgoud, L. (2007). On the evaluation of argumentation formalisms.
Artif. Intell., 171 (5-6), 286310.
Caminada, M., & Gabbay, D. (2009). A logical account of formal argumentation. Studia
Logica, 93 (2), 109145.
Cayrol, C., & Lagasquie-Schiex, M. (2009). Bipolar abstract argumentation systems. In
Rahwan, I., & Simari, G. (Eds.), Argumentation in Artificial Intelligence, pp. 6584.
Springer.
Coste-Marquis, S., Devred, C., & Marquis, P. (2005). Symmetric argumentation frameworks. In Godo, L. (Ed.), Proceedings of the 8th European Conference on Symbolic
and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2005), Vol.
3571 of Lecture Notes in Computer Science, pp. 317328. Springer.
de Bruijn, J., Eiter, T., & Tompits, H. (2008). Embedding approaches to combining rules
and ontologies into autoepistemic logic. In Brewka, G., & Lang, J. (Eds.), Proceedings
of the 11th International Conference on Principles of Knowledge Representation and
Reasoning (KR2008), pp. 485495. AAAI Press.
Delgrande, J. P., & Schaub, T. (2005). Expressing default logic variants in default logic. J.
Log. Comput., 15 (5), 593621.
473

fiDvorak & Woltran

Denecker, M., Marek, W., & Truszczynski, M. (2003). Uniform semantic treatment of
default and autoepistemic logics. Artif. Intell., 143 (1), 79122.
Dimopoulos, Y., & Torres, A. (1996). Graph theoretical structures in logic programs and
default theories. Theor. Comput. Sci., 170 (1-2), 209244.
Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell., 77 (2),
321358.
Dung, P. M., Mancarella, P., & Toni, F. (2007). Computing ideal sceptical argumentation.
Artif. Intell., 171 (10-15), 642674.
Dunne, P. E., & Bench-Capon, T. J. M. (2002). Coherence in finite argument systems.
Artif. Intell., 141 (1/2), 187203.
Dunne, P. E., & Caminada, M. (2008). Computational complexity of semi-stable semantics
in abstract argumentation frameworks. In Holldobler, S., Lutz, C., & Wansing, H.
(Eds.), Proceedings of the 11th European Conference on Logics in Artificial Intelligence
(JELIA 2008), Vol. 5293 of Lecture Notes in Computer Science, pp. 153165. Springer.
Dvorak, W., & Woltran, S. (2010). Complexity of semi-stable and stage semantics in argumentation frameworks. Inf. Process. Lett., 110 (11), 425430.
Gabbay, D. M. (2009). Fibring argumentation frames. Studia Logica, 93 (2-3), 231295.
Gabbay, D. M. (2011). Equational approach to argumentation networks. Unpublished draft.
Gottlob, G. (1995). Translating default logic into standard autoepistemic logic. J. ACM,
42 (4), 711740.
Imielinski, T. (1987). Results on translating defaults to circumscription. Artif. Intell.,
32 (1), 131146.
Janhunen, T. (1999). On the intertranslatability of non-monotonic logics. Ann. Math. Artif.
Intell., 27 (1-4), 79128.
Janhunen, T., Niemela, I., Seipel, D., Simons, P., & You, J.-H. (2006). Unfolding partiality
and disjunctions in stable model semantics. ACM Trans. Comput. Log., 7 (1), 137.
Konolige, K. (1988). On the relation between default and autoepistemic logic. Artif. Intell.,
35 (3), 343382.
Liberatore, P. (2007).
abs/0707.3781.

Bijective faithful translations among default logics.

CoRR,

Marek, W., & Truszczynski, M. (1993). Nonmonotonic Logic: Context Dependent Reasoning.
Springer.
Modgil, S. (2009). Reasoning about preferences in argumentation frameworks. Artif. Intell.,
173 (9-10), 901934.
Modgil, S., & Bench-Capon, T. J. M. (2011). Metalevel argumentation. Accepted for publication in J. Log. Comput.. Available at http://dx.doi.org/doi:10.1093/logcom/
exq054.
Moore, R. C. (1985). Semantical considerations on nonmonotonic logic. Artif. Intell., 25,
7594.
474

fiOn the Intertranslatability of Argumentation Semantics

Pearce, D., & Uridia, L. (2011). The Godel and the splitting translations. Submitted Draft.
Preliminary Version has been presented at the International Conference 30 Years of
Nonmonotonic Logic.
Reiter, R. (1980). A logic for default reasoning. Artif. Intell., 13 (12), 81132.
Verheij, B. (1996). Two approaches to dialectical argumentation: admissible sets and argumentation stages. In Meyer, J., & van der Gaag, L. (Eds.), Proceedings of the 8th
Dutch Conference on Artificial Intelligence (NAIC96), pp. 357368.
Villata, S. (2010). Meta-Argumentation for Multiagent Systems: Coalition Formation, Merging Views, Subsumption Relation and Dependence Networks. Ph.D. thesis, Universita
degli Studi di Torino.
Wu, Y., Caminada, M., & Gabbay, D. M. (2009). Complete extensions in argumentation
coincide with 3-valued stable models in logic programming. Studia Logica, 93 (2-3),
383403.

475

fiJournal of Artificial Intelligence Research 41 (2011) 131-154

Submitted 11/10; published 5/11

Redistribution Mechanisms for Assignment of
Heterogeneous Objects
Sujit Gujar
Y Narahari

sujit@csa.iisc.ernet.in
hari@csa.iisc.ernet.in

Dept of Computer Science and Automation
Indian Institute of Science, Bangalore, 560012

Abstract
There are p heterogeneous objects to be assigned to n competing agents (n > p)
each with unit demand. It is required to design a Groves mechanism for this assignment
problem satisfying weak budget balance, individual rationality, and minimizing the budget
imbalance. This calls for designing an appropriate rebate function. When the objects are
identical, this problem has been solved which we refer as WCO mechanism. We measure
the performance of such mechanisms by the redistribution index. We first prove an impossibility theorem which rules out linear rebate functions with non-zero redistribution index
in heterogeneous object assignment. Motivated by this theorem, we explore two approaches
to get around this impossibility. In the first approach, we show that linear rebate functions
with non-zero redistribution index are possible when the valuations for the objects have a
certain type of relationship and we design a mechanism with linear rebate function that
is worst case optimal. In the second approach, we show that rebate functions with nonzero efficiency are possible if linearity is relaxed. We extend the rebate functions of the
WCO mechanism to heterogeneous objects assignment and conjecture them to be worst
case optimal.

1. Introduction
Consider that p resources are available and each of n > p agents is interested in utilizing
one of them. It is desirable that we assign the resources to the agents who value them the
most. Since the classical Vickery-Clarke-Groves mechanisms (Vickrey, 1961; Clarke, 1971;
Groves, 1973) have attractive properties such as dominant strategy incentive compatibility
(DSIC) and allocative efficiency (AE), Groves mechanisms are quite appealing to use in
this context. However, in general, a Groves mechanism need not be budget balanced. That
is, the total transfer of money in the system may not be zero. So the system will be left
with a surplus or deficit. Using Clarkes (1971) mechanism, we can ensure under fairly
weak conditions, that there is no deficit of money (that is the mechanism is weakly budget
balanced). In such a case, the system or the auctioneer will be left with some money.
Often, the surplus money is not really needed in many social settings such as allocations
by the Government among its departments, etc. Since strict budget balance cannot coexist
with DSIC and AE (Green-Laffont theorem, see Green & Laffont, 1979), we would like to
redistribute the surplus to the participants as far as possible, preserving DSIC and AE. This
idea was originally proposed by Laffont (1979). The total payment made by the mechanism
as a redistribution will be referred to as the rebate to the agents.

c
2011
AI Access Foundation. All rights reserved.

fiGujar & Narahari

In this paper, we consider the following problem. There are n agents and p heterogeneous
objects (n > p > 1). Each agent desires one object out of these p objects. Each agents
valuation for any of the objects is independent of his valuations for the other objects.
Valuations of the different agents are also mutually independent. Our goal is to design a
mechanism for assignment of the p objects among the n agents which is allocatively efficient,
dominant strategy incentive compatible, and maximizes the rebate (which is equivalent to
minimizing the budget imbalance). In addition, we would like the mechanism to satisfy
feasibility and individual rationality. Thus, we seek to design a Groves mechanism for
assigning p heterogeneous objects among n agents satisfying:
1. Feasibility (F) or weak budget balance. That is, the total payment to the agents
should be less than or equal to the total received payment.
2. Individual Rationality (IR), which means that each agents utility by participating in
the mechanism should be non-negative.
3. Minimizes budget imbalance.
We call such a Groves mechanism that redistributes Clarkes Payment as Groves redistribution mechanism or simply redistribution mechanism. Designing a redistribution mechanism involves the design of an appropriate rebate function. If in a redistribution mechanism,
the rebate function for each agent is a linear function of the valuations of the remaining
agents, we refer to such a mechanism as a linear redistribution mechanism (LRM). In many
situations, design of an appropriate LRM reduces to a problem of solving a linear program.
Due to the Green-Laffont theorem , we cannot guarantee 100% redistribution for all type
profiles. So a performance index for the redistribution mechanism would be the worst case
redistribution, that is, the fraction of the surplus which is guaranteed to be redistributed
irrespective of the bid profiles. This fraction will be referred to as redistribution index in
the rest of the paper. The advantage of worst case analysis is that, it does not require any
distributional information on the type sets of the agents. It is desirable that the rebate
function is deterministic and anonymous. A rebate function is said to be anonymous if two
agents having the same bids get the same rebate. Also, when valuation spaces are identical
for all the agents, without loss of generality, we can restrict our attention to the anonymous
rebate functions. Thus, the aim is to design an anonymous, deterministic rebate function
which maximizes the redistribution index and satisfies feasibility and individual rationality.
Our work in this paper seeks to non-trivially extend the results of Moulin (2009) and
Guo and Conitzer (2009) who have independently designed a Groves mechanism in order
to redistribute the surplus when objects are identical (homogeneous objects case). Their
mechanism is deterministic, anonymous, and has maximum redistribution index over all
possible Groves redistribution mechanisms. We will refer to their mechanism as the worst
case optimal (WCO) mechanism. The WCO Mechanism is a linear redistribution mechanism. In this paper, we concentrate on designing a linear redistribution mechanism for the
heterogeneous objects case.

132

fiRedistribution Mechanisms

1.1 Relevant Work
As it is impossible to achieve allocative efficiency, DSIC, and strict budget balance simultaneously, we have to compromise on one of these properties. Faltings (2005) and Guo and
Conitzer (2008a) achieve budget balance by compromising on AE. If we are interested in
preserving AE and DSIC, we have to settle for a non-zero surplus or a non-zero deficit of
the money (budget imbalance) in the system. To reduce budget imbalance, various rebate
functions have been designed by Bailey (1997), Cavallo (2006), Moulin (2009), and Guo and
Conitzer (2009). Moulin (2009) and Guo and Conitzer (2009) designed a Groves redistribution mechanism for assignment of p homogeneous objects among n > p agents with unit
demand. Guo and Conitzer (2009) generalize the work in earlier paper (Guo & Conitzer,
2007) for multi-unit demand of identical items. In the work of Guo and Conitzer (2008b),
authors designed a redistribution mechanism which is optimal in the expected sense for the
homogeneous objects setting. Thus, it will require some distributional information over the
type sets of the agents. Clippel and co-authors (2009) use the idea of destroying some of
the items to maximize the agents utilities. A preliminary version of the results presented
in this paper have appeared in our earlier papers (Gujar & Narahari, 2009, 2008).
1.2 Contributions and Outline
Our objective in this paper is to design a Groves redistribution mechanism for assignment
of heterogeneous objects with unit demand. To the best of our knowledge, this is the first
attempt to design a redistribution mechanism for assignment of heterogeneous objects.
First, we investigate the question of existence of a linear rebate function for redistribution of surplus in assignment of heterogeneous objects. Our result shows that in general,
when the domain of valuations for each agent is Rp+ , it is impossible to design a linear rebate
function, with non-zero redistribution index, for the heterogeneous settings. However, we
can relax the assumption of independence of valuations of different objects to get a linear
rebate function with non-zero redistribution index. Another way to get around the impossibility theorem is to relax the linearity requirement of a rebate function. In particular, our
contributions in this paper can be summarized as follows.
 We first prove the impossibility of existence of a linear rebate function with non-zero
redistribution index for heterogeneous settings, when the domain of valuations for
each agent is Rp+ and the valuations for the objects are independent.
 When the objects are heterogeneous but the values for the objects of an agent can be
derived from one single number, we design a Groves redistribution mechanism that
is linear, anonymous, deterministic, feasible, individually rational, and efficient. In
addition, the mechanism is worst case optimal with non-zero redistribution index.
 We show the existence of a non-linear rebate function that has a non-zero redistribution index.
 We propose a mechanism, HETERO, which extends Moulin/WCO mechanism for
heterogeneous settings. We conjecture HETERO to have non-zero redistribution index
and to be worst case optimal.

133

fiGujar & Narahari

The paper is organized as follows. In Section 2, we introduce the notation followed in
the paper and describe relevant background work from the literature. We also explain the
WCO mechanism there. In Section 3, we state and prove our impossibility result. We derive
an extension of the WCO mechanism for heterogeneous objects but with single dimensional
private information in Section 4. The impossibility result does not rule out possibility
of non-linear rebate functions with strictly positive redistribution index. We show this
with a redistribution mechanism, BAILEY-CAVALLO, which is Baileys mechanism (1997)
applied to the settings under consideration in Section 5. We design another non-linear rebate
function, HETERO, that actually matches with Moulins rebate function when the objects
are identical. We describe the construction of HETERO in Section 5. We have carried
out simulations to provide empirical evidence for our conjecture regarding HETERO. The
experimental setup and results are described in Section 6. We conclude the paper in Section
7 and provide some directions for future work. In our analysis, we need an ordering of the
bids of the agents which we define in Appendix A. The proofs of some of the lemmas in the
paper are presented in Appendix B.

2. Preliminaries and Notation
In this section we will first define the notation used in the paper and preliminaries about
the redistribution mechanisms.
2.1 The Model and Notation
The notation used is summarized in Table 1. Where the context is clear, we will use
t, ti , ri , k, and vi to indicate t(b), ti (b), ri (b), k(b), and vi (k(b)) respectively. In this paper,
we assume that the payment made by agent i is of the form ti () P
ri (), where ti () is agent
is payment in the Clarke pivotal mechanism (1971). We refer to i ti , as the total Clarke
payment or the surplus in the system.
In general, we assume there are n agents and p distinct objects. We also assume that
the allocation rule satisfies the allocative efficiency (AE) property.
2.2 Important Definitions
We provide a few important definitions here in a conceptual way.
Definition 1 (DSIC) We say a mechanism is Dominant Strategy Incentive Compatible
(DSIC) if it is a best response for each agent to report its type truthfully, irrespective of the
types reported by the other agents.
Definition 2 (Allocative Efficiency) We say a mechanism is allocatively efficient (AE)
if the mechanism chooses, in every given type profile, an allocation of objects among the
agents such that sum of the valuations1 of the allocated agents is maximized.
Definition 3 (Redistribution Mechanism) We refer to a Groves mechanism as a Groves
redistribution mechanism or simply redistribution mechanism, if it allocates objects to the
1. Sum of the valuations of the allocated agents in an allocation is also referred as total value or value of
the allocation.

134

fiRedistribution Mechanisms

n
N
p
i
j
R+
i
bi
b
K
k(b)
k  (b)
 (b)
ki
vi (k(b))
v
ti (b)
t(b)
ti
ri (b)
e

Number of agents
Set of the agents = {1, 2, . . . , n}
Number of objects
Index for an agent, i = 1, 2, . . . , n
Index for object, j = 1, 2, . . . , p
Set of positive real numbers
The space of valuations of agent i, i = Rp+
Bid submitted by agent i, = (bi1 , bi2 , . . . , bip )  i
(b1 , b2 , . . . , bn ), the bid vector
The set of all allocations of p objects to n agents, each
getting at most one object
An allocation, k()  K, corresponding to the bid profile b
An allocatively efficient allocation when the bid profile is b
An allocatively efficient allocation when the bid profile is b and
agent i is excluded from the system
Valuation of the allocation k to the agent i,
when b is the bid profile
P
v : K  R, the valuation function, v(k(b)) = iN vi (k(b))
Payment made by agent i in the Clarke pivotal mechanism, when
 (b))
the bid profile is b, ti (b) = vi (k  (b))  v(k  (b))  v(ki
The Clarke payment,
P that is, the total payment received from all
the agents, t(b) = iN ti (b)
The Clarke payment received in the absence of the agent i
Rebate to agent i when bid profile is b
P
ri (b)
The redistribution index of the mechanism, = inf b:t(b)6=0 t(b)
Table 1: Notation: redistribution mechanisms

agents in an allocatively efficient way and redistributes the Clarke surplus in the system in
the form of rebates to the agents such that the net payment made by each agent still follows
the Groves payment structure.
Definition 4 (Linear Rebate Function) We say the rebates to an agent follow a linear
rebate function if the rebate is a linear combination of bid vectors of all the remaining
agents. Moreover, if a redistribution mechanism uses linear rebate functions for all the
agents, we say the mechanism is a linear redistribution mechanism.
Definition 5 (Redistribution Index) The redistribution index of a redistribution mechanism is defined to be the worst case fraction of Clarkes surplus that gets redistributed
among the agents. That is,
P
ri (b)
e = inf
b:t(b)6=0 t(b)

135

fiGujar & Narahari

2.3 Optimal Worst Case Redistribution when Objects are Identical
When the objects are identical, every agent i has the same value for each object, call it
vi . Without loss of generality, we will assume, v1  v2  . . .  vn . In Clarkes pivotal
mechanism, the first p agents will receive the objects and each of these p agents will pay
vp+1 . So, the surplus in the system is pvp+1 . For this situation, Moulin (2009) and Guo
and Conitzer (2009) have independently designed a redistribution mechanism.
Guo and Conitzer (2009) maximize the worst case fraction of the total surplus which gets
redistributed. This mechanism is called the WCO mechanism. Moulin (2009) minimizes
the ratio of budget imbalance to the value of an optimal allocation, that is the value of
an allocatively efficient allocation. The WCO mechanism coincides with Moulins feasible
and individually rational mechanism. Both the above mechanisms work as follows. After
receiving bids from the agents, bids are sorted in decreasing order. The first p agents
receive the objects. Each agents Clarke payment is calculated, say ti . Every agent i pays,
pi = ti  ri , where, ri is the rebate function for an agent i.
riW CO = cp+1 vp+2 + cp+2 vp+3 + . . . + cn1 vn
i = 1, . . . p + 1
W
CO
= cp+1 vp+1 + . . . + ci1 vi1 + ci vi+1 + . . . + cn1 vn i = p + 2, . . . n
ri

(1)

where,
(1)i+p1 (n

 p)



n1
p1




n1
X




ci = 
n  1 Pn1 n  1  j=i
i
j=p
i
j

n1
j




;

i = p + 1, . . . , n  1

(2)



Suppose y1  y2  . . .  yn1 are the bids of the (n  1) agents excluding the agent i,
then equivalently the rebate to the agent i is given by,
riW CO =

n1
X

c j yj

(3)

j=p+1

The redistribution index of this mechanism is e , where e is given by,


n1
p


e = 1 
Pn1 n  1
j=p
j

This is an optimal mechanism, since there is no other mechanism which can guarantee more
than e fraction redistribution in the worst case.
Before we proceed to present our impossibility theorem we state the following theorem
by Guo and Conitzer (2009) which will be used to design our mechanism.
Theorem 1 (Guo & Conitzer, 2009) For any x1  x2  . . . xn  0,
a1 x1 + a2 x2 + . . . an xn  0 iff

j
X
i=1

136

ai  0 j = 1, 2 . . . , n

fiRedistribution Mechanisms

3. Impossibility of Linear Rebate Function with Non-Zero Redistribution
Index
We have just reviewed the design of a redistribution mechanism for homogeneous objects.
We have seen that the WCO mechanism is a linear function of the types of agents. We now
explore the general case. In the homogeneous case, the bids are real numbers which can be
arranged in decreasing order. The Clarke surplus is a linear function of these ordered bids.
For the heterogeneous scenario, this would not be the case. Each bid bi belongs to Rp+ ;
hence, there is no unique way of defining an order among the bids. Moreover, the Clarke
surplus is not a linear function of the received bids in the heterogeneous case. So, we cannot
expect any linear/affine rebate function of types to work well at all type profiles. We will
prove this formally.
We first generalize a theorem from the work of Guo and Conitzer (2009). The context
in which Guo and Conitzer stated and proved the theorem is in the homogeneous setting.
We show that this result holds true in the heterogeneous objects case also. The symbol <
denotes the order over the bids of the agents, as defined in the A.2.
Theorem 2 In the Groves redistribution mechanism, any deterministic, anonymous rebate
function f is DSIC iff,
ri = f (v1 , v2 , . . . , vi1 , vi+1 , . . . , vn ) i  N

(4)

where, v1 < v2 < . . . < vn .
Proof:
 The if part: If ri takes the form given by equation (4), then the rebate of agent
i is independent of his valuation. The allocation rule satisfies allocative efficiency.
So, the mechanism is still Groves and hence DSIC. The rebate function defined is
deterministic. If two agents have the same bids, then, as per the ordering defined in
Appendix, <, they will have the same ranking. Suppose agents i and i + 1 have the
same bids. Thus vi < vi+1 and vi+1 < vi . So, ri = f (v1 , v2 , . . . , vi1 , vi+1 , . . . , vn ) and
ri+1 = f (v1 , v2 , . . . , vi , vi+2 , . . . , vn ). Since vi = vi+1 , ri = ri+1 . Thus the rebate
function is anonymous.
 The only if part: For the mechanism to be strategyproof, the rebate function for
agent i should be independent of his bid. So, ri should depend on only vi . So,
for deterministic rebate function, ri = fi (vi ). Now, we desire anonymous rebate
function. That is, rebate should be independent of the identity of the agent. Thus,
if vi = vj , then ri = rj . With out loss of generality, say vi = vi+1 , then vi =
v(i+1) . So, ri = ri+1 implies, fi = fi+1 . Similarly fi+1 = fi+2 and so on. Thus,
ri = f (vi ) i  N .

We now state and prove the main result of this paper.
Theorem 3 If a redistribution mechanism is feasible and individually rational, then there
cannot exist a linear rebate function which simultaneously satisfies all the following properties:
137

fiGujar & Narahari

 DSIC
 deterministic
 anonymous
 non-zero redistribution index.
Proof : Assume to the contrary that there exists a linear function, say f , which satisfies
the above properties. Let v1 < v2 < . . . < vn . Then according to Theorem 2, for each agent
i,
ri = f (v1 , v2 , . . . , vi1 , vi+1 , . . . , vn )
= (c0 , ep ) + (c1 , v1 ) + . . . + (cn1 , vn )
where, ci = (ci1 , ci2 , . . . , cip )  Rp , ep = (1, 1, . . . , 1)  Rp , and (, ) denotes the inner
product of two vectors in Rp . Now, we will show that the worst case performance of f will
be zero. To this end, we will study the structure of f , step by step.
Observation 1: Consider type profile (v1 , v2 , . . . , vn ) where v1 = v2 = . . . = vn = (0, 0, . . . , 0).
For this type profile, the total Clarke surplus is zero and ri = (c0 , ep ) i  N . Individual
rationality implies,
(c0 , ep )  0
(5)
Feasibility implies the total redistributed amount is less than the surplus, that is,
X
ri = n(c0 , ep ) 6 0

(6)

i

From, (5) and (6), it is easy to see that, (c0 , ep ) = 0.
Observation 2: Consider type profile (v1 , v2 , . . . , vn ) where v1 = (1, 0, 0, . . . , 0) and v2 =
. . . , vn = (0, 0, . . . , 0). For this type profile, r1 = 0 and if i 6= 1, ri = c11  0 for individual
rationality. For this type profile, it can be seen through
straight forward calculations that
P
the Clarke surplus is zero. Thus, for feasibility, i ri = (n  1)c11  t = 0. This implies,
c11 = 0.
In the above profile, by considering v1 = (0, 1, 0, . . . , 0), we get c12 = 0. Similarly, one
can show c13 = c14 = . . . = c1p = 0.
Observation 3: Continuing on the same lines as above with, v1 = v2 = . . . = vi = ep , and
vi+1 = (1, 0 . . . , 0)
or (0, 1, 0 . . . , 0), . . . or (0, . . . , 0, 1), we get, ci+1 = (0, 0, . . . , 0)  i  p  1. Thus,

ri


: if i  p + 1
 (cp+1 , vp+2 ) + . . . + (cn1 , vn )
(cp+1 , vp+1 ) + . . . + (ci1 , vi1 )
=

+(ci , vi+1 ) + . . . + (cn1 , vn ) : otherwise

(7)

Thus a rebate function in any linear redistribution mechanism has to be necessarily of the
form in the Equation (7). We now claim that the redistribution index of such a mechanism
138

fiRedistribution Mechanisms

is zero. For any individually rational redistribution mechanism, the trivial lower bound
on redistribution index is zero. We prove that in a linear redistribution mechanism, there
exists a type profile, at which the fraction of the Clarke surplus that gets redistributed is
zero. Consider the type profile:
v1
v2
..
.

= (2p  1, 2p  2, . . . , p + 1, p)
= (2p  2, 2p  3, . . . , p, p  1)

vp1 = (p + 1, p, . . . , 3, 2)
vp = (p, p  1, . . . , 2, 1)
and vp+1 = vp+2 . . . = vn = (0, 0, . . . , 0).
Now it can be seen, through straightforward calculations of the Clarke payments, that,
with this type profile, agent 1 pays (p  1), agent 2 pays (p  2), . . . , agent (p  1) pays
1 and the remaining agents pay 0. Thus, the Clarke payment received is non-zero but it
can be seen that ri = 0 for all the agents. Hence, the redistribution index for any linear
redistribution mechanism has to be zero.

The above theorem provides a disappointing piece of news. It rules out the possibility
of a linear redistribution mechanism for heterogeneous settings which will have non-zero
redistribution index. However, there are two ways to get around it.
1. The domain of types under which Theorem 3 holds is, i = Rp+ , i  N . One idea is
to restrict the domain of types. In Section 4, we design a worst case optimal linear
redistribution mechanism when the valuations of agents for the heterogeneous objects
have a certain type of relationship.
2. Explore the existence of a rebate function which is not linear but yields a non-zero
redistribution index. We explore this in Section 5.
It should be noted that our impossibility result holds true when we are defining a linear
rebate functions as in Definition 4. Our result may not hold for other types of linearity.
For example, sort bid components of other (n  1) agents and define rebate function to be
linear combination of these (n  1)p elements. At this point, we have not explored such
linear rebate functions.

4. A Redistribution Mechanism for Heterogeneous Objects when
Valuations have a Scaling Based Relationship
Consider a scenario where the objects are not identical but the valuations for the objects
are related and can be derived by a single parameter. As a motivating example, consider
there is a website where people can put up their ads for free and assume that there are p
slots available for advertisements and there are n agents interested in displaying their ads.
Naturally, every agent will have a higher preference for a higher slot. Another motivating
example could be, there is university web site which has p slots to display news about
139

fiGujar & Narahari

various departments. Define click through rate of a slot as the number of times the ad is
clicked, when the ad is displayed in that slot, divided by the number of impressions. Let
the click through rates for slots be 1  2  3 . . .  p . Assume that each agent has
the same value for each click by the user, say vi . So, the agents value for the j th slot will
be j vi . Let us use the phrase valuations with scaling based relationship to describe such
valuations. We define this more formally below.
Definition 6 We say the valuations of the agents have scaling based relationship if there
exist positive real numbers 1 , 2 , 3 , . . . , p > 0 such that, for each agent i  N , the valuation for object j, say i j , is of the form i j = j vi , where vi  R+ is a private signal
observed by agent i.
Without loss of generality, we assume, 1  2  3 . . .  p > 0. (For simplifying
equations, we will assume that there are (n  p) virtual objects, with p+1 = p+2 = . . . =
n = 0). We immediately note that the homogeneous setting is a special case that arises
when 1 = 2 = 3 = . . . = p > 0
For the above setting, we design a Groves mechanism which is almost budget balanced
and optimal in the worst case. Our mechanism is similar to that of Guo and Conitzer (2009)
and our proof uses the same line of arguments.
4.1 The Proposed Mechanism
We will use a linear rebate function. We propose the following mechanism:
 The agents submit their bids.
 The bids are sorted in decreasing order.
 The highest bidder will be allotted the first object, the second highest bidder will be
allotted the second object, and so on.
 Agent i will pay ti  ri , where ti is the Clarke payment and ri is the rebate.
ti =

p
X

(j  j+1 )vj+1

j=i

 Let agent is rebate be,
ri = c0 + c1 v1 + . . . + ci1 vi1 + ci vi+1 + . . . + cn1 vn
ci s are defined as follows.
The mechanism is required to be individually rational and feasible.
 The mechanism will be individually rational iff ri  0 i  N . That is, i  N ,
c0 + c1 v1 + . . . + ci1 vi1 + ci vi+1 + . . . + cn1 vn  0.

140

fiRedistribution Mechanisms

 The mechanism will be feasible
if theP
total redistributed
payment is less than or equal
P
P
to the surplus. That is, i ri  t = i ti or t  i ri  0, where,
t=

p
X

j(j  j+1 )vj+1 .

j=1

With the above setup, we now derive c0 , c1 , . . . , cn1 that will maximize the fraction of the
surplus which is redistributed among the agents.
Step 1: First, we claim that c0 = c1 = 0. This can be proved as follows. Consider the
type profile, v1 = v2 = . . . = vn = 0. For
Pthis type profile, individual rationality implies
ri = c0  0 and t = 0. So for feasibility, i ri = nc0  t = 0. That is, c0 should be zero.
Similarly, by considering type profile v1 = 1, v2 = . . . = vn = 0, we get c1 = 0.
Step 2: Using c0 = c1 = 0,
 The feasibility condition can be written as:
)
(
n1
X
(j  1)(j1  j )  (j  1)cj1  (n  j)cj vj  (n  1)cn1 vn  0

(8)

j=2

 The individual rationality condition can be written as
c2 v2 + . . . + ci1 vi1 + ci vi+1 + . . . + cn1 vn  0
Step 3: When we say our mechanisms redistribution index is e, we mean,
is,
n1
X
j=2


 e(j  1)(j1  j ) + (j  1)cj1 + (n  j)cj vj +

(n  1)cn1 vn  0

(9)
P

i ri

 et, that

(10)

Step 4: Define 1 = 1  2 , and for i = 2, . . . , n  1, let i = i(i  i+1 ) + i1 . Now,
inequalities (8), (9), and (10) have to be satisfied for all values of v1  v2  . . .  vn  0.
By Theorem (1), we need to satisfy the following set of inequalities:
Pj
i=2 ci  0 j = 2, . . . n  1
e  (n  2)c2  1
Pi1 1
ei1  n j=2 cj + (n  i)ci  i1 i = 3, . . . , p
P
ep  n i1
i  p i = p + 1, . . . , n  1
j=2 cj + (n  i)c
Pn1
ep  n j=2 cj  p

Now, the mechanism designer wishes to design a mechanism that maximizes e subject to
the above constraints.
P
Define xj = ji=2 ci for j = 2, . . . , n  1. This is equivalent to solving the following
linear program.
141

fiGujar & Narahari

maximize e
s.t.
e1  (n  2)x2  1
ei1  ixi1 + (n  i)xi  i1 i = 3, . . . , p
ep  ixi1 + (n  i)xi  p i = p + 1, . . . , n  1
ep  nxn1  p
xi  0 i = 2, . . . , n  1

(11)


So, given n and p, the social planner will have to solve the above optimization problem and
determine the optimal values of e, c2 , c3 , . . . , cn1 . It would be of interest to derive a closed
form solution for the above problem.
The discussion above can be summarized as the following theorem.
Theorem 4 When the valuations of the agents have scaling based relationship, for any p
and n > p + 1, the linear redistribution mechanism obtained by solving LP (11) is worst case
optimal among all Groves redistribution mechanisms that are feasible, individually rational,
deterministic, and anonymous. This mechanism is an example of a mechanism having
non-zero redistribution index.
Proof:
The worst case optimality of the mechanism can be proved following the line of arguments
of Guo and Conitzer (2009).
As per the impossibility Theorem 3, there is no linear redistribution mechanism for general heterogeneous setting having non-zero efficiency. However, when objects have scaling
based relationship, the linear redistribution mechanism, that is obtained by solving LP (11)
has non-zero efficiency at least for some (n, p) instances. This is obtained by actually solving
the LP (for example, using MATLAB) for various values of n and p. This certainly proves
that, at least for n = 10, 12, 14, p = 2, 3, 4, . . . , 8 and when valuations have scaling based
correlation, the worst case optimal mechanism given by the LP (11) has non-zero redistribution index. Now we obtain an upper bound on the redistribution index of a redistribution
mechanism LP (11).
Claim 1 If e is the solution of the LP (11), then


A B

e  min
,
B A




P
P
n
n
.
and B = i=2,4,6,... i1
where, A = i=1,3,5,... i1
i
i
The LP (11) can be written as

maximize e
s.t.
e  M x  
x0
142

fiRedistribution Mechanisms

where x = (x2 , x3 , . . . , xn1 )  R+ n2 and  = (1 , 2 , . . . , p , p , . . . , p )  R+ n1 and


n2
0
0

0
 3
n3
0

0 



 0
4
n

4



0


M = .
.. 
..
..
..
 ..
. 
.
.
.


 0
n1 1 
0
0
n
Now, y = (y1 , y2 , . . . , yn1 )  Rn1 is in the range of M iff








n
n
n
n
y1 +
y3 +    =
y2 +
y4 +   
2
4
3
5

(12)








n
n
n
Now,

(M x)i 
i i  {1, 2, 3, . . . , n  1}. Now
i
i+1
i+1
i+1
summing these inequalities for odd is and using (12), we get e A  B and summing over
even is we get e B  A. This proves our claim.
e 


We verified using MATLAB for n = 10, 12, 14 and p= 2, 3,
	 . . . 8, that the redistribution
A B
index of the proposed mechanism is in fact, e = min B
,A .

5. Non-linear Redistribution Mechanisms for the Heterogeneous Setting

We should note that the homogeneous objects case is a special case of the heterogeneous
objects case in which each bidder submits the same bid for all objects. Thus, we cannot
expect any redistribution mechanism to perform better than the homogeneous objects case.
For n  p + 1, the worst case redistribution is zero for the homogeneous case and so will
it be for the heterogeneous case (Guo & Conitzer, 2009; Moulin, 2009). So, we assume
n > p + 1. In this section, we propose two redistribution mechanisms with non-linear rebate
functions. We construct a redistribution scheme by applying the mechanism proposed by
Bailey (1997) to the heterogeneous settings. We refer to this proposed mechanism on
heterogeneous objects as BAILEY-CAVALLO redistribution mechanism. It is crucial to
note that the non-zero redistribution index of the BAILEY-CAVALLO mechanism does not
trivially follow from that of the mechanism in the work of Bailey. We rewrite the WCO
mechanism and extend the rebate functions to heterogeneous objects settings. We call this
mechanism as HETERO.
In each of the mechanisms, namely BAILEY-CAVALLO and HETERO, the objects are
assigned to those agents who value them most. The Clarke payments are collected from the
agents and the surplus is redistributed among the agents according to the rebate functions
defined in the mechanism. Hence, both are Groves redistribution mechanisms and hence
DSIC.
As stated above, for n  (p + 1), the redistribution index for any redistribution mechanism has to be zero. For the case n > p + 1, the redistribution index for any linear
redistribution mechanism has to be zero (Theorem 3). We prove, for n  (2p + 1), that
143

fiGujar & Narahari

BAILEY-CAVALLO has non-zero redistribution index. We only conjecture that HETERO
is worst case optimal, that is no mechanism can have better redistribution index than HETERO. We also conjecture that HETEROs redistribution index is the same as that of WCO,
which is non-zero when n > (p + 1). Thus, for n  {p + 2, p + 3, . . . , 2p}, there is still no
redistribution mechanism for which non-zero redistribution index is proved.
5.1 BAILEY-CAVALLO Mechanism
First, consider the case when p = 1. Let the valuations of the agents for the object be,
v1  v2  . . .  vn . The agent with the highest valuation will receive the object and would
pay the second highest bid. Cavallo (2006) proposed the rebate function as:
r1 = r2 = n1 v3
ri = n1 v2 i > 2
A similar mechanism was independently proposed by Porter et al (2004). Motivated by this
scheme, we propose a scheme for the heterogeneous setting. Suppose agent i is excluded
from the system. Then let ti be the Clarke surplus in the system (defined in Table 1).
Define,
1
ri B = ti i  N
(13)
n
 As the Clarke surplus is always positive, ri B  0 for all i. Thus, this scheme satisfies
individual rationality.
P 1 i
P B
 n n1 t = t. Thus,
=
 ti  t  i (revenue monotonicity). So,
i nt
i ri
this scheme is feasible. (The revenue monotonicity follows from the fact that the
valuations are non-negative and unit demand preferences. Gul and Stacchetti (1999)
showed that with unit demand preferences, the VCG payments coinside with the
smallest Walrasian prices which in turn would not decrease by addition of an agent.
Thus addition of any agent cannot decrease the total payments.)2
We now show that the BAILEY-CAVALLO scheme has non-zero redistribution index
if n  2p + 1. First we state two lemmas. The proof will be given in Appendix B. These
lemmas are useful in designing redistribution mechanisms for the heterogeneous settings as
well as in analysis of the mechanisms. Lemma 2 is used to show that the redistribution index
of the BAILEY-CAVALLO mechanism is non-zero. Lemma 1 is used to find an allocatively
efficient outcome for the settings under consideration. Lemma 1 is also useful in determining
the Clarke payments.
Lemma 1 If we sort the bids of all the agents for each object, then:
1. An optimal allocation, that is an allocatively efficient allocation, will consist of the
agents having bids among the p highest bids for each object.
2. Consider an optimal allocation k  . If any of the p agents receiving objects in k  is
 that is an optimal allocation (on
dropped, then there always exists an allocation ki
2. We thank the annonymous reviewer for pointing out this reference.

144

fiRedistribution Mechanisms

the remaining n  1 agents) which allocates objects to the remaining (p  1) agents.
 , may not however be the same as
The objects that these (p  1) agents receive in ki
the objects they are allocated in k  .
Lemma 2 There are at most 2p agents involved in deciding the Clarke payment.
Note: When the objects are identical, the bids of (p + 1) agents are involved in determining the Clarke payments.
Now, we show that the redistribution index of the BAILEY-CAVALLO mechanism is
non-zero.
Theorem 5 When there are sufficient number of agents (in particular, n > 2p), the
BAILEY-CAVALLO redistribution mechanism has non-zero redistribution index.
Proof: In Lemma 2, we have shown that there will be at most 2p agents involved in
determining the Clarke surplus. Thus, given a type profile, there will be (n  2p) agents,
for whom, ti = t and this implies that at least n2p
n t will be redistributed. That is the
redistribution index of the mechanism is at least n2p
n > 0.

Note that the above mechanism may not be worst case optimal. This is because, when
objects are identical, the WCO mechanism performs better on worst case analysis than
the above mechanism. So, we suspect that in heterogeneous settings as well, the above
mechanism would not be optimal on worst case analysis. In the next subsection, we explore
another rebate function, namely HETERO.
5.2 HETERO: A Redistribution Mechanism for the Heterogeneous Setting
When the objects are identical, the WCO mechanism is given by equation (3). We give
a novel interpretation to it. Consider the scenario in which one agent is absent from the
scene. Then the Clarke payment received is either pvp+1 or pvp+2 depending upon which
agent is absent. If we remove two agents, the surplus is pvp+1 or pvp+2 or pvp+3 , depending
upon which two agents are removed. Till (n  p  1) agents are removed, we get non-zero
surplus. If we remove (n  p) or more agents from the system, there is no need for any
mechanism for assignment of the objects. So, we will consider the cases when we remove k
agents, where, 1  k < n  p.
Now let ti,k be the average payment received when agent i is removed along with k
other agents that is, a total of (k + 1) agents are removed comprising of i. The average is
taken over all possible selections of k agents from the remaining (n  1) agents. We can
rewrite the WCO mechanism in terms of ti , ti,k . Observe that, ti , ti,k can be defined
in heterogeneous settings as well. We propose to use a rebate function defined as,
riH

= 1 t

i

+

k=np1
X

k ti,k1

(14)

k=2

where k are the suitable weights assigned to the surplus generated when a total of k
agents are removed from the system. By using different k s, we get different mechanisms.
However, we prefer to choose k s as the following.
145

fiGujar & Narahari

5.2.1 The Equivalence of HETERO and WCO when Objects are Identical
It is desirable that HETERO should match with the WCO mechanism when the objects
are homogeneous. So we choose s in Equation (14) in a way that ensures that, when
the objects are identical, riH in equation (14) is equal to riW CO in equation (3) for all
type profiles. Since the rebate is a function of the remaining (n  1) bids, we can write
it as, ri = f (x1 , x2 , . . . , xn1 ) where x1 , x2 , . . . , xn1 are the bids without the agent i, in
decreasing order. Note, in this case, that each bidder will be submitting a bid bi  R+ .
Now, we can write, ti,k , riH , and ri in terms of x1 , x2 , . . . , xn1 , as,



p+l
np2l
k1
X
p
k1l
i,k1


t
=
xp+1+l
n1
l=0
k1
riH

=

riW CO =

k=np1
X

k=1
np1
X

k ti,k1

(15)

cp+1+l xp+1+l

(16)

l=0

where, ci , i = p + 1, p + 2, . . . , n  1 are given by equation (2).
Consider the type profile (x1 = 1, x2 = 1, . . . , xp+1 = 1, xp+2 = 0, . . . , xn1 = 0). For
HETERO to agree with WCO, the coefficients of xp+1 in equation (15) and equation (16)
should be the same. Now consider the type profile (x1 = 1, x2 = 1, . . . , xp+2 = 1, xp+3 =
0, . . . , xn1 = 0). As the coefficients of xp+1 in equation (15) and equation (16) are the
same, the coefficients of xp+2 should also be equal in equation (15) and equation (16).
Thus, the coefficients of xp+1 , xp+2 , . . . , xn1 in equation (15) and equation (16) should
agree.
Let L = n  p  1. Thus, for i = p + 1, . . . , n  1,



i1
ni1
ni1
X
p
k


Lk
ci =
(17)
n

1
k=0
p+1+k
The above system of equations yields, for i = 1, 2, . . . , L,



 n1

Li 
(i+1)
X
X
(1)
(L  i)!p!
i+j1
n1
i =

j
l


(n  i)!
j=0

where  is given by,  =



n1
p1



n1
j

(np)

Pn1
j=p



l=p+i+j




.


146

(18)

fiRedistribution Mechanisms

5.2.2 Properties of HETERO
As the HETERO mechanism matches with the WCO when objects are identical, the HETERO mechanism satisfies individual rationality and feasibility in the homogeneous case.
These two properties, however, remain to be shown in the heterogeneous case.
Conjecture 1 The HETERO mechanism satisfies individual rationality, feasibility, is worst
case optimal, and has redistribution index same as WCO.
5.2.3 Intuition Behind Individual Rationality of HETERO
We have to show that for each agent i, riH  0 at all type profiles. For convenience,
i
i,j1 , j = 2, . . . , L.
we will assume i implicitly. So, say, riH = r and
P 1 = t , j = t
Now, the rebate is given by the equation, r =
j j j . We have to show that r  0.
Note that, 1  2  . . .  L  0. The s are monotone as the absence of more
agents
would either decrease the VCG payments or the payments remain the same. So, if
Pj

i=1 i  0  j = 1  L, individual rationality would follow from Theorem 1. We observe
that, in general, this is not true. The important observation is, though i s are decreasing
positive real numbers, they are related. For example, we can show that if 1 > 0, then
2 > 0. In our experiments, which we describe in the next section, we keep track of 12 . We
observed that this ratio is in [0.5, 1]. For Theorem 1 to be applicable, this ratio can be any
value in [0, 1].
Thus, though s are alternately positive and negative, the relation among s would
not make r to become negative and it will be within limits in such a way that total rebate to
the agents will be less than or equal to total Clarke payment. It remains to show individual
rationality analytically in the general case. We are, however, only able to show in the
following cases.
1. Consider the case when p = 2. (i). If n = 4, 1 = 14 . (ii). If n = 5, 1 = 0.27273,
2 = 0.18182. (iii). If n = 6, 1 = 0.29487, 2 = 0.25641, 3 = 0.12821.
2. Consider the case when p = 3. (i). If n = 5, 1 = 15 . (ii). If n = 6, 1 = 0.21875,
2 = 0.15625. (iii). If n = 7, 1 = 0.23810, 2 = 0.21429, 3 = 0.11905.
By Theorem 1, it follows that for the above cases, the proposed mechanism satisfies individual rationality.
5.2.4 Feasibility and Worst Case Optimality of HETERO
Similarly, we also believe that, s adjust the rebate functions optimally such that, HETERO remains feasible and is worst case optimal and has the same redistribution index as
WCO. Though we do not have analytical proof, we provide some empirical evidence for the
conjecture in Section 6.

6. Experimental Analysis
We perform our experiments in two sets. In the first set, we consider bids to be real numbers.
In the second set we consider the bidders submitting binary bids. We use these experiments
to provide an empirical evidance for our Conjecture 1.
147

fiGujar & Narahari

6.1 Empirical Evidence for Individual Rationality of HETERO
Solving equations (18) is a challenging task. Though the new mechanism is the extension
of the Moulin or the WCO mechanism, yet, we are not able to prove individual rationality
and feasibility of HETERO analytically. We therefore seek empirical evidence.
6.1.1 Simulation 1
We consider various combinations of n and p. For each agent, and for each object, the
valuation is generated as a uniform random variable in [0, 100]. We run our simulations for
the following combinations of n and p.
For p = 2, n = 5, 6, . . . , 14, for p = 3, n = 7, 8, . . . , 14 and for p = 4, n = 9, 10, . . . , 14.
For each combination of n and p = 2, we generated randomly 100,000 bid profiles and evaluated our mechanism. We also kept track of the worst case performance of our mechanism
over these 100,000 bid profiles. Our mechanism was feasible and individually rational in
these 100,000 bid profiles. The redistribution index of our mechanism is upper bounded
by that of the WCO mechanism. We observed that the worst case performance over these
100,000 random bid profiles was the same as that of WCO. This is a strong indication that
our mechanism will perform well in general.
6.1.2 Simulation 2: Bidders with Binary Valuation
Suppose each bidder has valuation for each object, either 0 or 1. Then there are 2np possible
bid profiles. We ran an experiment to evaluate our mechanism with all possible bid profiles
of agents with binary valuations. We considered p = 2 and n = 5, 6, . . . , 12. We found that
the mechanism is feasible, individually rational, and the worst case performance is the same
as that of the WCO mechanism. Note, as indicated earlier, no mechanism can perform
better than the WCO mechanism in the worst case. And our mechanism performs equally
well as the WCO. Thus, though an analytical proof is elusive, for binary valuation settings,
for p = 2 and n = 5, 6, . . . , 12, our mechanism is worst case optimal.
6.2 BAILEY-CAVALLO vs HETERO
In this subsection, we compare the worst case redistribution index of BAILEY-CAVALLO
with the worst case redistribution index of HETERO for varying number of objects when
there are 10 agents in the system. That is, we study worst case redistribution index for
various p when n = 10. The worst case is taken over randomly generated 50K bid profiles.
The comparison is depicted in Figure 1. The redistribution index of WCO is an upper bound
on any Redistribution Mechanism for heterogeneous settings. However, the simulations not
being exhaustive, the worst case performance of the mechanisms could perhaps be better
than that of WCO. Exact worst case may be worse than WCO. However, in the simulations,
we never encountered a situation where HETERO is worse than WCO. We can see from
Figure 1 that BAILEY-CAVALLO mechanisms worst case performance is better than that
of HETERO, for p = 3, 4, 5, 6, 7. This worst case is the worst over 50,000 randomly generated
bid profiles in our simulations.

148

fiRedistribution Mechanisms

The other observation we made in our simulations is that most of the time (70%),
BAILEY-CAVALLO redistributes more VCG surplus than HETERO ever though the worst
case performance is worse than that of HETERO.
These observations also lead to a question that Cavallo (2008) raised in the context of
dynamic redistribution mechanisms. Do we really need a highly sophisticated mechanism,
that is worst case optimal, when a simple mechanism performs quite well in general.

1

0.9

HETERO
BAILEYCAVELLO
WCO

0.8

redistribution index

0.7

0.6

n = 10

0.5

0.4

0.3

0.2

0.1

0

2

3

4

5
Number of objects, (p)

6

7

8

Figure 1: Redistribution index vs number of objects when number of agents = 10

7. Conclusion
We addressed the problem of assigning p heterogeneous objects among n > p competing
agents. When the valuations of the agents are independent of each other and their valuations for each object are independent of valuations on the other objects, we proved the
impossibility of existence of a linear redistribution mechanism with non-zero redistribution
index (Theorem 3). Then we explored two approaches to get around this impossibility.
 In the first approach, we showed that linear rebate functions with non-zero redistribution index are possible when the valuations for the objects have a scaling based
relationship. For these settings, we proposed a strategyproof linear redistribution
mechanism that is optimal on worst case analysis, individually rational, and feasible
(Theorem 4).

149

fiGujar & Narahari

 In the second approach, we relaxed linearity requirement. We showed that nonlinear rebate functions with non-zero redistribution index are possible by applying
the BAILEY-CAVALLO mechanism to the settings (Theorem 5).
 We proposed a mechanism, namely HETERO, for general settings when the objects
are heterogeneous and the private values of an agent for these objects are independent
of each other. The mechanism is deterministic, anonymous, and DSIC. The HETERO
mechanism extends the Moulin /WCO mechanism. Though we have not analytically
proved feasibility and individual rationality, we have sufficient empirical evidence to
conjecture that our mechanism is feasible and individually rational (Conjecture 1).
It would be interesting to see if we can characterize the situations under which linear redistribution mechanisms with non-zero redistribution indices are possible for heterogeneous
settings.
An interesting research direction is to investigate the individual rationality and feasibility
for the proposed HETERO mechanism. Also, we strongly believe that this mechanism is
a worst case optimal. An immediate future direction is to prove this fact or design a
mechanism which is worst case optimal.
Another interesting problem to explore is to characterize all redistribution mechanisms
that are worst case optimal in heterogeneous settings.

Acknowledgments
The first author would like to acknowledge Infosys Technologies Ltd., for awarding Infosys
fellowship to pursue Ph.D. The authors would like to thank Professor David Parkes for
useful comments. The authors would also like to thank the anonymous reviewers, whose
feedback has helped a lot in improving this paper.

Appendix A. Ordering of the Agents Based on Bid Profiles
We will define a ranking among the agents. This ranking is used in proving a Theorem
2 on rebate function. This theorem is similar to Cavallos theorem on characterization of
DSIC, deterministic, anonymous rebate functions for homogeneous objects. We would not
be actually computing the order among the bidders. We will use this order for proving
impossibility of the linear rebate function with the desired properties.
A.1 Properties of the Ranking System
When we are defining ranking/ordering among the agents, we expect the following properties
to hold true:
 Any permutation of the objects and the corresponding permutation on bid vector,(bi1 , bi2 ,
. . . , bi p ) for each agent i, should not change the ranking. That is, the ranking should
be independent of the order in which the agents are expected to bid for this objects.
 Two bidders with the same bid vectors should have the same rank.
 By increasing the bid on any of the objects, the rank of an agent should not decrease.
150

fiRedistribution Mechanisms

A.2 Ranking among the Agents
This is a very crucial step. First, find out all feasible allocations of the p objects among the
n agents, each agent receiving at most one object. Sort these allocations, according to the
valuation of an allocation. Call this list L. To find the ranking between i and j, we uses
the following algorithm.
1. Lij = L
2. Delete all the allocations from Lij which contain both i and j.
3. Find out the first allocation in Lij which contains one of the agent i or j. Say k  .
(a) Suppose this allocation contains i and has value strictly greater than any of
remaining allocations from Lij containing j, then we say, i  j.
(b) Suppose this allocation contains j and has value strictly greater than any of
remaining allocations from Lij containing i, then we say, j  i.
4. If the above step is not able to decide the ordering between i and j, let A = {k 
K|v(k) = v(k  )}. Update Lij = Lij \ A and recur to step (2) till EITHER
 there is no allocation containing the agent i or j OR
 the ordering between i and j is decided.
5. If the above steps do not give either of i  j or j  i, we say, i  j or i < j as well
as j < i.
Before we state some properties of this ranking system <, we will explain it with an
example. Let there be two items A and B, and four bidders. That is, p = 2, n = 4 and let
their bids be: b1 = (4, 5), b2 = (2, 1), b3 = (1, 4), and b4 = (1, 0).
Now, allocation (A = 1, B = 3) has the highest valuation among all the allocations. So,
agent
agent
agent
agent

1
1
3
3

agent
agent
agent
agent

2
4
2
4

Now, in L13 defined in the procedure above, the allocation (A = 2, B = 1) has strictly
higher value than any other allocation in which the agent 3 is present. So,
agent 1  agent 3.
Thus,
agent 1  agent 3  agent 2 and
agent 1  agent 3  agent 4
In L24 , the allocation (A = 2, B = 1) has strictly higher value than any other allocation in
which the agent 4 is present. Thus, the ranking of the agents is,
agent 1  agent 3  agent 2  agent 4
It can be seen that the ranking defined above, satisfies the following properties.
151

fiGujar & Narahari

1. < defines a total order on the set of bids.
2. < is independent of the order of the objects.
3. If two bids are the same, then they are equivalent in this order.
4. By increasing a bid, no agent will decrease his rank.
If agent i < agent j, we will also say vi < vj .

Appendix B. Some Proofs
B.1 Proof of Lemma 1
 Suppose an optimal allocation contains an agent whose bid for his winning object,
say j, is not in the top p bids for the j th object. There are other (p  1) winners
in an optimal allocation. So, there exists at least one agent whose bid is in the top
p bids for the j th object and does not win any object. Thus, allocating him the j th
object, we have an allocation which has higher valuation than the declared optimal
allocation.
 Suppose an agent i who receives an object in an optimal allocation is removed from
the system. The agent will have at most one bid in the top p bids for each object. So,
agents now having bids in the top p bids, will be at the pth position. It can be seen
that there will be at most one agent in an optimal allocation who is on the pth position
for the object he wins. If there is more than one agent in an optimal allocation on the
pth position for the object they win, then we can improve on this allocation. Hence,
after removing i, there will be at most one more agent who will be a part of a new
optimal allocation.

B.2 Proof of Lemma 2
The argument is as follows.
1. Sort the bids of the agents for each object.
2. The optimal allocation consists of agents having bids in the p highest bids for each of
the objects (Lemma 1).
3. For computing the Clarke payment of the agent i, we remove the agent and determine
an optimal allocation. And, using his bid, the valuation of optimal allocation with
him and without him will determine his payment. This is done for each agent i. As
per Lemma 1, if any agent from an optimal allocation is removed from the system,
there exists a new optimal allocation which consists of at least (p  1) agents who
received the objects in the original optimal allocation.

152

fiRedistribution Mechanisms

4. There will be p agents receiving the objects and determining their payments will
involve removing one of them at a time, there will be at most p more agents who will
influence the payment. Thus, there are at most 2p agents involved in determining the
Clarke payment.


References
Bailey, M. J. (1997). The demand revealing process: To distribute the surplus. Public
Choice, 91 (2), 10726.
Cavallo, R. (2006). Optimal decision-making with minimal waste: strategyproof redistribution of VCG payments. In AAMAS 06: Proceedings of the Fifth International Joint
Conference on Autonomous Agents and Multiagent Systems, pp. 882889, New York,
NY, USA. ACM.
Cavallo, R. (2008). Efficiency and redistribution in dynamic mechanism design. In EC 08:
Proceedings of the 9th ACM conference on Electronic commerce, pp. 220229, New
York, NY, USA. ACM.
Clarke, E. (1971). Multi-part pricing of public goods. Public Choice, 11, 1723.
de Clippel, G., Naroditskiy, V., & Greenwald, A. (2009). Destroy to save. In EC 09:
Proceedings of the tenth ACM conference on Electronic commerce, pp. 207214, New
York, NY, USA. ACM.
Faltings, B. (2005). A budget-balanced, incentive-compatible scheme for social choice. In
Agent-Mediated Electronic Commerce, AMEC, pp. 3043. Springer.
Green, J. R., & Laffont, J. J. (1979). Incentives in Public Decision Making. North-Holland
Publishing Company, Amsterdam.
Groves, T. (1973). Incentives in teams. Econometrica, 41, 617631.
Gujar, S., & Narahari, Y. (2009). Redistribution mechanisms for assignment of heterogeneous objects. In Formal Approaches to Multi-Agent Systems, (FAMAS09), Vol. 494
of CEUR Workshop Proceedings. CEUR-WS.org.
Gujar, S., & Narahari, Y. (2008). Redistribution of VCG payments in assignment of heterogeneous objects. In Papadimitriou, C. H., & Zhang, S. (Eds.), WINE, Vol. 5385 of
Lecture Notes in Computer Science, pp. 438445. Springer.
Gul, F., & Stacchetti, E. (1999). Walrasian equilibrium with gross substitutes. Journal of
Economic Theory, 87 (1), 95124.
Guo, M., & Conitzer, V. (2007). Worst-case optimal redistribution of VCG payments. In
EC 07: Proceedings of the 8th ACM conference on Electronic Commerce, pp. 3039,
New York, NY, USA. ACM.
Guo, M., & Conitzer, V. (2008a). Better redistribution with inefficient allocation in multiunit auctions with unit demand. In EC 08: Proceedings of the 9th ACM conference
on Electronic commerce, pp. 210219, New York, NY, USA. ACM.

153

fiGujar & Narahari

Guo, M., & Conitzer, V. (2008b). Optimal-in-expectation redistribution mechanisms. In
AAMAS 08: Proceedings of the 7th international joint conference on Autonomous
agents and multiagent systems, pp. 10471054, Richland, SC. International Foundation
for Autonomous Agents and Multiagent Systems.
Guo, M., & Conitzer, V. (2009). Worst-case optimal redistribution of vcg payments in
multi-unit auctions. Games and Economic Behavior, 67 (1), 6998.
Laffont, J., & Maskin, E. (1979). A differential approach to expected utility maximizing
mechanisms. In Laffont, J. J. (Ed.), Aggregation and Revelation of Preferences.
Moulin, H. (2009). Almost budget-balanced VCG mechanisms to assign multiple objects.
Journal of Economic Theory, 144, 96119.
Porter, R., Shoham, Y., & Tennenholtz, M. (2004). Fair imposition. Journal of Economic
Theory, 118 (2), 209228.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. Journal
of Finance, 16 (1), 837.

154

fiJournal of Artificial Intelligence Research 41 (2011) 367395

Submitted 03/2011; published 07/2011

The Opposite of Smoothing: A Language Model Approach
to Ranking Query-Specific Document Clusters
Oren Kurland
Eyal Krikon

kurland@ie.technion.ac.il
krikon@tx.technion.ac.il

Faculty of Industrial Engineering and Management
Technion  Israel Institute of Technology

Abstract
Exploiting information induced from (query-specific) clustering of top-retrieved documents has long been proposed as a means for improving precision at the very top ranks of
the returned results. We present a novel language model approach to ranking query-specic
clusters by the presumed percentage of relevant documents that they contain. While most
previous cluster ranking approaches focus on the cluster as a whole, our model utilizes
also information induced from documents associated with the cluster. Our model substantially outperforms previous approaches for identifying clusters containing a high relevantdocument percentage. Furthermore, using the model to produce document ranking yields
precision-at-top-ranks performance that is consistently better than that of the initial ranking upon which clustering is performed. The performance also favorably compares with
that of a state-of-the-art pseudo-feedback-based retrieval method.

1. Introduction
Users of search engines want to see the results most pertaining to their queries at the highest
ranks of the returned document lists. However, attaining high precision at top ranks is still
a very dicult challenge for search engines that have to cope with various (types of) queries
(Buckley, 2004; Harman & Buckley, 2004).
High precision at top ranks is also important for applications that rely on search as
an intermediate step; for example, question answering systems (Voorhees, 2002; CollinsThompson, Callan, Terra, & Clarke, 2004). These systems have to provide an answer to
a users query rather than return a list of documents. Often, question answering systems
employ a search over the given document corpus using the question at hand as a query
(Voorhees, 2002). Then, passages of the highest ranked documents are analyzed for extracting (compiling) an answer to the question. Hence, it is important that the documents
contain question-pertaining information.
To cope with the fact that a search engine can often return at the highest ranks of the
result list quite a few documents that are not relevant to the users query, researchers have
proposed, among others, cluster-based result interfaces (Hearst & Pedersen, 1996; Leuski,
2001). That is, the documents that are initially highest ranked are clustered into clusters
of similar documents. Then, the user can potentially exploit the clustering information to
more quickly locate relevant documents from the initial result list. An important question
in devising cluster-based result interfaces is the order by which to present the clusters to
the users (Leuski, 2001). This order should potentially reect the presumed percentage of
relevant documents in the clusters.
c
2011
AI Access Foundation. All rights reserved.

fiKurland & Krikon

Clusters of top-retrieved documents (a.k.a. query-specific clusters) can also be utilized
without the user (or an application that uses search as an intermediate step) being aware
that clustering has been performed. Indeed, researchers have proposed using information
induced from the clusters to automatically re-rank the initially retrieved list so as to improve precision at top ranks (Preece, 1973; Willett, 1985; Hearst & Pedersen, 1996; Liu &
Croft, 2004; Kurland & Lee, 2006; Yang, Ji, Zhou, Nie, & Xiao, 2006; Liu & Croft, 2008).
Much of the motivation for employing clustering of top-retrieved documents comes from van
Rijsbergens cluster hypothesis (van Rijsbergen, 1979), which states that closely associated
documents tend to be relevant to the same requests. Indeed, it was shown that applying
various clustering techniques to the documents most highly ranked by some initial search
produces some clusters that contain a very high percentage of relevant documents (Hearst
& Pedersen, 1996; Tombros, Villa, & van Rijsbergen, 2002; Kurland, 2006; Liu & Croft,
2006a). Moreover, positioning these clusters constituent documents at the very top ranks
of the returned results yields precision-at-top-ranks performance that is substantially better than that of state-of-the-art document-based retrieval approaches (Hearst & Pedersen,
1996; Tombros et al., 2002; Kurland, 2006).
Thus, whether used for creating eective result interfaces or for automatic re-ranking of
search results, whether utilized so as to help serve users of search engines or applications
that rely on search, query-specic clustering (i.e., clustering of top-retrieved documents)
can result in much merit. Yet, a long standing challenge  progress with which can yield
substantial retrieval eectiveness improvements over state-of-the-art retrieval approaches as
we show  is the ability to identify query-specic clusters that contain a high percentage
of documents relevant to the query.
We present a novel language-model-based approach to ranking query-specic clusters
by the presumed percentage of relevant documents that they contain. The key insight
that guides the derivation of our cluster-ranking model is that documents that are strongly
associated with a cluster can serve as proxies for ranking it. Since documents can be
considered as more focused units than clusters, they can serve, for example, as mediators
for estimating the cluster-query match. Thus, while most previous approaches to ranking
various types of clusters focus on the cluster as a whole unit (Jardine & van Rijsbergen,
1971; Croft, 1980; Voorhees, 1985; Willett, 1985; Kurland & Lee, 2004; Liu & Croft,
2004, 2006b), our model integrates whole-cluster-based information with that induced from
documents associated with the cluster. Hence, we conceptually take the opposite approach
to that of cluster-based smoothing of document language models that has recently been
proposed for document ranking (Azzopardi, Girolami, & van Rijsbergen, 2004; Kurland &
Lee, 2004; Liu & Croft, 2004; Tao, Wang, Mei, & Zhai, 2006; Wei & Croft, 2006); that
is, using cluster-based information to enrich a document representation for the purpose of
document ranking.
Our model integrates two types of information induced from clusters and their proxy
(associated) documents. The rst is the estimated similarity to the query. The second is the
centrality of an element (document or cluster) with respect to its reference set (documents
in the initially-retrieved list or clusters of these documents); centrality is dened in terms
of textual similarity to other central elements in the reference set (Kurland & Lee, 2005).
Using either, or both, types of information just described  induced from a cluster as a
whole and/or from its proxy documents  yields several novel cluster ranking criteria that
368

fiA Language Model Approach to Ranking Query-Specific Document Clusters

LM
Relevance model
Optimal cluster

AP
45.7
50.3
79.6

TREC8
50.0
54.4
83.6

WSJ
53.6
58.8
81.5

WT10G
33.9
35.7
65.9

Table 1: The resultant p@5 performance of nding the optimal cluster in comparison to
that of the initial LM-based ranking upon which clustering is performed, and that
of an optimized relevance model.

are integrated in our model. We study the relative contribution of each of these criteria to
the overall eectiveness of our approach. Furthermore, we show that previously proposed
cluster ranking methods, which were developed independently, can be derived or explained
using our ranking framework.
Empirical evaluation shows that our cluster ranking model consistently and substantially
outperforms previously proposed methods in identifying clusters that contain a high percentage of relevant documents. Furthermore, positioning the constituent documents of the
cluster most highly ranked by our model at the top of the list of results yields precision-attop-ranks performance that is substantially better than that of the initial document ranking
upon which clustering was performed. The resultant performance also favorably compares
with that of a state-of-the-art pseudo-feedback-based document retrieval method; and, with
that of approaches that utilize inter-document similarities (e.g., using clusters) to directly
(re-)rank documents.

2. Motivation
We rst start by demonstrating the performance merits of the ability to eectively rank
query-specic clusters by the presumed percentage of relevant documents that they contain.
Suppose that we have an initial list of documents that were retrieved in response to a
query by using a standard language model (LM) approach (Ponte & Croft, 1998; Laerty &
Zhai, 2001). Suppose also that some clustering algorithm is used to cluster the 50 highest
ranked documents, and that the resultant clusters contain 5 documents each. (Specic
details of the experimental setup are provided in Section 5.2.) We dene the optimal cluster
as the one that contains the highest percentage of relevant documents. If we position the
constituent documents of this cluster at the rst ve ranks of the document list returned in
response to the query, then the resultant precision at 5 (p@5) performance is the percentage
of relevant documents in this cluster. We contrast the p@5 performance with that of the
initial LM-based ranking. As an additional reference comparison we use an optimized
relevance model, RM3, which is a state-of-the-art pseudo-feedback-based query expansion
approach (Lavrenko & Croft, 2001; Abdul-Jaleel et al., 2004). The performance numbers
for four TREC corpora are presented in Table 1. (Details regarding the corpora and queries
used are provided in Section 5.2.)
The message rising from Table 1 is clear. If we were able to automatically identify
the optimal cluster, then the resultant performance would have been much better than
that of the initial LM-based ranking upon which clustering is performed. Furthermore, the
369

fiKurland & Krikon

performance is also substantially better than that of a state-of-the-art retrieval approach.
Similar conclusions were echoed in previous work on using clusters of top-retrieved documents (Hearst & Pedersen, 1996; Tombros et al., 2002; Crestani & Wu, 2006; Kurland,
2006; Liu & Croft, 2006a; Kurland & Domshlak, 2008).

3. Ranking Framework
Throughout this section we assume that the following have been xed: a query q, a corpus
N  D (henceforth D
of documents D, and an initial list of N documents Dinit
init ) that are the
highest ranked by some search performed in response to q. We assume that Dinit is clustered
into a set of document clusters C l(Dinit ) = {c1 , . . . , cM } by some clustering algorithm1 . Our
goal is to rank the clusters in C l(Dinit ) by the presumed percentage of relevant documents
that they contain. In what follows we use the term cluster to refer either to the set of
documents it is composed of, or to a (language) model induced from it. We use py (x) to
denote the language-model-based similarity between y (a document or a cluster) and x (a
query or a cluster); we describe our language-model induction method in Section 5.1.
3.1 Cluster Ranking
Similarly to the language model approach to ranking documents (Ponte & Croft, 1998;
Croft & Laerty, 2003), and in deference to the recent growing interest in automatically
labeling document clusters and topic models (Geraci, Pellegrini, Maggini, & Sebastiani,
2006; Treeratpituk & Callan, 2006; Mei, Shen, & Zhai, 2007), we state the problem of
ranking clusters as follows: estimate the probability p(c|q) that cluster c can be labeled
(i.e., its content can be described) by the terms in q. We hypothesize that the higher this
probability is, the higher the percentage of documents pertaining to q that c contains.
Since q is xed, we use the rank equivalence
rank

p(c|q) = p(q|c)  p(c)
to rank the clusters in C l(Dinit ). Thus, c is ranked by combining the probability p(q|c) that
q is generated2 as a label for c with cs prior probability (p(c)) of generating any label.
Indeed, most prior work on ranking various types of clusters (Jardine & van Rijsbergen,
1971; Croft, 1980; Willett, 1985; Voorhees, 1985; Kurland & Lee, 2004; Liu & Croft, 2004)
implicitly uses uniform distribution for p(c), and estimates p(q|c) (in spirit) by comparing
a representation of c as a whole unit with that of q.
Here, we suggest to incorporate a document mediated approach to estimating the probability p(q|c) of generating the label q for cluster c. Since documents can be considered as
more coherent units than clusters, they might help to generate more informative/focused
labels than those generated by using representations of clusters as whole units. Such an
1. Clustering the documents most highly ranked by a search performed in response to a query is often termed
query-specific clustering (Willett, 1985). We do not assume, however, that the clustering algorithm has
knowledge of the query in hand.
2. While the term generate is convenient, we do not assume that clusters or documents literally generate
labels, nor do we assume an underlying generative theory as that presented by Lavrenko and Croft (2001)
and Lavrenko (2004), inter alia.

370

fiA Language Model Approach to Ranking Query-Specific Document Clusters

approach is conceptually the opposite of smoothing a document representation (e.g., language model) with that of a cluster (Azzopardi et al., 2004; Kurland & Lee, 2004; Liu &
Croft, 2004; Wei & Croft, 2006). In what follows we use p(q|d) to denote the probability
that q is generated as a label describing document ds content  cf., the language modeling approach to ranking documents (Ponte & Croft, 1998; Croft & Laerty, 2003). Also,
we assume that p(d)  the prior probability that document d generates any label  is a
probability distribution over the documents in the corpus D.
We let all, and only, documents in the corpus D to serve as proxies for label generation
for any cluster in C l(Dinit ). Consequently, we assume that p(d|c), the probability that d
is chosen as a proxy of c for label generation, is a probability distribution dened over the
documents in D. Then, we can write using some probability algebra
rank

p(c|q) = p(c)

X

p(q|c, di )p(di |c).

(1)

di D

We use p(q|c) + (1  )p(q|di ), where  is a free parameter, as an estimate for p(q|c, di )
(Si, Jin, Callan, & Ogilvie, 2002; Kurland & Lee, 2004) in Equation 1, and by applying
probability algebra we get the following scoring principle3 for clusters
p(c)p(q|c) + (1  )

X

p(q|di )p(c|di )p(di ).

(2)

di D

Equation 2 scores c by a mixture of (i) the probability that q is directly generated from c
combined with cs prior probability of generating any label, and (ii) the (average) probability
that q is generated by documents that are both strongly associated with c (as measured
by p(c|di )) and that have a high prior probability p(di ) of generating labels.
We next derive specic ranking algorithms from Equation 2 by making some assumptions
and estimation choices.
3.2 Algorithms
We rst make the assumption, which underlies (in spirit) most pseudo-feedback-based retrieval models (Buckley, Salton, Allan, & Singhal, 1994; Xu & Croft, 1996; Lavrenko &
Croft, 2003), that the probability of generating q directly from di (p(q|di )) is quite small
for documents di that are not in the initially retrieved list Dinit ; hence, these documents
have relatively little eect on the summation in Equation 2. Furthermore, if the clusters in
C l(Dinit ) are produced by a reasonable clustering algorithm, then p(c|di )  the clusterdocument association strength  might be assumed to be signicantly higher for documents
from Dinit that are in c than for documents from Dinit that are not in c. Consequently, we
truncate the summation in Equation 2 by allowing only cs constituent documents to serve
as it proxies for generating q. Such truncation does not only alleviate the computational
cost of estimating Equation 2, but can also yield improved eectiveness as we show in Section 5.3. In addition, we follow common practice in the language model framework (Croft
rank

3. The shift in notation and terminology from p(c|q) =  to score of c echoes the transition from using
(model) probabilities to estimates of such probabilities.

371

fiKurland & Krikon

& Laerty, 2003), specically, in work on utilizing cluster-based language models for document retrieval (Liu & Croft, 2004; Kurland & Lee, 2004), and use language-model estimates
for conditional probabilities to produce our primary ranking principle:
def

S core(c) = p(c)pc (q) + (1  )

X

pdi (q)pdi (c)p(di ).

(3)

di c

Note that using pd (c) for p(c|d) means that we use the probability of generating the label c
(i.e., some term-based representation of c) from document d as a surrogate for the documentcluster association strength.
The remaining task is to estimate the document and cluster priors, p(d) and p(c), respectively.
3.2.1 Document and cluster biases
Following common practice in work on language-model-based retrieval we can use a uniform distribution for the document prior p(d) (Croft & Laerty, 2003), and similarly assume
a uniform distribution for the cluster prior p(c). Such practice would have been a natural choice if the clusters we want to rank were produced in a query-independent fashion.
However, we would like to exploit the fact that the clusters in C l(Dinit ) are composed of
documents in the initially retrieved list Dinit . A case in point, since Dinit was retrieved in
response to q, documents in Dinit that are considered as reecting Dinit s content might be
good candidates for generating the label q (Kurland & Lee, 2005); a similar argument can
be made for clusters in C l(Dinit ) that reect its content. Therefore, instead of using true
prior distributions, we use biases that represent the centrality (Kurland & Lee, 2005) of
documents with respect to Dinit and the centrality of clusters with respect to C l(Dinit ).4
We adopt a recently proposed approach to inducing document centrality that is based
on measuring the similarity of a document in Dinit to other central documents in Dinit (Kurland & Lee, 2005). To quantify this recursive centrality denition, we compute PageRanks
(Brin & Page, 1998) stationary distribution over a graph wherein vertices represent documents in Dinit and edge-weights represent inter-document language-model-based similarities
def

(Kurland & Lee, 2005). We then set p(d) = Cent(d) for d  Dinit and 0 otherwise, where
Cent(d) is ds PageRank score; hence, p(d) is a probability distribution over the entire
corpus D.
def

Analogously, we set p(c) = Cent(c) for c  C l(Dinit ), where Cent(c) is cs PageRank
score as computed over a graph wherein vertices are clusters in C l(Dinit ) and edge-weights
represent language-model-based inter-cluster similarities; therefore, p(c) is a probability distribution over the given set of clusters C l(Dinit ). The construction method of the document
and cluster graphs follows that of constructing document-solely graphs (Kurland & Lee,
2005), and is elaborated in Appendix A.
Using the document and cluster induced biases we can now fully instantiate Equation 3
to derive ClustRanker, our primary cluster ranking algorithm:
4. The biases are not true prior distributions, because of the virtue by which Dinit was created, that is,
in response to the query. However, we take care that the biases form valid probability distributions as
we show later.

372

fiA Language Model Approach to Ranking Query-Specific Document Clusters

Algorithm
ClustCent
ClustQueryGen
ClustCent  ClustQueryGen
DocCent
DocQueryGen
DocCent  DocQueryGen
ClustCent  DocCent
ClustQueryGen  DocQueryGen
ClustRanker

Scoring function (S core(c))
Cent(c)
pc (q)
Cent(c)p
c (q)
P
p
d
Pdi c i (c)Cent(di )
Pdi c pdi (q)pdi (c)
i)
di c pdi (q)pdi (c)Cent(d
P
Cent(c) + (1  P
) di c pdi (c)Cent(di )
pc (q) + (1  ) di c pdi (q)pdi (c)
P
Cent(c)pc (q) + (1  ) di c pdi (q)pdi (c)Cent(di )

Table 2: Summary of methods for ranking clusters.

def

S coreClustRanker (c) = Cent(c)pc (q) + (1  )

X

pdi (q)pdi (c)Cent(di ).

(4)

di c

3.2.2 Methods for ranking clusters
The ClustRanker algorithm ranks cluster c by integrating several criteria: (i) ClustCent
 cs centrality (Cent(c)), (ii) ClustQueryGen  the possibility to generate the label
q directly from c as measured by pc (q), (iii) DocCent  the centrality of cs constituent
documents (Cent(d)), and (iv) DocQueryGen  the possibility to generate q by cs
constituent documents as measured by pd (q). (Note that the latter two are combined with
the cluster-document association strength, pd (c)).
To study the eectiveness of each of these criteria (and some of their combinations)
for ranking clusters, we apply the following manipulations to the ClustRanker algorithm:
(i) setting  to 1 (0) to have only the cluster (documents) generate q, (ii) using uniform
distribution for Cent(c) (over C l(Dinit )) and/or for Cent(d) (over Dinit ) hence assuming that
all clusters in C l(Dinit ) and/or documents in Dinit are central to the same extent; we assume
that the number of clusters in C l(Dinit ) is the same as the number of documents in Dinit ,
as is the case for the clustering method that we employ in Section 5; hence, the document
uniform prior and the cluster uniform prior are the same; and (iv) setting pc (q) (pd (q)) to
the same constant value thereby assuming that for all clusters in C l(Dinit ) (documents in
Dinit ) the probability of directly generating q is the same. For instance, setting  to 0 and
pd (q) to some constant, we rank c byPDocCent  the weighted-average of the centrality
values of its constituent documents:
di c pdi (c)Cent(di ). Table 2 presents the resultant
cluster ranking methods that we explore. ( indicates that a method utilizes two criteria.)
3.3 Explaining Previous Methods for Ranking Clusters
The ClustRanker method, or more generally, Equation 3 on which it is based, can be used
so as to help explain, and derive, some previously proposed methods for ranking clusters.
While these methods were developed independently, and not as part of a single framework,
their foundations can be described in terms of our approach. In the following discussion
we use uniform prior for documents and clusters, and rank cluster c, using Equation 3, by
373

fiKurland & Krikon

P
pc (q)+(1) di c pdi (q)pdi (c). Furthermore, recall that our framework is not committed
to language models; i.e., px (y), which is a language-model-based estimate for p(y|x), can be
replaced with another estimate.
Now, setting  = 1, and consequently considering the cluster only as a whole unit, yields
the most common cluster ranking method. That is, ranking the cluster based on the match
of its representation as a whole unit with that of the query; the cluster can be represented,
for example, by using the concatenation of its constituent documents (Kurland & Lee,
2004; Liu & Croft, 2004) or by a centroid-based representation of its constituent document
representations (Voorhees, 1985; Leuski, 2001; Liu & Croft, 2008). The ClustQueryGen
method from Table 2, which was also used in previous work (Liu & Croft, 2004; Kurland
& Lee, 2004; Liu & Croft, 2006b; Kurland & Lee, 2006), is an example of this approach in
the language modeling framework.
On the other hand, setting  = 0 results
P in ranking c by using its constituent documents
rather than using c as a whole unit:
di c pdi (q)pdi (c). Several cluster ranking methods
that were proposed in past literature ignore the document-cluster association strength.
This practice amounts to setting pdi (c) to the same constant for all clusters and documents.
Assuming also that all clusters contain the same number of documents, as is the case in
our experimental setup in Section 5, we then rank
c by the arithmetic mean of the query1 P
match values of its constituent documents, |c|
di c pdi (q); |c| is the number of documents
in c. The arithmetic mean can be bounded from above by maxdi c pdi (q), which was used
in some previous work on ranking clusters (Leuski, 2001; Shanahan, Bennett, Evans, Hull,
& Montgomery, 2003; Liu & Croft,
qQ2008), or from below by the geometric mean of the
5
|c|
document-query match values,
di c pdi (q) (Liu & Croft, 2008; Seo & Croft, 2010).
Alternatively, the minimal query-document match value, mindi c pdi (q), which was also
utilized for ranking clusters (Leuski, 2001; Liu & Croft, 2008), also constitutes a lower
bound for the arithmetic mean.

4. Related Work
Query-specic clusters are often used to visualize the results of search so as to help users
to quickly detect the relevant documents (Hearst & Pedersen, 1996; Leuski & Allan, 1998;
Leuski, 2001; Palmer et al., 2001; Shanahan et al., 2003). Leuski (2001), for example,
orders (hard) clusters in an interactive retrieval system by the highest and lowest querysimilarity exhibited by any of their constituent documents. We showed in Section 3.3
that these ranking methods, and others, that were used in several reports on using queryspecic clustering (Shanahan et al., 2003; Liu & Croft, 2006a), can be explained using our
framework. Furthermore, in Section 5.3 we demonstrate the merits of ClustRanker with
respect to these approaches.
Some work uses information from query-specic clusters to smooth language models
of documents in the initial list so as to improve the document-query similarity estimate
(Liu & Croft, 2004; Kurland, 2009). In a related vein, graph-based approaches for reranking the initial list, some using document clusters, that utilize inter-document similarity
5. Liu and Croft (2008) and Seo and Croft (2010) used the geometric-mean-based language model representation of clusters, rather than the geometric mean of the query-match values.

374

fiA Language Model Approach to Ranking Query-Specific Document Clusters

information were also proposed (Diaz, 2005; Kurland & Lee, 2005, 2006; Yang et al., 2006).
These approaches can potentially help to improve the performance of our ClustRanker
algorithm, as they provide a higher quality document ranking to begin with. Graph-based
approaches for modeling inter-item textual similarities, some similar in spirit to our methods
of inducing document and cluster centrality, were also used for text summarization, question
answering, and clustering (Erkan & Radev, 2004; Mihalcea, 2004; Mihalcea & Tarau, 2004;
Otterbacher, Erkan, & Radev, 2005; Erkan, 2006a, 2006b).
Ranking query-specic (and query-independent) clusters in response to a query has traditionally been based on comparing a cluster representation with that of the query (Jardine
& van Rijsbergen, 1971; Croft, 1980; Voorhees, 1985; Willett, 1985; Kurland & Lee, 2004;
Liu & Croft, 2004, 2006b, 2006a). The ClustQueryGen criterion, which was used in work
on ranking query-specic clusters in the language model framework (Liu & Croft, 2004;
Kurland, 2009), is a language-model manifestation of this ranking approach. We show that
the eectiveness of ClustQueryGen is inferior to that of ClustRanker in Section 5.3.
Some previous cluster-based document-ranking models (Kurland & Lee, 2004; Kurland,
2009) can be viewed as the conceptual opposite of our ClustRanker method as they use
clusters as proxies for ranking documents. However, these models use only query-similarity
information while ClustRanker integrates such information with centrality information. In
fact, we show in Section 5.3 that centrality information is often more eective than querysimilarity (generation) information for ranking query-specic clusters; and, that their integration yields better performance than that of using each alone.
Recently, researchers have identied some properties of query-specic clusters that contain a high percentage of relevant documents (Liu & Croft, 2006b; Kurland & Domshlak, 2008); among which are the cluster-query similarity (ClustQueryGen) (Liu & Croft,
2006b), the query similarity of the clusters constituent documents (DocQueryGen) (Liu
& Croft, 2006b; Kurland & Domshlak, 2008), and the dierences between the two (Liu &
Croft, 2006b). These properties were utilized for automatically deciding whether to employ
cluster-based or document-based retrieval in response to a query (Liu & Croft, 2006b),
and for ranking query-specic clusters (Kurland & Domshlak, 2008). The latter approach
(Kurland & Domshlak, 2008) relies on rankings induced by clusters models over the entire
corpus, in contrast to our approach that focuses on the context within the initially retrieved
list. However, our centrality-based methods from Table 2 can potentially be incorporated
in this cluster-ranking framework (Kurland & Domshlak, 2008).
Some work on ranking query-specic clusters resembles ours in that it utilizes clustercentrality information (Kurland & Lee, 2006); in contrast to our approach, centrality is
induced based on cluster-document similarities. We further discuss this approach and compare it to ours in Section 5.3.

5. Evaluation
We next evaluate the eectiveness of our cluster ranking approach in detecting query-specic
clusters that contain a high percentage of relevant documents.
375

fiKurland & Krikon

5.1 Language-Model Induction
For language model induction, we treat documents and queries as term sequences. While
there are several possible approaches of representing clusters as whole units (Voorhees,
1985; Leuski, 2001; Liu & Croft, 2006b; Kurland & Domshlak, 2008), our focus here is
on the underlying principles of our ranking framework. Therefore, we adopt an approach
commonly used in work on cluster-based retrieval (Kurland & Lee, 2004; Liu & Croft, 2004;
Kurland & Lee, 2006; Liu & Croft, 2006a), and represent a cluster by the term sequence
that results from concatenating its constituent documents. The order of concatenation has
no eect since we only dene unigram language models that assume term independence.
Dir[]
We use px
() to denote the Dirichlet-smoothed unigram language model induced
from term sequence x (Zhai & Laerty, 2001);  is the smoothing parameter. To avoid
length bias and underow issues when assigning language-model probabilities to long texts
(Lavrenko et al., 2002; Kurland & Lee, 2005), as is the case for pd (c), we adopt the following
measure (Laerty & Zhai, 2001; Kurland & Lee, 2004, 2005, 2006), which is used for all
the language-model-based estimates in the experiments to follow, unless otherwise specied
(specically, for relevance-model construction):
fifi



def
fifi Dir[]
p
()
;
py (x) = exp D pDir[0]
()
fi
fi
y
x
x and y are term sequences, and D is the Kullback-Leibler (KL) divergence. The estimate
was empirically demonstrated as eective in settings wherein long texts are assigned with
language-model probabilities (Kurland & Lee, 2004, 2005, 2006).
Although the estimate just described does not constitute a probability distribution 
as is the case for unigram language models  some previous work demonstrates the merits
of using it as is without normalization (Kurland & Lee, 2005, 2006).
5.2 Experimental Setup
We conducted experiments with the following TREC corpora:
corpus
AP
TREC8
WSJ
WT10G

# of docs
242,918
528,155
173,252
1,692,096

queries
51-64, 66-150
401-450
151-200
451-550

disk(s)
1-3
4-5
1-2
WT10G

Some of these data sets were used in previous work on ranking query-specic clusters (Liu
& Croft, 2004; Kurland & Lee, 2006; Liu & Croft, 2008) with which we compare our
methods. We used the titles of TREC topics for queries. We applied tokenization and
Porter stemming via the Lemur toolkit (www.lemurproject.org), which was also used for
language model induction.
We set Dinit , the list upon which clustering is performed, to the 50 highest ranked
Dir[]
documents by an initial ranking induced over the entire corpus using pd
(q)  i.e., a
standard language-model approach. To have an initial ranking of a reasonable quality, we
set the smoothing parameter, , to a value that results in optimized MAP (calculated at
the standard 1000 cuto) performance. This practice also facilitates the comparison with
376

fiA Language Model Approach to Ranking Query-Specific Document Clusters

some previous work on cluster ranking (Kurland & Lee, 2006), which employs the same
approach for creating an initial list of 50 documents to be clustered. The motivation for
using a relatively short initial list rises from previous observations regarding the eectiveness
of methods that utilize inter-document similarities among top-retrieved documents (Liu &
Croft, 2004; Diaz, 2005; Kurland, 2006, 2009). The documents most highly ranked exhibit
high query similarity, and hence, short retrieved lists could be viewed as providing a more
concise corpus context for the query than longer lists. Similar considerations were echoed
in work on pseudo-feedback-based query expansion, wherein top-retrieved documents are
used for forming a new query model (Xu & Croft, 1996; Zhai & Laerty, 2001; Lavrenko &
Croft, 2001; Tao & Zhai, 2006).
To produce the set C l(Dinit ) of query-specic clusters, we use a simple nearest-neighborsbased clustering approach that is known to produce (some) clusters that contain a high
percentage of relevant documents (Kurland, 2006; Liu & Croft, 2006a). Given d  Dinit
we dene a cluster that contains d and the k  1 documents di  Dinit (di 6= d) that yield
the highest language-model similarity pdi (d). (We break ties by document IDs.) The high
percentages of relevant documents in an optimal cluster that were presented in Table 1
are for these clusters. More generally, this clustering approach was shown to be eective
for cluster-based retrieval (Griths, Luckhurst, & Willett, 1986; Kurland & Lee, 2004;
Kurland, 2006; Liu & Croft, 2006b, 2006a; Tao et al., 2006), specically, with respect to
using hard clusters (Kurland, 2009).
We posed our cluster ranking methods as a means for increasing precision at the very
top ranks of the returned document list. Thus, we evaluate a cluster ranking method by
the percentage of relevant documents in the highest ranked cluster. We use p@k to denote
the percentage of relevant documents in a cluster of size k (either 5 or 10), because it is the
precision of the top k documents that is obtained if the clusters (k) constituent documents
are positioned at the top ranks of the results. This cluster ranking evaluation approach
was also employed in previous work on ranking clusters (Kurland & Lee, 2006; Liu & Croft,
2008) with which we compare our methods. We determine statistically signicant dierences
of p@k performance using Wilcoxons two-sided test at a condence level of 95%.
To focus on the underlying principles of our approach and its potential eectiveness,
and more specically, to compare the relative eectiveness and contribution to the overall
performance of the dierent information types utilized by our methods, we rst ameliorate
free-parameter-values eects. To that end, we set the values of free parameters incorporated by our methods to optimize average (over all queries per corpus) p@k performance
for clusters of size k. (Optimization is based on a line search of the free-parameter values
ranges.) We employ the same practice for all reference comparisons. That is, we independently optimize performance with respect to free-parameter values for p@5 and p@10.
Then, in Section 5.3.5 we analyze the eect of free-parameter values on the eectiveness of
our approach. In addition, in Section 5.3.6 we study the performance of our approach when
free-parameter values are set using cross validation performed over queries. The value of ,
the interpolation parameter in the ClustRanker algorithm, is selected from {0, 0.1, . . . , 1}.
The values of the (two) parameters controlling the graph-construction methods (for inducing the document and cluster biases) are chosen from previously suggested ranges (Kurland
& Lee, 2005). (See Appendix A for further details on graph construction.) The value of ,
the language model smoothing parameter, is set to 2000 following previous recommenda377

fiKurland & Krikon

tions (Zhai & Laerty, 2001), except for estimating pd (q) where we use the value chosen for
creating Dinit so as to maintain consistency with the initial ranking.
It is important to point out that the computational overhead of our approach on top
of the initial search is not signicant. Clustering of top-retrieved documents (50 in our
case) can be performed quickly (Zamir & Etzioni, 1998); we note that our framework is
not committed to a specic clustering approach. Furthermore, computing PageRank scores
over a graph of 50 documents (clusters) to induce document (cluster) centrality takes only
a few iterations of the Power method (Golub & Van Loan, 1996). Finally, we note that the
number of documents in the corpus has no eect on the eciency of our approach, as our
methods are based on clustering the documents most highly ranked by the initial search.
5.3 Experimental Results
In what follows we present and analyze the performance numbers of our cluster ranking
approach, and study the impact of various factors on its eectiveness. In Section 5.3.1 we
study the eectiveness of ClustRanker as a means for improving precision at top ranks. To
that end, we use a comparison with the initial ranking upon which clustering is performed,
and with relevance models (Lavrenko & Croft, 2001). Then, in Section 5.3.2 we study the
relative performance eect of the various cluster ranking criteria integrated by ClustRanker.
We compare the eectiveness of ClustRanker with that of previously proposed methods for
cluster ranking in Section 5.3.3. In Section 5.3.4 we compare the performance of ClustRanker with that of document-based re-ranking approaches that utilize inter-documents
similarities in various ways. In Section 5.3.5 we analyze the performance sensitivity of
ClustRanker with respect to free-parameter values. Finally, in Section 5.3.6 we analyze the
performance of ClustRanker, and contrast it with that of various reference comparisons,
when free-parameter values are set using cross validation performed over queries.
5.3.1 Comparison with Document-Based Retrieval
The rst question we are interested in is the eectiveness (or lack thereof) of ClustRanker
in improving precision at the very top ranks. Recall that we use ClustRanker to rank
clusters of k ( {5, 10}) documents from Dinit  the initially retrieved document list.
As described above, we evaluate ClustRankers eectiveness by the percentage of relevant
documents in the cluster most highly ranked. This percentage is the p@k attained if the
clusters constituent documents are positioned at the highest ranks of the nal result list. In
Table 3 we compare the performance of ClustRanker with that of the initial ranking. Since
the initial ranking was created using a standard language-model-based document retrieval
performed over the corpus with pd (q) as a scoring function, and with the document language
model smoothing parameter () optimized for MAP, we also consider optimized baselines
as reference comparisons: ranking all documents in the corpus by pd (q) where  is set to
optimize (independently) p@5 and p@10.
As we can see in Table 3, ClustRanker posts performance that is substantially better
than that of the initial ranking in all relevant comparisons (corpus  evaluation measure). For AP and WT10G the performance improvements are also statistically signicant.
Furthermore, ClustRanker almost always outperforms the optimized baselines, often to a
substantial extent; in several cases, the improvements are also statistically signicant.
378

fiA Language Model Approach to Ranking Query-Specific Document Clusters

init. rank.
opt. base.
ClustRanker

AP
p@5 p@10
45.7
43.2
46.5
43.7
i
52.7 50.6io

TREC8
p@5 p@10
50.0
45.6
51.2
46.4
57.6 50.6

WSJ
p@5 p@10
53.6
48.4
56.0 49.4
56.0 51.2

WT10G
p@5 p@10
33.9
28.0
34.1
28.2
39.8io 33.9io

Table 3: Comparison of ClustRanker with the initial document ranking and optimized baselines. Boldface marks the best result in a column; i and o mark statistically
signicant dierences with the initial ranking, and optimized baselines, respectively.

init. rank.
Rel Model
Rel Model(Re-Rank)
ClustRanker

AP
p@5 p@10
45.7
43.2
i
50.3
48.6i
51.1i 48.3i
52.7i 50.6i

TREC8
p@5 p@10
50.0
45.6
54.4
50.2
53.6
49.8
57.6 50.6

WSJ
p@5 p@10
53.6
48.4
i
58.4 53.2i
58.8i 53.4i
56.0
51.2

WT10G
p@5 p@10
33.9
28.0
35.7
29.9
36.3
30.1
i
39.8 33.9i

Table 4: Comparison of ClustRanker with a relevance model (RM3) used to either rank
the entire corpus (Rel Model) or to re-rank the initial list (Rel Model(Re-Rank)).
Boldface marks the best result in a column; i marks statistically signicant difference with the initial ranking.

Comparison with Pseudo-Feedback-Based Retrieval The ClustRanker algorithm
helps to identify relevant documents in Dinit by exploiting clustering information. Pseudofeedback-based query expansion approaches, on the other hand, dene a query model based
on Dinit and use it for (re-)ranking the entire corpus (Buckley et al., 1994; Xu & Croft,
1996). To contrast the two paradigms, we use the relevance model RM3 (Lavrenko & Croft,
2001; Abdul-Jaleel et al., 2004; Diaz & Metzler, 2006), which is a state-of-the-art pseudofeedback-based query expansion approach. We use RM3 for ranking the entire corpus as
is standard, and refer to this implementation as Rel Model. Since ClustRanker can be
thought of as a means to re-ranking the initial list Dinit , we also experiment with using RM3
for re-ranking only Dinit , rather than the entire corpus; Rel Model(Re-Rank) denotes this
implementation. We set the values of the free parameters of Rel Model and Rel Model(ReRank) so as to independently optimize p@5 and p@10 performance. (See Appendix B for
details regarding the relevance model implementation.)
We can see in Table 4 that ClustRanker outperforms the relevance models on AP,
TREC8 and WT10G; for WSJ, the relevance models outperform ClustRanker. The performance dierences between ClustRanker and the relevance models, however, are not statistically signicant. Nevertheless, these results attest to the overall eectiveness of our
approach in attaining high precision at top ranks. As we later show, previous methods
for ranking clusters often yield performance that is only comparable to that of the initial
ranking, and much inferior to that of the relevance model.
379

fiKurland & Krikon

init. rank.

p@5
45.7

AP
p@10
43.2

TREC8
p@5
p@10
50.0
45.6

WSJ
p@5
p@10
53.6
48.4

WT10G
p@5
p@10
33.9
28.0

ClustCent
ClustQueryGen
ClustCent  ClustQueryGen

51.7
39.2i
49.7

48.6i
38.8i
48.0i

52.4
39.6i
55.2

49.4
40.6i
50.4

54.8
44.0i
52.4

50.0
37.0i
47.8

39.8i
30.0
39.6i

33.0i
24.1
33.1i

DocCent
DocQueryGen
DocCent  DocQueryGen

52.9i
43.6
52.7i

48.8
46.7
50.6i

52.0
47.6
54.8

48.8
43.2
49.0

55.6
55.2
56.0

50.6
47.0
51.2

31.0
33.5
37.1

28.1
27.0
31.4

ClustCent  DocCent
ClustQueryGen  DocQueryGen

53.5i
43.6

48.8i
46.7

54.8
47.6

49.8
43.2

56.0
55.2

51.4
47.8

39.8i
36.5

33.0i
29.1

ClustRanker

52.7i

50.6i

57.6

50.6

56.0

51.2

39.8i

33.9i

Table 5: Comparison of the cluster ranking methods from Table 2. Boldface marks the best
result in a column and i indicates a statistically signicant dierence with the
initial ranking.

5.3.2 Deeper Inside ClustRanker
We now turn to analyze the performance of the various cluster ranking criteria (methods)
that ClustRanker integrates so as to study their relative contribution to its overall eectiveness. (Refer back to Table 2 for specication of the dierent methods.) The performance
numbers are presented in Table 5.
We are rst interested in the comparison of the two types of information utilized for
ranking, that is, centrality and query-similarity (generation). We can see in Table 5 that
in almost all relevant comparisons (corpus  evaluation metric), using centrality information yields performance that is superior to that of using query-similarity (generation)
information. (Compare ClustCent with ClustQueryGen, DocCent with DocQueryGen, and
ClustCent  DocCent with ClustQueryGen  DocQueryGen.) Specically, we see that
cluster-query similarity (ClustQueryGen), which was the main ranking criterion in previous
work on cluster ranking, yields performance that is much worse than that of cluster centrality (ClustCent)  a cluster ranking criterion which is novel to this study. In addition,
we note that integrating centrality and query-similarity (generation) information can often
yield performance that is better than that of using each alone, as is the case for DocCent
 DocQueryGen with respect to DocCent and DocQueryGen.
We next turn to examine the relative eectiveness of using the cluster as a whole versus
using its constituent documents. When using only query-similarity (generation) information, we see that using the documents in the cluster is much more eective than using the
cluster as a whole. (Compare DocQueryGen with ClustQueryGen.) This nding further
attests to the merits of using documents as proxies for ranking clusters  the underlying
idea of our approach. When using centrality information, the picture is split across corpora:
for AP and WSJ using the documents in the cluster yields better performance than using
the whole cluster, while the reverse holds for TREC8 and WT10G. (Compare DocCent
with ClustCent.) Integrating whole-cluster-based and document-based information results
in performance that is for all corpora (much) better than that of using the less eective of
the two, and sometimes even better than the more eective of the two.
380

fiA Language Model Approach to Ranking Query-Specific Document Clusters

init. rank.
d  Dinit
dc

AP
p@5 p@10
45.7
43.2
49.5
47.6
i
52.7 50.6i

TREC8
p@5 p@10
50.0
45.6
54.0
49.8
57.6 50.6

WSJ
p@5 p@10
53.6
48.4
52.8
49.6
56.0 51.2

WT10G
p@5 p@10
33.9
28.0
39.6i 33.2i
39.8i 33.9i

Table 6: Performance numbers of ClustRanker when either all documents in Dinit serve as
proxies for cluster c (denoted d  Dinit ), or only cs constituent documents serve
as its proxies, as in the original implementation (denoted d  c). Boldface marks
the best result in a column; i marks statistically signicant dierences with the
initial ranking.

It is not a surprise, then, that the ClustRanker method, which integrates centrality
information and query-similarity (generation) information that are induced from both the
cluster as a whole and from its constituent documents, is in most relevant comparisons the
most eective cluster ranking method among those presented in Table 5.
Documents as Proxies for Clusters The ndings presented above demonstrated the
merits of using documents as proxies for clusters. We now turn to study the eect on performance of the documents selected as proxies. The derivation of ClustRanker was based on
truncating the summation in Equation 2 (Section 3) so as to allow only cs constituent documents to serve as its proxies. We examine a variant of ClustRanker wherein all documents
in the initial list Dinit can serve as cs proxies:
X
def
S core(c) = Cent(c)pc (q) + (1  )
pdi (q)pdi (c)Cent(di ).
di Dinit

As can be seen in Table 6, this variant (represented by the row labeled d  Dinit )
posts performance that is almost always better than that of the initial document ranking
from which Dinit is derived. However, the performance is also consistently worse than that
of the original implementation of ClustRanker (represented by the row labeled d  c)
that lets only cs constituent documents to serve as its proxies. Furthermore, this variant of
ClustRanker posts less statistically signicant improvements over the initial ranking than
the original implementation. (The performance dierences between the two variants of
ClustRanker, however, are not statistically signicant.) Thus, as was mentioned in Section
3, using only the clusters constituent documents as its proxies is not only computationally
convenient, but also yields performance improvements.
5.3.3 Comparison with Past Approaches for Ranking Clusters
In Table 7 we compare the performance of ClustRanker with that of previously proposed
methods for ranking clusters. In what follows we rst discuss these methods, and then
analyze the performance patterns.
Most previous approaches to ranking (various types of) clusters compare a cluster representation with that of the query (Jardine & van Rijsbergen, 1971; Croft, 1980; Kurland
& Lee, 2004; Liu & Croft, 2004, 2006b). Specically, in the language model framework,
381

fiKurland & Krikon

query-specic clusters were ranked by the probability assigned by their induced language
models to the query (Liu & Croft, 2004, 2006b; Kurland & Lee, 2006). Note that this is
exactly the ClustQueryGen method in our setup, which ranks c by pc (q).
There has been some work on using the maximal (Leuski, 2001; Shanahan et al., 2003;
Liu & Croft, 2008) and minimal (Leuski, 2001; Liu & Croft, 2008) query-similarity values
of the documents in a cluster for ranking it. We showed in Section 3.3 that these approaches can be explained in terms of our framework; specically, the score of cluster c is
maxdi c pdi (q) and mindi c pdi (q), respectively. However, these methods were originally proposed for ranking hard clusters. As the clusters we rank here are overlapping, these ranking
criteria are somewhat less appropriate as they result in many ties of cluster scores. Still,
we use these methods as baselines and break ties arbitrarily as they were also used in some
recent work on ranking nearest-neighbors-based clusters (Liu & Croft, 2008).6 Following
some observations with regard to the merits of representing clusters by using the geometric mean of their constituent documents representations (Liu & Croft, 2008; Seo & Croft,
2010), we also consider the
geometric mean of the query-similarity values of documents in
qQ
c for ranking it; that is, |c| di c pdi (q).7

An additional reference comparison that we consider, which was shown to yield eective
cluster ranking performance, is a recently proposed (bipartite-)graph-based approach (Kurland & Lee, 2006). Documents in Dinit are vertices on one side, and clusters in C l(Dinit )
are vertices on the other side; an edge connects document d with the  clusters ci that
yield the highest language-model similarity pci (d), which also serves as a weight function
for the edges. Then, Kleinbergs (1997) HITS (hubs and authorities) algorithm is run on
the graph, and clusters are ranked by their induced authority values. It was shown that
the cluster with the highest authority value tends to contain a high percentage of relevant
documents (Kurland & Lee, 2006). For implementation, we follow the details provided by
Kurland and Lee (2006); specically, we choose the value of  from {2, 4, 9, 19, 29, 39, 49} so
as to optimize p@k performance for clusters of size k.
Table 7 presents the comparison of ClustRanker with the reference comparisons just
described. The p@k (k  {5, 10}) reported for a cluster ranking method is the percentage
of relevant documents in the highest ranked cluster, wherein clusters contain k documents
each.
We can see in Table 7 that ClustRanker outperforms all reference comparisons in almost all cases. Many of the these performance dierences are also statistically signicant.
Furthermore, ClustRanker is the only cluster ranking method in Table 7 that consistently
outperforms the initial ranking. Moreover, ClustRanker posts more statistically signicant
improvements over the initial ranking than the other cluster ranking methods do.
6. As noted above, previous work on cluster-based retrieval has demonstrated the merits of using overlapping
nearest-neighbors-based clusters with respect to using hard clusters (Kurland, 2009). Indeed, recent work
on cluster ranking has focused on ranking nearest-neighbor-based clusters as we do here (Kurland & Lee,
2006; Liu & Croft, 2006b, 2006a, 2008; Seo & Croft, 2010).
7. We note that in the original proposals (Liu & Croft, 2008; Seo & Croft, 2010) the geometric mean of
language models was used at the term level rather than at the query-assigned score level as we use here.
To maintain consistency with the other cluster-ranking methods explored, we use the geometric mean
at the query-assigned score level; and, we hasten to point out that a geometric-mean-based language
model (Liu & Croft, 2008; Seo & Croft, 2010) could be used instead of the standard language model for
clusters in ClustRanker and in the reference comparisons so as to potentially improve performance.

382

fiA Language Model Approach to Ranking Query-Specific Document Clusters

p@5
45.7

AP
p@10
43.2

p@5
50.0

39.2i

38.8i

39.6i

40.6i

44.0i

37.0i

30.0

24.1

41.8

40.3

38.8i

41.6

51.2

46.6

33.9

29.2

SM in (c) = mindi Dinit pdi (q)
qQ
def
SGeoM ean (c) = |c| di c pdi (q)

47.0

46.7

46.4

48.4

48.4

47.8

31.4

25.9

44.4

46.7i

50.0

49.6

56.0

50.6

37.4

31.8i

SHIT S (c)

49.5

47.2

50.8

46.6

53.6

49.0

26.7i

23.9i

SClustRanker (c)

52.7icM
g

50.6icM

57.6cM
mh

50.6cM
m

56.0cm

51.2c

39.8icM
mh

33.9icM
mh

init. rank.
def

SClustQueryGen (c) = pc (q)
def

SM ax (c) = maxdi Dinit pdi (q)
def

TREC8
p@10
45.6

p@5
53.6

WSJ
p@10
48.4

p@5
33.9

WT10G
p@10
28.0

Table 7: Comparison of ClustRanker with previously proposed methods for ranking clusters. (S stands for the cluster-scoring function.) Boldface marks the best result
in a column; i marks statistically signicant dierence between a method and the
initial ranking; c, M, m, g, and h mark statistically signicant dierences of
ClustRanker with the ClustQueryGen, Max, Min, GeoMean and HITS methods,
respectively.

AP
init. rank.
HITS
ClustCent
DocCent
ClustCent  DocCent

p@5
45.7
49.5
51.7
52.9ih
53.5ih

p@10
43.2
47.2
48.6i
48.8
48.8i

TREC8
p@5 p@10
50.0
45.6
50.8
46.6
52.4
49.4
52.0
48.8
54.8 49.8

WSJ
p@5 p@10
53.6
48.4
53.6
49.0
54.8
50.0
55.6
50.6
56.0 51.4

WT10G
p@5
p@10
33.9
28.0
i
26.7
23.9i
39.8ih 33.0ih
31.0h
28.1h
ih
39.8
33.0ih

Table 8: Comparison of our centrality-solely approaches for ranking clusters with the HITSbased method (Kurland & Lee, 2006). Boldface marks the best performance in a
column; i and h mark statistically signicant dierences with the initial ranking
and the HITS method, respectively.

The Comparison with the HITS-Based Approach The HITS-based method (Kurland & Lee, 2006) utilizes cluster centrality information as induced over a cluster-document
graph. Our ClustRanker method, on the other hand, integrates centrality information 
induced over document-solely and cluster-solely graphs  with query-similarity (generation) information. In Table 8 we contrast the resultant performance of using the dierent
notions of centrality utilized by the two methods. We present the performance of our
centrality-solely-based methods ClustCent, DocCent, and ClustCent  DocCent and of the
HITS approach (Kurland & Lee, 2006).
We can see in Table 8 that all our centrality-solely-based approaches outperform the
HITS-based method in all relevant comparisons. These results attest to the eective utilization of (a specic type of) centrality information by our framework.
383

fiKurland & Krikon

5.3.4 Comparison with Utilizing Inter-Document Similarities to Directly
Rank Documents
Ranking clusters based on the presumed percentage of relevant documents that they contain,
as in ClustRanker, is one approach of utilizing inter-document similarities so as to improve
document-ranking eectiveness. Alternatively, inter-document similarities can be exploited
so as to directly rank documents. For example, a re-ranking principle that was demonstrated
to be eective is rewarding documents that are initially highly ranked, and which are highly
similar to many other documents in the initial list (Balinski & Danilowicz, 2005; Diaz,
2005; Kurland & Lee, 2005). Specically, using the DocCent component of ClustRanker,
that is, the PageRank score of document d (Cent(d)) as induced over a document-similarity
graph, and scaling this value by pd (q)  which is the initial (query similarity) score of
d  was shown to be an eective re-ranking criterion (Kurland & Lee, 2005). We use
PR+QuerySim to denote this method.
Another approach that we consider is ranking documents using clusters as a form of an
extended document representation. (In language model terms this translates to clusterbased smoothing.) Specically, we use the Interpolation algorithm (Kurland & Lee, 2004),
which was shown to be highlyPeective for re-ranking (Kurland, 2009). A document d
is scored by pd (q) + (1  ) cC l(Dinit) pc (q)pd (c);  is a free parameter. That is, a
document is rewarded by its direct match with the query (pd (q)), which is the criterion
used for creating the initial ranking, and by the query match of clusters (pc (q)) with
which it is strongly associated (as measured by pd (c)). In other words, the Interpolation
model backs o from a document representation to a cluster-based representation. The
Interpolation model could conceptually be viewed as a generalization of methods that use
a single cluster (Liu & Croft, 2004; Tao et al., 2006) or a topic model (Wei & Croft, 2006)
for smoothing a document language model. Furthermore, note that while Interpolation
uses clusters as document proxies for ranking documents, our ClustRanker method uses
documents as cluster proxies for ranking clusters.
In Table 9 we compare the performance of ClustRanker with that of the PR+QuerySim
and Interpolation methods just described. The performance of Interpolation is independently optimized for p@5 and p@10 using clusters of 5 and 10 documents, respectively, as
is the case for ClustRanker; the value of  is chosen from {0, 0.1, . . . , 0.9}. The performance of PR+QuerySim is also independently optimized for p@5 and p@10, when setting
the graph out degree parameter () to 4 and 9 respectively, and selecting the value of 
from {2, 4, 9, 19, 29, 39, 49}. (Setting the graph out-degree parameter to a value x amounts
to having a document transfer centrality support to x other documents; hence, using the
values of 4 and 9 amounts to considering local neighborhoods of 5 and 10 documents in
the similarity space, respectively; this is conceptually reminiscent of using clusters of size 5
and 10 respectively. Refer to Appendix A for further details.)
We can see in Table 9 that ClustRanker outperform PR+QuerySim and Interpolation
in most relevant comparisons. (Several of the performance dierences with the former
are statistically signicant, while those with the latter are not.) Specically, the relative
performance improvements for WT10G are quite substantial. (We hasten to point out,
however, that Interpolation posts more statistically signicant performance improvements
over the initial ranking than ClustRanker does.)
384

fiA Language Model Approach to Ranking Query-Specific Document Clusters

init. rank.
PR+QuerySim
Interpolation
ClustRanker

AP
p@5 p@10
45.7
43.2
49.5
49.5i
51.3i 50.3i
52.7ip 50.6i

TREC8
p@5 p@10
50.0
45.6
56.0 51.0i
55.6i 49.6i
57.6 50.6

WSJ
p@5 p@10
53.6
48.4
i
57.2
50.4
56.8
52.4
56.0
51.2

WT10G
p@5 p@10
33.9
28.0
35.9
30.4
36.1
31.8i
39.8ip 33.9ip

Table 9: Comparison with re-ranking methods that utilize inter-document similarities to
directly rank documents. Boldface marks the best result in a column. Statistically
signicant dierences with the initial ranking and PR+QuerySim are marked with
i and p, respectively. There are no statistically signicant dierences between
ClustRanker and Interpolation.

All in all, we see that ranking clusters as is done by ClustRanker can result in (document)
re-ranking performance that is at least as eective (and often more eective) than that of
methods that utilize inter-document similarities to directly rank documents.
5.3.5 Performance-Sensitivity Analysis
We next turn to analyze the eect of varying the values of the free parameters that ClustRanker incorporates on its performance. The rst parameter, , controls the reliance on
the cluster as a whole unit versus its constituent documents. (Refer back to Equation 4 in
Section 3 for details.) Figure 1 depicts the p@5 performance of ClustRanker, with clusters
of 5 documents, as a function of ; the p@10 performance patterns of ClustRanker with
clusters of 10 documents are similar, and are omitted to avoid cluttering the presentation.
We can see in Figure 1 that for AP, TREC8 and WT10G, all values of  result in
performance that is (much) better than that of the initial ranking; for WSJ,   0.4 results
in performance superior to that of the initial ranking. Furthermore, for AP, TREC8, and
WT10G,  = 0.4, which strikes a good balance between using the cluster as a whole and
using its constituent documents, yields (near) optimal performance; for WSJ, smaller values
of  (specically,  = 0), which result in more weight put on the clusters constituent
documents, are more eective. Thus, we witness again the importance of using the clusters
constituent documents as proxies when ranking the cluster.
The graph-based method used by ClustRanker for inducing document (and cluster)
centrality depends on two free parameters:  (the number of nearest neighbors considered
for each element in the graph), and  (PageRanks damping factor); see Appendix A for
details. As noted above, both the document graph and the cluster graph are constructed
using the same values of these two parameters. Figures 2 and 3 depict the eect of the
values of  and , respectively, on the p@5 performance of ClustRanker with clusters of 5
documents.
We can see in Figure 2 that all values of  result in performance that is (often much)
better than that of the initial ranking. In general, small values of  yield the best performance. This nding is in accordance with those reported in previous work on using
385

fiKurland & Krikon

AP

TREC8

Effect of  on p@5, corpus=AP

Effect of  on p@5, corpus=TREC8

54

58

52
56

54

48

p@5

p@5

50

46

52

44
50
42

ClustRanker
init. rank.

40
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

ClustRanker
init. rank.

48
0.9

1

0

0.1

0.2

0.3

0.4



0.5

0.6

0.7

0.8

0.9

1

0.9

1



WSJ

WT10G

Effect of  on p@5, corpus=WSJ

Effect of  on p@5, corpus=WT10G

58
40

57
56

38
54

p@5

p@5

55

53

36
34

52
51

32

50

ClustRanker
init. rank.

49
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

ClustRanker
init. rank.

30
0.9

1

0



0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8



Figure 1: Eect of varying  on the p@5 performance of ClustRanker.
nearest-neighbor-based graphs for ranking documents in an initially retrieved list using
document-solely graphs (Diaz, 2005; Kurland & Lee, 2005).
Figure 3 shows that for almost every value of , the resultant performance of ClustRanker
transcends that of the initial ranking; in many cases, the improvement is quite substantial.
5.3.6 Learning Free-Parameter Values
Heretofore, we studied the performance of ClustRanker, and analyzed the eectiveness of
the information types that it utilizes, while ameliorating free-parameter values eects. That
is, we reported performance for parameter values that result in optimized average p@k over
the entire set of queries per corpus. We have applied the same practice for all reference
comparisons that we considered, which resulted in comparing the potential eectiveness
of our approach with that of previously suggested ones. In addition, we studied in the
previous section the eect of the values of free parameters incorporated by ClustRanker on
its (average) performance.
We now turn to study the question of whether eective values of the free parameters
of ClustRanker generalize across queries; that is, whether these values can be learned. To
perform this study, we employ a leave-one-out cross validation procedure. For each query,
the free parameters of ClustRanker are set to values that yield optimal average p@k over
386

fiA Language Model Approach to Ranking Query-Specific Document Clusters

AP

TREC8

Effect of  on p@5, corpus=AP

Effect of  on p@5, corpus=TREC8

54

58

52

56

50
p@5

p@5

54
48

52
46
50

44
ClustRanker
init. rank.

42
2 4

9

19

29

39

ClustRanker
init. rank.

48
49

2 4

9

19



29

39

49



WSJ

WT10G

Effect of  on p@5, corpus=WSJ

Effect of  on p@5, corpus=WT10G

58

42

57
40

56

38

54

p@5

p@5

55

53

36

52
51

34

50

ClustRanker
init. rank.

49
2 4

9

19

29

39

ClustRanker
init. rank.

32
49

2 4



9

19

29

39

49



Figure 2: Eect of varying , one of the graph parameters (refer to Appendix A), on the
p@5 performance of ClustRanker.

all other queries for the same corpus. Then, we report the resultant average p@k over all
queries per corpus. Thus, the reported p@k numbers are based on learning performed with
p@k as the optimization metric.
We contrast the performance of ClustRanker with that of the reference comparisons
used above. Specically, the document-based ranking baselines: (i) the initial ranking, (ii)
the relevance model used to rank the entire corpus (Rel Model), (iii) the relevance model
used to re-rank the initial list (Rel Model(Re-Rank)); and, the cluster ranking methods: (i)
def

def

def

S coreClustQueryGen (c) = pc (q), (ii)S coreM ax (c) = maxdi Dinit pdi (q), (iii) S coreM in (c) =
qQ
def
mindi Dinit pdi (q), (iv) S coreGeoM ean (c) = |c| di c pdi (q), and (v) S coreHIT S (c). For the

reference comparisons that incorporate free parameters  Rel Model, Rel Model(Re-Rank),
and HITS we employ leave-one-out cross validation so as to set free-parameter values as
we do for ClustRanker. The performance numbers of all methods are presented in Table
10.
Our rst observation based on Table 10 is that ClustRanker outperforms the initial
ranking in most relevant comparisons; most of these improvements are quite substantial.
387

fiKurland & Krikon

AP

TREC8

Effect of  on p@5, corpus=AP

Effect of  on p@5, corpus=TREC8

54

58

52

56

50
p@5

p@5

54
48

52
46
50

44
42
0.050.1

ClustRanker
init. rank.
0.2

0.3

0.4

0.5

0.6

0.7

0.8

48
0.050.1

0.90.95

ClustRanker
init. rank.
0.2

0.3

0.4



0.5

0.6

0.7

0.8

0.90.95



WSJ

WT10G

Effect of  on p@5, corpus=WSJ

Effect of  on p@5, corpus=WT10G

58

42

57
40

56

38

54

p@5

p@5

55

53

36

52
51

34

50
49
0.050.1

ClustRanker
init. rank.
0.2

0.3

0.4

0.5

0.6

0.7

0.8

32
0.050.1

0.90.95



ClustRanker
init. rank.
0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.90.95



Figure 3: Eect of varying , one of the graph parameters (refer to Appendix A), on the
p@5 performance of ClustRanker.

The only exception is for the WSJ corpus, for which the relevance models also do not
outperform the initial ranking in terms of p@5. Thus, it seems that query-variability issues,
which aect the ability to learn eective free-parameter values, have quite an eect on the
performance of methods for WSJ.
We can also see in Table 10 that except for WSJ, ClustRanker outperforms the previously proposed cluster ranking methods in almost all relevant comparisons. Many of these
performance improvements are substantial and statistically signicant.
As can be seen in Table 10, the ClustRanker method also outperforms each of the
relevance models (Rel Model and Rel Model(Re-Rank)) in a majority of the relevant comparisons (corpus  evaluation measure). While there is a single case for which ClustRanker
is outperformed in a statistically signicantly manner by the relevance models (p@10 for
WSJ), for WT10G ClustRanker posts statistically signicant improvements over the relevance models, with some of the performance dierences being quite striking. Furthermore,
we observe that in contrast to ClustRanker, most previously proposed cluster ranking methods often post performance that is much worse  in many cases to a statistically signicant
degree  than that of the relevance models. Somewhat an exception is ranking clusters by
388

fiA Language Model Approach to Ranking Query-Specific Document Clusters

AP

TREC8
p@10
45.6

init. rank.

p@5
45.7

p@10
43.2

p@5
50.0

Rel Model
Rel Model(Re-Rank)

49.9
51.1i

48.4i
45.8

50.8
50.4

50.2
49.8

52.0
52.0

39.2ir

38.8ir

39.6ir

40.6ir

41.8r

40.3r

38.8ir

def

SClustQueryGen (c) = pc (q)
def

SM ax (c) = maxdi Dinit pdi (q)
def

p@5
53.6

WSJ
p@10
48.4

p@5
33.9

WT10G
p@10
28.0

30.4i
36.3

29.2
27.8

44.0ir 37.0ir

30.0

24.1r

41.6r

51.2r 46.6r

33.9

29.2

48.4r 47.8r

31.4

25.9

37.4r

31.8i

52.6
52.6

SM in (c) = mindi Dinit pdi (q)
qQ
def
SGeoM ean (c) = |c| di c pdi (q)

47.0

46.7

46.4

48.4r

44.4r

46.7i

50.0

49.6

56.0r 50.6r

SHIT S (c)

48.5

47.2

50.8

43.2r

53.6

46.8

25.5i

23.9r

ClustRanker

52.3icM g

48.3ic
M

53.2c

46.2c

38.6rcM m

31.2cm

56.8cM mh 49.4cM

Table 10: Performance results when using leave-one-out cross validation to set freeparameter values. Boldface marks the best performance per column. Statistically signicant dierences of a method with the initial ranking are marked with
i. Statistically signicant dierences of ClustRanker with the cluster ranking
methods, ClustQueryGen, Max, Min, GeoMean, and HITS are marked with c,
M, m, g, and h, respectively. Statistically signicant dierences of a cluster
ranking method with Rel Model and Rel Model(Re-Rank) are marked with r
and , respectively.

the geometric mean of their constituent documents query-similarity values (GeoMean): for
WT10G and WSJ the performance is better than that of the relevance models; however,
for TREC8 and AP the performance is somewhat inferior to that of the relevance models.
All in all, the ndings presented above attest that ClustRanker, when learning free
parameter values, is (i) a highly eective method for obtaining high precision at top ranks,
and (ii) much more eective than previously proposed methods for ranking clusters.

6. Conclusions and Future Work
We presented a novel language model approach to ranking query-specific clusters, that is,
clusters created from documents highly ranked by some initial search performed in response
to a query. The ranking of clusters is based on the presumed percentage of relevant documents that they contain.
Our cluster ranking model integrates information induced from the cluster as a whole
unit with that induced from documents that are associated with the cluster. Two types
of information are exploited by our approach: similarity to the query and centrality. The
latter reects similarity to other central items in the reference set, may they be documents
in the initial list, or clusters of these documents.
Empirical evaluation showed that using our approach results in precision-at-top-ranks
performance that is substantially better than that of the initial ranking upon which clustering is employed. Furthermore, the performance often transcends that of a state-of-the-art
pseudo-feedback-based query expansion method, namely, the relevance model. In addition,
we showed that our approach is substantially more eective in identifying clusters contain389

fiKurland & Krikon

ing a high percentage of relevant documents than previously proposed methods for ranking
clusters.
For future work we intend to explore additional characteristics of document clusters
that might attest to the percentage of relevant documents they contain; for example, cluster
density as measured by inter-document-similarities within the cluster. Incorporating such
characteristics in our framework in a principled way is an interesting challenge.
Acknowledgments
We thank the reviewers for their helpful comments. We also thank Lillian Lee for helpful
comments on the work presented in this paper, and for discussions that led to ideas presented
here; specically, the cluster-centrality induction method is a fruit of joint work with Lillian
Lee. This paper is based upon work supported in part by the Israel Science Foundation
under grant no. 557/09, by the National Science Foundation under grant no. IIS-0329064,
by Googles faculty research award, and by IBMs SUR award. Any opinions, ndings and
conclusions or recommendations expressed in this material are the authors and do not
necessarily reect those of the sponsoring institutions.

Appendix A. Centrality Induction
We briey describe a previously proposed graph-based approach for inducing document
centrality (Kurland & Lee, 2005), which we use for inducing document and cluster centrality.
Let S (either Dinit , the initial list of documents, or C l(Dinit ), the set of their clusters)
be a set of items, and G = (S, S  S) be the complete directed graph dened over S. The
weight w t(s1  s2 ) of the edge s1  s2 (s1 , s2  S) is dened as
(
ps2 (s1 ) if s2  N bhd(s1 ; ),
def
w t(s1  s2 ) =
0
otherwise,
where N bhd(s1 ; ) is the set of  items s  S  {s1 } that yield the highest ps (s1 ). (Ties
are broken by item ID.)
We use the PageRank approach (Brin & Page, 1998) to smooth the edge-weight function:
w t[] (s1  s2 ) = (1  ) 

w t(s1  s2 )
1
;
+ P

|S|
s S w t(s1  s )

 is a free parameter.
Thus, G with the edge-weight function w t[] constitutes an ergodic Markov chain, for
which a stationary distribution exists. We set Cent(s), the centrality value of s, to the
stationary probability of visiting s.
Following previous work (Kurland & Lee, 2005), the values of  and  are chosen from
{2, 4, 9, 19, 29, 39, 49} and {0.05, 0.1, . . . , 0.9, 0.95}, respectively, so as to optimize the p@k
performance of a given algorithm for clusters of size k. We use the same parameter setting
for the document-graph (S = Dinit ) and for the cluster-graph (S = C l(Dinit )), and therefore inducing document and cluster centrality in any of our methods is based on two free
parameters.
390

fiA Language Model Approach to Ranking Query-Specific Document Clusters

Appendix B. Relevance Model
To estimate the standard relevance model, RM1, which was shown to yield better performance than that of the RM2 relevance model (Lavrenko & Croft, 2003), we employ the
implementation detailed by Lavrenko and Croft (2003). Let w denote a term in the voJM []
cabulary, {qi } be the set of query terms, and pd
() denote a Jelinek-Mercer smoothed
document language model with smoothing parameter  (Zhai & Laerty, 2001). RM1 is
then dened by
def

pRM 1 (w; ) =

X

dDinit

JM []
(w) P
pd

JM []
(qi )
i pd
.
Q JM []
(qi )
dj Dinit
i pdj

Q

In practice, RM1 is clipped by setting pRM 1 (w; ) to 0 for all but the  terms with
the highest pRM 1 (w; ) to begin with (Connell et al., 2004; Diaz & Metzler, 2006); further normalization is performed to yield a probability distribution, which we denote by
pRM 1 (; , ). To improve performance, RM1 is anchored to the original query via interpolation using a free parameter  (Abdul-Jaleel et al., 2004; Diaz & Metzler, 2006). This
results in the RM3 model:
def

pRM 3 (w; , , ) = pqM LE (w) + (1  )pRM 1 (w; , );
in
pqM LE (w) is the maximum likelihood estimate of term w with respect to q. Documents
fifi


fifi Dir[]
() .
the corpus are then ranked by the minus cross entropy CE pRM 3 (; , , ) fifi pd
The free-parameter values are chosen from the following ranges to independently optimize p@5 and p@10 performance:   {0, 0.1, 0.3, . . . , 0.9},   {25, 50, 75, 100, 500,
1000, 5000, ALL}, where ALL stands for using all terms in the corpus (i.e., no clipping),
and   {0, 0.1, 0.2, . . . , 0.9};  is set to 2000, as in our cluster-based algorithms, following
previous recommendations (Zhai & Laerty, 2001).

References
Abdul-Jaleel, N., Allan, J., Croft, W. B., Diaz, F., Larkey, L., Li, X., Smucker, M. D., &
Wade, C. (2004). UMASS at TREC 2004  novelty and hard. In Proceedings of the
Thirteenth Text Retrieval Conference (TREC-13), pp. 715725.
Azzopardi, L., Girolami, M., & van Rijsbergen, K. (2004). Topic based language models for
ad hoc information retrieval. In Proceedings of International Conference on Neural
Networks and IEEE International Conference on Fuzzy Systems, pp. 32813286.
Balinski, J., & Danilowicz, C. (2005). Re-ranking method based on inter-document distances. Information Processing and Management, 41 (4), 759775.
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine.
In Proceedings of the 7th International World Wide Web Conference, pp. 107117.
Buckley, C. (2004). Why current IR engines fail. In Proceedings of SIGIR, pp. 584585.
Poster.
391

fiKurland & Krikon

Buckley, C., Salton, G., Allan, J., & Singhal, A. (1994). Automatic query expansion using
SMART: TREC3. In Proceedings of the Third Text Retrieval Conference (TREC-3),
pp. 6980.
Collins-Thompson, K., Callan, J., Terra, E., & Clarke, C. L. (2004). The eect of document
retrieval quality on factoid question answering performance. In Proceedings of SIGIR,
pp. 574575. Poster.
Connell, M., Feng, A., Kumaran, G., Raghavan, H., Shah, C., & Allan, J. (2004). UMass
at TDT 2004. TDT2004 System Description.
Crestani, F., & Wu, S. (2006). Testing the cluster hypothesis in distributed information
retrieval. Information Processing and Management, 42 (5), 11371150.
Croft, W. B. (1980). A model of cluster searching based on classication. Information
Systems, 5, 189195.
Croft, W. B., & Laerty, J. (Eds.). (2003). Language Modeling for Information Retrieval.
No. 13 in Information Retrieval Book Series. Kluwer.
Diaz, F. (2005). Regularizing ad hoc retrieval scores. In Proceedings of the Fourteenth
International Conference on Information and Knowledge Management (CIKM), pp.
672679.
Diaz, F., & Metzler, D. (2006). Improving the estimation of relevance models using large
external corpora. In Proceedings of SIGIR, pp. 154161.
Erkan, G. (2006a). Language model based document clustering using random walks. In
Proceedings of HLT/NAACL, pp. 479486.
Erkan, G. (2006b). Using biased random walks for focused summarization. In Proceedings
of Document Understanding Conference (DUC).
Erkan, G., & Radev, D. R. (2004). LexPageRank: Prestige in multi-document text summarization. In Proceedings of EMNLP, pp. 365371. Poster.
Geraci, F., Pellegrini, M., Maggini, M., & Sebastiani, F. (2006). Cluster generation and
cluster labeling for Web snippets: A fast and accurate hierarchical solution. In Proceedings of the 13th international conference on string processing and information
retrieval (SPIRE), pp. 2537.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). The Johns
Hopkins University Press.
Griths, A., Luckhurst, H. C., & Willett, P. (1986). Using interdocument similarity information in document retrieval systems. Journal of the American Society for Information
Science (JASIS), 37 (1), 311. Reprinted in Karen Sparck Jones and Peter Willett,
eds., Readings in Information Retrieval, Morgan Kaufmann, pp. 365373, 1997.
Harman, D., & Buckley, C. (2004). The NRRC reliable information access (RIA) workshop.
In Proceedings of SIGIR, pp. 528529. Poster.
Hearst, M. A., & Pedersen, J. O. (1996). Reexamining the cluster hypothesis: Scatter/Gather on retrieval results. In Proceedings of SIGIR, pp. 7684.
392

fiA Language Model Approach to Ranking Query-Specific Document Clusters

Jardine, N., & van Rijsbergen, C. J. (1971). The use of hierarchic clustering in information
retrieval. Information Storage and Retrieval, 7 (5), 217240.
Kleinberg, J. (1997). Authoritative sources in a hyperlinked environment. Tech. rep. Research Report RJ 10076, IBM.
Kurland, O. (2006). Inter-document similarities, language models, and ad hoc retrieval.
Ph.D. thesis, Cornell University.
Kurland, O. (2009). Re-ranking search results using language models of query-specic
clusters. Journal of Information Retrieval, 12 (4), 437460.
Kurland, O., & Domshlak, C. (2008). A rank-aggregation approach to searching for optimal
query-specic clusters. In Proceedings of SIGIR, pp. 547554.
Kurland, O., & Lee, L. (2004). Corpus structure, language models, and ad hoc information
retrieval. In Proceedings of SIGIR, pp. 194201.
Kurland, O., & Lee, L. (2005). PageRank without hyperlinks: Structural re-ranking using
links induced by language models. In Proceedings of SIGIR, pp. 306313.
Kurland, O., & Lee, L. (2006). Respect my authority! HITS without hyperlinks utilizing
cluster-based language models. In Proceedings of SIGIR, pp. 8390.
Laerty, J. D., & Zhai, C. (2001). Document language models, query models, and risk
minimization for information retrieval. In Proceedings of SIGIR, pp. 111119.
Lavrenko, V. (2004). A Generative Theory of Relevance. Ph.D. thesis, University of Massachusetts Amherst.
Lavrenko, V., Allan, J., DeGuzman, E., LaFlamme, D., Pollard, V., & Thomas, S. (2002).
Relevance models for topic detection and tracking. In Proceedings of the Human
Language Technology Conference (HLT), pp. 104110.
Lavrenko, V., & Croft, W. B. (2001). Relevance-based language models. In Proceedings of
SIGIR, pp. 120127.
Lavrenko, V., & Croft, W. B. (2003). Relevance models in information retrieval. In Croft,
& Laerty (Croft & Laerty, 2003), pp. 1156.
Leuski, A. (2001). Evaluating document clustering for interactive information retrieval.
In Proceedings of the Tenth International Conference on Information and Knowledge
Management (CIKM), pp. 3340.
Leuski, A., & Allan, J. (1998). Evaluating a visual navigation system for a digital library. In
Proceedings of the Second European conference on research and advanced technology
for digital libraries (ECDL), pp. 535554.
Liu, X., & Croft, W. B. (2004). Cluster-based retrieval using language models. In Proceedings
of SIGIR, pp. 186193.
Liu, X., & Croft, W. B. (2006a). Experiments on retrieval of optimal clusters. Tech. rep. IR478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts.
Liu, X., & Croft, W. B. (2006b). Representing clusters for retrieval. In Proceedings of
SIGIR, pp. 671672. Poster.
393

fiKurland & Krikon

Liu, X., & Croft, W. B. (2008). Evaluating text representations for retrieval of the best
group of documents. In Proceedings of ECIR, pp. 454462.
Mei, Q., Shen, X., & Zhai, C. (2007). Automatic labeling of multinomial topic models. In
Proceedings of the 13th ACM SIGKDD international conference, pp. 490499.
Mihalcea, R. (2004). Graph-based ranking algorithms for sentence extraction, applied to
text summarization. In The Companion Volume to the Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguistics, pp. 170173.
Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing order into texts. In Proceedings of
EMNLP, pp. 404411. Poster.
Otterbacher, J., Erkan, G., & Radev, D. R. (2005). Using random walks for question-focused
sentence retrieval. In Proceedings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),
pp. 915922.
Palmer, C. R., Pesenty, J., Veldes-Perez, R., Christel, M., Hauptmann, A. G., Ng, D., &
Wactlar, H. D. (2001). Demonstration of hierarchical document clustering of digital
library retrieval results. In Proceedings of the 1st ACM/IEEE-CS joint conference on
digital libraries, p. 451.
Ponte, J. M., & Croft, W. B. (1998). A language modeling approach to information retrieval.
In Proceedings of SIGIR, pp. 275281.
Preece, S. E. (1973). Clustering as an output option. In Proceedings of the American Society
for Information Science, pp. 189190.
Seo, J., & Croft, W. B. (2010). Geometric representations for multiple documents. In
Proceedings of SIGIR, pp. 251258.
Shanahan, J. G., Bennett, J., Evans, D. A., Hull, D. A., & Montgomery, J. (2003). Clairvoyance Corporation experiments in the TREC 2003. High accuracy retrieval from
documents (HARD) track. In Proceedings of the Twelfth Text Retrieval Conference
(TREC-12), pp. 152160.
Si, L., Jin, R., Callan, J., & Ogilvie, P. (2002). A language modeling framework for resource
selection and results merging. In Proceedings of the 11th International Conference on
Information and Knowledge Management (CIKM), pp. 391397.
Tao, T., Wang, X., Mei, Q., & Zhai, C. (2006). Language model information retrieval with
document expansion. In Proceedings of HLT/NAACL, pp. 407414.
Tao, T., & Zhai, C. (2006). Regularized esitmation of mixture models for robust pseudorelevance feedback. In Proceedings of SIGIR, pp. 162169.
Tombros, A., Villa, R., & van Rijsbergen, C. (2002). The eectiveness of query-specic hierarchic clustering in information retrieval. Information Processing and Management,
38 (4), 559582.
Treeratpituk, P., & Callan, J. (2006). Automatically labeling hierarchical clusters. In
Proceedings of the sixth national conference on digital government research, pp. 167
176.
394

fiA Language Model Approach to Ranking Query-Specific Document Clusters

van Rijsbergen, C. J. (1979). Information Retrieval (second edition). Butterworths.
Voorhees, E. M. (1985). The cluster hypothesis revisited. In Proceedings of SIGIR, pp.
188196.
Voorhees, E. M. (2002). Overview of the TREC 2002 question answering track. In The
Eleventh Text Retrieval Conference TREC-11, pp. 115123.
Wei, X., & Croft, W. B. (2006). LDA-based document models for ad-hoc retrieval. In
Proceedings of SIGIR, pp. 178185.
Willett, P. (1985). Query specic automatic document classication. International Forum
on Information and Documentation, 10 (2), 2832.
Xu, J., & Croft, W. B. (1996). Query expansion using local and global document analysis.
In Proceedings of SIGIR, pp. 411.
Yang, L., Ji, D., Zhou, G., Nie, Y., & Xiao, G. (2006). Document re-ranking using cluster
validation and label propagation. In Proceedings of CIKM, pp. 690697.
Zamir, O., & Etzioni, O. (1998). Web document clustering: a feasibility demonstration. In
Proceedings of SIGIR, pp. 4654.
Zhai, C., & Laerty, J. D. (2001). A study of smoothing methods for language models
applied to ad hoc information retrieval. In Proceedings of SIGIR, pp. 334342.

395

fiJournal of Artificial Intelligence Research 41 (2011) 477526

Submitted 11/10; published 08/11

A Probabilistic Framework for Learning Kinematic Models
of Articulated Objects
Jurgen Sturm
Cyrill Stachniss
Wolfram Burgard

sturm@informatik.uni-freiburg.de
stachnis@informatik.uni-freiburg.de
burgard@informatik.uni-freiburg.de

Department of Computer Science,
University of Freiburg,
Georges-Koehler-Allee 79, 79100 Freiburg, Germany

Abstract
Robots operating in domestic environments generally need to interact with articulated
objects, such as doors, cabinets, dishwashers or fridges. In this work, we present a novel,
probabilistic framework for modeling articulated objects as kinematic graphs. Vertices in
this graph correspond to object parts, while edges between them model their kinematic
relationship. In particular, we present a set of parametric and non-parametric edge models
and how they can robustly be estimated from noisy pose observations. We furthermore
describe how to estimate the kinematic structure and how to use the learned kinematic
models for pose prediction and for robotic manipulation tasks. We finally present how
the learned models can be generalized to new and previously unseen objects. In various
experiments using real robots with different camera systems as well as in simulation, we
show that our approach is valid, accurate and efficient. Further, we demonstrate that our
approach has a broad set of applications, in particular for the emerging fields of mobile
manipulation and service robotics.

1. Introduction
Service robots operating in domestic environments are typically faced with a variety of
objects they have to deal with or they have to manipulate to fulfill their task. A further
complicating factor is that many of the relevant objects are articulated, such as doors,
windows, but also pieces of furniture like cupboards, cabinets, or larger objects such as
garage doors, gates and cars. Understanding the spatial movements of the individual parts
of articulated objects is essential for service robots to allow them to plan relevant actions
such as door-opening trajectories and to assess whether they actually were successful. In
this work, we investigate the problem of learning kinematic models of articulated objects
and using them for robotic manipulation tasks. As an illustrating example, consider Fig. 1
where a mobile manipulation robot interacts with various articulated objects in a kitchen
environment, learns their kinematic properties and infers their kinematic structure.
Our problem can be formulated as follows: Given a sequence of pose observations of
object parts, our goal is to learn a compact kinematic model describing the whole articulated object. This kinematic model has to define (i) the connectivity between the parts,
(ii) the number of degrees of freedom of the object, and (iii) the kinematic function for the
articulated object. As a result, we obtain a generative model which can be used by the
robot for generating and reasoning about future or unseen configurations.

c
2011
AI Access Foundation. All rights reserved.

fiSturm, Stachniss, & Burgard

revolute

prismatic

Figure 1: A service robot learns kinematic models of articulated objects in a kitchen environment.

The contribution of this paper is a novel approach that enables a real robot to learn
kinematic models of articulated objects from its sensor data. These models describe the
kinematics of the object and include the part connectivity, degrees of freedom of the objects,
and kinematic constraints. We utilize these models subsequently to control the motion of the
manipulator. Furthermore, we show how a robot can improve model learning by exploiting
past experience. Finally, we show how our framework can be generalized to deal with
closed-chain objects, i.e., objects that contain kinematic loops.
In the past, several researchers have addressed the problem to handle doors and drawers
(Jain & Kemp, 2009a; Klingbeil, Saxena, & Ng, 2009; Meeussen et al., 2010; Wieland,
Gonzalez-Aguirre, Vahrenkamp, Asfour, & Dillmann, 2009; McClung, Zheng, & Morrell,
2010). Most of these approaches, however, are either entirely model-free or assume substantial knowledge about the model and its parameters. Whereas model-free approaches
release designers from providing any a-priori model information, the knowledge about objects and their articulation properties supports state estimation, motion prediction, and
planning.

478

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

pose
observations

model
fitting

candidate
link models

structure
selection

kinematic
graph

Figure 2: Schematic overview of our approach. The robot observes an articulated object in
different poses. It uses these observations to generate a set of candidate models,
and selects the kinematic structure that maximizes the posterior probability.

In our previous work, we introduced a simpler version of our probabilistic framework for
modeling articulated objects, presented estimators for fitting link models, and showed how
to efficiently find the kinematic structure for kinematic trees (Sturm, Pradeep, Stachniss,
Plagemann, Konolige, & Burgard, 2009). As observations, we used a motion capture studio
and data from simulation. Further, we used a stereo camera system for learning models
for kitchen furniture (Sturm, Konolige, Stachniss, & Burgard, 2010). We described how
a manipulation robot can learn kinematic models from direct interaction with articulated
objects, and can improve over time by learning from experience (Sturm, Jain, Stachniss,
Kemp, & Burgard, 2010). In this work, we present a unified framework for learning kinematic models of articulated objects with an extended set of experiments. In contrast to
our previous work, we generalize our approach from kinematic trees to general kinematic
graph objects and add a strategy to efficiently find a locally optimal graph. With this, it
becomes possible to model articulated objects that contain kinematic loops. Furthermore,
finding the effective number of degrees of freedom (DOFs) of an articulated object directly
follows from our approach. A general software framework that implements the presented
approach is available online1 under the BSD license, including source code, documentation,
and tutorials.
This paper is organized as follows. In Section 2, we introduce our unified framework for
modeling the kinematics of articulated objects. In Section 3, we present several extensions
including the exploitation of prior information, kinematic loops, and the estimation of
degrees of freedom. In Section 4, we describe different options to perceive and control the
motion of articulated objects. We analyze our approach in an extensive set of experiments
both in simulation and on real robots and report our results in Section 5. Finally, we
conclude this article with a discussion of related work in Section 6.

2. A Probabilistic Framework for Articulated Objects
We define an articulated object to consist of multiple object parts that have one or more
passively actuated mechanical links between them. These links constrain the motion between the parts. For example, the hinge of a door constrains the door to move on an arc,
and the shaft of a drawer constrains the drawer to move on a line segment. The simplest
articulated object consists of two rigid parts with one mechanical link. More complex ob1. http://www.ros.org/wiki/articulation

479

fiSturm, Stachniss, & Burgard

jects may consist of several articulated parts, like a door with a door handle, or a car with
several doors, windows, and wheels.
Fig. 2 gives a high-level overview of our proposed system. A robot observes the pose
of an articulated object being manipulated. For the relative motion of any two parts, it
fits different candidate models that describe different mechanical links. From this set of
candidate link models, it selects the kinematic structure that best explains the observed
motion, i.e., the kinematic structure that maximizes the posterior probability.
2.1 Notation
We assume that a robot, external to the object, observes the pose of an articulated object
consisting of p object parts. We denote the true pose of object part i  {1, . . . , p} by a
vector xi  SE (3) representing the 3D pose of that part (including position and orientation),
where SE (3) = R3  SO(3) stands for the special Euclidean group. Further, we refer to
the full object pose (containing the poses of all parts) with the vector x1:p = (x1 , . . . , xp )T .
Two object parts i and j are related by their relative transformation ij = xi 	 xj . We
use  and 	 for referring to the motion composition operator and its inverse2 .
We denote a kinematic link model between two object parts i and j as Mij and its
associated parameter vector as ij  Rkij , where kij  N0 denotes the number of parameters
of the model describing the link. A kinematic graph G = (VG , EG ) consists of a set of
vertices VG = {1, . . . , p}, corresponding to the parts of the articulated object, and a set of
undirected edges EG  VG  VG describing the kinematic link between two object parts.
With each edge (ij), a corresponding kinematic link model Mij with parameter vector ij
is associated.
All kinematic link models that we consider here (except for the trivial rigid link) have a
latent variable qij  Cij  Rd ij that describes the configuration of the link. For a door, this
can be the opening angle. Cij stands for the configuration space of the link. The variable
dij represents the number of DOFs of the mechanical link between the two parts.
While the object is being articulated, the robot observes the object pose; we denote the
n
n-th pose observation of object part i as yi . Correspondingly, we denote the n-th pose
n
1 , . . . , yn ).
observation of all parts as y1:p and a sequence of n pose observations as Dy = (y1:p
1:p
n
Further, we will refer to Dzij = (z1ij , . . . , zij ) as the sequence of relative transformations
zij = yi 	 yj that the robot has observed so far for the edge (ij).
Fig. 3a depicts a graphical model of a simple articulated object that consists of two
object parts. We use here the so-called plate notation to simplify the notation of the
graphical model. Here, the nodes inside the rectangle (the plate) are copied for n times,
i.e., for each time step t in which the object is observed. In each of these time steps, the
articulated object takes a particular configuration q12 defining  together with the model
and its parameters  the noise-free relative transformation 12 between the noise-free pose
of the object parts x1 and x2 . From that, the robot observes the noisy poses y1 and y2 ,
and infers from them a virtual measurement z12 = y1 	 y2 . During model learning, the
robot infers from these observations the link model M12 and link parameters 12 .
2. E.g., if the poses x1 , x2  R44 are represented as homogeneous matrices, then these operators correspond
to matrix multiplication x1  x2 = x1 x2 and inverse multiplication x1 	 x2 = (x1 )1 x2 , respectively.

480

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

M12 , 12

qt12
xt1

q
xt2

x1

x2

x1

x2
M12 , 12

t12
y1t

y2t

M12 , 12

zt12
t  1, . . . , T

(a) Full graphical model

(b) Reduced graphical model

(c) Kinematic graph

Figure 3: (a) Graphical model of an articulated object consisting of two parts x1 and x2 ,
being observed over t time steps. The model M12 , 12 is shared between all time
steps. (b) shows a simplified version of the same graphical model. (c) shows the
corresponding kinematic graph.

A reduced version of this graphical model is depicted in Fig. 3b. To improve readability,
we leave out some nodes, i.e., the node corresponding to the relative transformation 12
and the observation nodes y1 , y2 , and z12 . Instead, we visualize the dependency between x1
and x2 by a direct link and label it with the corresponding model. Further, we collapse the
configuration of the link into a single node corresponding to the configuration of the whole
object. Finally, we refer to the kinematic graph as the graph that models the connectivity
between object parts, as depicted in Fig. 3c).
2.2 Problem Definition
The problem that we consider here is to find the most likely kinematic graph G given a
sequence of pose observations Dy of an articulated object. In Bayesian terms, this means
that we aim at finding the kinematic graph G that maximizes the posterior probability of
observing the poses Dy of the articulated object, i.e.,
G = arg max p(G | Dy ).

(1)

G

However, finding the global maximum of the posterior p(G | Dy ) is difficult, because it is a
highly non-convex function over a high-dimensional parameter space consisting of discrete
as well as continuous dimensions that encode the kinematic structure and the kinematic
properties, respectively.
Therefore, in this section, we consider a simplified problem. We restrict the structure
space to kinematic trees only, and will focus on the general problem in Section 3. Kinematic
481

fiSturm, Stachniss, & Burgard

trees have the property that their individual edges are independent of each other. As a
result, we can estimate the link parameters independently of each other and also independent
of the kinematic structure. This means that for learning the local kinematic relationship
n
between object parts i and j, only their relative transformations Dzij = (z1ij , . . . , zij ) are
relevant for estimating the edge model. With this, we can rephrase the maximization
problem of (1) for kinematic trees now as
G = arg max p(G | Dz )

(2)

G

= arg max p({(Mij , ij ) | (ij)  EG } | Dz ).
G
Y
= arg max
p(Mij , ij | Dzij ).
G

(3)
(4)

(ij)EG

The latter transformation follows from the mutual independence of the edges of kinematic
trees.
An important insight in our work is that the kinematic link models representing the
edges can be estimated independently from the actual structure of the kinematic tree. As
a result, the problem can be solved efficiently: first, we estimate the link models of all
possibles edges (ij)  VG  VG :
(Mij , ij ) = arg max p(Mij , ij | Dzij ).

(5)

Mij ,ij

These link models are independent of each other and independent of whether they are
actually part of the kinematic structure EG . Second, given these link models, we estimate
the kinematic structure. This two-step process is also visualized in Fig. 2.
Solving (5) is still a two-step process (MacKay, 2003): at the first level of inference,
we assume that a particular model (e.g., like the revolute model) is true, and estimate its
parameters. By applying Bayes rule, we may write
ij = arg max p(ij | Dzij , Mij )

(6)

ij

= arg max

p(Dzij | ij , Mij ) p(ij | Mij )
p(Dzij | Mij )

ij

.

(7)

The term p(ij | Mij ) defines the model-dependent prior over the parameter space, that we
assume in our work to be uniform, and thus may be dropped. Further, we can ignore the
normalizing constant p(Dzij | Mij ), as it is has no influence on the choice of the parameter
vector. This results in
ij = arg max p(Dzij | ij , Mij ),

(8)

ij

which means that fitting of a link model to the observations corresponds to the problem of
maximizing the data likelihood.

482

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

At the second level of inference, we need to compare the probability of different models
given the data and select the model with the highest posterior:
Z
Mij = arg max p(Mij , ij | Dzij ) dij .
(9)
Mij

Computing the exact posterior probability of a model is in general difficult, and therefore
we use in our work the Bayesian information criterion (BIC) for selecting the best model
according to (9).
As a result of this inference, we obtain for each edge (ij)  VG  VG a model Mij with
parameter vector ij , that best describes the motions in Dzij observed between these two
parts. We denote this set of all possible link models with
M = {(Mij , ij ) | (ij)  VG  VG }.

(10)

Given these maximum-likelihood estimate for all links, we can now efficiently estimate
the kinematic structure EG  VG  VG . For this, we aim at finding the subset, that
maximizes the posterior probability of the resulting kinematic graph, i.e.,
Z
EG = arg max p(EG , M | Dz ) dM.
(11)
EG

We solve the equation again by maximizing the BIC over all possible structures EG , using
the maximum-likelihood estimate for M for approximating the integral.
With this, we provide an efficient way to solve (2), by first fitting all models to the data,
then selecting the best model for each link, and finally estimating the kinematic structure of
the whole articulated object. From Section 2.3 to Section 2.7, we will show how to solve the
model fitting problem of (8) and the model selection problem of (9) efficiently and robustly
from noisy observations. In Section 2.8, we will then show how one can efficiently solve (11),
given the link models. In Section 3.2, we will show how this solution for kinematic trees
can be generalized to general kinematic graphs, including kinematic structures containing
loops.
2.3 Observation Model
In the beginning, we consider simple objects consisting of only p = 2 rigid parts, and drop
the ij indices to increase readability. We consider the case that the robot has observed
a sequence of n relative transformations Dz = (z1 , . . . , zn ) between two adjacent rigid
parts of an articulated object. We assume the presence of Gaussian noise in each of the
measurements zn with zero mean and covariance z  R66 .
Further, we assume that a small fraction of the observations are real outliers that cannot
be explained by the Gaussian noise assumption alone. These outliers may be the result of
poor perception, bad data association, or other sensor failures that are hard to be modeled
explicitly. As these outliers are not related to the true value of  = x1 	x2 at all, we assume
that they come from a uniform prior, i.e., we assume a constant likelihood p(zoutlier ) = const.
One can think of this as a latent variable v  {0, 1} indicating whether an observation is an

483

fiSturm, Stachniss, & Burgard

inlier (v = 1) or an outlier (v = 0). Further, we denote with  the probability of drawing
an outlier, i.e., p(v = 0) = . Our full observation model then becomes

 + N (0, z ) if v = 1
z
.
(12)
U
if v = 0
The resulting data likelihood for a single observation z thus is a mixture of a Gaussian and
a uniform distribution with mixing constant :
p(z | , ) = (1  )p(z | v = 1) + p(z | v = 0).

(13)

Note that in general neither the true transformation  nor the outlier ratio  are directly
observable, and thus need to be estimated from the data. For comparing models with
different outlier ratios, we assume a global prior of p()  exp(w) with w being a
weighting constant, and thereby favor models with fewer outliers over models with more
outliers. The resulting data likelihood of an observation z given its true value  thus
becomes:
p(z | ) = p(z | , )p().

(14)

2.4 Candidate Models
When considering the set of objects relevant for a service robot, one quickly realizes that
the joints in many objects belong to a few generic model classes. In particular, revolute
and prismatic joints are used most often, although a few objects are composed of other
mechanical linkages, for example spherical joints, screws, or two-bar links. Examples of
revolute joints include doors, door handles, and windows. This also includes the doors
of dishwashers, microwave ovens or washing machines. Examples of articulated objects
belonging to the prismatic class include drawers, sliding doors, and window blinds. However,
there are also objects that have different mechanical linkages, such as garage doors or twobar office lamps. This motivates the use of a set of candidate models, that are well suited for
describing the kinematic properties of a particular class of articulated links. Our candidate
set consists of parametrized and non-parametrized models, in particular, it includes a model
for revolute joints (Mrevolute ), for prismatic joints (Mprismatic ), and rigid transformations
(Mrigid ). Additionally, there may be articulations that do not correspond to these standard
motions, for which we consider a parameter-free model (MGP ). We model such joints using
a combination of dimensionality reduction and Gaussian process regression.
In our framework, a model class defines the conditional probability distribution p( |
q, M, ) and p(q | , M, ) by means of a forward kinematic function fM, (q) = 
1
and the inverse kinematic function fM,
(z) = q. This means that we assume that our link
models are deterministic, and we attribute all noise to measurement noise in the observations
of the object parts, i.e., by means of the observation model p( | z) defined in Section 2.3.
Since we have no prior information about the nature of the connection between the two
rigid parts, we do not aim to fit only a single model, but instead aim at fitting all of the
candidate models to the observed data, and then we select the best model from this set.

484

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

candidate model
M
rigid model
prismatic model
revolute model
Gaussian process model

DOFs
d
0
1
1
1, . . . , 5

parameters
k
6
9
12
1 + d + 6n

Table 1: Overview over the different candidate models for articulated links.
2.5 Model Fitting using Maximum Likelihood Consensus
For estimating the parameters of any of the above-mentioned models, we need to find a
parameter vector   Rk that maximizes the data likelihood given the model, i.e.,
 = arg max p(Dz | M, ).

(15)



In the presence of noise and outliers, finding the right parameter vector  that minimizes (15)
is not trivial, as least squares estimation is sensitive to outliers and thus not sufficient given
our observation model. Therefore, we use the MLESAC (maximum likelihood consensus)
algorithm as introduced by Torr and Zisserman (2000). We estimate the initial kinematic
parameters from a minimal set of randomly drawn samples from the observation sequence
that we then refine using non-linear optimization of the data likelihood.
The MLESAC procedure for a model M works as follows: First, we generate a guess
for the parameter vector  in (15) from a minimal set of samples from Dz . For this guess,
we then compute the data likelihood of the whole observation sequence Dz as the product
over all data
p(Dz | M, ) =

n
Y

p(zt | M, ).

(16)

t=1

We repeat this sampling step for a fixed number of iterations, and finally select the parameter vector maximizing (16). On this initial guess, we apply non-linear optimization on
the data likelihood to refine the parameter vector using Broyden-Fletcher-Goldfarb-Shanno
(BFGS) optimization, which is a quasi-Newton method for function maximization. During
the maximization of the data likelihood, MLESAC iteratively also estimates the outlier
ratio , using the Expectation Maximization algorithm.
In the following, we show for each of our link models how to (i) estimate the parameter
vector  from a minimal sample set of observations, (ii) estimate a transformation z given
a configuration q, and (iii) estimate the configuration q given a transformation z. A brief
overview over all model candidates is given in Table 1.
2.5.1 Rigid Model
We parametrize a rigid link by a fixed relative transformation between two object parts.
Thus, the parameter vector  has k = 6 dimensions. During the sampling consensus step,
we draw a single observation z from the training data Dz that gives us an initial guess for
485

fiSturm, Stachniss, & Burgard

the parameter vector . This parameter vector thus corresponds to the estimated fixed
relative transformation between the two parts. For the rigid transformation model, the
forward kinematics function equals the parameter vector as it corresponds to the estimated
fixed relative transform between the two parts:
fMrigid , (q) = .

(17)

As the rigid model has zero DOFs (d = 0), an inverse kinematic function is not needed.
2.5.2 Prismatic Model
Prismatic joints move along a single axis, and thus have a one-dimensional configuration
space. The prismatic model describes a translation along a vector of unit length e  R3
relative to some fixed origin a  SE (3). This results in a parameter vector  = (a; e) with
k = 9 dimensions.
For estimating these parameters, we sample two observations from the training data.
For this, we pick the transformation of the first sample as the origin a and the normalized
vector between them as the prismatic axis e.
A configuration q  R then encodes the distance from the origin a along the direction
of motion e. The forward kinematics function for the prismatic model Mprismatic is
fMprismatic , (q) = a  qe.

(18)

Let trans() be the function that removes all rotational components. The inverse kinematic
function then becomes
1
fM
prismatic , (z) =< e, trans(a 	 z) >,

(19)

where < ,  > refers to the dot product.
2.5.3 Revolute Model
The revolute model describes the motion of a revolute joint, i.e., a one-dimensional motion
along a circular arc. We parametrize this model by the center of rotation c  SE (3), and a
rigid transformation r  SE (3), from the center to the moving part. This yields a parameter
vector  = (c; r) with k = 12 dimensions.
For the revolute model, we sample three observations zi , zj and zk from the training
data. First, we estimate the plane spanned by these three points; its plane normal then
is parallel to the rotation axis. Second, we compute the circle center as the intersection
of the perpendicular lines of the line segments between the three observations. Together
with the rotation axis, this gives us the center of rotation c. Finally, we estimate the rigid
transformation r of the circle from the first sample.
For the forward kinematic function, we obtain for revolute links
fMrevolute , (q) = c  RotZ (q)  r,

(20)

where RotZ (q) denotes a rotation around the Z-axis by q. Thus, q  R specifies the angle
of rotation. For estimating the configuration of a revolute joint we use
1
1
fM
revolute , (z) = RotZ ((z 	 c)  r),

where Rot1
Z () gives the rotation around the Z-axis.
486

(21)

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

2.5.4 Gaussian Process Model
Although rigid transformations in combination with revolute and prismatic joints might
seem at the first glance to be sufficient for a huge class of kinematic objects, many realworld objects cannot be described by a single shifting or rotation axis. Examples for such
objects include garage doors or office table lamps, but also furniture whose joints have aged
and became loose.
Therefore, we provide additionally a non-parametric model which is able to describe
more general kinematic links. This model is based on dimensionality reduction for discovering the latent manifold of the configuration space and Gaussian process regression for
learning a generative model. Consider the manifold that is described by the observations
Dz between two rigid bodies. Depending on the number of DOFs d of this link, the data
samples will lie on or close to a d-dimensional manifold with 1  d  6 being non-linearly
embedded in SE (3).
There are many different dimensionality reduction techniques such as principal component analysis (PCA) for linear manifolds, or Isomap and locally linear embedding (LLE)
for non-linear manifolds (Tenenbaum, de Silva, & Langford, 2000; Roweis & Saul, 2000). In
our experiments, we used both PCA and LLE for dimensionality reduction. PCA has the
advantage of being more robust against noise for near-linear manifolds, while LLE is more
general and can also model strongly non-linear manifolds.
The general idea here is that we use the dimensionality reduction technique to obtain the
1
d
inverse kinematics function fM
GP : SE (3)  R . As a result, we can assign configurations
to each of the observations, i.e.,
1
fM
GP (z) = q.

(22)

These assignments of observations to configurations can now be used to learn the forward
kinematics function fMGP , () from the observations. Except for linear actuators, we expect
this function to be strongly non-linear.
A flexible approach for solving such non-linear regression problems given noisy observations are Gaussian processes (GPs). One of the main features of the Gaussian process
framework is that the observed data points are explicitly included in the model. Therefore,
no parametric form of fMGP : Rd  SE (3) needs to be specified. Data points can be added
to a GP at any time, which facilitates incremental and online learning. For this model, we
aim to learn a GP that fits the dependency
fMGP (q) +  = z

(23)

for the unknown forward model underlying the articulated link under consideration. We
assume homoscedastic noise, i.e., independent and identically, normally distributed noise
terms   N (0, z ). For simplicity, we train 12 independent Gaussian processes for the
free components of a homogeneous 4  4 transformation matrix. As a consequence of
this over-parametrization, the predicted transformation matrices are not necessarily valid.
In practice, however, they are very close to valid transformation matrices, that can be
found using ortho-normalization via singular value decomposition. In our approach, we use
the standard choice for the covariance function, the squared exponential. It describes the

487

fiSturm, Stachniss, & Burgard

relationship between two configurations qi and qj in configuration space by


1
2
T 1
k(qi , qj ) = f exp  (qi  qj )  (qi  qj ) ,
2

(24)

where f2 is the signal variance, and 1 = diag(l1 , . . . , ld ) is the diagonal matrix of the
length-scale parameters. This results in a (1 + d)-dimensional hyper-parameter vector
 = (f2 , l1 , . . . , ld ). As GPs are data-driven, they require all training data when making predictions. Therefore, we count all data samples as parameters of our model, so that
the number of parameters becomes k = (1 + d) + 6n, where n = |Dz | is the number of
observations. We refer the interested reader to the text book by Rasmussen and Williams
(2006) for more details about GP regression.
Note that this GP link model directly generalizes to higher-dimensional configuration
spaces, i.e., with d > 1: after the dimensionality reduction from observations in SE (3)
to configurations in Rd , we can again learn a Gaussian process regression that learns the
mapping from the configuration space Rd back to transformations in SE (3). Note that this
GP model that we present here is similar to the GPLVM model introduced by Lawrence
(2005). In contrast to GPLVM, we do not optimize the latent configurations for maximizing
the data likelihood. This would invalidate our inverse kinematics function (22), and limits
the GPLVM model to map only from latent space to data space. With our approach, we can
also infer the configuration of new relative transformations not available during training.
2.6 Model Evaluation
To evaluate how well a single observation z is explained by a model, we have to evaluate
p(z | M, ). As the configuration is latent, i.e., not directly observable by the robot, we
have to integrate over all possible values of q, i.e.,
Z
p(z | M, ) = p(z | q, M, )p(q | M, ) dq.
(25)
Under the assumption that all DOFs of the link are independent of each other, and that
no configuration state q is more likely than another (or equivalently, that p(q | M, ) is
uniformly distributed), we may write
p(q | M, )  n d ,

(26)

where n = |Dz | is the number of observations so far, and thus the number of estimated
configurations in the d-dimensional configuration space. With this, (25) can be simplified
to
Z
p(z | M, )  n d p(z | q, M, ) dq.
(27)
If we assume that p(z | q, M, ) is an uni-modal distribution, an approximation of the
integral is to evaluate it only at the estimated configuration q given the observation z using
the inverse kinematics function of the model under consideration, i.e.,
1
q = fM,
(z).

488

(28)

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

 using the forward
For this configuration, we can compute the expected transformation 
kinematics function of the model,
 = fM, (q).


(29)

 we can now efficiently compute
Given the observation z and the expected transformation ,
the data likelihood of (27) using the observation model from (14) as

p(z | M, )  n d p(z | ).

(30)

Note that the approximation of the integral based on the forward and inverse kinematics
model corresponds to a projection of the noisy observations onto the model. Finally, the
marginal data likelihood over the whole observation sequence becomes
Y
p(Dz | M, ) =
p(z | M, ).
(31)
zDz

2.7 Model Selection
After having fitted all model candidates to an observation sequence Dz , we need to select
the model that explains the data best. For Bayesian model selection, this means that we
need to compare the posterior probability of the models given the data
Z
p(Dz | M, )p( | M)p(M)
p(M | Dz ) =
d.
(32)
p(Dz )
While the evaluation of the model posterior is in general difficult, it can be approximated
efficiently based on the Bayesian information criterion (BIC) (Schwarz, 1978). We denote
with k the number of parameters of the current model under consideration, and n the
number of observations in the training data. Then, the BIC is defined as
BIC(M) = 2 log p(Dz | M, ) + k log n,

(33)

where  is the maximum likelihood parameter vector. Model selection now reduces to
selecting the model that has the lowest BIC, i.e.,
M = arg min BIC(M).

(34)

M

We refer the interested reader to the work of Bishop (2007) for further information on the
BIC.
2.8 Finding the Connectivity
So far, we ignored the question of connectivity and described how to evaluate and select a
model M for a single link between two parts of an object only. In this section, we extend
our approach to efficiently find kinematic trees for articulated objects consisting of multiple
parts.
We adopt the connectivity model from Featherstone and Orin (2008) for modeling the
kinematic structure as an undirected graph G = (VG , EG ). The nodes VG in this graph
489

fiSturm, Stachniss, & Burgard

correspond to the poses of the individual object parts, while the edges EG correspond to
the links between these parts. We will now re-introduce the ij-indices, i.e., use Dzij to
refer to the observations of link (ij), and Dz to refer to the observations of the whole
articulated object. Dz thus contains the observations of all edges in the graph G, i.e.,
Dz = {Dzij | (ij)  EG }. In the previous section, we established an algorithm that fits and
selects for any given edge (ij) in this graph a corresponding link model Mij with parameter
vector ij . Given this, we now need to select the kinematic structure EG , i.e., which of these
link models are actually present in the articulated object under consideration.
For the moment, we will consider only kinematic tree mechanisms, i.e., mechanisms
without kinematic loops. Now, we consider a fully connected graph with p vertices, i.e., one
vertex for each object part of the articulated object. The set of possible kinematic trees
for the articulated object is now given by all spanning trees of this graph. The endeavor
of explicitly computing, evaluating, and reasoning with all kinematic trees, however, is not
tractable in practice.
We therefore seek to find the kinematic structure EG that maximizes the posterior as
stated previously in (11),
EG = arg max p(EG | Dz )

(35)

EG

= arg max p({(Mij , ij ) | (ij)  EG } | Dz )

(36)

EG

= arg max
EG

= arg max
EG

Y

p(Mij , ij | Dz )

(37)

log p(Mij , ij | Dz ).

(38)

(ij)EG

X
(ij)EG

Note that because of the independence assumption of the individual links for kinematic
trees, the posterior of the kinematic model for the whole object in (36) can be written as the
product over the posteriors of the individual links in (37). After taking the logarithm in (38),
the structure selection problem takes a form that can be solved efficiently. The key insight
here is that the kinematic tree that maximizes (38) corresponds to the problem of selecting
the minimum spanning tree in a fully connected graph with edge costs corresponding to the
negative log posterior,
costij =  log p(Mij ,  ij |Dzij ),

(39)

that we approximate with the BIC value. The sum over these edge costs then corresponds
to the negative log posterior of the kinematic tree, and a minimum spanning tree thus maximizes the posterior of (38). The best kinematic structure can now be found efficiently, i.e.,
in O(p2 log p) time, using for example Prims or Kruskals algorithm for finding minimum
spanning trees (Cormen, Leiserson, Rivest, & Stein, 2001).

3. Framework Extensions
The approach described so far enables a robot to learn kinematic models of articulated
objects from scratch. In the following, we will consider three extensions. The first extension
490

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

enables a robot to exploit priors learned from previous interactions when learning new
models. Second, we generalize our framework to general kinematic graphs, i.e., consider
additionally objects that contain closed kinematic chains. Third, we show that estimating
the number of DOFs of articulated objects follows directly from our approach.
3.1 Learning and Exploiting Priors
Using the approach described above, a robot always starts learning a model from scratch
when it observes movements of a new articulated object. From a learning perspective,
this may be seen as unsatisfactory since most articulated objects encountered in man-made
environments belong to few different classes with similar parameters. For example, in a
specific office or kitchen, many cabinet doors will open in the same way, i.e., have the
same radius and rotation axis. Furthermore, in some countries the size of such furniture
is standardized. Thus, a robot operating in such environments over extended periods of
time can significantly boost its performance by learning priors over the space of possible
articulated object models.
This section describes our approach for learning priors for articulated objects and a
means for exploiting them as early as possible while manipulating a previously unseen
articulated object. In addition to the previous section, we explicitly want to transfer model
information contained in already learned models to newly seen articulated objects. The key
idea here is to identify a small set of representative models for the articulated objects and
to utilize this as prior information to increase the prediction accuracy when handling new
objects.
To keep the notation simple, consider the case that we have previously encountered
two articulated objects consisting of two parts and thus a single link only. Their observed
motion is given by two observation sequences Dz,1 and Dz,2 . The question now is whether
both trajectories should be described by two distinct models M1 and M2 or by a joint
model M1+2 . In the first case, we can split the posterior as the two models are mutually
independent, i.e.,
p(M1 , M2 | Dz,1 , Dz,2 ) = p(M1 | Dz,1 )p(M2 | Dz,2 ).

(40)

In the latter case, both trajectories are explained by a single, joint model M1+2 with
a parameter vector 1+2 , that is estimated from the joint data Dz,1  Dz,2 . For future
reference, we denote the corresponding posterior probability as
p(M1+2 | Dz,1 , Dz,2 ).

(41)

We can determine whether a joint model is better than two separate models by comparing the posterior probabilities from (40) and (41), i.e, by evaluating
p(M1+2 | Dz,1 , Dz,2 ) > p(M1 | Dz,1 )p(M2 | Dz,2 ).

(42)

This expression can be efficiently evaluated by using the BIC as follows. The joint model
is learned from n = n1 + n2 data points, using k parameters, with a data likelihood of
L = p(M1+2 | Dz,1 , Dz,2 ), while the two separate models are learned from n1 and n2

491

fiSturm, Stachniss, & Burgard

samples, using k 1 and k 2 parameters, and data likelihoods of L1 = p(Dz,1 | M1 ) and
L2 = p(Dz,2 | M2 ), respectively. Accordingly, we check whether
BIC(M1+2 | Dz,1 , Dz,2 ) < BIC(M1 | Dz,1 ) + BIC(M2 | Dz,2 )

(43)

i.e., whether
2 log L + k log n < 2 log(L1 L2 ) + k 1 log n1 + k 2 log n2 .

(44)

Informally, merging two models into one is beneficial if the joint model can explain the data
equally well (i.e., L  L1 L2 ), while requiring only a single set of parameters.
If more than two trajectories are considered, one has to evaluate all possible assignments
of these trajectories to models and select the assignment with the highest posterior. As this
quickly becomes intractable due to combinatorial explosion, we use an approximation and
consider the trajectories sequentially and in the order the robot observes them. We check
whether merging the new trajectory with one of the existing models leads to a higher
posterior compared to adding a new model for that trajectory to the set of previously
encountered models.
After having identified a set of models as prior information, we can exploit this knowledge
for making better predictions when observing a so far unseen articulated object. Consider
the situation in which a partial trajectory of a new object has been observed. To exploit the
prior information, we proceed exactly as before. We compute and compare the posteriors
according to (44), treating the newly observed data points as a new model or respectively
merging them into one of the w previously identified models by evaluating
p(Mnew , M1 , . . . , Mw ) < max p(M1 , . . . , Mj+new , . . . , Mw ).
j=1,...,w

(45)

If the newly observed data is merged with an existing model, the parameter vector is
estimated from a much larger dataset Dz,j Dz,new instead of Dz,new which leads to a better
estimation. Note that this step is carried out after each observation of the new sequence.
Thus, if the currently manipulated object ceases to be explained by the known models, the
method instantaneously creates a new model. After successful object manipulation, this
model serves as additional prior information for the future.
3.2 Closed Kinematic Chains
Although most articulated objects have the connectivity of kinematic trees, there exist
mechanisms containing closed kinematic chains (Featherstone & Orin, 2008). An intuitive
example of a closed-loop system is a robot that opens a door with its manipulator. While
both the robot and the door can be described individually as a kinematic tree using our
approach, the combined system of the robot, the door and the floor creates a kinematic loop.
Another example is a humanoid robot that has multiple contact points, e.g., by standing
on both feet, or a robot that manipulates an object with two arms (Sentis, Park, & Khatib,
2010). To describe such closed-loop systems, we need to extend our approach.
Recall that for finding the kinematic structure in Section 2.8, we established the correspondence for finding the graph that maximizes the posterior probability. For that, we
needed to compute the data likelihood of the graph based on edge constraints, which was
492

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

easy for kinematic trees. In this case, all links can be evaluated independently of each other.
However, computing the data likelihood of a kinematic graph based on edge constraints is
more difficult. This results from the more complex (joint) prediction of the poses of the
object parts involved in such a kinematic loop. In general, the chained up predictions of the
relative transformations between the object parts will not lead to a globally consistent prediction, which is needed to compute the overall data likelihood in case of closed kinematic
chains.
This problem, however, is closely related to loop-closing in the graph-based formulation
of the simultaneous localization and mapping (SLAM) problem (Lu & Milios, 1997; Dellaert,
2005; Frese, 2006; Grisetti, Stachniss, & Burgard, 2009). For this type of problem, closedform solutions exist only for very simple cases. A popular solution for the general case
are iterative optimization approaches to deal with the underlying non-linear least squares
problem.
To obtain a consistent pose estimation for the whole graph, we use the optimization
engine HOG-Man by Grisetti, Kummerle, Stachniss, Frese, and Hertzberg (2010), originally
designed to solve the SLAM problem. To generate the input graph for HOG-Man, we
proceed as follows. We add a vertex for each object part representing its initial pose
x01 , . . . , x0n , that we estimate for an (arbitrary) spanning tree of the graph. Then, for each
link model Mij in our graph G, we add an edge that constrains the relative transformation
 (in SLAM, this corresponds to an
between x0i and x0j to the expected transformation 
ij
observation). The optimization approach will then compute a set of new poses x1 , . . . , xn
that is in line with the constraints in the sense that it is the best prediction in terms of
the squared error that can be obtained given the links (in SLAM, this corresponds to the
corrected trajectory of the robot).
For the pose observations yi , we assume Gaussian noise with zero mean and covariance
y , i.e.,
yi = xi + 
  N (0, y ).

(46)
(47)

The data likelihood for a single object part being observed at pose y while being expected
at pose x given the kinematic graph G and a configuration q then becomes


1
T 1
p(yi | G, q)  exp  (xi 	 yi ) y (xi 	 yi ) .
(48)
2
Using this, the global data likelihood of an articulated object in a particular configuration
can be computed as the product over the likelihoods of all individual object parts, i.e.,
Y
p(y1:p | G, q) =
p(yi | G, q).
(49)
i1,...,p

As the configuration q of the articulated object is latent and thus not known, we need to
integrate over all possible configurations, i.e., calculate
Z
p(y1:p | G) = p(y1:p | G, q)p(q | G)dq.
(50)

493

fiSturm, Stachniss, & Burgard

Similar to (25), we approximate this integral by evaluating it only at the most likely configuration q of the articulated object. We assume that the configurations q are uniformly
distributed, i.e., then p(q | G)  n D , where n is the number of pose observations and
D is the total number of DOFs of the articulated object. The data likelihood for a pose
observation y1:p then becomes
p(y1:p | G)  n D p(y1:p | G, q).

(51)
n

1 , . . . , y ) of a whole articulated
The data likelihood of an observation sequence Dy = (y1:p
1:p
object then is
Y
i
p(Dy | G) 
n D p(y1:p
| G, qi )
(52)
i1,...,n

= n nD

Y

i
p(y1:p
| G, qi ).

(53)

i1,...,n

This data likelihood can now be used to select the best kinematic structure. Note that
in principle, all possible graphs need to be evaluated  which is super-exponential in the
number of object parts, and polynomial in the number of template models. In contrast,
finding the exact solution in the case of kinematic trees has a polynomial complexity of only
O(mp2 ). Obviously, the massive set of possible graph structures can only be fully evaluated
for small articulated objects and few template models.
In the absence of an efficient and exact solution, we propose an efficient approximation
that is able to find the locally best graph from an initial guess using a randomized search
strategy in polynomial time. The idea is that given the spanning tree as an initial solution
we only evaluate the graphs in the neighborhood of the current structure, i.e., graphs whose
topology is similar to the current one, e.g., by adding or removing one edge at a time. As
we will see in the experimental section, this heuristic is able to find the optimal (or nearoptimal) graph structure in most of the cases. Additionally, we can guarantee that this
randomized search strategy never gets worse than the initial solution, i.e., in our case the
spanning tree.
3.3 Finding the Number of DOFs
The current configuration of an articulated object is given by the stacked vector of all
individual configurations of its articulated links, i.e.,


q1
 q 
 2 
qlinks =  .. 
(54)
 . 
qDlinks
. The question now is, whether the articulated object actually has as many DOFs as the
sum of DOFs of its individual links might suggest. Clearly, in the case that the articulated
object is a kinematic tree, the DOFs D object
P of the articulated object directly equals the
sum over the DOFs of its links D links = (ij)EG d ij as all of its links can be actuated
494

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

(a)

(b)

Figure 4: Example of an open and a closed kinematic chain. Whereas the open chain in (a)
has three DOFs, the closed chain (b) has also only a single DOF.

independently of each other. However, for articulated objects containing loops, finding the
number of DOFs of an articulated object is not trivial.
For an example, consider the object in Fig. 4a which consists of four object parts and
a total of three DOFs. In contrast, the object in Fig. 4b consists of four object parts,
connected by four revolute links in the form of a loop. Each of the four links has a single
DOF, and therefore the configuration vector defining the configuration of all of its links
is qlinks = (q1 , q2 , q3 , q4 )  R4 . Yet, the overall system has only a single DOF: when the
first joint is brought into a particular configuration, the other joints are fixed as well, as a
result of the loop closure. This means that the object configuration qobject  R has only
a single dimension, and thus the object configuration space is a one-dimensional manifold
embedded in the four-dimensional link configuration space.
Finding a mapping between the high-dimensional link configuration space RD links and
a lower-dimensional object configuration space RD object can for example be achieved using
PCA for linear manifolds, and LLE or ISOMAP for non-linear manifolds. In the case of
PCA, this results in finding a projection matrix P  RD object D links describing the mapping
qobject = P qlinks

(55)

Recall from (53), that the number of DOFs has a strong influence on the data likelihood
of a configuration, because a higher dimensional configuration space results in a lower
likelihood for a single configuration. As a result, a model with fewer DOFs is preferred over
a model with more DOFs. At the same time, if additional parameters need to be estimated
for the dimension reduction, then these parameters are also model parameters and thus
need to be considered during model selection.
Informally speaking, if a kinematic graph with fewer DOFs explains the data equally
well, it will have a higher data likelihood and thus it be favored in the structure selection
step. In the experimental section, we will see that we can use this to accurately and robustly
estimate the DOFs of various articulated objects.

495

fiSturm, Stachniss, & Burgard

4. Perception of Articulated Objects
For estimating the kinematic model of an articulated object, our approach needs a sequence
1 , . . . , yn ) that includes the poses of all p parts of the
of n pose observations Dy = (y1:p
1:p
object. For our experiments, we used different sources for acquiring these pose observations:
marker-based perception, as described in Section 4.1, domain-specific but marker-less perception as described in Section 4.2, and perception based on the internal forward kinematic
model of a manipulator using its joint encoders as described in Section 4.3.
4.1 Marker-Based Perception
For observing the pose of an articulated object, we used in our experiments three different
marker-based systems, each with different noise and outlier characteristics: a motion capture
studio with low noise and no outliers, ARToolkit markers with relatively high noise and
frequent outliers, and OpenCVs checkerboard detector with moderate noise and occasional
outliers.
4.1.1 Motion Capturing Studio
We conducted our first experiments in a PhaseSpace motion capture studio at Willow
Garage, in collaboration with Pradeep and Konolige (Sturm et al., 2009). This tracking
system uses several high-speed cameras installed on a rig along the ceiling, and active LED
markers attached on the individual parts of the articulated object. The data from the
PhaseSpace device is virtually noise- and outlier-free. The noise of the PhaseSpace system
is specified to be y,pos < 0.005 m and y,orient < 1 .
4.1.2 ARToolkit Markers
Additionally, we used the passive marker-based system ARToolkit for registering the 3D
pose of objects by Fiala (2005). This system has the advantage that it requires only a
single camera, and can be used without any further infrastructure. The ARToolkit markers
consist of a black rectangle and an error-correcting code imprinted on a 6x6-grid inside
the rectangle for distinguishing the individual markers. We found that the noise with this
system strongly depends on the distance and the angle of the marker to the camera. With
a marker size of 0.08 m and in a distance of 2 m from the camera, we typically obtained
noise values of y,pos = 0.05 m and y,orient = 15 .
4.1.3 Checkerboard Markers
OpenCVs checkerboard detector provides a much higher pose accuracy. The detector
searches the camera images for strong black and white corners at sub-pixel accuracy (Bradski & Kaehler, 2008). With this system, we typically obtained measurement noise around
y,pos = 0.005 m and y,orient = 5 with marker sizes of 0.08 m side length in 2 m distance from the camera. One can distinguish different markers with this system by using
checkerboards with varying numbers of rows and columns.

496

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

Figure 5: Marker-less pose estimation using a stereo camera. For each segmented plane,
we iteratively fit a rectangle (left image). The right two images show observed
tracks of a cabinet drawer and a cabinet door.

4.2 Marker-Less Pose Estimation
In contrast to using artificial markers, it is also possible to estimate the object pose directly,
for example, from dense depth images acquired by a stereo camera system. We recently
developed such a system in collaboration with Konolige (Sturm et al., 2010). Using a
marker-less camera-based tracking system has several advantages. First, it does not rely
on artificial markers attached to the articulated objects, and second, it does not require
expensive range scanners which have the additional disadvantage that they poorly deal
with moving objects, making them inconvenient for learning articulation models. However,
we recognize that in general the registration of arbitrary objects in point clouds is still an
open issue. Therefore, we restrict ourselves here to fronts of kitchen cabinets. This does not
solve the general perception problem, but provides a useful and working solution for mobile
manipulation robots performing service tasks in households. In our concrete scenario, the
perception of articulated drawers and doors in a kitchen environment requires the accurate
detection of rectangular objects in the depth image sequences.
From the stereo processing system, we obtain in each frame a disparity image D 
R640480 , that contains for each pixel (u, v) its perceived disparity D(u, v)  R. For more
details on the camera system, in particular on the choice of a suitable texture projection
pattern, we refer the interested reader to the recent work of Konolige (2010). The relationship between 2D pixels in the disparity image and 3D world points is defined by the
projection matrices of the calibrated stereo camera, and can be calculated by a single matrix
multiplication from the pixel coordinates and disparity.
We apply a RANSAC-based plane fitting algorithm for segmenting the dense depth
image into planes. The next step is to find rectangles in the segmented planes. We start
with a sampled candidate rectangle and optimize its pose and size iteratively, by minimizing
an objective function that evaluates how accurately the rectangle candidate matches the
points in the segmented plane. After the search converges, we determine the quality of the
found rectangle by evaluating its pixel precision and recall. An example of the iterative
pose fitting is given in the left image of Fig. 5: the rectangle candidate started in the lower
left of the door, and iteratively converged to the correct pose and size of the door.

497

fiSturm, Stachniss, & Burgard

Articulated Object

Position of
End Effector

Arm Control

Cartesian
Equilibrium
Point Generation

y1:t

Model Fitting and
Selection
M, 

xCEP
t

xt , Jt

Model Prediction

Figure 6: Overall control structure (Sturm et al., 2010). The robot uses the trajectory of
its end effector to estimate a model of the articulated object. Subsequently, it
uses this model for generating the next Cartesian equilibrium point.

Finally, for a sequence of depth images D1:n , the detected rectangles need to be integrated into a set of consistent tracks, one for each visible rectangle. As a result, we obtain a
1 , . . . , yn ) that we can use for model estimation and model
set of pose sequences Dy = (y1:p
1:p
selection. The middle and right image in Fig. 5 show the tracks that we obtained when
observing a drawer and a door in a kitchen cabinet. More details on this approach have
recently been described by Sturm et al. (2010).
4.3 Perception using the Joint Encoders of a Mobile Manipulation Robot
Next to visual observation of articulated objects, a mobile manipulation robot can also
estimate the kinematic model while it physically interacts with the articulated object. By
evaluating its joint encoders, the robot can compute the pose of its gripper by using the
forward model of its manipulator. When the robot establishes firm contact with the handle
of a cabinet door, the position of its end-effector directly corresponds to the position of the
door handle. As a result, the robot can both sense the position of the handle as well control
it by moving the manipulator.
The approach described in this section was developed in collaboration with Jain and
Kemp from the Healthcare Robotics Lab at Georgia Tech. The robot that we use for this
research is a statically stable mobile manipulator named Cody. It consists of two arms from
MEKA Robotics and an omni-directional mobile base from Segway. As an end-effector,
it uses a hook inspired by prosthetic hooks and human fingers, and is described in more
detail in the recent work of Jain and Kemp (2009a). Furthermore, we used a PR2 robot
from Willow Garage for additional experiments, using a standard 1-DOF gripper with two
fingers, located in our lab.
Fig. 6 shows a block diagram of our approach. The robot observes the pose of its
end effector in Cartesian space, denoted by y  SE (3). While operating the mechanism,
the robot records the trajectory y1:t over time as a sequence of poses. From this partial
trajectory, it continuously estimates the kinematic model of the articulated object, that the
robot uses in turn to predict the continuation of the trajectory (Sturm et al., 2010).

498

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

To actively operate an articulated object with a robot, we use a trajectory controller that
updates the Cartesian equilibrium point based on the estimated Jacobian of the kinematic
model of the articulated object. This controller uses the kinematic model to generate
Cartesian equilibrium point (CEP) trajectories in a fixed world frame, attached to the initial
location of the handle. At each time step t, the controller computes a new equilibrium point
xCEP
as
t
mechanism
xCEP
= xCEP
+ vthook ,
t
t1 + vt

(56)

where vtmechanism is a vector intended to operate the mechanism, and vthook is a vector
intended to keep the hook from slipping off the handle. The controller computes
vtmechanism = L mechanism

Jt
kJt k

(57)

as a vector of length L mechanism = 0.01 m along the Jacobian of the learned kinematic
function of the mechanism, i.e.,
fi
Jt = fM, (q)fiq=qt .
(58)
For vthook , we use a proportional controller that tries to maintain a force of 5 N between
the hook and the handle in a direction perpendicular to Jt . This controller uses the force
measured by the wrist force-torque sensor of the robot. We refer the reader to the work of
Jain and Kemp (2009b) for details about the implementation of equilibrium point control,
and how it can be used to coordinate the motion of a mobile base and a compliant arm
(Jain & Kemp, 2010).
The positional accuracy of the manipulator itself is very high, i.e., y,pos  0.01 m.
However, by using a hook as the end-effector, the robot cannot sense the orientation of the
handle. As the manipulator is mounted on a mobile base, the robot can move around, and
thus the positional accuracy of the sensed position of the hook in a global coordinate system
(and thus including localization errors of the base) reduces to about y,pos  0.05 m.

5. Experiments
In this section, we present the results of a thorough evaluation of all aspects of our approach.
First, we show that our approach accurately and robustly estimates the kinematic models of
typical household objects using markers. Second, we show that the same also holds for data
acquired with the marker-less pose estimation using our active stereo camera system. Third,
we show that our approach also works on data acquired with different mobile manipulation
robots operating various pieces of furniture in domestic environments.
5.1 Microwave Oven, Office Cabinet, and Garage Door
For our first experiments, we use pose observations from three typical objects in domestic
environments: the door of a microwave oven, the drawers of an office cabinet, and a garage
door. The goal of these experiments is to show that our approach both robustly and
accurately estimates link models, as well as the correct kinematic structure of the whole
499

fiSturm, Stachniss, & Burgard

articulated object. In addition, we show that the range of the configuration space can be
obtained during model estimation.
The motion of the microwave oven and the cabinet was tracked using a motion capture
studio in collaboration with Pradeep and Konolige (Sturm et al., 2009), and the garage
door using checkerboard markers. For each object, we recorded 200 data samples while
manually articulating each object. For the evaluation, we carry out 10 runs. For each run,
we sampled n = 20 observations that we use for fitting the model parameters. We used the
remaining observations for measuring the prediction accuracy of the fitted model (10-folds
cross-validation).
5.1.1 Model Fitting
The quantitative results of model fitting and model selection are given in Table 2. As
can be seen from this table, the revolute model is well suited for predicting the opening
movement of the microwave door (error below 0.001 m) while the prismatic model predicts
very accurately the motion of the drawer (error below 0.0016 m), which is the expected
result. Note that also the revolute model can also explain the motion of the drawer with an
accuracy of 0.0017 m, by estimating a rotary joint with a large radius. It should be noted
that the flexible GP model provides roughly the same accuracy as the parametric models
and is able to robustly predict the poses of both datasets (0.0020 m for the door, and
0.0017 m for the drawer). In the case of the simulated garage door, however, all parametric
models fail whereas the GP model provides accurate estimates.
The reader might wonder now why the GP model alone does not suffice, as the GP model
can represent many different types of kinematic models, including revolute and prismatic
ones. However, even if the GP model fits all data, it is not the best choice in terms of the
resulting posterior likelihoods. The GP model can  in some cases  be overly complex,
and then over-fit the data at hand. This high complexity of the GP model is penalized
by the BIC. In contrast, the specialized models have a smaller number of free parameters,
and are therefore more robust against noise and outliers. Furthermore, they require less
observations to converge. These experiments illustrate that our system takes advantage of
the expert-designed parametric models when appropriate while keeping the flexibility to
also learn accurate models for unforeseen mechanical constructions.
The learned kinematic models also provide the configuration range C of the articulated
object. For visualization purposes, we can now sample configurations from this range, and
project them to object poses using the learned forward function. Fig. 7, Fig. 8, and Fig. 9
illustrate the learned configuration range for the door of the microwave oven, the garage
door, and the two drawers of the office cabinet, respectively.
5.1.2 Model and Structure Selection
After fitting the model candidates to the observed data, the next goal is to select the model
that best explains the data, which corresponds to finding the model that maximizes the
posterior probability (or minimizes the BIC score).
The right image in Fig. 7 shows the resulting graph for the microwave oven dataset,
with the BIC score indicated at each edge. As expected, the revolute model is selected,

500

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

Dataset 

Rigid
Model

Prismatic
Model

Revolute
Model

GP
Model

Microwave
(z,pos. = 0.002 m,
z,orient. = 2.0 )

pos. error =
orient. error =
=

0.3086 m
37.40
0.891

0.1048 m
32.31
0.816

0.0003 m
0.15
0.000

0.0020 m
0.16
0.000

Drawer
(z,pos. = 0.002 m,
z,orient. = 2.0 )

pos. error =
orient. error =
=

0.0822 m
2.06
0.887

0.0016 m
1.36
0.000

0.0018 m
1.60
0.003

0.0017 m
1.09
0.000

Garage Door
(z,pos. = 0.050 m,
z,orient. = 5.0 )

pos. error =
orient. error =
=

1.0887 m
14.92
0.719

0.3856 m
10.79
0.238

0.4713 m
10.34
0.418

0.0450 m
0.93
0.021

Table 2: Prediction errors and estimated outlier ratios of the articulation models learned
of a microwave oven, an office cabinet, and a real garage door.

(a)

microwave oven

door

x1

x2

rigid
BIC(M12 ) =

2568507.7

prism.
BIC(M12
)=

686885.1

BIC(Mrev.
12 ) =

461.9

BIC(MGP
12 ) =

165.8
(b)

Figure 7: Visualization of the kinematic model learned for the door of a microwave oven.
(a) configuration range. (b) kinematic graph. The numbers on the edges indicate
the BIC score of the corresponding model candidate.

because it has the lowest BIC score. Correspondingly, the right image in Fig. 8 shows the
BIC scores for all edges for the garage door dataset, where the GP model gets selected.
A typical articulated object consisting of multiple parts is a cabinet with drawers as
depicted in Fig. 9. In this experiment, we track the poses of the cabinet itself (x1 ), and its
two drawers (x2 and x3 ). During the first 20 samples, we opened and closed only the lower
drawer. Accordingly, a prismatic joint model Mprism.
is selected (see top row of images
23
in Fig. 9). When also the upper drawer gets opened and closed, the rigid model Mrigid
is
12
prism.
prism.
prism.
replaced by a prismatic model M12 , and M23
is replaced by M13 , resulting in the
kinematic tree EG = {(1, 2), (1, 3)}. Note that it is not required to articulate the drawers
one after each other. This was done only for illustration purposes.
501

fiSturm, Stachniss, & Burgard

building

garage door

x1

x2

rigid
BIC(M12 ) =

9893.4

prism.
BIC(M12
)=

5450.8

BIC(Mrev.
12 ) =

5870.7

BIC(MGP
12 ) =

620.2

(a)

(b)

Figure 8: Visualization of the kinematic model learned for a garage door. (a) 10 uniformly
sampled configurations. (b) kinematic graph.

5.1.3 Multi-Dimensional Configuration Spaces
To illustrate that our approach is also able to find models with higher-dimensional configuration spaces with d > 1, we let the robot monitor a table that was moved on the floor. The
robot is equipped with a monocular camera tracking an Artoolkit marker attached to the
table. In this experiment, the table was only moved and was never turned, lifted, or tilted
and therefore the configuration space of the table has two dimensions. Fig. 10 shows four
snapshots during learning. Initially, the table is perfectly explained as a rigid object in the
room (top left). Then, a prismatic joint model best explains the data since the table was
moved in one direction only (top right). After moving sideways, the best model is a 1-DOF
Gaussian process model that follows a simple curved trajectory (bottom left). Finally, the
full planar movement is explained by a 2-DOF Gaussian process model (bottom right), that
can model movements that lie on 2D surfaces.
5.1.4 Additional Examples
We ran similar experiments on a large set of different articulated objects that typically
occur in domestic environments, including office cabinets, office doors, desk lamps, windows,
kitchen cabinets, fridges and dishwashers and a garage door. Four examples are given in
Fig. 11. Videos of these (and all other) experiments are available on the homepage of the
corresponding author3 . These videos show both the original movie as well as an overlay
of the inferred kinematic model. For these experiments, we attached checkerboards of
different sizes to all movable parts, and used both a consumer-grade video camera and a
low-cost laptop webcam for acquiring the image data. Our software also visualizes the
learned articulation models in 3D and back-projects them onto the image to allow for easy
visual inspection. The detected poses of the checkerboards are visualized as red/green/blue
coordinate axes systems, and selected links between them are indicated using a colored
connection. The software also displays the configuration range by generating poses in the
3. http://www.informatik.uni-freiburg.de/ sturm

502

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

cabinet

drawer 1

drawer 2

x1

x2

x3

rigid
BIC(Mij ) =
prism.
BIC(Mij
)=

189.3

997.2

993.0

63.9

61.7

62.6

BIC(Mrev. ) =

41.8

58.1

59.3

BIC(MGP
ij ) =

277.2

279.0

278.4

ij

(a)

(b)
cabinet

drawer 1

drawer 2

x1

x2

x3

rigid
BIC(Mij ) =
prism.

793.7

2892.2

3660.1

)=

88.8

86.9

84.7

BIC(Mrev. ) =

84.9

84.4

82.4

BIC(MGP
ij ) =

331.6

331.0

331.8

BIC(Mij

ij

(c)

(d)

Figure 9: Incrementally estimating a model of two drawers of a cabinet. (a) Initially, only
the lower drawer is opened and closed. (b) Corresponding kinematic graph. (c)
Both drawers are opened and closed independently. (d) Corresponding kinematic
graph.

estimated range. For revolute joints, it additionally indicates the rotation axis using a line
and a surrounding circle.
From visual inspection of the objects in Fig. 11, one can see how accurate the model
estimation works in conjunction with marker-based tracking: the motion of the drawers of
the cabinet is well matched, and the rotation axes of the door hinge and the door handle
are estimated very close to their true position. The upper part of the garage door moves in
a slider in the ceiling, while the lower part is connected via a revolute joint. The resulting
motion is clearly neither revolute nor prismatic, and consequently our approach selects the
GP model. The desk lamp consists of two-bar links that keep the light housing always
upright (or, loosely speaking, rigid in orientation), but move in the positional domain along
a circle. This link type can be well explained by the GP model. The existence of these
objects shows the necessity to supply a domestic service robot with such a general, nonparametric model that can deal with a wide variety of different articulated objects. It should
be noted that the majority of articulated objects in domestic environments will consist of
revolute and prismatic joints which can be more robustly estimated using parametrized
models. This motivates our approach that enables a robot to fit both parametric and

503

fiSturm, Stachniss, & Burgard

Figure 10: Learning a model for a table moving on the ground plane. The arrows indicate
the recovered manifold of the configuration space.

nonparametric models at the same time and compare them in terms of posterior likelihoods
in a consistent model selection framework.
Another interesting object is a car, as its doors and windows have both tree- and chainlike elements. In Fig. 12, we observed the motion of the drivers door and window. After
the first few observations, our approach estimates the structure to be rigid, and links both
the door and the window in parallel to the car body. After we open the window to the
half, our approach attaches the drivers window to the door, and selects a prismatic model.
Surprisingly to us, when we open the window further (and thus acquire more observations),
our approach switches to a revolute model for the drivers window associated with a large
radius (r = 1.9 m). By looking carefully at the data and the car, we can confirm that the
window indeed moves on a circular path, which is due to its curved window glass. Finally,
after the driver closes the door, also a revolute model for the link between the car body and
the door is selected.

504

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

x1
prismatic
x2

x1
revolute

prismatic
x2
x3

x3
revolute
(a) Office cabinet

(b) Room door

x2
GP

x2
x1

x3

GP

GP
x1

(c) Garage door

(d) Desk lamp

Figure 11: Visualization of the learned articulation models for several further domestic
objects. (a) cabinet with two drawers, (b) room door including handle, (c)
garage door, (d) desk lamp with two-bar links.

We conclude from these results, that our approach is able to estimate the kinematic
parameters and the kinematic structure of different household objects at high accuracy, i.e.,
the prediction error of the learned models is around 0.001 m and 1 for objects tracked in
a motion capture studio, and around 0.003 m and 3 for checkerboard markers. At this
accuracy, the learned models are well suited for mobile manipulation tasks.
5.2 Evaluation of Marker-Less Model Estimation
The goal of our next set of experiments is to show that the kinematic models can be learned
in certain environments without requiring artificial markers. In particular, we focus here
on kitchen environments with rectangular cabinet fronts, and employ our pose detector
described previously (Sturm et al., 2010).
505

fiSturm, Stachniss, & Burgard

x3
x3
rigid
x1

x1

rigid

prismatic
rigid

x2

x2

(a)

(b)

x3
x3
x1

rigid

revolute

x1

revolute
revolute

x2
(c)

x2
(d)

Figure 12: Snapshots of the learning process when incrementally observing the motion of a
car door and its window from camera images. Due to the shape of the glass the
drivers window actually moves on a circular arc with radius r = 1.9 m. Images
taken after (a) 10, (b) 40, (c) 60, and (d) 140 pose observations.

In a first experiment carried out in a motion capture studio, we evaluated our detector
and found that it detected the cabinet drawer in more than 75% of the images up to a
distance of 2.3 m from the camera.
We evaluated the robustness of our articulation model learner on detailed logfiles of both
a door (0.395 m 0.58 m) and a drawer (0.395 m 0.125 m) of a typical kitchen interior. We
repeatedly opened and closed these objects in approximately 1 m distance of the robot;
in total, we recorded 1,023 and 5,202 images. We downsampled these logs stochastically
to 100 images, and ran pose estimation, model estimation, and structure selection for 50
times. The outcome of the model selection process, and the accuracy of the selected model
is depicted in Fig. 14 for the door dataset.

506

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

(a)

(b)

Figure 13: Articulation model learned from observing a drawer (a) and a door (b).

p(model)

1

rigid
prismatic
revolute

0.75
0.5
0.25

error in position
error in orientation

0.05
0.04
0.03
0.02
0.01
0

10
8
6
4
2
0
10

20
30
40
number of observations (door)

50

orientation error [deg]

position error [m]

0

60

Figure 14: Evaluation of the articulation models learned for a cabinet door, averaged over
50 runs. The plot at the top shows the probability of the articulation model templates, the plot at the bottom shows the prediction error and standard deviation
of the learned model.

For both datasets, we found that roughly for the first 10 observations, mostly the rigid
model is selected, as no substantial motion of the drawer or door was yet detected. The
more observations are added to the track, the higher the error between the (rigid) model

507

fiSturm, Stachniss, & Burgard

Figure 15: Images showing the robot Cody at Georgia Tech operating the five mechanisms
using the approach described in Section 4.3. The objects are (from left to right):
a cabinet door that opens to the right, a cabinet door that opens to the left, a
dishwasher, a drawer, and a sliding cabinet door. Images courtesy of Jain and
Kemp.

predictions and the observations becomes. As a result, the prismatic and revolute models
are selected more frequently. After 30 observations, model selection has converged in all
cases to the true model.
The models learned of the drawer datasets have a predictive accuracy of approximately
0.01 m and 7 ; and 0.01 m and 3.5 for the door dataset. Although the predictive accuracy of
the learned models is slightly lower in comparison with the marker-based tracking systems
due to the higher noise of the tracking system, the learned models are at this accuracy
usable for a mobile manipulator operating these objects.
5.3 Operating Articulated Objects with Mobile Manipulators
In this section, we show that real robots can utilize our approach to learn the kinematic
models of objects for active manipulation. Here, control of the arm was done using the
equilibrium point control as described in Section 4.3, which was the result of a collaboration
with Jain and Kemp (Sturm et al., 2010). The experiments were conducted on two different
platforms, the robot Cody and a PR2 robot.

508

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

5.3.1 Task Performance
We evaluated the performance of our approach on five different mechanisms using the robot
Cody: a cabinet door that opens to the right, a cabinet door that opens to the left, a
dishwasher, a drawer, and a sliding cabinet door. We performed eight trials for each mechanism. The robot started approximately 1 m from the location of the handle. We manually
specified the grasp location by selecting a point in a 3D point cloud recorded by the robot,
an orientation for the hook end effector, and the initial pulling direction. The task for the
robot was to navigate up to the mechanism and operate it, while learning the articulation
model using the methods described in Section 3.1. We deemed a trial to be successful if
the robot navigated to the mechanism and opened it through an angle greater than 60 for
revolute mechanisms or 0.3 m for prismatic mechanisms.
Fig. 15 shows the robot after it has pulled open each of the five mechanisms in one of
the respective trials. The robot successfully opened the 3 rotary mechanisms in 21 out of
24 trials and the 2 linear mechanisms in all 16 trials. The robot was able to open the doors
more than 70 , and to estimate their radii on average with an error below 0.02 m. Further,
the robot pulled open the drawer and the sliding cabinet repeatedly on average over 0.49 m.
Overall the robot was successful in 37 out of 40 trials (92.5%).
All three failures were due to the robot failing to hook onto the handle prior to operating
the mechanism, most likely due to odometry errors and errors in the provided location of
the handle. In our experiments, we did not observe that the model learning caused any
errors. In principle, however, the hook could slip off the handle if a wrong model had been
estimated.
5.3.2 Model Fitting and Selection from End-Effector Trajectories
Fig. 1 and Fig. 16 show examples of the PR2 robot operating several articulated objects
common to domestic environments, i.e., a fridge, a drawer, a dishwasher door, the tray of a
dishwasher, and the valve of a heater. For these experiments, we did not use feedback control
as described in Section 4.3 but tele-operated the manipulator manually. First, we recorded a
set of trajectories by guiding the manipulator to operate various articulated objects. During
execution, we played these trajectories back using a different implementation of equilibrium
point control available on the PR2 platform, and recorded the end-effector trajectories of the
robot. We used these trajectories subsequently to learn the kinematic models. Finally, we
visualized these models by superimposing them on images taken by a calibrated wide-angle
camera mounted on the head of the robot, see Fig. 16. In our experiments, our approach
always selected the correct model candidate. One can easily verify by visual inspection that
our approach estimates the kinematic properties (like the rotation axis or the prismatic
axis) very accurately.
These experiments show that robots can successfully learn accurate kinematic models of
articulated objects from end-effector trajectories by using our approach. With the PR2, we
achieved an average predictive accuracy of the learned models below 0.002 m (in terms of
residual error of the observed trajectory with respect to the learned model), which is more
than sufficient for using our models for mobile manipulation tasks in domestic settings.

509

fiSturm, Stachniss, & Burgard

revolute

(a) Cabinet door

revolute

(b) Dishwasher door

prismatic

(c) Dishwasher tray

revolute

(d) Valve of a heater

Figure 16: A PR2 robot learns the kinematic models of different pieces of furniture by
actuating them using its manipulator. Objects from top to bottom: fridge,
cabinet door, drawer, dishwasher door, dishwasher tray, water tap, valve of a
heater.

510

fi0.4

0.4

0.2

0.2

0

0

-0.2

-0.2

-0.4

-0.4
-0.4

-0.2
x [m]

0

-0.4

-0.2
x [m]

z [m]

y [m]

A Probabilistic Framework for Learning Kinematic Models of Articulated Objects

0

Figure 17: These plots show the observed trajectories and the 5 recovered models when
minimizing the overall BIC using our approach. Trajectories assigned to the
same model are depicted in the same color.

prediction error [m]

without learned prior models
with learned prior models
0.3

0.2

0.1

0
0

0.1

0.2
0.3
0.4
0.5
0.6
0.7
0.8
ratio of the observed trajectory vs. the full trajectory

0.9

1

Figure 18: This graph shows the average prediction error (line) and standard deviation
(shaded area) of the learned model on the full trajectory with and without prior
information.

511

fiSturm, Stachniss, & Burgard

5.4 Improving Model Estimation Based on Experience
In the experiments described in the previous section, we learned the kinematic models
for the kitchen furniture independent of each other. By using the approach described in
Section 3.1 on data from Cody, we exploit the correlation between the models of different
objects by searching for the set of model clusters that maximize the posterior probability.
Fig. 17 shows the result of this experiment. The colors indicate the cluster to which the
trajectories have been assigned to. Our approach correctly recognized that the robot had
operated 5 different mechanisms and assigned the 37 different trajectories correctly to the
corresponding models.
We measured the average prediction error with and without learning prior models (see
Fig. 18), using leave-one-out cross-validation and a randomized ordering of the trajectories.
We found that the prior models reduce the prediction error considerably, especially if the
new trajectory is only partially observed. When 30% to 70% of the new trajectory have
been observed, the prediction error is reduced by a factor of three and more. As a result,
the robot comes up with a substantially more accurate model early and can utilize this
knowledge to better control its manipulator.
Throughout all experiments on Cody, we used a fixed noise term of z,pos = 0.05m. This
accounts for inaccuracies in the observation of the end effector position, due to variations
in the hooking position, and small errors in the kinematic forward model and robot base
localization. We found in repeated experiments that in the range between 0.02m  z,pos 
0.20m, the results are similar to our previous results obtained with z,pos = 0.05m. Only
for significantly smaller values of z,pos more models are created, for example due to small
variations of the grasping point and other inaccuracies. For much larger values, observations
from different mechanisms are clustered into a joint model. Thus, our results are insensitive
to moderate variations in the observation noise z,pos .
This experiment illustrates that our approach enables a mobile robot to learn from
experience or exploit prior information when manipulating new objects. The experience
increases the prediction accuracy by a factor of approximately three.
5.5 Detecting Kinematic Loops
In our final set of experiments, we evaluated our approach on objects containing kinematic
loops. The goal of these experiments is to show that our approach can estimate correctly
both the kinematic connectivity, as well as the correct number of DOFs.
For that purpose, we used the first four segments of a yardstick. This results in an
open kinematic chain consisting of three revolute joints (see top left image of Fig. 19). This
object has three DOFs, as all revolute joints are independent of each other. In a second
experiment, we taped the fifth segment of the yardstick together with the first one. This
creates an kinematic loop, see top right image of Fig. 19: the resulting object consists of
four revolute joints each having a single DOF. The resulting mechanism has effectively only
a single DOF. We articulated the objects manually, and recorded object pose datasets with
|Dy | = 200 samples each using checkerboard markers.
The second and the third row of Fig. 19 visualize the learned kinematic model for
the open and the closed kinematic model, respectively, while the fourth row shows the
kinematic structure of the learned model. From this figure, it can be seen that our ap512

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

proach correctly recognizes that the open kinematic chain consists of three revolute links
rev.
rev.
(Mrev.
12 , M23 , M34 ), having three DOFs q = (q1 , q2 , q3 ) in total. For the closed kinematic
rev.
rev.
rev.
chain, our approach selects four revolute links (Mrev.
12 , M23 , M34 , M14 ), and correctly
infers that the object only exhibits a single DOF q = (q1 ).
We also analyzed the progression of model selection while the training data is incorporated. The left plot of Fig. 20 shows the DOFs of the learned kinematic model for the
open kinematic chain. Note that we opened the yardstick segment by segment, therefore
the number of DOFs increases step-wise from zero to three. The right plot shows the estimated number of DOFs for the closed kinematic chain: our approach correctly estimates
the number of DOFs to one already after the first few observations.
In more detail, we have analyzed the evolution of the BIC scores and the runtime of the
different approaches for the closed kinematic chain in Fig. 21. The plot in the top shows
the evolution of the BIC scores of all possible kinematic structures. We have colorized
the curves corresponding to the spanning tree solution (solid red), heuristic search (dashed
blue) and the global optimum (dotted green). The spanning tree solution that we use as
the starting point for our heuristic search is on average 35.2% worse in terms of BIC than
the optimal solution. In contrast, the BIC of the heuristic search is only 4.3% worse, and
equals the optimal solution in 57.5% of the cases. The time complexity of computing the
spanning tree is independent of the number of training samples, see bottom plot in Fig. 21.
In contrast to that, the evaluation of kinematic graphs requires for each kinematic structure
under consideration the evaluation of whole object poses, and thus is linear in the number
of training samples n. The heuristic search only evaluates kinematic graphs along a trace
through the structure space. As a result, for the yardstick object with p = 4 object parts,
the heuristic search requires on average 82.6% less time than the full evaluation.
We conducted similar experiments on other objects containing kinematic loops and
reduced DOFs. Two examples are depicted in the first row of Fig. 22: an artificial object
consisting of four parts but only a single revolute joint, and a common domestic step ladder
consisting of two revolute joints with only a single, shared DOF. In all cases, our approach
was able to correctly estimate both the kinematic parameters and the correct number of
DOFs.
With these experiments, we have shown that our approach is able to detect closed
chains in articulated objects, and correctly estimates the correct number of DOFs. As loop
closures (or reduced DOFs) reduce the configuration space of an object significantly, this is
valuable information for a mobile manipulator, for example while reasoning about possible
configurations of an object.
5.5.1 Evaluation of Model Selection Robustness
Finally, we investigated the influence of the choice of the observation noise variable z on the
model selection process on artificial data. For this analysis, we sampled noisy observations
true = 0.05. On the resulting
from a revolute model with a true observation noise of z,pos
observation sequence, we fitted the candidate models, and selected the best model. We
repeated this experiment for 10 independent runs and evaluated the mean and the standard
deviation. When the number of training samples n is kept fixed, then a higher noise
assumption favors the selection of simpler models, and vice versa. Fig. 23 illustrates this

513

fiSturm, Stachniss, & Burgard

x3
revolute

x1

revolute

revolute

x2

x4
revolute
x2

x1
revolute

q1 q2 q3 . . .
x1
Mrev.
12

x4

x2

x3

revolute

revolute
x3

q1 q2 q3 . . .
x4

x1

Mrev.
Mrev.
23
34

Mrev.
12

x2

x3

x4

Mrev.
Mrev.
Mrev.
23
34
14

Figure 19: Open kinematic chain with three DOFs (left column) and closed kinematic chain
with only a single DOF (right column). First row: images of the objects. Second
and third row: learned kinematic models from two different perspectives. Fourth
row: learned graphical model, showing the connectivity and the DOFs of the
learned kinematic model. The selected kinematic model is visualized by bold
edges, the DOFs are given by the boldly type-set configuration variables.

514

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

open kinematic chain

closed kinematic chain
DOFs

DOFs

4

4

3

3

2

2

1

1

0

0

50

100

150

0

200

0

50

training samples n

100

150

200

training samples n

Figure 20: Estimated number of DOFs for the open and the closed kinematic chain object
(see Fig. 19). Left: open kinematic chain. Right: closed kinematic chain.

spanning tree

search heuristic

global optimum

BIC

10,000
0

time [s]

10,000
200
150
100
50
0

0

20

40

60

80

100

120

140

160

180

200

training samples n

Figure 21: Top: BIC scores of all possible kinematic structures for the closed kinematic
chain, as depicted in the top right image of Fig. 20. Bottom: Computation
times as a function of the number of training samples.

515

fiSturm, Stachniss, & Burgard

x3
x2
rigid
x1

revolute
x4

rigid
revolute

revolute

x2

x3
x1

q1 q2 q3 . . .
x1

x2

x3

q1 q2 . . .
x4

x1

x2

x3

rigid

Mrot.
12

M
Mrigid
23 24

Mrot.
Mrot.
13
12

Figure 22: An articulated object consisting of a single revolute joint (left), and a stepladder
consisting of two revolute joints (right).

516

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

2,000

BIC

1,500
1,000

500
0

Gaussian process model
revolute model
prismatic model
rigid model

selected model
0.0001

0.001

0.01

0.1

1.0

observation noise z,position

Figure 23: BIC score as a function of the assumed observation noise. A low noise assumption favors the selection of more complex models, and vice versa.

dependency: for n = 50 and an assumed noise level of z,pos  0.02, the GP model is
selected. Between 0.02  z,pos  0.2, the revolute model yields the best trade-off between
model complexity and data likelihood. Above 0.2  z,pos , the rigid model best explains
the observations as the noise level of this magnitudes hides the underlying model.
With this experiment, we demonstrated that our model selection procedure is robust over
large intervals of the observation noise assumption, i.e., even though the true observation
true = 0.05, our approach selected the revolute model when the noise
noise was set to z,pos
assumption was in between 0.02  z,pos  0.2, thus robust over a whole magnitude. We
also performed more experiments on synthetic data to verify that our estimators are robust
against normally distributed noise and that the MLESAC-based estimators are additionally
robust against uniformly distributed outliers.
5.6 Open-Source Availability
The source code, documentation, code samples, and tutorials are fully available as opensource, licensed under BSD. We also provide a step-by-step guide to repeat these experiments using a consumer-grade laptop and a webcam4 .

6. Related Work
In this paper, we have combined several techniques that come from different fields, i.e.,
system identification for fitting kinematic models, information theory for model comparison
and structure selection, computer vision for estimating and tracking objects, service robotics
and control for actually manipulating articulated objects with mobile manipulators. In the
4. http://www.ros.org/wiki/articulation

517

fiSturm, Stachniss, & Burgard

following, we will review related approaches, contrast them with our approach, and highlight
our contributions.
6.1 Kinematic Model Fitting
Calibrating kinematic models of manipulation robots to sensor data has a long history in
system identification. A good overview of existing techniques can be found in the work
of Hollerbach, Khalil, and Gautier (2008). He, Zhao, Yang, and Yang (2010) recently
analyzed the identifiability of parameters for serial-chain manipulators, and proposed a
generic approach for calibration. Pradeep, Konolige, and Berger (2010) recently presented a
system implementation of sensor-actuator calibration for a complex service robot consisting
of two arms, a laser scanner and several cameras. In all of these works, the kinematic
model is specified in advance, and it is typically expected that an initial parameter set is
available. By taking multiple pose observations of the robot in different configurations,
the error between prediction and observation can be computed, and finally the parameter
vector can optimized by using non-linear, iterative least-squares methods.
In our case, neither the kinematic model nor an initial parameter set is available, but
needs to be estimated from the observations alone. In particular when the observations are
disturbed by noise and outliers, sample consensus methods have been proven to provide
robust estimates (Fischler & Bolles, 1981; Torr & Zisserman, 2000; Nister, 2005; Rusu,
Marton, Blodow, Dolha, & Beetz, 2008). In our model estimators, we use MLESAC as
first described by Torr and Zisserman (2000) which is  in contrast to least-squares fitting
 robust against reasonable amounts of outliers in the training data.
6.2 Kinematic Structure Selection
Estimating kinematic structure from observations has been studied intensively before, however, without subsequently using these models for robotic manipulation (Taycher, Fisher,
& Darrell, 2002; Kirk, OBrien, & Forsyth, 2004; Yan & Pollefeys, 2006; Ross, Tarlow, &
Zemel, 2008; Pekelny & Gotsman, 2008). Taycher et al. (2002) address the task of estimating the underlying topology of an observed articulated body. Their focus lies on recovering
the topology of the object rather than on learning a generative model. Also, compared to
their work, our approach can handle links with more complex link models, e.g., multiple
DOFs and non-parametric models. Kirk et al. (2004) extract human skeletal topologies using 3D markers from a motion capture system, however assuming that all joints are revolute.
Yan and Pollefeys (2006) present an approach for learning the structure of an articulated
object from feature trajectories under affine projections. Other researchers have addressed
the problem of identifying different object parts from image data. Ross et al. (2008) use
multi-body structure from motion to extract links from an image sequence and then fit an
articulated model to these links using maximum likelihood learning.
There exist several approaches where tracking articulated objects is the key motivation
and often an a-priori model is assumed. Krainin, Henry, Ren, and Fox (2010), for example,
described recently an approach for tracking articulated objects such as a manipulator using
a depth camera with a texture projector. However, they require a geometric model of the
manipulator. Kragic, Petersson, and Christensen (2002) describe an integrated navigation
system for mobile robots which includes a vision-based system for the detection of door
518

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

handles that enables the robot to successfully open doors. Anguelov, Koller, Parker, and
Thrun (2004) model doors as line segments that rotate around a hinge. EM is then used to
find the model parameters both from 2D range data and images. Nieuwenhuisen, Stuckler,
and Behnke (2010) describe an approach where a mobile robot increases its localization
accuracy by learning the positions of doors.
Although learning the structure of general Bayesian networks has been proven to be
NP-complete (Chickering, 1996), many approximate methods have been proposed that can
solve the structure search problem efficiently. Such methods include greedy search, iterated hill climbing, genetic algorithms and ant colony optimization (Chickering, 2002; Daly
& Shen, 2009). In some cases, the size of the search space can be reduced significantly by
evaluating a number of statistical independence tests (Margaritis & Thrun, 1999; Bromberg,
Margaritis, & Honavar, 2009). In this paper, we consider special Bayesian networks representing kinematic structures. This allows us to exploit the mutual independence of the edges
of kinematic trees for efficiently recovering the kinematic structure. For closed kinematic
chains, we use a greedy search heuristic similar as in the work of Chickering (2002).
Estimating both the structure and the models requires to trade off data fit with model
complexity. This corresponds to a Bayesian model selection problem, as described by
MacKay (2003). We approach this problem in our work using the Bayesian Information
Criterion (BIC) introduced by Schwarz (1978). The BIC provides a method for selecting
between alternate model hypotheses, based on their data likelihood and model complexity.
In this paper, we use the BIC both to select the kinematic models for the individual links
as well as for multi-part articulated objects.
6.3 Pose Estimation
Many approaches for estimating the pose of objects from sensory data have been proposed in
the past, but solving the general problem is still an ongoing research effort. Marker-based
approaches using active or passive markers have the advantage of being easy to use and
providing full 3D pose information, but require artificial markers to be attached upon the
object parts of interest. Early work in the area of articulated object tracking was presented
by Lowe (1991) under the assumption that the object model (and a good initialization)
is known. Nieuwenhuisen et al. (2010) use a 2D laser range finder for detecting doors in
an office environment and storing them on a map. Tilting lasers or line stripe systems
provide dense 3D point clouds and have been used for localizing doors and door handles,
but cannot deal with moving objects (Rusu, Meeussen, Chitta, & Beetz, 2009; Quigley,
Batra, Gould, Klingbeil, Le, Wellman, & Ng, 2009). Camera-based approaches can provide
higher frame rates. A good survey on the state-of-the-art in camera-based pose estimation
techniques can be found in the work of Lepetit and Fua (2005). In our context, the work
of Murillo, Kosecka, Guerrero, and Sagues (2008) and Andreopoulos and Tsotsos (2008)
on visual door detection and pose estimation is of particular relevance. Stereo systems
that employ matching algorithms to produce dense results provide 3D point clouds at video
frame rates, but suffer from occasional dropouts in areas with low texture or illumination
(Konolige, 1997; Brox, Rosenhahn, Gall, & Cremers, 2010; Wedel, Rabe, Vaudrey, Brox,
Franke, & Cremers, 2008). This can be overcome by active camera systems that add texture

519

fiSturm, Stachniss, & Burgard

to the scene using a projector LED. Two examples of such systems have been described by
Konolige (2010) and Fox and Ren (2010).
In our work, we use several different approaches for estimating the pose of an articulated
object for showing that our approach is not specific to a specific data source. In particular,
we use marker-based pose estimation from monocular camera, marker-less pose estimation
from stereo data, and proprioceptive tracking using the robots joint encoders.
6.4 Operating Articulated Objects
Several researchers have addressed the problem of operating articulated objects with robotic
manipulators. A large number of these techniques have focused on handling doors and drawers (Klingbeil et al., 2009; Kragic et al., 2002; Meeussen et al., 2010; Petrovskaya & Ng,
2007; Parlitz, Hagele, Kleint, Seifertt, & Dautenhahn, 2008; Niemeyer & Slotine, 1997;
Andreopoulos & Tsotsos, 2008; Rusu et al., 2009; Chitta, Cohen, & Likhachev, 2010). The
majority of these approaches, however, assumes an implicit kinematic model of the articulated object. Meeussen et al. (2010) describe an integrated navigation system for mobile
robots including vision- and laser-based detection of doors and door handles that enables
the robot to successfully open doors using a compliant arm. Diankov, Srinivasa, Ferguson,
and Kuffner (2008) formulate door and drawer operation as a kinematically constrained
planning problem and propose to use caging grasps to enlarge the configuration space,
and demonstrate this on an integrated system performing various fetch-and-carry tasks
(Srinivasa, Ferguson, Helfrich, Berenson, Romea, Diankov, Gallagher, Hollinger, Kuffner,
& Vandeweghe, 2010). Wieland et al. (2009) combine force and visual feedback to reduce
the interaction forces when opening kitchen cabinets and drawers. In contrast to our work,
these approaches make strong assumptions on the articulated objects, and do not deal with
the problem of inferring their kinematic structure. Therefore, they neither deal with unknown objects, nor improve their performance through learning. Katz and Brock (2008)
have enabled a robot to first interact with a planar kinematic object on a table in order
to visually learn a kinematic model, and then manipulate the object using this model to
achieve a goal state. In contrast to our work, their approach assumes planar objects and
learns only 2D models. Jain and Kemp (2009b, 2010) recently presented an approach that
enabled a robot to estimate the radius and location of the axis for rotary joints that move
in a plane parallel to the ground, while opening novel doors and drawers using equilibrium
point control. Recently, we combined (in collaboration with Jain and Kemp) the model
learning approach with the equilibrium point controller (Sturm et al., 2010). This enabled
the robot to operate a larger class of articulated objects, i.e., objects with non-vertical
rotation axes.

7. Conclusion
In this paper, we presented a novel approach for learning kinematic models of articulated
objects. Our approach infers the connectivity of rigid parts that constitute the object
including the articulation models of the individual links. To model the links, our approach
considers both, parametrized as well as parameter-free representations. In extensive studies
on synthetic and real data, we have evaluated the behavior of model estimation, model
selection, and structure discovery. We have shown that our approach is applicable to a
520

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

wide range of articulated objects, and that it can be used in conjunction with a variety of
different sensor modalities. Our approach enables mobile manipulators to operate unknown
articulated objects, learn their models, and improve over time.
Despite the promising results presented in this paper, there are several open research
questions that remain for future investigation. In our current approach, we learn the kinematic models from static pose observations. It would be interesting to include the velocities
or accelerations of object or body parts. This would allow the robot to learn the dynamic
parameters as well and enable it to plan time-optimal motion trajectories. A dynamical
model would enable the robot to accurately execute motions at higher speeds. Furthermore,
a robot that can measure forces and torques while actuating an object could additionally
learn friction and damping profiles and include this information in the learned model as
well. The robot could benefit from this information to assess, for example, whether a door
or drawer is jammed.

8. Acknowledgments
The authors gratefully acknowledge the help of Advait Jain and Charlie Kemp from Georgia
Tech, in particular for collaboration and joint development of the online model estimation
and control approach as described in Section 4.3, and for evaluating the approach on their
mobile manipulation robot Cody as described in Section 5.3. Further, the authors would like
to thank Vijay Pradeep and Kurt Konolige from Willow Garage who inspired the authors to
work on this subject, and contributed to the experiments with the motion capture device as
reported in Section 5.1. Additional thanks go to Kurt Konolige for the joint development of
the marker-less perception algorithm from stereo data as outlined in Section 4.2 as well as the
evaluation presented in Section 5.2. This work has partly been supported by the European
Commission under grant agreement numbers FP7-248258-First-MM, FP7-260026-TAPAS,
FP7-ICT-248873-RADHAR, and by the DFG under contract number SFB/TR-8.

References
Andreopoulos, A., & Tsotsos, J. K. (2008). Active vision for door localization and door
opening using playbot. In Proc. of the Canadian Conf. on Computer and Robot Vision
(CRV), pp. 310 Washington, DC, USA.
Anguelov, D., Koller, D., Parker, E., & Thrun, S. (2004). Detecting and modeling doors with
mobile robots. In Proc. of the IEEE Int. Conf. on Robotics & Automation (ICRA),
pp. 37773784.
Bishop, C. (2007). Pattern Recognition and Machine Learning (Information Science and
Statistics). Springer.
Bradski, G., & Kaehler, A. (2008). Learning OpenCV: Computer Vision with the OpenCV
Library. OReilly Media, Inc.
Bromberg, F., Margaritis, D., & Honavar, V. (2009). Efficient markov network structure
discovery using independence tests. Journal of Artificial Intelligence Research (JAIR),
35.
521

fiSturm, Stachniss, & Burgard

Brox, T., Rosenhahn, B., Gall, J., & Cremers, D. (2010). Combined region- and motionbased 3D tracking of rigid and articulated objects. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 32(2), 402415.
Chickering, D. M. (1996). Learning Bayesian networks is NP-Complete. In Fisher, D.,
& Lenz, H. (Eds.), Learning from Data: Artificial Intelligence and Statistics V, pp.
121130. Springer-Verlag.
Chickering, D. M. (2002). Learning equivalence classes of bayesian-network structures.
Journal of Machine Learning Research (JMLR), 2, 445498.
Chitta, S., Cohen, B., & Likhachev, M. (2010). Planning for autonomous door opening with
a mobile manipulator. In Proc. of the IEEE Int. Conf. on Robotics & Automation
(ICRA) Anchorage, AK, USA.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to Algorithms. MIT Press.
Daly, R., & Shen, Q. (2009). Learning bayesian network equivalence classes with ant colony
optimization. Journal of Artificial Intelligence Research (JAIR), 35, 391447.
Dellaert, F. (2005). Square Root SAM. In Proc. of Robotics: Science and Systems (RSS),
pp. 177184 Cambridge, MA, USA.
Diankov, R., Srinivasa, S., Ferguson, D., & Kuffner, J. (2008). Manipulation planning with
caging grasps. In Proc. of IEEE-RAS Intl. Conf. on Humanoid Robots (Humanoids)
Daejeon, Korea.
Featherstone, R., & Orin, D. (2008). Dynamics. In Siciliano, B., & Khatib, O. (Eds.),
Handbook of Robotics, pp. 3566. Springer, Secaucus, NJ, USA.
Fiala, M. (2005). Artag, a fiducial marker system using digital techniques. In Proc. of the
IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).
Fischler, M., & Bolles, R. (1981). Random sample consensus: a paradigm for model fitting
with application to image analysis and automated cartography. Commun. ACM., 24,
381395.
Fox, D., & Ren, X. (2010). Overview of RGB-D cameras and open research issues. In
Proceedings of the Workshop on Advanced Reasoning with Depth Cameras at Robotics:
Science and Systems Conference (RSS) Zaragoza, Spain.
Frese, U. (2006). Treemap: An o(logn) algorithm for indoor simultaneous localization and
mapping. Autonomous Robots, 21 (2), 103122.
Grisetti, G., Kummerle, R., Stachniss, C., Frese, U., & Hertzberg, C. (2010). Hierarchical
optimization on manifolds for online 2D and 3D mapping. In Proc. of the IEEE Int.
Conf. on Robotics and Automation (ICRA) Anchorage, AK, USA.
Grisetti, G., Stachniss, C., & Burgard, W. (2009). Non-linear constraint network optimization for efficient map learning. Trans. Intell. Transport. Sys., 10 (3), 428439.
522

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

He, R., Zhao, Y., Yang, S., & Yang, S. (2010). Kinematic-parameter identification for
serial-robot calibration based on poe formula. IEEE Transactions on Robotics, 26 (3),
411 423.
Hollerbach, J., Khalil, W., & Gautier, M. (2008). Model identification. In Siciliano, B., &
Khatib, O. (Eds.), Handbook of Robotics, pp. 321344. Springer, Secaucus, NJ, USA.
Jain, A., & Kemp, C. (2009a). Behavior-based door opening with equilibrium point control. In Proc. of the RSS Workshop on Mobile Manipulation in Human Environments
Seattle, WA, USA.
Jain, A., & Kemp, C. (2009b). Pulling open novel doors and drawers with equilibrium point
control. In Proc. of IEEE-RAS Intl. Conf. on Humanoid Robots (Humanoids) Paris,
France.
Jain, A., & Kemp, C. (2010). Pulling open doors and drawers: Coordinating an omnidirectional base and a compliant arm with equilibrium point control. In Proc. of the
IEEE Int. Conf. on Robotics & Automation (ICRA) Anchorage, AK, USA.
Katz, D., & Brock, O. (2008). Manipulating articulated objects with interactive perception.
In Proc. of Robotics: Science and Systems (RSS), pp. 272277 Pasadena, CA, USA.
Kirk, A., OBrien, J. F., & Forsyth, D. A. (2004). Skeletal parameter estimation from
optical motion capture data. In Proc. of the Int. Conf. on Computer Graphics and
Interactive Techniques (SIGGRAPH).
Klingbeil, E., Saxena, A., & Ng, A. (2009). Learning to open new doors. In Proc. of the
RSS Workshop on Robot Manipulation Seattle, WA, USA.
Konolige, K. (1997). Small vision systems: hardware and implementation. In Proc. of the
Int. Symp. on Robotics Research, pp. 111116.
Konolige, K. (2010). Projected texture stereo. In Proc. of the IEEE Int. Conf. on Robotics
& Automation (ICRA) Anchorage, AK, USA.
Kragic, D., Petersson, L., & Christensen, H. (2002). Visually guided manipulation tasks.
Robotics and Autonomous Systems, 40 (2-3), 193  203.
Krainin, M., Henry, P., Ren, X., & Fox, D. (2010). Manipulator and object tracking for in
hand model acquisition. In Proc. of the IEEE Int. Conf. on Robotics & Automation
(ICRA) Anchorage, AK, USA.
Lawrence, N. (2005). Probabilistic non-linear principal component analysis with gaussian
process latent variable models. J. Mach. Learn. Res., 6, 17831816.
Lepetit, V., & Fua, P. (2005). Monocular model-based 3d tracking of rigid objects. Foundations and Trends in Computer Graphics and Vision, 1, 189.
Lowe, D. (1991). Fitting parameterized three-dimensional models to images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13, 441450.
523

fiSturm, Stachniss, & Burgard

Lu, F., & Milios, E. (1997). Globally consistent range scan alignment for environment
mapping. Autonomous Robots, 4, 333349.
MacKay, D. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge
University Press.
Margaritis, D., & Thrun, S. (1999). Bayesian network induction via local neighborhoods.
In Proc. of the Conf. on Neural Information Processing Systems (NIPS), pp. 505511.
MIT Press.
McClung, A., Zheng, Y., & Morrell, J. (2010). Contact feature extraction on a balancing
manipulation platform. In Proc. of the IEEE Int. Conf. on Robotics & Automation
(ICRA).
Meeussen, W., Wise, M., Glaser, S., Chitta, S., McGann, C., Patrick, M., Marder-Eppstein,
E., Muja, M., Eruhimov, V., Foote, T., Hsu, J., Rusu, R., Marthi, B., Bradski, G.,
Konolige, K., Gerkey, B., & Berger, E. (2010). Autonomous door opening and plugging
in with a personal robot. In Proc. of the IEEE Int. Conf. on Robotics & Automation
(ICRA) Anchorage, AK, USA.
Murillo, A. C., Kosecka, J., Guerrero, J. J., & Sagues, C. (2008). Visual door detection
integrating appearance and shape cues. Robotics and Autonomous Systems, 56(6),
pp. 512521.
Niemeyer, G., & Slotine, J.-J. (1997). A simple strategy for opening an unknown door. In
Proc. of the IEEE Int. Conf. on Robotics & Automation (ICRA) Albuquerque, NM,
USA.
Nieuwenhuisen, M., Stuckler, J., & Behnke, S. (2010). Improving indoor navigation of
autonomous robots by an explicit representation of doors. In Proc. of the IEEE
Int. Conf. on Robotics & Automation (ICRA) Anchorage, AK, USA.
Nister, D. (2005). Preemptive ransac for live structure and motion estimation. Mach. Vision
Appl., 16 (5), 321329.
Parlitz, C., Hagele, M., Kleint, P., Seifertt, J., & Dautenhahn, K. (2008). Care-o-bot 3
- rationale for human-robot interaction design. In Proc. of the Int. Symposium on
Robotics (ISR) Seoul, Korea.
Pekelny, Y., & Gotsman, C. (2008). Articulated object reconstruction and markerless motion capture from depth video. Computer Graphics Forum, 27 (2), 399408.
Petrovskaya, A., & Ng, A. (2007). Probabilistic mobile manipulation in dynamic environments, with application to opening doors. In Proc. of the Int. Conf. on Artificial
Intelligence (IJCAI) Hyderabad, India.
Pradeep, V., Konolige, K., & Berger, E. (2010). Calibrating a multi-arm multi-sensor robot:
A bundle adjustment approach. In Int. Symp. on Experimental Robotics (ISER) New
Delhi, India.
524

fiA Probabilistic Framework for Learning Kinematic Models of Articulated Objects

Quigley, M., Batra, S., Gould, S., Klingbeil, E., Le, Q., Wellman, A., & Ng, A. (2009).
High-accuracy 3D sensing for mobile manipulation: Improving object detection and
door opening. In Proc. of the IEEE Int. Conf. on Robotics & Automation (ICRA)
Kobe, Japan.
Rasmussen, C., & Williams, C. (2006). Gaussian Processes for Machine Learning. The
MIT Press, Cambridge, MA.
Ross, D., Tarlow, D., & Zemel, R. (2008). Unsupervised learning of skeletons from motion.
In Proc. of the European Conf. on Computer Vision (ECCV) Marseille, France.
Roweis, S., & Saul, L. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, 290 (5500), 23232326.
Rusu, R. B., Meeussen, W., Chitta, S., & Beetz, M. (2009). Laser-based perception for door
and handle identification. In Proc. of the Int. Conf. on Advanced Robotics (ICAR)
Munich, Germany.
Rusu, R. B., Marton, Z. C., Blodow, N., Dolha, M., & Beetz, M. (2008). Towards 3D point
cloud based object maps for household environments. Robot. Auton. Syst., 56 (11),
927941.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6 (2).
Sentis, L., Park, J., & Khatib, O. (2010). Compliant control of multi-contact and center of
mass behaviors in humanoid robots. IEEE Trans. on Robotics, 26 (3), 483501.
Srinivasa, S., Ferguson, D., Helfrich, C., Berenson, D., Romea, A. C., Diankov, R., Gallagher, G., Hollinger, G., Kuffner, J., & Vandeweghe, J. M. (2010). HERB: a home
exploring robotic butler. Autonomous Robots, 28 (1), 520.
Sturm, J., Konolige, K., Stachniss, C., & Burgard, W. (2010). Vision-based detection for
learning articulation models of cabinet doors and drawers in household environments.
In Proc. of the IEEE Int. Conf. on Robotics & Automation (ICRA) Anchorage, AK,
USA.
Sturm, J., Pradeep, V., Stachniss, C., Plagemann, C., Konolige, K., & Burgard, W. (2009).
Learning kinematic models for articulated objects. In Proc. of the Int. Joint Conf. on
Artificial Intelligence (IJCAI) Pasadena, CA, USA.
Sturm, J., Jain, A., Stachniss, C., Kemp, C., & Burgard, W. (2010). Operating articulated objects based on experience. In Proc. of the IEEE International Conference on
Intelligent Robot Systems (IROS) Taipei, Taiwan.
Taycher, L., Fisher, J., & Darrell, T. (2002). Recovering articulated model topology from
observed rigid motion. In Proc. of the Conf. on Neural Information Processing Systems
(NIPS) Vancouver, Canada.
Tenenbaum, J., de Silva, V., & Langford, J. (2000). A global geometric framework for
nonlinear dimensionality reduction.. Science, 290 (5500), 23192323.
525

fiSturm, Stachniss, & Burgard

Torr, P. H. S., & Zisserman, A. (2000). Mlesac: A new robust estimator with application
to estimating image geometry. Computer Vision and Image Understanding, 78, 2000.
Wedel, A., Rabe, C., Vaudrey, T., Brox, T., Franke, U., & Cremers, D. (2008). Efficient
dense scene flow from sparse or dense stereo data. In Proc. of the European Conf. on
Computer Vision (ECCV) Marseille, France.
Wieland, S., Gonzalez-Aguirre, D., Vahrenkamp, N., Asfour, T., & Dillmann, R. (2009).
Combining force and visual feedback for physical interaction tasks in humanoid robots.
In Proc. of IEEE-RAS Intl. Conf. on Humanoid Robots (Humanoids) Paris, France.
Yan, J., & Pollefeys, M. (2006). Automatic kinematic chain building from feature trajectories of articulated objects. In Proc. of the IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR) Washington, DC, USA.

526

fiJournal of Artificial Intelligence Research 41 (2011) 329-365

Submitted 02/11; published 06/11

Sequential Diagnosis by Abstraction
Sajjad Siddiqi
National University of Sciences and Technology
(NUST) Islamabad, Pakistan

sajjad.ahmed@seecs.edu.pk

Jinbo Huang
NICTA and Australian National University
Canberra, Australia

jinbo.huang@nicta.com.au

Abstract
When a system behaves abnormally, sequential diagnosis takes a sequence of measurements of the system until the faults causing the abnormality are identified, and the goal
is to reduce the diagnostic cost, defined here as the number of measurements. To propose
measurement points, previous work employs a heuristic based on reducing the entropy over
a computed set of diagnoses. This approach generally has good performance in terms of
diagnostic cost, but can fail to diagnose large systems when the set of diagnoses is too
large. Focusing on a smaller set of probable diagnoses scales the approach but generally
leads to increased average diagnostic costs. In this paper, we propose a new diagnostic
framework employing four new techniques, which scales to much larger systems with good
performance in terms of diagnostic cost. First, we propose a new heuristic for measurement
point selection that can be computed efficiently, without requiring the set of diagnoses, once
the system is modeled as a Bayesian network and compiled into a logical form known as
d-DNNF. Second, we extend hierarchical diagnosis, a technique based on system abstraction from our previous work, to handle probabilities so that it can be applied to sequential
diagnosis to allow larger systems to be diagnosed. Third, for the largest systems where
even hierarchical diagnosis fails, we propose a novel method that converts the system into
one that has a smaller abstraction and whose diagnoses form a superset of those of the
original system; the new system can then be diagnosed and the result mapped back to
the original system. Finally, we propose a novel cost estimation function which can be
used to choose an abstraction of the system that is more likely to provide optimal average
cost. Experiments with ISCAS-85 benchmark circuits indicate that our approach scales
to all circuits in the suite except one that has a flat structure not susceptible to useful
abstraction.

1. Introduction
When a system behaves abnormally, the task of diagnosis is to identify the reasons for
the abnormality. For example, in the combinational circuit in Figure 1, given the inputs
P  Q  R, the output V should be 0, but is actually 1 due to the faults at gates J and
B. Given a system comprising a set of components, and a knowledge base modeling the
behavior of the system, along with the (abnormal) observed values of some system variables,
a (consistency-based) diagnosis is a set of components whose failure (assuming the other
components to be healthy) together with the observation is logically consistent with the
system model. In our example, {V }, {K}, {A}, and {J, B} are some of the diagnoses given
c
2011
AI Access Foundation. All rights reserved.

fiSiddiqi & Huang

AND

BUFFER

1
P

1
Q

NOT

1

1
A

J

1
B

OR

1

1
D

V

1
K

0
R

Figure 1: A faulty circuit.

the observation. In general, the number of diagnoses can be exponential in the number of
system components, and only one of them will correspond to the set of actual faults.
In this paper, therefore, we consider the problem of sequential diagnosis (de Kleer &
Williams, 1987), where a sequence of measurements of system variables is taken until the
actual faults are identified. The goal is to reduce the diagnostic cost, defined here as the
number of measurements. To propose measurement points, the state-of-the-art gde (general
diagnosis engine) framework (de Kleer & Williams, 1987; de Kleer, Raiman, & Shirley, 1992;
de Kleer, 2006) considers a heuristic based on reducing the entropy over a set of computed
diagnoses. This approach generally has good performance in terms of diagnostic cost, but
can fail to diagnose large systems when the set of diagnoses is too large (de Kleer & Williams,
1987; de Kleer et al., 1992; de Kleer, 2006). Focusing on a smaller set of probable diagnoses
scales the approach but generally leads to increased average diagnostic costs (de Kleer,
1992).
We propose a new diagnostic framework employing four new techniques, which scales to
much larger systems with good performance in terms of diagnostic cost. First, we propose a
new heuristic that does not require computing the entropy of diagnoses. Instead we consider
the entropies of the system variables to be measured as well as the posterior probabilities
of component failures. The idea is to select a component that has the highest posterior
probability of failure (Heckerman, Breese, & Rommelse, 1995) and from the variables of
that component, measure the one that has the highest entropy. To compute probabilities,
we exploit system structure so that a joint probability distribution over the faults and
system variables is represented compactly as a Bayesian network (Pearl, 1988), which is then
compiled into deterministic decomposable negation normal form (d-DNNF) (Darwiche, 2001;
Darwiche & Marquis, 2002). d-DNNF is a logical form that can exploit the structure present
in many systems to achieve compactness and be used to compute probabilistic queries
efficiently. Specifically, all the required posterior probabilities can be exactly computed by
evaluating and differentiating the d-DNNF in time linear in the d-DNNF size (Darwiche,
2003).
330

fiSequential Diagnosis by Abstraction

Second, we extend hierarchical diagnosis, a technique from our previous work (Siddiqi
& Huang, 2007), to handle probabilities so that it can be applied to sequential diagnosis to
allow larger systems to be diagnosed. Specifically, self-contained subsystems, called cones,
are treated as single components and diagnosed only if they are found to be faulty in the
top-level diagnosis. This significantly reduces the number of system components, allowing
larger systems to be compiled and diagnosed. For example, the subcircuit in the dotted box
in Figure 1 is a cone (with A as output and {P, D} as inputs) which contains a fault. First,
cone A, as a whole, is determined as faulty. It is only then that A is compiled separately
and diagnosed. In previous work (Siddiqi & Huang, 2007) we only dealt with the task of
computing diagnoses, which did not involve measurements or probabilities; in the present
paper, we present several extensions that allow the technique to carry over to sequential
diagnosis.
Third, when the abstraction of a system is still too large to be compiled and diagnosed,
we use a novel structure based technique called cloning, which systematically modifies the
structure of a given system C to obtain a new system C0 that has a smaller abstraction
and whose diagnoses form a super-set of those of the original system; the new system can
then be diagnosed and the result mapped back to the original system. The idea is to select
a system component G that is not part of a cone and hence cannot be abstracted away in
hierarchical diagnosis, create one or more clones of G, and distribute Gs parents (from a
graph point of view) among the clones, in such a way that G and its clones now become parts
of cones and disappear from the abstraction. Repeated applications of this operation can
allow an otherwise unmanageable system to have a small enough abstraction for diagnosis
to succeed.
Finally, we propose a novel cost estimation function that can predict the expected
diagnostic cost when a given abstraction of the system is used for diagnosis. Our aim is
to find an abstraction of the system that is more likely to give optimal average cost. For
this purpose, we use this function on various abstractions of the system where different
abstractions are obtained by destroying different cones in the system (by destroying a
cone we mean to overlook the fact that it is a cone and include all its components in the
abstraction). The abstraction with the lowest predicted cost can then be used for the actual
diagnosis.
Experiments on ISCAS-85 benchmark circuits (Brglez & Fujiwara, 1985) indicate that
we can solve for the first time nontrivial multiple-fault diagnostic cases on all the benchmarks, with good diagnostic costs, except one circuit that has a flat structure not susceptible
to useful abstraction, and the new cost estimation function can often accurately predict the
abstraction which is more likely to give optimal average cost.

2. Background and Previous Work
Suppose that the system to be diagnosed is formally modeled by a joint probability distribution P r(X  H) over a set of variables partitioned into X and H. Variables X are
those whose values can be either observed or measured, and variables H are the health variables, one for each component describing its health mode. The joint probability distribution
P r(X  H) defines a set of system states.
331

fiSiddiqi & Huang

Diagnosis starts in the initial (belief) state
I0 = P r(X  H | Xo = xo )

(1)

where values xo of some variables Xo  X (we are using boldface uppercase letters to mean
both sets and vectors) are given by the observation, and we wish to reach a goal state
In = P r(X  H | Xo = xo , Xm = xm )

(2)

after measuring the values xm of some variables Xm  X\Xo , |Xm | = n, one at a time,
such that (the boldface 0 and 1 denote vectors of 0s and 1s):
Hf  H, P r(Hf = 0 | Xo = xo , Xm = xm ) = 1 and
P r(Hf = 0, H\Hf = 1 | Xo = xo , Xm = xm ) > 0.
That is, in a goal state a set of components Hf are known to be faulty with certainty
and no logical inconsistency arises if all other components are assumed to be healthy. Other
types of goal conditions are possible. For example, if the health states of all components are
to be determined with certainty, the condition will be that P r(H = 0 | Xo = xo , Xm = xm )
is 0 or 1 for all H  H (such goals are only possible to reach if strong fault models are given,
where strong fault models are explicit descriptions of abnormal behavior, as opposed to
weak fault models where only the normal behavior is known).
Two special cases are worth mentioning: (1) If the initial state I0 satisfies the goal
condition with Hf =  then the observation is normal and no diagnosis is required. (2)
If the initial state I0 satisfies the goal condition with some Hf 6= , then the observation
is abnormal but the diagnosis is already completed (assuming that we are able to check
probabilities as necessary); in other words, a sequence of length 0 solves the problem.
Following de Kleer and Williams (1987) we assume that all measurements have unit
cost. Hence the objective is to reach a goal state in the fewest measurements possible.
The classical gde framework, on receiving an abnormal observation Xo = xo , considers
the Shannons entropy of the probability distribution over a set of computed diagnoses,
which is either the set of minimum-cardinality diagnoses or a set of probable/leading diagnoses. It proposes to measure a variable X whose value will reduce that entropy the most,
on average. The idea is that the probability distribution over the diagnoses reflects the
uncertainty over the actual faults, and the entropy captures the amount of this uncertainty.
After a measurement is taken the entropy is updated by updating the posterior probabilities
of the diagnoses, potentially reducing some of them to 0.
The results reported by de Kleer et al. (1992) involving single-fault cases for ISCAS-85
circuits indicate that this method leads to measurement costs close to those of optimal
policies. However, a major drawback is that it can be impractical when the number of
diagnoses is large (e.g., the set of minimum-cardinality diagnoses can be exponentially
large). Focusing on a smaller set of probable diagnoses scales the approach but can increase
the likelihood of irrelevant measurements and generally leads to increased average diagnostic
costs (de Kleer, 1992).
From here on, we shall use combinational circuits as an example of the type of systems
we wish to diagnose. Our approach, however, applies as well to other types of systems as
332

fiSequential Diagnosis by Abstraction

P
1
0

P
0.5
0.5

P
1
1
1
1
0
0
0
0

okJ
1
1
0
0
1
1
0
0

okJ
1
0
J
1
0
1
0
1
0
1
0

okJ
0.9
0.1

J|P,okJ
0
1
0.5
0.5
1
0
0.5
0.5

Figure 2: Bayesian network for the circuit in Figure 1 (left). CPTs for nodes P , J, and
okJ (right).

long as a probabilistic model is given that defines the behavior of the system. In Sections 4
and 5 we will present the new techniques we have introduced to significantly enhance the
scalability of sequential diagnosis. We start, however, by presenting in the following section
the system modeling and compilation method that underlies our new diagnostic system.

3. System Modeling and Compilation
In order to define a joint probability distribution P r(X  H) over the system behavior, we
first assume that the prior probability of failure P r(H = 0) is given for each component
H  H as part of the input to the diagnosis task (de Kleer & Williams, 1987). For example,
the small table with two entries on the top-right of Figure 2 gives the prior probability of
failure for gate J as 0.1.
3.1 Conditional Probability Tables
Prior fault probabilities alone do not define the joint probability distribution P r(X  H).
In addition, we need to specify for each component how its output is related to its inputs
and health mode. A conditional probability table (CPT) for each component does this job.
The CPT shown on the bottom (right) of Figure 2, for example, defines the behavior
of gate J: Each entry gives the probability of its output (J) being a particular value given
the value of its input (P ) and the value of its health variable (okJ). In case okJ = 1,
the probabilities are always 0 or 1 as the behavior of a healthy gate is deterministic. The
case of okJ = 0 defines the fault model of the gate, which is also part of the input to the
diagnosis task. In our example, we assume that both output values have probability 0.5
when the gate is broken. For simplicity we assume that all gates have two health modes
333

fiSiddiqi & Huang

(i.e., each health variable is binary); the encoding and compilation to be described later,
however, allows an arbitrary number of health modes.
Given these tables, the joint probability distribution over the circuit behavior can be
obtained by realizing that the gates of a circuit satisfy an independence property, known as
the Markov property: Given its inputs and health mode, the output of a gate is independent
of any wire which is not a descendant of the gate (a wire X is a descendant of a gate Y if X
can be reached following a path from Y to an output of the circuit in the direction towards
the circuit outputs). This means that the circuit can be effectively treated as a Bayesian
network in the straightforward way, by having a node for each wire and each health variable,
and having an edge going from each input of a gate to its output, and also from the health
variable of a gate to its output. Figure 2 shows the result of this translation for the circuit
in Figure 1.
The joint probability distribution encoded in the Bayesian network provides the basis
for computing any posterior probabilities that we may need when proposing measurement
points (by the chain rule). However, it does not provide an efficient way of doing so.
Specifically, computing a posterior P r(X = x | Y = y) given the values y of all the variables
Y with known values involves summing out all variables other than X and Y, which has a
complexity exponential in the number of such variables if done naively.
3.2 Propositional Modeling
It is known that a Bayesian network can be encoded into a logical formula and compiled
into d-DNNF, which, if successful, allows posterior probabilities of all variables to be computed efficiently (Darwiche, 2003). For the purposes of sequential diagnosis, we encode the
Bayesian network as follows.
Consider the subcircuit in the dotted box in Figure 1 as an example, which can be
modeled as the following formula:
okJ  (J  P ), okA  (A  (J  D)).
Specifically, each signal of the circuit translates into a propositional variable (A, D,
P , J), and for each gate, an extra variable is introduced to model its health (okA, okJ).
The formula is such that when all health variables are true the remaining variables are
constrained to model the functionality of the gates. In general, for each component X, we
have okX  NormalBehavior(X).
Note that the above formula fails to encode half of the CPT entries, where okJ = 0. In
order to complete the encoding of the CPT of node J, we introduce an extra Boolean variable
J , and write okJ  (J  J ). Finally, the health variables (okA, okJ) are associated
with the probabilities of the respective gates being healthy (0.9 in our experiments), and
each -variable (J ) is associated with the probability of the corresponding gate giving an
output of 1 when broken (0.5 in our experiments; thus assuming that the output of a faulty
gate is probabilistically independent of its inputs).
The above encoding of the circuit is similar to the encoding of Bayesian networks described by Darwiche (2003) in the following way: According to the encoding by Darwiche,
for every node in a Bayesian network and for every value of it there is an indicator variable.
Similarly for every conditional probability there is a network parameter variable. In our
334

fiSequential Diagnosis by Abstraction

encoding, the variables for the wires are analogous to the network indicators, where the
encoding is optimized such that there is a single indicator for both values of the wire. Also,
our encoding exploits the logical constraints and does not generate network parameters for
zeros and ones in the CPT. Finally, the encoding for a node that represents a health variable has been optimized such that we only need a single ok-variable which serves both as
an indicator and as a network parameter.
Once all components are encoded as described above, the union (conjunction) of the
formulas is compiled into d-DNNF. The required probabilities can be exactly computed
by evaluating and differentiating the d-DNNF in time linear in its size (Darwiche, 2003).
Details of the compilation process are discussed by Darwiche (2004), and the computation
of probabilities is described in Appendix A.
We now present our hierarchical diagnosis approach and propose a new measurement
selection heuristic.

4. Hierarchical Sequential Diagnosis
An optimal solution to sequential diagnosis would be a policy, that is, a plan of measurements conditioned on previous measurement outcomes, where each path in the plan leads
to a diagnosis of the system (Heckerman et al., 1995). As computing optimal policies is
intractable in general, we follow the approach of heuristic measurement point selection as
in previous work.
We start with a definition of Shannons entropy , which is defined with respect to a
probability distribution of a discrete random variable X ranging over values x1 , x2 , . . . , xk .
Formally:
k
X
(X) = 
P r(X = xi ) log P r(X = xi ).
(3)
i=1

Entropy measures the amount of uncertainty over the value of the random variable. It
is maximal when all probabilities P r(X = xi ) are equal, and minimal when one of the
probabilities is 1, corresponding nicely to our intuitive notion of the degree of uncertainty.
In gde the entropy is computed for the probability distribution over the set of computed
diagnoses (i.e., the value of the random variable X here ranges over the set of diagnoses).
As mentioned earlier, this entropy can be difficult to compute when the number of diagnoses
is large (de Kleer & Williams, 1987; de Kleer, 2006).
4.1 Baseline Approach
Able to compute probabilities efficiently and exactly following successful d-DNNF compilation, we now propose a new two-part heuristic that circumvents this limitation in scalability.
First, we consider the entropy of a candidate variable to be measured.
4.1.1 Heuristic Based on Entropy of Variable
Since a wire X only has two values, its entropy can be written as:
(X) = (px log px + px log px )
335

(4)

fiSiddiqi & Huang

where px = P r(X = 1 | Y = y) and px = P r(X = 0 | Y = y) are the posterior probabilities
of X having values 1 and 0, respectively, given the values y of wires Y whose values are
known.
While (X) captures the uncertainty over the value of the variable, we can also interpret
it as the expected amount of information gain provided by measuring the variable. Hence
as a first idea we consider selecting a variable with maximal entropy for measurement at
each step.
4.1.2 Improving Heuristic Accuracy
This idea alone, however, did not work very well in our initial experiments. As would be
confirmed by subsequent experiments, this is largely due to the fact that the (implicit) space
of all diagnoses is generally very large and can include a large number of unlikely diagnoses,
which tends to compromise the accuracy of the information gain provided by the entropy.
The experiments to confirm this explanation are as follows.
When the d-DNNF compilation is produced, and before it is used to compute probabilities, we prune the d-DNNF graph so that models (satisfying variable assignments)
corresponding to diagnoses with more than k broken components are removed.1 We set the
initial k to the number of actual faults in the experiments, and observed that a significant
reduction of diagnostic cost resulted in almost all cases. This improved performance is apparently due to the fact that the pruning updates the posterior probabilities of all variables,
making them more accurate since many unlikely diagnoses have been eliminated.
In practice, however, the number of faults is not known beforehand and choosing an
appropriate k for the pruning can be nontrivial (note that k need not be exactly the same
as the number of actual faults for the pruning to help). Interestingly, the following heuristic,
which is the one we will actually use, appears to achieve a similar performance gain in an
automatic way: We select a component that has the highest posterior probability of failure
(an idea from Heckerman et al., 1995; see Section 8), and then from the variables of that
component, measure the one that has the highest entropy. This heuristic does not require
the above pruning of the d-DNNF, and appears to improve the diagnostic cost to a similar
extent by focusing the measurement selection on the component most likely to be broken
(empirical results to this effect are given and discussed in Section 7.1).
4.1.3 The Algorithm
We start by encoding the system as a logical formula as discussed in Section 3, where a
subset of the variables are associated with numbers representing the prior fault probabilities
and probabilities involved in the fault models of the components, which is then compiled
into d-DNNF .
The overall sequential diagnosis process we propose is summarized in Algorithm 1. The
inputs are a system C, its d-DNNF compilation , the set of faults D (which is empty
but will be used in the hierarchical approach), a set of known values y of variables, and
an integer k specifying the fault cardinality bound (this is for running the model pruning
experiments described in Section 4.1.2, and is not required for diagnosis using our final
1. A complete pruning is not easy; however, an approximation can be achieved in time linear in the d-DNNF
size, by a variant of the minimization procedure described by Darwiche (2001); see Appendix B.

336

fiSequential Diagnosis by Abstraction

Algorithm 1 Probabilistic sequential diagnosis
function psd(C, , D, y, k)
inputs: {C: system}, {: d-DNNF}, {y: measurements}, {k: fault cardinality}, {D: ordered set
of known faults}
output: {pair< D , y >}
1:   Reduce ( , D, k  |D| ) if D has changed
2: Given y on variables Y, Evaluate (, y) to obtain P r(y)
3: Differentiate () to obtain P r(X = 1, y)  variables X
4: Deduce fault as D = D  {X : P r(okX = 1, y) = 0}
5: if D has changed && MeetsCriteria(,D,y) then
6:
return < D , y >
7: Measure variable X which is the best under a given heuristic
8: Add the measured value x of X to y, and go back to line 1

heuristic). We reduce  by pruning some models (line 1) when the fault cardinality bound
k is given, using the function reduce(, D, k  |D|). reduce accepts as arguments the
current DNNF , the set of known faults D, and the upper bound given by k  D on the
cardinality of remaining faults, whereas it returns the pruned DNNF. Reduce excludes the
known faults in D when computing the minimum cardinality of , and then uses k  |D|
as the bound on the remaining faults (explained further in Appendix B).  is reduced first
time when psd is called and later each time D is changed (i.e., when a component is found
faulty). We then evaluate (line 2) and differentiate (line 3)  (see Appendix A), select a
measurement point and take the measurement (line 7), and repeat the process (line 8) until
the stopping criteria are met (line 5).
The stopping criteria on line 5 are given earlier in Section 2 as the goal condition, i.e.,
we stop when the abnormal observation is explained by all the faulty components D already
identified assuming that other components are healthy. A faulty component X is identified
when P r(okX = 1, y) = 0 where y are the values of variables that are already known,
and as mentioned earlier these probabilities are obtained for all variables simultaneously in
the d-DNNF differentiation process. Finally, the condition that the current set of faulty
components, with health modes Hf , explains the observation is satisfied when P r(Hf =
0, H\Hf = 1, y) > 0, which is checked by a single evaluation of the original d-DNNF. The
algorithm returns the actual faults together with the new set of known values of variables
(line 6).
4.2 Hierarchical Approach
We now scale our approach to handle larger systems using the idea of abstraction-based
hierarchical diagnosis (Siddiqi & Huang, 2007). The basic idea is that the compilation of
the system model into d-DNNF will be more efficient and scalable when the number of
system components is reduced. This can be achieved by abstraction, where subsystems,
known as cones, are treated as single components. An example of a cone is depicted in
Figure 1. The objective here is to use a single health variable and failure probability for
the entire cone, hence significantly reducing the size of the encoding and the difficulty of
compilation. Once a cone is identified as faulty in the top-level diagnosis, it can then be
compiled and diagnosed, in a recursive fashion.
337

fiSiddiqi & Huang

We now give formal definition of abstraction from our previous work:
4.2.1 Abstraction of System
Abstraction is based upon the structural dominators (Kirkland & Mercer, 1987) of a system.
A component X dominates a component Y , or X is called a dominator of Y , if any path
from Y to any output of the system contains X. A cone corresponds precisely to the set
of components dominated by a component. A cone may contain further cones leading to a
hierarchy of cones.
A system can be abstracted by treating all maximal cones in it as black boxes (a maximal
cone is one that is either contained in no other cone or contained in exactly one other cone
which is the whole system). In our example, cone A can be treated as a virtual gate with
two inputs {P, D} and the output A. The abstraction of a system can be formally defined
as:
Definition 1 (Abstraction of System). Given a system C, let C0 = C if C has a single
output; otherwise let C0 be C augmented with a dummy component collecting all outputs
of C. Let O be the only output of C0 . The abstraction AC of system C is then the set of
components X  C such that X is not dominated in C0 by any component other than X
and O.
For example, AC = {A, B, D, K, V }. J 6 AC as J cannot reach any output without
passing through A, which is a dominator of J.
In our previous work (Siddiqi & Huang, 2007), we only dealt with the task of computing minimum-cardinality diagnoses, which does not involve probabilities or measurement
selection. In the context of sequential diagnosis, several additional techniques have been
introduced, particularly in the computation of prior failure probabilities for the cones and
the way measurement points are selected, outlined below.
4.2.2 Propositional Encoding
We start with a discussion of the hierarchical encoding for probabilistic reasoning, which is
similar to the hierarchical encoding presented in our previous work (Siddiqi & Huang, 2007).
Specifically, for the diagnosis of the abstraction AC of the given system C, health variables
are only associated with the components AC \IC , which are the gates {A, B, D, K, V } in
our example (IC stands for the set of inputs of the system C). Thus the gate J in Figure 1
will not be associated with a health variable, as J is a wire internal to the cone rooted
at A. Consequently, only the nodes representing the components AC \IC will have health
nodes associated with them in the corresponding Bayesian network. Hence the node okJ is
removed from the Bayesian network in Figure 2.
In addition, we define the failure of a cone to be when it outputs the wrong value, and
introduce extra clauses to model the abnormal behavior of the cone. For example, the
encoding given in Section 3.2 for cone A in Figure 1 (in the dotted box) is as follows:
J  P, okA  (A  (J  D)), okA  (A 6 (J  D))
The first part of the formula encodes the normal behavior of gate J (without a health
variable); the next encodes the normal behavior of the cone; the last encodes that the
338

fiSequential Diagnosis by Abstraction

cone outputs a wrong value when it fails. Other gates (that are not roots of cones) in the
abstraction AC are encoded normally as described in Section 3.2.
Note that the formulas for all the components in a cone together encode a single CPT
for the whole cone, which provides the conditional probability of the cones output given
the health and inputs of the cone, instead of the health and inputs of the component at
the root of the cone. For example, the above encoding is meant to provide the conditional
probability of A given P , D, and okA (instead of J, D, and okA), where okA represents
the health mode of the whole cone and is associated with its prior failure probability, which
is initially unknown to us and has to be computed for all cones (explained below). Such
an encoding of the whole system provides a joint probability distribution over the variables
AC  IC  H, where H = {okX | X  AC \IC }.
4.2.3 Prior Failure Probabilities for Cones
When a cone is treated as a single component, its prior probability of failure as a whole can
be computed given the prior probabilities of components and cones inside it. We do this by
creating two copies h and f of the cone, where h models only the healthy behavior of
the cone (without health variables), and f includes the faulty behavior as well (i.e., the
full encoding described in Section 3.2). The outputs of both h and f are collected into
an XOR-gate X(when the output of XOR-gate X equals 1, both of its inputs are forced to
be different in value). We then compute the probability P r(X = 1) giving the probability
of the outputs of h and f being different. The probability is computed by compiling this
encoding into d-DNNF and evaluating it under X = 1.
Note that this procedure itself is also abstraction-based and hierarchical, performed
bottom-up with the probabilities for the inner cones computed before those for the outer
ones. Also note that it is performed only once per system as a pre-processing step.
4.2.4 Measurement Point Selection and Stopping Criteria
In principle, the heuristic to select variables for measurement and the stopping criteria are
the same as in the baseline approach; however, a couple of details are worth mentioning.
First, when diagnosing the abstraction of a given system (or cone) C, the measurement
candidates are restricted to variables AC IC , ignoring the internal variables of the maximal
conesthose are only measured if a cone as a whole has been found faulty.
Second, it is generally important to have full knowledge of the values of cones inputs
before a final diagnosis of the cone is concluded. A diagnosis of a cone concluded with only
partial knowledge of its inputs may not include some faults that are vital to the validity of
global diagnosis. The reason is that the diagnosis of the cone assumes that the unknown
inputs can take either value, while in reality their values may become fixed when variables
in other parts of the system are measured, causing the diagnosis of certain cones to become
invalid, and possibly requiring the affected cones to be diagnosed once again to meet the
global stopping criteria (see line 17 in Algorithm 2).
To avoid this situation while retaining the effectiveness of the heuristic, we modify the
measurement point selection as follows when diagnosing a cone. After selecting a component
with the highest probability of failure, we consider the variables of that component plus the
inputs of the cone, and measure the one with the highest entropy. We do not conclude a
339

fiSiddiqi & Huang

Algorithm 2 Hierarchical probabilistic sequential diagnosis
function hpsd(C, uC , k)
inputs: {C : system},{uC : obs. across system} {k: fault cardinality}
local variables: {B, D, T : set of components} {y, z, uG : set of measurements} {i, k 0 : integer}
output: {pair< D , uC >}
1:   Compile2dDNNF (AC , uC )
2: i  0 , D   , y  uC
3: < B, y > psd (C, , B, y, k)
4: for {; i < |B|; i + +} do
5:
G Element (B, i)
6:
if G is a cone then
7:
z  y  Implications (, y)
8:
uG  {x : x  z, X  IG  OG }
9:
k 0  k  |D|  |B| + i + 2
10:
< T, uG > hpsd(DG  IG , uG , k 0 )
11:
y  y  uG , D  D  T
12:
Evaluate (, y), Differentiate (  )
13:
else
14:
D  D  {G}
15: z  y  Implications (, y)
16: uC  uC  {x : x  z, X  IC  OC }
17: if MeetsCriteria (C, D, y) then
18:
return < D , uC >
19: else
20:
goto line 3

diagnosis for the cone until values of all its inputs become known (through measurement or
deduction), except when the health of all the components in the cone has been determined
without knowing all the inputs to the cone (it is possible to identify a faulty component,
and with strong fault models also a healthy component, without knowing all its inputs).
Note that the restriction of having to measure all the inputs of a cone can lead to significant
increase in the cost compared with the cost of baseline approach; especially when the number
of inputs of a cone is large. This is discussed in detail in Section 6.
4.2.5 The Algorithm
Pseudocode for the hierarchical approach is given in Algorithm 2 as a recursive function.
The inputs are a system C, a set of known values uC of variables at the inputs IC and
outputs OC of the system, and again the optional integer k specifying the fault cardinality
bound for the purpose of experimenting with the effect of model pruning. We start with
the d-DNNF compilation of the abstraction of the given system (line 1) and then use the
function psd from Algorithm 1 to get a diagnosis B of the abstraction (line 3), assuming that
the measurement point selection and stopping criteria in Algorithm 1 have been modified
according to what is described in Section 4.2.4. The abstract diagnosis B is then used to
get a concrete diagnosis D in a loop (lines 414). Specifically, if a component G  B is
not the root of a cone, then it is added to D (line 14); otherwise cone G is recursively
diagnosed (line 10) and the result of it added to D (line 11). When recursively diagnosing
340

fiSequential Diagnosis by Abstraction

a cone G, the subsystem contained in G is represented by DG  IG , where DG is the set of
components dominated by G and IG is the set of inputs of cone G.
Before recursively diagnosing a cone G, we compute an abnormal observation uG at the
inputs and the output (IG {G}) of the cone G. The values of some of Gs inputs and output
will have been either measured or deduced from the current set of measurements. The value
of a variable X is implied to be x under the measurements y if P r(X = x, y) = 0, which
is easy to check once  has been differentiated under y. The function Implications(, y)
(lines 7 and 15) implements this operation, which is used to compute the partial abnormal
observation uG (line 8). A fault cardinality bound k 0 for the cone G is then inferred (line 9),
and the algorithm called recursively to diagnose G, given uG and k 0 .
The recursive call returns the faults T inside the cone G together with the updated
observation uG . The observation uG may contain some new measurement results regarding
the variables IG  {G}, which are added to the set of measurements y of the abstraction
(line 11); other measurement results obtained inside the cone are ignored due to reasons
explained in Section 4.2.4. The concrete diagnosis D is augmented with the faults T found
inside the cone (line 11), and  is again evaluated and differentiated in light of the new
measurements (line 12).
After the loop ends, the variable uC is updated with the known values of the inputs
IC and outputs OC of the system C (line 16). The stopping criteria are checked for the
diagnosis D (line 17) and if met the function returns the pair < D, uC > (line 18); otherwise
more measurements are taken until the stopping criteria (line 17) have been met.
Since D can contain faults from inside the cones, the compilation  cannot be used
to check the stopping criteria for D (note the change in the parameters to the function
MeetsCriteria at line 17) as the probabilistic information regarding variables inside cones
is not available in . The criteria are checked as follows instead: We maintain the depth
level of every component in the system. The outputs of the system are at depth level 1 and
the rest of the components are assigned depth levels based upon the length of their shortest
route to an output of the system. For example, in Figure 1 gates B and J are at depth
level 3, while A is at depth level 2. Hence, B and J are deeper than A. We first propagate
the values of inputs in the system, and then propagate the fault effects of components in
D, one by one, by flipping their values to the abnormal ones and propagating them towards
the system outputs in such a way that deeper faults are propagated first (Siddiqi & Huang,
2007), and then check the values of system outputs obtained for equality with those in the
observation (y).
4.2.6 Example
Suppose that we diagnose the abstraction of the circuit in Figure 1, with the observation
uC = {P = 1, Q = 1, R = 0, V = 1}, and take the sequence of measurements y = {D =
1, K = 1, A = 1}. It is concluded, from the abstract system model, that given the values
of P and D, the value 1 at A is abnormal. So the algorithm concludes a fault at A. Note
that Q = 1 and D = 1 suggests the presence of another fault besides A, triggering the
measurement of gate B, which is also found faulty. The abstract diagnosis {A, B} meets
the stopping criteria with respect to the abstract circuit.
341

fiSiddiqi & Huang

1
P

1
Q

1

1
E

J

1
B

1
A

1

1
D

V

1
K

0
R

Figure 3: A faulty circuit with faults at B and J.
1
P

1
J

1
E

1
A

1
B
1
Q

1
B'

1

1
D

V

1
K

0
R

Figure 4: Creating a clone B 0 of B according to D.
We then enter the diagnosis of cone A by a recursive call with observation uA = {P =
1, B = 1, A = 1}. The diagnosis of the cone A immediately reveals that the cone E is
faulty. Hence we make a further recursive call in order to diagnose E with the observation
uE = {P = 1, B = 1, E = 1}. The only unknown wire J is measured and the gate J is found
faulty, which explains the observation at the outputs of the cones E as well as A, given the
inputs P and B. The recursion terminates and the abstract diagnosis B = {A, B} generates
the concrete diagnosis D = {J, B}, which meets the stopping criteria and the algorithm
terminates.

5. Component Cloning
In the preceding section, we have proposed an abstraction-based approach to sequential diagnosis, which reduces the complexity of compilation and diagnosis by reducing the number
of system components to be diagnosed. We now take one step further, aiming to handle
systems that are so large that they remain intractable even after abstraction, as is the case
for the largest circuits in the ISCAS-85 benchmark suite.
Our solution is a novel method that systematically modifies the structure of a system to
reduce the size of its abstraction. Specifically, we select a component G with parents P (a
component X is a parent of a component Y , and Y is a child of X, if the output of Y is an
input of X) that is not part of a cone and hence cannot be abstracted away in hierarchical
342

fiSequential Diagnosis by Abstraction

diagnosis, and create a clone G0 of it according to some of its parents P0  P in the sense
that G0 inherits all the children of G and feeds into P0 while G no longer feeds into P0 (see
Figures 3 and 4 for an example). The idea is to create a sufficient number of clones of G
so that G and its clones become part of some cones and hence can be abstracted away.
Repeated applications of this operation can allow an otherwise unmanageable system to
have a small enough abstraction for compilation and diagnosis to succeed. The hierarchical
algorithm is then extended to diagnose the new system and the result mapped to the
original system. We show that we can now solve almost all the benchmark circuits, using
this approach.
Before we go into the details of the new method, we differentiate it from a technique
known as node splitting (Choi, Chavira, & Darwiche, 2007), which is used to solve MPE
queries on a Bayesian network. Node splitting breaks enough number of edges between
nodes from the network such that the MPE query on the resulting network becomes easy
to solve. A broken edge is replaced with a root variable with a uniform prior. The resulting
network is a relaxation or approximation of the original in that its MPE solution, which
may be computed from its compilation, gives an upper bound on the MPE solution of the
original network. A depth-first branch and bound search algorithm then searches for an
optimal solution using these bounds to prune its search space. A similar approach is also
used to solve Weighted Max-SAT problems (Pipatsrisawat & Darwiche, 2007).
This version of node splitting is not directly applicable in the present setting for the
following reasons. If edges in a system are broken and redirected into new root variables
(primary inputs), the resulting system represents a different input-output function from
that of the original system. The abnormal observation on the original system may hence
become a normal one on the new system (if the edges through which the fault propagates
are broken), eliminating the basis for diagnosis. Our technique of component cloning, which
can also be viewed as a version of node splitting, introduces clones of a component instead
of primary inputs and preserves the input-output function of the system. Also, the new
system is a relaxation of the original in that its diagnoses are a superset of those of the
original.
We now formally define component cloning:
Definition 2 (Component Cloning). Let G be a component in a system C with parents
P. We say that G is cloned according to parents P0  P when system C results in a
system C0 as follows:
 The edges going from G to its parents P0 are removed.
 A new component G0 functionally equivalent to G is added to the system such that
G0 shares the inputs of G and feeds into each of P0 .
Figures 3 and 4 show an example where creating a clone B 0 of B according to {D}
results in a new circuit whose abstraction contains only the gates {A, D, K, V }, whereas
the abstraction of the original circuit contains also gate B.
5.1 Choices in Component Cloning
There are two choices to be made in component cloning: Which components do we clone,
and for each of them how many clones do we create and how do they split the parents?
343

fiSiddiqi & Huang

Since the goal of cloning is to reduce the abstraction size, it is clear that we only wish
to clone those components that lie in the abstraction (i.e., not within cones). Among these,
cloning of the root of a cone cannot reduce the abstraction size as it will destroy the existing
cone by reintroducing some of the components inside the cone into the abstraction. For
example, cloning D according to K in Figure 4 will produce a circuit where D and its clone
can be abstracted away but B 0 is no longer dominated by D and hence is reintroduced into
the abstraction. Therefore, the final candidates for cloning are precisely those components
in the abstract system that are not roots of cones. Note that the order in which these
candidates are processed is unimportant in that each when cloned will produce an equal
reduction, namely a reduction of precisely 1 in the abstraction size, if any.
It then remains to determine for each candidate how many clones to create and how
to connect them to the parents. To understand our final method, it helps to consider a
naive method that simply creates |P|  1 clones (where P is the set of parents) and has
each clone, as well as the original, feed into exactly one parent. This way every parent of
the component becomes the root of a cone and the component itself and all its clones are
abstracted away. In Figure 3, for example, B has three parents {E, A, D}, and this naive
method would create two clones of B for a total of three instances of the gate to split the
three parents, which would result in the same abstraction as in Figure 4.
The trick now is that the number of clones can be reduced by knowing that some parents
of the component may lie in the same cone and a single clone of the component according
to those parents will be sufficient for that clone to be abstracted away. In the example of
Figure 3, again, the parents E, A of B lie in the same cone A and it would suffice to create
a single clone of B according to {E, A}, resulting in the same, more efficient cloning as in
Figure 4.
More formally, we partition the parents of a component G into subsets P1 , P2 , . . . , Pq
such that those parents of G that lie in the same cone are placed in the same subset and
the rest in separate ones. We then create q  1 clones of G according to any q  1 of these
subsets, resulting in G and all its clones being abstracted away. This process is repeated for
each candidate component until the abstraction size is small enough or no further reduction
is possible.
5.2 Diagnosis with Component Cloning
The new system is functionally equivalent to the original and has a smaller abstraction,
but is not equivalent to the original for diagnostic purposes. As the new model allows
a component and its clones to fail independently of each other, it is a relaxation of the
original model in that the diagnoses of the new system form a superset of those of the
original. Specifically, each diagnosis of the new system that assigns the same health state
to a component and its clones for all components corresponds to a diagnosis of the original
system; other diagnoses are spurious and are to be ignored.
The core diagnosis process given in Algorithm 2 continues to be applicable on the new
system, with only two minor modifications necessary. First, the spurious diagnoses are
(implicitly) filtered out by assuming the same health state for all clones (including the
original) of a component as soon as the health state of any one of them is known. Second,
whenever measurement of a clone of a component is proposed, the actual measurement is
344

fiSequential Diagnosis by Abstraction

c7552
Number of Cone Inputs

60
50
40
30
20
10
0
0

500

1000

1500

2000

2500

Cones

Figure 5: Cones in ISCAS-85 circuits.

taken on the original component in the original system, for obvious reasons (in other words,
the new system is used for reasoning and the original for measurements).
In principle, the presence of spurious diagnoses in the model can potentially skew the
measurement point selection heuristic (at least in the early stages of diagnosis, before the
spurious diagnoses are gradually filtered out). However, by using smaller benchmarks that
could be diagnosed both with and without cloning, we conducted an empirical analysis
which indicates, interestingly, that the overall diagnostic cost is only slightly affected. We
discuss this in more detail in Section 7.3.

6. Diagnostic Cost Estimation
We now address an interesting issue stemming from an observation we made conducting experiments (to be detailed in the next section): While system abstraction is always beneficial
to compilation, the diagnostic cost does not always improve with the associated hierarchical
diagnosis. On the one hand, the hierarchical diagnosis approach can help in cases which
otherwise result in high costs using baseline approach by quickly finding faulty portions of
the system, represented by a set of faulty cones, and then directing the sequential diagnosis
to take measurements inside those cones, resulting in more useful measurements. On the
other hand, it can introduce overhead for cases where it has to needlessly go through hier345

fiSiddiqi & Huang

archies to locate the actual faults, and measure inputs of cones involved, while the baseline
version can find them more directly and efficiently.
The overhead of hierarchical approach can be quite high for faults that lie in cones with
a large number of inputs. For example, the graphs in Figure 5 show the number of inputs,
represented as dots, of various cones in ISCAS-85 circuits. Note that most of the cones have
a small number of inputs; however, some cones can have more than 30 inputs, especially
in c432 and the circuits beyond c1908, which contribute to increased diagnostic cost in
several cases (such increase in the cost due to cones was also confirmed by a separate set of
experiments using a large set of systematically generated combinational circuits, detailed
in Appendix C). To avoid the potential high cost of diagnosis for faults that lie in a cone
with a large number of inputs it is tempting to destroy that cone before compilation so
that any fault in it can now be directly found. However, due to the associated increase in
the abstraction size, destroying cones may cause increased costs for those cases that could
previously be solved more efficiently, and thus may show a negative impact, overall. This
calls for an automatic mechanism to predict the effect of destroying certain cones on the
overall diagnostic cost, which is the subject of this section.
We propose a novel cost estimation function to predict the average diagnostic cost when
a given abstraction of the system is considered for diagnosis, where different abstractions
can be obtained by destroying different cones in the system. Since cones can be destroyed
automatically, the function can be used to automatically propose an abstraction of the system, to be used for diagnosis, that is more likely to give optimal average cost. The function
uses only the hierarchical structure of the given abstraction to predict its cost and does not
take into account other parameters that may also contribute to the cost, such as the probabilities. In addition the function is limited to single fault cases only. Therefore, the expected
cost computed by this function is only indicative and cannot be always correct. However,
experiments show that the function is often quite useful in proposing an abstraction of the
system that is more likely to give optimal cost (to be discussed in the next section).
To estimate the expected diagnostic cost we assume that it is composed of two quantities
namely the isolation cost and the abstraction cost, which are inversely proportional to each
other. The isolation cost captures how well the given system abstraction can isolate the
faulty portions of the system. Therefore the isolation cost is minimum when a complete
abstraction of the system is used (i.e., all cones are considered) and generally increases as
cones are destroyed. The abstraction cost captures the overhead cost due to introduction
of cones. Hence, the abstraction cost is minimum (zero) when no abstraction is considered
and generally increases as cones are introduced.
We define the isolation cost of diagnosis considering an abstraction of the system to
be the average cost required to isolate a single fault in the system using that abstraction.
Similarly, we define the abstraction cost of diagnosis to be the average overhead cost required
to diagnose a single fault in the system using that abstraction. Then the expected average
cost of diagnosis when an abstraction of the system is considered for diagnosis is the sum of
the isolation and the abstraction costs for that abstraction. As different cones are destroyed
in a given abstraction of the system we expect changes in the values of the abstraction and
isolation costs, which determine whether the overall cost can go up or down (if the changes
are uneven) or stay constant (if the changes are even). The idea is to obtain an abstraction
346

fiSequential Diagnosis by Abstraction

of the system to strike a balance between the two quantities to get an overall optimal cost.
Below we discuss how the isolation and abstraction costs can be estimated.
We noted in our experiments when using the baseline approach that our heuristic can
isolate a single fault in the system with a cost that is on average comparable to the log2
of the number of measurement points in the system, which provided us with the basis for
computing the isolation cost. In the hierarchical approach, when a fault lies inside a cone
one can first estimate the isolation cost of diagnosing the cone, separately, and then add
it to the isolation cost of diagnosing the abstract system to get the average isolation cost
for all (single) faults that lie in that cone. For example, when no cones are considered the
cost of isolating a fault in the circuit in Figure 3 is log2 (6) = 2.58 (values of P , Q, R and
V are already known). However, when cones are considered the cost of isolating a fault
that lies inside the cone A is the sum of the isolation cost of the abstract circuit and the
isolation cost of the subcircuit inside cone A, which is log2 (4) + log2 (1) = 2. Similarly, to
get an average isolation cost for all single faults in the system, when using the hierarchical
approach, one can add the isolation cost of diagnosing the abstract system and the average
of the isolation costs of diagnosing all the abstract components (where the isolation cost
for an abstract component which is not a cone is zero). Note that the isolation cost of
diagnosing a cone can be computed by again taking the abstraction of the cone.
To estimate the abstraction cost of diagnosis under a given abstraction we first need
to estimate the overhead cost involved for each individual component in the system under
that abstraction. To estimate the overhead cost of a, possibly faulty, component one can
take the union of all the inputs and outputs of cones in which that component lies, and
the number of such measurement points (approximately) constitutes the required overhead
cost for that component. If a component does not lie in any cone then the overhead cost
for that component is zero. For example, when the circuit in Figure 3 is diagnosed using
the hierarchical approach, to find the gate J as faulty one must first find the cone A to be
faulty and then the cone E to be faulty and then the gate J to be faulty. So the overhead
cost for the gate J in this case will be 1 + 2 + 1 = 4 (i.e., we have to measure wires A, B, E,
J, assuming that Q is known). The abstraction cost of diagnosis under a given abstraction
of the system is then the average of the overhead costs of all the system components under
that abstraction.
We now give formal definitions related to the cost estimation function. Let M Pu (C)
be the set of those measurement points in the system C whose values are unknown, and
M Pu (G) the set of those inputs and output of an abstract or concrete component G whose
values are unknown. Let p be the number of abstract components in an abstraction AC of
system C. Let Gi  AC be an abstract component (either a concrete component or a cone
in the abstraction; a concrete component in the abstraction can be regarded as a trivial
cone containing only the component itself). Let DGi be the subsystem dominated by Gi
and AGi be the abstraction of the subsystem.
The isolation cost IC(C, AC ) when an abstraction AC of the system C is considered for
diagnosis is the sum of log2 (|M Pu (AC )|) and the average of the isolation costs computed,
in a similar manner, for the subsystems contained in the abstract components in AC :
347

fiSiddiqi & Huang

(
Pp
log2 (|M Pu (AC )|) + p1
i=1 IC(DGi , AGi ), if |M Pu (AC )| > 0
IC(C, AC ) = 1 Pp
otherwise
i=1 IC(DGi , AGi )
p

(5)

where IC(DGi , AGi ) recursively computes the isolation cost of the subsystem contained in
the abstract component Gi , using Equation 5, by taking its abstraction AGi . Note that
when computing IC(DGi , AGi ) we assume that the inputs and output of Gi have already
been measured. Thus M Pu (DGi ) excludes the inputs and output of cone Gi . If Gi is a
concrete
component then IC(DGi , AGi ) = 0. If no cones are considered (AC = C) then
Pp
IC(D
Gi , AGi ) = 0 and the isolation cost is simply equal to log2 (|M Pu (C)|).
i=1
To compute the abstraction cost of diagnosing the system under a given abstraction we
first compute the overhead costs of diagnosing individual cones in the abstraction. Then
we multiply the abstraction cost for a cone with the number of components contained in
that cone to get the total overhead cost for all the components in that cone. Adding up
the overhead costs computed this way from all the cones in the abstraction and dividing
this number by the total number of concrete components in the whole system gives us the
average overhead cost per component, which we call the abstraction cost. Formally: Let
there be q cones in AC . Then the abstraction cost AC(C, AC ) when the abstraction AC
of the system C is considered for diagnosis is given as:

AC(C, AC ) =

q
1 X
|DGi |  {M Pu (Gi ) + AC(DGi , AGi )} : Gi  AC is a cone
n

(6)

i=1

where |DGi | is the number of (concrete) components contained in the cone Gi , and M Pu (Gi )+
AC(DGi , AGi ) recursively computes the abstraction cost of diagnosing the cone Gi , using
Equation 6, by taking its abstraction AGi . When the abstraction cost of Gi is multiplied by
|DGi | we effectively add the cost of measuring cone inputs and output in the overhead cost
of every component inside the cone. Again note that when computing AC(DGi , AGi ) we
assume that all the variables in M Pu (Gi ) have already been measured. Thus M Pu (DGi )
excludes the inputs and output of cone Gi .
Finally the total expected cost EDC(C, AC ) of diagnosing a system C when an abstraction AC of the system is considered for diagnosis is given as:
EDC(C, AC ) = IC(C, AC ) + AC(C, AC ).

(7)

7. Experimental Results
This section provides an empirical evaluation of our new diagnostic system, referred to as
sda (sequential diagnosis by abstraction), that implements the baseline, hierarchical, and
cloning-based approaches described in Sections 4 and 5, and the cost estimation function
described in Section 6. All experiments were conducted on a cluster of 32 computers consisting of two types of (comparable) CPUs, Intel Core Duo 2.4 GHz and AMD Athlon 64
X2 Dual Core Processor 4600+, both with 4 GB of RAM running Linux. A time limit of 2
348

fiSequential Diagnosis by Abstraction

hours and a memory limit of 1.5 GB were imposed on each test case. The d-DNNF compilation was done using the publicly available d-DNNF compiler c2d (Darwiche, 2004, 2005).
The CNF was simplified before compilation using the given observation, which allowed us
to compile more circuits, at the expense of requiring a fresh compilation per observation
(see Algorithm 2, line 1).
We generated single- and multiple-fault scenarios using ISCAS-85 benchmark circuits,
where in each scenario a set of gates is assumed to be faulty. For single-fault cases of circuits
up to c1355 we simulated the equal prior probability of faults by generating n fault scenarios
for each circuit, where n equals the number of gates in the circuit: Each scenario contains a
different faulty gate. We then randomly generated 5 test cases (abnormal observations) for
each of these n scenarios. Doing the same for multiple-fault scenarios would not be practical
due to the large number of combinations, so for each circuit up to c1355 (respectively, larger
than c1355) we simply generated 500 (respectively, 100) random scenarios with the given
fault cardinality and a random test case for each scenario.
Thus in each test case we have a faulty circuit where some gate or gates give incorrect
outputs. The inputs and outputs of the circuit are observed. The values of internal wires are
then computed by propagating the inputs in the normal circuit towards the outputs followed
by propagating the outputs of the assumed faulty gates one by one such that deeper faults
are propagated first. The obtained values of internal wires are then used to simulate the
results of taking measurements. We use P r(okX = 1) = 0.9 for all gates X of the circuit.
Note that such cases, where all gates fail with equal probability, are conceivably harder to
solve as the diagnoses will tend to be less differentiable. Then, for each gate, the two output
values are given equal probability when the gate is faulty. Again, this will tend to make
the cases harder to solve due to the high degree of uncertainty. For each circuit and fault
cardinality, we report the cost (number of measurements taken) and time (including the
compilation time, in CPU seconds) to locate the faults, averaged over all test cases solved.
We present the experiments in four subsections demonstrating the effectiveness of the
four techniques proposed in this paper, namely the new heuristic, hierarchical sequential
diagnosis, component cloning, and the cost estimation function.
7.1 Effectiveness of Heuristic
We start with a comparison of the baseline algorithm of sda with gde and show that sda
achieves similar diagnostic costs and scales to much larger circuits, hence illustrating the
effectiveness of our new heuristic (along with the new way to compute probabilities).
7.1.1 Comparison with gde
We could obtain only the tutorial version of gde (Forbus & de Kleer, 1993) for the comparison, downloadable from http://www.qrg.northwestern.edu/BPS/readme.html. gde uses
ATCON, a constraint language developed using the LISP programming language, to represent diagnostic problem cases. A detailed account of this language is given by Forbus and
de Kleer (1993). Further, it employs an interactive user interface that proposes measurement points with their respective costs and lets the user enter outcomes of measurements.
For the purpose of comparison we translated our problem descriptions to the language accepted by gde, and also modified gde to automatically read in the measurement outcomes
349

fiSiddiqi & Huang

size system
13
14
15
16
17

gde
sda
gde
sda
gde
sda
gde
sda
gde
sda

single-fault
cost time
3.6
2.0
3.6 0.01
3.5 6.66
4.2 0.01
3.4
111
3.9 0.01
3.3
398
3.5 0.01
3.7 2876
3.8 0.01

double-fault
cost time
3.8
1.81
3.4
0.01
3.3
15.1
2.9
0.01
3.5
88
3.4
0.01
3.5
556
3.3
0.01
4.6
4103
4.2
0.01

triple-fault
cost time
4.0
1.9
2.8 0.01
3.0
14
2.9 0.01
4.3
299
3.7 0.01
3.2
509
2.8 0.01
4.5 2067
4.2 0.01

Table 1: Comparison with gde.
from the input problem description. We also compiled the LISP code to machine dependent
binary code using the native C compiler to improve run-time performance.
This version of gde, developed for tutorial purposes, computes the set of minimal diagnoses instead of probable diagnoses. This makes our comparison less informative. Nevertheless, we are able to make a reasonable comparison in terms of diagnostic cost as the set
of minimal diagnoses can also serve as a large set of probable diagnoses when components
have equal prior probabilities. According to de Kleer (1992) availability of more diagnoses
aids in heuristic accuracy, whereas focusing on a smaller set of probable diagnoses can be
computationally more efficient but increase the average diagnostic cost.
This version of gde was in fact unable to solve any circuit in ISCAS-85. To enable
a useful comparison, we extracted a set of small subcircuits from the ISCAS-85 circuits:
50 circuits of size 13, 14, 15 and 16, and 10 circuits of size 17. For each circuit we randomly generated 5 single-fault, 5 double-fault, and 5 triple-fault scenarios, and one test
case (input/output vector) for each fault scenario. The comparison between gde and sda
(baseline) on these benchmarks given in Table 1 shows that sda performs as well as gde in
terms of diagnostic cost.
7.1.2 Larger Benchmarks
To evaluate the performance of sda on the larger ISCAS-85 circuits, we have again conducted three sets of experiments, this time involving single, double, and five faults, respectively. As the version of gde available to us is unable to handle these circuits, in order to
provide a systematic reference point for comparison we have implemented a random strategy where a random order of measurement points is generated for each circuit and used
for all the test cases. This strategy also uses the d-DNNF to check whether the stopping
criteria have been met.
Table 2 shows the comparison between the random strategy and sda using the baseline
approach with two different heuristics, one based on entropies of wires alone (ew) and the
other based also on failure probabilities (fp). For each of the three systems we ran the same
set of experiments with and without pruning the d-DNNF (using the known fault cardinality
as described in Section 4.1.2), indicated in the third column of the table. Only the test
cases for the first four circuits could be solved. For other circuits the failure occurred during
the compilation phase, and hence affected both the random strategy and sda.
350

fiSequential Diagnosis by Abstraction

circuit system pruning
c432

rand

(160 gates)

sda(ew)
sda(fp)

c499

rand

(202 gates)

sda(ew)
sda(fp)

c880

rand

(383 gates)

sda(ew)
sda(fp)

c1355

rand

(546 gates)

sda(ew)
sda(fp)

no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes

single-fault
cost time
92.3 20.7
4.5 11.4
42.0 16.6
3.7 11.1
6.7 11.7
4.3 11.0
109.6 0.8
5.5
0.2
58.1 0.7
3.6
0.2
6.5
0.2
4.8
0.2
221.0 1.9
5.4
0.2
26.8 0.3
4.0
0.2
10.8 0.2
5.6
0.2
327.2 4.3
7.4
0.4
82.6 1.3
4.9
0.4
34.1 0.8
8.0
0.4

double-fault
cost time
97.7 23.2
36.8 12.4
42.5 21.3
8.6
12.0
6.4
12.5
5.0
12.3
120.6 1.2
20.1
0.2
54.0
0.5
3.7
0.2
4.3
0.2
3.0
0.2
251.3 1.9
47.3
0.3
32.8
0.4
6.8
0.2
9.2
0.2
6.7
0.2
365.7 5.7
59.0
1.0
91.2
1.5
5.5
0.4
14.8
0.5
9.4
0.6

five-fault
cost time
117.8 26.5
99.7 17.2
68.4 25.5
33.8 12.8
9.4 13.0
9.1 12.6
150.0 1.4
104.9 0.7
95.8 0.8
35.7 0.3
7.2 0.2
7.1 0.2
306.4 2.3
205.7 1.3
79.0 0.7
30.5 0.4
15.8 0.3
14.0 0.3
437.4 5.6
328.6 3.5
203.9 3.4
65.9 1.1
19.3 0.8
18.4 0.6

Table 2: Effectiveness of heuristic.
It is clear that the diagnostic cost is significantly lower with both heuristics of sda than
with the random strategy whether or not pruning has been used. It is also interesting
to note that pruning significantly reduces the diagnostic cost for the random and sda-ew
strategies, but has much less effect on sda-fp except in a few cases (c1355 single-fault).
Moreover, sda-fp generally dominates sda-ew, both with and without pruning.
We may also observe that (i) on the five-fault cases, sda-fp without pruning results in
much lower diagnostic cost than sda-ew with pruning; (ii) on the double-fault cases, the two
are largely comparable; and (iii) on the single-faults cases, the comparison is reversed. This
indicates that as the fault cardinality rises, the combination of failure probabilities and wire
entropies appears to achieve an effect similar to that of pruning. That sda-ew with pruning
performs better than sda-fp without pruning on single-fault cases can be attributed to the
fact that on these cases pruning is always exact and hence likely to result in maximum
benefit.
7.2 Effectiveness of Abstraction
We now report, in Table 3, the results of repeating the same experiments with sda-fp using
the hierarchical approach.
Most notably, the running time generally reduces for all cases and we are now able to
handle two more circuits, namely c1908 and c2670, solving 139 of 300 cases for c1908 (25
of single-, 15 of double-, and 99 of five-fault cases) and 258 of 300 cases for c2670 (100 of
351

fiSiddiqi & Huang

circuit

pruning

c432

no
yes
no
yes
no
yes
no
yes
no
yes
no
yes

(64 cones)

c499
(90 cones)

c880
(177 cones)

c1355
(162 cones)

c1908
(374 cones)

c2670
(580 cones)

single-fault
cost time
15.4
0.4
4.9
0.3
7.3
0.1
4.5
0.1
9.5
0.1
5.6
0.1
9.3
0.3
5.8
0.2
11.0
222
3.0
214
16.3
213
6.5
196

double-fault
cost
time
15.8
0.5
10.4
0.4
5.8
0.1
3.9
0.1
10.2
0.1
7.6
0.1
8.2
0.2
6.3
0.2
17.1
587
8.5
463
19.2
172
13.3
90

five-fault
cost time
22.2 0.5
21.5 0.4
10.5 0.2
9.6 0.2
17.4 0.2
16.3 0.2
14.0 0.3
14.4 0.3
34.9 505
32.4 383
25.4 58
24.3 45

Table 3: Effectiveness of abstraction.

circuit
c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

total
gates
160
202
383
58
880
1193
1669
2307
2416
3512

abstraction
size
59
58
77
58
160
167
353
385
1456
545

cloning
time
0.03
0.02
0.1
0.05
0.74
0.77
5.64
3.6
0.16
6.68

total
clones
27
0
24
0
237
110
489
358
0
562

abstraction size
after cloning
39
58
57
58
70
116
165
266
1456
378

Table 4: Results of preprocessing step of cloning.

single-, 60 of double-, and 98 of five-fault cases). Again all failures occurred during the
compilation phase. Note that some observations do not cause sufficient simplification of
the theory for it to be successfully compiled even after abstraction. In terms of diagnostic
cost, in most cases the hierarchical approach is comparable to the baseline approach. On
c432, the baseline approach consistently performs better than the hierarchical in each fault
cardinality, while the reverse is true on c1355. Note also that pruning helps further reduce
the diagnostic cost to various degrees as with the baseline approach.
As discussed earlier, the results confirm that the main advantage of hierarchical approach
is that larger circuits can be solved. For circuits that can also be solved by the baseline
approach, hierarchical approach may help reduce the diagnostic cost by quickly finding
faulty portions of the circuit, represented by a set of faulty cones, and then directing the
measurements inside them, which can result in more useful measurements (e.g. in the case
of c1355). On the other hand, it may suffer in cases where it has to needlessly go through
hierarchies to locate the actual faults, while the baseline version can find them more directly
and efficiently (e.g. in the case of c432). This is further discussed in Section 7.4.
352

fiSequential Diagnosis by Abstraction

circuit
c432
c880

single-fault
cost
time
7.2
10.3
11.2
0.2

double-fault
cost
time
6.6
7.8
9.3
0.2

five-fault
cost time
9.6
9.7
16.2
0.3

Table 5: Effect of component cloning on diagnostic performance.
circuit
c432
c880
c1908
c2670
c3540
c5315
c7552

single-fault
cost
time
15.2
0.1
8.8
0.1
13.6
2.8
13.5
4.5
27.8
382
7.2
2.5
70.6
1056

double-fault
cost
time
14.8
0.1
9.3
0.1
18.3
5.0
15.3
0.7
30.5
72.5
21.1
5.9
43.1
129.0

five-fault
cost
time
20.2
0.1
15.8
0.2
35.4
5.1
20.1
2.3
36.1
108.6
24.4
6.6
104.8 1108

Table 6: Hierarchical sequential diagnosis with component cloning (c499 and c1355 omitted as they are already easy to diagnose and cloning does not lead to reduced
abstraction).

7.3 Effectiveness of Component Cloning
In this subsection we discuss the experiments with component cloning. We show that cloning
does not significantly affect diagnostic cost and allows us to solve much larger circuits, in
particular, nearly all the circuits in the ISCAS-85 suite.
Table 4 shows the result of the pre-processing step of cloning on each circuit. The
columns give the name of the circuit, the total number of gates in that circuit, the size of
the abstraction of the circuit before cloning, the time spent on cloning, the total number of
clones created in the circuit, and the abstraction size of the circuit obtained after cloning.
On all circuits except c499, c1355, and c6288, a significant reduction in the abstraction size
has been achieved. c6288 appears to be an extreme case with a very large abstraction that
lacks hierarchy; while gates in the abstractions of c499 and c1355 are all roots of cones,
affording no opportunities for further reduction (note that these two circuits are already
very simple and easy to diagnose).
We start by investigating the effect of component cloning on diagnostic performance.
To isolate the effect of component cloning we use the baseline version of sda (i.e., without
abstraction), and without pruning. Table 5 summarizes the performance of baseline sda
with cloning on the circuits c432 and c880. Comparing these results with the corresponding entries in Table 2 shows that the overall diagnostic cost is only slightly affected by
cloning. We further observed that in a significant number of cases the proposed measurement sequence did not change after cloning, while in most of the other cases it changed
only insubstantially. Moreover, in a number of cases, although a substantially different
sequence of measurements was proposed, the actual diagnostic cost did not change much.
Finally, note that the diagnosis time in the case of c432 has reduced after cloning, which
can be ascribed to the general reduction in the complexity of compilation due to a smaller
abstraction.
353

fiSiddiqi & Huang

circuit
c432

c499

c880

c1355

c1908

c2670

total max. cone abstraction measurement
AC IC EDC
cases inputs
size
points
38
39
32
11.51 5.67 17.1
800
18
49
42
5.22 6.05 11.2
14
52
45
4.87 6.11 10.9
9
53
46
4.64 6.14 10.8
4
104
97
2.11 6.72 8.8
0
187
180
0.00 7.50 7.5
8
58
26
3.77 5.32 9.0
1010
5
74
42
3.13 5.91 9.0
3
170
138
0.71 7.10 7.8
0
202
170
0.0 7.40 7.4
16
57
31
6.54 5.42 11.9
1915
14
74
48
5.75 6.02 11.7
10
105
79
4.22 6.72 10.9
6
170
144
2.70 7.48 10.1
0
407
381
0.0 8.57 8.5
8
58
26
3.59 6.34 9.9
2730
5
98
66
2.74 7.20 9.9
4
114
82
2.47 7.27 9.7
3
266
234
1.43 8.23 9.6
2
426
394
0.43 8.77 9.2
0
546
514
0.0 9.00 9.0
40
70
45
14.37 7.07 21.4
859
29
76
51
12.85 7.15 20.0
28
80
55
12.70 7.23 19.9
27
82
57
12.62 7.27 19.8
20
138
113
8.36 7.82 16.2
18
150
125
7.79 7.92 15.7
55
56
52
17.84 6.40 24.2
989
34
58
57
16.19 6.53 22.7
33
128
64
15.63 6.68 22.3
25
178
114
11.52 7.44 18.9

cases
solved
800
800
800
800
800
800
1010
1010
1010
1010
1915
1915
1915
1915
1915
2730
2730
2730
2730
2730
2730
859
859
859
859
859
859
989
989
989
970

single-fault
cost time
15.2 0.06
11.0 0.1
11.0 0.1
10.7 0.1
8.8
0.3
7.3
7.3
7.3
0.1
7.7
0.1
9.4
0.1
6.4
0.1
8.7
0.1
8.5
0.1
8.0
0.1
8.6
0.1
10.8 0.2
9.30 0.1
12.55 0.2
12.39 0.2
22.5 0.3
33.5 0.4
34.0 0.4
18.7 2.6
17.8 5.8
18.3 5.9
18.2 5.9
17.7 15.0
17.7 47.5
19.2 0.7
19.1 0.8
18.6 0.8
16.1 79.0

Table 7: Effectiveness of diagnostic cost estimation.
Our final set of experimental results with ISCAS-85 circuits, summarized in Table 6,
illustrates the performance of hierarchical sequential diagnosis with component cloning
the most scalable version of sda. All the test cases for circuits c1908 and 2670 were now
solved, and the largest circuits in the benchmark suite could now be handled: All the cases
for c5315, 164 of the 300 cases for c3540 (34 of single-, 65 of double-, and 65 of five-fault
cases), and 157 of the 300 cases for c7552 (60 of single-, 26 of double-, and 71 of fivefault cases) were solved. In terms of diagnostic cost cloning generally resulted in a slight
improvement. In terms of time the difference is insignificant for c432 and c880, and for the
larger circuits (c1908 and c2670) diagnosis with cloning was clearly more than an order of
magnitude faster.
7.4 Effectiveness of Diagnostic Cost Estimation
Finally, we demonstrate the effectiveness of our cost estimation function. We show that it
is often possible to destroy different cones to obtain different abstractions of a system that
354

fiSequential Diagnosis by Abstraction

can all be successfully compiled, and then, using the cost estimation function, select an
abstraction to be used for diagnosis that is more likely to give optimal average cost. These
results also help explain why in some cases the hierarchical approach causes diagnostic cost
to increase compared with the baseline approach.
In these experiments, we use sda with cloning and include circuits up to c2670, considering only single-fault test cases. We did not include the largest circuits in our analysis as
these circuits often could not be compiled after some cones in them were destroyed; therefore it was not possible to obtain an overall picture of the actual cost for these circuits. Test
cases for circuits up to c1355 are the same as used before, whereas for circuits c1908 and
c2670, this time, we use a more complete set of cases as done for smaller circuits. Specifically, we generate n fault scenarios for each circuit, where n equals the number of gates
in the circuit: Each scenario contains a different faulty gate. We then randomly generate
1 test case for each of these n scenarios (in some cases, we could not obtain a test case in
reasonable time and the corresponding scenarios were not used).
The results of experiments are summarized in Table 7. For each circuit the first row
shows results when all cones have been considered and the subsequent rows show results
when all cones having more than a specified number of inputs (in column 3) have been
destroyed. When the value in column 3 is 0 we get the trivial abstraction, where all cones
have been destroyed, which is equivalent to using the baseline approach. The last two
columns show the (actual) average cost and time for diagnosing a circuit using the given
abstraction. The columns labeled with AC, IC, and EDC show values obtained using the
equations 6, 5, and 7, respectively, for a given abstraction.
The results show that we are often able to destroy several cones while still being able to
compile the circuit successfully. However, quite naturally, the compilation time increases as
more cones are destroyed such that at some point the circuits start to fail to compile, where
we stop destroying cones. The actual diagnostic cost on different circuits show different
trends each time some cones have been destroyed. For example, on c432 it shows significant
improvement while the reverse is true for c1355. On remaining circuits the actual cost shows
somewhat mixed trends; however, the relative increase or decrease in the costs is generally
less significant.
Comparison of the isolation and abstraction costs (i.e., IC and AC, respectively) for
various abstractions confirms that each time some cones are destroyed the isolation cost
increases while the abstraction cost decreases. It is the potentially imbalanced change in
the two costs that determines whether the cost might go up or down after the cones are
destroyed. For example, in the case of c432 the abstraction cost drops more rapidly than
the isolation cost increases when cones are destroyed, while in the case of c1355 the two
costs change almost at the same pace.
Comparison of the predicted costs EDC with the actual costs shows that for c432,
c499, c1908, and c2670 the predicted costs are often quite close to the actual costs, which
demonstrates the relative accuracy of our approach. As a result, for these circuits the
cost estimation function can accurately predict the abstraction that is more likely to give
optimal cost. For example, it correctly suggests that one should use the baseline approach
with c432. For the other two circuits, c880 and c1355, the predicted and actual costs are
significantly different, and the cost estimation function fails to give good predictions. c1355
355

fiSiddiqi & Huang

seems to be a special case in which the actual diagnostic cost increases quite rapidly as
cones are destroyed, the reason for which will be an interesting topic for future work.

8. Related Work
Out et al. (1994) considered two kinds of hierarchical models and discussed automatic
methods for constructing their abstractions. In the first kind, components of the given
detailed model are aggregated into single components of the abstract model, such that every
diagnosis of the detailed model, refined from a diagnosis of the abstract model, is guaranteed
to be valid. Thus there is no need to check the validity of detailed diagnoses afterwards.
In the second kind, the abstract model is constructed such that it is always possible to
determine a unique diagnosis at every level of the hierarchy with a reasonable cost, where the
measurements that are less costly to make appear in the most abstract model and the more
costly measurements appear in the most detailed model. More techniques for automatic
abstraction-based on system observability were discussed by Torta and Torasso (2003, 2008).
These papers provide alternative techniques to automatic abstraction; however, they do not
address sequential diagnosis.
The idea of testing the most likely failing component comes from Heckerman et al. (1995),
where the testing of a component was considered a unit operation and components were
tested in decreasing order of their likelihood of failure, which was computed assuming a
single fault (this assumption could compromise the quality of the measurement sequence
in multiple-fault cases as the authors pointed out). In our case, by contrast, the testing of
each variable of a component is a unit operation, calling for a more complex heuristic in
order to minimize the number of tests; also, we do not need to assume a single fault. Our
work also goes further in scalability using several structure-based techniques: compilation,
abstraction, and component cloning.
Chittaro & Ranon (2004) considered the computation of diagnoses using a hierarchical
algorithm. Their method takes a hierarchical decomposition of the system as input, where
sets of components are aggregated into units, and computes a set of diagnoses at the most
abstract level, which are then refined hierarchically to the most detailed level. Feldman &
van Gemund (2006) developed a hierarchical diagnosis algorithm and tested it on reverse
engineered ISCAS-85 circuits (Hansen, Yalcin, & Hayes, 1999) that are available in highlevel form. The idea is to decompose the system into hierarchies in such a way as to minimize
the sharing of variables between them. This can be done for well engineered problems and
they have formed hierarchies by hand for ISCAS-85 circuits. The system is represented
by a hierarchical logical formula where each hierarchy is represented by a traditional CNF
formula. This representation can be translated to a fully hierarchical DNF, a fully flattened
DNF, or a partially flattened DNF dictated by a depth parameter, after which a hierarchical
search algorithm is employed to find the diagnoses. The hierarchical aspect of these two
approaches is similar to that of ours; however, they require a hierarchical decomposition of
the system to be either given as part of the input, or obtained by hand, while our approach
searches for hierarchies automatically. Another major difference is that they consider only
the computation of diagnoses and do not address the problem of sequential diagnosis.
Based on the gde framework, de Kleer (2006) studied the sensitivity of diagnostic
cost to what is called the -policy, which is the policy that quantifies how the posterior
356

fiSequential Diagnosis by Abstraction

probabilities of diagnoses are to be estimated when gde computes its heuristic. In our
case, probabilities of diagnoses are not required at all, and the other probabilities that
are required can all be computed exactly by evaluating and differentiating the d-DNNF.
Nevertheless, our algorithm can be sensitive to the initial probabilistic model given and
sensitivity analysis in this regard may lead to interesting findings.
Recently, Flesch, Lucas, & van der Weide (2007) proposed a new framework to integrate
probabilistic reasoning into model-based diagnosis. The framework is based upon the notion
of conflict measure, which originated as a tool for the detection of conflicts between an
observation and a given Bayesian network (Jensen, 2001). When a system is modeled as
a Bayesian network for diagnostic reasoning, it is possible to use this conflict measure to
differentiate between diagnoses according to their degree of consistency with a given set of
observations. This work, however, does not address the problem of sequential diagnosis,
i.e., locating actual faults by taking measurements.
Most recently, Feldman, Provan, and van Gemund (2009) proposed a related method
for reducing diagnostic uncertainty. While our work attempts to identify the actual faults
with the fewest individual measurements, their heuristic was aimed at reducing the number
of diagnoses with the fewest test vectors.

9. Conclusion
We have presented a new system for sequential diagnosis, called sda, that employs four
new structure-based techniques to scale diagnosis to larger systems. Specifically, it uses
a heuristic for measurement selection that can be computed efficiently from the d-DNNF
compilation of the system. To diagnose larger systems, it automatically computes a structural abstraction of the system and performs diagnosis in a hierarchical fashion. It then
employs a structure-based technique for further reducing the abstraction size of the system,
which scales the diagnosis to the largest benchmark systems. Finally, it can automatically
select an abstraction of the system that is more likely to give optimal average cost.

Acknowledgments
We thank the anonymous reviewers for their comments. NICTA is funded by the Australian
Government as represented by the Department of Broadband, Communications and the
Digital Economy and the Australian Research Council through the ICT Centre of Excellence
program. Part of this work has appeared in KR 2010 (Siddiqi & Huang, 2010); another
part of this work was carried out during JulySeptember 2010 while the first author was
visiting NICTA.

Appendix A. Computing Probabilities on d-DNNF
Here we briefly describe the computation of probabilities based on d-DNNF compilations of
Bayesian networks. d-DNNF is a graph representation of a nested and/or expression where
negation only appears next to variables, children of every and-node have disjoint sets of
variables (decomposability), and children of every or-node are pairwise logically inconsistent
357

fiSiddiqi & Huang

and

0.0475

0.0475

or

and
1

and

and

0.0475

0.95

okA
0.9

or

0

0.1

and

1

D
1

A
1

or

0.95

and J and

and

0

0.05

0.05

okA

P

0

A okJ

0.5

0.1

0.05

J J

0.5

1

J
0.5

okJ
0.9

Figure 6: d-DNNF compilation of subcircuit (dotted) in Figure 1 given the observation
A  P  D and computation of the posterior probability of J = 1.

(determinism). For example, Figure 6 shows a d-DNNF compilation of the subcircuit in
the dotted box of Figure 1 under the observation A  P  D.
Given a d-DNNF compilation, the probability P r(E = e) for an instantiation e of any set
of variables E can be obtained by the following linear-time procedure: (i) Set all variables E
to Boolean constants according to the instantiation e, (ii) set all other literals (not in E) to
true except those that have numbers associated with them (negative literals are associated
with 1 minus the corresponding numbers for the positive literals), and (iii) evaluate the dDNNF bottom-up by treating true as 1, false as 0, the remaining leaves as their associated
numbers, or-nodes as additions, and and-nodes as multiplications. The number at the root
will be P r(E = e). For example, Figure 6 shows the computation of the probability of J = 1
given the observation A  P  D. Thus e = {A = 1, P = 1, D = 1, J = 1}. In the d-DNNF,
we set A = 1, P = 1, D = 1, J = 1, J = 0. The rest of the literals are given values that
are associated with them (discussed in Section 3.2).
Furthermore, a second traversal of the d-DNNF, from the top down, can effectively
differentiate the d-DNNF so that updated probabilities are computed at once for every
possible change in the value of a variable (e.g., from unknown to known) (Darwiche, 2003).
This is useful for our measurement point selection where we need to update the entropies
for all candidate measurement points.

Appendix B. Cardinality-based Model Pruning
Here we present the technique referred to in Section 4 that can be used to remove a significantly large number (if not all) of diagnoses of cardinality > k from the d-DNNF.
The value of k must be greater or equal to the minimum-cardinality of the d-DNNF
for pruning to occur. If k is equal to the minimum-cardinality of the d-DNNF then all
diagnoses with cardinality > k can be removed using the minimization procedure described
358

fiSequential Diagnosis by Abstraction

Figure 7: Pruning d-DNNF to improve heuristic accuracy.
by Darwiche (2001). If, however, k is greater than the minimum-cardinality of the d-DNNF
then we need a similar but modified minimization algorithm to make sure we do not remove
diagnoses of cardinality  k.
While a complete pruning is difficult to achieve in general, an approximation is possible.
In a naive approach, one may remove every child l of every or-node n for which minimumcardinality (mc) of l is greater than k, which will be sound in that it will never remove
diagnoses of cardinality  k but may result in too little pruning in many cases. We can
increase the amount of pruning performed by computing local value k(n) for every node n
given the global k for the whole d-DNNF using a top-down traversal through the d-DNNF:
Every node n suggests a value k(l) for its child l and the largest of these values is accepted to
be the final value of k(l) (this is essential to avoid possibly removing diagnoses of cardinality
 k). More pruning can occur in this way because k(n) can often be less than the global
k. Once k(n) has been computed for every node, every child l of every or-node n for which
mc(l) > k(l) can then be pruned.
We now give the pruning algorithm which performs a two pass traversal through the
d-DNNF. The mc(n) is updated during upward traversal and represents the minimumcardinality of diagnoses under a node n, whereas the k(n) is updated during downward
traversal and represents the upper bound on the fault-cardinality for a node which is used
to prune branches emanating from the node whose mc(n) exceeds the k(n).
The two passes of the procedure are as follows: Initialize mc(n) to 0 and k(n) to -
(least possible value) for all n. Traverse the d-DNNF so that children are visited before
parents and for every leaf node, set mc(n) to 1 if n is a negated health variable and 0
otherwise; for every or-node, set mc(n) to the minimum of the values of mc of its children;
for every and-node set mc(n) to the sum of the values of mc of its children. Now traverse
the d-DNNF so that parents are visited before children and set k(n) for the root node to
the value k; for every or-node, remove every child p of n for which mc(p) > k(n) and for
every remaining child v set k(v) to k(n) if k(n) > k(v); for every child p of every and-node,
let tp be the sum of the values of mc of all the other children and set k(p) to the value tp if
tp > k(p).
In the above procedure the conditions k(n) > k(v) and tp > k(p) while updating k for
a node ensure that only a safe value for k is set. An example is shown in Figure 7. The
mc (left) and k (right) values are shown for each node. The branches labeled , , , and
359

fiSiddiqi & Huang

I1

G1

I2
G2

C1

G3

I3
G5
G4
G6
C2

Figure 8: A combinational circuit generated randomly from a set of components consisting
of gates G1 , G2 , . . . , G6 and cones C1 , C2 , when they are processed in the order:
G1 , C1 , G2 , G3 , G4 , C2 , G5 , G6 .

N
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152

total
gates
104
130
156
182
208
234
260
286
312
338
364
390
416
442
468
494

average
depth
26.9
31.6
30.3
34.8
37.6
41.1
39.3
41.6
46.3
43.4
41.8
48.5
48.1
50.7
48.2
50.8

approx.
treewidth
13
16
17
21
24
26
29
32
34
36
39
43
45
48
51
51

abstraction
size
26
31
38
45
51
59
66
71
79
82
90
97
104
112
116
123

total
clones
32
42
69
68
84
108
128
158
172
177
194
218
194
243
265
272

abstraction size
after cloning
17
20
24
28
32
38
41
42
48
49
57
61
65
72
70
78

Table 8: Randomly generated combinational circuits (N, 25, 5).

 are subgraphs associated with hypothetical values for mc. The figure shows that the
minimum-cardinality for every node (mc) is less than or equal to the bound (k) except for
the branch labeled , which gets pruned accordingly.

Appendix C. Randomly Generated Combinational Circuits
In this section we use a novel method to systematically generate series of combinational
circuits such that their structure and size can be controlled. This enables the evaluation of
our techniques on circuits other than ISCAS-85 benchmarks, which has helped us identify
factors that affect the diagnostic cost, leading us to the cost estimation function given in
Section 6. Specifically, we observe that for circuits of a similar structure, diagnostic cost
generally increases with circuit size, which helped us devise the notion of isolation cost; and
360

fiSequential Diagnosis by Abstraction

that when circuit size is held constant, diagnostic cost generally increases with the number
of cones in the circuit, which helped us devise the notion of abstraction cost.
The circuits are generated by composing a set of pre-formed building blocks. The latter
consist of both gates and cones. The gates are taken from a pool of six gates of types OR,
NOR, AND, NAND, NOT, and BUFFER, and the cones from a pool of eight cones, each
of which has 10 gates and is extracted from ISCAS-85 benchmark circuits.
Our composition method is inspired from the method of generating random Bayesian
networks described by Marinescu, Kask, and Dechter (2003). The circuits are generated
according to a formula (N, P, I), where N is the number of components (building blocks)
to use, P the percentage of cones in the components, and I the maximum number of inputs
a gate can have. To generate the N components we randomly pick (P/100)  N cones (with
repetition) from the pool of cones and N  (P/100)  N gates (with repetition) from the
pool of gates and place them in a random order. The number of inputs of each gate is set
randomly between 2 and I, except for a NOT or BUFFER gate which can have only one
input.
We then process each component as follows: Suppose that the components are placed
in the order C1 , C2 , . . . , CN . Let Pi be the set of components that precede Ci in the
order. When we process a component Ci we connect every input of Ci to the output of a
randomly chosen component from Pi such that no two inputs of Ci are connected to the
same component. If an input of Ci cannot be connected (either because Pi is empty or all
the components in Pi have been used) then it is treated as a primary input of the circuit.
For example, the circuit in Figure 8 has been randomly generated according to the formula
(8, 25, 2), where the components shown in the boxes represent cones.
By varying the parameters (N, P, I) we can obtain circuits of varying size and structure.
First we fix P = 25, I = 5 and vary N to generate a range of circuits of increasing size. For
each N we generate 10 circuits. These circuits are summarized in Table 8. The numbers in
the columns are averaged over all circuits of a given size, and rounded off. Generally, when
N is increased we see an increase in the abstraction size as well as the estimated treewidth,
corresponding to an increase in the perceived difficulty of the circuit (e.g., note that the
largest circuit in this set is smaller than c1355, but the estimated treewidth of c1355 is much
lower, at 25; the actual compilation was indeed harder for the former circuit). For each
circuit we randomly generate 10 single-fault, 10 double-fault, and 10 five-fault scenarios and
a single test case for each scenario.
The results of experiments with these circuits are given in Tables 9, 10, and 11, using
the baseline, hierarchical, and cloning techniques, respectively. These results are generally
consistent with those obtained using the ISCAS-85 circuits. The baseline sda could not
solve any circuit beyond (72, 25, 5). The hierarchical sda solved more circuits but could
not solve any circuit beyond (80, 25, 5). The most scalable version of sda, with component
cloning, solved much larger circuits, up to (168, 25, 5).
Note that there is a general trend of increase in diagnostic cost with increase in N . This
is consistent with ones intuitive expectation that diagnostic uncertainty would increase with
system size. Also note that diagnostic cost is often significantly higher for the hierarchical
approach than the baseline approach. As discussed earlier, this can be attributed to the
fact that the hierarchical approach often has to go through hierarchies of cones to reach a
faulty gate, which the baseline approach may be able to reach more directly.
361

fiSiddiqi & Huang

total
single-fault
double-fault
five-fault
pruning
gates
solved cost time solved cost time solved cost
time
32 104
no
100 5.86 0.56
100 6.34 0.57
100
9.19
0.60
yes
100 4.81 0.54
100 5.24 0.55
100
8.22
0.59
40 130
no
100 5.82 4.31
100 7.05 4.51
100 11.53 5.09
yes
100
4.5
4.16
100 5.08 4.28
100 10.35 4.93
48 156
no
100 6.58 32.43
100 8.72 32.75
100 11.19 34.87
yes
100 4.73 31.27
100
5.9 31.14
100
9.46 33.84
56 182
no
80
5.26 190.99
80
6.9 192.69
80
11.05 202.4
yes
80
3.58 185.32
80
5.62 190.25
80
8.325 197.05
64 208
no
50
5.58 532.82
50
6.9 540.31
50
13.94 581.11
yes
50
5.02 527.24
50
4.72 525.02
50
9.84 558.79
72 234
no
10
6.2 207.89
10
9.5 230.72
10
27.5 354.80
yes
10
6.2 207.49
10
5.8 205.41
10
11.4 248.20
N

Table 9: Baseline heuristic on randomly generated circuits (N, 25, 5).
total
single-fault
double-fault
five-fault
pruning
gates
solved cost time solved cost time solved cost
time
32 104
no
100 7.81 0.15
100 8.78 0.16
100 12.59 0.18
yes
100 3.42 0.15
100 5.87 0.16
100 11.88 0.17
40 130
no
100
7.2
0.71
100 8.19 0.72
100 13.77 0.75
yes
100 3.07 0.70
100 5.18 0.71
100 12.94 0.73
48 156
no
100 7.03 4.10
100 8.12 4.14
100 12.78 4.26
yes
100 3.18 4.01
100 4.96 4.02
100 11.51 4.08
56 182
no
100 7.81 42.63
100
9.1 43.58
100 11.92 43.64
yes
100 2.98 41.60
100 6.31 42.23
100
11.1 42.19
64 208
no
80
8.35 108.61
80
9.11 107.96
80
14.85 111.04
yes
80
3.31 107.05
80
5.35 106.31
80
13.56 107.71
72 234
no
30
7.56 120.59
30
9.83 122.50
30
12.66 123.81
yes
30
2.8 118.35
30
5.53 118.57
30
11.2 119.93
80 260
no
10
6.9 190.66
10
9.2 193.58
10
12.4 197.29
yes
10
2.8 188.95
10
4.6 189.73
10
10.5 190.07
N

Table 10: Hierarchical heuristic on randomly generated circuits (N, 25, 5).
We also observe that, again, pruning leads to a general improvement in diagnostic cost.
The improvement is more significant for the hierarchical approach, which can be explained
by the fact that the effect of pruning is much greater on the abstract model, as each branch
pruned can correspond to a large part of the original system.
We now perform another set of experiments to study the impact of hierarchy in a
controlled manner. This time we hold the size of the circuits more or less constant and
vary the percentage of cones in them. Specifically, we generate a large number of random
circuits with P ranging from 0 to 50, such that for each value of P the generated circuits
contain 120 gates on average.
The experiments on these circuits are summarized in Table 12. Note that as P increases
the estimated treewidth of the circuits decreases, as would be expected, and the actual
compilation time indeed also decreases. The diagnostic cost, on the other hand, increases
steadily up to P = 25 and remains more or less flat afterwards. This confirms the potential
362

fiSequential Diagnosis by Abstraction

N
32
40
48
56
64
72
80
88
96
104
112
120
128
136
144
152
160
168

total
single-fault
double-fault
five-fault
gates solved cost
time solved cost
time solved cost
time
104
100
7.86
0.04
100
8.78
0.05
100 12.13 0.06
130
100
8.12
0.05
100
9.6
0.06
100 13.58 0.08
156
100
8.25
0.07
100
9.34
0.08
100
12.6
0.10
182
100
9.03
0.12
100
10.4
0.13
100 13.37 0.15
208
100 10.06 0.45
100 10.73 0.46
100 15.41 0.49
234
100
9.15
0.78
100 11.38 0.80
100 15.44 0.84
260
100
9.78
0.83
100 11.38 0.85
100
15.5
0.89
286
100
9.56
0.78
100 10.87 0.79
100
16.6
0.84
312
100
10.4
1.85
100 10.81 1.87
100 17.87 1.97
338
100 10.03 4.23
100 11.79 4.26
100 16.95 4.34
364
100 10.44 29.20
100 11.76 29.39
100 17.62 29.93
390
100 10.36 39.88
100
13.6 40.15
100 20.76 41.17
416
90
11.17 98.70
90
13.73 99.08
90
19.33 100.73
442
90
11.82 220.41
90
13.76 221.63
89
20.25 225.58
468
80
12.08 207.69
80
15.05 207.68
80
19.92 210.86
494
40
12.7 256.43
40
14.72 257.5
40
23.02 260.41
520
40
12.5 476.93
40
14.15 479.33
40
18.5 479.83
546
10
8.7
84.16
10
10.1 84.44
10
15.1 85.27

Table 11: Component cloning on randomly generated circuits (N, 25, 5).
P
0
5
10
15
20
25
50

total
single-fault
treewidth
circuits
cost time
1000
32
5.7
8.8
600
23
6.7
0.9
900
21
7.5
0.5
1000
18
7.7
0.1
1100
17
8.0
0.1
1300
15
9.2
0.1
800
12
8.6
0.06

double-fault
cost
time
7.7
8.8
8.0
0.9
8.7
0.5
9.1
0.1
9.4
0.1
10.3
0.1
10.0
0.07

five-fault
cost time
13.5 9.0
13.0 0.9
13.2 0.5
12.2 0.1
12.3 0.1
13.6 0.1
12.4 0.1

Table 12: Component cloning on randomly generated circuits (N ,P ,5).
negative impact of hierarchy on the diagnostic cost we hypothesized: As P increases the
likelihood of a fault occurring inside a cone also increases and thus on average one has to
take more measurements, many on inputs to cones, to locate a fault. That diagnostic cost
does not further increase after P = 25 is consistent with the observation that since the
circuit size is fixed at roughly 120 and each cone contributes 10 gates to the circuit, when
P increases to some point, there will be very few gates lying outside cones and hence the
likelihood of a fault occurring in a cone will have more or less plateaued.

References
Brglez, F., & Fujiwara, H. (1985). A neutral netlist of 10 combinational benchmark circuits and a target translator in Fortran. In Proceedings of the IEEE International
Symposium on Circuits and Systems (ISCAS), pp. 695698.
Chittaro, L., & Ranon, R. (2004). Hierarchical model-based diagnosis based on structural
abstraction. Artificial Intelligence, 155 (1-2), 147182.
363

fiSiddiqi & Huang

Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: A scheme for generating upper bounds in Bayesian networks. In Proceedings of the 23rd Conference on
Uncertainty in Artificial Intelligence (UAI), pp. 5766.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial
Intelligence Research, 17, 229264.
Darwiche, A. (2001). Decomposable negation normal form. Journal of the ACM, 48 (4),
608647.
Darwiche, A. (2003). A differential approach to inference in Bayesian networks. Journal of
the ACM, 50 (3), 280305.
Darwiche, A. (2004). New advances in compiling CNF into decomposable negation normal form. In Proceedings of the 16th European Conference on Artificial Intelligence
(ECAI), pp. 328332.
Darwiche, A. (2005). The c2d compiler user manual. Tech. rep. D-147, Computer Science
Department, UCLA. http://reasoning.cs.ucla.edu/c2d/.
de Kleer, J., & Williams, B. C. (1987). Diagnosing multiple faults. Artificial Intelligence,
32 (1), 97130.
de Kleer, J. (1992). Focusing on probable diagnosis. In Readings in model-based diagnosis,
pp. 131137. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
de Kleer, J. (2006). Improving probability estimates to lower diagnostic costs. In 17th
International Workshop on Principles of Diagnosis (DX).
de Kleer, J., Raiman, O., & Shirley, M. (1992). One step lookahead is pretty good. In
Readings in model-based diagnosis, pp. 138142. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Feldman, A., & van Gemund, A. (2006). A two-step hierarchical algorithm for modelbased diagnosis. In Proceedings of the 21st AAAI Conference on Artificial Intelligence
(AAAI), pp. 827833.
Feldman, A., Provan, G. M., & van Gemund, A. J. C. (2009). FRACTAL: Efficient fault
isolation using active testing. In Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI), pp. 778784.
Flesch, I., Lucas, P., & van der Weide, T. (2007). Conflict-based diagnosis: Adding uncertainty to model-based diagnosis. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI), pp. 380385.
Forbus, K. D., & de Kleer, J. (1993). Building problem solvers. MIT Press, Cambridge,
MA, USA.
Hansen, M. C., Yalcin, H., & Hayes, J. P. (1999). Unveiling the ISCAS-85 benchmarks: A
case study in reverse engineering. IEEE Design and Test of Computers, 16 (3), 7280.
364

fiSequential Diagnosis by Abstraction

Heckerman, D., Breese, J. S., & Rommelse, K. (1995). Decision-theoretic troubleshooting.
Communications of the ACM, 38 (3), 4957.
Jensen, F. V. (2001). Bayesian networks and decision graphs. Springer-Verlag New York,
Inc., Secaucus, NJ, USA.
Kirkland, T., & Mercer, M. R. (1987). A topological search algorithm for ATPG. In
Proceedings of the 24th Conference on Design Automation (DAC), pp. 502508.
Marinescu, R., Kask, K., & Dechter, R. (2003). Systematic vs. non-systematic algorithms
for solving the MPE task. In Proceedings of the 19th Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 394402.
Out, D.-J., van Rikxoort, R., & Bakker, R. (1994). On the construction of hierarchic models.
Annals of Mathematics and Artificial Intelligence, 11 (1-4), 283296.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
Pipatsrisawat, K., & Darwiche, A. (2007). Clone: Solving weighted Max-SAT in a reduced
search space. In Proceedings of the 20th Australian Joint Conference on Artificial
Intelligence (AI), pp. 223233.
Siddiqi, S., & Huang, J. (2007). Hierarchical diagnosis of multiple faults. In Proceedings of
the 20th International Joint Conference on Artificial Intelligence (IJCAI), pp. 581
586.
Siddiqi, S., & Huang, J. (2010). New advances in sequential diagnosis. In Proceedings of
the Twelfth International Conference on Principles of Knowledge Representation and
Reasoning (KR), pp. 1725.
Torta, G., & Torasso, P. (2003). Automatic abstraction in component-based diagnosis driven
by system observability. In Proceedings of the 18th International Joint Conference on
Artificial Intelligence (IJCAI), pp. 394402.
Torta, G., & Torasso, P. (2008). A symbolic approach for component abstraction in modelbased diagnosis. In 19th International Workshop on Principles of Diagnosis (DX).

365

fiJournal of Artificial Intelligence Research 41 (2011) 231-266

Submitted 11/2010; published 06/2011

Probabilistic Relational Planning
with First Order Decision Diagrams
Saket Joshi

joshi@eecs.oregonstate.edu

School of Electrical Engineering and Computer Science
Oregon State University
Corvallis, OR 97331, USA

Roni Khardon

roni@cs.tufts.edu

Department of Computer Science
Tufts University
Medford, MA, 02155, USA

Abstract
Dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, in particular algebraic decision
diagrams, to capture domain dynamics and value functions. Work on symbolic dynamic
programming lifted these ideas to first order logic using several representation schemes.
Recent work introduced a first order variant of decision diagrams (FODD) and developed a
value iteration algorithm for this representation. This paper develops several improvements
to the FODD algorithm that make the approach practical. These include, new reduction
operators that decrease the size of the representation, several speedup techniques, and
techniques for value approximation. Incorporating these, the paper presents a planning
system, FODD-Planner, for solving relational stochastic planning problems. The system
is evaluated on several domains, including problems from the recent international planning
competition, and shows competitive performance with top ranking systems. This is the
first demonstration of feasibility of this approach and it shows that abstraction through
compact representation is a promising approach to stochastic planning.

1. Introduction
Planning under uncertainty is one of the core problems of Artificial Intelligence. Over the
years research on automated planning has produced a number of planning formalisms and
systems. The STRIPS planning system (Fikes & Nilsson, 1971) led a generation of automated planning research. This produced a number of successful systems for deterministic
planning using various paradigms like partial order planning (Penberthy & Weld, 1992),
planning based on planning graphs (Blum & Furst, 1997), planning by satisfiability (Kautz
& Selman, 1996) and heuristic search (Bonet & Geffner, 2001). These ideas were later employed in solving the problem of planning under uncertainty (Blum & Langford, 1998; Weld,
Anderson, & Smith, 1998; Majercik & Littman, 2003; Yoon, Fern, & Givan, 2007; TeichteilKoenigsbuch, Infantes, & Kuter, 2008). Of these, approaches using forward heuristic search
related to the planning graph (Blum & Furst, 1997) have been very successful at the recent
international planning competitions (Yoon et al., 2007; Teichteil-Koenigsbuch et al., 2008).
Another approach to probabilistic planning is based on Markov decision processes (MDPs).
The fact that solutions to MDPs generate policies rather than action sequences is particuc
2011
AI Access Foundation. All rights reserved.

fiJoshi & Khardon

larly attractive for probabilistic planning, and this approach came to be known as Decision
Theoretic Planning (Boutilier, Dean, & Hanks, 1999a). Classical solution techniques for
MDPs, like value iteration (VI) (Bellman, 1957) and policy iteration (PI) (Howard, 1960),
are based on dynamic programming. These early solutions, however, require enumeration of
the state space. Owing to the curse of dimensionality (Bellman, 1957), even for reasonably
small problems, the state space can be very large. This can be seen easily for propositionally factored domains where the state is defined by N binary variables and the number of
possible states is 2N .
Several approaches were developed to handle such propositionally factored domains
(Boutilier, Dearden, & Goldszmidt, 1999b; Kearns & Koller, 1999; Guestrin, Koller, Parr, &
Venkataraman, 2003b; Hoey, St-Aubin, Hu, & Boutilier, 1999). One of the most successful,
SPUDD (Hoey et al., 1999), demonstrated that if the MDP can be represented using algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi,
1993), then VI can be performed entirely using the ADD representation thereby avoiding
the need to enumerate the state space. Propositionally factored representations show an
impressive speedup by taking advantage of the propositional domain structure. However,
they do not benefit from the structure that exists with objects and relations. Boutilier,
Reiter, and Price (2001) developed the foundations for provably optimal solutions of relational problems and provided the Symbolic Dynamic Programming (SDP) algorithm in
the context of situation calculus. This algorithm provided a framework for dynamic programming solutions of Relational MDPs that was later employed in several formalisms and
systems (Kersting, van Otterlo, & De Raedt, 2004; Holldobler, Karabaev, & Skvortsova,
2006; Sanner & Boutilier, 2009; Wang, Joshi, & Khardon, 2008).
The advantage of the relational representation is abstraction. One can plan at the abstract level without grounding the domain, potentially leading to more efficient algorithms.
In addition, the solution at the abstract level is optimal for every instantiation of the domain
and can be reused for multiple problems. However, this approach raises some difficult computational issues because one must use theorem proving to reason at the abstract level, and
because for some problems optimal solutions at the abstract level can be infinite in size. Following Boutilier et al. (2001) several abstract versions of the value iteration (VI) algorithm
have been developed using different representation schemes. For example, approximate
solutions based on linear function approximations have been developed and successfully applied to several problems from the international planning competitions (Sanner & Boutilier,
2009).
An alternative representation is motivated by the success of algebraic decision diagrams
in solving propositional MDPs (Hoey et al., 1999; St-Aubin, Hoey, & Boutilier, 2000).
Following this work, relational variants of decision diagrams have been defined and used for
VI algorithms (Wang et al., 2008; Sanner & Boutilier, 2009). Sanner and Boutilier report on
an implementation that did not scale well to yield exact solutions for large problems. Our
previous work (Wang et al., 2008) introduced First Order Decision Diagrams (FODD), and
developed algorithms and reduction operators for them. However, the FODD representation
requires non-trivial operations for reductions (to maintain small diagrams and efficiency)
leading to difficulties with implementation and scaling.
This paper develops several algorithmic improvements and extensions to the FODD
based solution that make the approach practical.
232

fiProbabilistic Planning with FODD

First, we introduce new reduction operators, named R10 and R11, that decrease the
size of the FODD representation. R10 makes a global analysis of the FODD and removes
many redundant portions of the diagram simultaneously. R11 works locally and targets a
particular redundancy that arises quite often when two FODDs are composed through a
binary operation; a procedure that is used repeatedly in the VI algorithm. We prove the
soundness of these reductions showing that when they are applied the diagrams maintain
their correct value.
Second, we present a novel FODD operation, sub-apart(A, B) that identifies minimal
conditions (in terms of variables) under which one FODD A dominates the value of another
FODD B. This new operation simultaneously expands the applicability of the R7 reduction
(Wang et al., 2008) to cover more situations and simplifies the test for its applicability, that
must be implemented in our system. We prove the soundness of this operation showing
that when it is applied with R7 the diagrams maintain their correct value.
Third, we present several techniques to speed up the FODD-based planning algorithm.
These include a sound simplification of one of the steps in the algorithm and in addition
several approximation techniques that can trade-off accuracy for improvements in run time.
Fourth, we extend the system to allow it to handle action costs and universal goals.
Incorporating all these ideas the paper presents FODD-Planner, a planning system for
solving relational stochastic planning problems using FODDs.
Fifth, we perform an experimental evaluation of the FODD-Planner system on several
domains, including problems from the recent international planning competition (IPC).
The experiments demonstrate that the new reductions provide significant speedup of the
algorithm and are crucial for its practicality. More importantly they show that the FODDPlanner exhibits competitive performance with top ranking systems from the IPC. To
our knowledge this is the first application of a pure relational VI algorithm without linear
function approximation to problems of this scale. Our results demonstrate that abstraction
through compact representation is a promising approach to stochastic planning.
The rest of the paper is organized as follows. Section 2 gives a short introduction
to relational MDPs and FODDs. Section 3 presents techniques to speed up the FODDPlanner. In section 4 we introduce new operators for removing redundancies in FODDs.
Section 5 describes the FODD-Planner system and in Section 6 we present the results of
experiments on planning domains from the IPC. Section 7 provides additional discussion of
related work and Section 8 concludes with a summary and ideas for future work.

2. Preliminaries
This section gives an overview of Relational MDPs, First Order Decision Diagrams and the
Symbolic Dynamic Programming algorithm.
2.1 Relational Markov Decision Processes
A Markov decision process (MDP) is a mathematical model of the interaction between an
agent and its environment (Puterman, 1994). Formally a MDP is a 5-tuple < S, A, T, R,  >
defining
 A set of fully observable states S.
233

fiJoshi & Khardon

 A set A of actions available to the agent.
 A state transition function T defining the probability P (s |s, a) of getting to state s
from state s on taking action a.
 A reward function R(s, a) defining the immediate reward achieved by the agent for
being in state s and taking action a. To simplify notation we assume that the reward
is independent of a so that R(s, a) = R(s). However the general case can be handled
in the same way.
 A discount factor 0    1 that captures the relative value of immediate actions over
future actions.
The objective of solving a MDP is to generate a policy that maximizes the agents total,
expected, discounted, reward. Intuitively, the expected utility or value of a state is equal
to the reward obtained in the state plus the discounted value of the state reached by the
best action in the state. This is captured by the Bellman equation as V (s) = M axa [R(s)
+ s P (s |s, a)V (s )]. The discount factor  < 1 guarantees that V (s) is finite even when
considering an infinite number of steps. For episodic tasks such as planning it provides
an incentive to find short solutions. The VI algorithm treats the Bellman equation as an
update rule V (s)  M axa [R(s) + s P (s |s, a)V (s )], and iteratively updates the value
of every state until convergence. Once the optimal value function is known, a policy can be
generated by assigning to each state the action that maximizes expected value.
A Relational MDP (RMDP) is a MDP where the world is represented by objects and
relations among them. A RMDP is specified by
1. A set of world predicates. Each literal, formed by instantiating a predicate using
objects from the domain, can be either true or false in a given state. For example in the boxworld domain, world literals are of the form box-in-city(box, city),
box-on-truck(box, truck), and truck-in-city(truck, city).
2. A set of action predicates. Each action literal formed by instantiating an action predicate using objects from the domain defines a concrete action. In the boxworld domain, actions have the form load-box-on-to-truck-in-city(box, truck, city), unload-box
-from-truck-in-city(box, truck, city), and drive-truck(truck, source.city, dest.city).
3. A state transition function that provides an abstract description of the probabilistic
move from one state to another. For example, using a STRIPS-like notation, the
transition defined by the action load-box-on-to-truck-in-city can be described as
Action: load-box-on-to-truck-in-city(box, truck, city):
Preconditions: box-in-city(box, city), truck-in-city(truck, city)
Outcome 1: Probability 0.8 box-on-truck(box, truck), box-in-city(box, city)
Outcome 2: Probability 0.2 nothing changes.
If the preconditions of the action, box-in-city(box, city) and truck-in-city(truck, city)
are satisfied, then with probability 0.8, the action will succeed generating the effect
box-on-truck(box, truck) and box-in-city(box, city). All other predicate instantiations remain unchanged. The state remains unchanged with probability 0.2.
234

fiProbabilistic Planning with FODD

4. An abstract reward function describing conditions under which rewards are obtained.
For example in the boxworld domain, the reward function is described as boxcity,
destination(box, city)  box-in-city(box, city) constructed so as to capture the goal
of transporting all boxes from their source cities to their respective destination cities.
Boutilier et al. (2001) developed SDP, the first VI algorithm for RMDPs. This was an
important theoretical result for RMDPs because for a finite horizon, SDP is guaranteed to
produce the optimal value function independent of the domain size. Thus the same value
function is applicable for a logistics problem with 2 cities, 2 trucks and 2 boxes, a logistics
problem with 100 cities, 1000 trucks and 2000 boxes, and any other instance of the domain.
One of the important ideas in SDP was to represent stochastic actions as deterministic
alternatives under natures control. This helps separate regression over deterministic action
alternatives from the probabilities of action effects. This separation is necessary when
transition functions are represented as relational schemas abstracting over the structure of
the states. The basic outline of the relational value iteration algorithm is as follows:
1. Regression: The n step-to-go value function Vn is regressed over every deterministic
variant Aj (~x) of every action A(~x) to produce Regr(Vn , Aj (~x)). At the first iteration V0 is assigned the reward function. This is not necessary for correctness of the
algorithm but is a convenient starting point for VI. Regr(Vn , Aj (~x)) describes the
conditions under which the action alternative Aj (~x) causes the state to transition to
some abstract state description in V n+1 .
2. Add Action Variants: The Q-function
A(~
x)

QVn

= R  [  j (prob(Aj (~x))  Regr(Vn , Aj (~x)))]

for each action A(~x) is generated. In this step the different alternatives of an action
are combined. Each alternative Aj (~x) produces Regr(Vn , Aj (~x)) from the regression
step. All the Regr(Vn , Aj (~x))s are then added each weighted by the probability of
A(~
x)
Aj (~x). This produces the parametrized function QVn which describes the utility of
being in a state and taking a concrete action A(~x).
A(~
x)

3. Object Maximization: Maximize over the action parameters of QVn to produce
x), thus obtaining the value achievable by the best ground
QA
Vn for each action A(~
instantiation of A(~x).
4. Maximize over Actions: The n + 1 step-to-go value function Vn+1 = maxA QA
Vn , is
generated.
In this description of the algorithm all intermediate constructs (R, P , V etc.) are
represented in some compact form and they capture a mapping from states to values or
probabilities. The operations of the Bellman update are performed over these functions
while maintaining the compact form. The variant of SDP developed in our previous work
(Wang et al., 2008) employed First Order Decision Diagrams to represent the intermediate
constructs.
235

fiJoshi & Khardon

p(x)
q(y)
q(y)
1
1

0

0
(a)

(b)

Figure 1: Two example FODDs. In these and all diagrams in the paper, left going edges
represent the branch taken when the predicate is true and right going edges
represent false branches.
2.2 First Order Decision Diagrams
This section briefly reviews previous work on FODDs and their use for relational MDPs
(Wang et al., 2008). We use standard terminology from First-Order logic (Lloyd, 1987).
A First Order Decision Diagram is a labeled directed acyclic graph, where each non-leaf
node has exactly 2 outgoing edges with true and false labels. The non-leaf nodes are
labeled by atoms generated from a predetermined signature of predicates, constants and an
enumerable set of variables. Leaf nodes have non-negative numeric values. The signature
also defines a total order on atoms, and the FODD is ordered with every parent smaller
than the child according to that order. Two examples of FODDs are given in Figure 1; in
these and all diagrams in the paper left going edges represent the true branches and right
edges are the false branches.
Thus, a FODD is similar to a formula in first order logic in that it shares some syntactic
elements. Its meaning is similarly defined relative to interpretations of the symbols. An
interpretation defines a domain of objects, identifies each constant with an object, and
specifies a truth value of each predicate over these objects. In the context of relational
MDPs, an interpretation represents a state of the world with the objects and relations
among them. Given a FODD and an interpretation, a valuation assigns each variable in
the FODD to an object in the interpretation. Following Groote and Tveretina (2003), the
semantics of FODDs are defined as follows. If B is a FODD and I is an interpretation,
a valuation  that assigns a domain element of I to each variable in B fixes the truth
value of every node atom in B under I. The FODD B can then be traversed in order to
reach a leaf. The value of the leaf is denoted M apB (I, ). M apB (I) is then defined as
max M apB (I, ), i.e. an aggregation of M apB (I, ) over all valuations . For example,
consider the FODD in Figure 1(a) and the interpretation I with objects a, b and where
the only true atoms are p(a), q(b). The valuations {x/a, y/a}, {x/a, y/b}, {x/b, y/a}, and
{x/b, y/b}, will produce the values 0, 1, 0, 0 respectively. By the max aggregation semantics,
M apB (I) = max{0, 1, 0, 0} = 1. Thus, this FODD is equivalent to the formula x, y, p(x)
q(y).
236

fiProbabilistic Planning with FODD

In general, max aggregation yields existential quantification when leaves are binary.
When using numerical values we can similarly capture value functions for relational MDPs.
Thus, every FODD with binary leaves has an equivalent formula in First-Order logic, where
all variables are existentially quantified. Conversely, every function free formula in FirstOrder logic, where the variables are existentially quantified, has an equivalent FODD representation.1 FODDs cannot capture universal quantification. Recently we introduced a
generalized FODD based formalism that does capture arbitrary quantifiers (Joshi, Kersting,
& Khardon, 2009); however it is more expensive to use computationally and it is not used
in this paper.
Akin to ADDs, FODDs can be combined under arithmetic operations, and reduced in
order to remove redundancies. Intuitively, redundancies in FODDs arise in two different
ways. The first, observes that some edges will never be traversed by any valuation. Reduction operators for such redundancies are called strong reduction operators. The second
requires more subtle analysis: there may be parts of the FODD that are traversed under
some valuations but because of the max aggregation, the valuations that traverse those
parts are never instrumental in determining the map. Operators for such redundancies are
called weak reductions operators. Strong reductions preserve M apB (I, ) for every valuation
 (thereby preserving M apB (I)) and weak reductions preserve M apB (I) but not necessarily
M apB (I, ) for every . Groote and Tveretina (2003) introduced four strong reduction operators (R1    R4). Wang et al. (2008) added the strong reduction operator R5. They also
introduced the notion of weak reductions and developed weak reduction operators (R6   
R9). Another subtlety arises because for RMDP domains we may have some background
knowledge about the predicates in the domain. For example, in the blocksworld, if block a
is clear then on(x, a) is false for all values of x. We denote such background knowledge by
B and allow reductions to rely on such knowledge. Below, we discuss the operator R7 in
some detail because of its relevance to the next section.
We use the following notation. If e is an edge from node n to node m, then source(e)
= n, target(e) = m and sibling(e) is the other edge out of n. For node n, the symbols
nt and nf denote the true and false edges out of n respectively. l(n) denotes the
atom associated with node n. Node formulas (NF) and edge formulas (EF) are defined
recursively as follows. For a node n labeled l(n) with incoming edges e1 , . . . , ek , the node
formula is NF(n) = (i EF(ei )). The edge formula for the true outgoing edge of n is
EF(nt ) = NF(n)  l(n). The edge formula for the false outgoing edge of n is EF(nf ) =
NF(n)  l(n). These formulas, where all variables are existentially quantified, capture
the conditions under which a node or edge are reached. Similarly, if B is a FODD and
p is a path from the root to a leaf in B, then the path formula for p, denoted by PF(p)
is the conjunction of literals along p. The variables of p, are denoted x~p . When x~p are
existentially quantified, satisfiability of PF(p) under an interpretation I is a necessary and
sufficient condition for the path p to be traversed by some valuation under I. If  is such
a valuation, then we define P athB (I, ) = p. The leaf reached by path p is denoted as
leaf (p). We let PF(p)\Lit denote the path formula of path p with the literal Lit removed
(if it was present) from the conjunction. B denotes background knowledge of the domain.
1. This can be seen by translating the formula f into a disjunctive normal form f = fi , representing
every conjunct fi as a FODD, and calculating their disjunction using the apply procedure of Wang et al.
(2008).

237

fiJoshi & Khardon

Figure 2: An example of the R7 reduction.

In the process of the algorithm, and also during reductions, we need to perform operations on functions represented as FODDs. Let B1 and B2 be two FODDs each representing
a function from states to real values (B1 : S  , B2 : S  ). Let B be the function such
that S, B(S) = B1 (S) + B2 (S). Wang et al. (2008) provide an algorithm for calculating
a FODD representation of B. We denote this operation by B = B1  B2 and similarly use
,  etc. to denote operations over diagrams.
The R7 Reduction: Weak reductions arise in two forms - edge redundancies and node redundancies. Corresponding to these, the R7 reduction operator (Wang et al., 2008) has two
variants - R7-replace (for removing redundant edges) and R7-drop (for removing redundant
nodes). An edge is redundant when all valuations going through it are dominated by other
valuations. Intuitively, given a FODD B and edges e1 and e2 in B, if for every valuation
going through edge e2 , there always is another valuation going through e1 that gives a better
value, we can replace target(e2 ) by 0 without affecting M apB (I) for any interpretation I.
Figure 2 shows an example of this reduction. In the FODD on the left, consider a valuation
reaching the 1 leaf by traversing the path p(x)p(y) under some interpretation I. Then we
can generate another valuation (by substituting the value of y for the value of x) that reaches
the 1 leaf through the path p(x). Therefore, intuitively the path p(x)  p(y) is redundant
and can be removed from the diagram. The R7-replace reduction formalizes this notion
with a number of conditions such that when certain combinations of these conditions are
satisfied, an edge reduction becomes applicable. For example, when the following conditions
occur together in a FODD, it can be reduced by replacing the target of edge e2 by the 0 leaf.
(P7.2) : B |= ~u, [[w,
~ EF(e2 )]  [~v , EF(e1 )]] where ~u are the variables that appear in
both target(e1 ) and target(e2 ), ~v the variables that appear in EF(e1 ) but are not in ~u, and
w
~ the variables that appear in EF(e2 ) but are not in ~u.
This condition requires that for every valuation 1 that reaches e2 there is a valuation
2 that reaches e1 such that 1 and 2 agree on all variables that appear in both target(e1 )
and target(e2 ).
(V7.3) : all leaves in D = target(e1 )  target(e2 ) have non-negative values, denoted as
D  0. In this case for any fixed valuation potentially reaching both e1 and e2 it is better
to follow e1 instead of e2 .
(S1) : There is no path from the root to a leaf that contains both e1 and e2 .

238

fiProbabilistic Planning with FODD

The operator R7-replace(e1 , e2 ) replaces target(e2 ) with a leaf valued 0. Notice that
the FODD in Figure 2 satisfies conditions P7.2, V7.3, and S1. For (P7.2) the shared
variable is z and it holds that z, [[xy, p(x)  p(y)]  [x, p(x)]]. (V7.3) holds because
target(e1 ) = target(e2 ) and D  0. With these definitions Wang et al. (2008) show that it
is safe to perform R7-replace when the conditions P7.2, V7.3, and S1 hold:
Lemma 1 ((Wang et al., 2008)) Let B be a FODD, e1 and e2 edges for which conditions
P7.2, V7.3, and S1 hold, and B  the result of R7-replace(e1 , e2 ), then for any interpretation
I we have MAPB (I) = MAPB  (I).
Similarly R7-drop formalizes conditions under which nodes can be dropped from the diagram. Several alternative conditions for the applicability of R7 (R7-replace and R7-drop)
are given by Wang et al. (2008). This provided a set of alternative conditions for applicability of R7 none of which dominates the others, with the result that effectively one has to
check all the conditions when reducing a diagram. The next section shows how the process
of applying R7 can be simplified and generalized.
R7 captures the fundamental intuition behind weak reductions and hence is widely
applicable. Unfortunately it is also very expensive to run. In practice R7-replace conditions
have to be tested for all pairs of edges in the diagram. Each test requires theorem proving
with disjunctive First-Order formulas.
2.3 VI with FODDs
In previous work (Wang et al., 2008) we showed how to capture the reward function and
the dynamics of the domain using FODDs and presented a value iteration algorithm along
the lines described in the last section. Reward and value functions are captured directly
using FODDs. Domains dynamics are captured by FODDs describing the probabilities of
action variants prob(Aj (~a)), and by special FODDs, Truth Value Diagrams (TVD), that
capture the deterministic effects of each action variant, similar to the successor state axioms
used by Boutilier et al. (2001). For every action variant Aj (~a) and each predicate schema
p(~x) the TVD T (A(~a), p(~x)) is a FODD with {0, 1} leaves. The TVD gives the truth value
of p(~x) in the next state when A(~a) has been performed in the current state. The TVDs
therefore capture action preconditions within the FODD structure and p(~x) is the potential
effect where the formalism specifies its truth value directly instead of saying whether it
changes or not. All operations that are needed in the SDP algorithm (regression, plus,
times, max) can be performed by special algorithms combining FODDs. The details of
these representations and algorithms were previously described (Wang et al., 2008) and
they are not directly needed for the discussion in this paper and thus omitted here.
On the other hand, direct application of these operations will yield large FODDs with
redundant structure and therefore, to keep the diagram size manageable, FODDs have
to be reduced at every step of the algorithm. Efficient and successful reductions are the
key to this procedure. The reductions R1-R9 (Groote & Tveretina, 2003; Wang et al.,
2008) provide a first step towards an efficient FODD system. However, they do not cover
all possible redundancies and they are expensive to apply in practice. Therefore a direct
implementation of these is not sufficient to yield an effective stochastic planner. In the
239

fiJoshi & Khardon

p(x)
e1

q(x)

p(y)
e2

10

q(x)

r(z)
3

q(x)
0

1

r(z)
2

0

Figure 3: FODD example showing applicability of Sub-Apart.

following sections we present new reduction operations and speedup techniques to make VI
with FODDs practical.

3. Speedup Techniques
This section presents two techniques to speed up the VI algorithm of Wang et al. (2008)
while maintaining an exact solution.
3.1 Subtracting Apart - Improving Applicability of R7
The applicability of R7 can be increased if certain branches have variables standardized
apart in a way that preserves the evaluation of the FODD under the max aggregation
semantics. Consider the FODD B in Figure 3. Intuitively a weak reduction is applicable
on this diagram because of the following argument. Consider a valuation  = {x \ 1, y \ 2,
z \ 3} crossing edge e2 under some interpretation I. Then I |= B  p(1)  p(2). Therefore
there must be a valuation  = {x \ 2, z \ 3} (and any value for y), that crosses edge e1 .
Now depending on the truth value of I |= B  q(1) and I |= B  q(2), we have four
possibilities of where  and  would reach after crossing the nodes target(e2 ) and target(e1 )
respectively. However, in all these cases, M apB (I, )  M apB (I, ). Therefore we should
be able to replace target(e2 ) by a 0 leaf. A similar argument shows that we should also be
able to drop the node source(e2 ). Surprisingly, though, none of the R7 conditions apply
in this case and this diagram cannot be reduced. On closer inspection we find that the
reason for this is that the conditions (P7.2) and (V7.3) are too restrictive. (V7.3)
holds but (P7.2) requires that x, z,[[y, p(x)  p(y)]  [p(x)]] implying that for every
valuation crossing edge e2 , there has to be another valuation crossing edge e1 such that
the valuations agree on the value of x and z and this does not hold. However, from our
argument above, for  to dominate , the two valuations need not agree on the value of x.
We observe that if we rename variable x so that its instances are different in the sub-FODDs
240

fiProbabilistic Planning with FODD

rooted at target(e1 ) and target(e2 ) (i.e. we standardized apart w.r.t. x) then both (P7.2)
and (V7.3) go through and the diagram can be reduced. Notice that for this type of
simplification to go through it must be the case that B1  B2  0 already holds. The more
variables we standardize apart the harder it is to keep this condition. To develop this
idea, we introduce a new FODD subtraction algorithm Sub-apart: Given diagrams B1 and
B2 the algorithm tries to standardize apart as many of their common variables as possible,
while keeping the condition B1  B2  0 true. The algorithm returns a 2-tuple {T, V },
where T is a Boolean variable indicating whether the combination can produce a diagram
that has no negative leaves when all variables except the ones in V are standardized apart.
The algorithm uses the standard recursive template for combining ADDs and FODDs
(Bahar et al., 1993; Wang et al., 2008) where a root node is chosen from the root of the two
diagrams and the operation is recursively performed on the corresponding sub-diagrams. In
addition when the roots of the two diagrams are identical Sub-apart considers the possibility
of making them different by standardizing apart. Sub-apart uses these recursive calls to
collect constraints specifying which variables cannot be standardized apart; these sets are
combined and returned to the calling procedure.
Procedure 1 Sub-apart(A, B)
1. If A and B are both leaves,
(a) If A  B  0 return {true, {}} else return {f alse, {}}
2. If l(A) < l(B), let
(a) {L, V1 } = Sub-apart(target(At ), B)
(b) {R, V2 } = Sub-apart(target(Af ), B)
Return {L  R, V1  V2 }
3. If l(A) > l(B), let
(a) {L, V1 } = Sub-apart(A, target(Bt ))
(b) {R, V2 } = Sub-apart(A, target(Bf ))
Return {L  R, V1  V2 }
4. If l(A) = l(B), let V be the variables of A (or B). Let
(a) {LL, V3 } = Sub-apart(target(At ), target(Bt ))
(b) {RR, V4 } = Sub-apart(target(Af ), target(Bf ))
(c) {LR, V5 } = Sub-apart(target(At ), target(Bf ))
(d) {RL, V6 } = Sub-apart(target(Af ), target(Bt )
(e) If LL  RR = f alse, return {f alse, V3  V4 }
(f ) If LR  RL = f alse return {true, V  V3  V4 }
(g) Return {true, V3  V4  V5  V6 }
241

fiJoshi & Khardon

The next theorem shows that the procedure is correct. The variables common to B1 and
B2 are denoted by ~u and B w~ denotes the combination diagram of B1 and B2 under the
subtract operation when all variables except the ones in w
~ are standardized apart. Let n1
and n2 be the roots nodes of B1 and B2 respectively.
Theorem 1 Sub-apart(n1 , n2 ) = {true, ~v } implies B~v contains no negative leaves and
Sub-apart(n1 , n2 ) = {f alse, ~v } implies w
~ such that w
~  ~u and B w~ contains no negative
leaves.
Proof: The proof is by induction on k, the sum of the number of nodes in B1 and B2 . For
the base case when k = 2, both B1 and B2 are single leaf diagrams and the statement is
trivially true. Assume that the statement is true for all k  m and consider the case where
k = m + 1. When l(n1 ) < l(n2 ), in the resultant diagram of combination under subtraction,
we expect n1 to be the root node and n1t  n2 and n1f  n2 to be the left and right
sub-FODDs respectively. Hence, the Sub-apart algorithm recursively calls Sub-apart(n1t ,
n2 ) and Sub-apart(n1f , n2 ). Since the sum of the number of nodes of the diagrams in the
recursive calls is always  m, the statement is true for both recursive calls. Clearly, the
top level can return a true iff both calls return true. In addition, if we keep the variables
in V1 and V2 (of step 2) in their original form (that is, not standardized apart) then for
both branches of the new root n1 we are guaranteed positive leaves and therefore the same
is true for the diagram rooted at n1 . A similar argument shows that the statement is true
when l(n1 ) > l(n2 ).
When l(n1 ) = l(n2 ), again by the inductive hypothesis, the statement of the theorem is
true for all recursive calls. Here we have 2 choices. We could either standardize apart the
variables V in l(n1 ) and l(n2 ) or keep them identical. If they are the same, in the resultant
diagram of combination under subtraction we expect n1 to be the root node and n1t  n2t
and n1f  n2f to be the left and right sub-FODDs respectively. Again the top level can
return a true iff both calls return true. The set of shared variables requires the variables
of l(n1 ) in addition to those from the recursive calls in order to ensure that l(n1 ) = l(n2 ).
If we standardize apart l(n1 ) and l(n2 ), then we fall back on one of the cases where n1
6= n2 except that the algorithm checks for the second level of recursive calls n1t  n2t ,
n1t  n2f , n1f  n2t and n1f  n2f . The top level of the algorithm can return true if
all four calls return true and return the union of the sets of variables returned by the four
calls. If not all four calls return true, the algorithm can still keep the variables in l(n1 ) and
l(n2 ) identical and return true if the conditions for that case are met.

The theorem shows that the algorithm is correct but does not guarantee minimality. In
fact, the smallest set of variables w
~ for B w~ to have no negative leaves may not be unique
(Wang et al., 2008). One can also show that the output of Sub-apart may not be minimal.
In principle, one can use a greedy procedure that standardizes apart one variable at a time
and arrives at a minimal set w.
~ However, although Sub-apart does not produce a minimal
set, we prefer it to the greedy approach because it is fast and often generates a small set w
~
in practice. We can now define new conditions for applicability of R7:
(V7.3S) : Sub-apart(target(e1 ), target(e2 )) = {true, V1 }.
(P7.2S) : B |=  V1 , [[w,
~ EF(e2 )]  [~v , EF(e1 )]] where as above and ~v , w
~ are the remaining variables (i.e. not in V1 ) in EF(e1 ), EF(e2 ) respectively.
242

fiProbabilistic Planning with FODD

(P7.2S) guarantees that whenever there is a 2 running through target(e2 ), there is always
a 1 running through target(e1 ) and 1 and 2 agree on V1 . (V7.3S) guarantees that under
this condition, 1 provides a better value than 2 . Using exactly the same proof as Lemma 1
given by Wang et al. (2008), we can show the following:
Lemma 2 Let B be a FODD, e1 and e2 edges for which conditions P7.2S, V7.3S, and
S1 hold, and B  the result of R7-replace(e1 , e2 ), then for any interpretation I we have
MAPB (I) = MAPB  (I).
Importantly, conditions (P7.2S) , (V7.3S) subsume all the previous conditions for applicability and safety of R7-replace that were previously given (Wang et al., 2008). Therefore, instead of testing for multiple conditions it is sufficient to test for (P7.2S) and
(V7.3S) . A very similar argument as the one above shows how Sub-apart extends and simplifies the conditions of R7-drop. Thus the use of Sub-apart both simplifies the conditions
to be tested and provides more opportunities for reductions. In our implementation, we
use the new conditions with Sub-apart whenever testing for applicability of R7-replace and
R7-drop.
3.2 Not Standardizing Apart
Recall that the FODD-based VI algorithm must add functions represented by FODDs (in
Steps 2 and 4) and take the maximum over functions represented by FODDs (in Step
4). Since the individual functions are independent functions of the state, the variables of
different functions are not related to one another. Therefore, before adding or maximizing,
the algorithm by Wang et al. (2008) standardizes apart the diagrams. That is, all variables
in the diagrams are given new names so they do not constrain each other. On the other hand,
since the different diagrams are structurally related this often introduces redundancies (in
the form of renamed copies of the same atoms) that must be removed by reduction operators.
However, our reduction operators are not ideal and avoiding this step can lead to significant
speedup in the system. Here we observe that for maximization (in Step 4) standardizing
apart is not needed and therefore can be avoided.
Theorem 2 Let B1 and B2 be FODDs. Let B be the result of combining B1 and B2
under the max operation when B1 and B2 are standardized apart. That is, s, MAPB (s) =
max{MAPB1 (s), MAPB2 (s)}. Let B  be the result of combining B1 and B2 under the max
operation when B1 and B2 are not standardized apart.  interpretations I, M apB (I) =
M apB  (I).
Proof: The theorem is proved by showing that for any I a valuation for the maximizing
diagram can be completed into a valuation over the combined diagram giving the same
value. Clearly M apB (I)  M apB  (I) since every substitution and path that exist for B 
are also possible for B. We show that the other direction holds as well. Let ~u be the
variables common to B1 and B2 . Let u~1 be the variables in B1 that are not in B2 and u~2
be the variables in B2 not in B1 . By definition, for any interpretation I,
M apB (I) = M ax[M apB1 (I), M apB2 (I)] = M ax[M apB1 (I, 1 ), M apB2 (I, 2 )]
243

fiJoshi & Khardon

Figure 4: FODD example illustrating the need for a DPO.

for some valuations 1 over ~uu~1 and 2 over ~uu~2 . Without loss of generality let us assume
that M apB1 (I, 1 ) = M ax[M apB1 (I, 1 ), M apB2 (I, 2 )]. We can construct valuation  over
~uu~1 u~2 such that  and 1 share the values of variables in ~u and u~1 . Obviously M apB1 (I, )
= M apB1 (I, 1 ). Also, by the definition of FODD combination, we have M apB  (I) 
M apB1 (I, ) = M apB (I).


4. Additional Reduction Operators
In this section we introduce two new reduction operators that improve the efficiency of the
VI algorithm. The following definitions are important in developing these reductions and
to understand potential scope for reducing diagrams.
Definition 1 A descending path ordering (DPO) is an ordered list of all paths from the
root to a leaf in FODD B, sorted in descending order by the value of the leaf reached by the
path. The relative order of paths reaching the same leaf can be set arbitrarily.
Definition 2 If B is a FODD, and P is the DPO for B, then a path pj  P is instrumental
with respect to P iff
1. there is an interpretation I and valuation, , such that P athB (I, ) = pj , and
2.  valuations , if P athB (I, ) = pk , then k  j.
The example in Figure 4 shows why a DPO is needed. The paths p(x)  p(y) and
p(x)  p(z) both imply each other. Whenever there is a valuation traversing one of the
paths there is always another valuation traversing the other. Removing any one path from
the diagram would be safe meaning that the map is not changed. But we cannot remove
both paths. Without an externally imposed order on the paths, it is not clear which path
should be labeled as redundant. A DPO does exactly that to make the reduction possible.
It is not clear at the outset how to best choose a DPO so as to maximally reduce the
size of a diagram. A lexicographic ordering over paths of equal value makes for an easy
implementation but may not be the best. We describe our heuristic approach for choosing
DPOs in the next section in the context of the implementation of the FODD-Planner.
244

fiProbabilistic Planning with FODD

4.1 The R10 Reduction
A path in FODD B is dominated if whenever a valuation traverses it, there is always
another valuation traversing another path and reaching a leaf of greater or equal value.
Now if all paths through an edge e are dominated, then no valuation crossing that edge will
ever determine the map under max aggregation semantics. In such cases we can replace
target(e) by a 0 leaf. This is the basic intuition behind the R10 operation.
Although its objective is the same as that of R7-replace, R10 is faster to compute in some
cases and has two advantages over R7-replace. First, because paths can be ranked by the
value of the leaf they reach, we can perform a single ranking and check for all dominated
paths (and hence all dominated edges). Hence, while all other reduction operators are
local, R10 is a global reduction. Second, the theorem proving required for R10 is always
on conjunctive formulas with existentially quantified variables, which is decidable in the
function free case (e.g., Khardon, 1999). This gives a speedup over R7-replace. On the
other hand R10 must explicitly enumerate the DPO and is therefore not efficient if the
FODD has an exponential number of non-zero valued paths. In such a case R7 or some
other edge based procedure is likely to be more efficient.
Consider the example shown in Figure 5. The following list specifies a DPO for this
diagram:
1. p(y), p(z), p(x)  3
2. p(y), p(z), p(x), q(x)  3
3. p(y), p(x), q(x)  2
4. p(y), p(z), p(x), q(x)  2
Notice that the relative order of paths reaching the same leaf in this DPO is defined by
ranking shorter paths higher than longer ones. This is not a requirement for the correctness
of the algorithm but is a good heuristic. According to the reduction procedure, all edges
of path 1 are important and cannot be reduced. However, since 1 subsumes 2, 3 and 4, all
the other edges (those belonging to paths 2, 3 and 4 and those not appearing in any of the
ranked paths) can be reduced. Therefore the reduction procedure replaces the targets of all
edges other than the ones in path 1, to the value 0. Path 1 is thus an instrumental path
but paths 2, 3 and 4 are not. This process is formalized in the following algorithm.
Procedure 2 R10(B)
1. Let E be the set of all edges in B
2. Let P = [p1 , p2    pn ] be a DPO for B. Thus p1 is a path reaching the highest leaf
and pn is a path reaching the lowest leaf.
3. For j = 1 to n, do the following
(a) Let Epj be the set of edges on pj
(b) If i, i < j such that B |= (x~pj , PF(pj ))  (x~pi , PF(pi )), then set E =
E  E pj
245

fiJoshi & Khardon

p(y)

p(y)
R10

p(z)

0

p(x)
q(x)

2

p(x)

q(x)

2

0

p(z)
0

0

0

p(x)

0

3

3
Figure 5: FODD example illustrating the R10 reduction.

4. For every edge e  E, set target(e) = 0 in B
In the example in Figure 5 none of the paths 2, 3 and 4 pass the conditions of step 3b
in the algorithm. Therefore their edges are not to be removed from E and are assigned
the value 0 by the algorithm. Here R10 is able to identify in one pass, the one path
(shown along a curved indicator line) that dominates all other paths. To achieve the same
reduction, R7-replace takes 2-3 passes depending on the order of application. Since every
pass of R7-replace has to check for implication of edge formulas for every pair of edges,
this can be expensive. On the other hand, there are cases where R10 is not applicable but
R7-replace is. An example of this is shown in the diagram in Figure 6. For this diagram it
is easy to see that if e2 is reached then so is e1 and e1 always gives a strictly better value.
R10 cannot be applied because it tests subsumption for complete paths. In this case the
path for e2 implies the disjunction of two paths going through e1 .
We next present a proof of correctness for R10. Lemma 3 shows that our test for
instrumental paths is correct. Lemma 4 shows that, as a result, edges marked for deletion
at the end of the algorithm do not belong to any instrumental path. The theorem uses this
fact to argue correctness of the algorithm.
Lemma 3 For any path pj  P , if pj is instrumental then i, i < j and B |= (x~pj ,
PF(pj ))  (x~pi , PF(pi )).
Proof: If pj is instrumental then by definition, there is an interpretation I and valuation,
, such that P athB (I, ) = pj , and  valuations ,  i < j such that P athB (I, ) = pi . In
other words, I |= [B  (x~pj , PF(pj ))] but I 6|= [B  (x~pi , PF(pi ))] for any i < j. This
implies that i, i < j and (B  x~pj , PF(pj )) |= (B  x~pi , PF(pi )). Hence i, i < j
and B |= [(x~pj , PF(pj ))  (x~pi , PF(pi ))].

Lemma 4 If E is the set of edges left at the end of the R10 procedure then if e  E then
there is no instrumental path that goes through e.
246

fiProbabilistic Planning with FODD

Figure 6: FODD example where R7 is applicable but R10 is not.

Proof: Lemma 3 proves that if a path pj is instrumental, then i, i < j and B |= [(x~pj ,
PF(pj ))  (x~pi , PF(pi ))]. Thus in step 3b of R10, if a path is instrumental, all its edges
are removed from E. Therefore if e  E at the end of the R10 procedure, it cannot be
in pj . Since pj is not constrained in any way in the argument above, e cannot be in any
instrumental path.

Theorem 3 Let B be any FODD. If B  = R10(B) then  interpretations I, M apB (I) =
M apB  (I).
Proof: By the definition of R10, the only difference between B and B  is that some
edges that pointed to sub-FODDs in B, point to the 0 leaf in B  . These are the edges
left in the set E at the end of the R10 procedure. Therefore any valuation crossing these
edges achieves a value of 0 in B  but could have achieved more value in B under the same
interpretation. Valuations not crossing these edges will achieve the same value in B  as they
did in B. Therefore for any interpretation I and valuation , M apB (I, )  M apB  (I, )
and hence M apB (I)  M apB  (I).
Fix any interpretation I and v = M apB (I). Let  be a valuation such that M apB (I, )
= v. If there is more than one  that gives the value v, we choose one whose path pj
has the least index in P . Now by definition pj is instrumental and by lemma 4, none of
the edges of pj are removed by R10. Therefore M apB  (I, ) = v = M apB (I). Finally, by
the definition of the max aggregation semantics, M apB  (I)  M apB  (I, ) and therefore

M apB  (I)  M apB (I).
The R10 procedure is similar to the reduction of decision list rules of ReBel (Kersting
et al., 2004). The difference, however, is that R10 is a reduction procedure for FODDs
and therefore uses the individual rules only as a subroutine to gather information about
redundant edges. Thus while ReBel removes paths R10 removes edges affecting multiple
paths in the diagram. The main potential disadvantage of R10 and the representation of
ReBel is the case where the number of paths is prohibitively large. In this case R7 or some
other edge based reduction is likely to be more efficient. As our experiments show this is not
the case on the IPC domains tested. In the general case, a meta-reduction heuristic trading
off the advantages of different operators would be useful. We discuss our implementation
and experimental results in the next sections.
247

fiJoshi & Khardon

4.2 The R11 Reduction
Consider the FODD B in Figure 1(a). Clearly, with no background knowledge this diagram cannot be reduced. Now assume that the background knowledge B contains a rule
x, [q(x)  p(x)]. In this case if there exists a valuation that reaches the 1 leaf, there must
be another such valuation  that agrees on the values of x and y.  dominates the other
valuations under the max aggregation semantics. The background knowledge rule implies
that for , the test at the root node is redundant. However, we cannot set the left child of
the root to 0 since the entire diagram will be eliminated. Therefore R7 is not applicable,
and similarly none of the other existing reductions is applicable. Yet redundancies like the
given example arise often in runs of the value iteration algorithm. This happens naturally,
without the artificial background knowledge used for our example but the corresponding
diagrams are too large to include in the text. The main reason for such redundancies is
that standardizing apart (which was discussed above) introduces multiple renamed copies
of the same atoms in the different diagrams. When the diagrams are added, many of the
atoms are redundant but some are not removed by old operators. These atoms may end
up in a parent-child relation with weak implication from child to parent, similar to the
example given. We introduce the R11 reduction operator that can handle such situations.
R11 reduces the FODD in Figure 1(a) to the FODD in Figure 1(b).
Let B be a FODD, n a node in B, e an edge such that e  {nt , nf }, e = sibling(e)
(so that when e = nt , e = nf and vice versa), and P the set of all paths from the root
to a non-zero leaf going through edge e. Then the reduction R11(B, n, e) drops node n
from diagram B and connects its parents to target(e). We need two conditions for the
applicability of R11. The first requires that the sibling is a zero valued leaf.
Condition 1 target(e ) = 0.
The second requires that valuations that are rerouted by R11 when traversing B  , that
is valuations that previously reached the 0 leaf and now traverse some path in P , are
dominated by other valuations giving the same value.


Condition 2 p  P , B |= [x~p , PF(p)\ne .lit  ne .lit]  [x~p , PF(p)].
The next theorem shows that R11 is sound. The proof shows that by condition 2 the
rerouted valuations do not add value to the diagram.
Theorem 4 If B  = R11(B, n, e), and conditions 1 and 2 hold, then  interpretations I,
M apB (I) = M apB  (I).
Proof: Let I be any interpretation and let Z be the set of all valuations. We can divide Z
into three disjoint sets depending on the path taken by valuations in B under I. Z e - the

set of all valuations crossing edge e, Z e - the set of all valuations crossing edge e and Z other
- the set of valuations not reaching node n. We analyze the behavior of the valuations in
these sets under I.
 Since structurally the only difference between B and B  is that in B  node n is bypassed, all paths from the root to a leaf that do not cross node n remain untouched.
Therefore   Z other , M apB (I, ) = M apB  (I, ).
248

fiProbabilistic Planning with FODD

 Since, in B  the parents of node n are connected to target(e), all valuations crossing
edge e and reaching target(e) in B under I will be unaffected in B  and will, therefore,
produce the same map. Thus   Z e , M apB (I, ) = M apB  (I, ).


 Now, let m denote the node target(e) in B. Under I, all valuations in Z e will reach
the 0 leaf in B but they will cross node m in B  . Depending on the leaf reached after

e
crossing node m, the set Z e can be further divided into 2 disjoint subsets. Zzero

e
the set of valuations reaching a 0 leaf and Znonzero - the set of valuations reaching a
e , M ap (I, ) = M ap  (I, ).
non-zero leaf. Clearly   Zzero
B
B


e
By the structure of B, every   Znonzero
, traverses some p  P , that is, (PF(p)\ne .lit

e
 n .lit) is true in I. Condition 2 states that for every such , there is another
valuation  such that (PF(p)) is true in I, so  traverses the same path. However,
every such valuation  must belong to the set Z e by the definition of the set Z e . In
e
is dominated by some valuation in Z e .
other words, in B  every valuation in Znonzero

From the above argument we conclude that in B  under I, every valuation either produces
the same map as in B or is dominated by some other valuation. Under the max aggregation

semantics, therefore, M apB (I) = M apB  (I).

5. FODD-Planner
In this section we discuss the system FODD-Planner that implements the VI algorithm
with FODDs. FODD-Planner employs a number of approximation techniques that yield
further speedup. The system also implements extensions of the basic VI algorithm that
allow it to handle action costs and universal goals. The following sections describe these
details.
5.1 Value Approximation
Reductions help keep the diagrams small in size by removing redundancies but when the
true n step-to-go value function itself is large, legal reductions cannot help. There are
domains where the true value function is unbounded. For example in the tireworld domain
from the international planning competition, where the goal is always to get the vehicle to
a destination city, one can have a chain of cities linked to one another up to the destination.
This chain can be of any length. Therefore when the value function is represented using
state abstraction, it must be unbounded. As a result SDP-like algorithms are less effective
on domains where the dynamics lead to such transitive structure and every iteration of value
iteration increases the size of the n step-to-go value function (Kersting et al., 2004; Sanner
& Boutilier, 2009). In other cases the value function is not infinite but is simply too large to
manipulate efficiently. When this happens we can resort to approximation keeping as much
of the structure of the value function as possible while maintaining efficiency. One must be
careful about the tradeoff here. Without approximation the runtime can be prohibitive and
too much approximation causes loss of structure and value. We next present three methods
to get approximations which act at different levels in the algorithm.
249

fiJoshi & Khardon

5.1.1 Not Standardizing Apart Action Variants
Standardizing apart the diagrams of action variants before adding them is required for the
correctness of the FODD based VI algorithm. That is, if we do not standardize apart action
variant diagrams before adding them, the value given to some states may be lower than the
true value (Wang et al., 2008). Intuitively, this is true since different paths in the value
function share atoms and variables. Now, for a fixed action, the best variable binding and
corresponding value for different action variants may be different. Thus, if the variables are
forced to be the same for the variants, we may rule out viable combinations of value. On the
other hand, the value obtained if we do not standardize apart is a lower bound on the true
value. This is because every path in the diagram resulting from not standardizing apart is
present in the diagram resulting from standardizing apart. Although the value is not exact,
not standardizing apart leads to more compact diagrams, and can therefore be useful in
speeding up the algorithm. We call this approximation method non-std-apart and use it as
a heuristic to speed up computation. Although this heuristic may cause loss of structure in
the representation of the value function, we have observed that in practice it gives significant
speedup while maintaining most of the relevant structure. This approximation is used in
some of the experiments described below.
5.1.2 Merging Leaves
The use of FODDs also allows us to approximate the value function in a simple and controlled way. Here we follow the approximation techniques of APRICODD (St-Aubin et al.,
2000) where they were used for propositional problems. The idea is to reduce the size of
the diagram by merging substructures that have similar values. One way of doing this is to
reduce the precision of the leaf values. That is, for a given precision value , we join leaves
whose value is within . This, in turn, leads to reduction of the diagram because subparts
of the diagram that previously pointed to different leaves, now point to the same leaf. The
granularity of approximation, however, becomes an extra parameter for the system and has
to be chosen carefully. Details are provided in the experiments below.
5.1.3 Domain Determinization
Previous work on stochastic planning has discovered that for some domains one can get good
performance by pretending that the domain is deterministic and re-planning if unexpected
outcomes are reached (Yoon et al., 2007). Here we use a similar idea and determinize the
domain in the process of policy generation. This saves significant amount of computation
and avoids the typical increase in size of the value function encountered in step 2 of the VI
algorithm. Domains can be determinized in many ways. We choose to perform determinization by replacing every stochastic action with its most probable deterministic alternative.
This is done only once prior to running VI. Although this method of determinization is
sub-optimal for many domains, it makes sense for domains where the most probable outcome corresponds to the successful execution of an action (Little & Thibaux, 2007) as is the
case in the domains we experimented with. Note that the determinization only applies to
the process of policy generation. When the generated policy is deployed to solve planning
problems, it does so under the original stochastic environment. This approximation is used
in some of the experiments described below.
250

fiProbabilistic Planning with FODD

5.2 Extensions of the VI Algorithm
FODD-Planner makes two additional extensions to the basic algorithm. This allows the
handling of action costs, arbitrary conjunctive goals as well as universal goals.
5.2.1 Handling Action Costs
The standard way to handle action costs is to replace R(s, a) by R(s, a)  Cost(s, a) in
the VI algorithm. However, our formalism using FODDs relies on the fact that all the
leaves (and thus values) are non-negative. To avoid this difficulty, we note that action costs
can be supported as long as there is at least one zero cost action. To see this recall the VI
algorithm. The appropriate place to add action costs is just before the Object Maximization
step. However, because this step is followed by maximizing over the action diagrams, if at
least one action has 0 cost (if not we can create a no-op action), the resultant diagram after
maximization will never have negative leaves. Therefore we safely convert negative leaves
before the maximization step to 0 and thereby avoid conflict with the reduction procedures.
5.2.2 Handling Universal Goals
FODDs with max aggregation cannot represent universal quantifiers. Therefore our VI
algorithm cannot handle universal goals at the abstract level (though see Joshi et al. (2009)
for a formalism that does accept arbitrary quantifiers). For a concrete planning problem
with a known set of objects we can instantiate the universal goal to get a large conjunctive
goal. In principle we can run VI and policy generation for this large conjunctive goal.
However, this would mean that we cannot plan off-line to get a generic policy and must
replan for each problem instance from scratch. Here we follow an alternative heuristic
approach previously introduced by Sanner and Boutilier (2009) and use an approximation
of the true value function, that results from a simple additive decomposition of the goal
predicates.
Concretely, during off-line planning we plan separately for a generic version of each
predicate. For example in the transportation domain discussed above we will plan for
the generic predicate box-in-city(box, city) as well as other individual predicates. Then at
execution time, when given a concrete goal, we approximate the true value function by the
sum of the generic versions over each ground goal predicate. This is clearly not an exact
calculation and will not work in every case. On the other hand, it considerably extends the
scope of the technique and works well in many situations.
5.3 The FODD-Planner System
We implemented the FODD-Planner system, plan execution routines and evaluation
routines under Yap Prolog 5.1.2. Our code and domain encodings as used in the experiments
reported in the next section are available at http://code.google.com/p/foddplanner/
under tag release11-JAIR2011.
Our implementation uses a simple theorem prover that supports background knowledge
by a procedure we call state flooding. That is, to prove B |= X  Y , where X is a ground
conjunction (represented in Prolog as a list), we flood X using rules of the background
knowledge using the following simple steps until convergence.
251

fiJoshi & Khardon

1. Generate Z, the set of all ground literals that can be derived from X and the rules of
background knowledge.

2. Set X = X  Z.

When X has converged we test for membership of Y in X. Because of our restricted
language, the reasoning problem is decidable and our theorem prover is complete.2
The overall algorithm is the same as SDP except that all operations are performed
on FODDs and reductions are applied to keep all intermediate diagrams compact. In
the experiments reported below, we use all previously mentioned reductions (R1    R11)
except R7-replace. We applied reductions iteratively until no reduction was applicable on
the FODD. There is no correct order to apply the reductions in the sense that any reduction
when applied can give rise to other reductions. Heuristically we chose an order where we
hope to get as much of the diagram reduced as soon as possible. We apply reductions in the
following order. We start by applying R10 twice with a different DPO each time. The first
DPO is generated by breaking ties in favor of shorter paths. The second is generated by
reversing the order of equal valued paths in the first DPO. With R10 we hope to catch many
redundant edges early. R10 is followed by R7-drop to remove redundant nodes connected
to the edges removed by R10. After this, we apply a round of all strong reductions followed
by R9 to remove the redundant equality nodes. R9 is followed by another round of strong
reductions. This sequence is performed iteratively until the diagram is stable. In the
FODD-Planner strong reductions are automatically applied every time two diagrams are
combined (using the apply algorithm (Wang et al., 2008)) and weak reductions are applied
every time two diagrams are combined except during regression by block combination.
We chose to apply R11 only twice in every iteration - once after regression and once just
before the next iteration. This setting for application of reduction operators is investigated
experimentally and discussed in Section 6.1.
To handle complex goals we use the additive goal decomposition. For each generic goal
atom g we run the system for the specified number of iterations, but at the last iteration we
do not perform step 4 of the algorithm. This yields a set of functions, Qg,A , parameterized
by action and generic goal that implicitly represent the policy. To improve on line execution
time using this policy we extract the set of paths from the Q functions and perform logical
simplification on these paths removing implied atoms and directly applying equalities when
they are in the path formula. This is the final form of the policy from the off-line planning
phase. for the on-line phase, given a concrete problem state and goal we identify potential
actions, and for each action find the top ranking rule for each concrete goal atom g. These
are combined to give the total value for each action and the action with the highest value
is chosen, breaking ties randomly if it is not unique.
2. An alternative to the list representation of X would have been to utilize the Prolog database to store
the literals of X and employ the Prolog engine to query Y . However, in our experience with Yap, it
becomes expensive to assert (and retract) the literals of X to (from) the Prolog database so that the list
representation is faster.

252

fiProbabilistic Planning with FODD

6. Experimental Results
We ran experiments on a standard benchmark problem as well as probabilistic planning
domains from the international planning competitions (IPC) held in 2004, 2006 and 2008.
The probabilistic track of the IPC provides domain descriptions in the PPDDL language
(Younes, Littman, Weissman, & Asmuth, 2005). We encoded the TVDs and probability and
reward functions for these domains by translating the PPDDL manually in a straightforward
manner.3 All experiments were run on a Linux machine with an Intel Pentium D processor
running at 3 GHz, with 2 GB of memory. Following IPC standards, all timings, rewards
and plan-lengths we report are averages over 30 rounds. For each domain, we constructed
by hand background knowledge restricting arguments of predicates (e.g. a box can only be
at one city in any time so Bin(b, c1 ), Bin(b, c2 )  (c1 = c2 )). As discussed above, this is
useful in the process of simplifying diagrams.
6.1 Merits of Reduction Operators
The following subsections present our main results showing performance in solving planning
problems from IPC. Before discussing these we first investigate and illustrate the merits of
the various reduction operators in terms of their effect on off-line planning time. The
experiments are performed on the tireworld and boxworld domains that are described in
more detail below. For this section is suffices to consider the domains as typical cases we
might have to address in solving planning problems and focus on the differences between
reductions.
In the first set of experiments we compare the run time with R7 and R10 in the context
of other reductions. Since R10 and R7 are both edge removal reductions and R7-drop is used
in conjunction with both, we compare R10 to R7-replace directly under all configurations
of R9 and R11. Except for the choice of reductions used the experimental setup is exactly
the same as detailed above. Figures 7 and 8 show the time to build a policy over varying
number of iterations for different settings of these weak reduction operators for the boxworld
and the tireworld domains. The figures clearly show the superiority of R10 over R7-replace.
All combinations with R7-replace have prohibitively large run times at 3 or 4 iterations.
With or without R9 and R11, R10 is orders of magnitude more efficient than R7-replace.
It is for this reason that in all future experiments we used R10 instead of R7-replace.
The experiments also demonstrate that without the new reduction operators presented in
this paper the FODD-Planner would be too slow to run sufficient iterations of the VI
algorithm as done in the following subsections to yield good planning performance. Figure 8
shows that for boxworld R11 hinders R7. It appears that in this case, the application of
R11 limits the applicability of R7 causing larger diagrams and thus further slowing down
VI.
Figures 9 and 10 show the relative merits of R9 and R11 in the presence of R10 for the
two domains. The figures are similar to the previous two plots except that we focus on the
relevant portion of the CPU time axis. Clearly R11 is an important reduction and it makes
3. The FODD formalism cannot capture all of PPDDL. In particular since FODDs cannot represent universal quantification, we cannot handle universal action preconditions. On the other hand FODDs can
handle universal action effects. Wang (2007) provides an algorithm and a detailed discussion of translation from PPDDL to FODDs.

253

fiJoshi & Khardon

Tireworld: R7 vs. R10

100000

CPU Time (seconds)

80000

R10
R10+R11
R10+R9
R10+R9+R11
R7
R7+R11
R7+R9
R7+R9+R11

60000

40000

20000

0

0

1

2

3

4
# of iterations

5

6

7

8

Figure 7: A comparison of planning time taken by various settings of reduction operators
over varying number of iterations for tireworld. Four settings of R10 are compared
against four settings of R7. All R7 variants do not complete 5 iterations within
the time range on the graph and therefore these points are not plotted.

Boxworld: R7 vs. R10
R10
R10+R11
R10+R9
R10+R9+R11
R7
R7+R11
R7+R9
R7+R9+R11

40000

CPU Time (seconds)

30000

20000

10000

0

0

1

2

3
# of iterations

4

5

6

Figure 8: A comparison of planning time taken by various settings of reduction operators
over varying number of iterations for boxworld. Four settings of R10 are compared
against four settings of R7. All R7 variants do not complete 5 iterations within
the time range on the graph and therefore these points are not plotted.

planning more efficient in both settings (just R10 and R10+R9). R9 is less effective in
tireworld. In boxworld, however the presence of R9 clearly improves planning efficiency for
both settings, and the best performance is achieved in the setting using R10+R9+R11. In
addition, R9 targets the removal of equality nodes which no other reduction does directly.
254

fiProbabilistic Planning with FODD

Tireworld: Merits of R9 and R11
25000

CPU Time (seconds)

20000

R10
R10+R11
R10+R9
R10+R9+R11

15000

10000

5000

0

0

1

2

3

4
# of iterations

5

6

7

8

Figure 9: A comparison of the merits of R9 and R11 in the presence of R10 for tireworld.
Boxworld: Merits of R9 and R11
30000

25000

CPU Time (seconds)

20000

R10
R10+R11
R10+R9
R10+R9+R11

15000

10000

5000

0
0

1

2

3
# of iterations

4

5

6

Figure 10: A comparison of the merits of R9 and R11 in the presence of R10 for boxworld.

Based on these results we choose the setting where we employ R10 along with R9 and R11
for the remaining experiments.
6.2 The Logistics Benchmark Problem
This is the boxworld problem introduced by Boutilier et al. (2001) that has been used as a
standard example for exact solution methods for relational MDPs. The domain consists of
boxes, cities and trucks. The objective is to get certain boxes to certain cities by loading,
unloading and driving. For the benchmark problem, the goal is the existence of a box in
Paris. The load and unload actions are probabilistic and the probability of success of unload
depends on whether it is raining or not. In this domain, all cities are reachable from each
other. As a result the domain has a compact abstract optimal value function. Note that for
this challenge domain there are no concrete planning instances to solve. Instead the goal
255

fiJoshi & Khardon

GPT
Policy Iteration with
policy language bias
Re-Engg NMRDPP
FODD-Planner

Coverage
100%

Time (ms)
2220

Reward
57.66

46.66%
10%
100%

60466
290830
231270

36
-387.7
70.0

Table 1: fileworld domain results
is to solve the off-line problem and produce the (abstract) optimal solution efficiently. The
domain description has 3 predicates of arity 2 and 3 actions each having 2 arguments.
Like ReBel (Kersting et al., 2004) and FOADD (Sanner & Boutilier, 2009) we are able
to solve this MDP and identify all relevant partitions of the optimal value function and
in fact the value function converges after 10 iterations. FODD-Planner performed 10
iterations in under 2 minutes.
6.3 The Fileworld Domain
This domain was part of the probabilistic track of IPC-4 (2004) (information on the competitions is accessible at http://ipc.icaps-conference.org/). The domain consists of
files and folders. Every file obtains a random assignment to a folder at execution time and
the goal is to place each file in its assigned folder. There is a cost of 100 to handle a folder
and a cost of 1 to place a file in a folder. The optimal policy for this domain is to first get
the assignments of files to folders and then handle each folder once, placing all files that
were assigned to it. The domain description has 8 predicates of arity 0 to 2 and 16 actions
with 0 to 1 arguments.
Results have been published for one problem instance which consisted of thirty files
and five folders. Since the goal is conjunctive we used the additive goal decomposition
discussed above. We used off-line planning for a generic goal f iled(a) and use the policy
to solve for any number of files. This domain is ideal for abstract solvers because the
optimal value function and policy for a generic goal are compact and can be found quickly.
The FODD-Planner was able to achieve convergence within 4 iterations even without
approximation. Policy generation and execution together took under 4 minutes. Of the 6
systems that competed on this track, results have been published for 3 on the website cited
above. Table 1 compares the performance of FODD-Planner to the others. We observe
that we rank ahead of all in terms of total reward and coverage (both FODD-Planner
and GPT achieve full coverage).
6.4 The Tireworld Domain
This domain was part of the probabilistic track of IPC-5 (2006). The domain consists of a
network of locations (or cities). A vehicle starts from one city and moves from city to city
with the objective of reaching a destination city. Moves can only be made between cities
that are directly connected by a road. In addition, on any move, the vehicle may lose a tire
with 40% probability. Some cities have a spare tire that can be loaded onto the vehicle. If
the vehicle contains a spare tire, the flat tire can be changed with 50% success probability.
This domain is simple but not trivial owing to the possibility of a complex network topology
256

fiProbabilistic Planning with FODD

Tireworld: Percentage Runs Solved vs. Problem Instance

100

Percentage Runs Solved

80

60

40
FOALP
FPG
Paragraph
FF-Replan
FODDPlanner

20

0
0

2

4

6

8
10
Problem Instance ID

12

14

16

Figure 11: Coverage result of tireworld experiments
Tireworld: Average Running Time vs. Problem Instance
1e+06

Average Running Time (ms)

FOALP
FPG
Paragraph
FF-Replan
100000 FODDPlanner

10000

1000

100

10
0

2

4

6

8

10

12

14

16

Problem Instance ID

Figure 12: Timing result of tireworld experiments
and high probabilities of failure. The IPC description of this domain has 5 predicates of
arity 0 to 2 and 3 actions with 0 to 2 arguments.
Participants at IPC-5 competed over 15 problem instances on this domain with varying
degree of difficulty. In problem 1 there were 16 locations which were progressively increased
by 2 per problem up to 44 locations in problem 15.
To limit off-line planning time we restricted FODD-Planner to 7 iterations without
any approximation for the first 3 iterations and with the non-std-apart approximation for the
remaining iterations. The policy was generated in 55 minutes; this together with the online
planning time is within the competition time bound. The performance of FODD-Planner
257

fiJoshi & Khardon

Tireworld: Average # Actions to Goal vs. Problem Instance

12

FOALP
FPG
Paragraph
FF-Replan
FODDPlanner

Average # Actions to Goal

10

8

6

4

2

0
0

2

4

6

8
10
Problem Instance ID

12

14

16

Figure 13: Plan length result of tireworld experiments
and systems competing in the probabilistic track of IPC-5, for which data is published, is
summarized in Figures 11, 12, and 13. The figures are indexed by problem instance and
show a comparison of the percentage of runs each planner was able to solve (coverage), the
average time per instance taken by each planner to generate an online solution, and the
average number of actions taken by each planner to reach the goal on every instance. We
observe that the overall performance of FODD-Planner is competitive with (and in a
few cases better than) the other systems. Runtimes to generate online solutions are high
for FODD-Planner but are comparable to FOALP which is the only other First-Order
planner. On the other hand, in comparison with the other systems, we are able to achieve
high coverage and short plans on many of the problems.
6.5 Value Approximation by Merging Leaves
Although the tireworld domain can be solved as above within the IPC time limit, one might
wish for even faster execution. As we show next, the heuristic of merging leaves provides
such a tool, potentially trading off quality of coverage and plan length for faster planning
and execution times. Table 2 shows the average reduction in planning time, coverage and
planning length achieved when the approximation merging leaves is used. The highest
reward obtained in any state is 500. We experimented with reducing precision on the leaves
with values between 50.0 and 150.0. As the results demonstrate, for some loss in coverage
and planning length, the system can gain in terms of execution time and planning time.
For example, with leaf precision of 50.0 (10% of the total value) we get 95.53% reduction
in planning time (22 fold speedup) but we lose 15.29% in coverage.4
4. Note that the measure of plan length, the average over problems solved, is not a good representation
of performance when coverage is not full. In this case, if coverage goes down by dropping the harder
problems with longer solutions, plan length will appear to be better, but this is clearly not an indication
of improved performance.

258

fiProbabilistic Planning with FODD

Precision
50
75
100
125
150

Reduction in
Planning Time
93.53%
98.13%
98.28%
99.65%
99.73%

Reduction in
Execution Time
88.3%
95.21%
95.21%
95.48%
95.61%

Reduction in
Coverage
15.29%
15.29%
15.29%
31.76%
31.76%

Reduction in
Plan length
14.54%
6.23%
6.23%
-30.86%
-30.86%

Table 2: Percentage average reduction in planning time, execution time, coverage and plan
length for tireworld under the approximation merging leaves for varying leaf precision values. For example, the first row of the table states that by reducing the
precision on the leaves to 50, which is 10% of the largest achievable reward in
any state, the planning time was reduced by 93.53% of its original value, average
execution time was reduced by 88.3%, average coverage was reduced by 15.29%
and average plan length was reduced by 14.54%

6.6 Boxworld
In this domain from IPC 2008, the world consists of boxes, trucks, planes and a map of
cities. The objective is to get boxes from source cities to destination cities using the trucks
and planes. Boxes can be loaded and unloaded from the trucks and planes. Trucks (and
planes) can be driven (flown) from one city to another as long as there is a direct road (or
air route) from the source to the destination city. The only probabilistic action is drive.
drive works as expected (transporting the truck from the source city to the destination
city) with probability 0.8. Occasionally drive teleports a truck to the wrong city. The IPC
description of this domain includes 11 predicates of arity 2 and 6 actions with 3 arguments.
IPC posted 15 problems with varying levels of difficulty for this domain. In all problems
the world consisted of 4 trucks and 2 airplanes. In problems 1 to 3 there were 10 boxes and
5 cities. Problems 4 and 5 had 10 boxes and 10 cities. Problems 6 and 7 had 10 boxes and
15 cities. Problems 8 and 9 had 15 boxes and 10 cities. Problems 10, 11 and 12 had 15 boxes
and 15 cities. Competition results show that RFF (Teichteil-Koenigsbuch et al., 2008) was
the only system that solved any of the 15 problems. Neither RFF nor FODD-Planner
could solve problems 13 to 15; hence we omit results for those.
To limit off-line planning time we determinized this domain (making drive deterministic)
and restricted FODD-Planner to 5 iterations. Since the domain was determinized, there
was only one alternative per action. Therefore the the non-std-apart approximation has
no effect here. The policy was generated in 42.6 minutes. The performance of FODDPlanner and RFF is summarized in Figures 14, 15, and 16. The figures show a comparison
of the percentage of runs each planner was able to solve (coverage), the average reward
achieved per problem instance, and the average number of actions taken by each planner
to reach the goal on every instance.
As can be seen FODD-Planner has lower coverage than RFF. However, our performance is close to RFF in terms of accumulated reward and consistently better in terms of
plan length even on problems where we achieve full coverage.
259

fiJoshi & Khardon

Boxworld: Percentage Runs Solved vs. Problem Instance

100

Percentage Runs Solved

80

FODDPlanner
RFF

60

40

20

0
0

2

4

6
8
Problem Instance ID

10

12

Figure 14: Coverage results of boxworld experiments

Boxworld: Average # Actions to Goal vs. Problem Instance
1800
1600

Average # Actions to Goal

1400

FODDPlanner
RFF

1200
1000
800
600
400
200
0
0

2

4

6

8

10

Problem Instance ID

Figure 15: Plan length results of boxworld experiments

260

12

fiProbabilistic Planning with FODD

Boxworld: Average Reward vs. Problem Instance
2000
1800
FODDPlanner
RFF

1600

Average Reward

1400
1200
1000
800
600
400
200
0
0

2

4

6
8
Problem Instance ID

10

12

Figure 16: Average reward results of boxworld experiments
In this domain we experienced long plan execution times (10 minutes per round on hard
problems and about 15 seconds per round on the easier problems). This points to the complexity of the instances and could be a one reason for the failure of other planning systems
at IPC where a strict time bound was observed, and for the failure of RFF on problems 13,
14 and 15. Thus, although the performance of our system is promising, reducing online execution time is crucial. As shown above, for some domains the technique of merging leaves
can lead to such improvement at the cost of some reduction in performance. Unfortunately,
for this domain merging leaves did not provide any advantage. As in tireworld, there is a
clear tradeoff between the quality of coverage and planning time. However the switch is
abrupt and to gain significantly in execution time one incurs a significant loss in coverage.
Improving the runtime for online application of our policies is an important aspect for future
work.

7. Related Work
The introduction briefly reviewed previous work on MDPs, propositionally factored MDPs
and RMDPs focusing on work that is directly related to the ideas used in this paper. There
have been several other solution formalisms for RMDPs that combine dynamic programming with other ideas to yield successful systems. These include approaches that combine
dynamic programming with linear function approximation (Sanner & Boutilier, 2009), forward search (Holldobler et al., 2006) and machine learning (Fern, Yoon, & Givan, 2006;
Gretton & Thiebaux, 2004). All of these yielded strong implementations that participated
in some planning competitions. Other works do not directly use dynamic programming.
For instance Guestrin, Koller, Gearhart, and Kanodia (2003a) present an approach using
additive value functions based on object classes and employ linear programming to solve
the RMDP. Mausam and Weld (2003) employ SPUDD (Hoey et al., 1999) to solve ground
instances of an RMDP, generate training data from the solutions and learn a lifted value
261

fiJoshi & Khardon

function from the training data using a relational tree learner. Gardiol and Kaelbling (2003)
apply methods from probabilistic planning to solve RMDPs.
In the most closely related work that preceded our effort, Sanner and Boutilier (2009)
developed a relational extension of linear function approximation techniques for factored
MDPs. The value function is represented as a weighted sum of basis functions, each denoting
a partition of the state space. The difference from the work on factored MDPs is that these
basis functions are First-Order formulas and thus the value function is valid for any domain
size (this is the same fundamental advantage that RMDP solvers have over ground MDP
solvers). They develop methods for automatic generation of First-Order constraints in a
linear program and automatic generation of basis functions that show promise in solving
some domains from the IPC. The work of Sanner and Boutilier is thus an extension of
the work on linear representations for propositionally factored MDPs (e.g., Guestrin et al.,
2003b) to capture relational structure. In a similar view the work on FODD-Planner is
a relational extension of the work on ADD based solvers for propositionally factored MDPs
(Hoey et al., 1999). In this context it is interesting to note that Sanner and Boutilier also
developed a relational extension of ADDs they call FOADDs. In contrast with FODDs,
nodes in FOADDs are labeled with closed First-Order formulas.5 Sanner and Boutilier
report on an implementation that was able to provide exact solutions for simple problems,
but they developed and applied the approach using linear function approximation for more
complex problems. Our experiments do use approximation and they demonstrate that
FODDs can be used to solve problems at least of the complexity currently employed in the
IPC.
Another important body of work is pursued by Relational Reinforcement Learning
(RRL) (Tadepalli, Givan, & Driessens, 2004) where techniques from reinforcement learning
are used to learn or construct value functions and policies for relational domains. RRL
followed from the seminal work of Dzeroski, De Raedt, and Driessens (2001) whose algorithm involved generating state-value pairs by state space exploration (biased in favor of
state-action pairs with high estimated value) and learning a relational value function tree
from the collected data. In a sense the First-Order decision trees used by Dzeroski et al.
(2001) are similar to FODDs. However, there is an important difference in the semantics
of these representations with strong implications for computational properties. While the
trees employ semantics based on traversal of a single path, FODD semantics are based on
aggregating values generated by traversal of multiple paths. We have previously argued
(Wang et al., 2008) that the FODD semantics are much better suited for dynamic programming solutions. There have been several approaches to RRL in recent years showing nice
performance (for example, Driessens & Dzeroski, 2004; Kersting & De Raedt, 2004; Walker,
5. As discussed by Sanner and Boutilier (2009) it is hard to characterize the exact relationship between
FOADDs and FODDs in terms of representation and computational properties. An anonymous reviewer
kindly provided the following example that shows that in some cases FODDs might be more compact than
FOADDs. Consider a domain with n unary predicates A1 (), . . . , An () capturing some object properties
and consider the formula x, A1 (x) Xor A2 (x) Xor . . . Xor An (x) where n is odd. The formula requires
that there exists an object for which an odd number of properties Ai () hold. Due to their restriction to
use only the connectives And, Or and Not, the FOADDs must rewrite this formula in a way that yields a
representation (for example in its DNF form) whose size is exponential in the number of predicates. On
the other hand, one can represent this formula with a linear size FODD, similar to the representation of
parity functions with propositional BDDs.

262

fiProbabilistic Planning with FODD

Torrey, Shavlik, & Maclin, 2007; Croonenborghs, Ramon, Blockeel, & Bruynooghe, 2007)
although they are applied to problems of smaller scale than the ones from the IPC. An
excellent overview of the various solutions methods for RMDPs is provided by van Otterlo
(2008).

8. Conclusion and Future Work
The main contribution of this paper is the introduction of FODD-Planner, a relational
planning system based on First Order Decision Diagrams. This is the first planning system
that uses lifted algebraic decision diagrams as its representation language and successfully
solves planning problems from the international planning competition. FODD-Planner
provides several improvements over previous work on FODDs (Wang et al., 2008). The improvements include the reduction operators R10, R11 the Sub-apart operator, and several
speedup and value approximation techniques. Taken together, these improvements provide substantial speedup making the approach practical. Therefore, the results show that
abstraction through compact representation is a promising approach to stochastic planning.
Our work raises many questions concerning foundations for FODDs and their application to solve RMDPs. The first is the question of reductions. Our set of reductions is
still heuristic and does not guarantee a canonical form for diagrams which is instrumental
for efficiency of propositional algorithms. Identifying such complete sets of reductions
operators and canonical forms is an interesting challenge. Identifying a practically good
set of operators trading off complexity for reduction power is crucial for further applicability. In recent work (Joshi, Kersting, & Khardon, 2010) we developed practical variants of
model-checking reductions (Joshi et al., 2009) demonstrating significant speedup over the
system presented here. Another improvement may be possible by using the FODD based
policy iteration algorithm (Wang & Khardon, 2007). This may allow us to avoid approximation of infinite size value functions in cases where the policy is still compact. Another
direction is the use of the more expressive GFODDs (Joshi et al., 2009) that can handle arbitrary quantification and can therefore be applied more widely. Finally this work suggests
the potential of using FODDs as the underlying representation for relational reinforcement
learning. Therefore, it will be interesting to develop learning algorithms for FODDs.

Acknowledgments
This work was partly supported by NSF grants IIS 0936687 and IIS 0964457. Saket Joshi
was additionally supported by a Computing Innovation Postdoctoral Fellowship. Some
of the experiments reported in this paper were performed on the Tufts Linux Research
Cluster supported by Tufts UIT Research Computing. We thank Kristian Kersting for
valuable input on the system and insightful discussions.

References
Bahar, R., Frohm, E., Gaona, C., Hachtel, G., Macii, E., Pardo, A., & Somenzi, F. (1993).
Algebraic decision diagrams and their applications. In IEEE /ACM ICCAD, pp.
188191.
263

fiJoshi & Khardon

Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90(1-2), 279298.
Blum, A., & Langford, J. (1998). Probabilistic planning in the graphplan framework. In
Proceedings of the Fifth European Conference on Planning, pp. 812.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129,
533.
Boutilier, C., Dean, T., & Hanks, S. (1999a). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research, 11,
194.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1999b). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121, 49107.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming for First-Order
MDPs. In Proceedings of the International Joint Conference of Artificial Intelligence,
pp. 690700.
Croonenborghs, T., Ramon, J., Blockeel, H., & Bruynooghe, M. (2007). Online learning
and exploiting relational models in reinforcement learning. In Proceedings of the
International Joint Conference of Artificial Intelligence, pp. 726731.
Driessens, K., & Dzeroski, S. (2004). Integrating guidance into relational reinforcement
learning. Machine Learning, 57, 271304.
Dzeroski, S., De Raedt, L., & Driessens, K. (2001). Relational reinforcement learning.
Machine Learning, 43, 752.
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration with a policy language
bias. Journal of Artificial Intelligence Research, 25(1), 75118.
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2(3-4), 189208.
Gardiol, N., & Kaelbling, L. (2003). Envelope-based planning in relational MDPs. In Proceedings of the International Conference on Neural Information Processing Systems,
pp. 10401046.
Gretton, C., & Thiebaux, S. (2004). Exploiting First-Order regression in inductive policy
selection. In Proceedings of the Workshop on Uncertainty in Artificial Intelligence.
Groote, J., & Tveretina, O. (2003). Binary decision diagrams for First-Order predicate
logic. Journal of Logic and Algebraic Programming, 57, 122.
Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans to new
environments in relational MDPs. In Proceedings of the International Joint Conference
of Artificial Intelligence, pp. 10031010.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003b). Efficient solution algorithms
for factored MDPs. Journal of Artificial Intelligence Research, 19, 399468.
264

fiProbabilistic Planning with FODD

Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. In Proceedings of the Workshop on Uncertainty in Artificial
Intelligence, pp. 279288.
Holldobler, S., Karabaev, E., & Skvortsova, O. (2006). FluCaP: a heuristic search planner
for First-Order MDPs. Journal of Artificial Intelligence Research, 27, 419439.
Howard, R. (1960). Dynamic Programming and Markov Processes. MIT Press.
Joshi, S., Kersting, K., & Khardon, R. (2009). Generalized First-Order decision diagrams
for First-Order Markov decision processes. In Proceedings of the International Joint
Conference of Artificial Intelligence, pp. 19161921.
Joshi, S., Kersting, K., & Khardon, R. (2010). Self-Taught decision theoretic planning
with First-Order decision diagrams. In Proceedings of the International Conference
on Automated Planning and Scheduling, pp. 8996.
Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and
stochastic search. In Proceedings of the National Conference of the American Association for Artificial Intelligence, pp. 11941201.
Kearns, M., & Koller, D. (1999). Efficient reinforcement learning in factored MDPs. In
Proceedings of the International Joint Conference of Artificial Intelligence, pp. 740
747.
Kersting, K., & De Raedt, L. (2004). Logical Markov decision programs and the convergence
of logical TD(). In Proceedings of Inductive Logic Programming, pp. 180197.
Kersting, K., van Otterlo, M., & De Raedt, L. (2004). Bellman goes relational. In Proceedings
of the International Conference on Machine Learning, pp. 465472.
Khardon, R. (1999). Learning function free Horn expressions. Machine Learning, 37 (3),
249275.
Little, I., & Thibaux, S. (2007). Probabilistic planning vs. replanning. In Proceedings of
the ICAPS Workshop on IPC: Past, Present and Future.
Lloyd, J. (1987). Foundations of Logic Programming. Springer Verlag. Second Edition.
Majercik, S., & Littman, M. (2003). Contingent planning under uncertainty via stochastic
satisfiability. Artificial Intelligence, 147 (1-2), 119162.
Mausam, & Weld, D. (2003). Solving relational MDPs with First-Order machine learning. In
Proceedings of the ICAPS Workshop on Planning under Uncertainty and Incomplete
Information.
Penberthy, J., & Weld, D. (1992). UCPOP: A sound, complete, partial order planner for
ADL. In Principles of Knowledge Representation and Reasoning, pp. 103114.
Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques for First-Order MDPs.
Artificial Intelligence, 173, 748788.
265

fiJoshi & Khardon

St-Aubin, R., Hoey, J., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. In Proceedings of the International Conference on Neural
Information Processing Systems, pp. 10891095.
Tadepalli, P., Givan, R., & Driessens, K. (2004). Relational reinforcement learning: An
overview. In Proceedings of the International Conference on Machine Learning 04
Workshop on Relational Reinforcement Learning.
Teichteil-Koenigsbuch, F., Infantes, G., & Kuter, U. (2008). RFF: A robust FF-based MDP
planning algorithm for generating policies with low probability of failure. In Sixth
IPC at ICAPS.
van Otterlo, M. (2008). The logic of Adaptive behavior: Knowledge representation and
algorithms for adaptive sequential decision making under uncertainty in First-Order
and relational domains. IOS Press.
Walker, T., Torrey, L., Shavlik, J., & Maclin, R. (2007). Building relational world models for
reinforcement learning. In Proceedings of Inductive Logic Programming, pp. 280291.
Wang, C. (2007). First-Order Markov decision processes. Ph.D. thesis, Tufts University.
Wang, C., Joshi, S., & Khardon, R. (2008). First-Order decision diagrams for relational
MDPs. Journal of Artificial Intelligence Research, 31, 431472.
Wang, C., & Khardon, R. (2007). Policy iteration for relational MDPs. In Proceedings of
the Workshop on Uncertainty in Artificial Intelligence, pp. 408415.
Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan to handle uncertainty and
sensing actions. In Proceedings of the National Conference on Artificial Intelligence.
Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: A baseline for probabilistic planning. In
Proceedings of the International Conference on Automated Planning and Scheduling,
pp. 352359.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). The first probabilistic track
of the international planning competition. Journal of Artificial Intelligence Research,
24 (1), 851887.

266

fiJournal of Artificial Intelligence Research 41 (2011) 407-444

Submitted 02/11; published 07/11

Efficient Multi-Start Strategies for Local Search Algorithms
Andras Gyorgy

gya@szit.bme.hu

Machine Learning Research Group
Computer and Automation Research Institute
of the Hungarian Academy of Sciences
1111 Budapest, Hungary

Levente Kocsis

kocsis@sztaki.hu

Data Mining and Web Search Research Group, Informatics Laboratory
Computer and Automation Research Institute
of the Hungarian Academy of Sciences
1111 Budapest, Hungary

Abstract
Local search algorithms applied to optimization problems often suffer from getting
trapped in a local optimum. The common solution for this deficiency is to restart the
algorithm when no progress is observed. Alternatively, one can start multiple instances of
a local search algorithm, and allocate computational resources (in particular, processing
time) to the instances depending on their behavior. Hence, a multi-start strategy has to
decide (dynamically) when to allocate additional resources to a particular instance and
when to start new instances. In this paper we propose multi-start strategies motivated
by works on multi-armed bandit problems and Lipschitz optimization with an unknown
constant. The strategies continuously estimate the potential performance of each algorithm
instance by supposing a convergence rate of the local search algorithm up to an unknown
constant, and in every phase allocate resources to those instances that could converge to
the optimum for a particular range of the constant. Asymptotic bounds are given on the
performance of the strategies. In particular, we prove that at most a quadratic increase in
the number of times the target function is evaluated is needed to achieve the performance
of a local search algorithm started from the attraction region of the optimum. Experiments
are provided using SPSA (Simultaneous Perturbation Stochastic Approximation) and kmeans as local search algorithms, and the results indicate that the proposed strategies work
well in practice, and, in all cases studied, need only logarithmically more evaluations of the
target function as opposed to the theoretically suggested quadratic increase.

1. Introduction
Local search algorithms applied to optimization problems often suffer from getting trapped
in a local optimum. Moreover, local search algorithms that are guaranteed to converge
to a global optimum under some conditions (such as Simulated Annealing or Simultaneous
Perturbation Stochastic Approximation, SPSA, see, e.g., Spall, Hill, & Stark, 2006), usually
converge at a very slow pace when the conditions are satisfied. On the other hand, if the
algorithms are employed with more aggressive settings, much faster convergence to local
optima is achievable, but with no guarantee to find the global optimum. The common soluc
2011
AI Access Foundation. All rights reserved.

fiGyorgy & Kocsis

tion to escape from a local optimum is to restart the algorithm when no progress is observed
(see e.g., Mart, Moreno-Vega, & Duarte, 2010; Zabinsky, Bulger, & Khompatraporn, 2010,
and the references therein).
Alternatively, one can start multiple instances of the local search algorithm, and allocate
computational resources, in particular, processing time, to the instances depending on their
behavior. Instances can be started at any time, and so the number of instances may grow
over time depending on the allocation strategy. (see, e.g., Chapter 10 of Battiti, Brunato, &
Mascia, 2008 and the references therein). In this type of problems the computational cost
is usually measured as the total number of steps made by all search algorithm instances:
this often reflects the situation that the evaluation of the target function to be optimized
is expensive, and the costs related to determine which algorithms to use next are negligible
compared to the former (e.g., this is clearly the case if the task is to tune the parameters of
a system whose performance can only be tested via lengthy experiments, see, e.g., BartzBeielstein, 2006; Hutter, Hoos, Leyton-Brown, & Stutzle, 2009). In this paper we address
the above problem of dynamically starting several instances of local search algorithms and
allocating resources to the instances based on their (potential) performance.
To our knowledge, solutions to the above problem have either been based on heuristics
or on the assumption that the local optima the search algorithms converge to have an
extreme value distribution (see Section 2 below). In this paper, we propose new multi-start
strategies under very mild conditions on the target function, with attractive theoretical
and practical properties: Supposing a convergence rate of the local search algorithms up
to an unknown constant, our strategies continuously estimate the potential performance of
each algorithm instance and in every phase allocate resources to those instances that could
converge to the optimum for a particular range of the constant. The selection mechanism
is analogous to the DIRECT algorithm (Jones, Perttunen, & Stuckman, 1993; Finkel &
Kelley, 2004; Horn, 2006) for optimizing Lipschitz-functions with an unknown constant,
where preference is given to rectangles that may contain the global optimum. The optimum
within each rectangle is estimated in an optimistic way, and the estimate depends on the
size of the rectangle. In our strategies we use the function describing the convergence rate
of the local search algorithms in a similar way as the size of the rectangles are used in the
DIRECT algorithm.
Since in the proposed multi-start strategies the potential performance of each local search
algorithm is continuously estimated from the currently best value of the target function
returned by that algorithm, our method is restricted to work with local search algorithms
that return the best known value of the target function after each step. This is the case,
for example, in certain meta-learning problems, where the goal is to find a good parameter
setting of a learning algorithm. Here the search space is the parameter space of the learning
algorithm, and one step of the local search methods means running the learning algorithm
completely on a possibly very large data set. On the other hand, if the local search algorithm
is some sort of a gradient search optimizing an error function over some training data, then
the value of the target function is usually available only in the case of batch learning
(potentially after some very cheap computations), but not when the gradient is estimated
only from a few samples.
The rest of the paper is organized as follows. Section 2 summarizes related research.
The problem is defined formally in Section 3. The new multi-start local search strategies of
408

fiEfficient Multi-Start Strategies for Local Search Algorithms

this paper are described and analyzed in Section 4: in Section 4.1 we deal with a selection
mechanism among a fixed number of instances of the local search algorithm, while, in
addition, simple schedules for starting new instances are also considered in Section 4.2,
which are natural extensions of the case of finitely many local search algorithm instances.
This section concludes with a discussion of the results in Section 4.3. Simulation results
on real and synthetic data are provided in Section 5. Conclusions and future work are
described in Section 6.

2. Related Work
The problem of allocating resources among several instances of search algorithms can be
comfortably handled in a generalized version of the maximum K-armed bandit problem.
The original version of this problem consists of several rounds, where in each round one
chooses one of K arms, receives some reward depending on the choice, with the goal of
maximizing the highest reward received over several rounds. This model can easily be used
for our problem by considering each local search algorithm instance as an arm: pulling an
arm means taking one additional step of the corresponding algorithm, that is, evaluating
the target function at a point suggested by that algorithm, and the reward received is the
value of the target function at the sampled point. A generic algorithm for the standard
maximum K-armed bandit problem, where each reward is assumed to have independent and
identical distribution, is provided by Adam (2001), where the so-called reservation price of
an instance is introduced, which gives the maximum amount of resources worth to spend
on an instance: if an instance achieves its reservation price, it is useless to select it again.
The computation of the reservation price depends on a model of the algorithm that can be
learnt under some specific constraints.
Now consider a scenario where several instances of some, possibly randomized local
search algorithms are run after each other with the goal of maximizing the expected performance. Each instance is run until it terminates. In this scenario it is natural to assume
that the values returned by the instances (usually some local optima of the target function) are independent. Furthermore, since good search algorithms follow (usually heuristic)
procedures that yield substantially better results than pure random guessing, Cicirello and
Smith (2004, 2005) suggested that the rewards (evaluated target function values) of the
search instances may be viewed as the maximum of many random variables (if the instances are run for sufficiently long time), and hence may be modeled by extreme value
distributions. Several algorithms are based on this assumption, and are hence developed
for the maximum K-armed bandit problem with returns following generalized extreme value
distributions: Cicirello and Smith apply (somewhat heuristic) methods that use the above
extreme-value-distribution assumption at each decision point of a meta-learning algorithm,
while Streeter and Smith (2006a) use this model to obtain upper confidence bounds on the
performance estimate of each type of algorithms used and then try only the algorithms
with the best expected result. The latter is a theoretically justified example of the natural
strategy to probe the algorithm instances for a while, estimate their future performance
based on the results of this trial phase, and then use the most promising algorithm for the
time remaining. Streeter and Smith (2006b) proposed a distribution free approach that
409

fiGyorgy & Kocsis

combines a multi-armed bandit exploration strategy with a heuristic selection among the
available arms.
While in the standard maximum K-armed bandit problem the rewards in each round
are assumed to be independent, this is clearly not the case in our situation where the
algorithm instances are run parallel and the reward for evaluating the target function at a
point is the improvement upon the current maximum, since the samples chosen by a local
search algorithm usually depend on previous samples. Nevertheless, the ideas and lessons
learnt from the maximum K-armed bandit problems can be used in our case, as well: for
example, the algorithm Threshold Ascent of Streeter and Smith (2006b) gives reasonably
good solutions in our case, or the principle of probing instances for a while and then using
the most promising in the time remaining also carries over to this situation easily: such
algorithms, having first an exploration then an exploitation phase, will be referred to in
the sequel as explore-and-exploit algorithms. In this class of algorithms, simple rules were
suggested by Beck and Freuder (2004) to predict the future performance of each algorithm,
while Carchrae and Beck (2004) employ Bayesian prediction.
Another related problem is to find fast algorithms among several ones that solve the
same problem. More precisely, several algorithm instances are available that all produce
the correct answer to a certain question if run for a sufficiently long time. The time needed
for an algorithm instance to find the answer is assumed to be a random quantity with
independent and identical distributions for all the instances, and the goal is to combine the
given algorithms to minimize the expected running time until the answer is found. When the
distribution of the running time is known, an optimal non-adaptive time-allocation strategy1
is to perform a sequence of runs with a certain cut-off time that depends on the distribution
(Luby, Sinclair, & Zuckerman, 1993). If the distribution is unknown, a particular running
time sequence can be chosen that results in an expected total running time that is only a
logarithmic factor larger than the optimum achievable if the distribution is known. We note
that this strategy is among the few that provide a schedule that increases the number of
algorithm instances. The above set-up can be specialized to our problem: the goal is to find
an -optimal approximation of the optimum and the running time is the number of steps
needed by the given search algorithm to achieve such an approximation. Note that in this
case the running time of any algorithm instance providing an -suboptimal solution has to
be defined to be infinity, but the results of Luby et al. remain valid if an -optimal solution
can be found with positive probability. For the same problem, Kautz, Horvitz, Ruan,
Gomes, and Selman (2002) proposed an allocation strategy based on updating dynamically
the belief over the run-time distribution. Concerning the latter, Hoos and Stutzle (1999)
found empirically that run-time distributions are approximately exponential in certain (NPhard) problems, while Ribeiro, Rosseti, and Vallejos (2009) dealt with the comparison of
different run-time distributions.
Finally, when a set of time allocation strategies are available and the optimization problem is to be solved several times, one can use the standard multi-armed bandit framework
as done by Gagliolo and Schmidhuber (2006, 2007, 2010).
Running several instances of an algorithm or several algorithms in parallel and selecting
among the algorithms have been intensively studied, for example, in the area of meta1. In a non-adaptive time-allocation strategy the running time of an algorithm instance is fixed in advance,
that is, the measured performance of the algorithm instances has no effect on the schedule.

410

fiEfficient Multi-Start Strategies for Local Search Algorithms

learning (Vilalta & Drissi, 2002) or automatic algorithm configuration (Hutter et al., 2009).
The underlying problem is very similar in both cases: automatic algorithm configuration
usually refers to tuning search algorithms, while meta-learning is used for a subset of these
problems, tuning machine learning algorithms (the latter often allows more specific use of
the data). The main problem here is to allocate time slices to particular algorithms with the
aim of maximizing the best result returned. This allocation may depend on the intermediate
performance of the algorithms. Most of the automatic algorithm configuration and metalearning systems use various heuristics to explore the space of algorithms and parameters
(see, e.g., Hutter et al., 2009).
Finally, it is important to note that, although multi-start local search strategies solve
global optimization problems, we concentrate on maximizing the performance given the
underlying family of local optimization methods. Since the choice of the latter has a major
effect on the achievable performance, we do not compare our results to the vast literature
on global optimization.

3. Preliminaries
Assume we wish to maximize a real valued function f on the d-dimensional unit hypercube
[0, 1]d , that is, the goal is to find a maximizer x  [0, 1]d such that f (x ) = f  where
f  = max f (x)
x[0,1]d

denotes the maximum of f in [0, 1]d . For simplicity, we assume that f is continuous on
[0, 1]d .2 The continuity of f implies the existence of x , and, in particular, that f is bounded.
Therefore, without loss of generality, we assume that f is non-negative.
If the form of f is not known explicitly, search algorithms usually evaluate f at several
locations and return an estimate of x and f  based on these observations. There is an
obvious trade-off between the number of samples used (i.e., the number of points where
the target function f is evaluated) and the quality of the estimate, and the performance of
any search strategy may be measured by the accuracy it achieves in estimating f  under a
constraint on the number of samples used.
Given a local search algorithm A, a general strategy for finding a good approximation
of the optimum x is to run several instances of A initialized at different starting points
and approximate f  with the maximum f value observed. We concentrate on local search
algorithms A defined formally by a sequence of possibly randomized sampling functions
sn : [0, 1]dn  [0, 1]d , n = 1, 2, . . .: A evaluates f at locations X1 , X2 , . . . where Xi+1 =
si (X1 , . . . , Xi ) for i  1, and the starting point X1 = s0 is chosen uniformly at random from
[0, 1]d ; after n observations A returns the estimate of x and the maximum f  , respectively,
by
bn = argmax f (Xk )
bn ).
X
and
f (X
1kn1

where ties in the argmax function may be broken arbitrarily, that is, if more samples Xk
bn can be chosen to be any of them. To avoid ambiguity and
achieve the maximum, X
2. The results can easily be extended to (arbitrary valued) bounded piecewise continuous functions with
finitely many continuous components.

411

fiGyorgy & Kocsis

simplify notation, here and in the following, unless stated explicitly otherwise, we adopt the
convention to use argmax to denote the maximizing sample with the smallest index, but
the results remain valid under any other choice to break ties.
For simplicity, we consider only starting a single local search algorithm A at different
random points, although the results of this work can be extended to allow varying the
parameters of A (including the situation of running different local search algorithms, where
a parameter would choose the actually employed search algorithm); as well as to allow
dependence among the initializations of A (that is, the starting point and parameters of
a local search instance may depend on information previously obtained about the target
function).
It is clear that if the starting points are sampled uniformly from [0, 1]d and each algorithm
bn ) converges
is evaluated at its starting point then this strategy is consistent, that is, f (X
to the maximum of f with probability 1 as the number of instances tends to infinity (in the
worst case we perform a random search that is known to converge to the maximum almost
surely). On the other hand, if algorithm A has some favorable properties then it is possible
to design multi-start strategies that still keep the random search based consistency, but
provide much faster convergence to the optimum in terms of the number of evaluations of
f.
bn ) is bounded and non-decreasing, it converges (no matter what
Since the sequence f (X
random effects occur during the search). The next lemma, proved in Appendix A, shows
that, with high probability, the convergence cannot be arbitrarily slow.
bn ) = f .3 If P (E) > 0,
Lemma 1 For any f  [0, 1]d , let E denote the event limn f (X
bn )  f
then, for any 0 <  < 1 there is an event E  E with 1  P (E ) <  such that f (X
uniformly almost everywhere on E . In other words, there exists a non-negative, nonincreasing function g (n) with limn g (n) = 0 such that


fi
bt ) = f  1   .
b t )  f (X
bn )  g (n) for all nfi lim f (X
P lim f (X
(1)
t

t

In certain cases, g (n) = O(en ), as shown by Nesterov (2004) for (gradient-based)
optimization for convex functions, by Gerencser and Vago (2001) for noise-free SPSA for
convex functions, or by Kieffer (1982) for k-means clustering (or Lloyds algorithm) in one
dimension for log-concave densities. While these results pertain to the simple situation
where there is only one local optimum which is the global one, many of these results can
be extended to more general situations, and we observed exponential rate of convergence in
our own experiments with functions with many local maxima.
The convergence property of local search algorithms guaranteed by Lemma 1 will be
exploited in the next section to derive efficient multi-start search strategies.

4. Multi-Start Search Strategies
Standard multi-start search strategies run an instance of A until it seems to converge to
a location where there is no hope to beat the currently best approximation of f  . An
3. In practice we can usually assume that local search algorithms converge to local optima, and so f may
be assumed to be a local optimum.

412

fiEfficient Multi-Start Strategies for Local Search Algorithms

alternative way of using multiple instances of local search algorithms is to run all algorithms
in parallel, and in each round decide which algorithms can take an extra step. This approach
may be based on estimating the potential performance of a local search algorithm A based
on Lemma 1. Note that if g were known, an obvious way would be to run each instance
until their possible performances become separated with high probability in the sense that
the margin between the performance of the actually best and the second best algorithm
is so large that the actually best algorithm is guaranteed to be the best, in the long run,
with high probability. Then we could just pick the best instance and run it until the given
computational budget is exhausted (this would be a simple adaptation of the explore-andexploit idea of choosing the best algorithm based on a trial phase as in Beck & Freuder,
2004; Carchrae & Beck, 2004).
In practice, however, g is usually not known, but for certain problem classes and local
search algorithms it may be known to belong to some function class, for example, g may
be known up to a (multiplicative) constant factor (here, for example, the constant may
depend on certain characteristics of f , such as its maximum local steepness). Even in the
latter case, the best instance still cannot be selected with high probability no matter how
large the margin is (as g may be arbitrarily large). However, using ideas from the general
methodology for Lipschitz optimization with an unknown constant (Jones et al., 1993),
we can get around this problem and estimate, in a certain optimistic way, the potential
performance of each algorithm instance, and in each round we can step the most promising
ones.
The main idea of the resulting strategy can be summarized as follows. Assume we have
K instances of an algorithm A, denoted by A1 , . . . , AK . Let Xi,n , i = 1, . . . , K denote the
location at which f is evaluated by Ai at the nth time it can take a step, where Xi,1 is the
starting point of Ai . The estimate of the location of the maximum by algorithm Ai after n
samples (steps) is
bi,n = argmax f (Xi,t )
X
1tn

bi,n ).
and the maximum value of the function is estimated by fi,n = f (X
For any i, let fi = limn fi,n denote the limiting estimate of the maximum of f
provided by Ai . Let g be defined as in Lemma 1 for the largest of these values,
f = max fi .
i=1,...,K

Since f is the best achievable estimate of the maximum of f given the actual algorithms
A1 , . . . , AK , g gives a high probability convergence rate for those algorithms that provide
the best estimate of the maximum in the long run (note that the assumption deals with
each limiting estimate  usually a local maximum  separately, that is, here no assumption
is made on algorithms whose limiting estimates are less than f ). Then, if Ai evaluates f
at ni,r points by the end of the rth round and Ai converges to the best achievable estimate
f , by Lemma 1 we have, with probability at least 1  ,
fi  fi,ni,r  g (ni,r ),
and so

fi,ni,r + g (ni,r )
413

(2)

fiGyorgy & Kocsis

is an optimistic estimate of f . If Ai is suboptimal in the sense that limn fi,n < f then
the above estimate is still optimistic if the rate of convergence is not slower than g , and
pessimistic if the rate of convergence is slower than g . The latter is desirable in the sense
that we have a negatively biased estimate on the expected performance of an algorithm
that we do not want to use (we should not waste samples on suboptimal choices).
As in practice g is usually not known exactly, the estimate (2) often cannot be constructed. On the other hand, if g is known up to a constant factor then we can construct
a family of estimates for all scales: Let g denote a normalized version of g such that
g (0) = 1 and g (n)/g (n) is a constant for all n, and construct the family of estimates
fi,ni,r + cg (ni,r )

(3)

where c ranges over all positive reals. Then it is reasonable to choose, in each round,
those algorithms to take another step that provide the largest estimate for some values of c
(typically, if an algorithm gives the largest estimate for some c = c then there is an interval
I containing c such that the algorithm provides the largest estimate for any c  I). In
this way we can get around the fact that we do not know the real scaling factor of g , as
we certainly use the algorithms that provide the largest value of (3) for c = g (1)/g (1),
and, as it will be discussed later, we do not waste too many samples for algorithms that
maximize (3) for other values of c. Using the optimistic estimate (3) is very similar, in spirit,
to the optimistic estimates in the standard upper confidence bound-type solution to the
multi-armed bandit problem (Auer, Cesa-Bianchi, & Fischer, 2002) or in the well-known
A search algorithm (Hart, Nilsson, & Raphael, 1968).
However, the exact (local) convergence rate is not known, even up to a constant factor,
for many local search algorithms, and even if it is, the corresponding bounds are usually
meaningful only in the asymptotic regime, which is often not of practical interest. Therefore,
to give more freedom in the design of the algorithm, we are going to use an estimate of the
form
fi,ni,r + ch(ni,r )
(4)
where, similarly to the requirements on g , h is a positive, monotone decreasing function
with limn h(n) = 0. We will also assume, without loss of generality, that h(0) = 1.
The actual form of h will be based on the theoretical analysis of the resulting algorithms
and some heuristic considerations. Essentially we will use h functions that converge to zero
exponentially fast, which is in agreement with the exponentially fast local convergence rates
in the examples given after Lemma 1. The optimal choice of h, given, for example, g , is
not known, and is left for future work.
4.1 Constant Number of Instances
The above idea can be translated to the algorithm MetaMax(K) shown in Figure 1. Here
we consider the case when we have a fixed number of instances, and our goal is to perform
(almost) as well as the best of them (in hindsight), while using the minimum number of
br
evaluations of f . Note the slight abuse of notation that in the MetaMax(K) algorithm X
and fr denote the estimates of the algorithm after r rounds (and not r steps/samples).
In the first part of step (a) of MetaMax we sweep over all positive c and select local
search algorithms that maximize the estimate (4). It is easy to see, that if Ai maximizes
414

fiEfficient Multi-Start Strategies for Local Search Algorithms

MetaMax(K): A multi-start strategy with K algorithm
instances.
Parameters: K > 0 and a positive, monotone decreasing function h with
limn h(n) = 0.
Initialization: For each i = 1, . . . , K, take a step with each algorithm Ai
once, and let ni,0 = 1 and fi,0 = f (Xi,1 ).
For each round r = 1, 2, . . .
(a) For i = 1, . . . , K select algorithm Ai if there exists a c > 0 such that
fi,ni,r1 + ch(ni,r1 ) > fj,nj,r1 + ch(nj,r1 )

(5)

for all j = 1, . . . , K such that (ni,r1 , fi,ni,r1 ) 6= (nj,r1 , fj,nj,r1 ). If
there are several values of i selected that have the same step number
ni,r1 then keep only one of these selected uniformly at random.
(b) Step each selected Ai , and update variables. That is, set ni,r =
ni,r1 + 1 if Ai is selected, and ni,r = ni,r1 otherwise. For each
bi,n
selected Ai evaluate f (Xi,ni,r ) and compute the new estimates X
i,r
and fi,ni,r .
(c) Let Ir = argmaxi=1,...,K fi,ni,r denote the index of the algorithm with
the currently largest estimate of f  , and estimate the location of the
br = X
bIr ,n
maximum with X
and its value with fr = fIr ,nIr ,r .
Ir ,r
Figure 1: The MetaMax(K) algorithm.
(4) for a particular c = u then there is a closed interval I containing u such that Ai also
maximizes (4) for any c  I. Therefore, in each round, the strategy MetaMax(K) selects
the local search algorithms Ai for which the corresponding point (h(ni,r1 ), fi,ni,r1 ) is a
corner of the upper convex hull of the set
Pr = {(h(nj,r1 ), fj,nj,r1 ) : j = 1, . . . , K}  {(0, max fj,nj,r1 )}.
1jK

(6)

The selection mechanism is illustrated in Figure 2.
To avoid confusion, note that the random selection in step (a) of MetaMax(K) implies
that if all algorithms are in exactly the same state, that is, (ni,r1 , fi,ni,r1 ) = (nj,r1 , fj,nj,r1 )
for all i, j, then one algorithm is selected uniformly at random (this pathological situation
may arise, e.g., at the beginning of the algorithm or if all the local search algorithms give
the same estimate of f  for some range of step numbers). Apart from the case when one of
the least used algorithms provides the currently best estimate, which happens surely in the
first round but usually does not happen later (and includes the previous pathological case),
it is guaranteed that in each round we use at least two algorithms, one with the largest
415

fiGyorgy & Kocsis

0.7

0.6

f(x)

0.5

0.4

0.3

0.2

0.1

0
0.2

0.3

0.4

0.5

h(n)

0.6

0.7

0.8

0.9

Figure 2: Selecting algorithm instances in MetaMax: the points represent the algorithm
instances, and the algorithms that lie on the corners of the upper convex hull
(drawn with blue lines) are selected.

estimate fi,ni,r1 = fr1 (for very small values of c), and one with the smallest step number
nj,r1 (for very large values of c). Thus, usually at most half of the total number of function
calls to f can be used by any optimal local search algorithm. This observation gives a practical lower bound (which is valid apart from the pathological situation mentioned above) on
the proportion of function calls to f made by optimal local search algorithms; surprisingly,
Theorem 6 below shows that this lower bound is achieved by the algorithm asymptotically.
The randomization in step (a) that precludes using multiple instances with the same
step number is introduced to speed up the algorithm in certain pathological cases. For
example, if A1 converges to the correct estimate, while all the other algorithms A2 , . . . , AK
produce the same estimate in each round, independently of their samples, that is inferior
to the estimates of A1 , then if we use the randomization, half of the calls to compute f will
be made by A1 , but without the randomization this would drop down to 1/K as in each
round we would use each algorithm. Furthermore, we could take a step with all algorithms
that lie on the convex hull, but similar pathological examples can be constructed when it is
more beneficial to use only algorithms on the corners. On the other hand, it almost never
happens in practice that three algorithms lie on the same line, and so algorithms typically
never fall at non-corner points of the convex hull.
In the remainder of this section we analyze the performance of the MetaMax(K)
algorithm. Proposition 2 shows that the algorithm is consistent in the sense that its performance asymptotically achieves that of the best algorithm instance as the number of rounds
increases. To understand the algorithm better, Lemma 3 provides a general sufficient condition that an algorithm instance is not advanced in a given round, while, based on this
result, Lemma 4 provides conditions that ensure that suboptimal algorithm instances are
not used in a round if they have been stepped too many times (i.e., they have evaluated f
at too many points) before. Lemma 5 gives an upper bound on the number of algorithm
416

fiEfficient Multi-Start Strategies for Local Search Algorithms

instances used in a round. The results of the lemmas are then used to show in Theorems 6
and 8 and Remark 9 that optimal algorithm instances are used (asymptotically) at least
at a minimum frequency that, in turn, yields the asymptotic rate of convergence of the
MetaMax(K) algorithm.
The following proposition shows that the MetaMax(K) algorithm is consistent in a
sense:
Proposition 2 The MetaMax(K) algorithm is consistent in the sense that fr  f  for
all r, and
o
n
f   lim fi,n .
f   lim fr = min
r

i=1,...,K

n

Proof The proof follows trivially from the fact that each algorithm is selected infinitely
often, that is, limr ni,r = . To see the latter, we show that in every K rounds the
number of steps taken by the least used algorithm, that is, mini=1,...,K ni,r , is guaranteed
to increase by one. That is, for all k  0,
min ni,kK  k.

i=1,...,K

(7)

As described above, in each round we select exactly one of the algorithms that have made
the least number of steps. Thus, if there are K such algorithms, the minimum step number
per algorithm will increase in K rounds, which completes the proof.
2
The MetaMax(K) algorithm is more efficient if suboptimal algorithms do not step too
often. The next lemma provides sufficient conditions that an algorithm is not used in a
given round.
Lemma 3 An algorithm instance Aj is not used in any round r + 1 of the MetaMax(K)
algorithm, if there are algorithms Ai and Ak such that fi,ni,r > fj,nj,r > fk,nk,r and either
ni,r  nj,r or


h(nj,r )
h(nj,r )


fj,nj,r  fi,ni,r 1 
(8)
+ fk,nk,r
h(nk,r )
h(nk,r )
Proof As in each round the algorithms at the corners of the convex hull Pr+1 are used, it
is easy to see that an algorithm Aj is not used in a round r if there are algorithms Ai and
Ak such that fi,ni,r > fj,nj,r > fk,nk,r and either ni,r  nj,r or
fi,ni,r  fk,nk,r
fi,ni,r  fj,nj,r

.
h(nj,r )  h(ni,r )
h(nk,r )  h(ni,r )

(9)

To finish the proof we show that (8) implies the latter. Indeed, (9) is equivalent to
h(nk,r )(fi,ni,r  fj,nj,r )  h(nj,r )(fi,ni,r  fk,nk,r ) + h(ni,r )(fk,nk,r  fj,nj,r ).
As the last term in the right hand side of the above inequality is negative by our assumptions,
the inequality is satisfied if
h(nk,r )(fi,ni,r  fj,nj,r )  h(nj,r )(fi,ni,r  fk,nk,r )
417

fiGyorgy & Kocsis

which is equivalent to (8).

2

The above lemma provides conditions on not using some algorithm instances in a certain
round that depend on the actual performance of the instances. The next result gives similar
conditions, however, based on the best estimates (usually local optima) achievable with the
algorithms. Let fi = limr fi,ni,r be the asymptotic estimate of algorithm Ai for f  , and
let f = max1iK fi denote the best estimate achievable using algorithms A1 , . . . , AK .
Let O  {1, . . . , K} be the set of optimal algorithms that converge to the best estimate
f (for these algorithms), and let |O| denote the cardinality of O (i.e., the number of
optimal algorithm instances). Note that O is a random variable that depends on the actual
realizations of the possibly randomized search sequences of the algorithms. The next lemma
shows that if j 6 O, then Aj is not used at a round r if it has been used too often so far.
Lemma 4 Let

 = f  max fj
j6O

denote the margin between the estimates of the best and the second best algorithms. Then
for any 0 < u <  there is a random index R(u) > 0 such that for any j 6 O, Aj is not
used by MetaMax(K) at a round r + 1 > R(u) if
!!



f
j
min ni,r
1
nj,r  h1 h
.
(10)
i=1,...,K
f  u
Furthermore, let 0 <  < 1, for any i  O, let g,i denote the convergence rate of algorithm
Ai guaranteed by Lemma 1, and let g (n) = maxiO g,i (n) for all n. Then P (R(u) 
 )  1   (where g 1 is the generalized inverse of g ), and no suboptimal
Kg1 (u)|f1 , . . . , fK


algorithm Aj , j 6 O, is used for any r > Kg1 (u) with probability at least 1   |O| given the
.
limiting estimates f1 , . . . , fK
Proof Let i  O. Since limn fi,n = fi = f by assumption and (7) implies that
(u)
limr ni,r = , there is an almost surely finite random index Ri > 0 such that for all
(u)

r > Ri we have f  fi,n
 u and so
i,r
f  fr  u.

(11)

Using Lemma 1 we can easily derive a high probability upper bound on R(u) . Since for any
r > Kg1 (u) = R , (7) implies ni,r  g1 (u), Lemma 1 yields


fi
P fi  fi,ni,r  u for all r > R fi fi = f  1  .

It follows that with probability at least 1   |O| there is an i  O such that fi  fi,ni,r  u,
 )   |O| . Thus, to
which implies that R(u) can be chosen such that P (R(u) > R |f1 , . . . , fK
prove the lemma, it is enough to show (10).
Clearly, by (11), the algorithm will pick the estimate of one of the best algorithms after
R(u) rounds. Let Ak be an algorithm with the least number of steps taken up to the end
418

fiEfficient Multi-Start Strategies for Local Search Algorithms

of round r, that is, nk,r = mini ni,r . If fk,nk,r  fj,nj,r then Aj is not used in round r + 1.
Moreover, since Aj 6 O, fj,nj,r  fj < fr and in this case Aj is not used in round r + 1
if nj,r  nIr ,r (recall that nIr ,r is the number of evaluations of f initiated by the actually
best algorithm Ir ). Therefore, Lemma 3 implies that Aj is not used for r > R() if
!
fj,nj,r  fk,nk,r
h(nj,r )  h(nk,r ) 1 
.
fr  fk,n
k,r

This is clearly satisfied if
h(nj,r )  h(min ni,r ) 1 
i

fj
f  u

!

(12)

for any 0 < u < , since by (11) we have
1

fj
fj,nj,r  fk,nk,r
fj,nj,r
1
.
1
fr  fk,nk,r
fr
f  u

Applying the inverse of h to both sides of (12) proves (10), and, hence, the lemma.

2

The above result provides an individual condition for each suboptimal algorithm for
not being used in a round. On the other hand, if one of the optimal algorithms has been
stepped sufficiently many times, we can give a cumulative upper bound on the number of
suboptimal algorithms used in each round.
Lemma 5 Assume that h decreases asymptotically at least exponentially fast, that is, there
exist 0 <  < 1 and n > 0 such that h(n+1)
h(n) <  for all n > n . Assume that r is large
enough so that ni,r > n for all i, and let r = 1  maxi:fi,n

6=fr
i,r

fi,ni,r
fr

> 0. Then at most

r
 ln
ln   + 1 algorithms are stepped in round r + 1, where x denotes the smallest integer at
least as large as x.

Proof Let i0 , i1 , . . . , im denote the indices of the algorithms chosen in round r + 1 with
fi0 ,r < fi1 ,r <    < fim ,r = fr . Then Lemma 3 implies that ni0 ,r < ni1 ,r <    < nim ,r and
fr  fik ,r < (fr  fik1 ,r )
for all k = 1, . . . , m  1. Repeated application of the above inequality implies
fr r  fr  fim1 ,r < (fr  fim2 ,r ) <    < m1 (fr  fi0 ,r )  fr m1
which yields

ln r
ln 
As we assumed that m + 1 algorithms were chosen in round r + 1, this fact finishes the
proof.
2
m1<

419

fiGyorgy & Kocsis

Based on Lemmas 4 and 5, the next theorem shows that if the local search algorithm
converges fast enough (exponentially with a problem dependent rate, or faster than exponential) then half of the function calls to evaluate f correspond to optimal algorithm
instances.
Theorem 6 Assume that the performance of the algorithms Ai , i = 1, . . . , K are not all
the same, that is, |O| < K, and suppose that
(
)
fj
h(n + 1)
lim sup
.
(13)
< min 1 
j6O
h(n)
n
f
Then asymptotically at least half of the function
respond to an optimal algorithm. That is,
P
ni,r
1
P lim inf PiO

K
r
2
i=1 ni,r

calls to evaluate f in MetaMax(K) cor!
fi
fi 

fi f1 , . . . , fK
= 1.
fi

Furthermore, for any 0 <  < 1 and  > 0 there is a constant R(,) > 0 such that
$P
%!
K
n
i=1 i,r
f  fr  g
(2 + )|O|

(14)

(15)

 simultaneously for all r > R(,) , where
with probability at least 1  |O| given f1 , . . . , fK
(,)
g is defined in Lemma 4, and the threshold R
> 0 depends on , , h, g1 and , where
1
g and  are also defined in Lemma 4.

Proof We show that a suboptimal Aj is not chosen for large enough r if nj,r > mink nk,r .
By Lemma 4, it is sufficient to prove that, for large enough r,
!!
fj
1
(16)
min nk,r + 1  h
h(min nk,r ) 1 
k
k
f  u
for some 0 < u <  (recall that here r should be larger than R(u) , the almost surely finite
random index of Lemma 4).
As the minimum in (13) is taken over a finite set, it follows that there exists a small
enough positive u <  such that
(
)
fj
h(n + 1)
lim sup
,
(17)
 min 1 
j6O
h(n)
n
f  u
which clearly implies (16) as limr mink nk,r =  by (7). This fact finishes the proof of
(14), the first part of the theorem.
Next we prove (15). Let Nu > 0 be a threshold such that (17) holds for all n  Nu .
Furthermore, by Lemma 1 and the union bound, (1) holds for each local search algorithm
Ai with g,i in place of g simultaneously for all i  O with probability at least 1  |O|.
420

fiEfficient Multi-Start Strategies for Local Search Algorithms

Then (7) and a slight modification of Lemma 4 imply that (16) holds simultaneously for
.
all r > K max{g1 (u), Nu } = R with probability at least 1  |O| given f1 , . . . , fK
Since
in each such round at most two algorithms are used, for any r > R + c we have
P
n
i,r
c+R
PiO
> 2c+KR
 with high probability. Since the latter is bounded from below by 1/(2+)
K
n
i,r
i=1
PK
P
ni,r
for c  R (K2)/, we have iO ni,r  i=1
for any r > R(,) = R +R (K2)/
2+
with
l P highmprobability. Then there is an algorithm Ai , i  O such that it is used in at least
K
i=1

ni,r
|O|(2+)

rounds, implying the statement of the theorem via Lemma 1.

2

Remark 7 The above proof of Theorem 6 is based on Lemma 4. A proof based on Lemma 5
is also possible, since setting  = minj6O (1  fj /f ) in the lemma, (13) implies that, for
large enough r, r  , and so each round is approximately of length 2 by the lemma.
It may happen that although the decay rate of h is exponential, it is not quite fast
enough to satisfy (13), so the optimal scenarios in the above theorem do not hold. In this
case it turns out that the number of algorithms converging to the same local maximum
plays the key role in determining the usage frequency of the optimal algorithms.
Theorem 8 Assume that the estimates provided by the algorithms A1 , . . . , AK converge to
N > 1 distinct limit points, such that k0 = |O| algorithms converge to f , and k1 , k2 , . . . , kN 1
algorithms converge to each suboptimal limit points, respectively. Suppose furthermore
that h decreases asymptotically at least exponentially fast, that is, for some 0 <  < 1,
lim supn h(n+1)
h(n) < . Then
P lim inf
r

P

iO ni,r
PK
i=1 ni,r

!
fi
fi 
kmax

fi f , . . . , fK = 1.

K  k0 + kmax fi 1

where kmax = max1iN 1 ki .
Furthermore, using the definitions of Lemma 4 for any 0 <  < 1 and  > 0 there is a
constant threshold R(,)
%!
$
PK
k
n
max
i=1 i,r
f  fr  cg
(K  k0 + kmax + )|O|
 simultaneously for all r > R(,) , where
with probability at least 1  |O| given f1 , . . . , fK
R(,) depends on , .,f1 , . . . , sfK and the convergence rate of all the algorithms4 .
 are given, and fix the random trajectories of all the algorithms. If
Proof Suppose f1 , . . . , fK
there is a single suboptimal algorithm then the statement is trivial as kmax /(K k0 +kmax ) =
1/2 and in each round at least two algorithms are used, and at most one of them can be
the suboptimal one. From now on assume there are at least two suboptimal algorithms.
Assume that Aj and Ak converge to suboptimal local maxima (strictly less than f ). For
any r large enough, an optimal algorithm Ai is better than any of the suboptimal ones, that

4. Instead of the convergence rate of all algorithms, R(,) may be defined to be dependent on g and .

421

fiGyorgy & Kocsis

is, if Ai converges to f then fi,ni,r > fj,nj,r , fk,nk,r . Assume, without loss of generality, that
fj,nj,r  fk,nk,r . If nj,r  nk,r then clearly only Aj can be chosen in round r + 1. Assume
nj,r > nk,r . Since fj,nj,r and fk,nk,r are convergent sequences (as r  ), for large enough
r we have, for some j,k < 1,
1

fj,nj,r  fk,nk,r
> j,k  tj,k


fi,n  fk,n
i,r

(18)

k,r

where tj,k = ln j,k / ln  is a positive integer. Note that if Aj and Ak converge to the
same point, that is, limr (fj,nj,r  fk,nk,r ) = 0, the second term on the left hand side of
(18) converges to 0, and so j,k can be chosen to be , implying tj,k = 1. Rearranging the
above inequality one obtains
(1  tj,k )fi,ni,r + tj,k fk,nk,r > fj,nj,r .

(19)

If nj  nk  tj,k , then the conditions on h and the fact that both nj,r and nk,r tend to
infinity as r   (recall (7)) imply that, for large enough r, h(nj,r )/h(nk,r ) < tj,k . Since
fi,ni,r  fk,nk,r for large enough r, from (19) we obtain


h(nj,r ) 
h(nj,r ) 
tj,k 
tj,k 

fk,nk,r .
fi,ni,r +
fj,nj,r < (1   )fi,ni,r +  fk,nk,r  1 
h(nk,r )
h(nk,r )
Thus, by Lemma 3, if r is large enough, Aj cannot be used in round r + 1 if nj,r  nk,r  tj,k .
Since both nj,r and nk,r tend to infinity, it follows that, for large enough r,
|nj,r  nk,r |  tj,k

(20)

for any two suboptimal algorithms Aj and Ak . Note that this fact also implies that from
any set of suboptimal algorithms converging to the same point, eventually at most one can
be used in any round (since the corresponding thresholds are tj,k = 1).
Clearly, (7) implies that both nj,r and nk,r grow linearly with r, and since their differences is bounded by (20), limr nj,r /nk,r = 1. Therefore, for any suboptimal algorithm
Aj , we have limn nj,r /r  1/kmax (this is the maximal rate of using elements from the
largest group of suboptimal algorithms converging to the same local optimum). Finally, as
an optimal algorithm is used in each round r, for large enough r, we have
P
P
iO ni,r
iO ni,r
P
lim inf PK
= lim inf P
r
r
n
+
i,r
n
i6O ni,r
iO
i,r
i=1
kmax
r
 lim
=
,
r
r r + (K  k0 )
K  k0 + kmax
kmax
where we used the fact that a/(a + b) is an increasing function of a for a, b > 0. Since the
,
above inequality holds for all realizations of the trajectories of A1 , . . . , AK , given f1 , . . . , fK
the first statement of the theorem follows.
The second statement follows similarly to (15) in Theorem 6. Since the exact value of
R(,) is not of particular interest, the derivation is omitted.
2

422

fiEfficient Multi-Start Strategies for Local Search Algorithms

Remark 9 The main message of the above theorem is the somewhat surprising observation
that suboptimal algorithms are slowed down if there is a large group of suboptimal algorithms
converging to the same local optimum; the rate suboptimal algorithms are used at is bounded
by the the size of the largest such group.
4.2 Unbounded Number of Instances
It is clear that if the local search algorithms are not consistent (i.e., they do not achieve
the global optimum f  ), then, despite its favorable properties, the MetaMax(K) strategy
is inconsistent, too. However, if we increase the number of algorithms to infinity then we
get the consistency from random search, while still keeping the reasonably fast convergence
rate from MetaMax(K).
Clearly, one needs to balance between exploration and exploitation, that is, we have
to control how often we introduce a new algorithm. One solution is to let the MetaMax
algorithm solve this problem: the MetaMax() algorithm, given in Figure 3, is the extension of MetaMax(K) that is able to run with infinitely many local search algorithm
instances. Here a major issue is that new local search algorithms have to be started from
time to time (this ensures that the algorithm will converge to the global maximum of f
since it also performs a random search): this is implemented by modifying step (a) of the
MetaMax(K) algorithm so that a new, randomly initialized local search algorithm is introduced in each round (randomly selecting one algorithm uniformly from the infinitely
many possible algorithms not used so far). Obviously, we have to skip the initialization
step of MetaMax(K) and start each algorithm with 0 samples. To better control the
length of each round (i.e., the exploration), in each round r we allow the use of a different
function h, denoted by hr1 that may depend on any value measured before round r (this
is suppressed in the notation). As before, we assume that hr (0) = 1, hr (n) is monotone decreasing in n, and limn hr (n) = 0 for all r. Typically we will make hr1 be dependent on
PKr1
the total number of steps (i.e., the function calls to evaluate f ) tr1 = i=1
ni,r1 made
by all the algorithms before round r, where Kr1 is the number of algorithm instances used
before round r; note that Kr1 = r  1 for all r, as we start exactly one new algorithm in
each round.
It is desired that, although the number of local search algorithms grows to infinity,
the number of times the best local search algorithm is advanced by the MetaMax()
algorithm approaches infinity reasonably fast. Somewhat relaxing the random initialization
condition, we may imagine a situation where the local search algorithms are initialized in
some clever, deterministic way, and so in the first few steps they do not find any better
value than their initial guesses. If all algorithms are optimal (this may be viewed as a result
of the clever initialization), then they may provide, for example, the identical estimates
0.5, 0.5, 1 for the first three steps. Then it is easy to see that each algorithm is stepped
exactly twice, thus no convergence to the optimum (which would be found after the third
step) is achieved. Although the random initialization of the search algorithms guarantees
the consistency of MetaMax() (see Proposition 10 below), robust behavior in even such
pathological cases is preferred.
This can be achieved by a slight modification of the algorithm: if in a round a local search
algorithm overtakes the currently best algorithm, that is, Ir 6= Ir1 , then algorithm AIr
423

fiGyorgy & Kocsis

MetaMax(): A multi-start strategy with infinitely many
algorithm instances.
Parameters: {hr }, a set of positive, monotone decreasing functions with
limn hr (n) = 0.
For each round r = 1, 2, . . .
(a) Initialize algorithm Ar by setting nr,r1 = 0, fr,0 = 0.
(b) For i = 1, . . . , r select algorithm Ai if there exists a c > 0 such that
fi,ni,r1 + chr1 (ni,r1 ) > fj,nj,r1 + chr1 (nj,r1 )
for all j = 1, . . . , r such that (ni,r1 , fi,ni,r1 ) 6= (nj,r1 , fj,nj,r1 ). If
there are several values of i selected that have the same step number
ni,r1 then keep only one of these selected uniformly at random.
(c) Step each selected Ai , and update variables. That is, set ni,r =
ni,r1 + 1 if Ai is selected, and ni,r = ni,r1 otherwise. For each
bi,n
selected Ai evaluate f (Xi,ni,r ) and compute the new estimates X
i,r
and fi,ni,r .

(d) Let Ir = argmaxi=1,...,r fi,ni,r denote the index of the algorithm with
the currently largest estimate of f  , and estimate the location of the
br = X
bIr ,n
maximum with X
and its value with fr = fIr ,nIr ,r .
Ir ,r
Figure 3: The MetaMax() algorithm.

is stepped several times until it is used more times than AIr1 .5 The resulting algorithm,
called MetaMax, is given in Figure 4. Note that the algorithms MetaMax() and
MetaMax conceptually differ only at one place: step (c) is extended with step (c) in
the new algorithm. As a result, a technical modification also appears in step (d), and, to
simplify the presentation of the MetaMax algorithm, a slight, insignificant modification is
also introduced in step (b), see the discussion below.
The modification in MetaMax is not really significant in the practical examples we
studied (see Section 5), as the number of steps taken by an algorithm that overtakes the
currently best algorithm grows very quickly also in the MetaMax() algorithm, since in
MetaMax an overtake usually introduces very short rounds (close to the minimum length
two in many cases) until the leading algorithm becomes also the most used one. The goal
of the modification in step (b) is only to synchronize the choice of the optimal algorithms
in steps (b) and (c). An equally good solution would be to choose, in case of a tie in step
5. In this way we achieve that the actually best algorithm dominates all others in terms of both accuracy
and the number of calls made by the algorithms to compute the target function. This is the same type
of dominance as used by Hutter et al. (2009) in a slightly different context.

424

fiEfficient Multi-Start Strategies for Local Search Algorithms

MetaMax: A multi-start strategy with infinitely many
algorithm instances.
Parameters: {hr }, a set of positive, monotone decreasing functions with
limn hr (n) = 0.
For each round r = 1, 2, . . .
(a) Initialize algorithm Ar by setting nr,r1 = 0, fr,0 = 0.
(b) For i = 1, . . . , r select algorithm Ai if there exists a c > 0 such that
fi,ni,r1 + chr1 (ni,r1 ) > fj,nj,r1 + chr1 (nj,r1 )
for all j = 1, . . . , r such that (ni,r1 , fi,ni,r1 ) 6= (nj,r1 , fj,nj,r1 ). If
there are several values of i selected that have the same step number
ni,r1 then keep only one of these that has the smallest index.
(c) Step each selected Ai , and update variables. That is, set ni,r =
ni,r1 + 1 if Ai is selected, and ni,r = ni,r1 otherwise. For each
bi,n
selected Ai evaluate f (Xi,ni,r ) and compute the new estimates X
i,r

and fi,ni,r .

(c) Let Ir = argmaxi=1,...,r fi,ni,r denote the index of the algorithm with
the currently largest estimate of f  (in case Ir is not unique, choose
the one with the smallest number of steps ni,r ). If Ir 6= Ir1 , step
algorithm AIr (nIr1 ,r  nIr ,r + 1) times and set nIr ,r = nIr1 ,r + 1.
br = X
bIr ,n
(d) Estimate the location of the maximum with X
and its
Ir ,r
value with fr = fIr ,nIr ,r .
Figure 4: The MetaMax algorithm.

(c), an algorithm that was used in the current round. Also note that, as a result of the
modifications, the currently best algorithm (with index Ir ) has taken the most steps, and so
the extra number of steps taken in step (c) is indeed positive. An important consequence
of the modifications is that, in any round r, the number of steps taken by the local search
algorithm AIr , which is the best at the end of the round, is between r and 2r (see Theorem 15
below).
The rest of the section is devoted to the theoretical analysis of MetaMax() and
MetaMax, following the lines of the analysis provided for MetaMax(K). First, in Proposition 10, it is shown that the algorithm is consistent, that is, the solution found by the
algorithm actually converges to f  . Lemma 12 (a counterpart of Lemma 4) shows that
suboptimal algorithms can make only finitely many steps, while Lemma 14 gives an upper
bound on the length of each round. The main theoretical results of this section apply only
425

fiGyorgy & Kocsis

to the MetaMax algorithm: Theorem 15 gives a lower bound on the number of steps taken
by the actually best algorithm by the end of a given round, while, as a consequence, Theorem 16 shows the rate of convergence of the algorithm as a function of the total number of
steps (i.e., function calls to evaluate f ) used by all algorithm instances: it turns out that at
most quadratically more steps are needed than for a generic local search algorithm instance
that converges to the optimum.
Since the MetaMax() and the MetaMax strategies perform a random search (the
number of algorithms tends to infinity as the length of each round is finite), the algorithms
are consistent:
Proposition 10 The strategies MetaMax() and the MetaMax are consistent. That
is,
lim fr = f 
r

almost surely.
Proof Clearly, the event that fr does not converge to f  can be written as
n

 \
 n
 [
o
o
[
lim fr 6= f  =
fr < f   1/n

r

(21)

n=1 R=1 r=R

Now the continuity of f implies that, for any n, if X is chosen uniformly from [0, 1]d then
qn P
= P(f (X) > f   1/n) > 0. Thus, for any round r, P(fr < f   1/n)  (1  qn )r , and


so 
r=1 P(fr < f  1/n) is finite. Therefore, the Borel-Cantelli lemma (see, e.g., Ash &
Doleans-Dade, 2000) implies that
!
 n
 [
o
\

=0
P
fr < f  1/n
R=1 r=R

for all n. This, together with (21) finishes the proof, as
P



lim fr 6= f

r








X

n=1

P

 n
 [
\

R=1 r=R

fr < f  1/n


o

!

= 0.
2

In the reminder of this section we will assume that local search algorithms that achieve
an almost optimal value eventually converge to the optimum.
Assumption 11 Let F   R denote the set of local maxima of f , and let  = f  
supfF  ,f<f  f. We assume that  > 0 and if an algorithm Ai is such that fi,n > f   
for some n, then limn fi,n = f  .
If the local search algorithms converge to local optima (which is a reasonable assumption
in practice), the above assumption is usually satisfied: the only situation when it does not
hold is the pathological case when f has infinitely many local maxima and the set of these
maxima is dense at the global maximum.
426

fiEfficient Multi-Start Strategies for Local Search Algorithms

Under Assumption 11 we can prove, similarly to Lemma 4, that any suboptimal algorithm is selected only a limited number of times that increases with h1
r . In particular, if
hr = h for all r large enough, then any suboptimal algorithm is chosen only finitely many
times.
Lemma 12 Suppose Assumption 11, and let q = P(f (X) > f   /2) for an X uniformly
distributed in [0, 1]d . Then, for both the MetaMax() and the MetaMax algorithms, a
suboptimal algorithm Aj started before round r +1 is not used at round r +1, with probability
at least 1  (1  q)r , if



1
nj,r  hr
.
2f   
In addition, if hr (n) is a non-decreasing function of r for all n, then
lim sup
r

h1
r1

n
 j,r



2f  

 <

almost surely.

(22)

In particular, if hr is a constant function, that is, hr = h0 for all r, then limr nj,r < 
almost surely.
Remark 13 Note that in the second
 part of the lemma we coulddrop the
 monotonicity
1
1


assumption on hr and replace hr1 2f   with max0r r1 hr 2f   in (22).
Proof Consider if algorithm Aj is used at a round r + 1. First note that with probability
at least 1  (1  q)r , fr > f   /2. Furthermore, the newly introduced algorithm, Ar+1 is
not used yet, and we have nr+1,r = 0 and fr+1,0 = 0. Thus, by Lemma 3, Aj is not used if


h(nj,r )


= fr (1  hr (nj,r )) .
fj,nj,r  fr 1 
hr (0)
Since this is equivalent to
nj,r 
and
h1
r

fj,nj,r
1
fr

h1
r

fj,nj,r
1
fr

!



by

h1
r



!

,


2f   



fr  fj,nj,r

fr  (f   )
(f   /2)  (f   )
= 
,
(23)

>

f  /2
2f  
fr
fr
the first statement of the proof follows.
b denote the first round in which there is an optimal
To prove the second part, let R


algorithm Ai with fi,ni,Rb > f  /2. Then for any suboptimal algorithm Aj , the first part
b
of the lemma implies that, for any r > R,










1
1
b
b
+ 1 = max R, hr1
+1
nj,r  max R, max
h
0r  r1 r
2f   
2f   
427

fiGyorgy & Kocsis

where the equality holds since h1
r (n) is non-decreasing in r. Thus




b
nj,r
R
1
  lim sup max
, 1 +




lim sup



 h1

r h1
r
h1
r1 2f  
r1 2f  
r1 2f  




b
1
R
, 1 +



 max



 h1
h1
0

0

2f  

(24)

2f  

b is finite, (24) is also finite with
where we used that h1
is non-decreasing in r. Since R
r
probability 1.
2
A simple modification of Lemma 5 implies that once a /2-optimal sample point is
found then only a limited number of suboptimal algorithms is chosen in each round.
Lemma 14 Consider algorithms MetaMax() and MetaMax. Suppose Assumption 11
holds, and assume that f   fR < /2 for some R > 0. In anyround r > R, if hr (n) = rn


for some 0 < r < 1 and for all n  0, then at most

ln 2f 
ln(1/r )

algorithms are chosen that

have estimates fj  f   .

Proof The proof follows from Lemma 5 taking into account that any suboptimal algorithm
Aj satisfies fj  f    and that at least one optimal algorithm is chosen in each round
r > R: Similarly to (23), r defined in Lemma 5 can be bounded as r > /(2f  ),
and so
l
m  2f   
ln
ln(1/r )

the number of suboptimal algorithms used in round r is bounded by ln(1/
.
 ln(1/
r)
r)
2
Finally we can derive the convergence rate of the algorithm MetaMax. First we bound
the number of steps taken by the currently best algorithm, both in terms of the number of
rounds and the total number of steps taken by all the local search algorithms.
Theorem 15 Consider the MetaMax algorithm. At the end of any round r the number
of steps taken by the currently best algorithm is between r and 2r. That is,
r  nIr ,r < 2r.

(25)

Furthermore, the number of calls nIr ,r to evaluate f by the currently
best algorithm AIr can
Pr
be bounded by a function of the total number of times tr = i=1 ni,r the target function f
is evaluated by all local search instances as

2tr + 7  1
nIr ,r 
.
(26)
2
Proof The first statement of the lemma is very simple, since in any round the actually
best algorithm takes one step if there is no overtaking, and one or two steps if there is
428

fiEfficient Multi-Start Strategies for Local Search Algorithms

overtaking. Indeed, in any round r  2, if there is no overtaking, that is, Ir = Ir1 ,
then nIr ,r = nIr ,r1 + 1. Otherwise, if Ir 6= Ir1 , then nIr ,r = nIr1 ,r + 1, and since
0  nIr1 ,r  nIr1 ,r1  1, we have
1  nIr ,r  nIr1 ,r1  2
in all situations. Since in the first round clearly the only algorithm used takes 1 step, that
is, nI1 ,1 = 1, (25) follows.
To prove the second part, notice that in any round r, at most nIr1 ,r1 + 1 algorithms
can be stepped in step (c) as no algorithm can be used that has taken more steps than the
currently best one. Also, in step (c) no extra samples are used if there is no overtaking.
In case of overtaking, AIr has to be advanced in step (c), as well as AIr1 , and so at most
nIr1 ,r1 + 1 extra steps has to be taken by AIr . Therefore,
tr  tr1 + 2nIr1 ,r1 + 2.
Thus, since no overtaking happens in round 1, we obtain
tr  1 +

r
X

2(nIs1 ,s1 + 1).

s=2

Then, by (25) we have
tr  1 + 4

r
X
s=2

s = 1 + 2(r + 2)(r  1)  1 + 2(nIr ,r + 2)(nIr ,r  1)

which yields (26).

2

Note that in the proof we used a crude estimate on the length of a usual round (without
overtaking) relative to, for example, Lemma 14. This, however, affects the result only by a
constant factor as long as we are not able to bound the number of rounds or the number
of extra steps taken when overtaking happens, since the effect of the overtakings itself
introduces the quadratic dependence in the proof of (26). Experimental results in Section 5
show (see Figure 10) that the number of algorithm instances (which is in turn the number r
of rounds) has a usual growth rate of (tr / ln tr ), which, if taken into account, may sharpen
the bound on how often the best algorithm is chosen.
Under Assumption 11, the random search component of MetaMax implies that eventually we will have an optimal algorithm that is the best. From that point the convergence
rate of the optimal local search algorithms determine the performance of the search, and
the number of steps taken by the best local search algorithm is bounded by Theorem 15.
Theorem 16 Suppose Assumption 11 holds. Then there is an almost surely finite random
index R such that for all rounds r > R, the estimate fr of the MetaMax algorithm and
the total number of steps tr taken by all local search algorithms up to the end of round r
satisfies


2t
+
7

1
r

f  fr  g
2
with probability at least 1  , where g is defined by Lemma 1 for the global maximum f  .
429

fiGyorgy & Kocsis

Remark 17 (i) The value of R can be bounded with high probability using the properties
of uniform random search for the actual problem; this would yield similar bounds as in
Theorems 6 and 8 for the MetaMax(K) algorithm. (ii) Note the exploration-exploitation
trade-off in the MetaMax algorithm: the value of R is potentially decreased if we introduce
new algorithms more often, while nIr ,r is reduced at the same time. (iii) Theorems 15 and 16
imply that, asymptotically, the MetaMax algorithm needs only quadratically more function
evaluations than a local search algorithm
Psthat is ensured to converge to the optimum. In
particular, if f is of the form f (x) =
i=1 fi (x)ISi (x) where the Si form a partition of
[0, 1]d , ISi denotes the indicator function of Si , and the fi belong to some nicely behaving
function class such that a local search algorithm started in Si converges to the maximum
of fi on Si (e.g., f is a piecewise concave function with exponential convergence rate for
the SPSA algorithm, which is used with a sufficiently small step size), then we can preserve
the performance of a local search algorithm for the original function class at the price of
an asymptotically quadratic increase in the number of function calls to evaluate f (i.e., the
total number of steps taken by all local search algorithm instances).
4.3 Discussion on the Results
In some sense the theoretical results presented in the previous sections are weak. The
consistency result for the MetaMax(K) algorithm follows easily from the fact that each
local search algorithm is used infinitely many times, and the consistency of MetaMax()
and MetaMax follows from the consistency of a random search. The performance bounds
provided have a disadvantage that they are asymptotic in the sense that they hold only after
a possibly large number of rounds (a weakness of the bounds is that the minimum number
of rounds is obtained from properties of uniform random search/sampling for the particular
problem, neglecting more attractive properties of the algorithms). In fact, it is quite easy
to construct scheduling strategies that are consistent and asymptotically an arbitrarily
large fraction of the function evaluations (even almost all) is used by optimal local search
algorithms: the explore-and-exploit algorithms achieve both of these goals if the number of
function evaluations to be used is known ahead and they use an arbitrarily small fraction
of the evaluations of the target function f for exploration. We compare the performance of
our algorithms to such explore-and-exploit algorithms in Section 5. In particular, to match
the performance guarantees for the MetaMax family, we use algorithms that spend half
of their time with exploration and half with exploitation, where in the exploration part a
uniform allocation strategy is used when there is a finite number of local search algorithms,
and the schedule of Luby et al. (1993) is used for infinitely many local search algorithms.
Although the theoretical guarantees proved in this paper for the MetaMax family also hold
for these explore-and-exploit algorithms, in all experiments the MetaMax family seems to
behave superior compared to these algorithms, as expected.
The theoretical results also do not give sufficient guidance on how to chose the parameter
h or hr (the time-varying version of h is not considered for the MetaMax(K) algorithm
for simplicity and ease of presentation). Most of our results require a sufficiently fast
exponential decay in h, which is problem dependent and cannot be determined in advance. A
sufficiently fast decay rate would ensure, for example, that for the MetaMax(K) algorithm
we could always use the stronger results of Theorem 6 and would never have to deal with
430

fiEfficient Multi-Start Strategies for Local Search Algorithms

the case when only the bound of Theorem 8 holds. One may easily choose h to be any
function that decreases super-exponentially: this would make the asymptotic bounds work,
however, would slow down exploration (in the extreme case of hr (n)  0, which is excluded
by our conditions, no exploration would be performed, and the algorithms would use only
the actually best local search algorithm). In practice we have always found that it is
appropriate to chose hr decay exponentially. Furthermore, we have found it even more
effective to gradually decrease the decay rate to enhance exploration as time elapses (the
rationale behind this approach is the assumption that good algorithms should have more
or less converged after a while, and so there may be a greater potential in exploration
to improve our estimates). Finally, the connection between g and h should be further
investigated.
Keeping the above limitations of the theoretical results in mind, we still believe that
the theoretical analyses given provide important insight to the algorithms and may guide
a potential user in practical applications, especially since the properties of the MetaMax
family that can be proved in the asymptotic regime (e.g., that the rounds are quite short)
can usually be observed in practice, as well. Furthermore, we think that it is possible to
improve the analysis to bound the thresholds from which the results become valid with
reasonable values, but this would require a different approach and, therefore, is left for
future work.

5. Experiments
The variants of the MetaMax algorithm are tested in synthetic and real examples. Since
there was only negligible difference in the performance of MetaMax() and MetaMax,6
in the following we present results only for MetaMax(K) and MetaMax. First we demonstrate the performance of our algorithm in optimizing a synthetic function (using SPSA as
the local search algorithm). Next the behavior of our algorithm is tested on standard data
sets. We show how MetaMax can be applied for tuning the parameters of machine learning
algorithms: a classification task is solved by a neural network, and the parameters of the
training algorithm (back-propagation) are fine-tuned by MetaMax combined with SPSA.
MetaMax is also applied to boost the performance of k-means clustering. At the end of
the section, we compare the results of the experiments to the theoretical bounds obtained
in Section 4.2.
In all the experiments, in accordance with our simplifying assumptions introduced in
Section 3, the main difference between the individual runs of the particular local search
algorithm is their starting point. Obviously, more general diversification techniques exist:
for example, the parameters of the local search algorithm could also vary from instance to
instance (including running instances of different local search algorithms, where a parameter would select the actually employed search algorithm), and the initialization (starting
point and parametrization) of a new instance could also depend on the results delivered by
6. For example, the relative difference between the average error eMetaMax() of MetaMax() and
eMetaMax of MetaMax in optimizing the parameters of a multi-layer perceptron for learning the letter
data set (see Section 5.1 and especially Figure 6, right for more details) was 0.033 with a standard
deviation of 0.06 (averaged
over 1000 experiments), where the relative difference is defined as
fi
fi
fieMetaMax()  eMetaMax fi / max(eMetaMax() , eMetaMax ).

431

fiGyorgy & Kocsis

existing instances. Although the MetaMax strategies could also be applied to these more
general scenarios, their behavior can be better studied in the simpler scenario; hence, our
experiments correspond only to this setup.
5.1 Optimizing Parameters with SPSA
In this section we compare the two versions of the MetaMax algorithm with six multi-start
strategies, including three with a constant and three with a variable number of algorithm
instances. The strategies are run for a fixed T time steps, that is, the target function can
be evaluated T times, together by all local search instances (note that several reference
strategies use T as a parameter).
We used SPSA (Simultaneous Perturbation Stochastic Approximation; Spall, 1992) as
the base local search algorithm in all cases. SPSA is a local search algorithm with a sampling
function that uses gradient descent with a stochastic approximation of the derivative: at
the actual location Xt = (Xt,1 , . . . , Xt,d ), SPSA estimates the lth partial derivative of f by
f (Xt + t Bt )  f (Xt  t Bt )
,
ft,l (Xt,l ) =
2t Bt,l
where the Bt,l are i.i.d. Bernoulli random variables that are the components of the vector
Bt , and then uses the sampling function st (Xt ) = Xt + at ft (Xt ) to choose the next point
to be sampled, that is,
Xt+1,l = Xt,l + at ft,l (Xt,l )
for l = 1, . . . , d (t and at are scalar parameters).
In the implementation of the algorithm we have followed the guidelines provided in
(Spall, 1998), with the gain sequence at = a/(A + t + 1) , and perturbation size t =
/(t + 1) , where A = 60,  = 0.602 and  = 0.101. The values of a and  vary in the
different experiments; they are chosen heuristically based on our experience with similar
problems (this should not cause any problem here, as the goal of the experiments is not to
provide fast solutions of the global optimization problems at hand but to demonstrate the
behavior of the multi-start algorithms to be compared). In addition to the two evaluations
required at the perturbed points, we also evaluate the function at the current point Xt .
The starting point is chosen randomly, and the function is evaluated first at this point.
The six reference algorithms the MetaMax(K) and MetaMax algorithms are compared to the following:
Unif: This algorithm selects from a constant number of instances of SPSA uniformly.
In our implementation the instance It = t mod K is selected at time t, where K denotes
the number of instances.
ThrAsc: The Threshold Ascent algorithm of Streeter and Smith (2006b). The algorithm begins with selecting each of a fixed number of instances once. After this phase at
each time step t ThrAsc selects the best s estimates produced so far by all algorithm
instances Ai , i = 1, . . . , K in all the previous time steps, and for each Ai it counts how
many of these estimates were produced by Ai . Denoting the latter value by Si,t , at time
t the algorithm selects the instance with index It = argmaxi U (Si,t /ni,t , ni,t ), where ni,t is
432

fiEfficient Multi-Start Strategies for Local Search Algorithms

the number of times the ith instance has been selected up to time t,
U (, n) =  +

+

p
2n + 2
n

and  = ln(2T K/). s and  are the parameters of the algorithm, and in the experiments
the best value for s appeared to be 100, while  was set to 0.01. We note that Threshold
Ascent has been developed for the maximum K-armed bandit problem; nevertheless, it
provides sufficiently good performance in our setup to test it in our experiments.
Rand: The random search algorithm. It can be seen as running a sequence of SPSA
algorithms such that each instance is used for exactly one step, which is the evaluation of
the random starting point of the SPSA algorithm.
Luby: The algorithm based on the work of Luby et al. (1993). This method runs several
instances of SPSA sequentially after each other, where the ith instance is run for ti steps,
with ti defined by

2k1 ,
if i = 2k  1
ti =
ti2k1 +1 ,
if 2k1  i < 2k  1
The above definition produces a schedule such that from the first 2k  1 algorithm instances
one is run for 2k1 steps, two for 2k2 steps, four for 2k3 steps, and so on.
EE-Unif: This algorithm is an instance of explore-and-exploit algorithms. For the first
T /2 steps the Unif algorithm is used for exploration, and, subsequently, in the exploration
phase, the SPSA instance that has achieved the highest value in the exploration phase is
selected.
EE-Luby: This algorithm is similar to EE-Unif, except that Luby is used for exploration.
Both versions of the MetaMax algorithm were tested. Motivated by the fact that SPSA
is known to converge to a global optimum exponentially fast if f satisfies some restrictive
conditions (Gerencser & Vago, 2001), we chose a hr (n) that decays exponentially fast. To
control the exploration of the so far suboptimal algorithm instances, we allowed hr (n) to
be a time-varying function, that is, it changes with tr , the total number of function calls to
evaluate f (or equally, the total number of steps taken) by all algorithms so far. Thus, at
round r + 1 we used


hr (n) = en/

tr

(27)

(note that we used the time-varying version of hr also in the case of MetaMax(K)  the
latter can easily be extended to this situation, but this has been omitted to simplify the
presentation).
For the algorithms with a fixed number of local search instances (MetaMax(K), Unif,
EE-Unif, and ThrAsc), the number of instances K was set to 100 in the simulations, as
this choice provided reasonably good performance in all problems analyzed.
The multi-start algorithms were tested using two versions of a synthetic function, and
by tuning the parameters of a learning algorithm on two standard data sets.
433

fiGyorgy & Kocsis

The synthetic function was a slightly modified7 version of the Griewank function (Griewank,
1981):
d
d
Y
2xl X 4 2 x2l
cos  
f (x) =
100
l
l=1
l=1
where x = (x1 , . . . , xd ) and the xl were constrained to the interval [1, 1]. We show the
results for the 2-dimensional and the 10-dimensional cases.
The parameters of SPSA were a = 0.05 and  = 0.1 for the 2-dimensional case, and
a = 0.5 and  = 0.1 for the 10-dimensional case. The performance of the search algorithms
were measured by the error defined as the difference between the maximum value of the
function (in this case 1) and the best result obtained by the search algorithm in a given
number of steps. The results for the above multi-start strategies for the two- and the 10dimensional test functions are shown in Figure 5. Each error curve is averaged over 10,000
runs, and each strategy was run for 100,000 steps (or iterations). One may observe that in
both cases the two versions of the MetaMax algorithm converge the fastest. ThrAsc is
better than Unif, while Luby seems fairly competitive with these two. The two exploreand-exploit-type algorithms (EE-Unif and EE-Luby) have similar performance on the 2dimensional function, and clearly better than their non-exploiting base algorithms, but on
the 10-dimensional function their behavior is somewhat pathological in the sense that for low
values of T their performances are the best among all algorithms, but with increasing T , the
error actually increases such that their respective base algorithms achieve smaller errors for
some values of T . The random search seems an option only for the 2-dimensional function.
Similar results were obtained for dimensions between 2 and 10. The pathological behavior of
the explore-and-exploit algorithms start to appear gradually starting from the 5-dimensional
function, and it is pronounced from 8 dimensions onwards. Limited experimental data
obtained for higher dimensions up to 100 (averaged over only a few hundred runs) shows
that the superiority of MetaMax is preserved for high-dimensional problems as well.
The reason for the pathological behavior of the explore-and-exploit strategies (i.e., why
the error curves are not monotone decreasing in the number of iterations) can be illustrated
as follows. Assume we have two SPSA instances, one converging to the global optimum
and another one converging to a suboptimal local optimum. Assume that in the first few
steps the optimal algorithm gives better result, then the suboptimal algorithm takes over
and reaches its local maximum, while if the algorithms are run even further, the optimal
algorithm beats the suboptimal one. If the exploration is stopped in the first or the last
regime, an explore-and-exploit algorithm will choose the first, optimal local search instance,
whose performance may get to quite close to the global optimum in the exploitation phase
(even if it is stopped in the first regime). If the exploration is stopped in the middle regime,
the suboptimal search instance will be selected for exploitation, whose performance may
not even get close to the global optimum. In this scenario, the error after the exploitation
phase (i.e. at the end) is lower if T is small, and increases with higher values of T . Decrease
in the error with increasing T is only assured when the optimal instance converges in the
exploration phase past all suboptimal local optima, which results in selecting the optimal
local search instance for exploitation. In the above scenario the error will decrease fast
7. The modification was made in order to have more significant differences between the values of the function
at the global maximum and at other local maxima.

434

fi10

10

1

1

0.1

0.1

average error

average error

Efficient Multi-Start Strategies for Local Search Algorithms

0.01

0.001

0.0001

0.001

0.0001
Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

1e-06

0.01

1

Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

10

100

1000

10000

1e-06

100000

1

10

iteration

100

1000

10000

100000

iteration

Figure 5: The average error for the multi-start strategies on the 2-dimensional (left) and
10-dimensional (right) modified Griewank function. 99% confidence intervals are
shown with the same color as the corresponding curves. Note that most of the
intervals are very small.
1

0.1

0.01

0.01

average error

average error

0.1

0.001

0.0001
Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

1e-06

1

0.001

0.0001

Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

10

100

1000

10000

100000

iteration

1e-06

1

10

100

1000

10000

100000

iteration

Figure 6: The average error for the multi-start strategies on tuning the parameters of Multilayer Perceptron for the vehicle data set (left) and the letter data set (right).
99% confidence intervals are also shown with the same color as the corresponding
curves.

initially, then increase for a while and then it may decrease again till it converges to 0,
which is quite similar to what we observe in Figure 5, right. This pathological behavior
becomes more transparent if there are many local search algorithms, as the length of the
exploitation phase scales with the number of local search instances if the length of the
exploration for each instance is kept fixed. Analyzing the experimental data shows that
more complex versions of the scenario outlined above have occurred in the simulations and
are the main cause of the observed pathological behavior (the non-monotonicity of the error
curves).
For tuning the parameters of a learning algorithm, we have used two standard data
sets from the UCI Machine Learning Repository (Asuncion & Newman, 2007): vehicle and
435

fiGyorgy & Kocsis

letter, and the Multilayer Perceptron learning algorithm of Weka (Witten & Frank, 2005)
(here the back-propagation algorithm is used in the training phase). Two parameters were
tuned: the learning rate and the momentum, both in the range of [0, 1]. The size of the
hidden layer for the Multilayer Perceptron was set to 8, while the number of epochs to
100. The parameters of the SPSA algorithm were a = 0.5 and  = 0.1, the same as for
the 10-dimensional Griewank function (as in the previous experiment, the parameters were
chosen based on our experience). The rate of correctly classified items on the test set for
vehicle using Multilayer Perceptron with varying values of the two parameters is shown in
Figure 7, with the highest rate being 0.910112. Similarly, the classification rate for letter
is shown in Figure 8, with the highest rate being 0.7505.
The error rates of the optimized Multilayer Perceptron on the data sets vehicle and
letter are shown in Figure 6, when the parameters of the learning algorithm were tuned
by the multi-start strategies above. The error in these cases is the difference between the
best classification rate that can be obtained (0.910112 and 0.7505, respectively) and the
best classification rate obtained by the multi-start strategies in a given number of steps.
The results shown are averaged over 1,000 runs. We observe that the MetaMax algorithm
(with an increasing number of algorithm instances) converged fastest in average, the three
strategies with a fixed number of algorithm instances had nearly identical results, Luby
(and its explore-and-exploit variant) was slightly worse than these, and the random search
was the slowest, although it performed not nearly as badly as for the synthetic functions.
The reason why the random search had relatively better performance (relative to those that
used SPSA) could be twofold: (i) large parts of the error surface offer fairly small error,
and (ii) the error surface is less smooth, and therefore SPSA is less successful in using
gradient information. The explore-and-exploit variants performed well on the vehicle data
set initially, but their performance worsened for larger values of T (compared to MetaMax,
and to other algorithms to some extent). This, coupled with the observation for Figure 5,
right would suggest that explore-and-exploit variants are more competitive for small values
of T , despite their asymptotic guarantees.
In summary, the MetaMax algorithm (with an increasing number of algorithm instances) provided by far the best performance in all tests, usually requiring significantly
fewer steps to find the optimum than the other algorithms. E.g., for the letter data set
only the MetaMax algorithm found the global optimum in all runs in 100,000 time steps.
We can conclude that MetaMax converged faster than the other multi-start strategies investigated in all four test cases, with a notable advantage on the difficult surfaces (at least
from a gradient-based optimization viewpoint) induced by the classification tasks.
5.2 k-Means Clustering
In this section we consider the problem of partitioning a set of M d-dimensional real vectors
xj  Rd , j = 1, . . . , M into N clusters, where each cluster Si is represented by a center (or
reconstruction) point ci  Rd , i = 1, . . . , N . The cost function to be minimized is the sum
of distances
PN(x,Pci ) from the data points to the corresponding centers, that is, we want to
minimize i=1 xSi (x, ci ). There are two necessary conditions for optimality (see, e.g.,
Linde, Buzo, & Gray, 1980; Gersho & Gray, 1992): for all i = 1, . . . , N ,
Si = {x : (x, ci )  (x, cj ) for all j = 1, . . . , N }
436

(28)

fiEfficient Multi-Start Strategies for Local Search Algorithms

Figure 7: Classification rate on the vehicle data set. The rates are plotted by subtracting
them from 0.911112 and thus global optima are at the scattered black spots
corresponding to a value equal to 0.001.

Figure 8: Classification rate on the letter data set. The rates are plotted by subtracting them from 0.7515 and thus global optima are at the scattered black spots
corresponding to a value equal to 0.001.

(with ties broken arbitrarily) and
ci = argmin
cRd

X

xSi

437

(x, c).

(29)

fiGyorgy & Kocsis

P

x

8
i
A usual choice for  is the squared Euclidean distance, in which case ci = xS
|Si | . According to the above necessary conditions, the k-means algorithm (or Generalized-Lloyd
algorithm, see, e.g., Linde et al., 1980; Gersho & Gray, 1992) alternates between partitioning the data set according to (28) with the centers being fixed, and recomputing the
centers by (29) while the partitioning is kept fixed. It can easily be seen that the cost
(or error) cannot increase in any of the above steps, hence the algorithm converges to a
local minimum of the cost function. In practice, the algorithm stops when there is no (or
insufficient) decrease in the cost function. However, the k-means algorithm is often trapped
in a local optimum, whose value is influenced by the initial set of centers. As with SPSA,
restarting k-means with a different initialization may result in finding the global optimum.
We consider two initialization techniques: the first, termed k-means, chooses the centers
uniformly at random from the data points; the second, k-means++ (Arthur & Vassilvitskii,
2007) chooses an initial center uniformly at random from the data set, and then chooses the
further centers from the data points with a probability that is proportional to the distance
between the data point and the closest center already selected.

The k-means algorithm usually terminates after a relatively small number of steps, and
thus multi-start strategies with bounded number of instances would run out of active local
search algorithms, and therefore do not appear particularly attractive. However, this is a
natural domain to consider the strategy that starts a new instance, when the previous has
finished. This strategy will be referred subsequently as Serial. For the above mentioned
considerations, we test only MetaMax, the variant of our algorithms applicable for an
unbounded number of instances.9 As in the experiments with SPSA, we used hr as in (27).
Note that some theoretical results indicate that k-means may converge at an exponential
rate (in particular, Kieffer, 1982 showed that the rate of convergence is exponential for
random variables with log-concave densities in 1-dimension provided that the logarithm of
the density is not piecewise affine).
Two multi-start strategies, Serial and MetaMax were tested for the data set cloud
from the UCI Machine Learning Repository (Asuncion & Newman, 2007). The data set
was employed by Arthur and Vassilvitskii (2007) as well. The number of clusters was set to
ten. The performance of the multi-start strategies is defined as the difference between the
smallest cost function obtained by the strategy in a given number of steps and the smallest
cost seen in any of the experiments (5626.6357). The results averaged over 1,000 runs are
plotted in Figure 9. With both initialization methods the MetaMax strategy converges
faster then the Serial strategy. We note that for this data set, k-means++ with its
more clever initialization procedure yields faster convergence than the standard k-means
with its uniform initialization, which is consistent with the results presented by Arthur and
Vassilvitskii (2007).

8. An extension to clustering random variables is well-known and straightforward, but is omitted here
because in this paper we only consider clustering finite data sets.
9. Note that in the MetaMax algorithm we have to do the practical modification that if a local search
algorithm has terminated then it will not be chosen anymore. This clearly improves the performance as
an algorithm is not chosen anymore when no improvement can be observed.

438

fiEfficient Multi-Start Strategies for Local Search Algorithms

100000

10000
1000
100

average error

average error

10000

1000

100

10
1
0.1
0.01
0.001

10

0.0001
1

Serial kmeans
MetaMax kmeans

1

10

100

1000

10000

100000

iteration

1e-05

Serial kmeans++
MetaMax kmeans++

1

10

100

1000

10000

100000

iteration

Figure 9: The average error for the multi-start strategies with k-means (left) and kmeans++ (right). 99% confidence intervals are shown with the same color as
the corresponding curves.

5.3 Practical Considerations
In all experiments with the MetaMax algorithm presented above, we observed that the
number of algorithm instances r (shown in Figure 10) grows at a rate of (tr / ln tr ) (recall
that tr is the total number of function calls to evaluate f , or the total number of steps,
by all algorithm instances by the end of round r). On the other hand, in the derivation of

the theoretical bounds (see Theorem 15 and Theorem 16) we used a bound r  ( tr ).
In contrast to the quadratic penalty suggested by Theorem 16, plugging the (tr / ln tr )
estimate of r into the theorem we would find that only a logarithmic factor more calls
to evaluate f (total number of steps) are needed to achieve the performance of a search
algorithm started from the attraction region of the optimum.
Finally, perhaps the main practical question concerning the MetaMax family of multistart algorithms is to decide when to use them. As a rule of thumb, we have to say that
there should be a sufficiently large performance difference between an average run of
the local search algorithm and the best one. Clearly, if a single local search produces an
acceptable result then it is not worth the effort to run several instances of the local search,
especially not with a complicated schedule. In many real problems it is often the case
that it is relatively easy to get close to the optimum, which may be acceptable for some
applications, but approaching the optimum with greater precision is hard; if the latter is of
importance, the MetaMax algorithm and its variants may be very useful. Last, one may
wonder about the computational costs of our algorithms. As it was discussed before, we
consider the case when the evaluation of the target function is very expensive: this is clearly
not the case for the Griewank function, which is only used to demonstrate basic properties
of the algorithm, but holds for many of the optimization problems in practice, including
all other experiments considered in this paper. In these problems the function evaluation
is indeed expensive (and depends on the available data), while the overhead introduced
by the MetaMax algorithms depends on the number of rounds. For the MetaMax(K)
algorithm we have to find the upper convex hull of a set of K points in each round; in
the worst case this can take as long as O(K 2 ) calculations, but in practice this is usually
439

fiGyorgy & Kocsis

number of algorithm instances * ln(t)/t

1.8

Griewank 2D
Griewank 10D
vehicle
letter
K-MEANS
K-MEANS++
min
max

1.6
1.4
1.2
1
0.8
0.6
0.4
0.2

1

10

100

1000

10000

100000

iteration

Figure 10: Number of algorithm instances (r) for MetaMax. The average number of
instances are shown on the six benchmarks: Griewank function (2- and 10dimensional), parameter tuning of Multilayer Perceptron (on the vehicle and
on the letter data set), and clustering with k-means and k-means++. The
maximum and minimum number of instances over all runs of all benchmarks
are also shown. One can notice that for larger values of tr , 0.45tr / ln tr  r 
1.65tr / ln tr .

much cheaper, as the upper convex hull is determined by the point that corresponds to
the actually best estimate and by the point that corresponds to the least used algorithm,
which requires only O(K) computations, or even less, if some special ordering tricks are
introduced. Since the target function f is evaluated at least twice in each round, on average
at most O(K 2 ) computational overhead is needed for each evaluation of f in the worst
case, which is practically reduced to O(K), or even less. Similar considerations hold for the
MetaMax() and the MetaMax algorithms, resulting in an average O(r2 ) worst-case
overhead for each call to f (in r rounds), which is closer to O(r) or even less in practice.
In all examples we considered (apart of the case for the Griewank function), this amount
of overhead has been negligible relative to the computational resources needed to evaluate
f at a single point.

6. Conclusions
In this paper we provided multi-start strategies for local search algorithms. The strategies
continuously estimate the potential performance of each algorithm instance in an optimistic
way, by supposing a convergence rate of the local search algorithms up to an unknown
constant, and in every phase resources are allocated to those instances that could converge
to the optimum for a particular range of the constant. Three versions of the algorithm
were presented, one that is able to follow the performance of the best of a fixed number of
local search algorithm instances, and two that, with gradually increasing the number of the
local search algorithms, achieve global consistency. A theoretical analysis of the asymptotic
440

fiEfficient Multi-Start Strategies for Local Search Algorithms

behavior of the algorithms was also given. Specifically, under some mild conditions on the
function to be maximized (e.g., the set of the values of the local maxima is not dense at
the global maximum), our best algorithm, MetaMax, preserves the performance of a local
search algorithm for the original function class with at most a quadratic increase in the
number of times the target function needs to be evaluated (asymptotically). Simulations
demonstrate that the algorithms work quite well in practice.
While the theoretical bound suggests that the target function has to be evaluated a
quadratic factor more times to achieve the performance of a search algorithm that is started
from the attraction region of the optimum, in the experiments we found only a logarithmic
penalty. It is not clear whether this difference is the result of our slightly conservative
(asymptotic) analysis or the choice of the experimental settings. Also, a finite sample
analysis of the algorithm is of interest, as the experiments indicate that the MetaMax
algorithm provides good performance even for a relatively small number of steps taken by
the local search algorithms, in the sense that it provides a speed-up compared to other
approaches even if the number of times the target function can be evaluated (i.e., the total
number of steps that can be taken by all algorithms together) is relatively small. Finally,
future work is needed to clarify the connection between the convergence rate of the optimal
algorithms (g ) and the function hr used in the exploration.

Acknowledgments
The authors would like to thank the anonymous referees for their numerous insightful
and constructive comments. This research was supported in part by the Mobile Innovation
Center of Hungary, by the National Development Agency of Hungary from the Research and
Technological Innovation Fund (KTIA-OTKA CNK 77782), and by the PASCAL2 Network
of Excellence (EC grant no. 216886). Parts of this paper were presented at ECML 2009
(Kocsis & Gyorgy, 2009).

Appendix A. Proof of Lemma 1
bn ). Since Un  0 almost everywhere on E, Egoroffs
Fix   (0, 1) and let Un = f  f (X
theorem (see, e.g. Ash & Doleans-Dade, 2000) implies that there is an event E  E with
1  P (E ) <  such that Un  0 uniformly almost everywhere on E . The second part of
the lemma follows from the definition of uniform convergence.
2

References
Adam, K. (2001). Learning while searching for the best alternative. Journal of Economic
Theory, 101, 252280.
Arthur, D., & Vassilvitskii, S. (2007). k-means++: The advantages of careful seeding. In
Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
10271035.
Ash, R. B., & Doleans-Dade, C. A. (2000). Probability & Measure Theory. Academic Press.
441

fiGyorgy & Kocsis

Asuncion, A., & Newman, D. J. (2007). UCI machine learning repository.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite time analysis of the multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Bartz-Beielstein, T. (2006). Experimental Research in Evolutionary Computation  The
New Experimentalism. Natural Computing Series. Springer, New York.
Battiti, R., Brunato, M., & Mascia, F. (2008). Reactive Search and Intelligent Optimization,
Vol. 45 of Operations research/Computer Science Interfaces. Springer Verlag.
Beck, C. J., & Freuder, E. C. (2004). Simple rules for low-knowledge algorithm selection.
In Regin, J. C., & Rueher, M. (Eds.), CPAIOR, Lecture Notes in Computer Science
3011, pp. 5064. Springer.
Carchrae, T., & Beck, J. C. (2004). Low-knowledge algorithm control. In Proceedings of
the Nineteenth National Conference on Artificial Intelligence (AAAI), pp. 4954.
Cicirello, V. A., & Smith, S. F. (2004). Heuristic selection for stochastic search optimization:
Modeling solution quality by extreme value theory. In In Proceedings of the 10th
International Conference on Principles and Practice of Constraint Programming, pp.
197211. Springer.
Cicirello, V. A., & Smith, S. F. (2005). The max k-armed bandit: A new model of exploration
applied to search heuristic selection. In In Proceedings of the Twentieth National
Conference on Artificial Intelligence, pp. 13551361.
Finkel, D. E., & Kelley, C. T. (2004). Convergence analysis of the direct algorithm. Tech.
rep. CRSC-TR04-28, NCSU Mathematics Department.
Gagliolo, M., & Schmidhuber, J. (2006). Learning dynamic algorithm portfolios. Annals of
Mathematics and Artificial Intelligence, 47 (34), 295328. AI&MATH 2006 Special
Issue.
Gagliolo, M., & Schmidhuber, J. (2007). Learning restart strategies. In Veloso, M. M. (Ed.),
IJCAI 2007  Twentieth International Joint Conference on Artificial Intelligence,
vol. 1, pp. 792797. AAAI Press.
Gagliolo, M., & Schmidhuber, J. (2010). Algorithm selection as a bandit problem with
unbounded losses. In Blum, C., & Battiti, R. (Eds.), Learning and Intelligent Optimization, Vol. 6073 of Lecture Notes in Computer Science, pp. 8296. Springer
Berlin/Heidelberg.
Gerencser, L., & Vago, Z. (2001). The mathematics of noise-free SPSA. In Proceedings of
the IEEE Conference on Decision and Control, pp. 44004405.
Gersho, A., & Gray, R. M. (1992). Vector Quantization and Signal Compression. Kluwer,
Boston.
Griewank, A. O. (1981). Generalized descent for global optimization. Journal of Optimization Theory and Applications, 34, 1139.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimum cost paths. Systems Science and Cybernetics, IEEE Transactions on,
4 (2), 100 107.
442

fiEfficient Multi-Start Strategies for Local Search Algorithms

Hoos, H. H., & Stutzle, T. (1999). Towards a characterisation of the behaviour of stochastic
local search algorithms for SAT. Artificial Intelligence, 112, 213232.
Horn, M. (2006). Optimal algorithms for global optimization in case of unknown lipschitz
constant. Journal of Complexity, 22 (1), 5070.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: an automatic
algorithm configuration framework. Journal of Artificial Intelligence Research, 36 (1),
267306.
Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization without
the lipschitz constant. Journal of Optimization Theory and Applications, 79 (1), 157
181.
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C., & Selman, B. (2002). Dynamic restart policies. In Proceedings of the Eighteenth National Conference on Artificial Intelligence
(AAAI), pp. 674681.
Kieffer, J. C. (1982). Exponential rate of convergence for Lloyds method I. IEEE Trans.
Inform. Theory, IT-28, 205210.
Kocsis, L., & Gyorgy, A. (2009). Efficient multi-start strategies for local search algorithms.
In Buntine, W., Grobelnik, M., Mladenic, D., & Shawe-Taylor, J. (Eds.), Machine
Learning and Knowledge Discovery in Databases, Vol. 5781 of Lecture Notes in Computer Science, pp. 705720. Springer Berlin/Heidelberg.
Linde, Y., Buzo, A., & Gray, R. M. (1980). An algorithm for vector quantizer design. IEEE
Transactions on Communications, COM-28, 8495.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup of Las Vegas algorithms.
Information Processing Letters, 47, 173180.
Mart, R., Moreno-Vega, J., & Duarte, A. (2010). Advanced multi-start methods. In Gendreau, M., & Potvin, J.-Y. (Eds.), Handbook of Metaheuristics, 2nd edition (2 edition).
Springer.
Nesterov, Y. (2004). Introductory Lectures on Convex Optimization: A Basic Course.
Kluwer Academic Publishers.
Ribeiro, C., Rosseti, I., & Vallejos, R. (2009). On the use of run time distributions to evaluate
and compare stochastic local search algorithms. In Stutzle, T., Birattari, M., & Hoos,
H. (Eds.), Engineering Stochastic Local Search Algorithms. Designing, Implementing
and Analyzing Effective Heuristic s, Vol. 5752 of Lecture Notes in Computer Science,
pp. 1630. Springer Berlin/Heidelberg.
Spall, J., Hill, S., & Stark, D. (2006). Theoretical framework for comparing several stochastic
optimization approaches. In Calafiore, G., & Dabbene, F. (Eds.), Probabilistic and
Randomized Methods for Design under Uncertainty, chap. 3, pp. 99117. SpringerVerlag, London.
Spall, J. C. (1992). Multivariate stochastic approximation using a simultaneous perturbation
gradient approximation. IEEE Transactions on Automatic Control, 37, 332341.
Spall, J. C. (1998). Implementation of the simultaneous perturbation algorithm for stochastic optimization. IEEE Transactions on Aerospace Electronic Systems, 34, 817823.
443

fiGyorgy & Kocsis

Streeter, M. J., & Smith, S. F. (2006a). An asymptotically optimal algorithm for the max
k-armed bandit problem. In Proceedings, The Twenty-First National Conference on
Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, pp. 135142.
Streeter, M. J., & Smith, S. F. (2006b). A simple distribution-free approach to the max
k-armed bandit problem. In Principles and Practice of Constraint Programming CP 2006, 12th International Conference, CP 2006, Nantes, France, September 25-29,
2006, Proceedings, pp. 560574.
Vilalta, R., & Drissi, Y. (2002). A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18 (2), 7795.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and
Techniques (2nd edition). Morgan Kaufmann, San Francisco.
Zabinsky, Z. B., Bulger, D., & Khompatraporn, C. (2010). Stopping and restarting strategy
for stochastic sequential search in global optimization. J. of Global Optimization,
46 (2), 273286.

444

fiJournal of Artificial Intelligence Research 41 (2011) 1-24

Submitted 10/2010; published 05/2011

Properties of Bethe Free Energies and Message Passing
in Gaussian Models
Botond Cseke
Tom Heskes

b.cseke@science.ru.nl
t.heskes@science.ru.nl

Institute for Computing and Information Sciences
Faculty of Science, Radboud University Nijmegen
Heyendaalseweg 135, 6525 AJ, The Netherlands

Abstract
We address the problem of computing approximate marginals in Gaussian probabilistic
models by using mean field and fractional Bethe approximations. We define the Gaussian fractional Bethe free energy in terms of the moment parameters of the approximate
marginals, derive a lower and an upper bound on the fractional Bethe free energy and
establish a necessary condition for the lower bound to be bounded from below. It turns out
that the condition is identical to the pairwise normalizability condition, which is known to
be a sufficient condition for the convergence of the message passing algorithm. We show
that stable fixed points of the Gaussian message passing algorithm are local minima of
the Gaussian Bethe free energy. By a counterexample, we disprove the conjecture stating
that the unboundedness of the free energy implies the divergence of the message passing
algorithm.

1. Introduction
One of the major tasks of probabilistic inference is calculating marginal posterior probabilities of a set of variables given some observations. In case of Gaussian models, the
computational complexity of computing marginals might scale cubically with the number of
variables, while for models with discrete variables it often leads to intractable computations.
Computations can be made faster or tractable by using approximate inference methods like
the mean field approximation (e.g., Jaakkola, 2000) and the Bethe-type approximation (e.g.,
Yedidia, Freeman, & Weiss, 2000). These methods were developed for discrete probabilistic
graphical models, but they are applicable to Gaussian models as well. However, there are
important differences in their behavior for the discrete and Gaussian cases. For example,
while in discrete models the error function of the Bethe approximationcalled Bethe free
energyis bounded from below (Heskes, 2004; Watanabe & Fukumizu, 2009), in Gaussian
models this might not always the case (Welling & Teh, 2001).
An understanding of properties of the Bethe free energy of Gaussian models might
also be help to understand the properties of the energy function in conditional Gaussian
models. Conditional Gaussian or hybrid graphical models, such as switching Kalman filters
(Zoeter & Heskes, 2005), combine both discrete and Gaussian variables. Approximate
inference in these models can be carried out by expectation propagation (e.g., Minka, 2004,
2005) which can be viewed as a generalization of the Bethe approximation, where the
marginal consistency constraints on the approximate marginals are replaced by expectation
constraints (Heskes, Opper, Wiegerinck, Winther, & Zoeter, 2005). In order to understand
c
2011
AI Access Foundation. All rights reserved.

fiCseke & Heskes

the properties of the Bethe free energy of hybrid models, a good understanding of the two
special cases of discrete and Gaussian models is needed. While the properties of the Bethe
free energy of discrete models have been studied extensively in the last decade and are well
understood (Yedidia et al., 2000; Heskes, 2003; Wainwright, Jaakkola, & Willsky, 2003;
Watanabe & Fukumizu, 2009), the properties of the Gaussian Bethe free energy have been
studied much less.
The message passing algorithm is a well established method for finding the stationary
points of the Bethe free energy (Yedidia et al., 2000; Heskes, 2003). It works by locally
updating the approximate marginals and has been successfully applied in both discrete (e.g.,
Murphy, Weiss, & Jordan, 1999; Wainwright et al., 2003) and Gaussian models (e.g., Weiss
& Freeman, 2001; Rusmevichientong & Roy, 2001; Malioutov, Johnson, & Willsky, 2006;
Johnson, Bickson, & Dolev, 2009; Nishiyama & Watanabe, 2009; Bickson, 2009). Gaussian
message passing is the simplest case of a free-energy based message passing algorithm on
models with continuous variables, therefore, it is important to understand its behavior.
Gaussian message passing has many practical applications like in distributed averaging
(Moallemi & Roy, 2006), peer-to-peer rating, linear detection, SVM regression (Bickson,
2009) and more generally in problems that involve solving large sparse linear systems or
approximating the marginal variances of large sparse Gaussian systems typically encountered in distributed computing settings. For further applications the reader is referred to
the work of Bickson (2009) and references therein.
Finding sufficient conditions for the convergence of message passing in Gaussian models
has been successfully addressed by many authors. Using the computation tree approach,
Weiss and Freeman (2001) proved that message passing converges whenever the precision
matrixinverse covarianceof the probability distribution is diagonally dominant1 . With
the help of an analogy between message passing and walksum analysis, (Malioutov et al.,
2006) derived the stronger condition of pairwise normalizability2 . A different approach was
taken by Welling and Teh (2001), who directly minimized the Bethe free energy with regard
to the parameters of approximate marginals, conjecturing that Gaussian message passing
converges if and only if the free energy is bounded from below. Their experiments showed
that message passing and direct minimization either converge to the same solution or both
fail to converge. We adopt a similar approach, that is, instead of analyzing the properties
of the Gaussian message passing algorithm using approaches like in Weiss and Freeman or
Malioutov et al., we choose to study the properties of the Gaussian Bethe free energy and
its stationary points. This will help us to draw conclusions about the existence of local
minima, the possible stable fixed points to which message passing can converge.
This paper is structured as follows. In Section 2 we introduce Gaussian Markov random
fields and the message passing algorithm. In Section 3 we define the Gaussian fractional
Bethe free energies parameterized by the moment parameters of the approximate marginals
and derive boundedness conditions for them. These two sections are based on the authors
earlier work (Cseke & Heskes, 2008). In Section 4 we analyze the stability properties of the
Gaussian message passing algorithm and, using a similar line of argument as Watanabe and
P
1. The matrix A is diagonally dominant if |Aii | > j6=i |Aij | for all i.
2. Following the work of Malioutov et al. (2006), we call a Gaussian distribution pairwise normalizable
if it
Q
can be factorized into a product of normalizable pair factors, that is, p(x1 , . . . , xn ) = ij ij (xi , xj )
such that all ij s are normalizable.

2

fiBethe Free Energies and Message Passing in Gaussian Models

Fukumizu (2009), we show that its stable fixed points are indeed local minima of the Bethe
free energy. We conclude the paper with a few experiments in Sections 5 and 6 supporting
our results and their implications.

2. Approximating Marginals in Gaussian Models
The probability density of a Gaussian random vector x  Rn is defined in terms of canonical
parameters h and Q as


1 T
T
p(x)  exp h x  x Qx ,
(1)
2
where Q is s positive definite matrix. The expectation m and the covariance V of x is
then given by m = Q1 h and V = Q1 respectively. In many real world applications the
matrix Q is sparse with and it typically has low density, that is, the number of non-zero
elements in Q scales with the number of variables n.
This probability density can also be defined in terms of an undirected probabilistic
graphical model commonly known as Gaussian Markov random field (GMRF). Since the
interactions between the variables in p are pairwise, we can associate the variables xi to the
nodes v  V = {1, . . . , n} of an undirected graph G = (V, E), where the edges e  E  V V
of the graph stand for the non-zero off-diagonal elements of Q. We use i  j as a proxy for
(i, j)  E. By using the notation introduced above, the density p in (1) can be written as
the product
Y
p(x) 
ij (xi , xj )
(2)
ij

of Gaussian functions ij (xi , xj ) (also called potentials) associated with the edges e = (i, j)
of the graph. If h and Q are given then we can define the potentials as
j
j
i
i
ij (xi , xj ) = exp {ij
hi xi + ij
hj xj  ij
Qii x2i /2  ij
Qjj x2j /2  Qij xi xj } ,

P
P
j
i
where
ij ij = 1 and
ji ij = 1 are partitioning h and Q into the corresponding
factors. In practice, however, the factors ij might be given by the problem at hand and
i and  j computed by summing their parameters and computing the
h and Q as well as ij
ij
partitioning respectively. Without loss of generality, we can and we will use Qii = 1, since
the results in the paper can be easily re-formulated for general Qs by a rescaling of the
variables (e.g., Malioutov et al., 2006).
The numerical calculation of all marginals, can be done by solving the linear system
m = Q1 h and performing a sparse Cholesky factorization LLT = Q followed by solving the Takahashi equations (Takahashi, Fagan, & Chin, 1973). An alternative option to
calculate the marginal means and to approximate marginal variances is to run the Gaussian message passing algorithm in the probabilistic graphical model associated with the
representation in (2). The Gaussian message passing algorithm is the Gaussian variant
of message passing algorithm (Pearl, 1988), which is a dynamical programming algorithm
introduced to compute marginal densities in discrete probabilistic models with pairwise interactions and tree-structured graphs G. However, it turned out that by running it in loops
on graphs with cycles, it yields good approximations of the marginal distributions (Murphy
et al., 1999). Weiss and Freeman (2001) showed that when the Gaussian message passing
3

fiCseke & Heskes

Figure 1: An illustration of the incoming and outgoing messages at adjacent nodes i and j.

algorithm is converging, it computes the exact mean parameters m, thus it can also be
used for solving linear systems (e.g., Bickson, 2009). Message passing works by updating
and passing directed messages along the edges of the graph G, which, in case the algorithm
converges, are then used to compute (approximate) marginal probability distributions. The
Gaussian and the discrete algorithms have the same functional form with the exception of
the summation (discrete case) and integration operators (Gaussian case). Each message
ij (xi ) is updated according to
Z
Y
new
ij (xi ) = dxj ij (xi , xj )
jk (xj ) ,
(3)
kj\i

where i = {j : j  i} denotes the index set of variables connected to xi in G. At each step
the current approximations qij (xi ,j ) of p(xi , xj ) can be computed according to
qij (xi , xj )  ij (xi , xj )

Y
li\j

il (xi )

Y

jk (xj ) .

(4)

kj\i

The update steps in (9) have to be iterated until convergence. The corresponding qij (xi , xj )s
yield the final approximation of the p(xi , xj )s. It is common to use damping, that is,
1 new (x ) with   (0, 1]. In practice, this helps to
to replace new
ij (xi ) by ij (xi )
ij i
dampen the possible periodic paths of (3), but it keeps the properties of the fixed points
unchanged. Figure 1 illustrates the incoming and outgoing messages at the nodes associated
with variables xi and xj . A quite significant difference between the discrete and Gaussian
the message passing is the replacement of the sum operator with the integral operator.
While finite sums always exist, the integral in (3) can become infinite. This problem can
be remedied technically by a canonical parameterization (see Section 4) which keeps the
algorithm running, but it can lead to non-normalizable approximate marginals qij , and thus
a (possible) break-down of the algorithm.
Message passing was introduced by Pearl (1988) as a heuristic algorithm (in discrete
models), however, Yedidia et al. (2000) showed that it can also be viewed as an algorithm for
4

fiBethe Free Energies and Message Passing in Gaussian Models

finding the stationary points of the so-called Bethe free energy, an error function measuring
the difference between p and a specific family of distributions to be detailed in the next
section. It has been shown by Heskes (2003) and later in a different way by Watanabe and
Fukumizu (2009) that stable fixed points of the (loopy) message passing algorithm are local
minima of the corresponding Bethe free energy. In this paper we show that this holds for
Gaussian models as well.
Our interest in the properties of the Gaussian Bethe free energy and the corresponding Gaussian message passing algorithm is motivated mainly by their implications in more
general models and inference algorithms like non-Gaussian models and expectation propagation, respectively. For this reason, we will not compare the speed of the method and the
accuracy of the approximation with the above mentioned exact linear algebraic methods.
As mentioned in the introduction, the approach we take is similar to that of Welling and
Teh (2001), that is, we study the properties of the Gaussian Bethe free energy, parameterized
in terms of the moment parameters of the approximate marginals. In the following we
introduce the mean field and the Bethe approximation in Gaussian models. Readers familiar
with this subject can continue with Section 3.
2.1 The Gaussian Bethe Free Energy
A popular method to approximate marginals is approximating p with a distribution q having
a form that makes marginals easy to identify, for example, it factorizes or it has a treelike form. The most common quantity to measure the difference between two probability
distributions is the Kullback-Leibler divergence D [q || p]. It is often used to characterize the
quality of the approximation and formulate the computation of approximate marginals as
the optimization problem


Z
q(x)

q (x) = argmin dx q(x) log
.
(5)
p(x)
qF
Here, F is the set of distributions with the above mentioned form. Since it is not symmetric,
the Kullback-Leibler divergence is not a distance, but D [q || p]  0 for any proper q and p,
D [q || p] = 0 if and only if p = q, and it is convex both in q and p.
A family F of densities possessing a form that
Q makes marginals easy to identify is the
family of distributions that factorize as q(x) = k qk (xk ). In other words, in problem (5) we
approximate p with a distribution that has independent variables. An approximation q of
thisQtype is called mean field approximation (e.g., Jaakkola, 2000). Defining FMF ({qk }) =
D [ qk || p] and writing out the right hand side of (5) in detail, one gets
Z
FMF ({qk }) = 

dx

Y

qk (xk ) log p(x) +

k

XZ

dxk qk (xk ) log qk (xk ).

k

Using the parameterization qk (xk ) = N (xk |mk , vk ), m = (m1 , . . . , mn )T and v = (v1 , . . . , vn )T ,
this reduces to
1
1X
1X
FMF (m, v) = hT m + mT Qm +
Qkk vk 
log(vk ) + CMF ,
2
2
2
k

5

k

fiCseke & Heskes

Q
where CMF is an irrelevant constant. Although D [ k qk || p] might not be convex in
(q1 , . . . , qn ), one can easily check that FMF is convex in its variables m and v and its
minimum is obtained for m = Q1 h and vk = 1/Qkk . Since

1
 1 

1
Q kk = Qkk  QTk,\k Q\k,\k
Q\k,k
,
one can easily see that the mean field approximation underestimates variances. The mean
field approximation computes a solution in which the means are exact, but the variances are
computed as if there were no interactions between the variables, namely, as if the matrix
Q were diagonal, thus giving poor estimates of the variances.
In order to improve the estimates for variances, one has to choose approximating distributions q that are able to capture dependencies between the variables in p. It can be
verified that any distribution in which the dependencies form a tree graph can be written
in the form
Y p(xi , xj ) Y
p(x) =
p(xk ),
p(xi )p(xj )
ij

k

where i and j run through the edges (i, j) of the tree and k through the nodes 1, . . . , n.
Although in most cases the undirected graph generated by the non-zero elements in Q is
not a tree, based on the tree intuition one can construct q from one and two variable
marginals as
Y qij (xi , xj ) Y
q(x) 
qk (xk )
(6)
qi (xi )qj (xj )
ij

k

andR constrain the functions qij and qk to be marginally
consistent and normalize to 1, that
R
is, dxj qij (xi , xj ) = qi (xi ) for any i  j and dxk qk (xk ) = 1 for any k. An approximation
of the form (6) together with the constraints on qij s and qk s is called a Bethe approximation.
Let us denote the family of such functions by FB . By choosing qij (xi , xj ) = qi (xi )qj (xj ) one
can easily check that FMF  FB , thus FB is non-empty. Assuming that the approximate
marginals are correct and q normalizes to 1 and then substituting (6) into (5), we get an
approximation of the KullbackLeibler divergence in (5) called the Bethe free energy.
Due to the factorization of p, we can write the Bethe free energy as
XZ
FB ({qij , qk }) = 
dxi,j qij (xi,j ) log ij (xi,j )
(7)
ij

+

XZ
ij



 XZ
qij (xi,j )
dxi,j qij (xi,j ) log
+
dxk qk (xk ) log qk (xk ).
qi (xi )qj (xj )
k

One can also define the free energy through the Bethe approximation
Z
XZ
dx q (x) log q (x) 
dxi,j q (xi,j ) log q (xi,j )
ij

+

X
k

6

Z
(1  nk )

dxk q (xk ) log q (xk )

fiBethe Free Energies and Message Passing in Gaussian Models

of the entropy (e.g., Yedidia et al., 2000) and substitute the marginals with functions qij and
qRk that normalize to one and are connected through the marginal consistency constraints
dxj qij (xi , xj ) = qi (xi ).
From the stationary conditions of the Lagrangian corresponding to the fractional Bethe
free energy (7) and the marginal consistency and normalization constraints, one can derive
the same iterative algorithm as in (3) for the corresponding Lagrange multipliers of the
consistency constraints (Yedidia et al., 2000). Similarly, approximate marginals can then
be computed according to (4). It can be shown that there is a one-to-one correspondence
between the stationary points of the Bethe free energy (7) and the fixed points of the
message passing algorithm (3). Later, in Section 4 we will link the stable fixed points of (3)
to the local minima of (7).
2.2 Fractional Free Energies and the Message Passing Algorithm
As mentioned in the introduction, in case of Gaussian models the message passing algorithm
does not always converge. The reason for this appears to be that the approximate marginals
may get indefinite or negative definite covariance matrices. Welling and Teh (2001) pointed
out that this can be due to the unboundedness of the Bethe free energy.
Since FMF is convex and bounded and the Bethe free energy might be unbounded, it
seems plausible to analyze the fractional Bethe free energy
XZ
F ({qij , qk }) = 
dxi,j qij (xi,j ) log ij (xi,j )
(8)
ij


 XZ
X 1 Z
qij (xi,j )
+
dxi,j qij (xi,j ) log
+
dxk qk (xk ) log qk (xk ).
ij
qi (xi )qj (xj )
ij

k

introduced by Wiegerinck and Heskes (2003). Here,  denotes the set of positive reals {ij }.
They showed that the fractional Bethe free energy interpolates between the mean field
and the Bethe approximation. That is, for ij = 1 we get the Bethe free energy, while in the
case when all ij s tend to 0, the mutual information between variables xi and xj is highly
penalized, therefore, (8) enforces solutions close to the mean field solution. They also showed
that the fractional message passing algorithm derived from (8) can be interpreted as Pearls
message passing algorithm with the difference that instead of computing local marginals
like in Pearls algorithmone computes local ij marginals.3 The local ij marginals
correspond to true local marginals when ij = 1 and to local mean field approximations
when ij = 0. The resulting algorithm is called the fractional message passing algorithm
and the message updates are defined as
Z
Y

new
(x
)
=
dxj ij (xi , xj )
jk (xj ) ji (xj )1 ,
(9)
ij i
kj\i

while the approximate marginals are computed according to
Y
Y
qij (xi , xj )  ij (xi , xj )
il (xi ) ij (xi )1
jk (xj ) ji (xj )1 .
li\j

(10)

kj\i



Q
3. We define the marginals of a distribution p as argmin{qk } D p k qk , where D is the divergence
k
R

R
R
D [p || q] = dxp(x) q(x)1 +  dxp(x) + (1  ) dxq(x) /(1  ) (e.g., Minka, 2005).

7

fiCseke & Heskes

Power expectation propagation by Minka (2004) is an approximate inference method
that uses local approximations with divergences. In case of Gaussian models power
expectation propagationwith a fully factorized approximating distributionleads to the
same message passing algorithm as the one derived from (8) and the appropriate constraints.
Starting from the idea of creating an upper bound on the log partition function when p and
q are exponential distributions, Wainwright et al. (2003) derived a form of (8) where the
ij s are chosen such that this bound is convex in {qij , qk }.
Message passing works well in practice, however, there are other ways to find the local
minima of the fractional free energies like the direct minimization w.r.t. some parameterization of the approximate marginals qij and qk (Welling & Teh, 2001). The latter method is
slower but more likely to converge. In the following we analyze the Bethe free energy when
expressed in terms of the moment parameters of the approximate marginals qij . Later in
Section 4 we analyze the stability conditions of the fractional message passing algorithm
and by expressing these conditions in term of the moment parameters of the approximate
marginals, we show that stable fixed points of the fractional Gaussian message passing are
local minima of the fractional Bethe free energy.

3. Bounds on the Gaussian Bethe Free Energy
In this section we analyze the parametric form of (8). We show that the fractional Gaussian Bethe free energy is a non-increasing function of . By letting all ij tend to infinity, we obtain a lower bound for the free energies. It turns out that the condition for
the lower bound to be bounded from below is the same as the pairwise normalizability
condition in the work of Malioutov et al. (2006).
As mentioned in Section 2, without loss of generality, we can work with a unit diagonal Q. We define R to be a matrix with zeros on its diagonal and Q = I + R,
where I is the identity matrix. |R| will be the matrix formed by the absolute values
of Rs elements. We use the moment parameterization qij (xi,j ) = N (xi,j |mij , Vij ) and
i , v ; v , v j ], with v = v .
qk (xk ) = N (xk |mk , vk ), where mij = (miij , mjij )T and Vij = [vij
ij
ji ij
ij
ji
i = mi and v  v i = v k for all i  j and i  k, we embed the
By using mi  m
i
ij
ik
ik
R ij
R
marginalization ( dxj qij (xi , xj ) = qi (xi ) for all i  j) and normalization ( dxj qj (xj ) = 1)
constraints into the parameterization. With a slight abuse of notation the matrix formed
by diagonal elements vk and off-diagonal elements vij is denoted by V (we can take vij = 0
for all i  j), the vector of means by m = (m1 , . . . , mn )T and the vector of variances by
v = (v1 , . . . , vn )T . Substituting qij and qk into (8) one gets
1
1
F (m, V ) =  hT m + mT Qm + tr(QT V )
2
2 !
2
X
v
1
1
1X
ij

log 1 

log (vk ) + C,
2
ij
vi vj
2
ij

(11)

k

where C is an irrelevant constant. Note that the variables m and V are independent, hence
the minimizations of F (m, V ) with regard to m and V can be carried out independently.
8

fiBethe Free Energies and Message Passing in Gaussian Models

Property 1. F (m, V ) is convex and bounded in (m, {vij }i6=j ) and at any stationary point
we have
m = Q1 h
vij



p
1 + (2ij Rij )2 vi vj  1
= sign(Rij )
.
2ij |Rij |

(12)

Proof: Q is positive definite by definition, therefore, the quadratic term in m is convex
and bounded. The variables m and V are independent and the minimum with regard
to m is achieved at m = Q1 h. One can check that the second order derivative of
F (m, V ) with regard to vij is non-negative and the first order derivative has only one
2  v v . Since the variables v are independent, one can conclude
solution when vi vj  vij
i j
ij
that F (m, V ) is convex in vij . From the independence of m and V , it follows that F is
convex in (m, {vij }i6=j ).

2 , thus the
Since the Vij s are constrained to be covariance matrices, we have vi vj > vij
first logarithmic term in (11) is negative. As a consequence,

F1 (m, V )  F2 (m, V )

for any

0 < 1  2 ,

where 1  2 is taken element by element. This observation leads to the following
property.
Property 2. With ij = , F is a non-increasing function of .
 into F we define the constrained function
Using Property 1 and substituting vij

1
1X
Fc (m, v) = hT m + mT Qm +
vk
2
2
k
q

1X 1
1 + (2ij Rij )2 vi vj  1

2

ij ij
!
p
1 + (2ij Rij )2 vi vj  1
1 X 1

log 2
2
ij
(2ij Rij )2 vi vj
n(i,j)

1X

log(vk ) + C c ,
2

(13)

k

where C c is an irrelevant constant. From Property 2, it follows that when choosing ij = ,
the function in (13) is a non-increasing function of . It then makes sense to take   
and verify whether we can get a lower bound for (13).
Lemma 1. For any v > 0, 0  1  1 and 2  1 the following inequalities hold.


FMF (m, v)  Fc1 (m, v)  FB m, {vij
}, v


FB m, {vij
}, v  Fc2 (m, v) . . .

1 T
. . .  FMF (m, v) 
v |R| v
2
Moreover, they are tight, that is,


lim F m, {vij
()}, v = FMF (m, v)
0

9

fiCseke & Heskes

and



1 T

v |R| v.
lim F m, {vij
()}, v = FMF (m, v) 
2
Proof: Since the Bethe free energy is the specific case of the fractional Bethe free energy for
 ()}, v) follow from Property 2. Now, we show that
 = 1, the inequalities on FB (m, {vij
the upper and lower bounds are tight. The function (1 + x2 )1/2  1 behaves as 12 x2 in the
neighborhood of 0, therefore,


v  2 ()
log 1  ijvi vj
 2 ()
vij
1

lim
lim vij
() = 0
and
lim
=
= 0,
0
0

vi vj 0 


showing that FMF (m, v) is a tight upper bound.
As  tends to infinity, we have
p
1 + (2Rij )2 vi vj  1
 
= |Rij | vi vj
lim

2
and
1
log
lim
 

!
p
1 + (2Rij )2 vi vj  1
= 0,
(2Rij )2 vi vj

yielding a tight lower bound


1 T

v |R| v.
lim F m, {vij
()}, v = FMF (m, v) 

2



Let max (|R|) be the largest eigenvalue of |R|. Analyzing the boundedness of the lower
bound, we arrive at the following theorem.
Theorem 1. For the fractional Bethe free energy in (11) corresponding to a connected
Gaussian model, the following statements hold
(1) if max (|R|) < 1, then F is bounded from below for all  > 0,
(2) if max (|R|) > 1, then F is unbounded from below for all  > 0,
P P 1
(3) if max (|R|) = 1, then F is bounded from below if and only if
ij  2n.
i ij

Proof: Since in F there is no interaction between the parameters m and V and the term
depending on m is bounded from below due to the positive definiteness of Q, we can simply
neglect this term when analyzing the boundedness of F . Let us write out in detail the
lower bound of the fractional Bethe free energies in the form

1 T
v |R| v =
2

1 T
1
1 T 1
m Q m  hT m +
v (I  |R|) v  1T log(v) + const.
2
2
2

FMF (m, v) 

(14)

Statement (1): The condition max (|R|) < 1 implies that I  |R| is positive definite. Now,
10

fiBethe Free Energies and Message Passing in Gaussian Models

 T
 T




log(x)  x  1, thus 12 v (I  |R|) v  1T log( v)  21 v (I  |R|) v  1T v + n.
The latter is bounded from below and so it follows that (14) is bounded from below as
well. According to Lemma 1, the boundedness of (14) implies that all fractional Bethe free
energies are bounded from below.
Statement (2): We assumed that the Gaussian network is connected and undirected. According to the Perron-Frobenius theory of non-negative matrices (e.g., Horn & Johnson,
2005), |R| has a simple maximal eigenvalue max (|R|) and all elements of the eigenvector umax corresponding to it are positive. Let us take the fractional Bethe free energy

and analyze its behavior when v = tumax and t  . For large values of t we have
(1 + (2ij Rij )2 (uimax ujmax )2 t4 )1/2 ' 2ij |Rij |uimax ujmax t2 , therefore, the sum of the second
and third term in (13) simplifies to (1  max (|R|))t2 and this term dominates over the
logarithmic ones as t  . As a result, the limit is independent of the choice of ij and it
tends to  whenever max (|R|) > 1.
Statement (3): If max (|R|) = 1, then the only direction in which the quadratic term will

not dominate is v = tumax . Therefore, we have to analyze the P
behavior of the loga1
rithmic terms in (13) when t  . For large ts these behave as ( ij ij
 2n) log(t).
c
For this reason, the boundedness of F and thus of F depends on the condition in
statement (3).

It was shown by Malioutov et al. (2006) that the condition max (|R|) < 1 is an equivalent
condition to pairwise normalizability. Therefore, pairwise normalizability is not only a
sufficient condition for the message passing algorithm to converge, but it is also a necessary
condition for the fractional Gaussian Bethe free energies to be bounded. Using Lemma 1, we
can show that for a suitably chosen  > 0 there always exists an  such that the constrained
fractional free energy Fc possesses a local minimum for any 0 <  <  (Property A2 in
Section A of the Appendix).
Example In the case of models with an adjacency matrix (non-zero entries of R) corresponding to a Kregular graph4 and equal interaction weights Rij = r, the maximal eigenvalue of |R| is max (|R|) = Kr and the eigenvector corresponding to this eigenvalue is 1.
(We define 1 as the vector that has all its elements equal to 1.) The model is symmetric
and by verifying the stationary point conditions, it turns out that for some choice of r and
 there exists a local minimum, which also lies in the direction 1. One can show that when
the model is not pairwise normalizable (Kr > 1), the critical r below
p which the fractional
Bethe free energy possesses this local minimum is rc (K, ) = 1/2 (K  ) and for any
valid r the critical  below which
p the fractional Bethe free energies possesses this local
minimum is c (K, r) = 21 K(1  1  1/(Kr)2 ). These results are illustrated in Figure 2.
(Note that for 2regular graphs, all valid models are pairwise normalizable and possess a
unique global minimum.)

For Kregular graphs, the convexity of the fractional Bethe free energy in terms of
{qij , qk } requires   K, a much stronger condition than   c (K, r). Thus, if we choose
 sufficiently large such that the Bethe free energy is guaranteed to have a unique global
minimum, this minimum is unbounded.

4. A Kregular graph is a graph in which all nodes are connected to K other nodes.

11

fiCseke & Heskes

$

!"

$

)

!"

+0")12&3,)*.&456
+)D)1"7+834.59
+0!:;
+< VTUWff.fi
+0+

#

%&'(&)*+&&)&,&+-.&/

%&'(&)*+&&)&,&+-.&/

#

!"

834.5



!"

"

!"



!"

)

_0")12&3,)*.&456
_)D)7"8"!9!"":
_0!)1%&'(&6
_0_;

!"

_)0)')14<=&+)><?,56


!"

"

!"

)





!"



!"

"

!"
m

!

!"

!"



)



!"

!"



!"

"

!"
m

!

!"



!"

Figure 2: Visualizing critical parameters for a symmetric K-regular Gaussian model with Rij = r.
Plots
in the left panel correspond to the constrained fractional Bethe free energies Fc for

v = 1 for an 8 node 4regular Gaussian model with r=0.27 (Kr > 1) and varying
.

Plots in the right panel correspond to the constrained Bethe free energies F1c for v = 1
in an 8 node 4regular Gaussian model with varying r. Here, rvalid is the supremum of
rs for which the model is valid, that is, Q is positive definite.

This example disproves the conjecture by Welling and Teh (2001), that is, even when
the Bethe free energy is not bounded from below, it can possess a finite local minimum to
which the message passing and the minimization algorithms can converge.

4. The Message Passing Algorithm in Gaussian Models
In this section, we turn our attention towards the properties of the message passing algorithm in Gaussian models. Following a similar line of argument as Watanabe and Fukumizu
(2009) we show that stable fixed points of the message passing algorithm correspond to local
minima of the Bethe free energy. We use the moment parameterization introduced in the
previous sections. The way we proceed is the following: (1) we make a linear expansion of
message passing iteration at a fixed point, (2) we express the linear expansion in terms of
moment parameters corresponding to the fixed point and finally (3) we connect the properties of the latter with the properties of the Hessian of the Bethe free energy by using the
matrix determinant lemma.
The form of the equation (9) implies that the messages ij (xi ) are univariate Gaussian
functions, thus we can express them in terms of two scalar (canonical) parameters ij and
ij such that log ij (xi ) = ij x2i /2 + ij xi + ijj , where the ij s are irrelevant constants.
When expressed in terms of ij and ij , the damped message passing algorithm (9) translates
12

fiBethe Free Energies and Message Passing in Gaussian Models

to
j
hj +
ij


new
ij

=

(1  )ij +

P

jk + (1  )ji



  i
kj\i

P
ij hi  Rij

j

ij
+
jk + (1  )ji

(15)

kj\i



new
ij

=

1 
X
  i

j
2 
 2 Rij
ij
+
jk + (1  )ji  
(1  )ij + ij



kj\i

(16)
i ,  j , h and R are parameters of  as in Section 2.1, with R = Q and the
where ij
i
ij
ij
ij
ij
ij
assumption that Qii = 1. The approximate marginals qij in (10) might not be normalizable,
but the message passing iteration in (15) and (16) stays well defined unless there is a zero
in the denominator on the rhs. This rarely happens in practice. However, it is more
common that message passing converges while there are some intermediate steps at which
the approximate marginals qij are not normalizable. This can often be remedied by choosing
an appropriate damping parameter .
The iteration (16) for the ij s is independent of ij s and the iteration (15) for the ij s
is linear in ij . It is interesting to see that when h = 0 neither the constrained Bethe
free energy (13) nor the message passing algorithm (16) depend on the sign of Rij . These
are only relevant to compute the meanswhen h 6= 0and the signs of the correlations
in (12). As a result, the marginal variances computed by either minimizing the Bethe free
energy or by running the message passing algorithm can only depend on |R|, similarly to
the constrained fractional free energy Fc .

4.1 Stability of the Gaussian Message Passing Algorithm
In the following we analyze the stability of the message passing iteration at its fixed points,
that is, at the stationary points of the Lagrangian corresponding to the constrained minimization of the Gaussian Bethe free energy. We reiterate that we use G = (V, E) to denote
the graph corresponding to Q, namely, V = {1, . . . , n} and E = {(i, j) : Qij 6= 0}. The vector   R|E| , corresponding to a set of messages {ij }ij , is composed by the concatenation
of ij s such that ij is followed by ji and the (ij, ji) blocks follow a lexicographic order w.r.t.
ij and i < j. The vector  consists of the variables ij and follows a similar structure as .
j
We define r, h,   R|E| as rij = rji = Rij , hij = hj and ij = ij
. We also define the
|E|  |E| matrix

1 if j = k

1   if kl = ji
Mij,kl () 

0 otherwise
which encodes the weighted edge adjacency corresponding to G and . The number of nonzero elements in M(), scales roughly with nnzeros (Q)2 /n, where nnzeros (Q) denotes
the number of non-zeros in Q. Since the parallel message update given Equations (15) and
(16) can be rewritten in terms of two matrix-vector multiplications and element by element
operations on vectors, the computational complexity of an update also scales as roughly
with nnzeros (Q)2 /n.
13

fiCseke & Heskes

With this notation, the local linearization of the update equations (15) and (16) can be
written as
 ( new , new )
(, ) = (1  )I . . .
 (, )






 h+M()
1
diag r +M() M() diag r (+M())2 M()

,


+ 
1

0
diag 2 r 2 (+M())
M()
2

(17)

where all operations on vectors are element by element. The stability of a fixed point
(  ,  ) depends on the union of the spectra of

J (  ,  )  1 diag r( + M() )1 M()
and

J (  ,  )  1 diag 2 r 2 ( + M() )2 M().
It is important to point out that the stability properties depend only on  and R and are
independent of   and h.
Our goal is to connect the stability properties of the message passing algorithm to the
properties of the Bethe free energy. Therefore, we express the stability properties in terms
of the moment parameters of approximate marginals. For any  that leads to normalizable approximate marginals qij (xi , xj ), we can use (10) to identify the local covariance
parameters Vij defined in Section 3, but now without enforcing the marginal matching
i = v i . The correspondence is given by
constraints vij
ik
"

i
vij
vij

vij
j
vij


=

#1

1

=

"

j
vij
vij

i vj  v2
vij
ij
ij
P
i +
il + (1  )ij
ij

vij
i
vij

#
(18)
Rij

li\j
j
ij

Rij

+

P

jk + (1  )ji



.

kj\i
i , v j and r
The approximate local covariances vij are fully determined by vij
ij and have
ij
the form as in (12). This leaves us with |E| moment parameters to be computed by the
i , v = v j and y (v) =
message passing algorithm. Let v  R|E| be defined as vij = vij
ji
ij
ij
i v j  v 2 ), where v
vij /(vij
ij
ij is computed according to (12). It can be checked that the
ij
mapping between y and v is continuous and bijective. This implies that the canonical to
moment parameter transformation in (18) can be written as y(v) =  + M(). Since
M() is singular only when  = K and the graph G is K-regularsee Property A1 in
Section A of the Appendix for detailsfor the rest of the cases, there is a continuous,
bijective mapping between the moment parameters v and the canonical parameters  that
lead to normalizable approximate marginals.
i = v i  v  for any
At any fixed point (  ,  ) we have moment matching, that is, vij
i
ik
k, j  i, therefore we can express the stability properties in terms of moment parameters

14

fiBethe Free Energies and Message Passing in Gaussian Models

v  = (vi , . . . , vn ). Using p
(18) and defining the diagonal matrix D  R|E||E| with the
diagonal elements Dij,ij = vi , we get

, v)
v
(,
v
ij
i
j 
= 1 diag  q
M()


vi vj


DJ ( (v  ))D 1

(19)

and
2





D J ( (v ))D

2

=

1

diag

vij (, vi , vj )2
vi vj

!
M().

(20)


Let (A) denote
the spectrum of the matrix A. Since we have  DJ D 1 =  (J ) and

 D 2 J D 2 =  (J ), it is sufficient to analyze the spectral properties of the right hand
sides in equations (19) and (20).
The message passing algorithm is asymptotically stable at  (v  ) if and only if
max { (J ( (v  ))) ,  (J ( (v  )))} < 1,

(21)

where () denotes the spectral radius. It is interesting to see that although the functional
forms of the free energies and the message passing algorithms are different in the Gaussian
and discrete case, the stability conditions have similar forms. This will allow us to use
some of the results of Watanabe and Fukumizu (2009). In the next section, we show the
implications of this condition for the properties of the Hessian of the free energy.
4.2 Stable Fixed Points and Local Minima
The Hessian H[F ] of the Bethe free energy (11) depends only on the moment parameters
vi , vj and vij . Note that now, the vij s are unconstrained parameters. It is an (|E|/2 + 2n)
(|E|/2 + 2n) matrix and it has the form


Q


 0
H[F ](V ) = 

0

diag
h 2

0 2

 F
 2 vij
iT

 h

 F
vij vi ij,i

0 i

 2 F
vij vi ij,i

h

i

 2 F
vi vj i,j




,


where we use V to denote the collection of parameters vi , i = 1, . . . , n and vij , i  j.
Since the block corresponding to the partial differentials w.r.t. vij is diagonal with positive
elements, the Hessian is positive definite at V if the Schur complement corresponding to
15

fiCseke & Heskes

the partial differentials w.r.t. vi s is positive definite at V . The latter is given by
X   2 F 2  F 1
 2 F
v

Hii [F ](V ) =
vi vi
vij vi
vij
ij


1 1 
1 X c4ij 
=
1
+
,
4
2 vi2

1

c
ij
ij

1
2
2
 F
 F  2 F  2 F
v
Hij [F ](V ) =

vi vj
vij vi vij vj  2 vij
1 1 1 c2ij
= 
,
2 vi vj  1  c4ij

where we use the notation cij = vij / vi vj .
Now, we would like to connect the condition in (21) to the positive definiteness of the
matrix H v [F ](V ). In the following we show that stable fixed points  (v  ) of the Gaussian
message passing algorithm, satisfying (21), correspond to local minima of the Gaussian free
energy F at v  and vij (, vi , vj ).
According to Watanabe and Fukumizu (2009), for any arbitrary vector w  R|E| one
has

Y
det I|E|  1 diag (w) M() = det In + 1 A(w)
(1  wij wji ),
(22)
ij

where
Aii (w) =

X
ij

wij wji
1  wij wji

and

Aij (w) = 

wij
.
1  wij wji

(23)

The proof is an application of the matrix determinant lemma and a reproduction of it can
be found in Section A of the Appendix. Equation (22) expresses the determinant of an
|E||E| matrix as the determinant of an nn matrix.

Let c  R|E| with cij (V ) = vij / vi vj . By substituting w = c(V )2 in (23), we find that


det I  1 diag c(V )2 M() = f (V ) det (H[F ](V )) ,
(24)
where f (V ) is a positive function defined as
f (V ) = 2n |E| |Q|1

Y
k

vk2


2
2
Y vi vj  vij
ij

2
vi vj + vij

2
vij
1
vi vj

!
.

for all V corresponding to normalizable approximate marginals. Now, adapting the theorem
of Watanabe and Fukumizu (2009)
 we have
 the following theorem.
Theorem If  1 diag c(V )2 M()  C \ R1 then the Hessian of the (Gaussian)
Bethe free energy H[F ] is positive definite at
 V. 
1
2
Proof: The assumption
   diag c(V ) M()  C \ R1 implies that we have
det I  1 diag(c(V )2 M()) > 0. By choosing Vij (t) = tvij with t  [0, 1], we find
2
that c(V(t))2 = t2 c(V )2 , therefore, det I  1 diag(c(V (t) )M()) > 0 for any t  [0, 1].
16

fiBethe Free Energies and Message Passing in Gaussian Models

This implies that det (H[F ](V (t))) > 0 for any t  [0, 1]. Since H[F ](V (0)) = I > 0
and the eigenvalues of H[F ](V (t)) change continuously w.r.t. t  [0, 1], it results that
H[F ](V (1)) > 0 for any V , thus satisfying the condition of the theorem.

 
 
The fixed point (  ,  ) is stable if and
 only if max{(J ( (v ))), (J ( (v )))} < 1.
1

2
This implies   diag(c(V ) )M()  C \ R1 and leads to the following property.

Property 3. Stable fixed points (  ,  ) of the damped Gaussian message passing algorithm (16) are local minima of the Gaussian Bethe free energy Fc in (13) at v  ( ).
The above shows that the boundedness of F or the existence of local minima in case
of an unbounded F plays a significant role in the convergence of Gaussian message passing. We illustrate this in Section 5. If the fractional message passing algorithm converges
then it converges to a set of messages that corresponds to a local minimum of the fractional free energy. This also implies that the mean parameters of the local approximate
marginals are exact (see Property 1. in Section 3). Note that the observations in Section 3
and Property A2 in the Appendix together with Property 3 imply that there is always a
range of  values for which the fractional free energy possesses a local minimum to which
the fractional message passing can converge.
4.3 The Damping and the Fractional Parameters
The local stability condition in (21) is independent of the damping parameter . Therefore,
it does not alter the local stability properties, it only makes the iteration slower and numerically more stable, that is, it can dampen the possible periodic trajectories of the message
passing algorithm.
The fractional parameter  characterizes the inference process and as we have seen in the
example in the previous sections, by choosing smaller s we can create local minima. In the
particular case when h = 0, there is a somewhat similar property for the message passing
updates as well. Let   R|E| be the set of messages  that lead to normalizable approximate
marginals. The set  is characterized by the model parameters |R|,  and . We reiterate
i and v j and there is a continuous bijective
that the elements of v are the local variances vij
ij
|E|

mapping between    and v  R+ given by y(v) =  + M(), unless  = K and G is
K-regular. This allows us to study q
the stability properties in terms of moment parameters
i , v j )/
v(). Let c(v, ) = [vij (, vij
ij

i v j ] be the vector of local correlations. By using
vij
ij ij

Gershgorins theorem (Horn & Johnson, 2005) and c(v, )2  c(v, ), we find that for any
eigenvalue  of 1 diag(c(v, ))M() or 1 diag(c(v, ))2 M() we have


||  max 1 c(v, ) [(nj  1) + |1  |] .
i,j

When h = 0, there are no updates in , the rhs of the above equation depends on
1 c(v, )2 (see Equations (17) and (20)) and we have lim 1 c(v, )2 = 0, thus, small
0

 values can help to achieve convergence. However, when h 6= 0 the term 1 c(v, ) is
dominating and the effects of decreasing  towards zero can be ambiguous.
17

fiCseke & Heskes

5. Experiments
We implemented both direct minimization and fractional message passing and analyzed
their behavior for different values of max (|R|). For reasons of simplicity, we set all ij s
equal. The results on an small scale model are summarized in Figure 3. Note that there
is a good correspondence between the behavior of the fractional Bethe free energies in the
direction of the eigenvalue corresponding to max (|R|) and the convergence of the Newton
method. The Newton method was started from different initial points. We experienced
that when max (|R|) > 1 and setting the initial value to v0 = t2 u2max , the algorithm
did not converge for high values of t. This can be explained by the top plots in Figure 3:
for high values of t, the initial point might not be in the convergence region of the local
minimum. For the fractional message passing algorithm we used two types of initialization:
i =
(1) when max (|R|) < 1 we set ij such that they are all normalizable by setting ij
i = 1/n ,
|Rij |ujmax /max uimax (Malioutov et al., 2006), (2) when max (|R|)  1, we used ij
i
that is, a symmetric partitioning of the diagonal elements. We set the initial messages such
that all approximate marginals are normalizable in the first step of the iteration.
We experienced a behavior similar to that described by Welling and Teh (2001) for
standard message passing, namely, fractional message passing and direct minimization either
both converge or both fail to converge. Our experiments in combination with Theorem 1
show that when max (|R|) > 1, standard message passing at best converges to a local
minimum of the Bethe free energy. If standard message passing fails to converge, one
can decrease  and search for a stationary pointpreferably a local minimumof the
corresponding fractional free energy.
It can be seen from the results in the right panels of Figure 2, that when the model is no
longer pairwise normalizable, the local minimum and not the unbounded global minimum
can be viewed the natural continuation of the (bounded) global minimum for pairwise
normalizable models. This explains why the quality of the approximation at the local
minimum for models that are not pairwise normalizable is still comparable to that at the
global minimum for models that are pairwise normalizable.

6. Conclusions
 T

As we have seen, FMF and FMF  21 v |R| v provide tight upper and lower bounds for
the Gaussian fractional Bethe free energies. It turns out that pairwise normalizability is
not only a sufficient condition for the message passing algorithm to converge, but it is also
a necessary condition for the Gaussian fractional Bethe free energies to be bounded from
below.
If the model is pairwise normalizable, then the lower bound is bounded, and both direct
minimization and message passing are converging. In our experiments both converged to
the same minimum. This suggests that in the pairwise normalizable case, fractional Bethe
free energies possess a unique global minimum.
If the model is not pairwise normalizable, then none of the fractional Bethe free energies
are bounded from below. However, there is always a range of  values for which the
fractional free energy possesses a local minimum to which both direct minimization and
fractional message passing can converge. Thus, by decreasing  towards zero, one gets
18

fiBethe Free Energies and Message Passing in Gaussian Models

&

!"

&

(

!"

)*+,(-.*/0
_(D(1"2"!3!""4
5*'6*(7_8!9
:;<*=(>;?,0

%

!"

%

!"

#

#

!"

!"

"

"

!"

!"



!"

(





"

!"

!"

!

#

!"
'

!"

$

!"

!"

Function value after convergence

Function value after convergence

6
5
4
3
2
1
0

"

!"

!

!"
'

#

$

!"

!"

2

0

10

10


7
6
5
4
3
2
1
0
1

2

10

Error in variances after convergence

Newton method
Message passing
1

10

0

10

1

10



8

7

1

(

!"

8

Error in variances after convergence

(

)*+,(-.*/0
_(D(1"2"!3!""4
5*'6*(7_8!9
:;<*=(>;?,0

2

10

0

10

0

10

10


2

Newton method
Message passing
1

10

0

10

1

2

10

0

10


10

2

10

2

10

10


2

Figure 3: The top panels show the constrained
 fractional Bethe free energies of an Gaussian model

with 8 variables in the direction v = tumax , where umax is the eigenvector corresponding to max (|R|) for max (|R|) = 0.9 (top-left) and max (|R|) = 1.1 (top-right). The
thick lines are the functions FMF (dashed), FB (dashed dotted) and the lower bound
 T

FMF  12 v |R| v (continuous). The thin lines are the constrained -fractional free
energies Fc for   [102 , 102 ]. Center panels show the final function values after the
convergence of the Newton method. The bottom panels 
show the ||  ||2 error in approximation for the single node standard deviations  = v. Missing values indicate
non-convergence.
19

fiCseke & Heskes

closer to the mean field energy and a finite local minimum will appear (Property A2 in
the Appendix). We experienced that for a suitable range of s,s and initial values the
fractional Gaussian message passing can be made to converge.
As mentioned in Section 2.1, ij s correspond to using local ij divergences when applying power expectation propagation with a fully factorized approximating distribution.
Seeger (2008) reports that when expectation propagation does not converge, applying power
expectation propagation with  < 1 helps to achieve convergence. In the case of the problem
addressed in this paper this behavior can be explained by the observation that small s
make a finite local minima more likely to occur and thus prevents the covariance matrices
from becoming indefinite or even non positive definite. Although the most common reason
for using  < 1 in EP is numerical robustness, it also implies finding the saddle point of the
-fractional EP free energy. It might be interesting to investigate whether it is the same
reason why convergence is more likely as in the case of Gaussian fractional message passing.
Wainwright et al. (2003) propose to convexify the Bethe free energy for discrete models
by choosing ij s sufficiently large such that the fractional Bethe free energy has a unique
global minimum. This strategy appears to fail for Gaussian models. Convexification makes
the possibly useful finite local minima disappear, leaving just the unbounded global minimum. In the case of the more general hybrid models, the use of the convexification is still
unclear.
The example in Section 3 disproves the conjecture in the work of Welling and Teh (2001):
even when the Bethe free energy is not bounded from below, it can possess a finite local
minimum to which the message passing and the minimization algorithms can converge.
We have shown that stable fixed points of the Gaussian fractional message passing
algorithms are local minima of the fractional Bethe free energy. Although the existence
of a local minimum does not guarantee the convergence of the message passing algorithm,
in practice we experienced that the existence of a local minimum implies convergence.
Based on these results, we hypothesize that when pairwise normalizability does not hold,
the Gaussian Bethe free energy and the Gaussian message passing algorithm ( = 1) can
have two types of behavior:
(1) the Gaussian Bethe free energy possesses a unique finite local minimum to which
optimization methods can converge by starting from, say, the mean field solution
vi = 1/Qii ; the Gaussian message passing has a corresponding unique stable fixed
point, to which it can converge with suitable starting point and sufficient damping,
(2) no finite local minimum exists, and thus, both the optimization and the message
passing algorithm diverge.
By using the fractional free energy and the fractional message passing and by varying ,
one can switch between these behaviors. Computing the critical c (|R|) for a general |R|
remains an open question. We believe that the properties of the free energies in K-regular
symmetric models (Section 3), where the critical values can be easily computed, give a good
insight into the properties of the free energies for general Gaussian models.
20

fiBethe Free Energies and Message Passing in Gaussian Models

Acknowledgments
We would like to thank Jason K. Johnson for sharing his ideas about the properties of the
message passing algorithm in K-regular models. We would also like to thank the anonymous
reviewers for their valuable comments on earlier versions of the manuscript. The research
reported in this paper was supported by VICI grant 639.023.604 from the Netherlands
Organization for Scientific Research (NWO).

Appendix A. Properties and Proofs
Lemma A1. (Watanabe & Fukumizu, 2009) For any graph G = (V, E), edge adjacency
matrix M() (defined in Section 4.1), and arbitrary vector w  R|E| , one has

Y
det I|E|  1 diag (w) M() = det I|V | + 1 A(w)
(1  wij wji ),
ij

where
Aii (w) =

X
ij

wij wji
1  wij wji

Aij (w) = 

and

wij
.
1  wij wji

Proof: We reproduce the proof in a somewhat simplified form. Let us define Uij, = eTj ,
Vij, = eTi where ek is the k th unit vector of Rn and S with


 
Sij,ij Sij,ji
0 1
,
=
Sji,ij Sji,ji
1 0
then we have M() = U V T  S. Let us define W  R|E||E| a diagonal matrix with
wij,ij = wij . Using the matrix determinant lemma this reads as

det I  1 W U V T  S

= det I + W S  1 W U V T



= det I  1 W U V T (I + W S)1 det (I + W S)


= det I  1 V T (I + W S)1 W U det (I + W S).
The (ij, ji) block of (I + W S)1 W is


1
1
wij
wij
1
0
1  wji wji wji

0
wji


=

1
1  wji wji



wij
wji wij

wij wji
wji



and thus, we can define A  V T (I + W S)1 W U such that
X wij wji
wij
Ai,i =
and Ai,j = 
.
1  wij wji
1  wij wji
ij

This completes the proof of the matrix determinant lemma (22) in Section 4.2.

21



fiCseke & Heskes

Property A1. The matrix M() = U V T  S is singular only for K-regular graphs
with  = K.
P
Proof: Let x  R|E| and y =PM()x. Then yij = kj xjk  xji . Let us fix j, then
yij = 0 for any i means that kj xjk = xji for any i. This can only hold if the graph is
K-regular,  = K and all xij s are equal or xij = 0 for all pair indices ij.

Property A2. For a suitably chosen  > 0, there exists an  such that the constrained
fractional free energy Fc possesses a local minimum for all 0 <  <  .

Proof: Let us define vM
F = argminv FM F (v) and


UM
F = {v : FM F (v)  FM F (vM F ) + 2} .

The form of FM F implies that we can always choose  such that UM
F is a proper subset of the
n

n
positive quadrant in R , in other words, UM F  R+ . Then due to the properties of FM F
(continuous and convex, with a unique finite global minimum attained at a finite value), the





domain UM
F is closed, bounded, convex and vM F  UM F \ UM F , that is, vM F is in the

n
c

interior of UM F . Since FM F and F (v) are continuous on R+ , the set UM F is closed and
bounded and lim Fc (v) = FM F (v) (pointwise convergence) for all v  Rn+ , it follows that Fc
0


c
converges uniformly on UM
F as   0. This, together with the monotonicity of F w.r.t. ,
implies that there exists  such that FM F (vM F )   < Fc (vM F ) < FM F (vM F ) for all 0 <
 . Let us fix . It is known that, since U 
 <  and all v  UM
F
M F is closed and bounded and
 ) + 2 for all

c
c
F is continuous, F attains its extrema on UM F . Since FM F (v) = FM F (vM
F
 )+
c

c

v  UM F and F (v) > FM F (v)   for all v  UM F it follows that F (v) > FM F (vM
F


c

 ).
for all v  UM F . We have chosen  such that FM F (vM F )   < F (vM F ) < FM F (vM
F
The latter two conditions imply that one of the extrema has to be a local minimum in the
 .
interior of UM

F

References
Bickson, D. (2009). Gaussian Belief Propagation: Theory and Application. Ph.D. thesis,
The Hebrew University of Jerusalem.
Cseke, B., & Heskes, T. (2008). Bounds on the Bethe free energy for Gaussian networks.
In McAllester, D. A., & Myllymaki, P. (Eds.), UAI 2008, Proceedings of the 24th
Conference in Uncertainty in Artificial Intelligence, pp. 97104. AUAI Press.
Heskes, T. (2003). Stable fixed points of loopy belief propagation are minima of the Bethe
free energy. In Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances in Neural
Information Processing Systems 15, pp. 359366, Cambridge, MA. The MIT Press.
Heskes, T., Opper, M., Wiegerinck, W., Winther, O., & Zoeter, O. (2005). Approximate
inference techniques with expectation constraints. Journal of Statistical Mechanics:
Theory and Experiment, 2005, P11015.
Heskes, T. (2004). On the uniqueness of loopy belief propagation fixed points. Neural
Computation, 16, 23792413.
Horn, R. A., & Johnson, C. (2005). Matrix Analysis. Cambridge University Press, Cambridge, UK.
22

fiBethe Free Energies and Message Passing in Gaussian Models

Jaakkola, T. (2000). Tutorial on variational approximation methods. In Opper, M., & Saad,
D. (Eds.), Advanced mean field methods: theory and practice, pp. 129160, Cambridge,
MA. The MIT Press.
Johnson, J. K., Bickson, D., & Dolev, D. (2009). Fixing convergence of Gaussian belief
propagation. CoRR, abs/0901.4192.
Malioutov, D., Johnson, J., & Willsky, A. (2006). Walk-sums and belief propagation in
Gaussian graphical models. Journal of Machine Learning Research, 7, 20312064.
Minka, T. P. (2004). Power EP. Tech. rep., Microsoft Research Ltd., Cambridge, UK,
MSR-TR-2004-149.
Minka, T. P. (2005). Divergence measures and message passing. Tech. rep. MSR-TR-2005173, Microsoft Research Ltd., Cambridge, UK.
Moallemi, C., & Roy, B. V. (2006). Consensus propagation. In Weiss, Y., Scholkopf, B., &
Platt, J. (Eds.), Advances in Neural Information Processing Systems 18, pp. 899906.
MIT Press, Cambridge, MA.
Murphy, K., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation for approximate
inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, Vol. 9, pp. 467475, San Francisco, USA. Morgan
Kaufman.
Nishiyama, Y., & Watanabe, S. (2009). Accuracy of loopy belief propagation in Gaussian
models. Neural Networks, 22 (4), 385  394.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufman Publishers, San Mateo, CA.
Rusmevichientong, P., & Roy, B. V. (2001). An analysis of belief propagation on the turbo
decoding graph with Gaussian densities. IEEE Transactions on Information Theory,
47, 745765.
Seeger, M. W. (2008). Bayesian inference and optimal design for the sparse linear model.
Journal of Machine Learning Research, 9, 759813.
Takahashi, K., Fagan, J., & Chin, M.-S. (1973). Formation of a sparse impedance matrix
and its application to short circuit study. In Proceedings of the 8th PICA Conference.
Wainwright, M., Jaakkola, T., & Willsky, A. (2003). Tree-reweighted belief propagation
algorithms and approximate ML estimation via pseudo-moment matching. In Bishop,
C., & Frey, B. (Eds.), Proceedings of the Ninth International Workshop on Artificial
Intelligence and Statistics. Society for Artificial Intelligence and Statistics.
Watanabe, Y., & Fukumizu, K. (2009). Graph zeta function in the Bethe free energy and
loopy belief propagation. In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C.
K. I., & Culotta, A. (Eds.), Advances in Neural Information Processing Systems 22,
pp. 20172025. The MIT Press.
Weiss, Y., & Freeman, W. T. (2001). Correctness of belief propagation in Gaussian graphical
models of arbitrary topology. Neural Computation, 13 (10), 21732200.
23

fiCseke & Heskes

Welling, M., & Teh, Y. W. (2001). Belief optimization for binary networks: a stable alternative to loopy belief propagation. In Breese, J. S., & Koller, D. (Eds.), Proceedings
of the 17th Conference in Uncertainty in Artificial Intelligence, pp. 554561. Morgan
Kaufmann Publishers.
Wiegerinck, W., & Heskes, T. (2003). Fractional belief propagation. In Becker, S., Thrun,
S., & Obermayer, K. (Eds.), Advances in Neural Information Processing Systems 15,
pp. 438445, Cambridge, MA. The MIT Press.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2000). Generalized belief propagation. In
Advances in Neural Information Processing Systems 12, pp. 689695, Cambridge, MA.
The MIT Press.
Zoeter, O., & Heskes, T. (2005). Change point problems in linear dynamical systems.
Journal of Machine Learning Research, 6, 19992026.

24

fiJournal of Artificial Intelligence Research 41 (2011) 527-551

Submitted 03/11; published 08/11

Controlling Complexity in Part-of-Speech Induction
Joo V. Graa

JOAO . GRACA @ L 2 F. INESC - ID . PT

L2 F INESC-ID
Lisboa, Portugal

Kuzman Ganchev

KUZMAN @ GOOGLE . COM

Google Inc.
New York, NY, USA

Lusa Coheur

LUISA . COHEUR @ L 2 F. INESC - ID . PT

2

L F INESC-ID
Lisboa, Portugal

Fernando Pereira

PEREIRA @ GOOGLE . COM

Google Inc.
Mountain View, CA, USA

Ben Taskar

TASKAR @ CIS . UPENN . EDU

Computer & Information Science
University of Pennsylvania

Abstract
We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this
task performs poorly, because of its weak inductive bias and large model capacity. We address this
problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity,
adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare
words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our
experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve
significant improvements compared with previous methods for the same task.

1. Introduction
Part-of-speech (POS) categories are elementary building blocks for the syntactic analysis of text
that play an important role in many natural-language-processing tasks, from machine translation to
information extraction. While English and a handful of other languages are fortunate enough to
have comprehensive POS-annotated corpora such as the Penn Treebank (Marcus, Marcinkiewicz,
& Santorini, 1993), most of the worlds languages have extremely limited linguistic resources. It
is unrealistic to expect annotation efforts to catch up with the explosion of unlabeled electronic
text anytime soon. This lack of supervised data will likely persist in the near future because of the
investment required for accurate linguistic annotation: it took two years to annotate 4,000 sentences
with syntactic parse trees for the Chinese Treebank (Hwa, Resnik, Weinberg, Cabezas, & Kolak,
2005) and four to seven years to annotate 50,000 sentences across a range of languages (Abeill,
2003).

c
2011
AI Access Foundation. All rights reserved.

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Supervised learning of taggers from POS-annotated training text is a well-studied task, with
several methods achieving near-human tagging accuracy (Ratnaparkhi, 1996; Toutanova, Klein,
Manning, & Singer, 2003; Shen, Satta, & Joshi, 2007). However, POS induction  where one does
not have access to a labeled corpus  is a more difficult task with much room for improvement. In
recent literature, POS induction has been used to refer to two different tasks. For the first one, in
addition to raw text, we are given a dictionary containing the possible tags for each word and the
goal is to disambiguate the tags of a particular word occurrence (Merialdo, 1994). For the second
task, we are given raw text, but no dictionary is provided; the goal is to cluster words that have the
same grammatical behavior. In this work, we target this latter, more challenging, unsupervised POS
induction task.
Recent work on this task typically relies on distributional or morphological features, since words
with the same grammatical function tend to occur in similar contexts and to have common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Schtze, 1995; Clark, 2003). However, those
statistical regularities are not enough to overcome several challenges. First, the algorithm has to
decide how many clusters to use for broad syntactic categories (for instance, whether to distinguish
between plural and singular nouns). Second, category size distribution tends to be uneven. For example, the vast majority of the word types are open class (nouns, verbs, adjectives), and even among
open class categories, there are many more nouns than adjectives. This runs contrary to the learning
biases in commonly-used statistical models. A common failure of those models is to clump several
rare categories together and split common categories.
For individual word types, a third challenge arises from ambiguity in grammatical role and
word sense. Many words can take on different POS tags in different occurrences, depending on
the context of occurrence (the word run can be either a verb or a noun). Some approaches assume
(for computational and statistical simplicity) that each word can only have one tag, aggregating
all local contexts through distributional clustering (Schtze, 1995). While this one-tag-per-word
assumption is clearly wrong, across many languages for which we have annotated corpora, such
methods perform competitively with methods that can assign different tags to the same word in
different contexts (Lamar, Maron, Johnson, & Bienenstock, 2010). This is partly due to the typical
statistical dominance of one of the tags for a word, especially if the corpus includes a single genre,
such as news. The other reason is that less restrictive models do not encode the useful bias that most
words typically take on a very small number of tags.
Most approaches that do not make the one-tag-per-word assumption take the form of a hidden
Markov model (HMM) where the hidden states represent word classes and the observations are
word sequences (Brown et al., 1992; Johnson, 2007). Unfortunately, standard HMMs trained to
maximize likelihood perform poorly, since the learned hidden classes do not align well with true
POS tags. Besides the potential model estimation errors due to non-convex optimization involved
in training, there is a more pernicious problem. Typical maxima of likelihood do not align well with
maxima of POS tag accuracy (Smith & Eisner, 2005; Graa, Ganchev, Pereira, & Taskar, 2009),
suggesting serious mismatch between model and data.
In this work, we significantly reduce that modeling mismatch by combining three ideas:
 The standard HMM treats words as atomic units, without using orthographic and morphological information. That information is critical to generalization in many languages (Clark,
2003). To address this problem, we reparameterize the standard HMM by replacing the multinomial emission distributions by maximum-entropy models (similar to the work of Berg528

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

Kirkpatrick, Bouchard-Ct, DeNero, & Klein, 2010 and Graa, 2010). This allows the use
of orthographic and morphological features in the emission model. Moreover, the standard
HMM model has a very large number of parameters: the number of tags times the number of
word types. This presents an extremely rich model space capable of fitting irrelevant correlations in the data. To address this problem we dramatically reduce the number of parameters
of the model by discarding features with small support in the corpus, that is, those involving
rare words or word parts.
 The HMM model allows a high level of ambiguity for the tags of each word. As a result, when
maximizing the marginal likelihood, common words typically tend to be associated with every
tag with some non-trivial probability (Johnson, 2007). However, a natural property of POS
categories across many languages and annotation standards is that each word only has a small
number of allowed tags. To address this problem we use the posterior regularization (PR)
framework (Graa, Ganchev, & Taskar, 2007; Ganchev, Graa, Gillenwater, & Taskar, 2010)
to constrain the ambiguity of word-tag associations via a sparsity-inducing penalty on the
model posteriors (Graa et al., 2009).
We show that each of the proposed extensions improves the standard HMM performance, and
moreover, that the gains are nearly additive. The improvements are significant across different
metrics previously proposed for this task. For instance, for the 1-Many metric, our method attains
a 10.4% average improvement over the regular HMM. We also compare the proposed method with
eleven previously proposed approaches. For all languages but English and all metrics except 1-1,
our method achieves the best published results. Furthermore, our method appears the most stable
across different testing scenarios and always shows competitive results. Finally, we show how
the induced tags can be used to improve the performance of a supervised POS tagging system in
a limited labeled data scenario. Our open-source software for POS induction and evaluation is
available at http://code.google.com/p/pr-toolkit/.
This paper is organized as follows. Section 2 describes the basic HMM for POS induction and its
maximum-entropy extension. Section 3 describes standard EM and our sparsity-inducing estimation
method. Section 4 presents a comprehensive survey of previous fully unsupervised POS induction
methods. In Section 5 we provide a detailed experimental evaluation of our method. Finally, in
Section 6, we summarize our results and suggest ideas for future work.

2. Models
The model for all our experiments is based on a first order HMM. We denote the sequence of
words in a sentence as boldface x and the sequence of hidden states which correspond to partof-speech tags as boldface y. For a sentence of length l, we have thus l hidden state variables
yi  {1, . . . , J}, 1  i  l where J is the number of possible POS tags, and l observation variables
xi  {1, . . . , V }, 1  i  l, where V is the number of word types. To simplify notation, we assume
that every tag sequence is prefixed with the conventional start tag y0 = start, allowing us to write
as p(y1 |y0 ) the initial state probability of the HMM.
The probability of a sentence x along with a particular hidden state sequence y is given by:
p(x, y) =

l
Y

pt (yi | yi1 )po (xi | yi ),

i=1

529

(1)

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

where po (xi | yi ) is the probability of observing word xi given that we are in state yi (emission
probability), and pt (yi | yi1 ) is the probability of being in state yi , given that the previous hidden
state was yi1 (transition probability).
2.1 Multinomial Emission Model
Standard HMMs use multinomial emission and transition probabilities. That is, for a generic word
xi and tag yi , the observation probability po (xi | yi ) and the transition probability pt (yi | yi1 ) are
multinomial distributions. In the experiments we refer to this model simply as the HMM. This model
has a very large number of parameters because of the large number of word types (see Table 1). A
common convention we follow is to lowercase words as well as to map words occurring only once
in the corpus to a special token unk.
2.2 Maximum Entropy Emission Model
In this work, we use a simple modification of the HMM model discussed in the previous section:
we represent conditional probability distributions as maximum entropy (log-linear) models. Specifically, the emission probability is expressed as:
exp(  f (x, y))
0
x0 exp(  f (x , y))

po (x|y) = P

(2)

where f (x, y) is a feature function, x ranges over all word types, and  are the model parameters. We
will refer to this model as HMM+ME. In addition to word identity, features include orthographyand morphology-inspired cues such as presence of capitalization, digits, and common suffixes. The
feature sets are described in Section 5. The idea of replacing the multinomial models of an HMM
by maximum entropy models is not new and has been applied before in different domains (Chen,
2003), as well as in POS induction (Berg-Kirkpatrick et al., 2010; Graa, 2010). A key advantage
of this representation is that it allows for a much tighter control over the expressiveness of the
model. For many languages it is helpful to exclude word identity features for rare words in order
to constrain the model and force generalization across words with similar features. Unlike mapping
all rare words to the unk token in the multinomial setting, the maxent model still captures some
information about the word through the other features. Moreover, we can reduce the number of
parameters even further by using lowercase word identities while still keeping the case information
by using a case feature. Table 1 shows the number of features we used for different corpora. Note
that the reduced feature set has an order of magnitude fewer parameters than the multinomial model.

3. Learning
In Section 5 we describe experiments comparing the HMM model to the ME model under three
learning scenarios: maximum likelihood training using the EM algorithm (Dempster, Laird, & Rubin, 1977) for both HMM and HMM+ME, gradient-based likelihood optimization for the HMM+ME
model, and PR with sparsity constraints (Graa et al., 2009) for both HMM and HMM+ME. This
section describes all three learning algorithms.
In the following, we denote the whole corpus, a list of sentences, by X = (x1 , x2 , . . . , xN ) and
the corresponding tag sequences by Y = (y1 , y2 , . . . , yN ).

530

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

3.1 Maximum Likelihood with EM
Standard HMM training seeks model parameters  that maximize the log-likelihood of the observed
data:
X
p (X, Y)
(3)
Log-Likelihood: L() = log
Y

where X is the whole corpus. Since the model assumes independence between sentences ,
log

X

p (X, Y) =

N
X

log

X

(4)

yn

n=1

Y

p (xn , yn ),

but we use the corpus notation for consistency with Section 3.3. Because of the latent variables
Y, the log-likelihood function for the HMM model is not convex in the model parameters, and the
model is fitted using the EM algorithm. EM maximizes L() via block-coordinate ascent on a lower
bound F (q, ) using an auxiliary distribution over the latent variables q(Y) (Neal & Hinton, 1998).
By Jensens inequality, we define a lower-bound F (q, ) as:
L() = log

X

q(Y)

Y

p (X, Y) X
p (X, Y)

q(Y) log
= F (q, ).
q(Y)
q(Y)

(5)

Y

We can rewrite F (q, ) as:
F (q, ) =

X

q(Y) log(p (X)p (Y|X)) 

Y

X

q(Y) log q(Y)

(6)

Y

q(Y)
q(Y) log
p (Y|X)

(7)

= L()  KL(q(Y)||p (Y|X)).

(8)

= L() 

X
Y

Using this interpretation, we can view EM as performing coordinate ascent on F (q, ). Starting
from an initial parameter estimate 0 , the algorithm iterates two block-coordinate ascent steps until
a convergence criterion is reached:
E : q t+1 = arg max F (q, t ) = arg min KL(q(Y) k pt (Y | X))
q

(9)

q

M : t+1 = arg max F (q t+1 , ) = arg max Eqt+1 [log p (X, Y)]


(10)



The E-step corresponds to maximizing Eq. 8 with respect to q and the M-step corresponds
to maximizing Eq. 6 with respect to . The EM algorithm is guaranteed to converge to a local
maximum of L() under mild conditions (Neal & Hinton, 1998). For an HMM POS tagger, the
E-Step computes the posteriors pt (y|x) over the latent variables (POS tags) given the observed
variables (words) and current parameters t for each sentence. This is accomplished by the forwardbackward algorithm for HMMs. The EM algorithm together with the forward-backward algorithm
for HMMs is usually referred to as the BaumWelch algorithm (Baum, Petrie, Soules, & Weiss,
1970).
The M step uses q t+1 (qnt+1 are the posteriors for a given sentence) to fill in the values of tags
Y and estimate parameters t+1 . Since the HMM model is locally normalized and the features used
531

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

only depend on the tag and word identities and not on the particular position where they occur, the
optimization decouples in the following way:
Eqt+1 [log p (X, Y)] =
=

N
X

Eqnt+1 [log

n=1
ln 
N X
X

ln
Y

n
pt (yin | yi1
)po (xni | yin )]

(11)

i=1
n
Eqnt+1 log pt (yin | yi1
) + Eqnt+1 log po (xni | yin )



(12)

n=1 i=1

For the multinomial emission model, this optimization is particularly easy and simply involves
normalizing (expected) counts for each parameter. For the maximum-entropy emission model parameterized as in Equation 2, there is no closed form solution so we need to solve an unconstrained
optimization problem. For each possible hidden tag value y we have to solve two problems: estimate the emission probabilities po (x|y) and estimate the transition probabilities pt (y 0 |y), where the
gradient for each one of those is given by


Eqt+1 [log p (X, Y)]
= Eqt+1 f (X, Y)  Ep (X0 |Y) [f (X0 , Y)] ,
(13)

which is similar to the gradient in supervised ME models, except for the expectation over all Y
under q t+1 (Y) instead of observed Y. The optimization is done using L-BFGS with Wolfes rule
line search (Nocedal & Wright, 1999).
3.2 Maximum Likelihood with Direct Gradient
While likelihood is traditionally optimized with EM, Berg-Kirkpatrick et al. (2010) find that for
the HMM with the maximum entropy emission model, higher likelihood and better accuracy can
be achieved by with a gradient-based likelihood-optimization method. They use L-BFGS in their
experiments. The derivative of the likelihood is,
L()



1

1
 X
log p (X) =
p (X) =
p (X, Y)

p (X) 
p (X) 
Y
X 1
X p (X, Y) 

=
p (X, Y) =
log p (X, Y)
p (X) 
p (X) 
Y
Y
X

=
p (Y|X) log p (X, Y),

=

(14)
(15)
(16)

Y

which is exactly the same as the derivative of the M-Step. Here in Equation 14 we apply the chain
rule to take the derivative of log p (X), while in Equation 15 we apply the chain rule in the reverse
direction. The biggest difference between the EM procedure and direct gradient is that for EM
we fix the counts on the E-Step and optimize the ME model using those counts. When directly
optimizing the likelihood we need to recompute the counts for each parameter setting, which can be
expensive. Appendix A gives a more detailed discussion of both methods.
3.3 Controlling Tag Ambiguity with PR
One problem with unsupervised HMM POS tagging is that the maximum likelihood objective may
encourage tag distributions that allow many different tags for a word in a given context. We do not
532

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

10

6

Supervised
HMM
HMM+ME
HMM+Sp
HMM+ME+Sp

8
L1L

8
L1L

10

Supervised
HMM
HMM+ME
HMM+Sp
HMM+ME+Sp

4
2

6
4
2

0

0
0

1000 2000 3000 4000 5000 6000 7000 8000

0

200 400 600 800 1000 1200 1400 1600 1800

rank of word by L1L

rank of word by L1L

Figure 1: Our ambiguity measure (`1 /` ) for each word type in two corpora for a supervised
model, EM training (HMM, HMM+ME), and when we train with an ambiguity penalty
as described in Section 3.3 (HMM+Sp, HMM+ME+Sp). Left:En, Right:Pt.

find that in actual text with linguist-designed tags, because tags are designed to be informative about
the words grammatical role. In the following paragraphs we describe a measure of tag ambiguity
proposed by Graa et al. (2009) that we will attempt to control. It is easier to understand this measure
with hard tag assignments, so we start with that and thene extend the discussion to distributions over
tags.
Consider a word such as stock. Intuitively, we would like all occurrences of stock to be
tagged with a small subset of all possible tags (noun and verb, in this case). For a hard assignment of
tags to the entire corpus, Y, we could count how many different tags are used in Y for occurrences
of the word stock.
If instead of a single tagging of the corpus, we have a distribution q(Y) over assignments, we
need to generalize this ambiguity measure. Instead of asking was a particular tag ever used for
the word stock, we would ask what is the maximum probability with which a particular tag was
used for the word stock. Then instead of counting the number of tags, we would sum these
probabilities.
As motivation, Figure 1 shows the distribution of tag ambiguity across words for two corpora.
As we see from Figure 1, when we train using the EM procedure described in Section 3.1, the
HMM and ME models grossly overestimates the tag ambiguity of almost all words. However
when the same models are trained using PR to penalize the tag ambiguity, both models (HMM+Sp,
HMM+ME+Sp) achieve a tag ambiguity closer to the truth.
More formally, Graa et al. (2009) define this measure in terms of constraint features (X, Y).
Constraint feature wvj (X, Y) takes on value 1 if the j th occurrence of word type w in X is assigned
to tag v in the tag assignment Y. Consequently, the probability that the j th occurrence of word w
has tag v under the label distribution q(Y) is Eq [wvj (X, Y)]. The ambiguity measurement for
word type w becomes:
X
Ambiguity Penalty for word type w:
max Eq(Y) [wvj (X, Y)] .
(17)
v

j

This sum of maxima is also called the `1 /` mixed norm. For brevity we use the norm notation ||Eq [w ]||1/ . For computational reasons, we do not add a penalty term based on the ambiguity of the model distribution p (Y|X), but instead introduce an auxiliary distribution q(Y) which
533

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

must be close to p but also must have low ambiguity. Our modified objective becomes
X
||Eq [w (X, Y)]||1/ .
max L()  KL(q(Y)||p (Y|X))  
,q

(18)

w

Graa et al. (2009) optimize this objective using an algorithm very similar to EM. The added complexity of implementing their algorithm lies only in computing the Kullback-Leibler projection in
a modified E-Step. However, this computation involves choosing a distribution over exponentially
many objects (label assignments). Luckily, Graa et al. (2009) show that the dual formulation for
the E-Step is more manageable. This is given by:
!
X
X
max  log
p (Y|X) exp(  (X, Y))
s. t.
wvj  
(19)
0

j

Y

where  is the vector of dual parameters wvj , one for each wvj . The projected distribution is then
given by: q(Y)  p (Y|X) exp (  (X, Y)). Note that when p is given by an HMM, q for
each sentence can be expressed as
q(yn ) 

I
Y

n
pt (yin | yi1
)qo (xni | yin ),

(20)

i=1

where qo (xi |yi ) = po (xi |yi ) exp(xi yi j ) act as modified (unnormalized) emission probabilities.
The objective of Equation 19 is just the negative sum of the log probabilities of all the sentences
under q plus a constant. We can compute this by running forward-backward on the corpus, similar
to the E-Step in normal EM. The gradient of the objective is also computed using the forwardbackward algorithm. Note that the objective in Eq. 19 is concave with respect to  and can be
optimized using a variety of methods. We perform the dual optimization by projected gradient,
using the fast simplex projection algorithm for  described by Bertsekas, Homer, Logan, and Patek
(1995). In our experiments we found that taking a few projected gradient steps was not enough, and
performing the optimization until convergence helps the results.

4. Related Work
POS tags place words into classes that share some commonalities as to what other (classes of) words
they cooccur with. Therefore, it is natural to ask whether word clustering methods based on word
context distributions might be able to recover the word classification inherent in a POS tag set.
Several influential methods, most notably mutual-information clustering (Brown et al., 1992), have
been used to cluster words according to how their immediately contiguous words are distributed.
Although those methods were not explicitly designed for POS induction, the resulting clusters capture some syntactic information (see also Martin, Liermann, & Ney, 1998, for a different method
with a similar objective). Clark (2003) refined the distributional clustering approach by adding
morphological and word frequency information, to obtain clusters that more closely resemble POS
tags.
Other forms of distributional clustering go beyond the immediate neighbors of a word to represent a whole vector of coocurrences with the target word within a text window, and compare those
vectors using some suitable metric, such as cosine similarity. However, these wider-range similarities have problems in capturing more local regularities. For instance, an adjective and a noun might
534

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

look similar if the noun tends to be used in noun-noun compounds; similarly, two adjectives with
different semantics or selectional preferences might be used with different contexts. Moreover, this
problem is aggravated with data sparsity. As an example, infrequent adjectives that modify different nouns tend to have completely disjoint context vectors (but even frequent words like a and
an might have completely different context vectors, since these articles are used in disjoint right
contexts). To alleviate these problems, Schtze (1995) used frequency cutoffs, singular-value decomposition of co-occurrence matrices, and approximate co-clustering through two stages of SVD,
with the clusters from the first stage used instead of individual words to provide vector representations for the second-stage clustering.
Lamar, Maron and Johnson (2010) have recently revised the two-stage SVD model of Schtze
(1995) and achieve close to state-of-the-art performance. The revisions are relatively small, but
touch several important aspects of the model: singular vectors are scaled by their singular values
to preserve the geometry of the original space; latent descriptors are normalized to unit length; and
cluster centroids are computed as a weighted average of their constituent vectors based on the word
frequency, so that rare and common words are treated differently and centroids are initialized in a
deterministic manner.
A final class of approaches  which include the work in this paper  uses a sequence model,
such as an HMM, to represent the probabilistic dependencies between consecutive tags. In these
approaches, each observation corresponds to a particular word and each hidden state corresponds
to a cluster. However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood
training for such models does not achieve good results: maximum likelihood training tends to result
in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004)
clusters the most frequent words using a distributional approach and co-clustering. To cluster the
remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the
remaining words are uniform.
Several studies propose using Bayesian inference with an improper Dirichlet prior to favor
sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge
(2010) by representing explicitly the different ambiguity patterns of function and content words.
Lee, Haghighi, and Barzilay (2010) take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type. Their model first generates a tag
dictionary that assigns mass to only one tag for each word type to reflect lexicon sparsity. This dictionary is then used to constrain a Dirichlet prior from which the emission probabilities are drawn
by only having support for word-tag pairs in the dictionary. Then a token-level HMM using those
emission parameters and transition parameters draw from a symmetric Dirichlet prior are used for
tagging the entire corpus. The authors also show improvements by using morphological features
when creating the dictionary. Their system achieves state-of-art results for several languages. It
should be noted that a common issue with the above sparsity-inducing approaches is that sparsity
is imposed at the parameter level, the probability of word given tag, while the desired sparsity is
at the posterior level, the probability of tag given word. Graa et al. (2009) use the PR framework
to penalize ambiguous posteriors distributions of words given tokens, which achieves better results
than the Bayesian sparsifying Dirichlet priors.

535

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Most recently, Berg-Kirkpatrick et al. (2010) and Graa (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick
et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English.
We also note briefly POS induction methods that rely on a prior tag dictionary indicating for
each word type what POS tags it can have. The POS induction task is then, for each word token in
the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much
of the later work tries to reduce the required dictionary size in different ways, by generalizing from
a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006;
Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although this approach greatly
simplifies the problem  most words can only have one tag and, furthermore, the cluster-tag mappings are predetermined, thus removing an extra level of ambiguity  the accuracy of such methods
is still significantly behind supervised methods. To address the remaining ambiguity by imposing
additional sparsity, Ravi and Knight (2009) minimize the number of possible tag-tag transitions in
the HMM via a integer program. Finally, Snyder, Naseem, Eisenstein, and Barzilay (2008) jointly
train a POS induction system over parallel corpora in several languages, exploiting the fact that
different languages present different ambiguities.

5. Experiments
In this section we present encouraging results validating the proposed method in six different testing
scenarios according to different metrics. The highlights are:
 A maximum-entropy emission model with a Markov transition model trained with the ambiguity penalty improves over the regular HMM in all cases with an average improvement of
10.4% (according to the 1-Many metric).
 When compared against a broad range of recent POS induction systems, our method produces
the best results for all languages except English. Furthermore, the method seems less sensitive
to particular test conditions than previous methods.
 The induced clusters are useful features in training supervised POS taggers, improving test
accuracy as much or more than the clusters learned by competing methods.
5.1 Corpora
In our experiments we test several POS induction methods on five languages with the help of manually POS-tagged corpora for those languages. Table 1 summarizes characteristics of the test corpora:
the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) (we consider both the
17-tag version of Smith & Eisner, 2005 (En17) and the 45-tag version (En45)); the Bosque subset
of the Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the
Bulgarian BulTreeBank (Simov et al., 2002) (Bg) (with only the 12 coarse tags); the Spanish corpus from the Cast3LB treebank (Civit & Mart, 2004) (Es); and the Danish Dependency Treebank
(DDT) (Kromann, Matthias T., 2003) (Dk).

536

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

En
Pt
Bg
Es
Dk

1
Sentences
49208
9359
14187
3512
5620

2
Types
49206
29489
34928
16858
19400

3
LUnk
49.28%
37.83%
39.26%
37.73%
26.30%

4
Tokens
1173766
212545
214930
95028
65057

5
Tags
17 (45)
22
12
47
30

6
Avg. `1 /`
1.08 (1.11)
1.02
1.02
1.05
1.05

7
Total `1 /`
6523.6 (6746.2)
1144.6
3252.9
818.9
795.8

8
|w|1
54334
33293
38633
18962
21678

9
|w|2
7856
2114
2287
951
969

Table 1: Corpus statistics. The third column shows the percentage of word types after lower-casing
and eliminating word types occurring only once. The sixth and seventh columns show
information about the word ambiguity in each corpus on average and in totality (corresponding to the penalty in Equation 17). The eighth and ninth columns show the number
of parameters for the different feature sets, as described in Section 5.3.

5.2 Experimental Setup
We compare our work with two kinds of methods: those that induce a single cluster for each word
type (type-level tagging), and those that allow different tags on different occurrences of a word type
(token-level tagging). For type-level tagging, we use two standard baselines, B ROWN and C LARK,
as described by Brown et al. (1992)1 and Clark (2003)2 . Following Headden, McClosky, and Charniak (2008), we trained the C LARK system with both 5 and 10 hidden states for the letter HMM and
ran it for 10 iterations; the B ROWN system was run according with the instructions accompanying
the code. We also ran the recently proposed LDC system (Lamar, Maron, & Bienenstock, 2010)3 ,
with the configuration described in their paper for PTB45 and PTB17, and the PTB17 configuration
for the other corpora. It should be noted that we did not carry out our experiments with the SVD2
system (Lamar, Maron and Johnson, 2010), since SVD2 is superseded by LDC according to its
authors.
For token-level tagging, we experimented with the feature-rich HMM as presented by BergKirkpatrick et al. (2010), trained both using EM training (BK+EM) and direct gradient (BK+DG),
using the configuration provided by the authors4 . We report results from the type-level HMM
(TLHMM) (Lee et al., 2010) when applicable, since we were not able to run that system. Moreover,
we compared those systems against our own implementation of various HMM-based approaches:
the HMM with a multinomial emission probabilities (Section 2.1), the HMM with maximumentropy emission probabilities (Section 2.2) trained with EM (HMM+ME), trained by direct gradient (HMM+ME+DG), and trained using PR with the ambiguity penalty, as described in Section 3.3
(HMM+Sp for multinomial emissions, and HMM+ME+Sp for maximum-entropy emissions). In
addition, we also compared to a multinomial HMM with a sparsifying Dirichlet prior on the parameters (HMM+VB) trained using variational Bayes (Johnson, 2007).
Following standard practice, for the multinomial HMMs that do not use morphological information, we lowercase the corpora and replace unique words by a special unknown token, as this
improves the multinomial HMM results by decreasing the number of parameters and eliminating
1.
2.
3.
4.

Implementation: http://www.cs.berkeley.edu/~pliang/software/brown-cluster-1.2.zip
Implementation: http://www.cs.rhul.ac.uk/home/alexc/pos2.tar.gz
Implementation provided by Lamar, Maron and Bienenstock (2010).
Implementation provided by Berg-Kirkpatrick et al. (2010).

537

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

very rare words (mostly nouns). Since the maximum-entropy emission models have access to morphological features, these preprocessing steps do not improve performance and we do not perform
them in that case.
At the start of EM, we randomly initialize all of our implementations of HMM-based models
from the same posteriors, obtained by running the E-step of the HMM model with a set of random
parameters: close to uniform with random uniform jitter of 0.01. This means that for each random
seed, the initialization is identical for all models.
For EM and variational Bayes training, we train the model for 200 iterations, since we found
that typically most models tend to converge by iteration 100. For the HMM+VB model we fix the
transition prior5  to 0.001 and test an emission prior  equal to 0.1 and 0.001, corresponding to
the best values reported by Johnson (2007).
For PR training, we initialize with 30 EM iterations and then run for 170 iterations of PR, following Graa et al. (2009). We used the results that worked best for English (En17) (Graa et al.,
2009), regularizing only words that occur at least 10 times, with  = 32, and use the same configuration for all the other scnenarios. This setting was not specifically tuned for the test languages, and
might not be optimal for every language. Setting such parameters in an unsupervised manner is a
difficult task and we do not address it here (Graa, 2010 discusses more experiments with different
values of those parameters).
We obtain hard assignments using posterior decoding, where for each position we pick the label
with highest posterior probability, since this showed small but consistent improvements over Viterbi
decoding. For all experiments that required random initialization of the parameters we report the
average of 5 random seeds.
All experiments were run using the number of true tags as the number of clusters, with results
obtained in the test set portion of each corpus. We evaluate all systems using four common metrics
for POS induction: 1-Many mapping, 1-1 mapping (Haghighi & Klein, 2006), variation of information (VI) (Meila, 2007), and validity measure (V) (Rosenberg & Hirschberg, 2007). These metrics
are described in detail in Appendix B.
5.3 HMM+ME+Sp Performance
This section compares the gains from using a feature-rich representation with those from the ambiguity penalty, as described in Section 3.3. Experiments show that having a feature-rich representation always improves performance, and that having an ambiguity penalty also always improves
performance. Then, we will see that the improvements from the two methods combine additively,
suggesting that they address independent aspects of POS induction.
We use two different feature sets: the large feature set is that of Berg-Kirkpatrick et al. (2010),
while the reduced feature set was described by Graa (2010). We apply count-based feature selection to both the identity and suffix features. Specifically, we only add identity features for words
occurring at least 10 times and suffix features for words occurring at least 20 times. We also add a
punctuation feature. In what follows, we refer to the large feature set as feature set 1 and the reduced
feature set as 2. The total number of features for each model and language is given in Table 1. The
results of these experiments are summarized in Table 2.
Table 2 shows the results for 10 training methods across six corpora and four evaluation metrics,
resulting in 240 experimental conditions. To simplify the discussion, we focus on the 1-Many metric
5. The transition prior does not significantly affect the results, and we do not report results with different values.

538

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10

HMM
HMM+Sp
HMM+ME1 Prior 1
HMM+ME1 Prior 10
HMM+ME2 Prior 1
HMM+ME2 Prior 10
HMM+ME+Sp1 Prior 1
HMM+ME+Sp1 Prior 10
HMM+ME+Sp2 Prior 1
HMM+ME+Sp2 Prior 10

En45
62.4
67.5
67.1
70.0
70.3
69.9
71.4
68.8
71.6
71.1

En17
65.6
70.3
72.2
69.2
71.8
71.0
71.7
71.1
72.5
72.0

1-Many
PT BG
64.9 58.9
71.2 65.0
72.3 61.1
66.8 58.2
73.9 62.4
74.1 63.9
75.1 63.0
71.6 62.2
73.4 60.9
76.9 67.1

DK
60.0
67.5
65.1
63.9
66.5
67.8
64.5
68.2
65.0
72.0

1-1
ES PTB45 PTB17 PT BG DK ES
60.2 42.5
43.5 42.2 40.6 37.4 30.6
69.0 46.1
52.5 47.7 46.3 40.0 35.3
71.8 45.0
51.1 46.3 46.1 42.6 40.8
66.2 48.4
45.6 40.8 41.7 41.1 35.1
72.9 45.1
49.8 46.8 46.7 45.0 42.6
73.4 47.3
51.2 48.8 48.8 44.7 37.1
72.8 49.4
52.5 46.6 46.7 43.1 41.1
69.3 45.1
52.1 48.0 49.7 42.0 38.5
72.1 52.5
53.9 45.5 49.5 43.0 38.6
75.2 46.7
48.5 49.6 53.4 48.7 40.8

HMM
HMM+Sp
HMM+ME1 Prior 1
HMM+ME1 Prior 10
HMM+ME2 Prior 1
HMM+ME2 Prior 10
HMM+ME+Sp1 Prior 1
HMM+ME+Sp1 Prior 10
HMM+ME+Sp2 Prior 1
HMM+ME+Sp2 Prior 10

En45
4.22
3.64
3.77
3.31
3.59
3.28
3.20
3.46
3.21
3.41

En17
3.75
3.20
3.11
3.38
3.12
3.24
3.09
3.15
3.04
3.25

VI
PT BG
3.90 4.04
3.27 3.49
3.21 3.46
3.66 3.83
3.05 3.46
3.12 3.44
3.00 3.38
3.15 3.43
3.16 3.37
2.86 3.12

DK
4.55
3.85
3.77
4.13
3.71
3.74
3.80
3.73
3.72
3.35

V
ES PTB45 PTB17 PT BG DK ES
4.89 .558
.479 .490 .383 .432 .474
3.85 .616
.549 .573 .467 .518 .581
3.56 .606
.564 .583 .460 .519 .608
4.11 .649
.530 .527 .406 .482 .553
3.46 .626
.559 .600 .460 .528 .617
3.58 .652
.546 .596 .471 .530 .610
3.49 .660
.560 .608 .478 .514 .617
3.76 .637
.557 .591 .470 .532 .589
3.46 .658
.567 .591 .473 .523 .616
3.34 .644
.541 .631 .519 .578 .636

Table 2: Results for different HMMs. HMM and HMM+Sp are HMMs with multinomial emission
functions trained using EM and PR with sparsity constraints, respectively. HMM+ME and
HMM+ME+Spare HMMs with a maximum entropy emission model trained using EM and
PR with sparsity constraints. For the feature-rich models, superscript 1 represents the large
feature set, and superscript 2 represents the reduced feature set. Prior 1 and 10 refers to
the regularization strength for the ME emission model. Table entries are results averaged
over 5 runs. Bold indicates best system overall.

(top left tab of Table 2), and just observe that the conclusions hold for the other three evaluation
metrics also. From Table 2 we can conclude the following:
 Adding a penalty for high word-tag ambiguity improves the performance of the multinomial
HMM. The multinomial HMM trained with EM (line 1 in Table 2) is always worse than the
multinomial HMM trained with PR and an ambiguity penalty, by 6.5% on average (line 2 in
Table 2).
 The feature-rich maximum entropy HMMs (lines 3-6 in Table 2) almost always perform better
than the multinomial HMM. This is true for both feature sets and both regularization strengths
used, with an average increase of 6.4%. The exceptions are possibly due to suboptimal regularization.
 Adding a penalty for high word-tag ambiguity to the maximum-entropy HMM improves performance. In almost all cases, comparing lines 3-6 to lines 7-10 in Table 2, the sparsity
constraints improve performance (average improvement of 1.6%). The combined system al539

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

most always outperforms the multinomial HMM trained using the ambiguity penalty with
an average improvement of 1.6%. For every corpus the best performance is achieved by the
model with an ambiguity penalty and maximum-entropy emission probabilities.
 For every language except English with 17 tags and a particular feature configuration, reducing the feature set by excluding rare features improves performance on average by 2.3% (lines
5-6 are better than lines 3-4 in Table 2).
 Regularizing the maximum-entropy model is more important when there are many features
and when we do not have a word-tag ambiguity penalty. Lines 3-4 of Table 2 have the
maximum-entropy HMM with many features, and we see that having a tight parameter prior
almost always out-performs having a looser prior. By contrast, looking at lines 9-10 of Table 2 we see that when we have an ambiguity penalty and fewer features a looser prior is
almost always better than a tighter parameter prior. This was observed also by Graa (2010).
It is very encouraging to see that the improvements of using a feature-rich model are additive
with the effects of penalizing tag-ambiguity. This is especially surprising since we did not optimize the strength of the tag-ambiguity penalty for the maximum-entropy emission HMM, but rather
used a value reported by Graa et al. (2009) to work for the multinomial emission HMM. Experiments reported by Graa (2010) show that tuning this parameter can further improve performance.
Nevertheless, both methods regularize the objective in different ways and their interaction should
be accounted for. It would be interesting to use L1 regularization on the ME models, instead of
L22 regularization together with a feature count cutoff. This way the model could learn which features to discard, instead of requiring a predefined parameter that depends on the particular corpus
characteristics.
As reported by Berg-Kirkpatrick et al. (2010), the way in which the objective is optimized can
have a big impact on the overall results. However, due to the non-convex objective function it
is unclear which optimization method works better and why. We briefly analyze this question in
Appendix A and leave it as an open question for future work.
5.4 Error Analysis
Figure 2 shows the distribution of true tags and clusters for both the HMM model (left) and the
HMM+ME+Sp model (right) on the En17 corpus. Each bar represents a cluster, labeled by the tag
assigned to it after performing the 1-Many mapping. The colors represent the number of words with
the corresponding true tag. To reduce clutter, true tags that were never used to label a cluster are
grouped into Others.
We observe that both models split common tags such as nouns into several hidden states. This
splitting accounts for many of the errors in both models. By using 5 states for nouns instead of
7, HMM+ME+Sp is able to use more states for adjectives. Another improvement comes from a
better grouping of prepositions. For example to is grouped with punctuation by the HMM while
for HMM+ME+Sp it is correctly mapped to prepositions. Although this should be the correct
behavior, it actually hurts, since the tagset has a special tag TO and all occurrences of the word
to are incorrectly assigned, resulting in the loss of 2.2% accuracy. In contrast, HMM has a state
mapped to the tag TO but the word to comprises only one fifth of that state. The most common
error made by HMM+ME+Sp is to include the word The with the second noun induced tag in
Figure 2 (Right). This induced tag contains mostly capitalized nouns and pronouns, which often
540

fiPREP
DET

N
ADJ

RPUNC
POS

V
INPUNC

CONJ
TO

EPUNC
Others

PREP
DET

N
ADJ

RPUNC
POS

V
INPUNC

CONJ
TO

CONJ

ENDPUNC

V

INPUNC

V

V

ADJ

RPUNC

ADJ

N

ADJ

N

N

N

N

DET

PREP

TO

ENDPUNC

CONJ

V

INPUNC

V

POS

N

ADJ

N

N

N

N

N

N

DET

PREP

C ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

EPUNC
Others

Figure 2: Induced tags by the HMM model (Left), and by the HMM+ME+Sp model (Right) on
the En17 corpus. Each column represents a hidden state, and is labeled by its 1-Many
mapping. Unused true tags are grouped into the cluster named Others.

25000

25000

20000

20000

15000

15000

10000

10000

5000

5000

0

0
art
n
adj

prop
v-fin
prp

punc
num
adv

v-pcp
v-inf
conj-c

pron-pers
sumOthers

art
n
adj

prop
v-fin
prp

punc
num
adv

v-pcp
v-inf
conj-c

pron-pers
sumOthers

Figure 3: Induced tags by the HMM model (Left), and by the HMM+ME+Sp model (Right) on the
Pt corpus. Each column represents a hidden state, and is labeled by its 1-Many mapping.
Unused true tags are grouped into the cluster named Others.

precede nouns of other induced tags. We suspect that the capitalization feature is the cause of this
error.
The better performance of feature-based models on Portuguese relative to English may be due
to the ability of features to better represent the richer morphology of Portuguese. Figure 3 shows
the induced clusters for Portuguese. The HMM+ME+Sp model improves over HMM for all tags
except for adjectives. Both models have trouble distinguishing nouns from adjectives. The reduced
accuracy for adjectives for HMM+ME+Sp is explained by the mapping of a single cluster containing
most of the adjectives to adjectives by the HMM model and to nouns in the HMM+ME+Sp model.
Removing the noun-adjective distinction, as suggested by Zhao and Marcus (2009), would increase
performance of both models by about 6%. Another qualitative difference we observed was that the
HMM+ME+Sp model used a single induced cluster for proper nouns rather than spreading them
across different clusters.

541

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

5.5 State-of-the-Art Comparison
We now compare our best POS induction system (based on the settings in line 10 of Table 2), to
other recent systems. Results are summarized in Table 3. As we have previously done with Table 2,
we focus the discussion on the 1-Many evaluation metric, as results are qualitatively the same for
the VI and V metrics, while the 1-1 metric shows more variance across languages.
1-Many
PT BG
69.6 63.2
66.0 62.3
67.1 57.0
69.2 61.1
64.9 58.9
61.5 51.5
63.2 53.5
71.2 65.0
72.3 64.3
72.5 56.1
74.5
72.0 76.9 67.1

DK
69.6
57.3
58.2
60.9
60.0
51.2
56.6
67.5
62.8
60.6
61.2
72.0

1-1
ES PTB45 PTB17 PT BG DK ES
69.7 53.3
56.6 43.8 47.9 44.3 41.2
67.6 52.3
43.1 47.3 50.3 37.5 37.4
70.1 52.3
44.2 48.1 45.2 37.5 40.0
67.9 48.6
50.0 42.6 50.1 35.2 38.8
60.2 42.5
43.5 42.2 40.6 37.4 30.6
45.5 48.1
50.3 51.4 42.0 42.7 35.9
55.9 44.1
51.4 45.1 38.3 38.1 34.4
69.0 46.1
52.5 47.7 46.3 40.0 35.3
72.0 48.3
54.4 45.5 50.6 41.5 37.2
73.7 54.5
47.9 42.9 38.8 41.5 40.4
68.9 50.9
64.1
52.1 58.3
75.2 46.7
48.5 49.6 53.4 48.7 40.8

VI
PT BG
3.34 3.30
3.38 3.30
3.28 3.60
3.50 3.51
3.90 4.04
3.65 3.90
3.97 4.07
3.27 3.49
3.19 3.30
3.29 3.89
2.86 3.12

DK
3.41
3.97
3.99
4.24
4.55
4.20
4.41
3.85
3.90
4.15
3.35

V
ES PTB45 PTB17 PT BG DK ES
3.40 .648
.559 .564 .473 .554 .613
3.76 .660
.498 .545 .475 .490 .588
3.55 .663
.499 .557 .424 .485 .610
4.00 .626
.585 .546 .450 .474 .569
4.89 .558
.479 .490 .383 .432 .474
4.40 .534
.500 .477 .368 .405 .402
4.69 .535
.501 .471 .368 .437 .474
3.85 .616
.549 .573 .467 .518 .581
4.15 .645
.568 .582 .479 .477 .596
3.56 .678
.534 .574 .392 .477 .611
3.34 .644
.541 .631 .519 .578 .636

1
2
3
4
5
6
7
8
9
10
11
12

En45
B ROWN
68.7
C LARK5
72.4
C LARK10
72.5
LDC
67.5
HMM
62.4
HMM+VB0.1
55.0
HMM+VB0.001
58.6
HMM+Sp
67.5
BK+EM
69.1
BK+DG
75.8
TLHMM
62.2
HMM+ME+Sp2 Prior 10 71.1

En17
68.7
63.5
63.2
74.7
65.6
67.2
67.7
70.3
72.1
67.9

1
2
3
4
5
6
7
8
9
10
12

En45
B ROWN
3.17
C LARK5
3.23
C LARK10
3.20
LDC
3.43
HMM
4.22
HMM+VB0.1
4.10
HMM+VB0.001
4.38
HMM+Sp
3.64
BK+EM
3.31
BK+DG
3.01
HMM+ME+Sp2 Prior 10 3.41

En17
3.07
3.45
3.46
3.00
3.75
3.52
3.55
3.20
3.04
3.21
3.25

Table 3: Comparing HMM+ME+Sp2 with several other POS induction systems. All results from
models with random initialization that we run (systems: 2,3,5,6,7,8,9,10,12) represent an
average over 5 runs. See Section 5.5 for details and discussion.

Lines 1-3 in Table 3 show clustering algorithms based on the information gain on various metrics. B ROWN wins 5/6 times (in the scenarios with fewer clusters) over the C LARK system, despite
the fact that C LARK uses morphology. Comparing lines 1-3 of Table 3 to line 4, we see that the
LDC system is particularly strong for En17 where it achieves state-of-the-art results, but behaves
worse than the B ROWN system for every other corpus.
For HMMs with multinomial emissions (lines 5-8 of Table 3), both maximum likelihood training (HMM) and parameter sparsity (HMM+VB) perform worse than adding an ambiguity penalty
(HMM+Sp). This holds for other evaluation metrics, with the exception of 1-1. This confirms previous results by Graa et al. (2009). Comparing the models in lines 5-8 to those in lines 1-3, we see

542

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

that the best HMM (HMM+Sp) performs comparably with the best clustering (B ROWN), with one
model winning for 3 languages and the other for the remaining 3.
The feature rich HMMs (BK+EM and BK+DG) perform very well, achieving results that are
better than HMM+Sp for 4 of 6 tests. Even though both optimize the same objective, they achieve
different results on different corpora. We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity,
Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our
implementation produces comparable, but not quite identical results.
Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have
a feature-rich representation to capture morphological information. The results for TLHMM are
taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora. Also,
because we were not able to rerun the experiments for TLHMM, we were not able to compute the
information-theoretic metrics. Consequently, the comparison for TLHMM is slightly less complete
than for the other methods. Both TLHMM and HMM+ME+Sp perform competitively or better
than the other systems. This is not surprising since they have the ability to model morphological
regularity while also penalizing high ambiguity. Comparing TLHMM with HMM+ME+Sp, we see
that HMM+ME+Sp performs better on the 1-Many metric. In contrast, TLHMM performs better
on 1-1. One possible explanation is that the underlying model in TLHMM is a Bayesian HMM with
sparsifying Dirichlet priors. As noted by Graa et al. (2009), models trained in this way tend to have
a cluster distribution that more closely resemble the true POS distribution (some clusters with lots
of words and some with few words) which favors the 1-1 metric (a description of the particularity
of the 1-1 metric is discussed in Appendix B).
To summarize, for all non-English languages and all metrics except 1-1, the HMM+ME+Sp
system performs better than all the other systems. For English, BK+DG wins for the 45-tag corpus,
while LDC wins for the 17-tag corpus. The HMM+ME+Sp system is fairly robust, performing well
on all corpora and best on several of them, which allow us to conclude that it is not tuned to any
particular corpus or evaluation metric.
The performance of HMM+ME+Sp is tightly related to the performance of the underlying
HMM+ME system. In Appendix A we present a discussion about the performance of different optimization methods for HMM+ME. We compare our HMM+ME implementation to that of BK+EM
and BK+DG and show that there are some significant differences in performance. However, its not
clear by the results which one is better, and why it performs better in a given situation.
As mentioned by Clark (2003), morphological information is particularly useful for rare words.
Table 4 compares different models accuracy for words according to their frequency. We compare clustering models based on information gain with and without morphological information
(B ROWN,C LARK), a distributional information-based model (LDC), and the feature rich HMM
with tag ambiguity control (HMM+ME+Sp). As expected we see that systems using morphology
do better on rare words. Moreover these systems improve over almost all categories except very
common words (words occurring more than 50 times). Comparing HMM+ME+Sp against C LARK,
we see that even for the condition where C LARK overall works better (En45), it still performs worse
for rare words than HMM+ME+Sp.

543

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

1
5
 10
 50
> 50

B ROWN
50.07
61.76
64.32
67.13
68.94

C LARK
49.48
62.89
66.53
67.62
73.87

1
5
 10
 50
> 50

B ROWN
31.44
48.06
63.14
66.30
80.61

C LARK
44.53
53.30
64.33
67.52
74.68

1
5
 10
 50
> 50

B ROWN
41.04
61.75
72.10
64.44
86.43

C LARK
49.93
64.03
69.76
57.67
73.69

En45
LDC
50.65
58.70
61.78
64.28
68.04
PT
LDC
19.19
36.67
54.16
62.56
84.58
ES
LDC
38.75
51.94
61.69
56.74
81.94

HMM+ME+Sp2
70.12
72.12
70.59
70.80
71.49

B ROWN
29.42
43.38
50.94
59.16
72.14

C LARK
60.62
69.30
71.13
71.50
62.04

HMM+ME+Sp2
63.51
68.60
72.35
71.32
79.52

B ROWN
32.55
48.18
56.59
60.15
74.01

C LARK
52.25
65.00
69.51
68.95
61.89

HMM+ME+Sp2
68.65
72.05
73.35
59.94
82.11

B ROWN
37.71
49.54
58.90
60.42
82.82

C LARK
43.58
48.62
51.92
55.87
60.88

En17
LDC
53.39
64.03
67.35
68.02
77.14
BG
LDC
40.61
53.58
60.82
62.06
67.02
DK
LDC
35.65
40.17
47.96
46.12
77.91

HMM+ME+Sp2
75.79
76.50
74.29
75.31
71.40
HMM+ME+Sp2
68.17
73.54
71.53
68.59
65.89
HMM+ME+Sp2
64.41
66.98
65.21
61.83
73.26

Table 4: 1-Many accuracy by word frequency for different corpora.
5.6 Using the Clusters
As a further comparison of the different POS induction methods, we experiment with a simple
semisupervised scheme where we use the learned clusters as features in a supervised POS tagger.
The basic supervised model has the same features as the HMM+ME model, except that we use
all word identities and suffixes regardless of frequency. We trained the supervised model using
averaged perceptron for a number of iterations chosen as follows: split the training set into 20% for
development and 80% for training and pick the number of iterations  to optimize accuracy on the
development set. Finally, trained on the full training set using  iterations and report results on a
500 sentence test set.
We augmented the standard features with the learned hidden state for the current token, for
each unsupervised method (B ROWN,C LARK,LDC, HMM+ME+Sp). Figure 4 shows the average
accuracy of the supervised model as we varied the type of unsupervised features. The average is
taken over 10 random samples for the training set at each training set size. We can see from Figure 4
that using sem-supervised features from any of the models improves performance even if we have
500 labeled sentences. Moreover, we see that HMM+ME+Sp either performs as well or better than
the other models.

544

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

LDC
Brown
HMM+ME+Sp
Clark

10
8
6
4
2
0

100 200 300 400 500
# Training samples

ES

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

10
8
6
4
2
0

Improvement:

En17

10
8
6
4
2
0

Improvement:

BG

Improvement:

10
8
6
4
2
0

100 200 300 400 500
# Training samples

10
8
6
4
2
0

Improvement:

LDC
Brown
HMM+ME+Sp
Clark

Improvement:

En45

Improvement:

10
8
6
4
2
0

PT

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

DK

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

Figure 4: Error reduction from using the induced clusters as features on a semi-supervised model
as a function of labeled data size. Top Left: En45. Top Middle: En17. Top Right: PT.
Bottom Left: BG. Bottom Middle: ES. Bottom Right: DK.

6. Conclusion
In this work we investigated the task of fully unsupervised POS induction in five different languages.
We identified and proposed solutions for three major problems of the simple hidden Markov model
that has been used extensively for this task: i) treating words atomically, ignoring orthographic
and morphological information  which we addressed by replacing multinomial word distributions
by small maximum-entropy models; ii) an excessive number of parameters that allows models to
fit irrelevant correlations  which we adressed by discarding parameters with small support in the
corpus; iii) a training regime (maximum likelihood) that allows very high word ambiguity  which
we addressed by training using the PR framework with a word ambiguity penalty. We show that all
these solutions improve the model performance and that the improvements are additive. Comparing
against the regular HMM we achieve an impressive improvement of 10.4% on average.
We also compared our system against the main competing systems and show that our approach
performs better in every language except English. Moreover, our approach performs well across
languages and learning conditions, even when hyperparameters are not tuned to the conditions.
When the induced clusters are used as features in a semi-supervised POS tagger trained with a small
amount of supervised data, we show significant improvements. Moreover, the clusters induced by
our system always perform as well as or better than the clusters produced by other systems.

545

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Acknowledgments
Joo V. Graa was supported by a fellowship from Fundao para a Cincia e Tecnologia (SFRH/
BD/ 27528/ 2006) and by FCT project CMU-PT/HuMach/0039/2008 and by FCT (INESC-ID multiannual funding) through the PIDDAC Program funds. Kuzman Ganchev was partially supported by
NSF ITR EIA 0205448. Ben Taskar was partially supported by the DARPA CSSG 2009 Award and
the ONR 2010 Young Investigator Award. Lusa Coheur was partially supported by FCT (INESC-ID
multiannual funding) through the PIDDAC Program funds.

Appendix A. Unsupervised Optimization
Berg-Kirkpatrick et al. (2010) describe the feature-rich HMM and show that training this model
using direct gradient rather than EM can lead to better results. However, they only report results
for the En45 corpus. Table 5 compares their implementation of both training regimes (BK+EM,
BK+DG) on the different languages. Comparing the two training regimes, we see that there is no
clear winner. BK+EM wins in 3 cases (Bg,En17,Dk) and loses on the other three.
It is also not clear how to predict which method is more suitable. In a follow up discussion
6 the authors propose that the difference arises from when each algorithm starts to fine-tune the
weights of rare features relative to when it trains the weights of common features such as short
suffixes. In the case of direct gradient training, at the start of optimization, the weights of common
features change more rapidly because weight gradient is proportional to feature frequency. As
training progresses, more weight is transferred to the rarer features. In contrast, for EM training,
the optimization is done to completion on each M-Step, so even in the first iterations of EM where
the counts are mostly random, the rarer features get a lot of the weight mass. This prevents the
model from generalizing, and optimization terminates at a local maximum closer to the starting
point. To allow EM to use common features for longer we tried some small experiments where
we initially had very permissive stopping criteria for the M-step. After a few EM iterations with
permissive stopping criteria, we require stricter stopping criteria. This tended to improve EM, but
we did not find a principled method of setting a schedule for the convergence criteria on the M-step.
Furthermore, these small experiments do not explain why direct gradient is only better than EM for
some languages while being worse on others.
A related study (Salakhutdinov et al., 2003) compares the convergence rate of EM and direct
gradient training, and identifies conditions when EM achieves Newton-like behavior, and when it
achieves first-order convergence. The conditions are based on the amount of missing information,
which in this case can be approximated by the number of hidden states. Potentially, this difference
can also lead to different local maxima, mainly due to the non-local nature of the line search procedure of gradient based methods. In fact, looking at the results, DG training seems to work better on
the corpora that have a higher number of hidden states (En45, Es) and work worse on corpora with
fewer hidden states (Bg,En17).
Also in Table 5 we compare our implementation of the HMM+ME model to the implementation
of Berg-Kirkpatrick et al. (2010), using the same conditions (regularization parameter, feature set,
convergence criteria, initialization) and observe significant differences in results. Communication
and code-comparison revealed small implementation differences: we use a bias feature while they
do not; for the same random seed, our parameters are initialized differently than theirs; we have
6. http://www.cs.berkeley.edu/~tberg/gradVsEM/main.html

546

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

En45
BK+EM 69.1
BK+DG 75.8
HMM+ME 67.1

1-Many
En17 Pt
Bg Dk Es
72.1 72.3 64.3 62.8 72.0
67.9 72.5 56.1 60.6 73.7
72.2 72.3 61.1 65.1 71.8

En45
48.3
54.5
45.0

1-1
En17 Pt
Bg Dk
Es
54.4 45.5 50.6 41.5 37.2
47.9 42.9 38.8 41.5 40.4
51.1 46.3 46.1 42.6 40.8

En45
BK+EM 3.31
BK+DG 3.01
HMM+ME 3.77

VI
En17 Pt
Bg Dk Es
3.04 3.19 3.30 3.90 4.15
3.21 3.29 3.89 4.15 3.56
3.11 3.21 3.46 3.77 3.56

En45
.645
.678
.606

V
En17 Pt
Bg Dk
Es
.568 .582 .479 .477 .596
.534 .574 .392 .477 .611
.564 .583 .460 .519 .608

Table 5: EM vs direct gradient from Berg-Kirkpatrick et al. (2010) implementation compared with
our implementaion of EM of the HMM with maximum-entropy emission probabilities.
The rows starting with BK are for the Berkeley implementation, while the rows starting
with ME are for our implementation.

different implementations of the optimization algorithm; and a different number of iterations. For
some corpora these differences result in better performance for their implementation, while for
other corpora our implementation gets better results. We leave these details as well as a better
understanding of the differences between each optimization procedure as future work, since this is
not the main focus of the present paper.

Appendix B. Evaluation Metrics
To compare the performance of the different models one needs to evaluate the quality of the induced
clusters. Several evaluation metrics for clustering have been proposed in previous work. The metrics
we use to evaluate can be divided into two types (Reichart & Rappoport, 2009): mapping-based and
information theoretic. Mapping based metrics require a post-processing step to map each cluster
to a POS tag and then evaluate accuracy as for supervised POS tagging. Information-theoretic (IT)
metrics compare the induced clusters directly with the true POS tags.
1-Many mapping and 1-1 mapping (Haghighi & Klein, 2006) are two widely-used mapping
metrics. In the 1-Many mapping, each hidden state is mapped to the tag with which it cooccurs the
most. This means that several hidden states can be mapped to the same tag, and some tags might not
be used at all. The 1-1 mapping greedily assigns each hidden state to a single tag. In the case where
the number of tags and hidden states is the same, this will give a 1-1 correspondence. A major
drawback of the latter mapping is that it fails to express all the information of the hidden states.
Typically, unsupervised models prefer to explain very frequent tags with several hidden states, and
combine some very rare tags. For example the Pt corpus has 3 tags that occur only once in the
corpus. Grouping these together but subdividing nouns still provides a lot of information about
the true tag assignments. However, this would not be captured by the 1-1 mapping. This metric
tends to favor systems that produce an exponential distribution on the size of each induced cluster
independent of the clusters true quality, and it does not correlate well with the information theoretic
metrics (Graa et al., 2009). Nevertheless, the 1-Many mapping also has drawbacks, since it can
only distinguish clusters based on their most frequent tag. So, having a cluster split almost evenly

547

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

between nouns and adjectives, or having a cluster with the same number of nouns, but a mixture of
words with different tags gives the same 1-Many accuracy.
The information-theoretic measures we use for evaluation are variation of information (VI)
(Meila, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007). Both are based on the
entropy and conditional entropy of the tags and induced clusters. VI has desirable geometric properties  it is a metric and is convexly additive (Meila, 2007). However, the range of VI values is
dataset-dependent (VI lies in [0, 2 log N ] where N is the number of POS tags) which does not allow
a comparison across datasets with different N . The validity-measure (V) is also an entropy-based
measure and always lies in the range [0, 1], but does not satisfy the same geometric properties as
VI. It has been reported to give a high score when a large number of clusters exist, even if these
are of low quality (Reichart & Rappoport, 2009). Other information-theoretic measures have been
proposed that better handle different numbers of clusters, for instance NVI (Reichart & Rappoport,
2009). However, in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist. Christodoulopoulos, Goldwater, and Steedman (2010)
present an extensive comparison between evaluation metrics. In related work Maron, Lamar, and
Bienenstock (2010) present another empirical study about metrics and conclude that the VI metric
can produce results that contradict the true quality of the induced clustering, by giving very high
scores to very simple baseline systems, for instance assigning the same label to all words. They
also point out several problems with the 1-1 metric some of which we explained previously. Since
metric comparison is not the focus of this work we will compare all methods using the four metrics
described in this section.

References
Abeill, A. (2003). Treebanks: Building and Using Parsed Corpora. Springer.
Afonso, S., Bick, E., Haber, R., & Santos, D. (2002). Floresta Sinta(c)tica: a treebank for Portuguese. In Proc. LREC, pp. 16981703.
Baum, L., Petrie, T., Soules, G., & Weiss, N. (1970). A maximization technique occurring in the
statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical
Statistics, 41(1), 164171.
Berg-Kirkpatrick, T., Bouchard-Ct, A., DeNero, J., & Klein, D. (2010). Painless unsupervised
learning with features. In Proc. NAACL.
Bertsekas, D., Homer, M., Logan, D., & Patek, S. (1995). Nonlinear programming. Athena Scientific.
Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-gram
models of natural language. Computational Linguistics, 18, 467479.
Chen, S. (2003). Conditional and joint models for grapheme-to-phoneme conversion. In Proc.
ECSCT.
Christodoulopoulos, C., Goldwater, S., & Steedman, M. (2010). Two decades of unsupervised POS
induction: How far have we come?. In Proc. EMNLP, Cambridge, MA.
Civit, M., & Mart, M. (2004). Building cast3lb: A spanish treebank. Research on Language &
Computation, 2(4), 549574.

548

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

Clark, A. (2003). Combining distributional and morphological information for part of speech induction. In Proc. EACL.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1).
Freitag, D. (2004). Toward unsupervised whole-corpus tagging. In Proc. COLING. Association for
Computational Linguistics.
Ganchev, K., Graa, J., Gillenwater, J., & Taskar, B. (2010). Posterior regularization for structured
latent variable models. Journal of Machine Learning Research, 11, 20012049.
Gao, J., & Johnson, M. (2008). A comparison of Bayesian estimators for unsupervised hidden
Markov model POS taggers. In In Proc. EMNLP, pp. 344352, Honolulu, Hawaii. ACL.
Goldwater, S., & Griffiths, T. (2007). A fully Bayesian approach to unsupervised part-of-speech
tagging. In In Proc. ACL, Vol. 45, p. 744.
Graa, J., Ganchev, K., Pereira, F., & Taskar, B. (2009). Parameter vs. posterior sparisty in latent
variable models. In Proc. NIPS.
Graa, J., Ganchev, K., & Taskar, B. (2007). Expectation maximization and posterior constraints.
In In Proc. NIPS. MIT Press.
Graa, J. a. d. A. V. (2010). Posterior Regularization Framework: Learning Tractable Models with
Intractable Constraints. Ph.D. thesis, Universidade Tcnica de Lisboa, Instituto Superior
Tcnico.
Haghighi, A., & Klein, D. (2006). Prototype-driven learning for sequence models. In Proc. HTLNAACL. ACL.
Headden, III, W. P., McClosky, D., & Charniak, E. (2008). Evaluating unsupervised part-of-speech
tagging for grammar induction. In Proc. COLING, pp. 329336.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers via
syntactic projection across parallel texts. Special Issue of the Journal of Natural Language
Engineering on Parallel Texts, 11(3), 311325.
Johnson, M. (2007). Why doesnt EM find good HMM POS-taggers. In In Proc. EMNLP-CoNLL.
Kromann, Matthias T. (2003). The Danish Dependency Treebank and the underlying linguistic
theory. In Second Workshop on Treebanks and Linguistic Theories (TLT), pp. 217220, Vxj,
Sweden.
Lamar, M., Maron, Y., & Bienenstock, E. (2010). Latent-descriptor clustering for unsupervised POS
induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pp. 799809, Cambridge, MA. Association for Computational Linguistics.
Lamar, M., Maron, Y., Johnson, M., & Bienenstock, E. (2010). SVD and clustering for unsupervised POS tagging. In Proceedings of the ACL 2010 Conference: Short Papers, pp. 215219,
Uppsala, Sweden. Association for Computational Linguistics.
Lee, Y. K., Haghighi, A., & Barzilay, R. (2010). Simple type-level unsupervised POS tagging. In
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,
pp. 853861, Cambridge, MA. Association for Computational Linguistics.

549

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Marcus, M., Marcinkiewicz, M., & Santorini, B. (1993). Building a large annotated corpus of
English: The Penn Treebank. Computational linguistics, 19(2), 313330.
Maron, Y., Lamar, M., & Bienenstock, E. (2010). Evaluation criteria for unsupervised POS induction. Tech. rep., Indiana University.
Martin, S., Liermann, J., & Ney, H. (1998). Algorithms for bigram and trigram word clustering. In
Speech Communication, pp. 12531256.
Meila, M. (2007). Comparing clusteringsan information based distance. J. Multivar. Anal., 98(5),
873895.
Merialdo, B. (1994). Tagging English text with a probabilistic model. Computational linguistics,
20(2), 155171.
Moon, T., Erk, K., & Baldridge, J. (2010). Crouching Dirichlet, hidden Markov model: Unsupervised POS tagging with context local tag generation. In Proc. EMNLP, Cambridge, MA.
Neal, R. M., & Hinton, G. E. (1998). A new view of the EM algorithm that justifies incremental,
sparse and other variants. In Jordan, M. I. (Ed.), Learning in Graphical Models, pp. 355368.
Kluwer.
Nocedal, J., & Wright, S. J. (1999). Numerical optimization. Springer.
Ratnaparkhi, A. (1996). A maximum entropy model for part-of-speech tagging. In Proc. EMNLP.
ACL.
Ravi, S., & Knight, K. (2009). Minimized models for unsupervised part-of-speech tagging. In In
Proc. ACL.
Reichart, R., & Rappoport, A. (2009). The NVI clustering evaluation measure. In Proc. CONLL.
Rosenberg, A., & Hirschberg, J. (2007). V-measure: A conditional entropy-based external cluster
evaluation measure. In EMNLP-CoNLL, pp. 410420.
Salakhutdinov, R., Roweis, S., & Ghahramani, Z. (2003). Optimization with EM and expectationconjugate-gradient. In Proc. ICML, Vol. 20.
Schtze, H. (1995). Distributional part-of-speech tagging. In Proc. EACL, pp. 141148.
Shen, L., Satta, G., & Joshi, A. (2007). Guided learning for bidirectional sequence classification. In
Proc. ACL, Prague, Czech Republic.
Simov, K., Osenova, P., Slavcheva, M., Kolkovska, S., Balabanova, E., Doikoff, D., Ivanova, K.,
Simov, A., Simov, E., & Kouylekov, M. (2002). Building a Linguistically Interpreted Corpus
of Bulgarian: the BulTreeBank. In Proc. LREC.
Smith, N., & Eisner, J. (2005). Contrastive estimation: Training log-linear models on unlabeled
data. In Proc. ACL. ACL.
Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2008). Unsupervised multilingual learning for
POS tagging. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, pp. 10411050. Association for Computational Linguistics.
Toutanova, K., & Johnson, M. (2007). A Bayesian LDA-based model for semi-supervised part-ofspeech tagging. In Proc. NIPS, 20.

550

fiC ONTROLLING C OMPLEXITY IN PART- OF -S PEECH I NDUCTION

Toutanova, K., Klein, D., Manning, C., & Singer, Y. (2003). Feature-rich part-of-speech tagging
with a cyclic dependency network. In In Proc. HLT-NAACL.
Zhao, Q., & Marcus, M. (2009). A simple unsupervised learner for POS disambiguation rules given
only a minimal lexicon. In Proc. EMNLP.

551

fiJournal of Artificial Intelligence Research 41 (2011) 297-327

Submitted 01/11; published 06/11

Stackelberg vs. Nash in Security Games: An Extended Investigation
of Interchangeability, Equivalence, and Uniqueness
Dmytro Korzhyk

DIMA @ CS . DUKE . EDU

Department of Computer Science, Duke University
LSRC, Campus Box 90129, Durham, NC 27708, USA

Zhengyu Yin

ZHENGYUY @ USC . EDU

Computer Science Department, University of Southern California
3737 Watt Way, Powell Hall of Engg. 208, Los Angeles, CA 90089, USA

Christopher Kiekintveld

CDKIEKINTVELD @ UTEP. EDU

Department of Computer Science, The University of Texas at El Paso
500 W. University Ave., El Paso, TX 79968, USA

Vincent Conitzer

CONITZER @ CS . DUKE . EDU

Department of Computer Science, Duke University
LSRC, Campus Box 90129, Durham, NC 27708, USA

Milind Tambe

TAMBE @ USC . EDU

Computer Science Department, University of Southern California
3737 Watt Way, Powell Hall of Engg. 410, Los Angeles, CA 90089, USA

Abstract
There has been significant recent interest in game-theoretic approaches to security, with much
of the recent research focused on utilizing the leader-follower Stackelberg game model. Among
the major applications are the ARMOR program deployed at LAX Airport and the IRIS program in
use by the US Federal Air Marshals (FAMS). The foundational assumption for using Stackelberg
games is that security forces (leaders), acting first, commit to a randomized strategy; while their
adversaries (followers) choose their best response after surveillance of this randomized strategy.
Yet, in many situations, a leader may face uncertainty about the followers surveillance capability.
Previous work fails to address how a leader should compute her strategy given such uncertainty.
We provide five contributions in the context of a general class of security games. First, we
show that the Nash equilibria in security games are interchangeable, thus alleviating the equilibrium
selection problem. Second, under a natural restriction on security games, any Stackelberg strategy
is also a Nash equilibrium strategy; and furthermore, the solution is unique in a class of security
games of which ARMOR is a key exemplar. Third, when faced with a follower that can attack
multiple targets, many of these properties no longer hold. Fourth, we show experimentally that in
most (but not all) games where the restriction does not hold, the Stackelberg strategy is still a Nash
equilibrium strategy, but this is no longer true when the attacker can attack multiple targets. Finally,
as a possible direction for future research, we propose an extensive-form game model that makes
the defenders uncertainty about the attackers ability to observe explicit.

1. Introduction
There has been significant recent research interest in game-theoretic approaches to security at airports, ports, transportation, shipping and other infrastructure (Pita et al., 2008; Pita, Jain, Ordonez,
Portway et al., 2009; Jain et al., 2010). Much of this work has used a Stackelberg game framec
2011
AI Access Foundation. All rights reserved.

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

work to model interactions between the security forces and attackers and to compute strategies for
the security forces (Conitzer & Sandholm, 2006; Paruchuri et al., 2008; Kiekintveld et al., 2009;
Basilico, Gatti, & Amigoni, 2009; Letchford, Conitzer, & Munagala, 2009; Korzhyk, Conitzer,
& Parr, 2010). In this framework, the defender (i.e., the security forces) acts first by committing
to a patrolling or inspection strategy, and the attacker chooses where to attack after observing the
defenders choice. The typical solution concept applied to these games is Strong Stackelberg Equilibrium (SSE), which assumes that the defender will choose an optimal mixed (randomized) strategy
based on the assumption that the attacker will observe this strategy and choose an optimal response.
This leader-follower paradigm appears to fit many real-world security situations.
Indeed, Stackelberg games are at the heart of two major deployed decision-support applications. The first is the ARMOR security system, deployed at the Los Angeles International Airport
(LAX) (Pita et al., 2008; Jain et al., 2010). In this domain police are able to set up checkpoints
on roads leading to particular terminals, and assign canine units (bomb-sniffing dogs) to patrol
terminals. Police resources in this domain are homogeneous, and do not have significant scheduling constraints. The second is IRIS, a similar application deployed by the Federal Air Marshals
Service (FAMS) (Tsai, Rathi, Kiekintveld, Ordonez, & Tambe, 2009; Jain et al., 2010). Armed
marshals are assigned to commercial flights to deter and defeat terrorist attacks. This domain has
more complex constraints. In particular, marshals are assigned to tours of flights that return to the
same destination, and the tours on which any given marshal is available to fly are limited by the
marshals current location and timing constraints. The types of scheduling and resource constraints
we consider in the work in this paper are motivated by those necessary to represent this domain.
Additionally, there are other security applications that are currently under evaluation and even more
in the pipeline. For example, the Transportation Security Administration (TSA) is testing and evaluating the GUARDS system for potential national deployment (at over 400 airports)  GUARDS
also uses Stackelberg games for TSA security resource allocation for conducting security activities
aimed at protection of the airport infrastructure (Pita, Bellamane et al., 2009). Another example is
an application under development for the United States Coast Guard for suggesting patrolling strategies to protect ports to ensure the safety and security of all passenger, cargo, and vessel operations.
Other potential examples include protecting electric power grids, oil pipelines, and subway systems
infrastructure (Brown, Carlyle, Salmeron, & Wood, 2005); as well as border security and computer
network security.
However, there are legitimate concerns about whether the Stackelberg model is appropriate in all
cases. In some situations attackers may choose to act without acquiring costly information about the
security strategy, especially if security measures are difficult to observe (e.g., undercover officers)
and insiders are unavailable. In such cases, a simultaneous-move game model may be a better
reflection of the real situation. The defender faces an unclear choice about which strategy to adopt:
the recommendation of the Stackelberg model, or of the simultaneous-move model, or something
else entirely? In general settings, the equilibrium strategy can in fact differ between these models.
Consider the normal-form game in Table 1. If the row player has the ability to commit, the SSE
strategy is to play a with .5 and b with .5, so that the best response for the column player is to play
d, which gives the row player an expected utility of 2.5.1 On the other hand, if the players move
simultaneously the only Nash Equilibrium (NE) of this game is for the row player to play a and
the column player c. This can be seen by noticing that b is strictly dominated for the row player.
1. In these games it is assumed that if the follower is indifferent, he breaks the tie in the leaders favor (otherwise, the
optimal solution is not well defined).

298

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

a
b

c
1,1
0,0

d
3,0
2,1

Table 1: Example game where the Stackelberg Equilibrium is not a Nash Equilibrium.
Previous work has failed to resolve the defenders dilemma of which strategy to select when the
attackers observation capability is unclear.
In this paper, we conduct theoretical and experimental analysis of the leaders dilemma, focusing
on security games (Kiekintveld et al., 2009). This is a formally defined class of not-necessarily-zerosum2 games motivated by the applications discussed earlier. We make four primary contributions.
First, we show that Nash equilibria are interchangeable in security games, avoiding equilibrium
selection problems. Second, if the game satisfies the SSAS (Subsets of Schedules Are Schedules)
property, the defenders set of SSE strategies is a subset of her NE strategies. In this case, the defender is always playing a best response by using an SSE regardless of whether the attacker observes
the defenders strategy or not. Third, we provide counter-examples to this (partial) equivalence in
two cases: (1) when the SSAS property does not hold for defender schedules, and (2) when the
attacker can attack multiple targets simultaneously. In these cases, the defenders SSE strategy may
not be part of any NE profile. Finally, our experimental tests show that the fraction of games where
the SSE strategy played is not part of any NE profile is vanishingly small. However, when the attacker can attack multiple targets, then the SSE strategy fails to be an NE strategy in a relatively
large number of games.
Section 2 contains the formal definition of the security games considered in this paper. Section 3
contains the theoretical results about Nash and Stackelberg equilibria in security games, which we
consider to be the main contributions of this paper. In Section 4, we show that our results do not
hold in an extension of security games that allows the attacker to attack multiple targets at once.
Section 5 contains the experimental results. To initiate future research on cases where the properties
from Section 3 do not hold, we present in Section 6 an extensive-form game model that makes the
defenders uncertainty about the attackers ability to observe explicit. We discuss additional related
work in Section 7, and conclude in Section 8.

2. Definitions and Notation
A security game (Kiekintveld et al., 2009) is a two-player game between a defender and an attacker.
The attacker may choose to attack any target from the set T = {t1 , t2 , . . . , tn }. The defender tries
to prevent attacks by covering targets using resources from the set R = {r1 , r2 , . . . , rK }. As shown
in Figure 1, Udc (ti ) is the defenders utility if ti is attacked while ti is covered by some defender
resource. If ti is not covered, the defender gets Udu (ti ). The attackers utility is denoted similarly by
2. The not-necessarily-zero-sumness of games used for counter-terrorism or security resource allocation analysis is
further emphasized by Bier (2007), Keeney (2007), Rosoff and John (2009). They focus on preference elicitation of
defenders and attackers and explicitly outline that the objectives of different terrorist groups or individuals are often
different from each other, and that defenders and attackers objectives are not exact opposites of each other. For
instance, Bier (2007) notes that the attackers utility can also depend on factors that may not have a significant effect
on the defenders utility, such as the cost of mounting the attack as well as the propaganda value of the target to the
attacker.

299

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

Uac (ti ) and Uau (ti ). We use Ud (ti ) = Udc (ti )  Udu (ti ) to denote the difference between defenders
covered and uncovered utilities. Similarly, Ua (ti ) = Uau (ti )  Uac (ti ). As a key property of
security games, we assume Ud (ti ) > 0 and Ua (ti ) > 0. In words, adding resources to cover a
target helps the defender and hurts the attacker.
Attacker

Defender
Udc(ti)

Uac(ti)

Ua(ti) > 0

Udu(ti)

Uau(ti)

Ud(ti) > 0
Not covered

Covered

Figure 1: Payoff structure of security games.
Motivated by FAMS and similar domains, we introduce resource and scheduling constraints for
the defender. Resources may be assigned to schedules covering multiple targets, s  T . For each
resource ri , there is a subset Si of the schedules S that resource ri can potentially cover. That is,
ri can cover any s  Si . In the FAMS domain, flights are targets and air marshals are resources.
Schedules capture the idea that air marshals fly tours, and must return to a particular starting point.
Heterogeneous resources can express additional timing and location constraints that limit the tours
on which any particular marshal can be assigned to fly. An important subset of the FAMS domain
can be modeled using fixed schedules of size 2 (i.e., a pair of departing and returning flights). The
LAX domain is also a subclass of security games as defined here, with schedules of size 1 and
homogeneous resources.
A security game described above can be represented as a normal form game, as follows. The
attackers pure strategy space A is the set of targets. The attackers mixed strategy a = hai i is a
vector where ai represents the probability of attacking
QK ti . The defenders pure strategy is a feasible
assignment of resources to schedules, i.e., hsi i  i=1 Si . Since covering a target with one resource
is essentially the same as covering it with any positive number of resources, the defenders pure
strategy can also be represented by a coverage vector d = hdi i  {0, 1}n where di represents
whether ti is covered or not. For example, h{t1 , t4 }, {t2 }i can be a possible assignment, and the
corresponding coverage vector is h1, 1, 0, 1i. However, not all the coverage vectors are feasible due
to resource and schedule constraints. We denote the set of feasible coverage vectors by D  {0, 1}n .
The defenders mixed strategy C specifies the probabilities of playing each d  D, where each
individual probability is denoted
P by Cd . Let c = hci i be the vector of coverage probabilities corresponding to C, where ci = dD di Cd is the marginal probability of covering ti . For example,
suppose the defender has two coverage vectors: d1 = h1, 1, 0i and d2 = h0, 1, 1i. For the mixed
strategy C = h.5, .5i, the corresponding vector of coverage probabilities is c = h.5, 1, .5i. Denote
the mapping from C to c by , so that c = (C).
If strategy profile hC, ai is played, the defenders utility is

Ud (C, a) =

n
X

ai (ci Udc (ti ) + (1  ci )Udu (ti )) ,

i=1

300

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

while the attackers utility is
Ua (C, a) =

n
X

ai (ci Uac (ti ) + (1  ci )Uau (ti )) .

i=1

If the players move simultaneously, the standard solution concept is Nash equilibrium.
Definition 1. A pair of strategies hC, ai forms a Nash Equilibrium (NE) if they satisfy the following:
1. The defender plays a best-response:
Ud (C, a)  Ud (C0 , a) C0 .
2. The attacker plays a best-response:
Ua (C, a)  Ua (C, a0 )  a0 .
In our Stackelberg model, the defender chooses a mixed strategy first, and the attacker chooses
a strategy after observing the defenders choice. The attackers response function is g(C) : C  a.
In this case, the standard solution concept is Strong Stackelberg Equilibrium (Leitmann, 1978; von
Stengel & Zamir, 2010).
Definition 2. A pair of strategies hC, gi forms a Strong Stackelberg Equilibrium (SSE) if they satisfy
the following:
1. The leader (defender) plays a best-response:
Ud (C, g(C))  Ud (C0 , g(C0 )), for all C0 .
2. The follower (attacker) plays a best-response:
Ua (C, g(C))  Ua (C, g 0 (C)), for all C, g 0 .
3. The follower breaks ties optimally for the leader:
Ud (C, g(C))  Ud (C,  (C)), for all C, where  (C) is the set of follower best-responses to
C.
We denote the set of mixed strategies for the defender that are played in some Nash Equilibrium
by N E , and the corresponding set for Strong Stackelberg Equilibrium by SSE . The defenders
SSE utility is always at least as high as the defenders utility in any NE profile. This holds for any
game, not just security games. This follows from the following: in the SSE model, the leader can
at the very least choose to commit to her NE strategy. If she does so, then the follower will choose
from among his best responses one that maximizes the utility of the leader (due to the tie-breaking
assumption), whereas in the NE the follower will also choose from his best responses to this defender strategy (but not necessarily the ones that maximize the leaders utility). In fact a stronger
claim holds: the leaders SSE utility is at least as high as in any correlated equilibrium. These observations are due to von Stengel and Zamir (2010) who give a much more detailed discussion of these
points (including, implicitly, to what extent this still holds without any tie-breaking assumption).
In the basic model, it is assumed that both players utility functions are common knowledge.
Because this is at best an approximation of the truth, it is useful to reflect on the importance of this
assumption. In the SSE model, the defender needs to know the attackers utility function in order to
compute her SSE strategy, but the attacker does not need to know the defenders utility function; all
301

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

he needs to best-respond is to know the mixed strategy to which the defender committed.3 On the
other hand, in the NE model, the attacker does not observe the defenders mixed strategy and needs
to know the defenders utility function. Arguably, this is much harder to justify in practice, and this
may be related to why it is the SSE model that is used in the applications discussed earlier. Our goal
in this paper is not to argue for the NE model, but rather to discuss the relationship between SSE and
NE strategies for the defender. We do show that the Nash equilibria are interchangeable in security
games, suggesting that NE strategies have better properties in these security games than they do in
general. We also show that in a large class of games, the defenders SSE strategy is guaranteed to
be an NE strategy as well, so that this is no longer an issue for the defender; while the attackers NE
strategy will indeed depend on the defenders utility function, as we will see this does not affect the
defenders NE strategy.
Of course, in practice, the defender generally does not know the attackers utility function exactly. One way to address this is to make this uncertainty explicit and model the game as a Bayesian
game (Harsanyi, 1968), but the known algorithms for solving for SSE strategies in Bayesian games
(e.g., Paruchuri et al., 2008) are practical only for small security games, because they depend on
writing out the complete action space for each player, which is of exponential size in security games.
In addition, even when the complete action space is written out, the problem is NP-hard (Conitzer &
Sandholm, 2006) and no good approximation guarantee is possible unless P=NP (Letchford et al.,
2009). A recent paper by Kiekintveld, Marecki, and Tambe (2011) discusses approximation methods for such models. Another issue is that the attacker is assumed to respond optimally, which
may not be true in practice; several models of Stackelberg games with an imperfect follower have
been proposed by Pita, Jain, Ordonez, Tambe et al. (2009). These solution concepts also make the
solution more robust to errors in estimation of the attackers utility function. We do not consider
Bayesian games or imperfect attackers in this paper.

3. Equilibria in Security Games
The challenge for us is to understand the fundamental relationships between the SSE and NE strategies in security games. A special case is zero-sum security games, where the defenders utility is
the exact opposite of the attackers utility. For finite two-person zero-sum games, it is known that
the different game theoretic solution concepts of NE, minimax, maximin and SSE all give the same
answer. In addition, Nash equilibrium strategies of zero-sum games have a very useful property in
that they are interchangeable: an equilibrium strategy for one player can be paired with the other
players strategy from any equilibrium profile, and the result is an equilibrium, where the payoffs
for both players remain the same.
Unfortunately, security games are not necessarily zero-sum (and are not zero-sum in deployed
applications). Many properties of zero-sum games do not hold in security games. For instance,
a minimax strategy in a security game may not be a maximin strategy. Consider the example in
Table 2, in which there are 3 targets and one defender resource. The defender has three actions; each
of defenders actions can only cover one target at a time, leaving the other targets uncovered. While
3. Technically, this is not exactly true because the attacker needs to break ties in the defenders favor. However, when
the attacker is indifferent among multiple actions, the defender can generally modify her strategy slightly to make the
attacker strictly prefer the action that is optimal for the defender; the point of the tiebreaking assumption is merely to
make the optimal solution well defined. See also the work of von Stengel and Zamir (2010) and their discussion of
generic games in particular.

302

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

all three targets are equally appealing to the attacker, the defender has varying utilities of capturing
the attacker at different targets. For the defender, the unique minimax strategy, h1/3, 1/3, 1/3i, is
different from the unique maximin strategy, h6/11, 3/11, 2/11i.
t1
Def
Att

t2

t3

C

U

C

U

C

U

1
0

0
1

2
0

0
1

3
0

0
1

Table 2: Security game which is not strategically zero-sum.
Strategically zero-sum games (Moulin & Vial, 1978) are a natural and strict superset of zerosum games for which most of the desirable properties of zero-sum games still hold. This is exactly
the class of games for which no completely mixed Nash equilibrium can be improved upon. Moulin
and Vial proved a game (A, B) is strategically zero-sum if and only if there exist u > 0 and v > 0
such that uA + vB = U + V , where U is a matrix with identical columns and V is a matrix with
identical rows (Moulin & Vial, 1978). Unfortunately, security games are not even strategically zerosum. The game in Table 2 is a counterexample, because otherwise there must exist u, v > 0 such
that,




1 0 0
0 1 1
u 0 2 0  + v 1 0 1 
0 0 3
1 1 0

 

a a a
x y z
= b b b  +  x y z 
c c c
x y z
From these equations, a + y = a + z = b + x = b + z = c + x = c + y = v, which implies
x = y = z and a = b = c. We also know a + x = u, b + y = 2u, c + z = 3u. However since
a + x = b + y = c + z, u must be 0, which contradicts the assumption u > 0.
Another concept that is worth mentioning is that of unilaterally competitive games (Kats &
Thisse, 1992). If a game is unilaterally competitive (or weakly unilaterally competitive), this implies
that if a player unilaterally changes his action in a way that increases his own utility, then this must
result in a (weak) decrease in utility for every other players utility. This does not hold for security
games: for example, if the attacker switches from a heavily defended but very sensitive target to an
undefended target that is of little value to the defender, this change may make both players strictly
better off. An example is shown in Table 3. If the attacker switches from attacking t1 to attacking
t2 , each players utility increases.
Nevertheless, we show in the rest of this section that security games still have some important
properties. We start by establishing equivalence between the set of defenders minimax strategies
and the set of defenders NE strategies. Second, we show Nash equilibria in security games are
interchangeable, resolving the defenders equilibrium strategy selection problem in simultaneousmove games. Third, we show that under a natural restriction on schedules, any SSE strategy for the
defender is also a minimax strategy and hence an NE strategy. This resolves the defenders dilemma
about whether to play according to SSE or NE when there is uncertainty about the attackers ability
303

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

t1
Def
Att

t2

C

U

C

U

1
0

0
1

3
2

2
3

Table 3: A security game which is not unilaterally competitive (or weakly unilaterally competitive).

to observe the strategy: the defender can safely play the SSE strategy, because it is guaranteed to
be an NE strategy as well, and moreover the Nash equilibria are interchangeable so there is no risk
of choosing the wrong equilibrium strategy. Finally, for a restricted class of games (including
the games from the LAX domain), we find that there is a unique SSE/NE defender strategy and a
unique attacker NE strategy.
3.1 Equivalence of NE and Minimax
We first prove that any defenders NE strategy is also a minimax strategy. Then for every defenders
minimax strategy C we construct a strategy a for the attacker such that hC, ai is an NE profile.
Definition 3. For a defenders mixed strategy C, define the attackers best response utility by
E(C) = maxni=1 Ua (C, ti ). Denote the minimum of the attackers best response utilities over
all defenders strategies by E  = minC E(C). The set of defenders minimax strategies is defined
as:
M = {C|E(C) = E  }.
We define the function f as follows. If a is an attackers strategy in which target ti is attacked
with probability ai , then f (a) = a is an attackers strategy such that
Ud (ti )
Ua (ti )
Pn
where  > 0 is a normalizing constant such that i=1 ai = 1. The intuition behind the function f
is that the defender prefers playing a strategy C to playing another strategy C0 in a security game
G when the attacker plays a strategy a if and only if the defender also prefers playing C to playing
C0 when the attacker plays f (a) in the corresponding zero-sum security game G, which is defined
in Lemma 3.1 below. Also, the supports of attacker strategies a and f (a) are the same. As we will
show in Lemma 3.1, function f provides a one-to-one mapping of the attackers NE strategies in G
to the attackers NE strategies in G, with the inverse function f 1 (a) = a given by the following
equation.
1 Ua (ti )
ai = ai
(1)
 Ud (ti )
ai = ai

Lemma 3.1. Consider a security game G. Construct the corresponding zero-sum security game G
in which the defenders utilities are re-defined as follows.
Udc (t) = Uac (t)
Udu (t) = Uau (t)
Then hC, ai is an NE profile in G if and only if hC, f (a)i is an NE profile in G.
304

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

Proof. Note that the supports of strategies a and a = f (a) are the same, and also that the attackers
utility function is the same in games G and G. Thus a is a best response to C in G if and only if a is
a best response to C in G.
Denote the utility that the defender gets if profile hC, ai is played in game G by UdG (C, a). To
show that C is a best response to a in game G if and only if C is a best response to a in G, it is
sufficient to show equivalence of the following two inequalities.
UdG (C, a)  UdG (C0 , a)  0
 UdG (C, a)  UdG (C0 , a)  0
We will prove the equivalence by starting from the first inequality and transforming it into the second
one. On the one hand, we have,
UdG (C, a)  UdG (C0 , a) =

n
X

ai (ci  c0i )Ud (ti ).

i=1

Similarly, on the other hand, we have,
UdG (C, a)  UdG (C0 , a) =

n
X

ai (ci  c0i )Ua (ti ).

i=1

Given Equation (1) and  > 0, we have,
UdG (C, a)  UdG (C0 , a)  0
n
X

ai (ci  c0i )Ud (ti )  0



i=1
n
X

1 Ua (ti )
ai
(ci  c0i )Ud (ti )  0
 Ud (ti )

i=1
n
X

1


ai (ci  c0i )Ua (ti )  0

i=1


1  G

Ud (C, a)  UdG (C0 , a)  0

 UdG (C, a)  UdG (C0 , a)  0

Lemma 3.2. Suppose C is a defender NE strategy in a security game. Then E(C) = E  , i.e.,
N E  M .
Proof. Suppose hC, ai is an NE profile in the security game G. According to Lemma 3.1, hC, f (a)i
must be an NE profile in the corresponding zero-sum security game G. Since C is an NE strategy
in the zero-sum game G, it must also be a minimax strategy in G (Fudenberg & Tirole, 1991). The
attackers utility function in G is the same as in G, thus C must also be a minimax strategy in G, and
E(C) = E  .
305

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

Lemma 3.3. In a security game G, any defenders strategy C such that E(C) = E  is an NE
strategy, i.e., M  N E .
Proof. C is a minimax strategy in both G and the corresponding zero-sum game G. Any minimax
strategy is also an NE strategy in a zero-sum game (Fudenberg & Tirole, 1991). Then there must
exist an NE profile hC, ai in G. By Lemma 3.1, hC, f 1 (a)i is an NE profile in G. Thus C is an
NE strategy in G.
Theorem 3.4. In a security game, the set of defenders minimax strategies is equal to the set of
defenders NE strategies, i.e., M = N E .
Proof. Lemma 3.2 shows that every defenders NE strategy is a minimax strategy, and Lemma 3.3
shows that every defenders minimax strategy is an NE strategy. Thus the sets of defenders NE and
minimax strategies must be equal.
It is important to emphasize again that while the defenders equilibrium strategies are the same
in G and G, this is not true for the attackers equilibrium strategies: attacker probabilities that leave
the defender indifferent across her support in G do not necessarily leave her indifferent in G. This is
the reason for the function f (a) above.
3.2 Interchangeability of Nash Equilibria
We now show that Nash equilibria in security games are interchangeable. This result indicates that,
for the case where the attacker cannot observe the defenders mixed strategy, there is effectively no
equilibrium selection problem: as long as each player plays a strategy from some equilibrium, the
result is guaranteed to be an equilibrium. Of course, this still does not resolve the issue of what to
do when it is not clear whether the attacker can observe the mixed strategy; we return to this issue
in Subsection 3.3.
Theorem 3.5. Suppose hC, ai and hC0 , a0 i are two NE profiles in a security game G. Then hC, a0 i
and hC0 , ai are also NE profiles in G.
Proof. Consider the corresponding zero-sum game G. From Lemma 3.1, both hC, f (a)i and hC0 , f (a0 )i
must be NE profiles in G. By the interchange property of NE in zero-sum games (Fudenberg & Tirole, 1991), hC, f (a0 )i and hC0 , f (a)i must also be NE profiles in G. Applying Lemma 3.1 again
in the other direction, we get that hC, a0 i and hC0 , ai must be NE profiles in G.
By Theorem 3.5, the defenders equilibrium selection problem in a simultaneous-move security
game is resolved. The reason is that given the attackers NE strategy a, the defender must get the
same utility by responding with any NE strategy. Next, we give some insights on expected utilities
in NE profiles. We first show the attackers expected utility is the same in all NE profiles, followed
by an example demonstrating that the defender may have varying expected utilities corresponding
to different attackers strategies.
Theorem 3.6. Suppose hC, ai is an NE profile in a security game. Then, Ua (C, a) = E  .
Proof. From Lemma 3.2, C is a minimax strategy and E(C) = E  . On the one hand,
Ua (C, a) =

n
X

ai Ua (C, ti ) 

i=1

n
X
i=1

306

ai E(C) = E  .

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

On the other hand, because a is a best response to C, it should be at least as good as the strategy of
attacking t  arg maxt Ua (C, t) with probability 1, that is,
Ua (C, a)  Ua (C, t ) = E(C) = E  .
Therefore we know Ua (C, a) = E  .
Unlike the attacker who gets the same utility in all NE profiles, the defender may get varying
expected utilities depending on the attackers strategy selection. Consider the game shown in Table 4. The defender can choose to cover one of the two targets at a time. The only defender NE
strategy is to cover t1 with 100% probability, making the attacker indifferent between attacking t1
and t2 . One attacker NE strategy is to always attack t1 , which gives the defender an expected utility
of 1. Another attackers NE strategy is h2/3, 1/3i, given which the defender is indifferent between
defending t1 and t2 . In this case, the defenders utility decreases to 2/3 because she captures the
attacker with a lower probability.
t1
Def
Att

t2

C

U

C

U

1
1

0
2

2
0

0
1

Table 4: A security game where the defenders expected utility varies in different NE profiles.

3.3 SSE Strategies Are Also Minimax/NE Strategies
We have already shown that the set of defenders NE strategies coincides with her minimax strategies. If every defenders SSE strategy is also a minimax strategy, then SSE strategies must also be
NE strategies. The defender can then safely commit to an SSE strategy; there is no selection problem for the defender. Unfortunately, if a security game has arbitrary scheduling constraints, then an
SSE strategy may not be part of any NE profile. For example, consider the game in Table 5 with 4
targets {t1 , . . . , t4 }, 2 schedules s1 = {t1 , t2 }, s2 = {t3 , t4 }, and a single defender resource. The
defender always prefers that t1 is attacked, and t3 and t4 are never appealing to the attacker.
t1
Def
Att

t2

t3

t4

C

U

C

U

C

U

C

U

10
2

9
5

-2
3

-3
4

1
0

0
1

1
0

0
1

Table 5: A schedule-constrained security game where the defenders SSE strategy is not an NE
strategy.

There is a unique SSE strategy for the defender, which places as much coverage probability on
s1 as possible without making t2 more appealing to the attacker than t1 . The rest of the coverage
probability is placed on s2 . The result is that s1 and s2 are both covered with probability 0.5. In
307

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

contrast, in a simultaneous-move game, t3 and t4 are dominated for the attacker. Thus, there is no
reason for the defender to place resources on targets that are never attacked, so the defenders unique
NE strategy covers s1 with probability 1. That is, the defenders SSE strategy is different from the
NE strategy. The difference between the defenders payoffs in these cases can also be arbitrarily
large because t1 is always attacked in an SSE and t2 is always attacked in a NE.
The above example restricts the defender to protect t1 and t2 together, which makes it impossible
for the defender to put more coverage on t2 without making t1 less appealing. If the defender could
assign resources to any subset of a schedule, this difficulty is resolved. More formally, we assume
that for any resource ri , any subset of a schedule in Si is also a possible schedule in Si :
1  i  K : s0  s  Si  s0  Si .

(2)

If a security game satisfies Equation (2), we say it has the SSAS property. This is natural in many
security domains, since it is often possible to cover fewer targets than the maximum number that a
resource could possible cover in a schedule. We find that this property is sufficient to ensure that
the defenders SSE strategy must also be an NE strategy.
Lemma 3.7. Suppose C is a defender strategy in a security game which satisfies the SSAS property
and c = (C) is the corresponding vector of marginal probabilities. Then for any c0 such that
0  c0i  ci for all ti  T , there must exist a defender strategy C0 such that (C0 ) = c0 .
Proof. The proof is by induction on the number of ti where c0i 6= ci , as denoted by (c, c0 ). As
the base case, if there is no i such that c0i 6= ci , the existence trivially holds because (C) = c0 .
Suppose the existence holds for all c, c0 such that (c, c0 ) = k, where 0  k  n  1. We consider
any c, c0 where (c, c0 ) = k + 1. Then for some j, c0j 6= cj . Since c0j  0 and c0j < cj , we have
cj > 0. There must be a nonempty set of coverage vectors Dj that cover tj and receive positive
probability in C. Because the security game satisfies the SSAS property, for every d  Dj , there
is a valid d which covers all targets in d except for tj . From the defender strategy C, by shifting
Cd (cj c0j )
probability from every d  Dj to the corresponding d , we get a defender strategy C
cj
where ci = ci for i 6= j, and ci = c0i for i = j. Hence (c , c0 ) = k, implying there exists a C0 such
that (C0 ) = c0 by the induction assumption. By induction, the existence holds for any c, c0 .

Theorem 3.8. Suppose C is a defender SSE strategy in a security game which satisfies the SSAS
property. Then E(C) = E  , i.e., SSE  M = N E .
Proof. The proof is by contradiction. Suppose hC, gi is an SSE profile in a security game which
satisfies the SSAS property, and E(C) > E  . Let Ta = {ti |Ua (C, ti ) = E(C)} be the set of targets
that give the attacker the maximum utility given the defender strategy C. By the definition of SSE,
we have
Ud (C, g(C)) = max Ud (C, ti ).
ti Ta

Consider a defender mixed strategy C such that E(C ) = E  . Then for any ti  Ta , Ua (C , ti ) 
E  . Consider a vector c0 :



 c  E  Ua (C , ti ) +  ,
ti  Ta ,
(3a)
i
Uau (ti )  Uac (ti )
c0i =
 
ci ,
ti 
/ Ta ,
(3b)
308

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

where  is an infinitesimal positive number. Since E   Ua (C , ti ) +  > 0, we have c0i < ci for all
ti  Ta . On the other hand, since for all ti  Ta ,
Ua (c0 , ti ) = E  +  < E(C) = Ua (C, ti ),
we have c0i > ci  0. Then for any ti  T , we have 0  c0i  ci . From Lemma 3.7, there exists a
defender strategy C0 corresponding to c0 . The attackers utility of attacking each target is as follows:
 
E + ,
ti  Ta ,
(4a)
0
Ua (C , ti ) =


Ua (C , ti )  E ,
ti 
/ Ta .
(4b)
Thus, the attackers best responses to C0 are still Ta . For all ti  Ta , since c0i > ci , it must be the
case that Ud (C, ti ) < Ud (C0 , ti ). By definition of attackers SSE response g, we have,
Ud (C0 , g(C0 )) = max Ud (C0 , ti )
ti Ta

> max Ud (C, ti ) = Ud (C, g(C)).
ti Ta

It follows that the defender is better off using C0 , which contradicts the assumption C is an SSE
strategy of the defender.
Theorem 3.4 and 3.8 together imply the following corollary.
Corollary 3.9. In security games with the SSAS property, any defenders SSE strategy is also an
NE strategy.
We can now answer the original question posed in this paper: when there is uncertainty over
the type of game played, should the defender choose an SSE strategy or a mixed strategy Nash
equilibrium or some combination of the two?4 For domains that satisfy the SSAS property, we have
proven that the defender can safely play an SSE strategy, because it is guaranteed to be a Nash
equilibrium strategy as well, and moreover the Nash equilibria are interchangeable so there is no
risk of choosing the wrong equilibrium strategy.
Among our motivating domains, the LAX domain satisfies the SSAS property since all schedules
are of size 1. Other patrolling domains, such as patrolling a port, also satisfy the SSAS property. In
such domains, the defender could thus commit to an SSE strategy, which is also now known to be
an NE strategy. The defender retains the ability to commit, but is still playing a best-response to
an attacker in a simultaneous-move setting (assuming the attacker plays an equilibrium strategy 
it does not matter which one, due to the interchange property shown above). However, the FAMS
domain does not naturally satisfy the SSAS property because marshals must fly complete tours.5 The
question of selecting SSE vs. NE strategies in this case is addressed experimentally in Section 5.
4. Of course, one may not agree that, in cases where its common knowledge that the players move simultaneously,
playing an NE strategy is the right thing to do in practice. This is a question at the heart of game theory that
is far beyond the scope of this paper to resolve. In this paper, our goal is not to argue for using NE strategies
in simultaneous-move settings in general; rather, it is to assess the robustness of SSE strategies to changes in the
information structure of specific classes of security games. For this purpose, NE seems like the natural representative
solution concept for simultaneous-move security games, especially in light of the interchangeability properties that
we show.
5. In principle, the FAMs could fly as civilians on some legs of a tour. However, they would need to be able to commit
to acting as civilians (i.e., not intervening in an attempt to hijack the aircraft) and the attacker would need to believe
that a FAM would not intervene, which is difficult to achieve in practice.

309

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

3.4 Uniqueness in Restricted Games
The previous sections show that SSE strategies are NE strategies in many cases. However, there
may still be multiple equilibria to select from (though this difficulty is alleviated by the interchange
property). Here we prove an even stronger uniqueness result for an important restricted class of
security domains, which includes the LAX domain. In particular, we consider security games where
the defender has homogeneous resources that can cover any single target. The SSAS property is
trivially satisfied,
since all schedules are of size 1. Any vector of coverage probabilities c = hci i
P
such that ni=1 ci  K is a feasible strategy for the defender, so we can represent the defender
strategy by marginal coverage probabilities. With a minor restriction on the attackers payoff matrix,
the defender always has a unique minimax strategy which is also the unique SSE and NE strategy.
Furthermore, the attacker also has a unique NE response to this strategy.
Theorem 3.10. In a security game with homogeneous resources that can cover any single target,
if for every target ti  T , Uac (ti ) 6= E  , then the defender has a unique minimax, NE, and SSE
strategy.
Proof. We first show the defender has a unique minimax strategy. Let T  = {t|Uau (t)  E  }.
Define c = hci i as

u

 Ua (ti )  E ,
ti  T  ,
(5a)
ci =
Uau (ti )  Uac (ti )

0,
ti 
/ T .
(5b)
Note that E  cannot be less than any Uac (ti )  otherwise, regardless of the defenders strategy,
the attacker could always get at least Uac (ti ) > E  by attacking ti , which contradicts the fact that
E  is the attackers best response utility to a defenders minimax strategy. Since E   Uac (ti ) and
we assume E  6= Uac (ti ),
1  ci =

E   Uac (ti )
> 0  ci < 1.
Uau (ti )  Uac (ti )

P
P
Next, we will prove ni=1 ci  K. For the sake ofPcontradiction, suppose ni=1 ci < K. Let
n
0



 > 0 such that
c0 = hc0i i, where
Pn ci0 = ci + . Since ci < 1 and i=1 ci < K, we can find
0
ci < 1 and i=1 ci < K. Then every target has strictly higher coverage in c0 than in c , hence
E(c0 ) < E(c ) = E  , which contradicts the fact that E  is the minimum of all E(c).
Next, we show that if c is a minimax strategy, then c = c . By the P
definition of a minimax
n
 . Hence, U (c, t )  E   c  c . On the one hand
strategy, E(c)
=
E
i
i
i
i=1 ci  K and on the
Pn
Pn a
other hand i=1 ci  i=1 ci  K. Therefore it must be the case that ci = ci for any i. Hence,
c is the unique minimax strategy of the defender.
Furthermore, by Theorem 3.4, we have that c is the unique defenders NE strategy. By Theorem 3.8 and the existence of SSE (Basar & Olsder, 1995), we have that c is the unique defenders
SSE strategy.
In the following example, we show that Theorem 3.10 does not work without the condition
Uac (ti ) 6= E  for every ti . Consider a security game with 4 targets in which the defender has two
homogeneous resources, each resource can cover any single target, and the players utility functions
are as defined in Table 5. The defender can guarantee the minimum attackers best-response utility
310

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

of E  = 3 by covering t1 with probability 2/3 or more and covering t2 with probability 1. Since
E  = Uac (t2 ), Theorem 3.10 does not apply. The defender prefers an attack on t1 , so the defender
must cover t1 with probability exactly 2/3 in an SSE strategy. Thus the defenders SSE strategies
can have coverage vectors (2/3, 1, 1/3, 0), (2/3, 1, 0, 1/3), or any convex combination of those two
vectors. According to Theorem 3.8, each of those SSE strategies is also a minimax/NE strategy, so
the defenders SSE, minimax, and NE strategies are all not unique in this example.
Theorem 3.11. In a security game with homogeneous resources that can cover any one target, if
for every target ti  T , Uac (ti ) 6= E  and Uau (ti ) 6= E  , then the attacker has a unique NE strategy.
Proof. c and T  are the same as in the proof of Theorem 3.10. Given the defenders unique NE
strategy c , in any attackers best response, only ti  T  can be attacked with positive probability,
because,
 
E
ti  T 
(6a)

Ua (c , ti ) =
u

Ua (ti ) < E
ti 
/ T
(6b)
Suppose hc , ai forms an NE profile. We have
X

ai = 1

(7)

ti T 

For any ti  T  , we know from the proof of Theorem 3.10 that ci < 1. In addition, because
Uau (t) 6= E  , we have ci 6= 0. Thus we have 0 < ci < 1 for any ti  T  . For any ti , tj  T  ,
necessarily ai Ud (ti ) = aj Ud (tj ). Otherwise, assume ai Ud (ti ) > aj Ud (tj ). Consider
another defenders strategy c0 where c0i = ci +  < 1, c0j = cj   > 0, and c0k = ck for any k 6= i, j.
Ud (c0 , a)  Ud (c , a) = ai Ud (ti )  aj Ud (tj ) > 0
Hence, c is not a best response to a, which contradicts the assumption that hc , ai is an NE profile.
Therefore, there exists  > 0 such that, for any ti  T  , ai Ud (ti ) = . Substituting ai with
/Ud (ti ) in Equation (7), we have
= X
ti T 

1
1
Ud (ti )

Then we can explicitly write down a as
ai =





,
Ud (ti )

0,

ti  T  ,

(8a)

ti 
/ T .

(8b)

As we can see, a defined by (8a) and (8b) is the unique attacker NE strategy.
In the following example, we show that Theorem 3.11 does not work without the condition
Uau (ti ) 6= E  for every ti . Consider a game with three targets in which the defender has one resource that can cover any single target and the utilities are as defined in Table 6. The defender can
guarantee the minimum attackers best-response utility of E  = 2 by covering targets t1 and t2 with
311

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

probability 1/2 each. Since Uac (ti ) 6= E  for every ti , Theorem 3.10 applies, and the defenders
strategy with coverage vector (.5, .5, 0) is the unique minimax/NE/SSE strategy. However, Theorem 3.11 does not apply because Uau (t3 ) = E  . The attackers NE strategy is indeed not unique,
because both attacker strategies (.5, .5, 0) and (1/3, 1/3, 1/3) (as well as any convex combination
of these strategies) are valid NE best-responses.
t1
Def
Att

t2

t3

C

U

C

U

C

U

0
1

1
3

0
1

1
3

0
0

1
2

Table 6: An example game in which the defender has a unique minimax/NE/SSE strategy with coverage vector (.5, .5, 0), but the attacker does not have a unique NE strategy. Two possible
attackers NE strategies are (.5, .5, 0) and (1/3, 1/3, 1/3).

The implication of Theorem 3.10 and Theorem 3.11 is that under certain conditions in the
simultaneous-move game, both the defender and the attacker have a unique NE strategy, which
gives each player a unique expected utility as a result.

4. Multiple Attacker Resources
To this point we have assumed that the attacker will attack exactly one target. We now extend our
security game definition to allow the attacker to use multiple resources to attack multiple targets
simultaneously.
4.1 Model Description
To keep the model simple, we assume homogeneous resources (for both players) and schedules of
size 1. The defender has K < n resources which can be assigned to protect any target, and the
attacker has L < n resources which can be used to attack any target. Attacking the same target with
multiple resources is equivalent to attacking with a single resource. The defenders pure strategy
is a coverage vector d = hdi i  D, where di  {0, 1} represents whether ti P
is covered or not.
n
Similarly,
the
attackers
pure
strategy
is
an
attack
vector
q
=
hq
i

Q.
We
have
i
i=1 di = K and
Pn
i=1 qi = L. If pure strategies hd, qi are played, the attacker gets a utility of
Ua (d, q) =

n
X

qi (di Uac (ti ) + (1  di )Uau (ti ))

i=1

while the defenders utility is given by
Ud (d, q) =

n
X

qi (di Udc (ti ) + (1  di )Udu (ti ))

i=1

The defenders mixed strategy is a vector C which specifies the probability of playing each
d  D. Similarly, the attackers mixed strategy A is a vector of probabilities corresponding to all
312

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

q  Q. As defined in Section 2, we will describe the players mixed strategies by a pair of vectors
hc, ai, where ci is the probability of target ti being defended, and ai is the probability of ti being
attacked.
4.2 Overview of the Results
In some games with multiple attacker resources, the defenders SSE strategy is also an NE strategy,
just like in the single-attacker-resource case. For example, suppose all targets are interchangeable
for both the defender and the attacker. Then, the defenders SSE strategy is to defend all targets
with equal probabilities, so that the defenders utility from an attack on the least defended targets
is maximized. If the attacker best-responds by attacking all targets with equal probabilities, the
resulting strategy profile will be an NE. Thus the defenders SSE strategy is also an NE strategy
in this case. Example 1 below discusses this case in more detail. We observe that the defenders
SSE strategy in this example is the same no matter if the attacker has 1 or 2 resources. We use
this observation to construct a sufficient condition under which the defenders SSE strategy is also
an NE strategy in security games with multiple attacker resources (Proposition 4.2). This modest
positive result, however, is not exhaustive in the sense that it does not explain all cases in which the
defenders SSE strategy is also an NE strategy. Example 2 describes a game in which the defenders
SSE strategy is also an NE strategy, but the condition of Proposition 4.2 is not met.
In other games with multiple attacker resources, the defenders SSE strategy is not part of any
NE profile. The following gives some intuition about how this can happen. Suppose that there
is a target ti that the defender strongly hopes will not be attacked (even Udc (ti ) is very negative),
but given that ti is in fact attacked, defending it does not help the defender much (Ud (ti ) =
Udc (ti )  Udu (ti ) is very small). In the SSE model, the defender is likely to want to devote defensive
resources to ti , because the attacker will observe this and will not want to attack ti . However, in the
NE model, the defenders strategy cannot influence what the attacker does, so the marginal utility
for assigning defensive resources to ti is small; and, when the attacker has multiple resources, there
may well be another target that the attacker will also attack that is more valuable to defend, so the
defender will send her defensive resources there instead. We provide detailed descriptions of games
in which the defenders SSE strategy is not part of any NE profile in Examples 3, 4, and 5.
Since the condition in Proposition 4.2 implies that the defenders SSE and NE strategies do not
change if the number of attacker resources varies, we provide an exhaustive set of example games
in which such equality between the SSE and NE strategies is broken in a number of different ways
(Examples 2, 3, 4, and 5). This set of examples rules out a number of ways in which Proposition 4.2
might have been generalized to a larger set of games.
4.3 Detailed Proofs and Examples
Under certain assumptions, SSE defender strategies will still be NE defender strategies in the model
with multiple attacker resources. We will give a simple sufficient condition for this to hold. First,
we need the following lemma.
Lemma 4.1. Given a security game G L with L attacker resources, let G 1 be the same game except
with only one attacker resource. Let hc, ai be a Nash equilibrium of G 1 . Suppose that for any target
ti , Lai  1. Then, hc, Lai is a Nash equilibrium of G L .
313

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

Proof. If Lai  1 for any ti , then La is in fact a feasible attacker strategy in G L . All that is left to
prove is that hd, Lai is in fact an equilibrium. The attacker is best-responding because the utility of
attacking any given target is unchanged for him relative to the equilibrium of G 1 . The defender is
best-responding because the utility of defending any schedule has been multiplied by L relative to
G 1 , and so it is still optimal for the defender to defend the schedules in the support of c.
This lemma immediately gives us the following proposition:
Proposition 4.2. Given a game G L with L attacker resources for which SSAS holds, let G 1 be the
same game except with only one attacker resource. Suppose d is an SSE strategy in both G L and
G 1 . Let a be a strategy for the attacker such that hd, ai is a Nash equilibrium of G 1 (we know that
such an a exists by Corollary 3.9). If Lai  1 for any target ti , then hd, Lai is an NE profile in G L ,
which means d is both an SSE and an NE strategy in G L .
A simple example where Proposition 4.2 applies can be constructed as follows.
Example 1. Suppose there are 3 targets, which are completely interchangeable for both players.
Suppose the defender has 1 resource. If the attacker has 1 resource, the defenders SSE strategy is
d = (1/3, 1/3, 1/3) and the attackers NE best-response to d is a = (1/3, 1/3, 1/3). If the attacker
has 2 resources, the defenders SSE strategy is still d. Since for all ti , 2ai  1, Proposition 4.2
applies, and profile hd, 2ai is an NE profile.
We denote the defenders SSE strategy in a game with L attacker resources by cS,L and denote
the defenders NE strategy in the same game by cN ,L . In Example 1, we have cN ,1 = cS,1 =
cS,2 = cN ,2 . Hence, under some conditions, the defenders strategy is always the sameregardless
of whether we use SSE or NE and regardless of whether the attacker has 1 or 2 resources. We will
show several examples of games where this is not true, even though SSAS holds. For each of the
following cases, we will show an example game for which SSAS holds and the relation between
the defenders equilibrium strategies is as specified in the case description. In the first case, the
SSE strategy is equal to the NE strategy for L = 2, but the condition of Proposition 4.2 is not met
because the SSE strategy for L = 1 is different from the SSE strategy for L = 2, and also because
multiplying the attackers NE strategy in the game with L = 1 attacker resource by 2 does not result
in a feasible attackers strategy (in the game that has L = 2 attacker resources but is otherwise the
same). In the last three cases, the SSE strategy is not equal to the NE strategy for L = 2.
 cS,2 = cN ,2 6= cN ,1 = cS,1 (SSE vs. NE makes no difference, but L makes a difference);
 cN ,2 6= cS,2 = cS,1 = cN ,1 (NE with L = 2 is different from the other cases);
 cS,2 6= cN ,2 = cN ,1 = cS,1 (SSE with L = 2 is different from the other cases);
 cS,2 6= cN ,2 ; cS,2 6= cS,1 = cN ,1 ; cN ,2 6= cN ,1 = cS,1 (all cases are different, except SSE
and NE are the same with L = 1 as implied by Corollary 3.9).
It is easy to see that these cases are exhaustive, because of the following. Corollary 3.9 necessitates that cS,1 = cN ,1 (because we want SSAS to hold and each cS,L or cN ,L strategy to be unique),
so there are effectively only three potentially different strategies, cN ,2 , cS,2 , and cS,1 = cN ,1 . They
can either all be the same (as in Example 1 after Proposition 4.2), all different (the last case), or we
can have exactly two that are the same (the first three cases).
314

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

We now give the examples. In all our examples, we only have schedules of size 1, and the
defender has a single resource.
Example 2 (cS,2 = cN ,2 6= cN ,1 = cS,1 ). Consider the game shown in Table 7. The defender
has 1 resource. If the attacker has 1 resource, target t1 is attacked with probability 1, and hence
it is defended with probability 1 as well (whether we are in the SSE or NE model). If the attacker
has 2 resources, both targets are attacked, and target t2 is defended because Ud (t2 ) > Ud (t1 )
(whether we are in the SSE or NE model).
t1
Def
Att

t2

C

U

C

U

0
2

1
3

0
0

2
1

Table 7: The example game for cS,2 = cN ,2 6= cN ,1 = cS,1 . With a single attacker resource,
the attacker will always attack t1 , and so the defender will defend t1 . With two attacker
resources, the attacker will attack both targets, and in this case the defender prefers to
defend t2 .

Example 3 (cN ,2 6= cS,2 = cS,1 = cN ,1 ). Consider the game shown in Table 8. The defender has
1 resource. If the attacker has 1 resource, it follows from Theorem 3.10 that the unique defender
minimax/NE/SSE strategy is cS,1 = cN ,1 = (2/3, 1/6, 1/6).
t1
Def
Att

t2

t3

C

U

C

U

C

U

10
1

11
3

0
0

3
2

0
0

3
2

Table 8: The example game for cN ,2 6= cS,2 = cS,1 = cN ,1 . This example corresponds to the
intuition given earlier. Target t1 is a sensitive target for the defender: the defender suffers
a large loss if t1 is attacked. However, if t1 is attacked, then allocating defensive resources
to it does not benefit the defender much, because of the low marginal utility Ud (t1 ) = 1.
As a result, target t1 is not defended in the NE profile h(0, .5, .5), (1, .5, .5)i, but it is
defended in the SSE profile h(1, 0, 0), (0, 1, 1)i.
Now suppose the attacker has 2 resources. In SSE, the defender wants primarily to avoid an
attack on t1 (so that t2 and t3 are attacked with probability 1 each). Under this constraint, the
defender wants to maximize the total probability on t2 and t3 (they are interchangeable and both
are attacked, so probability is equally valuable on either one). The defender strategy (2/3, 1/6, 1/6)
is the unique optimal solution to this optimization problem.
However, it is straightforward to verify that the following is an NE profile if the attacker has 2
resources: h(0, .5, .5), (1, .5, .5)i. We now prove that this is the unique NE. First, we show that t1
315

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

is defended with probability 0 in any NE. This is because one of the targets t2 , t3 must be attacked
with probability at least .5. Thus, the defender always has an incentive to move probability from
t1 to this target. It follows that t1 is not defended in any NE. Now, if t1 is not defended, then t1 is
attacked with probability 1. What remains is effectively a single-attacker-resource security game on
t2 and t3 with a clear unique equilibrium h(.5, .5), (.5, .5)i, thereby proving uniqueness.
Example 4 (cS,2 6= cN ,2 = cN ,1 = cS,1 ). Consider the game shown in Table 9. The defender has
1 resource. If the attacker has 1 resource, then the defenders unique minimax/NE/SSE strategy is
the minimax strategy (1, 0, 0).
Now suppose the attacker has 2 resources. t1 must be attacked with probability 1. Because
Ud (t1 ) = 2 > 1 = Ud (t2 ) = Ud (t3 ), in NE, this implies that the defender must put her full
probability 1 on t1 . Hence, the attacker will attack t2 with his other resource. So, the unique NE
profile is h(1, 0, 0), (1, 1, 0)i.
In contrast, in SSE, the defenders primary goal is to avoid an attack on t2 , which requires
putting probability at least .5 on t2 (so that the attacker prefers t3 over t2 ). This will result in t1
and t3 being attacked; the defender prefers to defend t1 with her remaining probability because
Ud (t1 ) = 2 > 1 = Ud (t3 ). Hence, the unique SSE profile is h(.5, .5, 0), (1, 0, 1)i.
t1
Def
Att

t2

t3

C

U

C

U

C

U

0
5

2
6

9
2

10
4

0
1

1
3

Table 9: The example game for cS,2 6= cN ,2 = cN ,1 = cS,1 . t1 will certainly be attacked by the
attacker, and will hence be more valuable to defend than any other target in NE because
Ud (t1 ) = 2 > 1 = Ud (t2 ) = Ud (t3 ). However, in SSE with two attacker resources,
it is more valuable for the defender to use her resource to prevent an attack on t2 by the
second attacker resource.

Example 5 (cS,2 6= cN ,2 ; cS,2 6= cS,1 = cN ,1 ; cN ,2 6= cN ,1 = cS,1 ). Consider the game in
Table 10. The defender has 1 resource. If the attacker has 1 resource, it follows from Theorem 3.10
that the unique defender minimax/NE/SSE strategy is cS,1 = cN ,1 = (1/6, 2/3, 1/6).
If the attacker has 2 resources, then in SSE, the defenders primary goal is to prevent t1 from
being attacked. This requires putting at least as much defender probability on t1 as on t3 , and will
result in t2 and t3 being attacked. Given that t2 and t3 are attacked, placing defender probability
on t3 is more than twice as valuable as placing it on t2 (Ud (t3 ) = 7, Ud (t2 ) = 3). Hence, even
though for every unit of probability placed on t3 , we also need to place a unit on t1 (to keep t1 from
being attacked), it is still uniquely optimal for the defender to allocate all her probability mass in
this way. So, the unique defender SSE strategy is (.5, 0, .5).
However, it is straightforward to verify that the following is an NE profile if the attacker has
2 resources: h(0, 3/4, 1/4), (1, 7/10, 3/10)i. We now prove that this is the unique NE. First, we
show that t1 is not defended in any NE. This is because at least one of t2 and t3 must be attacked
with probability at least .5, and hence the defender would be better off defending that target instead.
316

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

t1
Def
Att

t2

t3

C

U

C

U

C

U

11
0

12
2

0
1

3
3

0
0

7
2

Table 10: The example game for cS,2 6= cN ,2 ; cS,2 6= cS,1 = cN ,1 ; cN ,2 6= cN ,1 = cS,1 . With one
attacker resource, t1 and t3 each get some small probability (regardless of the solution
concept). With two attacker resources, in the unique NE, it turns out not to be worthwhile
to defend t1 at all even though it is always attacked, because Ud (t1 ) is low; in contrast,
in the unique SSE, t1 is defended with relatively high probability to prevent an attack on
it.

Next, we show that t1 is attacked with probability 1 in any NE. If t3 has positive defender probability,
then (because t1 is not defended) t1 is definitely more attractive to attack than t3 , and hence will be
attacked with probability 1. On the other hand, if the defender only defends t2 , then t1 and t3 are
attacked with probability 1. What remains is effectively a single-attacker-resource security game on
t2 and t3 with a clear unique equilibrium h(3/4, 1/4), (7/10, 3/10)i, thereby proving uniqueness.

5. Experimental Results
While our theoretical results resolve the leaders dilemma for many interesting and important classes
of security games, as we have seen, there are still some cases where SSE strategies are distinct from
NE strategies for the defender. One case is when the schedules do not satisfy the SSAS property,
and another is when the attacker has multiple resources. In this section, we conduct experiments to
further investigate these two cases, offering evidence about the frequency with which SSE strategies
differ from all NE strategies across randomly generated games, for a variety of parameter settings.
Our methodology is as follows. For a particular game instance, we first compute an SSE strategy C using the DOBSS mixed-integer linear program (Pita et al., 2008). We then use the linear
feasibility program below to determine whether or not this SSE strategy is part of some NE profile
by attempting to find an appropriate attacker response strategy.
Aq  [0, 1] for all q  Q
X
Aq = 1

(9)
(10)

qQ

Aq = 0 for all Ua (q, C) < E(C)
X
Aq Ud (d, q)  Z, for all d  D

(11)
(12)

qQ

X

Aq Ud (d, q) = Z, for all d  D with Cd > 0

(13)

qQ

Here Q is the set of attacker pure strategies, which is just the set of targets when there is only one attacker resource. The probability that the attacker plays q is denoted by Aq , which must be between
0 and 1 (Constraint (9)). Constraint (10) forces these probabilities to sum to 1. Constraint (11)
317

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

prevents the attacker from placing positive probabilities on pure strategies that give the attacker a
utility less than the best response utility E(C). In constraints (12) and (13), Z is a variable which
represents the maximum expected utility the defender can get among all pure strategies given the
attackers strategy A, and Cd denotes the probability of playing d in C. These two constraints require the defenders strategy C to be a best response to the attackers mixed strategy. Therefore, any
feasible solution A to this linear feasibility program, taken together with the Stackelberg strategy
C, constitutes a Nash equilibrium. Conversely, if hC, Ai is a Nash equilibrium, A must satisfy all
of the LP constraints.
In our experiment, we varied:
 the number of attacker resources,
 the number of (homogeneous) defender resources,
 the size of the schedules that resources can cover,
 the number of schedules.
For each parameter setting, we generated 1000 games with 10 targets. For each target t, a
pair of defender payoffs (Udc (t), Udu (t)) and a pair of attacker payoffs (Uau (t), Uac (t)) were drawn
uniformly at random from the set {(x, y)  Z2 : x  [10, 10], y  [10, 10], x > y}. In each
game in the experiment, all of the schedules have the same size, except there is also always the
empty scheduleassigning a resource to the empty schedule corresponds to the resource not being
used. The schedules are randomly chosen from the set of all subsets of the targets that have the size
specified by the corresponding parameter.
The results of our experiments are shown in Figure 2. The plots show the percentage of games
in which the SSE strategy is not an NE strategy, for different numbers of defender and attacker
resources, different schedule sizes, and different numbers of schedules. For the case where there
is a single attacker resource and schedules have size 1, the SSAS property holds, and the experimental results confirm our theoretical result that the SSE strategy is always an NE strategy. If we
increase either the number of attacker resources or the schedule size, then we no longer have such a
theoretical result, and indeed we start to see cases where the SSE strategy is not an NE strategy.
Let us first consider the effect of increasing the number of attacker resources. We can see that the
number of games in which the defenders SSE strategy is not an NE strategy increases significantly
as the number of attacker resources increases, especially as it goes from 1 to 2 (note the different
scales on the y-axes). In fact, when there are 2 or 3 attacker resources, the phenomenon that in many
cases the SSE strategy is not an NE strategy is consistent across a wide range of values for the other
parameters.6
Now, let us consider the effect of increasing the schedule size. When we increase the schedule
size (with a single attacker resource), the SSAS property no longer holds because we do not include
the subschedules as schedules, and so we do find some games where the SSE strategy is not an
NE strategybut there are generally few cases (< 6%) of this. Also, as we generate more random
schedules, the number of games where the SSE strategy is not an NE strategy drops to zero. This
is particularly encouraging for domains like FAMS, where the schedule sizes are relatively small (2
6. Of course, if we increase the number of attacker resources while keeping the number of targets fixed, eventually,
every defender SSE strategy will be an NE strategy again, simply because when the number of attacker resources is
equal to the number of targets, the attacker has only one pure strategy available.

318

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

Figure 2: The number of games in which the SSE strategy is not an NE strategy, for different
parameter settings. Each row corresponds to a different number of attacker resources,
and each column to a different schedule size. The number of defender resources is on the
x-axis, and each number of schedules is plotted separately. For each parameter setting,
1000 random games with 10 targets were generated. The SSAS property holds in the
games with schedule size 1 (shown in column 1); SSAS does not hold in the games with
schedule sizes 2 and 3 (columns 2 and 3).

in most cases), and the number of possible schedules is large relative to the number of targets. The
effect of increasing the number of defender resources is more ambiguous. When there are multiple
attacker resources, increasing the schedule size sometimes increases and sometimes decreases the
number of games where the SSE strategy is not an NE strategy.
The main message to take away from the experimental results appears to be that for the case of
a single attacker resource, SSE strategies are usually also NE strategies even when SSAS does not
hold, which appears to further justify the practice of playing an SSE strategy. On the other hand,
when there are multiple attacker resources, there are generally many cases where the SSE strategy is
not an NE strategy. This strongly poses the question of what should be done in the case of multiple
attacker resources (in settings where it is not clear whether the attacker can observe the defenders
mixed strategy).
319

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

6. Uncertainty About the Attackers Ability to Observe: A Model for Future
Research
So far, for security games in which the attacker has only a single resource, we have shown that if
the SSAS property is satisfied, then a Stackelberg strategy is necessarily a Nash equilibrium strategy (Section 3.3). This, combined with the fact that, as we have shown, the equilibria of these
games satisfy the interchangeability property (Section 3.2), provides strong justification for playing
a Stackelberg strategy when the SSAS property is satisfied. Also, our experiments (Section 5) suggest that even when the SSAS property is not satisfied, a Stackelberg strategy is usually a Nash
equilibrium strategy. However, this is not the case if we consider security games where the attacker
has multiple resources.
This leaves the question of how the defender should play in games where the Stackelberg strategy is not necessarily a Nash equilibrium strategy (which is the case in many games with multiple
attacker resources, and also a few games with a single attacker resource where SSAS is not satisfied), especially when it is not clear whether the attacker can observe the defenders mixed strategy.
This is a difficult question that cuts to the heart of the normative foundations of game theory, and
addressing it is beyond the scope of this paper. Nevertheless, given the real-world implications of
this line of research, we believe that it is important for future research to tackle this problem. Rather
than leave the question of how to do so completely open-ended, in this section we propose a model
that may be useful as a starting point for future research. We also provide a result that this model at
least leads to sensible solutions in SSAS games, which, while it is not among the main results in this
paper, does provide a useful sanity check before adopting this model in future research.
In the model that we propose in this section, the defender is uncertain about whether the attacker
can observe the mixed strategy to which the defender commits. Specifically, the game is played as
follows. First, the defender commits to a mixed strategy. After that, with probability pobs , the attacker observes the defenders strategy; with probability 1  pobs , he does not observe the defenders
mixed strategy. Figure 3 represents this model as a larger extensive-form game.7 In this game, first
Nature decides whether the attacker will be able to observe the defenders choice of distribution.
Then, the defender chooses a distribution over defender resource allocations (hence, the defender
has a continuum of possible moves; in particular, it is important to emphasize here that committing to a distribution over allocations is not the same as randomizing over which pure allocation to
commit to, because in the latter case an observing attacker will know the realized allocation). The
defender does not observe the outcome of Natures movehence, it would make no difference if
Nature moved after the defender, but having Nature move first is more convenient for drawing and
discussing the game tree. Finally, the attacker moves (chooses one or more targets to attack): on the
left side of the tree, he does so knowing the distribution to which the defender has committed, and
on the right side of the tree, he does so without knowing the distribution.
Given this extensive-form representation of the situation, a natural approach is to solve for an
equilibrium of this larger game. It is not possible to apply standard algorithms for solving extensiveform games directly to this game, because the tree has infinite size due to the defender choice of
distributions; nevertheless, one straightforward way of addressing this is to discretize the space of
7. At this point, there is a risk of confusion between defender mixed strategies as we have used the phrase so far, and
defender strategies in the extensive-form game. In the rest of this section, to avoid confusion, we will usually refer to
the former as distributions over allocationsbecause, technically, a distribution over allocations is a pure strategy
in the extensive-form game, so that a defender mixed strategy in the extensive-form game would be a distribution
over such distributions.

320

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

Nature

Defender

observed
(pobs)

not observed
(1-pobs)

(infinite
number of
actions)

(infinite
number of
actions)

Attacker

attacker moves with knowledge
of the defender's distribution

attacker moves without knowledge
of the defender's distribution

Figure 3: Extensive form of the larger game in which the defender is uncertain about the attackers
ability to observe.

distributions. An important question, of course, is whether it is the right thing to do to play an
equilibrium of this game. We now state some simple propositions that serve as sanity checks on this
model. First, we show that if pobs = 1, we just obtain the Stackelberg model.
Proposition 6.1. If pobs = 1, then any subgame-perfect equilibrium of the extensive-form game
corresponds to an SSE of the underlying security game.
Proof. We are guaranteed to end up on the left-hand side of the tree, where the attacker observes
the distribution to which the defender has committed; in subgame-perfect equilibrium, he must
best-respond to this distribution. The defender, in turn, must choose her distribution optimally with
respect to this. Hence, the result corresponds to an SSE.
Next, we show that if pobs = 0, we obtain a standard simultaneous-move model.
Proposition 6.2. If pobs = 0, then any Nash equilibrium of the extensive-form game corresponds to
a Nash equilibrium of the underlying security game.
Proof. We are guaranteed to end up on the right-hand side of the tree, where the attacker observes
nothing about the distribution to which the defender has committed. In a Nash equilibrium of the
extensive-form game, the defenders strategy leads to some probability distribution over allocations.
In the attackers information set on the right-hand side of the tree, the attacker can only place positive
probability on actions that are best responses to this distribution over allocations. Conversely, the
defender can only put positive probability on allocations that are best responses to the attackers
distribution over actions. Hence, the result is a Nash equilibrium of the underlying security game.

At intermediate values of pobs , in sufficiently general settings, an equilibrium of the extensiveform game may correspond to neither an SSE or an NE of the basic security game. However,
we would hope that in security games where the Stackelberg strategy is also a Nash equilibrium
strategysuch as the SSAS security games discussed earlier in this paperthis strategy also corresponds to an equilibrium of the extensive-form game. The next proposition shows that this is indeed
the case.
321

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

Proposition 6.3. If in the underlying security game, there is a Stackelberg strategy for the defender
which is also the defenders strategy in some Nash equilibrium, then this strategy is also the defenders strategy in a subgame-perfect equilibrium of the extensive-form game.8
Proof. Suppose that d is a distribution over allocations that is both a Stackelberg strategy and a
Nash equilibrium strategy of the underlying security game. Let aS be the best response that the
attacker plays in the corresponding SSE, and let aN be a distribution over attacker actions such that
hd , aN i is a Nash equilibrium of the security game.
We now show how to construct a subgame-perfect equilibrium of the extensive-form game. Let
the defender commit to the distribution d in her information set. The attackers strategy in the
extensive form is defined as follows. On the left-hand side of the tree, if the attacker observes that
the defender has committed to d , he responds with aS ; if the attacker observes that the defender
has committed to any other distribution over allocations, he responds with some best response to
that distribution. In the information set on the right-hand side of the tree, the attacker plays aN . It
is straightforward to check that the attacker is best-responding to the defenders strategy in every
one of his information sets. All that remains to show is that the defender is best-responding to the
attackers strategy in the extensive-form game. If the defender commits to any other distribution d0 ,
this cannot help her on the left side of the tree relative to d , because d is a Stackelberg strategy; it
also cannot help her on the right side of the tree, because d is a best response to aN . It follows that
the defender is best-responding, and hence we have identified a subgame-perfect equilibrium of the
game.
This proposition can immediately be applied to SSAS games:
Corollary 6.4. In security games that satisfy the SSAS property (and have a single attacker resource), if d is a Stackelberg strategy of the underlying security game, then it is also the defenders
strategy in a subgame-perfect equilibrium of the extensive-form game.
Proof. This follows immediately from Proposition 6.3 and Corollary 3.9.
Of course, Proposition 6.3 also applies to games in which SSAS does not hold but the Stackelberg
strategy is still a Nash equilibrium strategywhich was the case in many of the games in our
experiments in Section 5. In general, of course, if the SSAS property does not hold, the Stackelberg
strategy may not be a Nash equilibrium strategy in the underlying security game; if so, the defenders
strategies in equilibria of the extensive-form game may correspond to neither Stackelberg nor Nash
strategies in the underlying security game. If that is the case, then some other method can be used
to solve the extensive-form game directlyfor example, discretizing the space of distributions for
the attacker and then applying a standard algorithm for solving for an equilibrium of the resulting
game. The latter method will not scale very well, and we leave the design of better algorithms for
future research.

7. Additional Related Work
In the first few sections of this paper, we discussed recent uses of game theory in security domains,
the formal model of security games, and how this model differs from existing classes of games such
8. This will also hold for stronger solution concepts than subgame-perfect equilibrium.

322

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

as strategically zero-sum and unilaterally competitive games. We discuss additional related work in
this section.
There has been significant interest in understanding the interaction of observability and commitment in general Stackelberg games. Bagwells early work (1995) questions the value of commitment
to pure strategies given noisy observations by followers, but the ensuing and on-going debate illustrated that the leader retains her advantage in case of commitment to mixed strategies (van Damme
& Hurkens, 1997; Huck & Muller, 2000). Guth, Kirchsteiger, and Ritzberger (1998) extend these
observations to n-player games. Maggi (1998) shows that in games with private information, the
leader advantage appears even with pure strategies. There has also been work on the value of commitment for the leader when observations are costly (Morgan & Vardy, 2007).
Several examples of applications of Stackelberg games to model terrorist attacks on electric
power grids, subways, airports, and other critical infrastructure were described by Brown et al.
(2005) and Sandler and Arce M. (2003). Drake (1998) and Pluchinsky (2005) studied different
aspects of terrorist planning operations and target selection. These studies indicate that terrorist
attacks are planned with a certain level of sophistication. In addition, a terrorist manual shows that
a significant amount of information used to plan such attacks is collected from public sources (U.S.
Department of Justice, 2001). Zhuang and Bier (2010) studied reasons for secrecy and deception
on the defenders side. A broader interest in Stackelberg games is indicated by applications in other
areas, such as network routing and scheduling (Korilis, Lazar, & Orda, 1997; Roughgarden, 2004).
In contrast with all this existing research, our work focuses on real-world security games, illustrating subset, equivalence, interchangeability, and uniqueness properties that are non-existent in
general Stackelberg games studied previously. Of course, results of this general nature date back to
the beginning of game theory: von Neumanns minimax theorem (1928) implies that in two-player
zero-sum games, equilibria are interchangeable and an optimal SSE strategy is also a minimax /
NE strategy. However, as we have discussed earlier, the security games we studied are generally
not zero-sum games, nor are they captured by more general classes of games such as strategically
zero-sum (Moulin & Vial, 1978) or unilaterally competitive (Kats & Thisse, 1992) games.
Tennenholtz (2002) studies safety-level strategies. With two players, a safety-level (or maximin)
strategy for player 1 is a mixed strategy that maximizes the expected utility for player 1, under the
assumption that player 2 acts to minimize player 1s expected utility (rather than maximize his own
utility). Tennenholtz shows that under some conditions, the utility guaranteed by a safety-level
strategy is equal or close to the utility obtained by player 1 in Nash equilibrium. This may sound
reminiscent of our result that Nash strategies coincide with minimax strategies, but in fact the results
are quite different: in particular, for non-zero-sum games, maximin and minimax strategies are not
identical. The following example gives a simple game for which our result holds, but the safety-level
strategy does not result in a utility that is close to the equilibrium solution.
Example 6. Consider the game shown in Table 11. Each player has 1 resource. In this game,
the safety-level (maximin) strategy for the defender is to place her resource on target 2, thereby
guaranteeing herself a utility of at least 2. However, the attacker has a dominant strategy to
attack target 1 (so that if the defender actually plays the safety-level strategy, she can expect utility
1). On the other hand, in the minimax/Stackelberg/Nash solution, she will defend target 1 and
receive utility 0.
Kalai (2004) studies the idea that as the number of players of a game grows, the equilibria
become robust to certain changes in the extensive form, such as which players move before which
323

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

t1
Def
Att

t2

C

U

C

U

0
2

1
3

2
0

3
1

Table 11: An example game in which the defenders utility from playing the competitive safety
strategy is not close to the defenders Nash/Stackelberg equilibrium utility.

other ones, and what they learn about each others actions. At a high level this is reminiscent of our
results, in the sense that we also show that for a class of security games, a particular choice between
two structures of the game (one player committing to a mixed strategy first, or both players moving
at the same time) does not affect what the defender should play (though the attackers strategy is
affected). However, there does not seem to be any significant technical similarityour result relies
on the structure of this class of security games and not on the number of players becoming large
(after all, we only consider games with two players).
Pita, Jain, Ordonez, Tambe et al. (2009) provide experimental results on observability in Stackelberg games: they test a variety of defender strategies against human players (attackers) who choose
their optimal attack when provided with limited observations of the defender strategies. Results
show the superiority of a defenders strategy computed assuming human anchoring bias in attributing a probability distribution over the defenders actions. This research complements our
paper, which provides new mathematical foundations. Testing the insights of our research with
the experimental paradigm of Pita, Jain, Ordonez, Tambe et al. (2009) with expert players, is an
interesting topic for future research.

8. Summary
This paper is focused on a general class of defender-attacker Stackelberg games that are directly
inspired by real-world security applications. The paper confronts fundamental questions of how a
defender should compute her mixed strategy. In this context, this paper provides four key contributions. First, exploiting the structure of these security games, the paper shows that the Nash equilibria in security games are interchangeable, thus alleviating the defenders equilibrium selection
problem for simultaneous-move games. Second, resolving the defenders dilemma, it shows that
under the SSAS restriction on security games, any Stackelberg strategy is also a Nash equilibrium
strategy; and furthermore, this strategy is unique in a class of security games of which ARMOR is
a key exemplar. Third, when faced with a follower that can attack multiple targets, many of these
properties no longer hold, providing a key direction for future research. Fourth, our experimental
results emphasize positive properties of security games that do not fit the SSAS property. In practical
terms, these contributions imply that defenders in applications such as ARMOR (Pita et al., 2008)
and IRIS (Tsai et al., 2009) can simply commit to SSE strategies, thus helping to resolve a major
dilemma in real-world security applications.
324

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

Acknowledgments
Dmytro Korzhyk and Zhengyu Yin are both first authors of this paper. An earlier conference version
of this paper was published in AAMAS-2010 (Yin, Korzhyk, Kiekintveld, Conitzer, & Tambe,
2010). The major additions to this full version include (i) a set of new experiments with analysis
of the results; (ii) a new model for addressing uncertainty about the attackers ability to observe;
(iii) more thorough treatment of the multiple attacker resources case; (iv) additional discussion of
related research.
This research was supported by the United States Department of Homeland Security through
the National Center for Risk and Economic Analysis of Terrorism Events (CREATE) under award
number 2010-ST-061-RE0001. Korzhyk and Conitzer are supported by NSF IIS-0812113 and
CAREER-0953756, ARO 56698-CI, and an Alfred P. Sloan Research Fellowship. However, any
opinions, findings, and conclusions or recommendations in this document are those of the authors
and do not necessarily reflect views of the funding agencies. We thank Ronald Parr for many detailed comments and discussions. We also thank the anonymous reviewers for valuable suggestions.

References
Bagwell, K. (1995). Commitment and observability in games. Games and Economic Behavior, 8,
271280.
Basar, T., & Olsder, G. J. (1995). Dynamic Noncooperative Game Theory (2nd edition). Academic
Press, San Diego, CA.
Basilico, N., Gatti, N., & Amigoni, F. (2009). Leader-follower strategies for robotic patrolling
in environments with arbitrary topologies. In Proceedings of the Eighth International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 5764, Budapest,
Hungary.
Bier, V. M. (2007). Choosing what to protect. Risk Analysis, 27(3), 607620.
Brown, G., Carlyle, W. M., Salmeron, J., & Wood, K. (2005). Analyzing the vulnerability of critical
infrastructure to attack and planning defenses. In INFORMS Tutorials in Operations Research: Emerging Theory, Methods, and Applications, pp. 102123. Institute for Operations
Research and Management Science, Hanover, MD.
Conitzer, V., & Sandholm, T. (2006). Computing the optimal strategy to commit to. In Proceedings
of the ACM Conference on Electronic Commerce (EC), pp. 8290, Ann Arbor, MI, USA.
Drake, C. J. M. (1998). Terrorists Target Selection. St. Martins Press, Inc.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Guth, W., Kirchsteiger, G., & Ritzberger, K. (1998). Imperfectly observable commitments in nplayer games. Games and Economic Behavior, 23(1), 5474.
Harsanyi, J. (19671968). Game with incomplete information played by Bayesian players. Management Science, 14, 159182; 320334; 486502.
Huck, S., & Muller, W. (2000). Perfect versus imperfect observabilityan experimental test of
Bagwells result. Games and Economic Behavior, 31(2), 174190.
325

fiKORZHYK , Y IN , K IEKINTVELD , C ONITZER , & TAMBE

Jain, M., Tsai, J., Pita, J., Kiekintveld, C., Rathi, S., Ordonez, F., & Tambe, M. (2010). Software
assistants for randomized patrol planning for the LAX airport police and the Federal Air
Marshals Service. Interfaces, 40(4), 267290.
Kalai, E. (2004). Large robust games. Econometrica, 72(6), 16311665.
Kats, A., & Thisse, J. (1992). Unilaterally competitive games. International Journal of Game
Theory, 21(3), 29199.
Keeney, R. (2007). Modeling values for anti-terrorism analysis. Risk Analysis, 27, 585596.
Kiekintveld, C., Jain, M., Tsai, J., Pita, J., Ordonez, F., & Tambe, M. (2009). Computing optimal
randomized resource allocations for massive security games. In Proceedings of the Eighth
International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),
pp. 689696, Budapest, Hungary.
Kiekintveld, C., Marecki, J., & Tambe, M. (2011). Approximation methods for infinite Bayesian
Stackelberg games: Modeling distributional uncertainty. In Proceedings of the International
Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10051012.
Korilis, Y. A., Lazar, A. A., & Orda, A. (1997). Achieving network optima using Stackelberg routing
strategies. IEEE/ACM Transactions on Networking, 5(1), 161173.
Korzhyk, D., Conitzer, V., & Parr, R. (2010). Complexity of computing optimal Stackelberg strategies in security resource allocation games. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), pp. 805810, Atlanta, GA, USA.
Leitmann, G. (1978). On generalized Stackelberg strategies. Optimization Theory and Applications,
26(4), 637643.
Letchford, J., Conitzer, V., & Munagala, K. (2009). Learning and approximating the optimal strategy
to commit to. In Proceedings of the Second Symposium on Algorithmic Game Theory (SAGT09), pp. 250262, Paphos, Cyprus.
Maggi, G. (1998). The value of commitment with imperfect observability and private information.
RAND Journal of Economics, 30(4), 555574.
Morgan, J., & Vardy, F. (2007). The value of commitment in contests and tournaments when observation is costly. Games and Economic Behavior, 60(2), 326338.
Moulin, H., & Vial, J.-P. (1978). Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon. International Journal of Game Theory,
7(3-4), 201221.
Paruchuri, P., Pearce, J. P., Marecki, J., Tambe, M., Ordonez, F., & Kraus, S. (2008). Playing
games for security: An efficient exact algorithm for solving Bayesian Stackelberg games. In
Proceedings of the Seventh International Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pp. 895902, Estoril, Portugal.
Pita, J., Bellamane, H., Jain, M., Kiekintveld, C., Tsai, J., Ordonez, F., & Tambe, M. (2009). Security
applications: Lessons of real-world deployment. In SIGECOM Issue 8.2.
Pita, J., Jain, M., Ordonez, F., Portway, C., Tambe, M., Western, C., Paruchuri, P., & Kraus, S.
(2009). Using game theory for Los Angeles airport security. AI Magazine, 30(1), 4357.
326

fiS TACKELBERG VS . NASH IN S ECURITY G AMES

Pita, J., Jain, M., Ordonez, F., Tambe, M., Kraus, S., & Magori-Cohen, R. (2009). Effective solutions for real-world Stackelberg games: When agents must deal with human uncertainties. In
Proceedings of the Eighth International Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pp. 369376, Budapest, Hungary.
Pita, J., Jain, M., Western, C., Portway, C., Tambe, M., Ordonez, F., Kraus, S., & Parachuri, P.
(2008). Deployed ARMOR protection: The application of a game-theoretic model for security
at the Los Angeles International Airport. In Proceedings of the 7th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2008)  Industry and Applications
Track, pp. 125132, Estoril, Portugal.
Pluchinsky, D. A. (2005). A Typology and Anatomy of Terrorist Operations, chap. 25. The McGrawHill Homeland Security Book. McGraw-Hill.
Rosoff, H., & John, R. (2009). Decision analysis by proxy of the rational terrorist. In Quantitative risk analysis for security applications workshop (QRASA) held in conjunction with the
International Joint Conference on AI, pp. 2532, Pasadena, CA, USA.
Roughgarden, T. (2004). Stackelberg scheduling strategies. SIAM Journal on Computing, 33(2),
332350.
Sandler, T., & Arce M., D. G. (2003). Terrorism and game theory. Simulation and Gaming, 34(3),
319337.
Tennenholtz, M. (2002). Competitive safety analysis: Robust decision-making in multi-agent systems. Journal of Artificial Intelligence Research, 17, 363378.
Tsai, J., Rathi, S., Kiekintveld, C., Ordonez, F., & Tambe, M. (2009). IRIS - a tool for strategic
security allocation in transportation networks. In The Eighth International Conference on
Autonomous Agents and Multiagent Systems - Industry Track, pp. 3744.
U.S. Department of Justice (2001). Al Qaeda training manual. http://www.au.af.mil/au/
awc/awcgate/terrorism/alqaida_manual. Online release 7 December 2001.
van Damme, E., & Hurkens, S. (1997). Games with imperfectly observable commitment. Games
and Economic Behavior, 21(1-2), 282308.
von Neumann, J. (1928). Zur Theorie der Gesellschaftsspiele. Mathematische Annalen, 100, 295
320.
von Stengel, B., & Zamir, S. (2010). Leadership games with convex strategy sets. Games and
Economic Behavior, 69, 446457.
Yin, Z., Korzhyk, D., Kiekintveld, C., Conitzer, V., & Tambe, M. (2010). Stackelberg vs. Nash in
security games: Interchangeability, equivalence, and uniqueness. In Proceedings of the Ninth
International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),
pp. 11391146, Toronto, Canada.
Zhuang, J., & Bier, V. M. (2010). Reasons for secrecy and deception in homeland-security resource
allocation. Risk Analysis, 30(12), 17371743.

327

fiJournal of Artificial Intelligence Research 41 (2011) 69-95

Submitted 10/10; published 05/11

Value of Information Lattice: Exploiting Probabilistic
Independence for Effective Feature Subset Acquisition
Mustafa Bilgic

mbilgic@iit.edu

Illinois Institute of Technology
Chicago, IL 60616 USA

Lise Getoor

getoor@cs.umd.edu

University of Maryland
College Park, MD 20742 USA

Abstract
We address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the
values of the missing features. Because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition
cost and misclassification cost is minimized. We describe the Value of Information Lattice
(VOILA), an optimal and efficient feature subset acquisition framework. Unlike the common
practice, which is to acquire features greedily, VOILA can reason with subsets of features.
VOILA efficiently searches the space of possible feature subsets by discovering and exploiting
conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. Through empirical evaluation on five
medical datasets, we show that the greedy strategy is often reluctant to acquire features,
as it cannot forecast the benefit of acquiring multiple features in combination.

1. Introduction
We often need to make decisions and take appropriate actions in a complex and uncertain
world. An important subset of decisions can be formulated as a classification problem,
where an instance is described by a set of features and one of finite categorical options is
chosen based on these features. Examples include medical diagnosis where the patients are
described by lab tests and a diagnosis is to be made about the disease state of the patient,
and spam detection where an email is described by its content and the email client needs
to decide whether or not the email is spam.
Much research has been done on how to learn effective and efficient classifiers assuming
that the features describing the entities are fully given. Even though this complete data
assumption might hold on a few domains, in practice features that describe the entities
often have missing values. In certain domains such as medical diagnosis where a decision is
made based on a number of features that include laboratory test results, the missing feature
values can be acquired at a cost by performing the related tests. In such cases, we need
to decide which tests to perform in which order. The answer to this question, of course,
depends on how important it is to get the correct classification decision. Put alternatively,
the cost of an incorrect classification (e.g., a misdiagnosis) determines how much we are
willing to spend on expensive tests. Thus, we need to devise a feature acquisition policy
that can determine which tests to perform in which order and when to stop and make the
c
2011
AI Access Foundation. All rights reserved.

fiBilgic & Getoor

final classification decision so that the total incurred cost, the feature acquisition cost and
the expected misclassification cost, is minimized.
Devising the optimal policy in general requires considering all possible permutations of
the features and their expected values. To provide some intuition, some features might be
useful only if acquired together, and the cost and benefit of acquiring some features can
depend on which other features have been acquired and what their values turned out to
be. Because devising the optimal policy is intractable in general, previous work has been
greedy (Gaag & Wessels, 1993; Yang, Ling, Chai, & Pan, 2006), has approximated value
of information calculations (Heckerman, Horvitz, & Middleton, 1993), and has developed
heuristic feature scoring techniques (Nunez, 1991; Turney, 1995).
The greedy approach, however, has at least two major limitations. First, because it
considers each feature in isolation, it cannot accurately forecast the value of acquiring
multiple features together, causing it to produce sub-optimal policies. Second, the greedy
strategy assumes that features can be acquired sequentially and the value of a feature can
be observed before acquiring the next one. This assumption, however, is often not very
practical. For example, doctors typically order batches of measurements simultaneously
such as blood count, cholesterol level, etc., and then possibly order another batch after the
results arrive. These two limitations of the greedy approach make it necessary to reason
with sets of features.
Reasoning with sets of features, on the other hand, poses serious tractability challenges.
First of all, the number of subsets is exponential in the size of the feature set. Second,
judging the value of acquiring a set of features requires taking an expectation over the
possible values of the features in the set, which is also exponential in the number of the
features. The good news, however, is that we do not need to consider all possible subsets of
features in practice; certain features can render other features useless, while some features
are useful only if acquired together. For example, an X-Ray might render a skin test
useless for diagnosing tuberculosis. Similarly, a chest pain alone might not be useful for
differentiating between a cold and a heart disease; it becomes useful only if it is combined
with other features, such as a blood test.
In this article, we describe a data structure that discovers and exploits these types of
constraints (features that render other features useless and features that are useful only
if acquired together) from the underlying probability distribution. We propose Value of
Information Lattice (VOILA) that reduces the space of all possible subsets by exploiting the
constraints between the features. Additionally, VOILA makes it possible to share value of
information calculations between different feature sets to reduce the computation time.
This article builds upon our earlier work (Bilgic & Getoor, 2007). Our contributions in
this article include:
 We introduce two additional techniques for sharing computations between different
subsets of features. These new techniques are based on information caching and
utilizing paths in the underlying Bayesian network.
 We experiment with asymmetric misclassification costs in addition to the symmetric
costs. The asymmetric setup reflects a more realistic case and provides new insights.
 In addition to the feature acquisition costs defined by Turney (1995), we generate
and experiment with synthetic feature costs. The synthetic feature costs capture
70

fiValue of Information Lattice

more complex feature acquisition costs and allows for leeway for various acquisition
strategies to differ.
The remainder of this article is organized as follows. We describe the notation and
problem formulation in Section 2. We describe how we can reduce the search space and
share computations using VOILA in Section 3. We show experimental results in Section 4,
discuss related work in Section 5, and discuss future work in Section 6. We then conclude
in Section 7.

2. Notation and Problem Formulation
Our main task is to classify a given instance that has missing feature values and incur
the minimum acquisition and misclassification cost. Let the instance be described by a set of
features X = {X1 , X2 , . . . , Xn } and let Y be the random variable representing its class. We
assume that the joint probability distribution P (Y, X) is given and concern ourselves with
feature acquisition during only inference (note that the conditional distribution P (Y |X) is
not appropriate, as most features are assumed to be unobserved initially). For the purpose
of this article, we assume that we are given a Bayesian network, but any joint probabilistic
model that allows us to efficiently answer conditional independence queries can be used.
In the notation, a bold face letter represents a set of random variables and non-bold face
letter represents a single random variable. For example X represents the set of features,
whereas Xi  X represents a feature in X and Y represents the class variable. Additionally,
a capital letter represents a random variable, and a lowercase letter represents a particular
value of that variable; this applies to both individual variables and sets of variables. For
example, Y represents a variable, and y represents a particular value that Y can take.
In addition to the probabilistic model, we are also given the cost models that specify
feature acquisition costs and misclassification costs. Formally, we assume that we have a
feature acquisition cost function that given a subset of features, S, and the set of features
whose values are known (evidence) E, returns a non-negative real number C(S | e). We
also assume that we have a misclassification cost model that returns the misclassification
cost cij incurred when Y is assigned yi when the correct assignment is yj . With these cost
functions, we can model non-static feature acquisition costs; that is, the cost of acquiring
the feature Xi can depend on what has been acquired so far and what their values are
(e) as well what is acquired in conjunction with this feature (S \ {Xi }). Moreover, the
misclassification cost model does not assume symmetric costs; different kids of errors (false
positives or negatives) can have different costs.
Figure 1 shows a simple example configuration with two features, X1 and X2 , and the
class variable Y . In this simple example, the joint distribution P (X, Y ) is represented as
a table, the feature costs are simple independent costs for X1 and X2 , and the misclassification cost is symmetric where both types of misclassifications cost the same and correct
classification does not cost anything.
A diagnostic policy  is a decision tree where each node represents a feature and the
branches from the nodes represent the possible values of the features. Each path of the
policy, ps  , represents an ordered sequence feature values s. We will often use ps to
represent an ordered version of s. Typically, the order of the features in the set will be
important for computing the feature costs, as the cost of a feature can depend on the values
71

fiBilgic & Getoor

Figure 1: Example configuration with two features X1 and X2 and class variable Y . The
table from left to right represent: the joint probability distribution P (X1 , X2 , Y ),
the feature costs, and the misclassification costs.

of previously acquired features. The order of the features will be irrelevant for computing
the probability P (s). An example conditional policy using the example configuration of
Figure 1 is given in Figure 2.
Each policy  has two types of costs: the feature acquisition cost and a misclassification
cost. These costs are defined in terms of the costs associated with following the paths in
the policy. We first describe how to compute the feature acquisition cost of a path and then
describe how to compute the associated expected misclassification cost. Finally, we show
how to compute the expected total cost of a policy using the total costs associated with
each path.
In the most naive version, the feature cost of a path ps is the sum of the costs of the
features that appear on the path. However, in practice, the cost of a feature can depend on
which features have been acquired so far and the observed values of the acquired features.
For example, performing the treadmill test (asking the patient to run a treadmill and
measure his heart beat, etc.) can be riskier if we had ordered a cholesterol test and its
result turned out to be high, putting the patient in high risk for heart disease. To account
for these types of costs, the order of the features in ps matters, and the total feature cost
of a path is the summation of the individual feature costs conditioned on the values of the
features that precede the features in consideration:
F C(ps ) =

n
X

C(ps [j] | ps [1 : j])

j=1

where ps [j] represents the j th feature in ps and ps [1 : j] represents feature values 1 through
j in ps .
When we reach the end of a path, we need to make a classification decision. In this
case, we simply utilize the Bayesian decision theory and choose the decision with minimum
risk (i.e., misclassification cost). We find such a decision by using the probabilistic model to
72

fiValue of Information Lattice

Figure 2: An example conditional policy with features X1 , X2 and class variable Y . Each
non-leaf node represents a feature acquisition, with probability distribution of the
possible values, and the cost of the feature. Each path (e.g., X1 = T, X2 = T ) has
an acquisition cost and expected misclassification cost. The policy overall has an
expected total cost ETC, which is the sum of total costs of each path, weighted
by the probability of following that path.

compute the probability distribution P (Y | ps ) and choose the value of Y that leads to the
minimum expected cost. Note that the order of the features values do not matter in this
case; that is P (Y | ps ) = P (Y | s). The expected misclassification of a path, EM C(ps ), is
73

fiBilgic & Getoor

computed as follows:
EM C(ps ) = EM C(s) = min
yi

X

P (Y = yj | s)  cij

(1)

yj

The total cost that we incur by following a path of a policy is simply the sum of the feature
and the expected misclassification costs of that path:
T C(ps ) = F C(ps ) + EM C(ps )
Finally, we compute the expected total cost of a policy  using the total costs of the
individual paths ps  . Each path ps   has a probability of occurrence in real world.
Such probability can be easily computed by the generative probability model that we assumed. It is simply P (s). The expected total cost of a policy is then the sum of the total
cost of each path, T C(ps ), weighted by the probability of following that path, P (s):
ET C() =

X

P (s)T C(ps )

(2)

ps 

The objective of feature acquisition during inference is, given the joint probabilistic
model and the cost models for acquisition and misclassification, find the policy that has the
minimum expected total cost. However, building the optimal decision tree is known to be
NP-complete (Hyafil & Rivest, 1976). Thus, most research have been greedy choosing the
best feature that reduces the misclassification costs the most and has the lowest cost (e.g.,
Gaag & Wessels, 1993; Dittmer & Jensen, 1997) or have developed heuristic feature scoring
techniques (e.g., Nunez, 1991; Tan, 1990).
In the greedy strategy, each path of the policy is extended with the feature that reduces
the misclassification cost the most and that has the lowest cost. More specifically, the path
ps is replaced with new paths psx1 , psx2 , . . . , psxni where x1i , x2i , . . . , xni are the values
i
i
that Xi can take and Xi is the feature that has the highest benefit. We define the benefit of
a feature Xi given a path ps as the reduction in the total cost of the path when the path
is expanded with the possible values of Xi . More formally,
Benef it(Xi | ps ) , T C(ps ) 

n
X

P (xji | s)T C(psxj )
i

j=1

= F C(ps ) + EM C(s) 

n
X



P (xji | s) F C(psxj ) + EM C(s  xji )
i

j=1


= F C(ps )  

n
X


P (xji | s)F C(psxj ) + EM C(s) 
i

j=1

n
X

P (xji | s)EM C(s  xji )

j=1

= C(Xi | s) + EM C(s) 

P (xji | s)EM C(s  xji )

j=1

= F C(ps )  (F C(ps ) + C(Xi | s)) + EM C(s) 
n
X

n
X

P (xji | s)EM C(s  xji )

j=1

74

fiValue of Information Lattice

Note that, the last two terms are equivalent to the definition of expected value of information, EVI, (Howard, 1966):
EV I(Xi | s) = EM C(s) 

n
X

P (xji | s)EM C(s  xji )

(3)

j=1

Substituting EVI, the definition of benefit becomes very intuitive:
Benef it(Xi | ps ) = Benef it(Xi | s) = EV I(Xi | s)  C(Xi | s)

(4)

With this definition, the greedy strategy iteratively finds the feature that has the highest
positive benefit (value cost difference), acquires it, and stops acquisition when there are no
more features with a positive benefit value.
We also note that it is straightforward to define EVI and Benefit for a set S0 of features
just like we did for a single feature. The only difference is that the expectation needs to be
taken over the joint assignments, s0 , to the features in the set S0 .
EV I(S0 | s) = EM C(s) 

X

P (s0 | s)EM C(s  s0 )

(5)

s0

and,
Benef it(S0 | s) = EV I(S0 | s)  C(S0 | s)

(6)

There are a few problems with the greedy strategy as we have mentioned
P earlier. First,
it is short-sighted. There exist sets S  X such that Benef it(S) >
Benef it(Xi ).
Xi S

This is easier to see, for example, for the XOR function, Y = X1 XOR X2 , where X1 and X2
alone are not useful but they are determinative together. Due to this relationship, a greedy
policy is not guaranteed to be optimal. Moreover, the greedy policy can prematurely stop
acquisition because no single feature seems to provide positive benefit.
The second problem with the greedy strategy is that we often need to acquire a set of
features simultaneously. For example, a doctor orders a set of lab tests when s/he sends the
patient to a lab, such as blood count, cholesterol level, etc. rather than ordering a single test,
waiting for its result and ordering the next one. However, the traditional greedy strategy
cannot handle reasoning with sets of features naturally.
We would like to be able to reason with sets of features for these two reasons. Our
objective in this article is, given an existing potentially empty set of already observed
features E and their observed values e, find the set that has the highest benefit:
L(X | e) , argmax Benef it(S | e)

(7)

SX\E

There are two problems with this formulation: first, the number of subsets of X \ E is
exponential in the size of X \ E, and second, for each set S, we need to take an expectation
over the joint assignments to all features in the set. We address these two problems using
a data structure that we describe next.
75

fiBilgic & Getoor

3. Value of Information Lattice (VOILA)
VOILA makes reasoning with sets of features tractable by reducing the space of possible
sets and allowing sharing of EVI computations between different sets. In this section, we
will first explain how we can reduce the space and then explain techniques for computation
sharing.
3.1 Reducing the Space of Possible Sets
In most domains, there are often complex interactions between the features and the
class label. Contrary to the Naive Bayes assumption, features are often not conditionally
independent given the class label. Some features are useless once some other features are
already acquired. For example a chest X-Ray is typically more determinative than a skin
test for tuberculosis. Similarly, some features are useless alone unless they are accompanied
with other features. For example, a chest pain alone might be due to a variety of sicknesses;
if it is accompanied with high cholesterol, it could indicate a heart disease, whereas if it is
combined with fever, a cold might be more probable. These types of interactions between
the features allow us to reduce the space of candidate feature sets.
As we have mentioned in the problem formulation, we have assumed that we already have
a joint probabilistic model over the features and the class variable, P (Y, X). We will find
these two types of feature interactions by asking probabilistic independence queries using
P (Y, X). Specifically, we assume that we are given a Bayesian network that represents
P (Y, X). The Bayesian network will allow us to find these types of interactions through
standard d-separation algorithms.
Definition 1 A set S  X \ E is irreducible with respect to evidence e if Xi  S, Xi is
not conditionally independent of Y given e and S \ {Xi }.
Given a Bayesian network over X and Y , it is straightforward to check irreducibility through
d-separation (Pearl, 1988).
Proposition 1 Let S0 be a maximal irreducible subset of S with respect to e. Then, EV I(S |
e) = EV I(S0 | e).
Proof: Let S00 = S \ S0 . If S0 is a maximal irreducible set, S0  E d-separates Y and S00 .
Otherwise, we could make S0 larger by including the non-d-separated element(s) from S00 in
S0 . Thus, we have P (Y | e, s) = P (Y | e, S0 , S00 ) = P (Y | e, S0 ). Substitution in Equations
1 and 5 yields the desired property.
Note that under the assumption that C(S0 | e)  C(S | e) for any S0  S, it suffices to
consider only the irreducible sets to find the optimal solution to the objective function in
Equation (7). VOILA is a data structure that contains only the irreducible feature subsets
of X, with respect to a particular set of evidence e. We next define VOILA formally.
Definition 2 A VOILA V is a directed acyclic graph in which there is a node corresponding
to each possible irreducible set of features, and there is a directed edge from a feature set S
to each node that corresponds to a direct (maximal) subset of S. Other subset relationships
in the lattice are then defined through the directed paths in V.
76

fiValue of Information Lattice

(a)

(b)

Figure 3: (a) A simple Bayesian network illustrating dependencies between attributes and
the class variable. (b) The VOILA corresponding to the network.

Figure 3(a) shows a simple Bayesian network and its corresponding VOILA, with respect
to the empty evidence set, is shown in Figure 3(b). Notice that the VOILA contains only the
irreducible subsets given the Bayesian network; for instance, the VOILA does not contain
sets that include both X1 and X2 because X1 d-separates X2 from Y . We also observe that
the number of irreducible subsets is 9 in contrast to 24 = 16 possible subsets. Moreover,
note that the largest subset size is now 3 in contrast to 4. Having smaller feature sets sizes
has a dramatic effect on the value of information calculations. In fact, these savings can
make solving the objective function optimally (Equation (7)) feasible in practice.
3.2 Sharing EVI Calculations
Finding the set S that has the highest Benefit (Equation 6) requires computing EV I(S)
(Equation 5). However, computing EV I(S) requires taking an expectation over all possible
values of the features in S. Moreover, searching for the best set among all the irreducible sets
requires us to compute EVI for all irreducible sets. To make such computations tractable in
practice, VOILA allows computation sharing between its nodes. In this article, we describe
three possible ways of sharing computations between the nodes of VOILA.
77

fiBilgic & Getoor

3.2.1 Subset Relationships
VOILA exploits the subset relationships between different feature sets in order to avoid
computing EVI for some nodes. First of all, if there is a directed path from node S1 to S2
in VOILA, then S1  S2 and thus EV I(S1 | e)  EV I(S2 | e)1 . Now assume that there is
a directed path from Si to Sj and EV I(Si | e) = EV I(Sj | e). Then, all of the nodes on
this path will also have the same EVI, thus we do not need to do the computation for those
subsets. An algorithm that makes use of this observation is given in Algorithm 1.
Algorithm 1: Efficient EVI computation using VOILA.
Input: VOILA V and current evidence E
Output: VOILA updated with correct EVI values
1 for all root node(s) S
2
value  EV I(S | e); ub(S)  value; lb(S)  value
3
ub(descendants(S))  value
4
5
6
7
8
9
10

for all leaf node(s) S
value  EV I(S | e); ub(S)  value; lb(S)  value
lb(ancestors(S))  value
for all node S where lb(S) 6= ub(S)
value  EV I(S | e); ub(S)  value; lb(S)  value
lb(ancestors(S))  value
ub(descendants(S))  value

It is important to point out that all nodes of VOILA are irreducible sets. Unless there are
totally useless features that do not change P (Y ) when observed, then we should not have
any two distinct nodes where the EVI values are exactly equal. However, this statement is
true only if we do not have any context-specific independencies (independencies that hold
only under certain assignments to the variables) in the underlying Bayesian network. In our
description and implementation, we used standard d-separation at the variable level; one
can imagine going one step further and define the irreducible sets through both the variable
level d-separation and context specific independencies.
In order to share computations between different nodes of the lattice, we keep lower
and upper bounds on the EVI of a node. The lower bound is determined by the values of
the descendants of the node whereas the upper bound is determined by the values of its
ancestors. First, we initialize these bounds by computing the value of the information at
the boundary of the lattice, i.e., the root node(s) and the leaf node(s) (lines 16) 2 . Then,
we loop over the nodes whose upper bounds and lower bounds are not equal (line 710),
computing their values and updating the bounds at their ancestors and descendants. The
algorithm terminates when the upper bounds and lower bounds for all the nodes become
tight. The order in which to choose the nodes in line 7 so that the number of sets for which
a value is calculated is minimum is still an open question. A possible heuristic is to perform
1. A superset has always a higher or equivalent EVI (Equation (5)) than its subset.
2. We do not need to compute EVI for all root nodes; it suffices to compute it for the node that corresponds
to the Markov blanket of Y . This will be explained in more detail in the next section.

78

fiValue of Information Lattice

a binary search and choose a middle node on a path between two nodes for which the values
have already been calculated.
3.2.2 Information Pathways at the Underlying Bayesian Network
The second mechanism that VOILA uses to share EVI computations is through the edges
in the underlying Bayesian network. We specifically make use of the following fact:
Proposition 2 For all S1 and S2 , if S1 d-separates Y from S2 with respect to e, then
EV I(S1 | e)  EV I(S2 | e).
Proof: Consider S12 = S1 S2 . Because of the subset relationship, we know that EV I(S12 |
e)  EV I(S1 | e) and EV I(S12 | e)  EV I(S2 | e).
EV I(S12 | e) = EM C(Y | e) 

X

P (s12 | e)EM C(Y | e, s12 )

s12

= EM C(Y | e) 

XX
s1

= EM C(Y | e) 

XX
s1

= EM C(Y | e) 

X

P (s1 , s2 | e)EM C(Y | e, s1 , s2 )

s2

P (s1 , s2 | e)EM C(Y | e, s1 )

s2

P (s1 | e)EM C(Y | e, s1 )

s1

= EV I(S1 | e)
 EV I(S2 | e)
The third line follows from the second by the fact that S1 d-separates Y from S2 and thus
P (Y | s1 , s2 ) = P (Y | s1 ).
Corollary: The Markov blanket of Y , (i.e., Y s parents, Y s children, and Y s childrens
other parents), is the set that has the highest EVI in our search space, as it d-separates all
of the remaining variables from Y . Using this corollary, we do not need to compute the EVI
for all root nodes in Algorithm 1; we can compute EVI for the root node that corresponds
to the Markov blanket of Y and it serves as the upper bound for the EVI of the remaining
root nodes.
These relationships can very well be exploited like we exploited the subset relationships
above. Instead of just using the subset relationships, we can use both subset and independence relationships. One simple way to make use of Algorithm 1 without modification
is to add edges between any S1 and S2 where the independence property holds. An example S1 and S2 according to our toy network in Figure 3(a) would be S1 = {X1 } and
S2 = {X2 }. Thus, we can add a directed edge from X1 to X2 in our VOILA in Figure 3(b)
and Algorithm 1 will work just fine.
3.2.3 Incremental Inference
The third and the last mechanism that VOILA uses for computation sharing is through
caching of probabilities at its nodes. For each candidate set S  V, we need to compute
EV I(S | e) which requires computing P (S | e) and EM C(Y | S, e). If we cache the
79

fiBilgic & Getoor

conditional probabilities at each node of V, then to compute
P (S | e), we find one of its
P
supersets Si = S  {Xi } and then compute P (S | e) = xi P (S, Xi = xi | e).
Computing EM C(Y | S, e) requires computing P (Y | S, e). To perform this computation efficiently, we cache the state of the junction tree at each node of the VOILA. Then, we
find a subset, Sj , such that S = Sj  {Xj }. We compute P (Y | S, e) by integrating the
extra evidence to the junction tree at node Sj that is used to compute P (Y | Sj , e).
3.3 Constructing VOILA
Efficient construction of VOILA is not a straightforward task. The brute force approach
would be to enumerate all possible subsets of X \ E and for each subset check whether it is
irreducible. However, this brute force approach is clearly impractical. Because the number
of nodes in VOILA is expected to be much fewer than the number of possible subsets of X\E,
if we can be smart about which sets we consider for inclusion in V, we can construct it
more efficiently. That is, instead of generating all possible candidates and checking whether
they are irreducible or not, we try to generate only irreducible sets. We first introduce the
notion of a dependency constraint and then explain how we can use dependency constraints
to efficiently construct VOILA.
Definition 3 A dependency constraint for a feature Xi  S with respect to S and E is the
constraint on S  E that ensures a dependency between Xi and Y exists.
For instance, in our running example, a dependency constraint for X2 is X1 ; in other
words, in order for X2 to be relevant, X1 should not be included in S  E. Similarly, the
dependency constraint for X4 is X3 , meaning that X3 must be included in SE. Specifically,
a dependency constraint for a feature Xi requires that all Xj on the path from Y to Xi
not to be included in S  E if Xj is not part of a v-structure; if Xj is part of a v-structure,
then either Xj or one of its descendants must be included in S  E (we refer to these latter
constraints as positivity constraints). The algorithm that uses these ideas to compute the
dependency constraints for each feature is given in Algorithm 2.
Algorithm 2: Dependency constraint computation for Xi .
Input: Xi , Y
Output: Dependency constraint for Xi , denoted DC(Xi )
1 DC(Xi )  false
2 for each undirected path pj between Xi and Y
3
DCj (Xi )  true
4
for each Xk on the path pj
5
if Xk does not a cause a v-structure then
6
DCj (Xi )  DCj (Xi )  Xk
7
else
8
DCj (Xi )  DCj (Xi )  (Xk  Descendants(Xk ))
9

DC(Xi )  DC(Xi )  DCj (Xi )

80

fiValue of Information Lattice

These dependency constraints can be used to check whether a set is irreducible or
potentially irreducible. Intuitively, a set is potentially irreducible if it is not irreducible but
it is possible to make the set irreducible by adding more features into it. More formally,
Definition 4 A set S  X \ E is potentially irreducible with respect to evidence e if,
S is not irreducible but there exists a non-empty set of features S0  X \ {E  S} such that
S  S0 is irreducible.
Potential irreducibility is possible due to the non-monotonic nature of d-separation. That is,
a feature that is d-separated from Y can become dependent if we consider it in combination
with other features. For example, in our running example, {X4 } is not irreducible, as X4
is d-separated from Y , whereas {X3 , X4 } is irreducible.
We use the dependency constraints to check whether a set is irreducible or potentially
irreducible. Because a set S is irreducible only if a dependency between all of its elements
and Y exists, the dependency constraint for the set S is the conjunction of the dependency
constraints of its members. The irreducibility of S can be checked by setting the elements
of S and E to true and setting the remaining elements of X to false and evaluating the
sets dependency constraint. In our running example, the dependency constraint for the set
{X2 , X4 } is X1  X3 . Assuming E = , when we set the members of {X2 , X4 } to true, and
set the remaining features, X1 and X3 , to false, X1  X3 then evaluates to false and thus
this set is not irreducible. This makes sense because given no evidence, X4 is independent
of Y , so while {X2 } is a useful feature set to consider for acquisition, {X2 , X4 } is not.
Checking for potential irreducibility is very similar. Set the elements of S and E to true
like we did above. Then, set the positivity constraints of the members of S to true. Finally,
set everything else to false. Using the same example above, to check whether {X2 , X4 } is
potentially irreducible, set X2 = true, X4 = true. Also set X3 = true because it is the
positivity constraint for X4 . Set the remaining features, that is X1 , to false. Evaluating the
constraint X1  X3 yields to true, showing that {X2 , X4 } is potentially irreducible (while
it was not irreducible).
Given the definitions of irreducibility and potential irreducibility and the mechanisms to
check for these properties through the notion of dependency constraints, we next describe
the algorithm to construct VOILA.
VOILA construction proceeds in a bottom up fashion, beginning with the lowest level,
which initially contains only the empty set and constructs new irreducible feature sets by
adding one feature at a time into the VOILA structure. Algorithm 3 gives the details of
the algorithm. The algorithm keeps track of the irreducible feature sets IS, and the set
of potentially irreducible feature sets PS. When we are done processing feature Xij , we
remove from PS any potentially irreducible set that cannot become irreducible because Xij
will not be re-considered (line 11).
3.3.1 Analysis of VOILA Construction Algorithm
The construction algorithm inserts a node into the VOILA only if the corresponding set
is irreducible (lines 6 and 7). Moreover, by keeping track of potentially irreducible sets
(lines 810), we generate every possible irreducible set that can be generated. Thus, VOILA
contains only and all of the possible irreducible subsets of X.
81

fiBilgic & Getoor

Algorithm 3: The VOILA construction algorithm.
Input: Set of features X and class variable Y .
Output: The VOILA data structure V, given E.
1 Pick an ordering of elements of X = Xi1 , Xi2 , . . . , Xin
2 IS  {}; PS  
3 for j = 1 to n
4
for each S  IS  PS
5
S0  S  Xij ; DC(S0 )  DC(S)  DC(Xij )
6
if S0 is irreducible then
7
IS  IS  {S0 }; Add a node corresponding to S0 to V
8
else
9
if S0 is potentially irreducible then
10
PS  PS  {S0 }
11
12
13
14
15
16
17

Remove from PS all sets that are no longer potentially irreducible
max = size of largest S in IS; Ll = {S | S  IS and |S| = l}
for l = 0 to max  1
for each S  Ll
for each S0  Ll+1
if S  S0 then
Add an edge from S0 to S to V

The worst-case running time of the algorithm is still exponential in the number of
initially unobserved features, X \ E, because number of irreducible sets can potentially
be exponential. The running time in practice, though, depends on the structure of the
Bayesian network that the VOILA is based upon and the ordering of the variables in line 1.
For example, if the Bayesian network is naive Bayes, then all subsets are irreducible (no
feature d-separates any other feature from the class variable); thus, the search space cannot
be reduced at all. However, naive Bayes makes extremely strong assumptions which are
unlikely to hold in practice. In fact, as we empirically show in the experiments section on five
real-world datasets, features often are not conditionally independent given the class variable;
there are more complex interactions between them and thus the number of irreducible
subsets is substantially smaller than the number of all possible subsets.
The for loop at line 4 iterates over each irreducible and potentially irreducible sets
that have been generated so far, and the number of potentially-irreducible sets generated
depends on the ordering chosen. A good ordering processes features with literals with
positivity constraints in other features dependency constraints earlier. That is, for each
undirected path from Y to Xi that includes Xj in a v-structure, a good ordering puts Xj
earlier in the ordering than everything between Xj and Xi . For instance, in our sample
Bayesian network in Figure 3(a), we should consider X3 earlier than X4 . We refer to an
ordering as perfect if it satisfies all the positivity constraints. If a perfect ordering is used,
VOILA construction algorithm never generates a potentially irreducible set. Unfortunately, it
82

fiValue of Information Lattice

is not always possible to find a perfect ordering. A perfect ordering is not possible when two
features have each other as a positivity constraint literal in their dependency constraints.
This case occurs only when there is a loop from Y to Y that has two or more v-structures
(Note that even though a Bayesian network is a directed acyclic graph, it can still contain
loops, i.e., undirected cycles). A perfect ordering was possible in four of the five real world
datasets that we used.
3.4 Using VOILA for Feature-value Acquisition
VOILA makes searching the space of all possible subsets tractable in practice. Using
this flexibility, it is possible to devise several different acquisition policies. We describe two
policies as example policies in this section.
The first acquisition policy aims to capture the practical setting where more than one
feature is acquired at once. The policy can be constructed using VOILA as follows. Each
path ps of the policy  (which is initially empty) is repeatedly extended by acquiring the
set S0  V that has the best Benef it(S0 | s, e). The policy construction ends when no path
can be extended, i.e., all candidate sets have non-positive Benefit values for each path of .
The second acquisition policy adds a look-ahead capability to the greedy policy. That
is, rather than repeatedly extending each path ps of policy  with the feature Xi that has
the highest Benef it(Xi | s, e), we add a look-ahead capability, and first find the set S0  V
that has the highest Benef it(S0 | s, e). Then, instead of acquiring all features in S0 all
at once, like we did in the above policy, we find the feature Xi  S0 that has the highest
Benef it(Xi | s, e) and acquire it to extend ps .

4. Experiments
We experimented with five real-world medical datasets that Turney (1995) described
and used in his paper. These datasets are Bupa Liver Disorders, Heart Disease, Hepatitis,
Pima Indians Diabetes, and Thyroid Disease, which are all available from the UCI Machine
Learning Repository (Frank & Asuncion, 2010). The datasets had a varying number of
features ranging from five to 20. Four out of five datasets had binary labels, whereas the
Thyroid dataset had three labels.
For each dataset, we first learned a Bayesian Network that both provides the joint
probability distribution P (Y, X) and efficiently answers conditional independence queries
thorough d-separation (Pearl, 1988). We built a VOILA for each dataset using the learned
Bayesian Network. We first present statistics on each dataset, such as the number of features
and number of nodes in the VOILA, and then compare various acquisition policies.
4.1 Search Space Reduction
Table 1 shows aggregate statistics about each dataset, describing the number of features,
the number of all possible subsets, the number of subsets in VOILA, and the percent reduction
in the search space. As this table shows, the number of irreducible subsets is substantially
fewer than all possible subsets. For the Thyroid Disease dataset, for example, the number
of possible subsets is over a million whereas the number of irreducible subsets is fewer than
83

fiBilgic & Getoor

Table 1: Aggregate statistics about each dataset. The number of irreducible subsets, i.e.,
the number of nodes in VOILA, is substantially fewer than the number of all possible
subsets.
Dataset
Bupa Liver Disorders
Pima Indians Diabetes
Heart Disease
Hepatitis
Thyroid Disease

Features

All Subsets

Nodes in VOILA

Reduction

5
8
13
19
20

32
256
8,192
524,288
1,048,576

26
139
990
18,132
28,806

19%
46%
88%
97%
97%

thirty thousand. This enormous reduction in the search space makes searching through the
possible sets of features tractable in practice.
4.2 Expected Total Cost Comparisons
We compared the expected total costs (Equation 2) of four different acquisition policies
for each dataset. These policies are as follows:
 No Acquisition: This policy does not acquire any features; it aims to minimize the
expected misclassification cost based on the prior probability distribution of the class
variable, P (Y ).
 Markov Blanket: This policy acquires every relevant feature, regardless of the misclassification costs. The Market Blanket of Y in a Bayesian network is defined as Y s
parents, children, and its childrens other parents (Pearl, 1988). Intuitively, it is the
minimal set S  X such that Y  (X \ S) | S.
 Greedy: This policy repeatedly expands each path ps of an initially empty policy 
by acquiring the feature Xi that has the highest positive Benef it(Xi | s) (Equation
4). The policy construction ends when no path can be extended with a feature with
a positive Benefit value.
 Greedy-LA: This policy adds a look-ahead capability to the Greedy strategy. This
policy repeatedly expands each path ps of an initially empty policy  by first finding
the set S0 that has the highest positive Benef it(S0 | s) (Equation 6) and then acquiring
the feature Xi  S0 that has the maximum Benef it(Xi | s) (Equation 4). The policy
construction ends when no set with a positive Benefit value can be found for any path
of the policy.
The feature costs for each dataset are described in detail by Turney (1995). In summary,
each feature can either have an independent cost, or can belong to a group of features, where
the first feature in that group incurs an additional cost. For example, the first feature from
a group of blood measurements incurs the overhead cost of drawing blood from the patient.
The feature costs are based on the data from Ontario Ministry of Health (1992).
84

fiValue of Information Lattice

Table 2: Example misclassification cost matrix (cij ) for the symmetric and asymmetric misclassification costs. cij are set in way to achieve a prior expected misclassification
cost of 1. In the symmetric cost case, choosing the most probable class leads
to EM C = 1, whereas, in the asymmetric cost case, the choosing either class is
indifferent and both leads to the same EMC of 1.
Actual Class

Prior Probability

Pred. Class

Symm. Cost

Asymm. Cost

y1

P (y1 ) = 0.6510

y1
y2

0
2.866

0
2.866

y2

P (y2 ) = 0.3490

y1
y2

2.866
0

1.536
0

We observed that most of the features were assigned the same cost. For example, four
out of five features in the Bupa Liver Disorders dataset, 13 out of 19 features in the Hepatitis
dataset, six out of eight features in the Diabetes dataset, and 16 out of 20 features in the
Thyroid Disease dataset were assigned the same cost. When the costs are so similar, the
problem is practically equivalent to finding the minimum size decision tree. To provide more
structure into the feature acquisition costs, we also experimented with randomly generated
feature and group costs. For each feature, we randomly generated a cost between 1 and 100,
and for each group we generated a cost between 100 and 200. We repeated the experiments
with three different seeds for each dataset.
The misclassification costs were not defined in the paper by Turney (1995). One reason
could be that it is easier to define the feature costs, but defining the cost of a misclassification can be non-trivial. Instead, Turney tests different acquisition strategies using
various misclassification costs. We follow a similar technique with a slight modification. We
compare the above acquisition policies under both symmetric (cij = cji ) and asymmetric
misclassification costs. To be able to judge how the misclassification cost structure affects
feature acquisition, we unify the presentation, and compare different acquisition strategies
under the same a priori expected misclassification costs, as defined in Equation (1). Specifically, we compare the acquisition policies under various a priori EMC that are achieved by
varying the cij accordingly. We show an example misclassification table for an EMC value
of 1 in Table 2. For the real feature cost case, we varied the EMC between 0 and 2000, and
varied it from 0 to 4000 for the synthetic feature cost case.
We compare the Greedy, Greedy-LA, and Markov Blanket policies by plotting how
much cost each policy saves with respect to the No Acquisition policy. In the X axis
of the plots, we vary a priori expected misclassification cost using the methodology we
described above. We plot the savings on the Y axis. For each dataset, we plot four different
scenarios: the cross product of {symmetric, asymmetric} misclassification costs, and {real,
synthetic} feature costs.
The results for the Liver Disorders, Diabetes, Heart Disease, Hepatitis, and Thyroid
Disease are given in Figures 4, 5, 6, 7, and 8 respectively. For each figure, symmetric
misclassification cost scenarios are given in sub-figures (a) and (c), whereas the asymmetric
85

fiBilgic & Getoor

(a)

(b)

(c)

(d)

Figure 4: Expected Total Cost (ETC ) comparisons for the Bupa Liver Disorders dataset.
The a priori class distribution is as follows: P (Y ) = [0.4959, 0.5041].

misclassification cost scenarios are presented in (b) and (d). Similarly, the real feature cost
scenarios are given in (a) and (b) and the synthetic feature cost scenarios are presented in
(c) and (d). We next summarize the results.
 We found that the Greedy policy often prematurely stopped acquisition, performing
even worse than the Markov Blanket strategy. This is true for most of the datasets,
regardless of the feature and misclassification cost structures. The fact that the Greedy
strategy can perform worse than Markov Blanket strategy is really troubling. At first,
it might seem rather unintuitive that Greedy strategy can perform worse than Markov
Blanket strategy. Part of the reason is that the features belong to groups and the first
feature from its group incurs an overhead cost. In Greedy strategy where each feature
is considered in isolation, the overhead costs can outweigh each single features benefit,
and because Greedy does not look ahead, it is reluctant to commit to acquiring the
first feature from any group.
86

fiValue of Information Lattice

(a)

(b)

(c)

(d)

Figure 5: Expected Total Cost (ETC ) comparisons for the Pima Indian Diabetes dataset.
A priori class distribution is as follows: P (Y ) = [0.6510, 0.3490].

 Greedy-LA strategy never performs worse than any other strategy under any setting.
 The misclassification cost structure (symmetric or asymmetric) had a considerable
effect on how the policies behaved. The differences between symmetric and asymmetric cases were particularly evident for datasets where the class distribution was more
imbalanced, such as the Diabetes (Figure 5), Hepatitis (Figure 7), and the Thyroid
Disease (Figure 8) datasets. The differences due to the misclassification cost structure
can be summarized as follows:
 When the class distribution is imbalanced and the misclassification cost is symmetric, acquiring more information cannot change the classification decisions
easily due to the class imbalance, thus the features do not have high EVI values.
On the other hand, if the misclassification costs are asymmetric, features tend
to have higher EVI values. Thus, the Greedy and Greedy-LA strategies start
acquiring features earlier in the X axis for the asymmetric cases compared to
87

fiBilgic & Getoor

(a)

(b)

(c)

(d)

Figure 6: Expected Total Cost (ETC ) comparisons for the Heart Disease dataset. A priori
class distribution is as follows: P (Y ) = [0.5444, 0.4556].

their symmetric counterparts. For example, for the Thyroid disease dataset with
real feature costs, the Greedy strategy starts acquisition only when the EMC is
greater than 600 for symmetric misclassification costs (Figure 8(a)) whereas it
starts acquiring when the EMC reaches only 100 for the asymmetric case (Figure 8(b)). For the synthetic feature costs, the results are more dramatic; neither
Greedy or Greedy-LA acquires any features for the symmetric cost case (Figure 8(c)), whereas they start acquisition when EM C = 200 for the asymmetric
case (Figure 8(d)).
 In the same realm with the above results, the slope of the savings for the asymmetric case is much higher compared to the symmetric case.
 The misclassification cost structure causes differences between the Greedy and
Greedy-LA policies in a few cases. For the Diabetes dataset Greedy policy performs worse when the misclassification costs are symmetric (Figures 5(a) and
88

fiValue of Information Lattice

(a)

(b)

(c)

(d)

Figure 7: Expected Total Cost (ETC ) comparisons for the Hepatitis dataset. A priori class
distribution is as follows: P (Y ) = [0.7908, 0.2092].

5(c)), whereas for the Hepatitis dataset, it performs worse for the asymmetric
misclassification costs (Figures 7(b) and 7(d)).
 The Greedy policy sometimes has an erratic, unpredictable, and unreliable performance as the expected misclassification changes. It possibly hits a local minima, gets
out of it later, and hits local minima again (Figures 6 and 8(d)).
We finally present an aggregate summary of the results in Table 3. Table 3 shows how
much the Greedy policy and the Greedy-LA policy saves over the Markov Blanket policy.
The results are presented as the average saving over various intervals, such as [0-500). As
this table also shows, the Greedy-LA policy never loses compared to the Markov Blanket
policy, as one would expect. Additionally, the Greedy-LA policy wins over the Greedy
policy for most of the cases, and it never looses. Finally, Greedy policy prematurely stops
acquisition, having negative savings with respect to the Markov Blanket strategy.
89

fiBilgic & Getoor

(a)

(b)

(c)

(d)

Figure 8: Expected Total Cost (ETC ) comparisons for the Thyroid Disease dataset. A
priori class distribution is as follows: P (Y ) = [0.0244, 0.0507, 0.9249].

5. Related Work
Decision theoretic value of information calculations provide a principled methodology
for information gathering in general (Howard, 1966; Lindley, 1956). Influence diagrams,
for example, are popular tools for representing decisions and utility functions (Howard &
Matheson, 1984). However, because devising the optimal acquisition policy (i.e., constructing the optimal decision tree) is intractable in general, most of the approaches to feature
acquisition have been myopic (Dittmer & Jensen, 1997), greedily acquiring one feature at
a time. The greedy approaches typically differ in i) the problem setup they assume, ii)
the way the features are scored, and iii) the classification model being learned. We review
existing work here, highlighting the differences between different techniques in these three
dimensions.
Gaag and Wessels (1993) consider the problem of evidence gathering for diagnosis
using a Bayesian Network. In their setup, they gather evidence (i.e., observe the values of
the variables) until the hypothesis is confirmed or disconfirmed to a desired extent. They
90

fiValue of Information Lattice

Table 3: Savings of Greedy (GR) and Greedy-LA (LA) with respect to the Markov Blanket
policy, averaged over different intervals. An entry is in bold if it is worse than
Greedy-LA, and it is in red if it is worse than Markov Blanket.

Liver
GR

LA

Diabetes
GR
LA

Heart
GR

LA

Hepatitis
GR
LA

Thyroid
GR
LA

Real Feature Costs & Symmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000]

6.77
-18.84
-42.12
-67.59

9.08
2.70
2.66
2.85

15.49
-18.28
-48.35
-81.43

24.27
17.06
17.35
17.34

240.59
121.31
79.07
-24.98

243.31
144.87
116.68
111.34

4.19
-6.06
-14.32
-23.40

5.86
3.90
3.90
3.85

28.07
13.90
13.41
13.41

28.07
13.90
13.41
13.41

5.84
2.57
2.57
2.57

17.7
1.56
1.56
1.56

17.7
1.56
1.56
1.56

231.93
106.54
96.39
88.14
79.88
71.63
68.67
63.66

298.01
277.70
257.40
237.09
216.79
196.48
176.18
153.84

298.01
277.70
257.40
237.09
216.79
196.48
176.18
153.84

276.32
213.60
162.52
113.82
65.12
28.72
0.78
-18.10

276.32
213.60
162.52
113.82
68.39
34.73
14.67
9.50

Real Feature Costs & Asymmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000]

7.33
-16.78
-38.26
-61.88

9.3
2.66
3.04
2.97

22.74
9.85
3.99
-2.54

23.84
13.31
11.7
13.7

245.79
131.36
46.20
-40.96

245.79
143.3
114.23
107.14

-9.55
-47.61
-84.79
-125.69

Synthetic Feature Costs & Symmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000)
[2000-2500)
[2500-3000)
[3000-3500)
[3500-4000]

307.39
160.95
60.30
31.86
10.43
-14.60
-39.64
-67.18

307.39
160.95
79.76
53.97
53.01
62.66
59.96
63.68

418.34
245.75
163.80
138.78
108.69
78.90
48.83
15.75

418.34
288.65
224.09
163.45
163.78
164.75
172.76
172.13

723.36
579.25
444.42
378.43
364.03
268.00
171.91
109.91

723.36
585.72
539.88
490.23
482.24
458.89
422.06
412.11

231.93
63.59
96.39
88.14
79.88
71.63
63.38
54.30

Synthetic Feature Costs & Asymmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000)
[2000-2500)
[2500-3000)
[3000-3500)
[3500-4000]

306.19
156.78
66.57
37.47
14.84
-9.19
-33.22
-59.66

306.19
156.78
79.79
60.62
55.70
58.85
59.33
63.13

441.29
341.28
260.09
201.19
161.24
144.24
132.84
126.43

441.29
341.28
261.21
204.31
164.17
151.22
139.54
136.51

728.57
599.02
505.80
420.56
320.32
211.26
248.73
206.06

728.57
603.12
517.37
519.29
512.75
500.75
400.16
389.32

219.04
88.34
-16.86
-54.31
-64.75
-101.93
-139.11
-180.01

219.04
91.53
49.39
61.90
61.90
61.90
61.90
61.90

propose an acquisition algorithm that greedily computes the expected utility of acquiring a
feature and chooses the one with the highest utility. They define the utility as the absolute
value of the change in the probability distribution of the hypothesis being tested.
In more recent work, Sent and Gaag (2007) consider the problem of acquiring more than
a single feature at each step. They define subgoals and cluster the features for each subgoal.
The subgoals and clustering of the features are provided by the domain experts. Then, they
in the non-myopic case, they pick a cluster by calculating their expected values. However,
91

fiBilgic & Getoor

because clusters can be big, calculating the expected value of a cluster can be problematic;
thus, they also provide a semi-myopic algorithm where they pick the cluster that has the
best (myopic) feature.
Nunez (1991) introduces a decision tree algorithm called EG2 that is sensitive to the
feature costs. Rather than splitting the decision tree at a feature that has high information
gain, EG2 chooses a feature that has least information cost function, which is defined as
the ratio of a features cost to its discriminative efficiency. EG2 is, however, is not directly
optimized to balance the misclassification cost and feature acquisition cost; rather it is
optimized for 0/1 loss while taking the feature costs into account. Similarly, Tan (1990)
modifies the ID3 algorithm (Quinlan, 1986) to account for feature costs. Tan considers the
domain where a robot needs to sense, recognize, and act, and the number of features is very
large. For the robot to act efficiently, it needs to trade-off accuracy for efficiency.
Turney (1995) builds a decision tree called ICET (standing for Inexpensive Classification
with Expensive Tests) using a genetic search algorithm (Grefenstette, 1986) and using
Nunezs (1991) criteria to build C4.5 decision trees (Quinlan, 1993). Unlike Nunez, Turney
takes misclassification costs into account (in addition to the feature costs) to evaluate a
given decision tree and looks for a good decision tree using genetic search algorithms.
Yang et al. (2006) build cost-sensitive decision trees and Naive Bayes classifiers that
take both feature costs and misclassification costs into account. Unlike Nunez (1991), who
scores features based on information gain and cost ratio, Yang et al. score features based on
expected reduction in the total cost (i.e., sum of the feature cost and the misclassification
cost) on the training data. By doing so, they take feature costs and misclassification costs
into account directly at learning time.
Bayer-Zubek (2004) formulates the feature acquisition problem as a Markov Decision
Process and provides both greedy and systematic search algorithms to develop diagnostic
policies. Bayer-Zubek takes both feature cost and misclassification costs into account and
automatically finds an acquisition plan that balances the two costs. She introduces an
admissible heuristic for AO* search and describes regularization techniques to reduce overfitting to the training data.
Saar-Tsechansky, Melville, and Provost (2009) consider active feature acquisition for
classifier induction. Specifically, they are given a training data with missing feature values, and a cost matrix that defines the cost of acquiring each feature value, they describe
an incremental algorithm that can select the best feature to acquire iteratively to build a
model that is expected to have high future performance. The utility of acquiring a feature
is estimated in terms of expected performance improvement per unit cost. The two characteristics that make this work different from most of the previous work is that i) the authors
do not assume a fixed budget a priori; rather they build the model incrementally, ii) each
feature can have a different cost for each instance.
Finally, Greiner, Grove, and Roth (2002) analyze the sample complexity of dynamic
programming algorithms that performs value iteration to search for the best diagnostic
policies. They analyze the problem of learning the optimal policy, using a variant of the
probably-approximately-correct (PAC) model. They show that the learning can be achieved
efficiently when the active classifier is allowed to perform only (at most) a constant number
of tests and show that learning the optimal policy is often intractable in more general
environments.
92

fiValue of Information Lattice

6. Future Work
In this article, we have only scratched the surface of incorporating constraints between
features in order to reduce the search space and make reasoning with sets tractable. We
have discovered two types of constraints (features that render other features useless, and
features that are useless without other features) purely from the underlying probability
distribution. We have shown that these automatically discovered constraints helped reduce
the search space dramatically. In practice, it is possible to discover additional types of
constraints that can potentially be used reduce the search space further (for e.g., ordering
constraints where certain procedures always precede other procedures). Constraints can
also be defined based on observed feature values; for example, a treadmill test might not be
performed for patients of old age. Patients can decline certain procedures and medications.
Eliciting these constraints from the domain experts and utilizing them to further reduce
the search space is a promising future direction.
Most of the existing feature acquisition frameworks, including this one, are a major
simplification of what happens in practice; we have assumed that acquiring the values of the
features does not change the class value or values of other variables. However, in practice,
feature value measurements can have side-effects, for example, in medical diagnosis while
certain measurements are non-invasive and do not change the status of the patient, others
might include medications that can affect the outcome. Similarly, in fault diagnosis and
repair, the purpose is not only to diagnose but it is to repair the fault, so some actions can
in fact repair the fault, in essence changing the class value. Taking these extra side-effects
into account will make feature acquisition frameworks more realistic.

7. Conclusion
The typical approach to feature acquisition has been greedy in the past primarily due to
the sheer size of the possible subsets of features. We described a general technique that can
optimally prune the search space by exploiting the conditional independence relationships
between the features and the class variable. We empirically showed that exploiting the conditional independence relationships can substantially reduce the number of possible subsets.
We also introduced a novel data structure called Value of Information Lattice (VOILA) that
can both efficiently reduce the search space using the conditional independence relationships and also can share probabilistic inference computations between different subsets of
features. By using VOILA, we were able to add a full look-ahead capability to the greedy
acquisition policy, which would not be practical otherwise. We experimentally showed on
five real-world medical datasets that the greedy strategy often stopped feature acquisition
prematurely, performing worse than even a policy that acquires all the features.

Acknowledgments
We thank the reviewers for their helpful and constructive feedback. This material is
based on work supported by the National Science Foundation under Grant No. 0746930.
93

fiBilgic & Getoor

References
Bayer-Zubek, V. (2004). Learning diagnostic policies from examples by systematic search.
In Annual Conference on Uncertainty in Artificial Intelligence.
Bilgic, M., & Getoor, L. (2007). VOILA: Efficient feature-value acquisition for classification.
In AAAI Conference on Artificial Intelligence, pp. 12251230.
Dittmer, S., & Jensen, F. (1997). Myopic value of information in influence diagrams. In
Annual Conference on Uncertainty in Artificial Intelligence, pp. 142149.
Frank, A., & Asuncion, A. (2010). UCI machine learning repository..
Gaag, L., & Wessels, M. (1993). Selective evidence gathering for diagnostic belief networks.
AISB Quarterly, pp. 2334.
Grefenstette, J. (1986). Optimization of control parameters for genetic algorithms. IEEE
Transactions on Systems, Man and Cybernetics, 16 (1), 122128.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137174.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). An approximate nonmyopic computation for value of information. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15 (3), 292298.
Howard, R. A., & Matheson, J. E. (1984). Readings on the Principles and Applications of
Decision Analysis, chap. Influence Diagrams. Strategic Decision Group.
Howard, R. A. (1966). Information value theory. IEEE Transactions on Systems Science
and Cybernetics, 2 (1), 2226.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees is NPComplete. Information Processing Letters, 5 (1), 1517.
Lindley, D. V. (1956). On a measure of the information provided by an experiment. Annals
of Mathematical Statistics, 27, 9861005.
Nunez, M. (1991). The use of background knowledge in decision tree induction. Machine
Learning, 6 (3), 231250.
of Health, O. M. (1992). Schedule of benefits: Physician services under the health insurance
act..
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Francisco.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1 (1), 81106.
Quinlan, J. R. (1993). C4.5: programs for machine learning. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.
Saar-Tsechansky, M., Melville, P., & Provost, F. (2009). Active feature-value acquisition.
Management Science, 55 (4), 664684.
Sent, D., & Gaag, L. C. (2007). Enhancing automated test selection in probabilistic networks. In Proceedings of the 11th conference on Artificial Intelligence in Medicine,
pp. 331335.
94

fiValue of Information Lattice

Tan, M. (1990). CSL: A cost-sensitive learning system for sensing and grasping objects. In
IEEE International Conference on Robotics and Automation.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation of a hybrid genetic
decision tree induction algorithm. Journal of Artificial Intelligence Research, 2, 369
409.
Yang, Q., Ling, C., Chai, X., & Pan, R. (2006). Test-cost sensitive classification on data
with missing values. IEEE Transactions on Knowledge and Data Engineering, 18 (5),
626638.

95

fiJournal of Artificial Intelligence Research 41 (2011) 155-229

Submitted 01/11; published 06/11

Analyzing Search Topology Without Running Any Search:
On the Connection Between Causal Graphs and h+
Jorg Hoffmann

joerg.hoffmann@inria.fr

INRIA
Nancy, France

Abstract
The ignoring delete lists relaxation is of paramount importance for both satisficing and
optimal planning. In earlier work, it was observed that the optimal relaxation heuristic
h+ has amazing qualities in many classical planning benchmarks, in particular pertaining
to the complete absence of local minima. The proofs of this are hand-made, raising the
question whether such proofs can be lead automatically by domain analysis techniques.
In contrast to earlier disappointing results  the analysis method has exponential runtime
and succeeds only in two extremely simple benchmark domains  we herein answer this
question in the affirmative. We establish connections between causal graph structure and
h+ topology. This results in low-order polynomial time analysis methods, implemented in
a tool we call TorchLight. Of the 12 domains where the absence of local minima has been
proved, TorchLight gives strong success guarantees in 8 domains. Empirically, its analysis
exhibits strong performance in a further 2 of these domains, plus in 4 more domains where
local minima may exist but are rare. In this way, TorchLight can distinguish easy domains
from hard ones. By summarizing structural reasons for analysis failure, TorchLight also
provides diagnostic output indicating domain aspects that may cause local minima.

1. Introduction
The ignoring delete lists relaxation has been since a decade, and still is, of paramount
importance for effective satisficing planning (e.g., McDermott, 1999; Bonet & Geffner, 2001;
Hoffmann & Nebel, 2001a; Gerevini, Saetti, & Serina, 2003; Helmert, 2006; Richter &
Westphal, 2010). More recently, heuristics making this relaxation have also been shown
to boost optimal planning (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009). The
planners using the relaxation approximate, in a variety of ways, the optimal relaxation
heuristic h+ which itself is NP-hard to compute (Bylander, 1994). As was observed in
earlier work (Hoffmann, 2005), h+ has some rather amazing qualities in many classical
planning benchmarks. Figure 1 gives an overview of these results.1
The results divide domains into classes along two dimensions. We herein ignore the horizontal dimension, pertaining to dead ends, for which domain analysis is already available:
easy-to-test powerful criteria implying that a task is undirected/harmless are known
(e.g., Hoffmann, 2005). The vertical dimension divides the domains into three classes, with
respect to the behavior of exit distance, defined as d  1 where d is the distance to a state
with strictly smaller h+ value. In the easiest bottom class, there exist constant upper
1. We omit ADL domains, and we add the more recent IPC benchmarks Elevators and Transport (without
action costs), for which these properties are trivial to prove based on the earlier results. Blocksworld-Arm
is the classical blocksworld, Blocksworld-NoArm is a variant allowing to move A from B to C directly.
c
2011
AI Access Foundation. All rights reserved.

fiHoffmann

PipesworldTank
PipesworldNoTank
PSR

Rovers
OpticalTelegraph

Mystery
Mprime
Freecell
Airport

Hanoi [0]
BlocksworldNoArm [0]
Grid [0]
Transport [0]
bench ed <= c

local minima ed <= c

BlocksworldArm
Depots
Driverlog

Elevators [0,1]
Logistics [0,1]
Ferry [0,1]
Gripper [0,1]
undirected

Tyreworld [0,6]
Satellite [4,4]
Zenotravel [2,2]
MiconicSTRIPS [0,1]
Movie [0,1]
SimpleTsp [0,0]
harmless

DiningPhil. [31,31]

recognized

unrecognized

Figure 1: Overview of h+ topology (Hoffmann, 2005).
bounds on exit distance from both, states on local minima and states on benches (flat regions). In the figure, the bounds are given in square brackets. For example, in Logistics,
the bound for local minima is 0  meaning that no local minima exist at all  and the bound
for benches is 1. In the middle class, a bound exists only for local minima; that bound is
0 (no local minima at all) for all domains shown. In the hardest top class, both local
minima and benches may take arbitrarily many steps to escape.
The proofs underlying Figure 1 are hand-made. For dealing with unseen domains,
the question arises whether we can design domain analysis methods leading such proofs
automatically. The potential uses of such analysis methods are manifold; we discuss this at
the end of the paper. For now, note that addressing this question is a formidable challenge.
We are trying to automatically infer properties characterizing the informativeness (or lack
thereof ) of a heuristic function. We wish to do this based on a static analysis, not actually
running any search. Formally characterizing the informativeness of a heuristic function
is, in most cases, hardly possible even for experienced researchers, which explains perhaps
why no-one so far has even attempted to do it automatically. The single exception, to
the best of the authors knowledge, is an analysis method mentioned on the side in the
authors earlier work (Hoffmann, 2005). This analysis method builds an exponentially
large tree structure summarizing all ways in which relaxed plans may generate facts. The
tree size, and therewith the analysis runtime, explodes quickly with task size. Worse, the
analysis succeeds only in Movie and Simple-TSP  arguably the two most simplistic planning
benchmarks in existence.2
By contrast, the TorchLight tool developed herein has low-order polynomial runtime and
usually terminates in split seconds. Distinguishing between global (per task) and local (per
state) analysis, it proves the global absence of local minima in Movie, Simple-TSP, Logistics,
and Miconic-STRIPS. It gives a strong guarantee for local analysis  to succeed in every state
 in Ferry, Gripper, Elevators, and Transport. Taking the success rate to be the fraction of
states for which local analysis succeeds, TorchLight empirically exhibits strong performance
 delivering high success rates  also in Zenotravel, Satellite, Tyreworld, Grid, Driverlog, and
2. Simple-TSP encodes TSP but on a fully connected graph with uniform edge cost. The domain was
introduced by Fox and Long (1999) as a benchmark for symmetry detection.

156

fiAnalyzing Search Topology Without Running Any Search

Rovers. Thus TorchLights success rates tend to be high in the easy domains of Figure 1,
while they are low in the hard ones, serving to automatically distinguish between these two
groups.3 By summarizing structural reasons for analysis failure, TorchLight finally provides
diagnostic output indicating problematic aspects of the domain, i.e., operator effects that
potentially cause local minima under h+ .
What is the key to this performance boost? Consider Logistics and Blocksworld-Arm.
At the level of their PDDL domain descriptions, the difference is not evident  both have
delete effects, so why do those in Blocksworld-Arm hurt and those in Logistics dont?
What does the trick is to move to the finite-domain variable representation (e.g., Jonsson &
Backstrom, 1998; Helmert, 2006, 2009) and to consider the associated structures, notably
the causal graph (e.g., Knoblock, 1994; Jonsson & Backstrom, 1995; Domshlak & Dinitz,
2001; Helmert, 2006) capturing the precondition and effect dependencies between variables.
The causal graph of Blocksworld-Arm contains cycles. That of Logistics doesnt. Looking
into this, it was surprisingly easy to derive the following basic result:
If the causal graph is acyclic, and every variable transition is invertible,
then there are no local minima under h+ .
This result is certainly interesting in that, for the first time, it establishes a connection
between causal graph structure and h+ topology. However, by itself the result is much
too weak for domain analysis  of the considered benchmarks, it applies only in Logistics. We devise generalizations and approximations yielding the analysis results described
above. Aside from their significance for domain analysis, our techniques are also interesting
with respect to research on causal graphs. Whereas traditional methods (e.g., Jonsson &
Backstrom, 1995; Brafman & Domshlak, 2003; Jonsson, 2009; Gimenez & Jonsson, 2009a)
seek execution paths solving the overall task, we seek only execution paths decreasing the
value of h+ . In local analysis, this enables us to consider only small fragments of the causal
graph, creating the potential to successfully analyze states in tasks whose causal graphs are
otherwise arbitrarily complex.
The next section gives a brief background on planning with finite-domain variables, and
the associated notions such as causal graphs and the definition of h+ and its topology. Section 3 then gives an illustrative example explaining our basic result, and Section 4 provides
a synopsis of our full technical results relating causal graphs and h+ topology. Sections 5
and 6 present these results in some detail, explaining first how we can analyze a state s
provided we are given an optimal relaxed plan for s as the input, and thereafter providing
criteria on causal graph structure implying that such analysis will always succeed. We evaluate the domain analysis technique by proving a number of domain-specific performance
guarantees in Section 7, and reporting on a large-scale experiment with TorchLight in Section 8. We point to related work within its context where appropriate, and discuss details
in Section 9. We close the paper with a discussion of future work in Section 10. To improve
readability, the main text omits many technical details and only outlines the proofs. The
full details including proofs are in Appendix A.
3. To some extent, this particular result can also be achieved by simpler means (limited search probing).
We discuss this along with the experiments in Section 8.

157

fiHoffmann

2. Background
We adopt the terminology and notation of Helmert (2006), with a number of modifications
suiting our purposes. A (finite-domain variable) planning task is a 4-tuple (X, sI , sG , O). X
is a finite set of variables, where each x  X is associated with a finite domain Dx . A partial
state over X is a function s on a subset Xs of X, so that s(x)  Dx for all x  Xs ; s is a state
if Xs = X. The initial state sI is a state. The goal sG is a partial state. O is a finite set of
operators. Each o  O is a pair o = (preo , eff o ) of partial states, called its precondition and
effect. As simple non-restricting sanity conditions, we assume that |Dx | > 1 for all x  X,
and preo (x) 6= eff o (x) for all o  O and x  Xpreo  Xeff o .
We identify partial states with sets of variable-value pairs, which we will often refer to
as facts. The state space S of the task is the directed graph whose vertices are all states over
X, with an arc (s, s0 ) iff there exists o  O such that preo  s, eff o  s0 , and s(x) = s0 (x)
for all x  X \ Xeff o . A plan is a path in S leading from sI to a state s with sG  s.
We next define the two basic structures in our analysis: domain transition graphs and
causal graphs. For the former, we diverge from Helmerts definition (only) in that we
introduce additional notations indicating the operator responsible for the transition, as well
as the side effects of the transition, i.e., any other variable values set when executing the
responsible operator. In detail, let x  X. The domain transition graph DT Gx of x is the
labeled directed graph with vertex set Dx and the following arcs. For each o  O where
x  Xpreo  Xeff o with c := preo (x) and c0 := eff o (x), DT Gx contains an arc (c, c0 ) labeled
with responsible operator rop(c, c0 ) := o, with conditions cond(c, c0 ) := preo \ {(x, c)}, and
with side effects seff(c, c0 ) := eff o \ {(x, c0 )}. For each o  O where x  Xeff o \ Xpreo with
c0 := eff o (x), for every c  Dx with c 6= c0 , DT Gx contains an arc (c, c0 ) labeled with
rop(c, c0 ) := o, cond(c, c0 ) := preo , and seff(c, c0 ) := eff o \ {(x, c0 )}.
The reader familiar with causal graphs may have wondered why we introduced a notion
of side effects, seeing as causal graphs can be acyclic only if all operators are unary (affect
only a single variable). The reason is that we do handle cases where operators are nonunary. The variant of causal graphs we use can still be acyclic in such cases, and indeed this
happens in some of our benchmark domains, specifically in Simple-TSP, Movie, MiconicSTRIPS, and Satellite. We define the support graph SG to be the directed graph with vertex
set X, and with an arc (x, y) iff DT Gy has a relevant transition (c, c0 ) so that
S x  Xcond(c,c0 ) .
0
0
Here, a transition (c, c ) on variable x is called relevant iff (x, c )  sG  oO preo .
Our definition modifies the most commonly used one in that it uses relevant transitions
only, and that it does not introduce arcs between variables co-occurring in the same operator
effect (unless these variables occur also in the precondition). Transitions with side effects
are handled separately in our analysis. Note that irrelevant transitions occur naturally, in
domains with non-unary operators. For example, unstacking a block induces the irrelevant
transition making the arm non-empty, and departing a passenger in Miconic-STRIPS makes
the passenger not-boarded.4
Consider now the definition of h+ . In the more common Boolean-variable setting of
PDDL, this is defined as the length of a shortest plan solving the problem when ignoring
4. We remark that relevant transitions correspond to what has been called requestable values in some
works, (e.g., Jonsson & Backstrom, 1998; Haslum, 2007). In Fast Downwards implementation, the
causal graph includes only precondition-effect arcs, similarly as the support graph defined here.

158

fiAnalyzing Search Topology Without Running Any Search

all delete lists, i.e., the negative operator effects (Bylander, 1994; McDermott, 1999; Bonet
& Geffner, 2001). This raises the question what h+ actually is, in finite-domain variable
planning, where there are no delete lists. That question is easily answered. Ignoring
deletes essentially means to act as if what was true once will remain true forever. In
the finite-domain variable setting, this simply means to not over-write any values that
the variables had previously. To our knowledge, this generalization was first described by
Helmert (2006). Consider the directed graph S + whose vertices are all sets s+ of variable+
+
value pairs over X, with an arc (s+
1 , s2 ) iff there exists o  O such that preo  s1 and
+
+
+
s2 = s1  eff o . If s is a state, then a relaxed plan for s is a path in S leading from
s to s+ with sG  s+ . By h+ (s) we denote the length of a shortest relaxed plan for s,
or h+ (s) =  if no such plan exists. It is easy to see that this definition corresponds to
the common Boolean one: if we translate the finite-domain variables into Boolean ones by
creating one Boolean variable is-(x, c)-true? for every fact (x, c), then standard h+ in the
Boolean task is identical to h+ in the finite-domain variable task.
Bylander (1994) proved that it is intractable to compute h+ . Many state-of-the-art
planners approximate h+ , in a variety of ways (e.g., McDermott, 1999; Bonet & Geffner,
2001; Hoffmann & Nebel, 2001a; Gerevini et al., 2003; Helmert, 2006; Richter, Helmert,
& Westphal, 2008; Richter & Westphal, 2010). A popular approximation in satisficing
planning  that gives no guarantees on the quality of the relaxed plan returned  is the
so-called relaxed plan heuristic first proposed in the FF system (Hoffmann & Nebel, 2001a),
which approximates h+ in terms of the length of some not necessarily shortest relaxed plan.
Such relaxed plans can be computed in low-order polynomial time using techniques inspired
by Graphplan (Blum & Furst, 1997).
We next introduce the relevant notations pertaining to search space topology under h+ .
Let s  S be a state where 0 < h+ (s) < . Then an exit is a state s0 reachable from s
in S, so that h+ (s0 ) = h+ (s) and there exists a neighbor s00 of s0 so that h+ (s00 ) < h+ (s0 )
(and thus h+ (s00 ) < h+ (s)). The exit distance ed(s) of s is the length of a shortest path to
an exit, or ed(s) =  if no exit exists. A path in S is called monotone iff there exist no
two consecutive states s1 and s2 on it so that h+ (s1 ) < h+ (s2 ). We say that s is a local
minimum if there exists no monotone path to an exit.
The topology definitions, adapted from the authors previous work (Hoffmann, 2005),
are specific to h+ only for the sake of simplicity (we will herein not consider any heuristics
other than h+ ).5 States with infinite heuristic value are ignored because they are correctly
identified, by the heuristic, to be dead ends (relaxed-plan based approximations like that
of FF do identify all these cases). If the heuristic value is 0 then we have already reached
the goal, so this case can also be safely ignored. Note that we do not force exit paths to
be monotone, i.e., we will also talk about exit distances in situations where s may be a
local minimum. This is necessary to capture the structure of domains like Satellite and
Zenotravel, where local minima exist but their exit distance is bounded. Also, some of our
analysis methods guarantee an upper bound on the length of an exit path only, not that
the heuristic values on that path will decrease monotonically.
5. We remark that the original definitions are significantly more involved, e.g., defining local minima not
based on individual states but based on strongly connected sub-graphs of the state space. None of these
complications is relevant to the results herein.

159

fiHoffmann

Finally, let us say a few words on domain analysis. Generally speaking, domain analysis
aims at automatically obtaining non-trivial information about a domain or planning task.
Such analysis has a long tradition in planning (e.g., Nebel, Dimopoulos, & Koehler, 1997;
Fox & Long, 1998; Gerevini & Schubert, 1998; Edelkamp & Helmert, 1999; Rintanen,
2000). Most often, the information sought pertains to reachability or relevance properties,
i.e., which entities or combinations thereof are reachable from the initial state/relevant to
the goal. A notable exception is the work of Long and Fox (2000) which automatically
recognizes certain generic types of domains, like transportation. However, there exists no
prior work at all trying to automatically infer topological properties of a heuristic function.
The single exception are the aforementioned disappointing results reported (as an aside)
in the authors previous work (Hoffmann, 2005). This method builds a structure called
fact generation tree, enumerating all ways in which facts may support each other in a
non-redundant relaxed plan. If there is no conflict then h+ is the exact solution distance.
Clearly, this is a far too strong property to be applicable in any reasonably complex domain.
Of the considered benchmarks, the property applies only in Simple-TSP. A slightly more
general property, also identified in this work, applies in Movie as well as trivial Logistics
tasks with 2 locations, 1 truck, and 1 package.
It is worth noting that analyzing the topology of h+ is computationally hard:
Theorem 1. It is PSPACE-complete to decide whether or not the state space of a given
planning task contains a local minimum, and given an integer K it is PSPACE-complete to
decide whether or not for all states s we have ed(s)  K. Further, it is PSPACE-complete
to decide whether or not a given state s is a local minimum, and given an integer K it is
PSPACE-complete to decide whether or not ed(s)  K.
These results are hardly surprising, but have not been stated anywhere yet. The membership results in Theorem 1 are easy to prove based on guess-and-check arguments similar
as given by Bylander (1994), exploiting the fact that NPSPACE=PSPACE. The hardness results still hold when restricting the input to solvable tasks/states. Their proofs work
by reducing plan existence, respectively bounded plan existence (with a bound in non-unary
representation). Given a task whose plan existence we wish to decide, we flatten h+ by a
new operator that can always achieve the goal but that has a fatal side effect. Then we give
the planner the choice between solving this task, or solving a new alternative task. That latter task is designed so that a local minimum exists/that the exit distance exceeds the bound
iff the planner must choose the alternative task, i.e., iff the original task is unsolvable/iff it
cannot be solved within a given number of steps. The full proof is in Appendix A.1.
In practice, computational hardness here is particularly challenging because, in most
applications of domain analysis, we are not willing to run a worst-case exponential search.
After all, the analysis will not actually solve the problem. Consequently, in the present
research, we restrict ourselves to analysis methods with low-order polynomial runtime.
The reader will have noticed the state-specific analysis problems in Theorem 1. We
distinguish between global analysis per-task, and local analysis per-state. More precisely,
we herein devise three kinds of analyses:
(I) Guaranteed global analysis. Taking as input the planning task description, this
analysis returns yes, d only if the state space does not contain any local minima
and the exit distance from any state is bounded by d.
160

fiAnalyzing Search Topology Without Running Any Search

(II) Guaranteed local analysis. Taking as input the planning task description and a
state s, this analysis returns yes, d only if s is not a local minimum, and the exit
distance from s is bounded by d.
(III) Approximate local analysis. Taking as input the planning task description and a
state s, this analysis returns yes, d to indicate that s is not a local minimum, and
that the exit distance from s is bounded by d. Both may be wrong, i.e., the analysis
is not guaranteed to be sound. Compared to analysis (II), this trades soundness for
the ability to successfully analyze more states.
Domain analysis traditionally considers only the global variant (I), or even more generalizing
variants looking at only the PDDL domain file. While global once-and-for-all analysis is
also the holy grail in our work, local analysis has strong advantages. If a planning task
does contain local minima  which one would expect to typically be the case in interesting
domains  then analysis (I) is useless. It will simply answer no. By contrast, local analysis
(II,III) may still detect some individual states, that we sample randomly in our experiments,
to not be local minima. The percentage of such states, which we refer to as the success rate,
can deliver useful information no matter what the structure of the planning task is. Note
also that, while the contrast between a PSPACE-hard problem and low-order polynomial
analysis runtime necessarily implies that all analyses are incomplete, the local analyses have
a chance to ameliorate this by averaging their outcome over a set of sample states.

3. An Illustrative Example
The basic connection we identify between causal graphs and h+ topology  more precisely,
between support graphs, domain transition graphs, and h+ topology  is quite simple. It
is instructive to understand this first, before delving into the full results. Figure 2 shows
fragments of the domain transition graphs (DTGs) of three variables x0 , x1 , and x2 . All
DTG transitions here are assumed to be invertible, and to have no side effects.
t0

x0
g0
T2

T1

x1

L1

L2

L3

s1 R1

R2

R3

x2
c1

c2

s2

Figure 2: An example illustrating our basic result.
The imaginative reader is invited to think of x0 as a car whose battery is currently
empty and that therefore requires the help of two people, x1 and x2 , in order to push-start
it. The people may, to solve different parts of the task, be required for other purposes too,
but here we consider only the sub-problem of achieving the goal x0 = g0 . We wish to take
161

fiHoffmann

the x0 transition t0 , which has the two conditions c1 and c2 . These conditions are currently
not fulfilled. In the state s at hand, x1 is in s1 and x2 is in s2 . We must move to a different
state, s0 , in which x1 = c1 and x2 = c2 . What will happen to h+ along the way?
Say that an optimal relaxed plan P + (s) for s moves x1 to c1 along the path marked T1 ,
and moves x2 to c2 along the path marked T2  clearly, some such paths will have to be taken
by any P + (s). Key observation (1) is similar to a phenomenon known from transportation
benchmarks. When moving x1 and x2 , whichever state s0 we are in, as long as s0 remains
within the boundaries of the values traversed by T1 and T2 , we can construct a relaxed plan
P + (s0 ) for s0 so that |P + (s0 )|  |P + (s)|. Namely, to obtain P + (s0 ), we simply replace the


respective move sequence 
o i in P + (s), for i = 1, 2, with its inverse 
o i . For example, say


0
we got to s by o 1 = hR1, R2, R3i moving x1 to c1 , as indicated in Figure 2. Then wlog
P + (s) has the form hR1, R2, R3i  P . We define P + (s0 ) := hL3, L2, L1i  P . The postfix P
of both relaxed plans is the same; at the end of the prefix, the set of values achieved for x1 ,
namely s1 , c1 , and the two values in between, is also the same. Thus P + (s0 ) is a relaxed

plan for s0 .6 This is true in general, i.e., 
o 1 is necessarily applicable in s0 , and will achieve,

+
0
within relaxed execution of P (s ), the same set of facts as achieved by 
o 1 in P + (s). Thus
h+ (s0 )  h+ (s) for any state s0 , including the state s0 were after.
Key observation (2) pertains to the leaf variable, x0 . Say that x0 moves only for its
own sake, i.e., the car position is not important for any other goal. Then executing t0 in
s0 does not delete anything needed anywhere else. Thus we can remove rop(t0 ) from the
relaxed plan P + (s0 ) for s0  constructed as per observation (1)  to obtain a relaxed plan for
the state s1 that results from executing t0 in s0 . Hence h+ (s1 ) < h+ (s). With observation
(1), the heuristic values along the path to s1 are all  h+ (s). We know that at least one
state s00 on the path has a heuristic value strictly smaller than h+ (s): this happens at the
latest in s00 = s1 , and may happen earlier on in case the relaxed plan P + (s00 ) as constructed
here is not optimal (cf. Footnote 6). Let s00 be the earliest state with h+ (s00 ) < h+ (s) on
the path, and let s0 be the state preceding s00 . Then s0 is an exit for s, and the path to that
exit is monotone. Thus s is not a local minimum. As for the exit distance, in the worst
case we have s00 = s1 and s0 = s0 , so ed(s) is bounded by the length of the path up to s0 .
It is not difficult to imagine that the above works also if preconditions need to be
established recursively, as long as no cyclic dependencies exist. A third person may be
needed to first persuade x1 and x2 , the third person may need to take a bus, and so on.
The length of the path to s0 may grow exponentially  if x1 depends on x3 then each
move of x1 may require several moves of x3 , and so forth  but we will still be able to
construct P + (s0 ) by inverting the moves of all variables individually. Further, the inverting
transitions may have conditions, too, provided these conditions are the same as required
by the original moves. For example, in the above, the inverting operator L1 may have an
arbitrary condition p if that condition is also required for R1 . This is because any conditions
that are required for the original moves (like p for R1 ) are established in P + (s), and thus
will be established in P + (s0 ) in time for the inverse moves (like L1 ).
6. Note that P + (s0 ) may not be an optimal relaxed plan for s0 . If P + (s) does not move x1 for anything
other than attaining c1 , then the postfix P alone is a relaxed plan for s0 : there is no need to insert the
inverted prefix hL3, L2, L1i. In cases like this, we obtain an exit state already on the path to s0 ; we get
back to this below.

162

fiAnalyzing Search Topology Without Running Any Search

Now, say that the support graph is acyclic, and that all transitions are invertible and
have no side effects. Given any state s, unless s is already a goal state, some variable x0
moving only for its own sake necessarily exists. But then, within any optimal relaxed plan
for s, a situation as above exists, and therefore we have a monotone exit path, Q.E.D. for
no local minima under h+ .
The execution path construction just discussed is not so different from known results
exploiting causal graph acyclicity and notions of connectedness or invertibility of domain
transition graphs (e.g., Jonsson & Backstrom, 1995; Williams & Nayak, 1997). What is
new here is the connection to h+ .
We remark that the hand-made analysis of h+ (Hoffmann, 2005) uses a notion of operators respected by the relaxation. An operator o is respected by the relaxation iff,
whenever o starts an optimal plan for s, then o also starts an optimal relaxed plan for s. A
core property of many of the hand-made proofs is that all operators are respected by the
relaxation. This motivated the speculation that recognizing this property automatically
could be key to domain analysis recognizing the absence of local minima under h+ . We do
not explore this option herein, however we note that even the basic result we just outlined
contains cases not covered by this property. Even with acyclic support graph and invertible
transitions without side effects, there are examples where an operator is not respected by
the relaxation. We give such a construction in Example 1, Appendix A.4.

4. Synopsis of Technical Results
Our technical results in what follows are structured in a way similar to the proof argument
outlined in the previous section. The results are structured into two parts, (A) and (B).
In (A), Section 5, we identify circumstances under which we can deduce from an optimal
relaxed plan that a monotone exit path exists. In (B), Section 6, we devise support-graph
based sufficient criteria implying that analysis (A) will always succeed. Technique (B)
underlies TorchLights conservative analysis methods, i.e., guaranteed global analysis (I)
and guaranteed local analysis (II) as described at the end of Section 2. By feeding technique
(A) with the usual relaxed plans as computed, e.g., by FFs heuristic function, we obtain
TorchLights approximate local analysis (III). That analysis does not give a guarantee,
because (and only because) FFs relaxed plans are not guaranteed to be optimal.
For ease of reading, we now give a brief synopsis of the results obtained in (A) and
(B), and how they provide the analysis methods (I)(III). The synopsis contains sufficient
information to understand the rest of the paper, so the reader may choose to skip Sections 5
and 6, moving directly to the evaluation.
Each analysis method is based on a particular kind of sub-graph of the support graph.
Table 1 overviews these. Their role in parts (A) and (B) is as follows:
(A) Given an optimal relaxed plan P + (s) for a state s, an optimal rplan dependency graph
oDG+ is a sub-graph of SG with a single leaf variable x0 with transition t0 as in our
example (rop(t0 ) will be frequently referred to as o0 ). An arc (x, x0 ) is in oDG+ if
P + (s) relies on x0 to achieve the conditions of t0 , and P + (s) relies on x for moving x0 .
We say that oDG+ is successful if it is acyclic, all involved transitions will be usable in
our exit path construction (e.g., they have no harmful side effects), and the deletes of t0
163

fiHoffmann

Name
Support graph

Symbol
SG

Analysis

Approximate
local analysis (III)
Theorem 2

Optimal rplan
dependency graph

oDG+

Local
dependency graph

lDG

Guaranteed
local analysis (II)
Theorem 3

Global
dependency graph

gDG

Guaranteed
global analysis (I)
Theorem 4

Leaves
All
Single leaf x0 s.t. applying
t0 does not affect the
remainder of P + (s)
Single leaf x0  XsG ,
s(x0 ) 6= sG (x0 ) and x0 has
no transitive SG successor
with same property
Single leaf x0  XsG

Arcs
All
(x, x0 ) where x is used in
P + (s) to support x0 for
obtaining cond(t0 )
(x, x0 ) where
s(x) 6= cond(t0 )(x); and
(x, x0 ) where x0 is in lDG
and (x, x0 ) is in SG
(x, x0 ) where x 6= x0 ; and
(x, x0 ) where x0 is in gDG
and (x, x0 ) is in SG

Table 1: Overview of the different support graph sub-graphs underlying our results.
are either not relevant to P + (s) at all, or are being recovered inside P + (s). The main
result, Theorem 2, states that s is no local minimum if there exists a successful oDG+
for s. It also derives an exit distance bound from oDG+ . Approximating Theorem 2
by applying it to a relaxed plan as computed by FFs heuristic yields analysis (III).
(B) Given a state s, a local dependency graph lDG is a sub-graph of SG with a single leaf
variable x0 , whose goal value is yet unachieved, and all of whose transitive successors
in SG have already attained their goal values. In this setting, x0 moves for its own
sake as in the example. The graph lDG simply includes all SG predecessors of x0 , the
single exception pertaining to arcs (x, x0 ) into x0 itself, which are not inserted if the
corresponding condition of t0 is already satisfied in s. We say that lDG is successful if
it is acyclic, all involved transitions will be usable in our exit path construction, and t0
does not have any relevant deletes. This implies that there exists a successful oDG+
contained in lDG, and thus we have Theorem 3, stating that s is no local minimum
and giving a corresponding exit distance bound. This result underlies analysis (II).
A global dependency graph gDG is a sub-graph of SG that identifies any goal variable
x0 , and includes all SG predecessors of x0 . Being successful is defined in the same
way as for lDGs. If all gDGs are successful, then Theorem 3 will apply to every state
because each lDG is contained in a successful gDG. Thus we have Theorem 4, stating
that the state space does not contain any local minima. The exit distance bound is
obtained by maximizing over all gDGs. This result underlies analysis (I).
For understanding the practical performance of TorchLight, it is important to note that
(A) is not only a minimal result that would suffice to prove (B). The cases identified by
Theorem 2 are much richer than what we can actually infer from support graphs. For this
reason, analysis (III), while not sound due to the use of potentially non-optimal relaxed
plans, is able to analyze a much larger class of states than analysis (II). In a little detail,
the difference between the two methods pertains to (1) whether P + (s) relies on values
of x for moving x0 , and (2) whether the deletes of t0 are being recovered inside P + (s).
Neither (1) nor (2) are visible in the support graph, because both rely on details of the
form of the relaxed plan P + (s). For example, consider the Gripper domain. Notion (1)
is important because the support graph contains the arcs (carry-ball-b, free-gripper)
 due to dropping ball b  and (free-gripper, carry-ball-b)  due to picking up ball b.
Thus, looking only at SG, it seems that carry-ball-b may support itself (free the gripper
164

fiAnalyzing Search Topology Without Running Any Search

by dropping the ball we want to pick up). Of course, that doesnt happen in an optimal
relaxed plan. Notion (2) is important because some operators (picking up a ball) do have
harmful side effects (making the gripper hand non-empty), but these side effects are always
recovered inside the relaxed plan (when dropping the ball again later on). It remains future
work to extend analyses (I,II) so that they can detect these kinds of phenomenona.

5. Analyzing Optimal Relaxed Plans
We consider a state s and an optimal relaxed plan P + (s) for s. To describe the circumstances
under which a monotone exit path is guaranteed to exist, we will need a number of notations
pertaining to properties of transitions etc. We will introduce these notations along the way,
rather than up front, in the hope that this makes them easier to digest.
+
+
Given o0  P + (s), by P<0
(s) and P>0
(s) we denote the parts of P + (s) in front of o0
+
and behind o0 , respectively. By P (s, x) we denote the sub-sequence of P + (s) affecting
x. We capture the dependencies between the variables used in P + (s) for achieving the
precondition of o0 , as follows:
Definition 1. Let (X, sI , sG , O) be a planning task, let s  S with 0 < h+ (s) < , let
P + (s) be an optimal relaxed plan for s, let x0  X, and let o0  P + (s) be an operator
taking a relevant transition of the form t0 = (s(x0 ), c).
An optimal rplan dependency graph for P + (s), x0 and o0 , or optimal rplan dependency
graph for P + (s) in brief, is a graph oDG+ = (V, A) with unique leaf vertex x0 , and where
x  V and (x, x0 )  A if either: x0 = x0 , x  Xpreo , and preo0 (x) 6= s(x); or x 6= x0 
0
+
(s) taking a relevant transition on x0 so that x  Xpreo
V \ {x0 } and there exists o  P<0
and preo (x) 6= s(x).
For x  V \ {x0 }, by oDT G+
x we denote the sub-graph of DT Gx that includes only
+
(s, x), the relevant transitions t using an operator in
the values true at some point in P<0
+
P<0 (s, x), and at least one relevant inverse of such t where a relevant inverse exists. We
+
(s, x) transitions as original, and to the inverse transitions as induced.
refer to the P<0
The transition t0 with responsible operator o0 will be our candidate for reaching the
exit state, like t0 in Figure 2. oDG+ collects all variables x connected to a variable x0
+
insofar as P<0
(s) uses an operator preconditioned on x in order to move x0 . These are the
variables we will need to move, like x1 and x2 in Figure 2, to obtain a state s0 where t0 can
be taken. For any such variable x, oDT G+
x captures the domain transition graph fragment
+
that P<0
(s) traverses and within which we will stay, like T1 and T2 in Figure 2.
+
Note that there is no need to consider the operators P>0
(s) behind o0 , simply because
these operators are not used in order to establish o0 s precondition. This is of paramount
importance in practice. An example is the Gripper situation mentioned above. if o0 picks
+
up a ball b in Gripper, then P + (s) will also contain  behind o0 , i.e., in P>0
(s)  an
0
0
+
operator o dropping b. If we considered o in Definition 1, then oDG would contain the
mentioned cycle assuming that o0 is used for making the gripper hand free for picking up b.
In TorchLights approximate local analysis, whenever we consider an operator o0 , before we
build oDG+ we re-order P + (s) by moving operators behind o0 if possible. This minimizes
+
P<0
(s), and oDG+ thus indeed contains only the necessary variables and arcs.
165

fiHoffmann

Under which circumstances will t0 actually do the job? The sufficient criterion we
identify is rather complex. To provide an overview of the criterion, we next state its definition. The items in this definition will be explained below.
Definition 2. Let (X, sI , sG , O), s, P + (s), x0 , o0 , t0 , and oDG+ = (V, A) be as in Definition 1. We say that oDG+ is successful if all of the following holds:
(1) oDG+ is acyclic.
(2) We have that either:
+
(a) the oDG+ -relevant deletes of t0 are P>0
(s)-recoverable; or
+
(b) s(x0 ) is not oDG -relevant, and t0 has replaceable side effect deletes; or
(c) s(x0 ) is not oDG+ -relevant, and t0 has recoverable side effect deletes.

(3) For x  V \ {x0 }, all oDT G+
x transitions either have self-irrelevant deletes, or are
invertible/induced and have irrelevant side effect deletes and no side effects on V \{x0 }.
As already outlined, our exit path construction works by staying within the ranges of
oDT G+
x , for x  V \ {x0 }, until we have reached a state s0 where the transition t0 can be
taken. To make this a little more precise, consider a topological order xk , . . . , x1 of V \ {x0 }
with respect to oDG+  such an order exists due to Definition 2 condition (1). (If there
are cycles, then moving a variable may involve moving itself in the first place, which is
not covered by our exit path construction.) Now consider, for 0  d  k, the d-abstracted
task. This is like the original task except that, for every transition t of one of the graphs
oDT G+
xi with i  d, we remove each condition (xj , c)  cond(t) where j > d. The exit
path construction can then be understood as an induction over d, proving the existence


of an execution path 
o at whose end t0 can be taken. We construct 
o exclusively by
,
for
x

V
\
{x
}.
For
the
base case, in the
operators responsible for transitions in oDT G+
0
x
0-abstracted task, t0 is directly applicable. For the inductive case, if we have constructed


a suitable path 
o d for the d-abstracted task, then a suitable path 
o d+1 for the d + 1
abstracted task can be constructed as follows. Assume that o is an operator in 
o d , and


that o has a precondition (xd+1 , c) that is not true in the current state. Then, in o d+1 , in
front of o we simply insert a path through oDT G+
xd+1 that ends in c. Note here that, by
construction, (xd+1 , c) is a condition of a transition t in oDT G+
xi , for some i < d + 1. If
+
+
t is taken in P<0
(s, x), then (xd+1 , c) must be achieved by P<0
(s) and thus c is a node in
+
oDT G+
xd+1 . If t is an induced transition  inverting a transition taken in P<0 (s, x)  then
the same is the case unless the inverse may introduce new outside conditions. We thus need
to exclude this case, leading to the following definition of invertibility:
 Let t = (c, c0 ) be a transition on variable x. We say that t is invertible iff there exists
a transition (c0 , c) in DT Gx so that cond(c0 , c)  cond(c, c0 ).
A transition is invertible if we can go back without introducing any new conditions (e.g.,
driving trucks in Logistics). There are subtle differences to previous definitions of invertible
operators, like the authors (Hoffmann, 2005). We do not allow new conditions even if they

are actually established by the operator rop(t) responsible for t. This is because, on 
o , we
do not necessarily execute t before executing its inverse  we may have got to the endpoint
of t via a different path in oDT G+
x . On the other hand, our definition is also more generous
166

fiAnalyzing Search Topology Without Running Any Search

than common ones because, per se, it does not care about any side effects the inverse
transition may have (side effects are constrained separately as stated in Definition 2).
Consider Definition 2 condition (3). Apart from the constraints on conditions of induced


transitions, for the oDT G+
x transitions taken by o , we must also make sure that there are
no harmful side effects. Obviously, this is the case if, as in the example from Section 3, the
transitions have no side effects at all. However, we can easily generalize this condition. Let
t = (c, c0 ) be a transition on variable x.
 The context of t is the set ctx(t) of all facts that may be deleted by side effects of t.
For each (y, d)  seff(t), (y, cond(t)(y))  ctx(t) if a condition on y is defined; else all
Dy values 6= d are inserted.
S
 We say that t has irrelevant side effect deletes iff ctx(t)  (sG  oO preo ) = .
S
 We say that t has self-irrelevant side effect deletes iff ctx(t)  (sG  rop(t)6=oO preo ) =
.
 We say that tShas self-irrelevant deletes iff it has self-irrelevant side effect deletes and
(x, c) 6 sG  rop(t)6=oO preo .
Irrelevant side effect deletes capture the case where no side effect delete occurs in the goal
or in the precondition of any operator. Self-irrelevant side effect deletes are slightly more
generous in that they allow to delete conditions needed only for the responsible operator
rop(t) itself. Self-irrelevant deletes, finally, extend the latter notion also to ts own delete.
In a nutshell, we need to postulate irrelevant side effect deletes for transitions that may
be executed again, on our path. Examples of irrelevant side effect deletes are transitions
with no side effects at all, or a move in Simple-TSP, whose side effect, when x0 =at,
deletes the target locations being not-visited. An example of an operator with selfirrelevant side effect deletes, but no irrelevant side effect deletes, is departing a passenger
in Miconic-STRIPS, whose side effect, when x0 =served, deletes boarded(passenger)
which is used only for the purpose of this departure. In fact, this transition has selfirrelevant deletes because its own effect deletes not-served(passenger) which obviously is
irrelevant. Another example of self-irrelevant deletes is inflating a spare wheel in Tyreworld
 the wheel is no longer not-inflated.


Clearly, if all oDT G+
x transitions t we may be using on o have irrelevant side effect
deletes, then, as far as not invalidating any facts needed elsewhere is concerned, this is just
as good as having no side effects at all. To understand why we need to require that ts
side effect is not used to move another variable x0  V \ {x0 }, recall that, for the states s0

visited by 
o , we construct relaxed plans P + (s0 ) with |P + (s0 )|  |P + (s)| by inverting such
transitions t. Now, say that ts side effect is used to move another variable x0  V \ {x0 }.
Then we may have to invert both transitions separately (with different operators), and thus
we would have |P + (s0 )| > |P + (s)|.
Regarding the own delete of t, this may be important for two reasons. First, the deleted
fact may be needed in the relaxed plan for s0 . Second, x may have to traverse oDT G+
x several
times, and thus we may need to traverse the deleted value again later on. Both are covered if
t is invertible, like we earlier on assumed for all transitions. Now, what if t is not invertible?
This does not constitute a problem in case that t has self-irrelevant deletes: in that case,
167

fiHoffmann

all deletes of t are irrelevant except maybe for the responsible operator itself. Therefore,
to obtain P + (s0 ), we can simply remove rop(t) from the relaxed plan constructed for the
predecessor state s00 . Thus |P + (s0 )| < |P + (s)| so we have reached an exit and there is no

need to continue the construction of 
o . For example, consider t that inflates a spare wheel
W in Tyreworld. This deletes only not-inflated(W), and thus has self-irrelevant deletes
(not-inflated(W) is irrelevant for the goal and any other operator). Say that we are in a
state s00 with relaxed plan P + (s00 ) constructed as described. We have |P + (s00 )|  |P + (s)|.
We also have rop(t) =inflate-W P + (s00 ), because inflate-W P + (s), and because
inflate-W was not executed as yet on our path, and was hence not removed from the
relaxed plan. Applying inflate-W to s00 , we get to a state s0 identical to s00 except that W
is now inflated. Clearly, the relaxed plan for s0 no longer needs to apply inflate-W, and
the rest of the relaxed plan P + (s00 ) still works unchanged. Thus P + (s0 ) can be obtained by
removing inflate-W from P + (s00 ), yielding |P + (s0 )| < |P + (s)| as desired.
Consider now our endpoint transition t0 and its responsible operator o0 . We previously
demanded that x0 moves for its own sake, i.e., that x0 has a goal value and is not
important for achieving any other goal. This is unnecessarily restrictive. For example, in
Miconic-STRIPS, if we board a passenger then h+ decreases because we can remove the
boarding operator from the relaxed plan. However, boarding is only a means for serving
the passenger later on, so this variable x0 has no own goal. In Driverlog, a driver may
have its own goal and be needed to drive vehicles, and still t0 moving the driver results in
decreased h+ if the location moved away from is not actually needed anymore. The latter
example immediately leads to a definition capturing also the first one: all we want is that
any deletes of t0 are not needed in the rest of the relaxed plan. We can then remove o0
from the relaxed plan for s0 , and have reached an exit as desired.
To make this precise, recall the situation we are addressing. We have reached a state s0
in which t0 = (s(x0 ), c) can be applied, yielding a state s1 . We have a relaxed plan P + (s0 )
for s0 so that |P + (s0 )|  |P + (s)|, where P + (s0 ) is constructed from P + (s) by replacing
+
(s) with operators responsible for induced oDT G+
some operators of P<0
x transitions for
x  V \ {x0 }. We construct P1+ by removing o0 from P + (s0 ), and we need P1+ to be a
relaxed plan for s1 . What are the facts possibly needed in P1+ ? A safe approximation is
the union of sG , the precondition of any o0 6= o  P + (s), and any oDT G+
x values needed
7 Denote that set with R+ . The values potentially deleted
by induced oDT G+
transitions.
x
1
by t0 are contained in C0 := {(x0 , s(x0 ))}  ctx(t0 ). Thus if R1+  C0 =  then we are
fine. Simple examples for this have been given above already. In Miconic-STRIPS, the
only delete of o0 boarding passenger P is not-boarded(P), which is not contained in
any operator precondition or the goal and thus the intersection of R1+ with C0 = {notboarded(P)} is empty. In Driverlog, C0 = {at(D,A)} is the delete of o0 moving driver
D away from location A. If that location is irrelevant to the rest of the task, then we
will have at(D,A)6 R1+ and thus, again, R1+  C0 = .
S
We can sharpen this further. Consider the set of facts F0 := s  oP + (s) eff o that
<0

+
are true after relaxed execution of P<0
(s). Say that p 6 F0 . Then p is not needed for

7. To understand the latter two items, note first that operators preceding o0 in P + (s), i.e., operators from
+
P<0
(s), may still be contained in P1+ and thus it does not suffice to include the preconditions only of
+
+
operators o  P>0
(s). As for oDT G+
x values needed by induced oDT Gx transitions, these may be needed
+
+
in P1 but not in P<0
(s).

168

fiAnalyzing Search Topology Without Running Any Search

P1+ to be a relaxed plan for s1 . To see this, note first that p is not needed in the part of
+
+
P1+ pertaining to P<0
(s). More precisely, p cannot be an operator precondition in P<0
(s)
+
because this condition would not be satisfied in (relaxed) execution of P (s). Also, p
cannot be the start value of an induced oDT G+
x transition because, by definition, all such
+
values are added by operators in P<0 (s). Now, what about the part of P1+ pertaining to
+
+
P>0
(s)? Assume that p is either a goal, or is an operator precondition in P>0
(s). Then,
+
+
since p 6 F0 and P (s) is a relaxed plan, either o0 or an operator in P>0 (s) must establish
+
p. As for o0 , all its effects are true in s1 anyway. As for P>0
(s), this remains unchanged in
+
P1 and thus this part is covered, too. Altogether, it thus suffices if R1+  C0  F0 = . An
example where this helps is the Satellite domain. Say that o0 switches on instrument I.
This deletes calibration, i.e., calibrated(I) C0 . The only purpose of switching I on
can be to take images with it, and thus calibrated(I) R1+  C0 . However, the instrument
may not actually be calibrated in s. If that is so, then we need to switch I on before it
can be calibrated  because the calibration operator requires to have power in I  and
thus calibrated(I) will be false in the relaxed execution of P + (s), up to at least o0 . In
particular, we have calibrated(I)6 F0 and thus R1+  C0  F0 = .
Even the condition R1+  C0  F0 =  can still be sharpened. Say that there exists a


+
(s) so that 
o0 is guaranteed to be applicable at
(possibly empty) sub-sequence 
o0 of P>0


+
the start of P1 , and so that o0 re-achieves all facts in R1+  C0  F0 (both are easy to

define and test). Then moving 
o0 to the start of P1+ does the job. We say in this case that
+
+
(s)-recoverable  Definition 2 condition (2a). For
the oDG -relevant deletes of t0 are P>0
example, consider o0 that picks up a ball b in the Gripper domain. This operator deletes a
fact p =free-gripper which may be needed in the remainder of the relaxed plan, and thus

+
(s) will necessarily contain a sub-sequence 
o0 that moves
p  R1+  C0  F0 . However, P>0


+
to another room and then puts b down again. We can re-order P1 to put o0 right at the
start, re-achieving p. Similar patterns occur in any transportation domain with capacity
constraints, or more generally in domains with renewable resources.
Finally, we have identified two simple alternative sufficient conditions under which t0
is suitable, Definition 2 conditions (2b) and (2c). For the sake of brevity, we only sketch
them here. Both require that s(x0 ), i.e., the start value of t0 , is not contained in R1+ as
defined above. We say in this case that s(x0 ) is not oDG+ -relevant. Note that, then,
R1+  C0 =  unless t0 has side effects. Side effects do not hurt if t0 has replaceable side
effect deletes, i.e., if any operator whose precondition may be deleted can be replaced with
an alternative operator o0 that is applicable and has the same effect (this happens, e.g., in
Simple-TSP). Another possibility is that where t0 has recoverable side effect deletes: there
exists an operator o0 that is necessarily applicable directly after execution of t0 , and that
recovers all relevant side effect deletes. This happens quite frequently, for example in Rovers
where taking a rock/soil sample fills a store, but we can free the store again simply by
emptying it anywhere. We can replace o0 with o0 to obtain a relaxed plan P1+ for s1 (and
thus h+ (s1 )  h+ (s)). Then we can apply o0 , yielding a state s2 which has h+ (s2 ) < h+ (s)
because we can obtain a relaxed plan for s2 by removing o0 from P1+ .
What will the length of the exit path be? We have one move for x0 . Each nonleaf variable x must provide a new value at most once for every move of a variable x0
depending on it, i.e., where (x, x0 )  A. The new value can be reached by a oDT G+
x
traversal. Denote the maximum length of such a traversal, i.e., the diameter of oDT G+
x,
169

fiHoffmann

8 Now, we may have diam(oDT G+ ) > diam(DT G ) because oDT G+
by diam(oDT G+
x
x ).
x
x
removes not only vertices but also arcs. There may be short-cuts not traversed by P + (s).
Under certain circumstances it is safe to take these short-cuts, namely if:

(*) all oDT G+
x transitions are invertible/induced and have irrelevant side effect deletes
and no side effects on V \ {x0 }, and all other DT Gx transitions either are irrelevant, or
have empty conditions and irrelevant side effect deletes.
When traversing a short-cut under this condition, as soon as we reach the end of the shortcut, we are back in the region of states s0 where a relaxed plan P + (s0 ) can be constructed
as before. The rest of our exit path construction remains unaffected. Thus,
denote by V 
P
the subset of V \ {x0 } for which (*) holds. We define costd (oDG+ ) := xV costd (x),
where costd (x) :=

1



P
d 0
diam(oDT G+
x)
x0 :(x,x0 )A cost (x )


 min(diam(oDT G+ ), diam(DT G ))  P
x

x

x = x0
x 6= x0 , x 6 V 
x0 :(x,x0 )A cost

d (x0 )

x 6= x0 , x  V 

Note that costd (.) is exponential in the depth of the graph. This is not an artifact of our
length estimation. It is easy to construct examples where exit distance is exponential in
that parameter. This is because, as hinted, a variable may have to move several times for
each value required by other variables depending on it. See Example 6 in Appendix A.4 for
such a construction (following an earlier construction in Domshlak & Dinitz, 2001).
That said, of course costd (.) may over-estimate the length of a shortest exit path. It
assumes that, whenever a variable x0 with (x, x0 )  A makes a move, then x must move
through its entire oDT G+ respectively DT G. This is very conservative: (1) it may be that
the move of x0 does not actually have a condition on x; (2) even if such a condition exists,
x may need less steps in order to reach it. One might be able to ameliorate (1) by making
more fine-grained distinctions which part of costd (x0 ) pertains to moves conditioned on
x. We leave this open for future work. For now, we note that the over-estimation can be
exponential even just due to (2), i.e., costd (oDG+ ) may be exponentially larger than the
length of a shortest exit path even if, for all (x, x0 )  A, all moves of x0 depend on x. This
can be shown by a simple variant of Example 6; we discuss this in Appendix A.4.
Exit paths using short-cuts in the described way may be non-monotone. Example 5
in Appendix A.4 contains a construction showing this. For an intuitive understanding,
imagine a line l0 , . . . , ln where our current task, to achieve the precondition of another
operator, is to move from l0 to ln . Say that all locations on the line need to be visited, in
the relaxed plan, e.g. because we need to load or unload something at all of these locations.
Say further that there is a shortcut via l0 that needs not be visited. If we move to l0 then h+
increases because we have made it 1 step more costly  for the relaxed plan  to reach all the
locations l0 , . . . , ln . For the same reason, costd (oDG+ ) is not an upper bound on the length
of a shortest monotone exit path. This is also shown in Example 5, where we construct a
8. More precisely, diam(.) is not the diameter of a graph but the maximum distance from vertex v to vertex
v 0 where there exists a path from v to v 0 .

170

fiAnalyzing Search Topology Without Running Any Search

situation in which the shortest monotone exit path is longer than costd (oDG+ ).9 To obtain
a bound on monotone exit paths, we can simply set V  :=  in the definition of costd .
If we have Definition 2 condition (2a) or (2b), then the exit distance is bounded by
costd (oDG+ )  1 because costd (oDG+ ) counts the last step reducing h+ . If we have
Definition 2 condition (2c), then after that last step we need 1 additional operator to reduce
h+ , and so the exit distance is bounded by costd (oDG+ ). Putting the pieces together yields
our main result of this section:
Theorem 2. Let (X, sI , sG , O), s, P + (s), and oDG+ be as in Definition 1. If oDG+ is successful, then s is not a local minimum, and ed(s)  costd (oDG+ ). If we have Definition 2
condition (2a) or (2b), then ed(s)  costd (oDG+ )  1.
The full proof is in Appendix A.2. As pointed out earlier, for approximate local analysis
(III) we simply feed Theorem 2 with the relaxed plans returned by FFs heuristic function
(Hoffmann & Nebel, 2001a). It is important to note that, this way, we do not give any
guarantees, i.e., Theorem 2 does not hold if P + (s) is not optimal, and even if P + (s) is
non-redundant and parallel-optimal like those computed by FF. At the end of the exit
path we may obtain a relaxed plan shorter than P + (s) but not shorter than h+ (s). In
a nutshell, the reason is that a parallel-optimal relaxed plan  more generally, a relaxed
plan not minimizing the number of operators  may take very different decisions than a
sequentially-optimal relaxed plan, thus constructing an exit path leading into the wrong
direction. Example 8 in Appendix A.4 gives a full construction proving this.
Feeding Theorem 2 with non-optimal relaxed plans can of course also be imprecise in
the other direction, i.e., Theorem 2 may not apply although it does apply for an optimal
relaxed plan. Thus good cases may go unrecognized. We demonstrate this with a simple
modification of Example 8, explained below the example in Appendix A.4. Importantly, as
we will point out in Section 8, our empirical results suggest that this weakness does not
tend to occur in practice, at least as far as represented by the benchmarks.

6. Conservative Approximations
We now identify sufficient criteria guaranteeing that the prerequisites of Theorem 2 hold
true. We consider both the local case where a particular state s is given, and the global
case where the criterion implies the prerequisites of Theorem 2 for every state s in the task
at hand. We approximate optimal rplan dependency graphs as follows:
Definition 3. Let (X, sI , sG , O) be a planning task, let s  S with 0 < h+ (s) < , let
x0  XsG , and let t0 = (s(x0 ), c) be a relevant transition in DT Gx0 with o0 := rop(t0 ).
A local dependency graph for s, x0 , and o0 , or local dependency graph in brief, is a
graph lDG = (V, A) with unique leaf vertex x0 , and where x  V and (x, x0 )  A if either:
x0 = x0 , x  Xpreo , and preo0 (x) 6= s(x); or x0  V \ {x0 } and (x, x0 ) is an arc in SG.
0
A global dependency graph for x0 and o0 , or global dependency graph in brief, is a
graph gDG = (V, A) with unique leaf vertex x0 , and where x  V and (x, x0 )  A if either:
x0 = x0 and x0 6= x  Xpreo ; or x0  V \ {x0 } and (x, x0 ) is an arc in SG
0

9. We remark that, due to the mentioned sources of over-estimation in costd , constructing such an example
requires fairly awkward constructs that do not appear likely to occur in practice.

171

fiHoffmann

If an optimal relaxed plan P + (s) for s contains o0 , then oDG+ as per Definition 1 will
be a sub-graph of lDG and gDG as defined here. This is simply because any optimal rplan
dependency graph has only arcs (x, x0 ) contained in the support graph of the task.10 As
previously indicated, the support graph may contain a lot more arcs than actually necessary.
SG captures what may ever support what else, not what will support what else in an optimal
relaxed plan. Consider our earlier point that, when constructing oDG+ , we take into account
only the operators in front of o0 in P + (s). This information is not contained in SG, thus
in Gripper we get the aforementioned cycle dropping a ball to support free-gripper for
picking up the same ball.
The reader who has waded through the cumbersome details in the previous section will
be delighted to hear that defining when an lDG respectively gDG is successful does not
involve any additional notation:
Definition 4. Let (X, sI , sG , O), s, x0 , t0 , o0 , and G = lDG or G = gDG be as in
Definition 3. We say that G = (V, A) is successful if all of the following hold:
(1) G is acyclic.
(2) If G = lDG then sG (x0 ) 6= s(x0 ), and there exists no transitive successor x0 of x0 in
SG so that x0  XsG and sG (x0 ) 6= s(x0 ).
(3) We have that t0 either:
(a) has self-irrelevant side effect deletes; or
(b) has replaceable side effect deletes; or
(c) has recoverable side effect deletes.
(4) For x  V \ {x0 }, all DT Gx transitions either are irrelevant, or have self-irrelevant
deletes, or are invertible and have irrelevant side effect deletes and no side effects on
V \ {x0 }.
Consider first only local dependency graphs G = lDG; we will discuss G = gDG below.
Assume that we have an optimal relaxed plan P + (s) for s that contains o0 , and thus oDG+
is a sub-graph of lDG. Then condition (1) obviously implies Definition 2 condition (1).
Condition (4) implies Definition 2 condition (3) because oDT G+
x does not contain any
irrelevant transitions. Condition (2) implies that (*) s(x0 ) is not oDG+ -relevant, i.e., s(x0 )
is not needed in the rest of the relaxed plan. This is simply because no other un-achieved
goal depends on x0 . With (*), condition (3a) implies Definition 2 condition (2a), because
R1+  C0 = , in the notation introduced previously. Conditions (3b) and Definition 2
condition (2b), respectively (3c) and Definition 2 condition (2c), are equivalent given (*).
Regarding exit distance, we do not know which parts of the domain transition graphs of
the variables x  V \ {x0 } will be traversed by P + (s). An obvious bound on diam(oDT G+
x)
is the length maxPath(DT Gx ) of a longest non-redundant path through the graph (a path
visiting each vertex at most once). Unfortunately, we cannot compute maxPath(.) efficiently. A Hamiltonian path (Garey & Johnson, 1979) exists in a graph G = (V, A) iff
10. For gDG, note that preo0 (x0 ), if defined, will be = s(x0 ) and thus x0 does not need to be recorded as
its own predecessor.

172

fiAnalyzing Search Topology Without Running Any Search

maxPath(G) = |V |  1. Thus the corresponding decision problem is NP-hard. TorchLight over-approximates maxPath(G) simply by |V |  1. However, we can sometimes use
diam(DT Gx ) instead of maxPath(DT Gx ), namely if we are certain that x is one of the
variables V  used in the definition of costd (oDG+ ). This is certain if:
(**) all DT Gx transitions either are irrelevant, or are invertible and have empty
conditions, irrelevant side effect deletes, and no side effects on V \ {x0 }.
Note that this is a strictly stronger requirement than Definition 4 condition (4). Clearly, it
implies Definition 2 condition (3) as well as condition (*) in SectionP5. Denote by V  the
subset of V \ {x0 } for which (**) holds. We define costD (G) := xV costD (x), where
costD (x) :=

1
x = x0



P
D
0
maxPath(DT Gx )  x0 :(x,x0 )A cost (x ) x 6= x0 , x 6 V 


 diam(DT G )  P
costD (x0 )
x 6= x , x  V 
x

0

x0 :(x,x0 )A

Because x0 must move  to attain its own goal  every optimal relaxed plan must take
at least one transition leaving s(x0 ). Thus, with Theorem 2 and the above, we have that:
Theorem 3. Let (X, sI , sG , O) be a planning task, and let s  S be a state with 0 < h+ (s) <
. Say that x0  X so that, for every o0 = rop(s(x0 ), c) in DT Gx0 where (s(x0 ), c) is
relevant, lDGo0 is a successful local dependency graph. Then s is not a local minimum, and
ed(s)  maxo0 costD (lDGo0 ). If, for every lDGo0 , we have Definition 4 condition (3a) or
(3b), then ed(s)  maxo0 costD (lDGo0 )  1.
Theorem 3 is our tool for guaranteed local analysis (II). For guaranteed global analysis
(I), we simply look at the set of all global dependency graphs gDG, requiring them to be
successful. In particular, all gDG are then acyclic, from which it is not difficult to deduce
that any non-goal state s will have a variable x0 fulfilling Definition 4 (2). For that x0 , we
can apply Theorem 3 and thus get:
Theorem 4. Let (X, sI , sG , O) be a planning task. Say that all global dependency graphs
gDG are successful. Then S does not contain any local minima and, for any state s  S with
0 < h+ (s) < , ed(s)  maxgDG costD (gDG). If, for every gDG, we have Definition 4
condition (3a) or (3b), then ed(s)  maxgDG costD (gDG)  1.
The full proofs of Theorems 3 and 4 are in Appendix A.3. If SG is acyclic and all
transitions are invertible and have no side effects, then Theorem 4 applies, whereby we have
now in particular proved our basic result. Vice versa, note that, if Theorem 4 applies, then
SG is acyclic. As far as local minima are concerned, one may thus reformulate Theorem 4
in simpler terms not relying on a notion of successful dependency graphs. Apart from
allowing to also determine an exit distance bound, the present formulation already paves
the way for future research: a gDG is defined relative to a concrete variable x0 and operator
o0 , and may thus allow for more accurate analysis of which other variables may actually
become important for x0 and o0 , in an optimal relaxed plan.
The use of diam(DT Gx ) instead of maxPath(DT Gx ) in costD (.), for the variables
in V  , has a rather significant effect on the quality of the bounds computed in many
173

fiHoffmann

benchmarks. A typical example is a transportation domain where vehicle positions are leaf
variables in SG whose transitions have no side effects. Such variables qualify for V  . Using
maxPath(DT Gx ) instead, we would obtain exceedingly large bounds even for trivial road
maps. For example, consider Logistics where the road map is fully connected. We have
diam(DT Gx ) = 1 and thus costD (.) delivers the correct bound 1. Using maxPath(DT Gx )
we instead get the bound N  1, N being the total number of locations in DT Gx .
Note that, within the scope of Theorem 4, i.e., the class of planning tasks to which
Theorem 4 applies, plan existence is tractable. Namely, there exists a plan for the task iff
there exists a relaxed plan for the initial state. This is because, starting from an optimal
relaxed plan, we are guaranteed to be able to construct an exit path; iterating this argument
gets us to the goal. In our view, this tractability is a weakness of this form of global
analysis. The analysis does not apply in intractable classes of tasks that do not contain
local minima. Note that such classes do exist, cf. Theorem 1. On the other hand, plan
existence is tractable in all known benchmark domains where local minima are absent, so in
practice this does not appear to be a major limitation. Also, note that plan construction,
as well as optimal planning, are still intractable within the scope of Theorem 4. Plan
construction is intractable because the plans may be exponentially long, cf. Example 6 in
Appendix A.4. As for optimal planning, just consider Logistics and Miconic-STRIPS. We
will see shortly (Proposition 1, next section) that these are fully covered by Theorem 4.
However, in both of them, deciding bounded plan existence is NP-hard (Helmert, 2003).
Interestingly, the fact that Theorem 2, and therewith indirectly also Theorem 4, rely on
optimal relaxed plans is not a source of intractability of plan construction here. If Theorem 4
applies, then any non-redundant relaxed plan P + has a successful oDG+ , enabling us to
construct a path to a state where that particular relaxed plan (although not necessarily
an optimal relaxed plan) can be shortened. Iterating this argument gives us a constructive
method for obtaining a plan, where the only worst-case exponential behavior lies in the
length of the individual path segments. That said, of course the plan constructed in this
way may be highly non-optimal. Indeed, as is shown in Example 7 in Appendix A.4, this
plan may be exponentially longer than an optimal plan. Thus, even if Theorem 4 applies
and we do not need an optimality guarantee, running a planner still makes sense.
We will discuss the relation of the scope of Theorem 4 to known tractable classes in
Section 9. A basic fact is that one can construct local minima even in very small examples
involving only two variables and complying with our basic result except that either the
support graph is cyclic (Example 2, Appendix A.4), or there is a non-invertible transition
whose own delete is relevant (Example 3, Appendix A.4), or there is a transition with a
relevant side effect delete (Example 4, Appendix A.4). These examples are contained in
many known tractable classes, thus underlining that the automatic analysis of h+ topology
and the identification of tractable classes are different (although related) enterprises.

7. Benchmark Performance Guarantees
We now state some guarantees that our analyses (I)(III) give in benchmark domains.
The underlying finite-domain variable formalizations are straightforward, and correspond
174

fiAnalyzing Search Topology Without Running Any Search

to formulations that can be found automatically by Fast Downward. They are listed in
Appendix A.5, where we also give the proofs of the following two simple observations.11
In four of our benchmark domains, guaranteed global analysis (I) will always succeed :
Proposition 1. Let (X, sI , sG , O) be a planning task from the Logistics, Miconic-STRIPS,
Movie, or Simple-TSP domain. Then Theorem 4 applies, and the bound delivered is at most
1, 3, 1, and 1 respectively.
It follows trivially from Proposition 1 that guaranteed local analysis (II) succeeds in
these domains as well. If s is any state in one of the four listed domains, then Theorem 3
applies to s, and the bound delivered is as stated.
Note that the bounds for Logistics and Movie are the correct ones, i.e., they are tight.
For Miconic-STRIPS, the over-estimation of the actual bound (which is 1, not 3) arises
because the analysis does not realize that boarding a passenger can be used as the leaf
variable x0 . For Simple-TSP, the correct bound is 0 (since h+ is the exact goal distance).
The over-estimation arises because, in every goal variable x0 =visited(location), the gDG
includes also the variable at, not realizing that the value of at does not matter because
any location can be visited from any other one.
For the transportation benchmarks involving capacity constraints, approximate local
analysis (III) will always succeed, if provided with suitable optimal relaxed plans:
Proposition 2. Let (X, sI , sG , O) be a planning task from the Elevators, Ferry, Gripper,
or Transport domain, and let s  S. In Ferry and Gripper, for every optimal relaxed plan
P + (s) there exists oDG+ so that Theorem 2 applies, the bound being at most 1. In Elevators
and Transport, there exists at least one P + (s) and oDG+ so that Theorem 2 applies, the
bound being at most 1 in Elevators and at most the road map diameter in Transport.
The relevant deletes of t0 , in all these cases, are due to the effects decreasing the remaining vehicle capacity, like free-gripper in the Gripper domain. A decrease of capacity is
always due to a load type of operator, which is matched by an unload type of operator
later on inside the relaxed plan. Thus these deletes are always recovered inside P + (s) (we
have Definition 2 condition (2a)). Further, relaxed plans never use an unload action to
free a capacity for loading the same object, thus the oDG+ s are cycle-free. Hence the
oDG+ s are successful, and Theorem 2 applies. For Elevators and Transport, Proposition 2
is slightly weaker because a vehicle may have capacity > 1, allowing  but not forcing 
relaxed plans to use unloading operators recovering a capacity not actually present.
We note that similar patterns are likely to occur in any domain with renewable resources,
and will be recognized by Definition 2 condition (2a) in the same way.
Proposition 2 does not hold for Theorems 3 and 4, i.e., for lDGs and gDGs. This is due
to two deficiencies (cf. the discussion at the end of Section 4). First, SG contains cycles
unloading an object in order to free the capacity for loading it. Second, Definition 2
condition (3a) is more restrictive than Definition 2 condition (2a), postulating the deletes
of t0 to be entirely irrelevant. If we had a way of removing these deficiencies, then the
guaranteed analyses (I,II) would succeed in the four domains from Proposition 2.
11. We say can be found automatically here because Fast Downwards translator is not deterministic, i.e.,
it may return different finite-domain variable encodings even when run several times on the same planning
task. Some but not all of these encodings correspond to our domain formalizations. For Elevators, we
do not give a full definition because, without action costs, this is merely a variant of Transport.

175

fiHoffmann

8. Experiments
We report on a large-scale experiment with TorchLight. We fill in a few details on TorchLights implementation, and we describe a simple alternative analysis technique based on
search probing. We explain the experiments set-up, report runtime results for the different
stages of TorchLight, and describe TorchLights analysis results on a per-domain basis. We
assess the quality of that analysis in terms of its predictive capability. We finally summarize
the outcome of TorchLights diagnosis facility in our benchmarks.
8.1 TorchLight
TorchLight is implemented in C based on FF.12 TorchLight currently handles STRIPS only,
i.e., no ADL domains. It uses Fast Downwards translator (Helmert, 2009) to find the finitedomain variables. Establishing the correspondence between these variables (respectively
their values) and FFs internally used ground facts is mostly straightforward. There are a
few details to take care of; we omit these for brevity.
After parsing Fast Downwards variables, TorchLight creates data structures representing the support graph and the domain transition graphs. It then enters a phase we refer
to as static analysis, where it determines fixed properties such as, for every transition t,
whether t is irrelevant, invertible, etc. The next step is guaranteed global analysis (I),
checking the preconditions of Theorem 4 by enumerating all global dependency graphs and
testing whether they are successful. To be able to report the percentage of successful gDGs,
we do not stop at the first unsuccessful one.
The local analysis techniques  guaranteed local analysis (II) using Theorem 3 and
approximate local analysis (III) using Theorem 2  are run on a set LS of states comprising
the initial state as well as a number R of sample states obtained by random walks starting
in sI . The set LS is identical for both analyses, and we run each technique on each state
s  LS regardless of what the outcome of running the respective other technique on s is.
Given s, analysis (II) checks Theorem 3 by constructing the local dependency graph for
every suitable variable x0 and every transition t0 leaving s(x0 ). If we find a non-successful
t0 , we stop considering x0 . We minimize exit distance bounds across different x0 .
Analysis (III) checks Theorem 2 on a relaxed plan P + (s) computed by FFs heuristic
function. In case that no relaxed plan exists for s, the analysis reports failure. Otherwise,
the analysis proceeds over all operators o0 in P + (s), from start to end, and over all variables
x0 affected by o0 . For each pair o0 , x0 we build the optimal rplan dependency graph oDG+ as
per Definition 1. We skip variables x0 where eff o0 (x0 ) is not actually used as a precondition
or goal, in the rest of P + (s). If oDG+ is successful, we stop. (Relaxed plans can be big
in large examples, so continuing the analysis for exit bound minimization was sometimes
costly.) As mentioned in Section 5, before we build oDG+ we re-order P + (s) by moving
operators behind o0 if possible. This is of paramount importance because it avoids including
unnecessary variables into oDG+ . The re-ordering process is straightforward. It starts at
the direct predecessor o of o0 , and tests whether P + (s) is still a relaxed plan when moving
o directly behind o0 . If yes, this arrangement is kept. Then we iterate to the predecessor
of o, and so forth. It is easy to see that, this way, oDG+ will contain exactly the variables
12. The source code of TorchLight is an online appendix to this paper. It is available for download also at
http://www.loria.fr/~hoffmanj/TorchLight.zip.

176

fiAnalyzing Search Topology Without Running Any Search

and transitions used in P + (s) to achieve preo0 . Finally, when we check whether the oDG+ +
relevant deletes of t0 are P>0
(s)-recoverable, we use a simple technique allowing to recognize
situations where failure due to one operator can be avoided by replacing with an alternative
operator. For example, if in Transport o0 is a loading operator reducing capacity level k to
k  1, then P + (s) may still contain an unloading operator relying on level k. Thus level k
will be contained in R1+  C0 , causing failure. However, the unloading can just as well be
performed based on capacity level k  1, removing this difficulty. We catch cases like this
during construction of R1+ . Whenever we find o whose precondition overlaps C0 , we test
whether we can replace o with a similar operator.
The local analyses return simple statistics, namely the minimum, mean, and maximal
exit distance bound found, as well as the success rate, i.e., the fraction of sample states
where guaranteed local analysis (II)/approximate local analysis (III) succeeded. Analysis
(III) success rates will be a main focus, because these turn out to be very informative.
We run R = 1, 10, 100, 1000 in the experiment. The length of each random walk is
chosen uniformly between 0 and 5  hFF (sI ), i.e., 5 times the FF heuristic value for the
initial state. We do not play with the parameter 5. It is important, however, that this
parameter is not chosen too small. In domains with many dead ends  where one may do
things that are fatally wrong  it is likely that the bad things will happen only if doing
a sufficiently large number of random choices. Consequently, the dead-end rate, i.e., the
fraction of sample states for which no relaxed plan exists, tends to be larger for longer
random walks. Since analysis (III) fails on states that have no relaxed plan, this exerts an
important influence on analysis (III) success rates. We illustrate this below by comparing
some results for sampled states to results obtained using the initial states only.
8.2 Search Probing
For approximate analysis of sample states, there exists a simple (and rather obvious) alternative to TorchLights causal graph based technology. One can use search to determine
whether or not a given sample state s is a local minimum, and what its exit distance is. Since
we cannot compute h+ effectively, such a search-based analysis is necessarily approximate.
The straightforward method is to replace h+ with a relaxed-plan based approximation.
Herein, we replace h+ with hFF , i.e., with FFs heuristic function. Precisely, given a state
s, we run a single iteration of FFs Enforced Hill-Climbing, i.e., a breadth-first search for
a state with better heuristic value. In this search, like FF does, we use helpful actions
pruning to avoid huge search spaces. Unlike FF, to focus on the detection of states not on
local minima, we allow only monotone paths (thus restricting the search space to states s0
where hFF (s0 ) = hFF (s)). We refer to this technique as search probing, SP in brief. We also
experiment with a variant imposing a 1 second runtime cut-off on the search. We refer to
this as limited search probing, SP1s in brief. SP and SP1s are run on the same set LS of
states as TorchLights local analyses (II,III).
As it turns out, empirically  in the present benchmarks  SP and SP1s are very competitive with TorchLights analysis (III). Since that analysis is a main focus of our experiments,
it is relevant to understand the commonalities and differences between these techniques.
As far as analysis quality guarantees are concerned, all 3 techniques  analysis (III),
SP, SP1s  have similar properties: there are no guarantees whatsoever. Each may report
177

fiHoffmann

success although s is a local minimum (false positives), and each may fail although s is
not a local minimum (false negatives). In all cases, false positives are due to the use of
non-optimal relaxed plans (hFF instead of h+ ). False negatives are inherent in analysis (III)
because this covers only certain special cases; they are inherent in SP1s due to the search
limit. SP can have false negatives due to helpful actions pruning, however that could in
principle be turned off; the more fundamental source of false negatives are the non-optimal
relaxed plans. These are also responsible for a lack of connections across the techniques.
The only implication is the trivial one that SP1s success on a state s implies SP success on
s. In particular, if analysis (III) correctly identifies s to not be a local minimum, then this
does not imply that SP will do so as well. The causal graph analysis may be less affected
by irregularities in the hFF surface. This happens, for example, in the Transport domain of
IPC 2008, resulting in higher success rates for analysis (III).
There are some obvious  but important  differences regarding runtime performance
and the danger of false negatives. SP runtime is worst-case exponential in the size of the
(grounded) input, whereas analysis (III) and SP1s runtime is low-order polynomial in that
size. For SP, decreasing the number R of sample states merely reduces the chance of hitting
a bad state (a sample state on a large flat region), whereas analysis (III) and SP1s scale
linearly in R. On the other hand, both analysis (III) and SP1s buy their efficiency with
incompleteness, i.e., increased danger of false negatives. Analysis (III) simply recognizes
only special cases. SP1s effectively bounds the lookahead depth, i.e., the search depth in
which exit states can be detected.
As indicated, SP and SP1s turn out to be competitive in the benchmarks. Large search
spaces are rare for SP. The success rates of SP and SP1s are similar, and as far as predictive
capability is concerned are similarly informative as those of analysis (III). Thus goodquality success rates can be obtained with much simpler techniques than TorchLight.13
This notwithstanding, (a) TorchLight has other functions  the guaranteed analyses (I,II)
as well as diagnosis  that cannot be simulated, and (b) results in benchmarks only ever
pertain to these examples. TorchLights analysis (III) offers unlimited lookahead depth at
low-order polynomial cost. This does not appear to matter much in the present benchmarks,
but there are natural cases where it does matter. We get back to this below.
8.3 Experiments Set-Up
We run experiments in a set of 37 domains. These include the domains investigated in
the hand-made analysis of h+ topology (Hoffmann, 2005), as shown in Figure 1, which
include all domains from the international planning competitions (IPC) up to IPC 2004.
Our remaining domains are the STRIPS (versions of the) domains from IPC 2006 and IPC
2008, except IPC 2008 Cyber-Security which we omit due to parsing difficulties.14 The test
instances were collected from the IPC collection(s) where applicable (removing action cost
constructs from the IPC 2008 domains), and randomly generated elsewhere. In total, our
test set contains 1160 instances.
13. In particular, search probing appears to be a rather useful technique, raising the question why such
techniques have not yet been used for performance prediction purposes. Roberts and Howe (2009), for
example, use very simple features only. We get back to this in the conclusion.
14. The instances are too large for FFs parser in its standard configuration. When tweaking bison to allow
larger parse trees, we obtained a segmentation fault even in the smallest instance of IPC 2008.

178

fiAnalyzing Search Topology Without Running Any Search

tool/phase
FD Translator
SG/DTG
Static Analysis
Analysis (I)
Sample States
Analysis (II)
Analysis (III)
TorchLight total
TorchLight (III)
TorchLight (III) no FD
SP
SP total
SP1s
SP1s total
FF
LAMA

single-shot/R = 1
mean
max
6.12
690.59
0.14
6.91
0.25
31.42
0.40
53.29
0.01
0.53
0.00
0.18
0.02
1.31
6.92
727.63
6.52
724.54
0.40
33.95
0.06
58.02
0.07
58.03
0.01
1.08
0.01
1.48
268.20

185.05


R = 10
mean
max

0.08
0.01
0.03
7.04
6.64
0.49
0.23
0.32
0.07
0.15

4.81
1.11
2.46
736.98
732.98
40.50
138.54
138.59
4.46
9.27

R = 100
mean
max

0.76
0.10
0.23
8.00
7.51
1.37
5.47
6.23
0.66
1.42

50.35
9.56
20.09
807.70
795.16
103.67


56.18
106.53

R = 1000
mean
max

7.50
0.98
2.15
17.57
16.19
10.04
26.24
33.74
5.89
13.39

491.20
94.59
194.79
1510.74
1413.23
719.27


391.59
882.79

Table 2: Summary of runtime data. Mean/max is over all instances of all domains. For
empty fields, the respective tool/phase is single-shot, i.e., does not depend on R.
A dash means time-out, 1800 seconds, which is inserted as the runtime for each respective instance into the mean computation. Rows FD Translator . . . Analysis
(III) time the different stages of TorchLight. TorchLight total is overall runtime, TorchLight (III) does not run analyses (II) and (III), TorchLight (III) no
FD is the latter when disregarding the translation costs. SP determines a success rate (fraction of sample states deemed to not be on local minima) via search
probing, i.e., search around each sample state; SP1s  imposes a 1 second time-out
on these searches. SP total and SP1s total include the time for generating the
sample states.
All experiments are run on a 1.8 GHZ CPU, with a 30 minute runtime and 2 GB
memory cut-off. We run 4 different planners/tools. Apart from TorchLight (and SP/SP1s ),
these include FF (Hoffmann & Nebel, 2001a), and LAMA (Richter et al., 2008; Richter
& Westphal, 2010). The purpose of running these planners is to assess to what extent
TorchLights output  in particular analysis (III) success rate  can predict planner success
or failure. To examine this also for a very plain planner, we also run a version of FF that uses
no goal ordering techniques, and that runs only Enforced Hill-Climbing, without resorting
to best-first search if that fails. We will refer to this planner as EHC in what follows.
8.4 Runtime
Our code is currently optimized much more for readability than for speed. Still, TorchLight
is fast. Up to R = 100, the bottleneck is Fast Downwards translator. With R = 1, 10, 100,
the actual analysis takes at most as much time as the translator in 99.74%, 99.74%, and
96.21% of the instances respectively. To assess this in more detail, consider Table 2 which
gives the timing of the different stages of TorchLight, and of the other planners/tools.
The translation runtime sometimes hurts considerably, with a peak of 690.59 seconds
in the most costly instance of the Scanalyzer domain. This is rather exceptional, however.
The second most costly domain is Blocksworld-NoArm, with a peak of 138.33 seconds. In
179

fiHoffmann

20 of the 37 domains, the most costly instance is translated in less than 10 seconds. In
57.24% of the instances, Fast Downwards translator takes at most 1 second.
For static analysis, the peak behavior of 31.42 seconds (also in Scanalyzer) is even more
exceptional: in 95.34% of the instances, static analysis takes at most 1 second. The second
highest domain peak is 7.88 seconds in Pipesworld-Tankage. Similarly, while analysis (I)
takes a peak of 53.29 seconds  in Blocksworld-NoArm  in 96.12% of the instances it
completes in at most 1 second. The only domain other than Blocksworld-NoArm where the
peak instance takes more than 10 seconds is Airport, with a peak of 41.71 seconds; the next
highest domain peaks are Pipesworld-Tankage (6.8), Scanalyzer (2.91), Logistics (1.89), and
Woodworking (1.17). In all other domains, analysis (I) always completes within a second.
Turning focus on the local analyses, we see that they are even more effective. In particular, we will concentrate below mostly on approximate local analysis (III). We will see
that R = 1000 does not offer advantages over R  100 as far as the information obtained
goes, so we will mostly concentrate on R  100. For R = 1, 10, 100, analysis (III) completes in at most 1 second for 99.66%, 99.40%, 95.60% of the instances respectively. For
R = 1000 this still holds for 76.55% of the instances. The peak runtime of 20.09 seconds
for R = 100 occurs in Scanalyzer. The next highest domain peaks are Blocksworld-NoArm
(9.23), Pipesworld-Tankage (4.24), Ferry(3.21), Logistics (2.99), Blocksworld-Arm (2.77),
Optical-Telegraph (1.97), and Airport (1.41). In all other 29 domains, analysis (III) with
R = 100 always completes within a second.
The bottleneck in local analysis is the generation of sample states. This can be costly
because it involves the repeated computation of applicable operators during the random
walks. Its R  100 peak of 50.35 seconds is in the Scanalyzer domain. However, once
again, this peak behavior is exceptional. With R = 1, 10, 100, the sampling completes
within at most 1 second for 100%, 98.28%, 87.41% of the instances respectively.
The main competitor of TorchLight analysis (III) success rates is search probing, i.e.,
SP and SP1s . Consider for the moment only the analysis methods themselves, i.e., row
Analysis (III) vs. rows SP and SP1s  in Table 2. Compared to SP1s , analysis (III) is
consistently in the advantage (except for maximum runtime with R = 1), but the difference
is not dramatic. This is to be expected, given that SP1s trades completeness against a small
fixed maximum runtime. Compared to the complete search in SP, analysis (III) consistently
has a significant advantage. However, for R  10 the mean runtime of SP is tolerable, and
even the maximum runtime is not too bad. Further, bad runtime behavior is exceptional.
For R = 1, 10, SP completes in at most 1 second for 99.83% and 98.45% of the instances
respectively. In 35 (R = 1) respectively 32 (R = 10) of the 37 domains even the maximum
runtime is below 1 second. With R = 100, SP has two time-outs, both in Blocksworld-Arm.
With R = 1000, there are 11 time-outs, in Blocksworld-Arm, Blocksworld-NoArm, Freecell,
and Pipesworld-NoTankage. With R = 100, the maximum runtime is above 10 seconds in
7 domains; with R = 1000, in 12. However, with R = 100, 1000, SP still completes in at
most 1 second for 92.33% and 71.98% of the instances respectively (compared to 95.60%
and 76.55% for analysis (III), cf. above).
Neither analysis (III) nor search probing are stand-alone methods. The former requires
all of TorchLight except analyses (I,II). The latter requires the sampling of random states.
The respective total data is given in rows TorchLight (III) and SP total/ SP1s total in
Table 2. Here the picture changes dramatically in favor of SP and especially SP1s . It should
180

fiAnalyzing Search Topology Without Running Any Search

be noted, though, that this is mostly due to the overhead for the translation to finite-domain
variables. This overhead is an artifact of the implementation. Our approach is defined
for finite-domain variables, while the benchmarks are not, even though the finite-domain
representation is in most cases more natural than the Boolean one. Further, many planners
(notably Fast Downward and its quickly growing set of derivatives) use the translation
anyway. The runtimes without translation are given in the row TorchLight (III) no FD.
As one would hope and expect, the analysis methods are much faster than actual planners. LAMA has 112 time-outs in our test suite, FF has 173.
8.5 Analyzing Domains
We now discuss the actual analysis outcomes, on a per-domain basis. We first consider
only TorchLight, then give some details on the comparison of analysis (III) success rates to
those obtained by search probing. Before we begin, a few words are in order regarding the
comparison between SP and SP1s . With R = 1, 10, 100, 1000, the success rates are identical
in 99.83%, 99.14%, 97.5%, 94.66% of our 1160 benchmark instances respectively; in 99.83%,
99.14%, 99.31%, 98.97% of the instances, the success rates differ by at most 5%. Thus, a
small runtime cut-off does not adversely affect the success rates of search probing (because
long searches are rare). This being so, we henceforth do not discuss the data for SP vs.
SP1s separately. We compare TorchLights analysis (III) success rates to those of SP only.
The guarantees of Proposition 1 are confirmed, i.e., guaranteed global analysis (I) succeeds as described in Logistics, Miconic-STRIPS, Movie, and Simple-TSP. It never succeeds
in any other domain, though. In some domains, fractions of the gDGs are successful. Precisely, the maximum fraction of successful gDGs is 97% in Satellite, 50% in Ferry, 33.33% in
TPP, 22.22% in Driverlog, 20% in Depots, 13.33% in Tyreworld, and 12.5% in BlocksworldArm. However, if the fraction is below 100% then nothing is proved, so this data may at
best be used to give an indication of which aspects of the domain are good-natured.
Guaranteed local analysis (II) generally is not much more applicable than global analysis.
Thus we now concentrate on approximate local analysis (III) exclusively.
Proposition 2 is backed up impressively. Even with R = 1000, analysis (III) succeeds
in every single sample state of Ferry, Gripper, Elevators, and Transport.15 This indicates
strongly that the potentially sub-optimal relaxed plans do not result in a loss of information
here. Indeed, the analysis yields high success rates in almost all domains where local minima
are non-present or limited. This is not the case for the other domains, and thus TorchLight
can distinguish domains with easy h+ topology from the hard ones. Consider Figure 3,
showing mean analysis (III) success rates per-domain with R = 1. (The picture is similar
for R = 10, 100, 1000; cf. Table 3 below.)
The domains whose h+ topology is not known are shown separately on the right hand
side in Figure 3. For the other domains, we see quite nicely that harder domains tend
to have lower success rates. In particular, the easiest domains in the bottom class all have
100% success rates (95% in the case of Zenotravel), whereas the hardest domains in the
top right corner only have around 50% or less. In the latter domains, to some extent the
15. Historically, this observation preceded Proposition 2, as well as the h+ topology categorization of Elevators and Transport as per Figure 1. That is, these hand-made analyses were motivated by observing
TorchLights analysis outcome.

181

fiHoffmann

PipesTank [40]
PipesNoTank [76]
PSR [50]

Rovers [100]
OptTele [7]

Mystery [39]
Mprime [49]
Freecell [55]
Airport [0]

Hanoi [0]
BlocksNoArm [57]
Grid [80]
Transport [+,100]
bench ed <= c

local minima ed <= c

BlocksArm [30]
Depots [82]
Driverlog [100]

Elevators [+,100]
Logistics [*,100]
Ferry [+,100]
Gripper [+,100]
undirected

Woodwork [13]
Trucks [0]
TPP [80]
Storage [93]
Sokoban [13]
Scanalyzer [30]

Tyreworld [100]
DinPhil [24]
Satellite [100]
Zenotravel [95]
MiconicSTR [*,100]
Movie [*,100]
SimpleTsp [*,100]
harmless

recognized

PegSol [0]
Pathways [10]
ParcPrinter [3]
Openstacks [0]
unrecognized

Figure 3: Overview of TorchLight domain analysis results. *: guaranteed global analysis
(I) always succeeds. +: approximate local analysis (III) always succeeds if
provided an optimal relaxed plan. Numbers shown are mean success rates per
domain, for approximate local analysis (III) with R = 1, i.e., when sampling a
single state per domain instance.
low success rates result from the recognition of dead ends by FFs heuristic function. For
example, if during random sampling we make random vehicle moves consuming fuel, like
in Mystery and Mprime, then of course chances are we will end up in a state where fuel
is so scarce that even a relaxed plan does not exist anymore. This is most pronounced in
Airport, where all sample states here have infinite heuristic values. However, the capabilities
of the analysis go far beyond counting states on recognized dead ends. In Blocksworld-Arm,
for example, there are no dead ends at all and still the success rate is only 30%, clearly
indicating this as a domain with a difficult topology.
To some extent, based on the success rates we can even distinguish Pipesworld-Tankage
from Pipesworld-NoTankage, and Mprime from Mystery (in Mprime, fuel can be transferred
between locations). The relatively high success rate in Depots probably relates to its transportation aspects. In Grid, in 20% of cases our analysis is not strong enough to recognize
the reasons behind non-existence of local minima; these reasons can be quite complicated
(Hoffmann, 2003). Dining-Philosophers does not really have a favorable h+ topology. Its
rather excessive bound 31 is due to the very particular domain structure where philosophers
behave in strictly symmetrical ways (Hoffmann, 2005). Apart from this, the only strong
outliers are Driverlog, Rovers, Hanoi, and Blocksworld-NoArm. All of these are more problems of the hand-made analysis than of TorchLights. In Driverlog and Rovers, deep local
minima do exist, but only in awkward situations that dont tend to arise in the IPC instances. Thus the hand-made analysis, which is of a worst-case nature, is too pessimistic
here. The opposite happens in Hanoi and Blocksworld-NoArm, where the absence of local
minima is due to rather idiosyncratic reasons. For example, in Hanoi the reason is that h+
is always equal to the number of discs not yet in goal position  in the relaxation, one can
always accomplish the remaining goals one-by-one, regardless of the constraints entailed
by their positioning. Hanoi and Blocksworld-NoArm are not actually easy to solve for
182

fiAnalyzing Search Topology Without Running Any Search

domain
Airport
Blocks-Arm
Blocks-NoArm
Depots
Din-Phil
Driverlog
Elevators
Ferry
Freecell
Grid
Gripper
Hanoi
Logistics
Miconic
Movie
Mprime
Mystery
Opt-Tele
Pipes-NoTank
Pipes-Tank
PSR
Rovers
Satellite
Simple-TSP
Transport
Tyreworld
Zenotravel
Openstacks
Parc-Printer
Pathways
Peg-Sol
Scanalyzer
Sokoban
Storage
TPP
Trucks
Woodworking

sI
(III)
96.0
38.3
70.0
100
100
100
100
100
97.5
60.0
100
0.0
100
100
100
74.3
75.0
0
40.0
34.0
66.0
100
85
100
100
100
90
100
100
100
0
0
30.0
100
100
56.3
100

R=1
(III)
SP
0.0
0.0
30.0 93.3
56.7
100
81.8
100
24.1 27.6
100
100
100
100
100
100
55.0 60.0
80.0
100
100
100
0.0 33.3
100
100
100
100
100
100
48.6 74.3
39.3 42.9
7.1 14.3
76.0 98.0
40.0 92.0
50.0 62.0
100
100
100
100
100
100
100 93.3
100
100
95
100
0
4.4
3.3
6.7
10.0 10.0
0
10
30.0 96.7
13.3 33.3
93.3 96.7
80.0 80.0
0
0
13.3 13.3

R = 10
(III)
SP
2.0
2.0
28.2 94.5
57.2
100
85.9 99.1
22.8 23.1
97.5
100
100
100
100
100
57.4 62.8
74.0 92.0
100
100
11.1 44.4
100
100
100
100
100
100
61.1 76.3
37.1 43.9
1.4
2.9
75.4 97.4
50.6 90.0
57.6 69.8
100 99.5
98.5
100
100
100
100 93.0
95.6
100
94.5 99.5
14.8 21.3
8.0
8.3
6.0
6.0
13.3 22.7
33.0 99.7
20.3 38.3
89.0 96.3
68.0 67.0
2.5
3.1
14.3 14.3

R = 100
(III)
SP
2.8
2.9
26.9 91.7
55.9 99.9
86.3 99.7
22.8 22.9
97.4 99.9
100
100
100
100
57.9 63.5
69.0 93.8
100
100
10.2 41.9
100
100
100
100
100
100
64.3 79.0
37.6 45.6
0.9
1.4
75.2 97.4
49.4 88.1
58.3 71.1
100 99.8
98.4
100
100
100
100 94.8
96.3
100
95.8 98.4
17.7 22.0
6.3
7.2
5.4
5.4
13.1 22.3
33.5 97.9
19.1 38.2
89.8 96.8
65.4 63.8
1.9
2.9
15.3 15.4

(III)
2.9
26.5
56.2
86.2
22.0
97.9
100
100
58.0
69.5
100
10.6
100
100
100
64.1
36.3
1.1
75.1
48.7
57.0
100
98.0
100
100
95.5
95.4
16.6
6.0
4.6
12.6
33.9
18.5
89.3
65.5
1.4
15.3

R = 1000
SP
3.0
82.1
98.3
99.6
22.3
99.8
100
100
63.2
93.5
100
41.9
100
100
100
78.2
44.4
1.7
95.4
88.2
70.4
99.8
99.8
100
94.4
100
98.2
20.8
6.8
4.6
22.2
98.5
37.7
96.9
63.9
2.7
15.4

DE
97.0
0
0
0
77.2
0
0
0
35.4
0
0
0
0
0
0
7.2
46.8
98.3
0
8.7
0
0
0
0
0
0
0
79.1
93.0
95.3
75.2
0
54.2
0
34.5
97.3
84.6

Table 3: Mean success rates per domain. Upper part: domains whose h+ topology was previously examined by hand (Hoffmann, 2005) or is trivial to examine based on these
results; lower part: IPC 2006/2008 domains where that is not the case. Columns
sI  show data for analyzing the initial state only, columns R = 1, 10, 100, 1000
for analyzing the respective number of sample states. Columns (III) give data
for approximate local analysis (III), columns SP give data for search probing,
column DE gives dead-end rates for R = 1000.
FF, and in that sense, from a practical perspective, the low success rates of TorchLights
analysis (III) provide the more accurate picture.
Table 3 gives a complete account of per-domain averaged success rates data, including
all domains, all values of R, the rates obtained on initial states, and using SP instead of
TorchLight. This serves to answer three questions:
(1) Is it important to sample random states, rather than only analyzing the initial state?
(2) Is it important to sample many random states?
183

fiHoffmann

(3) How competitive is analysis (III) with respect to a search-based analysis?
The answer to question (1) is a clear yes. Most importantly, this pertains to domains
with dead ends, cf. our brief discussion above. It is clear from Table 3 that, in such domains,
analyzing sI results in a tendency to be too optimistic. To see this, just consider the entries
for Airport, Dining-Philosophers, Freecell, Mystery, Openstacks, Parc-Printer, Pathways,
TPP, Trucks, and Woodworking. All these domains have dead ends, for a variety of reasons.
The dead ends do not occur frequently at initial state level, but do occur frequently during
random walks  cf. column DE in Table 3. (Interestingly, in a few domains  most notably
the two Pipesworlds  the opposite happens, i.e., success rates are lower for sI than for the
sample states. It is not clear to us what causes this phenomenon.)
If we simply compare the sI column with the R = 1000 column for analysis (III), then
we find that the result is a lot different  more than 10%  in 22 of the 37 domains. To
some extent, this difference between initial states and sample states may be just due to the
way these benchmarks are designed. Often, the initial states of every instance are similar
in certain ways (no package loaded yet, etc). On the other hand, it seems quite natural, at
least for offline problems, that the initial state is different from states deeper down in the
state space (consider transportation problems or card games, for example).
The answer to question (2) is a clear no. For example, compare the R = 1 and
R = 1000 columns for analysis (III). The difference is greater than 10% in only 6 of the
37 domains. The peak difference is in Openstacks, with 16.6% for R = 1000 vs. 0% for
R = 1. The average difference over all domains is 4.17%. Similarly, comparing the R = 1
and R = 1000 columns for SP results in only 5 of 37 domains where the difference is greater
than 10%, the peak being again in Openstacks, 20.8% for R = 1000 vs. 4.4% for R = 1.
The average difference over all domains is 3.7%.
The answer to question (3) is a bit more complicated. Look at the columns for analysis
(III) respectively SP with R = 1000. The number of domains where the difference is larger
than 10% is now 11 out of 37, with a peak of 64.6% difference in Scanalyzer. On the one
hand, this still means that in 26 out of 37 domains the analysis result we get is very close
to that of search (average difference 2.18%), without actually running any search! On the
other hand, what happens in the other 11 domains? In all of these, the success rate of SP is
higher than that of TorchLight. This is not surprising  it basically means that TorchLights
analysis is not strong enough here to recognize all states that are not on local minima.
Interestingly, this weakness can turn into an unexpected advantage. Of the 11 domains in
question, 8 domains  Blocksworld-Arm, Depots, Mprime, Pipesworld-Tankage, PipesworldNoTankage, PSR, Scanalyzer, and Sokoban  do contain deep local minima.16 Thus, in these
8 domains, we would wish our analysis to return small success rates. TorchLight grants this
wish much more than SP does. Consider what happens when using SP instead of analysis
(III) in Figure 3. For Mystery, PSR, and Sokoban, the change is not dramatic. However,
Blocksworld-Arm is marked with average success rate 93 instead of 30, putting it almost
on par with the very-simple-topology domains in the bottom class. Similarly, PipesworldTankage, Pipesworld-NoTankage, and Scanalyzer are put almost on par with these. Depots
16. Sokoban has unrecognized dead-ends (in the relaxation, blocks can be pushed across each other) and
therefore local minima. In Scanalyzer, analyzing plants misplaces them as a side effect, and bringing
them back to their start position, across a large circle of conveyor belts, may take arbitrarily many steps.
See Figure 3 for the other 6 domains.

184

fiAnalyzing Search Topology Without Running Any Search

actually receives a 100, putting it exactly on par with them. Thus the SP analysis outcome
actually looks quite a bit worse, in 5 of the domains.
What causes these undesirably high success rates for SP? The authors best guess is that,
in many domains, the chance of randomly finding a state on a local minimum is low. In
large-scale experiments measuring statistics on the search space surface under FFs heuristic
function (Hoffmann, 2003), it was observed that many sampled states were not local minima
themselves, but where contained in valleys. Within a valley, there is no monotonically
decreasing path to a goal state. Such a state may not be a local minimum because, and only
because, one can descend deeper into the valley. It seems that SP correctly identifies most
valley states to not be local minima, thus counting as good many states that actually are
located in difficult regions of the search space. This is a weakness not of SP, but of success
rate as a search space feature.17 Why does this weakness not manifest itself as much in
analysis (III)? Because that analysis is more picky  it takes as good only states that
qualify for particular special cases. These tend to not occur as often in the difficult domains.
Of course, it is easy to construct examples turning the discussed strength into a real
weakness of TorchLights analysis quality. This just does not seem to happen a lot in the
present benchmarks. Now, having said that, the present benchmarks arent well suited to
bring out the theoretical advantage of analysis (III) either. The analysis offers unlimited
lookahead depth at low-order polynomial cost. However, even with R = 1000, in 23 of the 37
domains the highest exit distance bound returned is 0, i.e., every exit path identified consists
of a single operator. These cases could be handled with a much simpler variant of analysis
(III), looking only at operators o0 that are directly applicable in s, and thus removing the
entire machinery pertaining to SG predecessors of x0 . Still, that machinery does matter in
cases that are quite natural. The highest exit distance bound returned is 10 in Grid and 7 in
Transport. More generally, in any transportation domain with a non-trivial road-map, it is
easy to construct relevant situations. For example, say the road map in Transport forms N
cities, each with diameter D and at least one vehicle, distances between cities being large
relative to D. Then, in a typical state, around N vehicle moves will be considered helpful
by FF: at least 1 per city since local vehicles will be preferred by the relaxed plan. All
successor states will have identical h+ until a package can be loaded/unloaded. The typical
number of steps required to do so will grow with D. If, for example, the vehicle is in the
outskirts and the packages are in the city center, then around D/2 steps are required,
and finding an exit takes runtime around N D/2 . Then small values of N and D already
render search probing either devoid of information (if the runtime cut-off is too small), or
computationally infeasible (recall that the probing should be a quick pre-process to the
actual planning). By contrast, analysis (III) easily delivers the correct success rate 100%.
8.6 Predicting Planner Performance
As a direct measure of the predictive quality of success rates, we conducted preliminary
experiments examining the behavior of primitive classifiers, and of runtime distributions
for large vs. small success rates. We consider first the classifiers. They predict, given a
planning task, whether EHC/FF/LAMA will succeed in solving the task, within the given
17. Note that we cannot use valley rate instead, in a cheap domain analysis, since determining whether
or not s lies on a valley implies finding a plan for s and thus solving the task as a side effect.

185

fiHoffmann

time and memory limits. The classifiers answer yes iff the success rate is  a threshold
T in 0, 10, . . . , 100. Obviously, to do this, we need R > 1. We consider in what follows only
R = 10 and R = 100 because, as shown above, R = 1000 can be costly.
For EHC, both TorchLight analysis (III) and SP deliver fairly good-quality predictions,
considering that no actual machine learning is involved. The prediction quality of TorchLight is just as good as  sometimes slightly better than  that of search. Whether we use
R = 10 or R = 100 does not make a big difference. EHC solves 60.69% of the instances, so
that is the rate of correct predictions for a trivial baseline classifier always answering yes.
For R = 10, the best rate of correct predictions is 71.90% for TorchLight (with T = 80)
and 70.17% for SP (with T = 90). For R = 100, these numbers are 71.76% (T = 60) and
71.16% (T = 100). Dead-end rate is a very bad predictor. Its best prediction is for the
baseline classifier T = 0, and the second best classifier (T = 100) is only 36.79% correct.
Interestingly, there are major differences between the different sets of domains. On the
domains previously analyzed by hand (Hoffmann, 2005; as in Figure 1 but without Elevators
and Transport), the best prediction is 75.75% correct for TorchLight with T = 70, and
74.07% correct for SP with T = 100, vs. a baseline of 63.81%. On the IPC 2006 domains,
these numbers are 57.98% and 61.34% vs. baseline 55.46%, and T = 10 in both cases, i.e.,
the best classifier is very close to the baseline. IPC 2008, on the other hand, appears to be
exceptionally good-natured, the numbers being 79.52% (T = 60) and 82.38% (T = 80) vs.
baseline 51.90%. It is not clear to us what causes these phenomena.18
In summary, the quality of prediction is always clearly above the baseline, around 10%
when looking at all domains, and even up to 30% when looking at the IPC 2008 domains
only. For comparison, using state-of-the-art classification techniques but only simple features, Roberts and Howe (2009) get 69.47% correctness vs. baseline 74% (for saying no),
on unseen testing domains for FF. Having said that, if setting T in the above is considered
to be the learning, then the above does not actually distinguish between learning data
and testing data. Roberts and Howes unseen testing domains are those of IPC 2006 (in
a different setting than ours including also all ADL test suites). If we set T on only the
domains from before 2006 (Figure 1 without Elevators and Transport), then we get the
best prediction at T = 70 for TorchLight and T = 100 for SP. With this setting of T , the
prediction correctness on our IPC 2006 suite is 29.41% respectively 51.26% only, vs. the
baseline 55.46%. On the other hand, this seems to pertain only to IPC 2006 specifically.
For IPC 2008, T = 70 respectively T = 100 are good settings, giving 76.67% respectively
76.19% correctness vs. the baseline 51.90%.
Importantly, Roberts and Howe are not predicting the performance of EHC but that of
FF, which is a more complex algorithm. For FF and LAMA, the prediction quality of both
TorchLight and SP is rather bleak, using the described primitive classifiers. In all cases,
the best prediction correctness is obtained when always answering yes. The best that
can be said is that success rate still predicts much better than dead-end rate. To give some
example data, with R = 10 across all domains for FF, the baseline is 85.09% correct. With
T = 10, this goes down to 77.50% for TorchLight, 79.31% for SP, and 34.57% for dead-end
rate. For LAMA, the baseline is 90.26% correct, and with T = 10 this goes down to 81.81%
18. The bad prediction quality in IPC 2006 domains might be related to the fact that these are fully grounded,
potentially impeding the ability of Fast Downwards translator to find useful finite-domain variables.

186

fiAnalyzing Search Topology Without Running Any Search

for TorchLight, 83.97% for SP, and 29.91% for dead-end rate. For both FF and LAMA,
with growing T the prediction quality decreases monotonically in all cases.
Why is prediction quality so much worse for FF than for EHC, which after all is the
main building block of FF? Whereas EHC typically fails on tasks whose h+ topology is
not favorable, FFs and LAMAs complete search algorithms are able to solve many of
these cases, too. For example, with TorchLight success rates and R = 10, EHC solves only
34.07% of the tasks with success rate 0, and solves less than 50% up to success rate 70%.
By contrast, FF and LAMA solve 74.18% respectively 76.92% of the tasks with success rate
0, and solve at least 70% for all success rates.
Despite this, success rates are far from devoid of information for FF and LAMA. Setting
the threshold T in 10, . . . , 100, we look at the distribution of planner runtime in the instance
subset (A) where success rate is < T , vs. instance subset (B) where success rate is  T .
Taking the null hypothesis to be that the means of the two runtime distributions are the
same, we run the Students T-test for unequal sample sizes to determine the confidence with
which the null hypothesis can be rejected. That is, we determine the confidence with which
distribution (B) has a lower mean than distribution (A). Using TorchLights success rate on
FF runtimes, with both R = 10 and R = 100, and in all 10 settings of T , we get a confidence
of at least 99.9%. The difference between the means in our data, i.e., the mean runtime of
(A) minus the mean runtime of (B), tends to grow over T . It peaks at 336 respectively 361
seconds for R = 10 respectively R = 100; the average difference over all values of T is 239
respectively 240. Likewise, for LAMA runtimes all settings of T and R yield a confidence of
99.9%, with average differences 242 respectively 235. The results for SP are comparable for
LAMA. They are slightly worse for FF, though. With R = 10 the confidence is 99.9% only
for T = 10, 20; the confidence is 95% for all other values of T . The difference peaks at 241
seconds (vs. 336 for TorchLight), with an average of 150 seconds (vs. 239). With R = 100,
thresholds T = 30, 40, 50, 100 yield 99.9% confidence, the average difference being 160.
Again perhaps a little surprisingly, for the simpler planner EHC the runtime distributions behave very differently. For TorchLight success rates, we do get several cases with
confidence < 95%, and average differences of around 80 seconds. For SP, in most cases we
get a 99.9% confidence that the mean of (B) is larger than that of (A). Again, the reason
is simple. On many tasks with unfavorable h+ topology, enforced hill-climbing quickly exhausts the space of states reachable by FFs helpful actions. EHC then gives up on solving
the task, although it has consumed only little runtime  a peculiar behavior that one would
certainly not expect from a planner trying to be competitive.
Summing up, success rates as a planning task feature provide a very good coverage
predictor for EHC even without any significant learning. For FF and LAMA, things are
not that easy, however the consideration of runtime distributions clearly shows that the
feature is highly informative. Exploiting this informativeness for predicting planner performance presumably requires combination with other features, and actual machine learning
techniques, along the lines of Roberts and Howe (2009). This is a topic for future research.
8.7 Diagnosis
Let us finally consider TorchLights diagnosis facility. The idea behind this facility is to
summarize the reasons for analysis failure. Testing sufficient criteria for the absence of local
187

fiHoffmann

minima, such diagnosis is not guaranteed to identify domain features causing their presence.
Still, at least for analysis using Theorem 2, the diagnosis can be quite accurate.
The current diagnosis facility is merely a first-shot implementation based on reporting
all pairs (operator o0 , variable x) that caused an oDG+ for o0 to not be successful. That is,
we report the pair (o0 , x) if o0 has an effect on x, and a context fact (x, c) of the transition
t0 taken by o0 is contained in R1+  C0  F0 , and is not recoverable by a sub-sequence
+
of P>0
(s). In brief, we record (o0 , x) if o0 has a harmful effect on x. We perform a test
whether the main effect of o0 , i.e., that on x0 , is invertible; in this case we do not record
x0 since the problem appear to be the side effects. To avoid redundancies in the reporting,
we record not the grounded operator o0 but only the name of the action schema (load
instead of load(package1 truck7)). Similarly, as an option we record not x but the name
of the predicate underlying the fact (x, c). In that configuration, the diagnosis comes in the
form of action-name, predicate-name, which has a direct match with the high-level PDDL
input files. To have some measure of which parts of the diagnosis are more important,
we associate each pair with a count of occurrences, and weigh the pairs by frequency.
In Zenotravel, the diagnosis output always has the form fly, fuel-level and zoom,
fuel-level, indicating correctly that its the fuel consumption which is causing the local
minima. In Mprime and Mystery, the cause of local minima is the same, however the
diagnosis is not as reliable because of the specific structure of the domain, associating fuel
with locations instead of vehicles. This sometimes causes the diagnosis to conclude that it
is the effect changing locations which is causing the trouble. Concretely, with R = 1000
in Mystery, fuel consumption is the top-weighted diagnosis in 17 out of the 28 tasks; in
Mprime, this happens in 30 out of the 35 tasks. In Satellite and Rovers, the diagnosis
always takes the form switch-on, calibrated respectively take-image, calibrated, thus
reporting the problem to be that switching on an instrument, respectively taking an image,
deletes calibration. This is precisely the only reason why local minima exist here.19 In
Tyreworld, most often the diagnosis reports the problem to be that jacking up a hub results
in no longer having the jack (which is needed elsewhere, too). While this does not actually
cause local minima (there are none), it indeed appears to be a crucial aspect of the domain.
Similarly, in Grid the most frequent diagnosis is that picking up a key results in the arm
no longer being empty  again, not actually a cause of local minima, but a critical resource
in the domain. In Blocksworld-Arm, the dominant diagnoses are that a block is no longer
clear if we stack something on top of it, and that the hand is no longer empty when picking
up a block. Similarly, in Freecell, the dominant diagnoses are send-to-free, cellspace and
send-to-new-col, colspace.
One could make the above list much longer, however it seems clear already that this
diagnosis facility, although as yet primitive, has the potential to identify interesting aspects
of the domain. Note that we are making use of only one of the information sources in
TorchLight. There are many other things to be recorded, pertaining to other reasons for
analysis failure, like support graph cycles etc, and also to reasons for analysis success, like
successful gDGs and x0 , o0 pairs yielding successful oDG+ s. It appears promising to try
to improve diagnosis by combining some of these information sources. A combination with
19. Since analysis failure is rare in these two domains, often diagnosis does not give any output at all. With
R = 1000, the output is non-empty in 10 instances of Satellite and in 8 instances of Rovers. For R = 100
this reduces to 4 instances in Satellite, and not a single one in Rovers.

188

fiAnalyzing Search Topology Without Running Any Search

other domain analysis techniques, like landmarks or invariants extraction, could also be
useful. This is a direction for future work.20

9. Related Work
There is no prior work  other than the aforementioned one of the author (Hoffmann, 2005)
 trying to automatically infer topological properties of a heuristic function. Thus our work
does not relate strongly to other domain analysis techniques. The closest relation is to other
techniques relying on causal graphs. In what follows we discuss this in some detail, along
with some other connections arising in this context.
If local analysis succeeds, then we can construct a path to the exit identified. In this,
our work relates to work on macro-actions (e.g., Botea, Muller, & Schaeffer, 2004; Vidal,
2004). Its distinguishing feature is that this macro-action is (would be) constructed in a
very targeted and analytical way, even giving a guarantee, in the conservative case, to make
progress towards the goal. The machinery behind the analysis is based on causal graphs, and
shares some similarities with known causal-graph based execution path generation methods
(e.g., Jonsson & Backstrom, 1995; Williams & Nayak, 1997; Brafman & Domshlak, 2003).
The distinguishing feature here is that we focus on h+ and individual states rather than
the whole task. This allows us to consider small fragments of otherwise arbitrarily complex
planning tasks  we look at oDG+ instead of SG. Note that this ability is quite powerful
as far as applicability goes. As we have seen in Section 8, the success rate of (local)
approximate analysis  and therewith the fraction of states for which we would be able to
generate a macro-action  is non-zero in almost all benchmark domains. Of course, this
broad applicability comes with a prize. While traditional causal graph methods guarantee to
reach the goal, in the worst case the macro-actions may only lead into h+ local minima. Still,
it may be interesting to look into whether other, traditional, causal-graph based methods
can be localized in this (or a similar) manner as well.
Global analysis, where we focus on the whole planning task and thus the whole causal
graph, is even more closely related to research on causal graphs based tractability analysis.
The major difference between tractability analysis and h+ topology analysis, in principle,
is that tractability and absence of local minima are orthogonal properties  in general,
neither one implies the other. Now, as we pointed out at the end of Section 6, our global
analysis does imply tractability (of plan existence). Vice versa, do the restrictions made in
known tractable classes imply the absence of local minima? In many cases, we can answer
this question with a definite no; some interesting questions are open; in a single case 
corresponding to our basic result  the answer is yes.
Example 3 in Appendix A.4 shows that one can construct a local minimum with just 2
variables of domain size 3, 1-arc SG, unary operators, and strongly connected DTGs with a
single non-invertible transition. This example (and various scaling extensions not breaking
the respective conditions) falls into a variety of known tractable classes. The example is in
20. In particular, Fast Downwards translator is not always perfect in detecting the finite-domain variables
underlying benchmarks. For example, in Satellite it often does not detect that electricity is available
in exactly one of the instruments mounted on a satellite. This can lead to pointless diagnosis output,
which for now is handled using a simple notion of predicates exchanged by every operator. For doing
things like this in a more principled manner, further invariants analysis would be useful.

189

fiHoffmann

the tractable class F
n identified by Domshlak and Dinitz (2001), because every transition of
the dependent variable depends on the other variable. The example is in Helmerts (2004,
2006) SAS+ -1 class with strongly connected DTGs. The example is solved, i.e., reduced
to the empty task, by Haslums (2007) simplification techniques (also, these techniques
solve tasks from the Satellite domain, which do contain local minima). The example has
a fork and inverted fork causal graph, with bounded domain size and 1-dependent actions
only (actions with at most 1 prevail condition), thus it qualifies for the tractable classes
identified by Katz and Domshlak (2008b). The examples causal graph is a chain, and
thus in particular a polytree with bounded indegree, corresponding to the tractable class
identified by Brafman and Domshlak (2003) except that, there, variables are restricted to
be binary (domain size 2). It is an open question whether plan existence with chain causal
graphs and domain size 3 is tractable; the strongest known result is that it is NP-hard for
domain size 5 (Gimenez & Jonsson, 2009b).21 Similarly, the example fits the prerequisites
stated by Katz and Domshlak (2008a) except that these are for binary variables only; it
is an open question whether local minima exist in the tractable classes identified there.
Finally, the example, and a suitable scaling extension, obviously qualifies for two theorems
stated by Chen and Gimenez (2010). Their Theorem 3.1 (more precisely, the first part of
that theorem) requires only a constant bound on the size of the connected components in
the undirected graph induced by the causal graph. The first part of their Theorem 4.1
requires a constant bound on the size of the strongly connected components in the causal
graph, and pertains to a notion of reversible tasks requiring that we can always go back
to the initial state.
Next, consider the line of works restricting not the causal graph but the DTGs of the
task (Backstrom & Klein, 1991; Backstrom & Nebel, 1995; Jonsson & Backstrom, 1998).
The simplest class identified here, contained in all other classes, is SAS+ -PUBS where each
fact is achieved by at most one operator (post-unique, P), all operators are unary
(U), all variables are binary (B), and all variables have at most one value required in
the condition of a transition on any other variable (single-valued, S). Now, Example 2
in Appendix A.4 shows a local minimum in an example that has the U and S properties.
The example has two variables, x and y, and the local minimum arises because a cyclic
dependency prevents y from attaining its goal value dn via the shortest path as taken by
an optimal relaxed plan. If we remove all but two values from the domain of y, and remove
the alternative way of reaching dn ,22 then the example still contains a local minimum and
also has the P and B properties. We remark that the modified example is unsolvable. It
remains an open question whether solvable SAS+ -PUBS tasks with local minima exist; more
generally, this question is open even for the larger SAS+ -PUS class, and for the (yet larger)
SAS+ -IAO class identified by Jonsson and Backstrom (1998).
Another open question is whether the 3S class of Jonsson and Backstrom (1995)
contains local minima. The class works on binary variables only; it requires unary operators
and acyclic causal graphs, however it allows facts to be splitting instead of reversible. If
p is splitting then, intuitively, the task can be decomposed into three independent subtasks with respect to p; it is an open question whether local minima can be constructed
21. Although, of course, it is clear that, if the DTGs are strongly connected as in our case, then deciding
plan existence is tractable no matter what the domain size is.
22. This modification is given in detail below the example in Appendix A.4.

190

fiAnalyzing Search Topology Without Running Any Search

while satisfying this property. Disallowing the splitting option in 3S, we obtain the single
positive case, where a known tractable class does not contain any local minima. This
class corresponds to our basic result  acyclic causal graphs and invertible transitions 
except that the variables are restricted to be binary. Williams and Nayak (1997) mention
restrictions (but do not make formal claims regarding tractability) corresponding exactly
to our basic result except that they allow irreversible repair actions. The latter actions
are defined relative to a specialized formal framework for control systems, but in spirit they
are similar to what we term transitions with self-irrelevant deletes herein.
Finally, it is easy to see that, of Bylanders (1994) three tractability criteria, those two
allowing several effects do not imply the absence of local minima. For his third criterion,
restricting action effects to a single literal and preconditions to positive literals (but allowing
negative goals), we leave it as an open question whether or not local minima exist. We
remark that this criterion does not apply in any benchmark we are aware of.
To close this section, while we certainly do not wish to claim the identification of
tractable classes to be a contribution of our work, we note that the scope of Theorem 4 
which is a tractable class, cf. the above  is not covered by the known tractable classes.23
The tractable cases identified by Bylander (1994) obviously do not cover any of Logistics,
Miconic-STRIPS, Movie, and Simple-TSP. Many causal graph based tractability results
require unary operators (Jonsson & Backstrom, 1995; Domshlak & Dinitz, 2001; Brafman & Domshlak, 2003; Helmert, 2004, 2006; Katz & Domshlak, 2008a, 2008b; Jonsson,
2009; Gimenez & Jonsson, 2008, 2009a), which does not cover Miconic-STRIPS, Movie,
and Simple-TSP. In the work of Chen and Gimenez (2010), Theorem 4.1 requires reversibility which is not given in either of Movie, Miconic-STRIPS, or Simple-TSP, and
Theorem 3.1 requires a constant bound on the size of the connected components in the
undirected graph induced by the causal graph, which is given in none of Logistics, MiconicSTRIPS, and Simple-TSP. Other known tractability results make very different restrictions
on the DTGs (Backstrom & Klein, 1991; Backstrom & Nebel, 1995; Jonsson & Backstrom,
1998). Even the most general tractable class identified there, SAS+ -IAO, covers none of
Miconic-STRIPS, Logistics, and Simple-TSP (because vehicle variables are not acyclic
with respect to requestable values), and neither does it cover Movie (because rewinding a
movie is neither unary nor irreplaceable: it has a side effect un-setting the counter, while
not breaking the DTG of the counter into two disjoint components).
As far as coverage of the benchmarks goes, the strongest competitor of Theorem 4 are
Haslums (2007) simplification techniques. These iteratively remove variables where all
paths relevant for attaining required conditions are free, i.e., can be traversed using transitions that have neither conditions nor side effects. Haslums Theorem 1 states that such
removal can be done without jeopardizing solution existence, i.e., a plan for the original
task can be reconstructed easily from a plan for the simplified task. In particular, if the
task is solved  simplified completely, to the empty task  then a plan can be constructed
in polynomial time. Haslum combines this basic technique with a number of domain reformulation techniques, e.g., replacing action sequences by macros under certain conditions.
The choice which combination of such techniques to apply is not fully automated, and parts
23. This is not true of our basic result, which as just explained is essentially covered by the works of Jonsson
and Backstrom (1995) and Williams and Nayak (1997). Formally, its prerequisites imply those of (the
first part of) Theorem 4.1 in the work of Chen and Gimenez (2010), namely, the postulated bound is 1.

191

fiHoffmann

of these techniques are not fully described, making a comparison to Theorem 4 difficult.
Haslum reports his techniques to solve tasks from Logistics, Miconic-STRIPS, and Movie,
plus Gripper and Satellite. Haslum does not experiment with Simple-TSP. His Theorem 1,
in its stated form, does not solve Simple-TSP, because there the transitions of the root
variable have side effects (with irrelevant deletes). Extending the theorem to cover such
irrelevant deletes should be straightforward. A more subtle weakness of Haslums Theorem 1 relative to our Theorem 4 pertains to reaching required values from externally caused
values. Haslum requires these moves to be free, whereas, in the definition of recoverable
side effect deletes, Theorem 4 allows the recovering operators to affect several variables and
to take their precondition from the prevails and effects of o0 .

10. Conclusion
We identified a connection between causal graphs and h+ , and devised a tool allowing to
analyze search space topology without actually running any search. The tool is not yet
an automatic Hoffmann, but its analysis quality is impressive even when compared to
unlimited search probing.
At a very generic level, a conclusion of this work is that, sometimes, it is possible to
automatically infer topological properties of a heuristic function. An interesting question
for future work is whether this can also be done for heuristics other than h+ (cf. also the
comments regarding causal graph research below). Methodologically, it is noteworthy that
the analysis is based on syntactic restrictions on the problem description, which has traditionally been used to identify tractable fragments (of planning and other computationally
hard problems). The present work showcases that very similar techniques can apply to the
analysis of the search spaces of general problem solvers.
A main open question is whether global analysis can more tightly approximate the scope
of Theorem 2. As indicated, a good starting point appears to be trying to include, in a gDG
for operator o0 , only variable dependencies induced by operators o that may actually precede
o0 in an optimal relaxed plan. An approach automatically recognizing such operators could
possibly be developed along the lines of Hoffmann and Nebel (2001b), or using a simplified
version of the aforementioned fact generation tree analysis technique (Hoffmann, 2005).
Additionally, it would be great to recognize situations in which harmful side effects of o0
 like making the hand non-empty if we pick up a ball in Gripper  will necessarily be
recovered inside the relaxed plan. Possibly, such analysis could be based on a variant of
action landmarks (Hoffmann, Porteous, & Sebastia, 2004; Karpas & Domshlak, 2009).
Another interesting line of research is to start from results given for individual states s
by local analysis, then extract the reasons for success on s, and generalize those reasons to
determine a generic property under which success is guaranteed. Taken to the extreme, it
might be possible to automatically identify domain sub-classes, i.e., particular combinations
of initial state and goal state, in which the absence of local minima is proved.
This work highlights two new aspects of causal graph research. First, it shows that, in
certain situations, one can localize the causal graph analysis, and consider only the causal
graph fragment relevant for solving a particular state. Second, one can use causal graphs
for constructing paths not to the global goal, but to a state where the value of a heuristic h
is decreased. The former enables the analysis to succeed in tasks whose causal graphs are
192

fiAnalyzing Search Topology Without Running Any Search

otherwise arbitrarily complex, and thus has the potential to greatly broaden the scope of
applicability. The latter is not necessarily limited to only h+  as a simple example, it is
obvious that similar constructions can be made for the trivial heuristic counting the number
of unsatisfied goals  and thus opens up a completely new avenue of causal graph research.
Another possibility is planner performance prediction, along the lines of Roberts and
Howe (2009). Our experimental results indicate that TorchLights problem features, and
also those of search probing, are highly informative. This has the potential to significantly
improve the results of Roberts and Howe for unseen domains  they currently use only very
simple features, like counts of predicates and action schemes, that hardly capture a domainindependent structure relevant to planner performance. Like limited search probing (SP1s ),
TorchLight generates its features without jeopardizing runtime, thus enabling automatic
planner configuration. Unlike for search probing, this may even work on-line during search:
a single relaxed plan can already deliver interesting information. For example, one might
make the search more or less greedy  choosing a different search strategy, switching helpful
actions on or off, etc.  depending on the outcome of checking Theorem 2.
As mentioned in Section 9, a direction worth trying is to use local analysis for generating
macro-actions. In domains with high success rate, it seems likely that the macro-actions
would lead to the goal with no search at all. It is a priori not clear, though, whether such
an approach would significantly strengthen, at least in the present benchmarks, existing
techniques for executing (parts of) a relaxed plan (e.g., Vidal, 2004).
One could use TorchLights diagnosis facility as the basis of an abstraction technique
for deriving search guidance, much as is currently done with other relaxation/abstraction
techniques. The diagnosis can pin-point which operator effects are causing problems for
search. If we remove enough harmful effects to end up with a task to which Theorem 4
applies, then the abstracted problem is tractable. For example, in transportation domains,
this process could abstract away the fuel consumption. If we do not abstract that much,
then the information provided may still outweigh the effort for abstract planning, i.e., for
using an actual planner inside the heuristic function. For example, in Grid the abstract task
could be a problem variant allowing to carry several keys at once. One could also focus the
construction of different heuristics  not based on ignoring deletes  on the harmful effects.
Finally, an interesting research line is domain reformulation. As is well known, the
domain formulation can make a huge difference for planner performance. However, it is
very difficult to choose a good formulation, for a given planner. This is a black art even if
the reformulation is done by the developer of the planner in question. The lack of guidance is
one of the main open problems identified by Haslum (2007) for his automatic reformulation
approach. The most frequent question the author has been asked by non-expert users is
how to model a domain so that FF can handle it more easily.
TorchLights diagnosis facility, pin-pointing problematic effects, might be instrumental
for addressing these difficulties. For the case where the reformulation is done by a computer,
one possibility to use the analysis outcome could be to produce macro-actions hiding
within them the operators having harmful effects. Another possibility could be to precompose variable subsets touched by the harmful effects.
For the case where the reformulation is done by a human user, the sky is the limit.
To name just one example, the local minima in Satellite could be removed by allowing
to switch on an instrument only when pointing in a direction where that instrument can
193

fiHoffmann

be calibrated. More generally, note that end-user PDDL modeling  writing of PDDL
by a non-expert user wanting to solve her problem using off-the-shelf planners  is quite
different from the PDDL modeling that planning experts do when developing benchmarks.
For example, if an expert models a transportation benchmark with fuel consumption, then
it may seem quite pointless for TorchLight to determine that fuel consumption will hurt
planner performance. Indeed this may be the reason why the fuel consumption was included
in the first place. By contrast, for an end-user (a) this information may come as a surprise,
and (b) the user may actually choose to omit fuel consumption because this may yield a
better point in the trade-off between planner performance and plan usability. Generally
speaking, such an approach could give the user guidance in designing a natural hierarchy
of increasingly detailed  and increasingly problematic  domain formulations. This could
help making planning technology more accessible, and thus contribute to a challenge that
should be taken much more seriously by the planning community.

Acknowledgments
I would like to thank the anonymous reviewers of both, the article at hand and the ICAPS
2011 short version, for their constructive comments. In particular, one of the reviewers
proved the completeness results in Theorem 1, and another reviewer suggested the future
research line trying to generalize the reasons for success in local analysis.
I thank Carmel Domshlak for discussions, feedback on early stages of this work  contributing in particular the d-abstracted task construction in the proof of Lemma 3  and
an executive summary of the status quo of causal graph research.
A very special thanks goes to Carlos Areces and Luciana Benotti, for inspiring this
work in the first place. I had long ago given up on this problem. It was Carlos and
Lucianas insistence that finally made me see the connection to causal graphs  while trying
to convince them that an analysis like this is impossible.

Appendix A. Technical Details and Proofs
We give the full proofs and, where needed, fill in some technical definitions. We first
prove our complexity result (Appendix A.1, Theorem 1), then the result pertaining to the
analysis of optimal relaxed plans (Appendix A.2, Theorem 2), then the result pertaining to
conservative approximations (Appendix A.3, Theorems 3 and 4). We construct a number
of examples relevant to both kinds of analysis (Appendix A.4), before giving the proofs of
domain-specific performance guarantees (Appendix A.5, Propositions 1 and 2).
A.1 Computational Complexity
Theorem 1. It is PSPACE-complete to decide whether or not the state space of a given
planning task contains a local minimum, and given an integer K it is PSPACE-complete to
decide whether or not for all states s we have ed(s)  K. Further, it is PSPACE-complete
to decide whether or not a given state s is a local minimum, and given an integer K it is
PSPACE-complete to decide whether or not ed(s)  K.
194

fiAnalyzing Search Topology Without Running Any Search

Proof. Throughout the proof, since PSPACE is closed under complementation, we do not
distinguish the mentioned PSPACE-complete decision problems from their complements.
The membership results are all easy to prove. Note first that, given a state s, we can
compute h+ (s) within polynomial space: generate a potentially non-optimal relaxed plan,
of length n, with the known methods; then iteratively decrement n and test for each value
whether a relaxed plan of that length still exists; stop when that test answers no. The
test for bounded relaxed plan existence is in NP and thus in PSPACE. From here, we
can prove the membership results by simple modifications of the guess-and-check argument
showing that PLANSAT, the problem of deciding whether a given planning task is solvable,
is in NPSPACE and hence in PSPACE (Bylander, 1994). That argument works by
starting in the initial state, guessing actions, and terminating successfully if a goal state
is reached. Unsuccessful termination occurs if the guessed path is longer than the trivial
upper bound B := xX |Dx | on the number of different states. To be able to check this
condition in polynomial space, the path length is maintained in a binary counter.
To decide whether a given state s is (not) a local minimum, we run this guess-and-check
algorithm from s, modified to: compute h+ for each encountered state; to terminate unsuccessfully if the bound B is exceeded or if h+ increases after an operator application; and
to terminate successfully if h+ decreases after an operator application. To decide whether
ed(s)  K, we use the same algorithm except that the bound B is replaced by the bound K,
increases of h+ are permitted, and success occurs if h+ decreases from h+ (s) to h+ (s)1. To
decide whether the state space of an entire planning task contains local minima, or whether
all states s in the state space have ed(s)  K, we simply run Bylanders guess-and-check
algorithm as a way of enumerating all reachable states, then for each individual state s we
run the modified guess-and-check algorithms just described. Clearly, all these algorithms
run in non-deterministic polynomial space, which shows this part of the claim.
We now show the PSPACE-hardness results. We first consider the problem of deciding whether or not a given state s is a local minimum. The proof works by reducing
PLANSAT, which is known to be PSPACE-hard for propositional STRIPS (Bylander,
1994), from which it trivially follows that PLANSAT is PSPACE-hard also for the finitedomain variable planning tasks we use herein.
Let (X, sI , sG , O) be the planning task whose solvability we wish to decide. We design
a modified task (X 0 , s0I , s0G , O0 ) by starting with (X, sI , sG , O) and making the following
modifications:
 Add a new variable ChooseT ask to X 0 , with
s0I (ChooseT ask) = nil, and s0G (ChooseT ask) undefined.

domain

{nil, org, alt},

The role of this variable will be to give the planner a choice whether to solve the
original task (X, sI , sG , O), or whether to solve an alternative task custom-designed
for this proof.
 Add a new variable DistAlt to X 0 , with domain {0, 1}, s0I (DistAlt) = 1, and
s0G (DistAlt) = 1.
This variable simply serves to control the length of the solution of the alternative task.
That solution length will be 1 plus the number of steps needed to bring DistAlt from
195

fiHoffmann

value 0 to its goal value. (Here, only 1 step will be needed for doing so; later on in
this proof, we will increase this distance.)
 Add two new operators oOrg = ({(ChooseT ask, nil)}, {(ChooseT ask, org)}) and
oAlt = ({(ChooseT ask, nil)}, {(ChooseT ask, alt), (DistAlt, 0)}).
This implements the choice of planning task. Note that, if we choose the alternative
task, then DistAlt is set to 0, thus forcing the solution to bridge this distance. By
contrast, for the original task, this variable keeps residing in its goal value as was
already assigned by s0I (DistAlt).
 Add a new operator oDistAlt = ({(ChooseT ask, alt), (DistAlt, 0)}, {(DistAlt, 1)}).
This allows to bridge the distance intended for the solution of the alternative task.
 Add a new operator osG Alt = ({(ChooseT ask, alt), (DistAlt, 1)}, sG ).
This allows us to accomplish the original goal, as the final step in solving the alternative task.
 Add (ChooseT ask, org) as a new precondition into all original operators, i.e., those
taken from O.
This forces the planner to choose the original task, for executing any of its operators.
 Add a new variable StillAlive to X, with domain {yes, no}, s0I (StillAlive) = yes,
and sG (StillAlive) = yes. Add a new operator osG Dead = (, sG  {(StillAlive, no)}).
The osG Dead operator allows us to accomplish the original goal in a single step, no
matter which task we have chosen to solve, and also in the new initial state s0I already.
However, the operator also sets the new variable StillAlive to value no, whereas the
goal value of that variable is yes. That value cannot be re-achieved, and thus the
operator leads into a dead-end. Its function in the proof is to flatten the value of
h+ in the original task, and in s0I , to be constantly 1 unless we are in a goal state.
This extreme flattening does not happen in the alternative task because, there, the
distance variable DistAlt also needs to be handled.
In summary, (X 0 , s0I , s0G , O0 ) is designed by setting:
 X 0 := X  {ChooseT ask, DistAlt, StillAlive}
 s0I := sI  {(ChooseT ask, nil), (DistAlt, 1), (StillAlive, yes)}
 s0G := sG  {(DistAlt, 1), (StillAlive, yes)}
 O0 := {(pre  {(ChooseT ask, org)}, eff) | (pre, eff)  O}  {oOrg , oAlt , oDistAlt , osG Alt ,
osG Dead }
Now consider the new initial state s0I . It has exactly three successor states: sDead produced by osG Dead , sOrg produced by oOrg , and sAlt produced by oAlt . We have h+ (sDead ) =
 because sDead (StillAlive) = no. We have h+ (s0I ) = h+ (sOrg ) = 1 due to the relaxed
196

fiAnalyzing Search Topology Without Running Any Search

plan hosG Dead i. Finally, we have h+ (sAlt ) = 2 because oAlt sets the DistAlt variable to 0
whereas its goal is 1. Thus a shortest relaxed plan for sAlt is hoDistAlt , osG Alt i.
From this, it clearly follows that s0I is not a local minimum iff sOrg has a monotone
path to a state s with h+ (s) < h+ (sOrg ). Since h+ (sOrg ) = 1, the latter is equivalent to
the existence of a monotone path from sOrg to a goal state, i.e., a path to a goal state on
which h+ is constantly 1. Since, for all states reachable from sOrg , the single-step sequence
hosG Dead i is a relaxed plan, this is equivalent to the existence of a path from sOrg to a goal
state. Clearly, the latter is equivalent to solvability of the original task (X, sI , sG , O). Thus
s0I is not a local minimum iff (X, sI , sG , O) is solvable, which shows this part of the claim.
We next prove PSPACE-hardness of deciding whether or not a given planning task
contains a local minimum. This follows easily from the above. Observe that the alternative
task does not contain any local minima. As described, we have h+ (sAlt ) = 2. If we apply
oDistAlt to sAlt , then we obtain a state sAltDist where h+ (sAltDist ) = 1 because of the relaxed
plan hosG Alt i. Applying osG Alt in sAltDist yields a goal state, and thus both sAlt and sAltDist
have better evaluated neighbors. Any other states descending from sAlt must be produced
by osG Dead and thus have h+ value . So, (X 0 , s0I , s0G , O0 ) contains a local minimum iff the
part of its state space descended from sOrg does. Since all those states have h+ value 1 unless
they are goal states, cf. the above, the latter is equivalent to unsolvability of (X, sI , sG , O)
which shows this part of the claim.
Assume now that we are given an integer K and need to decide for an individual state s
whether or not ed(s)  K. We reduce Bounded-PLANSAT, the problem of deciding whether
any given planning task is solvable within a given number of steps. Bounded-PLANSAT
is known to be PSPACE-complete if the bound is given in non-unary representation. We
modify the task (X 0 , s0I , s0G , O0 ) given above, in a way that increases the solution length of
the alternative task to be K. We introduce a binary counter using dlog2 (K 2)e new binary
variables Biti that are all at 0 in sI . We introduce an operator for each bit, allowing to set
the bit to 1 if all the lower bits are already 1, and in effect setting all these lower bits back
to O. Each such operator has the additional precondition (ChooseT ask, alt), but has no
effect other than modifying the bits. We then modify the operator oDistAlt by adding new
preconditions encoding counter position K 2. With this construction, clearly h+ (sAlt ) > 1,
and the distance to goal of sAlt is K: a plan is to count up to K  2, then apply oDistAlt ,
then apply osG Alt . Thus, the shortest exit path for sI via oAlt has length K + 1. But then,
with the above, ed(sI )  K iff (X, sI , sG , O) has a plan of length at most K  1, which
concludes this part of the claim.
Finally, say we need to decide whether or not, for all s  S, we have ed(s)  K. Note
first that sAlt and all its successors necessarily have exit distance at most K (the goal can be
reached in at most that many steps), and that the exit distance of sOrg and all its successors
is equal to the length of a shortest plan for the corresponding state in (X, sI , sG , O). The
latter length may, for some states in (X, sI , sG , O), be longer than K even if the shortest
plan for (X, sI , sG , O) (i.e., for the original initial state) has length K. We thus introduce
another binary counter, this time counting up to K  1, conditioned on (ChooseT ask, org),
and with a new operator whose precondition demands the new counter to be at K  1 and
that achieves all goals. Then, clearly, sOrg and all its descendants have exit distance at most
K. Thus the only state that may have exit distance greater than K is s0I  precisely, we
197

fiHoffmann

have ed(s0I ) = K + 1 iff the new counter is the shortest plan for sOrg , which obviously is the
case iff (X, sI , sG , O) has no plan of length at most K 1. This concludes the argument.
A.2 Analyzing Optimal Relaxed Plans
We need to fill in some notations. For the sake of self-containedness of this section, we first
re-state the definitions given in Section 5:
Definition 1. Let (X, sI , sG , O) be a planning task, let s  S with 0 < h+ (s) < , let
P + (s) be an optimal relaxed plan for s, let x0  X, and let o0  P + (s) be an operator taking
a relevant transition of the form t0 = (s(x0 ), c).
An optimal rplan dependency graph for P + (s), x0 and o0 , or optimal rplan dependency
graph for P + (s) in brief, is a graph oDG+ = (V, A) with unique leaf vertex x0 , and where
x  V and (x, x0 )  A if either: x0 = x0 , x  Xpreo , and preo0 (x) 6= s(x); or x 6= x0 
0
+
(s) taking a relevant transition on x0 so that x  Xpreo
V \ {x0 } and there exists o  P<0
and preo (x) 6= s(x).
For x  V \ {x0 }, by oDT G+
x we denote the sub-graph of DT Gx that includes only
+
(s, x), the relevant transitions t using an operator in
the values true at some point in P<0
+
P<0 (s, x), and at least one relevant inverse of such t where a relevant inverse exists. We
+
(s, x) transitions as original, and to the inverse transitions as induced.
refer to the P<0
Definition 2. Let (X, sI , sG , O), s, P + (s), x0 , t0 , and oDG+ = (V, A) be as in Definition 1.
We say that oDG+ is successful if all of the following holds:
(1) oDG+ is acyclic.
(2) We have that either:
+
(s)-recoverable; or
(a) the oDG+ -relevant deletes of t0 are P>0
+
(b) s(x0 ) is not oDG -relevant, and t0 has replaceable side effect deletes; or
(c) s(x0 ) is not oDG+ -relevant, and t0 has recoverable side effect deletes.

(3) For x  V \ {x0 }, all oDT G+
x transitions either have self-irrelevant deletes, or are
invertible/induced and have irrelevant side effect deletes and no side effects on V \{x0 }.
We next define two general notions that will be helpful to state our proofs.
 The prevail condition prevo of an operator o  O results from restricting preo to the
set of variables Xpreo \ Xeff o .
 Let x  X, let (c, c0 ) be a transition in DT Gx , and let (y, d)  seff(c, c0 ) be a side
effect of the transition. The context of (y, d) in (c, c0 ) is ctx(c, c0 , y, d) :=
(
(y, prerop(c,c0 ) (y))
y  Xprerop(c,c0 )
{(y, d0 ) | d0  Dy , d0 6= d} y 6 Xprerop(c,c0 )
The context of (c, c0 ) is the set ctx(c, c0 ) of all partial variable assignments  so that, for
every (y, d)  seff(c, c0 ), y  X and (y, (y))  ctx(c, c0 , y, d). We identify ctx(c, c0 )
with the set of all facts that occur in any of its assignments.
198

fiAnalyzing Search Topology Without Running Any Search

Note here that the definition of ctx(c, c0 ) over-writes our previous one from Section 5,
but only in the sense that we now also distinguish all possible tuples of context values,
rather than just collecting the overall set. We need the more fine-grained definition to
precisely formulate Definition 2 condition (2c), i.e., under which conditions a transition
has recoverable side effect deletes. Namely, Definition 2 conditions (2b) and (2c) are
formalized as follows:
 A transition (c, c0 ) has replaceable side effect deletes iff ctx(c, c0 )sG =  and, for every
rop(c, c0 ) 6= o  O where preo  ctx(c, c0 ) 6=  there exists o0  O so that eff o0 = eff o
and preo0  prevrop(c,c0 )  eff rop(c,c0 ) .
 A transition (c, c0 ) has recoverable side effect deletes iff the following two conditions
hold:
 Either (c, c0 ) has irrelevant side effect deletes or, for every   ctx(c, c0 ), there
exists a recovering
S operator o so that preo  prevrop(c,c0 )  eff rop(c,c0 ) and eff o  ,
eff o    (sG  rop(c,c0 )6=o0 O preo0 ).
 Every (y, d)  seff(c, c0 ) is not in the goal and appears in no operator precondition
other than possibly those of the recovering operators.
If t0 has replaceable side effect deletes, then upon its execution we can remove o0 from
the relaxed plan because any operator relying on deleted facts can be replaced. If t0 has
recoverable side effect deletes, then, due to the first clause of this definition, no matter what
the state s0 in which we apply t0 is  no matter which context  holds in s0  we have a
recovering operator o that is applicable after t0 and that re-achieves all relevant facts. Due
to the second clause, o will not delete any facts relevant elsewhere in the relaxed plan (note
here that anything deleted by o must have been a side effect of t0 ).
Finally, to formally define the notion used in Definition 2 condition (2a)  the oDG+ +
(s)-recoverable  we now assume the surroundings pertaining
relevant deletes of t0 are P>0
to Theorem 2, i.e., (X, sI , sG , O) is a planning task, s is a state, P + (s) is an optimal relaxed
plan for s, oDG+ = (V, A) is an optimal rplan dependency graph with leaf variable x0 and
transition t0 = (s(x0 ), c) with responsible operator o0 . We are considering a state s0 where
t0 can be executed, reaching a state s1 , and we are examining a relaxed plan P1+ for s1
+
constructed from P + (s) by removing o0 , and by replacing some operators of P<0
(s) with
+
operators responsible for induced oDT Gx transitions for x  V \ {x0 }.
 By C0 := {(x0 , s(x0 ))}  ctx(t0 ) we denote the values potentially deleted by t0 .
 By R1+ we denote the union of sG , the precondition of any P + (s) operator other than
o0 , and the precondition of any operator which is the responsible operator for an
induced transition in oDT G+
x , with x  V \ {x0 }. As discussed in Section 5, this is a
super-set of the facts possibly needed in P1+ .
S
 By F0 := s  oP + (s) eff o we denote the set of facts true after the relaxed execution
<0

+
of P<0
(s) in s. As discussed in Section 5, if p 6 F0 then p is not needed in s1 for P1+
to be a relaxed plan.

199

fiHoffmann

 By S1 we denote the union of: (1) prevo0  eff o0 ; (2) the set of facts (x, c)  s where
+
there exists no o such that x  Xeff o and o is either o0 or in P<0
(s) or is the responsible
+
operator for an induced transition in oDT Gx , with x  V \ {x0 }; (3) the set F defined
as F := {(x, c) | (x, c)  F0 , x  V \ {x0 }} if Xeff o0  (V \ {x0 }) = , else F := . Here,
(1) and (2) are facts of which we are certain that they will be true in s1 ; (3) is a set of
facts that we will be able to achieve at the start of P1+ , by appropriately re-ordering
the operators.

+ (s), then the relaxed-plan macro-precondition
 If 
o = ho1 , . . . , on i is a sub-sequence
of P S
Sn
i1


+
of o is defined as pre
:= i=1 (preoi \ j=1
eff oj ). The relaxed-plan macro-effect of

o S
n




+
o is defined as eff 
:= i=1 eff oi . If o is empty then both sets default to the empty

o
set. These notions simply capture the outside needs and effects of a relaxed plan
sub-sequence.
+
+
(s)-recoverable iff P>0
(s) contains a sub The oDG+ -relevant deletes of t0 are P>0


+
+
+
sequence o0 so that pre

S
and
eff

C

F
.
The
first condition here

R



1
0
0
1
o0
o0


ensures that o0 will be applicable at the appropriate point within P1+ . The second

o0 .
clause ensures that all facts relevant for P1+ will be re-achieved by 

We now proceed with our exit path construction. In what follows, we first consider the
part of the path leading up to s0 , i.e., where we move only the non-leaf variables x  V \{x0 }.
We show how to construct the relaxed plans P + (s0 ) for the states s0 visited on this path.
First, note that we can assume P + (s) to be sorted according to the optimal rplan
dependency graph oDG+ = (V, A). Precisely, let xk , . . . , x1 be a topological ordering of
V \ {x0 } according to the arcs A. Due to the construction of (V, A) as per Definition 1,
and because previous values are never removed in the relaxed state space, we can re-order
+
+
(s, x1 )  P . That is, we can perform all moves
(s, xk )      P<0
P + (s) to take the form P<0
+
within each oDT Gx up front, in an order conforming with A. We will henceforth assume,
wlog, that P + (s) has this form.
+
Recall in what follows that original oDT G+
x transitions are those taken by P<0 (s),
whereas induced oDT G+
x transitions are those included as the inverse of an original tran

sition. For a path 
p of invertible transitions traversing hc0 , . . . , cn i, the inverse path 
p


traverses hcn , . . . , c0 i by replacing each transition with its inverse. By rop( p ) we denote
the operator sequence responsible for the path.
We say that a state s0  S is in the invertible surroundings of s according to oDG+ if s0 is

reachable from s by executing a sequence 
o of responsible operators of invertible/induced
+
transitions in oDT Gx for x  V \ {x0 }. The adapted relaxed plan for such s0 , denoted
P + (ss0 ), is constructed as follows. Let xk , . . . , x1 be a topological ordering of V \ {x0 }
according to A, and denote P + (s) = P + (s, xk )      P + (s, x1 )  P . Initialize P + (ss0 ) :=

P + (s). Then, for each xi  V \ {x0 }, let 
p be a path of original invertible transitions in

+
0
oDT Gxi leading from s(xi ) to s (xi )  clearly, such a path must exist. Remove rop(
p ) from


+
0
+
0
P (ss ), and insert rop( p ) at the start of P (ss , xi ).
We next show that adapted relaxed plans indeed are relaxed plans, under restricting
conditions that are in correspondence with Definition 2 condition (3):
Lemma 1. Let (X, sI , sG , O) be a planning task, let s  S be a state with 0 < h+ (s) < ,
and let P + (s) be an optimal relaxed plan for s. Say that oDG+ = (V, A) is an optimal rplan
200

fiAnalyzing Search Topology Without Running Any Search

dependency graph for P + (s) where, for every x  V \ {x0 }, the invertible/induced oDT G+
x
transitions have irrelevant side effect deletes and no side effects on V \ {x0 }. Let s0  S be
a state in the invertible surroundings of s according to oDG+ . Then P + (ss0 ) is a relaxed
plan for s0 , and |P + (ss0 )|  |P + (s)|.
+
+
Proof. By definition, we know that P + (s) takes the form P<0
(s, xk )      P<0
(s, x1 )  P ,
+
+
+
0
0
0
and that P (ss ) takes the form P<0 (s , xk )      P<0 (s , x1 )  P , where xk , . . . , x0 is a
topological ordering of V , and P is some operator sequence that is common to both, but
whose content will not be important for this proof. For simplicity, we denote in the rest of
the proof P + (ss0 ) as P + (s0 ), and we leave away the < 0 subscripts.

Consider first the (relaxed) execution of P + (s, xk ) and P + (s0 , xk ). Say that 
p is the
+ (s0 ), i.e., a path of original invertible
path in oDT G+
considered
in
the
definition
of
P
xk


0
transitions in oDT G+
xi leading from s(xk ) to s (xk ). Clearly, ho1 , . . . , on i := rop( p ) is a

sub-sequence of P + (s, xk ). Say that 
p visits the vertices s(xk ) = c0 , . . . , cn = s0 (xk ); denote
C := {c0 , . . . , cn }. Assume wlog that P + (s, xk ) starts with ho1 , . . . , on i  note here that we
can re-order P + (s, xk ) (and relaxed plans in general) in any way we want as long as we
do not violate operator preconditions. The latter is not the case here because: ho1 , . . . , on i
constitutes a path in oDT G+
xk ; because all other operators depending on a value in C are

ordered to occur later on in P + (s, xk ); and because, since all transitions in 
p have no side
effects on V \{x0 }, by construction of (V, A) as per Definition 1 the operators in ho1 , . . . , on i
do not support each other in any way, in P + (s), other than by affecting the variable xk .
Given the above, wlog P + (s, xk ) has the form ho1 , . . . , on i  P1 . By construction,



+
P (s0 , xk ) has the form rop(
pS
)  P1 =: h
o
n , . . . , o1 i  PS
1 . Consider now the endpoints
n
+
+
0
of the prefixes, i.e., s1 := s  i=1 eff oi and s2 := s  1i=n eff 
oi . Clearly, since all the

transitions on 
p have irrelevant side effect deletes, we have that the relevant part of s is
contained in s0 . But then, as far as the variables outside V \ {x0 , xk } are concerned, the
+
relevant part of s+
1 is contained in s2 : any relevant side effects of ho1 , . . . , on i are already
0
contained in s ; the values C are obviously true in s+
2 ; if the induced transitions have side


+
o
effects, then these can only increase the fact set s2 . Further, the sequence h
n , . . . , o1 i is
applicable in the relaxation. To see this, note first that the preconditions on xk itself are


satisfied by definition, because h
o
n , . . . , o1 i constitutes a path in DT Gxk . Any side effects, if
they occur, are not harmful because old values are not over-written in the relaxation. As for
preconditions on other variables, due to invertibility  the outside conditions of 
oi are contained in those of oi  those are a subset of those for ho1 , . . . , on i. Hence, with Definition 1
and since xk has no incoming edges in oDG+ , all these preconditions are satisfied in s. They
are then also satisfied in s0 because (vk being a root of oDG+ ) these variables x are not
contained in V and hence s0 (x) = s(x) by prerequisite  note here that precondition facts
cannot have been deleted by the side effects whose deletes are irrelevant by prerequisite.
The above has shown that the relevant part of the outcome of relaxed execution of
P + (s, xk ) in s is contained in the outcome of relaxed execution of P + (s0 , xk ) in s0 , on all
variables outside V \ {x0 , xk }. We can now iterate this argument. Assume as induction
hypothesis that we have already shown that the relevant part of the outcome of relaxed
execution of P + (s, xk )  . . . P + (s, xi+1 ) in s is contained in the outcome of relaxed execution
of P + (s0 , xk )      P + (s0 , xi+1 ) in s0 , on all variables outside V \ {x0 , xk , . . . , xi+1 }. Now
consider P + (s, xi ) and P + (s0 , xi ). The only thing that changes with respect to xk above
is that there may be preconditions on variables xj that are not true in s; we have j > i

201

fiHoffmann

because such preconditions must belong to predecessors of xi in oDG+ by Definition 1.
Since P + (s) = P + (s, xk )      P + (s, x1 )  P is a relaxed plan for s, those conditions are
established after relaxed execution of P + (s, xk )      P + (s, xi+1 ) in s. Given this, by
induction hypothesis the conditions  which are clearly not irrelevant  are established also
after relaxed execution of P + (s0 , xk )      P + (s0 , xi+1 ) in s0 , which concludes the argument
for the inductive case. With i = 1, it follows that the relevant part of the outcome of relaxed
execution of P + (s, xk )      P + (s, x1 ) in s is contained (on all variables) in the outcome of
relaxed execution of P + (s0 , xk )      P + (s0 , x1 ) in s0 . From this, the claim follows trivially
because P + (s) is a relaxed plan for s, and the remainder P of both operator sequences is
identical.
The second part of the claim follows because, for any i 6= j, we have that the original
transitions we use for xi respectively xj have no operators in common. This is because, as
argued above, all the relevant operators have no side effects on V \ {x0 }. Since each of these
operators affects the variable xi , it cannot affect any other variable in V \ {x0 }. Thus, for
each inverse transition that we introduce via an inverse operator, P + (s) contains a separate
operator. From this, obviously we get that |P + (ss0 )|  |P + (s)|.
Lemma 1 captures the second case of Definition 2 condition (3), transitions that are
invertible/induced and have irrelevant side effect deletes and no side effects on V \ {x0 }.
The next lemma captures the first case of Definition 2 condition (3):
Lemma 2. Let (X, sI , sG , O) be a planning task, let s  S be a state with 0 < h+ (s) < ,
and let P + (s) be an optimal relaxed plan for s. Say that oDG+ = (V, A) is an optimal rplan
dependency graph for P + (s) where, for every x  V \ {x0 }, the invertible/induced oDT G+
x
transitions have irrelevant side effect deletes and no side effects on V \ {x0 }. Let s0  S be a
state in the invertible surroundings of s according to oDG+ . Let s00 be a state reached from
s0 by a P + (ss0 , x) operator o constituting a transition (c, c0 ) for x  V , where s0 (x) = c,
that has self-irrelevant deletes. Then removing o from P + (ss0 ) yields a relaxed plan for
s00 .
Proof. By Lemma 1, P + (ss0 ) is a relaxed plan for s0 . Now, upon execution of o, in s00 ,
its effects are true, i.e., we have (x, c0 ) and any side effects (if present). On the other hand,
obviously the only facts (z, e) that are true in s0 but not in s00 are in ctx(c, c0 ){(x, c)}. Since,
by prerequisite, the transition (c, c0 ) has self-irrelevant deletes, all facts in ctx(c, c0 ){(x, c)}
are either irrelevant or rop(c, c0 )-only relevant, meaning they are not in the goal and occur
in no operator precondition other than, possibly, that of o itself. The claim follows directly
from that.
We remark that a much more easily formulated, and more general, version of Lemma 2
could be proved simply by associating the notion of self-irrelevant deletes with operators
rather than transitions, and postulating only that o be used in P + (s). That argument
corresponds to part (A) in the proof to Lemma 3 in the authors previous work (Hoffmann,
2005). We state the argument in the particular form above since that will be the form we
need below.
We are now almost ready to prove the main lemma behind our exit path construction.
We need one last notation, capturing a simpler form of the cost function costd (oDG+ )
202

fiAnalyzing Search Topology Without Running Any Search

that we considered in Section 5. The simpler function does not make use of the shortcut construction;Pthat construction will be introduced separately further below. We define
costd (oDG+ ) := xV costd (x), where costd (x) :=
(
1
x = x0
P
+
d
0
diam(oDT Gx )  x0 :(x,x0 )A cost (x ) x 6= x0
Lemma 3. Let (X, sI , sG , O) be a planning task, let s  S be a state with 0 < h+ (s) < ,
and let P + (s) be an optimal relaxed plan for s. Say that oDG+ = (V, A) is a successful

optimal rplan dependency graph for P + (s). Then there exists an operator sequence 
o so
that:

(I) 
o constitutes a monotone path in S from s to a state s1 with h+ (s) > h+ (s1 ).

(II) The length of 
o is at most costd (oDG+ ) if we have Definition 2 condition (2a) or
(2b), and is at most costd (oDG+ ) + 1 if we have Definition 2 condition (2c).
Proof. Let xk , . . . , x1 be a topological ordering of V \{x0 } according to the arcs A. Consider
a state s0 where for every x  V \ {x0 } we have that s0 (x) is a vertex in oDT G+
x , and for
every variable x outside V \ {x0 } we have that s0 (x) = s(x) unless s(x) is irrelevant. Say
that preo0  s0 . Note first that such a state s0 exists. By definition, we have that either
preo0 (x0 ) is undefined or that preo0 (x0 ) = s(x0 ) = s0 (x0 ). (Note that for every variable
x outside V \ {x0 } we have that s0 (x) = s(x) unless s(x) is irrelevant covers also the
case where a transition on V \ {x0 } has a side effect on x0 , whose delete must then by
prerequisite be irrelevant and thus either the side effect is x0 := s(x0 ) or o0 is not actually
preconditioned on x0 .) By Definition 1 and because P + (s) is a relaxed plan for s, each
variable x  Xpreo is contained in V unless preo0 (x) = s(x). For the same reasons, by
0
+
construction of oDT G+
x , we have that preo0 (x) is a vertex in oDT Gx .
Now, consider the state s1 that results from applying o0 to s0 . We first consider the
situation where s0 is in the invertible surroundings of s according to oDG+ ; the opposite
case will be discussed further below. We can apply Lemma 1 to s0 , and hence have a relaxed
+
(s, x), for x 
plan P + (ss0 ) for s0 that results from replacing, in P + (s), some moves of P<0
+
+
+
0
V \ {x0 }, with their inverses. In particular, h (s)  h (s0 ), and P (ss0 , x ) = P + (s, x0 )
for all x0 6 V . What is a relaxed plan for s1 ? We distinguish Definition 2 condition (2)
cases (a), (b), and (c).

+
In case (a), by definition we have that P>0
(s) contains a sub-sequence 
o0 so that pre+



o0
+
+
+
S1 and eff 
 R1  C0  F0 . This implies that we can remove o0 from P (s  s0 ) and

o0
obtain a relaxed plan P1+ for s1 , thus getting h+ (s) > h+ (s1 ). More precisely, we construct

P1+ by: removing o0 from P + (ss0 ); if Xeff o0  (V \ {x0 }) 6=  then moving 
o0 to occur at


+
+
the start of P1 ; if Xeff o0  (V \ {x0 }) =  then moving o0 to occur at the start of P>0
(s)
+
(which is unchanged in P (ss0 )).

Observe first that o0  P + (s  s0 ) and 
o0 is a sub-sequence of P + (s  s0 ) since the
adaptation pertains exclusively to operators that precede o0 in P + (s). Second, of course
the values established by o0 are true in s1 .

Third, 
o0 is applicable (in the relaxation) at its assigned point in P1+ . To see this,
consider first the case where Xeff o0  (V \ {x0 }) 6= . Then, by definition of S1 , pre+
is


o
0

203

fiHoffmann

contained in (prevo0  eff o0 ) and the set of facts (x, c)  s where there exists no o such that
+
x  Xeff o and o is either o0 or in P<0
(s) or is the responsible operator for the inverse of
+
0
a transition taken by an operator o  P<0
(s). All these facts will be true in s1 . This is
obvious for prevo0  eff o0 and follows for the other facts because they were true in s and
cannot have been affected by any operator on the path to s1 . Consider now the case where
Xeff o0  (V \ {x0 }) = . By definition of S1 , pre+
is contained in the previous sets of facts,


o0
plus {(x, c) | (x, c)  F0 , x  V \ {x0 }}. The latter facts, as far as relevant, will all be true

at the start of 
o0 in P1+ . This is because execution of o0 does not affect the execution of
+
P (ss0 ), and thus of P1+ , up to this point. But then, with what was argued in Lemma 1,
we have that the outcome of such execution in s0 contains, on the variables V \ {x0 }, the
+
relevant part of the outcome of P<0
(s) in s  that is, the relevant part of F0 . Since o0 does
not affect these variables, the same is true of s1 , which concludes this point.
Finally, consider any facts (z, e) that are true in s0 but not in s1 , and that may be

needed by P1+ behind 
o0 , i.e., that either are in the goal or in the precondition of any of
these operators. Observe that, since inverse operators are performed only for transitions
on variables V \ {x0 }, and since they do not include any new outside preconditions, any
such (z, e) is contained in R1+ .24 Now, say first that (z, e)  F0 . Then, with the above,
(z, e)  (ctx(s(x0 ), c){(x0 , s(x0 ))})F0 R1+ and thus (z, e)  eff +
by prerequisite and we


o0
+
(s)  else, this
are done. What if (z, e) 6 F0 ? Note that, then, (z, e) 6 preo for any o  P<0
+
precondition would not be true in the relaxed execution of P (s) and thus P + (s) would not
+
(s), and thus (z, e) is not needed
be a relaxed plan. Neither is (z, e) added by any o  P<0
+
as the precondition of any inverse operator used in P (s  s0 )  these operators do not
introduce new outside preconditions, and of course use only own-preconditions previously
added by other operators affecting the respective variable. Thus the only reason why (z, e)
+
could be needed in P1+ is if either (z, e)  sG or (z, e)  preo for some o  P>0
(s). If
+
(z, e)  sG then certainly, since P (s) is a relaxed plan, it is achieved by some operator o
in P + (s). We cannot have o = o0 since the effect of o0 is true in s1 , and we cannot have
+
+
(s), and thus o is contained in P1+ and we are
(s) since (z, e) 6 F0 . Thus o  P>0
o  P<0
+
(s), the same arguments apply, i.e., there must be
done. If (z, e)  preo0 for some o0  P>0
+
0
o  P>0 (s), ordered before o , that adds (z, e). This concludes the proof for case (a).
Consider now case (b), where s(x0 ) 6 R1+ , and the transition (s(x0 ), c) has replaceable
side effect deletes, i.e., ctx(s(x0 ), c)  sG =  and, for every o0 6= o  O where preo 
ctx(s(x0 ), c) 6=  there exists o0  O so that eff o0 = eff o and preo0  prevo0  eff o0 . We
obtain a relaxed plan for P1+ by removing o0 from P + (s  s0 ), and replacing any other
operators o with the respective o0 if needed. Precisely, say that (z, e) is true in s0 but not
in s1 . If z = x0 then e = s(x0 ) is not needed in P1+ by construction. For every other z, we
must have (z, e)  ctx(s(x0 ), c). Then (z, e) is not a goal by prerequisite. For any operator
o  P1+ that has (z, e) as a precondition, we can replace o with the postulated operator o1
that is obviously applicable in s1 and has the same effect. This concludes this case.
Consider last case (c), where by definition s(x0 ) 6 R1+ , and the transition (s(x0 ), c) has
recoverable side effect deletes. Here, the guarantee to decrease h+ is obtained not for s1
24. Note in particular the special case of inverse transitions on non-leaf variables x, which may have a
precondition in x that is added by, but not needed as a prerequisite of, the operators in P + (s, x). Such
preconditions  and only such preconditions  may be needed in P + (ss0 ) and thus in P1+ , but not in
P + (s). It is for this reason that we include these facts in the definition of R1+ .

204

fiAnalyzing Search Topology Without Running Any Search

itself, but for a successor state s2 of s1 . Namely, let o0 be the operator recovering the relevant
side effect deletes of (s(x0 ), c). Precisely, let   ctx(s(x0 ), c) so that   s0 (such a  exists
by definition of ctx(s(x0 ), c)). Then,
S let o0 be an operator so that preo0  (prevo0  eff o0 )
and eff o0  , eff o0    (sG  o0 6=o0 O preo0 ) (such an operator exists by case (b)). Say
that we obtain P1+ by replacing, in P + (ss0 ), o0 with o0 . Then P1+ is a relaxed plan for
s1 . To see this, note first that o0 is applicable in s1 by virtue of preo0  (prevo0  eff o0 ).
Further, note that the only values deleted by o0 are those in  plus (x0 , s0 (x0 )). Since
s0 (x0 ) = s(x0 ), by s(x0 ) 6 R1+ we know that s0 (x0 ) 6SR1+ and thus this delete is of no
consequence. As for , by virtue of eff o0    (sG  o0 6=o0 O preo0 ) all facts that could
possibly be relevant are re-achieved by o0 . Finally, the values established by o0 are true in
s1 .
Now, say we obtain s2 by applying o0 in s1 . Then removing o0 from P1+ yields a relaxed
plan for s2 . This is simply because its established effects are true in s2 , and by virtue of
eff o0   the only facts it deletes are side-effects of the transition (s(x0 ), c). By case (c),
these are not relevant for anything except possibly the recovering operators. The recovering
operator o0 we have just removed from P1+ . As for any other recovering
S operators o that
could still be contained in P1+ , since eff o   and eff o0    (sG  o0 6=o0 O preo0 ), all
relevant facts that o could possibly achieve are already true in s2 and thus we can remove
o as well. Hence, overall, h+ (s) > h+ (s2 ).
In cases (a) and (b) we can prove (I) by constructing a monotone path to s1 , in case (c)
the same is true of s2 . (Of course, we will also show (II), by constructing a path that has at
most the specified length; we will ignore this issue for the moment.) The only difficulty in
constructing such a path is achieving the preconditions of o0 . These preconditions may not
be satisfied in s, so we need to reach the state s0 where they are satisfied. We need to do so
without ever increasing the value of h+ . Note that, if we decrease the value of h+ somewhere
along the way, then we have already reached an exit on a monotone path, and are done. Thus
in what follows we will only show the upper bound h+ (s). With Lemma 1, this bounding can
be accomplished by starting at s, and always taking only oDT G+
x transitions of variables
x  V pertaining to the second case in Definition 2 condition (3), i.e., transitions that are
invertible/induced and have irrelevant side effect deletes and no side effects on V \ {x0 }. In
what follows we will, for brevity, refer to such transitions as case2. Note here that, this
way, we will reach only states in the invertible surroundings of s according to oDG+ . For

any such operator sequence 
o , by Lemma 1 we know that h+ (s)  h+ (s0 ) for all states
0
s along the way. Now, what if we cannot reach s0 by using such a sequence, i.e., what if
0
we would have to take a non-case2 oDT G+
x transition (c, c ) of variable x, at some state
s0 ? By prerequisite we know that transition (c, c0 ) has self-irrelevant deletes. We can apply
Lemma 2 because: s0 is in the invertible surroundings of s according to oDG+ ; since were
following a transition path, clearly s0 (x) = c, i.e., the value of the relevant variable in s0 is
the start value of the last transition we are taking; and by construction, P + (ss0 ) changes
P + (s) only in the case2 transitions, and thus the responsible operator rop(c, c0 ) (which is
not case2) is guaranteed to be contained in P + (ss0 ). Note here that rop(c, c0 ) cannot be
used in any of the case2 transitions for any other V \ {x0 } variable we might have taken on
the path to s0 , because by prerequisite all these transitions have no side effects on V \ {x0 },
in contradiction to o constituting a transition for the variable x at hand. Thus we know
that h+ (s) > h+ (s0 ) so we have already constructed our desired monotone path to an exit
205

fiHoffmann



and can stop. Else, if we can reach s0 by such a sequence 
o , then with the above, 
o  ho0 i


(respectively o  ho0 , o0 i, in case (c)) constitutes the desired path.

It remains to show how exactly to construct the operator sequence 
o . Consider a
topological ordering of V , xk , . . . , x1 . In what follows, we consider depth indices k  d 
0, and we say that a variable x  V has depth d, written depth(x) = d, iff x = xd . Each
d characterizes the d-abstracted planning task which is identical to the original planning
task except that all (and only) those outside preconditions, of all oDT G+
x transitions for
variables x where depth(x)  d, are removed that pertain to values of variables x0 where
depth(x0 ) > d. We prove by induction over d that:

(*) For the d-abstracted task, there exists an operator sequence 
o d so that:


(a) either (1) 
o d ho0 i is an execution path applicable in s, or (2) 
o d is an execution path

0
applicable in s, and the last transition (c, c ) for variable x taken in 
o d is relevant,
has self-irrelevant deletes, its responsible operator is contained in the adapted relaxed
plan for the state s0 it is applied to, and s0 (x) = c;

(b) 
o d , except in the last step in case (2) of (a), uses only case2 oDT G+
x transitions for
variables x with 1  depth(x)  d;

(c) the number of operators in 
o d  ho0 i pertaining to any x  V is at most costd (x).

Our desired path 
o then results from setting d := k. To see this, note that the kabstracted planning task is identical to the original planning task. The claim then follows
with our discussion above: (a) and (b) together mean that h+ decreases monotonically




+
on
P o d and is less dthan h (s) at its end. Given (c), the length of o d is bounded by
xV,depth(x)d cost (x). This proves the claim when adding the trivial observation that,
if we have Definition 2 condition (2) case (c) as discussed above, then we need to add one
additional operator at the end of the path.

We now give the proof of (*). The base case, d = 0, is trivial. Just set 
o 0 to be empty.
By the construction of (V, A) as per Definition 1, and by construction of the 0-abstracted
task, all outside preconditions of o0 are either true in s or have been removed. All of (a)
(case (1)), (b), (c) are obvious.

Inductive case, d  d + 1. Exploiting the induction hypothesis, let 
o d be the operator




sequence as per (*). We now turn o d into the requested sequence o d+1 for the d + 1abstracted planning task.
For the remainder of this proof, we will consider oDT G+
x , for any x  V \ {x0 }, to
contain also any irrelevant transitions, i.e., we omit this restriction from Definition 1. This
is just to simplify our argumentation  as we will show, the oDT G+
x paths we consider do
not contain any irrelevant transitions, and hence are contained in the actual oDT G+
x as per
Definition 1.

Let o be the first operator in 
o d  ho0 i. o may not be applicable in s, in the d +
1-abstracted planning task. The only reason for that, however, may be a precondition
that was removed in the d-abstracted planning task but that is not removed in the d + 1abstracted planning task. By construction, that precondition must pertain to xd+1 . Say
the precondition is (xd+1 , c). By induction hypothesis, we know that o is contained in
+
P<0
(s), or is responsible for an inverse transition of such an operator. In both cases, since
inverse transitions introduce no new outside preconditions, (xd+1 , c) is a precondition of an
206

fiAnalyzing Search Topology Without Running Any Search

+
operator in P<0
(s). Thus c is a vertex in oDT G+
xd+1  this is trivial if (xd+1 , c) is true in
s (which actually cannot be the case here because else o would be applicable in s in the
d + 1-abstracted planning task), and if (xd+1 , c) is not true in s it follows because P + (s)
is a relaxed plan and must thus achieve (xd+1 , c) before it is needed as a precondition.

+
Hence, P<0
(s, xd+1 ) must contain a shortest path 
q in oDT G+
xd+1 from s(xd+1 ) to c. All
the transitions on the path are not irrelevant. To see this, note first that the endpoint is an
operator precondition by construction, and thus the last transition (c1 , c) is not irrelevant.
But then, neither is the previous transition, (c2 , c1 ): if it was, then (xd+1 , c1 ) would be in no
+
operator precondition; but then, rop(c1 , c)  which is contained in P<0
(s) by construction

+
 would also constitute the transition (c2 , c) in oDT Gxd+1 and thus 
q would not be a


shortest path in contradiction. Iterating the argument, q does not contain any irrelevant
transitions. Thus, since depth(xd+1 ) = d + 1, by Definition 1 (which includes all nonsatisfied preconditions of relevant transitions) and by construction of the d + 1-abstracted

planning task, all the outside preconditions used in rop(
q ) are either true in s or have been


removed. Hence we can execute rop( q ). We do so until either we have reached the end of
the sequence, or until the last transition taken in oDT G+
xd+1 was not case2, and hence has
self-irrelevant deletes by prerequisite. In the latter case, since we are following a path and
since as discussed above the adapted relaxed plan exchanges only operators pertaining to
case2 transitions and thus not the last one we just executed, we clearly have attained (a)

case (2) and can stop  the part of rop(
q ) that we executed is, on its own, an operator

sequence 
o d+1 as desired. In the former case, we reach a state s0 where s0 (xd+1 ) = c (and
nothing else of relevance has been deleted, due to the non-existence of relevant side-effect
deletes). In s0 , o can be applied, leading to the state s00 .

Let now o0 be the second operator in 
o  ho i. Like above, if o0 is not applicable in s00 ,
d

0

then the only reason may be an unsatisfied precondition of the form (xd+1 , c0 ). Like above,
+
(s), and hence c0 is a vertex in oDT G+
o0 or its inverse is contained in P<0
xd+1 . Likewise,
00
+
s (xd+1 ) = c is a vertex in oDT Gxd+1 . Now, we have not as yet used any non-case2
transition in oDT G+
xd+1 , or else we wouldnt get here. This means that we are still in
the invertible surroundings around s(xd+1 ) of oDT G+
xd+1 . Clearly, this implies that there
+
0
exists a path in oDT Gxd+1 from c to c (we could simply go back to s(xd+1 ) and move

to c0 from there). Taking the shortest such path 
q , clearly the path length is bounded
+
by the diameter of oDT Gxd+1 . The path does not contain any irrelevant transitions  the
endpoint c0 has been selected for being an operator precondition, the values in between are
part of a shortest path in oDT G+
xd+1 , and thus the same argument as given above applies.

Thus the outside preconditions used by the operators constituting 
q are either true in s or
have been removed  this follows from the construction of (V, A) as per Definition 1 and by
+
construction of the d + 1-abstracted planning task for operators in P<0
(s), and follows for
inverses thereof because inverse operators introduce no new outside preconditions. Hence

we can execute 
q in s00 . We do so until either we have reached the end of the path, or until
the last transition taken was not case2, and hence has self-irrelevant deletes by prerequisite.
Consider the latter case. The state s0 just before the last transition is reached only by
case2 transitions, and since the transition is in oDT G+
xd+1 but not case2, the responsible
+
operator must be contained in P (s) and with that in the adapted relaxed plan P + (ss0 )
for s0  recall here that, as pointed out above, since case2 transitions are postulated to have
207

fiHoffmann

no side effects on V \{x0 }, the responsible operator cannot be used by any of them. Further,
clearly since we are following a path of transitions, we have that the value of xd+1 in s0 is
the start value of the transition. Hence we have attained (a) case (2) and can stop. In the
former case, we have reached a state where o0 can be applied (and nothing of relevance has
been deleted, due to the postulated non-existence of relevant side-effect deletes, for case2

transitions). Iterating the argument, we get to a state where the last operator of 
o d  ho0 i
can be applied, by induction hypothesis reaching a state s1 as desired by (a) case (1).
Properties (a) and (b) are clear from construction. As for property (c), to support any

operator of 
o d  ho0 i, clearly in the above we apply at most diam(oDT G+
xd+1 ) operators
pertaining to xd+1 (or we stop the sequence earlier than that). Note further that, for all

operators o in 
o d  ho0 i with unsatisfied preconditions on xd+1 in the above, if o pertains to
variable x then we have (xd+1 , x)  A. This is a consequence of the construction of (V, A)
as per Definition 1, and the fact that inverse transitions do not introduce new outside

preconditions. Thus, in comparison to 
o d  ho0 i, overall we execute at most
X
diam(oDT G+
k(x)
xd+1 ) 
x:(xd+1 ,x)A



additional operators in 
o d+1  ho0 i, where k(x) is the number of operators in 
o d  ho0 i
pertaining to variable x. By induction hypothesis, property (c) of (*), we have that k(x) 
costd (x), for all x with depth(x) < d + 1, and thus for all x with (xd+1 , x)  A. Hence we
get, for the newly inserted steps affecting xd+1 , the upper bound
X
diam(oDT G+
)

costd (x)
xd+1
x:(xd+1 ,x)A

which is identical to costd (xd+1 ). This concludes the argument.
We next note that we can improve the exit distance bound in case we do not insist on
monotone exit paths:
Lemma 4. Let (X, sI , sG , O) be a planning task, let s  S be a state with 0 < h+ (s) < ,
and let P + (s) be an optimal relaxed plan for s. Say that oDG+ = (V, A) is a successful
optimal rplan dependency graph for P + (s). Let V   V \ {x0 } so that, for every x  V  ,
all oDT G+
x transitions are invertible/induced and have irrelevant side effect deletes and no
side effects on V \{x0 }, and all other DT Gx transitions either are irrelevant, or have empty

conditions and irrelevant side effect deletes. Then there exists an operator sequence 
o so
that:

(I) 
o constitutes a path in S from s to a state s1 with h+ (s) > h+ (s1 ).

(II) The length of 
o is at most costd (oDG+ ) if we have Definition 2 condition (2a) or
(2b), and is at most costd (oDG+ ) + 1 if we have Definition 2 condition (2c).
Proof. This is a simple adaptation of Lemma 3, and we adopt in what follows the terminology of the proof of that lemma. The only thing that changes is that the bound imposed on
exit path length is sharper, and that we do not insist on that path being monotone. At the
level of the proof mechanics, what happens is that, whenever xd+1  V  , when we choose a
208

fiAnalyzing Search Topology Without Running Any Search


path 
q to achieve the next open precondition of an operator o already chosen to participate


in o d  ho0 i, then we do not restrict ourselves to paths within oDT G+
xd+1 , but allow also any
shortest path through DT Gxd+1 . Being a shortest path in DT Gxd+1 to a value that occurs

as an operator precondition, 
q contains no irrelevant transitions (same argument as in the

proof of Lemma 3). Further, 
q will be executable because by prerequisite the alternative
(non-oDT G+
)
transitions
in
it
have no outside conditions; for original/induced transitions,
x
precondition achievement works exactly as before. Note here the important property that
open preconditions to be achieved for xd+1 will only ever pertain to values contained in
oDT G+
xd+1 . This is trivial to see by induction because alternative transitions do not have
any outside preconditions. Since by prerequisite any deletes of the alternative transitions
are irrelevant, executing them does no harm  all we need is a minor extension to Lemma 1,
allowing s0 to be identical with a state s00 in the invertible surroundings of s, modulo a
set of irrelevant values that hold in s00 but not in s; it is obvious that this extension is
valid. With this extension, it is also obvious that the arguments pertaining to s0 and s1

remain valid. Finally, consider the case where 
q involves a non-case2 oDT G+
xd+1 transition.
Then the state where this transition is applied is in the invertible surroundings of s. This
holds for any x 6 V  because for these our construction remains the same. It holds for
any x  V  because, first, alternative transitions have no outside conditions, hence cause
no higher-depth transitions to be inserted in between, hence the value of all lower-depth
+
variables x is in oDT G+
x ; second, by prerequisite, oDT Gx does not contain any non-case2
transitions, and thus the value of x were at clearly can be reached by case2 transitions.
Theorem 2. Let (X, sI , sG , O), s, P + (s), and oDG+ be as in Definition 1. If oDG+ is successful, then s is not a local minimum, and ed(s)  costd (oDG+ ). If we have Definition 2
condition (2a) or (2b), then ed(s)  costd (oDG+ )  1.
Proof. This is a direct consequence of Lemmas 3 and 4.
We note that the prerequisites of Lemma 4 could be weakened by allowing, for x  V  ,
outside conditions that are already true in s. This extension obviously does not break the
proof arguments. We have omitted it here to not make the lemma prerequisite even more
awkward than it already is.
As indicated, the exit path constructed in Lemma 4 is not necessarily monotone. Example 5 in Appendix A.4 contains a construction showing this.
A.3 Conservative Approximations
For the sake of self-containedness of this section, we re-state the definitions given in Section 6:
Definition 3. Let (X, sI , sG , O) be a planning task, let s  S with 0 < h+ (s) < , let
x0  XsG , and let t0 = (s(x0 ), c) be a relevant transition in DT Gx0 with o0 := rop(t0 ).
A local dependency graph for s, x0 , and o0 , or local dependency graph in brief, is a
graph lDG = (V, A) with unique leaf vertex x0 , and where x  V and (x, x0 )  A if either:
x0 = x0 , x  Xpreo , and preo0 (x) 6= s(x); or x0  V \ {x0 } and (x, x0 ) is an arc in SG.
0

209

fiHoffmann

A global dependency graph for x0 and o0 , or global dependency graph in brief, is a
graph gDG = (V, A) with unique leaf vertex x0 , and where x  V and (x, x0 )  A if either:
x0 = x0 and x0 6= x  Xpreo ; or x0  V \ {x0 } and (x, x0 ) is an arc in SG.
0

Definition 4. Let (X, sI , sG , O), s, t0 , o0 , and G = lDG or G = gDG be as in Definition 3.
We say that G = (V, A) is successful if all of the following holds:
(1) G is acyclic.
(2) If G = lDG then sG (x0 ) 6= s(x0 ), and there exists no transitive successor x0 of x0 in
SG so that x0  XsG and sG (x0 ) 6= s(x0 ).
(3) We have that t0 either:
(a) has self-irrelevant side effect deletes; or
(b) has replaceable side effect deletes; or
(c) has recoverable side effect deletes.
(4) For x  V \ {x0 }, all DT Gx transitions either are irrelevant, or have self-irrelevant
deletes, or are invertible and have irrelevant side effect deletes and no side effects on
V \ {x0 }.
Lemma 5. Let (X, sI , sG , O) be a planning task, and let s  S be a state with 0 < h+ (s) <
. Say that x0  X and, for every o0 = rop(s(x0 ), c) in DT Gx0 where t0 = (s(x0 ), c) is
relevant, lDGo0 is a successful local dependency graph for s, x0 , and o0 . Then, for at least
one of the o0 , there exist an optimal relaxed plan P + (s) for s, and a successful optimal rplan
dependency graph oDG+ for P + (s), x0 , and o0 , where oDG+ is a sub-graph of lDGo0 .
Proof. Observe first that Definition 4 property (2) forces any relaxed plan P + (s) to move
x0 , i.e., we have that P + (s, x0 ) is non-empty. In particular, P + (s, x0 ) takes a path in

DT Gx0 from s(x0 ) to sG (x0 ). Let 
q be a shortest such path taken by P + (s, x0 ), and let

o0 be the responsible operator of the first transition in 
q . Clearly, this transition has the
form (s(x0 ), c), i.e., o0 is one of the operators o0 in the claim. Lying on a shortest path from
s(x0 ) to sG (x0 ) in the sub-graph of DT Gx0 taken by P + (s, x0 ), the transition (s(x0 ), c) is
not irrelevant. This can be seen with exactly the same argument as given in the proof to

Lemma 3 for the transitions on the paths 
q constructed there, except that the endpoint is
now a goal instead of an operator precondition.
Next, observe that any optimal P + (s) contains at most one operator o with x0  Xpreo
and preo (x0 ) = s(x0 ). This also follows from Definition 4 property (2): x0 cannot become important for any non-achieved goal, i.e., no P + (s) operator outside P + (s, x0 ) relies on a precondition on x0 . To see this, assume that such an operator o does exist.
Then, since P + (s) is optimal, there exists a reason for the inclusion of o. Precisely,
o must achieve at least one fact that is needed in the terms of Hoffmann and Nebel
(2001b): a fact that is either in the goal or in the precondition of another operator o0
behind o in P + (s). Iterating this argument for o0 (if necessary), we obtain a sequence
o = o1 , (x1 , c1 ), o2 , (x2 , c2 ), . . . , on , (xn , cn ) where (xn , cn ) is a goal fact not satisfied in s and
where oi achieves (xi , ci ) in P + (s). Obviously, SG then contains a path from x0 to xn , and
xn  XsG and sG (xn ) 6= s(xn ), in contradiction to Definition 4 property (2). Thus such o
does not exist. With the same argument, it follows also that every operator in P + (s, x0 )
210

fiAnalyzing Search Topology Without Running Any Search

either has no side effect used elsewhere in the relaxed plan, or has no precondition on x0 .
Thus those operators in P + (s, x0 ) that are preconditioned on x0 serve only to transform
s(x0 ) into sG (x0 ). Of course, then, at most a single one of these operators relies on s(x0 )
or else P + (s) is not optimal.
Say in what follows that lDGo0 = (V, A). Denote by (V 0 , A0 ) the result of backchaining
+
by Definition 1 from o0 with P<0
(s). Definition 3 will include all variables and arcs included
by Definition 1. To see this, just note that all arcs (x, x0 ) included by Definition 1 are due
to relevant transitions. Hence (V 0 , A0 ) is a sub-graph of (V, A). In particular, since (V, A)
is acyclic, (V 0 , A0 ) is acyclic as well.
Our next observation is that, assuming that Definition 4 condition (2) holds true, Definition 4 condition (3a) implies Definition 2 condition (2a), Definition 4 condition (3b) implies
Definition 2 condition (2b), and Definition 4 condition (3c) implies Definition 2 condition
(2c).
Consider first case (a) where t0 has self-irrelevant side effect deletes. We show that
+
R1  C0 = . Recall here the notations of Appendix A.2  C0 = {(x0 , s(x0 ))}  ctx(t0 ), and
R1+ is a super-set of the set of facts that we will need for the relaxed plan after removing o0 .
For all variables except x0 , it is clear that there is no fact in this intersection: all facts in
ctx(t0 ) are irrelevant or o0 -only relevant by prerequisite, and are thus not contained in R1+ .
Hence, (x0 , s(x0 )) remains as the only possible content of R1+  C0 . We show in what follows
that (x0 , s(x0 )) 6 R1+ , and thus (x0 , s(x0 )) 6 R1+ C0 and the latter intersection is empty, as
desired. Recall that R1+ denotes the union of sG , the precondition of any o0 6= o  P + (s),
and the precondition of any operator which is the responsible operator for an induced
transition in oDT G+
x , with x  V \ {x0 }. By Definition 4 condition (2), (x0 , s(x0 )) 6 sG .
As argued above, o0 is the only operator in P + (s) that may be preconditioned on s(x0 ) and
thus it is not in the precondition of any o0 6= o  P + (s). Lastly, say that p is a precondition
of a responsible operator for an induced transition in oDT G+
x , the corresponding original
transition being t. Then, since inverse transitions do not introduce any new conditions,
+
(s). But then, since
p  cond(t) and thus p  prerop(t) where, by definition, rop(t)  P<0
+
o0 6= rop(t)  P (s), we have (x0 , s(x0 )) 6 prerop(t) , which implies that p 6= (x0 , s(x0 )).
Thus (x0 , s(x0 )) 6 R1+ like we needed to show.
Consider now case (b) where t0 has recoverable side effect deletes. To show Definition 2
condition (2b) for o0 = rop(t0 ), all we need to prove is that s(x0 ) is not oDG+ -relevant,
i.e., that s(x0 ) 6 R1+ . This was already shown above.
For case (c), t0 has replaceable side effect deletes. Again, to show Definition 2 condition
(2c) for t0 ), all we need to prove is that s(x0 ) is not oDG+ -relevant.
Consider finally the conditions imposed on non-leaf variables x  V \ {x0 }, i.e., Definition 4 condition (4) and Definition 2 condition (3). By Definition 4 condition (4), the
DT Gx transitions of every x  V \ {x0 } either are irrelevant, or have self-irrelevant deletes,
or are invertible and have irrelevant side effect deletes and no side effects on V \ {x0 }. If
a DT Gx transitions is irrelevant then it cannot be in oDT G+
x , thus the 2nd or 3rd case is
+
0
true of the oDT Gx transitions of every x  V \ {x0 }. This concludes the argument.
Theorem 3. Let (X, sI , sG , O) be a planning task, and let s  S be a state with 0 <
h+ (s) < . Say that x0  X so that, for every o0 = rop(s(x0 ), c) in DT Gx0 where
(s(x0 ), c) is relevant, lDGo0 is a successful local dependency graph. Then s is not a local
211

fiHoffmann

minimum, and ed(s)  maxo0 costD (lDGo0 ). If, for every lDGo0 , we have Definition 4
condition (3a) or (3b), then ed(s)  maxo0 costD (lDGo0 )  1.
Proof. By Lemma 5, for some choice of o0 = rop(s(x0 ), c) there exists an optimal relaxed
plan P + (s) and a successful optimal rplan dependency graph oDG+ = (V 0 , A0 ) for P + (s),
so that oDG+ is a sub-graph of lDGo0 with the same unique leaf vertex x0 . We can apply
Lemma 3 and obtain that s is not a local minimum.
To see the other part of the claim, let V  be defined as in Section 6, i.e., V  is the subset
of V \ {x0 } for which all DT Gx transitions either are irrelevant, or are invertible and have
empty conditions, irrelevant side effect deletes, and no side effects on V \ {x0 }. Then, for
each DT Gx transition t where x  V  , t satisfies both the restriction required by Lemma 4
+
on oDT G+
x transitions  if t is irrelevant, then it cannot be in oDT Gx , else it is invertible
and has irrelevant side effect deletes and no side effects on V \ {x0 }  and the restriction
required by Lemma 4 on the other transitions  either irrelevant, or empty conditions and
irrelevant side effect deletes. We can hence apply Lemma 4 to oDG+ , and obtain a (not
necessarily monotone) path to an exit, with length bound costd (oDG+ ) if (s(x0 ), c) has
irrelevant side effect deletes or replaceable side effect deletes, and costd (oDG+ ) + 1 if
(s(x0 ), c) has recoverable side effect deletes. It thus suffices to show that costD (lDGo0 ) 
costd (oDG+ ). That, however, is obvious because V  V 0 , costD (x)  0 for all x, and
0
maxPath(DT Gx )  diam(oDT G+
x ) for all x  V .
Theorem 4. Let (X, sI , sG , O) be a planning task. Say that all global dependency graphs
gDG are successful. Then S does not contain any local minima and, for any state s  S with
0 < h+ (s) < , ed(s)  maxgDG costD (gDG). If, for every gDG, we have Definition 4
condition (3a) or (3b), then ed(s)  maxgDG costD (gDG)  1.
Proof. Let s  S be a state. We need to prove that s is no local minimum. If h+ (s) = 0 or
h+ (s) = , there is nothing to show. Else, assume that the variables X are topologically
ordered according to the strongly connected components of SG, and let x0  X be the
uppermost variable so that x0  XsG and sG (x0 ) 6= s(x0 ); obviously, such x0 exists. Clearly,
the only chance for x0 to not satisfy Definition 4 condition (2)  there exists no transitive
successor x0 of x0 in SG so that x0  XsG and sG (x0 ) 6= s(x0 )  is if there exists x0 in
the same strongly connected SG component, with x0  XsG (and sG (x0 ) 6= s(x0 )). But
then, there exists a transition t0 in DT Gx0 with an outside condition eventually leading, by
backwards chaining in SG, to x0 . Let gDG0 be the global dependency graph for x0 and
rop(t0 ) (such a gDG0 exists because x0  XsG ). Since Definition 3 includes all transitive
SG-predecessors of x0 pertaining to the conditions of t0 , gDG0 includes x0 . But then, since
x0 and x0 lie in the same strongly connected component, Definition 3 eventually reaches
x0 . Thus gDG0 contains a cycle, in contradiction to the prerequisite. It follows that the
strongly connected SG component of x0 contains only x0 , and thus Definition 4 condition
(2) holds true.
Now, say that o0 is responsible for a relevant transition of the form (s(x0 ), c) in DT Gx0 .
Then there exists a local dependency graph lDG for s, x0 , and o0 so that lDG is a sub-graph
of gDG. This follows from the simple observation that Definition 3 will include, for gDG,
all variables and arcs that it will include for lDG. (Note here that any precondition of o0
212

fiAnalyzing Search Topology Without Running Any Search

on x0 , if present, is satisfied in s because o0 = rop(s(x0 ), c), and thus Definition 3 will not
include x0 as a predecessor for achieving o0 preconditions in lDG.)
Obviously, given the above, lDG is successful. Since this works for any choice of notirrelevant (s(x0 ), c), we can apply Theorem 3. The claim follows directly from this and
the fact that costD (gDG)  costD (lDG). The latter is obvious because costD increases
monotonically when adding additional variables.
A.4 Example Constructions
Our first example shows that, even within the scope of our basic result, operators are not
necessarily respected by the relaxation, i.e., an operator may start an optimal real plan yet
not occur in any optimal relaxed plan.
Example 1. Consider the planning task in Figure 4. Variables are shown (in dark green)
on the left hand side of their respective DTG. Circles represent variable values, and lines
represent DTG transitions. Transitions with a condition are longer lines, with the condition
inscribed below the line (in blue). For each variable, a dashed arrow indicates the value in
the initial state sI . Where a goal value is defined, this is indicated by a circled value. Where
needed, we will refer to the operators responsible for a transition in terms of the respective
variable followed by the indices of the start and end value. For example, the operator moving
x from c1 to c2 will be referred to as x12. We abbreviate states {(x, c), (y, d)} as (c, d).
We stick to these conventions throughout this section.

x

d1

c1

c2

d3

c3

d7

y
d1

d2

d3

d7

Figure 4: Planning task underlying Example 1.
As shown in Figure 4, the DTG of x consists of three vertices whose connection requires
the conditions d1 and d3 , or alternatively d7 as a shortcut. The domain of y is a line of
length 6 requiring no conditions.
Clearly, the support graph of this planning task is acyclic, and all transitions in all DTGs
have no side effects and are invertible. However, operator y34 (for example) is not respected
by the relaxation. To see this, note first that h+ (sI ) = 4: the only optimal relaxed plan is
hy32, y21, x12, x23i because the relaxed plan ignores the need to move back to d3 for operator x23. On the other hand, the only optimal (real) plan for sI is hy34, y45, y56, y67, x17i.
If we choose to use y32 instead, like the optimal relaxed plan does, then we end up with the
sequence hy32, y21, x12, y12, y23, x23i which is 1 step longer. Hence, in sI , y34 starts an
optimal plan, but does not start an optimal relaxed plan.
213

fiHoffmann

We next give three examples showing how local minima can arise in very simple situations generalizing our basic result only minimally. We consider, in this order: cyclic support
graphs; non-invertible transitions; transitions with side effects.
Example 2. Consider the planning task in Figure 5.

x
c1

d1

c2

y
d1

d2

dn1

dn

c1

Figure 5: Planning task underlying Example 2.
The DTG of x is just two vertices whose connection requires the condition d1 . The
domain of y is a line of length n requiring no conditions, with a shortcut between d1 and
dn that requires c1 as condition. Clearly, all transitions in all DTGs have no side effects
and are invertible. However, SG contains a cycle between x and y because they mutually
depend on each other. We will show now that this mutual dependence causes the initial state
sI = {(x, c1 ), (y, d1 )} to be a local minimum, for n  5. We abbreviate, as before, states
{(x, c), (y, d)} as (c, d). We have h+ (sI ) = 2: the only optimal relaxed plan is hx12, y1ni.
Now consider the operators applicable to sI = (c1 , d1 ):
 Execute x12, leading to s1 = (c2 , d1 ) with h+ (s1 ) = 2 due to hx21, y1ni. From here,
the only new state to be reached is via y12, giving s2 = (c2 , d2 ) with h+ (s2 ) = 3 due to
hy21, x21, y1ni. (Note here that n  2  3 by prerequisite, so a relaxed plan composed
of yi(i + 1) operators also has  3 steps.) We have h+ (s2 ) > h+ (sI ) so this way we
cannot reach an exit on a monotone path.
 Execute y12, leading to s3 = (c1 , d2 ) with h+ (s3 ) = 3 due to hy21, x12, y1ni. (Note
here that n  2  3 by prerequisite, so a relaxed plan moving y by ypp operators has
 4 steps.) Again, the path is not monotone.
 Execute y1n, leading to s4 = (c1 , dn ) with h+ (s4 ) = 2 due to hyn1, x12i. From here,
the only new state to be reached is via yn(n1), giving s5 = (c1 , dn1 ) with h+ (s5 ) = 3
due to hy(n1)n, yn1, x12i. (Note here that n2  3 by prerequisite, so a relaxed plan
moving y to d1 via dn2 , . . . , d2 has  3 + 2 steps.) Again, the path is not monotone.
No other operators are applicable to sI , thus we have explored all states reachable from sI on
monotone paths. None of those states is an exit, proving that sI is a local minimum (as are
s1 and s4 ). There is, in fact, only a single state s with h+ (s) = 1, namely s = (c2 , dn1 ).
Clearly, reaching s from sI takes n  1 steps: first apply x12, then traverse d2 , . . . , dn2 . So
the exit distance of sI is n  3, thus this distance is unbounded.
214

fiAnalyzing Search Topology Without Running Any Search

In Section 9, the following modification of Example 2 is considered. We set n := 2, i.e.,
the domain of y is reduced to the two values d1 , d2 ; and we remove the line d2 , . . . , dn2 ,
i.e., y can move only via what was previously the short-cut. This modified example falls
into the SAS+ -PUBS tractable class identified by Backstrom and Klein (1991), and it still
contains a local minimum (the example is unsolvable, though).
Example 3. Consider the planning task in Figure 6.

x
c1

d2

c2

d1

c3

y
d1

d2

dn

Figure 6: Planning task underlying Example 3. The arrow between d1 and d2 indicates that
the respective DTG transition is directed, i.e., there exists no transition from d2
to d1 .
The DTG of x is three vertices whose connection requires (starting from the initial value
c1 ) first condition d2 , then condition d1 . The domain of y is a circle of length n requiring
no conditions, and being invertible except for the arc from d1 to d2 .
Clearly, the support graph is acyclic and all transitions in all DTGs have no side effects.
However, the non-invertible arc from d1 to d2 causes the initial state sI = (c1 , d1 ) to be a
local minimum for all n  3. This is very easy to see. We have h+ (sI ) = 3 due to the
only optimal relaxed plan hy12, x12, x23i. Note here that the relaxed plan does not have to
move y back because (y, d1 ) is still true after executing y12. Now, the operators applicable
to sI are y12 and y1n. The latter, reaching the state sn = (c1 , dn ), immediately increases
the value of h+ . This is because, with n  3, y1n does not get y closer to d2 , while moving
it farther away from d1 (both of which need to be achieved). The shortest relaxed for sn is
hyn1, y12, x12, x23i. Alternatively, say we apply y12 in sI , reaching the state s2 = (c1 , d2 ).
We have h+ (s2 ) = n + 1: we need to apply, in the relaxation, x12, n  1 steps to complete
the circle from d2 back to d1 , and x23. Thus, for n  3, s2 has a larger h+ value than sI .
It follows that sI is a local minimum. The nearest exit to sI is sn1 = (c2 , dn1 ): sn1 has
the relaxed plan hy(n  1)n, yn1, x23i of length 3, and after applying y(n  1)n we get h+
value 2. Reaching sn1 from sI takes 1 step moving x and n  2 steps moving y. So the
exit distance of sI is n  1, thus this distance is unbounded.
Example 4. Consider the planning task in Figure 7.
The DTG of x consists of two kinds of transitions. First, there is a line c1 , . . . , cn of
transitions requiring no conditions. Second, there are direct links, called short-cuts in what
follows, between cn and every other ci , conditioned on value d1 of y. The DTG of y contains
just two vertices that are connected unconditionally. Moving from d1 to d2 has the side-effect
cn . (That side-effect is responsible for the towards-cn direction of the short-cuts in the
DTG of x.)
215

fiHoffmann

d1
d1

x
c1

c2

cn

cn

y
d1

d2

Figure 7: Planning task underlying Example 4. The (red) inscription cn above the line
between d1 and d2 indicates that the transition from d1 to d2 has the side effect
cn .
The support graph is acyclic. Its only arc goes from y to x, due to the short-cuts in the
DTG of x, and due to the operator y12 which has an effect on x and a precondition on y.
The transitions are all invertible; in particular each short-cut has both, a direction towards
cn and vice versa. However, the side-effect of y12 causes the initial state sI = (c1 , d1 ) to be
a local minimum for all n  3.
We have h+ (sI ) = 1 due to the only optimal relaxed plan hy12i. Note here that the
relaxed plan does not care about the side effect of y12, because c1 is still true afterward.
Now, if we apply any operator in sI that leaves c1 , then clearly we increase h+ by 1: no
matter what move we make, the relaxed plan must include both y12 and a move back to c1 .
The only other available option in sI is to apply y12. We get the state s1 = (cn , d2 ). There,
h+ (s1 ) = 2 as well, because the relaxed plan needs to re-achieve c1 . Since n  3, doing so
via the unconditional sequence cn , . . . , c1 takes  2 steps. The only alternative is to use the
short-cut xn1 from cn to c1 ; doing so involves applying y21 in the first place, giving us a
relaxed plan of length 2. Hence all direct successors of sI have a heuristic value > 1, and
so sI is a local minimum. Note also that the exit distance of sI grows with n. The nearest
exit is a state from which the goal can be reached in a single step. Clearly, the only such
state is (c2 , d2 ). The shortest path to that state, from sI , applies y12 and then moves along
the unconditional line cn , . . . , c2 , taking 1 + (n  2) = n  1 steps.
We next show that the exit path constructed using short-cuts, leading to the improved
bound costd instead of costd , may be non-monotone, and that the improved bound may
indeed under-estimate the length of a shortest monotone exit path.
Example 5. Consider the planning task in Figure 8.
In this example, the only optimal relaxed plan for the initial state moves z along the
path e0 , . . . , e2n  note here that all these values are needed for moving y  then moves y to
d2k+2n , then moves x to c1 . This gives a total of h+ (sI ) = 2n + (2k + 2n) + 1 = 4n + 2k + 1
steps.
The only operators applicable to sI move z. If we move along the line e0 , . . . , e2n , then
h+ remains constant: we always need to include the moves back in order to achieve the
own goal of z. Once we reach e2n , we can move y one step, then need to move z back,
etc. During all these moves, up to the state where y = d2k+2n , as long as z stays within
216

fiAnalyzing Search Topology Without Running Any Search

x
c0

y
d0

e 2n

d1

e0

e0

e 2n

d2

d2k+2n

c1

d2k

e1

d2k+1 e2

e 2n

d2k+2n

z
e0

e1

e 2n1

e 2n

e

Figure 8: Planning task underlying Example 5.
e0 , . . . , e2n , h+ remains constant. To see this, observe first that of course it suffices for a
relaxed plan to reach once, with z, all the values on this line, taking 2n moves wherever we
are on the line; the moves for y are as before. Second, observe that indeed all these moves
are needed: wherever y is on the line d0 , . . . , d2k+2n , it needs to move to d2k+2n in order to
suit x, and it needs to move to d0 to suit its own goal. Every value in e0 , . . . , e2n appears
as a condition of at least one of these y moves. Thus, from sI , the nearest exit reached
this way is the state s where y = d2k+2n and z = e2n : there, we can move x to c1 which

decreases h+ to 4n + 2k. The length of the exit path 
o we just described, from sI to s,
obviously is 2k  (2n + 1) + 2n  2 = 4kn + 2k + 4n.
What happens if we move z to e0 ? Consider first that we do this in sI . Then h+ increases
to 4n + 2k + 2: we need to reach all values on the line e0 , . . . , e2n , which from e0 takes one

step more. The same argument applies for any state traversed by 
o , because, as argued,

in any such state we still need to reach all values on the line e0 , . . . , e2n . Thus 
o is the
shortest monotone path to an exit.
The only optimal rplan dependency graph oDG+ for sI is the entire SG, and oDT G+
z
contains all of DT Gz except e0 . The only global dependency graph gDG is the entire SG.
Clearly, in sI , the next required value to reach for any variable is e2n , so the construction
in the proof to Theorem 2 will first try to reach that value. When using short-cuts as
accounted for by costd (.), the exit path constructed will move to e2n via e0 rather than via
the line e0 , . . . , e2n , and thus as claimed this exit path is not monotone.
Finally, consider the bound returned by costd (oDG+ ). Obviously, costd (oDG+ ) =
costD (gDG). We obtain the bound (1) + costd (oDG+ ) = (1) + 1[costd (x)] + 1  (2k +
d
2n)[costd (x)  diam(oDT G+
y )] + (2k + 2n)  (n + 1)[cost (y)  diam(DT Gz )]. Note here
that diam(DT Gz ) = n + 1 because DT Gz is a circle with 2n + 2 nodes. Overall, we have
(1)+costd (oDG+ ) = (2k+2n)(n+2) = 2kn+4k+2n2 +4n. For sufficiently large k, this
is less than 4kn+2k +4n, as claimed. In detail, we have 4kn+2k +4n > 2kn+4k +2n2 +4n
n2
iff 2kn  2k > 2n2 iff kn  k > n2 iff k > n1
. This holds, for example, if we set n := 2
and k := 5.
The reader will have noticed that Example 5 is very contrived. The reason why we need
such a complicated unrealistic example is that costd , and with that costd , contains two
sources of over-estimation, cf. the discussion in Section 5. In particular, every move of non217

fiHoffmann

leaf variables is supposed to take a whole oDT G+ /DT G diameter. To show that costd is not
in general an upper bound on the length of a monotone exit path, we thus need the presented
construction around k so that its under-estimation  considering diam(DT Gz ) instead of
diam(oDT G+
z )  outweighs this over-estimation. Importantly, constructing examples where
the short-cuts temporarily increase h+ (but costd nevertheless delivers an upper bound
on monotone exit path length) is much easier. All that needs to happen is that, for whatever
reason, we have a variable z like here, where the currently required value (e2n in Example 5)
is reached in oDT G+
z values along an unnecessarily long path all of whose values are needed
in the relaxed plan. This happens quite naturally, e.g., in transportation domains if the
same vehicle needs to load/unload objects along such a longer path.
We now demonstrate that, in a case where our analyses apply, exit distance may be
exponential.
Example 6. Consider the planning task in Figure 9.

x0
c 10

x1
c 11

c 52

c 21

c 12

c 51

c 31

c 20

c 52

c 12

c 41

c 51

xn
c 1n

c 2n

c 3n

c 4n

c 5n

Figure 9: Planning task underlying Example 6.
The DTG of x0 is two vertices whose connection is conditioned on c15 . For all other
variables xi , we have five vertices on a line, alternatingly requiring the last vertex ci+1
of
5
i+1
xi+1 and the first vertex c1 of xi+1 . Clearly, the only optimal rplan dependency graph
oDG+ for sI , and the only global dependency graph gDG for the task is the full support
graph SG. This is acyclic, and all transitions are invertible and have no side effects, thus
our analyses apply.
What are h+ (sI ) and ed(sI )? For a relaxed plan, we need to move x0 to c02 . Due to
the conditioning, for each variable both extreme values  left and right hand side  are
required so we need 4 moves for each xi with 1  i  n. Thus h+ (sI ) = 1 + 4n.
Now, consider any state s where s(x0 ) = c01 . To construct a relaxed plan, obviously we
still need 1 move for x0 . We also still need 4 moves for each other variable. Consider x1 .
If s(x1 ) = c11 then we need to move it to c15 in order to be able to move x0 . If s(x1 ) = c12
then we need to move it to c15 in order to be able to move x0 , and to c11 for its own goal,
and so forth. In all cases, all four transitions must be taken in the relaxed plan. Due to the
conditioning, recursively the same is true for all other variables. Thus, h+ (s) = 1 + 4n.
218

fiAnalyzing Search Topology Without Running Any Search

This means that the nearest exit is a state s0 where x0 has value c01 and x1 has value c15 :
in s0 , we can move x0 and afterward, definitely, 4n steps suffice for a relaxed plan. What is
the distance to a state s0 ? We need to move x1 four times. Lets denote this as d(x1 ) = 4.
Each move requires 4 moves of x2 , so d(x2 ) = 16. The sequence of moves for x2 inverses
direction three times. At these points, x3 does not need to move so d(x3 ) = (d(x2 )  3)  4.
Generalizing this, we get d(xi+1 ) = [d(xi )  ( d(x4 i )  1)]  4 = 3d(xi ) + 4, so the growth over
n is exponential.
Obviously, Example 6 also shows that plan length can be exponential in cases where
Theorem 4 applies. We remark that Example 6 is very similar to an example given by
Domshlak and Dinitz (2001). The only difference is that Domshlak and Dinitzs example
uses different conditions for transitions to the left/to the right, which enables them to
use smaller DTGs with only 3 nodes. In our setting, we cannot use different conditions
because we need the transitions to be invertible. This causes the loss of exit path steps
in those situations where the next lower variable inverses direction and thus relies on
the same outside condition as in the previous step. Indeed, for DTGs of size 3, this loss
of steps results in a polynomially bounded exit distance. The recursive formula for d(xi )
becomes d(xi+1 ) = [d(xi )  ( d(x2 i )  1)]  2 = d(xi ) + 2, resulting in ed(sI ) = n2 + n.
On the other hand, costd and costD still remain exponential in this case, because they
do not consider the loss incurred by inversing
directions. Precisely, it is easy to see that
P
costd (oDG+ ) = costD (gDG) = 1 + ni=1 2i = 2n+1  1. This proves that these bounds
can over-estimate by an exponential amount.
The next example shows that the exit path constructed (implicitly) by our analyses may
be exponentially longer than an optimal plan for the task.
Example 7. Consider the planning task in Figure 10.

x0
c 10

c 51

c 20

c10

x1
c 11

c 52

c 21

0
c4n+1

c 12

c 31

c 52

c 12

c 41

xn
c 1n

c 2n

c 3n

c 4n

c 5n

Figure 10: Planning task underlying Example 7.
219

c 51

fiHoffmann

In this example, the only optimal relaxed plan for the initial state is the same as in
Example 6, because the alternative route via c001 , . . . , c00(4n+1) takes 1 + 4n + 1 = 4n + 2 >
4n + 1 steps. Thus the exit path constructed remains the same, too, with length exponential
in n. However, the length of the shortest plan is 4n + 2.
Note in Example 7 that the observed weakness  being guided into the wrong direction
 is caused by a weakness of optimal relaxed planning, rather than by a weakness of our
analysis. The relaxation overlooks the fact that moving via x1 , . . . , xn will incur high costs
due to the need to repeatedly undo and re-do conditions achieved beforehand. Note also
that, in this example too, we get an exponential over-estimation of exit distance.
We finally show that feeding Theorem 2 with non-optimal relaxed plans does not give
any guarantees:
Example 8. Consider the planning task in Figure 11.

x
c1
g21

d2

c2

g2n+2

c

v1

y
d1

en

d2

g11

g21

g1n+2

g2n+2

v n+2

z
e1

e n1

en

Figure 11: Planning task underlying Example 8. The arrow between en1 and en indicates
that the respective DTG transition is directed, i.e., there exists no transition
from en to en1 .
There are two ways to achieve the goal c2 : either via moving y and z, or by moving
v1 , . . . , vn+2 . The only optimal relaxed plan chooses the former option, giving h+ (sI ) = n+1.
As soon as n  3, however, the only parallel-optimal relaxed plan P + (sI ) chooses the latter
option because moving y and z results in n + 1 sequential moves, whereas v1 , . . . , vn+2 can
be moved in parallel, giving parallel length 3.
Consider what happens to h+ in either of the options. If we move z, then h+ remains
constant because we need to move z back into its own goal. As soon as we reach z = en ,
h+ =  because the last transition is uni-directional and we can no longer achieve the own
goal of z. Thus there is no exit path, and in particular no monotone exit path, via this
option.
Say we move v1 , . . . , vn+2 instead. In the first move (whichever vi we choose), h+
increases because the shortest option is to undo this move and go via y and z: this takes
n + 2 steps whereas completing the vi moves and going via c0 takes (n + 1) + 2 = n + 3 steps.
220

fiAnalyzing Search Topology Without Running Any Search

Thus there is no monotone exit path via this option either, and sI is a local minimum. After
completing the n + 2 moves of vi and moving to x = c0 , we have h+ = (n + 2) + 1 due to the
shortest relaxed plan that moves back all vi and moves to x = c2 . To reduce this heuristic
value to the initial value h+ (sI ) = n + 1, we need to execute a further 2 of these steps. The
state we have then reached has a better evaluated neighbor, so the exit distance is n + 5.
Consider now the effect of feeding Theorem 2 with the parallel-optimal plan P + (sI ).
Clearly, the optimal rplan dependency graph oDG+ constructed for P + (sI ) consists of x
and all the vi variables, but does not include y nor z. Thus the theorem applies, and
it wrongly concludes that sI is not a local minimum.
The exit distance bound computed is
P
(1

1)[costd (x)  diam(DT Gvi )] = n + 2.
(1) + costd (oDG+ ) = (1) + 1[costd (x)] + n+2
i=1
This is less than the actual distance ed(sI ) = n + 5, and thus this result is also wrong.
Say we modify Example 8 by making the last transition of z undirected, but making
one of the vi transitions unidirectional to the right. Then the v1 , . . . , vn+2 option leads into
a dead end, whereas the y, z option succeeds. In particular, Theorem 2 does not apply to
oDG+ constructed for the parallel-optimal relaxed plan P + (sI ), and thus this is an example
where using non-optimal relaxed plans results in a loss of information.
A.5 Benchmark Performance Guarantees
We give definitions of the 7 domains mentioned in Propositions 1 and 2. For each domain,
we explain why the respective property claimed holds true. In most of the domains, we
assume some static properties as are used in PDDL to capture unchanging things like the
shape of the road network in a transportation domain. We assume in what follows that
such static predicates have been removed prior to the analysis, i.e., prior to testing the
prerequisites of Theorem 4.
Definition 5. The Logistics domain is the set of all planning tasks  = (V, O, sI , sG ) whose
components are defined as follows. V = P V where P is a set of package-location variables
p, with Dp = L  V where L is some set representing all possible locations, and V is a set
of vehicle-location variables v, with Dv = Lv for a subset Lv  L of locations. O contains
three types of operators: move, load, and unload, where move(v, l1, l2) = ({v =
l1}, {v = l2}) for l1 6= l2, load(v, l, p) = ({v = l, p = l}, {p = v}), and unload(v, l, p) =
({v = l, p = v}, {p = l}). sI assigns an arbitrary value to each of the variables, and sG
assigns an arbitrary value to some subset of the variables.
Every global dependency graph gDG in Logistics either has a package p as the leaf
variable x0 , or has a vehicle variable v as the leaf variable x0 . In the latter case gDG
consists of only x0 , with no arcs. In the former case, o0 is preconditioned on a single vehicle
v only, leading to a single non-leaf variable v. In both cases, gDG is acyclic, all involved
transitions have no side effects, and all involved transitions are invertible. Thus we can
apply Theorem 4. We have costD (gDG) = 1 + 1  1 for packages and costD (gDG) = 1 for
vehicles, thus overall we obtain the correct bound 1.
Definition 6. The Miconic-STRIPS domain is the set of all planning tasks  =
(V, O, sI , sG ) whose components are defined as follows. V = O  D  B  S  {e} where
|O| = |D| = |B| = |S| and: O is a set of passenger-origin variables o, with Do = L where L
221

fiHoffmann

is some set representing all possible locations (floors); D is a set of passenger-destination
variables d with Dd = L; B is a set of passenger-boarded variables b with Db = {1, 0}; S is
a set of passenger-served variables s with Ds = {1, 0}; e is the elevator-location variable
with De = L. O contains three types of operators: move, board, and depart, where
move(l1, l2) = ({e = l1}, {e = l2}) for l1 6= l2, board(l, i) = ({e = l, oi = l}, {bi = 1}), and
depart(l, i) = ({e = l, di = l, bi = 1}, {bi = 0, si = 1}). sI assigns arbitrary locations to the
variables O, D, and e, and assigns 0 to the variables B and S. sG assigns 1 to the variables
S.
Passenger-origin and passenger-destination variables are static, i.e., not affected by any
operator. Thus the common pre-processes will remove these variables, using them only to
statically prune the set of operators that are reachable. We assume in what follows that
such removal has taken place.
Every global dependency graph gDG in Miconic-STRIPS has a passenger-served variable
si as the leaf variable x0 . This leads to non-leaf variables bi and e, with arcs from e to
both other variables and from bi to si . Clearly, gDG is acyclic. The transitions of e are
all invertible and have no side effects. The transition (0, 1) of bi (is not invertible since
departing has a different condition on e but) has an irrelevant own-delete  bi = 0 does not
occur anywhere in the goal or preconditions  and has no side effects and thus irrelevant
side effect deletes. The transition (1, 0) of bi (is not invertible but) is irrelevant  bi = 0
doesnt occur anywhere. The transition (0, 1) of the leaf variable si has self-irrelevant side
effect deletes  bi = 1 occurs only in the precondition of the transitions own responsible
operator rop(0, 1) = depart(ld , i). Hence we can apply Theorem 4. This delivers the bound
costD (gDG)  1 = 1 + 1[si ] + (1  1)[costD (si )  maxPath(DT Gbi )] + (2  1)[(costD (si ) +
costD (bi ))  diam(DT Ge )] = 3.
Definition 7. The Simple-TSP domain is the set of all planning tasks  = (V, O, sI , sG )
whose components are defined as follows. V = {p}  V where: p is the position variable,
with Dp = L where L is some set representing all possible locations; and V , with |V | = |L|,
is a set of location-visited variables v, with Dv = {1, 0}. O contains a single type of
operators: move(l1, l2) = ({p = l1}, {p = l2, vl2 = 1}) for l1 6= l2. sI assigns an arbitrary
value to p and assigns 0 to the variables V . sG assigns 1 to the variables V .
Every global dependency graph gDG in Simple-TSP has a location-visited variable vi
as the leaf variable x0 . This leads to the single non-leaf variable p. Clearly, gDG is acyclic.
Every transition (0, 1) of vi considered, induced by o0 = move(l1, li), has replaceable side
effect deletes. Any operator o = move(l1, x) can be replaced by the equivalent operator
move(li, x) unless x = li. In the latter case, we have o0 = o which is excluded in the
definition of replaceable side effect deletes. Every transition (l1, l2) of p clearly is invertible;
it has the irrelevant side effect delete vl2 = 0; its side effect is only on vl2 which is not
a non-leaf variable of gDG. Hence we can apply Theorem 4. This delivers the bound
costD (gDG)  1 = 1 + 1[vi ] + (1  1)[costD (vi )  diam(DT Gp )] = 1.
We consider an extended version of the Movie domain, in the sense that, whereas the
original domain version considers only a fixed range of snacks (and thus the state space is
constant across all domain instances), we allow to scale the number of different snacks.25
25. The original domain version allows to scale the number of operators adding the same snack. All these
operators are identical, and can be removed by trivial pre-processes.

222

fiAnalyzing Search Topology Without Running Any Search

Definition 8. The Movie domain is the set of all planning tasks  = (V, O, sI , sG )
whose components are defined as follows. V = {c0, c2, re}  H. Here, c0 is the counterat-zero variable, with Dc0 = {1, 0}; c2 is the counter-at-two-hours variable, with
Dc2 = {1, 0}; re is the movie-rewound variable, with Dre = {1, 0}; H are have-snack
variables h with Dh = {1, 0}. O contains four types of operators: rewindTwo, rewindOther, resetCounter, and getSnack, where rewindT wo = ({c2 = 1}, {re = 1}),
rewindOther = ({c2 = 0}, {re = 1, c0 = 0}), resetCounter = (, {c0 = 1}), and
getSnack(i) = (, {hi = 1}). sI assigns an arbitrary value to all variables. sG assigns
the re, c0, and H variables to 1.
Note that, depending on the value of the static variable c2, the operator set will be
different: if sI (c2) = 1 then rewindOther is removed, if sI (c2) = 0 then rewindT wo is
removed. We refer to the former as case (a) and to the latter as case (b).
Every global dependency graph gDG consists of a single (leaf) variable. The transitions
of each h variable have no side effects and thus have irrelevant side effect deletes. The
transition (0, 1) of c0 has no side effects and thus has irrelevant side effect deletes. The
transition (1, 0) of c0 is irrelevant. For case (a), the transition (0, 1) of re has no side
effects and thus has irrelevant side effect deletes so we can apply Theorem 4. For case (b),
the transition (0, 1) of re has the side effect c0 = 0. Observe that (1) this fact itself is
irrelevant; and (2) that the only   ctx(0, 1) is {c0 = 1}, and o := resetCounter satisfies
 = preo  (prevrop(0,1)  eff rop(0,1) ) = {re = 1, c0 = 0}, {c0
S = 1} = eff o   = {c0 = 1},
and {c0 = 1} = eff o  {(y, d) | (y, d)  , (y, d)  sG  rop(c,c0 )6=o0 O preo0 } = {c0 = 1}.
Thus the transition has recoverable side effect deletes, and again we can apply Theorem 4.
In case (a), for all gDGs the bound costD (gDG)  1 applies. Obviously, costD (gDG) = 1
and thus we obtain the correct bound 0. In case (b), the bound costD (gDG) applies, and
again costD (gDG) = 1 so we obtain the correct bound 1.
Definition 9. The Ferry domain is the set of all planning tasks  = (V, O, sI , sG ) whose
components are defined as follows. V = C  {f, e} where: C is a set of car-location
variables c, with Dc = L  {f } where L is some set representing all possible locations; f is
the ferry-location variable with Df = L; e is the ferry-empty variable with De = {1, 0}.
O contains three types of operators: sail, board, and debark, where sail(l1, l2) =
({f = l1}, {f = l2}) for l1 6= l2, board(l, c) = ({f = l, c = l, e = 1}, {c = f, e = 0}),
and debark(l, c) = ({f = l, c = f }, {c = l, e = 1}). sI assigns 1 to variable e, assigns an
arbitrary value to variable f , and assigns an arbitrary value other than f to the variables
C. sG assigns an arbitrary value 6= f to (some subset of ) the variables C and f .
Let s be an arbitrary reachable state where 0 < h+ (s) < , and let P + (s) be an
arbitrary optimal relaxed plan for s. Then we can always apply Theorem 2. To show this,
we distinguish three cases: (a) s(e) = 1, o0 = board(l, c) is the first board operator in P + (s),
and we set x0 = c; (b) s(e) = 0, o0 = debark(l, c) is the first debark operator in P + (s),
and we set x0 = c; (c) P + (s) contains no board or debark operator and we set o0 to be the
first operator, sail(l1, l2), in P + (s), with x0 = f . Obviously, exactly one of these cases will
hold in s. Let oDG+ = (V, A) be the sub-graph of SG including x0 and the variables/arcs
included as per Definition 1. Let t0 be the transition taken by o0 .
In case (a), obviously we can reorder P + (s) so that either board(l, c) is the first operator
in P + (s), or all its predecessors are sail operators. oDG+ then either (1) includes no new
223

fiHoffmann

(non-leaf) variables at all, or (2) includes only f . As for f , clearly all its transitions are
invertible and have no side effects. The transition t0 has the own effect (c, f ) deleting (c, l)
which clearly is not needed in the rest of P + (s). It has the side effect e = 0 deleting e = 1.
That latter fact may be needed by other board operators in P + (s). However, necessarily
P + (s) contains an operator of the form debark(l0 , c), which is applicable after board(l, c)
and a sequence of moves that P + (s) must contain from l to l0 ; debark(l0 , c) recovers e = 1.
+
Thus the oDG+ -relevant deletes of t0 are P>0
(s)-recoverable. In case (b), similarly we can
+
reorder P (s) so that either (1) debark(l, c) is the first operator in P + (s), or (2) all its
predecessors are sail operators. The transition t0 has the own effect (c, l) deleting (c, f )
which clearly is not needed in the rest of P + (s); it has the side effect e = 1 deleting e = 0
which clearly is not needed in the rest of P + (s). Thus, again, the oDG+ -relevant deletes
+
+
of t0 are P>0
(s)-recoverable (the recovering sub-sequence of P>0
(s) being empty because
+
no recovery is required). In case (c), finally, oDG contains only f , t0 has no side effects,
and its own delete (f, l1) is not needed anymore (in fact, in this case l2 must be the goal
for f , and P + (s) contains only the single operator o0 ). Hence, in all cases, we can apply
Theorem 2. costd (oDG+ ) = 1 in cases (a1), (b1), and (c) so there we get the bound 0.
costd (oDG+ ) = 1 + diam(DT Gf ) = 2 in cases (a2) and (b2) so there we get the bound 1.
Definition 10. The Gripper domain is the set of all planning tasks  = (V, O, sI , sG )
whose components are defined as follows. V = {ro, f1 , f2 }  B. Here, ro is the robotlocation variable, with Dro = {L, R}; f1 , f2 are gripper-free variables, with Df1 = Df2 =
{1, 0}; and B are ball-location variables, with Db = {L, R, 1, 2}. O contains three types of
operators: move, pickup, and drop, where move(l1, l2) = ({ro = l1}, {ro = l2}) for
l1 6= l2, pickup(g, b, l) = ({ro = l, b = l, fg = 1}, {b = g, fg = 0}), and drop(g, b, l) = ({ro =
l, b = g}, {b = l, fg = 1}). sI assigns L to ro, assigns 1 to f1 and f2 , and assigns L to the
variables B. sG assigns R to the variables B.
Let s be an arbitrary reachable state where 0 < h+ (s) < , and let P + (s) be an
arbitrary optimal relaxed plan for s. Then we can always apply Theorem 2. We distinguish
two cases: (a) there exists b  B so that s(b) = g for g  {1, 2}, o0 = drop(g, b, R), and we
set x0 = b; (b) there exists no b  B so that s(b) = g for g  {1, 2}, o0 = pickup(g, b, L)
for some b  B is in P + (s), and we set x0 = b. Obviously, exactly one of these cases will
hold in s. Let oDG+ = (V, A) be the sub-graph of SG including x0 and the variables/arcs
included as per Definition 1. Let t0 be the transition taken by o0 .
In case (a), obviously we can reorder P + (s) so that either drop(g, b, R) is the first
operator in P + (s), or its only predecessor is move(L, R). oDG+ then either (1) includes no
new (non-leaf) variables at all, or (2) includes only ro. As for ro, clearly all its transitions
are invertible and have no side effects. The transition t0 has the own effect (b, R) deleting
(b, g) which clearly is not needed in the rest of P + (s); it has the side effect fg = 1 deleting
fg = 0 which clearly is not needed in the rest of P + (s). Thus the oDG+ -relevant deletes
+
of t0 are P>0
(s)-recoverable. In case (b), similarly we can reorder P + (s) so that either (1)
pickup(g, b, L) is the first operator in P + (s), or (2) its only predecessor is move(R, L). The
transition t0 has the own effect (b, g) deleting (b, L) which clearly is not needed in the rest of
P + (s). It has the side effect fg = 0 deleting fg = 1; that latter fact may be needed by other
pickup operators in P + (s). However, necessarily P + (s) contains the operators move(L, R)
and drop(g, b, R), which are applicable after board(l, c); drop(g, b, R) recovers fg = 1. Thus,
224

fiAnalyzing Search Topology Without Running Any Search

+
again, the oDG+ -relevant deletes of t0 are P>0
(s)-recoverable. Hence, in both cases, we can
d
+
apply Theorem 2. cost (oDG ) = 1 in cases (a1) and (b1), so there we get the bound 0.
costd (oDG+ ) = 1 + diam(ro) = 2 in cases (a2) and (b2) so there we get the bound 1.

Definition 11. The Transport domain is the set of all planning tasks  = (V, O, sI , sG )
whose components are defined as follows. V = P  V E  C where: P is a set of packagelocation variables p, with Dp = L  V E where L is some set representing all possible
locations; V E is a set of vehicle-location variables v, with Dv = L; and C is a set of
vehicle-capacity variables cv , with Dcv = {0, . . . , K} where K is the maximum capacity.
O contains three types of operators: drive, pickup, and drop, where: drive(v, l1, l2) =
({v = l1}, {v = l2}) for (l1, l2)  R where GR = (L, R) is an undirected graph of roads
over L; pickup(v, l, p, c) = ({v = l, p = l, cv = c}, {p = v, cv = c  1}), and drop(v, l, p, c) =
({v = l, p = v, cv = c}, {p = l, cv = c + 1}). sI assigns an arbitrary value in L to each of
the variables P  V E, and assigns K to the variables C. sG assigns an arbitrary value in
L to some subset of the variables P  V E.
Note here the use of numbers and addition/subtraction. These are, of course, not part
of the planning language we consider here. However, they can be easily encoded (on the
finite set of number {0, . . . , K}) via static predicates. After pre-processing, in effect the
resulting task will be isomorphic to the one obtained by the simple arithmetic above, which
we thus choose to reduce notational clutter.
Let s be an arbitrary reachable state where 0 < h+ (s) < . Then there exists an
optimal relaxed plan P + (s) for s so that we can apply Theorem 2. We distinguish three
cases: (a) there exists p  P so that s(p) = v for v  V E, o0 = drop(v, l, p, c) where
s(cv ) = c is in P + (s), and we set x0 = p; (b) there exists no p  P so that s(p) = v for
v  V E, o0 = pickup(v, l, p, K) for some p  P is in P + (s), and we set x0 = p; (c) P + (s)
contains no drop or pickup operator and we set o0 to be the first operator, drive(v, l1, l2), in
P + (s), with x0 = v. Obviously, we can choose P + (s) so that exactly one of these cases will
hold in s (the choice of P + (s) is arbitrary for (b) and (c), but in (a) there may exist optimal
relaxed plans where s(cv ) 6= c). Let oDG+ = (V, A) be the sub-graph of SG including x0
and the variables/arcs included as per Definition 1. Let t0 be the transition taken by o0 .
In case (a), obviously we can reorder P + (s) so that either o0 = drop(v, l, p, c) is the first
operator in P + (s), or all its predecessors are drive operators. oDG+ then either (1) includes
no new (non-leaf) variables at all, or (2) includes only v. As for v, clearly all its transitions
are invertible and have no side effects. The transition t0 has the own effect (p, v) deleting
(p, l) which clearly is not needed in the rest of P + (s). It has the side effect cv = c+1 deleting
cv = c. That latter fact may be needed by other operators in P + (s), either taking the form
drop(v, l0 , p0 , c) or the form pickup(v, l0 , p0 , c). Clearly, if P + (s) contains these operators
then we can replace them with drop(v, l0 , p0 , c + 1) and pickup(v, l0 , p0 , c + 1) respectively
 the value (cv , c + 1) will be true at their point of (relaxed) execution. Thus we can
choose P + (s) so that the P + (s)-relevant deletes of t0 are P + (s)-recoverable on V \ {x0 }.
In case (b), similarly we can reorder P + (s) so that either (1) o0 = pickup(v, l, p, K) is the
first operator in P + (s), or (2) all its predecessors are drive operators. The transition t0
has the own effect (p, v) deleting (p, l) which clearly is not needed in the rest of P + (s).
It has the side effect cv = K  1 deleting cv = K. That latter fact may be needed by
other operators in P + (s), taking the form pickup(v, l0 , p0 , K). However, necessarily P + (s)
225

fiHoffmann

contains an operator of the form drop(v, l0 , p, c0 ). If c0 6= K  1 then we can replace this
operator with drop(v, l0 , p, K  1) since, clearly, the value (cv , K  1) will be true at the
point of (relaxed) execution. Now, drop(v, l0 , p, K  1) is applicable after pickup(v, l, p, K)
and a sequence of drive operators that P + (s) must contain from l to l0 ; drop(v, l0 , p, K  1)
recovers cv = K. Thus, again, we can choose P + (s) so that the P + (s)-relevant deletes of
t0 are P + (s)-recoverable on V \ {x0 }. In case (c), finally, oDG+ contains only v, t0 has no
side effects, and its own delete (v, l1) is not needed anymore. Hence, in all cases, we can
apply Theorem 2. costd (oDG+ ) = 1 in cases (a1), (b1), and (c) so there we get the bound
0. costd (oDG+ ) = 1 + min(diam(oDT G+
v ), diam(DT Gv )) in cases (a2) and (b2) so there
the bound is at most the diameter of the road map GR .
When ignoring action costs, the Elevators domain of IPC 2008 is essentially a variant
of Transport. The variant is more general in that (a) each vehicle (each elevator) may have
its own maximal capacity, and (b) each vehicle can reach only a subset of the locations, i.e.,
each vehicle has an individual road map. On the other hand, Elevators is more restricted
than Transport in that (c) each vehicle road map is fully connected (every reachable floor
can be navigated to directly from every other reachable floor), and (d) goals exist only for
packages (passengers, that is), not for vehicles. Even when ignoring restrictions (c) and (d),
it is trivial to see that the arguments given above for Transport still hold true. Therefore,
whenever s is a reachable state with 0 < h+ (s) < , there exists an optimal relaxed plan
P + (s) for s so that we can apply Theorem 2. As before, the bound is at most the diameter
of the road map. Due to (c), this diameter is 1.

References
Backstrom, C., & Klein, I. (1991). Planning in polynomial time: The SAS-PUBS class.
Computational Intelligence, 7 (4).
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90 (1-2), 279298.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1
2), 533.
Botea, A., Muller, M., & Schaeffer, J. (2004). Using component abstraction for automatic
generation of macro-actions. In Koenig et al. (Koenig, Zilberstein, & Koehler, 2004),
pp. 181190.
Brafman, R., & Domshlak, C. (2003). Structure and complexity in planning with unary
operators. Journal of Artificial Intelligence Research, 18, 315349.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (12), 165204.
Cesta, A., & Borrajo, D. (Eds.), ECP01 (2001). Recent Advances in AI Planning. 6th
European Conference on Planning (ECP01), Lecture Notes in Artificial Intelligence,
Toledo, Spain. Springer-Verlag.
226

fiAnalyzing Search Topology Without Running Any Search

Chen, H., & Gimenez, O. (2010). Causal graphs and structurally restricted planning. Journal of Computer and System Sciences, 76 (7), 579592.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent offline coordination: Structure and complexity. In Cesta & Borrajo (Cesta & Borrajo, 2001), pp. 3443.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge in planning problems to minimize state encoding length. In Biundo, S., & Fox, M. (Eds.), Recent Advances in AI
Planning. 5th European Conference on Planning (ECP99), Lecture Notes in Artificial
Intelligence, pp. 135147, Durham, UK. Springer-Verlag.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal
of Artificial Intelligence Research, 9, 367421.
Fox, M., & Long, D. (1999). The detection and exploitation of symmetry in planning
problems. In Pollack, M. (Ed.), Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI99), pp. 956961, Stockholm, Sweden. Morgan
Kaufmann.
Garey, M. R., & Johnson, D. S. (1979). Computers and IntractabilityA Guide to the
Theory of NP-Completeness. Freeman, San Francisco, CA.
Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), ICAPS09 (2009). Proceedings of
the 19th International Conference on Automated Planning and Scheduling (ICAPS9),
Thessaloniki, Greece. AAAI.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239290.
Gerevini, A., & Schubert, L. (1998). Inferring state-constraints for domain independent
planning. In Mostow, J., & Rich, C. (Eds.), Proceedings of the 15th National Conference of the American Association for Artificial Intelligence (AAAI98), pp. 905912,
Madison, WI, USA. MIT Press.
Gimenez, O., & Jonsson, A. (2008). The complexity of planning problems with simple
causal graphs. Journal of Artificial Intelligence Research, 31, 319351.
Gimenez, O., & Jonsson, A. (2009a). The influence of k-dependence on the complexity of
planning. In Gerevini et al. (Gerevini, Howe, Cesta, & Refanidis, 2009), pp. 138145.
Gimenez, O., & Jonsson, A. (2009b). Planning over chain causal graphs for variables with
domains of size 5 is NP-hard. Journal of Artificial Intelligence Research, 34, 675706.
Haslum, P. (2007). Reducing accidental complexity in planning problems. In Veloso, M.
(Ed.), Proceedings of the 20th International Joint Conference on Artificial Intelligence
(IJCAI07), pp. 18981903, Hyderabad, India. Morgan Kaufmann.
Helmert, M. (2003). Complexity results for standard benchmark domains in planning.
Artificial Intelligence, 143, 219262.
Helmert, M. (2004). A planning heuristic based on causal graph analysis.. In Koenig et al.
(Koenig et al., 2004), pp. 161170.
Helmert, M. (2006). The fast downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
227

fiHoffmann

Helmert, M. (2009). Concise finite-domain representations for PDDL planning tasks. Artificial Intelligence, 173 (5-6), 503535.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats
the difference anyway? In Gerevini et al. (Gerevini et al., 2009), pp. 162169.
Hoffmann, J. (2003). Utilizing Problem Structure in Planning: A Local Search Approach,
Vol. 2854 of Lecture Notes in Artificial Intelligence. Springer-Verlag.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Nebel, B. (2001a). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J., & Nebel, B. (2001b). RIFO revisited: Detecting relaxed irrelevance. In Cesta
& Borrajo (Cesta & Borrajo, 2001), pp. 325336.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal
of Artificial Intelligence Research, 22, 215278.
Jonsson, A. (2009). The role of macros in tractable planning. Journal of Artificial Intelligence Research, 36, 471511.
Jonsson, P., & Backstrom, C. (1995). Incremental planning. In European Workshop on
Planning.
Jonsson, P., & Backstrom, C. (1998). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100 (1-2), 125176.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Boutilier, C.
(Ed.), Proceedings of the 21st International Joint Conference on Artificial Intelligence
(IJCAI09), pp. 17281733, Pasadena, CA, USA. Morgan Kaufmann.
Katz, M., & Domshlak, C. (2008a). New islands of tractability of cost-optimal planning.
Journal of Artificial Intelligence Research, 32, 203288.
Katz, M., & Domshlak, C. (2008b). Structural patterns heuristics via fork decomposition.
In Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. A. (Eds.), Proceedings of the
18th International Conference on Automated Planning and Scheduling (ICAPS08),
pp. 182189, Sydney, Australia. AAAI.
Knoblock, C. (1994). Automatically generating abstractions for planning. Artificial Intelligence, 68 (2), 243302.
Koenig, S., Zilberstein, S., & Koehler, J. (Eds.), ICAPS04 (2004). Proceedings of the
14th International Conference on Automated Planning and Scheduling (ICAPS04),
Whistler, Canada. AAAI.
Long, D., & Fox, M. (2000). Automatic synthesis and use of generic types in planning. In
Chien, S., Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the 5th International Conference on Artificial Intelligence Planning Systems (AIPS00), pp. 196205,
Breckenridge, CO. AAAI Press, Menlo Park.
McDermott, D. V. (1999). Using regression-match graphs to control search in planning.
Artificial Intelligence, 109 (1-2), 111159.
228

fiAnalyzing Search Topology Without Running Any Search

Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts and operators in
plan generation. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning. 4th
European Conference on Planning (ECP97), Vol. 1348 of Lecture Notes in Artificial
Intelligence, pp. 338350, Toulouse, France. Springer-Verlag.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. In Fox, D., & Gomes,
C. (Eds.), Proceedings of the 23rd National Conference of the American Association
for Artificial Intelligence (AAAI08), pp. 975982, Chicago, Illinois, USA. MIT Press.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. Journal of Artificial Intelligence Research, 39, 127177.
Rintanen, J. (2000). An iterative algorithm for synthesizing invariants. In Kautz, H. A.,
& Porter, B. (Eds.), Proceedings of the 17th National Conference of the American
Association for Artificial Intelligence (AAAI00), pp. 806811, Austin, TX, USA.
MIT Press.
Roberts, M., & Howe, A. (2009). Learning from planner performance. Artificial Intelligence,
173, 636561.
Vidal, V. (2004). A lookahead strategy for heuristic search planning. In Koenig et al.
(Koenig et al., 2004), pp. 150160.
Williams, B. C., & Nayak, P. P. (1997). A reactive planner for a model-based executive. In
Pollack, M. (Ed.), Proceedings of the 15th International Joint Conference on Artificial
Intelligence (IJCAI97), pp. 11781185, Nagoya, Japan. Morgan Kaufmann.

229

fiJournal of Artificial Intelligence Research 41 (2011) 397-406

Submitted 05/11; published 07/11

Research Note
Policy Invariance under Reward Transformations for
General-Sum Stochastic Games
Xiaosong Lu
Howard M. Schwartz

LUXIAOS @ SCE . CARLETON . CA
SCHWARTZ @ SCE . CARLETON . CA

Department of Systems and Computer Engineering
Carleton University
1125 Colonel By Drive, Ottawa, ON K1S 5B6 Canada

Sidney N. Givigi Jr.

S IDNEY.G IVIGI @ RMC . CA

Department of Electrical and Computer Engineering
Royal Military College of Canada
13 General Crerar Cres, Kingston, ON K7K 7B4 Canada

Abstract
We extend the potential-based shaping method from Markov decision processes to multi-player
general-sum stochastic games. We prove that the Nash equilibria in a stochastic game remains
unchanged after potential-based shaping is applied to the environment. The property of policy
invariance provides a possible way of speeding convergence when learning to play a stochastic
game.

1. Introduction
In reinforcement learning, one may suffer from the temporal credit assignment problem (Sutton &
Barto, 1998) where a reward is received after a sequence of actions. The delayed reward will lead
to difficulty in distributing credit or punishment to each action from a long sequence of actions and
this will cause the algorithm to learn slowly. An example of this problem can be found in some
episodic tasks such as a soccer game where the player is only given credit or punishment after a
goal is scored. If the number of states in the soccer game is large, it will take a long time for a
player to learn its equilibrium policy.
Reward shaping is a technique to improve the learning performance of a reinforcement learner
by introducing shaping rewards to the environment (Gullapalli & Barto, 1992; Mataric, 1994).
When the state space is large, the delayed reward will slow down the learning dramatically. To
speed up the learning, the learner may apply shaping rewards to the environment as a supplement
to the delayed reward. In this way, a reinforcement learning algorithm can improve its learning
performance by combining a "good" shaping reward function with the original delayed reward.
The applications of reward shaping can be found in the literature (Gullapalli & Barto, 1992;
Dorigo & Colombetti, 1994; Mataric, 1994; Randlv & Alstrm, 1998). Gullapalli and Barto (1992)
demonstrated the application of shaping to a key-press task where a robot was trained to press keys
on a keyboard. Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a
predefined animate-like behavior. Mataric (1994) presented an intermediate reinforcement function
for a group of mobile robots to learn a foraging task. Randlv and Alstrm (1998) combined reinforcement learning with shaping to make an agent learn to drive a bicycle to a goal. The theoretical
c
2011
AI Access Foundation. All rights reserved.

fiL U , S CHWARTZ , & G IVIGI

analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora,
2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented a potential-based shaping
reward that can guarantee the policy invariance for a single agent in a Markov decision process
(MDP). Ng et al. proved that the optimal policy keeps unchanged after adding the potential-based
shaping reward to an MDP environment. Following Ng et al., Wiewiora (2003) showed that the effects of potential-based shaping can be achieved by a particular initialization of Q-values for agents
using Q-learning. Asmuth et al. (2008) applied the potential-based shaping reward to a model-based
reinforcement learning approach.
The above articles focus on applications of reward shaping to a single agent in an MDP. For the
applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008)
introduced a social shaping reward for players to learn their equilibrium policies in the iterated
prisoners dilemma game. But there is no theoretical proof of policy invariance under the reward
transformation. In our research, we prove that the Nash equilibria under the potential-based shaping
reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under
the framework of general-sum stochastic games. Note that the similar work of Devlin and Kudenko
(2011) was published while this article was under review. But Devlin and Kudenko only proved
sufficiency based on a proof technique introduced by Asmuth et al. (2008), while we prove both
sufficiency and necessity using a different proof technique in this article.

2. Framework of Stochastic Games
Stochastic games were first introduced by Shapley (1953). In a stochastic game, players choose the
joint action and move from one state to another state based on the joint action they choose. In this
section, under the framework of stochastic games, we introduce Markov decision processes, matrix
games and stochastic games respectively.
2.1 Markov Decision Processes
A Markov decision process is a tuple (S, A, T,  , R) where S is the state space, A is the action space,
T : S  A  S  [0, 1] is the transition function,   [0, 1] is the discount factor and R : S  A  S  R
is the reward function. The transition function denotes a probability distribution over next states
given the current state and action. The reward function denotes the received reward at the next state
given the current action and the current state. A Markov decision process has the following Markov
property: the players next state and reward only depend on the players current state and action.
A players policy  : S  A is defined as a probability distribution over the players actions given
a state. An optimal policy   will maximize the players discounted future reward. For any MDP,
there exists a deterministic optimal policy for the player (Bertsekas, 1987).
Starting in the current state s and following the optimal policy thereafter, we can get the optimal
state-value function as the expected sum of discounted rewards (Sutton & Barto, 1998)
)
(
V  (s) = E


T

  j rk+ j+1 |sk = s,  

(1)

j=0

where k is the current time step, rk+ j+1 is the received immediate reward at the time step k + j + 1,
  [0, 1] is a discount factor, and T is a final time step. In (1), we have T   if the task is an
infinite-horizon task such that the task will run over infinite period. If the task is episodic, T is
398

fiP OLICY I NVARIANCE

UNDER

R EWARD T RANSFORMATIONS

defined as the terminal time when each episode is terminated at the time step T . Then we call the
state where each episode ends as the terminal state sT . In a terminal state, the state-value function is
always zero such that V (sT ) = 0 for all sT  S. Given the current state s and action a, and following
the optimal policy thereafter, we can define an optimal action-value function (Sutton & Barto, 1998)
h
i


(2)
Q (s, a) =  T (s, a, s ) R(s, a, s ) + V  (s )
s S

where T (s, a, s ) = Pr {sk+1 = s |sk = s, ak = a} is the probability of the next state being sk+1 = s
given the current state sk = s and action ak = a at time step k, and R(s, a, s ) = E{rk+1 |sk = s, ak = a,
sk+1 = s } is the expected immediate reward received at state s given the current state s and action
a. In a terminal state, the action-value function is always zero such that Q(sT , a) = 0 for all sT  S.
2.2 Matrix Games
A matrix game is a tuple (n, A1 , . . . , An , R1 , . . . , Rn ) where n is the number of players, Ai (i = 1, . . . , n)
is the action set for the player i and Ri : A1      An  R is the payoff function for the player i.
A matrix game is a game involving multiple players and a single state. Each player i(i = 1, . . . , n)
selects an action from its action set Ai and receives a payoff. The player is payoff function Ri is
determined by all players joint action from joint action space A1      An . For a two-player matrix
game, we can set up a matrix with each element containing a payoff for each joint action pair. Then
the payoff function Ri for player i(i = 1, 2) becomes a matrix. If the two players in the game are
fully competitive, we will have a two-player zero-sum matrix game with R1 = R2 .
In a matrix game, each player tries to maximize its own payoff based on the players strategy. A
players strategy in a matrix game is a probability distribution over the players action set. To evaluate a players strategy, we introduce the following concept of Nash equilibrium. A Nash equilibrium
in a matrix game is a collection of all players policies (1 ,    , n ) such that
Vi (1 ,    , i ,    , n )  Vi (1 ,    , i ,    , n ), i  i , i = 1,    , n

(3)

where Vi () is the expected payoff for player i given all players current strategies and i is any
strategy of player i from the strategy space i . In other words, a Nash equilibrium is a collection
of strategies for all players such that no player can do better by changing its own strategy given that
other players continue playing their Nash equilibrium policies (Basar & Olsder, 1999). We define
Qi (a1 , . . . , an ) as the received payoff of the player i given players joint action a1 , . . . , an , and i (ai )
(i = 1, . . . , n) as the probability of player i choosing action a1 . Then the Nash equilibrium defined
in (3) becomes





Qi (a1 , . . . , an )1 (a1 )    i (ai )    n (an ) 

a1 ,...,an A1 An

Qi (a1 , . . . , an )1 (a1 )    i (ai )    n (an ), i  i , i = 1,    , n

(4)

a1 ,...,an A1 An

where i (ai ) is the probability of player i choosing action ai under the player is Nash equilibrium
strategy i .
A two-player matrix game is called a zero-sum game if the two players are fully competitive.
In this way, we have R1 = R2 . A zero-sum game has a unique Nash equilibrium in the sense
of the expected payoff. It means that, although each player may have multiple Nash equilibrium
399

fiL U , S CHWARTZ , & G IVIGI

strategies in a zero-sum game, the value of the expected payoff Vi under these Nash equilibrium
strategies will be the same. If the players in the game are not fully competitive or the summation
of the players payoffs is not zero, the game is called a general-sum game. In a general-sum game,
the Nash equilibrium is no longer unique and the game might have multiple Nash equilibria. Unlike
the deterministic optimal policy for a single player in an MDP, the equilibrium strategies in a multiplayer matrix game may be stochastic.
2.3 Stochastic Games
A Markov decision process contains a single player and multiple states while a matrix game contains
multiple players and a single state. For a game with more than one player and multiple states,
we define a stochastic game (or Markov game) as the combination of Markov decision processes
and matrix games. A stochastic game is a tuple (n, S, A1 , . . . , An , T,  , R1 , . . . , Rn ) where n is the
number of the players, T : S  A1      An  S  [0, 1] is the transition function, Ai (i = 1, . . . , n)
is the action set for the player i,   [0, 1] is the discount factor and Ri : S  A1      An  S  R
is the reward function for player i. The transition function in a stochastic game is a probability
distribution over next states given the current state and joint action of the players. The reward
function Ri (s, a1 , . . . , an , s ) denotes the reward received by player i in state s after taking joint
action (a1 , . . . , an ) in state s. Similar to Markov decision processes, stochastic games also have the
Markov property. That is, the players next state and reward only depend on the current state and all
the players current actions.
To solve a stochastic game, we need to find a policy i : S  Ai that can maximize player is
discounted future reward with a discount factor  . Similar to matrix games, the players policy in
a stochastic game is probabilistic. An example is the soccer game introduced by Littman (Littman,
1994) where an agent on the offensive side must use a probabilistic policy to pass an unknown
defender. In the literature, a solution to a stochastic game can be described as Nash equilibrium
strategies in a set of associated state-specific matrix games (Bowling, 2003; Littman, 1994). In
these state-specific matrix games, we define the action-value function Qi (s, a1 , . . . , an ) as the expected reward for player i when all the players take joint action a1 , . . . , an in state s and follow the
Nash equilibrium policies thereafter. If the value of Qi (s, a1 , . . . , an ) is known for all the states,
we can find player is Nash equilibrium policy by solving the associated state-specific matrix game
(Bowling, 2003). Therefore, for each state s, we have a matrix game and we can find the Nash
equilibrium strategies in this matrix game. Then the Nash equilibrium policies for the game are the
collection of Nash equilibrium strategies in each state-specific matrix game for all the states.
2.4 Multi-Player General-Sum Stochastic Games
For a multi-player general-sum stochastic game, we want to find the Nash equilibria in the game if
we know the reward function and transition function in the game. A Nash equilibrium in a stochastic
game can be described as a tuple of n policies (1 , . . . , n ) such that for all s  S and i = 1,    , n,
Vi (s, 1 , . . . , i , . . . , n )  Vi (s, 1 , . . . , i , . . . , n ) for all i  i

(5)

where i is the set of policies available to player i and Vi (s, 1 , . . . , n ) is the expected sum of
discounted rewards for player i given the current state and all the players equilibrium policies. To
simplify notation, we use Vi (s) to represent Vi (s, 1 ,    , n ) as the state-value function under Nash
equilibrium policies. We can also define the action-value function Q (s, a1 ,    , an ) as the expected
400

fiP OLICY I NVARIANCE

UNDER

R EWARD T RANSFORMATIONS

sum of discounted rewards for player i given the current state and the current joint action of all the
players, and following the Nash equilibrium policies thereafter. Then we can get



Vi (s) =

Qi (s, a1 ,    , an )1 (s, a1 )    n (s, an ),

(6)

a1 , ,an A1 An

Qi (s, a1 , . . . , an ) =

 T (s, a1 , . . . , an , s )

s S




Ri (s, a1 , . . . , an , s ) + Vi (s ) ,

(7)

where i (s, ai )  PD(Ai ) is a probability distribution over action ai under player is Nash equilibrium policy, T (s, a1 , . . . , an , s ) = Pr {sk+1 = s |sk = s, a1 , . . . , an } is the probability of the next state
being s given the current state s and joint action (a1 , . . . , an ), and Ri (s, a1 , . . . , an , s ) is the expected
immediate reward received in state s given the current state s and joint action (a1 , . . . , an ). Based
on (6) and (7), the Nash equilibrium in (5) can be rewritten as



Qi (s, a1 , . . . , an )1 (s, a1 )    i (s, ai )    n (s, an ) 

a1 ,...,an A1 An



Qi (s, a1 , . . . , an )1 (s, a1 )    i (s, ai )    n (s, an ).

(8)

a1 ,...,an A1 An

3. Potential-Based Shaping in General-Sum Stochastic Games
Ng et al. (1999) presented a reward shaping method to deal with the credit assignment problem
by adding a potential-based shaping reward to the environment. The combination of the shaping
reward with the original reward may improve the learning performance of a reinforcement learning
algorithm and speed up the convergence to the optimal policy. The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in
an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008). In our research, we extend the
potential-based shaping method from Markov decision processes to multi-player stochastic games.
We prove that the Nash equilibria under the potential-based shaping reward transformation will be
the Nash equilibria for the original game under the framework of general-sum stochastic games.
We define a potential-based shaping reward Fi (s, s ) for player i as
Fi (s, s ) =  i (s )  i (s),

(9)

where  : S  R is a real-valued shaping function and (sT ) = 0 for any terminal state sT . We
define a multi-player stochastic game as a tuple M = (S, A1 , . . . , An , T,  , R1 , . . . , Rn ) where S is a set
of states, A1 , . . . , An are players action sets, T is the transition function,  is the discount factor, and
Ri (s, a1 , . . . , an , s )(i = 1, . . . , n) is the reward function for player i. After adding the shaping reward
function Fi (s, s ) to the reward function Ri (s, a1 , . . . , an , s ), we define a transformed multi-player
stochastic game as a tuple M  = (S, A1 , . . . , An , T,  , R1 , . . . , Rn ) where Ri (i = 1, . . . , n) is the new
reward function given by Ri (s, a1 , . . . , an , s ) = Fi (s, s ) + Ri (s, a1 , . . . , an , s ). Inspired by Ng et al.
(1999)s proof of policy invariance in an MDP, we prove the policy invariance in a multi-player
general-sum stochastic game as follows.
Theorem 1. Given an n-player discounted stochastic game M = (S, A1 , . . . , An , T,  , R1 , . . . , Rn ), we
define a transformed n-player discounted stochastic game M  = (S, A1 , . . . , An , T,  , R1 + F1 , . . . , Rn +
Fn ) where Fi  S  S is a shaping reward function for player i. We call Fi a potential-based shaping
function if Fi has the form of (9). Then, the potential-based shaping function Fi is a necessary and
sufficient condition to guarantee the Nash equilibrium policy invariance such that
401

fiL U , S CHWARTZ , & G IVIGI

 (Sufficiency) If Fi (i = 1, . . . , n) is a potential-based shaping function, then every Nash equilibrium policy in M  will also be a Nash equilibrium policy in M (and vice versa).
 (Necessity) If Fi (i = 1, . . . , n) is not a potential-based shaping function, then there may exist
a transition function T and reward function R such that the Nash equilibrium policy in M 
will not be the Nash equilibrium policy in M.
Proof. (Proof of Sufficiency)
Based on (8), a Nash equilibrium in the stochastic game M can be represented as a set of policies
such that for all i = 1, . . . , n, s  S and Mi  






QMi (s, a1 , . . . , an )M
(s, a1 )    M
(s, ai )    M
(s, an ) 
1
i
n

a1 ,...,an A1 An





(s, a1 )    Mi (s, ai )    M
(s, an ).
QMi (s, a1 , . . . , an )M
1
n

(10)

a1 ,...,an A1 An

We subtract i (s) on both sides of (10) and get



a1 ,...,an A1 An




QMi (s, a1 , . . . , an )M
(s, a1 )    M
(s, ai )    M
(s, an )  i (s) 
1
i
n





QMi (s, a1 , . . . , an )M
(s, a1 )    Mi (s, ai )    M
(s, an )  i (s).
1
n

(11)

a1 ,...,an A1 An
 (s, a )      (s, a )      (s, a ) = 1, we can get
Since a1 ,...,an A1 An M
1
i
n
Mi
Mn
1






(s, a1 )    M
(s, ai )    M
(s, an ) 
[QMi (s, a1 , . . . , an )  i (s)]M
1
i
n

a1 ,...,an A1 An





[QMi (s, a1 , . . . , an )  i (s)]M
(s, a1 )    Mi (s, ai )    M
(s, an ).
1
n

(12)

a1 ,...,an A1 An

We define
QMi (s, a1 , . . . , an ) = QMi (s, a1 , . . . , an )  i (s).

(13)

Then we can get



a1 ,...,an A1 An






QMi (s, a1 , . . . , an )M
(s, a1 )    M
(s, ai )    M
(s, an ) 
1
i
n

a1 ,...,an A1 An



(s, a1 )    Mi (s, ai )    M
(s, an ).
QMi (s, a1 , . . . , an )M
1
n

(14)

We now use some algebraic manipulations to rewrite the action-value function under the Nash equilibrium in (7) for player i in the stochastic game M as

QMi (s, a1 , . . . , an )  i (s) =  T (s, a1 , . . . , an , s ) RMi (s, a1 , . . . , an , s ) + VM i (s )
s S


+ i (s )   i (s )  i (s).

(15)

Since s S T (s, a1 , . . . , an , s ) = 1, the above equation becomes
QMi (s, a1 , . . . , an )  i (s) =

 T (s, a1 , . . . , an , s )

s S



RMi (s, a1 , . . . , an , s )


+ i (s )  i (s) + VM i (s )   i (s ) .
402

(16)

fiP OLICY I NVARIANCE

UNDER

R EWARD T RANSFORMATIONS

According to (6), we can rewrite the above equation as
QMi (s, a1 , . . . , an )  i (s) =



+

a1 ,...,an A1 An

=
+



a1 ,...,an A1 An

 T (s, a1 , . . . , an , s )

s S



RMi (s, a1 , . . . , an , s ) +  i (s )  i (s)



 

 

(s
,
a
)



(s
,
a
)

QMi (s , a1 , . . . , an )M



(s
)
i
1
M
n
1
i

 T (s, a1 , . . . , an , s )



RMi (s, a1 , . . . , an , s ) +  i (s )  i (s)

s S
	
   
   

(s , a1 )    M
(s , an ) .
QMi (s , a1 , . . . , an )  i (s ) M
1
i

(17)

Based on the definitions of Fi (s, s ) in (9) and QMi (s, a1 , . . . , an ) in (13), the above equation becomes
QMi (s, a1 , . . . , an ) =
+



a1 ,...,an A1 An

 T (s, a1 , . . . , an , s )

s S



RMi (s, a1 , . . . , an , s ) + Fi(s, s )




QMi (s , a1 , . . . , an ) M
(s , a1 )    M
(s , an ) .
1
i

(18)

Since equations (14) and (18) have the same form as equations (6)-(8), we can conclude that
QMi (s, a1 , . . . , an ) is the action-value function under the Nash equilibrium for player i in the stochastic game M  . Therefore, we can obtain
QMi (s, a1 , . . . , an ) = QM (s, a1 , . . . , an ) = QMi (s, a1 , . . . , an )  i (s).
i

(19)

If the state s is the terminal state sT , then we have QMi (sT , a1 , . . . , an ) = QMi (sT , a1 , . . . , an ) 
i (sT ) = 0  0 = 0. Based on (14) and QMi (s, a1 , . . . , an ) = QM (s, a1 , . . . , an ), we can find that
i
the Nash equilibrium in M is also the Nash equilibrium in M  . Then the state-value function under
the Nash equilibrium in the stochastic game M  can be given as
VM  (s) = VM i (s)  i (s).
i

(20)

(Proof of Necessity)
If Fi (i = 1, . . . , n) is not a potential-based shaping function, we will have Fi (s, s ) 6=  i (s )  i (s).
Similar to Ng et al. (1999)s proof of necessity, we define  = Fi (s, s )  [ i (s )  i (s)]. Then we
can build a stochastic game M by giving the following transition function T and player 1s reward
function RM1 ()
T (s1 , a11 , a2 , . . . , an , s3 ) = 1,
T (s1 , a21 , a2 , . . . , an , s2 ) = 1,
T (s2 , a1 , . . . , an , s3 ) = 1,
T (s3 , a1 , . . . , an , s3 ) = 1,

RM1 (s1 , a1 , . . . , an , s3 ) = ,
2
RM1 (s1 , a1 , . . . , an , s2 ) = 0,
RM1 (s2 , a1 , . . . , an , s3 ) = 0,
RM1 (s3 , a1 , . . . , an , s3 ) = 0,
403

(21)

fiL U , S CHWARTZ , & G IVIGI

a11
S3

S1

a12
S2
Figure 1: possible states of the stochastic model in the proof of necessity

where ai (i = 1, . . . , n) represents any possible action ai  Ai from player i, and a11 and a21 represent
player 1s action 1 and action 2 respectively. Equation T (s1 , a11 , a2 , . . . , an , s3 ) = 1 in (21) denotes
that, given the current state s1 , player 1s action a11 will lead to the next state s3 no matter what
joint action the other players take. Based on the above transition function and reward function, we
can get the game model including states (s1 , s2 , s3 ) shown in Figure 1. We now define 1 (si ) =
F1 (si , s3 )(i = 1, 2, 3). Based on (6), (7), (19), (20) and (21), we can obtain player 1s action-value
function at state s1 in M and M 

,
2
QM1 (s1 , a21 , . . . ) = 0,

QM1 (s1 , a11 , . . . ) =


QM (s1 , a11 , . . . ) = F1 (s1 , s2 ) +  F1 (s2 , s3 )  ,
1
2
QM (s1 , a21 , . . . ) = F1 (s1 , s2 ) +  F1 (s2 , s3 ).
1

Then the Nash equilibrium policy for player 1 at state s1 is


(s1 , a1 ) =
M
1

 1
a1 if  > 0,


a21


, M
 (s1 , a1 ) =
1

otherwise

 2
a1 if  > 0,


a11

.

(22)

otherwise

Therefore, in the above case, the Nash equilibrium policy for player 1 at state s1 in M is not the
Nash equilibrium policy in M  .

The above analysis shows that the potential-based shaping reward with the form of Fi (s, s ) =
 i (s )  i (s) guarantees the Nash equilibrium policy invariance. Now the question becomes
how to select a shaping function i (s) to improve the learning performance of the learner. Ng
et al. (1999) showed that i (s) = VM i (s) is a good candidate for improving the players learning
404

fiP OLICY I NVARIANCE

UNDER

R EWARD T RANSFORMATIONS

performance in an MDP. We substitute i (s) = VM i (s) into (18) and get
QMi (s, a1 , . . . , an ) = QM (s, a1 , . . . , an )
i

=
+

 T (s, a1 , . . . , an , s )

s S





RMi (s, a1 , . . . , an , s ) + Fi (s, s )



RMi (s, a1 , . . . , an , s ) + Fi (s, s )



 

 
QM (s , a1 , . . . , an ) M

(s
,
a
)



(s
,
a
)
1
M
n
1
i
i

a1 ,...,an A1 An

=

 T (s, a1 , . . . , an , s )

s S


+  (VM i (s )  i (s ))


=  T (s, a1 , . . . , an , s ) RMi (s, a1 , . . . , an , s ) + Fi (s, s ) .

(23)

s S

Equation (23) shows that the action-value function QM (s, a1 , . . . , an ) in state s can be easily obtained
i
by checking the immediate reward RMi (s, a1 , . . . , an , s ) + Fi (s, s ) that player i received in state s .
However, in practical applications, we will not have all the information of the environment such as
T (s, a1 , . . . , an , s ) and Ri (s, a1 , . . . , an , s ). This means that we cannot find a shaping function i (s)
such that i (s) = VM i (s) without knowing the model of the environment. Therefore, the goal for
designing a shaping function is to find a i (s) as a good approximation to VM i (s).

4. Conclusion
A potential-based shaping method can be used to deal with the temporal credit assignment problem
and speed up the learning process in MDPs. In this article, we extend the potential-based shaping
method to general-sum stochastic games. We prove that the proposed potential-based shaping reward applied to a general-sum stochastic game will not change the original Nash equilibrium of the
game. The analysis result in this article has the potential to improve the learning performance of the
players in a stochastic game.

References
Asmuth, J., Littman, M. L., & Zinkov, R. (2008). Potential-based shaping in model-based reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,
pp. 604609.
Babes, M., Munoz de Cote, E., & Littman, M. L. (2008). Social reward shaping in the prisoners
dilemma. In Proceedings of the 7th International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS 2008), pp. 13891392.
Basar, T., & Olsder, G. J. (1999). Dynamic Noncooperative Game Theory. SIAM Series in Classics
in Applied Mathematics 2nd, London, U.K.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models. PrenticeHall, Englewood Cliffs, NJ.
Bowling, M. (2003). Multiagent Learning in the Presence of Agents with Limitations. Ph.D. thesis,
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA.
405

fiL U , S CHWARTZ , & G IVIGI

Devlin, S., & Kudenko, D. (2011). Theoretical considerations of potential-based reward shaping for
multi-agent systems.. In Proceedings of the 10th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), Taipei, Taiwan.
Dorigo, M., & Colombetti, M. (1994). Robot shaping: developing autonomous agents through
learning. Artificial Intelligence, 71, 321370.
Gullapalli, V., & Barto, A. (1992). Shaping as a method for accelerating reinforcement learning. In
Proceedings of the 1992 IEEE International Symposium on Intelligent Control, pp. 554 559.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the 11th International Conference on Machine Learning, pp. 157163.
Mataric, M. J. (1994). Reward functions for accelerated learning. In Proceedings of the 11th
International Conference on Machine Learning.
Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: theory
and application to reward shaping. In Proceedings of the 16th International Conference on
Machine Learning, pp. 278287.
Randlv, J., & Alstrm, P. (1998). Learning to drive a bicycle using reinforcement learning and
shaping. In Proceedings of the 15th International Conference on Machine Learning.
Shapley, L. S. (1953). Stochastic games. In Proceedings of the National Academy of Sciences,
Vol. 39, pp. 10951100.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. The MIT Press,
Cambridge, Massachusetts.
Wiewiora, E. (2003). Potential-based shaping and Q-value initialization are equivalent. Journal of
Artificial Intelligence Research, 19, 205208.

406

fiJournal of Artificial Intelligence Research 41 (2011) 267296

Submitted 11/10; published 06/11

From Identical to Similar: Fusing Retrieved Lists Based
on Inter-Document Similarities
Anna Khudyak Kozorovitsky
Oren Kurland

annak@tx.technion.ac.il
kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management
Technion  Israel Institute of Technology

Abstract
Methods for fusing document lists that were retrieved in response to a query often utilize the retrieval scores and/or ranks of documents in the lists. We present a novel fusion
approach that is based on using, in addition, information induced from inter-document
similarities. Specically, our methods let similar documents from dierent lists provide
relevance-status support to each other. We use a graph-based method to model relevancestatus propagation between documents. The propagation is governed by inter-documentsimilarities and by retrieval scores of documents in the lists. Empirical evaluation demonstrates the eectiveness of our methods in fusing TREC runs. The performance of our
most eective methods transcends that of eective fusion methods that utilize only retrieval scores or ranks.

1. Introduction
The ad hoc retrieval task is to nd the documents most pertaining to an information need
underlying a given query. Naturally, there is considerable uncertainty in the retrieval process
 e.g., accurately inferring what the actual information need is. Thus, researchers proposed to utilize dierent information sources and types to address the retrieval task (Croft,
2000b). For example, utilizing multiple document representations (Katzer, McGill, Tessier,
Frakes, & Dasgupta, 1982), query representations (Saracevic & Kantor, 1988; Belkin, Cool,
Croft, & Callan, 1993), and search techniques (Croft & Thompson, 1984; Fox & Shaw,
1994), have been proposed as a means for improving retrieval eectiveness.
Many of the approaches just mentioned depend on the ability to eectively fuse several
retrieved lists so as to produce a single list of results. Fusion might be performed under a
single retrieval system (Croft & Thompson, 1984), or upon the results produced by dierent
search systems (Fox & Shaw, 1994; Callan, Lu, & Croft, 1995; Dwork, Kumar, Naor, &
Sivakumar, 2001). Conceptually, fusion can be viewed as integrating experts recommendations (Croft, 2000b), where the expert is a retrieval model used to produce a ranked list
of results  the experts recommendation.
A principle underlying many fusion methods is that the documents that are highly
ranked in many of the lists, i.e., that are highly recommended by many of the experts,
should be ranked high in the nal result list (Fox & Shaw, 1994; Lee, 1997). The eectiveness
of approaches that employ this principle often depends on the overlap1 between non-relevant
1. We use the term overlap to refer to the number of documents shared by the retrieved lists rather than
in reference to content overlap.
c
2011
AI Access Foundation. All rights reserved.

fiKhudyak Kozorovitsky & Kurland

documents in the lists being much smaller than that between relevant documents (Lee,
1997). However, several studies have shown that this is often not the case, more specically,
that on many occasions there are (many) dierent relevant documents across the lists to
be fused (Das-Gupta & Katzer, 1983; Griths, Luckhurst, & Willett, 1986; Chowdhury,
Frieder, Grossman, & McCabe, 2001; Soboro, Nicholas, & Cahan, 2001; Beitzel et al.,
2003).
We propose a novel approach to fusion of retrieved lists that addresses, among others, the
relevant-documents mismatch issue just mentioned. A principle guiding the development of
our methods is that similar documents  from dierent lists, as well as those in the same
list  can provide relevance-status support to each other, as they potentially discuss the
same topics (Shou & Sanderson, 2002; Balinski & Danilowicz, 2005; Diaz, 2005; Kurland
& Lee, 2005; Meister, Kurland, & Kalmanovich, 2010). Specically, if relevant documents
are assumed to be similar following the cluster hypothesis (van Rijsbergen, 1979), then they
can provide support to each other via inter-document similarities.
Inspired by work on re-ranking a single retrieved list using inter-document similarities
within the list (Balinski & Danilowicz, 2005; Diaz, 2005; Kurland & Lee, 2005), our approach
uses a graph-based method to model relevance-status propagation between documents in
the lists to be fused. The propagation is governed by inter-document-similarities and by
the retrieval scores of documents in the lists. Specically, documents that are highly ranked
in the lists, and are similar to other documents that are highly ranked, are rewarded. If
inter-document-similarities are not utilized  i.e., only retrieval scores are used  then
some of our methods reduce to standard fusion approaches.
Empirical evaluation shows that our methods are highly eective in fusing TREC runs
(Voorhees & Harman, 2005); that is, lists of document that were created in response to
queries by search systems that participated in TREC. Our most eective methods post
performance that is superior to that of eective standard fusion methods that utilize only
retrieval scores. We show that these ndings hold whether the runs to be fused, which are
selected from all available runs per track (challenge) in TREC, are the most eective ones,
or are randomly selected. Using an additional array of experiments we study the eect of
various factors on the performance of our approach.

2. Fusion Framework
Let q and d denote a query and a document, respectively. We assume that documents are
assigned with unique IDs; we write d1  d2 if d1 and d2 have the same ID, i.e., they are the
[q;k]
[q;k]
same document. We assume that the document lists L1 , . . . , Lm , or L1 , . . . , Lm in short,
were retrieved in response to q by m retrievals performed over a given corpus, respectively;
each list contains k documents. We write d  Li to indicate that d is a member of Li , and
def

use SLi (d) to denote the (positive) retrieval score of d in Li ; if d 6 Li then SLi (d) = 0. The
document instance Lji is the document at rank j in list Li . To simplify notation, we often
def

use S(Lji ) to denote the retrieval score of Lji (i.e., S(Lji ) = SLi (Lji )). The methods that
we present consider the similarity sim(d1 , d2 ) between documents d1 and d2 . The methods
are not committed to a specic way of computing inter-document similarities. For example,
the cosine measure between vector-space representations of documents can be used as in
268

fiFusing Retrieved Lists Based on Inter-Document Similarities

some previous work on re-ranking a single retrieved list (Diaz, 2005; Kurland & Lee, 2005).
In Section 4.1 we describe our specic choice of a language-model-based inter-document
similarity measure used for experiments following previous recommendations (Kurland &
Lee, 2010).
2.1 Fusion Essentials
Our goal is to produce a single list of results from the retrieved lists L1 , . . . , Lm . To that end,
we opt to detect those documents that are highly recommended by the lists L1 , . . . , Lm ,
or in other words, that are prestigious with respect to the lists. Given the virtue by which
the lists were created, that is, in response to the query, we hypothesize that prestige implies
relevance. The key challenge is then to formally dene and quantify prestige.
Many current fusion approaches (implicitly) regard a document as prestigious if it is
highly ranked in many of the lists. The CombSUM method (Fox & Shaw, 1994), for
example, quanties this prestige notion by summing the document retrieval scores2 :
def

PCombSU M (d) =

X

SLi (d).

Li :dLi

To emphasize even more the importance of occurrence in many lists, the CombMNZ method
(Fox & Shaw, 1994; Lee, 1997), which is a highly eective fusion approach (Montague &
Aslam, 2002), scales CombSUMs score by the number of lists a document is a member of:
def

PCombM N Z (d) = #{Li : d  Li }

X

SLi (d).

Li :dLi

A potentially helpful source of information not utilized by standard fusion methods is
inter-document relationships. For example, documents that are similar to each other can
provide support for prestige as they potentially discuss the same topics. Indeed, work on
re-ranking a single retrieved list has shown that prestige induced from inter-document similarities is connected with relevance (Kurland & Lee, 2005). In the multiple-lists setting that
we address here, information induced from inter-document similarities across lists could be
a rich source of helpful information as well. A case in point, a document that is a member
of a single list, but which is similar to other documents that are highly ranked in many of
the lists could be deemed prestigious. Furthermore, similarity-based prestige can be viewed
as a generalization of the prestige notion taken by standard fusion methods, if we consider
documents to be similar if and only if they are the same document.

2.2 Similarity-Based Fusion
We use graphs to represent propagation of prestige status between documents; the propagation is based on inter-document similarities and retrieval scores. The nodes of a graph
2. We assume that retrieval scores are normalized for inter-list compatibility; details about the normalization scheme we employ in the experiments are provided in Section 4.1.

269

fiKhudyak Kozorovitsky & Kurland

represent either documents, or document instances (appearances of documents) in the retrieved lists. In the latter case, the same document can be represented by several nodes,
each of which corresponds to its appearance in a list, while in the former case, each node
corresponds to a dierent document.
The development of the following graph-construction method and prestige-induction
technique is inspired by work on inducing prestige in a single retrieved list (Kurland & Lee,
2005). In contrast to this work, however, we would like to exploit the special characteristics
of the fusion setup. That is, the fact that documents can appear in several retrieved
lists with dierent retrieval scores that might be produced by dierent retrieval methods.
Accordingly, all fusion methods that we develop are novel to this study.
Formally, given a set of documents (document instances) V , we construct a weighted
def

(directed) complete graph G = (V, V  V, w t) with the edge-weight function w t:

w t(v1  v2 )

def

=

(

sim(v1 , v2 ) if v2  N bhd(v1 ; ),
0
otherwise;

v1 , v2  V ; and, N bhd(v; ) is the  elements v  in V  {v  : v   v} that yield the highest
sim(v, v  )  i.e., vs nearest neighbors in V ;  is a free parameter.3 Previous work has
demonstrated the merits of using directed nearest-neighbor-based graphs, as we use here,
for modeling prestige-status propagation in setups wherein prestige implies relevance to
information need (Kurland & Lee, 2005). (See Kurland, 2006 for elaborated discussion.)
As in work on inducing (i) journal prestige in bibliometrics (Pinski & Narin, 1976), (ii)
Web-page prestige in Web retrieval (Brin & Page, 1998), and (iii) plain-text prestige for reranking a single list (Kurland & Lee, 2005), we can say that a node v in G is prestigious to
the extent it receives prestige-status support from other prestigious nodes. We can quantify
def P


this prestige notion using P (v; G) =
v V w t(v  v)P (v ; G). However, this recursive
equation does not necessarily have a solution.
To address this issue, we dene a smoothed version of the edge-weight function, which
echoes PageRanks (Brin & Page, 1998) approach:
def

w t[] (v1  v2 ) =   P

d 2 , q)
w t(v1  v2 )
sim(v
+ (1  )  P
;


d
sim(v , q)

v V w t(v1  v )

(1)

v V

d q) is vs estimated query similarity. (Below we present
 is a free parameter, and sim(v,
def

various query-similarity measures.) The resultant graph is G[] = (V, V  V, w t[] ).
Note that each node in G[] receives prestige-status support to an extent partially controlled by the similarity of the document it represents to the query. Nodes that are among
the nearest-neighbors of other nodes get an additional support. Moreover, w t[] can be
thought of as a probability transition function, because the sum of weights on edges going
out from a node is 1; furthermore, every node has outgoing edges to all nodes in the graph
(self loops included). Hence, G[] represents an ergodic Markov chain for which a unique stationary distribution exists (Golub & Van Loan, 1996). This distribution, which can be found
3. Note that N bhd(v; ) contains only nodes that represent documents that are not that represented by v.

270

fiFusing Retrieved Lists Based on Inter-Document Similarities

Algorithm
SetUni
SetSum
SetMNZ
BagUni
BagSum
BagDupUni
BagDupMNZ

d q)
sim(v,

V

S
{d : d  i Li }
S
{d : d  i Li }
S
{d : d  i Li }
{Lji }i,j
{Lji }i,j
{Dup(Lji )}i,j
{Dup(Lji )}i,j

1

PCombSU M (v)
PCombM N Z (v)
1
S(v)
1
S(v)

S core(d)
P (d; G[] )
P (d; G[] )
P (d; G[] )
P
P (v; G[] )
PvV :vd
P (v; G[] )
PvV :vd
P (v; G[] )
PvV :vd
[]
vV :vd P (v; G )

Table 1: Similarity-based fusion methods; S core(d) is ds nal retrieval score.
L1
: d1
: d2
: d3

L11
L21
L31

L2
: d2
: d4
: d1

L12
L22
L32

Table 2: Example of two retrieved lists to be fused.
using, for example, the Power method (Golub & Van Loan, 1996),
P is the unique solution to
the following prestige-induction equation under the constraint v V P (v  ; G[] ) = 1:
def

P (v; G[] ) =

X

w t[] (v   v)P (v  ; G[] ).

(2)

v V

2.2.1 Methods
To derive specic fusion methods, we need to specify the graph G[] using which prestige
is induced in Equation 2. More specically, given the lists L1 , . . . , Lm , we have to dene a
set of nodes V that represent documents (or document instances); and, we have to devise
d q)) to be used by the edge-weight function w t[] from
a query-similarity estimate (sim(v,
Equation 1. The alternatives that we consider, which represent some ways of utilizing our
graph-based approach, and the resultant fusion methods, are presented in Table 1. It is
important to note that each fusion method produces a ranking of documents wherein a
document cannot have more than one instance. To facilitate the discussion of the various
methods from Table 1, we will refer to the example of fusing the two lists from Table 2, L1
and L2 , each of which contains three documents.
The rst group of methods does not consider occurrences of a document in multiple lists
when utilizing inter-document similarities. Specically, V , the set of nodes, is dened to
def

be the set-union of the retrieved lists. For the example in Table 2, V = {d1 , d2 , d3 , d4 }.
Thus, each document is represented in the graph by a single node. The prestige value of
this node serves as the nal retrieval score of the document. The SetUni method ignores
the retrieval scores of documents by using a uniform query-similarity estimate; hence, only
inter-document similarity information is utilized. The SetSum and SetMNZ methods,
on the other hand, integrate also retrieval scores by using the CombSUM and CombMNZ
prestige scores for query-similarity estimates, respectively.
271

fiKhudyak Kozorovitsky & Kurland

The SetSum and SetMNZ methods are, in fact, generalized forms of CombSUM and
CombMNZ, respectively. If we use the edge-weight function w t[1] (i.e., set  = 1 in Equation 1), that is, do not exploit inter-document-similarity information, then SetSum and
SetMNZ amount to CombSUM and CombMNZ, respectively; lower values of  result in
more emphasis put on inter-document-similarities information. Furthermore, the set-based
paradigm can be used so as to incorporate and generalize any fusion method by using the
methods retrieval score as the query-similarity estimate. Then, setting  = 1 amounts to
using only the fusion methods retrieval scores. (See Appendix A for a proof.)
In contrast to the set-based methods, the bag-based methods consider occurrences of
a document in multiple lists in utilizing inter-document similarity information. Each node
in the graph represents an instance of a document in a list. Hence, the set of nodes (V )
in the graph could be viewed as the bag-union of the retrieved lists. In the example from
def

Table 2, V = {L11 , L21 , L31 , L12 , L22 , L32 }. The nal retrieval score of a document is set to the
sum of prestige scores of the nodes that represent it  i.e., that correspond to its instances
in the lists. For example, the score of d1 would be the sum of the scores of the nodes L11
and L32 . It is also important to note that while the neighborhood set N bhd(v; ) of node
v cannot contain nodes representing the same document represented by v, it can contain
multiple instances of a dierent document. Thus, documents with many instances tend to
receive more inter-document-similarity-based prestige-status support than documents with
fewer instances.
The rst representative of the bag-based methods, BagUni, ignores retrieval scores and
considers only inter-document-similarities. Hence, BagUni diers from SetUni only by the
virtue of rewarding documents with multiple instances. In addition to exploiting interdocument similarities, the BagSum method also uses the retrieval score of a document
instance as the query-similarity estimate of the corresponding node. We note that CombSUM is a specic case of BagSum with  = 1, as was the case for SetSum. (See Appendix
A for a proof.) Furthermore, BagSum resembles SetSum in that it uses  for controlling
the balance between using retrieval scores and utilizing inter-document similarities. However, documents with many instances get more prestige-status support in BagSum than in
SetSum due to the bag-based representation of the lists.
Naturally, then, we opt to create a bag-based generalized version of the CombMNZ
method. To that end, for each document instance Lji that corresponds to document d, we
dene a new list Dup(Lji ). This list contains n copies of d, each assigned to an arbitrary
def

dierent rank between 1 and n with S(Lji ) as a retrieval score; n = #{Li : d  Li }  the
number of original lists that d belongs to. The set of nodes V is composed of all document
instances in the newly dened lists. For the example from Table 2, we get the following
newly created lists:
def

La = Dup(L11 )
L1a : L11  d1
L2a : L11  d1

def

Lb = Dup(L32 )
L1b : L32  d1
L2b : L32  d1

def

Lc = Dup(L21 )
L1c : L21  d2
L2c : L21  d2

def

Ld = Dup(L12 )
L1d : L12  d2
L2d : L12  d2

def

Le = Dup(L31 )
L1e : L31  d3

def

Lf = Dup(L22 )
L1f : L22  d4

The set of nodes, V , is {L1a , L2a , L1b , L2b , L1c , L2c , L1d , L2d , L1e , L1f }. Note, for example, that while
d1 was represented by a single node under the set-based representation, and by two nodes
under the bag-based representation, here it is represented by four nodes. More generally,
272

fiFusing Retrieved Lists Based on Inter-Document Similarities

the number of nodes by which each document is represented here is the square of the number
of appearances of the document in the lists.
The BagDupUni method, then, uses a uniform query-similarity estimate. Hence, as
SetUni and BagUni it utilizes only inter-document similarities; but, in doing so, BagDupUni
rewards to a larger extent documents with multiple instances due to the bag representation
and the duplicated instances. The BagDupMNZ method integrates also retrieval-scores
information by using the retrieval score of a document instance in a new list as the querysimilarity estimate of the corresponding node. For w t[1] (i.e.,  = 1), BagDupMNZ amounts
to CombMNZ, as was the case for SetMNZ. (See Appendix A for a proof.) Yet, BagDupMNZ
rewards to a larger extent documents with multiple instances than SetMNZ does due to the
bag representation of the lists and the duplicated document instances.

3. Related Work
Fusion methods often use the ranks of documents in the lists, or their retrieval scores, but not
the documents content (Fox & Shaw, 1994; Voorhees, Gupta, & Johnson-Laird, 1994; Lee,
1997; Vogt & Cottrell, 1999; Croft, 2000b; Dwork et al., 2001; Aslam & Montague, 2001;
Montague & Aslam, 2002; Lillis, Toolan, Collier, & Dunnion, 2006; Shokouhi, 2007). For
example, Dwork et al. (2001), as us, use Markov chains so as to nd prestigious documents
in the lists. However, the propagation of relevance status is governed only by information
regarding the ranks of documents in the lists. We show in Section 4.2 that using both
retrieval scores and inter-document similarities to guide relevance-status propagation is
more eective than using each alone. Also, we note that previous work on fusion has
demonstrated the relative merits of using retrieval scores rather than rank information (Lee,
1997). Furthermore, as stated in Section 2.2.1, our methods can incorporate and generalize
fusion methods that rely on scores/ranks by using the set-based graph representation. We
used in Section 2.2.1 the CombSUM and CombMNZ methods, which are based on retrieval
scores, as examples. CombSUM is a (non supervised) representative of a general family
of linear combination techniques (Vogt & Cottrell, 1999), and CombMNZ is considered a
highly eective approach which therefore often serves as a baseline in work on fusion (Lee,
1997; Aslam & Montague, 2001; Montague & Aslam, 2002; Lillis et al., 2006; Shokouhi,
2007). In Section 4.2 we demonstrate the performance merits of our approach with respect
to CombSUM and CombMNZ, and additional rank-based fusion methods.
There are several fusion methods that utilize document-based features, some of which
are based on the document content, e.g., snippets (summaries) of documents (Lawrence &
Giles, 1998; Craswell, Hawking, & Thistlewaite, 1999; Tsikrika & Lalmas, 2001; Beitzel,
Jensen, Frieder, Chowdhury, & Pass, 2005; Selvadurai, 2007). However, in contrast to
our methods, inter-document similarities were not used in these approaches. Thus, these
methods can potentially be incorporated in our fusion framework using the set-based graph
representation. Furthermore, we note that our methods can potentially utilize document
snippets to estimate inter-document similarities, rather than use the entire document content, if the content is not (quickly) accessible. Indeed, snippets were used for inducing
inter-document similarities so as to cluster results of Web search engines (Zamir & Etzioni,
1998).
273

fiKhudyak Kozorovitsky & Kurland

There is a large body of work on re-ranking an initially retrieved list using graphbased methods that model inter-document similarities within the list (e.g., Danilowicz &
Balinski, 2000; Balinski & Danilowicz, 2005; Diaz, 2005; Kurland & Lee, 2005, 2006; Zhang
et al., 2005; Yang, Ji, Zhou, Nie, & Xiao, 2006). As mentioned in Section 2, our fusion
methods could conceptually be viewed as a generalization of some of these approaches
(Danilowicz & Balinski, 2000; Diaz, 2005; Kurland & Lee, 2005); specically, of methods
that utilize both retrieval scores and inter-document-similarities for modeling relevancestatus propagation within the list (Danilowicz & Balinski, 2000; Diaz, 2005). A similar
relevance-status propagation method was also employed in work on sentence retrieval for
question answering (Otterbacher, Erkan, & Radev, 2005).
Similarities between document headlines were used for merging document lists that were
retrieved in response to a query from non-overlapping corpora (Shou & Sanderson, 2002).
Specically, a document is ranked by the sum of similarities between its headline and headlines of other documents. In contrast to our approach, which operates on a single corpus,
and which accordingly exploits information regarding multiple occurrences of a document
in the lists, retrieval scores were not integrated with these similarities; and, a graph-based
approach as we use here was not employed. In Section 4.2 we show that using retrieval
scores in the single-corpus-based fusion setup that we explore is highly important; specifically, integrating retrieval scores and inter-document-similarities results in much better
performance than that of using only inter-document similarities.
Similarities between documents in (potentially non-overlapping) dierent corpora were
also used to form document clusters (Xu & Croft, 1999; Crestani & Wu, 2006) so as to
(potentially) improve results browsing (Crestani & Wu, 2006) and to improve collection
selection (Xu & Croft, 1999) for search. In contrast to our approach, fusion methods
that are based on utilizing information induced from inter-document similarities were not
proposed.
There is some recent work on re-ranking a retrieved list using inter-document similarities
with a second retrieved list (Meister et al., 2010). The idea is that documents that are highly
ranked in the original list, and that are similar to documents highly ranked in the second
list, should be rewarded. However, in contrast to fusion approaches, documents that are
members of the second list, but not of the rst list, cannot appear in the nal result list.
Furthermore, in contrast to our approach, there is no recursive denition for prestige. Most
importantly, there is no apparent way of generalizing this method so as to fuse several lists,
in contrast to our approach.
Methods utilizing inter-item textual similarities  some using a variant of PageRank as
we do here  were also used, for example, for cross-lingual retrieval (Diaz, 2008), prediction of retrieval eectiveness (Diaz, 2007), and text summarization and clustering (Erkan
& Radev, 2004; Mihalcea & Tarau, 2004; Erkan, 2006). Specically, some recent work
(Krikon, Kurland, & Bendersky, 2010) has demonstrated the merits of integrating wholedocument-based inter-document similarities with inter-passage-similarities for re-ranking a
single retrieved list; especially, when using corpora containing long and/or topically heterogeneous documents. Incorporating inter-passage similarities in our methods is a future
venue we intend to explore.
274

fiFusing Retrieved Lists Based on Inter-Document Similarities

4. Evaluation
We next study the eectiveness of our similarity-based fusion approach, and the dierent
factors that aect its performance.
4.1 Experimental Setup
In what follows we describe the setup used for the evaluation.
4.1.1 Measuring Inter-Document Similarities.
We use a previously proposed language-model-based similarity estimate that was shown to
be eective in work on re-ranking a single retrieved list (Kurland & Lee, 2005, 2006, 2010).
[]
Let pd () denote the unigram, Dirichlet-smoothed, language model induced from document d, where  is the smoothing parameter (Zhai & Laerty, 2001). We set  = 1000
following previous recommendations (Zhai & Laerty, 2001). For documents d1 and d2 we
dene:
fifi



def
fifi []
[0]
sim(d1 , d2 ) = exp D pd1 () fifi pd2 () ;

D is the KL divergence. The closer the language models of d1 and d2 are, the lower the
KL divergence is, and the higher the similarity estimate is.
4.1.2 Data, Evaluation Measures, and Parameters.
We evaluate the performance of our fusion methods using TREC datasets (Voorhees &
Harman, 2005), which were also used in some previous work on fusion (e.g., Lee, 1997;
Aslam & Montague, 2001; Montague & Aslam, 2002): the ad hoc track of trec3, the web
tracks of trec9 and trec10, and the robust track of trec12. Tokenization, Porter stemming,
and stopword removal (using the INQUERY list) were applied to documents using the
Lemur toolkit4 , which was also used for computing sim(d1 , d2 ).
Retrieval methods that utilize inter-document similarities in a query context  e.g.,
for re-ranking a single retrieved list using graph-based techniques  are known to be most
eective when employed over relatively short lists (Willett, 1985; Diaz, 2005; Kurland & Lee,
2010). The reason is that such lists often contain documents that exhibit high surface-level
query similarity. Hence, the lists could be thought of as providing eective query-based
corpus context. Similar arguments were echoed in work on pseudo-feedback-based query
expansion (Xu & Croft, 1996; Lavrenko & Croft, 2001; Zhai & Laerty, 2002; Tao & Zhai,
2006). Furthermore, utilizing inter-document similarities in such short lists was shown to be
highly eective in improving precision at the very top ranks (Kurland & Lee, 2005, 2006).5
Indeed, users of Web search engines, for example, are often interested in the most highly
ranked documents (a.k.a., rst page of results). Given the considerations just mentioned,
we take the following design decisions with respect to the evaluation measures that we focus
on, the number of lists to be fused, and the number of documents in each list.
4. www.lemurproject.org
5. Improving precision at top ranks often results in improving MAP (mean average precision) by the virtue
of the way MAP is defined. We show in Section 4.2.1 that our approach improves both precision at top
ranks and MAP.

275

fiKhudyak Kozorovitsky & Kurland

As our focus is on precision at top ranks, we use precision of the top 5 and 10 documents
(p@5, p@10) as the main evaluation measures. To determine statistically-signicant performance dierences, we use the two-tailed Wilcoxon test at the 95% condence level. This
means that, on average, the result of a signicance test might be erroneous in one out of
every twenty tests. Thus, we employ Bonferroni correction over the corpora per evaluation
measure (i.e., a condence level of 98.75% is also used). Specically, in the results tables
that we present, a statistical-signicance mark corresponds to a 95% condence level; and,
the mark is boldfaced if the corresponding performance dierence is statistically signicant
after Bonferroni correction was employed (i.e., using a 98.75% condence level).
We use our methods to fuse three lists, each of which corresponds to the top-k documents
in a submitted run within a track; that is, we use the actual result lists (runs) submitted
by TRECs participants. The main focus of our evaluation, up to (and including) Section
4.2.5, is on fusing the three runs that are the most eective among all submitted runs in a
track (both automatic and manual); eectiveness is measured by MAP@k , that is, mean
average non-interpolated precision at cuto k, henceforth referred to as MAP (Voorhees
& Harman, 2005). The three runs to be fused are denoted, by descending order of MAP
performance, run1, run2, and run3, respectively. Although MAP is not an evaluation
measure we focus on  albeit, we do present MAP performance numbers in Section 4.2.1
 this practice ensures that the initial ranking of the lists to be fused is of relatively high
quality; that is, in terms of recall and relative positioning of relevant documents. Yet, the
lists to be fused could still be sub-optimal with respect to precision at top ranks. Thus, we
use for reference comparisons to our methods the Optimal Runs (opt. run in short)
per evaluation metric and track; that is, for each track, and evaluation metric m (p@5 or
p@10), we report the best (average over queries per track) m-performance obtained by any
submitted run in this track. Note that the MAP performance of run1 is the best in a track
by the virtue of the way run1 was selected. However, run1 is not necessarily the optimal
run with respect to p@5 or p@10. In addition, we compare the performance of our methods
with that of the CombSUM and CombMNZ fusion techniques; recall that these methods,
which rely solely on retrieval scores, are special cases of some of our methods. In nutshell,
we evaluate the eectiveness of our fusion approach, and that of the fusion methods that
serve as reference comparisons, in attaining high precision at top ranks with respect to that
of (i) the lists to be fused, and (ii) the best performing runs in a track with respect to
precision at top ranks.
We note that fusing the three most (MAP) eective runs in a track does not constitute
a real-life retrieval scenario as the quality of the lists to be fused is not known in practice,
but can rather potentially be predicted (Carmel & Yom-Tov, 2010). Yet, such setup is
suitable for a conservative evaluation of our methods, specically, for studying their ability
to eectively fuse lists of high quality. Nevertheless, in Section 4.2.6 we also present the
performance of our methods when fusing three runs that are randomly selected from all
runs in a track.
Experiments with setting k, the number of documents in each list to be fused, to values
in {10, 20, 30, 40, 50, 75, 100} showed that the fusion methods with which we compare our
approach, specically CombMNZ (Fox & Shaw, 1994; Lee, 1997), often attain (near) optimal
precision-at-top-ranks performance for k = 20. As it turns out, this is also the case for our
most eective fusion methods. Hence, the experiments to follow are based on using the top
276

fiFusing Retrieved Lists Based on Inter-Document Similarities

k = 20 documents from each run to be fused. In Section 4.2.4 we present the eect of k on
performance.
Our main goal in the evaluation to follow is to focus on the underlying principles of our
proposed fusion approach, and on its potential eectiveness. We would like to thoroughly
compare the dierent proposed methods of utilizing inter-document similarities, and the
factors that aect their performance, rather than engage in excessive performance optimization. With these goals in mind, we start by ameliorating the eects of free-parameter
values. We do so by setting the values of the free parameters that our methods incorporate,
and those of the reference comparisons, so as to optimize the average p@5 performance over
the entire set of queries in a track6 . Thus, we note that the p@10 performance numbers that
we present are not necessarily the optimal ones that could be attained. Yet, such an experimental setup is more realistic than that of optimizing performance for each of the evaluation
metrics separately. Then, in Sections 4.2.3 and 4.2.4 we present the eect on performance
of varying the values of free parameters of our methods. Furthermore, in Section 4.2.5 we
present the performance of our most eective methods when values of free parameters are
learned using cross-validation performed over queries. The value of the ancestry parameter
, which is incorporated by our methods, is chosen from {5, 10, 20, 30, 40, 50}. The value
of , which controls the reliance on retrieval scores versus inter-document-similarities, is
chosen from {0.1, 0.2, . . . , 1}.
For inter-list compatibility of retrieval scores, we normalize the score of a document in
a list with respect to the sum of all scores in the list; if a list is of negative retrieval scores,
which is usually due to using logs, we use the exponent of a score for normalization7 .
4.1.3 Efficiency Considerations.
The number of documents (document instances) in the graphs we construct is at most a
few hundreds8 . Hence, computing inter-document similarities does not incur a signicant
computational overhead. Even if the entire document content is not quickly accessible,
document snippets, for example, could be used for computing inter-document similarities.
(This is a future venue we intend to explore.) Similar eciency considerations were made
in work on clustering the results retrieved by Web search engines (Zamir & Etzioni, 1998),
and in work on re-ranking search results using clusters of top-retrieved documents (Willett,
1985; Liu & Croft, 2004; Kurland & Lee, 2006). In addition, we note that computing
prestige over such small graphs takes only a few iterations of the Power method (Golub &
Van Loan, 1996).
4.2 Experimental Results
We next present the performance numbers of our fusion approach. In Section 4.2.1 we
present the main result  the performance of our best-performing models with respect
to that of the reference comparisons. Then, in Section 4.2.2 we compare and analyze the
6. If two parameter settings yield the same p@5, we choose the one minimizing p@10 so as to provide
conservative estimates of performance.
7. Normalizing retrieval scores with respect to the maximum and minimum scores in a list yields almost
exactly the same performance numbers as those we report here.
8. Note that each of the three fused lists contains 20 documents, and each document instance is duplicated,
if at all, at most three times.

277

fiKhudyak Kozorovitsky & Kurland

opt. run
run1
run2
run3
CombSUM
BagSum
CombMNZ
BagDupMNZ

trec3
p@5
p@10
76.0
72.2
74.4
72.2
72.8
67.6
76.0
71.2
80.8ab
74.6b
83.2oabc 78.8om
abc
80.8ab
74.6b
83.2ab 79.0om
abc

trec9
p@5
p@10
60.0
53.1
60.0
53.1
45.8o
38.8o
38.3o
34.6o
52.9bc 48.5bc
59.6m
48.1bc
bc
55.0bc 48.8bc
60.4m
47.9bc
bc

trec10
p@5
p@10
63.2
58.8
63.2
58.8
54.4
50.2
55.6
46.8o
o
71.2abc 61.0bc
71.2oabc 61.0bc
71.2oabc 61.0bc
72.0oabc 61.0bc

trec12
p@5
p@10
54.5
48.6
51.1
44.8
52.5
48.6
51.5
45.2o
53.7
49.2ac
55.4ac
49.2ac
53.9
49.2ac
56.6m
49.0ac
abc

Table 3: Main result table. The performance of two of our most eective fusion methods,
BagSum and BagDupMNZ; for  = 1, these amount to CombSUM and CombMNZ,
respectively. The best performance in each column is boldfaced. Statisticallysignicant dierences with opt. run, run1, run2, and run3, are marked with o, a,
b, and c, respectively. (Here and after, we do not mark statistically-signicant
dierences between run1, run2 and run3 to avoid cluttering the presentation, as
these convey no additional insight.) Statistically-signicant dierences between
BagSum and CombSUM, and between BagDupMNZ and CombMNZ, are marked
with m. The values of (, ) that yield optimal average p@5 performance for
BagSum are (0.4, 5), (0.6, 40), (0, 5) and (0.1, 5) over trec3, trec9, trec10, and
trec12, respectively; for BagDupMNZ, (0.7, 5), (0.1, 20), (0.1, 5), and (0.1, 20) yield
optimal average p@5 performance for trec3, trec9, trec10, and trec12, respectively.

performance of all proposed fusion methods. We futher study the merits of using interdocument similarities in Section 4.2.3. The eect on performance of additional factors, e.g.,
the number of documents in the lists to be fused (k), is presented in Section 4.2.4. Section
4.2.5 presents the performance numbers of our most eective models when the values of free
parameters are learned using cross validation performed across queries. As noted above, up
to (and including) Section 4.2.5, the evaluation is based on fusing the three most eective
runs in a track. In Section 4.2.6 we evaluate the performance of our methods when fusing
runs that are randomly selected. In Section 4.2.7 we present an analysis of the overlap of
relevant and non-relevant documents in the lists to be fused that sheds some more light on
the reasons for the relative eectiveness of our approach with respect to that of standard
fusion.
4.2.1 Main Result
Table 3 presents our main result. We present the performance numbers of two of our most
eective methods, namely, BagSum and BagDupMNZ. (See Section 4.2.2 for an in-depth
analysis of the performance of all our fusion methods.) Recall that for  = 1  i.e.,
using no inter-document-similarity information  these methods amount to CombSUM
and CombMNZ, respectively.
Our rst observation based on Table 3 is that in most reference comparisons (track
 evaluation measure) the BagSum and BagDupMNZ methods outperform  often to a
278

fiFusing Retrieved Lists Based on Inter-Document Similarities

substantial and statistically-signicant degree  each of the three fused runs. Furthermore,
in many cases, the performance of our methods is also superior to that of opt. run. This also
holds, for example, for p@5 of BagDupMNZ for trec12, a track for which the performance
of the runs to be fused (specically, run2 and run3) can be quite below that of opt. run.
For trec9, our methods performance is in several cases below that of run1, which is also
opt. run with respect to p@5 and p@10. However, these performance dierences are not
statistically signicant. Also, note that run1 is by far more eective than run2 and run3;
hence, run2 and run3 potentially have a relatively few relevant documents to contribute
in addition to those in run1. Nevertheless, the performance of our methods for trec9 is
substantially better than that of the other two fused runs (run2 and run3); and, in terms
of p@5  the metric for which performance is optimized  the performance for trec9 of
each of BagSum and BagDupMNZ is statistically-signicantly better (for BagDupMNZ also
after employing Bonferroni correction) than that of its special case, that is, CombSUM and
CombMNZ, respectively. The fact that the p@5 performance of CombSUM and CombMNZ
is much worse than that of run1 for trec9, which is not the case for other tracks, could
be attributed to the fact that the number of relevant documents that are shared among
the three runs is the lowest observed with respect to all considered tracks. (We present an
analysis of the number of relevant documents shared by the runs in Section 4.2.7.) This is a
scenario in which our methods can yield much merit by using inter-document similarities, as
is evident in the p@5 performance improvements they post over CombSUM and CombMNZ
for trec9.
More generally, we can see in Table 3 that in a majority of the relevant comparisons
our methods performance is superior to that of their special cases that do not utilize interdocument similarities (CombSUM and CombMNZ). The p@5 improvements for trec9, for
example, and as noted above, are both substantial and statistically signicant. Furthermore,
our methods post more statistically signicant improvements over the runs to be fused, and
over opt. run, than CombSUM and CombMNZ do. Thus, these ndings attest to the merits
of utilizing inter-document similarities for fusion.
Analysis of MAP Performance. Although the focus of the evaluation we present is
on precision at top ranks, we are also interested in the general quality of the ranking
induced by our methods. Accordingly, we present the MAP performance of the BagSum and
BagDupMNZ methods in Table 4. To avoid potential metric-divergence issues (Azzopardi,
Girolami, & van Rijsbergen, 2003; Morgan, Grei, & Henderson, 2004; Metzler & Croft,
2005), that is, optimizing performance for one retrieval metric and presenting performance
numbers for a dierent retrieval metric, we optimize the performance of our methods only
in this case with respect to MAP.
We can see in Table 4 that except for trec9, our methods outperform  and in quite a few
cases, statistically signicantly so  the fused runs, and opt. run. (Recall that run1 is the
best MAP-performing run in a track; i.e., in terms of MAP, run1 is opt. run.) Moreover,
our methods consistently outperform their corresponding special cases, CombSUM and
CombMNZ.
Comparison with Rank-Based Fusion Methods While the focus of this paper is
on fusion methods that utilize retrieval scores, in Table 5 we compare the performance of
BagDupMNZ (one of our two best-performing methods) with that of two fusion methods
279

fiKhudyak Kozorovitsky & Kurland

opt. run
run1
run2
run3
CombSUM
BagSum
CombMNZ
BagDupMNZ

trec3
MAP
10.4
10.4
9.6
9.5
10.9bc
11.4om
abc
10.9bc
11.5om
abc

trec9
MAP
28.2
28.2
18.4o
16.8o
24.9bc
26.6bc
25.5bc
27.0bc

trec10
MAP
30.7
30.7
27.7o
21.6o
37.2bc
37.3bc
37.2bc
38.4bc

trec12
MAP
28.8
28.8
28.4
28.1
30.3oa
30.5oa
30.3oa
30.5oa

Table 4: MAP performance numbers. The best performance in each column is boldfaced.
Statistically signicant dierences with opt. run, run1, run2, and run3, are marked
with o, a, b, and c, respectively. Statistically signicant dierences between
BagSum and CombSUM, and between BagDupMNZ and CombMNZ, are marked
with m.

round robin
Borda
BagDupMNZ

trec3
p@5 p@10
76.4
73.2
80.4
78.6
83.2 79.0rb

trec9
p@5 p@10
50.4
45.6
55.0
48.3
60.4rb 47.9

trec10
p@5 p@10
61.6
55.2
71.2
62.0
72.0r 61.0r

trec12
p@5 p@10
53.9
47.3
54.3
48.8
56.6 49.0

Table 5: Comparison with rank-based fusion methods. Statistically-signicant dierences
between BagDupMNZ and round robin and Borda are marked with r and b,
respectively.

that utilize ranks of documents rather than their scores. The rst is a simple round robin
approach wherein the order of runs used is run1, run2 and run3. The second rank-based
fusion method is Borda (Young, 1974), in which d is scored by the number of documents
not ranked higher than it in the lists:
def X
#{d  Li : SLi (d ) <= SLi (d)}.
PBorda (d) =
Li

We can see in Table 5 that BagDupMNZ outperforms both the round robin and the
Borda methods in most reference comparisons. Many of the performance dierences (especially those with round robin) are quite substantial and some are also statistically signicant.
Upper Bound Analysis To study the potential of our approach when completely neutralizing the eects of free-parameter values, we present in Table 6 an upper bound analysis
of the p@5 performance of BagDupMNZ. To that end, for each query we use free-parameter
values of BagDupMNZ that yield optimized p@5 for this query. Recall that the performance
of BagDupMNZ reported above was based on free-parameter values set to optimize average
280

fiFusing Retrieved Lists Based on Inter-Document Similarities

OptRunPerQuery
opt. run
run1
CombMNZ
BagDupMNZ

trec3
91.6
76.0p
74.4p
80.8pa
89.6oam

trec9
79.6
60.0p
60.0p
55.0p
68.3po
am

trec10
84.4
63.2p
63.2p
71.2po
a
79.6po
am

trec12
84.8
54.5p
51.1p
53.9p
66.1po
am

Table 6: Upper bound analysis of the p@5 performance of BagDupMNZ. Specically, for
each query we use BagDupMNZ with free-parameter values optimized for p@5 for
that query. As a reference comparison, for each query we consider the best p@5performing run (OptRunPerQuery). The performance of run1 (the best (MAP)
performing among the three fused runs), opt. run (the run that yields the best
average p@5 per track), and CombMNZ is presented for reference. p, o, a,
and m mark statistically signicant dierences with OptRunPerQuery, opt. run,
run1, and CombMNZ, respectively.

opt. run
run1
run2
run3
SetUni
SetSum
SetMNZ
BagUni
BagSum
BagDupUni
BagDupMNZ

trec3
p@5
p@10
76.0
72.2
74.4
72.2
72.8
67.6
76.0
71.2
79.2b
75.0b
82.8oabc 78.0oabc
82.0ab
77.2oabc
82.4ab
78.8oabc
o
83.2abc 78.8oabc
82.0ab
78.6oabc
83.2ab 79.0oabc

trec9
p@5
p@10
60.0
53.1
60.0
53.1
45.8o
38.8o
38.3o
34.6o
o
42.5a
39.2oa
59.2bc 49.2bc
61.3bc 49.2bc
59.2bc 47.9bc
59.6bc 48.1bc
57.5bc 48.1bc
60.4bc 47.9bc

trec10
p@5
p@10
63.2
58.8
63.2
58.8
54.4
50.2
55.6
46.8o
56.8
48.2oa
o
71.2abc 61.0bc
71.2oabc 61.0bc
70.8bc
61.2bc
71.2oabc 61.0bc
72.0oabc 60.4bc
72.0oabc 61.0bc

trec12
p@5
p@10
54.5
48.6
51.1
44.8
52.5
48.6
51.5
45.2o
o
47.3
41.5ob
55.4a
48.5ac
55.6ac
48.5ac
53.1
46.5
55.4ac
49.2ac
52.9
47.8
56.6abc 49.0ac

Table 7: Performance comparison of all proposed fusion methods. The best result in a
column is boldfaced. Statistically signicant dierences with opt. run, run1, run2,
and run3, are marked with o, a, b, and c, respectively.

p@5 performance for a track. (The same runs used above are fused here). For reference
comparison, we consider for each query the run in the track that yields the best p@5 for
that query (denoted OptRunPerQuery). We also present the performance of the opt. run
baseline, used above, which is the run that yields the best average p@5 performance per
track. The performance of run1 (the most (MAP) eective of the three fused runs) and
CombMNZ is presented for reference as well.
As we can see in Table 6 the performance of BagDupMNZ is substantially (and statistically signicantly) better than that of opt. run, run1 and CombMNZ with the performance
dierences being, naturally, much higher than those in Table 3. Thus, we see that using
inter-document similarities for fusion can yield substantial merits; and, that optimizing the
281

fiKhudyak Kozorovitsky & Kurland

free-parameter values of our approach per query yields better performance than using the
same values for all queries, as could be expected. As already noted, in Section 4.2.5 we
study the performance of our approach when using cross-validation to set free-parameter
values.
We can also see in Table 6 that except for trec3, the performance of BagDupMNZ is
much inferior (and statistically signicantly so) to that of selecting the best-performing
run per each query (OptRunPerQuery). This is not a surprise as the performance of run1
(the most eective  on average  among the runs to be fused) is also substantially (and
statistically signicantly) worse than that of OptRunPerQuery; the same observation holds
for opt. run, which shows that dierent runs are the most eective for dierent queries.
4.2.2 Performance Analysis of All Proposed Fusion Methods
In Table 7 we compare the performance of all proposed fusion methods. Our rst observation is that using the retrieval scores of documents in the lists, on top of inter-documentsimilarity information, is important. Indeed, the methods with the sux Uni that use
a uniform query-similarity estimate, i.e., that disregard the retrieval scores of documents
in the lists, post performance that is almost always worse than that of their counterparts
that do utilize retrieval scores for inducing query similarity. (Compare SetUni with SetSum
and SetMNZ; BagUni with BagSum; and, BagDupUni with BagDupMNZ.) Furthermore,
utilizing retrieval scores results in performance that is almost always better  and in many
cases to a statistically signicant degree  than that of run2 and run3; the performance
also transcends that of run1 and opt. run, except for trec9.
We can also see in Table 7 that the bag representation of the lists yields better performance, in general, than that of using a set representation. (Compare, for example, BagUni
with SetUni, and BagSum with SetSum.) Recall that under a bag representation a document is represented by the nodes corresponding to its instances in lists, while under a set
representation each document is represented by a single node. Hence, the fact that documents with occurrences in many of the fused lists can draw more prestige-status support
via inter-document-similarities than documents with fewer occurrences has positive impact
on performance.
Thus, it is not a surprise that the BagSum and BagDupMNZ methods that use a bagrepresentation of the lists, and which utilize the retrieval scores of documents in the lists,
are among the most eective fusion methods that we proposed.
4.2.3 The Performance Impact of Using Inter-Document-Similarities
The  parameter in Equation 1 (Section 2) controls the reliance on retrieval scores versus inter-document similarity information. Setting  = 1, i.e., using no inter-documentsimilarity information, results in the XSum methods being equivalent to CombSUM, and
the XMNZ methods being equivalent to CombMNZ. In Table 3 we showed that BagSum
outperforms CombSUM and that BagDupMNZ outperforms CombMNZ. We now turn to
study the performance of all XSum and XMNZ methods with respect to their special
cases, that is, CombSUM and CombMNZ, respectively.
We can see in Table 8 that in a majority of the relevant comparisons (track  evaluation
metric), each of our methods outperforms its special case, with several of the dierences
282

fiFusing Retrieved Lists Based on Inter-Document Similarities

CombSUM
SetSum
BagSum

trec3
p@5 p@10
80.8
74.6
82.8 78.0m
83.2 78.8m

trec9
p@5
p@10
52.9
48.5
m
59.2
49.2
m
59.6
48.1

trec10
p@5 p@10
71.2 61.0
71.2 61.0
71.2 61.0

CombMNZ
SetMNZ
BagDupMNZ

80.8
82.0
83.2

55.0
61.3m
60.4m

71.2
71.2
72.0

74.6
77.2
79.0m

48.8
49.2
47.9

61.0
61.0
61.0

trec12
p@5
p@10
53.7
49.2
55.4
48.5
55.4
49.2
53.9
55.6
56.6m

49.2
48.5
49.0

Table 8: Comparison of the similarity-based fusion methods with their special cases, CombSUM and CombMNZ. Best performance in a column is boldfaced. Statistically
signicant dierence between a method and its special case is marked with m.

being statistically signicant. We therefore conclude that inter-document-similarities are
indeed a helpful source of information for fusion.
We further study the eect of varying the value of  on the p@5 performance of one of
our two most eective methods, BagDupMNZ, in Figure 1. Our rst observation is that
except for trec9, and for all values of , BagDupMNZ yields performance that transcends
that of run1, which is the most eective among the three fused runs; for most values of  the
performance of BagDupMNZ is also better than that of opt. run. Trec9 is an exception in
that BagDupMNZ outperforms run1, which is also opt. run, for a single value of . Recall
that for trec9 the performance of run1 is by far better than that of the other fused runs.
Another observation that we make based on Figure 1 is that for most tracks   0.6
yields better performance than that attained by using lower values of . This nding further
demonstrates the importance of utilizing retrieval scores of documents as specied above.
For  = 1 no inter-document-similarities are used and BagDupMNZ amounts to CombMNZ.
We can also see that in many cases wherein   {0.7, 0.8, 0.9} BagDupMNZ outperforms
CombMNZ; for trec9 and trec12 these improvements are quite substantial. These ndings
echo those specied above with regard to the merits of utilizing inter-document-similarities
for fusion. Finally, we note that the performance merits attained by using inter-document
similarities are even more emphasized when the runs to be fused are randomly selected from
all those available for a track (as we will show in Section 4.2.6), rather than being the best
(MAP) performing ones as those used here.
4.2.4 Further Analysis
Effect of . The similarity-based fusion methods incorporate two free parameters: ,
which controls the reliance on retrieval scores versus inter-document-similarities; the eect
of  was studied above; and, , which is the number of nearest neighbors considered for a
node in the graphs we use. In Figure 2 we analyze the eect of  on the p@5 performance
of BagDupMNZ.
We can see in Figure 2 that small values of  ( {5, 10, 20}) often yield better performance than larger values. The same nding was reported in work on utilizing nearest283

fiKhudyak Kozorovitsky & Kurland

trec3

trec9

84

61
60

82

59
58

78

p@5

p@5

80

76

57
56
55

74

54

opt. run
run1
CombMNZ
BagDupMNZ

72
70
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

opt. run
run1
CombMNZ
BagDupMNZ

53
52
0.9

1

0.1

0.2

0.3

0.4

0.5

0.6





trec10

trec12

0.7

0.8

0.9

1

0.9

1

74
56

72
70

54
p@5

p@5

68
66

52

64
50

62

opt. run
run1
CombMNZ
BagDupMNZ

60
58
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

opt. run
run1
CombMNZ
BagDupMNZ

48
0.9

1



0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8



Figure 1: Eect of varying the value of  (refer to Equation 1 in Section 2) on the p@5
performance of BagDupMNZ;  = 1 amounts to CombMNZ. The performance
of opt. run, run1 and CombMNZ is depicted with horizontal lines for reference.
Note: gures are not to the same scale.

neighbors-based graphs for re-ranking a single retrieved list (Diaz, 2005; Kurland & Lee,
2005). Furthermore, we can see that small values of  yield performance that transcends
that of run1 and opt. run, except for trec9. Another observation that we make based
on Figure 2 is that for most corpora, and most values of , BagDupMNZ outperforms its
special case, CombMNZ.
Effect of k. The experimental design that was used insofar, and which was presented in
Section 4.1, was based on the observation that attaining high precision at top ranks calls for
fusion of relatively short retrieved lists. Indeed, the performance numbers presented above
demonstrated the eectiveness of fusing lists of 20 documents each. In Figure 3 we present
the eect of k (the number of documents in each retrieved list) on the p@5 performance of
BagDupMNZ and CombMNZ.
284

fiFusing Retrieved Lists Based on Inter-Document Similarities

trec3

trec9

84

61
60

82

59
58

78

p@5

p@5

80

76

57
56
55

74

54

opt. run
run1
CombMNZ
BagDupMNZ

72
70
5 10

20

30

40

50

75

opt. run
run1
CombMNZ
BagDupMNZ

53
52
100

5 10

20

30

40

50





trec10

trec12

75

100

74
56

72
70

54
p@5

p@5

68
66

52

64
50

62

opt. run
run1
CombMNZ
BagDupMNZ

60
58
5 10

20

30

40

50

75

opt. run
run1
CombMNZ
BagDupMNZ

48
100



5 10

20

30

40

50

75

100



Figure 2: Eect of  on p@5 performance. The performance of opt. run, run1 and
CombMNZ is depicted with horizontal lines for reference. Note: gures are not
to the same scale

We can see in Figure 3 that for almost all values of k the performance of BagDupMNZ
transcends that of CombMNZ. This nding also holds with respect to opt. run, except
for the trec9 case. These ndings further attest to the merits of utilizing inter-document
similarities for fusion. Furthermore, small values of k, specically k = 20 which was used
heretofore, often yield (near) optimal performance for both BagDupMNZ and CombMNZ.
Thus, we indeed see that fusing short lists, specically, when utilizing inter-document similarities, often leads to very eective precision-at-top-ranks performance.
4.2.5 Learning Free-Parameter Values
The performance numbers presented insofar were based on free-parameter values that yield
optimal average p@5 performance with respect to the set of queries for track. This experimental setup enabled us to study the potential performance of our approach, and to
carefully analyze the dierent factors that aect it.
285

fiKhudyak Kozorovitsky & Kurland

trec3

trec9

84

62

82

60
58

80
p@5

p@5

56
78
76

54
52
50

74
opt. run
run1
CombMNZ
BagDupMNZ

72
70
10

20

30

40

50

75

opt. run
run1
CombMNZ
BagDupMNZ

48
46
100

10

20

30

40

50

75

k

k

trec10

trec12

100

74
56

72
70

54
p@5

p@5

68
66

52

64
50

62

opt. run
run1
CombMNZ
BagDupMNZ

60
58
10

20

30

40

50

75

opt. run
run1
CombMNZ
BagDupMNZ

48
100

k

10

20

30

40

50

75

100

k

Figure 3: Eect of varying k, the number of documents in each fused list (run), on the p@5
performance of BagDupMNZ. The performance of opt. run, run1, and CombMNZ
is depicted for reference. Note: gures are not to the same scale.

Now, we turn to explore the question of whether eective values of the free parameters
of our methods,  and , generalize across queries; that is, whether such values can be
learned9 . To that end, we employ a leave-one-out cross validation procedure wherein the
free parameters of a method are set for each query to values that optimize average p@5
performance over all other queries in a track. The resultant performance numbers of our
best performing methods, BagSum and BagDupMNZ, are presented in Table 9.
We can see in Table 9 that both BagSum and BagDupMNZ post better performance, in
a vast majority of the relevant comparisons (track  evaluation measure), than that of opt.
run, and that of the three runs that are fused; many of these performance improvements
are statistically signicant.
9. Note that such analysis is different than that of studying the effect of free-parameter values on the
average performance that was presented above.

286

fiFusing Retrieved Lists Based on Inter-Document Similarities

opt. run
run1
run2
run3
CombSUM
BagSum
CombMNZ
BagDupMNZ

trec3
p@5
p@10
76.0
72.2
74.4
72.2
72.8
67.6
76.0
71.2
80.8ab
74.6b
83.2oabc 78.8om
abc
80.8ab
74.6b
82.4ab
79.0om
abc

trec9
p@5
p@10
60.0
53.1
60.0
53.1
45.8o
38.8o
38.3o
34.6o
52.9bc 48.5bc
57.9bc 47.3bc
55.0bc 48.8bc
60.4m
47.9bc
bc

trec10
p@5
p@10
63.2
58.8
63.2
58.8
54.4
50.2
55.6
46.8o
o
71.2abc 61.0bc
67.6m
61.4bc
bc
o
71.2abc 61.0bc
70.8bc
60.4bc

trec12
p@5
p@10
54.5
48.6
51.1
44.8
52.5
48.6
51.5
45.2o
53.7
49.2ac
54.7a
49.2ac
53.9
49.2ac
56.6m
49.0ac
abc

Table 9: Performance numbers when employing leave-one-out cross validation to set freeparameter values. The best performance in each column is boldfaced. Statisticallysignicant dierences with opt. run, run1, run2, and run3, are marked with o,
a, b, and c, respectively. Statistically signicant dierences between BagSum
and CombSUM, and between BagDupMNZ and CombMNZ, are marked with m.

We next compare our methods with their special cases that do not utilize inter-document
similarities. That is, we compare BagSum with CombSUM and BagDupMNZ with
CombMNZ. With respect to p@5  the metric for which performance was optimized in
the learning phase  our methods outperform their special cases for all tracks, except for
that of trec10; some of these improvements are also statistically signicant (e.g., refer to
BagDupMNZ versus CombMNZ on trec9 and trec12). Furthermore, we note that most
cases in which CombSUM and CombMNZ outperform our methods are for p@10. We
attribute this nding to the metric divergence issue (Azzopardi et al., 2003; Morgan et al.,
2004; Metzler & Croft, 2005)  optimizing performance in the learning phase with respect
to one metric (p@5 in our case), and testing the performance with respect to another
metric (p@10 in our case), albeit somewhat connected. Recall that while our methods
incorporate two free parameters, the CombSUM and CombMNZ methods do not incorporate
free parameters. Additional examination of Table 9 reveals that our methods post more
statistically signicant improvements over the runs to be fused and opt. run than CombSUM
and CombMNZ do.
All in all, these results demonstrate the eectiveness of our methods when employing
cross validation so as to set free-parameter values.
4.2.6 Fusing Randomly Selected Runs
Heretofore, the evaluation of our approach was based on fusing the most (MAP) eective
runs in a track. We now turn to study the eectiveness of our best performing fusion
methods when fusing randomly selected runs.
We select 20 random triplets of runs from each track. The best performing run among
the three is denoted run1, the second best is denoted run2, and the worst among the three
is denoted run3. We then fuse the three runs using either the standard fusion methods,
CombSUM and CombMNZ, or our methods that generalize these, namely, BagSum and
287

fiKhudyak Kozorovitsky & Kurland

run1
run2
run3
CombSUM
BagSum
CombMNZ
BagDupMNZ

trec3
p@5
p@10
68.9
57.4
57.4
55.4
42.3
41.4
65.6bc 61.3abc
76.1m
70.4m
bc
abc
65.7bc 61.3abc
75.7m
70.1m
bc
bc

trec9
p@5
p@10
22.1
19.6
16.2
14.7
10.9
10.2
19.6abc 17.6abc
22.0m
18.2m
abc
abc
20.0abc 17.5abc
21.3m
18.0m
abc
abc

trec10
p@5
p@10
32.7
28.5
28.5
24.9
18.3
16.0
32.4bc
28.3bc
36.6m
30.5m
abc
abc
33.6bc
28.7bc
36.5m
30.2m
abc
abc

trec12
p@5
p@10
46.0
39.9
39.9
34.4
27.4
23.2
44.4abc 37.7abc
47.8bc 40.6m
bc
44.4abc 37.7abc
46.6bc
40.4bc

Table 10: Fusing randomly selected runs. The performance numbers represent averages
over 20 random samples of triplets of runs. The best performance in a column
is boldfaced. Statistically signicant dierences of a fusion method with run1,
run2, and run3, are marked with a, b, and c, respectively. Statistically signicant dierences between BagSum and CombSUM, and between BagDupMNZ
and CombMNZ, are marked with m.

BagDupMNZ, respectively. The performance numbers presented in Table 10 represent
averages over the 20 samples.10 The free parameters of BagSum and BagDupMNZ were
set to values optimizing average p@5 performance over queries for a track per each triplet
of runs. (The optimization procedure described in Section 4.1 was used.) Statistically
signicant dierences between two methods are determined based on the average (over 20
samples) performance per each query.
Our rst observation based on Table 10 is that our methods are highly eective in fusing
randomly selected runs. In almost all reference comparisons (track  evaluation measure),
they outperform each of the three fused runs; most of these improvements are substantial
and statistically signicant. The only exception is for run1 for trec9, which outperforms all
fusion methods.
We can also see in Table 10 that BagSum is slightly more eective than BagDupMNZ.
However, when fusing the best-performing runs in a track, as was the case above, the
picture was somewhat reversed. We attribute this nding to the relatively low overlap of
relevant documents in the randomly selected runs. Specically, we show below that this
overlap is much smaller than that for the best-performing runs. Thus, the use of information
regarding multiple appearances in lists, which is quite emphasized by BagDupMNZ, is not
of signicant merit. Note that this also holds for the standard fusion methods. That is, the
superiority of CombMNZ to CombSUM  the former emphasizes appearances in multiple
lists more than the latter  is less substantial than that for fusing the best performing
runs.
Perhaps the most important observation that we can make based on Table 10 is that
our methods are always more eective than the standard fusion approaches, which constitute their special cases; that is, compare BagSum with CombSUM and BagDupMNZ
10. We note that the drop in performance when moving from run1 to run2 and run3 is the highest for trec9.
This is because many runs in trec9 are of very low quality and contain very few relevant documents if
any (Meister et al., 2010).

288

fiFusing Retrieved Lists Based on Inter-Document Similarities

trec3

Best runs
Random runs

1
59.2
66.9

Rel
2
24.6
25.3

1
56.9
66.6

Rel
2
26.6
22.8

3
16.2
7.7

trec9

Non-Rel
1
2
3
81.1 14.5 4.3
84.9 12.9 2.2

1
61.4
78.6

Rel
2
25.3
24.4

1
32.6
48.5

Rel
2
24.6
27.8

trec10

Best runs
Random runs

3
16.5
10.6

3
13.3
6.3

1
79.4
78.6

Non-Rel
2
3
14.0 6.6
17.8 3.6

trec12

Non-Rel
1
2
3
77.4 16.0 6.6
79.6 14.9 5.5

3
42.8
23.6

1
51.9
68.0

Non-Rel
2
3
23.3 24.8
20.0 12.4

Table 11: The percentage of (non-) relevant documents (of those that appear in at least
one of the three runs to be fused) that appear in one (1), two (2) , or all three (3)
runs. The number of documents, k, considered for each run is 20. The three runs
are either the best (MAP) performing in the track, or randomly selected; in the
latter case, percentages represent averages over 20 random samples. Percentages
may not sum to 100 due to rounding.

with CombMNZ. Many of the performance dierences are also statistically signicant. Furthermore, in most relevant comparisons, CombSUM and CombMNZ are outperformed by
run1  the best performing run among the three fused  while the reverse holds for our
methods. Thus, these results support the merits of utilizing inter-document similarities for
fusion.
4.2.7 Analysis of (non-) Relevant Documents Overlap in the Lists
The results just presented show that using inter-document-similarities is highly eective
when fusing randomly selected runs. In fact, the relative performance improvements over
the standard fusion methods that do not utilize inter-document similarities are larger than
those observed above when fusing the best-performing runs. Furthermore, the ndings
presented above attested to the relative limited merit of heavily emphasizing information
regarding multiple appearances of documents in the randomly selected runs with respect
to the case with the best-performing runs. Hence, we turn to analyze the relevant and
non-relevant document overlap between runs when using randomly-selected runs and when
using the best-performing runs.
In Table 11 we present the percentage of (non-) relevant documents, of those appearing
in at least one of the three runs to be fused, that appear in one, two, or all three runs. We
use the top-20 documents from each run as above. We present percentages for the three
best-performing runs in a track and for three randomly selected runs; in the latter case, we
report averages over 20 samples of triplets of runs. In all cases, percentages are averages
over all queries in a track.
Our rst observation based on Table 11 is that for most tracks, a majority of the relevant
documents appears in only one of the three runs. This nding supports the motivation
for our approach; that is, using inter-document similarities so as to transfer relevancestatus support between dierent (similar) relevant documents across the lists. We can also
289

fiKhudyak Kozorovitsky & Kurland

see that the same nding holds for non-relevant documents  a majority of non-relevant
documents appears in only one of the three fused runs. We note that previous reports,
supporting to a certain extent the cluster hypothesis, have already shown that a majority
of the nearest neighbors of a relevant document in the similarity space tend to be relevant;
while, those of a non-relevant document tend to be both relevant and non-relevant (Kurland,
2006). This study was performed upon documents retrieved in response to a query as is the
case here. Hence, while relevant documents tend to maintain most prestige-status support
within the set of relevant documents, non-relevant documents tend to spread this support
among relevant and non-relevant documents. Furthermore, the percentage of non-relevant
documents that appears in exactly one run is larger than that for relevant documents.
This nding echoes those used to explain the eectiveness of standard fusion methods that
emphasize appearance in many lists  i.e., that the overlap between relevant documents in
the lists is higher than that for non-relevant documents (Lee, 1997).
We can also see in Table 11 that the percentage of relevant documents that appear in
only one run is much larger when using randomly selected runs than when using the bestperforming runs. In other words, the relevant-document overlap across lists in the bestperforming-runs case is higher than that in the randomly-selected runs case. This nding
helps to explain the observations made above: (i) the relative performance gains posted
by our methods with respect to the standard fusion approaches, which do not utilize interdocument similarities, are larger for randomly selected runs than for the best performing
runs, and (ii) heavily emphasizing document appearances in multiple runs is not as eective
in the random-runs case as it is in the best-runs case.
To further explore the ndings just stated, we present in Figure 4 the percentage of
relevant documents that appear in only one of the three runs as a function of the number of
documents (k) in each run. It is evident in Figure 4 that for the best-performing runs case
the percentages are lower than for the randomly-selected runs case, for most values of k.
This nding further supports the conclusion above with regard to the relative eectiveness
of our approach with the best-performing runs versus randomly-selected runs. Furthermore,
for most tracks, and for most values of k, at least 40% of the relevant documents in the
runs to be fused appear in only one of the three runs. This nding further demonstrates
the mismatch between relevant-document sets in the runs  a scenario motivating the
development of our fusion approach.

5. Conclusion and Future Work
We presented a novel approach to fusing document lists that were retrieved in response to
a query. Our approach lets similar documents across (and within) lists provide relevance
status support to each other. We use a graph-based method to model the propagation of
relevance status between documents in the lists. The propagation is governed by interdocument-similarities and by the retrieval scores of documents in the lists.
Empirical evaluation demonstrated the eectiveness of our approach. We showed that
our methods are highly eective in fusing TREC runs. This nding holds whether the runs
are the most eective per TRECs track (challenge), or randomly selected from the track.
We also showed that the performance of our methods transcends that of eective standard
fusion methods that utilize only retrieval scores or ranks of documents.
290

fiFusing Retrieved Lists Based on Inter-Document Similarities

trec3

trec9
100
% of rel docs that appear in a single run

% of rel docs that appear in a single run

100

80

60

40

20
best runs
random runs

0
10

20

30

40

50

75

80

60

40

20
best runs
random runs

0
100

10

20

30

40

50

k
trec10

100

trec12
100
% of rel docs that appear in a single run

100
% of rel docs that appear in a single run

75
k

80

60

40

20
best runs
random runs

0
10

20

30

40

50

75

80

60

40

20
best runs
random runs

0
100

k

10

20

30

40

50

75

100

k

Figure 4: The percentage of relevant documents (of those that appear in at least one of the
three runs to be fused) that appear in only one of the runs as a function of the
number of documents in a run (k). The runs are either the best-performing in a
track, or randomly selected; in the latter case, numbers represent averages over
20 random samples.

One family of our proposed methods, namely, the set-based family, can incorporate
any fusion method that relies on retrieval scores/ranks. More specically, we showed that
if inter-document-similarities are not utilized, then these set-based methods reduce to the
standard fusion method that they incorporate. We have used the CombSUM and CombMNZ
fusion methods as examples for instantiating set-based fusion approaches. Naturally then,
utilizing additional fusion methods that rely on retrieval scores/ranks is a future venue we
intend to explore.
Another venue we intend to explore is the eect of our approach on the diversity of
results in the nal result list (Carbonell & Goldstein, 1998); and, exploring ways to adapt
our methods so as to improve aspect coverage in the result list (Zhai, Cohen, & Laerty,
2003).
291

fiKhudyak Kozorovitsky & Kurland

Acknowledgments
We thank the reviewers for their helpful comments. We also thank Malka Gorne for her
helpful comments. This paper is based upon work supported in part by the Israel Science
Foundation under grant no. 557/09, and by IBMs SUR award. Any opinions, ndings
and conclusions or recommendations expressed in this material are the authors and do not
necessarily reect those of the sponsoring institutions.

Appendix A
Proposition 1. Let f be some fusion method that is based on retrieval scores/ranks, e.g.,
CombSUM or CombMNZ; f (d) is the score assigned by f to document d that appears in
at least one of the lists to be fused. Suppose we use f (d) as the query-similarity estimate
d q) def
of d in the set-based group of methods, that is, sim(d,
= f (d). Then, using w t[1] (i.e.,
setting  = 1 in Equation 1) results in the final retrieval score of d in Table 1 (S core(d))
being rank-equivalent to f (d).
Proof. Each node v in the graph corresponds to a different document d, and has |V | incoming
d
d q) is f (d). Hence, this weight is the unique
; sim(d,
edges the weight of each is P sim(d,q)
v  V

d  ,q)
sim(v

solution to Equation 2, which serves as ds nal retrieval score, and is rank-equivalent to
f (d).
Proposition 2. Using w t[1] in the BagSum algorithm amounts to the CombSUM algorithm.
Proof. A node v in the graph corresponds to a document-instance Lji of some document d;
and, has |V | incoming edges, the weight of each is

S(Lji )
d  .
v  V sim(v ,q)

P

This weight is, therefore,

the prestige score P (Lji ; G[] ) of v as computed in Equation 2 . By denition, the nal
P
retrieval score of d is i,j:Lj d P (Lji ; G[] ). This score is (following the denitions and the
i
P
S (d)
above) Li :dLi P Lid  , which is rank-equivalent to PCombSU M (d).
v  V

sim(v ,q)

Proposition 3. Using w t[1] in the BagDupMNZ algorithm amounts to the CombMNZ algorithm.

Proof. Let Lji be a document-instance of document d. Following the denitions and Proposition 2, the prestige value of a single copy of Lji from the newly dened list is

P

SLi (d)
d  ,q) .
sim(v

v  V

There are n = #{Li : d  Li } such copies in the new list. Therefore, by denition, the nal
P
S (d)
retrieval score of d is n Li :dLi P Lid  , which is rank equivalent to PCombM N Z (d).
v  V

sim(v ,q)

References
Aslam, J. A., & Montague, M. (2001). Models for metasearch. In Proceedings of SIGIR,
pp. 276284.
Azzopardi, L., Girolami, M., & van Rijsbergen, K. (2003). Investigating the relationship
between language model preplexity and IR precision-recall measures. In Proceedings
of SIGIR, pp. 369370. Poster.
292

fiFusing Retrieved Lists Based on Inter-Document Similarities

Balinski, J., & Danilowicz, C. (2005). Re-ranking method based on inter-document distances. Information Processing and Management, 41 (4), 759775.
Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., Grossman, D. A., & Goharian,
N. (2003). Disproving the fusion hypothesis: An analysis of data fusion via eective
information retrieval strategies. In Proceedings of SAC, pp. 823827.
Beitzel, S. M., Jensen, E. C., Frieder, O., Chowdhury, A., & Pass, G. (2005). Surrogate
scoring for improved metasearch precision. In Proceedings of SIGIR, pp. 583584.
Belkin, N. J., Cool, C., Croft, W. B., & Callan, J. P. (1993). The eect of multiple query
representations on information retrieval system performance. In Proceedings of SIGIR,
pp. 339346.
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine.
In Proceedings of the 7th International World Wide Web Conference, pp. 107117.
Callan, J. P., Lu, Z., & Croft, W. B. (1995). Searching distributed collections with inference
networks. In SIGIR, pp. 2128.
Carbonell, J. G., & Goldstein, J. (1998). The use of MMR, diversity-based reranking for
reordering documents and producing summaries. In Proceedings of SIGIR, pp. 335
336.
Carmel, D., & Yom-Tov, E. (2010). Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan &
Claypool.
Chowdhury, A., Frieder, O., Grossman, D. A., & McCabe, M. C. (2001). Analyses of
multiple-evidence combinations for retrieval strategies. In Proceedings of SIGIR, pp.
394395. poster.
Craswell, N., Hawking, D., & Thistlewaite, P. B. (1999). Merging results from isolated
search engines. In Proceedings of the Australian Database Conference, pp. 189200.
Crestani, F., & Wu, S. (2006). Testing the cluster hypothesis in distributed information
retrieval. Information Processing and Management, 42 (5), 11371150.
Croft, W. B. (Ed.). (2000a). Advances in Information Retrieval: Recent Research from the
Center for Intelligent Information Retrieval. No. 7 in The Kluwer International Series
on Information Retrieval. Kluwer.
Croft, W. B. (2000b). Combining approaches to information retrieval. In Croft (Croft,
2000a), chap. 1, pp. 136.
Croft, W. B., & Thompson, R. H. (1984). I3 R: A new approach to the design of document retrieval systems. Journal of the American Society for Information Science and
Technology, 38 (6), 389404.
Danilowicz, C., & Balinski, J. (2000). Document ranking based upon Markov chains. Information Processing and Management, 41 (4), 759775.
Das-Gupta, P., & Katzer, J. (1983). A study of the overlap among document representations.
In Proceedgins of SIGIR, pp. 106114.
293

fiKhudyak Kozorovitsky & Kurland

Diaz, F. (2005). Regularizing ad hoc retrieval scores. In Proceedings of the Fourteenth
International Conference on Information and Knowledge Management (CIKM), pp.
672679.
Diaz, F. (2007). Performance prediction using spatial autocorrelation. In Proceedings of
SIGIR, pp. 583590.
Diaz, F. (2008). A method for transferring retrieval scores between collections with non
overlapping vocabularies. In Proceedings of SIGIR, pp. 805806. poster.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods
for the Web. In Proceedings of the World Wide Web Conference, pp. 613622, Hong
Kong.
Erkan, G. (2006). Language model based document clustering using random walks. In
Proceedings of HLT/NAACL.
Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based lexical centrality as salience in
text summarization. Journal of Artificial Intelligence Research, 22, 457479.
Fox, E. A., & Shaw, J. A. (1994). Combination of multiple searches. In Proceedings of
TREC-2.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). The Johns
Hopkins University Press.
Griths, A., Luckhurst, H. C., & Willett, P. (1986). Using interdocument similarity information in document retrieval systems. Journal of the American Society for Information
Science (JASIS), 37 (1), 311. Reprinted in Karen Sparck Jones and Peter Willett,
eds., Readings in Information Retrieval, Morgan Kaufmann, pp. 365373, 1997.
Katzer, J., McGill, M., Tessier, J., Frakes, W., & Dasgupta, P. (1982). A study of the
overlap among document representations. Information Technology: Research and Development, 1 (2), 261274.
Krikon, E., Kurland, O., & Bendersky, M. (2010). Utilizing inter-passage and interdocument similarities for re-ranking search results. ACM Transactions on Information
Systems, 29 (1).
Kurland, O. (2006). Inter-document similarities, language models, and ad hoc retrieval.
Ph.D. thesis, Cornell University.
Kurland, O., & Lee, L. (2005). PageRank without hyperlinks: Structural re-ranking using
links induced by language models. In Proceedings of SIGIR, pp. 306313.
Kurland, O., & Lee, L. (2006). Respect my authority! HITS without hyperlinks utilizing
cluster-based language models. In Proceedings of SIGIR, pp. 8390.
Kurland, O., & Lee, L. (2010). Pagerank without hyperlinks: Structural reranking using
links induced by language models. ACM Transactions om Information Systems, 28 (4).
Lavrenko, V., & Croft, W. B. (2001). Relevance-based language models. In Proceedings of
SIGIR, pp. 120127.
Lawrence, S., & Giles, C. L. (1998). Inquirus, the neci meta search engine. In Proceedings
of the World Wide WEB conference, pp. 95105.
294

fiFusing Retrieved Lists Based on Inter-Document Similarities

Lee, J. H. (1997). Analyses of multiple evidence combination. In Proceedings of SIGIR, pp.
267276.
Lillis, D., Toolan, F., Collier, R. W., & Dunnion, J. (2006). Probfuse: a probabilistic
approach to data fusion. In Proceedings of SIGIR, pp. 139146.
Liu, X., & Croft, W. B. (2004). Cluster-based retrieval using language models. In Proceedings
of SIGIR, pp. 186193.
Meister, L., Kurland, O., & Kalmanovich, I. G. (2010). Re-ranking search results using an
additional retrieved list. Information Retrieval, 1.
Metzler, D., & Croft, W. B. (2005). A Markov random eld model for term dependencies.
In Proceedings of SIGIR, pp. 472479.
Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing order into texts. In Proceedings of
EMNLP, pp. 404411. Poster.
Montague, M., & Aslam, J. A. (2002). Condorcet fusion for improved retrieval. In Proceedings of CIKM, pp. 538548.
Morgan, W., Grei, W., & Henderson, J. (2004). Direct maximization of average precision
by hill-climbing, with a comparison to a maximum entropy approach. Tech. rep.
04-0367, The MITRE Corporation.
Otterbacher, J., Erkan, G., & Radev, D. R. (2005). Using random walks for question-focused
sentence retrieval. In Proceedings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),
pp. 915922.
Pinski, G., & Narin, F. (1976). Citation inuence for journal aggregates of scientic publications: Theory, with application to the literature of physics. Information Processing
and Management, 12, 297312.
Saracevic, T., & Kantor, P. (1988). A study of information seeking and retrieving. iii.
searchers, searches, and overlap. Journal of the American Society for Information
Science, 39 (3), 197216.
Selvadurai, S. B. (2007). Implementing a metasearch framework with content-directed result
merging. Masters thesis, North Carolina State University.
Shokouhi, M. (2007). Segmentation of search engine results for eective data-fusion. In
Proceedings of ECIR, pp. 185197.
Shou, X. M., & Sanderson, M. (2002). Experiments on data fusion using headline information. In Proceedgins of SIGIR, pp. 413414.
Soboro, I., Nicholas, C. K., & Cahan, P. (2001). Ranking retrieval systems without relevance judgments. In Proceedings of SIGIR, pp. 6673.
Tao, T., & Zhai, C. (2006). Regularized esitmation of mixture models for robust pseudorelevance feedback. In Proceedings of SIGIR, pp. 162169.
Tsikrika, T., & Lalmas, M. (2001). Merging techniques for performing data fusion on the
web. In Proceedings of CIKM, pp. 127134.
van Rijsbergen, C. J. (1979). Information Retrieval (second edition). Butterworths.
295

fiKhudyak Kozorovitsky & Kurland

Vogt, C. C., & Cottrell, G. W. (1999). Fusion via linear combination of scores. Information
Retrieval, 1 (3), 151173.
Voorhees, E. M., Gupta, N. K., & Johnson-Laird, B. (1994). The collection fusion problem.
In In Proceedings of TREC-3.
Voorhees, E. M., & Harman, D. K. (2005). TREC: Experiments and evaluation in information retrieval. The MIT Press.
Willett, P. (1985). Query specic automatic document classication. International Forum
on Information and Documentation, 10 (2), 2832.
Xu, J., & Croft, W. B. (1996). Query expansion using local and global document analysis.
In Proceedings of SIGIR, pp. 411.
Xu, J., & Croft, W. B. (1999). Cluster-based language models for distributed retrieval. In
Proceedings of SIGIR, pp. 254261.
Yang, L., Ji, D., Zhou, G., Nie, Y., & Xiao, G. (2006). Document re-ranking using cluster
validation and label propagation. In Proceedings of CIKM, pp. 690697.
Young, H. P. (1974). An axiomatization of Bordas rule. Journal of Economic Theory, 9,
4352.
Zamir, O., & Etzioni, O. (1998). Web document clustering: a feasibility demonstration. In
Proceedings of SIGIR, pp. 4654.
Zhai, C., Cohen, W. W., & Laerty, J. D. (2003). Beyond independent relevance: methods
and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR, pp. 1017.
Zhai, C., & Laerty, J. (2002). Two-stage language models for information retrieval. In
Proceedings of SIGIR, pp. 4956.
Zhai, C., & Laerty, J. D. (2001). A study of smoothing methods for language models
applied to ad hoc information retrieval. In Proceedings of SIGIR, pp. 334342.
Zhang, B., Li, H., Liu, Y., Ji, L., Xi, W., Fan, W., Chen, Z., & Ma, W.-Y. (2005). Improving
web search results using anity graph. In Proceedings of SIGIR, pp. 504511.

296

fiJournal of Artificial Intelligence Research 41 (2011) 97-130

Submitted 10/10; published 05/11

Soft Constraints of Difference and Equality
Emmanuel Hebrard

hebrard@laas.fr

CNRS; LAAS
Universite de Toulouse
Toulouse, France

Daniel Marx

dmarx@cs.bme.hu

Humboldt-Universitat zu Berlin
Berlin, Germany

Barry OSullivan

b.osullivan@cs.ucc.ie

Cork Constraint Computation Centre
Department of Computer Science, University College Cork
Cork, Ireland

Igor Razgon

ir45@mcs.le.ac.uk

Department of Computer Science, University of Leicester
Leicester, United Kingdom

Abstract
In many combinatorial problems one may need to model the diversity or similarity of
sets of assignments. For example, one may wish to maximise or minimise the number of
distinct values in a solution. To formulate problems of this type we can use soft variants of
the well known AllDifferent and AllEqual constraints. We present a taxonomy of six
soft global constraints, generated by combining the two latter ones and the two standard
cost functions, which are either maximised or minimised. We characterise the complexity
of achieving arc and bounds consistency on these constraints, resolving those cases for
which NP-hardness was neither proven nor disproven. In particular, we explore in depth
the constraint ensuring that at least k pairs of variables have a common value. We show
that achieving arc consistency is NP-hard, however bounds consistency can be achieved in
polynomial time through dynamic programming. Moreover, we show that the maximum
number of pairs of equal variables can be approximated by a factor of 12 with a linear time
greedy algorithm. Finally, we provide a fixed parameter tractable algorithm with respect
to the number of values appearing in more than two distinct domains. Interestingly, this
taxonomy shows that enforcing equality is harder than enforcing difference.

1. Introduction
Constraints for reasoning about equality and difference within assignments to a set of variables are ubiquitous in constraint programming. In many settings, one needs to enforce a
given degree of diversity or similarity in a solution. For example, in a university timetabling
problem we will want to ensure that all courses taken by a particular student are held at
different times. Similarly, in meeting scheduling we will want to ensure that the participants
of the same meeting are scheduled to meet at the same time and in the same place. Sometimes, when the problem is over-constrained, we might wish to maximise the extent to which
these constraints are satisfied. Consider again our timetabling example: we might wish to

c
2011
AI Access Foundation. All rights reserved.

fiHebrard, Marx, OSullivan & Razgon

maximise the number of courses that are scheduled at different times when a students
preferences cannot all be met.
In a constraint programming setting requirements on the diversity and similarity amongst
variables can be specified using global constraints. One of the most commonly used global
constraints is the AllDifferent (Regin, 1994), which enforces that all variables take pairwise different values. A soft version of the AllDifferent constraint, named SoftAllDiff,
has been proposed by Petit, Regin, and Bessiere (2001). They proposed two cost metrics
for measuring the degree of satisfaction of the constraint, which are to be minimised or
maximised: graph- and variable-based cost. These two cost metrics are generic and widely
used (e.g., van Hoeve, 2004). The former counts the number of equalities, whilst the latter counts the number of variables to change in order to satisfy the corresponding hard
constraint. When we wish to enforce that a set of variables take equal values, we can
use the AllEqual, or its soft variant for the graph-based cost, the SoftAllEqual constraint (Hebrard, OSullivan, & Razgon, 2008), or its soft variant for the variable-based
cost, the AtMostNValue constraint (Beldiceanu, 2001).
When considering these two constraints (AllDifferent and AllEqual), these two
costs (graph-based and variable-based) and objectives (minimisation and maximisation) we
can define eight algorithmic problems related to constraints of difference and equality. In
fact, because the graph-based costs of AllDifferent and AllEqual are dual, only six
distinct problems are thus defined. The structure of this class of constraints is illustrated
in Figure 1. For each one, we give the complexity of the best known algorithm for achieving ac and bc. Three of these problems were studied in the past: minimising the cost of
SoftAllDiff variable (Petit et al., 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere,
Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for
bc. A fourth one, maximising the variable-based cost of the SoftAllEqual constraint,
can directly be mapped to a known problem: the Global Cardinality constraint. In
this paper,1 we introduce two efficient algorithms for achieving, respectively, Arc consistency (ac) and Bounds consistency (bc) on the fifth case, minimising the variable-based
cost for SoftAllEqual. Moreover, the computational complexity of the last remaining
case, maximising the graph-based cost for SoftAllDiff (or, equivalently, minimising the
graph-based cost for SoftAllEqual) was still unknown. Informally, this problem is to
maximise the number of pairs of variables assigned to a common value. It turns out to be
a challenging and interesting problem, in that it is hard but yet can be addressed in several
ways. In particular, we show that:
 Finding a solution with at least k pairs of equal variables is NP-complete, hence
achieving ac on the corresponding constraint is NP-hard.
 When domains are contiguous, it can be solved in a polynomial number of steps
through dynamic programming, hence achieving bc on the corresponding constraint
is polynomial.
 There exists a linear approximation by a factor of

1
2

for the general case.

1. Part of the material presented in this paper is based on two conference publications (Hebrard et al.,
2008; Hebrard, Marx, OSullivan, & Razgon, 2009).

98

fiSoft Constraints of Difference and Equality

 If no value appears in the domains of more than two distinct variables, then the
problem can be solved by a general matching, thus defining another tractable class.
 There exists a fixed parameter tractable algorithm for this problem for a parameter
k equal to the number of values that appear in more than two distinct domains.
Moreover, we show that the constraint defined by setting a lower bound on the graphbased cost of SoftAllEqual can be used to efficiently find a set of similar solutions to a
set of problems, for instance to promote stability or regularity. Similarly, the dual constraint
(SoftAllDiff) can be used to find a set of diverse solutions, for instance to sample a set
of configurations. Notice that these two applications have motivated, in part, our choice of
cost metrics.
The remainder of this paper is organised as follows. In Section 2 we introduce the necessary technical background. A complete taxonomy of constraints of equality and difference,
based on results by other authors as well as original material is presented in Section 3. Then,
in the following sections, we present the new results allowing us to close the gaps in this
taxonomy. First, in Section 4 we present two efficient algorithm for achieving ac and bc
when minimising the variable-based cost of SoftAllEqual. Second, in Section 5 we give
a proof of NP-hardness for the problem of achieving ac when maximising the graph-based
cost of SoftAllDiff. Third, in Section 6 we present a polynomial algorithm to achieve
bc on the same constraint. Finally, in the remaining sections, we explore the algorithmic
properties of this preference cost. In Section 7, we show that a natural greedy algorithm
approximates the maximum number of equalities within a factor of 21 , and that its complexity can be brought down to linear time. Next, in Section 8, we identify a polynomial
class for this constraint. Then, in Section 9, we identify a parameter based on this class
and show that the SoftAllEqualG constraint is fixed-parameter tractable with respect
to this parameter. Finally, in Section 10, we show how the results obtained in this paper
can be applied to sample solutions or, conversely, to promote stability. In particular, we
describe two constructions using SoftAllDiffmin
and SoftAllEqualmin
respectively.
G
G
Concluding remarks are made in Section 11.

2. Background
In this section we present the necessary background required by the reader and introduce
the notation we use throughout the paper.
2.1 Constraint Satisfaction
A constraint satisfaction problem (CSP) is a triplet P = (X , D, C) where X is a set of
variables, D is a mapping of variables to finite sets of values and C is a set of constraints that
specify allowed combinations of values for subsets of variables. Without loss of generality,
we assume D(X)  Z for all X  X , and we denote by min(X) and max(X) the minimum
and maximum values in D(X), respectively. An assignment of a set of variables X is a set
of pairs S such that |X | = |S| and for each X  X , there exists (X, v)  S with v  D(X).
A constraint C  C is arc consistent (ac) iff, when a variable in the scope of C is assigned
any value, there exists an assignment to the other variables in C such that C is satisfied.
This satisfying assignment is called a domain support for the value. Similarly, we call a
99

fiHebrard, Marx, OSullivan & Razgon

range support an assignment satisfying C, but where values, instead of being taken from
the domain of each variable (v  D(X)), can be any integer between the minimum and
maximum of this domain following the natural order on Z (v  [min(X), . . . , max(X)]) . A
constraint C  C is range consistent (rc) iff every value of every variable in the scope of C
has a range support. A constraint C  C is bounds consistent (bc) iff for every variable X
in the scope of C, min(X) and max(X) have a range support. Given a CSP P = (X , D, C),
we shall use the following notation throughout the paper: n shall denote the number of
variables,
P i.e., n = |X |; m shall denote the number of distinct unary
S assignments, i.e.,
m = XX |D(X)|;  shall denote the total set of values, i.e.,  = XX D(X); finally, 
shall denote the total number of distinct values, i.e.,  = ||.
2.2 Soft Global Constraints
Adding a cost variable to a constraint to represent its degree of violation is now common practice in constraint programming. This model was introduced by Petit, Regin, and
Bessiere (2000). It offers the advantage of unifying hard and soft constraints since arc consistency, along with other types of consistencies, can be applied to such constraints with
no extra effort. As a consequence, classical constraint solvers can model over-constrained
problems in this way without modification. This approach was applied to a number of
other constraints, for instance by van Hoeve, Pesant, and Rousseau (2006). Several cost
metrics have been explored for the AllDifferent constraint, as well as several others
(e.g., Beldiceanu & Petit, 2004). It is important, if one uses such a unifying model, that
the cost metric chosen can be evaluated in polynomial time given a complete assignment of
the variables that are constrained. This is the case for the two metrics considered in this
paper for the constraints AllDifferent and AllEqual.
The variable-based cost counts how many variables need to change in order to obtain a
valid assignment for the hard constraint. It can be viewed as the smallest Hamming distance
with respect to a satisfying assignment. The graph-based cost counts how many times a
component of a decomposition of the constraint is violated. Typically these components
correspond to edges of a decomposition graph, e.g. for an AllDifferent constraint, the
decomposition graph is a clique and an edge is violated if and only if both variables connected
by this edge share the same value. The following example, still for the AllDifferent
constraint, shows two solutions involving four variables X1 , . . . , X4 each with domain {a, b}:
S1 = {(X1 , a), (X2 , b), (X3 , a), (X4 , b)}.
S2 = {(X1 , a), (X2 , b), (X3 , b), (X4 , b)}.
In both solutions, at least two variables must change (e.g., X3 and X4 ) to obtain a valid
solution. Therefore, the variable-based cost is 2 for S1 and S2 . However, in S1 only two
edges are violated, (X1 , X3 ) and (X2 , X4 ), whilst in S2 , three edges are violated, (X2 , X3 ),
(X2 , X4 ) and (X3 , X4 ). Thus, the graph-based cost of S1 is 2 whereas it is 3 for S2 .
2.3 Parameterised Complexity
We shall use the notion of parameterised complexity in Section 9. We refer the reader
to Niedermeiers (2006) book for a comprehensive introduction. Given a problem A, a
100

fiSoft Constraints of Difference and Equality

parameterised version of A is obtained by specifying a parameter of this problem and getting
as additional input a non-negative integer k which restricts the value of this parameter. The
resulting parameterised problem hA, ki is fixed-parameter tractable (FPT) with respect to
k if it can be solved in time f (k)  nO(1) , where f (k) is a function depending only on k.
When the size of the problem is significantly larger than the parameter k, a fixed-parameter
algorithm essentially has polynomial behaviour. For instance if f (k) = 2k then, as long as
k is bounded by log n, the problem can be solved in polynomial time.

3. Taxonomy
In this section we introduce a taxonomy of soft constraints based on AllDifferent and
AllEqual. We consider the eight algorithmic problems related to constraints of difference and equality defined by combining these two constraints, two costs (graph-based and
variable-based), and two objectives (minimisation and maximisation). In fact, because the
graph-based costs of AllDifferent and AllEqual are dual, only six different problems
are defined. Observe that we consider only costs defined through inequalities, rather than
equalities. There are several reasons for doing so. First, reasoning about the lower bound or
the upper bound of the cost variable can yield two extremely different problems, and hence
different algorithmic solutions. For instance, we shall see that in some cases the problem is
tractable in one direction, and NP-hard in the other direction. When reasoning about cost
equality, one will often separate the inference procedures relative to the lower bound, upper bound, and intermediate values. Reasoning about lower and upper bounds is sufficient
to model an equality although it might hinder domain filtering when intermediate values
for the cost are forbidden. We thus cover equalities in a restricted way, albeit arguably
reasonable in practice. Indeed, when dealing with costs and objectives, reasoning about
inequalities and bounds is more useful in practice than imposing (dis)equalities.
We close the last remaining cases: the complexity of achieving ac and bc SoftAllEqualmin
V
in Section 4, that of achieving ac on SoftAllEqualmin
in
Section
5
and
that
of
achievG
ing bc on SoftAllEqualmin
in Section 6. Based on these results, Figure 1 can now be
G
completed (fourth and fifth columns).
The next six paragraphs correspond to the six columns of Figure 1, that is, to the twelve
elements of the taxonomy. For each of them, we briefly outline the current state of the art,
using the following assignment as a recurring example to illustrate the various costs:
S3 = {(X1 , a), (X2 , a), (X3 , a), (X4 , a), (X5 , b), (X6 , b), (X7 , c)}.
3.1 SoftAllDiff: Variable-based cost, Minimisation
Definition 1 (SoftAllDiffmin
V )
SoftAllDiffmin
V ({X1 , . . . , Xn }, N )  N  n  |{v | Xi = v}|.
Here the cost to minimise is the number of variables that need to be changed in order
to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3
is 4 since three of the four variables assigned to a as well as one of the variables assigned
to b must change. This objective function was first studied by Petit et al. (2001), and an

algorithm for achieving ac in O(n m) was introduced. To the best of our knowledge, no
101

fiHebrard, Marx, OSullivan & Razgon

AllEqual

AllDifferent
gra
p

in

min

x

ma

ax

ax

in

x

m

ma

le

min

m

m

var
iab

ph
gra

h

va

m

ble
ria

E
ll

A
ft
So

m

fV
if

m

in

ax

m

in

m

fV
if

V
al
qu

D
ll

D
ll

V
al
qu
E
ll
A
ft
in
m
So
G
al
qu ax
E m
ll f G
A if
ft llD
So A
ft
ax
So
m
G
al
qu in
E m
ll f G
A if
ft llD
So A
ft
So
ax

A
ft
So

A
ft
So



O(n m) NP-hard O(nm) NP-hard O(nm) O(n m)
[1]
[2]
[4]
[5]
[6]
[8]

O(n m) O(n log n) O(nm) O(min( , n )nm)O(n log n)O(n log n)
2

[1]

[3]

[4]

2

[6]

[7]

[8]

Figure 1: Complexity of optimising difference and equality  first row: ac, second row: bc.
Parameter n denotes the number of variables, m the sum of the domain sizes and
 the number of distinct values. References: [1] (Petit et al., 2001), [2] (Bessiere
et al., 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al.,
2008), [6] (Hebrard et al., 2009), [7] (present paper), [8] (Quimper et al., 2004).

algorithm with better time complexity for the special case of bounds consistency has been
proposed for this constraint. Notice however that Mehlhorn and Thiels (2000) algorithm
achieves bc on the AllDifferent constraint with an O(n log n) time complexity. The
question of whether this algorithm could be adapted to achieve bc on SoftAllDiffmin
V
remains open.
3.2 SoftAllDiff: Variable-based cost, Maximisation
Definition 2 (SoftAllDiffmax
)
V
SoftAllDiffmax
({X1 , . . . , Xn }, N )  N  n  |{v | Xi = v}|.
V
Here the same cost is to be maximised. In other words, we want to minimise the number
of distinct values assigned to the given set of variables, since the complement of this number
to n is exactly the number of variables to modify in order to obtain a solution satisfying
an AllDifferent constraint. For instance, the cost of S3 is 4 and the number of distinct
values is 7  4 = 3. This constraint was studied under the name AtMostNValue. An
algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that
achieving ac is NP-hard was given by Bessiere et al. (2006).

102

fiSoft Constraints of Difference and Equality

3.3 SoftAllDiff: Graph-based cost, Minimisation & SoftAllEqual:
Graph-based cost, Maximisation
Definition 3 (SoftAllDiffmin
 SoftAllEqualmax
G
G )
SoftAllDiffmin
G ({X1 , . . . , Xn }, N )  N  |{{i, j} | Xi = Xj & i < j}|.
Here the cost to minimise is the number of violated constraints when decomposing
AllDifferent into a clique of binary NotEqual constraints. For instance, the cost
of S3 is 7 since four variables share the value a (six violations) and two share the value
b (one violation). Clearly, it is equivalent to maximising the number of violated binary
Equal constraints in a decomposition
of a global AllEqual. Indeed, these two costs

are complementary to n2 of each other (on S3 : 7 + 14 = 21). An algorithm in O(nm)
for achieving ac on this constraint was introduced by van Hoeve (2004). Again, to our
knowledge there is no algorithm improving this complexity for the special case of bc.
3.4 SoftAllEqual: Graph-based cost, Minimisation & SoftAllDiff:
Graph-based cost Maximisation
 SoftAllDiffmax
Definition 4 (SoftAllEqualmin
G )
G
SoftAllEqualmin
G ({X1 , . . . , Xn }, N )  N  |{{i, j} | Xi 6= Xj & i < j}|.
Here we consider the same two complementary costs, however we aim at optimising in
the opposite way. In Section 5 we show that achieving ac on this constraint is NP-hard and,
in Section 6 we show that, when domains are contiguous intervals, computing the optimal
cost can be done in O(min(n2 , n3 )). As a consequence, bc can be achieved in polynomial
time.
3.5 SoftAllEqual: Variable-based cost, Minimisation
Definition 5 (SoftAllEqualmin
V )
SoftAllEqualmin
V ({X1 , . . . , Xn }, N )  N  n  max(|{i | Xi = v}|).
v

Here the cost to minimise is the number of variables that need to be changed in order to
obtain a solution satisfying an AllEqual constraint. For instance, the cost of S3 is 3 since
four variables already share the same value. This is equivalent to maximising the number of
variables sharing a given value. Therefore this bound can be computed trivially by counting
the occurrences of every value in the domains. However, pruning the domains according
to this bound without degrading the time complexity is not as trivial. In Section 4, we
introduce two filtering algorithms, achieving ac and rc in the same complexity as that of
counting values.
3.6 SoftAllEqual: Variable-based cost, Maximisation
Definition 6 (SoftAllEqualmax
)
V
({X1 , . . . , Xn }, N )  N  n  max(|{i | Xi = v}|).
SoftAllEqualmax
V
v

103

fiHebrard, Marx, OSullivan & Razgon

Here the same cost has to be maximised. In other words we want to minimise the
maximum cardinality of each value. For instance, the cost of S3 is 3, that is, the complement
to n of the maximum cardinality of a value (3 = 7  4). This is exactly equivalent to
applying a Global Cardinality constraint (considering only the upper bounds on the
cardinalities). Two algorithms, for achieving ac and bc on this constraint and running in

O( nm) and O(n log n) respectively, was introduced by Quimper et al. (2004).

4. The Complexity of Arc and Bounds Consistency on SoftAllEqualmin
V
Here we show how to achieve ac, rc and bc on the SoftAllEqualmin
constraints (see
V
Definition 5). This constraint is satisfied if and only if n minus the cardinality of any
set of variables assigned to a single value is less than or equal to the value of the cost
variable N . In other words, it is satisfied if there are at least k variables sharing a value,
where k = n  max(N ). Therefore, for simplicity sake, we shall consider the following
equivalent formulation, where N is a lower bound on the complement to n of the same cost
(N 0 = n  N ):
N 0  max(|{i | Xi = v}|).
v

We shall see that to filter the domain of N 0 and the Xi s we need to compute two properties:
1. An upper bound k  on the number of occurrences amongst all values.
2. The set of values that can actually appear k  times.
Computing the set of values that appear in the largest possible number of variable domains
can be performed trivially in O(m), by counting the number of occurrences of every value,
i.e., the number of variables whose domain contains v.
However, if domains are discrete intervals defined by lower and upper bounds, it can be
done even more efficiently. Given two integers a and b, a  b, we say that the set of all
integers x, a  x  b, is an interval and denote it byS[a, b]. In the rest of this section we
shall assume that the overall set of values values  = XX D(X) is the interval [1, ].
Definition 7 (Occurrence function and derivative) Given a constraint network P =
(X , D, C), the occurrence function occ is the mapping from values in  to N defined as
follows:
occ(v) = |{X | X  X & v  doms(X)}|.
The  derivative of occ, occ , maps each value v   to the difference between the value of
occ(v  1) and occ(v):
occ (0) = 0,
occ (v) = occ(v)  occ(v  1).
We give an example of the occurrence function for a set of variables with interval domains
in Figure 2.
Algorithm 1 computes occ1 , that is, the inverse of the occurrence function, which maps
every element in the interval [1, n] to the set of values appearing that many times. It runs
104

fiX6

5

X5

4

variables

variables

Soft Constraints of Difference and Equality

X4
X3

3
2

X2

1

X1

0
1

15

40

60

70

90

100

1

15

values

40

60

70

90

100

values

(a) Intervals

(b) Occurrence function

Figure 2: A set of intervals (a) and the corresponding occurrence function (b).
Algorithm 1: Computing the inverse occurrence function.
Data: A set of variables: X
Result: occ1 : [1, n] 7 2
occ (v)  ;
1 foreach X  X do
occ (min(X))  occ (min(X)) + 1;
occ (max(X) + 1)  occ (max(X) + 1)  1;
2 x  [1, n], occ1 (x)  ;

x  0;
pop first element (v, a) of occ ;
repeat
pop first element (w, b) of occ ;
x  x + occ (a);
occ1 (x)  occ1 (x)  [a, b  1];
a  b;
until occ = ;

in O(n log n) worst-case time complexity if we assume it is easy to extract both an upper
bound (k   N 0 ) and the set of values that can appear k  times from occ1 .
The idea behind this algorithm, which we shall reuse throughout this paper, is that when
domains are given as discrete intervals one can compute the non-null values of the derivative
occ of the occurrence function occ in O(n log n) time. The procedure is closely related to the
concept of sweep algorithms (Beldiceanu & Carlsson, 2001) used, for instance, to implement
filtering algorithms for the Cumulative constraint. Instead of scanning the entire horizon,
one can jump from an event to the next, assuming that nothing changes between two events.
As in the case of the Cumulative constraint, events here correspond to start and end points
of the domains. In fact, it is possible to compute the same lower bound, with the same
complexity, by using Petit, Regin, and Bessieres (2002) Range-based Max-CSP Algorithm
(RMA)2 on a reformulation as a Max-CSP. Given a set of variables XS, we add an extra
variable Z whose domain is the union of all domains in X : D(Z) =  = XX D(X). Then
2. We thank the anonymous reviewer who made this observation.

105

fiHebrard, Marx, OSullivan & Razgon

we link it to other variables in X through binary equality constraints:
X  X , Z = X.
There is a one-to-one mapping between the solutions of this Max-CSP and the satisfying assignments of a SoftAllEqualmin
constraint on (X , N ), where the value of N corresponds
V
to the number of violated constraints in the Max-CSP. The lower bound on the number of
violations computed by RMA and the lower bound k  on N computed in Algorithm 1 are,
therefore, the same. Moreover the procedures are essentially equivalent, i.e., modulo the
modelling step. Algorithm 1 can be seen as a particular case of RMA: the same ordered set
of intervals is computed, and subsequently associated with a violation cost. However, we
use our formalism, since the notion of occurrence function and its derivative is important
and used throughout the paper.
We first define a simple data structure that we shall use to compute and represent the
function occ . A specific data structure is required since indexing the image of occ (v) by the
value v would add a factor of  to the (space and therefore time) complexity. The non-zero
values of occ are stored as a list of pairs whose first element is a value v  [1, . . . , ] and
second element stands for occ (v). The list is maintained in increasing order of the pairs
first element. Given an ordered list occ = [(v1 , o1 ), . . . , (vk , ok )], the assignment operation
occ (vi )  oi can therefore been done in O(log |occ |) steps as follows:
1. The rank r of the pair (vj , oj ) such that vj is minimum and vj  vi is computed
through a dichotomic search.
2. If vi = vj , the pair (vj , oj ) is removed.
3. The pair (vi , oi ) is inserted at rank r.
Moreover, one can access the element with minimum (resp. maximum) first element in
constant time since it is first (resp. last) in the list. Finally, the value of occ (vi ) is oi if
there exists a pair (vj , oj ) in the list, and 0 otherwise. Computing this value can also be
done in logarithmic time.
The derivative occ (v) is computed in Loop 1 of Algorithm 1 using the assignment
operator defined above. Observe that if D(X) = [a, b], then X contributes only to two
values of occ : it increases occ (a) by 1 and decreases occ (b + 1) by 1. For every value w
such that there is no X with min(X) = w or max(X) + 1 = w, occ (w) is null. In other
words, we can define occ (v) for any value v, as follows:
occ (v) = (|{i | min(Xi ) = v}|  |{i | max(Xi ) = v  1}|).
Therefore, by going through every variable X  X , we can compute the non-null values of
occ in time O(n log n) using the simple list structure described above.
Then, starting from Line 2, we compute occ1 by going through the non-zero values v
of the derivative, i.e. such that occ (v) 6= 0, in increasing order of v. Recall that we use an
ordered list, so this is trivially done in linear time. By definition, the occurrence function is
constant on the interval defined by two such successive values. Since the number of non-zero
values of occ is bounded by O(n), the overall worst-case time complexity is in O(n log n).
We use Figure 3 (a,c & d) to illustrate an execution of Algorithm 1. First, six variables and
106

fiX6

X6

X5

X5

variables

variables

Soft Constraints of Difference and Equality

X4
X3

X4
X3

X2

X2

X1

X1
1

15

40

60

70

90

100

1

values

=
=
=
=
=
=
=
=

+2
+2
2
+1
+1
1
1
2

40

60

70

90

100

values

(a) Intervals

occ (1)
occ (15)
occ (41)
occ (60)
occ (70)
occ (71)
occ (91)
occ (101)

15

(b) Pruning

occ1 (2)
occ1 (3)
occ1 (4)

=
=
=

{[1, 14]  [41, 59]  [91, 100]}
{[60, 69]  [71, 90]}
{[15, 40]  [70, 70]}

(d) Inverse of the occurrence function.

(c) Derivative of the occurrence function.

Figure 3: Execution of Algorithm 1: A set of intervals (a). The same set of intervals where
inconsistent sub-intervals for a lower bound on the number of equalities of 4 (N
 4) are represented as dashed lines (b). (c) and (d) represent the derivative, and
the inverse of the occurrence function for the initial set of intervals, respectively.

their domains are represented in Figure 3(a). Then, in Figures 3(c) and 3(d) we show the
derivative and the inverse, respectively, of the occurrence function.
Alternatively, when  < n log n, it is possible to compute occ1 in O(n + ) by replacing
the data structure used to store occ by a simple array, indexed by values in [1, ]. Accessing
and updating a value of occ can thus be done in constant time.
Now we show how to prune the variables in X with respect to this bound without
degrading the time complexity. According to the method used we can, therefore, achieve
ac or rc in a worst-case time complexity of O(m) or O(min(n + , n log n), respectively.
Theorem 1 Enforcing ac (resp. rc) on SoftAllEqualmin
can be achieved in in O(m)
V
steps (resp. O(min(n + , n log n)).
Proof. We suppose, without loss of generality, that the current lower bound on N 0 is k.
We first compute the inverse occurrence function either by counting values, or considering
interval domains using Algorithm 1. From this we can define the set of values with highest
number of occurrences. Let this number of occurrences be k  , and the corresponding set of
values be V (i.e. occ1 (k  ) = V ). Then there are three cases to consider:
107

fiHebrard, Marx, OSullivan & Razgon

1. First, if every value appears in strictly fewer than k domains (k  < k) then the
constraint is violated.
2. Second, if at least one value v appears in the domains of at least k + 1 variables
(k  > k), then we can build a support for every value w  D(X). Let v  V , we
assign all the variables in X \ X with v when possible. The resulting assignment has
at least k occurrences of v, hence it is consistent. Consequently, since k  > k, every
value is consistent.
3. Otherwise, if neither of the two cases above hold, we know that no value appears in
more than k domains, and that at least one appears k times. Recall that V denotes
the set of such values. In this case, the pair (X, v) is inconsistent if and only if
v 6 V & V  D(X).
We first suppose that this condition does not hold and show that we can build a
support. If v  V then clearly we can assign every possible variable to v and achieve
a cost of k. If V 6 D(X), then we consider w such that w  V and w 6 D(X). By
assigning every variable with w when possible we achieve a cost of k no matter what
value is assigned to X.
Now we suppose that v 6 V & V  D(X) holds and show that (X, v) does not have
an ac support. Indeed, once X is assigned to v the domains are such that no value
appears in k domains or more, since every value in V has now one fewer occurrence,
hence we are back to Case 1.
Computing the set V of values satisfying the condition above can be done easily once the
inverse occurrence function has been computed. On the one hand, if this function occ1
has been computed by counting every value in every domain, then the supports used in the
proofs are all domain supports, hence ac is achieved. On the other hand, if domains are
approximated by their bounds and Algorithm 1 is used instead, the supports are all range
supports, hence rc is achieved. In Case 3, the domain can be pruned down to the set V of
values whose number of occurrences is k, as illustrated in Figure 3 (b).
2
can be achieved in O(min(n+, n log n)
Corollary 1 Enforcing bc on SoftAllEqualmin
V
steps.
Proof. This is a direct implication of Theorem 1.

2

The proof of Theorem 1 yields a domain filtering procedure. Algorithm 2 achieves either
ac or rc depending on the version of Algorithm 1 used in Line 1 to compute the inverse
occurrence function. The later function occ1 is then used in Line 2, 3 and 4 to, respectively,
catch a global inconsistency, prune the upper bound of N 0 and prune the domains of the
variables in X .
Figure 3(b) illustrates the pruning that one can achieve on X provided that the lower
bound on N 0 is equal to 4. Dashed lines represent inconsistent intervals. The set V of
values used in Line 4 of Algorithm 2 is occ1 (4) = {[15, 40]  [70, 70]}.

108

fiSoft Constraints of Difference and Equality

0
Algorithm 2: Propagation of SoftAllEqualmin
V ({X1 , . . . , Xn }, N ).
1 occ1  Algorithm 1;

ub  n;
while occ1 (ub) =  do
ub  ub  1;
2 if min(N 0 ) > ub then fail;

else
max(N 0 )  ub;
if min(N 0 ) = max(N 0 ) then
V  occ1 (min(N 0 ));
4
foreach X  X do if V  D(X) then D(X)  V ;

3

5. The Complexity of Arc Consistency on SoftAllEqualmin
G
Here we show that achieving ac on SoftAllEqualmin
is NP-hard. In order to achieve
G
ac we need to compute an arc consistent lower bound on the cost variable N constrained
as follows:
N  |{{i, j} | Xi 6= Xj & i < j}|.
In other words, we want to find an assignment of the variables in X minimising the number
of pairwise disequalities, or maximising the number of pairwise equalities. We consider
the corresponding decision problem (SoftAllEqualmin
G -decision), and show that it is
NP-hard through a reduction from 3dMatching (Garey & Johnson, 1979).
Definition 8 (SoftAllEqualmin
G -decision)
Data: An integer N , a set X of variables.
Question: Does there exist a mapping s : X 7  such that X  X , s[X]  D(X) and
|{{i, j} | s[Xi ] = s[Xj ] & i 6= j}|  N ?
Definition 9 (3dMatching)
Data: An integer K, three disjoint sets X, Y, Z, and T  X  Y  Z.
Question: Does there exist M  T such that |M |  K and m1 , m2  M, i  {1, 2, 3}, m1 [i] 6=
m2 [i]?
Theorem 2 (The Complexity of SoftAllEqualmin
G ) Finding a satisfying assignment
min
for the SoftAllEqualG constraint is NP-complete even if no value appears in more
than three domains.
Proof. The problem SoftAllEqualmin
G -decision is clearly in NP: checking the number
of equalities in an assignment can be done in O(n2 ) time.
We use a reduction from 3dMatching to show completeness. Let P = (X, Y, Z, T, K)
be an instance of 3dMatching, where: K is an integer; X, Y, Z are three disjoint sets such
that X  Y  Z = {x1 , . . . , xn }; and T = {t1 , . . . , tm } is a set of triplets over X  Y  Z.
We build an instance I of SoftAllEqualmin
as follows:
G
1. Let n = |X| + |Y | + |Z|, we build n variables {X1 , . . . , Xn }.
2. For each tl = hxi , xj , xk i  T , we have l  D(Xi ), l  D(Xj ) and l  D(Xk ).
109

fiHebrard, Marx, OSullivan & Razgon

3. For each pair (i, j) such that 1  i < j  n, we put the value (|T | + (i  1)  n + j) in
both D(Xi ) and D(Xj ).
We show there exists a matching of P of size K if and only if there exists a solution of
I with b 3K+n
2 c equalities. We refer to a matching of P  and to a solution of I as a
matching and a solution throughout this proof, respectively.
: We show that if there exists a matching of cardinality K then there exists a solution
with at least b 3K+n
2 c equalities. Let M be a matching of cardinality K. We build a solution
as follows. For all tl = hxi , xj , xk i  M we assign Xi , Xj and Xk to l (item 2 above).
Observe that there remain exactly n  3K unassigned variables after this process. We pick
an arbitrary pair of unassigned variables and assign them with their common value (item 3
above), until at most one variable is left (if one variable is left we assign it to an arbitrary
value). Therefore, the solution obtained in this way has exactly b 3K+n
2 c equalities, 3K from
the variables corresponding to the matching and b n3K
c
for
the
remaining
variables.
2
: We show that if the cardinality of the maximal matching is K, then there is no solution
with more than b 3K+n
2 c equalities. Let S be a solution. Furthermore, let L be the number
of values appearing three times in S. Observe that this set of values corresponds to a
matching. Indeed, a value l appears in three domains D(Xi ), D(Xj ) and D(Xk ) if and only
if there exists a triplet tl = hxi , xj , xk i  T (item 2 above). Since a variable can only be
assigned to a single value, the values appearing three times in a solution form a matching.
Moreover, since no value appears in more than three domains, all other values can appear
at most twice. Hence the number of equalities in S is less than or equal to b 3L+n
2 c, where L
is the size of a matching. It follows that if there is no matching of cardinality greater than
K, there is no solution with more than b 3K+n
2
2 c equalities.
Cohen, Cooper, Jeavons, and Krokhin (2004) showed that the language of soft binary
equality constraints is NP-complete, for as few as three distinct values. On the one hand,
Theorem 2 applies to a more specific class of problems where the constraint network formed
by the soft binary constraints is a clique. On the other hand, the proof requires an unbounded number of values, these two results are therefore incomparable. However, we shall
see in Section 9 that this problem is fixed parameter tractable with respect to the number
of values, hence polynomial when it is bounded.

6. The Complexity of Bounds Consistency on SoftAllEqualmin
G
In this section we introduce an efficient algorithm that, assuming the domains are discrete
intervals, computes the maximum possible pairs of equal values in an assignment. We
therefore need to solve the optimisation version of the problem defined in the previous
section (Definition 8):
Definition 10 (SoftAllEqualmin
G -optimisation)
Data: A set X of variables.
Question: What is the maximum integer K such that there exists a mapping s : X 7 
satisfying X  X , s[X]  D(X) and |{{i, j} | s[Xi ] = s[Xj ] & i 6= j}| = K?
The algorithm we introduce allows us to close the last remaining open complexity question
in Figure 1: bc on the SoftAllEqualmin
constraint. We then improve it by reducing the
G
time complexity thanks to a preprocessing step.
110

fiSoft Constraints of Difference and Equality

We use the same terminology as in Section 4, and refer to the set of all integers x such
that a  x  b as the interval [a, b]. Let X be the set of variables of the considered CSP
and assume that the domains of all the variables of X are sub-intervals of [1, ]. We denote
by ME(X ) the set of all assignments P to the variables of X such that the number of pairs
of equal values of P is the maximum possible. The subset of X containing all the variables
whose domains are subsets of [a, b] is denoted by Xa,b . The subset of Xa,b including all the
variables containing the given value c in their domains is denoted by Xa,b,c . Finally the
number of pairs of equal values in an element of ME(Xa,b ) is denoted by Ca,b (X ) or just
Ca,b if the considered set of variables is clear from the context. For notational convenience,
if b < a, then we set Xa,b =  and Ca,b = 0. The value C1, (X ) is the number of equal pairs
of values in an element of ME(X ).
Theorem 3 C1, (X ) can be computed in O((n + )2 ) steps.
Proof. The problem is solved by a dynamic programming approach: for every a, b such
that 1  a  b  , we compute Ca,b . The main observation that makes it possible to use
dynamic programming is the following: in every P  ME(Xa,b ) there is a value c (a  c  b)
such that every variable X  Xa,b,c is assigned value c. To see this, let value c be a value
that is assigned by P to a maximum number of variables. Suppose that there is a variable
X with c  D(X) that is assigned by P to a different value, say c0 . Suppose that c and
c0 appear on x and y variables, respectively. By changing the value of X from c0 to c,
we increase the number of equalities by x  (y  1)  1 (since x  y), contradicting the
optimality of P .
Notice that Xa,b \ Xa,b,c is the disjoint union of Xa,c1 and Xc+1,b (if c  1 < a or
c + 1 > b, then the corresponding set is empty). These two sets are independent in the
sense that there is no value that can appear on variables from both sets. Thus it can be
assumed that P  ME(Xa,b ) restricted to Xa,c1 and Xc+1,b are elements of ME(Xa,c1 )
and ME(Xc+1,b ), respectively. Taking into consideration all possible values c, we get



|Xa,b,c |
Ca,b = max
+ Ca,c1 + Cc+1,b .
(1)
c,acb
2
In the first step of Algorithm 3, we compute |Xa,b,c | for all values of a, b, c. For each
triple a, b, c, it is easy to compute |Xa,b,c | in time O(n), hence all these values can be
computed in time O(n3 ). However, the running time can be reduced to O((n + )2 ) by
using the same idea as in Algorithm 1.
For each pair a, b, we compute the number of
occurrences of each value c by first computing a derivative a,b . More precisely, we define
a,b (c) = |Xa,b,c |  |Xa,b,c1 | and compute a,b (c) for every a < c  b (Algorithm 3, Line 1-2).
Thus by going through all the variables, we can compute the a,b (c) values for a fixed a, b
and for all a  c  b in time O(n) and we can also compute |Xa,b,a | in the same time
bound. Now it is possible to compute the values |Xa,b,c |, a < c  b in time O() by using
the equality |Xa,b,c | = |Xa,b,c1 | + a,b (c) iteratively (Algorithm 3, Line 3).
In the second step of the algorithm, we compute all the values Ca,b . We compute these

|
values in increasing order of b  a. If a = b, then Ca,b = |Xa,a,a
. Otherwise, values Ca,c1
2
and Cc+1,b are already available for every a  c  b, hence Ca,b can be determined in time
O() using Eq. (1) (Algorithm 3, Line 4). Thus all the values Ca,b can be computed in time
111

fiHebrard, Marx, OSullivan & Razgon

Algorithm 3: Computing the maximum number of equalities.
Data: A set of variables: X
Result: C1, (X )
 1  a, b, c  , a,b (c)  |Xa,b,c |  Ca,b  0;
foreach k  [0,   1] do
foreach a  [1,   k] do
b  a + k;
foreach X  Xa,b do
1
a,b (min(X))  a,b (min(X)) + 1;
2
a,b (max(X) + 1)  a,b (max(X) + 1)  1;
3
4

foreach c  [a, b] do
|Xa,b,c |  |Xa,b,c1 | + a,b (c);

|
+ Ca,c1 + Cc+1,b ));
Ca,b  max(Ca,b , ( |Xa,b,c
2
return C1, ;

O(3 ), including C1, , which is the value of the optimum solution of the problem. Using
standard techniques (storing for each Ca,b a value c that minimises (1)), a third step of the
algorithm can actually produce a variable assignment that obtains the maximum value. 2
Ca,b
b=1
b=2
b=3
b=4

a=1
1
X1,2,1 + C2,2 = 3
X1,3,1 + C2,3 = 6
X1,4,1 + C2,4 = 16

a=2

a=3

a=4

0
0
X2,4,4 + C2,3 = 6

0
X3,4,4 + C3,3 = 3

1

X10
X9
X8

variables

X7
X6
X5
X4
X3
X2
X1
1

2

3

4

values

Figure 4: A set of intervals, and the corresponding dynamic programming Table (Ca,b ).
Algorithm 3 computes the largest number of equalities one can achieve by assigning a
set of variables with interval domains. It can therefore be used to find an optimal solution
to either SoftAllDiffmax
or SoftAllEqualmin
G
G . Notice that for the latter one needs
to take the complement to n2 in order to get the value of the violation cost. Clearly, it
follows that achieving range or bounds consistency on these two constraints can be done
112

fiSoft Constraints of Difference and Equality

in polynomial time, since Algorithm 3 can be used as an oracle for testing the existence of
a range support. We give an example of the execution of Algorithm 3 in Figure 4. A set
of ten variables, from X1 to X10 are represented. Then we give the table Ca,b for all pairs
a, b  [1, ].
The complexity can be further reduced if   n. Here again, we will use the occurrence
function, albeit in a slightly different way. The intuition is that some values and intervals of
values are dominated by other. When the occurrence function is monotonically increasing,
it means that we are moving toward dominating values (they can be taken by a larger
set of variables), and conversely, a monotonic decrease denotes dominated values. Notice
that since we are considering discrete values, some variations may not be apparent in the
occurrence function. For instance, consider two variables X and Y with respective domains
[a, b] and [b + 1, c] such that a  b  c. The occurrence function for these two variables
is constant on [a, c]. However, for our purpose, we need to distinguish between true
monotonicity and that induced by the discrete nature of the problem. We therefore consider
some rational values when defining the occurrence function. In the example above, by
introducing an extra point b + 12 to the occurrence function, we can now capture the fact
that in fact it is not monotonic on [a, c].
Let X be a set of variables with interval domains in [1, ]. Consider the occurrence
function occ : Q 7 [0..n], where Q  Q is a set of values of the form a/2 for some a  N,
such that min(Q) = 1 and max(Q) = . Intuitively, the value of occ(a) is the number of
variables whose domain interval encloses the value a, more formally:
a  Q, occ(a) = |{X | X  X , min(X)  a  max(X)}|.
Such a function, along with the corresponding set of intervals, is depicted in Figure 5.
A crest of the function occ is an interval [a, b]  Q such that for some c  [a, b], occ is
monotonically increasing on [a, c] and monotonically decreasing on [c, b]. For instance, on
the set intervals represented in Figure 5, [1, 15] is a crest since it is monotonically increasing
on [1, 12] and monotonically decreasing on [12, 15].
Let I be a partition of [1, ] into a set of intervals such that every element of I is a
crest. For instance, I = {[1, 15], [16, 20], [21, 29], [30, 42]} is such a partition for the set of
intervals shown in Figure 5. We shall map each element of I to an integer corresponding to
its rank in the natural order. We denote by RI (X ) the reduction of X by the partition I.
The reduction has as many variables as X (equation 2 below) but the domains are replaced
with the set of intervals in I that overlap with the corresponding variable in X (equation 3
below). Observe that the domains remain intervals after the reduction.
0 }.
{X10 , . . . , X|X
|

RI (X ) =
Xi0

 RI (X ),

D(Xi0 )

= {I | I  I & D(Xi )  I 6= }.

(2)
(3)

For instance, the set of intervals depicted in Figure 5 can be reduced to the set shown
in Figure 4, where each element in I is mapped to an integer in [1, 4].
Theorem 4 If I is a partition of [1, ] such that every element of I is a crest of occ, then
ME(X ) = ME(RI (X )).
113

fiHebrard, Marx, OSullivan & Razgon

X10 in [26,42]
X9 in [10,26]
X8 in [7,15]
X7 in [18,32]
X6 in [16,40]
X5 in [9,19]
X4 in [32,38]
X3 in [1,13]
X2 in [21,26]
X1 in [30,40]
[1

15] [16

20] [21
values

29] [30

42]

Figure 5: Some intervals and the corresponding occ function.
Proof. First, we show that for any optimal solution s  ME(X ), we can produce a solution
s0  ME(RI (X )) that has at least as many equalities as s. Indeed, for any value a, consider
every variable X assigned to this value, that is, such that s[X] = a. Let I  I be the crest
containing a, by definition we have I  D(X 0 ). Therefore we can assign all these variables
to the same value I.
Now we show the opposite, that is, given a solution to the reduced problem, one can build
a solution to the original problem with at least as many equalities. The key observation
is that, for a given crest [a, b], all intervals overlapping with [a, b] have a common value.
Indeed, suppose that this is not the case, that is, there exists [c1 , d1 ] and [c2 , d2 ] both
overlapping with [a, b] and such that d1 < c2 . Then occ(d1 ) > occ(d1 + 21 ) and similarly
occ(c2  12 ) < occ(c2 ). However, since a  d1 < c2  b, [a, b] would not satisfy the conditions
for being a crest, hence a contradiction. Therefore, for a given crest I, and for every variable
X 0 such that s0 [X 0 ] = I, we can assign X to this common value, hence obtaining as many
equalities.
2
We show that this transformation can be achieved in O(n log n) steps. We once again
use the derivative of the occurrence function (occ ), however, defined on Q rather than [1, ]:
1
occ (v)  (|{i | min(Xi ) = v}|  |{i | max(Xi ) = v  }|).
2
Moreover, we can compute it in O(n log n) steps as shown in Algorithm 4. We first compute
the non-null values of occ by looping through each variable X  X (Line 1). We use the
114

fiSoft Constraints of Difference and Equality

same data structure as for Algorithm 1, hence the complexity of this step is O(n log n). Next,
we create the partition into crests by going through the derivative once and identifying the
inflection points. The variable polarity (Line 3) is used to keep track of the evolution of the
function occ. The decreasing phases are denoted by polarity = neg whilst the increasing
phases correspond to polarity = pos. We know that a value v is the end of a crest interval
when the variable polarity switches from neg to pos. Clearly, the number of elements in occ
is bounded by 2n. Recall that the list data structure is sorted. Therefore, going through
the values occ (v) in increasing order of v can be done in linear time, hence the overall
O(n log n) worst-case time complexity.
Algorithm 4: Computing a partition into crests.
Data: A set of variables: X
Result: I
occ  ;
1 foreach X  X do
occ (min(X))  occ (min(X)) + 1;
occ (max(X) + 12 )  occ (max(X) + 12 )  1;
I  ;
min  max  1;
2 while occ 6=  do
3
polarity  pos;
k = 1;
repeat
pick and remove the first element (a, k) of occ ;
max  round(a)  1;
if polarity = pos & k < 0 then polarity  neg;
until polarity = pos or k < 0 ;
add [min, max] to I;
min  max + 1;
return I

Therefore, we can replace every crest by a single value at the preprocessing stage and
then run Algorithm 3. Moreover, observe that the number of crests is bounded by n, since
each needs at least one interval to start and one interval to end. Thus we obtain the
following theorem, where n stands for the number of variables,  for the number of distinct
values, and m for the sum of all domain sizes.
Theorem 5 Enforcing rc on SoftAllEqualmin
can be achieved in O(min(2 , n2 )nm)
G
steps.
Proof. If   n then one can achieve range consistency by iteratively calling Algorithm 3
after assigning each of the O(m) unit assignments ((X, v) X  X , v  D(X)). The
resulting complexity is O(n2 )m (see Theorem 3, the term 3 is absorbed by n2 due to
  n).
Otherwise, if  > n, the same procedure is used, but after applying the reformulation
described in Algorithm 4. The complexity of the Algorithm 4 is O(n log n), and since after
the reformulation we have  = O(n), the resulting complexity is O(n3 m).
2

115

fiHebrard, Marx, OSullivan & Razgon

7. Approximation Algorithm
We have completed the taxonomy of soft global constraints introduced in Section 3. However, in this section and in the rest of the paper we refine our analysis of the problem of
maximising the number of pairs of variables sharing a value, that is, SoftAllEqualmin
G optimisation (Definition 10).
Given a solution s over a set of variable X , we denote by obj(s) the number of equalities
in X .
obj(s) = |{{i, j} | s[Xi ] = s[X[j] & i 6= j}|.
Furthermore, we shall denote as s and obj(s ) an optimal solution and the number of
equalities in this solution, respectively. We first study a natural greedy algorithm for approximating the maximum number of equalities in a set of variables (Algorithm 5). This
algorithm picks the value that occurs in the largest number of domains, and assigns as
many variables as possible to this value (this can be achieved in O(m)). Then it recursively
repeats the process on the resulting sub-problem until all variables are assigned (at most
O(n) times). We show that, surprisingly, this straightforward algorithm approximates the
maximum number of equalities with a factor of 12 in the worst case. Moreover, it can be
implemented to run in O(m) amortised time. We use the following data structures3 :
 var :  7 2X maps every value v to the set of variables whose domains contain v.
 occ :  7 N maps every value v to the number of variables whose domains contain v.
 val : N 7 2 maps every integer i  [0..n] to the set of values appearing in exactly i
domains.
These data structures are initialised in Lines 1, 2 and 3 of Algorithm 5, respectively. Then,
Algorithm 6 recursively chooses the value with largest number of occurrences (Line 2),
makes the corresponding assignments (Line 7) while updating the current state of the data
structures (Loop 3).
Algorithm 5: Computing a lower bound on the maximum number of equalities.
Data: A set of variables: X
Result: An integer E such that obj(s )/2  E  obj(s )
S
1 var(v)  , v  XX D(X);
foreach X  X do
foreach v  D(X) do
add X to var(v);
S
2 occ(v)  |var(v)|, v  XX D(X);
3 val(k)  , k  [0..n];
S
foreach v  XX D(X) do
add v to val(|var(v)|);
return AssignAndRecurse(var, val, occ, n);

Theorem 6 (Algorithm Correctness) Algorithm 5 approximates the optimal satisfying
assignment of the SoftAllEqualG constraint within a factor of 21 and - provided that the
data-structure for representing domains respects some assumptions - runs in O(m).
3. We describe these structures at a lower level in the subsequent proof of complexity.

116

fiSoft Constraints of Difference and Equality

Algorithm 6: procedure AssignAndRecurse of Algorithm 5.

1

2
3
4
5
6
7

Data: A mapping: var :  7 2X , A mapping: val : N 7 2 , A mapping: occ :  7 [0..n], An
integer: k
while val(k) =  do k  k  1;
if k  1 then
return 0;
else
pick and remove any v  val(k);
foreach X  var(v) do if v  D(X) then
foreach w 6= v  D(X) do
remove w from val(occ(w));
occ(w)  occ(w)  1;
add w to val(occ(w));
assign X with v;
return

k(k1)
+AssignAndRecurse(var, val, k);
2

Proof. We first prove the correctness of the approximation ratio, the soundness of the
algorithm and then the complexity of the algorithm.
Approximation Factor. We proceed using induction on the number of distinct values  in the
current subproblem involving all unassigned variables. Let s be the solution computed by
Algorithm 5 and let s be an optimal solution. We denote as P () the proposition If there
are no more than  values in the union of the domains of X , then obj(s)  obj(s )/2. P (1)
implies that every unassigned variable can be assigned to a unique value v. Algorithm 6
therefore chooses this value and assigns all variables to it. In this case obj(s) = obj(s ).
Now we suppose that P () holds and we
S show that P ( + 1) also holds. Let the set of
variables X of the problem be such that | XX D(X)| =  + 1 and let v be the first value
chosen by Algorithm 6. We partition the variables into two subset Xv and Xv depending
on the presence of the value v in their domains.
 Xv = {X  X | v  D(X)} is the set of variables whose domains contain v.
 Xv = X \ Xv is the complementary set of variables which do not contain the value v.
Using these notations, we will partition the equalities into two subsets in order to count
them. The first subset of equalities are those involving at least one variable in Xv , the
second subset are those restricted to variables in Xv .
We first compute a bound on the number of equalities that one can achieve on X . Let
k = |Xv |, let sv be an optimal solution on Xv and let obj(sv ) be the number of equalities
in sv . For each variable X  Xv , given any value w in D(X), there are no more than k
variables in X containing w. Indeed, v was chosen for maximising this criterion and belongs
to the domains of exactly k variables. Therefore, there are at most k(k  1) equalities that
involve at least a variable in Xv , since each one can be involved in at most k  1 equalities,
and there are k of them. Consequently, on the set of variables X , one can achieve at most
k(k  1) + obj(sv ) equalities.
On the other hand, Algorithm 6 assigns every variable in Xv to v and therefore produces
k(k  1)/2 equalities involving at least one variable in Xv . Moreover, observe that since v
does not belong to any domain in Xv , the number of distinct values in Xv is at most .
117

fiHebrard, Marx, OSullivan & Razgon

The induction hypothesis P () can therefore be used, hence we know that the number of
equalities achieved by Algorithm 5 on the subset Xv is at least obj(sv )/2. Consequently, on
the set of variables X , Algorithm 5 achieves at least k(k  1)/2 + obj(sv )/2 equalities.
Since the lower bound on the number of equalities achieved by the greedy algorithm
is half of the upper bound computed above, we can conclude that if P ( + 1) holds, then
P ( + 1) also holds.
Correctness. Here we show that the mappings occ and val are correctly updated in a call to
Algorithm 6. The domain of a variable X changes only when it is assigned to a value v in
Line 7. In that case, the occurrence of every value w  D(X) such that w 6= v is decreased
by one when assigning X to v. Indeed, for every such value w, occ(w) is decremented and
w is removed from val(occ(w) + 1) and added to val(occ(w)).
Complexity. Now we show that Algorithm 5 runs in O(m) steps under the following assumptions:
 The values are consecutive and taken from the set {1, . . . , }.
 Assigning a variable to a value can be done in constant time.
 Checking membership of a value in a variables domain can be done in constant time.
Notice that if the first assumption does not hold, one can rename values. However, it
would require a further O( log ) time complexity to sort them, as well as O(n) to create
a new set of domains.
For every 0  k  n, we use a doubly linked list to represent val(k). Moreover we use a
single array index with  + 1 elements to store the current position of every value v in the
list it appears in (observe that each value appears in exactly one list). To add a value v in
val(k) we simply append it at the tail of the list and set its index to the previous length.
To remove a value v from val(k), we delete the element at position index[v] in val(k). The
total space complexity for this data-structure is therefore O(). For each value v, the set
of variables var(k) is implemented as a simple list, hence a O(m) space complexity. The
mapping occ(v) is represented as an array with one element per value, hence a O() space
complexity.
Initialising all three mappings is done in linear time since each addition requires only
constant time. This step can therefore be achieved in O(m) steps. In Line 1 of Algorithm 6,
k can be decremented at most n times in total, hence Line 2 is executed at most n times in
total.
Observe that no value is chosen more than once in Line 2. Moreover, the total space
complexity of var is O(m). Therefore, the total number of steps in Loop 3 is O(m).
Last, observe that no pair variable/value (X, w) will be explored more than once in
Lines 4, 5 and 6. Indeed, since X is assigned to v in Line 7, it will never pass the condition
in Line 3 since subsequent chosen values will not be equal to v. The overall time complexity
is thus in O(m).
2
Theorem 7 (Tightness of the Approximation Ratio) The approximation factor of
for Algorithm 5 is tight.
118

1
2

fiSoft Constraints of Difference and Equality

Proof. Let {X1 , . . . , X4 } be a set of four variables with domains as follows:
X1  {a}; X2  {b}; X3  {a, c}; X4  {b, c}.
Every value appears in exactly two domains, hence Algorithm 5 can choose any value. We
suppose that the value c is chosen first. At this point no other value can contribute to an
equality, hence Algorithm 5 returns 1. However, it is possible to achieve two equalities with
the following solution: X1 = a, X3 = a, X2 = b, X4 = b.
2

8. Tractable Class
In this section we explore further the connection between the SoftAllEqualmin
constraint
G
and vertex matching. We showed earlier that the general case was linked to 3dMatching.
We now show that the particular case where no value appears in more than two domains
solving the SoftAllEqualG constraint is equivalent to the vertex matching problem on
general graphs, and therefore can be solved by a polynomial time algorithm. We shall then
use this tractable class to show that SoftAllEqualG is NP-hard only if an unbounded
number of values appear in more than two domains.
Definition 11 (The VertexMatching Problem)
Data: An integer K, an undirected graph G = (V, E).
Question: Does there exist M  E such that |M |  K and e1 , e2  M , e1 and e2 do not
share a vertex.
Theorem 8 (Tractable Class of SoftAllEqualmin
G ) If all triplets of variables X, Y, Z 
X are such that D(X)  D(Y )  D(Z) =  then finding an optimal satisfying assignment to
SoftAllEqualmin
is in P .
G
Proof. In order to solve this problem, we build a graph GX = (V, E) with a vertex
xi for each variable Xi  X , that is, V = {xi | Xi  X }. Then for each pair {i, j}
such that D(Xi )  D(Xj ) 6= , we create an undirected edge {i, j}; let E = {{i, j} | i 6=
j & D(Xi )  D(Xj ) 6= }.
We first show that if there exists a matching of cardinality K, then there exists a solution
with at least K equalities. Let M be a matching of cardinality K of GX , for each edge
e = (i, j)  M we assign Xi and Xj to any value v  D(Xi )  D(Xj ) (by construction, we
know that there exists such a value). Observe that no variable is considered twice since it
would mean that two edges of the matching have a common vertex. The obtained solution
therefore has at least |M | equalities.
Now we show that if there exists a solution S with K equalities, then there exists a
matching of cardinality K. Let S be a solution, and let M = {{i, j} | S[Xi ] = S[Xj ]}.
Observe that M is a matching of GX . Indeed, suppose that two edges sharing a vertex
(say {i, j}, {j, k}) are both in M . It follows that S[Xi ] = S[Xj ] = S[Xk ], however this is in
contradiction with the hypothesis. We can therefore compute a solution S maximising the
number of equalities by computing a maximal matching in GX .
2
This tractable class can be generalised by restricting the number of occurrences of values
in the domains of variables. The notion of heavy values is key to this result.
119

fiHebrard, Marx, OSullivan & Razgon

Definition 12 (Heavy Value) A heavy value is a value that occurs more than twice in
the domains of the variables of the problem.
Theorem 9 (Tractable Class with Heavy Values) If the domain D(Xi ) of each variable Xi contains at most one heavy value then finding an optimal satisfying assignment of
SoftAllEqualmin
is in P .
G
Proof. Consider a two stage algorithm. In the first stage, we explore every heavy value
w and assign w to every variable whose domain contains it. Notice that no variable will
be assigned twice. In the second stage, the CSP created by the domains of unassigned
variables consists of only values having at most two occurrences, so we solve this CSP by
transforming it to the matching problem as suggested in the proof of Theorem 8.
We show that there exists an optimal solution where each variable that can be assigned
to a heavy value is assigned to this value. Let s be an optimal solution and w be a heavy
value over a set T of variables of cardinality t. We suppose that only z < t of them are
assigned to w in s . Consider the solution s0 obtained by assigning all these t variables
to w: we add exactly t(t  1)/2  z(z  1)/2 equalities. However, we potentially remove
t  z equalities since values other than w do not appear more than twice. We therefore have
obj(s0 )  obj(s )  t2  3t  z 2 + 3z, which is non-negative for t  3 and z < t. By iteratively
applying this transformation, we obtain an optimal solution where each variable that can
be assigned to a heavy value is assigned to this value. The first stage of the algorithm is
thus correct. The second stage is correct by Theorem 8.
2

9. Parameterised Complexity
We further advance our analysis of the complexity of the SoftAllEqualmin
constraint
G
by introducing a fixed-parameter tractable (FPT) algorithm with respect to the number of
values. This result is important because it shows that the complexity of propagating this
constraint grows only polynomially in the number of variables. It may therefore be possible
to achieve ac at a reasonable computational cost even for a very large set of variables,
provided that the total number of distinct values is relatively small.
We first show that the SoftAllEqualmin
G -optimisation problem is FPT with respect
to the number of values . We use the tractable class introduced in the previous section
to generalise this result, showing that the problem is FPT with respect to the number of
heavy values occurring in domains containing two or more heavy values. We begin with a
definition.
Definition 13 (Solution from a Total Order) A solution s is induced by a total order
 over the values if and only if
s[X] = v  w  v, w 6 D(X).
We now prove the following key lemma.
Lemma 1 There exists a total order  over the set of values, such that the solution s
induced by  is optimal.
120

fiSoft Constraints of Difference and Equality

Proof. Let s be an optimal solution, v be a value, and occ(s , v) be the number of
variables assigned to v in s . Moreover, let occ be a total order such that values are
ranked by decreasing number of occurrences (occ(s , v)) and ties are broken arbitrarily. We
show that occ induces s .
Consider, without loss of generality, a pair of values v, w such that v occ w. By
definition we have occ(s , v)  occ(s , w). We suppose that the hypothesis is falsified and
show that this leads to a contradiction. Suppose that there exists a variable X such that
{v, w}  D(X) and s [X] = w (that is, occ does not induce s ). The objective value
of the solution s0 such that s0 [X] = v and s0 [Y ] = s [Y ] y 6= x is given by: obj(s0 ) =
obj(s ) + occ(s , v)  (occ(s , w)  1). Therefore, obj(s0 ) > obj(s ). However, s is optimal,
hence this is a contradiction.
2
An interesting consequence of Lemma 1 is that searching over the space of total orders on
values is enough to compute an optimal solution. Moreover, the fixed-parameter tractability
of the SoftAllEqualmin
constraint follows easily from the same lemma.
G
Theorem 10 (FPT  number of values) Finding an optimal satisfying assignment of
the SoftAllEqualmin
constraint is fixed-parameter tractable with respect to , the number
G
of values in the domains of the constrained variables.
Proof. Explore all possible ! permutations of values. For each permutation create a
solution induced by this permutation. Compute the cost of this solution. Return the
solution having the highest cost. According to Lemma 1, this solution is optimal. Creating
an induced solution can be done by selecting for each domain the first value in the order.
Clearly, this can be done in O(m). Computing the cost of the given solution can be done by
computing the number of occurrences occ(w) and then summing up occ(w)  (occ(w)  1)/2
for all values w. Clearly, this can be done in O(m) as well. Hence the theorem follows. 2
We can also derive the following corollary from Lemma 1:
Corollary 2 The number of optimal solutions of the CSP with the SoftAllEqualG is at
most !.
Proof. According to Lemma 1, each optimal solution is induced by an order over the values
of the given problem. Clearly each order induces exactly one solution. Thus the number of
optimal solution does not exceed the number of total orders which is at most !.
2
Corollary 2 shows that the number of optimal solutions of the considered problem does
not depend on the number of variables and they all can be explored by considering all
possible orders of values. We believe this fact is interesting from the practical point of
view because in essence it means that even enumerating all optimal solutions is scalable
with respect to the number of variables. Moreover, we can show that SoftAllEqualmin
G
is fixed-parameter tractable with respect to the number of conflicting values, defined as
follows.
Definition 14 (Conflicting Value) A value w of a given CSP is a conflicting value if
and only if it is a heavy value and there is a domain D(X) that contains w and another
heavy value.

121

fiHebrard, Marx, OSullivan & Razgon

Theorem 11 (FPT  number of conflicting values) Let k be the number of conflicting values of a CSP comprising only one SoftAllEqualG constraint. Then the CSP can

be solved in time O(k! n), hence SoftAllEqualmin
is fixed-parameter tractable with
G
respect to k.
Proof. Consider all the permutations of the conflicting values. For each permutation
perform the following two steps. In the first step for each variable X where there are two
or more conflicting values, remove all the conflicting values except the one which is the first
in the order among the conflicting values of D(X) according to the given permutation. In
the second stage we obtain a problem where each domain contains exactly one heavy value.
Solve this problem polynomially by the algorithm provided in the proof of Theorem 9.
Let s be the solution obtained by this algorithm. We show that this solution is optimal.
Let p be a permutation of all the values of the considered CSP so that the solution s
induced by p has the highest possible cost. By Lemma 1, s is an optimal solution. Let p1
be the permutation of the conflicting values which is induced by p and let s1 be the solution
obtained by the algorithm above with respect to p1 . By definition of s, obj(s)  obj(s1 ).
We show that obj(s1 )  obj(s ) from which the optimality of s immediately follows.
Observe that there is no X such that s [X] = w and w was removed from D(X) in the
first stage of the above algorithm where the permutation p1 is considered. Indeed, w can
only be removed from D(X) if it is preceded in p1 by a value v  D(X). It follows that w
is also preceded in p by v and consequently s (X) 6= w. Thus s is a solution of the CSP
obtained as a result of the first stage. However s1 is an optimal solution of that CSP by
Theorem 9 and, consequently, obj(s1 )  obj(s ) as required.
Regarding the runtime, observe that the execution of the algorithm consists of k! running
an algorithm for finding the largest bipartite matching of the given graph. This graph has
n vertices (corresponding to the variables). Moreover, each edge is associated with a value
and no two edges are associated with the same value (because when the matching applies
each value has at most two occurrences). It follows that the graph has at most  edges.

According to Micali and Vazirani (1980), the largest matching can be found in O( n),
hence the upper bound.
2
constraint
This result shows that the complexity of propagating the SoftAllEqualmin
G
comes primarily from the number of (conflicting) values, whereas other factors, such as the
number of variables, have little impact. Notice that detecting conflicting values can be done
in linear time (O(m)), by first counting occurrences of every value, then flagging any value
with at least two occurrences as heavy and finally flagging heavy values as conflicting
in every domain containing at least two of them.
Observe, moreover, the exponential part of this algorithm is based on the exploration
of all possible orders over the given set of conflicting values. In fact the ordering relation
between two values matters only if these values belong to a domain of the same variable.
In other words consider a graph H on values of the given CSP instance. Two values a and
b are connected by an edge if and only if they belong to the domain of the same variable.
Instead of considering all possible orders over the given set of values we may consider all
possible ways of transforming the given graph into an acyclic digraph. The upper bound
on the number of possible transformations is 2E(H) where E(H) is the number of edges of

122

fiSoft Constraints of Difference and Equality

H. For sparse graphs such a bound is much more optimistic that k!. For example, if the
average degree of a vertex is 4 then the number of considered partial orders is 22k = 4k .

10. Finding a Set of Similar or Diverse Solutions
Problems of similarity and diversity have a wide range of applications. Finding several
diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson,
2001) or constraint elicitation (Bessiere, Coletta, Koriche, & OSullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005).
Conversely, similarity is important for problems with a periodic aspect. For instance, a
schedule or timetable may need to be computed on a weekly basis, but the constraints might
change slightly from week to week. In this type of problems the regularity of the solutions,
that is, the similarity between each weeks solution, is a very valuable property (Groer,
Golden, & Wasil, 2009).
Finally, finding similar solutions to a set of variants of a problem can be useful to
find solutions that are robust to uncertainty. Suppose, for example, that we are to solve
a Travelling Salesman Problem (TSP), however, the costs associated with a set of k  1
links between pairs of cities are uncertain or variable over time. We would like to find
an optimal, or near-optimal, route such that when the cost of traversing a link changes, a
limited amount of re-routing is sufficient to obtain another near-optimal solution. For that
purpose, one can build a similar structure as that pictured in Figure 6 by duplicating the
TSP once per uncertain link, the last being the original formulation. In each duplicate, the
cost of the corresponding link is then set to some expected upper bound. If we minimise
the distance between solutions, we obtain a solution with good properties of robustness: if
the cost associated with the ith link increases, the solution of the ith duplicate is a valid
alternative avoiding this link (if it degrades the solution quality too much) whilst requiring
a small amount of re-routing.
We therefore want to find a set of k solutions  either pairwise similar or different  to
a set of k problems, distinct or not. A heuristic method was introduced to solve the problem
of finding k solutions of a constraint network, such that the minimum (resp. maximum)
distance between all pairs of solutions is maximum (resp. minimum) by Hebrard, Hnich,
OSullivan, and Walsh (2005). Since reasoning on the maximum minimum distance is NPhard (Frances & Litman, 1997), it was proposed to use the sum of the Hamming distances
instead. In this section, we first formally define the notion of Hamming distance between
variables and between solutions. Next, we show that the constraints studied in this paper
can help achieve ac and rc in polynomial time for respectively maximising and minimising
the sum of pairwise distances between solutions to a set of problem instances.
10.1 Hamming Distance:
The Hamming distance between the instantiation of two variables X and Y is defined as
follows:

1 iff X 6= Y
h (X, Y ) =
0 otherwise

123

fiHebrard, Marx, OSullivan & Razgon

P1 : (X11 , X21 , X31 , . . . , Xn1 )
P2 : (X12 , X22 , X32 , . . . , Xn2 )
...
Pk : (X1k , X2k , X3k , . . . , Xnk )
Figure 6: The problem P , duplicated k times.
Whereas the Hamming distance between two solutions si and sj (over the sets of variables
{X1i , . . . , Xni } and {X1j , . . . , Xnj }, respectively) is defined as:
X
h (si , sj ) =
h (X`i , X`j )
1`n

Given a problem P with n variables {X1 , . . . , Xn }, we duplicate P k times, with identical
constraints if we seek a set of diverse solutions to P , or altered constraints to model the
expected scenarios if we seek for a set of similar solutions for some variations of P (see
Figure 6).
Then the objective to maximise or minimise is the sum of the pairwise distances between
the (sub-)solutions of the duplicated problems:
X
h (si , sj )
(4)
1i<jk

10.2 Constraint Formulation:
The first approaches to this problem relied on heuristic methods (Hebrard et al., 2005;
Hentenryck, Coffrin, & Gutkovich, 2009), It was also shown that when the problem P allows
it, knowledge compilation methods could efficiently solve this problem (Hadzic, Holland, &
OSullivan, 2009).
Here we show that one can achieve arc or bound consistency for maximising this objective
function. Whilst arc consistency is NP-hard for minimisation, bounds consistency can be
achieved in polynomial time both for minimisation and maximisation. First, we decompose
the objective function described previously (Equation 4) using the SoftAllEqualmin
or
G
SoftAllEqualmax
constraints
for
optimising,
respectively,
solution
similarity
or
diversity.
G
Then we shall see that achieving ac (resp. bc) on this decomposition is equivalent to
achieving ac (resp. bc) on the global constraint defined by bounding the objective.
Remember that each row in Figure 6 represents a duplicate of the original set of variables
{X1 , . . . , Xn }. The objective function is defined as the sum of the Hamming distances
between every pair of rows. However, consider now Figure 6 by vertical slices. Each
column corresponds to the set of duplicates {Xij | 1  j  k} of an original original
variable Xi . One can compute the contribution of this set of variables to the sum of
Hamming distances between pairs of rows as the number of pairwise disequalities in the
set: |{{j, k} | Xij 6= Xik & j < k}|. Notice that this is precisely the definition of the
cost to minimise (resp. maximise) in SoftAllEqualmin
(resp. SoftAllEqualmax
G
G ).
124

fiSoft Constraints of Difference and Equality

Therefore we can model the objective function as the constraint networks shown in Figure 7,
respectively for minimisation and maximisation. Notice that, to simplify the model, we use
the following, equivalent formulation for SoftAllEqualmax
G , rather than Definition 3:
SoftAllEqualmax
G ({X1 , . . . , Xn }, N )  N  |{{i, j} | Xi 6= Xj & i < j}|.

minimise
1  i  n
maximise
1  i  n

P

1in Ni subject to
1
k
SoftAllEqualmin
G (Xi , . . . , Xi , Ni )

P

1in Ni subject to
1
k
SoftAllEqualmax
G (Xi , . . . , Xi , Ni )

Figure 7: A constraint network that minimises (resp. maximises) the sum of distances
between pairs of solutions to k vectors of variables.

Second, notice that the constraint networks depicted in Figure 7 are such that no two
constraints share more than one variable, and there is no Berge-cycle (Berge, 1970) in the
constraint hypergraph, that is, a sequence C1 , X1 , C2 , . . . , Xk , Ck+1 such that:
 X1 , . . . , Xk are distinct variables,
 C1 , . . . , Ck+1 are distinct constraints,
 k  2 and C1 = Ck+1 ,
 Xi is in the scope of Ci and Ci+1 .
Indeed, the SoftAllEqual constraints do not share any variable, and the overlap with the
sum constraint is limited to a single variable with each SoftAllEqual. The constraint
hypergraph is therefore Berge-acyclic, and in such constraint networks it was shown that
propagating ac is sufficient to filter all globally inconsistent values (Janssen & Vilarem,
1988; Jegou, 1991).
Therefore, when every constraint in this network is ac (resp. rc), the network is
globally arc consistent (resp. globally range consistent). We can view these two constraint
networks as two global constraints, respectively CN div and CN sim , over the set variables
{Xij | 1  i  n, 1  j  k} and a variable N to represent the objective:
CN div ({Xij | 1  i  n, 1  j  k}, N ) 
N

X

1
k
Ni & 1  i  n SoftAllEqualmax
G (Xi , . . . , Xi , Ni )

1in

CN sim ({Xij | 1  i  n, 1  j  k}, N ) 
N

X

1
k
Ni & 1  i  n SoftAllEqualmin
G (Xi , . . . , Xi , Ni )

1in

125

fiHebrard, Marx, OSullivan & Razgon

P
Theorems 12 and 13 (where m = 1in |D(Xi1 )| denotes the sum of the domain sizes in
one copy of the problem) follow from, respectively, (van Hoeve, 2004) and Theorem 5:
Theorem 12 Enforcing ac on CN div ({Xij | 1  i  n, 1  j  k}, N ) can be achieved in
O(k 2 m) steps.
Proof. Since the constraint network equivalent to CN div is Berge-acyclic, we know that
it is ac iff every constraint in the decomposition is ac. Moreover, we describe a filtering
algorithm that requires only a bounded number of calls to the propagator of each constraint
in the decomposition.
We assume that no variables domain is completely wiped out during the process. If
it was the case, the process would be interrupted earlier (as soon as an inconsistency is
detected while achieving ac on a component).
We introduce some terminology:
 property (1) denotes the fact that for all 1  i  n the domains of the variables in Xij
are consistent with the upper bounds of Ni ,
 property (2) denotes the fact that for all 1  i  n the domains of the variables in Xij
are consistent with the lower bounds of Ni ,
P
 property (3) denotes the fact that the sum constraint (N 
1in Ni ) is bc (or
equivalently ac).
First, the domain of some variable Xij for 1  i  n and 1  j  k might have changed,
as well as the lower bound of N . A change on the upper bound of N either results on an
immediate failure, or bears no consequences.
1. For every i  [1..n], we update the upper bound of the variable Ni by calling the
procedure proposed by van Hoeve (2004) to find the maximum possible number of
disequalities. Hence property (1) holds.
2. We achieve bc (equivalent to ac in this case) on the sum constraint. Notice that only
the upper bound of N and the lower bounds of Ni for some 1  i  n will be updated,
therefore property (1) and (3) hold.
3. For every i  [1..n], we prune the domains of the variables in {Xij | 1  j  k}
by calling the filtering procedure proposed by van Hoeve (2004). Since this domain
reduction will not trigger any further changes in the bounds of Ni , we know property
(1), (2) and (3) hold, hence CN div is ac.
P
The first phase requires O( 1in k 2 |D(Xi1 )|), that is O(k 2 m) steps. The second phase
requires O(n) steps. Finally, the third phase, like the first, requires O(k 2 m) steps. Hence
an overall O(k 2 m) time complexity.
Theorem 13 Enforcing rc on CN sim ({Xij | 1  i  n, 1  j  k}, N ) can be achieved in
O(k 4 m) steps.

126

fiSoft Constraints of Difference and Equality

Proof.
This proof is very similar to that of Theorem 12, if we swap upper and lower
bounds, and if we use the procedure described in Section 6 for phase (1) and (3).
3
The first phase requires O(k
The second phase requires O(n) steps. Finally,
P n) steps.
the third phase requires O( 1in k 4 |D(Xi1 )|), that is O(k 4 m) steps. Hence an overall
O(k 4 m) time complexity.

11. Conclusion
In many applications we are concerned with stating constraints on the similarity and diversity amongst assignments to variables. To formulate such problems we can use soft
variants of the well known AllDifferent and AllEqual constraints. In this paper we
considered the global constraints AllDifferent and AllEqual, and their optimisation
variants, SoftAllDiff and SoftAllEqual, respectively. Furthermore, we considered
two cost functions, based either on the Hamming distance to a satisfying assignment or
on the number of violations on the decomposition graph. We have shown that the constraint ensuring an upper bound on the Hamming distance with a solution satisfying the
AllEqual constraint can be propagated efficiently, both for arc and bounds consistency.
Then we have shown that, on the one hand, deciding the existence of an assignment minimising the number of violation in the decomposition graph of the AllEqual constraint is
NP-complete, hence propagating arc consistency on the constraint ensuring this property is
NP-hard. On the other hand, propagating bounds consistency on the same constraint can
be done in polynomial time. Moreover, we have shown that this problem is fixed parameter
tractable in the number of distinct values of the problem. This work complements nicely
some earlier results of Cohen et al. (2004) showing that the language of soft binary equality
constraints was NP-complete, for as few as three distinct values in the domains. In this
paper we have shown that the problem remains NP-complete even if the graph of soft binary
equality constraints forms a clique, however, becomes polynomial if the number of values is
bounded.
This paper therefore provides a comprehensive complexity analysis of achieving ac and
bc on an important class of soft constraints of difference and equality. Interestingly, this
taxonomy shows that enforcing equality is harder than enforcing difference.

Acknowledgments
Hebrard, OSullivan and Razgon are supported by Science Foundation Ireland (Grant Number 05/IN/I886). Marx is supported in part by the ERC Advanced grant DMMCA, the
Alexander von Humboldt Foundation, and the Hungarian National Research Fund (Grant
Number OTKA 67651).

References
Aha, D. W., & Watson, I. (Eds.). (2001). Case-Based Reasoning Research and Development,
4th International Conference on Case-Based Reasoning, ICCBR 2001, Vancouver,
BC, Canada, July 30 - August 2, 2001, Proceedings, Vol. 2080 of Lecture Notes in
Computer Science. Springer.

127

fiHebrard, Marx, OSullivan & Razgon

Beldiceanu, N., & Carlsson, M. (2001). Sweep as a Generic Pruning Technique Applied to
the Non-Overlapping Rectangles Constraint. In Walsh, T. (Ed.), Proceedings of the 7th
International Conference on Principles and Practice of Constraint Programming (CP01), Vol. 2239 of Lecture Notes in Computer Science, pp. 377391, Paphos, Cyprus.
Springer-Verlag.
Beldiceanu, N., & Petit, T. (2004). Cost evaluation of soft global constraints. In Regin, J.-C.,
& Rueher, M. (Eds.), Proceedings of the 6th International Conference on Integration
of AI and OR Techniques in Constraint Programming for Combinatorial Optimization
Problems (CPAIOR-04), Vol. 3011 of Lecture Notes in Computer Science, pp. 8095,
Nice, France. Springer-Verlag.
Beldiceanu, N. (2001). Pruning for the Minimum Constraint Family and for the Number
of Distinct Values Constraint Family. In Walsh, T. (Ed.), Proceedings of the 7th
International Conference on Principles and Practice of Constraint Programming (CP01), Vol. 2239 of Lecture Notes in Computer Science, pp. 211224, Paphos, Cyprus.
Springer-Verlag.
Berge, C. (1970). Graphs and Hypergraphs. Dunod.
Bessiere, C., Coletta, R., Koriche, F., & OSullivan, B. (2005). A SAT-Based Version Space
Algorithm for Acquiring Constraint Satisfaction Problems.. In Gama et al. (Gama
et al., 2005), pp. 2334.
Bessiere, C., Hebrard, E., Hnich, B., Kiziltan, Z., & Walsh, T. (2006). Filtering algorithms
for the nvalue constraint. Constraints, 11 (4), 271293.
Cohen, D., Cooper, M., Jeavons, P., & Krokhin, A. (2004). A maximal tractable class of
soft constraints. Journal of Artificial Intelligence Research, 22, 122.
Frances, M., & Litman, A. (1997). On Covering Problems of Codes. Theory of Computing
Systems, 30, 113119.
Gama, J., Camacho, R., Brazdil, P., Jorge, A., & Torgo, L. (Eds.). (2005). Machine Learning: ECML 2005, 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005, Proceedings, Vol. 3720 of Lecture Notes in Computer Science.
Springer.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-completeness. W.H. Freeman and Company.
Groer, C., Golden, B., & Wasil, E. (2009). The Consistent Vehicle Routing Problem.
Manufacturing & Service Operations Management, 11 (4), 630643.
Hadzic, T., Holland, A., & OSullivan, B. (2009). Reasoning about Optimal Collections
of Solutions. In Gent, I. P. (Ed.), Proceedings of the 15th International Conference
on Principles and Practice of Constraint Programming (CP-09), Vol. 5732 of Lecture
Notes in Computer Science, pp. 409423, Lisbon, Portugal. Springer-Verlag.
Hebrard, E., Hnich, B., OSullivan, B., & Walsh, T. (2005). Finding Diverse and Similar
Solutions in Constraint Programming. In Veloso, M. M., & Kambhampati, S. (Eds.),
Proceedings of the 20th National Conference on Artificial Intelligence and the Seventeenth Conference on Innovative Applications of Artificial Intelligence (AAAI-05 /
IAAI-05), pp. 372377, Pittsburgh, PE, USA. AAAI Press / The MIT Press.
128

fiSoft Constraints of Difference and Equality

Hebrard, E., Marx, D., OSullivan, B., & Razgon, I. (2009). Constraints of Difference and
Equality: A Complete Taxonomic Characterisation. In Gent, I. P. (Ed.), Proceedings of
the 15th International Conference on Principles and Practice of Constraint Programming (CP-09), Lecture Notes in Computer Science, pp. 424438, Lisbon, Portugal.
Springer-Verlag.
Hebrard, E., OSullivan, B., & Razgon, I. (2008). A Soft Constraint of Equality: Complexity
and approximability. In Stuckey, P. J. (Ed.), Proceedings of the 14th International
Conference on Principles and Practice of Constraint Programming (CP-08), Lecture
Notes in Computer Science, pp. 358371, Sydney, Australia. Springer-Verlag.
Hentenryck, P. V., Coffrin, C., & Gutkovich, B. (2009). Constraint-based local search for the
automatic generation of architectural tests. In Gent, I. P. (Ed.), Proceedings of the 15th
International Conference on Principles and Practice of Constraint Programming (CP09), Vol. 5732 of Lecture Notes in Computer Science, pp. 787801, Lisbon, Portugal.
Springer-Verlag.
Janssen, P., & Vilarem, M.-C. (1988). Problemes de satisfaction de contraintes: techniques
de resolution et application a la synthese de peptides. C.R.I.M. Research Report..
Jegou, P. (1991). Contribution a letude des problemes de satisfaction de contraintes: algorithmes de propagation et de resolution. Propagation de contraintes dans les reseaux
dynamiques. Ph.D. thesis.
Mehlhorn, K., & Thiel, S. (2000). Faster Algorithms for Bound-Consistency of the Sortedness and the Alldifferent Constraint. In Dechter, R. (Ed.), Proceedings of the 6th International Conference on Principles and Practice of Constraint Programming (CP-00),
Vol. 1894 of Lecture Notes in Computer Science, pp. 306319, Singapore. SpringerVerlag.
p
Micali, S., & Vazirani, V. V. (1980). An o( (|v|)|e|) algorithm for finding maximum matching in general graphs. In FOCS, pp. 1727.
Niedermeier, R. (2006). Invitation to Fixed-Parameter Algorithms. Oxford University Press.
Petit, T., Regin, J.-C., & Bessiere, C. (2000). Meta-constraints on Violations for over
Constrained Problems. In 12th IEEE International Conference on Tools with Artificial
Intelligence (ICTAI-00), pp. 358365.
Petit, T., Regin, J.-C., & Bessiere, C. (2001). Specific Filtering Algorithms for OverConstrained Problems. In Walsh, T. (Ed.), Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP-01), Vol. 2239 of
Lecture Notes in Computer Science, pp. 451463, Paphos, Cyprus. Springer-Verlag.
Petit, T., Regin, J.-C., & Bessiere, C. (2002). Range-Based Algorithm for Max-CSP. In
van Hentenryck, P. (Ed.), Proceedings of the 8th International Conference on Principles and Practice of Constraint Programming (CP-02), Vol. 2470 of Lecture Notes in
Computer Science, pp. 113132, Ithaca, NY, USA. Springer-Verlag.
Quimper, C.-G., Lopez-Ortiz, A., van Beek, P., & Golynski, A. (2004). Improved Algorithms
for the Global Cardinality Constraint. In Wallace, M. (Ed.), Proceedings of the 10th
International Conference on Principles and Practice of Constraint Programming (CP-

129

fiHebrard, Marx, OSullivan & Razgon

04), Vol. 3258 of Lecture Notes in Computer Science, pp. 542556, Toronto, Canada.
Springer-Verlag.
Regin, J.-C. (1994). A Filtering Algorithm for Constraints of Difference in CSPs. In
Hayes-Roth, B., & Korf, R. E. (Eds.), Proceedings of the 12th National Conference on
Artificial Intelligence (AAAI-94), pp. 362367, Seattle, WA, USA. AAAI Press.
Shimazu, H. (2001). Expertclerk: Navigating Shoppers Buying Process with the Combination of Asking and Proposing. In Nebel, B. (Ed.), Proceedings of the 17th International
Joint Conference on Artificial Intelligence (IJCAI-01), pp. 14431450, Seattle, WA,
USA. Morgan Kaufmann.
Smyth, B., & McClave, P. (2001). Similarity vs. diversity.. In Aha, & Watson (Aha &
Watson, 2001), pp. 347361.
van Hoeve, W.-J. (2004). A hyper-arc consistency algorithm for the soft alldifferent constraint. In Wallace, M. (Ed.), Proceedings of the 10th International Conference on
Principles and Practice of Constraint Programming (CP-04), Vol. 3258 of Lecture
Notes in Computer Science, pp. 679689, Toronto, Canada. Springer-Verlag.
van Hoeve, W.-J., Pesant, G., & Rousseau, L.-M. (2006). On Global Warming: Flow-Based
Soft Global Constraints. Journal of Heuristics, 12 (4-5), 347373.

130

fiJournal of Artificial Intelligence Research 41 (2011) pages 2567

Submitted 09/10; published 05/11

Determining Possible and Necessary Winners under
Common Voting Rules Given Partial Orders
Lirong Xia
Vincent Conitzer

lxia@cs.duke.edu
conitzer@cs.duke.edu

Department of Computer Science, Duke University,
Durham, NC 27708, USA

Abstract
Usually a voting rule requires agents to give their preferences as linear orders. However,
in some cases it is impractical for an agent to give a linear order over all the alternatives. It
has been suggested to let agents submit partial orders instead. Then, given a voting rule,
a profile of partial orders, and an alternative (candidate) c, two important questions arise:
first, is it still possible for c to win, and second, is c guaranteed to win? These are the
possible winner and necessary winner problems, respectively. Each of these two problems
is further divided into two sub-problems: determining whether c is a unique winner (that
is, c is the only winner), or determining whether c is a co-winner (that is, c is in the set of
winners).
We consider the setting where the number of alternatives is unbounded and the votes
are unweighted. We completely characterize the complexity of possible/necessary winner
problems for the following common voting rules: a class of positional scoring rules (including
Borda), Copeland, maximin, Bucklin, ranked pairs, voting trees, and plurality with runoff.

1. Introduction
In multiagent systems, often, the agents must make a joint decision in spite of the fact
that they have different preferences over the alternatives. For example, the agents may
have to decide on a joint plan or an allocation of tasks/resources. A general solution to
this problem is to have the agents vote over the alternatives. That is, each agent i gives a
ranking (linear order) i of all the alternatives; then a voting rule takes all of the submitted
rankings as input, and based on this produces a chosen alternative (the winner), or a set
of chosen alternatives. The design of good voting rules has been studied for centuries by
the social choice community. More recently, computer scientists have become interested in
social choicemotivated in part by applications in multiagent systems, but also by other
applications. Hence, a community interested in computational social choice has emerged.
In traditional social choice, agents are usually required to give a linear order over all
the alternatives. However, especially in multiagent systems applications, this is not always
practical. For one, sometimes, the set of alternatives is too large. For example, there are
generally too many possible joint plans or allocations of tasks/resources for an agent to
give a linear order over them. In such settings, agents must use a different voting language
to represent their preferences; for example, they can use CP-nets (Boutilier, Brafman,
Domshlak, Hoos, & Poole, 2004; Lang, 2007; Xia, Lang, & Ying, 2007a, 2007b; Lang &
Xia, 2009). However, when an agent uses a CP-net (or a similar language) to represent
its preferences, this generally only gives us a partial order over the alternatives. Another
c
2011
AI Access Foundation. All rights reserved.

fiXia & Conitzer

issue is that it is not always possible for an agent to compare two alternatives (Pini, Rossi,
Venable, & Walsh, 2007). Such incomparabilities also result in a partial order.
In this paper, we study the setting where for each agent, we have a partial order corresponding to that agents preferences. We study the following two questions. (1) Is it
the case that, for some extension of the partial orders to linear orders, alternative c wins?
(2) Is it the case that, for any extension of the partial orders to linear orders, alternative c
wins? These problems are known as the possible winner and necessary winner problems, respectively, introduced by Konczak and Lang (2005). Depending on the interpretation of c
wins, the possible/necessary winner problems are further divided into two sub-problems:
one is called the possible/neccessary unique winner problem (here unique is often omitted
when causing no confusion), in which c wins means that c is the only winner of the election; the other is called the possible/necessary co-winner problem, in which c wins means
that c is one of the winners. It should be noted that the answer depends on the voting
rule used. Previous research has also investigated the setting where there is uncertainty
about the voting rule; here, a necessary (possible) winner is an alternative that wins for any
(some) realization of the rule (Lang, Pini, Rossi, Venable, & Walsh, 2007). In this paper,
we will not study this setting; that is, the rule is always fixed.
While these problems are motivated by the above observations on the impracticality
of submitting linear orders, they also relate to preference elicitation and manipulation. In
preference elicitation, the idea is that, instead of having each agent report its preferences
all at once, we ask them simple queries about their preferences (e.g. Do you prefer a
to b?), until we have enough information to determine the winner. Preference elicitation has found many applications in multiagent systems, especially in combinatorial auctions (for overviews, see Parkes, 2006; Sandholm & Boutilier, 2006) and in voting settings
as well (Conitzer & Sandholm, 2002, 2005b; Conitzer, 2009). The problem of deciding
whether we can terminate preference elicitation and declare a winner is exactly the necessary winner problem. Manipulation is said to occur when an agent casts a vote that
does not correspond to its true preferences, in order to obtain a result that it prefers. By
the GibbardSatterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975), for any reasonable voting rule, there are situations where an agent can successfully manipulate the rule.
To prevent manipulation, one approach that has been taken in the computational social
choice community is to study whether manipulation is (or can be made) computationally
hard (Bartholdi, Tovey, & Trick, 1989a; Bartholdi & Orlin, 1991; Hemaspaandra & Hemaspaandra, 2007; Elkind & Lipmaa, 2005; Conitzer, Sandholm, & Lang, 2007; Faliszewski,
Hemaspaandra, & Schnoor, 2008; Zuckerman, Procaccia, & Rosenschein, 2009; Xia, Zuckerman, Procaccia, Conitzer, & Rosenschein, 2009; Faliszewski, Hemaspaandra, & Schnoor,
2010). The fundamental questions that have been studied here are Given the other votes,
can a coalition of agents cast their votes so that alternative c wins? (so-called constructive manipulation) and Given the other votes, can this coalition of agents cast their votes
so that alternative c does not win? (so-called destructive manipulation). These problems
correspond to the possible winner problem and (the complement of) the necessary winner
problem, respectively. To be precise, they only correspond to restricted versions of the possible winner problem and (the complement of) the necessary winner problem in which some
of the partial orders are linear orders (the nonmanipulators votes) and the other partial
orders are empty (the manipulators votes). However, if there is uncertainty about parts
26

fiDetermining Possible and Necessary Winners Given Partial Orders

of the nonmanipulators votes, or if parts of the manipulators votes are already fixed (for
example due to preference elicitation), then they can correspond to the general versions of
the possible winner problem and (the complement of) the necessary winner problem.
Another related problem is the evaluation problem (Conitzer et al., 2007). We are
given a probability distribution over each voters vote, and we are asked for the probability
that a given alternative wins. It has been shown that for any anonymous voting rule, when
the number of alternatives is no more than a constant, there is a polynomial-time algorithm
that solves the evaluation problem; when the number of alternatives is not bounded
above by a constant, the problem becomes #P hard for plurality, Borda, and Copeland
rules (Hazon, Aumann, Kraus, & Wooldridge, 2008). The complexity of influencing the
distribution over the voters votes on multiple binary issues to make a given alternative
(a valuation of all these issues) win has also been studied (Erdelyi, Fernau, Goldsmith,
Mattei, Raible, & Rothe, 2009). The possible/necessary winner problems are related to the
evaluation problem in the following way. If every voter assigns positive probability to
every one of the linear orders that extend her partial order, then, for any alternative c, c
is a possible winner if and only if the probability that c wins the election is positive; c is
a necessary winner if and only if the probability that c wins the election is 1. We must
note that this reduction from the possible/necessary winner problem to the evaluation
problem is in general not polynomial, because for any partial order, it is possible that
there are exponentially many linear orders that extend it. For example, if the partial
order is empty, then any linear order is an extension of it. However, in this paper, we
prove results that show that the possible/necessary winner problem is hard even when the
number of undetermined pairs in each partial order is a constant, so that there are in fact
only polynomially many linear orders that extend it. Hence, our hardness results also imply
(only NP-)hardness results for the evaluation problem.
Because of the variety of different interpretations of the possible and necessary winner
problems, it is not surprising that there have already been significant studies of these problems. Two main settings have been studied (see Walsh, 2007 for a good survey). In the first
setting, the number of alternatives is bounded, and the votes are weighted. Here, for the
Borda, veto, Copeland, maximin, STV, and plurality with runoff rules, the possible winner
problem is NP-complete; for the STV and plurality with runoff rules, the necessary winner
problem is coNP-complete (Conitzer et al., 2007; Pini et al., 2007; Walsh, 2007). However,
in many elections, votes are unweighted (that is, each agents vote counts the same). If
the votes are unweighted, and the number of alternatives is bounded, then the possible and
necessary winner problems can always be solved in polynomial time, assuming the voting
rule can be executed in polynomial time (Conitzer et al., 2007; Walsh, 2007). Hence, the
other setting that has been studied is that where the votes are unweighted and the number
of alternatives is not bounded; this is the setting that we will study in this paper. In this setting, the possible and necessary winner problems are known to be hard for STV (Bartholdi
& Orlin, 1991; Pini et al., 2007; Walsh, 2007). Computing whether an alternative is a
possible or necessary Condorcet winner can be done in polynomial time (Konczak & Lang,
2005). However, at the time of the conference version of this work (Xia & Conitzer, 2008),
for most of the other common rules, there were no prior results (except for the fact that the
27

fiXia & Conitzer

problems are easy for many of these rules when each partial order is either a linear order
or empty, that is, the standard manipulation problem).1
1.1 Our Contributions
In this paper, we characterize the complexity of the possible and necessary winner problems
for some of the most important other rulesspecifically, a class of positional scoring rules,
Copeland, maximin, Bucklin, ranked pairs, voting trees, and plurality with runoff. We
show that the possible winner problems are NP-complete for all these rules except the
possible unique winner problem with respect to plurality with runoff. We also show that the
necessary winner problems are coNP-complete for the Copeland, ranked pairs, and voting
trees; and the necessary co-winner problem is coNP-complete for plurality with runoff. For
the remaining cases, we present polynomial-time algorithms. Our results are summarized
in Table 1.

STV
Plurality
Veto
Pos. scoring
(incl. Borda, k-approval)

Copeland
Maximin
Bucklin
Ranked pairs
Voting trees
(incl. balanced trees)

Plu. w/ runoff

Possible Winner
NP-complete
(Bartholdi & Orlin, 1991)
P2
P3
NP-complete

4

NP-complete
NP-complete
NP-complete
NP-complete

4

NP-complete

4

4
4
4

NP-complete (unique winner)
P (co-winner)

Necessary Winner
coNP-complete
(Bartholdi & Orlin, 1991)
P2
P3
P
coNP-complete
P
P
coNP-complete

4

coNP-complete

4

4

P (unique winner)
coNP-complete (co-winner)

4

Table 1: Summary of complexity of possible/necessary winner problems with respect to
common voting rules. Unless otherwise mentioned, the results do not depend on
whether we consider the unique-winner or the co-winner version of the problem.

1. An earlier paper (Konczak & Lang, 2005) studied these problems for positional scoring rules, and claimed
that the problems are polynomial-time solvable for positional scoring rules; however, there was a subtle
mistake in their proofs. We will show that the possible winner problem is in fact NP-complete for some
positional scoring rules. We will also give a correct proof that the necessary winner problem is indeed
polynomial-time solvable for all positional scoring rules.
2. Easy to prove; also proved in the work of Betzler and Dorn (2010), and follows from the bribery algorithm
by Faliszewski (2008).
3. Easy to prove, also proved in the work of Betzler and Dorn (2010).
4. Hardness results hold even when the number of unknown pairs in each partial order is no more than a
constant.

28

fiDetermining Possible and Necessary Winners Given Partial Orders

This paper is a significant extension of the conference version of this work (Xia &
Conitzer, 2008): this extended version includes all the proofs, and the results on voting trees,
plurality with runoff, and k-approval are new. The conference version also did not mention
plurality and veto; these results are easy and follow from known results, as explained in the
footnotes under the table.
1.2 Subsequent Work since the Conference Version
Since the conference version of this work, the complexity of the possible winner problem
with respect to any positional scoring rule has been fully characterized (Betzler & Dorn,
2010; Baumeister & Rothe, 2010). By the theorems of Betzler and Dorn (2010), the possible
winner problem is NP-complete with respect to Borda and k-approval. Still, these hardness
results do not directly imply the hardness results obtained for positional scoring rules in
this paperwe prove that the hardness results for Borda and k-approval hold even when
the number of undetermined pairs in each vote is no more than 4.
Also, a special case of the possible and necessary winner problems where new alternatives
join the election after the voters preferences over the initial alternatives have been fully
revealed has been proposed and studied in the work of Chevaleyre, Lang, Maudet, and
Monnot (2010). It has been shown that the possible-winner-with-new-alternatives problem
is NP-complete for maximin, Copeland (Xia, Lang, & Monnot, 2011), and k-approval when
k  3 and there are at least 3 new alternatives (Chevaleyre, Lang, Maudet, Monnot, & Xia,
2010); the problem is in P for Bucklin (when k  3) (Xia, Lang, & Monnot, 2011), Borda,
and k-approval (when k  2 or there are no more than two new alternatives) (Chevaleyre,
Lang, Maudet, Monnot, & Xia, 2010).
Meanwhile, a number of new results on the complexity of the unweighted coalitional
manipulation problem have also been obtained. Specifically, the unweighted coalitional
manipulation problem has been shown to be NP-hard for Copeland for any 0    1
(except for  = 12 ; these results even hold with two manipulators) (Faliszewski et al., 2008,
2010),5 maximin (two manipulators) and ranked pairs (one manipulator) (Xia et al., 2009),
and a specific positional scoring rule (two manipulators) (Xia, Conitzer, & Procaccia, 2010).
As we mentioned before, the unweighted coalitional manipulation problem is a special case
of the possible winner problem studied in this paper (where some partial orders are linear
orders and the others are empty); as a result, NP-hardness results for the unweighted
coalitional manipulation problem also imply NP-hardness of the possible winner problem
for these rules. We note that the NP-hardness results proved in this paper (except the
possible unique winner problem for plurality with runoff) hold even when for each partial
order, the number of pairs of alternatives for which the order is unknown is a constant.
Therefore, the subsequent research on the unweighted coalitional manipulation does not
completely imply the NP-hardness results that we prove in this paper for the possible
winner problem for Copeland, maximin, ranked pairs, and positional scoring rules.
Elkind et al. (2009) showed that the possible winner problem also reduces to the swap
bribery problem, in which an interested party can pay voters to swap adjacent alternatives
5. Faliszewksi et al. (2008) also study the case of weighted coalitional manipulation with three alternatives
for Copeland, and show that how hard this problem is depends both on  and whether we consider the
unique-winner or the co-winner variant of the problem. We do not study weighted votes in this paper.

29

fiXia & Conitzer

in their rankings, but the price to swap two alternatives depends on both the identity of
the alternatives and the identity of the voter. That is, (with respect to a fixed voting rule)
the computational complexity of the swap bribery problem is at least as high as that of the
possible winner problem, in terms of polynomial-time reductions.
The complexity of the possible winner problem has also been studied from a fixedparameter tractability perspective, for parameters such as the number of alternatives, the
number of voters, and the number of unknown pairs in each vote (Betzler, Hemmann, &
Niedermeier, 2009). Finally, the counting version of the possible winner problem has also
been studied (Bachrach, Betzler, & Faliszewski, 2010).

2. Preliminaries
Let C = {c1 , . . . , cm } be the set of alternatives (or candidates). A linear order on C is
a transitive, antisymmetric, and total relation on C. The set of all linear orders on C is
denoted by L(C). An n-voter profile P on C consists of n linear orders on C. That is,
P = (V1 , . . . , Vn ), where for every i  n, Vi  L(C). The set of all profiles on C is denoted
by P (C). In the remainder of the paper, m denotes the number of alternatives and n denotes
the number of voters.
A voting rule r is a function from the set of all profiles on C to the set of (nonempty)
subsets of C, that is, r : P (C)  2C \ . The following are some common voting rules.
1. (Positional) scoring rules: A positional scoring rule is defined by a scoring vector
~sm = (~sm (1), . . . , ~sm (m)) of m non-negative integers, where ~sm (1)      ~sm (m).
For any vote V  L(C) and any c  C, let s(V, c) = ~smP
(j), where j is the rank of
c in V . For any profile P = (V1 , . . . , Vn ), let s(P, c) = ni=1 s(Vi , c). The rule will
select c  C so that s(P, c) is maximized. Some examples of positional scoring rules
are Borda, for which the scoring vector is (m  1, m  2, . . . , 0), plurality, for which
the scoring vector is (1, 0, . . . , 0), veto, for which the scoring vector is (1, . . . , 1, 0), and
k-approval (1  k  m  1), for which the scoring vector is (1, . . . , 1, 0, . . . , 0). In this
| {z }
k

paper, we assume that the scoring vector can be computed in polynomial time.
2. Copeland: For any two alternatives ci and cj , we can simulate a pairwise election
between them, by seeing how many votes rank ci ahead of cj , and how many rank cj
ahead of ci . ci wins if and only if the majority of votes rank ci ahead of cj . Then,
an alternative receives one point for each win in a pairwise election. (Typically, an
alternative also receives half a point for each pairwise tie, but this will not matter for
our results.) A winner is an alternative who has the highest score.
3. Maximin (a.k.a. Simpson): Let NP (ci , cj ) denote the number of votes that rank ci
ahead of cj in the profile P . A winner is an alternative c that maximizes min{NP (c, c ) :
c  C, c 6= c}.
4. Bucklin: An alternative cs Bucklin score is the smallest number k such that more
than half of the votes rank c among the top k alternatives. A winner is an alternative
who has the smallest Bucklin score. (Sometimes, ties are broken by the number of
30

fiDetermining Possible and Necessary Winners Given Partial Orders

votes that rank an alternative among the top k position, but for simplicity we will
not consider this tiebreaking rule here.)
5. Ranked pairs: This rule first creates an entire ranking of all the alternatives. NP (ci , cj )
is defined as for the maximin rule. In each step, we will consider a pair of alternatives
ci , cj that we have not previously considered; specifically, we choose the remaining
pair with the highest NP (ci , cj ). We then fix the order ci > cj , unless this contradicts
previous orders that we fixed (that is, it violates transitivity). We continue until we
have considered all pairs of alternatives (hence we have a full ranking). The alternative
at the top of the ranking wins.
6. Voting trees: A voting tree is a binary tree with m leaves, where each leaf is associated
with an alternative. In each round, there is a pairwise election between an alternative
ci and its sibling cj : if the majority of voters prefer ci to cj , then cj is eliminated,
and ci is associated with the parent of these two nodes; similarly, if the majority of
voters prefer cj to ci , then ci is eliminated, and cj is associated with the parent of
these two nodes. The alternative that is associated with the root of the tree (wins all
its rounds) wins. Balanced voting trees are also known as cup, knockout tournaments
or single-elimination tournaments.
7. Plurality with runoff: The rule has two steps. In the first step, all alternatives except
the two that are ranked in the top position for most times are eliminated, and the
votes transfer to the second round, in which the plurality rule (a.k.a. majority rule in
case of two alternatives) is used to select the winner.
8. Single transferable vote (STV): The election has m rounds. In each round, the alternative that gets the minimal plurality score drops out, and is removed from all of
the votes (so that votes for this alternative transfer to another alternative in the next
round). The last-remaining alternative is the winner.
Given a profile P , the pairwise score difference DP (c, c ) of alternatives c and c is defined
as follows.
DP (c, c ) = NP (c, c )  NP (c , c)
The subscript P is omitted when there is no risk of confusion. For a linear order V over
C, we let DV denote the pairwise score difference function of the profile that consists of a
single vote V . That is, DV = D{V } . It follows from the definition that D(c, c ) = D(c , c).
We note that although maximin, ranked pairs, and voting trees are based on pairwise scores,
they can also be computed by pairwise score differences in the same way, because for any
profile P of n votes, and any pair of alternatives (c, c ), we have DP (c, c ) = 2NP (c, c )  n.
We adopt the parallel-universes tiebreaking (Conitzer, Rognlie, & Xia, 2009) to define
the winning alternatives for the rules that have multiple rounds (i.e., ranked pairs, voting
trees, plurality with runoff, and STV). That is, an alternative c is a winner if and only if
there exists a way to break ties in all of the steps such that c is the winner. For example,
an alternative c is a winner for a voting tree, if there exists a way to break ties in the
pairwise elections in the voting process, such that c wins. A partial order on C is a reflexive,
transitive, and antisymmetric relation on C. We say a linear order V extends a partial order
O if O  V .
31

fiXia & Conitzer

Definition 1 A linear order V on C extends a partial order O on C if for every pair of
alternatives c, c  C, c O c  c V c .
Throughout the paper we use the following notation. Let V denote a linear order over C;
let O denote a partial order over C; let P denote a profile of linear orders; let Pposet denote
a profile of partial orders.

3. Possible/Necessary Winners
We are now ready to define possible (necessary) winners, which were first introduced by
Konczak and Lang (2005).
Definition 2 Given a profile of partial orders Pposet = (O1 , . . . , On ) on C, we say that an
alternative c  C is: (1) a possible winner if there exists P = (V1 , . . . , Vn ) such that each Vi
extends Oi , and r(P ) = {c}; (2) a necessary winner if for every P = (V1 , . . . , Vn ) such that
each Vi extends Oi , r(P ) = {c}; (3) a possible co-winner if there exists P = (V1 , . . . , Vn ) such
that each Vi extends Oi , and c  r(P ); (4) a necessary co-winner if for any P = (V1 , . . . , Vn )
such that each Vi extends Oi , c  r(P ).
Example 1 Let there be three alternatives {c1 , c2 , c3 }. Three partial orders are illustrated
in Figure 1. Let Pposet = (O1 , O2 , O3 ). c1 is a possible (co-)winner of Pposet with respect to
plurality, because we can complete O1 by adding c2  c3 , complete O2 by adding c1  c2 ,
and complete O3 by adding c1  c2 and c1  c3 ; then, c1 is the only winner. However, c1 is
not a necessary (co-)winner, because we can complete O1 by adding c2  c3 , complete O2 by
adding c2  c1 , and complete O3 by adding c2  c1 and c1  c3 ; then, c2 is the only winner.
O1

c2

c1

O2

O3
c1

c3

c1
c3

c2

c2

c3

Figure 1: Partial orders.

However, if we let Pposet
= (O1 , O1 , O2 ), then c1 is the (only) necessary winner, because
c1 will be ranked first in at least two votes.

Now, we define the computational problems studied in this paper:
Definition 3 Define the problem Possible Winner (PW) with respect to voting rule r to be:
given a profile Pposet of partial orders and an alternative c, we are asked whether or not c
is a possible winner for Pposet with respect to r.
Necessary Winner (NW), Possible co-Winner (PcW), and Necessary co-Winner (NcW)
are defined similarly.
A natural first question is how these problems are related to each other. It turns out
that (holding the voting rule fixed) there exists a polynomial-time Turing reduction from
NW to PcW. That is, if PcW is in P, then NW is also in P.
Proposition 1 For any voting rule r, if computing PcW with respect to r is in P, then
computing NW with respect to r is also in P .
32

fiDetermining Possible and Necessary Winners Given Partial Orders

Proof. Because r never outputs , an alternative c is a necessary unique winner with respect
to r if and only if for every alternative d (d 6= c), d is not a possible co-winner. Therefore, if
we have a polynomial-time algorithm that solves the PcW problem with respect to r, then
to solve the NW problem, we simply run the algorithm for every alternative d (d 6= c). If
any d 6= c is a possible co-winner, then we output that c is not a necessary unique winner;
otherwise, we output that c is a necessary unique winner.
2
There is no similar relationship between the PW and NcW problems. It is true that if
some alternative d (d 6= c) is a possible unique winner, then c is not a necessary co-winner.
However, it is possible that even if no alternative d (d 6= c) is a possible unique winner, c
is still not a necessary co-winner. For example, let Pposet = (c2  c3  c1 , c3  c2  c1 ).
Because Pposet is already composed of linear orders, it has only one extension (itself). It
follows that there is no possible unique winner for Pposet with respect to plurality, but clearly
c1 is not a necessary co-winner. More generally, we have the following proposition, which
says that for any pair of different problems X, Y  {PW, PcW, NW, NcW}, the answer to
Y cannot be computed from only the answers to X for all alternatives, unless X =NW and
Y =PcW. (This holds even when the rule is plurality).
Proposition 2 Suppose m  3. Let X, Y  {P W, P cW, N W, N cW } be such that (1) X6=Y
and (2) X6=PcW or Y6=NW. There exist two profiles Pposet and Pposet of partial orders (with
|Pposet | = |Pposet |), such that (1) for every alternative c, the answers to X with respect to
plurality are the same for both Pposet and Pposet , and (2) there exists an alternative d for
which the answers to Y with respect to plurality for Pposet and Pposet are different.
Proof. The proof is by construction. For any partial order O on C, we let T op(O) denote
the set of alternatives c for which there exists at least one extension of O where c is in the
top position. For any set C   C, we define OC  to be an arbitrary partial order such that
T op(OC  ) = C  . For simplicity, we write Oc for O{c } . For example, when m = 3, we can let
Oc1 = O{c1 } = c1  c2  c3 , that is, Oc1 is a linear order. As another example, O{c1 ,c2 } is
the partial order that is obtained from [c1  c2  c3 ] by removing c1  c2 .
Let d = c1 . We next specify the profiles Pposet and Pposet for the following (exhaustive)
list of cases (X, Y ).
 (PW, NW) and (PW, NcW). (1) Let Pposet be composed of 5 copies of Oc1 . c1 is a
necessary unique/co-winner. (2) Let Pposet = (Oc1 , Oc2 , Oc2 , O{c1 ,c3 } , O{c1 ,c3 } ). c1 is
the unique winner for one extension, {c1 , c2 } are the winners for two extensions, and
{c2 , c3 } are the winners for one extension. Therefore, c1 is not a necessary unique/cowinner. We note that c1 is the only possible unique winner for both profiles.
 (PW, PcW). (1) Let Pposet = (Oc1 , Oc2 ). {c1 , c2 } are the winners for the only extension, which means that c1 is a possible co-winner. (2) Let Pposet = (Oc2 , Oc3 ).
{c2 , c3 } are the winners for the only extension, which means that c1 is not a possible
co-winner. We note that there is no possible unique winner for either profile.
 (NcW, NW). (1) Let Pposet = (Oc1 , Oc1 ). c1 is the only winner in the only extension,
which means that c1 is the necessary unique winner. (2) Let Pposet = (Oc1 , O{c1 ,c2 } ).
c1 is the only winner for one extension, and {c1 , c2 } are the winners for the other
33

fiXia & Conitzer

extension, which means that c1 is not the necessary unique winner. We note that c1
is the only necessary co-winner for both profiles.
 (PcW, PW) and (PcW, NcW). (1) Let Pposet = (O{c1 ,c2 } , O{c1 ,c2 } ). c1 is the unique
winner for one extension, c2 is the unique winner for one extension, and {c1 , c2 } are the
winners for two extensions. Therefore, c1 is a possible unique winner (meanwhile, c1
is not a necessary co-winner). (2) Let Pposet = (Oc1 , Oc2 ). {c1 , c2 } are the winners for
the only extension, which means that c1 is not a possible unique winner (meanwhile,
c1 is a necessary co-winner). We note that c1 and c2 are the only possible co-winners
for both profiles.
 (NW, PW), (NW, PcW), (NcW, PW), (NcW, PcW). (1) Let Pposet = (O{c1 ,c2 } , O{c1 ,c2 } ).
c1 is the unique winner for one extension, c2 is the unique winner for one extension,
and {c1 , c2 } are the winners for two extensions. Therefore, c1 is a possible unique/cowinner. (2) Let Pposet = (O{c2 ,c3 } , O{c2 ,c3 } ). c2 is the unique winner for one extension,
c3 is the unique winner for one extension, and {c2 , c3 } are the winners for two extensions. Therefore, c1 is not a possible unique/co-winner. We note that there is no
necessary unique/co-winner for either profile.
 (NW, NcW). (1) Let Pposet = (Oc1 , Oc2 ). {c1 , c2 } are the winners for the only extension, which means that c1 is a necessary co-winner. (2) Let Pposet = (Oc2 , O{c1 ,c2 } ).
{c1 , c2 } are the winners for one extension, and c2 is the unique winner for the other
extension, which means that c1 is not a necessary co-winner. We note that there is
no necessary unique winner for either profile.
2

4. Hardness Results
In this section, we prove that PW (PcW) is NP-complete with respect to a class of positional
scoring rules, Copeland, maximin, Bucklin, ranked pairs, and voting trees; NW (NcW) is
coNP-complete with respect to Copeland, ranked pairs, and voting trees; and PW is NPcomplete and NcW is coNP-complete with respect to plurality with runoff. For positional
scoring rules, we will not show that PW is hard for all positional scoring rulesin fact, for
plurality and veto, PW is easy; rather, we will give a sufficient condition on a positional
scoring rule under which PW is hard. Most notably, Borda satisfies this condition. kapproval does not satisfy this condition, and we will provide a distinct proof for PW (PcW)
with respect to k-approval (k  2).6 Similarly for voting trees, we provide a necessary
condition under which the hardness results hold, and most notably, balanced voting trees
satisfy this condition. All of these results (except the one for PW with respect to plurality
with runoff) hold even when the partial orders are almost linear orders. That is, the
number of undetermined pairs in each partial order is bounded above by a constant.
6. After the conference version of this paper (Xia & Conitzer, 2008), Betzler and Dorn proved a dichotomy
theorem for possible winner problems with respect to positional scoring rules (Betzler & Dorn, 2010).
According to their theorem, PW with respect to k-approval (k  2) is NP-complete. In this paper, we
prove that the problem is NP-complete, even when the number of undetermined pairs in each vote is no
more than 4.

34

fiDetermining Possible and Necessary Winners Given Partial Orders

All the hardness results are proved by reductions from the exact 3-cover (X3C)
problem, except for the result for k-approval, which is proved by a reduction from 3-SAT.
X3C and 3-SAT are known to be NP-complete (Garey & Johnson, 1979). The two problems
are defined as follows.
Definition 4 (X3C) We are given a set V = {v1 , . . . , vq } and a collection S = {S1 , . . . , St },
where for each i  t, Si = {vl(i,1) , vl(i,2) , vl(i,3) }  V, with 1  l(i, 1), l(i, 2), l(i, 3)  q. We
are asked whether we can cover all of the elements in V with non-overlapping sets in S.
Definition 5 (3-SAT) We are given a formula in conjunctive normal form: F = C1 
. . .  Ct over binary variables x1 , . . . , xq , where for any j  t, Cj is called a clause. For any
j  t, Cj = lj1  lj2  lj3 , where for any   {1, 2, 3}, lj is called a literal, and there exists
i  q such that either lj = xi or lj = xi . We are asked whether there exists a valuation
of the variables under which F is true.
In each proof, the election instance that we construct from an arbitrary X3C (or 3-SAT)
instance consists of two parts. The first part is a set of partial orders that encode the X3C
(or 3-SAT) instance.7 For example, in some of our PW reductions from X3C, the first part
is structured as follows: in order for c to win, there is an alternative c that needs to be
placed in a high position in the extensions of the partial orders at least some number
of times. However, for each of the partial orders, there is a set of three alternatives such
that if we put c in a high position in an extension of that partial order, then these three
alternatives must be ranked in even higher positions (that is, c pushes up these three
alternatives in the extension). These sets of three alternatives that must sometimes be
pushed up correspond to the sets of three elements in the X3C instance. The PW instance
is set up in such a way that if the same X3C-element alternative is pushed up by c in two
different votes in the first part, then c cannot win. Thus, the sets of alternatives that we
push up must be disjoint, and the instance is set up in such a way that we need to put c in
a high position often enough that the pushed-up 3-sets actually must constitute an exact
cover. The second part is a set of linear orders (that is, in the second part, everything is
determined) whose purpose is, informally stated, to adjust the scores of the alternatives so
that we get the properties just described.
First we introduce some notation to represent the set of all pairwise comparisons in a
linear order.
Definition 6 For any set {a1 , . . . , al }, let O(a1 , . . . , al ) = {(ai , aj ) : i < j}.
That is, O(a1 , . . . , al ) is the set of all ordered pairs consistent with the linear order a1 
. . .  al . For example, O(a, b, c) = {(a, b), (b, c), (a, c)}. The following notation will be
frequently used in the proofs.
Definition 7 For any set A and any partition A1 , . . . , Ak of A, let O(A1 , . . . , Ak ) denote
an arbitrary linear order on A that is consistent with A1  A2  . . .  Ak .
7. Typically, we define the partial orders by first defining some linear orders and then removing some of
the pairwise ordering constraints.

35

fiXia & Conitzer

The proofs that make use of this notation only use the fact that O(A1 , . . . , Ak ) is consistent
with A1  . . .  Ak , so that the order within each Ai (i  k) does not matter. For example,
let A = {a, b, c, d}, A1 = {a}, A2 = {b, c}, A3 = {d}. There are two linear orders that are
consistent with A1  A2  A3 . They are a  b  c  d and a  c  b  d. O(A1 , A2 , A3 )
can denote either of them, e.g., O(A1 , A2 , A3 ) = a  b  c  d. Sometimes we use the
notation Others to denote the set of all objects that are not mentioned in the context.
For example, O(A1 , A2 , A3 ) = O(Others, A2 , A3 ) = O(A1 , Others, A3 ) = O(A1 , A2 , Others).
Usually, a positional scoring rule is defined for a fixed number of alternatives (that is,
m is fixed). If we hold m fixed, then there exist polynomial-time algorithms for both PW
and NW (Walsh, 2007; Conitzer et al., 2007). However, there are positional scoring rules
that are defined for any number of alternativesfor example, Borda, plurality, and veto.
For such positional scoring rules, the number of alternatives is not bounded, and indeed, we
will prove that PW is not always easy with respect to such rules. To study the complexity
of social choice problems that involve a growing number of alternatives, it is necessary to
associate a scoring vector with every natural number of alternatives. In the remainder of the
paper, a positional scoring rule r consists of a sequence of scoring vectors {~s1 , ~s2 , . . .} such
that for each i  N, ~si is the scoring vector for i alternatives. The next theorem provides a
sufficient condition on a positional scoring rule for PW to be NP-complete. In this paper,
all the PW/PcW problems are in NP, and all the NW/NcW problems are in coNP. This
follows from the fact that, given an extension of the partial orders to linear orders, we can
compute the winner(s) in polynomial-time for the rules studied in this paper. With this
in mind, we only prove the hardness direction in the NP-completeness/coNP-completeness
proofs. There do exist rules for which computing the winner(s) is NP-hard, for example,
Dodgsons rule (Bartholdi, Tovey, & Trick, 1989b; Hemaspaandra, Hemaspaandra, & Rothe,
1997) and Youngs rule (Rothe, Spakowski, & Vogel, 2003), but we will not study any rules
for which computing the winners is hard here.
Theorem 1 Let r be a positional scoring rule with scoring vectors {~s1 , ~s2 , . . .}. Suppose
there exists a polynomial function f (x) such that for any x  N, there exist l and k with
x  l  f (x) and k  l  4, and satisfy the following conditions:
(1) ~sl (k)  ~sl (k + 1) = ~sl (k + 1)  ~sl (k + 2) = ~sl (k + 2)  ~sl (k + 3) > 0,
(2) ~sl (k + 3)  ~sl (k + 4) > 0,
Then, PW and PcW are both NP-complete with respect to r, even when the number of
undetermined pairs in each vote is no more than 4.
Proof. Given an X3C instance V = {v1 , . . . , vq }, S = {S1 , . . . , St }, let q + 3  l  f (q + 3)
(where q is the number of elements in the X3C instance) satisfy the two conditions in
the assumption, and let k  l  4 satisfy ~sl (k)  ~sl (k + 1) = ~sl (k + 1)  ~sl (k + 2) =
~sl (k + 2)  ~sl (k + 3) > 0, and ~sl (k + 3)  ~sl (k + 4) > 0. Let K1 = ~sl (k)  ~sl (k + 1) and
K2 = ~sl (k + 3)  ~sl (k + 4). We construct the PW instance as follows.
Alternatives: C = {c, w, d}  V  A, where d and A = {a1 , . . . , alq3 } are auxiliary
alternatives.
First part (P1 ) of the profile: For each j  t, choose an arbitrary set Bj  C\(Si {w, d})
36

fiDetermining Possible and Necessary Winners Given Partial Orders

with |Bj | = k  1. We define a partial order Oj as follows.
Oj = O(Bj , w, Si , d, Others) \ [{w}  (Sj  {d})]
That is, Oj is a partial order that agrees with Bj  w  Sj  d  Others, except that the
pairwise relations between (w, Sj ) and (w, d) are not determined (and these are the only 4
undetermined relations). Let P1 = {O1 , . . . , Ot }.
Second part (P2 ) of the profile: We first give the properties that we need P2 to satisfy;
we will show how to construct P2 in polynomial time later in the proof. All votes in P2
are linear orders. Let P1 = {O(Bj , w, Sj , d, Others) : j  t}. That is, P1 (|P1 | = t) is an
extension of P1 (in fact, P1 is the set of linear orders that we started with to obtain P1 ,
before removing some of the pairwise relations). P2 is a set of linear orders such that the
following holds for Q = P1  P2 :
(1) For every i  q, ~sl (Q, c)  ~sl (Q, vi ) = 2K1 , ~sl (Q, w)  ~sl (Q, c) =

q
3

 (3K1 + K2 )  K2 .

(2) For every i  q, the scores of vi and w, c are higher than those of the other alternatives
in any extension of P1  P2 .
(3) P2 s size is polynomial in t + q.
Suppose there exists an extension P1 of P1 such that c is the winner for P1  P2 . For each
i  q, vi is not ranked higher than w more than once in P1 , because otherwise the total
score of vi will be higher than or equal to the total score of c. We recall that the score
difference between w and c in P1  P2 is 3q  (3K1 + K2 )  K2 . Therefore, if there exists j  t
such that in the extension of Oj , w is ranked above c, and is ranked below some alternative
in Sj , then there must exist an alternative in V that is ranked above w at least two times
in P1 , which contradicts the assumption that c is the winner. It follows that in order for
the total score of w to be lower than the total score of c, w is ranked lower than d at least
q

3 times. Let I denote the set of subscripts of votes in P1 for which w is ranked lower than
d; then, SI = {Si : i  I} is a solution to the X3C instance.
Conversely, given a solution to the X3C instance, let I be the set of indices of Si that
are included in the X3C. Then, a solution to the possible winner instance can be obtained
by ranking d ahead of w exactly in the votes with subscripts in I. Therefore, c is a possible
winner if and only if there exists a solution to the X3C problem, which means that PW and
PcW are NP-complete with respect to positional scoring rules that satisfy the conditions
stated in the theorem.
For possible co-winner, we replace (1) by the following condition.
(1) For every i  q, s(Q, c)  s(Q, vi ) = K1 , s(Q, w)  s(Q, c) = 3q  (3K1 + K2 ).
Next, we show how to construct the profile P2 so that it satisfies the three conditions.
P2 consists of the following three parts.
The first part, P2 . Let MV denote the cyclic permutation among V  {c, w}. That
is, MV = c  w  v1  v2  . . .  vq  c. For any j  N, and any e 
V  {c, w}, we let MV0 (e) = e, and MVj (e) = MV (MVj1 (e)). The first part of P2 is
P2 = MV (P1 )  MV2 (P1 )  . . .  MVq+1 (P1 ). It follows that for any e, e  V  {c, w},
~sl (P1  P2 , e) = ~sl (P1  P2 , e ).
37

fiXia & Conitzer

The second part, P2 . Choose an arbitrary set B  C\{d, w, c} such that |B| = k1,
and an arbitrary set A  C \ (B  {d, w}) such that |A | = 3. We define the following
partial orders.
V1
V2
V3
V4

= O(B, d, w, c, Others),
= O(B, d, c, w, Others),
= O(B, d, A , w, Others),
= O(B, A , d, w, Others),

V1
V2
V3
V4

= O(B, c, w, d, Others)
= O(B, w, c, d, Others)
= O(B, w, A , d, Others)
= O(B, A , w, d, Others)

P2 is defined as follows.
P2 ={V1 , V2 , MV (V1 ), MV (V2 ), . . . , MVq+1 (V1 ), MVq+1 (V2 )}
q
  {V3 , MV (V3 ), . . . , MVq+1 (V3 )}  {V4 , MV (V4 ), . . . , MVq+1 (V4 )}
3
q
q
Here {V3 , MV (V3 ), . . . , MVq+1 (V3 )} represents copies of {V3 , MV (V3 ), . . . , MVq+1 (V3 )}.
3
3
Putting P2 and P2 together, the condition (1) in the description of P2 is satisfied.
The third part, P2 . P2 is defined in a way such that in P2 , the total scores of each
pair of alternatives in V  {c, w} are the same, and the total score of any alternative
in V  {c, w} is significantly higher than the total score of any alternative in A  {d}.
Let MO be a cyclic permutation among A  {d}. That is, we let MO = d  a1 
a2  . . .  alq3  d. Let V5 = O(V, c, w, Others). We define the third part P2 as
follows.
P2 = (|P1  P2  P2 | + 1)  {MVi (MOj (V5 )) : i  q + 2, j  l  q  2}
We note that |P1  P2  P2 | + 1 is polynomial in t + q. Therefore, the size of P2 is
polynomial in t + q.
2
Theorem 1 provides a sufficient condition on positional scoring rules for PW and PcW
to be NP-complete. It can be applied to prove NP-completeness of PW and PcW for Borda,
as the following corollary shows.
Corollary 1 PW and PcW are NP-complete with respect to Borda, even when the number
of undetermined pairs in each vote is no more than 4.
Proof. For any l  N, the scoring vector ~sl for Borda is (l  1, l  2, . . . , 0). If we let
f (x) = x, l = x, and k = l  4, then the conditions in Theorem 1 are all satisfied, and the
claim follows.
2
Theorem 1 does not apply to k-approval. As we noted in Table 1, the possible and
necessary winner problems with respect to plurality (1-approval) are in P. We next show
that for any fixed k  N with k  2, PW and PcW with respect to k-approval are NPcomplete.
Theorem 2 For any fixed natural number k  2, PW and PcW are NP-complete with
respect to k-approval, even when the number of undetermined pairs in each vote is no more
than 4.
38

fiDetermining Possible and Necessary Winners Given Partial Orders

Proof. We first prove the NP-hardness for PW with respect to 2-approval. Then, we show
how to extend the proof to any k  N, where k  2.
We prove the NP-hardness by a reduction from 3-SAT. Given an instance of 3-SAT,
where there are q variables x1 , . . . , xq and a formula F = C1  . . .  Ct , we construct an
instance of PW with respect to 2-approval as follows. Without loss of generality, we assume
that q + t  2 (generally, for any fixed k  N, we can assume that q + t  k), and that in
each clause of F , no variable appears more than once.
Alternatives: C = {c}  C  X  X1  X1  . . .  Xq  Xq  D1  D1  . . .  Dq  Dq ,
where C = {c1 , . . . , ct }, X = {x1 , . . . , xq , x1 , . . . , xq }, and for each i  q,
 Xi = {x1i , . . . , xti , x1i , . . . , xti }, Xi = {x1i , . . . , xti , x1i , . . . , xti };
 Di = {d1i , . . . , dti }, Di = {d1i , . . . , dti }.
In words, C represents the set of clauses in F ; xi and xi represent the values that the
Boolean variable xi can take; Xi (respectively, Xi ) represents a set of duplicates of
xi (respectively, xi ); Di (respectively, Di ) represents a set of auxiliary alternatives
that are associated with xi (respectively, xi ).
First part P1 of the profile: For each i  q, we let Vi = O(c, xi , xi , Others).
Then, we obtain Oi by removing (xi , xi ) from Vi . That is, in any extension of Oi ,
c must be in the top position, and one of xi and xi must be in the second position
(and the other, in the third). We will see later in the proof that the two extensions
of Oi correspond to the two valuations of the variable xi , i.e., xi being ranked in the
second position (while xi is ranked in the third position) corresponds to xi = f alse.
For each i  q, we define the following linear orders.
Vi1 = O(xi , d1i , x1i , x1i , Others)
2  j  t, Vij = O(xij1 , dji , xji , xji , Others)
Then, we obtain Oi1 from Vi1 by removing {xi , d1i }  {x1i , x1i }; for each 2  j  t,
we obtain Oij from Vij by removing {xij1 , dji }  {xji , xji }. We define Vij, and Oij,
similarly by adding  to each alternative explicitly written in the definition of Vij and
Oij , respectively (that is, the alternatives that are not in Others). For example,
Vi1, = O(xi , d1i , x1i , x1i , Others).
For each j  t, let fj : X  X1  X1  . . .  Xq  Xq be the mapping such
that for any x  X, fj (x) is obtained from x by adding j to the superscript of
x. For example, fj (x1 ) = xji and fj (x2 ) = xj2 . For each j  t, let Wj =
O(c, fj (lj1 ), fj (lj2 ), fj (lj3 ), Others). Then, we obtain Qj from Wj by removing
{fj (lj1 ), fj (lj2 ), fj (lj3 )}  {fj (lj1 ), fj (lj2 ), fj (lj3 )}
That is, in any extension of Qj , c must be in the top position, and one of {fj (lj1 ), fj (lj2 ),
fj (lj3 )} must be in the second position. We will see that the extensions of Qj correspond to how Cj (the jth clause) is satisfied under a valuation of x1 , . . . , xq .
We let P1 = {O1 , . . . , Oq }  {Oij , Oij, : i  q, j  t}  {Qj : j  t}.
39

fiXia & Conitzer

Second part P2 of the profile: for any profile P and any alternative c , we let
s2 (P, c ) denote the score of c in P , under 2-approval. That is, s2 (P, c ) is the number
of times that c is ranked in the top two positions in P . We let P2 be an arbitrary
profile of linear orders that satisfies the following conditions.
 s2 (P2 , c) = 0.
 For every i  q and every j  t, s2 (P2 , xi ) = s2 (P2 , xi ) = s2 (P2 , xji ) =
s2 (P2 , xji ) = s2 (P2 , xji ) = s2 (P2 , xji ) = q + t  2.
 For any c not mentioned above, s2 (P2 , c )  1.
Because t + q  2, P2 is well-defined and |P2 | is bounded above by a polynomial of t and q
(we try to fit q + t  2 copies of {xi , xi , xji , xji , xji , xji : i  q, j  t} into the top two
positions of (q + t  2)(2q + 4qt)/2 = q(q + t  2)(2t + 1) votes). We note that the number
of undetermined pairs in each vote in P1  P2 is no more than 4.
Suppose there is a feasible solution to the 3-SAT instance. Let g denote a valuation of
x1 , . . . , xq under which F is satisfied. We define an extension of P1  P2 as follows.
 For every i  q, if g(xi ) = true, then we define the following extensions of partial
orders in P1 .
 Let Vi be the extension of Oi in which xi is ranked in the second position.
 Let Vi1 be an extension of Oi1 in which xi and d1i are ranked in the top two
positions; let Vi1, be an extension of Oi1, in which x1i and x1i are ranked in
the top two positions.
 For every 2  j  t, let Vij be an extension of Oij in which xij1 and dji are
ranked in the top two positions.
 For every 2  j  t, let Vij, be an extension of Oij, in which xji and xji are
ranked in the top two positions.
 For every i  q, if g(xi ) = f alse, then we define the following extensions (which are
similar to the extensions in the case where g(xi ) = true).
 Let Vi be the extension of Oi in which xi is ranked in the second position.
 Let Vi1, be an extension of Oi1, in which xi and d1i are ranked in the top two
positions; let Vi1 be an extension of Oi1 in which x1i and x1i are ranked in the top
two positions.
 For every 2  j  t, let Vij, be an extension of Oij, in which xij1 and dji are
ranked in the top two positions.
 For every 2  j  t, let Vij be an extension of Oij in which xji and xji are ranked
in the top two positions.
 For every j  t, if Cj is satisfied by xi = true (respectively, xi = f alse) for some
i  q, then, we let Wj be an extension of Qj in which xji (respectively, xji ) is ranked
in the second position.
 Let P  = {V1 , . . . , Vq }  {Vij , Vij, : i  q, j  t}  {W1 , . . . , Wt }  P2 .
40

fiDetermining Possible and Necessary Winners Given Partial Orders

It can be checked that in P  \ P2 , every alternative c (c 6= c) is ranked in the two top
positions at most once. We recall that s2 (P2 , c )  q + t  2 and s2 (P  , c) = q + t. Therefore,
c is the unique winner.
Next, we show how to convert a feasible solution to PW to a feasible solution to the
3-SAT instance. Let P  be an extension for which c is the unique winner. Let g be the
valuation such that for any i  q, g(xi ) = true if and only if in the extension of Oi in P  ,
xi is ranked in the second position. We prove the following claim to show that under g,
all clauses are satisfied.
Claim 1 For any i  q, if g(xi ) = true (respectively, g(xi ) = f alse), then for every j  t,
xji and xji (respectively, xji and xji ) are ranked in the top two positions in the extension
of Oij, (respectively, Oij ) in P  .
Proof. For any i  q, we prove the claim by induction on j. We only prove the case where
g(xi ) = true; the case where g(xi ) = f alse can be proved similarly.
Suppose g(xi ) = true. By definition, xi is ranked in the second position in the extension of Oi in P  . We recall that s2 (P2 , xi ) = q + t  2 = s2 (P  , c)  2. Because c is the
unique winner, xi is not ranked in the top two positions in any extension of P1 \ {Oi }.
Specifically, xi is not ranked in the top two positions in the extension of Oi1, . We recall
that xi  d1i in Oi1, . Therefore, d1i is not ranked in the top two positions in the
extension of Oi1, (otherwise, xi would also be ranked in the top two positions, which immediately prevents c from being the unique winner). We also note that xi , d1i , x1i , x1i
are the only four alternatives that can be ranked in the top two positions in an extension of
Oi1, . It follows that in the extension of Oi1, , x1i , x1i are ranked in the top two positions.
This means that the claim holds for j = 1.
Suppose the claim holds for all j with j  j  . Following similar reasoning as in the
case where j = 1, we can prove that the claim holds for j = j  + 1. More precisely, by


the induction hypothesis, xji is ranked in the top two positions in the extension of Oij , .


Therefore, xji is not ranked in the top two positions in the extension of Oij +1, (otherwise

the score of xji is at least as large as the score of c, which means that c is not a unique




winner). We recall that xji  dji +1 in Oij +1, . Therefore, dji +1 is not ranked in the


top two positions in the extension of Oij +1, (otherwise xji must also be ranked in the
top two positions, which immediately prevents c from being the unique winner). We also




note that xji , dji +1 , xji +1 , xji +1 are the only four alternatives that can be ranked in


the top two positions in an extension of Oij +1, . It follows that in the extension of Oij +1, ,


xji +1 and xji +1 are ranked in the top two positions. This means that the claim holds for
j = j  + 1.
Therefore, the claim holds for every j  t.
2
We are now ready to show that under g, all the clauses are satisfied. Let j be a number
no more than t. If xji is ranked in the second position in the extension of Qj , then we must
have that g(xi ) = true. If not, then, from Claim 1, xji is ranked in the top two positions in
the extension of Oij , which means that xji is ranked in the top two positions in P  \P2 at least
twice: once in Oij , and once in Qj . It follows that s2 (P  , xji )  q+t2+2  q+t = s2 (P  , c),
which contradicts the assumption that c is the unique winner. Similarly, if in the extension of
41

fiXia & Conitzer

Qj , xji is ranked in the second position, then we must have that g(xi ) = f alse. This means
that under g, every clause Cj is satisfied by the valuation of the variable that corresponds
to the alternative that is ranked in the second position in the extension of Qj . Hence, F is
satisfied.
For PcW, we simply replace s2 (P2 , xi ) = s2 (P2 , xi ) = s2 (P2 , xji ) = s2 (P2 , xji ) =
s2 (P2 , xji ) = s2 (P2 , xji ) = q + t  2 in the definition for P2 by s2 (P2 , xi ) = s2 (P2 , xi ) =
s2 (P2 , xji ) = s2 (P2 , xji ) = s2 (P2 , xji ) = s2 (P2 , xji ) = q + t  1.
The reduction for k > 2 is similar to the case where k = 2. For any 3-SAT instance, let
P1 and P2 be the profile of partial orders defined for the case k = 2. For k > 2, we add
|P1  P2 |  (k  2) new alternatives to the instance, and in each partial order in P1  P2 , we
let the top k  2 positions be occupied by the new alternatives, and we put the remaining
new alternatives in the bottom positions, such that none of the new alternatives is ranked
in the top k positions more than once. Let P1 and P2 denote the profiles of partial orders
obtained in this way. It follows that c is a possible (co-)winner for P1  P2 with respect to
k-approval if and only if c is a possible (co-)winner for P1  P2 with respect to 2-approval.
2
Theorem 3 PW and PcW are NP-complete and NW and NcW are coNP-complete with
respect to Copeland, even when the number of undetermined pairs in each vote is at most
8.
Proof. We first prove the PW and NcW parts, in one reduction from X3C. Without loss
of generality, we can always assume that in the X3C instance, t is odd and t = q, because
if not, then we make the following changes to the X3C instance.

and 2(t  q) sets
 If t > q, then we add 3(t  q) dummy elements v1 , . . . , v3(tq)








S1 , S1 , . . . , Stq , Stq , where for each i  t  q, Si = {v3i2 , v3i1 , v3i }.

 If q > t, then we add q  t copies of S1 .
 If q = t and t is even, then we add three dummy elements v1 , v2 , v3 , and three copies
of S1 = {v1 , v2 , v3 }.
In the new X3C instance, t = q, t is odd, the size of the instance is polynomial in the size
of the old one, and the new X3C instance has a feasible solution if and only if the old one
has.
Given an X3C instance V = {v1 , . . . , vq }, S = {S1 , . . . , St }, where q = t and t is odd, we
construct a PW instance as follows.
Alternatives: {c, w, d}  V  A  B, where A = {a1 , . . . , at2 }, B = {b1 , . . . , b7t }.
First part P1 of the profile: Let M be a cyclic permutation among B. That is, M =
b1  b2  . . .  b7t  b1 . Let VB = b1  b2  . . .  b7t . For each i  t, we obtain a partial
order by starting with O((V \ Si ), d, Si , w, c, M i (VB ), A), and then removing the ordering
relationships in ({d}  Si )  {w, c}.
Second part P2 of the profile:
2q
2q
+ 1 votes: for each i such that t + 1  i  2t 
+ 1, there is a vote that is
3
3
i
consistent with w  c  d  V  M (VB )  A.

 t

42

fiDetermining Possible and Necessary Winners Given Partial Orders





2q
q
q
 2 votes: for each i such that 2t 
+ 2  i  2t   1, there is a vote that is
3
3
3
consistent with w  c  d  V  M i (VB )  A.
q
q
 2 votes: for each i such that 2t   i  2t  3, there is a vote that is consistent
3
3
with w  d  c  V  M i (VB )  A.

 2 votes: for each i such that 2t  2  i  2t  1, there is a vote that is consistent with
c  w  d  V  M i (VB )  A.
 2 votes: for each i such that 2t  i  2t + 1, there is a vote that is consistent with
d  c  V  w  M i (VB )  A.




1
1
(5t  1) votes: for each i such that 2t + 2  i  (9t + 1), there is a vote that is
2
2
consistent with w  A  c  M i (VB )  V  d.
1
1
(5t  1) votes: for each i such that (9t + 3)  i  7t, there is a vote that is
2
2
consistent with M i (VB )  V  w  d  A  c.

We note that the number of undetermined pairs in each vote is no more than 8.
Let P1 denote the profile that extends P1 such that in each vote d and Si are ranked
higher than w and c, that is, P1 = {O((V \ Si ), d, Si , w, c, B, A) : i  t}. We make the
following observations on each pairwise election:
 w always defeats c, d, B, A, and for each i  q, DP1 P2 (vi , w) = 3.
 c always defeats V, B, always loses to A, and DP1 P2 (d, c) =

2q
 1.
3

 B always defeats d, V, A, and due to its cyclic order in the profile, bj always defeats
bj+1 , . . . , bj+ 1 (7t1) , where for any i  N, bi = bi+7t , and always loses to the other
2
alternatives in B.
Therefore, in P1  P2 , the total number of pairwise elections won by each alternative is:
 w wins |B| + |A| + 2 = 8t,
 c wins |V| + |B| = q + 7t = 8t,
 d, any v  V, and any a  A wins at most 8t + q + 1  7t = t + q + 1, because they
all lose to B,
 any b  B wins at most 12 (|B|  1) + |A| + |V| + 1 = 12 (9t + 2q  3) pairwise elections.
We recall that in the X3C instance t = q, which means in P1  P2 , the winners are
{w, c}. In order for c to be the unique winner, the only possibility is for c to win the
q
pairwise election against d by putting c  d in at least votes in P1 . However, when we
3
put c ahead of d in a vote corresponding to Si , for all v  Si the pairwise score difference
between w and v increases by 2. Moreover, if w  v for some v  V at least twice in an
43

fiXia & Conitzer

extension P  of P1 , then DP  P2 (v, w)  1, which means that w defeats v in their pairwise
election. In this case, w would win 8m + 1 pairwise elections, which means that c cannot
be the unique winner. Therefore, c is a possible unique winner if and only if there exists
q
an extension P  of P1 such that c  d in exactly votes in P  , and the corresponding Si
3
do not overlap, that is, they constitute an exact cover of V. This means that PW has a
solution if and only if the X3C problem has a solution. So PW is NP-complete.
Because in the above reduction, w would always be a co-winner if c is not the unique
winner, NcW is coNP-complete. For PcW and NW, we just need to slightly modify the
reduction for PW and NcW: let |A| = t  1 and keep the rest unchanged. Then, w will
initially win 8t + 1 pairwise elections, and c is a possible co-winner (w is not the necessary
unique winner) if and only if there exists a feasible solution to the X3C instance.
2
Theorem 4 PW and PcW are NP-complete with respect to Bucklin, even when the number
of undetermined pairs in each vote is at most 16.
Proof.
First, we give a reduction from X3C to PW. Given any X3C instance V =
{v1 , . . . , vq }, S = {S1 , . . . , St }, we construct a PW instance as follows.
Alternatives: W  D  V  {c, w}, where W = {w1 , . . . , wq+1 }, D = {d1 , . . . , dq+1 }.
First part P1 of the profile: for each i  t, we start with O(w1 , . . . , wq+1 , Si , c, (V \
Si ), D), and then obtain a partial order by removing the relations in
{wq2 , wq1 , wq , wq+1 }  (Si  {c})
Second part P2 of the profile:
 t copies of V  c  Others,
q
  1 copies of V  w  c  Others,
3
q
 + 2 copies of D  w1  Others.
3
We note that the number of undetermined pairs in each vote is no more than 16. Notice
2q
q
|P1  P2 | = 2t +
+ 1, and w1 is ranked within top q + 2 positions in t + + 2 votes
3
3
in any extension of P1  P2 . Therefore, in order for c to win, c  wq2 must hold in at
q
least votes in the extension of P1 . However, whenever we put c ahead of wq2 in a vote,
3
we are forcing the alternatives in the Si corresponding to that vote be ranked within top q
positions. If some v  V is ranked within top q positions at least twice in an extension of
q
P1 , then overall it will be ranked within top q positions in at least t + + 1 votes, which
3
means c will not be the unique winner.
If there exists a feasible solution to the X3C problem, then we can put c ahead of wq2
in the votes corresponding to this solution, so that we obtain an extension P1 of P1 such
q
that c is ranked within top q + 1 positions in votes, while for any v  V, v is ranked within
3
top q (and, in fact, the first q + 1) positions just once. As a result, c is the unique winner
of the profile P1  P2 , because no other alternative is ranked within top q + 1 positions in
44

fiDetermining Possible and Necessary Winners Given Partial Orders

q
at least t + votes. Conversely, if c is the unique winner in some profile P1  P2 , then
3
P1 corresponds to a feasible solution to the X3C problem. Therefore, PW with respect to
Bucklin is NP-complete.
q
For PcW, we just need to modify the reduction slightly, by changing the last + 1 votes
3
from [D  w1  Others] to [d1  . . .  dq  w1  Others]. In this case, the Bucklin score
of w1 is q + 1, which means c can at best hope to be a co-winner. As a result, PcW is also
NP-complete.
2
To prove our hardness results for maximin, ranked pairs, and voting trees, we present
two helpful lemmas. We first show that given any pair of alternatives c, c , there exist two
linear orders that increase D(c, c ) by two while keeping all other pairwise score differences
unchanged. This lemma has been used previously (McGarvey, 1953; Conitzer & Sandholm,
2005a). We will use this technique in the second (score-adjusting) part of the reductions
for maximin, ranked pairs, and voting trees.
Lemma 1 Given any profile P and any pair of different alternatives c, c , let the remaining
alternatives be {c1 , . . . , cm2 }. Let P  be the profile consisting of P plus the following two
votes:
1. [c  c  c1  . . .  cm2 ], and
2. [cm2  . . .  c1  c  c ].
Then, DP  (c, c ) = DP (c, c ) + 2, and for any alternatives d, d such that {d, d } =
6 {c, c },
DP  (d, d ) = DP (d, d ).
This lemma tells us that the pairwise score differences can be changed almost arbitrarily.
The only constraint is that the parity of the pairwise score differences remains the same.
The following lemma is a direct corollary.
Lemma 2 (The main theorem in McGarvey, 1953) Given a profile P and any skewsymmetric function F : C  C  Z (that is, F (c1 , c2 ) = F (c2 , c1 ) for all c1 , c2 ), such
that for all pairs of alternatives c, c  C, F (c, c )  DP (c, c ) are all even (or all odd), then
there exists a profile P  such that
1. |P  | 

1P


 (|F (c, c )  DP (c, c )| + 1),
2 c,c

2. DP P  = F .
That is, for any skew-symmetric function F such that for all pairs of alternatives (c, c )
(with c 6= c ), F (c, c )  DP (c, c ) has the same parity, we can change the pairwise score
1P


differences from DP to F by adding no more than
 (|F (c, c )DP (c, c )|+1) votes to P .
2 c,c
Here, the factor 12 comes from the fact that for any pair of alternatives c and c , the absolute
value of the difference between F and DP is counted twice, i.e., |F (c, c )  DP (c, c )| =
|F (c , c)  DP (c , c)|. In fact, it is possible to obtain even tighter bounds on the needed size
of P  (Erdos & Moser, 1964), but for the purpose of our NP-hardness proofs this does not
matter.
45

fiXia & Conitzer

Now we are ready to prove the hardness results for maximin and ranked pairs. As we
mentioned in the beginning of this section, in all hardness proofs in this section, the profile
consists of P1 and P2 , where P1 is a set of partial orders used to encode the X3C instance,
and P2 is a set of linear orders used to adjust the scores of the alternatives. For maximin,
ranked pairs, and voting trees, P2 is used to adjust the pairwise score differences. We do not
explicitly give P2 in the reductions for these rules. Instead, we present the properties of P2 ,
then appeal to Lemma 2 to assert that P2 does exist, and can be constructed in polynomial
time.
Theorem 5 PW and PcW are NP-complete with respect to maximin, even when the number of undetermined pairs in each vote is at most 4.
Proof. We first prove that PW is NP-complete. Given an X3C instance V = {v1 , . . . , vq },
S = {S1 , . . . , St }, we construct a PW instance as follows.
Alternatives: V  {c, w, w }.
First part P1 of the profile: for each i  t, we start with O(w, Si , c, (V \Si ), w ), and
subsequently obtain a partial order Oi by removing the relations in {w}  (Si  {c}).
Second part P2 of the profile: according to Lemma 2, P2 is defined to be a set of
votes such that the pairwise score differences of {O(w, Si , c, (V \ Si ), w ) : i  t}  P2
satisfy:
2q
2; for each i  q, D(w, vi ) = t+2; D(w , w) = D(v1 , w ) = t+4;
3
D(w , c) = t  2.

(1) D(w, c) = t+

(2) For all other pairwise scores not defined in (1), D(l, r)  1.
We note that the number of undetermined pairs in each vote is no more than 4.
Lemma 2 implies that the size of P2 is polynomial in q + t.
We note that the minimum pairwise score difference of w is D(w, w ) = t  4; the
minimum pairwise score difference of w is also t  4 = D(w , v1 ).
Suppose there exists a profile P1 extending P1 such that c wins in P1  P2 . If c is raised
q
higher than w in at least one and at most  1 votes in P1 , then, D(c, w)  t, and there
3
exists i  q such that D(vi , w)  t (the smallest pairwise score difference of vi ), which
means that c is not the unique winner because vi is performing at least as well. If c is ranked
q
higher than w in at least + 1 votes in P1 , then we still have D(c, w ) = t + 2, and there
3
exists i  q such that vi is ranked higher than w in at least two votes in P1 , which means
that D(vi , w)  t + 2 (the smallest pairwise score difference of vi ). It follows that in this
case, c is not the unique winner because vi is performing at least as well. Therefore, the
q
only way for c to win is to decrease D(w, c) by raising c higher than w in exactly votes in
3
P1 . However, each time that we decrease D(w, c) by 2 due to adding c  w to Oi  P1 , for
each v  Si , D(w, v) is also decreased by two. Because D(w , c) = t  2, decreasing D(w, c)
to less than t  2 would not raise the minimum pairwise score difference of c. But if for
some i  q, D(w, vi ) is decreased by 4 or more, then the minimum pairwise score of vj is
46

fiDetermining Possible and Necessary Winners Given Partial Orders

at least t + 2, which means that in this case c cannot be the unique winner. Therefore,
the sets Si in the votes in P1 where c  w cannot overlap. Because there must be at least
q/3 of these votes, the corresponding subsets Si constitute a feasible solution to the X3C
instance.
Conversely, suppose the X3C instance has a solution. Without loss of generality, let the
solution be {S1 , . . . , Sq/3 }. We define an extension P1 of P1 by adding c  w in Oi for all
i  q/3, and then adding w  Si for all i > q/3. It follows that c is the unique winner for
the profile P1  P2 with respect to the maximin rule. Therefore PW is NP-complete.
For PcW, we just need to slightly modify the above reduction: we replace the condition
D(w, vi ) = t + 2 by D(w, vi ) = t when constructing P2 . Therefore PcW is NP-complete. 2
Theorem 6 PW and PcW are NP-complete and NW and NcW are coNP-complete with
respect to ranked pairs, even when the number of undetermined pairs in each vote is at most
8.
Proof. We first prove the NP-hardness of PW and NcW in one reduction. Given an X3C
instance V = {v1 , . . . , vq }, S = {S1 , . . . , St }, we construct a PW instance as follows.
Alternatives: V  {c, a, b, w}.
First part P1 of the profile: for each i  t, we start with O(a, c, Si , b, Others), and
subsequently obtain a partial order Oi by removing the relations in ({a, c}(Si {b})).
Second part P2 of the profile: according to Lemma 2, P2 is defined to be a set
of votes such that the pairwise score differences of {O(a, c, Si , b, Others) : i  t}  P2
satisfy:
1. For all i  q, D(c, b) = D(w, a) = D(w, vi ) = 3t +

2q
.
3

2q
2q
2q
, D(c, w) = t +
 2, D(vi , c) = t +
 6, D(b, a) = t + 2.
3
3
3
3. D(l, r) = 0 in all other cases.
2. D(a, c) = t +

We note that the number of undetermined pairs in each vote is no more than 8.
Lemma 2 implies that the size of P2 is polynomial in q + t.
We note that D(c, b), D(w, a), and D(w, vi ) (for every i  q) are much larger than the
remaining pairwise score differences in any extension of P1  P2 . Therefore, c  b, w  a,
and w  vi (for every i  q) are fixed first in any extension of P1  P2 . It follows that in
the output (a linear order over C) for any extension of P1  P2 , we must have that c  b,
w  a, and w  vi (for every i  q). We note that the only way for c to be the unique
2q
winner is to lock b  a before a  c. That is, D(b, a) must be at least t + 2 + . However,
3
whenever we let b  a in an extension of Oi , we are forcing Si  c. Let P1 be an extension
of P1 such that c is the unique winner for the profile P1  P2 (or, equivalently, such that
w is not a co-winner for the profile P1  P2 ). We note that if there exists i  q such that
2q
2q
6+4 = t+
 2 = D(c, w),
vi  c in at least two votes in P1 , then D(vi , c)  t +
3
3
which means that w is a co-winner (by locking vi  c before c  w). Therefore, in P1 , we
47

fiXia & Conitzer

q
must have that b  a in exactly votes, and for all i  q, vi  c in exactly one vote. This
3
naturally corresponds to a solution to the X3C instance.
Conversely, suppose that the X3C instance has a solution. Without loss of generality, let
the solution be {S1 , . . . , Sq/3 }. We define an extension P1 of P1 by adding b  a in Oi for all
i  q/3, and then for all i > q/3, letting the extension of Oi be [a  c  Si  b  Others].
It follows that c is the unique winner for this profile (and hence, w is not a co-winner).
Therefore, PW is NP-complete and NcW is coNP-complete with respect to ranked pairs.
For PcW and NW, we just need to slightly modify the above reduction by letting
2q
D(b, a) = t and for all i  q, letting D(vi , c) = t +
 4.
2
3
Next, we consider voting trees. Because a voting tree is defined for a fixed number
of alternatives, to study the complexity of the possible/necessary winner problems with
respect to voting trees, we need to consider an infinite sequence of trees, one for each
natural number (representing the number of alternatives).8 Therefore, we let a voting tree
rule T be composed of an infinite sequence of voting trees {T1 , T2 , . . .}, where for any m  N,
Tm is a voting tree for m alternatives (that is, Tm is a binary tree that has m leaf nodes,
and each leaf is associated with an alternative).
For any t  N, a voting tree Tm is t-well-spread if there exist t pairs of leaves (c1 , a1 ), . . . ,
(ct , at ), such that for each i  t, ci and ai are siblings. We say that any leaf in such a pair
is a rich leaf. A voting tree is balanced if the depths of any pair of leaves differ at most by
one, and the number of leaves whose (unique) sibling is not a leaf is at most one.
Example 2 Two voting trees are illustrated in Figure 2. The voting tree in (a) is 1-wellspread, and c1 and c2 are rich leaves; the voting tree in (b) is balanced and 3-well-spread,
and all leaves except c5 are rich leaves.

c4
c5

c3
c1

c2

c1

c2

c3

(a)

c4

c6

c7

(b)
Figure 2: Voting trees.

Theorem 7 For any voting tree rule T = {T1 , T2 , . . .}, if there exists a polynomial function
f (x) such that for any x  N, there exists l  N with x  l  f (x) such that Tl is x-wellspread, then PW and PcW are NP-complete, and NW and NcW are coNP-complete with
respect to T , even when the number of undetermined pairs in each vote is at most 16.
8. This is similar to the case of positional scoring rules, which are technically defined only for a specific
number of alternatives.

48

fiDetermining Possible and Necessary Winners Given Partial Orders

Proof. Let j2 , j3 , . . . be the index of the voting trees such that for any z  N (z  2), Tjz
is 2(z + 1)-well-spread and jz  f (2(z + 1)). For any z, we let c be an arbitrary rich leaf in
Tj z .
We first prove the NP-hardness of PW and PcW in a single reduction. Given an X3C
instance V = {v1 , . . . , vq }, S = {S1 , . . . , St }, we construct a PW instance as follows.
Alternatives: Let C be the leaves of Tjq , where C = {c, d, w}  V  A  E, and
A = {a1 , . . . , aq }, E = {e1 , . . . , emq 2q3 }, where mq is the number of leaves in Tjq .
Let the tree be such that {c, d}V A are rich leaves in a subtree whose root is a child
of the root of Tjq (because Tjq is 2(q + 1)-well-spread, this is always possible); d is the
sibling of c; the only common ancestor of c and w is the root; and for each 1  i  q,
vi and ai are siblings. The positions of {c, d, w}V A are illustrated in Figure 3. E is
the set of all other alternatives in Tjq . For each i  t, if Si = {vl(i,1) , vl(i,2) , vl(i,3) }, then
we let Ai = {al(i,1) , al(i,2) , al(i,3) }that is, Ai consists of the siblings of the elements
in Si .

w

c

d

v1

a1

vq

aq

Figure 3: Positions of the alternatives in Tjq .
First part P1 of the profile: for each i  t, we start with O(d, Ai , Si , c, Others), and
subsequently obtain a partial order Oi by removing relations in ({d}  Ai )(Si {c}).
Second part P2 of the profile: according to Lemma 2, P2 is defined to be a
set of votes (linear orders) such that the pairwise score differences for the profile
{O(d, Ai , Si , c, Others) : i  t}  P2 satisfy:
(1) D(c, d) = 2q/3 + 1, D(c, w) = 2q + 1.
(2) For each i  q, D(ai , vi ) = 3, D(vi , c) = D(c, ai ) = 2q + 1.
(3) For each c  C (with c 6= c), D(w, c ) = 2q + 1.
(4) For each pair i, i  q (with i 6= i ), D(vi , ai ) = 2q + 1.
(5) For each x  C \ E, and each e  E, D(x, e) = 2q + 1.
We note that the number of undetermined pairs in each vote is no more than 16.
Lemma 2 implies that the size of P2 is polynomial in q + t.
49

fiXia & Conitzer

The only way for c to win is to beat d in the first round, and not to meet any of {v1 , . . . , vq }
in later rounds, which can only happen if every vi is beaten by the corresponding ai in
the first round. This is because by item (4), for each i 6= i , D(vi , ai ) = 2q + 1, which
means that if for some i  q, vi wins in the first round, it will only be beaten by w or vj
for some j  q in subsequent rounds.In this case the winner must be w. It follows that in
any extension of P1 that makes c win, c must be ranked higher than d at least q/3 times.
However, if we rank c higher than d in an extension of Oi , then in the same extension we
must have that Si  Ai . In order for every ai to defeat vi , for every i  q, vi can be ranked
higher than ai at most once in the extension of P1 . Therefore, if there exists a profile P1
extending P1 such that c is the unique winner (or co-winner) in P1  P2 , then the votes in
P1 where c  d make up a feasible solution to the X3C problem instance. Conversely, for
any feasible solution to the X3C problem instance, we can find a P1 extending P1 such that
c is the unique winner of the profile P1  P2 with respect to Tij . Therefore, PW and PcW
are NP-complete.
Because if c is not the unique winner, then w is always the unique winner. Therefore,
NW and NcW are coNP-complete.
2
From Theorem 7, we immediately obtain the following hardness results for voting tree
rules composed of balanced trees, by setting f (x) = 4x (because there will exist some integer
y such that 2x  2y  4x, so in the balanced tree for 2y alternatives there will be at least
x pairs of siblings).
Corollary 2 PW and PcW are NP-complete and NW and NcW are coNP-complete with
respect to the voting tree rule that is composed of balanced binary trees, even when the
number of undetermined pairs in each vote is at most 16.
Finally, we have the following theorems on the complexity of PW and NcW with respect
to plurality with runoff.
Theorem 8 PW is NP-complete with respect to plurality with runoff.
Proof. We prove NP-hardness by a reduction from X3C. Given an X3C instance V =
{v1 , . . . , vq }, S = {S1 , . . . , St }, we construct a PW instance as follows.
Alternatives: C = {c, d, e}  SV  E, where SV = {s1 , . . . , st } and
E = {e1 , . . . , e(q+4)2 (t+4)4 }.
First part P1 of the profile: P1 = P11  P12 , where P11 and P12 are defined as follows.
 P11 : for each i  q, we start with a linear order O(d, SV , c, Others), and subsequently obtain a partial order Oi by removing ({d}  SV )  {sj : vi  Sj }.
That is, we remove a minimum set of constraints such that any alternative in
{sj : vi  Sj } can be ranked in the top position in at least one extension of Oi .
Let P11 = {Oi : i  q}.
 P12 : for each j  t, we start with a linear order O(d, e, c, Others), and subsequently obtain a partial order Q1j by removing ({d}  {e})  (C  {sj }). That is,
in an extension of Q1j , only d, e, and sj can be ranked in the top position. We
let Q2j = Q1j , and P12 = {Q1j : j  t}  {Q2j : j  t}.
50

fiDetermining Possible and Necessary Winners Given Partial Orders

Second part P2 of the profile: P2 = P21  P22 , where P21 and P22 are defined as
follows.
 P21 : a set of q(t + 7/3) + 8 votes, in which c is ranked in the top position q + 4
times, d is ranked in the top position q + 2 times, e is ranked in the top position
q/3 + 2 times, and for each j  t, sj is ranked in the top position q times. It
does not matter how the remaining alternatives are ranked in P21 .
 P22 : we first obtain, according to Lemma 2, a profile P22 such that the pairwise
score differences of the following profile:
{q copies of O(d, SV , c, Others)}  {2t copies of O(d, e, c, Others)}  P21  P22
satisfy the following conditions.
1. D(d, c) = D(e, c) = 1;
2. for all j  t, D(c, sj ) = 1.
By Lemma 2, the size of P22 is polynomial in p + t. Next, we obtain P22 from P22
by moving an alternative in E to the top position in each vote of P22 , in such a
way that each vote in P22 ranks a different alternative in the top position. P22 is
well-defined, because |E|  |P22 |.
For any profile P , and any alternative c , we let P luP (c ) denote the plurality score of c in P ,
that is, P luP (c ) is the number of times where c is ranked in the top position in P . The subscript P is omitted when there is no risk of confusion. We make the following observations
on the profile {q copies of O(d, SV , c, Others)}  {2t copies of O(d, e, c, Others)}  P21  P22 :
 D(d, c) = D(e, c) = 1, and for all j  t, D(c, sj ) = 1;
 P lu(c) = q + 4, P lu(d) = 2t + 2q + 2, P lu(e) = q/3 + 2; for each j  t, P lu(sj ) = q;
for each e  E, P lu(e )  1.
We also note that in any extension of P1  P2 , P lu(c) = q + 4.
If the X3C instance has a solution Sj1 , . . . , Sjq/3 , then we construct a solution to the
PW instance as follows.
 For each i  q, let Vi = [sjl  d  (SV \ {sjl })  c  Others], where jl is such that
ci  Sjl ; we note that Vi extends Oi ;
 for each l  q/3, let Vj1l = Vj2l = [e  d  c  Others]; we note that Vj1l and Vj2l extend
Q1jl and Q2jl , respectively;
 for each j  t (with j 6= jl for all l  q/3), let Vj1 = Vj2 = [sj  d  e  c  Others];
we note that Vj1 and Vj2 extend Q1j and Q2j , respectively;
 then, we use these votes to extend the partial orders in P1 : let P1 = {Vi : i 
q}  {Vj1 , Vj2 : j  t}.
51

fiXia & Conitzer

In P1  P2 , we have P lu(c) = q + 4, P lu(d) = P lu(e) = q + 2; for each l  q/3, P lu(sjl ) =
q + 3; for each j 6= jl (l = 1, . . . , q/3), P lu(sj ) = q + 2; and for each e  E, P lu(e )  1.
Also, we have that for each l  q/3, D(c, sjl ) = 1. It follows that the pairs that enter the
runoff (in some parallel universe) are (c, sj1 ), . . . , (c, sjq/3 ), and c wins all of these pairwise
elections. Therefore, c is the unique winner for P1  P2 .
Next, we show how to convert a solution to the PW instance to a solution to the X3C
instance. Let P1 = P11  P12 be an extension of P1 such that c is the unique winner for
P1  P2 , where P11 = {Vi : i  q} extends P11 , and P12 = {Vj1 : j  t}  {Vj2 : j  t}
extends P12 . We make the following sequence of claims.
Claim 2 Neither d nor e can enter the runoff, which means that the only pairs that could
potentially still enter the runoff are (c, sj ), for some j  t.
Proof. If d or e entered the runoff in some parallel universe, then it would defeat c
in the runoff (unless c is not even in the runoff, in which case c also does not win in this
parallel universe), contradicting that c is the unique winner.
2
Claim 3 For each j  t, P luP1 (sj )  3.
Proof. If this does not hold, then we let j  be an index that maximizes P luP1 (sj  ).
It follows that P luP12 (sj  )  1, because P luP11 (sj  )  3. However, by putting sj  in the
top position in a partial order in P12 , we are forcing D(c, sj  ) to be reduced by 2, which
means that sj  defeats c in their pairwise election. Moreover, because, by Claim 2, one of
the sj must enter the runoff, and because sj  has the maximum plurality score among the
alternatives in SV , sj  must be in the runoff in one of the parallel universes. However, c
cannot win in this parallel universe, which contradicts the assumption that c is the unique
winner.
2
Claim 4 P luP1 (d) = 0, P luP1 (e)  2q/3.
Proof. It follows from Claim 3 that for each j  t, P luP1 P2 (sj )  q + 3. Therefore,
by Claim 2 we must have that P luP1 P2 (d)  q + 2 and P luP1 P2 (e)  q + 2. The claim
follows.
2
Claim 5 For any j  t, if P luP12 (sj )  1, then P luP1 (sj )  2.
Proof. If P luP12 (sj )  1 and sj enters the runoff in some parallel universe, then c
cannot win in that parallel universe. For the sake of contradiction, suppose P luP1 (sj )  3.
By Claim 3 and Claim 2, sj enters the runoff in some parallel universe, which contradicts
the assumption that c is the unique winner.
2
Claim 6 Let X1 = {sj : P luP11 (sj ) > 0, P luP12 (sj ) = 0}, and X2 = {sj : P luP11 (sj ) =
0, P luP12 (sj ) > 0}. We have X1  X2 = SV and |X1 | = q/3.
52

fiDetermining Possible and Necessary Winners Given Partial Orders

Proof. Let x1 = |X1 |, x2 = |X2 |, and x3 = t  x1  x2 . By Claim 5, for each
sj  SV \ (X1  X2 ), P luP11 (sj ) = P luP12 (sj ) = 1. We recall that for each O  P11 , the
top-ranked alternative in any extension of O must be either d or an element in SV ; for each
Q  P12 , the top-ranked alternative in any extension of Q must be d, e, or an element in
SV . We then use these observations to obtain two inequalities.
First, in order for c to be the unique winner, d cannot be in the top position in any vote
in P11 . Therefore, all of the q top positions in P11 must be taken by alternatives in SV .
Now, any alternative in X1 can take at most three of these top positions; any alternative
in X2 takes none of these top positions by definition; and any alternative in SV \ (X1  X2 )
takes one of these top positions. It follows that 3x1 + x3  q.
Now, we apply a similar analysis to P12 . In order for c to be the unique winner, e
cannot be in the top position in more than 2q/3 votes in P12 , leaving at least 2t  2q/3
top positions to be filled. Now, any alternative in X1 takes none of these top positions; any
alternative in X2 can take at most two of these top positions (Claim 5); and any alternative
in SV \ (X1  X2 ) takes one of these top positions. It follows that 2x2 + x3  2t  2q/3.
By substituting q in the second inequality by the q in the first inequality, we obtain
2x1 + 2x2 + 53 x3  2t. We recall that x1 + x2 + x3 = t. Therefore, x3 = 0, x1 + x2 = t. Now
the first inequality becomes x1  q/3 and the second inequality becomes x2  t  q/3. It
follows from x1 + x2 = t that x1 = q/3 and x2 = t  q/3.
2
Based on all these claims, we can now construct a solution to the X3C instance. Let
X1 = {sj1 , . . . , sjq/3 }. From Claim 3, Claim 6, |P11 | = q, and the fact that every top position
in P11 must be occupied by one of the alternatives in X1 , it follows that Sj1 , . . . , Sjq/3 is
a solution to the X3C instance. Therefore, PW with respect to plurality with runoff is
NP-complete.
2
Theorem 9 NcW is coNP-complete with respect to plurality with runoff, even when the
number of undetermined pairs in each vote is at most 4.
Proof. We prove coNP-hardness by a reduction from X3C. Given an X3C instance V =
{v1 , . . . , vq }, S = {S1 , . . . , St }, we construct a NcW instance as follows.
Alternatives: {c, d}  V  E, where E = {e1 , . . . , et(q+2)3 }.
First part P1 of the profile: for each j  t, we start with O(d, Sj , c, Others), and
subsequently obtain a partial order Oj by removing the orderings in ({d}  Sj )  {c}.
Second part P2 of the profile: P2 = P21  P22 , where P21 and P22 are defined as
follows.
 P21 : a set of t(q + 1) + q/3 votes, such that c is ranked in the top position t + 1
times; d is ranked in the top position q/3  1 times; and for each i  q, vi is
ranked in the top position t times.
 P22 : we first obtain, according to Lemma 2, a profile P22 such that the pairwise
score differences of {O(d, Sj , c, Others) : j  t}  P21  P22 satisfy the following
conditions.
1. D(c, d) = 2t + 1;
53

fiXia & Conitzer

2. for all i  q, D(vi , c) = 3.
By Lemma 2, the size of P22 is polynomial in t + q. Next, we obtain P22 from
P22 by raising an alternative in E to the top position in each vote, in such a way
that each vote in P22 ranks a different alternative in the top position.
We recall that for any profile P and any alternative c , P luP (c ) denotes the number of
times that c is ranked in the top position in P . We make the following observations on
{O(d, Sj , c, Others) : j  t}  P2 .
 D(c, d) = 2t + 1, and for all i  q, D(vi , c) = 3;
 P lu(c) = t + 1, P lu(d) = t  1 + q/3; for each i  q, P lu(vi ) = t; for each e  E,
P lu(e)  1.
It follows from the observations that in any extension of P1  P2 , c must enter the runoff;
also, in any extension, c defeats d in the pairwise election. Let P1  P2 (where P1 is an
extension of P1 ) be a profile in which c is not a co-winner. We must have that d does not
enter the runoff, which means that P luP1 P2 (d)  t  1. It follows that c  d in at least q/3
votes in P1 . However, by ranking c  d in a partial order Oi , we are forcing c  Si . Now,
the pairs of alternatives that enter the runoff (in parallel universes) are (c, v1 ), . . . , (c, vq ).
Since c loses in all these pairwise elections in the runoff (because, by assumption, c is not
a co-winner), we must have that for each vj , c  vj in at most one vote in P1 . Hence, a
solution to the complement of the NcW instance naturally corresponds to a solution to the
X3C instance. Conversely, a solution to the X3C instance corresponds to a solution to the
complement of the NcW instance. Therefore, NcW with respect to plurality with runoff is
coNP-complete.
2

5. Polynomial-time Algorithms for Possible and Necessary Winner
Problems
In this section we present polynomial-time algorithms for (1) NW and NcW with respect
to all positional scoring rules, maximin, and Bucklin, (2) PcW and NW with respect to
plurality with runoff. We recall that PW is NP-complete (Theorem 8) and NcW is coNPcomplete (Theorem 9), both with respect to plurality with runoff.
We note that positional scoring rules, maximin, and Bucklin are all based on some type
of scores, so if we can find an extension of the partial orders to linear orders so that the
score of c, denoted by S(c), is no more than the score of another alternative w, then c is
not the (unique) winner in this profile, and hence c is not a necessary winner. Therefore,
in the following algorithms for these rules, we check all alternatives w 6= c, and try to make
S(c)  S(w) as low as possible on a vote-by-vote basis (or equivalently, make S(w)  S(c)
as high as possible). For each vote O (partial order), there can be two cases. In the first
case, c 6O w. In this case, we only need to consider c and w separately, raising w as high
as possible and lowering c as low as possible. (This part of the algorithm has already been
considered in Konczak & Lang, 2005.) The following example, Example 3, illustrates this
idea.

54

fiDetermining Possible and Necessary Winners Given Partial Orders

Example 3 A partial order O is illustrated in Figure 4 (a). Let c = c2 and w = c5 . Since
c2 6O c5 , we can raise c5 as high as possible while lowering c2 as low as possible, as shown
in Figure 4 (b).
c5

c6

c2

c3

c1
c4

c1

c5

(a) A partial order O.

c6

c2

c3

c4

(b) An extension of O.

Figure 4: A partial order and its extension.
In the second case, c O w. This case is more complicated, and in what follows we
show how to minimize S(c)  S(w) for positional scoring rules, maximin, and Bucklin. For
plurality with runoff, we convert PcW into a maximum flow problem to solve it; this also
gives an algorithm for NW, simply by checking whether any other alternative is a possible
co-winner (see Proposition 1).
In this section, the input consists of C = {c, c1 , . . . , cm1 }, c (the alternative for which
we wish to decide whether or not it is a necessary (co-)winner), a profile Pposet of n partial
orders over C, and the voting rule r.
We first define some notation that will be used in the algorithms.
Definition 8 Given a partial order O and an alternative c, let UpO (c) = {c  C : c O c}
and DownO (c) = {c  C : c O c }. Given another alternative w such that c O w, let Os
c  w block be defined as follows: BlockO (c, w) = {c  C : c O c O w}.
That is, UpO (c) is the set of alternatives that are weakly preferred to c in O (including c
itself), and DownO (c) is the set of alternatives that c is weakly preferred to in O (including
c itself). If c O w, then BlockO (c, w) is the set of all the alternatives, including c and w,
that are ranked between c and w. It is easy to check that for any partial order O, and any
pair of alternatives c, w (with c O w), BlockO (c, w) = DownO (c)  UpO (w).
Example 4 Let O be the partial order illustrated in Figure 4 (a). We have that UpO (c2 ) =
{c1 , c2 }, UpO (c4 ) = {c1 , c2 , c3 , c4 , c5 }, DownO (c2 ) = {c2 , c3 , c4 }, DownO (c4 ) = {c4 }, and
BlockO (c2 , c4 ) = {c2 , c3 , c4 }.
The notion of a block is useful for the following reason. In the algorithm, we want to
think about an extension of the partial orders in which w does as well as possible, and c
does as poorly as possible. When c O w in some partial order O, we cannot rank c below
w; but at least it makes sense to have as few alternatives between them as possible. The
alternatives in the block are exactly the ones that need to be between them; we will rank
the other alternatives outside of the block. Then, the question is where to position the
block, and we will slide the block through the ranking.
Now we are ready to present the algorithms. We note that given a partial order O,
computing the UpO and DownO sets takes polynomial time. Let ~sm denote the scoring
vector of the positional scoring rule.
Algorithm 1 (Computing NW with respect to a positional scoring rule)
55

fiXia & Conitzer

1. For each partial order O  Pposet and each alternative c, compute UpO (c) and
DownO (c).
2. Repeat Steps 3ac for each w 6= c:
3a. Let S(w) = S(c) = 0.
3b. For each partial order O in Pposet ,
 if c 6O w, then (following Example 3) the lowest possible position for c is
the m + 1  |DownO (c)|th position, and the highest possible position for w is
the |UpO (w)|th position, so we add the scores ~sm (|UpO (w)|) and ~sm (m + 1 
|DownO (c)|) to S(w) and S(c), respectively;
 if c O w, then the highest that we can slide Os c  w block (as measured by cs
position, which is at the top of the block) is position |UpO (w) \ DownO (c)| + 1 (if
an alternative a is ranked above w in the partial order, then we will place it above
c, unless the partial order ranks c above a), and the lowest (as measured by ws
position, which is at the bottom of the block) is position m|DownO (c)\UpO (w)|
(if an alternative a is ranked below c in the partial order, then we will place it
below w, unless the partial order ranks a above w). Any position between these
extremes is also possible. We find the position that minimizes the score of c
minus the score of w, then add the scores that c and w get for these positions to
S(c) and S(w), respectively.
3c. If the result is that S(w)  S(c), then output that c is not a necessary winner
(terminating the algorithm).
4. Output that c is a necessary winner (if we reach this point).
The algorithm for computing NcW is obtained simply by checking whether S(w) > S(c)
in Step 4.
Proposition 3 Algorithm 1 checks whether or not c is a necessary winner for Pposet with
respect to a given positional scoring rule in polynomial time.
Proof. It is equivalent to check whether there exists an extension P of Pposet and an
alternative w 6= c, such that s(P, w)  s(P, c)that is, whether c is not a necessary (unique)
winner. To this end, for each O  Pposet , we maximize s(VO , w)s(VO , c) over all extensions
VO of O.
We recall that for each i  m, ~sm (i) is the score of the alternative that is ranked at
the ith position. For any extension VO of O, s(VO , w)  ~sm (|UpO (w)|) (because w cannot
be ranked higher than the |UpO (w)|th position) and s(VO , c)  ~sm (m + 1  |DownO (c)|)
(because c cannot be ranked lower than the (m + 1  |DownO (c)|)th position). These
two bounds can be achieved if c 6O w: for every d  C \ UpO (w), we add w  d to
O; and for every d  C \ DownO (c), we add d  c to O. We obtain a partial order O
this way, and we let VO be an (arbitrary) linear order that extends O . It follows that
s(VO , w)  s(VO , c) = ~sm (|UpO (w)|)  ~sm (m + 1  |DownO (c)|).
However, if c O w, there may not exist VO in which s(VO , w) = ~sm (|UpO (w)|) and
s(VO , c) = ~sm (m + 1  |DownO (c)|) hold simultaneously. We note that in any VO that
56

fiDetermining Possible and Necessary Winners Given Partial Orders

maximizes s(VO , w)  s(VO , c), the only alternatives between c and w must be those in
BlockO (c, w). Therefore, for each d  C such that d O w and c 6O d, we must have that
d VO c; and for each d  C such that c O d and d 6O w, we must have that w VO d. It
follows that s(VO , w)  s(VO , c)  maxl (~sm (l + |BlockO (c, w)|  1)  ~sm (l)), where l ranges
between |UpO (w) \ DownO (c)| + 1 and m  |DownO (c) \ UpO (w)|. Let VO be an extension
of O restricted to C \ BlockO (c, w) in which UpO (w) \ DownO (c) is ranked at the top and
DownO (c) \ UpO (w) is ranked at the bottom. For each d  C \ (UpO (w)  DownO (c))
and each d  BlockO (c, w), we must have d 6O d and d 6O d. Therefore, for each
|UpO (w) \ DownO (c)| + 1  l  m  |DownO (c) \ UpO (w)|, we can put BlockO (c, w) between
the (l  1)th position and the lth position in VO , to obtain a linear order that extends O.
This proves the correctness of Step 3b, which computes maxVO (s(VO , w)  s(VO , c)). It
follows that the algorithm correctly checks whether or not c is a necessary winner.
2
We now move on to the maximin rule. We note that c is not a necessary winner for Pposet
with respect to maximin if and only if there exists a profile of linear orders P extending
Pposet , and two alternatives w and w , such that for all alternatives d, NP (w, d)  NP (c, w ).
We recall that NP (w, d) is the number of votes in P where w  d. Therefore, our algorithm
considers all pairs (w, w ), and then checks whether there exists an extension of the input
partial orders for which the inequality holds for all alternatives d. To perform such a check,
in each partial order, we would like to rank w ahead of c, and also to rank w as high as
possible. However, these two objectives may conflict: it may be the case that if we rank
c ahead of w , then we can rank w higher than in the case where we rank w ahead of c.
In this case, we first place w ahead of c, and then rank w as high as possible under this
additional constraint. This works for the following reason. Let O  Pposet be a partial
order where c 6O w and w 6O c; let V be an arbitrary extension of O in which w V c
and let V  be an arbitrary extension of O in which c V  w . For any d  C, we have
that N{V } (w, d)  N{V } (c, w )  0  N{V  } (w, d)  N{V  } (c, w ), which means that enforcing
w  c is always at least as good as enforcing c  w .
Algorithm 2 (Computing NW with respect to maximin)
1. For each partial order O  Pposet and each alternative c, compute UpO (c).
2. Repeat 3ac for all pairs w, w , where c 6= w and c 6= w .
3a. Let S(c, w ) = 0, and for each alternative d 6= w, let S(w, d) = 0.
3b. For each partial order O in Pposet ,
 if c 6O w , then add w  c to O and raise w as high as possible; for each
d 6= w, if, in the resulting vote, w is ahead of d (that is, d 6 UpO (w) and if
c  UpO (w), then d 6 UpO (w )), then add 1 to S(w, d).
 if c O w , then raise w as high as possible; add 1 to S(c, w ); for each d 6= w,
if, in the resulting vote, w is ahead of d (that is, d 6 UpO (w)), then add 1 to
S(w, d).
3c. Check if for all d 6= w, S(w, d)  S(c, w ); if the answer is yes, then output that
c is not a necessary winner (terminating the algorithm).
4. Output that c is a necessary winner.
57

fiXia & Conitzer

The algorithm for computing NcW with respect to maximin is similar: the only modification
is that in Step 3, we check if for all alternatives d 6= w, S(w, d) > S(c, w ).
Proposition 4 Algorithm 2 checks whether or not c is a necessary winner for Pposet with
respect to maximin in polynomial time.
Proof. The function S(x, y) computed in the algorithm is the number of times x is preferred
to y in an extension of Pposet . For any partial order O, we let VO be the extension computed
in Step 3b. Let g(V, d) = NV (w, d)  NV (c, w ). We next prove that for each d 6= w and
each extension VO of O, g(VO , d)  g(VO , d). If c 6O w and c VO w , then g(VO , d) 
0  g(VO , d) (because NVO (c, w ) = 0 and NVO (c, w ) = 1). If c 6O w and w VO c, then
NVO (c, w ) = NVO (c, w ). We note that VO is obtained by raising w as high as possible in O
while w  c, which means that NVO (w, d)  NVO (w, d). It follows that g(VO , d)  g(VO , d).
Similarly, if c O w , then we also have that for all d 6= w, NVO (w, d)  NVO (w, d).
Therefore,Pfor any extension P of Pposet and any d 6= w, S(w, d)  S(c, w ) = NP (w, d) 
NP (c, w )  OPposet g(VO , d), and when P is the profile computed in Step 3b, the inequality becomes an equality. It follows that the algorithm is correct.
2
Now we move on to the Bucklin rule. We note that c is not a necessary winner of
Pposet with respect to Bucklin, if and only if there exists an extension P of Pposet and an
alternative w, such that either ws Bucklin score is 1, or there exists 2  k  m, such that
w is among the top k for more than n2 votes (meaning that ws Bucklin score is no more
than k), and c is among the top k  1 for at most n2 votes (meaning that cs Bucklin score
is no less than k). Therefore, like Algorithm 1, the algorithm for Bucklin considers each
alternative w, computes the possible positions for the blocks BlockO (c, w), and then checks
for all k from 1 to m whether the above condition can be made to hold.
In the algorithm, if c 6Oj w, then High(j) is the highest position that w reaches in an
extension of Oj , and Low(j) is the lowest position that c reaches in an extension of Oj . If
c Oj w, then High(j) is the highest position of c given that c and w are ranked as close to
each other as possible, Low(j) is the lowest position of c given that c and w are ranked as
close to each other as possible, and Length(j) is the size of BlockOj (c, w). For any i  m
and any d  {c, w}, let S(i, d) denote the minimum number of times that d is ranked in
the top i positions, where the minimum is taken over all optimal extensions of Pposet (we
will elaborate on the meaning of optimality later). U (k) is the number of partial orders
for which we will have to compute where to put the block BlockOj (c, w) to make c not a
necessary unique winner. That is, U (k) is the number of partial orders for which there
exists an extension in which c is in the top k  1 positions and w is in the top k positions,
as well as another extension in which c is not in the top k  1 positions and w is not in the
top k positions.
Algorithm 3 (Computing NW with respect to Bucklin)
1. For each partial order O  Pposet and each alternative c, compute UpO (c) and
DownO (c).
2. Repeat Steps 3ad for all w 6= c:
3a. For each j  n, let High(j) = Low(j) = Length(j) = 0. For each i  m, let
S(i, c) = S(i, w) = U (i) = 0.
58

fiDetermining Possible and Necessary Winners Given Partial Orders

3b. For each partial order Oj in Pposet ,
 if c 6Oj w, then let Length(j) = 0, and let High(j) = |UpOj (w)|, Low(j) =
m + 1  |DownOj (c)|;
 if c Oj w, then let Length(j) = |BlockOj (c, w)|, High(j) = |UpOj (w) \
DownOj (c)| + 1, Low(j) = m + 1  |DownOj (c)|.
3c. For each k  m, each j  n,
 if Length(j) = 0, then add 1 to S(k, w) if High(j)  k, and add 1 to S(k  1, c)
if Low(j)  k  1;
 if Length(j) > 0, then add 1 to S(k, w) if either Low(j) + Length(j)  1  k, or
the following two conditions both hold: Low(j)  k1 and High(j)+Length(j)
1  k. Also, add 1 to S(k1, c) if Low(j)  k1; add 1 to U (k) if Low(j) > k1
and High(j) + Length(j)  1  k.
3d. If S(1, w) + U (1) > n2 , or there exists 2  k  m such that S(k, w) > S(k  1, c),
S(k  1, c)  n2 , and S(k, w) + U (k) > n2 , then output that c is not a necessary
winner (terminating the algorithm).
4. Output that c is a necessary winner.
The algorithm for computing NcW is obtained by making following changes to Steps 3c and
3d as follows.
3c . For each k  m, each j  n,
 if Length(j) = 0, then add 1 to S(k, w) if High(j)  k, and add 1 to S(k, c) if
Low(j)  k;
 if Length(j) > 0, then add 1 to S(k, w) if either Low(j) + Length(j)  1  k, or
the following two conditions both hold: Low(j)  k and High(j) + Length(j)  1 
k. Also, add 1 to S(k, c) if Low(j)  k; add 1 to U (k) if Low(j)  k + 1 and
High(j) + Length(j)  1  k.
3d . If there exists 0  l  U (1) such that S(1, w) + l > n2  S(1, c) + l, or there exists
2  k  m and l  U (k) such that S(k, w) + l > n2  S(k, c) + l, then output that c
is not a necessary co-winner (terminating the algorithm).
Proposition 5 Algorithm 3 checks whether or not c is a necessary winner for Pposet with
respect to Bucklin in polynomial time.
Proof. Similarly as in the case of positional scoring rules, for Bucklin, if c 6O w, then we
can simply rank c as low as possible while rank w as high as possible, independently. On
the other hand, if c O w, then we can without loss of generality place as few alternatives
between c and w as possible, but the question is where to place the c  w block. The
algorithm will consider a particular k, and try to make it so that w is among the top k for
more than half the votes, and c is among the top k  1 for at most half the votes. For a
particular vote with c O w, depending on where the block is placed, either (1) c is among
the top k  1 and w is among the top k; or, (2) c is among the top k  1 and w is not among
the top k; or, (3) c is not among the top k  1 and w is not among the top k. However,
59

fiXia & Conitzer

not all three of these possibilities may exist for a particular vote. The algorithm will never
choose (2) unless that is the only option, so that the only difficult case is when a decision
must be made between (1) and (3).
We recall that for any i  m and any d  {c, w}, S(i, d) is the minimum number of
times that d is ranked within top i positions, where the minimum is taken over all extensions
of Pposet that are consistent with the observations in the previous paragraph (specifically,
option (2) is never chosen unless there is no other choice). U (k) is the number of partial
orders for which there exists an extension in which c is ranked within top k  1 positions
and w is ranked within top k positions, as well as an extension in which c is not ranked
within top k  1 positions and w is not ranked within top k positions (that is, we have a
choice between (1) and (3)).
For each k  m, and each j  n, we consider how to extend Oj .
 If c 6Oj w, then the positions of c and w are already determined by our previous
observations (w is ranked as high as possible and c is ranked as low as possible).
 If c Oj w and High(j)  k, then c cannot be ranked within top k  1 positions and
w cannot be ranked within top k positions; therefore, we add 0 to S(k  1, c) and
S(k, w).
 If c Oj w, High(j) < k and High(j) + Length(j)  1 > k, then c can be ranked within
top k  1 positions, but w cannot be ranked within top k positions. There are two
sub-cases: (1) if Low(j)  k, then we rank c in the Low(j)th position, and henceforth
add 0 to both S(k  1, c) and S(k, w); (2) if Low(j) < k, then c is inevitably ranked
within top k  1 positions, and w cannot be ranked within top k positions, which
means that we add 1 to S(k  1, c) and 0 to S(k, w).
 The final case is where c Oj w, High(j) < k and High(j) + Length(j)  1  k. Again,
there are two subcases: (1) if Low(j) < k, then it means that c must be ranked within
top k  1 positions. Therefore we rank w in the top k positions, and add 1 to both
S(k 1, c) and S(k, w); (2) if Low(j)  k, then it means that we have three options for
an extension of Oj , corresponding to the cases (1), (2), (3) discussed in the beginning
of the proof.
(1) cs position is within top k  1 and ws position is within top k.
(2) cs position is within top k 1 and ws position is not within top k (which implies
that Length(i) > 2).
(3) cs position is not within top k  1 and ws position is not within top k.
As we already discussed, option (2) is suboptimal. Therefore, we add 0 to both
S(k  1, c) and S(k, w), and add 1 to U (k).
The only remaining decision is for how many of the votes corresponding to the number
U (k) to choose option (1) (as opposed to option (3)). This corresponds to Step 3d of the
algorithm, where it checks whether there exists a way of choosing the number of extensions
(but no more than U (k)) that choose (1) in such a way that c is not the winner.
Therefore, the algorithm is correct.
2
60

fiDetermining Possible and Necessary Winners Given Partial Orders

Finally, we consider the possible co-winner problem with respect to plurality with runoff.
We will show that this problem can be solved in polynomial time. From this, it also follows
that the necessary (unique) winner problem can be solved in polynomial time (Proposition 1). In contrast, we have already shown that for plurality with runoff, the possible
unique winner problem is NP-complete (Theorem 8) and the necessary co-winner problem
is coNP-complete (Theorem 9).
Our algorithm for determining whether c is a possible co-winner is based on the following
key observation: c is a possible co-winner for Pposet with respect to plurality with runoff
if and only if there exists an extension of Pposet , denoted by P  , an alternative d 6= c, and
two natural numbers l1 , l2 , such that (1) c is preferred to d in at least half of votes (linear
orders) in P  , and (2) P luP  (c) = l1 , P luP  (d) = l2 , and for each alternative c (c 6= c and
c 6= d), P luP  (c )  min{l1 , l2 }. That is, c and d can enter the runoff (there could be other
pairs of alternatives who enter the runoff in some parallel universe) and c can then defeat
d in the runoff.
For each i  m  1, we let i denote the number of partial orders O  Pposet such
that ci O c. We recall that T op(O) denote the set of alternatives c for which there exists
at least one extension of O where c is in the top position. Based on the observations in
the previous paragraph, we will consider all possibilities for l1 , l2 , and d (we will use i to
denote possibilities for the index of d), and solve a maximum flow problem instance for each
possibility.9 Specifically, for every l1 , l2  n and every i  m  1 (with i  n/2), we
define a maximum flow problem Fl1 ,l2 ,i as follows (illustrated in Figure 5, in which i = 1).
c

1

O1
1

1

c1

n/2  1

l1

c1
l2

1

c2
..
.

s

lmin

1

..
.

1

cm1

t

t

1

On

n  l1  l2

lmin

Figure 5: The maximum flow problem Fl1 ,l2 ,1 .
Vertices: s, O1 , . . . , On , ci , c, c1 , . . . , cm1 , t , t.
Edges: we have the following five types of edges.
 Edges from s to {O1 , . . . , On }: for every i  n, there is an edge (s, Oi ) with
capacity 1.
9. Our original proof used a minimum cost flow problem, but one of the anonymous reviewers pointed out
how to modify this approach into the simpler maximum flow approach presented here, as well as two
papers (Gusfield & Martel, 2002; Russell & Walsh, 2009) where maximum flow problems were used to
solve other election problems, for which we thank the reviewer.

61

fiXia & Conitzer

 Edges from {O1 , . . . , On } to {ci , c, c1 , . . . , cm1 }: we have
 for every j  n and every d  C such that d 6= ci , if d  T op(Oj ), then there
is an edge (Oj , d) with capacity 1;
 for every j  n, if ci  T op(O) and ci Oj c, then there is an edge (Oj , ci )
with capacity 1;
 for any j  n, there is an edge (Oj , ci ) with capacity 1 if ci  T op(O) and
ci 6Oj c.
 Edge from ci to ci : there is an edge (ci , ci ) with capacity n/2  i .
 Edges from C \ {c, ci } to t : for every c  C \ {c, ci }, we have an edge (c , t )
with capacity lmin = min{l1 , l2 }.
 Edges from {c, ci , t } to t: we have
 an edge (c, t) with capacity l1 ;
 an edge (ci , t) with capacity l2 ;
 an edge (t , t) with capacity n  l1  l2 .
Next, we prove that c is a possible co-winner for Pposet with respect to plurality with runoff
if and only if there exist l1 , l2  n and i  m  1 such that Fl1 ,l2 ,i has a solution in which
the value of the flow is n.
Because all parameters in Fl1 ,l2 ,i are integers, if there exists a solution to Fl1 ,l2 ,i , then
there must also exists an integer solution. First, we show how to convert an integer solution
to Fl1 ,l2 ,i to a solution to the PcW problem with respect to plurality with runoff. Let f
be an integer solution to Fl1 ,l2 ,i , that is, f : Vertices  Vertices  Z. We construct an
extension P  = (V1 , . . . , Vn ) of Pposet as follows:
 for each j  n, if f (Oj , ci ) = 1 then we let Vj be an extension of Oj in which ci is
ranked in the top position;
 for each j  n and each d  C \ {ci }, if f (Oj , d) = 1 then we let Oj be an extension
of Oj in which d is ranked in the top position, and c is ranked as high as possible.
Because the value of f is n, the plurality score of c is l1 and the plurality score of ci is
l2 , while the plurality score of ci (i 6= i ) is at most lmin . Therefore, c and ci enter the
runoff together in one parallel universe. Now, the capacity constraint on the edge (ci , ci )
ensures that c will win the runoff: the reason is that if we rank ci first in a vote in which
we could have ranked c ahead of ci , then it will contribute 1 to the flow on this edge.
Moreover, the capacity of the edge (ci , ci ) is n/2  i , which means that ci  c in at
most i + (n/2  i )  n/2 votes of P  . Hence, c is a co-winner for P  .
Conversely, if there exists an extension P  of P such that c is a co-winner of P  , then
there exists a ci such that in some parallel universe, {c, ci } enter the runoff, and c wins
this runoff. Let l1 , l2 be the plurality scores of c, ci , respectively. Then, this extension can
be converted to a solution to Fl1 ,l2 ,i (we omit the details because they are similar to the
details for the other direction).
Therefore, the following algorithm solves PcW with respect to plurality with runoff.
Algorithm 4 (Computing PcW with respect to plurality with runoff )
62

fiDetermining Possible and Necessary Winners Given Partial Orders

1. For each O  Pposet , compute T op(O) and UpO (c). For each i  m  1, let i = |{O 
Pposet : ci  UpO (c)}|.
2. Repeat Steps 3ab for all i  m  1 and l1 , l2  n:
3a. Construct the maximum flow problem Fl1 ,l2 ,i .
3b. Solve Fl1 ,l2 ,i by the FordFulkerson algorithm (Cormen, Leiserson, Rivest, &
Stein, 2001). If the maximum flow is n, then output that c is a possible cowinner. Terminate the algorithm.
4. Output that c is not a possible co-winner.
Proposition 6 Algorithm 4 checks whether or not c is a possible co-winner for Pposet with
respect to plurality with runoff in polynomial time.
We recall from the proof of Proposition 1 that c is a necessary unique winner if and only
if no other alternative is a possible co-winner. Therefore, we naturally obtain an algorithm
for NW, simply by using Algorithm 4 to check if any alternative other than c is a possible
co-winner.
Proposition 7 Algorithm 4 can be used to check whether or not c is a necessary unique
winner for Pposet with respect to plurality with runoff in polynomial time.

6. Conclusion and Future Work
We considered the following problem: given a set of alternatives, a voting rule, and a
set of partial orders, which alternatives are possible/necessary winners? That is, which
alternatives would win for some/all extension of the partial orders? We considered the case
where the votes are not weighted and the number of alternatives is not bounded. Table 1 in
the introduction summarizes our results. These results hold whether or not the alternative
must be the unique winner, or merely a co-winner, unless specifically mentioned.
In this paper, there was no restriction on the partial orders. However, if the reason
that we have partial orders is that preferences are submitted as CP-nets, this introduces
additional structure on the partial orders; that is, not all partial orders correspond to a
CP-net. Hence, while our positive results would still apply, it is not immediately obvious
that our negative results would still apply.
Another approach is to approximate the sets of possible/necessary winners. More precisely, we are asked to output a superset (respectively, subset) of possible (respectively,
necessary) winners such that the size of the output set should be within a fixed ratio of
the number of the possible (respectively, necessary) winners. Pini et al. (2007) proved the
inapproximability of the set of possible/necessary winners for the single transferable vote
rule (STV) rule. We conjecture that similar inapproximability results hold for most of
the common voting rules studied in this paper (for which the possible/necessary winner
problems are (co-)NP-complete).
63

fiXia & Conitzer

Acknowledgments
We thank Nadja Betzler, Jerome Lang, Toby Walsh, the anonymous reviewers for AAAI-08
and JAIR, and all participants of the Dagstuhl Seminar 07431: Computational Issues in
Social Choice for helpful discussions and comments. Lirong Xia is supported by a James
B. Duke Fellowship and Vincent Conitzer is supported by an Alfred P. Sloan Research Fellowship. This work is supported by NSF under award numbers IIS-0812113 and CAREER
0953756.

References
Bachrach, Y., Betzler, N., & Faliszewski, P. (2010). Probabilistic possible winner determination. In Proceedings of the National Conference on Artificial Intelligence (AAAI),
pp. 697702, Atlanta, GA, USA.
Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice and Welfare, 8 (4), 341354.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989a). The computational difficulty of manipulating an election. Social Choice and Welfare, 6 (3), 227241.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989b). Voting schemes for which it can be
difficult to tell who won the election. Social Choice and Welfare, 6, 157165.
Baumeister, D., & Rothe, J. (2010). Taking the final step to a full dichotomy of the possible
winner problem in pure scoring rules. In Proceedings of The 19th European Conference
on Artificial Intelligence (ECAI), pp. 10191020, Lisbon, Portugal.
Betzler, N., & Dorn, B. (2010). Towards a dichotomy for the possible winner problem in
elections based on scoring rules. Journal of Computer and System Sciences, 76 (8),
812836.
Betzler, N., Hemmann, S., & Niedermeier, R. (2009). A multivariate complexity analysis of
determining possible winners given incomplete votes. In Proceedings of the TwentyFirst International Joint Conference on Artificial Intelligence (IJCAI), pp. 5358,
Pasadena, CA, USA.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: A tool
for representing and reasoning with conditional ceteris paribus statements. Journal
of Artificial Intelligence Research, 21, 135191.
Chevaleyre, Y., Lang, J., Maudet, N., & Monnot, J. (2010). Possible winners when new candidates are added: the case of scoring rules. In Proceedings of the National Conference
on Artificial Intelligence (AAAI), Atlanta, GA, USA.
Chevaleyre, Y., Lang, J., Maudet, N., Monnot, J., & Xia, L. (2010). New candidates
welcome! Possible winners with respect to the addition of new candidates. Technical
report, Cahiers du LAMSADE 302, Universite Paris-Dauphine.
Conitzer, V. (2009). Eliciting single-peaked preferences using comparison queries. Journal
of Artificial Intelligence Research, 35, 161191.
64

fiDetermining Possible and Necessary Winners Given Partial Orders

Conitzer, V., Rognlie, M., & Xia, L. (2009). Preference functions that score rankings and
maximum likelihood estimation. In Proceedings of the Twenty-First International
Joint Conference on Artificial Intelligence (IJCAI), pp. 109115, Pasadena, CA, USA.
Conitzer, V., & Sandholm, T. (2002). Vote elicitation: Complexity and strategy-proofness.
In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 392
397, Edmonton, AB, Canada.
Conitzer, V., & Sandholm, T. (2005a). Common voting rules as maximum likelihood estimators. In Proceedings of the 21st Annual Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 145152, Edinburgh, UK.
Conitzer, V., & Sandholm, T. (2005b). Communication complexity of common voting rules.
In Proceedings of the ACM Conference on Electronic Commerce (EC), pp. 7887,
Vancouver, BC, Canada.
Conitzer, V., Sandholm, T., & Lang, J. (2007). When are elections with few candidates
hard to manipulate?. Journal of the ACM, 54 (3), 133.
Cormen, T., Leiserson, C., Rivest, R., & Stein, C. (2001). Introduction to Algorithms
(Second edition). MIT Press.
Elkind, E., Faliszewski, P., & Slinko, A. (2009). Swap bribery. In Proceedings of the 2nd
International Symposium on Algorithmic Game Theory.
Elkind, E., & Lipmaa, H. (2005). Hybrid voting protocols and hardness of manipulation.
In Annual International Symposium on Algorithms and Computation (ISAAC), 3827
of Lecture Notes in Computer Science, pp. 206215, Sanya, Hainan, China.
Erdelyi, G., Fernau, H., Goldsmith, J., Mattei, N., Raible, D., & Rothe, J. (2009). The complexity of probabilistic lobbying. In The 1st International Conference on Algorithmic
Decision Theory, pp. 8697, Venice, Italy.
Erdos, P., & Moser, L. (1964). On the representation of directed graphs as unions of
orderings. Math. Inst. Hung. Acad. Sci., 9, 125132.
Faliszewski, P. (2008). Nonuniform bribery. In Proceedings of the Seventh International
Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp.
15691572, Estoril, Portugal.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: ties matter.
In Proceedings of the Seventh International Joint Conference on Autonomous Agents
and Multi-Agent Systems (AAMAS), pp. 983990, Estoril, Portugal.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2010). Manipulation of copeland elections. In Proceedings of the Nineth International Joint Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS), pp. 367374, Toronto, Canada.
Garey, M., & Johnson, D. (1979). Computers and Intractability. W. H. Freeman and
Company.
Gibbard, A. (1973). Manipulation of voting schemes: a general result. Econometrica, 41,
587602.
Gusfield, D., & Martel, C. (2002). The structure and complexity of sports elimination
numbers. Algorithmica, 32, 7386.
65

fiXia & Conitzer

Hazon, N., Aumann, Y., Kraus, S., & Wooldridge, M. (2008). Evaluation of election outcomes under uncertainty. In Proceedings of the Seventh International Joint Conference
on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 959966, Estoril,
Portugal.
Hemaspaandra, E., & Hemaspaandra, L. A. (2007). Dichotomy for voting systems. Journal
of Computer and System Sciences, 73 (1), 7383.
Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (1997). Exact analysis of Dodgson
elections: Lewis Carrolls 1876 voting system is complete for parallel access to NP.
Journal of the ACM, 44 (6), 806825.
Konczak, K., & Lang, J. (2005). Voting procedures with incomplete preferences. In Multidisciplinary Workshop on Advances in Preference Handling.
Lang, J. (2007). Vote and aggregation in combinatorial domains with structured preferences. In Proceedings of the Twentieth International Joint Conference on Artificial
Intelligence (IJCAI), pp. 13661371, Hyderabad, India.
Lang, J., Pini, M. S., Rossi, F., Venable, K. B., & Walsh, T. (2007). Winner determination
in sequential majority voting. In Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence (IJCAI), pp. 13721377, Hyderabad, India.
Lang, J., & Xia, L. (2009). Sequential composition of voting rules in multi-issue domains.
Mathematical Social Sciences, 57 (3), 304324.
McGarvey, D. C. (1953). A theorem on the construction of voting paradoxes. Econometrica,
21 (4), 608610.
Parkes, D. (2006). Iterative combinatorial auctions. In Cramton, P., Shoham, Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 2, pp. 4177. MIT Press.
Pini, M. S., Rossi, F., Venable, K. B., & Walsh, T. (2007). Incompleteness and incomparability in preference aggregation. In Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence (IJCAI), pp. 14641469, Hyderabad, India.
Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity of the winner problem for
Young elections. In Theory of Computing Systems, Vol. 36(4), pp. 375386. SpringerVerlag.
Russell, T., & Walsh, T. (2009). Manipulating tournaments in cup and round robin competitions. In Proceedings of the First International Conference on Algorithmic Decision
Theory (ADT), Lecture Notes in Artificial Intelligence 5783, pp. 2637.
Sandholm, T., & Boutilier, C. (2006). Preference elicitation in combinatorial auctions. In
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 10,
pp. 233263. MIT Press.
Satterthwaite, M. (1975). Strategy-proofness and Arrows conditions: Existence and correspondence theorems for voting procedures and social welfare functions. Journal of
Economic Theory, 10, 187217.
Walsh, T. (2007). Uncertainty in preference elicitation and aggregation. In Proceedings of
the National Conference on Artificial Intelligence (AAAI), pp. 38, Vancouver, BC,
Canada.
66

fiDetermining Possible and Necessary Winners Given Partial Orders

Xia, L., Lang, J., & Monnot, J. (2011). Possible winners when new alternatives join:
New results coming up!. To apprea in Proceedings of the Tenth International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS).
Xia, L., & Conitzer, V. (2008). Determining possible and necessary winners under common voting rules given partial orders. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), pp. 196201, Chicago, IL, USA.
Xia, L., Conitzer, V., & Procaccia, A. D. (2010). A scheduling approach to coalitional
manipulation. In Proceedings of the ACM Conference on Electronic Commerce (EC),
pp. 275284, Boston, MA, USA.
Xia, L., Lang, J., & Ying, M. (2007a). Sequential voting rules and multiple elections paradoxes. In Proceedings of the Eleventh Conference on Theoretical Aspects of Rationality
and Knowledge (TARK), pp. 279288, Brussels, Belgium.
Xia, L., Lang, J., & Ying, M. (2007b). Strongly decomposable voting rules on multiattribute
domains. In Proceedings of the National Conference on Artificial Intelligence (AAAI),
pp. 776781, Vancouver, BC, Canada.
Xia, L., Zuckerman, M., Procaccia, A. D., Conitzer, V., & Rosenschein, J. (2009). Complexity of unweighted coalitional manipulation under some common voting rules. In
Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI), pp. 348353, Pasadena, CA, USA.
Zuckerman, M., Procaccia, A. D., & Rosenschein, J. S. (2009). Algorithms for the coalitional
manipulation problem. Artificial Intelligence, 173 (2), 392412.

67

fi