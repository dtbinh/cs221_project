Journal of Articial Intelligence Research 42 (2011) 529-573

Submitted 08/11; published 11/11

Cloning in Elections: Finding the Possible Winners
Edith Elkind

eelkind@ntu.edu.sg

School of Physical and Mathematical Sciences
Nanyang Technological University, Singapore

Piotr Faliszewski

faliszew@agh.edu.pl

AGH University of Science and Technology
Krakow, Poland

Arkadii Slinko

slinko@math.auckland.ac.nz

Department of Mathematics
University of Auckland, Auckland, New Zealand

Abstract
We consider the problem of manipulating elections by cloning candidates. In our model,
a manipulator can replace each candidate c by several clones, i.e., new candidates that are
so similar to c that each voter simply replaces c in his vote with a block of these new
candidates, ranked consecutively. The outcome of the resulting election may then depend
on the number of clones as well as on how each voter orders the clones within the block. We
formalize what it means for a cloning manipulation to be successful (which turns out to be
a surprisingly delicate issue), and, for a number of common voting rules, characterize the
preference proles for which a successful cloning manipulation exists. We also consider the
model where there is a cost associated with producing each clone, and study the complexity
of nding a minimum-cost cloning manipulation. Finally, we compare cloning with two
related problems: the problem of control by adding candidates and the problem of possible
(co)winners when new alternatives can join.

1. Introduction
In many real-life elections, some of the candidates may have fairly similar positions on major
issues, yet disagree on what is the best way to implement their common goals. Under many
voting rules, and most glaringly under Plurality voting, these candidates run the risk of
splitting the vote and losing to a candidate with an opposing program. This phenomenon
can be exploited to alter the election outcome. For instance, New York Times wrote about
a Republican political operative who recruited drifters and homeless people onto the Green
Party ballot and freely admitted that these candidacies may siphon some support from the
Democrats and therefore help Republicans (Lacey, 2010).
Such scenarios have been extensively studied in the (computational) social choice literature (see Section 6 for an overview). Depending on whether the manipulation is contemplated by one of the candidates or by an external party, this issue is known as strategic
candidacy problem (this term was coined in Dutta, Jackson, & Le Breton, 2001, 2002) or
the problem of control by adding candidates.
In this paper we will address a variant of this problem that is known as cloning. It is
characterized by the following feature: each new candidate must be very similar to one of
the existing candidates. This form of manipulative behavior was rst identied and studied
c
2011
AI Access Foundation. All rights reserved.

fiElkind, Faliszewski & Slinko

by Tideman (1987), who also gave a now classic example of a cloning strategy. Tideman
wrote: When I was 12 years old I was nominated to be treasurer of my class at school.
A girl named Michelle was also nominated. I relished the prospect of being treasurer, so I
made a quick calculation and nominated Michelles best friend, Charlotte. In the ensuing
election I received 13 votes, Michelle received 12, and Charlotte received 11, so I became
treasurer(Tideman, 1987, p. 1). The calculation was that, being friends, Michelle and
Charlotte are similar and that their electorate will be split.
In Tidemans example, the cloned alternative lost the election. However, one can also
imagine scenarios where by cloning an alternative we increase its chances of winning. For
example, suppose that an electronics website runs a competition for the best digital camera
by asking consumers to vote for their two favorite models from a given list. If the list
contains one model of each brand, and 60% of the consumers prefer Sony to Nikon to
Kodak, while the remaining consumers prefer Kodak to Nikon to Sony, then Nikon will win
the competition. On the other hand, if Sony is represented by two similar models, then the
Sony customers are likely to vote for these two models of Sony, and the competition will
be won by a Sony camera.
Just as in the general candidate addition scenario, cloning presents an opportunity for a
party that is interested in manipulating the outcome of a preference aggregation procedure,
such as an election or a consumer survey. Such a partymost likely, a campaign manager for
one of the candidatesmay invest in creating clones of one or more alternatives in order to
make its most preferred alternative (or one of its clones) win the election. This campaign
management strategy has certain advantages over introducing an entirely new candidate:
In the latter case, it may be hard to predict how the voters rank the new candidate, so the
campaign manager would have to either invest in eliciting the new candidates rankings, or
be prepared to deal with rankings that dier from her initial expectations. In comparison,
the outcomes of cloning are much more predictable, and therefore manipulation by cloning
may be easier to implement. A natural question, then, is which voting rules are resistant to
such manipulation, and whether the manipulator can compute the optimal cloning strategy
for a given election in a reasonable amount of time.
As mentioned above, the rst study of cloning was undertaken by Tideman (1987), who
introduced the concept of independence of clones as a criterion for voting rules. Apparently
unaware of Tidemans work, Laond, Laine, and Laslier (1996) introduced the notion of
composition consistency, which is an analogue of independence of clones for tournament
solutions (Laslier, 1997), i.e., voting rules that are dened on the majority relation that
corresponds to the voters preferences. Later, Laslier (1996, 2000) introduced the notion of
cloning consistency, which is equivalent to independence of clones. We will discuss these
results in more detail in Section 6.
In all these papers, the authors concentrated on nding out whether a certain voting
rule is independent of clones or on constructing new rules with this property. In our work,
we take a somewhat dierent perspective: Instead of looking at cloning as a manipulative
action that should be prevented, we view cloning as a campaign management tool. This
point of view raises a number of questions that have not been consideredor have been
considered from a dierent anglein the previous work:
What does it mean for cloning to be successful? We assume that the campaign manager can produce clones of existing candidates, and the voters rank them in response.
530

fiCloning in Elections

We assume that clones are similar enough to be ranked as a group by each voter;
however, the order of clones in such groups may dier from one voter to another.
Since the campaign manager cannot control or predict the order of clones in each
voters ranking, we assume that this order is random, i.e., each voter ranks the cloned
candidates in each possible order with the same probability; indeed, this would be
the case if all clones were indistinguishable. In this probabilistic model, each cloning
strategy succeeds with a certain probability. Let q be some real number between 0 and
1. We say that manipulation by cloning is q-successful if the probability of electing a
desired candidate p is at least q. We focus on two extreme cases: (1) p wins under
each possible ordering of the clones, and (2) p wins under at least one ordering of
the clones. In case (1), the cloning is 1-successful; in case (2), following the notation
typically used when dealing with limits in continuous mathematics, we will say that
the cloning is 0+ -successful.
In which instances of elections can cloning be successful? While the previous work
shows that many well-known voting rules are susceptible to cloning, no attempt has
been made to characterize the elections in which a specic candidate can be made
a winner with respect to a given voting rule by means of cloning. However, from
the point of view of a campaign manager who considers cloning as one of the ways
to run the campaign, it would be important to know if he can change the outcome
of a given election by a cloning manipulation. Thus, in this paper we provide such
characterization results for several prominent voting rules. Often, the candidates for
which a successful cloning manipulation exists can be characterized in terms of wellknown notions of social choice such as Pareto undominated alternative (a candidate
c is Pareto undominated if for every other candidate c there is a voter that prefers c
to c ), Condorcet loser (candidate c is a Condorcet loser if for every other candidate
c more than half of the voters prefer c and c), or Uncovered Set (see Section 5.3 or
Miller, 1977; Fishburn, 1977; Laslier, 1997).
Which candidates can be cloned and to what extent? The existing research on cloning does not place any restrictions on the number of clones that can be introduced,
or on the identities of the candidates that can be cloned. On the other hand, it is clear
that in practical campaign management scenarios these issues cannot be ignored; not
all candidates can be cloned, and creating a clone of a given candidate may be costly.
Thus, we consider settings in which each clone of each candidate comes at a cost, and
we seek a least expensive successful cloning strategy. We will mostly focus on the
standard model where clones come at zero cost, and on the unit cost model, where
each clone has the same cost.
What is the computational complexity of nding cloning strategies? Finally, we
investigate the computational complexity of nding successful cloning strategies. In
practice, it is not sucient to know that cloning might work: We need to know
exactly which strategy to use. We believe that our paper is the rst to consider the
computational aspect of cloning. Following the line of work initiated by the seminal
papers of Bartholdi, Tovey, and Trick (1989, 1992), we seek to classify prominent
531

fiElkind, Faliszewski & Slinko

voting rules according to whether they admit ecient algorithms for nding a cloning
manipulation.
One might argue that in real-life elections cloning is not a practical campaign management tool: After all, recruiting a new candidate that is suciently similar to the existing
ones may be very dicult, if not impossible. Nonetheless, there are natural scenarios where
our model of cloning is practical and well-motivated. Below, we provide two such examples.
First, let us consider an election in which parties nominate candidates for some position,
and each party can nominate several candidates. The voters, especially those not following
the political scene closely, are likely to perceive candidates who belong to the same party
as clones. A partys campaign manager might attempt to strategically choose the number
of candidates nominated by her party. In fact, she might even be able to aect the number
of candidates nominated by other parties (e.g., by accusing them of not giving the voters
enough choice).
Second, let us consider an environment where, as suggested by Ephrati and Rosenschein
(1997), software agents vote to choose a joint plan (that is, the alternatives are possible
joint plans or steps of possible joint plans). In such a system, the agents can easily come up
with minor variations of the (steps of the) plan, eectively creating clones of the candidates.
(A very similar example regarding a society of agents choosing a project to implement is
given in Laslier, 1996).
In both cases it is reasonable to assume that all clones are ranked contiguously and the
cost of creating each clone is the same; moreover, to be successful, a cloning strategy should
be easy to compute. Therefore our model provides a good t for these scenarios.

2. Preliminaries
We start by presenting a brief overview of the social choice concepts that will be used in this
paper; we point the reader to the book of Arrow, Sen, and Suzumura (2002) for additional
background.
Given a set A of alternatives (also called candidates), a voters preference R is a linear
order over A, i.e., a total transitive antisymmetric binary relation over A. An election E
with n voters is given by its set of alternatives A and a preference prole R = (R1 , . . . , Rn ),
where Ri is the preference of voter i; we write E = (A, R). For readability, we sometimes
write i in place of Ri . Sometimes when specifying a preference order Ri we write X i Y ,
where X and Y are two disjoint subsets of A. This notation means that each member of
X is preferred to each member of Y but the relative ordering of candidates within X and
within Y is irrelevant to the discussion (unless specied separately). Also, we denote by
|R| the number of voters in the election. Given an election E = (A, R), we say that an
alternative c  A is Pareto undominated if for each alternative c  A, c = c, at least one
voter ranks c ahead of c . Given two candidates a, c  A, we set W (c, a) = |{i | c i a}|.
We say that c beats a in their pairwise contest if W (c, a) > W (a, c); if W (c, a) = W (a, c),
the pairwise contest between a and c is said to be tied.
A voting rule F is often dened as a mapping from elections with a xed set of alternatives A to the set 2A \ {} of all nonempty subsets of A. However, in this work, we are
interested in situations where the number of alternatives may change. Thus, we require
voting rules to be dened for arbitrary nite sets of alternatives and preference proles
532

fiCloning in Elections

over those alternatives. We say that a voting rule F is a mapping from pairs of the form
E = (A, R), where A is some nite set and R is a preference prole over A, to nonempty
subsets of A. The elements of F(E) are called the winners of the election E. Thus, we allow
an election to have more than one winner, i.e., we work with social choice correspondences;
this model is also called the non-unique winner model.
In this paper we consider the following voting rules (for all rules described in terms of
scores the winners are the alternatives with the maximum score):
Plurality. The Plurality score Sc P (c) of a candidate c  A is the number of voters that
rank c rst.
Veto. The Veto score Sc V (c) of a candidate c  A is the number of voters that do not rank
c last.
Borda. Given an election (A, R)with |R| = n, the Borda score Sc B (c) of a candidate
c  A is given by Sc B (c) = ni=1 |{a  A | c i a}|.
k-Approval. For any k  1, the k-Approval score Sc k (c) of a candidate c  A is the
number of voters that rank c in the top k positions. Plurality is simply 1-Approval.
Plurality with Runo. In the rst stage, all but two candidates with the top two Plurality scores are eliminated. Then, we run a pairwise contest between the two survivors;
the winner(s) are the candidate(s) who get at least |R|/2 votes at this stage, i.e.,
win or tie the pairwise contest. We may need to break a tie after the rst stage if there
are more than two candidates with the maximum Plurality score, or one top-scorer
and several candidates with the second-best Plurality score. To this end we use the
parallel universes tie-breaking rule (Conitzer, Rognlie, & Xia, 2009): a candidate c is
considered to be a winner if he wins or ties in the runo for some way of breaking ties
after the rst stage.
Maximin. The Maximin score Sc M (c) of a candidate c  A is Sc M (c) = minaA W (c, a),
i.e., it is the number of votes that c gets in his worst pairwise contest.
Copeland. The Copeland score Sc C (c) of a candidate c  A is |{a | W (c, a) > W (a, c)}| 
|{a | W (a, c) > W (c, a)}|. That is, c receives 1 point for each pairwise contest she
wins, 0 points for a tie, and 1 point for each pairwise contest she loses. This is
equivalent to the more conventional denition, in which, for each candidate a, c gets
1 point if she wins the pairwise contest against a, 0.5 points if there is a tie, and 0 if
she loses the contest.
A candidate c is a Condorcet winner (respectively, Condorcet loser ) if for each other
candidate a it holds that W (c, a) > W (a, c) (respectively, W (c, a) < W (a, c)). Naturally,
not every election has a Condorcet winner or a Condorcet loser. Observe that in a Copeland
election with m candidates the score of a Condorcet winner is m  1 and the score of a
Condorcet loser is (m  1).
Many results of this paper are computational and thus we assume that the reader is
familiar with the standard notions of computational complexity such as classes P and NP,
many-one reductions, NP-hardness and NP-completeness. Most of our NP-hardness results
follow by reductions from the Exact Cover by 3-Sets problem, dened below.
533

fiElkind, Faliszewski & Slinko

Denition 2.1 (Garey & Johnson, 1979). An instance (G, S) of Exact Cover by 3Sets (X3C) is given by a ground set G = {g1 , . . . , g3K } and a family S = {S1 , . . . , SM }
of subsets of G, where |Si | = 3 for each i = 1, . . . , M . It is a yes-instance if there is a
subfamily S   S, |S  | = K, such that for each gi  G there is an Sj  S  such that gi  Sj ,
and a no-instance otherwise.

3. Our Framework
Cloning and independence of clones were previously dened by Laslier (2000), Tideman
(1987), and Zavist and Tideman (1989). However, we need to modify the denition given
in these papers in order to model the manipulators intentions and the budget constraints.
We will now describe our model formally.
Denition 3.1. Let E = (A, (R1 , . . . , Rn )) be an election with a set of candidates A =
{c1 , . . . , cm }. We say that an election E  = (A , (R1 , . . . , Rn )) is obtained from E by replac(1)
(k)
ing a candidate cj  A with k clones for some k > 0 if A = (A \ {cj })  {cj , . . . , cj } and
for each i  [n], Ri is a total order over A such that for any a  A \ {cj } and any s  [k]
(s)
it holds that cj i a if and only if cj i a.
We say that an election E  = (A , R ) is cloned from an election E = (A, R) if there
is a vector of nonnegative integers (k1 , . . . , km ) such that E  is derived from E by replacing
each cj , j = 1, . . . , m, with kj clones.
Thus, when we clone a candidate c, we replace her with a group of new candidates that
are ranked together in all voters preferences. Observe that according to the denition above,
(1)
cloning a candidate cj once means simply changing his name to cj rather than producing
an additional copy of cj . While not completely intuitive, this choice of terminology simplies
some of the arguments in the rest of the paper.
The denition above is essentially equivalent to the one given by Zavist and Tideman
(1989); the main dierence is that we explicitly model cloning of more than one candidate.
However, we still need to introduce the two other components of our model: a denition of
what it means for a cloning to be successful, and the budget.
We start with the former, assuming throughout this discussion that the voting rule
is xed. Observe that the nal outcome of cloning depends on the relative ranking of
the clones chosen by each voter, which is typically not under the manipulators control.1
Thus, a cloning may succeed for some orderings of the clones, but not for others. The
election authorities may approach this issue from the worst-case perspective, and consider it
unacceptable when a given cloning succeeds for at least one ordering of clones by the voters.
Alternatively, they can take an average-case perspective, i.e., assume that the voters rank
the clones randomly and independently, with each ordering of the clones being equally likely
(due to similarities among the clones), and consider it acceptable for a cloning manipulation
to succeed with probability that does not exceed a certain threshold. Similarly, while an
extremely cautious manipulator would view cloning as successful only if it succeeds for all
1. Our general model, and, specically, the notion of 0+ -successful cloning (to be dened in a few paragraphs), captures the situation where the manipulator does have full control over the ordering of the
clones in voters preferences.

534

fiCloning in Elections

orderings, a more practically-minded one would be happy with a cloning that succeeds with
high probability. We will now present a denition that captures all of these attitudes.
Denition 3.2. Given a positive real q, 0 < q  1, we say that the manipulation by
cloning (or simply cloning) is q-successful if (a) the manipulators preferred candidate is
not a winner of the original election, and (b) a clone of the manipulators preferred candidate
is a winner of the cloned election with probability at least q.
The two worst-case approaches discussed above are special cases of this framework.
Indeed, a cloning succeeds for all orderings if and only if it is 1-successful. Similarly, it
succeeds for some ordering if and only if it is q-successful for some q > 0 (where q may
depend on |A| and |R|). In the latter case, we will say that the cloning is 0+ -successful; this
is equivalent to saying that the manipulator would succeed if he could dictate each voter
how to order the clones. We will use this equivalent formulation very often as it simplies
proofs.
Observe that, according to our denition, the manipulator succeeds as long as any one of
the clones of the preferred candidate wins. This assumption is natural if the clones represent
the same company (e.g., Coke Light and Coke Zero) or political party. However, if a
campaign manager has created a clone of his candidate simply by recruiting an independent
candidate to run on a similar platform, he may nd the outcome in which this new candidate
wins less than optimal. We could instead dene success as a victory by the original candidate
c (i.e., the clone c(1) ), but, at least for neutral voting rules, this is equivalent to scaling down
the success threshold q by a factor of k, where k is the number of clones of the preferred
candidate. Indeed, any preference prole in which the original candidate wins can be
transformed into one in which some clone wins, by switching their order in each voters
preferences, so c(1) wins with the same probability as any other clone. In particular, this
means that under this denition a cloning manipulation can only be 1-successful if it does
not clone the manipulators preferred candidate.
Another issue that we need to address is that of the costs associated with cloning. Indeed,
the costs are an important aspect of realistic campaign management, as the manager is
always restricted by the budget of the campaign. The most general way to model the cloning
costs for an election with the initial set of candidates A = {c1 , . . . , cm } is by introducing a
cost function p : [m]  Z+  Z+  {0, +}, where p(i, j) denotes the cost of producing the
j-th copy of candidate ci . Note that p(i, 1) corresponds to not producing additional copies
of i, so we require p(i, 1) = 0 for all i  [m]. We remark that it is natural to assume that all
costs are nonnegative (though some of them may equal zero), whereas the assumption that
all costs are integer-valued is made for computational reasons; this is not a real restriction
as monetary values are discrete.
We assume that the marginal cost of introducing an additional cloned candidate eventually becomes constant, that is, there exists a t > 1 such that p(i, j) = p(i, t) for each j > t.
This ensures that the cost function has a nite representation; specically, we can encode
p as an m-by-t table with entries in Z+  {0, +}.
Denition 3.3. An instance of the q-Cloning problem for q  {0+ }  (0, 1] is given by
the initial set of candidates A = {c1 , . . . , cm }, a preference prole R over A with |R| = n, a
manipulators preferred candidate c  A, a parameter t > 1, a cost function p : [m]  [t] 
535

fiElkind, Faliszewski & Slinko

Z+ {0, +} (with the interpretation that p(i, j) = p(i, t) for all i = 1, . . . , m and all j > t),
a budget B, and a voting rule F. We ask if there exists a q-successful cloning with respect
to F that costs at most B.
For many of the voting rules that we consider, it is easy to bound the number of clones
needed for 0+ -successful or 1-successful cloning (if one exists), and this bound is usually
polynomial in n and m. Thus, t can often be assumed to be polynomial in n and m.
We focus on two natural special cases of q-Cloning:
1. Zero Cost (ZC): p(i, j) = 0 for all i  [m], j  Z+ . In this case we would like to
decide if an election is manipulable by cloning when money is not a concern.
2. Unit Cost (UC): p(i, j) = 1 for all i  [m], j  2. This model assumes that creating
each new clone has a xed cost that is equal for all candidates.
We say that an election E is q-manipulable by cloning with respect to a voting rule F if
it admits a q-successful manipulation by cloning with respect to F in the ZC model.
In the rest of the paper, we characterize the q-manipulable elections and discuss the
complexity of the q-Cloning problem for a number of well-known voting rules, focusing on
the ZC and UC models. Clearly, hardness results for these special cases also imply hardness
results for the general cost model. Similarly, hardness results for the ZC q-Cloning imply
hardness results for UC q-Cloning; to reduce ZC q-Cloning to UC q-Cloning it suces
to set B = +. We emphasize that whenever we say that q-Cloning is easy, we refer
to the general cost model; in contrast, q-manipulability refers to susceptibility to cloning
manipulation with zero costs and/or unlimited budget.

4. Prominent Voting Rules for Which Cloning is Easy
In this section we study q-Cloning for Plurality, Plurality with Runo, Veto, and Maximin.
Surprisingly, these four rules exhibit very similar behavior with respect to cloning.
4.1 Plurality
We start by considering Plurality, which is arguably the simplest voting rule.
Theorem 4.1. An election is 0+ -manipulable by cloning with respect to Plurality if and
only if the manipulators preferred candidate c does not win, but is ranked rst by at least
one voter. Moreover, for Plurality 0+ -Cloning can be solved in linear time.
Proof. Clearly, if c has no rst-place votes, no cloning manipulation can make her a winner.
Now, assume that cs Plurality score Sc P (c) is at least 1, and let C = {a  A | Sc P (a) >
Sc P (c)} denote the set of candidates whose Plurality score is greater than that of c. For
P (a)
each a  C, we create ka =  Sc
Sc P (c)  clones of a.
Recall that to show that a cloning is 0+ -successful, we only need to specify an ordering
of the clones that makes c a winner. One such ordering can be obtained as follows. For
each a  C, let Ra denote the set of orders in which a is ranked rst. Split Ra into
ka groups, where the rst ka  1 groups have size Sc P (c), while the last group has size
Sc P (a)  (ka  1)Sc P (c)  Sc P (c). Let the voters in the i-th group rank the i-th clone of
536

fiCloning in Elections

a rst, followed by the rest of the clones in an arbitrary order. Under this ordering of the
clones, the Plurality score of each candidate is at most Sc P (c), so c is a winner, i.e., our
cloning is 0+ -successful.
To prove the second statement of the theorem, note that the above-described algorithm
for nding a 0+ -successful cloning is optimal: we can reduce a candidates Plurality score
only by cloning this candidate, and if a candidate a with score Sc P (a) > Sc P (c) is cloned
less than ka times, at least one of its clones will obtain more than Sc P (c) Plurality votes.
Thus, we simply need to compute the cost of cloning each candidate a  C exactly ka times,
and compare it with our budget B.
It is not too hard to strengthen the rst statement of Theorem 4.1 from 0+ -manipulability
to q-manipulability for any q  (0, 1).
Theorem 4.2. For any q  (0, 1), a Plurality election is q-manipulable by cloning if and
only if the manipulators preferred candidate c does not win, but is ranked rst by at least
one voter. However, no Plurality election is 1-manipulable by cloning.
Proof. Fix q  (0, 1). Suppose that the manipulators preferred candidate c does not win,
but Sc P (c) >0. For each candidate a  A with Sc P (a) > Sc P (c), we set s = Sc P (a) and
that one of the clones of a will be
create k = 2s  m1
1q  clones of a. Then the probability
s 1
1q
top-ranked two or more times is at most k  2  k2  m1
. By the union bound, with
probability at least q none of the newly introduced clones gets more than one vote. On the
other hand, none of the uncloned candidates had more Plurality votes than c, and cloning
did not change that. Therefore, c is among the winners of the resulting election. However,
obviously the manipulator cannot make c a winner with probability 1: if all voters order
the clones in the same way, the most popular clone of each candidate will have the same
Plurality score as the original candidate.
 
The procedure described in the proof of Theorem 4.2 introduces at most (m1) n2  m1
1q 
clones; this number is polynomial in m and n for any constant q. However, this number of
clones is not necessarily optimal, i.e., our procedure is not a polynomial-time algorithm for
q-Cloning. In fact, the complexity of Plurality q-Cloning for q  (0, 1) remains an open
problem.
4.2 Veto and Plurality with Runo
The Veto rule exhibits extreme vulnerability to cloning.
Theorem 4.3. Any election in which the manipulators preferred candidate c does not win
is 1-manipulable by cloning with respect to Veto. Moreover, for Veto both 0+ -Cloning and
1-Cloning can be solved in linear time.
Proof. Consider a prole in which the Veto score of the manipulators preferred candidate
c is k. This means that c is ranked last n  k times. If we clone c at least n  k + 1 times,
we are guaranteed that at least one clone of c is never ranked last, so it is among the Veto
winners.
537

fiElkind, Faliszewski & Slinko

For 0+ -Cloning, observe that it is not useful to clone candidates other than c. Thus,
the optimal solution is to make two copies of c and ask all voters to order them in the same
way. Then the better clone is never ranked last and is among the winners.
For 1-cloning, let  be the Veto score of the election winner(s), and let k be the Veto
score of c. As argued above, we do not benet from cloning candidates other than c, so we
only need to determine the optimal number of cs clones. Suppose rst that  = n. Then,
by the argument above, it is sucient to create n  k + 1 clones of c. It is easy to see that
this number is also necessary: if there are n  k or fewer clones, each of them may be ranked
last by some voter.

Now, suppose that  < n. Let  = n  , k  = n  k, set r =  k+1 , and create r + 1

clones of c. This number of clones is clearly sucient: we have r + 1 > k+1 , so if each
clone of c is ranked last at least  + 1 times, then the total number of voters that rank c
last in the original prole would be at least ( + 1)(r + 1) > k  , a contradiction. On the
other hand, it is necessary to introduce at least r + 1 clones. Indeed, if there are only r
clones, we can split the voters that rank c last into r groups, where the rst r  1 groups
have size  + 1, the last group has size k   (r  1)( + 1)   + 1 (where the inequality

follows from r  k+1 ), and the voters in the i-th group rank the i-th clone of c last. In this
preference prole, each clone of c is vetoed as least  + 1 times and therefore is not among
the winners.
We now consider Plurality with Runo.
Theorem 4.4. An election is 0+ -manipulable by cloning with respect to Plurality with
Runo if and only if the manipulators preferred candidate c is not a current winner, and
either
(1) ScP (c)  2, or
(2) ScP (c) = 1 and c wins or ties its pairwise contest against some alternative w whose
Plurality score is strictly positive.
Moreover, for Plurality with Runo 0+ -Cloning can be solved in polynomial time.
Proof. Suppose rst that ScP (c)  2. We can introduce two clones of c, which we will
denote by c(1) and c(2) , split the voters who rank c rst into two nonempty groups, ask the
voters in the rst group to rank c(1) rst, and ask the voters in the second group to rank
c(2) rst. Next, for each candidate a = c we create Sc P (a) clones of a, and ask the i-th
voter among those who rank a rst to rank the i-th clone of a rst. In the resulting election,
the Plurality score of both c(1) and c(2) is at least 1, and the Plurality score of any other
candidate is at most 1. Thus, there is a way to break ties after the rst round so that both
c(1) and c(2) progress to the nal round, and one of them wins. As we apply the parallel
universes tie-breaking rule, this means that this cloning is 0+ -successful.
If ScP (c) = 1, we use the same strategy as in the previous case, except that we do not
clone c. In the resulting election the Plurality score of any candidate is at most 1. Let w be
an alternative with nonzero Plurality score that loses or ties its pairwise election against c.
It is easy to see that any clone of w also loses or ties its pairwise election against c. Since
ws Plurality score in the original election is positive, there exists a parallel universe where
c and a clone of w meet in the nal, which means that c is a winner of the resulting election.
538

fiCloning in Elections

Thus, we see that the conditions of the theorem are sucient. We will now show that
they are necessary. First, note that if c is not ranked rst by any voter, then c will not
win irrespective of which candidates we clone. Indeed, if in the cloned election there are
two candidates with nonzero Plurality scores, c will not reach the nal, and if all voters
rank some candidate a = c rst, c may reach the nal, but will lose to a in the nal. Now,
suppose that Sc P (c) = 1, but any alternative w with Sc P (w) > 0 beats c in their pairwise
contest; as argued above this also holds for any clone of w. Since c is not a winner prior to
cloning, there exists at least one other candidate with nonzero Plurality score, and therefore
at most one clone of c can progress to the second round; hence, there is no benet to cloning
c. Moreover, even if c reaches the second round, he has to face (a clone of) some alternative
with nonzero Plurality score in the original election, and by our assumption any such clone
would beat c in the nal.
To complete the proof of the theorem, it remains to give a polynomial-time algorithm
for 0+ -Cloning under Plurality with Runo. As suggested by the discussion above, there
are two ways to make c a winner: we can either (1) try to clone c (and possibly some other
alternatives) in order to ensure that only clones of c go to the runo, or (2) try to clone
alternatives other than c only, to ensure that c goes to the runo against an alternative
that she can defeat. Our algorithm implements both options and accepts if either of them
is within the budget.
For option (1), we introduce two clones of c, which we will denote by c(1) and c(2) (clearly,
there is no benet to creating more than two clones of c). It is not hard to see that our
optimal strategy is to ask the voters to order these clones so that they get (almost) identical
Plurality scores, i.e., so that the Plurality scores of c(1) and c(2) are  Sc P2 (c)  and  Sc P2 (c) ,
respectively. Let k =  Sc P2 (c) , and let C = {a  A \ {c} | Sc P (a) > k} denote the set of
candidates whose Plurality score is greater than that of c(2) . For each candidate a  C, we
introduce ka =  Sc Pk(a)  clones of a. With nonzero probability, this action ensures that the
Plurality score of each alternative except c(1) and c(2) does not exceed k, and thus there is
a parallel universe in which c(1) competes against c(2) in the runo and one them wins. It
is easy to see that this strategy gives the cheapest way of implementing option (1).
Option (2) can be used only if candidate c wins or ties her pairwise contest against
at least one other candidate with nonzero Plurality score. Let D = {a  A \ {c} |
Sc P (a)  1 and W (c, a) > W (a, c)}. For each w  D we compute the cost of the manipulation that results in c competing against w in the runo in one of the parallel universes;
we then pick the cheapest of these manipulations. It remains to explain how to compute
such a manipulation for a specic w  D.
Let k = min{Sc P (c), Sc P (w)}, and let C = {a  A \ {c, w} | Sc P (a) > k}. If not cloned,
the candidates in C can prevent c and w from meeting each other in the nal round. Thus,
we create ka =  Sc Pk(a)  clones of each a  C. It is easy to see that this manipulation
results in a nonzero chance of c winning and that we cannot produce fewer clones for a
given w  D. This completes the proof.
We can also characterize elections that are q-manipulable with respect to Plurality with
Runo for q  (0, 1].

539

fiElkind, Faliszewski & Slinko

Theorem 4.5. For any q  (0, 1), an election is q-manipulable by cloning with respect
to Plurality with Runo if and only if it is 0+ -manipulable by cloning with respect to it.
However, no election is 1-manipulable.
Proof. It is immediate that under Plurality with Runo no election is 1-manipulable: if our
preferred candidate is not a winner and after cloning all voters rank the clones identically,
then our preferred candidate still loses.
To see that for each q  (0, 1) an election is q-manipulable with respect to Plurality
with Runo if and only if it is 0+ -manipulable with respect to it, it suces to combine the
proofs of Theorems 4.4 and 4.2. In more detail, the proof of Theorem 4.2 explains how to
clone some of the candidates so that with probability at least q the Plurality scores of all
candidates in the resulting prole R do not exceed 1. We will now argue that whenever this
happens (i.e., with probability q) c is a Plurality with Runo winner as long as he satises
the conditions of Theorem 4.4.
Indeed, suppose that in R the Plurality score of each candidate is at most 1. Then if
cs Plurality score in the original election is at least 2, it has to be the case that at least two
clones of c have positive Plurality scores in R , and hence there is a parallel universe where
they meet in the nal. On the other hand, if cs Plurality score in the original election is 1,
but he beats or ties some candidate w with nonzero Plurality score in a pairwise election,
for R there is a parallel universe where c meets (a clone of) w in the nal. Thus, c is a
Plurality with Runo winner in R , and the proof is complete.
We remark that the cloning strategy presented in the proof of Proposition 4.5 is, in a
sense, degenerate: it operates in the same way irrespective of the identity of the preferred
candidate c, and has the eect of making all eligible candidates the Plurality with Runo
winners. (This can be seen as an artifact of the non-unique winner model, where this
outcome is viewed as acceptable. If we are interested in making c the unique Plurality with
Runo winner, we may need a more sophisticated cloning strategy; however, this question
is outside of the scope of this paper.) Observe also that under Plurality with Runo cloning
can be used to manipulate in favor of a Condorcet loser.2
Example 4.6. Let us consider the following Plurality with Runo election: A = {a, b, c, d},
and there are 17 voters whose preference orders are:
cabd
abdc
badc
dabc

8
3
3
3

voters
voters
voters
voters

Clearly, c is a Condorcet loser. Further, if we apply Plurality with Runo, c gets into the
second round, but then loses there. Yet if we produce two clones of c, it is possible that
each of them receives four Plurality points, enters the runo and, as a result, one of them
wins the election.
2. We are grateful to one of the JAIR referees for this example.

540

fiCloning in Elections

4.3 Maximin
Surprisingly, Maximin behaves in essentially the same way as Plurality with respect to
cloning, i.e., by cloning a candidate, we can reduce its Maximin score to 1 with nonzero
probability.
Consider the following election, which will be used in our constructions throughout this
section. Let E = (A, R) with A = {a1 , . . . , ak }, R = (R1 , . . . , Rk ), where for i  [k] the
preferences of the i-th voter are given by ai i ai+1 i . . . i ak i a1 i . . . i ai1 .
We will refer to any election that can be obtained from E by renaming the candidates as a
k-cyclic election. In this election, assuming a0 = ak , for each i = 1, . . . , k there are k  1
voters that prefer ai1 to ai and 1 voter that prefers ai to ai1 . Thus, the Maximin score
of each candidate in A is 1. Further, this remains true if we add arbitrary candidates to
the election, no matter how the voters rank the additional candidates. This means that by
cloning a candidate a in an n-voter election n times and telling the voters to order the n
clones of a as in an n-cyclic election, we can reduce as Maximin score to 1.
Theorem 4.7. An election is 0+ -manipulable by cloning with respect to Maximin if and
only if the manipulators preferred candidate c does not win, but is Pareto undominated.
Further, for Maximin 0+ -Cloning can be solved in linear time. However, no election is
1-manipulable by cloning with respect to Maximin.
Proof. Clearly, if all voters prefer some alternative a to c, then after any cloning the Maximin
score of c will be 0, whereas the Maximin score of at least one alternative is positive, and
hence c cannot win. On the other hand, if c is undominated, its Maximin score is at least
1. Now, we can use the construction described above to reduce the Maximin score of any
other candidate to 1, thus making c a winner.
Our algorithm for Maximin 0+ -Cloning relies on the observation that the only way to
change the Maximin score of a candidate is to clone her, thereby reducing her score. Now,
suppose that Sc M (c) = s. We will argue that we have a yes-instance of 0+ -Cloning if
and only if our budget allows us to introduce  ns  clones for each candidate a such that
ScM (a) > s; clearly, this condition can be checked in linear time.
Indeed, for each candidate a whose Maximin score exceeds s we can do the following.
We create  ns  clones of a, divide the voters in s groups, where the size of the rst s  1
groups is  ns , and the last group consists of the remaining t   ns  voters. For each of the
rst s  1 groups, we ask the voters to rank the clones according to an  ns -cyclic election;
the voters in the last group vote as the rst t voters in an  ns -cyclic election. Clearly, in
each group, for any i = 1, . . . ,  ns  there is at most one voter who ranks the i-th clone above
the (i  1)-st clone. Thus, in the resulting election, the Maximin score of any clone of a is
at most s, and therefore c is a winner of that election.
For the converse direction, we need to show that if we create less than  ns  clones of a,
the Maximin score of at least one of them will exceed s. Indeed, suppose that we create
t <  ns  clones of a; denote these clones by a(1) , . . . , a(t) . Given an arbitrary preference
prole over these clones, consider a directed graph whose vertices are the clones and there
is an edge from a(i) to a(j) if at least ns voters prefer a(i) to a(j) . Note that if the Maximin
score of each clone is at most s, each vertex of this graph must have an incoming edge. In
particular, this means that our graph cannot be acyclic. We will now argue that each cycle
541

fiElkind, Faliszewski & Slinko

in this graph has length at least  ns ; clearly, this implies that the graph contains at least
 ns  vertices, a contradiction.
To see this, suppose that there is a cycle of length r <  ns . Relabel the clones along this
cycle as a(1) , . . . , a(r) , i.e., assume that for each i = 1, . . . , r at least ns voters prefer a(i) to
a(i+1) (where a(r+1) = a(1) ). By induction on i it is easy to see that for each i = 1, . . . , r  1,
there are at least n  si voters whose preference order  satises a(1)  . . .  a(i+1) . For
i = r  1, this implies that at least n  s(r  1) > s voters prefer a(1) to a(r) , a contradiction
with the assumption that there is an edge from a(r) to a(1) . This establishes that our
algorithm for 0+ -Cloning is correct.
Finally, it is easy to see that no election is 1-manipulable with respect to Maximin.
Indeed, the only way to change a candidates Maximin score is to clone him. However, after
the cloning, all voters may order all clones in the same way, in which case the most popular
clone will have the same Maximin score as the original alternative.
It is not clear if one can strengthen the result of Theorem 4.7 to q-manipulability for
q  (0, 1). This amounts to the following question: Suppose that for a xed n we randomly
draw n permutations of {1, . . . , k}. Let P (n, k) be the probability that for each i  [k] there
is a j  [k] such that j precedes i in at least n  1 permutations. Is it the case that, as
k  , the probability P (n, k) approaches 1?
Our computations show that this is unlikely to be the case.3 For (n, k) = (5, 20) there
was only one success out of 106 random trials and only three for (n, k) = (5, 50). For both
(n, k) = (7, 20) and (n, k) = (7, 50) not a single random trial out of 106 trials was successful.
This means that, even if Maximin is q-manipulable for some xed q > 0, the number of
clones needed would be astronomical.

5. Three Rules for Which Cloning May Be Dicult
We now consider Borda, k-Approval, and Copeland rules, for which cloning-related problems
are signicantly more dicult.
5.1 Borda Rule
For the Borda rule, the necessary and sucient condition for the existence of a 0+ -successful
manipulation by cloning is the same as for Maximin: the manipulators favorite alternative
has to be Pareto undominated. However, Borda and Maximin exhibit dierent behavior
with respect to 1-manipulability. Moreover, from the point of view of nding a minimumcost cloning, Borda appears to be harder to deal with than Maximin.
Theorem 5.1. An election is 0+ -manipulable by cloning with respect to Borda if and only if
the manipulators preferred candidate c does not win, but is Pareto undominated. Moreover,
UC 0+ -Cloning for Borda can be solved in linear time.
Proof. Note rst that if we create k clones of an alternative a that was ranked in position
j in some order Ri , then Ri s contribution to the scores of all alternatives ranked above a
increases by k  1, Ri s contribution to the scores of all alternatives ranked below a does
3. We are grateful to Danny Chang for his help in performing these experiments.

542

fiCloning in Elections

not change, and, nally, the top-ranked clone of a receives k  1 more points from Ri than
a used to receive. We will now use this observation to prove the theorem.
Suppose rst that all voters prefer some alternative a to c. Note that this implies that
Sc B (a) > Sc B (c). If we create k clones of some alternative x = c, a, for each preference
ordering Ri there are three possibilities:
1. In Ri , x is ranked above both a and c. Then cloning x does not change the Borda
scores of a and c.
2. In Ri , x is ranked below a and above c. Then cloning x increases the Borda score of
a by k  1 and does not change the score of c.
3. In Ri , x is ranked below both a and c. Then cloning x increases the Borda scores of
both a and c by k  1.
Thus, any single act of cloning an alternative in A \ {c, a} (and hence any combination of
them) does not reduce the gap between the scores of a and c. Further, if we clone a and/or
c, in every ordering Ri each clone of a is ranked above each clone of c, and hence has a
higher Borda score. Thus, c cannot be made a winner by cloning.
For the converse direction, let C = {a  A | Sc B (a) > Sc B (c)}. For each a  C, let
na be the number of voters that prefer c to a, i.e., na = W (c, a). Let sa denote the score
dierence between a and c, i.e., sa = Sc B (a)  Sc B (c). Now, set k = maxaC  nsaa , and
create k + 1 clones of c. Consider the preference prole in which all voters rank all clones of
c in the same way. Let c be the top-ranked clone of c. Observe that the Borda score of c
in the new prole exceeds the original Borda score of c by kn, where n is the total number
of voters. Now, consider some alternative a  C. For each order Ri in which a was ranked
above c, Ri s contribution to the score of a increased by k, and for each order Ri in which a
was ranked below c, Ri s contribution to the score of a remained the same. Thus, as score
has increased by k(n  na ). Hence, using Sc B (a) and Sc B (c ) to denote the scores of a and
c after cloning, by our choice of k we have Sc B (a)  Sc B (c ) = Sc B (a)  Sc B (c)  kna  0.
We conclude that in the resulting preference prole the Borda score of c is at least as high
as that of any other alternative, i.e., our cloning is 0+ -successful.
We will now argue that our input constitutes a yes-instance of UC 0+ -Cloning if
and only if maxaC  nsaa  + 1  B, where B is the cloning budget, i.e., the manipulation
constructed above is optimal in the UC model. Indeed, consider an alternative a  C that
maximizes the expression nsaa . By creating t + 1 clones of some alternative d = c, a we can
increase the distance between c and a in the orders in which d is ranked below c but above
a by t, and thus reduce the gap between a and c by t. Obviously, there are at most na such
orders. For any other order Ri , cloning d may increase Ri s contribution to the score of a,
or aect Ri s contributions to scores of a and c in the same way. Thus, creating t + 1 clones
of an alternative d  A \ {a, c} can contribute at most tna to closing the gap between c and
a. A similar argument applies to cloning c or a, showing that the contribution from t + 1
clones of either of these alternatives is also bounded by tna . Hence, we have to create at
least  nsaa  + 1 clones.
Is it possible to strengthen Theorem 5.1 to q-manipulability for some constant q  (0, 1]?
It turns out that for Borda these questions are signicantly more dicult than for the rules
we have considered so far.
543

fiElkind, Faliszewski & Slinko

We will rst consider the situation where the manipulator is restricted to cloning her
favorite candidate. We remark that this special case of our model is very natural: For
instance, a party may be able to nominate several candidates, but is not in a position to
force other parties to do so. In this case, for an even number of voters, we can characterize
the elections that can be 1-manipulated by cloning with respect to Borda.
Let c be the manipulators preferred candidate. We will show that, by cloning c, we
can deal with the candidates who currently have a higher Borda score than c, as long as
they lose to c in a pairwise election. However, we have to be careful to ensure that cs
Borda score remains higher than that of the candidates who beat c in a pairwise election.
Formally, let
A+ = {a  A \ {c} | SB (a) > SB (c)},

A = {a  A \ {c} | SB (a)  SB (c)}.

Let sa = |SB (a)  SB (c)| for each a  A \ {c}, and set

W (c, a)  W (a, c) if a  A+ ,
na =
W (a, c)  W (c, a) if a  A .
+
a
Finally, let r+ = + if na  0 for some a  A+ and r+ = max{ 2s
na | a  A } otherwise,

a
and let r = min{ 2s
na | a  A , na > 0}. We are now ready to state our criterion.

Theorem 5.2. An election with an even number of voters can be 1-manipulated with respect
to Borda by cloning the manipulators preferred candidate c if and only if c does not win
and r+   r .
Proof. Let n denote the number of voters. Suppose that r+  > r . Consider any cloning
that involves c only. Suppose it results in k clones of c, which we denote by c(1) , . . . , c(k) .
Let s denote the original Borda score of c. To show that this cloning is not 1-successful, it
suces to describe an ordering of the clones that results in all clones of c losing the election.
Consider a prole where the rst n2 voters rank the clones as c(1)      c(k) , while the
remaining n2 voters rank the clones in the opposite order. Clearly, the Borda score of any
clone is s + n2 (k  1). We will now consider two cases.
Case 1 (r + = +). This means that there is an alternative a such that na  0, i.e.,
a is preferred to c by at least n2 voters and has a higher Borda score than c. Our
cloning increases as score by at least n2 (k  1), so its nal Borda score is at least
sB (a) + n2 (k  1) > s + n2 (k  1). Thus, after cloning, all clones of c will still have
lower scores than a, i.e., the cloning is not 1-successful.
Case 2 (r + < +). In this case, there exists a candidate a  A+ such that na > 0 and
2sa


a
r+ = 2s
na . Consider a candidate b  A such that r = na ; we have nb > 0. The
2sb
a
condition r+  > r  can be rewritten as  2s
na  >  nb . After the cloning, as Borda
n+nb
a
score is sB (a) + nn
2 (k  1), and bs Borda score is sB (b) + 2 (k  1). Thus, for c
to be a winner, k must satisfy
s+

n  na
n(k  1)
 sB (a) +
(k  1);
2
2
544

s+

n + nb
n(k  1)
 sB (b) +
(k  1).
2
2

fiCloning in Elections

Hence, we obtain
sa = sB (a)  s 

na
(k  1),
2

sb = s  sB (b) 

nb
(k  1).
2

2sb

a
Since k is integer, this implies r+ =  2s
na   k  1   nb  = r , a contradiction.

For the opposite direction, we will show that the ordering of the clones constructed above
is the worst possible for the manipulator. Formally, suppose that we generate k clones of
c for some k  1. Consider a voter who gives j points to c. It is not hard to see that this
voter gives j + (j + 1) +    + (j + k  1) = kj + k(k1)
points to all clones of c. Thus, the
2
k(k1)
total Borda score of all clones is equal to ks + n 2 , where s is the Borda score of c prior
to cloning. It follows that the Borda score of at least one clone is s + n2 (k  1) or higher.
Since r+   r , we have r+ < +. Set k = 1 + r+ , and consider an arbitrary
a
alternative a  A+ . As r+ < +, we have na > 0. Therefore, a gains nn
2 (k  1)
2sa
n
points from cloning. Now, it is not hard to see that k  1 + na implies s + 2 (k  1) 
a
SB (a) + nn
2 (k  1), which means that after the cloning some clone of c beats a.
To nish the proof, consider a candidate b  A . If nb  0, after the cloning the score
k1
of b is at most SB (b) + n k1
2  s + n 2 , so b still loses to or ties with some clone of c. On
b
the other hand, if nb > 0, the cloning increases the score of b by n+n
2 (k  1) points. We
have
ff


   	
2sb
2sb

.
k  1 = r+  r 
nb
nb
Therefore, after the cloning, the score of b is
SB (b) +

n + nb
n
k1
(k  1)  SB (b) + (k  1) + sb = s + n
,
2
2
2

i.e., bs Borda score does not exceed that of some clone of c.
It is not hard to see that the second part of the proof (the if direction) works for
the odd number of voters as well. However, for the only if direction, the argument does
not go through. We can, however, prove a slightly weaker necessary condition. Dene
r+ = + if na  0 for some a  A+ and r+ = max{ 2snaa1 | a  A+ } otherwise, and let
r = min{ 2snaa+1 | a  A , na > 0}.
Proposition 5.3. If an election with an odd number of voters can be 1-manipulated with
respect to Borda by cloning the manipulators preferred candidate c, then r+   r .
We relegate the proof of Proposition 5.3 to Appendix A. We remark that it is not clear
if the converse direction of Proposition 5.3 holds: the condition r+   r  is weaker than
r+   r , and we were unable to prove that it is sucient for 1-manipulability with
respect to Borda.
The proof of Theorem 5.2 indicates which orderings of the clones are most problematic
for the manipulator: these are the orderings that grant each clone roughly the same number
of points. But this is exactly the expected outcome if the orderings are generated uniformly
at random! Thus, our proof shows that for Borda a manipulator who clones c only should
be prepared for the worst-case scenario.
545

fiElkind, Faliszewski & Slinko

Note, however, that we are not limited to cloning c. Indeed, cloning other candidates
might be more useful with respect to 1-manipulability. For example, suppose that c is Pareto
undominated, and, moreover, the original preference prole contains a candidate c that is
ranked right under c by all voters (one can think of this candidate as an inferior clone of
c; however, we emphasize that it is present in the original prole). Then one can show that
by cloning c suciently many times we can make c a winner with probability 1. However,
cloning c itself does not have the same eect if the voters order the clones randomly or
adversarially to the manipulator. This is illustrated by the following example.
Example 5.4. Let us consider the following Borda election. The set of candidates is
C = {a, b, c, d}, and there are four voters, whose preference orders are:
R1
R2
R3
R4

:acbd
:acbd
:acbd
:dcba

Sc B (a) = 9
Sc B (b) = 4
Sc B (c) = 8
Sc B (d) = 3

The winner here is a with 9 points. However, replacing b with three clones b1 , b2 , b3 is a
1-successful manipulation in favor of c since the new score of a is 15, while the new score
of c is 16, no matter how the clones are ordered. At the same time, we cannot make c a
winner with probability 1 by cloning him alone. Indeed, if we split c into k + 1 clones, and
each voter orders the clones uniformly at random, the expected score of each clone of c is
4(2 + k2 ) = 8 + 2k (and, since the number of voters is even, the proof of Theorem 5.2 gives
an explicit ordering of the clones in which each clone gets 8 + 2k points), whereas as score
is 9 + 3k.
This shows that, in general, we may need to clone several candidates that are placed
between c and its competitors in a large number of votes, and determining the right
candidates to clone might be dicult. Indeed, it is not clear if a 1-successful manipulation
for Borda can be found in polynomial time; answering this question is a challenging open
problem.
A related question that is not answered by Theorem 5.1 is the complexity of 0+ -Cloning
(and, more generally, q-Cloning for q  {0+ }  (0, 1]) in the general cost model. Note that
there is a certain similarity between this problem and that of 1-manipulability: in both cases,
it may be suboptimal to clone c. Indeed, for general costs, we can prove that q-Cloning
is NP-hard for any q.
Theorem 5.5. For Borda, q-Cloning in the general cost model is NP-hard for any rational
q  (0, 1] as well as for q = 0+ . Moreover, this is the case even if p(i, j)  {0, 1, } for all
i  [m], j  Z+ .
Proof. We provide a reduction from X3C. Let (G, S) be an instance of X3C, where G =
{g1 , . . . , g3K } is the ground set and S = {S1 , . . . , SM } is a family of 3-element subsets of G.
Let N = 3K. We construct an instance of our problem as follows. Let T = {t1 , . . . , tM }.
We let the set of alternatives be A = GT {c, u, w}, where c is the manipulators preferred
alternative. For each i = 1, . . . , M , we create two voters with preference orders R2i1 and
R2i . The preference order R2i1 is given by
R2i1 : G \ Si  c  ti  Si  u  w  T \ {ti },
546

fiCloning in Elections

while the preference order R2i is given by
R2i : w  Si  u  c  G \ Si  T.
Further, we require that the candidates in G are ranked in the opposite order in R2i and
R2i1 , i.e., for any j,  = 1, . . . , M we have gj 2i g if and only if g 2i1 gj . Finally, there
is one voter whose preference order is given by
R2M +1 : G  c  T  u  w.
The cloning costs are dened as follows. For each i = 1, . . . , M , producing one additional
clone of ti costs 1, and producing further clones of ti costs 0. Cloning any other alternative
costs +. Finally, we set B = K.
It is easy to verify that from each pair of votes (R2i1 , R2i )
1. c gets (M + 5) + (N  3 + M ) = 2M + N + 2 points,
2. each gj gets (M + N + 2) + M = 2M + N + 2 points (this remains true if gj  Si ),
3. each tj gets at most (M + 4) + (M  1) = 2M + 3 points,
4. u gets M + (M + N  2) = 2M + N  2 points, and
5. w gets (M  1) + (M + N + 2) = 2M + N + 1 points.
Thus, because of the last voter, the overall Borda score of each alternative in G exceeds
that of c by at least 1 and at most N , while the score of any other alternative is lower than
that of c.
Note that if we create N + 1 clones of ti , which we can do at cost 1, we ensure that cs
score is at least as high as that of the alternatives in Si , irrespective of how the voters order
the clones. However, cloning ti does not change the dierence in scores between c and the
alternatives in G \ Si . Within our budget, we can clone K alternatives from T , N + 1 times
each. Thus, any cover of G of size K = N/3, i.e., any exact cover, can be transformed into
a 1-successful cloning manipulation for our instance.
Conversely, consider any cloning manipulation of cost at most K. Suppose that it clones
the alternatives ti1 , . . . , tis , s  K. If {Si1 , . . . , Sis } is not a cover of G, there is some element
gi  G that is not covered by sj=1 Sij . This means that our cloning manipulation did not
change the dierence in scores between gi and c, i.e., gi still has a higher Borda score than
c. Hence, this cloning is not 0+ -successful. Thus, any 0+ -successful cloning of cost at most
K corresponds to a cover of G.
Now, consider any q  (0, 1]. If our input instance of X3C is a yes-instance, the
election that we have constructed admits a 1-successful cloning, which is also a q-successful
cloning (as well as a 0+ -successful cloning). On the other hand, if we start with a noinstance of X3C, our election does not admit a 0+ -successful cloning, and hence a qsuccessful cloning. Thus, the proof is complete.
Observe that the cost function used in the proof of Theorem 5.5 is very natural: some
candidates cannot be cloned at all, while for others, there is a certain upfront cost associated
with creating the rst clone (e.g., researching the platform and/or identity of the candidate),
but all subsequent clones can be created for free.
547

fiElkind, Faliszewski & Slinko

5.2 k-Approval
We will now demonstrate that there is a family of scoring rules for which deciding whether
a given election is 0+ -manipulable is computationally hard. Specically, this is the case for
k-Approval for any k  2. Our proof gives a reduction from the problem Dominating Set,
dened below.
Denition 5.6. An instance of the Dominating Set problem is a triple (V, E, s), where
(V, E) is an undirected graph and s is an integer. We ask if there is a subset W of V such
that (a) |W |  s and (b) for each v  V either v  W or v is connected to a vertex in W .
Theorem 5.7. For the k-Approval rule, it is NP-hard to decide whether a given election is
0+ -manipulable by cloning.
Proof. First, we remark that one can always make an alternative a a k-Approval winner
with nonzero probability as long as a is ranked rst by at least one voter. Indeed, we can
simply clone all other alternatives suciently many times (e.g., kn times, where n is the
number of voters). This ensures that there exists a preference prole over the resulting set
of alternatives in which each candidate other than a gets at most one k-Approval point.
However, this condition is not necessary. Indeed, a candidate can be a k-Approval winner
even if no voter ranks him rst. Thus, in our hardness reduction, we ask whether we can
help a candidate that is never ranked rst.
Consider an instance (V, E, s) of Dominating Set, where V = {v1 , . . . , vt }. We can
assume without loss of generality that the graph (V, E) has no isolated vertices and s < t.
We will construct an election based on (V, E, s) as follows. There is a candidate vi for each
vertex of the graph, a candidate c whom we would like to make a winner, an additional
candidate w, and a set D of dummy candidates. The exact number of dummy candidates,
which will be polynomial in the size of (V, E), will become clear after we describe the set
of voters. For each voter, we only specify which of the nondummy candidates she ranks in
the top k positions. Clearly, the order of candidates ranked in positions k + 1 or lower does
not aect the outcome of the election. Further, we assume that each dummy candidate
is ranked among the top k positions by exactly one voter. (Thus, the number of dummy
candidates is bounded from above by k times the number of voters.)
For each i = 1, . . . , t, the i-th voter places vi rst and ranks c in position k. Each of the
next 4t2  t  s voters places w rst and ranks c in position k. Further, for each (undirected)
edge (vi , vj )  E, there are 2s voters that rank vi rst and vj second and 2s voters that
rank vj rst and vi second.
v1   
..
. 
c


t

vt w   
.. ..
. . 
c

w 
..
..
.
.

c  c 
4t2  t  s

vi



vi

vj



vj

vj
..
.



vj
..
.

vi
..
.



vi
..
.


2s


2s


..
.
..
.

Observe that based on the votes constructed so far, the score of c is 4t2  s, the score of
w is 4t2  t  s, and the score of each vi , i = 1, . . . , t, is at most 4s(t  1) + 1. We now
add polynomially many voters, each of which ranks w or some candidate from V rst (and
some dummy candidates in positions 2, . . . , k), so that w gets 4t2  2s points in total and
548

fiCloning in Elections

each candidate in V gets exactly 4t2 points in total (note that this is possible since t > s).
Clearly, the number of the voters constructed at this stage is polynomially bounded in our
input size.
We claim that the constructed election is 0+ -manipulable if and only if the graph (V, E)
has a dominating set of size at most s. Indeed, suppose rst that (V, E) has a dominating
set W of size at most s. We clone each candidate from W exactly k + 2s(k  1) times. Now,
consider the following preference prole over the cloned alternatives. Split the clones into
2s + 1 groups, so that the rst group has size k and the remaining groups have size k  1.
For each vi  W , we order the clones as follows:
1. The i-th voter ranks the rst group of vi s clones in the rst k positions.
2. For each j such that (vj , vi )  E, the -th voter among the 2s voters that used to
rank vj rst and vi second now ranks the ( + 1)-st group of vi s clones in positions
2, . . . , k.
3. All other voters order the clones arbitrarily.
Under this preference prole, c gets 4t2  2s points: he has been pushed out of the
top k slots in s out of the rst t votes, but did not lose any other points. The clone of
each candidate vi  W gets at most 4t2  2s points, too. Indeed, x some vi  W . By
our assumption, the graph (V, E) has no isolated vertices. Thus, there is some vertex vj
connected to vi , and so there are 2s voters that (before cloning) rank vj in the rst position
and vi in the second position. Let us denote this set of voters by Eji . Now, consider the
prole after cloning. The rst k clones of vi are not ranked in the rst k positions by any
voter in Eji , and thus each of them receives at most 4t2  2s points. Now consider the -th
group of vi s clones,  = 2, . . . , 2s + 1: these clones are ranked in the top k positions by
exactly one voter in Eji , and by none of the rst t voters. Thus, these clones scores are at
most 4t2  (2s  1)  1 = 4t2  2s. We conclude that the score of each clone of vi is at most
4t2  2s.
Now, consider any candidate vj  V \ W . Since W is a dominating set for (V, E), there
exists a vi  W such that (vi , vj )  E. Since the 2s voters that used to rank vi rst and
vj second now rank some clones of vi in the top k positions, vj is pushed out of the top
k positions in at least 2s votes. Consequently, its k-Approval score is at most 4t2  2s. As
the k-Approval score of w was 4t2  2s prior to cloning and was not aected by cloning, we
conclude that after cloning c is a winner of the election with nonzero probability.
Conversely, suppose that we can make c a winner with nonzero probability by cloning
some of the candidates. Observe rst that we could not have cloned w. Indeed, if we clone
w, c is pushed out of the top k positions in 4t2  t  s votes, so her score is at most t.
On the other hand, in the original preferences, each vi  V is ranked rst at least 4t times.
Therefore, to make c a winner after cloning w, we would have to clone each of vi  V .
However, if we do that, cs k-Approval score goes down to 0, so she cannot be a winner.
We conclude that w is not cloned. As w is not ranked in positions 2, . . . , k in any of the
votes, and therefore cannot be pushed out, this means that ws nal score is 4t2  2s,
and therefore cs nal score must be at least 4t2  2s. This, in turn, means that we can
clone at most s candidates in V , as cloning any such candidate reduces cs score by 1. On
the other hand, we need to reduce the score of each vi  V by at least s. This can be done
549

fiElkind, Faliszewski & Slinko

either by cloning vi or by cloning some alternative that is ranked rst by some voter that
ranks vi second. This means that each vi  V either is cloned or has a neighbor in (V, E)
that has been cloned, i.e., the set of cloned alternatives forms a dominating set of (V, E).
As we have argued that the size of this set is at most s, this completes the reduction.
We can also show that it is NP-hard to decide whether an election is 1-manipulable with
respect to k-Approval.
Theorem 5.8. For any given k  2, it is NP-hard to decide whether a given election is
1-manipulable by cloning with respect to k-Approval.
Proof. We will rst present the proof for k = 2, and then show how to generalize it to all
integer k  2. The reduction is from X3C. Let an instance (G, S) of X3C be given by a
ground set G = {g1 , . . . , g3K } and a family S = {S1 , . . . , SM } of subsets of G, where for
each i = 1, . . . , M we write gi1 , gi2 , gi3 to denote the members of Si . We can assume without
loss of generality that M > K > 3. Given such an instance, we construct an election as
follows. The set of alternatives is A = G  T  {c, w}  D, where T = {t1 , . . . , tM }, and D
is the set of dummy candidates. In what follows, we place the dummy candidates so that
each of them appears at most once in the rst two rows.
We will now specify the rst two positions in each vote. For i = 1, . . . , M , the i-th
voter ranks ti rst and c second. Each of the next M 2 voters ranks w rst and a dummy
alternative second. Then we also have M groups of 3M voters, where in the i-th group (a)
each voter ranks ti rst, and (b) each of the candidates gi1 , gi2 , and gi3 is ranked second by
exactly M voters. Further, for each i  G, we add Ni = M (M + 1  |{j | gi  Sj }|) voters
that rank gi rst and a dummy candidate second. Finally, we add N = M 2  M + K voters
who rank c rst and a dummy candidate second.
t1   
c 
M

tM
c

w


 w
 
M2




ti
gi1

ti
gi2
3

ti
gi3




gi




Ni

gi





c




N

c


In this prole, the score of each ti is 3M + 1, the score of c is M 2 + K, the score of w is
and the score of each gi is M 2 + M . The scores of dummy candidates are equal to 1,
and their number is clearly polynomial in M . As K < M , the set of winners in this prole
is G, and c is M  K points behind. Now, if we clone some candidate gi  G, the voters
may still rank all clones of gi in the same order, so no manipulation that relies on cloning
candidates in G is 1-successful. Therefore, to bridge the gap between c and the candidates
in G, we need to clone some of the candidates in T . However, we cannot clone more than
K of them, since otherwise the score of c will fall below the score of w (while we can clone
w as well, the voters may still order all clones of w in the same way, in which case the
top-ranked clone of w still gets M 2 points). On the other hand, for each gi  G we need to
clone at least one tj  T such that gi  Sj , since otherwise the score of gi will not go down.
Thus, a 1-successful manipulation corresponds to a cover of G of size at most K, i.e., an
exact cover. Conversely, cloning a set of candidates T   T such that {Sj | tj  T  } is an
exact cover of G is a 1-successful cloning.
For k > 2, this construction can be modied as follows. In each vote, we insert a group
of new k  2 dummy candidates between the rst and the second position (so that in each
M 2,

550

fiCloning in Elections

vote the candidate that used to be ranked second is now ranked in position k and each
dummy candidate is ranked in top k positions exactly once). It is easy to see that the proof
goes through without change.
5.3 Copeland
To analyze cloning under the Copeland rule, we need some additinal denitions. For an
election E with a set of candidates A, its pairwise majority graph is a directed graph (A, X),
where X contains an edge from a to b if more than half of the voters prefer a to b; we say
that a beats b if (a, b)  X. If exactly half of the voters prefer a to b, we say that a and
b are tied (this does not mean that their Copeland scores are equal). For any a  A, we
denote by U (a), D(a) and T (a) the sets of all alternatives that beat a, are beaten by a, or
are tied with a, respectively.
For an odd number of voters, the graph (A, X) is a tournament, i.e., for each pair
(a, b)  A2 , a = b, we have either (a, b)  X or (b, a)  X. In this case, we can make
use of a well-known tournament solution concept of Uncovered Set (Miller, 1977; Fishburn,
1977; Laslier, 1997), dened as follows. Given a tournament (A, X), a candidate a is said
to cover another candidate b if a beats b as well as every other candidate beaten by b. The
Uncovered Set of (A, X) is the set of all candidates not covered by other candidates.
It turns out that if the number of voters is odd, the Uncovered Set coincides with the set
of candidates that can be made Copeland winners by cloning. In contrast with our previous
characterization results, this holds for all values of q  (0, 1]  {0+ }.
Theorem 5.9. For any q  (0, 1]  {0+ }, a Copeland election E with an odd number of
voters is q-manipulable by cloning if and only if the manipulators preferred candidate c
does not win, but is in the Uncovered Set of the pairwise majority graph of E.
Proof. Consider an election E with a set of m alternatives A and an odd number of voters.
Let (A, X) be its pairwise majority graph. Suppose rst that c is covered by some a  A.
In this case Sc C (c) < Sc C (a). Creating k clones of an alternative x increases by k  1 the
score of each alternative that beats x and decreases by k  1 the score of each alternative
that is beaten by x. Hence, the gap between c and a cannot be reduced by cloning a third
alternative. Moreover, if we replace c with k clones, the score of each clone of c will be at
most Sc C (c) + k  1, while the score of a becomes Sc C (a) + k  1. Similarly, if we replace
a with k clones, the scores of each clone of a will be at least Sc C (a)  (k  1), while the
score of c becomes Sc C (c)  (k  1). This shows that cloning c or a would not help either.
Thus, no matter which alternatives we clone, we cannot close the gap between a and c (or
its highest-scoring clone).
Conversely, suppose that c is in the Uncovered Set. Since the number of candidates
is odd, we have T (c) = , and since c is uncovered, we have U (c) = A \ {c}. Therefore,
D(c) = . We will proceed in two stages. First, we will secure that either c or its clone has
a higher Copeland score than any alternative from D(c), and then we will take care of the
alternatives from U (c). At stage one we create 2m + 1 clones of c. This lowers the score
of each alternative in D(c) by 2m and raises the score of each alternative in U (c) by 2m.
A simple counting argument shows that, for any ordering of the clones by the voters, there
exists a clone of c whose score is greater or equal to the original score of c; denote this clone
551

fiElkind, Faliszewski & Slinko

by c . For each alternative x, let Sc C (x) denote xs score after stage one. For each x  D(c)
we have
Sc C (c )  Sc C (c)  (m  1) > Sc C (x)  2m = Sc C (x),
i.e., c has a higher Copeland score than x. On the other hand, for any x  U (c) we have
Sc C (c )  Sc C (c)  (m  1)  Sc C (x)  (2m  2) = Sc C (x)  (4m  2).
At stage two we create 4m + 1 clones of each alternative in D(c). This increases the
score of c by 4m|D(c)|. Further, for any a  D(c), the score of any clone of a constructed
at this stage exceeds Sc C (a) by at most 4m|D(c)|. Thus, at this stage, c has a higher
Copeland score than any of the newly-generated clones. Finally, since c was not covered,
no candidate in U (c) beats all candidates in D(c); in fact, any a  U (c) is beaten by some
b  D(c). Thus, the last step increases the score of each candidate in U (c) by at most
4m(|D(c)|  1)  4m = 4m(|D(c)|  2). It follows that c now has a higher Copeland score
than any candidate that is not a clone of c.
For elections with an even number of voters, the situation is signicantly more complicated. The notion of Uncovered Set can be extended to pairwise majority graphs of arbitrary
elections in a natural way (see, e.g., Brandt & Fischer, 2007): we say that u covers c if u
beats c and all alternatives beaten by c, and, in addition, c loses to all alternatives that beat
u. In particular, this means that u does not cover c if it is beaten by some alternative that
is tied with c. This denition generalizes the one for the odd number of voters. However,
for an even number of voters, the condition that c is in the Uncovered Set turns out to be
necessary, but not sucient for manipulability by cloning.
Proposition 5.10. In a Copeland election E with an even number of voters in which the
manipulators preferred candidate c is covered, c cannot be made a winner by cloning.
Proof. Suppose that c is covered by u. The Copeland score of c is given by |D(c)|  |U (c)|.
Now, u beats all alternatives in D(c), and any alternative that beats u is necessarily in U (c).
Thus, u has a strictly higher Copeland score than c. Now, suppose we create k + 1 clones
of some a  A. This can increase the score of c (or one of its clones) only if a  D(c)  {c};
however, then the score of u also increases by k. On the other hand, this can decrease the
score of u only if a beats u. However, in this case the score of c also decreases by k. We
conclude that c cannot be made a winner by cloning.
However, the converse is not true, as illustrated by the following example.
Example 5.11. Consider an election with A = {a, b, c, u, w}. Suppose that a beats u, u
beats b, b beats w, w beats a, u and w beat c, and any other pair of candidates is tied.
Note that by McGarveys theorem (1953) there are voters preferences that produce this
pairwise majority graph. In this election, c is undominated. Indeed, while he is beaten by
u and w, he is tied with an alternative that beats u, namely, a, and with an alternative
that beats w, namely, b. However, cs score is negative, and will remain negative after any
cloning. On the other hand, by a counting argument, in any election where some alternative
has a negative Copeland score, at least one other alternative has a positive Copeland score.
Hence, c cannot be made a winner by cloning.
552

fiCloning in Elections

u
11
00
00
11

11
00
a
00
11

1
0
c
0
1

b
11
00

1
0

w

Figure 1: Candidate c is undominated, but cannot be made a winner by cloning
Instead, we can characterize 0+ -manipulable proles in terms of the properties of the
induced (bipartite) subgraph of (A, X) whose vertices are, on the one hand, the candidates
that are tied with c, and, on the other hand, the candidates that beat both c and all
candidates beaten by c. However, it is not clear if this characterization leads to a polynomialtime algorithm for detecting such proles.
Specically, for a prole in which c is not covered to be 0+ -manipulable by cloning in
favor of c, the following must hold. Let Y = T (c) and let Z be the set of all candidates
that beat both c and all candidates beaten by c. We associate with each candidate z  Z
a number sz = Sc C (z)  Sc C (c). Our goal is to assign a nonnegative integer q(y) to every
y  Y so that for each z  Z we have
fi
(y,z)X

q(y) 

fi

q(y)  sz .

(1)

(z,y)X

Indeed, if this linear program has an integer nonnegative solution (q  (y))yY
, we can replace
 (y)+1 clones. This will lower the score of each z  Z by

each
y

Y
with
q
(y,z)X q (y)


(z,y)X q (y) and thus ensure that cs Copeland score is at least as high as that of any
candidate in Z. We can then take care of the rest of the candidates by cloning c (and asking
all voters to order all clones of c in the same way) and the candidates in D(c), as in the
proof of Theorem 5.9. Thus, condition (1) is sucient for 0+ -manipulability. (We remark,
however, that additional constraints may be needed for 1-manipulability, since cloning c
may have a stronger eect on candidates in Z than on the average clone of c.)
Conversely, if such an assignment is impossible, there is no way to ensure that all
candidates in Z have a lower Copeland score than c. Indeed, cloning c or the candidates
in D(c) will not help, as any such candidate is also beaten by any candidate in Z. On the
other hand, cloning a candidate in U (c) will harm c (or its clones) at least as much as it
will harm the candidates in Z. Thus, the only way to close the gap between c and the
candidates in Z is to clone candidates in T (c), as captured by our integer linear program.
Note also that if the linear program (1) does not admit an integer solution, this remains
to be the case if we clone some of the candidates. Indeed, cloning candidates in A \ (Y  Z)
does not change (1). Cloning a candidate z  Z replaces an existing constraint with several
identical ones. Finally, if the program obtained by cloning a candidate y  Y has a feasible
solution, it can be easily transformed into a feasible solution for the original program.
Once we introduce costs, optimal cloning becomes hard even for elections with an odd
number of voters and even in the UC model.
553

fiElkind, Faliszewski & Slinko

Theorem 5.12. For Copeland, UC q-Cloning is NP-hard for each q  {0+ }  (0, 1].
Proof. We give a reduction from X3C. Let I = (G, S) be our input instance, where G =
{g1 , . . . , g3K } is the ground set and S = {S1 , . . . , SM } is a family of 3-element subsets of G.
It will be convenient to assume that M  2K; we can always achieve this by duplicating
some sets in S. We will construct an instance of Copeland elections with a preferred
candidate c such that if I is a yes-instance of X3C, then there is a 1-successful cloning
that introduces at most K clones, and, furthermore, if I is a no-instance of X3C, then
there does not exist a 0+ -successful cloning that introduces at most K clones.
The set of candidates for our election will be A = G  F  D  {c}, where c is the
manipulators preferred candidate, F = {f1 , . . . , fM }, and D = {d1 , . . . , d3M +2 }. The
cloning budget is set to K. We describe the voters by providing the outcomes of all headto-head contests between candidates in A. By McGarveys theorem (McGarvey, 1953), any
such tournament can be realized as the majority relation of a certain preference prole,
which can be computed in polynomial time. The results of head-to-head contests between
the candidates in G  F  {c} will be as follows:
1. c beats each fj  F .
2. Each gi  G beats c.
3. For each gi  G and each Sj  S, gi beats fj if and only if gi 
/ Sj .
4. All the remaining contests within G  F  {c} result in a tie.
If we limit ourselves to candidates in G  F  {c}, we have the following scores: (a) c has
M  3K points, (b) each gi  G has between 1 and M + 1 points, and (c) each fj  F has
at most 0 points.
Further, we set the results of head-to-head contests between candidates in G  F  {c}
and those in D as follows:
5. c beats 2M + 2K  3M candidates in {d1 , . . . , d3M }.
6. Each gi  G beats exactly as many candidates in {d1 , . . . , d3M } as to have score
3M  K + 1.
7. All the other contests between candidates in G  F  {c} and those in D result in a
tie.
Finally, we set the contests within D as follows: Each di  {d1 , . . . , d3M } loses to d3M +1
and to d3M +2 , and all the remaining contests within D result in a tie. Let N = 3M . In the
resulting election we have the following scores:
1. c has N  K points.
2. Each gi  G has N  K + 1 points.
3. dN +1 and dN +2 have N points each.
4. Every other candidate has at most 0 points.
554

fiCloning in Elections

Consider some nonnegative integer k. Replacing a single candidate with k+1 clones increases
other candidates scores by at most k, and the score of each clone can dier from that of the
cloned candidate by at most k. As a result, the only candidates that can be winners after
a cloning manipulation of cost K are those in G  {c, dN +1 , dN +2 }. Thus, in the following
discussion we will consider the scores of these candidates only.
Introducing a single clone of some fj  F increases by 1 the scores of c and of each
/ Sj . Thus, if there is a set J  {1, . . . , M } such that |J| = K and iJ Si = G, then
gi 
introducing a single clone of each candidate in {fi | i  J} ensures that c is a winner of
the election. This does not depend on how the voters order the introduced clones, i.e., this
cloning strategy is 1-successful.
For the converse direction, we will now argue that if there is a 0+ -successful cloning of
cost at most K, then I is a yes-instance of X3C. Let us consider such a 0+ -successful
cloning. We have to introduce K clones, because each additional clone can increase cs
score by at most 1, c trails dN +1 and dN +2 by K points, and after any cloning some clones
of dN +1 and dN +2 will have at least N points. Consequently, we cannot clone any of the
candidates in D: cloning any di  D \ {dN +1 , dN +2 } increases the scores of dN +1 and dN +2 ,
and cloning dN +1 or dN +2 does not increase cs score. Cloning any gi  G also does not
increase cs score, and thus we can only clone members of F  {c}. We will now argue that
the cloned members of F correspond to a cover of G (note that this implies that there is
exactly K of them, and each of them is cloned exactly once). Indeed, suppose otherwise,
and let gi be an element of G that is not covered by the union of the sets that correspond
to cloned members of F . Then our cloning manipulation increases the score of gi by K, as
each new clone contributes to the increase of gi s score. On the other hand, any cloning that
is within our budget can increase cs score by at most K, so c still trails gi , a contradiction
with the assumption that our cloning manipulation is 0+ -successful. This completes the
proof.

6. Related Work
We will now review several lines of research that are related to our work. We start by
discussing the relevant work that originates in the social choice community (Section 6.1),
and then move on to the computational study of voting problems. In Section 6.2 we provide a
detailed comparison between cloning and the classic problem of control by adding candidates
(Bartholdi et al., 1992), and in Section 6.3 we discuss other related work in computational
social choice.
6.1 Cloning in Social Choice Literature
The rst study of cloning was undertaken by Tideman (1987). He started by dening which
subsets of the alternative set A are clones for a given prole. Specically, he dened a
proper subset S of A that contains at least two alternatives to be a clone-set if no voter
ranks any candidate outside of S in between the members of S or as tied with some member
of S. This denition reects the idea that each voter nds the candidates in S similar. Now,
every prole R on A denes a set of clone-sets C(R)  2A . Tideman denes a voting rule
to be independent of clones if and only if the following two conditions are met when clones
are on the ballot:
555

fiElkind, Faliszewski & Slinko

1. A candidate that is a member of a set of clones wins if and only if some member of
that set of clones wins after a member of the set is eliminated from the ballot.
2. A candidate that is not a member of a set of clones wins if and only if that candidate
wins after any clone is eliminated from the ballot.
Tideman considered a number of well-known voting rules, and discovered that among these
rules STV was the only one that satised his criterion. However, STV does not satisfy
many other important criteria for voting rules, such as Condorcet consistency or monotonicity. Thus, Tideman proposed a new voting rule, the ranked pairs rule, that was
both Condorcet consistent and independent of clones in all but a small fraction of settings. Subsequently, Zavist and Tideman (1989) proposed a modication of this rule that
is completely independent of clones. Later it was shown that some other voting rules, such
as Schulzes rule (2003), are also independent of clones. Tideman considered resistance
to cloning to be an important normative requirement for voting rules, and the method of
ranked pairs proposed by him and Zavist showed how this condition may be satised.
Another notion related to cloning is that of composition consistency, due to Laond et al.
(1996). It is dened for tournament solution concepts (i.e., social choice correspondences
that take tournaments as inputs). For a given alternative set A, let T (A) denote the set of
all tournaments on A. A non-empty subset C of A is a component of T  T (A) if for any
c, c  C and for any a  A \ C it holds that (a, c)  T if and only if (a, c )  T ; a component
C is said to be nontrivial if 1 < |C| < |A|. A tournament T is said to be composed if it has
at least one nontrivial component. Components of a tournament are natural counterparts
of clone-sets of a preference prole.
Let A1 , . . . , AK be K disjoint nite sets of alternatives, and let T1 , . . . , TK be K tournaments such that for each k = 1, . . . , K it holds that Tk  T (Ak ). Moreover, let T  be a
tournament in T ([K]), where [K] = {1, 2, . . . , K}. The composition product of T  by tournaments T1 , . . . , TK is the tournament T = (T  ; T1 , . . . , TK ) dened on A = A1  . . .  AK
as follows. For each k, k   [K] and each (a, b)  Ak  Ak the tournament T contains the
pair (a, b) if and only if (i) k = k  and (k, k  )  T  or (ii) k = k  and (a, b)  Tk .
A tournament solution concept  is said to be composition-consistent
 if for each composition product T = (T  ; T1 , . . . , TK ) in T (A) it holds that (T ) = {(Tk ) | k  (T  )}.
This property may be illustrated in the following way. Suppose that there are K dierent
projects that a given society can choose to implement. Further, each project Ak , k  [K],
has nk dierent variants. Composition consistency guarantees that we will choose the best
variant of the best project irrespectively of whether we use a two-stage procedure that rst
chooses the best project and then its best variant, or simply set up a single tournament in
order to choose among all variants of all projects.
Laond et al. (1996) and Laslier (1996) suggested a similar construction for social choice
correspondences (which are referred to as voting rules in this paper). Given a set X, let
Rn (X) be the set of all n-voter proles on X. Let A1 , . . . , AK be K disjoint nite sets of
alternatives, and consider K preference proles R1 , . . . , RK , where Rk = (R1k , . . . , Rnk ) is a
prole in Rn (Ak ) for k  [K]. Let R = (R1 , . . . , Rn )  Rn ([K]). The composition product
(R ; R1 , . . . , RK ) of R by proles R1 , . . . , RK is the n-voter prole R = (R1 , . . . , Rn )
dened on A = A1  . . .  AK as follows. For each k, k   [K] and each (a, b)  Ak  Ak
we set a Ri b if and only if (i) k = k  and k Ri k  or (ii) k = k  and a Rik b. A social choice
556

fiCloning in Elections

correspondence  is said to be composition-consistent if for any n
> 0 and any composition

1
K
product R = (R ; R , . . . , R ) in Rn (A) it holds that (R) = {(Rk ) | k  (R )}.
Laond et al. (1996) and Laslier (1996) proved that a number of tournament solution
concepts such as the Banks Set, the Uncovered Set, the Tournament Equilibrium Set (TEQ),
and the Minimal Covering Set are composition-consistent, but several other tournament
solution concepts and social choice correspondences such as the Top Cycle, the Slater rule,
the Copeland rule, and all scoring rules are not composition-consistent.
Laslier (2000) also introduced the notion of cloning consistency. A social choice correspondence  is said to be cloning-consistent if for any n >
product
 0 and any composition

1
K

R = (R ; R , . . . , R ) in Rn (A), it holds that (R) = {Ak | k  (R )}. This requirement says that if one clone of an alternative is winning, then all clones of that alternative
must win as well. This property is useful when the set of alternatives is fuzzy, butas
the author himself acknowledgedterrible if the number of alternatives is xed and clearly
dened.
The concept of a composed prole is not necessarily useful for manipulation by cloning,
but is very relevant to what one might call decloning. The idea of decloning is to reveal
whether or not a prole (or, a tournament) could have been obtained as a result of cloning
and, if possible, identify the underlying composition product. Recently, decloning proved
to be a useful preprocessing tool for dealing with voting rules that have a computationally
hard winner determination problem (Conitzer, 2006; Betzler, Fellows, Guo, Niedermeier, &
Rosamond, 2009; Brandt, Brill, & Seedig, 2011). The extended abstract of the current paper, which was presented at the Twenty-Fourth AAAI Conference on Articial Intelligence
(AAAI-2010), initiated a complexity-theoretic study of decloning in voting; this topic was
further investigated by Elkind, Faliszewski, and Slinko (2011). However, decloning deserves
careful study on its own and thus has been omitted from this paper.
The concept of resistance to cloning appeared also in the context of study of selfselectivity of social choice functions. Koray and Slinko (2008) discovered that self-selectivity
is a stronger requirement than resistance to cloning, even if deletion of candidates is viewed
as a special form of cloning (one that replaces a candidate with zero clones). This partially
explains why the universally self-selective functions are necessarily dictatorial, as discovered
earlier by Koray (2008). Koray and Slinko (2008) circumvent this impossibility result by
relaxing the property of self-selectivity: they require the social choice function to select itself only among other reasonable social choice functions. The concept of being reasonable
involves a social choice correspondence (for example, the one that selects all Pareto optimal
alternatives), and it is essential that this social choice correspondence is resistant to cloning
of essential alternatives (for the denition of an essential alternative see Koray & Slinko,
2008).
6.2 Comparison of Cloning and Other Models of Adding Candidates
A problem that is closely related to cloning is that of election control. In general, this term
refers to manipulating the result of an election by changing its structure (e.g., by either
adding or deleting candidates or voters). The computational study of election control was
initiated by Bartholdi et al. (1992) who, among other issues, considered constructive control
by adding candidates (CCAC). In their model, we are given a set of registered candidates,
557

fiElkind, Faliszewski & Slinko

a set of spoiler candidates, and a set of voters, with preferences over both the registered
candidates and the spoiler candidates (however, before we take any action, only the registered candidates participate in the election). The task is to decide if it is possible to select
a subset of spoiler candidates so that when these candidates are registered, the preferred
candidate becomes a winner. Subsequently, Faliszewski, Hemaspaandra, Hemaspaandra,
and Rothe (2009) rened this model by introducing a bound on the number of candidates
that can be added.
Formally, we will use the following denition of the CCAC problem, which is based on
the one given by Faliszewski et al. (2009).
Denition 6.1. Let F be a voting rule. In the constructive control by adding candidates
problem (F-CCAC) we are given an election (C  A, R), where C  A = , a designated
candidate p  C, and a nonnegative integer t. We ask if there is a set A  A of size at
most t such that p is the unique F-winner of the election (C  A , R).4
In the denition above, the set C corresponds to already registered candidates and the set
A is the set of spoiler candidates that the manipulator can introduce into the election.
Bartholdi, Tovey, and Trick considered only two rules, Plurality and Condorcets rule
(i.e., the rule that selects a Condorcet winner if one exists and no winners otherwise), and
focused on standard worst-case complexity results, classifying control problems as either
NP-complete or in P. Many researchers followed up on their work by studying various
other voting rules (Erdelyi, Nowak, & Rothe, 2009b; Faliszewski et al., 2009; Faliszewski,
Hemaspaandra, & Hemaspaandra, 2011) and various other settings (Liu, Feng, Zhu, &
Luan, 2009; Betzler & Uhlmann, 2009; Faliszewski et al., 2011), of which perhaps most
prominent are destructive control of Hemaspaandra, Hemaspaandra, and Rothe (2007) and
control in multi-winner elections of Meir, Procaccia, Rosenschein, and Zohar (2008). We
point the reader to the recent survey of Faliszewski, Hemaspaandra, and Hemaspaandra
(2010) for more details on election control.
While q-Cloning (and, in particular, UC q-Cloning) and CCAC control are similar
in that both of them deal with adding new candidates, neither of these problems is a special
case of the other. Indeed, they place dierent restrictions on the candidates to be added
and their positions in the votes. Specically, in q-Cloning the new candidates must be
clones of existing candidates, but (especially in 0+ -Cloning) we have some freedom as to
how to arrange the new candidates in the votes. In contrast, in CCAC control problems,
the spoiler candidates need not be adjacent to each other in all votes, but the order of all
the candidates in each vote is predetermined.
A somewhat dierent model of adding candidates has been recently proposed by Chevaleyre, Lang, Maudet, and Monnot (2010). In this paper, the authors consider the following
scenario. An election is happening over a period of time and candidates may still join in.
At a given point, we know all the candidates that have registered by then and the voters preferences over those candidates. Each voter may place new candidates arbitrarily
in her vote. Given that at most k new candidates may still appear, which of the already
registered ones still have a chance of winning? (Note that, as in the case of cloning, the
addition of new candidates may benet some of the original candidates and hurt some of
4. Unique-winner model is standard for control problems.

558

fiCloning in Elections

the others.) Chevaleyre et al. (2010), andin a very recent follow-up workXia, Lang,
and Monnot (2011), give computational complexity results for the problem of nding the
possible (co)winners when new alternatives join (PcWNA). Their work diers from ours in
that they do not require the new candidates to be clones of preexisting ones (in particular,
they do not require the new candidates to be ranked consistently, consecutively by all the
voters), and diers from control by adding candidates in that they allow introducing arbitrary new candidates (as opposed to introducing already-ranked-by-voters candidates from
a predened set of spoilers). Formally, their model is a special case of the possible winner
problem, introduced by Konczak and Lang (2005), and further studied by many other researchers (Xia & Conitzer, 2011; Betzler & Dorn, 2010; Bachrach, Betzler, & Faliszewski,
2010; Baumeister & Rothe, 2010).
Table 1 provides a comparison of the complexity of control by adding candidates (CCAC
control), possible co-winner determination when new alternatives can join (PcWNA), and
q-Cloning for q  {0+ , 1} in the UC model (using the UC model is analogous to counting
the number of new candidates in the CCAC control problems). To ll the rst column
of Table 1, we provide complexity results for CCAC control for Plurality with Runo, kapproval, Borda, and Veto, which were missing from the existing literature. Since these
results are tangential to the topic of our paper, we relegate them to Appendix B.
Table 1 shows that cloning and PcWNA are computationally incomparable (assuming
P = NP). For example, for 2-Approval 0+ -Cloning is NP-hard, whereas PcWNA is in P.
On the other hand, for Maximin 0+ -Cloning is in P, while PcWNA is NP-complete.
In contrast, Table 1 appears to indicate that CCAC control is harder than cloning.
These results are not entirely surprising: we can construct contrived instances of CCAC
control by placing the spoiler candidates in any way we like, so as to facilitate computational
hardness proofs. One may therefore conjecture that UC 0+ -Cloning is always easier than
CCAC control. However, it turns out that this is not the case.
Theorem 6.2. There is a voting rule for which constructive control by adding candidates
is in P, but UC 0+ -Cloning is NP-hard.
The proof of Theorem 6.2 is given in Appendix B; while the voting rule constructed in
this proof is highly articial, it demonstrates that UC 0+ -Cloning cannot be reduced to
CCAC control (unless P = NP).
To wrap up our discussion of control by adding candidates, we mention an interesting
twist to the standard model of election control that was recently studied by Faliszewski,
Hemaspaandra, Hemaspaandra, and Rothe (2011) and Brandt, Brill, Hemaspaandra, and
Hemaspaandra (2010). In these papers, the authors study the complexity of control (as
well as manipulation and bribery) for single-peaked electorates. Their main nding is that
many control problems that are known to be NP-hard for unrestricted preferences (and
most control problems belong to this category) turn out to be solvable in polynomial time
when the preferences are single-peaked. In comparison, cloning is computationally feasible
for many rules even without assuming special properties of the electorate. As a side note,
we mention that cloning a candidate may destroy single-peakedness of an election: if each
voter ranks the clones uniformly at random, the resulting ranking of the clones is unlikely
to be single-peaked. The problem of collapsing the minimum number of clones in order
to make a given election single-peaked is studied in detail by Elkind et al. (2011).
559

fiElkind, Faliszewski & Slinko

Voting rule
Plurality
Maximin
Plurality
w/Runo
Veto

CCAC control
NPC (Bartholdi
et al., 1992)
NPC (Faliszewski
et al., 2011)
NPC

PcWNA
P (Betzler & Dorn,
2010)
NPC (Xia et al., 2011)

0+ -Cloning
P

1-Cloning


P



P (Xia et al., 2011)

P



NPC

P (Betzler & Dorn,
2010)
P (Chevaleyre et al.,
2010)
P (Chevaleyre et al.,
2010)
NPC
(Chevaleyre
et al., 2010)
?

P

P

P

?

NP-hard

NP-hard

NP-hard

NP-hard

NP-hard

NP-hard

Borda

NPC

2-Approval

NPC

k-Approval,
k3
Copeland

NPC
NPC (Faliszewski
et al., 2009)

Table 1: The complexity of control via adding candidates (CCAC), of possible co-winner
determination when new alternatives can join (PcWNA), and of q-Cloning in
the UC model for q  {0+ , 1}. Note that for Plurality, Plurality with Runo and
Maximin 1-successful cloning is impossible. All the results on CCAC control are in
the unique-winner model (though some of them are also proved in the non-unique
winner model), whereas we work in the non-unique winner model. The PcWNA
results for Plurality and Veto follow directly from the more general results of
Betzler and Dorn (2010) for the possible winner problem. Xia et al. (2011) also
give an NP-completeness result for a variant of Copeland rule known as Copeland0 ,
where the score of a candidate is simply the number of head-to-head contests that
this candidate wins.

560

fiCloning in Elections

6.3 Cloning and Campaign Management
To a large extent, our work on cloning is motivated by applications of cloning in campaign
management. However, campaign management can be understood in multiple other ways
as well. In particular, the issue of campaign management in voting has been previously
studied from the computational perspective by Elkind, Faliszewski, and Slinko (2009), who
introduced the problem of swap bribery. In their model, each voter is associated with a
certain cost function, which describes how dicult it is to make local changes to this voters
preferences. The goal of the campaign manager is to ensure that a given candidate becomes a
winner at the smallest possible cost. While this problem turns out to be NP-hard for almost
all voting rules, some of its special cases admit polynomial-time solutions. Further, if one
focuses on a variant of swap bribery where one is only allowed to shift forward the preferred
candidate, it is possible to nd eective (approximation) algorithms (Elkind et al., 2009;
Elkind & Faliszewski, 2010; Schlotter, Elkind, & Faliszewski, 2011). Going in a dierent
research direction, Dorn and Schlotter (2010) provide parameterized complexity study of
swap bribery. Of course, the standard model of bribery (Faliszewski, Hemaspaandra, &
Hemaspaandra, 2009), where one can pay a voter to change his vote arbitrarily, can also be
interpreted in the context of campaign management.
Also, the probabilistic model put forward in this paper, and, in particular, our denition
of q-successful cloning is similar in spirit to the model of Erdelyi, Fernau, Goldsmith, Mattei,
Raible, and Rothe (2009a), where voters are bribed to increase their probabilities of voting
in favor of a particular alternative.
Finally, the problem of cloning is particularly relevant in open, anonymous environments,
such as the Internet. In such settings, a problem closely related to cloning candidates is that
of cloning voters. Specically, in an anonymous environment an agent might be capable of
creating several instances of itself and vote multiple times. Voting rules that are resistant
to this kind of manipulation are called false-name-proof; they were studied by Conitzer
(2008). A variant of this framework in which, similarly to our general model, creating new
identities is costly was subsequently considered by Wagman and Conitzer (2008).

7. Conclusions
We have provided a formal model of manipulating elections by cloning, characterized 0+ manipulable and 1-manipulable proles for many well-known voting rules, and explored the
complexity of nding a minimum-cost cloning manipulation. The grouping of voting rules
according to their susceptibility to manipulation diers from most standard classications
of voting rules: e.g., scoring rules behave very dierently from each other, and Maximin is
more similar to Plurality than to Copeland. Future research directions include designing
approximation algorithms for the minimum-cost cloning under voting rules for which this
problem is known to be NP-hard, and extending our results to other voting rules.

Acknowledgments
We would like to thank the anonymous JAIR referees for their very useful feedback. Edith
Elkind is supported by NRF (Singapore) Research Fellowship (NRF-RF2009-08) and an
NTU start-up grant. Piotr Faliszewski is supported by AGH University of Technology
561

fiElkind, Faliszewski & Slinko

Grant no. 11.11.120.865, by Polish Ministry of Science and Higher Education grant N-N206378637, and by Foundation for Polish Sciences program Homing/Powroty. Arkadii Slinko is
supported by the Faculty of Science Research and Development Fund grant 3624495/9844.

Appendix A. Cloning the Manipulators Preferred Candidate under
Borda: Odd Number of Voters
In this section, we present the proof of Proposition 5.3. (We use the notation introduced in
the paragraph preceding Theorem 5.2.)
Proposition 5.3. If an election with an odd number of voters can be 1-manipulated with
respect to Borda by cloning the manipulators preferred candidate c, then r+   r .
Proof. Let n be the number of voters. Suppose that r+  > r . Consider any cloning
that involves c only. Suppose it results in k clones of c, which we denote by c(1) , . . . , c(k) .
Let s denote the original Borda score of c. To show that this cloning is not 1-successful, it
suces to describe an ordering of the clones that results in c losing the election. Since for
n = 1 no cloning is successful, we can assume without loss of generality that n  3.
Suppose rst that k is odd. Consider the prole5 where the rst voter ranks the clones
as
c(k)  c(k2)  . . .  c(1)  c(k1)  c(k3)  . . .  c(2) ,
the second voter ranks the clones as
c(k1)  c(k3)  . . .  c(2)  c(k)  c(k2)  . . .  c(1) ,
the third voter ranks the clones as
c(1)  . . .  c(k) ,
and the remaining voters are split into n3
2 pairs, where in each pair the rst voter ranks
(k)
(1)
the clones as c  . . .  c , while the second voter ranks the clones as c(1)  . . .  c(k) .
.
It is not hard to see that in this prole the Borda score of each clone is s + n(k1)
2
Indeed, let us rst consider c(k) . The rst voter ranks k  1 other clones below c(k) , the
(k)
second voter ranks k1
2 other clones below c , and the third voter ranks no other clones
3(k1)
below c(k) , i.e., c(k) gets 2 additional points from the rst 3 voters. Also, he gets k  1
additional points.
additional point from each pair of the remaining voters, i.e., (n3)(k1)
2
n(k1)
Thus, his Borda score is s + 2 . A similar calculation shows that the Borda score of
(n3)(k1)
c(k1) is s + k3
= s + n(k1)
. Now, if we compare two consecutive
2 + (k  1) + 1 +
2
2
odd-numbered clones, i.e., c(j) and c(j2) for j odd, we can see that c(j) is ranked just above
c(j2) in the rst two votes, and two positions below c(j) in the third vote, so they get the
same number of extra points from the rst three voters. Since any two candidates get the
same number of votes from the last n  3 voters, it follows that c(j) and c(j2) have the same
Borda scores. The same argument applies to any pair of consecutive even-numbered clones.
.
Hence, a simple inductive argument shows that the Borda score of each clone is s + n(k1)
2
5. We are grateful to Dima Shiryaev for suggesting this construction.

562

fiCloning in Elections

Now, if k is even, we set k  = k  1, and rank the rst k  clones using the construction
for odd k given above. We then place c(k) above all other clones in the rst n1
2 votes and
below all other clones in the remaining n+1
votes.
Clearly,
the
score
of
any
clone
c(j) , j < k,
2
is


n(k  2) n + 1
n(k  1) + 1
n(k  1)
;
s+
+
=s+
=s+
2
2
2
2
the last equality holds since n is odd and k is even. Moreover, the score of c(k) is s + (k 
n(k1)
. Thus, for all values of k the Borda score of each clone is at most
1) n1
2  s+
2
n(k1)
s +  2 .
To show that in this prole c does not win, we will consider two cases.
Case 1 (r + = +). Then there is an alternative a that is preferred to c by at least n+1
2
voters and has a higher Borda score than c. Our cloning increases as score by at
n(k1)
n+1
.
least n+1
2 (k  1), so its nal Borda score is at least sB (a) + 2 (k  1) > s + 
2
Thus, after cloning, all clones of c will still have lower scores than a, i.e., the cloning
is not 1-successful.
Case 2 (r + < +). In this case, there exist candidates a  A+ , b  A such that
a
. After the cloning, as Borda score is sB (a) + nn
 2snaa1  >  2snb +1
2 (k  1), and bs
b
b
Borda score is sB (b) + n+n
2 (k  1). Thus, for c to be the winner, k must satisfy

n(k  1)
n  na
 sB (a) +
s+
(k  1);
2
2



We have  n(k1)
2

n(k1)+1
.
2

sa = sB (a)  s 


n + nb
n(k  1)
 sB (b) +
s+
(k  1).
2
2


Thus, by rewriting the above inequalities, we obtain

na
1
(k  1) + ,
2
2

sb = s  sB (b) 

nb
1
(k  1)  .
2
2

Since k is an integer, this implies
ff




2sb + 1
2sa  1
+
k1
= r ,
r =
na
nb
a contradiction.

Appendix B. Proofs for Section 6.2
The proof of Theorem B.1 proceeds by a fairly straightforward reduction from PluralityCCAC. In contrast, the remaining proofs in this section employ reductions from X3C, and
are quite technical.
Theorem B.1. For each xed k, k  1, constructive control by adding candidates for
k-Approval is NP-complete.
563

fiElkind, Faliszewski & Slinko

Proof. For k = 1, k-Approval is simply Plurality and Plurality-CCAC has been shown to
be NP-complete (Bartholdi et al., 1992). Thus, let us assume that k  2.
Clearly, k-Approval-CCAC is in NP. To prove hardness, we give a reduction from
Plurality-CCAC to k-Approval-CCAC. Given an instance I = (C, A, R, p, t) of PluralityCCAC with R = (R1 , . . . , Rn ) (see Denition 6.1), we build an instance I = (C, A, R, p, t)
of k-Approval-CCAC as follows:
1. If |R| = 1, we solve I in polynomial time (which is easy to do in this case) and output
a xed instance of k-Approval-CCAC with the same answer.
2. If n = |R| > 1, we set D = {di,j | 1  i  n, 1  j  k  1} and let C = C  D. For
i = 1, . . . , n, in the i-th preference order Ri the candidates di,1 , . . . , di,k1 are ranked
in positions 1, . . . , k  1, and the candidate that was ranked rst in Ri is ranked in
position k. We set R = (R1 , . . . , Rn ).
Clearly, if n = 1, the reduction works correctly. Now suppose n > 1. For each A  A,
and each candidate c  C  A , the Plurality score of c in (C  A , R) is the same as his
k-Approval score in (C  A , R), while the k-Approval score of any d  D in (C  A , R) is 1.
To complete the proof, it remains to observe that if some candidate c is a unique winner
of a Plurality election with at least two voters, then c receives at least two points.
Theorem B.2. Constructive control by adding candidates for Plurality with Runo is NPcomplete.
Proof. The problem is clearly in NP. Our hardness reduction is from X3C. Let I = (G, S) be
an input instance of X3C, where G = {g1 , . . . , g3K } is the ground set and S = {S1 , . . . , SM }
is a family of 3-element subsets of G.
We construct an instance of CCAC for Plurality with Runo as follows. We let the
candidate set be C = {p, u, w}, and we let the set of spoiler candidates be A = {a1 , . . . , aM }.
The preference prole R consists of 6K + 20 preference orders R1 , . . . , R6K+20 , described
as follows. For i = 1, . . . , 3K, let Ai = {aj  A | gi  Sj }. Preference orders Ri and R3K+i ,
i = 1, . . . , 3K, are given by
Ai  u  A \ Ai  p  w

and

Ai  w  A \ Ai  p  u,

respectively. There are also 7 voters whose preference order is p  A  u  w, 7 voters
whose preference order is u  A  p  w, and 6 voters whose preference order is w  A 
p  u. Finally, we set t = K.
We claim that p can become the unique winner of election (C, R) by adding at most
t = K spoiler candidates from A if and only if I is a yes-instance of X3C.
Let us rst consider Plurality scores of the candidates in (C, R). We have Sc P (u) =
3K + 7, Sc P (w) = 3K + 6, and Sc P (p) = 7. The runo is between u and w, and thus p
does not win.
Now, consider some subset A of A and an election (C  A , R). Let Sc P (c) denote the
Plurality score of a candidate c  C  A in election (C  A , R). We have Sc P (p) = 7,
Sc P (u)  7, Sc P (w) = Sc P (u)  1, and, moreover, Sc P (ai )  6 for any ai  A . This
implies that no candidate from A can ever participate in the runo. Thus, depending on
the participating spoiler candidates, the following runo scenarios are possible:
564

fiCloning in Elections

1. Sc P (u)  9. Then Sc P (w)  8, and the runo is between u and w.
2. Sc P (u) = 8. Then Sc P (w) = 7, and the runo is between u and either p or w.
3. Sc P (u) = 7. Then Sc P (w) = 6, and the runo is between u and p.
As we use the parallel-universe tie-breaking rule, and more than half of the voters prefer
p to u, p is the unique winner of the election if and only if Sc P (u) = 7. That is, p can be
made the unique winner of the election by adding at most t candidates if and only if it is
possible to choose a subset A of A such that |A |  K and u receives no Plurality points
from the rst 6K voters in election (C  A , R). Clearly, such a set A has the property
that in every preference order among R1 , . . . , R3K some member of A is ranked above u.
As |A |  K, this is possible if and only if the collection S  = {Si | ai  A } is an exact
3-cover of G.
To prove that CCAC control for Borda is NP-complete as well, we need a tool to
construct Borda votes conveniently.
Lemma B.3. Let C = {c1 , . . . , c2t1 , d}, t  2, be a set of candidates and let A =
{a1 , . . . , as } be a set of
spoiler candidates. Let 1 , . . . , 2t1 be a sequence of nonnegative
integers, and set L = 2t1
i=1 i . Then there is a preference prole R = (R1 , . . . , R2L ) over
C  A such that for each A  A the Borda scores in the election (C  A , R) are as follows:
1. For each ci  C, Sc B (ci ) = L(2|A | + |C|  1) + i .
2. Sc B (d) = L(2|A | + |C|  1)  L.
3. For each ai  A , Sc B (ai )  L(2|A | + |C|  1)  2L.
Moreover, the preference prole R is computable in time polynomial in |C| + |A| + L.
Proof. For each i = 1, . . . , 2t  1, set e = ci , renumber the candidates in C \ {d, e} as
b1 , . . . , b2t2 , and consider preference orders R2i1 and R2i given by
R2i1 : b1  b2  . . .  bt1  e  d  bt  . . .  b2t2  A,
R2i : b2t2  b2t3  . . .  bt  e  d  bt1  . . .  b1  A.
Let Ri = (R2i1 , R2i ). For any given A  A, in election (C  A , Ri ) each bi  C \ {d, e}
receives 2|A | + |C|  1 points, e receives 2|A | + |C| points, and d receives 2|A | + |C|  2
points. Moreover, each candidate ai in A receives at most 2|A |  2 points.
Our preference prole R has i copies of R2i1 and i copies of R2i for each i = 1, . . . , 2t
1. It is easy to see that it satises the condition of the lemma.
Theorem B.4. Constructive control by adding candidates for Borda is NP-complete.
Proof. It is easy to see that the CCAC control problem for Borda is in NP. We will now
show that this problem is NP-hard by giving a reduction from X3C.
Let (G, S) be our input instance of X3C, where G = {g1 , . . . , g3K } is the ground set
and S = {S1 , . . . , SM } is a collection of 3-element subsets of G. We assume without loss of
generality that K is even and K > 2; this can be achieved, e.g., by duplicating the instance.
565

fiElkind, Faliszewski & Slinko

We construct an instance of our problem as follows. The set of registered candidates
C is {p, d}  G (we will sometimes refer to p as g3K+1 ) and the set of spoiler candidates
is A = {a1 , . . . , aM }. The preference prole R consists of two parts, R and R . We rst
describe R , and dene R based on the number of Borda points the candidates get from
R . For each i = 1, . . . , M , the preference prole R contains exactly one preference order
A \ {ai }  Si  p  ai  G \ Si  d.
For each candidate c  C, let Sc B (c) be the number of Borda points thatc gets from
R , assuming that no candidate from A participates in the election. Set S = cC Sc B (c).
Observe that irrespective of what subset A of spoiler candidates participates in the election,
each spoiler candidate ai gets at most T = M (3K + M + 1) points from R . Also, in an
election with the set of alternatives C  A , candidate p gets Sc B (p) + |A | points from R ,
while each gi  G gets Sc B (gi ) + |{aj | aj  A  gi  Sj }| points from R .
For each i = 1, . . . , 3K, we set
i = T + S  Sc B (gi )  2,
and for p = g3K+1 we set

3K+1 = T + S  Sc B (p)  K.

We obtain R by applying Lemma B.3 to the candidate set C (where each gi takes the
role of ci and p takes the role of c3K+1 ), the spoiler candidate set A, and the sequence
1 , . . . , 3K+1 ; note that here we use the assumption that K is even (and hence |C| is even).
Finally, we set
t = K.
3K+1
i , and for any A  A set f (A ) = L(2|A | + |C|  1) + T + S. For any
Let L = i=1
A  A, in the election (C  A , R + R ) the candidates have the following Borda scores:
1. Sc B (p) = f (A )  K + |A |.
2. For each gi  G, we have Sc B (gi ) = f (A )  2 + |{aj | aj  A  gi  Sj }|.
3. Irrespective of the choice of A , for each ai  A we have Sc B (ai ) < Sc B (p) and
Sc B (d) < Sc B (p).
Now it is easy to see that p is not a Borda winner in the election (C, R + R ). Let
us assume that there is a subset A of the spoiler candidates such that |A |  K and p is
the unique winner of election (C  A , R + R ). By the argument above, A is non-empty.
Now, for any aj  A and any gi  Sj we have Sc B (gi )  f (A )  1. Therefore, for p to
be the unique winner of (C  A , R + R ), it has to be the case that Sc B (p)  f (A ), i.e.,
|A | = K, and, furthermore, for each gi  G there is at most one aj  A such that gi  Sj .
Hence, the collection S  = {Sj | aj  A } consists of K non-overlapping sets of size 3, i.e.,
it is an exact cover of G. Conversely, it is easy to see that if S  is an exact cover of G by
sets from S and A = {aj | Sj  S  }, then p is the unique winner of (C  A , R + R ). This
completes the proof.
We remark that certain aspects of control under Borda have already been studied by
Russel (2007); in fact, Russel mentions the idea of cloning in his work, but does not provide
any results for cloning or CCAC control under Borda.
566

fiCloning in Elections

Theorem B.5. Constructive control by adding candidates for Veto is NP-complete.
Proof. The problem is clearly in NP. To show NP-hardness, we give a reduction from X3C.
Let (G, S) be an input instance of X3C, where G = {g1 , . . . , g3K } is the ground set and
S = {S1 , . . . , SM } is a family of 3-element subsets of G. Without loss of generality we can
assume that K > 2. For each i = 1, . . . , 3K, let i = |{Sj  S | gi  Sj }|.
We form the following instance of our problem. Our set of registered candidates is
C = G  {p}, and the set of spoiler candidates is A = {a1 , . . . , aM }. The preference prole
R consists of four subproles R1 , R2 , R3 , and R4 .
R1 : R1 contains 4M preference orders in M groups of four, one group for each set in S.
If Si = {gi1 , gi2 , gi3 }, the i-th group contains the following four preference orders:
A \ {ai }  p  G \ {gi1 }  gi1  ai ,
A \ {ai }  p  G \ {gi2 }  gi2  ai ,
A \ {ai }  p  G \ {gi3 }  gi3  ai ,
A \ {ai }  G  p  ai .
consists of M  i preference
R2 : R2 contains 3K groups
 of voters, where the i-th
group
3K
(M


)
=
3KM


=
3M (K  1). For each i =
orders, i.e., |R2 | = 3K
i
i
i=1
i=1
1, . . . , 3K, each of the preference orders in the i-th group is given by A  p  G\{gi } 
gi .
R3 : R3 contains K  2 preference orders of the form A  G  p.
R4 : R4 contains (3K + 1)M 2 preference orders. For each i = 1, . . . , M , j = 1, . . . , 3K,
there are M preference orders of the form p  G \ {gj }  gj  A \ {ai }  ai and for
each i = 1, . . . , M , there are M preference orders of the form G  p  A \ {ai }  ai .
Intuitively, R1 models the input X3C instance, R2 ensures that within R1 and R2 all
candidates receive the same number of points (assuming only candidates from C participate), R3 models the constraint that the added candidates must correspond to a cover, and
R4 ensures that none of the spoiler candidates can become a winner. Let us make these
observations more formal by calculating the scores of candidates, assuming that the set of
candidates is C  A for some A  A.
For each j = 1, . . . , 3K, let t(A , gj ) be the number of sets Si  S such that gj  Si and
ai  A . Candidate gj receives |R1 |  j + t(A , gj ) points from R1 , |R2 |  (M  j ) points
from R2 , and |R3 | points from R3 . Thus, in total gj receives
4M  j + t(A , gj ) + 3M (K  1)  (M  j ) + (K  2) = 3KM + t(A , gj ) + (K  2)
points from voters in R1 + R2 + R3 . Similarly, p receives 3M + |A | + |R2 | = 3KM + |A |
points from R1 + R2 + R3 and each ai  A receives |R1 |  4 + |R2 | + |R3 | = 4M  4 +
3M (K  1) + (K  2) = M (3K + 1) + (K  6) points from voters in R1 + R2 + R3 .
It remains to calculate how many points each candidate receives from R4 . Note that
there are (3K + 1)M 2 preference orders in R4 and each ai  A is ranked last in at least
(3K + 1)M of them. Thus, each ai  A receives at most (3K + 1)M 2  (3K + 1)M points
567

fiElkind, Faliszewski & Slinko

from R4 . On the other hand, if A = , each candidate in C receives exactly (3K + 1)M 2
points from R4 , whereas if A = , each candidate in C receives 3KM 2 points from R4 .
We set t = K, and ask whether p can be made the unique winner by adding at most
t spoiler candidates. It is easy to see that if A = , candidate p loses to all candidates
in G, and thus is not the unique winner of the election. Now suppose that A = . Set
F = (3K + 1)M 2 + 3KM + K  1. The candidates have the following scores:
1. Sc V (p) = F + |A |  K + 1,
2. for each gj  G, we have Sc V (gj ) = F  1 + t(A , gj ), and
3. for each ai  A , we have Sc V (ai )  F  3KM  5.
Thus, clearly no member of A is a winner. Let us now assume that 0 < |A |  K and p
is the unique winner of the election. Since t(A , gj )  1 for at least one gj  G, it holds
that some gj  G has at least F points, and therefore the score of p is at least F + 1. This
implies that |A | = K. Further, for each gj  G it must be the case that t(A , gj )  1. Since
|A | = K, this means that the collection {Si | ai  A } is an exact cover of G by sets from
S.
It is easy to see that the converse direction is also true. Thus, it is possible to ensure
that p is a winner of our election by adding at most k candidates from A if and only if
(G, S) is a yes-instance of X3C.
Theorem 6.2. There is a voting rule for which constructive control by adding candidates
is in P, but UC 0+ -Cloning is NP-hard.
Proof. The idea of the proof, common to several results of this type (see, e.g., the paper of
Faliszewski et al., 2009, where the authors construct a voting rule for which the problem of
bribery is in P, but the problem of manipulation is NP-hard), is to embed an NP-complete
problem into the winner determination procedure of the newly constructed voting rule.
Let L be an NP-complete language over the alphabet  = {0, 1}. We will soon provide
some further assumptions regarding L (all easily satised), but before doing so, we describe
how an election can encode a pair of strings over .
Fix an election E = (A, R) with R = (R1 , . . . , R , R+1 ). We can assume without loss
of generality that A = {c1 , . . . , cm } and that the last voter ranks the candidates as c1 +1
c2 +1    +1 cm . We say that E encodes two length- binary strings, x = x1 . . . x and
y = y1 . . . y , if the following conditions hold:
1.  > 0 and m  4.
2. Each voter ranks c1 and c2 in the top two positions.
3. For each i = 1, . . . , , if yi = 0 then c1 i c2 and if yi = 1 then c2 i c1 .
4. For each i = 1, . . . , , if xi = 0 then c3 i c4 i    i cm and if xi = 1 then
cm i cm1 i    i c3 .
568

fiCloning in Elections

Otherwise E does not encode any strings. Note that the last preference order R+1 is
special, as it denes the roles of the candidates; also, by requiring  > 0 we explicitly forbid
encoding a pair of two empty strings.
By denition of NP and by basic properties of NP-complete languages, we can assume
that language L admits a polynomial-time algorithm B such that for every binary string x
of length  it holds that x  L if and only if there is a binary string y of length  such that
B(x, y) accepts.6 Further, we assume that L does not contain the empty string and that it
does not contain any all-0 strings.
We will now dene our voting rule, which we call L. Let E = (A, R) be an election. If E
encodes strings x and y such that B(x, y) accepts (that is, such that x  L and y witnesses
that this is the case), then all candidates in A are winners. Otherwise, the winner is the
candidate that is ranked last by the last voter.
First, we claim that L-CCAC is in P. Let I = (E, p, t) be an instance of L-CCAC,
where E = (C  A, R), C = {c1 , . . . , cm }, A = {a1 , . . . , am }, R = (R1 , . . . , Rn ), p  C is
the preferred candidate, and t  Z+ is the bound on the number of candidates that we can
add. If p is a winner prior to adding candidates, we accept. Note that if p is not ranked
last by the last voter, we cannot change that by adding candidates from A. Therefore, to
make p a winner, we have to add candidates so as to inuence the strings x, y encoded by
the election or to move the election from the state in which it does not encode any strings
to the state where it does. However, if this goal can be achieved by adding some s  t
candidates, then it can also be achieved by adding at most 4 candidates. Indeed, suppose
that we add some candidates ai1 , . . . , ais , 4 < s  t, so that ai1 n ai2 n    n ais and
the resulting election encodes some strings x and y. It is easy to see that if we now remove
the candidates ai5 , . . . , ais , the resulting election also encodes the same strings x and y.
Thus, to test if it is possible to make p a winner by adding candidates from A, it suces
to try adding all subsets of A of size at most 4, and, for each of them, verify whether the
resulting election encodes two strings x and y such that B(x, y) accepts. Clearly, this can
be done in polynomial time.
On the other hand, UC 0+ -Cloning is NP-complete for L (in fact, this remains true for
ZC 0+ -Cloning or any other cost model that allows adding at least one clone). We give
a reduction from L. Let x be our input binary string of length ; we can assume without
loss of generality that  > 0. We create an election E = (A , R) where A = {a, c1 , c2 , c3 } is
the set of candidates and R = (R1 , . . . , R , R+1 ) is the preference prole, where
1. R+1 is given by a +1 c1 +1 c2 +1 c3 .
2. For each i = 1, . . . , , if xi = 0 then Ri is given by a i c1 i c2 i c3 and if xi = 1
then Ri is given by a i c3 i c2 i c1 .
It is easy to see that either this election encodes a pair of strings (0 , 0 ) or it does not
encode any strings. We pick a as our preferred candidate; note that a is not ranked last by
the last voter.
Suppose that x  L, i.e., there exists a string y, |y| = , such that B(x, y) accepts. Then
we can clone a into a(1) and a(2) and ask the voters to order the clones so that the top two
6. Indeed, if we were to require that the length of y is polynomially bounded in |x|, this condition would
be simply the denition of membership in NP. Our requirement that x and y have the same length can
be easily satised by appropriate padding.

569

fiElkind, Faliszewski & Slinko

positions in their preference orders encode y. Clearly, in the resulting election all candidates
win.
Conversely, suppose that it is possible to clone some candidates in A so as to make a
a winner (i.e., so that all candidates are winners). Let E  be the election after cloning. It
must be the case that E  encodes two strings, x and y  , such that B(x , y  ) accepts. For
this to happen, each voter must rank clones of a in the rst two positions and the clones
of c1 , c2 and c3 in the remaining positions. This implies that x = x. Hence, if there is a
0+ -successful cloning for E, then there is one that replaces a with two clones, a(1) and a(2) ,
and asks the voters to rank the clones so that they encode a string y with the property that
B(x, y) accepts.
Our reduction can be computed in polynomial time and thus the proof is complete.

References
Arrow, K., Sen, A., & Suzumura, K. (Eds.). (2002). Handbook of Social Choice and Welfare,
Volume 1. Elsevier.
Bachrach, Y., Betzler, N., & Faliszewski, P. (2010). Probabilistic possible winner determination. In Proceedings of the 24th AAAI Conference on Articial Intelligence, pp.
697702. AAAI Press.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989). The computational diculty of manipulating an election. Social Choice and Welfare, 6 (3), 227241.
Bartholdi, III, J., Tovey, C., & Trick, M. (1992). How hard is it to control an election?.
Mathematical and Computer Modeling, 16 (8/9), 2740.
Baumeister, D., & Rothe, J. (2010). Taking the nal step to a full dichotomy of the possible
winner problem in pure scoring rules. In Proceedings of the 19th European Conference
on Articial Intelligence, pp. 10211022. IOS Press.
Betzler, N., & Dorn, B. (2010). Towards a dichotomy of nding possible winners in elections
based on scoring rules. Journal of Computer and System Sciences, 76 (8), 812836.
Betzler, N., Fellows, M., Guo, J., Niedermeier, R., & Rosamond, F. (2009). Fixed-parameter
algorithms for Kemeny scores. Theoretical Computer Science, 410 (45), 45544570.
Betzler, N., & Uhlmann, J. (2009). Parameterized complexity of candidate control in elections and related digraph problems. Theoretical Computer Science, 410 (52), 4353.
Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. (2010). Bypassing combinatorial protections: Polynomial-time algorithms for single-peaked electorates. In Proceedings of the 24th AAAI Conference on Articial Intelligence, pp. 715722. AAAI
Press.
Brandt, F., Brill, M., & Seedig, G. (2011). On the xed-parameter tractability of
composition-consistent tournament solutions. In Proceedings of the 22nd International
Joint Conference on Articial Intelligence, pp. 8590. AAAI Press.
Brandt, F., & Fischer, F. (2007). Computational aspects of covering in dominance graphs.
In Proceedings of the 22nd AAAI Conference on Articial Intelligence, pp. 694699.
AAAI Press.
570

fiCloning in Elections

Chevaleyre, Y., Lang, J., Maudet, N., & Monnot, J. (2010). Possible winners when new
candidates are added: The case of scoring rules. In Proceedings of the 24th AAAI
Conference on Articial Intelligence, pp. 762767. AAAI Press.
Conitzer, V. (2006). Computing Slater rankings using similarities among candidates. In
Proceedings of the 21st National Conference on Articial Intelligence, pp. 613619.
AAAI Press.
Conitzer, V. (2008). Anonymity-proof voting rules. In Proceedings of the 4th International
Workshop on Internet and Network Economics, pp. 295306. Springer-Verlag Lecture
Notes in Computer Science #5385.
Conitzer, V., Rognlie, M., & Xia, L. (2009). Preference functions that score rankings and
maximum likelihood estimation. In Proceedings of the 21st International Joint Conference on Articial Intelligence, pp. 109115. AAAI Press.
Dorn, B., & Schlotter, I. (2010). Multivariate complexity analysis of swap bribery. In
Proceedings of the 5th International Symposium on Parameterized and Exact Computation, pp. 107122.
Dutta, B., Jackson, M., & Le Breton, M. (2001). Strategic candidacy and voting procedures.
Econometrica, 69(4), 10131037.
Dutta, B., Jackson, M., & Le Breton, M. (2002). Voting by successive elimination and
strategic candidacy. Journal of Economic Theory, 103, 190218.
Elkind, E., & Faliszewski, P. (2010). Approximation algorithms for campaign management.
In Proceedings of the 6th International Workshop on Internet and Network Economics,
pp. 473482. Springer-Verlag Lecture Notes in Computer Science #6484.
Elkind, E., Faliszewski, P., & Slinko, A. (2009). Swap bribery. In Proceedings of the 2nd
International Symposium on Algorithmic Game Theory, pp. 299310. Springer-Verlag
Lecture Notes in Computer Science #5814.
Elkind, E., Faliszewski, P., & Slinko, A. (2011). Clone structures in voters preferences.
Tech. rep. arXiv:1110.3939 [cs.GT], arXiv.org.
Ephrati, E., & Rosenschein, J. (1997). A heuristic technique for multi-agent planning.
Annals of Mathematics and Articial Intelligence, 20 (14), 1367.
Erdelyi, G., Fernau, H., Goldsmith, J., Mattei, N., Raible, D., & Rothe, J. (2009a). The
complexity of probabilistic lobbying. In Proceedings of the 1st International Conference on Algorithmic Decision Theory, pp. 8697. Springer-Verlag Lecture Notes in
Computer Science #5783.
Erdelyi, G., Nowak, M., & Rothe, J. (2009b). Sincere-strategy preference-based approval
voting fully resists constructive control and broadly resists destructive control. Mathematical Logic Quarterly, 55 (4), 425443.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2009). How hard is bribery in
elections?. Journal of Articial Intelligence Research, 35, 485532.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity to protect
elections. Communications of the ACM, 53 (11), 7482.
571

fiElkind, Faliszewski & Slinko

Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2011). Multimode control attacks
on elections. Journal of Articial Intelligence Research, 40, 305351.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Llull and
Copeland voting computationally resist bribery and constructive control. Journal
of Articial Intelligence Research, 35, 275341.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2011). The shield that
never was: Societies with single-peaked preferences are more open to manipulation
and control. Information and Computation, 209 (2), 89107.
Fishburn, P. (1977). Condorcet social choice functions. SIAM Journal on Applied Mathematics, 33 (3), 469489.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone but him: The complexity
of precluding an alternative. Articial Intelligence, 171 (56), 255285.
Konczak, K., & Lang, J. (2005). Voting procedures with incomplete preferences. In Proceedins of the Multidisciplinary IJCAI-05 Worshop on Advances in Preference Handling,
pp. 124129.
Koray, S. (2008). Self-selective social choice functions verify Arrow and GibbardSatterthwaite theorems. Econometrica, 68 (4), 981995.
Koray, S., & Slinko, A. (2008). Self-selective social choice functions. Social Choice and
Welfare, 31 (1), 129149.
Lacey, M. (2010). Republican runs street people on green ticket. New York Times.
Laond, G., Laine, J., & Laslier, J. (1996). Composition consistent tournament solutions
and social choice functions. Social Choice and Welfare, 13 (1), 7593.
Laslier, J. (1996). Rank-based choice correspondencies. Economics Letters, 52 (3), 279286.
Laslier, J. (1997). Tournament Solutions and Majority Voting. Springer-Verlag.
Laslier, J. (2000). Aggregation of preferences with a variable set of alternatives. Social
Choice and Welfare, 17 (2), 269282.
Liu, H., Feng, H., Zhu, D., & Luan, J. (2009). Parameterized computational complexity
of control problems in voting systems. Theoretical Computer Science, 410 (2729),
27462753.
McGarvey, D. (1953). A theorem on the construction of voting paradoxes. Econometrica,
21 (4), 608610.
Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). The complexity of strategic
behavior in multi-winner elections. Journal of Articial Intelligence Research, 33,
149178.
Miller, N. (1977). Graph theoretical approaches to the theory of voting. American Journal
of Political Science, 21 (4), 769803.
Russel, N. (2007). Complexity of control of Borda count elections. Masters thesis, Rochester
Institute of Technology.
572

fiCloning in Elections

Schlotter, I., Elkind, E., & Faliszewski, P. (2011). Campaign management under approvaldriven voting rules. In Proceedings of the 25th AAAI Conference on Articial Intelligence, pp. 726731. AAAI Press.
Schulze, M. (2003). A new monotonic and clone-independent single-winner election method.
Voting Matters, 17, 919.
Tideman, T. (1987). Independence of clones as a criterion for voting rules. Social Choice
and Welfare, 4 (3), 185206.
Wagman, L., & Conitzer, V. (2008). Optimal false-name-proof voting rules with costly
voting. In Proceedings of the 23rd AAAI Conference on Articial Intelligence, pp.
190195. AAAI Press.
Xia, L., & Conitzer, V. (2011). Determining possible and necessary winners given partial
orders. Journal of Articial Intelligence Research, 41, 2567.
Xia, L., Lang, J., & Monnot, J. (2011). Possible winners when new alternatives join:
New results coming up!. In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems, pp. 829836. International Foundation for
Autonomous Agents and Multiagent Systems.
Zavist, T., & Tideman, T. (1989). Complete independence of clones in the ranked pairs
rule. Social Choice and Welfare, 64 (2), 167173.

573

fiJournal of Artificial Intelligence Research 42 (2011) 661-687

Submitted 05/11; published 12/11

Finding Consensus Bayesian Network Structures
Jose M. Pena

jose.m.pena@liu.se

ADIT
Department of Computer and Information Science
Linkoping University
SE-58183 Linkoping
Sweden

Abstract
Suppose that multiple experts (or learning algorithms) provide us with alternative
Bayesian network (BN) structures over a domain, and that we are interested in combining
them into a single consensus BN structure. Specifically, we are interested in that the
consensus BN structure only represents independences all the given BN structures agree
upon and that it has as few parameters associated as possible. In this paper, we prove
that there may exist several non-equivalent consensus BN structures and that finding one
of them is NP-hard. Thus, we decide to resort to heuristics to find an approximated
consensus BN structure. In this paper, we consider the heuristic proposed by Matzkevich
and Abramson, which builds upon two algorithms, called Methods A and B, for efficiently
deriving the minimal directed independence map of a BN structure relative to a given node
ordering. Methods A and B are claimed to be correct although no proof is provided (a
proof is just sketched). In this paper, we show that Methods A and B are not correct and
propose a correction of them.

1. Introduction
Bayesian networks (BNs) are a popular graphical formalism for representing probability distributions. A BN consists of structure and parameters. The structure, a directed and acyclic
graph (DAG), induces a set of independencies that the represented probability distribution
satisfies. The parameters specify the conditional probability distribution of each node given
its parents in the structure. The BN represents the probability distribution that results
from the product of these conditional probability distributions. Typically, a single expert
(or learning algorithm) is consulted to construct a BN of the domain at hand. Therefore,
there is a risk that the so-constructed BN is not as accurate as it could be if, for instance,
the expert has a bias or overlooks certain details. One way to minimize this risk consists
in obtaining multiple BNs of the domain from multiple experts and, then, combining them
into a single consensus BN. This approach has received significant attention in the literature (Matzkevich & Abramson, 1992, 1993b; Maynard-Reid II & Chajewska, 2001; Nielsen
& Parsons, 2007; Pennock & Wellman, 1999; Richardson & Domingos, 2003; del Sagrado
& Moral, 2003). The most relevant of these references is probably the work of Pennock
and Wellman (1999), because it shows that even if the experts agree on the BN structure,
no method for combining the experts BNs produces a consensus BN that respects some
reasonable assumptions and whose structure is the agreed BN structure. Unfortunately,
this problem is often overlooked. To avoid it, we propose to combine the experts BNs
c
2011
AI Access Foundation. All rights reserved.

fiPena

in two steps. First, finding the consensus BN structure and, then, finding the consensus
parameters for the consensus BN structure. This paper focuses only on the first step (we
briefly discuss the second step in Section 8). Specifically, we assume that multiple experts
provide us with alternative DAG models of a domain, and we are interested in combining
them into a single consensus DAG. Specifically, we are interested in that the consensus
DAG only represents independences all the given DAGs agree upon and as many of them
as possible. In other words, the consensus DAG is the DAG that represents the most independences among all the minimal directed independence (MDI) maps of the intersection
of the independence models induced by the given DAGs.1 To our knowledge, whether the
consensus DAG can or cannot be found efficiently is still an open problem. See the work of
Matzkevich and Abramson (1992, 1993b) for more information. In this paper, we redefine
the consensus DAG as the DAG that has the fewest parameters associated among all the
MDI maps of the intersection of the independence models induced by the given DAGs. This
definition is in line with that of finding a DAG to represent a probability distribution p.
The desired DAG is typically defined as the MDI map of p that has the fewest parameters
associated rather than as the MDI map of p that represents the most independences. See,
for instance, the work of Chickering et al. (2004). The number of parameters associated
with a DAG is a measure of the complexity of the DAG, since it is the number of parameters
required to specify all the probability distributions that can be represented by the DAG.
In this paper, we prove that there may exist several non-equivalent consensus DAGs
and that finding one of them is NP-hard. Thus, we decide to resort to heuristics to find
an approximated consensus DAG. In this paper, we consider the following heuristic due to
Matzkevich and Abramson (1992, 1993b). See also the work of Matzkevich and Abramson
(1993a) for related information. First, let  denote any ordering of the nodes in the given
DAGs, which we denote here as G1 , . . . , Gm . Then, find the MDI map Gi of each Gi relative
to . Finally, let the approximated consensus DAG be the DAG whose arcs are exactly
the union of the arcs in G1 , . . . , Gm
 . It should be mentioned that our formulation of the
heuristic differs from that by Matzkevich and Abramson (1992, 1993b) in the following two
points. First, the heuristic was introduced under the original definition of consensus DAG.
We justify later that the heuristic also makes sense under our definition of consensus DAG.
Second,  was originally required to be consistent with one of the given DAGs. We remove
this requirement. All in all, a key step in the heuristic is finding the MDI map Gi of each
Gi . Since this task is not trivial, Matzkevich and Abramson (1993b) present two algorithms,
called Methods A and B, for efficiently deriving Gi from Gi . Methods A and B are claimed
to be correct although no proof is provided (a proof is just sketched). In this paper, we
show that Methods A and B are not correct and propose a correction of them.
As said, we are not the first to study the problem of finding the consensus DAG. In addition to the works discussed above by Matzkevich and Abramson (1992, 1993b) and Pennock
and Wellman (1999), some other works devoted to this problem are those by Maynard-Reid
II and Chajewska (2001); Nielsen and Parsons (2007); Richardson and Domingos (2003);
1. It is worth mentioning that the term consensus DAG has a different meaning in computational biology
(Jackson et al., 2005). There, the consensus DAG of a given set of DAGs G1 , . . . , Gm is defined as the
DAG that contains the most of the arcs in G1 , . . . , Gm . Therefore, the difficulty lies in keeping as many
arcs as possible without creating cycles. Note that, unlike in the present work, a DAG is not interpreted
as inducing an independence model by Jackson et al.

662

fiFinding Consensus Bayesian Network Structures

del Sagrado and Moral (2003). We elaborate below on the differences between these works
and ours. Maynard-Reid II and Chajewska (2001) propose to adapt existing score-based algorithms for learning DAGs from data to the case where the learning data is replaced by the
BNs provided by some experts. Their approach suffers the problem pointed out by Pennock
and Wellman (1999), because it consists essentially in learning a consensus DAG from a
combination of the given BNs. A somehow related approach is proposed by Richardson and
Domingos (2003). Specifically, they propose a Bayesian approach to learning DAGs from
data, where the prior probability distribution over DAGs is constructed from the DAGs
provided by some experts. Since their approach requires data and does not combine the
given DAGs into a single DAG, it addresses a problem rather different from the one in this
paper. Moreover, the construction of the prior probability distribution over DAGs ignores
the fact that some given DAGs may be different but equivalent. That is, unlike in the
present work, a DAG is not interpreted as inducing an independence model. A work that
is relatively close to ours is that by del Sagrado and Moral (2003). Specifically, they show
how to construct a MDI map of the intersection and union of the independence models
induced by the DAGs provided by some experts. However, there are three main differences
between their work and ours. First, unlike us, they do not assume that the given DAGs
are defined over the same set of nodes. Second, unlike us, they assume that there exists a
node ordering that is consistent with all the given DAGs. Third, their goal is to find a MDI
map whereas ours is to find the MDI map that has the fewest parameters associated among
all the MDI maps, i.e. the consensus DAG. Finally, Nielsen and Parsons (2007) develop a
general framework to construct the consensus DAG gradually. Their framework is general
in the sense that it is not tailored to any particular definition of consensus DAG. Instead, it
relies upon a score to be defined by the user and that each expert will use to score different
extensions to the current partial consensus DAG. The individual scores are then combined
to choose the extension to perform. Unfortunately, we do not see how this framework could
be applied to our definition of consensus DAG.
It is worth recalling that this paper deals with the combination of probability distributions expressed as BNs. Those readers interested in the combination of probability distributions expressed in non-graphical numerical forms are referred to, for instance, the work
of Genest and Zidek (1986). Note also that we are interested in the combination before any
data is observed. Those readers interested in the combination after some data has been
observed and each expert has updated her beliefs accordingly are referred to, for instance,
the work of Ng and Abramson (1994). Finally, note also that we aim at combining the given
DAGs into a DAG, the consensus DAG. Those readers interested in finding not a DAG but
graphical features (e.g. arcs or paths) all or a significant number of experts agree upon may
want to consult the works of Friedman and Koller (2003); Hartemink et al. (2002); Pena et
al. (2004), since these works deal with a similar problem.
The rest of the paper is organized as follows. We start by reviewing some preliminary
concepts in Section 2. We analyze the complexity of finding the consensus DAG in Section
3. We discuss the heuristic for finding an approximated consensus DAG in more detail in
Section 4. We introduce Methods A and B in Section 5 and show that they are not correct.
We correct them in Section 6. We analyze the complexity of the corrected Methods A and
B in Section 7 and show that they are more efficient than any other approach we can think
of to solve the same problem. We close with some discussion in Section 8.
663

fiPena

2. Preliminaries
In this section, we review some concepts used in this paper. All the DAGs, probability
distributions and independence models in this paper are defined over V, unless otherwise
stated. If A  B is in a DAG G, then we say that A and B are adjacent in G. Moreover,
we say that A is a parent of B and B a child of A in G. We denote the parents of B in G
by P aG (B). A node is called a sink node in G if it has no children in G. A route between
two nodes A and B in G is a sequence of nodes starting with A and ending with B such
that every two consecutive nodes in the sequence are adjacent in G. Note that the nodes in
a route are not necessarily distinct. The length of a route is the number of (not necessarily
distinct) arcs in the route. We treat all the nodes in G as routes of length zero. A route
between A and B is called descending from A to B if all the arcs in the route are directed
towards B. If there is a descending route from A to B, then B is called a descendant of A.
Note that A is a descendant of itself, since we allow routes of length zero. Given a subset
X  V, a node A  X is called maximal in G if A is not descendant of any node in X \ {A}
in G. Given a route  between A and B in G and a route 0 between B and C in G,   0
denotes the route between A and C in G resulting from appending 0 to .
P
Q
The number of parameters associated with a DAG G is BV [ AP aG (B) rA ](rB  1),
where rA and rB are the numbers of states of the random variables corresponding to the
node A and B. An arc A  B in G is said to be covered if P aG (A) = P aG (B) \ {A}. By
covering an arc A  B in G we mean adding to G the smallest set of arcs so that A  B
becomes covered. We say that a node C is a collider in a route in a DAG if there exist two
nodes A and B such that A  C  B is a subroute of the route. Note that A and B may
coincide. Let X, Y and Z denote three disjoint subsets of V. A route in a DAG is said to
be Z-active when (i) every collider node in the route is in Z, and (ii) every non-collider node
in the route is outside Z. When there is no route in a DAG G between a node in X and a
node in Y that is Z-active, we say that X is separated from Y given Z in G and denote it
as X  G Y|Z. We denote by X 6 G Y|Z that X  G Y|Z does not hold. This definition of
separation is equivalent to other more common definitions (Studeny, 1998, Section 5.1).
Let X, Y, Z and W denote four disjoint subsets of V. Let us abbreviate X  Y as
XY. An independence model M is a set of statements of the form X  M Y|Z, meaning
that X is independent of Y given Z. Given a subset U  V, we denote by [M ]U all the
statements in M such that X, Y, Z  U. Given two independence models M and N , we
denote by M  N that if X  M Y|Z then X  N Y|Z. We say that M is a graphoid if
it satisfies the following properties: symmetry X  M Y|Z  Y  M X|Z, decomposition
X  M YW|Z  X  M Y|Z, weak union X  M YW|Z  X  M Y|ZW, contraction
X  M Y|ZW  X  M W|Z  X  M YW|Z, and intersection X  M Y|ZW  X  M
W|ZY  X  M YW|Z. The independence model induced by a probability distribution p,
denoted as I(p), is the set of probabilistic independences in p. The independence model
induced by a DAG G, denoted as I(G), is the set of separation statements X  G Y|Z. It is
known that I(G) is a graphoid (Studeny & Bouckaert, 1998, Lemma 3.1). Moreover, I(G)
satisfies the composition property X  G Y|Z  X  G W|Z  X  G YW|Z (Chickering &
Meek, 2002, Proposition 1). Two DAGs G and H are called equivalent if I(G) = I(H).
A DAG G is a directed independence map of an independence model M if I(G)  M .
Moreover, G is a minimal directed independence (MDI) map of M if removing any arc
664

fiFinding Consensus Bayesian Network Structures

from G makes it cease to be a directed independence map of M . We say that G and an
ordering of its nodes are consistent when, for every arc A  B in G, A precedes B in
the node ordering. We say that a DAG G is a MDI map of an independence model M
relative to a node ordering  if G is a MDI map of M and G is consistent with . If M
is a graphoid, then G is unique (Pearl, 1988, Thms. 4 and 9). Specifically, for each node
A, P aG (A) is the smallest subset X of the predecessors of A in , P re (A), such that
A  M P re (A) \ X|X.

3. Finding a Consensus DAG is NP-Hard
Recall that we have defined the consensus DAG of a given set of DAGs G1 , . . . , Gm as the
i
DAG that has the fewest parameters associated among all the MDI maps of m
i=1 I(G ). A
sensible way to start the quest for the consensus DAG is by investigating whether there can
exist several non-equivalent consensus DAGs. The following theorem answers this question.
Theorem 1. There exists a set of DAGs that has two non-equivalent consensus DAGs.
Proof. Consider the following two DAGs over four random variables with the same number
of states each:
I

K



J

I





L

K



J

L

Any of the following two non-equivalent DAGs is the consensus DAG of the two DAGs
above:
I

K


&


I

K

J

L


%


J

L

A natural follow-up question to investigate is whether a consensus DAG can be found
efficiently. Unfortunately, finding a consensus DAG is NP-hard, as we prove below. Specifically, we prove that the following decision problem is NP-hard:
CONSENSUS
 INSTANCE: A set of DAGs G1 , . . . , Gm over V, and a positive integer d.
i
 QUESTION: Does there exist a DAG G over V such that I(G)  m
i=1 I(G ) and the
number of parameters associated with G is not greater than d ?

Proving that CONSENSUS is NP-hard implies that finding the consensus DAG is also
NP-hard, because if there existed an efficient algorithm for finding the consensus DAG, then
we could use it to solve CONSENSUS efficiently. Our proof makes use of the following two
665

fiPena

decision problems:
FEEDBACK ARC SET
 INSTANCE: A directed graph G = (V, A) and a positive integer k.
 QUESTION: Does there exist a subset B  A such that |B|  k and B has at least
one arc from every directed cycle in G ?
LEARN
 INSTANCE: A probability distribution p over V, and a positive integer d.
 QUESTION: Does there exist a DAG G over V such that I(G)  I(p) and the number
of parameters associated with G is not greater than d ?
FEEDBACK ARC SET is NP-complete (Garey & Johnson, 1979). FEEDBACK ARC
SET remains NP-complete for directed graphs in which the total degree of each vertex is at
most three (Gavril, 1977). This degree-bounded FEEDBACK ARC SET problem is used
by Chickering et al. (2004) to prove that LEARN is NP-hard. In their proof, Chickering
et al. (2004) use the following polynomial reduction of any instance of the degree-bounded
FEEDBACK ARC SET into an instance of LEARN:
 Let the instance of the degree-bounded FEEDBACK ARC SET consist of the directed
graph F = (VF , AF ) and the positive integer k.
 Let L denote a DAG whose nodes and arcs are determined from F as follows. For
every arc ViF  VjF in AF , create the following nodes and arcs in L:

ViF(9)



Aij (9)

Bij (2)

Cij (3)

Hij

(2)

.

&

Dij (9)

Eij (2)

Fij (2)



Gij



VjF(9)

(9)

The number in parenthesis besides each node is the number of states of the corresponding random variable. Let HL denote all the nodes Hij in L, and let VL denote
the rest of the nodes in L.
 Specify a (join) probability distribution p(HL , VL ) such that I(p(HL , VL )) = I(L).
 Let the instance of LEARN consist of the (marginal) probability distribution p(VL )
and the positive integer d, where d is computed from F and k as shown in the work
of Chickering et al. (2004, Equation 2).
We now describe how the instance of LEARN resulting from the reduction above can
be further reduced into an instance of CONSENSUS in polynomial time:
 Let C 1 denote the DAG over VL that has all and only the arcs in L whose both
endpoints are in VL .
666

fiFinding Consensus Bayesian Network Structures

 Let C 2 denote the DAG over VL that only has the arcs Bij  Cij  Fij for all i and
j.
 Let C 3 denote the DAG over VL that only has the arcs Cij  Fij  Eij for all i and
j.
 Let the instance of CONSENSUS consist of the DAGs C 1 , C 2 and C 3 , and the positive
integer d.
Theorem 2. CONSENSUS is NP-hard.
Proof. We start by proving that there is a polynomial reduction of any instance F of the
degree-bounded FEEDBACK ARC SET into an instance C of CONSENSUS. First, reduce
F into an instance L of LEARN as shown in the work of Chickering et al. (2004) and, then,
reduce L into C as shown above.
We now prove that there is a solution to F iff there is a solution to C. Chickering et
al. (2004, Thms. 8 and 9) prove that there is a solution to F iff there is a solution to L.
Therefore, it only remains to prove that there is a solution to L iff there is a solution to
C (note that the parameter d of L and the parameter d of C are the same). Let L and
p(HL , VL ) denote the DAG and the probability distribution constructed in the reduction
of F into L. Recall that I(p(HL , VL )) = I(L). Moreover:
 Let L1 denote the DAG over (HL , VL ) that has all and only the arcs in L whose both
endpoints are in VL .
 Let L2 denote the DAG over (HL , VL ) that only has the arcs Bij  Cij  Hij  Fij
for all i and j.
 Let L3 denote the DAG over (HL , VL ) that only has the arcs Cij  Hij  Fij  Eij
for all i and j.
Note that any separation statement that holds in L also holds in L1 , L2 and L3 . Then,
I(p(HL , VL )) = I(L)  3i=1 I(Li ) and, thus, I(p(VL ))  [3i=1 I(Li )]VL = 3i=1 [I(Li )]VL .
Let C 1 , C 2 and C 3 denote the DAGs constructed in the reduction of L into C. Note that
[I(Li )]VL = I(C i ) for all i. Then, I(p(VL ))  3i=1 I(C i ) and, thus, if there is a solution to
L then there is a solution to C. We now prove the opposite. The proof is essentially the
same as that in the work of Chickering et al. (2004, Thm. 9). Let us define the (Vi , Vj )
edge component of a DAG G over VL as the subgraph of G that has all and only the arcs
in G whose both endpoints are in {Vi , Aij , Bij , Cij , Dij , Eij , Fij , Gij , Vj }. Given a solution
C to C, we create another solution C 0 to C as follows:
 Initialize C 0 to C 1 .
 For every (Vi , Vj ) edge component of C, if there is no directed path in C from Vi to
Vj , then add to C 0 the arcs Eij  Cij  Fij .
 For every (Vi , Vj ) edge component of C, if there is a directed path in C from Vi to Vj ,
then add to C 0 the arcs Bij  Fij  Cij .
667

fiPena

Note that C 0 is acyclic because C is acyclic. Moreover, I(C 0 )  3i=1 I(C i ) because
I(C 0 )  I(C i ) for all i. In order to be able to conclude that C 0 is a solution to C, it only
remains to prove that the number of parameters associated with C 0 is not greater than
d. Specifically, we prove below that C 0 does not have more parameters associated than C,
which has less than d parameters associated because it is a solution to C.
As seen before, I(C 0 )  I(C 1 ). Likewise, I(C)  I(C 1 ) because C is a solution to C.
Thus, there exists a sequence S (resp. S 0 ) of covered arc reversals and arc additions that
transforms C 1 into C (resp. C 0 ) (Chickering, 2002, Thm. 4). Note that a covered arc
reversal does not modify the number of parameters associated with a DAG, whereas an arc
addition increases it (Chickering, 1995, Thm. 3). Thus, S and S 0 monotonically increase
the number of parameters associated with C 1 as they transform it. Recall that C 1 consists
of a series of edge components of the form

ViF(9)



Aij (9)

Bij (2)

Cij (3)

Dij (9)

Eij (2)

Fij (2)



Gij



VjF(9)

(9)

The number in parenthesis besides each node is the number of states of the corresponding
random variable. Let us study how the sequences S and S 0 modify each edge component
of C 1 . S 0 simply adds the arcs Bij  Fij  Cij or the arcs Eij  Cij  Fij . Note that
adding the first pair of arcs results in an increase of 10 parameters, whereas adding the
second pair of arcs results in an increase of 12 parameters. Unlike S 0 , S may reverse some
arc in the edge component. If that is the case, then S must cover the arc first, which implies
an increase of at least 16 parameters (covering Fij  Vj by adding Eij  Vj implies an
increase of exactly 16 parameters, whereas any other arc covering implies a larger increase).
Then, S implies a larger increase in the number of parameters than S 0 . On the other hand,
if S does not reverse any arc in the edge component, then S simply adds the arcs that are
in C but not in C 1 . Note that either Cij  Fij or Cij  Fij is in C, because otherwise
Cij  C Fij |Z for some Z  VL which contradicts the fact that C is a solution to C since
Cij 6 C 2 Fij |Z. If Cij  Fij is in C, then either Bij  Fij or Bij  Fij is in C because
otherwise Bij  C Fij |Z for some Z  VL such that Cij  Z, which contradicts the fact that
C is a solution to C since Bij 6 C 2 Fij |Z. As Bij  Fij would create a cycle in C, Bij  Fij
is in C. Therefore, S adds the arcs Bij  Fij  Cij and, by construction of C 0 , S 0 also
adds them. Thus, S implies an increase of at least as many parameters as S 0 . On the other
hand, if Cij  Fij is in C, then either Cij  Eij or Cij  Eij is in C because otherwise
Cij  C Eij |Z for some Z  VL such that Fij  Z, which contradicts the fact that C is a
solution to C since Cij 6 C 3 Eij |Z. As Cij  Eij would create a cycle in C, Cij  Eij is in
C. Therefore, S adds the arcs Eij  Cij  Fij and, by construction of C 0 , S 0 adds either
the arcs Eij  Cij  Fij or the arcs Bij  Fij  Cij . In any case, S implies an increase
of at least as many parameters as S 0 . Consequently, C 0 does not have more parameters
associated than C.
Finally, note that I(p(VL ))  I(C 0 ) by Chickering et al. (2004, Lemma 7). Thus, if
there is a solution to C then there is a solution to L.
668

fiFinding Consensus Bayesian Network Structures

It is worth noting that our proof above contains two restrictions. First, the number of
DAGs to consensuate is three. Second, the number of states of each random variable in
VL is not arbitrary but prescribed. The first restriction is easy to relax: Our proof can be
extended to consensuate more than three DAGs by simply letting C i be a DAG over VL
with no arcs for all i > 3. However, it is an open question whether CONSENSUS remains
NP-hard when the number of DAGs to consensuate is two and/or the number of states of
each random variable in VL is arbitrary.
The following theorem strengthens the previous one.
Theorem 3. CONSENSUS is NP-complete.
Proof. By Theorem 2, all that remains to prove is that CONSENSUS is in NP, i.e. that
we can verify in polynomial time if a given DAG G is a solution to a given instance of
CONSENSUS.
Let  denote any node ordering that is consistent with G. The causal list of G relative
to  is the set of separation statements A  G P re (A) \ P aG (A)|P aG (A) for all node A.
It is known that I(G) coincides with the closure with respect to the graphoid properties of
i
the causal list of G relative to  (Pearl, 1988, Corollary 7). Therefore, I(G)  m
i=1 I(G ) iff
i
A  Gi P re (A) \ P aG (A)|P aG (A) for all 1  i  m, because m
i=1 I(G ) is a graphoid (del
Sagrado & Moral, 2003, Corollary 1). Let n, a and ai denote, respectively, the number of
nodes in G, the number of arcs in G, and the number of arcs in Gi . Let b = max1im ai .
Checking a separation statement in Gi takes O(ai ) time (Geiger et al., 1990, p. 530). Then,
i
checking whether I(G)  m
i=1 I(G ) takes O(mnb) time. Finally, note that computing the
number of parameters associated with G takes O(a).

4. Finding an Approximated Consensus DAG
Since finding a consensus DAG of some given DAGs is NP-hard, we decide to resort to
heuristics to find an approximated consensus DAG. This does not mean that we discard
the existence of fast super-polynomial algorithms. It simply means that we do not pursue
that possibility in this paper. Specifically, in this paper we consider the following heuristic
due to Matzkevich and Abramson (1992, 1993b). See also the work of Matzkevich and
Abramson (1993a) for related information. First, let  denote any ordering of the nodes
in the given DAGs, which we denote here as G1 , . . . , Gm . Then, find the MDI map Gi of
each Gi relative to . Finally, let the approximated consensus DAG be the DAG whose
arcs are exactly the union of the arcs in G1 , . . . , Gm
 . The following theorem justifies taking
the union of the arcs. Specifically, it proves that the DAG returned by the heuristic is the
consensus DAG if this was required to be consistent with .
Theorem 4. The DAG H returned by the heuristic above is the DAG that has the fewest
i
parameters associated among all the MDI maps of m
i=1 I(G ) relative to .
i
Proof. We start by proving that H is a MDI map of m
i=1 I(G ). First, we show that
i
m
i
i
I(H)  m
i=1 I(G ). It suffices to note that I(H)  i=1 I(G ) because each G is a subm
i
m
i
i
i
graph of H, and that i=1 I(G )  i=1 I(G ) because I(G )  I(G ) for all i. Now,

669

fiPena

assume to the contrary that the DAG H 0 resulting from removing an arc A  B from H
i
i
satisfies that I(H 0 )  m
i=1 I(G ). By construction of H, A  B is in G for some i, say
i = j. Note that B  H 0 P re (B) \ P aH 0 (B)|P aH 0 (B), which implies B  Gj P re (B) \
m
m
((m
i=1 P aGi (B)) \ {A})|(i=1 P aGi (B)) \ {A} because P aH 0 (B) = (i=1 P aGi (B)) \ {A}
i
and I(H 0 )  m
i=1 I(G ). Note also that B  Gj P re (B) \ P aGj (B)|P aGj (B), which implies B  Gj P re (B) \ P aGj (B)|P aGj (B) because I(Gj )  I(Gj ). Therefore, B  Gj


P re (B) \ (P aGj (B) \ {A})|P aGj (B) \ {A} by intersection. However, this contradicts the


i
fact that Gj is the MDI map of Gj relative to . Then, H is a MDI map of m
i=1 I(G )
relative to .
i
Finally, note that m
i=1 I(G ) is a graphoid (del Sagrado & Moral, 2003, Corollary 1).
i
Consequently, H is the only MDI map of m
i=1 I(G ) relative to .

A key step in the heuristic above is, of course, choosing a good node ordering . Unfortunately, the fact that CONSENSUS is NP-hard implies that it is also NP-hard to find the
best node ordering , i.e. the node ordering that makes the heuristic to return the MDI
i
map of m
i=1 I(G ) that has the fewest parameters associated. To see it, note that if there
existed an efficient algorithm for finding the best node ordering, then Theorem 4 would
imply that we could solve CONSENSUS efficiently by running the heuristic with the best
node ordering.
In the last sentence, we have implicitly assumed that the heuristic is efficient, which
implies that we have implicitly assumed that we can efficiently find the MDI map Gi of
each Gi . The rest of this paper shows that this assumption is correct.

5. Methods A and B are not Correct
Matzkevich and Abramson (1993b) do not only propose the heuristic discussed in the previous section, but they also present two algorithms, called Methods A and B, for efficiently
deriving the MDI map G of a DAG G relative to a node ordering . The algorithms work
iteratively by covering and reversing an arc in G until the resulting DAG is consistent with
. It is obvious that such a way of working produces a directed independence map of G.
However, in order to arrive at G , the arc to cover and reverse in each iteration must be
carefully chosen. The pseudocode of Methods A and B can be seen in Figure 1. Method
A starts by calling Construct  to derive a node ordering  that is consistent with G and
as close to  as possible (line 6). By  being as close to  as possible, we mean that the
number of arcs Methods A and B will later cover and reverse is kept at a minimum, because
Methods A and B will use  to choose the arc to cover and reverse in each iteration. In
particular, Method A finds the leftmost node in  that should be interchanged with its left
neighbor (line 2) and it repeatedly interchanges this node with its left neighbor (lines 3-4
and 6-7). Each of these interchanges is preceded by covering and reversing the corresponding arc in G (line 5). Method B is essentially identical to Method A. The only differences
between them are that the word right is replaced by the word left and vice versa in
lines 2-4, and that the arcs point in opposite directions in line 5. Note that Methods A and
B do not reverse an arc more than once.
670

fiFinding Consensus Bayesian Network Structures

Construct (G, )
/* Given a DAG G and a node ordering , the algorithm returns a node ordering  that
is consistent with G and as close to  as possible */
1
2
3
/* 3
4
5
6
7
8
9
10
11

=
G0 = G
Let A denote a sink node in G0
Let A denote the rightmost node in  that is a sink node in G0 */
Add A as the leftmost node in 
Let B denote the right neighbor of A in 
If B 6=  and A 
/ P aG (B) and A is to the right of B in  then
Interchange A and B in 
Go to line 5
Remove A and all its incoming arcs from G0
If G0 6=  then go to line 3
Return 
Method A(G, )
/* Given a DAG G and a node ordering , the algorithm returns G */

1
2
3
4
5
6
7
8
9

=Construct (G, )
Let Y denote the leftmost node in  whose left neighbor in  is to its right in 
Let Z denote the left neighbor of Y in 
If Z is to the right of Y in  then
If Z  Y is in G then cover and reverse Z  Y in G
Interchange Y and Z in 
Go to line 3
If  6=  then go to line 2
Return G
Method B(G, )
/* Given a DAG G and a node ordering , the algorithm returns G */

1
2
3
4
5
6
7
8
9

=Construct (G, )
Let Y denote the leftmost node in  whose right neighbor in  is to its left in 
Let Z denote the right neighbor of Y in 
If Z is to the left of Y in  then
If Y  Z is in G then cover and reverse Y  Z in G
Interchange Y and Z in 
Go to line 3
If  6=  then go to line 2
Return G

Figure 1: Construct , and Methods A and B. Our correction of Construct  consists in (i)
replacing line 3 with the line in comments under it, and (ii) removing lines 5-8.
671

fiPena

Figure 2: A counterexample to the correctness of Methods A and B.
Methods A and B are claimed to be correct in the work of Matzkevich and Abramson
(1993b, Thm. 4 and Corollary 2) although no proof is provided (a proof is just sketched).
The following counterexample shows that Methods A and B are actually not correct. Let G
be the DAG in the left-hand side of Figure 2. Let  = (M, I, K, J, L). Then, we can make
use of the characterization introduced in Section 2 to see that G is the DAG in the center
of Figure 2. However, Methods A and B return the DAG in the right-hand side of Figure
2. To see it, we follow the execution of Methods A and B step by step. First, Methods A
and B construct  by calling Construct , which runs as follows:
1. Initially,  =  and G0 = G.
2. Select the sink node M in G0 . Then,  = (M ). Remove M and its incoming arcs from
G0 .
3. Select the sink node L in G0 . Then,  = (L, M ). No interchange in  is performed
because L  P aG (M ). Remove L and its incoming arcs from G0 .
4. Select the sink node K in G0 . Then,  = (K, L, M ). No interchange in  is performed
because K is to the left of L in . Remove K and its incoming arcs from G0 .
5. Select the sink node J in G0 . Then,  = (J, K, L, M ). No interchange in  is performed
because J  P aG (K).
6. Select the sink node I in G0 . Then,  = (I, J, K, L, M ). No interchange in  is
performed because I is to the left of J in .
When Construct  ends, Methods A and B continue as follows:
672

fiFinding Consensus Bayesian Network Structures

7. Initially,  = (I, J, K, L, M ).
8. Add the arc I  J and reverse the arc J  K in G. Interchange J and K in .
Then,  = (I, K, J, L, M ).
9. Add the arc J  M and reverse the arc L  M in G. Interchange L and M in .
Then,  = (I, K, J, M, L).
10. Add the arcs I  M and K  M , and reverse the arc J  M in G. Interchange J
and M in . Then,  = (I, K, M, J, L).
11. Reverse the arc K  M in G. Interchange K and M in . Then,  = (I, M, K, J, L).
12. Reverse the arc I  M in G. Interchange I and M in . Then,  = (M, I, K, J, L) =
.
As a matter of fact, one can see as early as in step 8 above that Methods A and B will
fail: One can see that I and M are not separated in the DAG resulting from step 8, which
implies that I and M will not be separated in the DAG returned by Methods A and B,
because covering and reversing arcs never introduces new separation statements. However,
I and M are separated in G .
Note that we constructed  by selecting first M , then L, then K, then J, and finally I.
However, we could have selected first K, then I, then M , then L, and finally J, which would
have resulted in  = (J, L, M, I, K). With this , Methods A and B return G . Therefore, it
makes a difference which sink node is selected in line 3 of Construct . However, Construct
 overlooks this detail. We propose correcting Construct  by (i) replacing line 3 by Let A
denote the rightmost node in  that is a sink node in G0 , and (ii) removing lines 5-8 since
they will never be executed. Hereinafter, we assume that any call to Construct  is a call
to the corrected version thereof. The rest of this paper is devoted to prove that Methods A
and B now do return G .

6. The Corrected Methods A and B are Correct
Before proving that Methods A and B are correct, we introduce some auxiliary lemmas.
Their proof can be found in Appendix A. Let us call percolating Y right-to-left in  to
iterating through lines 3-7 in Method A while possible. Let us modify Method A by replacing
line 2 by Let Y denote the leftmost node in  that has not been considered before and
by adding the check Z 6=  to line 4. The pseudocode of the resulting algorithm, which we
call Method A2, can be seen in Figure 3. Method A2 percolates right-to-left in  one by
one all the nodes in the order in which they appear in .
Lemma 1. Method A(G, ) and Method A2(G, ) return the same DAG.
Lemma 2. Method A2(G, ) and Method B(G, ) return the same DAG.
Let us call percolating Y left-to-right in  to iterating through lines 3-7 in Method B
while possible. Let us modify Method B by replacing line 2 by Let Y denote the rightmost
node in  that has not been considered before and by adding the check Z 6=  to line 4.
The pseudocode of the resulting algorithm, which we call Method B2, can be seen in Figure
673

fiPena

Method A2(G, )
/* Given a DAG G and a node ordering , the algorithm returns G */
1
2
3
4
5
6
7
8
9

=Construct (G, )
Let Y denote the leftmost node in  that has not been considered before
Let Z denote the left neighbor of Y in 
If Z 6=  and Z is to the right of Y in  then
If Z  Y is in G then cover and reverse Z  Y in G
Interchange Y and Z in 
Go to line 3
If  6=  then go to line 2
Return G
Method B2(G, )
/* Given a DAG G and a node ordering , the algorithm returns G */

1
2
3
4
5
6
7
8
9

=Construct (G, )
Let Y denote the rightmost node in  that has not been considered before
Let Z denote the right neighbor of Y in 
If Z 6=  and Z is to the left of Y in  then
If Y  Z is in G then cover and reverse Y  Z in G
Interchange Y and Z in 
Go to line 3
If  6=  then go to line 2
Return G

Figure 3: Methods A2 and B2.
3. Method B2 percolates left-to-right in  one by one all the nodes in the reverse order in
which they appear in .
Lemma 3. Method B(G, ) and Method B2(G, ) return the same DAG.
We are now ready to prove the main result of this paper.
Theorem 5. Let G denote the MDI map of a DAG G relative to a node ordering . Then,
Method A(G, ) and Method B(G, ) return G .
Proof. By Lemmas 1-3, it suffices to prove that Method B2(G, ) returns G . It is evident
that Method B2 transforms  into  and, thus, that it halts at some point. Therefore,
Method B2 performs a finite sequence of n modifications (arc additions and covered arc
reversals) to G. Let Gi denote the DAG resulting from the first i modifications to G, and
let G0 = G. Specifically, Method B2 constructs Gi+1 from Gi by either (i) reversing the
covered arc Y  Z, or (ii) adding the arc X  Z for some X  P aGi (Y ) \ P aGi (Z), or
(iii) adding the arc X  Y for some X  P aGi (Z) \ P aGi (Y ). Note that I(Gi+1 )  I(Gi )
for all 0  i < n and, thus, that I(Gn )  I(G0 ).
674

fiFinding Consensus Bayesian Network Structures

We start by proving that Gi is a DAG that is consistent with  for all 0  i  n. Since
this is true for G0 due to line 1, it suffices to prove that if Gi is a DAG that is consistent
with  then so is Gi+1 for all 0  i < n. We consider the following four cases.
Case 1 Method B2 constructs Gi+1 from Gi by reversing the covered arc Y  Z. Then,
Gi+1 is a DAG because reversing a covered arc does not create any cycle (Chickering,
1995, Lemma 1). Moreover, note that Y and Z are interchanged in  immediately
after the covered arc reversal. Thus, Gi+1 is consistent with .
Case 2 Method B2 constructs Gi+1 from Gi by adding the arc X  Z for some X 
P aGi (Y ) \ P aGi (Z). Note that X is to the left of Y and Y to the left of Z in ,
because Gi is consistent with . Then, X is to the left of Z in  and, thus, Gi+1 is a
DAG that is consistent with .
Case 3 Method B2 constructs Gi+1 from Gi by adding the arc X  Y for some X 
P aGi (Z) \ P aGi (Y ). Note that X is to the left of Z in  because Gi is consistent with
, and Y is the left neighbor of Z in  (recall line 3). Then, X is to the left of Y in
 and, thus, Gi+1 is a DAG that is consistent with .
Case 4 Note that  may get modified before Method B2 constructs Gi+1 from Gi . Specifically, this happens when Method B2 executes lines 5-6 but there is no arc between
Y and Z in Gi . However, the fact that Gi is consistent with  before Y and Z are
interchanged in  and the fact that Y and Z are neighbors in  (recall line 3) imply
that Gi is consistent with  after Y and Z have been interchanged.
Since Method B2 transforms  into , it follows from the result proven above that Gn
is a DAG that is consistent with . In order to prove the theorem, i.e. that Gn = G , all
that remains to prove is that I(G )  I(Gn ). To see it, note that Gn = G follows from
I(G )  I(Gn ), I(Gn )  I(G0 ), the fact that Gn is a DAG that is consistent with , and
the fact that G is the unique MDI map of G0 relative to . Recall that G is guaranteed
to be unique because I(G0 ) is a graphoid.
The rest of the proof is devoted to prove that I(G )  I(Gn ). Specifically, we prove
that if I(G )  I(Gi ) then I(G )  I(Gi+1 ) for all 0  i < n. Note that this implies
that I(G )  I(Gn ) because I(G )  I(G0 ) by definition of MDI map. First, we prove it
when Method B2 constructs Gi+1 from Gi by reversing the covered arc Y  Z. That the
arc reversed is covered implies that I(Gi+1 ) = I(Gi ) (Chickering, 1995, Lemma 1). Thus,
I(G )  I(Gi+1 ) because I(G )  I(Gi ).
Now, we prove that if I(G )  I(Gi ) then I(G )  I(Gi+1 ) for all 0  i < n when
Method B2 constructs Gi+1 from Gi by adding an arc. Specifically, we prove that if there
is an S-active route (S  V) AB
i+1 between two nodes A and B in Gi+1 , then there is an
S-active route between A and B in G . We prove this result by induction on the number of
occurrences of the added arc in AB
i+1 . We assume without loss of generality that the added
AB
arc occurs in i+1 as few or fewer times than in any other S-active route between A and B
2
in Gi+1 . We call this the minimality property of AB
i+1 . If the number of occurrences of the
2. It is not difficult to show that the number of occurrences of the added arc in AB
i+1 is then at most two
(see Case 2.1 for some intuition). However, the proof of the theorem is simpler if we ignore this fact.

675

fiPena

Figure 4: Different cases in the proof of Theorem 5. Only the relevant subgraphs of Gi+1 and
G are depicted. An undirected edge between two nodes denotes that the nodes
are adjacent. A curved edge between two nodes denotes an S-active route between
the two nodes. If the curved edge is directed, then the route is descending. A
grey node denotes a node that is in S.

AB
added arc in AB
i+1 is zero, then i+1 is an S-active route between A and B in Gi too and,
thus, there is an S-active route between A and B in G since I(G )  I(Gi ). Assume as
induction hypothesis that the result holds for up to k occurrences of the added arc in AB
i+1 .
We now prove it for k + 1 occurrences. We consider the following two cases. Each case is
illustrated in Figure 4.

Case 1 Method B2 constructs Gi+1 from Gi by adding the arc X  Z for some X 
3
AB
AX
ZB
P aGi (Y )\P aGi (Z). Note that X  Z occurs in AB
i+1 . Let i+1 = i+1 X  Zi+1 .
AX
AB
Note that X 
/ S and i+1 is S-active in Gi+1 because, otherwise, i+1 would not be
3. Note that maybe A = X and/or B = Z.

676

fiFinding Consensus Bayesian Network Structures

S-active in Gi+1 . Then, there is an S-active route AX
between A and X in G by the

ZB
induction hypothesis. Moreover, Y  S because, otherwise, AX
i+1  X  Y  Z  i+1
would be an S-active route between A and B in Gi+1 that would violate the minimality
property of AB
i+1 . Note that Y  Z is in G because (i) Y and Z are adjacent in
G since I(G )  I(Gi ), and (ii) Z is to the left of Y in  (recall line 4). Note
also that X  Y is in G . To see it, note that X and Y are adjacent in G since
I(G )  I(Gi ). Recall that Method B2 percolates left-to-right in  one by one all
the nodes in the reverse order in which they appear in . Method B2 is currently
percolating Y and, thus, the nodes to the right of Y in  are to the right of Y in 
too. If X  Y were in G then X would be to the right of Y in  and, thus, X would
be to the right of Y in . However, this would contradict the fact that X is to the
left of Y in , which follows from the fact that Gi is consistent with . Thus, X  Y
is in G . We now consider two cases.
Case 1.1 Assume that Z 
/ S. Then, ZB
i+1 is S-active in Gi+1 because, otherwise,
AB
i+1 would not be S-active in Gi+1 . Then, there is an S-active route ZB
 between
Z and B in G by the induction hypothesis. Then, AX

X

Y

Z  ZB

 is
an S-active route between A and B in G .
WB 4
Case 1.2 Assume that Z  S. Then, ZB
/ S and
i+1 = Z  W  i+1 . Note that W 
W
B
AB
i+1 is S-active in Gi+1 because, otherwise, i+1 would not be S-active in Gi+1 .
B between W and B in G by the induction
Then, there is an S-active route W


hypothesis. Note that W and Z are adjacent in G since I(G )  I(Gi ). This
and the fact proven above that Y  Z is in G imply that Y and W are adjacent
in G because, otherwise, Y 6 Gi W |U but Y  G W |U for some U  V such
that Z  U, which would contradict that I(G )  I(Gi ). In fact, Y  W is in
G . To see it, recall that the nodes to the right of Y in  are to the right of Y in
 too. If Y  W were in G then W would be to the right of Y in  and, thus,
W would be to the right of Y in  too. However, this would contradict the fact
that W is to the left of Y in , which follows from the fact that W is to the left of
Z in  because Gi is consistent with , and the fact that Y is the left neighbor of
WB
Z in  (recall line 3). Thus, Y  W is in G . Then, AX
  X  Y  W  
is an S-active route between A and B in G .

Case 2 Method B2 constructs Gi+1 from Gi by adding the arc X  Y for some X 
5
AB
AX
YB
P aGi (Z)\P aGi (Y ). Note that X  Y occurs in AB
i+1 . Let i+1 = i+1 X  Y i+1 .
AX
AB
Note that X 
/ S and i+1 is S-active in Gi+1 because, otherwise, i+1 would not be
S-active in Gi+1 . Then, there is an S-active route AX
between A and X in G by the

induction hypothesis. Note that Y  Z is in G because (i) Y and Z are adjacent in
G since I(G )  I(Gi ), and (ii) Z is to the left of Y in  (recall line 4). Note also
that X and Z are adjacent in G since I(G )  I(Gi ). This and the fact that Y  Z
is in G imply that X and Y are adjacent in G because, otherwise, X 6 Gi Y |U
but X  G Y |U for some U  V such that Z  U, which would contradict that
WB
4. Note that maybe W = B. Note also that W 6= X because, otherwise, AX
i+1  X  Y  X  i+1 would
be an S-active route between A and B in Gi+1 that would violate the minimality property of AB
i+1 .
5. Note that maybe A = X and/or B = Y .

677

fiPena

I(G )  I(Gi ). In fact, X  Y is in G . To see it, recall that Method B2 percolates
left-to-right in  one by one all the nodes in the reverse order in which they appear
in . Method B2 is currently percolating Y and, thus, the nodes to the right of Y
in  are to the right of Y in  too. If X  Y were in G then X would be to the
right of Y in  and, thus, X would be to the right of Y in  too. However, this would
contradict the fact that X is to the left of Y in , which follows from the fact that
X is to the left of Z in  because Gi is consistent with , and the fact that Y is the
left neighbor of Z in  (recall line 3). Thus, X  Y is in G . We now consider three
cases.
B = Y  X  XB . Note that XB is S-active
Case 2.1 Assume that Y  S and Yi+1
i+1
i+1
AB
in Gi+1 because, otherwise, i+1 would not be S-active in Gi+1 . Then, there
is an S-active route XB
between X and B in G by the induction hypothesis.

AX
Then,   X  Y  X  XB
is an S-active route between A and B in G .

B = Y  W  W B .6 Note that W 
Case 2.2 Assume that Y  S and Yi+1
/ S and
i+1
W
B
i+1 is S-active in Gi+1 because, otherwise, AB
would
not
be
S-active
in Gi+1 .
i+1
B between W and B in G by the induction
Then, there is an S-active route W


hypothesis. Note also that Y  W is in G . To see it, note that Y and W
are adjacent in G since I(G )  I(Gi ). Recall that the nodes to the right of
Y in  are to the right of Y in  too. If Y  W were in G then W would
be to the right of Y in  and, thus, W would be to the right of Y in  too.
However, this would contradict the fact that W is to the left of Y in , which
follows from the fact that Gi is consistent with . Thus, Y  W is in G . Then,
W B is an S-active route between A and B in G .
AX

  X  Y  W  

Case 2.3 Assume that Y 
/ S. The proof of this case is based on that of step 8 in the
work of Chickering (2002, Lemma 30). Let D denote the node that is maximal
in G from the set of descendants of Y in Gi . Note that D is guaranteed to be
unique by Chickering (2002, Lemma 29), because I(G )  I(Gi ). Note also that
D 6= Y , because Z is a descendant of Y in Gi and, as shown above, Y  Z is in
G . We now show that D is a descendant of Z in Gi . We consider three cases.
Case 2.3.1 Assume that D = Z. Then, D is a descendant of Z in Gi .
Case 2.3.2 Assume that D 6= Z and D was a descendant of Z in G0 . Recall
that Method B2 percolates left-to-right in  one by one all the nodes in the
reverse order in which they appear in . Method B2 is currently percolating
Y and, thus, it has not yet percolated Z because Z is to the left of Y in 
(recall line 4). Therefore, none of the descendants of Z in G0 (among which
is D) is to the left of Z in . This and the fact that  is consistent with Gi
imply that Z is a node that is maximal in Gi from the set of descendants of
Z in G0 . Actually, Z is the only such node by Chickering (2002, Lemma 29),
because I(Gi )  I(G0 ). Then, the descendants of Z in G0 are descendants
of Z in Gi too. Thus, D is a descendant of Z in Gi .
6. Note that maybe W = B. Note also that W 6= X, because the case where W = X is covered by Case
2.1.

678

fiFinding Consensus Bayesian Network Structures

Case 2.3.3 Assume that D 6= Z and D was not a descendant of Z in G0 . As
shown in Case 2.3.2, the descendants of Z in G0 are descendants of Z in
Gi too. Therefore, none of the descendants of Z in G0 was to the left of D
in  because, otherwise, some descendant of Z and thus of Y in Gi would
be to the left of D in , which would contradict the definition of D. This
and the fact that D was not a descendant of Z in G0 imply that D was still
in G0 when Z became a sink node of G0 in Construct  (recall Figure 1).
Therefore, Construct  added D to  after having added Z (recall lines 3-4),
because D is to the left of Z in  by definition of D.7 For the same reason,
Method B2 has not interchanged D and Z in  (recall line 4). Thus, D is
currently still to the left of Z in , which implies that D is to the left of Y
in , because Y is the left neighbor of Z in  (recall line 3). However, this
contradicts the fact that Gi is consistent with , because D is a descendant
of Y in Gi . Thus, this case never occurs.
B is
We continue with the proof of Case 2.3. Note that Y 
/ S implies that Yi+1
S-active in Gi+1 because, otherwise, AB
i+1 would not be S-active in Gi+1 . Note
also that no descendant of Z in Gi is in S because, otherwise, there would be
XY  Y B
an S-active route XY
between X and Y in Gi and, thus, AX
i
i+1  i
i+1
would be an S-active route between A and B in Gi+1 that would violate the
minimality property of AB
/ S because, as shown above,
i+1 . This implies that D 
D is a descendant of Z in Gi . It also implies that there is an S-active descending
ZD is an S-active route
route ZD
from Z to D in Gi . Then, AX
i
i+1  X  Z  i
ZD is an S-active route
between A and D in Gi+1 . Likewise, BY
i+1  Y  Z  i
BY
between B and D in Gi+1 , where i+1 denotes the route resulting from reversing
B . Therefore, there are S-active routes AD and BD between A and D and
Yi+1


between B and D in G by the induction hypothesis.
Consider the subroute of AB
i+1 that starts with the arc X  Y and continues in
the direction of this arc until it reaches a node E such that E = B or E  S.
Note that E is a descendant of Y in Gi and, thus, E is a descendant of D in G
by definition of D. Let DE
denote the descending route from D to E in G .

Assume without loss of generality that G has no descending route from D to
B or to a node in S that is shorter than DE
 . This implies that if E = B then
DE
DE is an
 is S-active in G because, as shown above, D 
/ S. Thus, AD
  
S-active route between A and B in G . On the other hand, if E  S then E 6= D
DE  ED  DB is an S-active route between
because D 
/ S. Thus, AD
  


ED
A and B in G , where  and DB
denote
the
routes resulting from reversing

BD .
DE
and




Finally, we show how the correctness of Method B2 leads to an alternative proof of the
so-called Meeks conjecture (1997). Given two DAGs G and H such that I(H)  I(G),
Meeks conjecture states that we can transform G into H by a sequence of arc additions
and covered arc reversals such that after each operation in the sequence G is a DAG and
7. Note that this statement is true thanks to our correction of Construct .

679

fiPena

Method G2H(G, H)
/* Given two DAGs G and H such that I(H)  I(G), the algorithm transforms
G into H by a sequence of arc additions and covered arc reversals such that
after each operation in the sequence G is a DAG and I(H)  I(G) */
1
2
3

Let  denote a node ordering that is consistent with H
G=Method B2(G, )
Add to G the arcs that are in H but not in G

Figure 5: Method G2H.
I(H)  I(G). The importance of Meeks conjecture lies in that it allows to develop efficient
and asymptotically correct algorithms for learning BNs from data under mild assumptions
(Chickering, 2002; Chickering & Meek, 2002; Meek, 1997; Nielsen et al., 2003). Meeks
conjecture was proven to be true in the work of Chickering (2002, Thm. 4) by developing
an algorithm that constructs a valid sequence of arc additions and covered arc reversals.
We propose an alternative algorithm to construct such a sequence. The pseudocode of our
algorithm, called Method G2H, can be seen in Figure 5. The following corollary proves that
Method G2H is correct.
Corollary 1. Given two DAGs G and H such that I(H)  I(G), Method G2H(G, H)
transforms G into H by a sequence of arc additions and covered arc reversals such that
after each operation in the sequence G is a DAG and I(H)  I(G).
Proof. Note from Method G2Hs line 1 that  denotes a node ordering that is consistent
with H. Let G denote the MDI map of G relative to . Recall that G is guaranteed to be
unique because I(G) is a graphoid. Note that I(H)  I(G) implies that G is a subgraph
of H. To see it, note that I(H)  I(G) implies that we can obtain a MDI map of G relative
to  by just removing arcs from H. However, G is the only MDI map of G relative to .
Then, it follows from the proof of Theorem 5 that Method G2Hs line 2 transforms
G into G by a sequence of arc additions and covered arc reversals, and that after each
operation in the sequence G is a DAG and I(G )  I(G). Thus, after each operation in
the sequence I(H)  I(G) because I(H)  I(G ) since, as shown above, G is a subgraph
of H. Moreover, Method G2Hs line 3 transforms G from G to H by a sequence of arc
additions. Of course, after each arc addition G is a DAG and I(H)  I(G) because G is
a subgraph of H.

7. The Corrected Methods A and B are Efficient
In this section, we show that Methods A and B are more efficient than any other solution
to the same problem we can think of. Let n and a denote, respectively, the number of
nodes and arcs in G. Moreover, let us assume hereinafter that a DAG is implemented as an
680

fiFinding Consensus Bayesian Network Structures

adjacency matrix, whereas a node ordering is implemented as an array with an entry per
node indicating the position of the node in the ordering. Since I(G) is a graphoid, the first
solution we can think of consists in applying the following characterization of G : For each
node A, P aG (A) is the smallest subset X  P re (A) such that A  G P re (A) \ X|X. This
solution implies evaluating for each node A all the O(2n ) subsets of P re (A). Evaluating a
subset implies checking a separation statement in G, which takes O(a) time (Geiger et al.,
1990, p. 530). Therefore, the overall runtime of this solution is O(an2n ).
Since I(G) satisfies the composition property in addition to the graphoid properties,
a more efficient solution consists in running the incremental association Markov boundary
(IAMB) algorithm (Pena et al., 2007, Thm. 8) for each node A to find P aG (A). The IAMB
algorithm first sets P aG (A) =  and, then, proceeds with the following two steps. The
first step consists in iterating through the following line until P aG (A) does not change:
Take any node B  P re (A) \ P aG (A) such that A 6 G B|P aG (A) and add it to P aG (A).
The second step consists in iterating through the following line until P aG (A) does not
change: Take any node B  P aG (A) that has not been considered before and such that
A  G B|P aG (A)\{B}, and remove it from P aG (A). The first step of the IAMB algorithm
can add O(n) nodes to P aG (A). Each addition implies evaluating O(n) candidates for
the addition, since P re (A) has O(n) nodes. Evaluating a candidate implies checking a
separation statement in G, which takes O(a) time (Geiger et al., 1990, p. 530). Then, the
first step of the IAMB algorithm runs in O(an2 ) time. Similarly, the second step of the
IAMB algorithm runs in O(an) time. Therefore, the IAMB algorithm runs in O(an2 ) time.
Since the IAMB algorithm has to be run once for each of the n nodes, the overall runtime
of this solution is O(an3 ).
We now analyze the efficiency of Methods A and B. To be more exact, we analyze
Methods A2 and B2 (recall Figure 3) rather than the original Methods A and B (recall
Figure 1), because the former are more efficient than the latter. Methods A2 and B2 run
in O(n3 ) time. First, note that Construct  runs in O(n3 ) time. The algorithm iterates n
times through lines 3-10 and, in each of these iterations, it iterates O(n) times through lines
5-8. Moreover, line 3 takes O(n2 ) time, line 6 takes O(1) time, and line 9 takes O(n) time.
Now, note that Methods A2 and B2 iterate n times through lines 2-8 and, in each of these
iterations, they iterate O(n) times through lines 3-7. Moreover, line 4 takes O(1) time,
and line 5 takes O(n) time because covering an arc implies updating the adjacency matrix
accordingly. Consequently, Methods A and B are more efficient than any other solution to
the same problem we can think of.
Finally, we analyze the complexity of Method G2H. Method G2H runs in O(n3 ) time:
 can be constructed in O(n3 ) time by calling Construct (H, ) where  is any node
ordering, running Method B2 takes O(n3 ) time, and adding to G the arcs that are in H
but not in G can be done in O(n2 ) time. Recall that Method G2H is an alternative to
the algorithm in the work of Chickering (2002). Unfortunately, no implementation details
are provided in the work of Chickering and, thus, a comparison with the runtime of the
algorithm there is not possible. However, we believe that our algorithm is more efficient.
681

fiPena

8. Discussion
In this paper, we have studied the problem of combining several given DAGs into a consensus
DAG that only represents independences all the given DAGs agree upon and that has as few
parameters associated as possible. Although our definition of consensus DAG is reasonable,
we would like to leave out the number of parameters associated and focus solely on the
independencies represented by the consensus DAG. In other words, we would like to define
the consensus DAG as the DAG that only represents independences all the given DAGs
agree upon and as many of them as possible. We are currently investigating whether both
definitions are equivalent. In this paper, we have proven that there may exist several nonequivalent consensus DAGs. In principle, any of them is equally good. If we were able
to conclude that one represents more independencies than the rest, then we would prefer
that one. In this paper, we have proven that finding a consensus DAG is NP-hard. This
made us resort to heuristics to find an approximated consensus DAG. This does not mean
that we discard the existence of fast super-polynomial algorithms for the general case, or
polynomial algorithms for constrained cases such as when the given DAGs have bounded
in-degree. This is a question that we are currently investigating. In this paper, we have
considered the heuristic originally proposed by Matzkevich and Abramson (1992, 1993b).
This heuristic takes as input a node ordering, and we have shown that finding the best
node ordering for the heuristic is NP-hard. We are currently investigating the application
of meta-heuristics in the space of node orderings to find a good node ordering for the
heuristic. Our preliminary experiments indicate that this approach is highly beneficial, and
that the best node ordering almost never coincides with any of the node orderings that are
consistent with some of the given DAGs.
As said in Section 1, we aim at combining the BNs provided by multiple experts (or
learning algorithms) into a single consensus BN that is more robust than the individual
BNs. In this paper, we have proposed to combine the experts BNs in two steps to avoid
the problems discussed by Pennock and Wellman (1999). First, finding a consensus BN
structure and, then, finding some consensus parameters for the consensus BN structure.
This paper has focused only on the first step. We are currently working on the second
step along the following lines. Let (G1 , 1 ), . . . , (Gm , m ) denote the BNs provided by the
experts. The first element in each pair denotes the BN structure whereas the second denotes
the BN parameters. Let p1 , . . . , pm denote the probability distributions represented by the
BNs provided by the experts. Then, we call p0 = f (p1 , . . . , pm ) the consensus probability
distribution, where f is any combination function, e.g. the weighted arithmetic or geometric
mean. Let G denote a consensus BN structure obtained from G1 , . . . , Gm as described
in this paper. We propose to obtain a consensus BN by parameterizing G such that
p (A|P aG (A)) = p0 (A|P aG (A)) for all A  V, where p is the probability distribution
represented by the consensus BN. The motivation is that such a parameterization minimizes
the Kullback-Leibler divergence between p and p0 (Koller & Friedman, 2009, Thm. 8.7).
Some hints about how to speed up the computation of this parameterization by performing
inference in the experts BNs can be found in the work of Pennock and Wellman (1999,
Properties 3 and 4, and Section 5). Alternatively, one could first sample p0 and, then,
parameterize G such that p (A|P aG (A)) = p0 (A|P aG (A)) for all A  V, where p0 is
the empirical probability distribution obtained from the sample. Again, the motivation is
682

fiFinding Consensus Bayesian Network Structures

that such a parameterization minimizes the Kullback-Leibler divergence between p and p0
(Koller & Friedman, 2009, Thm. 17.1) and, of course, p0  p0 if the sample is sufficiently
large. Note that we use p0 to parameterize G but not to construct G which, as discussed
in Section 1, allows us to avoid the problems discussed by Pennock and Wellman (1999).
Finally, note that the present work combines the DAGs G1 , . . . , Gm although there is
no guarantee that each Gi is a MDI map of I(pi ), i.e. Gi may have superfluous arcs.
Therefore, one may want to check if Gi contains superfluous arcs and remove them before
the combination takes place. In general, several MDI maps of I(pi ) may exist, and they
may differ in the number of parameters associated with them. It would be interesting to
study how the number of parameters associated with the MDI map of I(pi ) chosen affects
the number of parameters associated with the consensus DAG obtained by the method
proposed in this paper.

Acknowledgments
We thank the anonymous referees and the editor for their thorough review of this manuscript.
We thank Dr. Jens D. Nielsen and Dag Sonntag for proof-reading this manuscript. This
work is funded by the Center for Industrial Information Technology (CENIIT) and a socalled career contract at Linkoping University.

Appendix A. Proofs of Lemmas 1-3
Lemma 1. Method A(G, ) and Method A2(G, ) return the same DAG.

Proof. It is evident that Methods A and A2 transform  into  and, thus, that they halt
at some point. We now prove that they return the same DAG. We prove this result by
induction on the number of times that Method A executes line 6 before halting. It is
evident that the result holds if the number of executions is one, because Methods A and A2
share line 1. Assume as induction hypothesis that the result holds for up to k 1 executions.
We now prove it for k executions. Let Y and Z denote the nodes involved in the first of
the k executions. Since the induction hypothesis applies for the remaining k  1 executions,
the run of Method A can be summarized as

If Z  Y is in G then cover and reverse Z  Y in G
Interchange Y and Z in 
For i = 1 to n do
Percolate right-to-left in  the leftmost node in  that has not been percolated before

where n is the number of nodes in G. Now, assume that Y is percolated when i = j. Note
that the first j  1 percolations only involve nodes to the left of Y in . Thus, the run
above is equivalent to
683

fiPena

For i = 1 to j  1 do
Percolate right-to-left in  the leftmost node in  that has not been percolated before
If Z  Y is in G then cover and reverse Z  Y in G
Interchange Y and Z in 
Percolate Y right-to-left in 
Percolate Z right-to-left in 
For i = j + 2 to n do
Percolate right-to-left in  the leftmost node in  that has not been percolated before.
Now, let W denote the nodes to the left of Z in  before the first of the k executions of
line 6. Note that the fact that Y and Z are the nodes involved in the first execution implies
that the nodes in W are also to the left of Z in . Note also that, when Z is percolated
in the latter run above, the nodes to the left of Z in  are exactly W  {Y }. Since all the
nodes in W  {Y } are also to the left of Z in , the percolation of Z in the latter run above
does not perform any arc covering and reversal or node interchange. Thus, the latter run
above is equivalent to
For i = 1 to j  1 do
Percolate right-to-left in  the leftmost node in  that has not been percolated before
Percolate Z right-to-left in 
Percolate Y right-to-left in 
For i = j + 2 to n do
Percolate right-to-left in  the leftmost node in  that has not been percolated before
which is exactly the run of Method A2. Consequently, Methods A and A2 return the same
DAG.

Lemma 2. Method A2(G, ) and Method B(G, ) return the same DAG.
Proof. We can prove the lemma in much the same way as Lemma 1. We simply need to
replace Y by Z and vice versa in the proof of Lemma 1.

Lemma 3. Method B(G, ) and Method B2(G, ) return the same DAG.
Proof. It is evident that Methods B and B2 transform  into  and, thus, that they halt
at some point. We now prove that they return the same DAG. We prove this result by
induction on the number of times that Method B executes line 6 before halting. It is
evident that the result holds if the number of executions is one, because Methods B and B2
share line 1. Assume as induction hypothesis that the result holds for up to k 1 executions.
We now prove it for k executions. Let Y and Z denote the nodes involved in the first of
the k executions. Since the induction hypothesis applies for the remaining k  1 executions,
the run of Method B can be summarized as
684

fiFinding Consensus Bayesian Network Structures

If Y  Z is in G then cover and reverse Y  Z in G
Interchange Y and Z in 
For i = 1 to n do
Percolate left-to-right in  the rightmost node in  that has not been percolated before
where n is the number of nodes in G. Now, assume that Y is the j-th rightmost node in
. Note that, for all 1  i < j, the i-th rightmost node Wi in  is to the right of Y in 
when Wi is percolated in the run above. To see it, assume to the contrary that Wi is to
the left of Y in . This implies that Wi is also to the left of Z in , because Y and Z are
neighbors in . However, this is a contradiction because Wi would have been selected in
line 2 instead of Y for the first execution of line 6. Thus, the first j  1 percolations in the
run above only involve nodes to the right of Z in . Then, the run above is equivalent to
For i = 1 to j  1 do
Percolate left-to-right in  the rightmost node in  that has not been percolated before
If Y  Z is in G then cover and reverse Y  Z in G
Interchange Y and Z in 
For i = j to n do
Percolate left-to-right in  the rightmost node in  that has not been percolated before
which is exactly the run of Method B2.

References
Chickering, D. M. A Transformational Characterization of Equivalent Bayesian Network
Structures. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, 87-98, 1995.
Chickering, D. M. Optimal Structure Identification with Greedy Search. Journal of Machine
Learning Research, 3:507-554, 2002.
Chickering, D. M. & Meek, C. Finding Optimal Bayesian Networks. In Proceedings of the
Eighteenth Conference on Uncertainty in Artificial Intelligence, 94-102, 2002.
Chickering, D. M., Heckerman, D. & Meek, C. Large-Sample Learning of Bayesian Networks
is NP-Hard. Journal of Machine Learning Research, 5:1287-1330, 2004.
Friedman, N. & Koller, D. Being Bayesian About Network Structure. A Bayesian Approach
to Structure Discovery in Bayesian Networks. Machine Learning, 50:95-12, 2003.
Gavril, F. Some NP-Complete Problems on Graphs. In Proceedings of the Eleventh Conference on Information Sciences and Systems, 91-95, 1977.
Garey, M. & Johnson, D. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman, 1979.
685

fiPena

Geiger, D., Verma, T. & Pearl, J. Identifying Independence in Bayesian Networks. Networks,
20:507-534, 1990.
Genest, C. & Zidek, J. V. Combining Probability Distributions: A Critique and an Annotated Bibliography. Statistical Science, 1:114-148, 1986.
Hartemink, A. J., Gifford, D. K., Jaakkola, T. S. & Young, R. A. Combining Location
and Expression Data for Principled Discovery of Genetic Regulatory Network Models. In
Pacific Symposium on Biocomputing 7, 437-449, 2002.
Jackson, B. N., Aluru, S. & Schnable, P. S. Consensus Genetic Maps: A Graph Theoretic Approach. In Proceedings of the 2005 IEEE Computational Systems Bioinformatics
Conference, 35-43, 2005.
Koller, D. & Friedman, N. Probabilistic Graphical Models: Principles and Techniques. MIT
Press, 2009.
Matzkevich, I. & Abramson, B. The Topological Fusion of Bayes Nets. In Proceedings of
the Eight Conference Conference on Uncertainty in Artificial Intelligence, 191-198, 1992.
Matzkevich, I. & Abramson, B. Some Complexity Considerations in the Combination of
Belief Networks. In Proceedings of the Ninth Conference Conference on Uncertainty in
Artificial Intelligence, 152-158, 1993a.
Matzkevich, I. & Abramson, B. Deriving a Minimal I-Map of a Belief Network Relative to
a Target Ordering of its Nodes. In Proceedings of the Ninth Conference Conference on
Uncertainty in Artificial Intelligence, 159-165, 1993b.
Maynard-Reid II, P. & Chajewska, U. Agregating Learned Probabilistic Beliefs. In Proceedings of the Seventeenth Conference in Uncertainty in Artificial Intelligence, 354-361,
2001.
Meek, C. Graphical Models: Selecting Causal and Statistical Models. PhD thesis, Carnegie
Mellon Unversity, 1997.
Ng, K.-C. & Abramson, B. Probabilistic Multi-Knowledge-Base Systems. Journal of Applied
Intelligence, 4:219-236, 1994.
Nielsen, J. D., Kocka, T. & Pena, J. M. On Local Optima in Learning Bayesian Networks.
In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence,
435-442, 2003.
Nielsen, S. H. & Parsons, S. An Application of Formal Argumentation: Fusing Bayesian
Networks in Multi-Agent Systems. Artificial Intelligence 171:754-775, 2007.
Pearl, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, 1988.
Pennock, D. M. & Wellman, M. P. Graphical Representations of Consensus Belief. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, 531-540,
1999.
686

fiFinding Consensus Bayesian Network Structures

Pena, J. M., Nilsson, R., Bjorkegren, J. & Tegner, J. Towards Scalable and Data Efficient
Learning of Markov Boundaries. International Journal of Approximate Reasoning, 45:211232, 2007.
Pena, J. M., Kocka, T. & Nielsen, J. D. Featuring Multiple Local Optima to Assist the User
in the Interpretation of Induced Bayesian Network Models. In Proceedings of the Tenth
International Conference on Information Processing and Management of Uncertainty in
Knowledge-Based Systems, 1683-1690, 2004.
Richardson, M. & Domingos, P. Learning with Knowledge from Multiple Experts. In Proceedings of the Twentieth International Conference on Machine Learning, 624-631, 2003.
del Sagrado, J. & Moral, S. Qualitative Combination of Bayesian Networks. International
Journal of Intelligent Systems, 18:237-249, 2003.
Studeny, M. Bayesian Networks from the Point of View of Chain Graphs. In Proceedings of
the Fourteenth Conference Conference on Uncertainty in Artificial Intelligence, 496-503,
1998.
Studeny, M. & Bouckaert, R. R. On Chain Graph Models for Description of Conditional
Independence Structures. The Annals of Statistics, 26:1434-1495, 1998.

687

fiJournal of Artificial Intelligence Research 42 (2011) 125-180

Submitted 03/11; published 10/11

First-Order Stable Model Semantics
and First-Order Loop Formulas
Joohyung Lee
Yunsong Meng

joolee@asu.edu
Yunsong.Meng@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287, USA

Abstract
Lin and Zhaos theorem on loop formulas states that in the propositional case the stable
model semantics of a logic program can be completely characterized by propositional loop
formulas, but this result does not fully carry over to the first-order case. We investigate
the precise relationship between the first-order stable model semantics and first-order loop
formulas, and study conditions under which the former can be represented by the latter.
In order to facilitate the comparison, we extend the definition of a first-order loop formula
which was limited to a nondisjunctive program, to a disjunctive program and to an arbitrary
first-order theory. Based on the studied relationship we extend the syntax of a logic program
with explicit quantifiers, which allows us to do reasoning involving non-Herbrand stable
models using first-order reasoners. Such programs can be viewed as a special class of firstorder theories under the stable model semantics, which yields more succinct loop formulas
than the general language due to their restricted syntax.

1. Introduction
According to the theorem on loop formulas (Lin & Zhao, 2004), the stable models of a
logic program (Gelfond & Lifschitz, 1988) can be characterized as the models of the logic
program that satisfy all its loop formulas. This idea has turned out to be widely applicable
in relating the stable model semantics to propositional logic, and has resulted in an efficient
method for computing answer sets using SAT solvers. Since the original invention of loop
formulas for nondisjunctive logic programs by Lin and Zhao (2004), the theorem has been
extended to more general classes of logic programs, such as disjunctive programs (Lee & Lifschitz, 2003), infinite programs and programs containing classical negation (Lee, 2005; Lee,
Lierler, Lifschitz, & Yang, 2010), arbitrary propositional formulas under the stable model
semantics (Ferraris, Lee, & Lifschitz, 2006), and programs containing aggregates (Liu &
Truszczynski, 2006; You & Liu, 2008). The theorem has also been applied to other nonmonotonic formalisms, such as nonmonotonic causal theories (Lee, 2004) and McCarthys
circumscription (Lee & Lin, 2006). The notion of a loop was further refined as an elementary loop (Gebser & Schaub, 2005; Gebser, Lee, & Lierler, 2006, 2011). However,
all this work is restricted to the propositional case. Variables contained in a program are
first eliminated by groundingthe process which replaces every variable with every object
constantand then loop formulas are obtained from the ground program. As a result, loop
formulas were defined as formulas in propositional logic.
c
2011
AI Access Foundation. All rights reserved.

fiLee & Meng

Chen, Lin, Wang, and Zhangs definition (2006) of a first-order loop formula is different
in that loop formulas are directly obtained from a non-ground program, so that they are
first-order logic formulas which retain variables. However, since the semantics of a logic
program that they refer to is based on grounding, these first-order loop formulas are simply
understood as schemas for ground loop formulas, and only Herbrand models of the loop
formulas were considered in this context.
The stable model semantics that does not involve grounding appeared a year later
(Ferraris, Lee, & Lifschitz, 2007, 2011). The authors define the stable models of a firstorder sentence F as the models of the second-order sentence that is obtained by applying
the stable model operator SM to F . The definition of SM is close to the definition of the
circumscription operator CIRC (McCarthy, 1980, 1986). Under the first-order stable model
semantics, logic programs are viewed as a special class of first-order theories. A similar
definition of a stable model was given by Lin and Zhou (2011), via logic of knowledge
and justified assumption (Lin & Shoham, 1992). The first-order stable model semantics is
also closely related to quantified equilibrium logic (Pearce & Valverde, 2005), and indeed,
Ferraris et al. (2011) showed that they are essentially equivalent.
A natural question arising is how first-order loop formulas and the first-order stable
model semantics are related to each other. In general, the first-order stable model semantics
is more expressive than first-order logic, and as such cannot be completely characterized
by first-order loop formulas. Like circumscription, the concept of transitive closure can
be represented in the first-order stable model semantics, but not in any set of first-order
formulas, even if that set is allowed to be infinite.1 However, as we show in this paper,
understanding the precise relationship between them gives us insights into the first-order
stable model semantics and its computational properties.
In order to facilitate the comparison, we extend the definition of a first-order loop
formula which was limited to nondisjunctive programs, to disjunctive programs and to
arbitrary first-order theories. Also we present a reformulation of SM[F ] in the style of loop
formulas, which includes the characterization of a loop by a syntactic formula. From this
formulation, we derive several conditions, under which a first-order theory under the stable
model semantics can be equivalently rewritten as first-order loop formulas.
Based on the relationship between the first-order stable model semantics and first-order
loop formulas, we extend the syntax of logic programs with explicit quantifiers, which may be
useful in overcoming some limitations of traditional answer set programs in reasoning about
non-Herbrand models. We define the semantics of such extended programs by identifying
them as a special class of first-order theories under the stable model semantics. Such
programs inherit from the general language the ability to handle nonmonotonic reasoning
under the stable model semantics even in the absence of the unique name and the domain
closure assumptions that are built into the grounding-based answer set semantics. On
the other hand, the restricted syntax of an extended program leads to more succinct loop
formulas. The following program 1 is a simple insurance policy example represented in

1. Vladimir Lifschitz, personal communication.

126

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

this syntax.
HasWife(x)
HasWife(x)
Married (x)
w Discount(x, w)






y Spouse(x, y)
Man(x), Married (x)
Man(x), HasWife(x)
Married (x), not z Accident(x, z).

The second and the third rules express that Married (x) and HasWife(x) are synonymous
to each other when x is a Man. The last rule states that x is eligible for some discount
plan (with the name unknown) if x is married and has no record of accident. The quantifier
in the first rule can be dropped without affecting the meaning, but the other quantifiers
cannot. We will say that a program  entails a query F (under the stable model semantics)
if every stable model of  satisfies F . For example,
 1 conjoined with 2 = {Man(John)} entails each of x Married (x) and
xy Discount(x, y).
 1  2 conjoined with 3 = {y Spouse(John, y)} entails neither x Married (x)
nor xy Discount(x, y), but entails each of x Married (x), xyDiscount(x, y), and
xy(Discount(x, y)  x = John).
 1  2  3 conjoined with 4 = {z Accident(John, z)} does not entail
xy(Discount(x, y)  x = John), but entails w Discount(John, w).
The nonmonotonic reasoning of this kind requires non-Herbrand models since the names
(or identifiers) of discount plans, spouses and accident records may be unknown. However,
the traditional answer set semantics is limited to Herbrand models due to the reference
to grounding. By turning the program into first-order loop formulas we can automate the
example reasoning using a first-order theorem prover.
The paper is organized as follows. The next section reviews the first-order stable model
semantics by Ferraris et al. (2007, 2011). Section 3 reviews the theorem on first-order loop
formulas by Chen et al. (2006) and extends it to disjunctive programs and to arbitrary
first-order sentences, limiting attention to Herbrand stable models. Section 4 extends these
results to allow non-Herbrand stable models as well (possibly allowing functions) under a
certain semantic condition, and compare the first-order stable model semantics with loop
formulas by reformulating the former in terms of the latter. In Section 5, we present a series
of syntactic conditions that imply the semantic condition in Section 4. Section 6 provides an
extension of logic programs that contain explicit quantifiers and shows how query answering
for such extended programs can sometimes be reduced to entailment checking in first-order
logic via loop formulas. In Section 7, the results are further extended to distinguish between
intensional and non-intensional predicates. Related work is described in Section 8, and long
proofs are given in Appendix A.
This article is an extended version of a conference paper by Lee and Meng (2008).

2. Review of the First-Order Stable Model Semantics
This review follows a journal paper by Ferraris et al. (2011) that extends a conference
paper by the same authors (Ferraris et al., 2007) by distinguishing between intensional and
non-intensional predicates.
127

fiLee & Meng

A formula is defined the same as in first-order logic. A signature consists of function
constants and predicate constants. Function constants of arity 0 are called object constants.
We assume the following set of primitive propositional connectives and quantifiers:
 (falsity), , , , ,  .
F is an abbreviation of F  , symbol > stands for   , and F  G stands for
(F  G)  (G  F ). We distinguish between atoms and atomic formulas as follows:
an atom of a signature  is an n-ary predicate constant followed by a list of n terms
that can be formed from function constants in  (including object constants) and object
variables; atomic formulas of  are atoms of , equalities between terms of , and the
0-place connective .
The stable models of F relative to a list of predicates p = (p1 , . . . , pn ) are defined via
the stable model operator with the intensional predicates p, denoted by SM[F ; p].2 Let u
be a list of distinct predicate variables u1 , . . . , un of the same length as p. By u = p we
denote the conjunction of the formulas x(ui (x)  pi (x)), where x is a list of distinct object
variables of the same length as the arity of pi , for all i = 1, . . . , n. By u  p we denote the
conjunction of the formulas x(ui (x)  pi (x)) for all i = 1, . . . , n, and u < p stands for
(u  p)  (u = p). For any first-order sentence F , the expression SM[F ; p] stands for the
second-order sentence
F  u((u < p)  F  (u)),
(1)
where F  (u) is defined recursively:
 pi (t) = ui (t) for any list t of terms;
 F  = F for any atomic formula F (including  and equality) that does not contain
members of p;
 (F  G) = F   G ;
 (F  G) = F   G ;
 (F  G) = (F   G )  (F  G);
 (xF ) = xF  ;
 (xF ) = xF  .
(There is no clause for negation here, because we treat F as shorthand for F  .)
A model of a sentence F (in the sense of first-order logic) is called p-stable if it satisfies
SM[F ; p]. We will often simply write SM[F ] instead of SM[F ; p] when p is the list of all
predicate constants occurring in F , and call a model of SM[F ] simply a stable model of F .
We distinguish between the terms stable models and answer sets as follows.3 By (F )
we denote the signature consisting of the function and predicate constants occurring in F .
2. The intensional predicates p are the predicates that we intend to characterize by F .
3. The distinction is useful because in the first-order setting, stable models are no longer Herbrand interpretations and may not be represented by sets of atoms.

128

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

If F contains at least one object constant, an Herbrand interpretation4 of (F ) that satisfies
SM[F ] is called an answer set of F . The answer sets of a logic program  are defined as the
answer sets of the FOL-representation of  (i.e., the conjunction of the universal closures
of implications corresponding to the rules).
Example 1 For program  that contains three rules
p(a)
q(b)
r(x)  p(x), not q(x)
the FOL-representation F of  is
p(a)  q(b)  x((p(x)  q(x))  r(x))

(2)

and SM[F ] is
p(a)  q(b)  x((p(x)  q(x))  r(x))
uvw(((u, v, w) < (p, q, r))  u(a)  v(b)
x(((u(x)  (v(x)  q(x)))  w(x))  ((p(x)  q(x))  r(x)))),
which is equivalent to the first-order sentence
x(p(x)  x = a)  x(q(x)  x = b)  x(r(x)  (p(x)  q(x)))

(3)

(See Example 3 in the work of Ferraris et al., 2007). The stable models of F are any firstorder models of (3). The only answer set of F is the Herbrand model {p(a), q(b), r(a)}.

3. First-Order Loop Formulas and Herbrand Models
We review the definition of a first-order loop formula for a nondisjunctive program given
by Chen et al. (2006) and extend it to a disjunctive program and to an arbitrary first-order
sentence.
3.1 Review of First-Order Loop Formulas Defined by Chen et al. (2006)
We call a formula negative if every occurrence of every predicate constant in it belongs
to the antecedent of an implication. For instance, any formula of the form F is negative
because this expression is shorthand for F  . An equality t1 = t2 is also negative because
it contains no predicate constants.
A nondisjunctive program is a finite set of rules of the form
A  B, N,

(4)

4. Recall that an Herbrand interpretation of a signature  (containing at least one object constant) is an
interpretation of  such that its universe is the set of all ground terms of , and every ground term
represents itself. An Herbrand interpretation can be identified with the set of ground atoms to which it
assigns the value true.

129

fiLee & Meng

where A is an atom, B is a set of atoms, and N is a negative formula. The rules may
contain function constants of positive arity.5
We will say that a nondisjunctive program is in normal form if, for all rules (4) in it, A
is of the form p(x) where x is a list of distinct variables. It is clear that every program can
be turned into normal form using equality in the body. For instance, p(a, b)  q(a) can be
rewritten as p(x, y)  x = a, y = b, q(a).
Let  be a nondisjunctive program and let Norm() be a normal form of . By () we
denote the signature consisting of function and predicate constants occurring in . Given a
finite set Y of atoms, we assume that Norm() does not contain variables in Y , by renaming
the variables in Norm(). The (first-order) external support formula of Y for , denoted
by ES  (Y ), is the disjunction of
_


z B  N  

:AY

^

0

(t 6= t )


(5)

p(t)B
p(t0 )Y

for all rules (4) in Norm(),6 where  is a substitution that maps variables in A to terms
occurring in Y , and z is the list of all variables that occur in
A  B, N 
but not in Y .
The (first-order) loop formula of Y for , denoted by LF  (Y ), is the universal closure
of
^
Y  ES  (Y ).
(6)
V
(The expression Y in the antecedent stands for the conjunction of all elements of Y .)
When  is a propositional program, LF  (Y ) is equivalent to a conjunctive loop formula as
defined by Ferraris et al. (2006).
The definition of a first-order dependency graph and the definition of a first-order loop
are as follows. We say that an atom p(t) depends on an atom q(t0 ) in a rule (4) if p(t) is
A and q(t0 ) is in B. The (first-order) dependency graph of  is an infinite directed graph
(V, E) such that
 V is the set of atoms of signature ();7
 (p(t), q(t0 )) is in E if p(t) depends on q(t0 ) in a rule of  and  is a substitution
that maps variables in t and t0 to terms (including variables) of ().
A nonempty subset L of V is called a (first-order) loop of  if the subgraph of the
first-order dependency graph of  induced by L is strongly connected.

5. The original definition by Chen et al. (2006) does not allow function constants of positive arity.
6. For any lists of terms t = (t1 , . . . , tn ) and t0 = (t01 , . . . , t0n ) of the same length, t = t0 stands for
(t1 = t01 )      (tn = t0n ).
7. Note that V is infinite since infinitely many object variables can be used to form atoms.

130

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

Example 2 Let  be the following program:
p(x)  q(x)
q(y)  p(y)
p(z)  not r(z).

(7)

The following sets of atoms are first-order loops (among many others): Y1 = {p(u)}, Y2 =
{q(u)}, Y3 = {r(u)}, Y4 = {p(u), q(u)}. Their loop formulas are
LF  (Y1 )
LF  (Y2 )
LF  (Y3 )
LF  (Y4 )

=
=
=
=

u(p(u)  (q(u)  r(u))),
u(q(u)  p(u)),
u(r(u)  ),
u(p(u)  q(u)  (q(u)  u 6= u)  (p(u)  u 6= u)  r(u)).

Example 3 Let  be the one-rule program
p(x)  p(y).

(8)

Its finite first-order loops are Yk = {p(x1 ), . . . , p(xk )} where k > 0. Formula LF  (Yk ) is


(9)
x1 . . . xk p(x1 )  . . .  p(xk )  y(p(y)  (y 6= x1 )  . . .  (y 6= xk )) .
The following is a reformulation of Theorem 1 from the work of Chen et al. (2006).
Theorem 1 Let  be a nondisjunctive program that contains at least one object constant
but no function constants of positive arity, and let I be an Herbrand interpretation of ()
that satisfies .8 The following conditions are equivalent to each other:
(a) I is a stable model of ;
(b) for every nonempty finite set Y of atoms of (), I satisfies LF  (Y );

9

(c) for every finite first-order loop Y of , I satisfies LF  (Y ).
The sets of first-order loop formulas considered in conditions (b) and (c) above have
obvious redundancies. For instance, the loop formula of {p(x)} is equivalent to the loop
formula of {p(y)}; the loop formula of {p(x), p(y)} entails the loop formula of {p(z)}. Following the definition by Chen et al. (2006), given two sets of atoms Y1 and Y2 , we say
that Y1 subsumes Y2 if there is a substitution  that maps variables in Y1 to terms so that
Y1  = Y2 .
Proposition 1 (Chen et al., 2006, Proposition 7) For any nondisjunctive program  and
any loops Y1 and Y2 of , if Y1 subsumes Y2 , then LF  (Y1 ) entails LF  (Y2 ).
Therefore in condition (c) from Theorem 1, it is sufficient to consider a set  of loops
such that, for every loop L of , there is a loop L0 in  that subsumes L. Chen et al. (2006)
called such  a complete set of loops. In Example 2, set {Y1 , Y2 , Y3 , Y4 } is a finite complete
set of loops of program (7). Program (8) in Example 3 has no finite complete set of loops.
8. We say that I satisfies  if I satisfies the FOL-representation of .
9. Note that Y may contain variables.

131

fiLee & Meng

3.2 Extension to Disjunctive Programs
A disjunctive program is a finite set of rules of the form
A  B, N,

(10)

where A and B are sets of atoms, and N is a negative formula. Similar to a nondisjunctive
program, we say that a disjunctive program is in normal form if, for all rules (10) in it, all
atoms in A are of the form p(x) where x is a list of distinct variables.
Let  be a disjunctive program and let Norm() be a normal form of . Given a finite
set Y of atoms, we first rename variables in Norm() so that no variables in Norm()
occur in Y . The (first-order) external support formula of Y for , denoted by ES  (Y ), is
the disjunction of

_
^
_
^

z B  N  
(t 6= t0 )  
p(t) 
t 6= t0
(11)
:AY 6=

p(t)B
p(t0 )Y

p(t)A

p(t0 )Y

for all rules (10) in Norm(), where  is a substitution that maps variables in A to terms
occurring in Y or to themselves, and z is the list of all variables that occur in
A  B, N 
but not in Y . The (first-order) loop formula of Y for , denoted by LF  (Y ), is the universal
closure of
^
Y  ES  (Y ).
Clearly, (11) is equivalent to (5) when  is nondisjunctive. When  and Y are propositional,
LF  (Y ) is equivalent to the conjunctive loop formula for a disjunctive program as defined
by Ferraris et al. (2006).
Example 4 Let  be the program
p(x, y) ; p(y, z)  q(x)
and let Y = {p(u, v)}. Formula LF  (Y ) is the universal closure of
p(u, v)  z(q(u)  (p(v, z)  ((v, z) 6= (u, v))))
 x(q(x)  (p(x, u)  ((x, u) 6= (u, v)))).
Similar to the nondisjunctive case, we say that p(t) depends on q(t0 ) in  if there is
a rule (10) in  such that p(t) is in A and q(t0 ) is in B. The definitions of a first-order
dependency graph and a first-order loop are extended to disjunctive programs in a straightforward way. Using these extended notions, the following theorem extends Theorem 1 to
a disjunctive program. It is also a generalization of the main theorem by Ferraris et al.
(2006) which was restricted to a propositional disjunctive program.
Theorem 1 d Let  be a disjunctive program that contains at least one object constant but
no function constants of positive arity, and let I be an Herbrand interpretation of () that
satisfies . The following conditions are equivalent to each other:
132

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

(a) I is a stable model of ;
(b) for every nonempty finite set Y of atoms of (), I satisfies LF  (Y );
(c) for every finite first-order loop Y of , I satisfies LF  (Y ).

3.3 Extension to Arbitrary Sentences
In this section we extend the definition of a first-order loop formula to an arbitrary firstorder sentence.
As with a propositional loop formula defined for an arbitrary propositional theory (Ferraris et al., 2006), it is convenient to introduce a formula whose negation is close to ES . We
define formula NES F (Y ) (Negation of (First-order) External Support Formula), where F
is a first-order formula and Y is a finite set of atoms, as follows. As before we assume that
no variables in Y occur in F , by renaming variables.
 NES pi (t) (Y ) = pi (t) 

V

pi (t0 )Y

t 6= t0 ;

 NES t1 =t2 (Y ) = (t1 = t2 );
 NES  (Y ) = ;
 NES F G (Y ) = NES F (Y )  NES G (Y );
 NES F G (Y ) = NES F (Y )  NES G (Y );
 NES F G (Y ) = (NES F (Y )  NES G (Y ))  (F  G);
 NES xG (Y ) = xNES G (Y );
 NES xG (Y ) = xNES G (Y ).
The (first-order) loop formula of Y for F , denoted by LF F (Y ), is the universal closure
of
^

Y  NES F (Y ).

(12)

Note that the definition of NES looks similar to the definition of F  given in Section 2.
When F and Y are propositional, LF F (Y ) is equivalent to a conjunctive loop formula for
a propositional formula that is defined by Ferraris et al. (2006). The following lemma tells
us that the definition of a loop formula in this section generalizes the definition of a loop
formula for a disjunctive program in the previous section.
Lemma 1 Let  be a disjunctive program in normal form, F an FOL-representation of
, and Y a finite set of atoms. Formula NES F (Y ) is equivalent to ES  (Y ) under the
assumption F .
133

fiLee & Meng

In order to extend the first-order dependency graph to an arbitrary formula, we introduce
a few notions. We say that an occurrence of a subformula G in a formula F is positive if
the number of implications in F containing that occurrence in the antecedent is even; it is
strictly positive if that number is 0. A rule of a first-order formula F is an implication that
occurs strictly positively in F . We will say that a formula is rectified if it has no variables
that are both bound and free, and if all quantifiers in the formula refer to different variables.
Any formula can be easily rewritten into a rectified formula by renaming bound variables.
We say that an atom p(t) depends on an atom q(t0 ) in an implication G  H if
 p(t) has a strictly positive occurrence in H, and
 q(t0 ) has a positive occurrence in G that does not belong to any negative subformula
of G.10
The definition of a first-order dependency graph is extended to formulas as follows. The
(first-order) dependency graph of a rectified formula F is the infinite directed graph (V, E)
such that
 V is the set of atoms of signature (F );
 (p(t), q(t0 )) is in E if p(t) depends on q(t0 ) in a rule of F and  is a substitution
that maps variables in t and t0 to terms of (F ).
Note that the rectified formula assumption is required in order to distinguish between
dependency graphs of formulas such as
x(p(x)  q(x))
and
x p(x)  x q(x).
Once the definition of a dependency graph is given, a loop of a first-order formula is
defined in the same way as with a disjunctive program. Theorem 1 can be extended to
first-order sentences using these extended notions.

Theorem 1 f Let F be a rectified sentence that contains at least one object constant but
no function constants of positive arity, and let I be an Herbrand interpretation of (F ) that
satisfies F . The following conditions are equivalent to each other:
(a) I is a stable model of F (i.e., I satisfies SM[F ]);
(b) for every nonempty finite set Y of atoms of (F ), I satisfies LF F (Y );
(c) for every finite first-order loop Y of F , I satisfies LF F (Y ).
Example 2 (continued) Consider the FOL-representation F of the program in Example 2,
for which {Y1 , Y2 , Y3 , Y4 } is a complete set of loops. Under the assumption F ,
10. Recall the definition of a negative formula in Section 3.1.

134

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

 LF F (Y1 ) is equivalent to the universal closure of

p(u)   x(q(x)  p(x)  x 6= u)  y(p(y)  y 6= u  q(y))

 z(r(z)  p(z)  z 6= u) ;
 LF F (Y2 ) is equivalent to the universal closure of


q(u)   x(q(x)  x 6= u  p(x))  y(p(y)  q(y)  y 6= u) ;
 LF F (Y3 ) is equivalent to the universal closure of
r(u)  ;
 LF F (Y4 ) is equivalent to the universal closure of

p(u)  q(u)   x(q(x)  x 6= u  p(x)  x 6= u)

 y(p(y)  y 6= u  q(y)  y 6= u)  z(r(z)  p(z)  z 6= u) .

Proposition 1 can be straightforwardly extended to arbitrary sentences even without
restricting the attention to loops.
Proposition 1 f For any sentence F and any nonempty finite sets of atoms Y1 and Y2
of (F ), if Y1 subsumes Y2 , then LF F (Y1 ) entails LF F (Y2 ).
Proof. Note that LF F (Y1 ) is
z

^


Y1  NES F (Y1 ) ,

(13)

where z is the set of all variables in Y1 . If Y1 subsumes Y2 , by definition, there is a
substitution  from variables in Y1 to terms in Y2 such that Y1  = Y2 . It is clear that (13)
entails
^

z0
Y1   NES F (Y1 ) ,
(14)
where z0 is the set of all variables in Y1 . (14) is exactly LF F (Y2 ).



Theorem 2 from the work of Ferraris et al. (2006) is a special case of Theorem 1f when
F is restricted to a propositional formula.
Corollary 1 (Ferraris et al., 2006, Thm. 2) For any propositional formula F , the following
formulas are equivalent to each other under the assumption F .
(a) SM[F ];
(b) the conjunction of LF F (Y ) for all nonempty sets Y of atoms occurring in F ;
(c) the conjunction of LF F (Y ) for all (ground) loops Y of F .
135

fiLee & Meng

4. Comparing First-Order Stable Model Semantics and First-Order Loop
Formulas
The theorems in the previous section were restricted to Herbrand stable models. This
section extends the results to allow non-Herbrand stable models as well, and compare the
idea of loop formulas with SM by reformulating the latter in the style of loop formulas.
4.1 Loop Formulas Relative to an Interpretation
Recall that Theorem 1 and its extensions do not allow function constants of positive arity
and are limited to Herbrand models of the particular signature obtained from the given
theory. Indeed, the statements become wrong if these conditions are dropped.
Example 5 The following program contains a unary function constant f .
p(a)
p(x)  p(f (x)).
The loops of this program are all singleton sets of atoms, and their loop formulas are satisfied
by the Herbrand model {p(a), p(f (a)), p(f (f (a))), . . . } of the program, but this model is not
stable.

Example 3 (continued) The mismatch can happen even in the absence of function constants of positive arity. Consider the program in Example 3 and an interpretation I such
that the universe is the set of all integers, and pI contains all integers. Interpretation I
satisfies all first-order loop formulas (9), but it is not a stable model.
These examples suggest that the mismatch between the first-order stable model semantics and the first-order loop formulas is related to the presence of an infinite path in the
dependency graph that visits infinitely many vertices. In the following we will make this
idea more precise, and extend Theorem 1f to allow non-Herbrand interpretations under a
certain condition.
First, we define a dependency graph relative to an interpretation. Let F be a rectified
formula whose signature is  and let I be an interpretation of . For each element  of the
universe |I| of I, we introduce a new symbol   , called an object name. By  I we denote the
signature obtained from  by adding all object names   as additional object constants. We
will identify an interpretation I of signature  with its extension to  I defined by I(  ) = 
(For details, see the work of Lifschitz, Morgenstern, & Plaisted, 2008).
The dependency graph of F w.r.t. I is the directed graph (V, E) where
 V is the set of all atoms of the form pi (  ) where pi belongs to (F ) and   is a list
of object names for |I|, and
 (pi (  ), pj (  )) is in E if there are atoms pi (t), pj (t0 ) such that pi (t) depends on pj (t0 )
in a rule of F and there is a substitution  that maps variables in t and t0 to object
names such that (t)I =  and (t0 )I = .
136

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

We call a nonempty subset L of V a loop of F w.r.t. I if the subgraph of the dependency
graph of F w.r.t. I that is induced by L is strongly connected. We say that F is bounded
w.r.t. I if every infinite path in the dependency graph of F w.r.t. I whose vertices are
satisfied by I visits only finitely many vertices. If F is bounded w.r.t. I, then, clearly, every
loop L of F w.r.t. I such that I |= L is finite. The definition is extended to a non-rectified
formula by first rewriting it as a rectified formula. It also applies to the program syntax by
referring to its FOL-representation.
Theorem 2 Let F be a rectified sentence of a signature  (possibly containing function
constants of positive arity), and let I be an interpretation of  that satisfies F . If F is
bounded w.r.t. I, then the following conditions are equivalent to each other:
(a) I |= SM[F ];
(b) for every nonempty finite set Y of atoms formed from predicate constants in (F ) and
object names for |I|, I satisfies LF F (Y );
(c) for every finite loop Y of F w.r.t. I, I satisfies LF F (Y ).
The condition that F is bounded w.r.t. I is sufficient for ensuring the equivalence among
(a), (b), and (c), but it is not a necessary condition. For instance, consider F to be
x p(x)  xy(p(x)  p(y))
and I to be a model of F whose universe is infinite. Formula F is not bounded w.r.t. I, but
I satisfies every loop formula, as well as SM[F ].
When I is an Herbrand model of (F ), the dependency graph of F w.r.t. I is isomorphic to the subgraph of the first-order dependency graph of F that is induced by vertices
containing ground atoms. A set of ground atoms of (F ) is a loop of F iff it is a loop of F
w.r.t. I. Hence Theorem 2 is essentially a generalization of Theorem 1f .
Note that the programs considered in Examples 3 and 5 are not bounded w.r.t. the
interpretations considered there.
Clearly, if the universe of I is finite, then F is bounded w.r.t. I. This fact leads to the
following corollary.
Corollary 2 For any rectified sentence F and any model I of F whose universe is finite,
conditions (a), (b), and (c) of Theorem 2 are equivalent to each other.
In view of Proposition 1f and Corollary 2, if the size of the universe is known to be a
finite number n, it is sufficient to consider at most 2|p|  1 loop formulas, where p is the
set of all predicate constants
occurring in the sentence. Each loop formula is to check the
S
external support of pK {p(x1 ), . . . , p(xnr )} for each K where
 K is a nonempty subset of p;
 r is the arity of p and each xi is a list of variables of the length r such that all variables
in x1 , . . . , xnr are pairwise distinct.
137

fiLee & Meng

For instance, consider program (8). If the size of the universe is known to be 3, it is sufficient
to consider only one loop formula (9) where k = 3.
Theorem 1f essentially follows from Corollary 2 as the Herbrand universe of (F ) is
finite when F contains no function constants of positive arity.
Another corollary to Theorem 2 is acquired when F has only trivial loops. We say
that a formula F is atomic-tight w.r.t. I if every path in the dependency graph of F w.r.t. I
whose vertices are satisfied by I is finite. Clearly, this is a special case of boundedness
condition, and every loop L of an atomic-tight formula F w.r.t. I such that I |= L is a
singleton. The following is a corollary to Theorem 2, which tells us the condition under
which stable models can be characterized by loop formulas of singleton loops only. By
SLF[F ] (loop formulas of singletons) we denote
{LF F ({p(x)}) | p is a predicate constant in (F ), and x is a list
of distinct object variables whose length is the same as the arity of p}.

(15)

Corollary 3 Let F be a rectified sentence (possibly containing function constants of positive
arity), and let I be a model of F . If F is atomic-tight w.r.t. I, then I satisfies SM[F ] iff I
satisfies SLF[F ].
SLF[F ] is similar to Clarks completion. In the propositional case, the relationship
between the loop formulas of singletons and the completion is studied by Lee (2005). Below
we describe their relationship in the first-order case. A sentence F is in Clark normal form
(Ferraris et al., 2011) if it is a conjunction of formulas of the form
x(G  p(x)),

(16)

one for each predicate constant p occurring in F , where x is a list of distinct variables, and
G has no free variables other than x. The completion of a sentence F in Clark normal form,
denoted by Comp[F ], is obtained from F by replacing each conjunctive term (16) with
x(p(x)  G).
Any nondisjunctive program can be turned into Clark normal form (Ferraris et al., 2011,
Section 6.1).
Corollary 4 Let F be the FOL-representation of a nondisjunctive program , and let F 0
be the Clark normal form of F as obtained by the process described in the work of Ferraris
et al. (2011, Section 6.1). If F is atomic-tight w.r.t. an interpretation I, then I |= SM[F ]
iff I |= Comp[F 0 ].
Proof. Since F is atomic-tight w.r.t. I, by Corollary 3, I |= SM[F ] iff I |= F  SLF[F ]. It
is sufficient to show that, for each predicate constant p occurring in F , under the assumption
that F is atomic-tight w.r.t. I,


_
^

I |= x p(x) 
z (x = t0 )  B  N 
(t 6= x)
(17)
p(t0 )B,N 

p(t)B

138

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

iff




z (x = t )  B  N ,

_

I |= x p(x) 

0



(18)

p(t0 )B,N 

where z is the list of all free variables in p(x)  (x = t0 ), B, N that are not in x.
Note that (17) is equivalent to saying that


^
_

0
0
(t 6= t ) .
z (x = t )  B  N 
I |= x p(x) 
p(t0 )B,N 

(19)

p(t)B

From the assumption that F is atomic-tight w.r.t. I, it follows that, for any rule p(t0 ) 
B, N in  and any atom of p(t) in B, I |= y(t 6= t0 ), where y is the list of all variables in
t and t0 (otherwise we find a singleton loop with a self-cycle, which contradicts that F is
atomic tight w.r.t. I). Consequently, (19) is equivalent to (18).

For example, let F be the FOL-representation of the program
p(b)  p(a)
 a 6= b

(20)

SLF[p(a)  p(b)] is x(p(x)  x = b  p(a)  x 6= a), while Comp[x(x = b  p(a)  p(x))]
is x(p(x)  x = b  p(a)). The additional conjunctive term x 6= a can be dropped when
we consider any model I of F , for which aI 6= bI .
Corollary 4 is an enhancement of Theorem 11 from the work of Ferraris et al. (2011),
which states the equivalence between SM[F ] and Comp[F ] for any tight sentence F in Clark
normal form. (Tight sentences are defined in a similar way, but in terms of a predicate
dependency graph, whose vertices are predicate constants instead of atoms.) Every tight
sentence is atomic-tight w.r.t. any model of the sentence. On the other hand, program (20)
is atomic-tight w.r.t. any model of the program, but is not tight.
Theorem 2 tells us that one of the limitations of first-order loop formulas is that, even
if infinitely many first-order loop formulas are considered, they cannot ensure the external
support of a certain infinite set that forms an infinite path in the dependency graph of F
w.r.t. I. In the next section, by reformulating SM[F ], we show that the definition of SM[F ]
essentially encompasses loop formulas, ensuring the external support of any sets of atoms,
including those difficult infinite sets.
4.2 A Reformulation of SM
As before, let F be a first-order formula of a signature , let p = (p1 , . . . , pn ) be the list
of all predicate constants occurring in F , and let u and v be lists of predicate variables of
the same length as p. We define NSES F (u) (Negation of Second-Order External Support
Formula) recursively as follows.
 NSES pi (t) (u) = pi (t)  ui (t);
 NSES t1 =t2 (u) = (t1 = t2 );
 NSES  (u) = ;
139

fiLee & Meng

 NSES F G (u) = NSES F (u)  NSES G (u);
 NSES F G (u) = NSES F (u)  NSES G (u);
 NSES F G (u) = (NSES F (u)  NSES G (u))  (F G);
 NSES xF (u) = xNSES F (u);
 NSES xF (u) = xNSES F (u).
Lemma 2 Let F be a rectified sentence of a signature , I an interpretation of , p the
list of predicate constants occurring in F , q a list of predicate names 11 of the same length
as p and Y a set of atoms formed from predicate constants from (F ) and object names
such that
pi (  )  Y iff I |= qi (  ),
where   is a list of object names. If Y is finite, then
I |= NSES F (q) iff I |= NES F (Y ).
Proof. By induction on F . We only list the case when F is an atom. The other cases are
straightforward. Let F be an atom pi (  ).
iff
iff
iff
iff
iff

I |= NSES F (q)
I |= pi (  )  qi (  )
I |= pi (  ) and pi (  ) 
/Y
 such that p (  )  Y, it holds that   6=  
I |= pi (  ) and
for
all

i
V
I |= pi (  )  pi ( )Y   6=  
I |= NES F (Y ).


SM[F ] can be written in terms of NSES as follows. By Nonempty(u) we denote the
formula
x1 u1 (x1 )      xn un (xn ),
where each xi is a list of distinct variables whose length is the same as the arity of pi .
Proposition 2 For any sentence F , SM[F ] is equivalent to
F  u((u  p)  Nonempty(u)  NSES F (u)).

(21)

Now we represent the notion of a loop by a second-order formula. Given a rectified
formula F , by EF (v, u) we denote
_
z(vi (t)  uj (t0 )  vj (t0 )),
(pi (t),pj (t0 )) :
pi (t) depends on pj (t0 ) in a rule of F

11. Like object names, for every n > 0, each subset of |I|n has a name, which is an n-ary predicate constant
not from the underlying signature.

140

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

where z is the list of all object variables in t and t0 . By Loop F (u) we denote the second-order
formula
Nonempty(u)  v((v < u)  Nonempty(v)  EF (v, u)).
(22)
Formula (22) represents the concept of a loop without referring to the notion of a dependency
graph explicitly. This is based on the following observation. Consider a finite propositional
program . A nonempty set U of atoms that occur in  is a loop of  iff, for every
nonempty proper subset V of U , there is an edge from an atom in V to an atom in U \ V
in the dependency graph of  (Gebser et al., 2006).
Recall the definition of a dependency graph relative to an interpretation. Let F be
a rectified sentence of a signature , and let I be an interpretation of . The following
proposition describes the relationship between formula (22) and a loop of F w.r.t. I.
Proposition 3 Let q be a list of predicate names corresponding to p, and let Y be a set of
atoms in the dependency graph of F w.r.t. I such that
pi (  )  Y iff I |= qi (  ),
where   is a list of object names. Then I |= Loop F (q) iff Y is a loop of F w.r.t. I.
One might expect that, similar to the equivalence between conditions (a) and (c) from
Theorem 2, formula SM[F ] is equivalent to the following formula:
F  u((u  p)  Loop F (u)  NSES F (u)).

(23)

However, the equivalence does not hold in general, as the following example illustrates.
Example 6 Consider the FOL-representation F of the following program
p(x, y)  q(x, z)
q(x, z)  p(y, z),
and an interpretation I whose universe is the set of all nonnegative integers such that
pI = {(m, m) | m is a nonnegative integer},
q I = {(m, m+1) | m is a nonnegative integer}.
Formula F is not bounded w.r.t. I since the dependency graph of F w.r.t. I contains an
infinite path such as
hp(0 , 0 ), q(0 , 1 ), p(1 , 1 ), q(1 , 2 ), . . .i.

(24)

The interpretation I satisfies every loop formula of every finite loop of F w.r.t. I, but it is
not a stable model.
In the example, what distinguishes the set
{p(0 , 0 ), q(0 , 1 ), p(1 , 1 ), q(1 , 2 ), . . . }

(25)

from a loop is that, for every loop contained in (25), there is an outgoing edge in the dependency graph. This is an instance of what we call unbounded set. Given a dependency
graph of F w.r.t. I, we say that a nonempty set Y of vertices is unbounded w.r.t. I if, for
every subset Z of Y that is a loop, there is an edge from a vertex in Z to a vertex in Y \ Z.
The following proposition tells us how an unbounded set can be characterized by a
second-order formula.
141

fiLee & Meng

Proposition 4 Let q be a list of predicate names corresponding to p, and let Y be a set of
atoms in the dependency graph of F w.r.t. I such that
pi (  )  Y iff I |= qi (  ),
where   is a list of object names. Then
I |= Nonempty(q)  v((v  q)  Loop F (v)  EF (v, q))
iff Y is an unbounded set of F w.r.t. I.
In order to check the stability of a model, we need to check the external support of every
loop and every unbounded set. An extended loop of F w.r.t. I is a loop or an unbounded
set of F w.r.t. I. We define Ext-Loop F (u) as
Loop F (u)  (Nonempty(u)  v((v  u)  Loop F (v)  EF (v, u))).

(26)

From Propositions 3 and 4, it follows that I |= Ext-Loop F (q) iff Y is an extended loop of
F w.r.t. I.
If we replace Loop F (u) with Ext-Loop F (u) in (23), the formula is equivalent to SM[F ],
as the following theorem states.
Theorem 3 For any rectified sentence F , the following sentences are equivalent to each
other:
(a) SM[F ];
(b) F  u((u  p)  Nonempty(u)  NSES F (u));
(c) F  u((u  p)  Ext-Loop F (u)  NSES F (u)).
In the following example we use the following fact to simplify the formulas.
Proposition 5 For any negative formula F , formula
NSES F (u)  F
is logically valid.
Example 2 (continued) Consider program (7) from Example 2:
p(x)  q(x)
q(y)  p(y)
p(z)  not r(z).
Let F be the FOL-representation of the program:



x q(x)  p(x)  y p(y)  q(y)  z r(z)  p(z) .
142

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

1. SM[F ] is equivalent to
F  u1 u2 u3 ((u1 , u2 , u3 ) < (p, q, r))
x(u2 (x)  u1 (x))  y(u1 (y)  u2 (y))  z(r(z)  u1 (z))).
2. Formula in Theorem 3 (b):
F  u(u  p  Nonempty(u)  NSES F (u))
is equivalent to
F  u1 u2 u3 ((u1 , u2 , u3 )  (p, q, r)  (x u1 (x)  x u2 (x)  x u3 (x))
 (x[q(x)  u2 (x)  p(x)  u1 (x)]
y[p(y)  u1 (y)  q(y)  u2 (y)]
z[r(z)  p(z)  u1 (z)])).

(27)

3. Formula in Theorem 3 (c): Similar to (27) except that
x u1 (x)  x u2 (x)  x u3 (x)
in (27) is replaced with Ext-Loop F (u), which is
Loop F (u)  [(x u1 (x)  x u2 (x)  x u3 (x))
 v1 v2 v3 (((v1 , v2 , v3 )  (u1 , u2 , u3 ))  Loop F (v)
 (x(v1 (x)  u2 (x)  v2 (x))  y(v2 (y)  u1 (y)  v1 (y))))],
where Loop F (u) is
(x u1 (x)  x u2 (x)  x u3 (x))
 v1 v2 v3 (((x v1 (x)  x v2 (x)  x v3 (x))  (v1 , v2 , v3 ) < (u1 , u2 , u3 ))
 (x(v1 (x)  u2 (x)  v2 (x))  y(v2 (y)  u1 (y)  v1 (y)))).
The proof of Theorem 2 follows from Theorem 3 using the following lemma.
Lemma 3 Let F be a rectified sentence of a signature  (possibly containing function constants of positive arity), and let I be an interpretation of  that satisfies F . If F is bounded
w.r.t. I,
I |= u(u  p  Ext-Loop F (u)  NSES F (u))
iff there is a finite loop Y of F w.r.t. I such that
^

I |=
Y  NES F (Y ) .

5. Representing First-Order Stable Model Semantics by First-Order
Loop Formulas
We noted in the previous section that if a sentence is bounded w.r.t. a model, then loop
formulas can be used to check the stability of the model. In this section, we provide a few
syntactic counterparts of the boundedness condition.
143

fiLee & Meng

5.1 Bounded Formulas
We say that a rectified formula F is bounded if every infinite path in the first-order dependency graph of F visits only finitely many vertices. If F is bounded, then, clearly, every loop
of F is finite. Again, the definition is extended to a non-rectified formula by first rewriting
it as a rectified formula. It also applies to a program by referring to its FOL-representation.
One might wonder if the syntactic notion of boundedness ensures the semantic notion of
boundedness: that is, if a formula is bounded, then it is bounded w.r.t. any interpretation.
However, the following example tells us that this is not the case in general.
Example 7 Consider the FOL-representation F of the following program
p(a)  q(x)
q(x)  p(b),

(28)

and an interpretation I whose universe |I| is the set of all nonnegative integers, aI = bI = 0,
pI = {0} and q I = |I|. Formula (28) is bounded according to the above definition, but not
bounded w.r.t. I: the dependency graph of F w.r.t. I contains an infinite path such as
hp(0 ), q(1 ), p(0 ), q(2 ), . . . i.
5.1.1 Bounded Formulas and Clarks Equational Theory
On the other hand, such a relationship holds if the interpretation satisfies Clarks equational
theory (1978). Clarks equational theory of a signature , denoted by CET , is the union
of the universal closures of the following formulas
f (x1 , . . . , xm ) 6= g(y1 , . . . , yn ),

(29)

for all pairs of distinct function constants f , g,
f (x1 , . . . , xn ) = f (y1 , . . . , yn )  (x1 = y1  . . .  xn = yn ),

(30)

for all function constants f of arity > 0, and
t 6= x,

(31)

where t is any term which contains the variable x.
Proposition 6 If a rectified formula F of a signature  is bounded, then F is bounded
w.r.t. any interpretation of  that satisfies CET .
The following lemma relates loops and loop formulas of different notions of dependency
graphs.
Proposition 7 For any rectified sentence F of a signature  and for any interpretation I
of  that satisfies CET , I is a model of
{LF F (Y ) | Y is a finite first-order loop of F }
iff I is a model of
{LF F (Y ) | Y is a finite loop of F w.r.t. I}.
144

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

The following theorem follows from Theorem 2, Proposition 6 and Proposition 7.
Theorem 4 Let F be a rectified sentence of a signature  (possibly containing function
constants of positive arity), and let I be an interpretation of  that satisfies F and CET .
If F is bounded, then the following conditions are equivalent to each other:
(a) I |= SM[F ];
(b) for every nonempty finite set Y of atoms of (F ), I satisfies LF F (Y );
(c) for every finite first-order loop Y of F , I satisfies LF F (Y ).
Proof. By Proposition 6, if F is bounded then F is bounded w.r.t. any interpretation
that satisfies CET . Then the equivalence between (a) and (b) follows from the equivalence
between (a) and (b) of Theorem 2. The equivalence between (a) and (c) follows from the
equivalence between (a) and (c) of Theorem 2 and by Proposition 7.

As every Herbrand interpretation of  satisfies CET , Theorem 4 applies to Herbrand
interpretations as a special case.
The theorem also applies to logic programs, since they can be viewed as a special case
of formulas. For example, consider the following program, which is bounded.
p(f (x))  q(x)
q(x)  p(x), r(x)
p(a)
r(a)
r(f (a)).

(32)

The set {p(a), p(f (a)), p(f (f (a))), q(a), q(f (a)), r(a), r(f (a))} is an answer set of (32). In
accordance with Theorem 4, it is also the Herbrand interpretation of the signature obtained
from the program that satisfies the FOL-representation of (32) and the loop formulas, which
are the universal closures of
p(z)  (q(x)  z = f (x))  z = a
q(z)  p(z)  r(z)
r(z)  z = a  z = f (a).
Consider another example program by Bonatti (2004), where a, . . . , z, nil are object
constants.
letter (a)
...
letter (z)
(33)
atomic([x])  letter (x)
atomic([x|y])  letter (x), atomic(y).
The expression [x|y] is a list whose head is x and whose tail is y, which stands for a function
cons(x, y). The expression [x] stands for cons(x, nil) where nil is a special symbol for
145

fiLee & Meng

the empty list. This program is bounded. The only answer set of the program is the only
Herbrand interpretation of the FOL-representation of (33) and the universal closures of
letter (u)  u = a  . . .  u = z
atomic(u)  v (letter (v)  u = cons(v, nil))
 xy (letter (x)  atomic(y)  y 6= u  u = cons(x, y)).
In fact, the definitions of standard list processing predicates, such as member, append, and
reverse (Bonatti, 2004, Figure 1) are bounded, so they can be represented by first-order
formulas on Herbrand interpretations.12
We say that a formula F is atomic-tight if the first-order dependency graph of F has no
infinite paths. Every tight sentence is atomic-tight, but not vice versa. For example, the
FOL-representations of programs (32) and (33) are atomic-tight, but are not tight. Similar
to Proposition 6, if F is atomic-tight, then F is atomic-tight w.r.t. any interpretation that
satisfies CET , so that the following statement is derived from Corollary 3.
Corollary 5 Let F be a rectified sentence of a signature  (possibly containing function
constants of positive arity), and let I be an interpretation of  that satisfies F and CET .
If F is atomic-tight, then I satisfies SM[F ] iff I satisfies SLF[F ].
The statement of Corollary 5 is restricted to interpretations that satisfy CET . Indeed,
the statement becomes wrong if this restriction is dropped. For example, program (28)
in Example 7 is atomic-tight, but the non-stable model considered there satisfies all loop
formulas, including those of singleton loops.
5.1.2 Bounded Formulas and Normal Form
Normal form is another syntactic condition that can be imposed so that the syntactic notion
of boundedness ensures the semantic notion of boundedness. We say that a formula is in
normal form if every strictly positive occurrence of an atom is of the form p(x), where x is
a list of distinct variables. It is clear that every formula can be turned into normal form
using equality.
Proposition 8 If a rectified formula F in normal form is bounded, then F is bounded w.r.t.
any interpretation.
Proposition 9 If a rectified sentence F in normal form is bounded, then for any interpretation I, I is a model of
{LF F (Y ) | Y is a finite first-order loop of F }
iff I is a model of
{LF F (Y ) | Y is a finite loop of F w.r.t. I}.
The following theorem follows from Theorem 2, Proposition 8 and Proposition 9.
12. They actually satisfy a stronger condition called finitely recursive (Bonatti, 2004). See Section 8 for
more details.

146

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

Theorem 5 Let F be a rectified sentence in normal form (possibly containing function
constants of positive arity). If F is bounded, then the following formulas are equivalent to
each other:
(a) SM[F ];
(b) {F }  {LF F (Y ) | Y is a nonempty finite set of atoms of (F )};
(c) {F }  {LF F (Y ) | Y is a finite first-order loop of F }.
Proof. By Proposition 8, if F is bounded then F is bounded w.r.t. any interpretation I.
Then the equivalence between (a) and (b) follows from the equivalence between (a) and (b)
of Theorem 2. The equivalence between (a) and (c) follows from the equivalence between
(a) and (c) of Theorem 2 and by Proposition 9.

Consider a program in normal form
p(x)  x = a, q(a)
q(y)  p(b)

(34)

and an interpretation I such that |I| = {1}, aI = bI = 1 and pI = q I = {1}. This
interpretation does not satisfy Clarks equational theory, and is not a stable model. In
accordance with Theorem 5, I does not satisfy the loop formula of the loop {p(b), q(a)},
which is
p(b)  q(a)  (b = a  q(a)  a 6= a)  (p(b)  b 6= b).
On the other hand, consider another program in non-normal form that has the same
stable models as (34):
p(a)  q(a)
(35)
q(y)  p(b)
Program (35) has a finite complete set of loops, {{p(z)}, {q(z)}}; their loop formulas are
the universal closures of
p(z)  z = a  q(a)
q(z)  p(b)
and I satisfies all loop formulas. This example illustrates the role of normal form assumption
in Theorem 5 (in place of Clarks equational theory in Theorem 4).
Note that a normal form conversion may turn a bounded sentence into a non-bounded
sentence. For instance, the normal form of the bounded program (32) is
p(y)  y = f (x), q(x)
q(x)  p(x), r(x)
p(x)  x = a
r(x)  x = a
r(x)  x = f (a),

(36)

which is not bounded.
Unlike in Corollary 5, if a program is in normal form, atomic-tightness is not more
general than tightness. It is not difficult to check that a program in normal form is atomictight iff it is tight.
147

fiLee & Meng

5.1.3 Decidability of Boundedness and Finite Complete Set of Loops
In general, checking whether F is bounded is not decidable, but it becomes decidable if F
contains no function constants of positive arity. The same is the case for checking whether
F is atomic-tight.
Proposition 10 For any rectified sentence F (allowing function constants of positive arity),
(a) checking whether F is bounded is not decidable;
(b) checking whether F is atomic-tight is not decidable.
If F contains no function constants of positive arity,
(c) checking whether F is bounded is decidable;
(d) checking whether F is atomic-tight is decidable.
The proof of Proposition 10 (c) is based on the following fact and the straightforward
extension of Theorem 2 by Chen et al. (2006) to first-order formulas, which asserts that
checking if F has a finite complete set of loops is decidable.
Proposition 11 For any rectified formula F that contains no function constants of positive
arity, F is bounded iff F has a finite complete set of loops.
Note that Proposition 11 does not hold if F is allowed to contain function constants of
positive arity. For instance,
p(x)  p(f (x))
is not bounded, but has a finite complete set of loops {{p(x)}}.
The following corollary follows from Theorem 4 and Proposition 11.
Corollary 6 Let F be a rectified sentence of a signature  that has no function constants
of positive arity, and let I be an interpretation of  that satisfies F and CET . If F has a
finite complete set of loops, then conditions (a), (b), and (c) of Theorem 4 are equivalent
to each other.
The following corollary follows from Theorem 5 and Proposition 11.
Corollary 7 Let F be a rectified sentence in normal form that has no function constants
of positive arity. If F has a finite complete set of loops, formulas in (a), (b), and (c) of
Theorem 5 are equivalent to each other.
148

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

5.2 Semi-Safe Formulas
Semi-safety is another decidable syntactic condition that ensures that SM[F ] can be expressed by first-order sentences.
We assume that there are no function constants of positive arity. According to Lee,
Lifschitz, and Palla (2009), a semi-safe sentence has the small predicate property: the
relation represented by any of its predicate constants p can hold for a tuple of arguments
only if each member of the tuple is represented by an object constant occurring in F . We
will show that any semi-safe sentence under the stable model semantics can be turned into
a sentence in first-order logic.
First, we review the notion of semi-safety by Lee et al. (2009).13 As a preliminary step,
we assign to every formula F a set RV(F ) of its restricted variables as follows:
 For an atomic formula F ,
 if F is an equality between two variables, then RV(F ) = ;
 otherwise, RV(F ) is the set of all variables occurring in F ;
 RV(G  H) = RV(G)  RV(H);
 RV(G  H) = RV(G)  RV(H);
 RV(G  H) = ;
 RV(QvG) = RV(G) \ {v} where Q  {, }.
We say that a variable x is restricted in F if x  RV(F ). A rectified formula F is semisafe if every strictly positive occurrence of every variable x belongs to a subformula G  H
where x is restricted in G.
If a sentence has no strictly positive occurrence of a variable, then it is obviously semisafe. The FOL-representation of a disjunctive program is semi-safe if, for each rule (10)
of the program, every variable occurring in the head of the rule occurs in B as well.
Example 8 The FOL-representation of (8) is not semi-safe. Formula
p(a)  q(b)  xy((p(x)  q(y))  p(y))
is not semi-safe, while
p(a)  q(b)  xy((p(x)  q(y))  p(y))

(37)

is semi-safe.
For any finite set c of object constants, in c (x) stands for the formula
_
x = c.
cc

13. The definition here is slightly more general in that it does not refer to prenex form. Instead we require
a formula to be rectified.

149

fiLee & Meng

The small predicate property can be expressed by the conjunction of the sentences


^
v1 , . . . , vn p(v1 , . . . , vn ) 
inc (vi )
i=1,...,n

for all predicate constants p occurring in F , where v1 , . . . , vn are distinct variables. We
denote this conjunction of the sentences by SPP c . By c(F ) we denote the set of all object
constants occurring in F .
Proposition 12 (Lee et al., 2009) For any semi-safe sentence F , formula SM[F ] entails
SPP c(F ) .
For example, for the semi-safe sentence (37), SM[(37)] entails


x p(x)  (x = a  x = b))  x(q(x)  (x = a  x = b) .

(38)

The following proposition tells us that for a semi-safe sentence F , formula SM[F ] can
be equivalently rewritten as a first-order sentence.
Theorem 6 Let F be a rectified sentence that has no function constants of positive arity.
If F is semi-safe, then SM[F ] is equivalent to the conjunction of F , SPP c(F ) and a finite
number of first-order loop formulas.
Proof. If F is semi-safe, then SM[F ] entails SPP c(F ) . So it is sufficient to prove that under
the assumption SPP c(F ) , SM[F ] is equivalent to the conjunction of F and a finite number
of first-order loop formulas. It follows from I |= SPP c(F ) that F is bounded w.r.t. I. Since
every finite loop of F w.r.t. I can be represented by a finite set of atoms whose terms are
object variables, it follows from Theorem 2 that I satisfies SM[F ] iff I satisfies the loop
formulas of those sets.

For example, SM[(37)] is equivalent to the conjunction of F , (38) and the universal
closures of
p(z)  z = a  (p(x)  q(z)  z 6= x)
q(z)  z = b
Note that the condition on a finite complete set of loops in Corollaries 6 and 7, and the
condition on semi-safety in Theorem 6 do not entail each other. For instance, formula (37)
is semi-safe, but has no finite complete set of first-order loops, while x p(x) has a finite
complete set of loops {{p(x)}}, but it is not semi-safe. Also program 1 in Section 1 has a
finite complete set of loops, but it is not semi-safe due to w in the fourth rule.

6. Programs with Explicit Quantifiers
In the following we extend the syntax of a logic program by allowing explicit quantifiers. A
rule with quantifiers is of the form
H  G,
(39)
where G and H are first-order formulas such that every occurrence of every implication
in G and H belongs to a negative formula. A program with quantifiers is a finite set of rules
150

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

with quantifiers. Program 1 in Section 1 is an example. The semantics of such a program
is defined by identifying the program with its FOL-representation under the stable model
semantics. By restricting the syntax of a program like the one above, in comparison with
the syntax of an arbitrary formula, we are able to write a more succinct loop formulas, as
we show below.
Let F be a formula and Y a finite set of atoms. By FY we denote the formula obtained
from F by replacing every occurrence
of every atom p(t) in F that does not belong to a
V
negative formula with p(t)  p(t0 )Y t 6= t0 . Let  be a program with quantifiers. Given
a finite set Y of atoms of (), we first rename variables in  so that no variables in 
occur in Y . We define the formula QES  (Y ) (External Support Formula for Programs
with Quantifiers) to be the disjunction of
z(GY  HY )

(40)

for every rule (39) such that H contains a strictly positive occurrence of a predicate constant
that occurs in Y , and z is the list of all free variables in the rule that do not occur in Y .
The loop formula of Y for  is the universal closure of
^
Y  QES  (Y ).
(41)
The following proposition tells us that (41) is equivalent to (12) when the notions are
applied to a program with explicit quantifiers. It also shows that (41) is a generalization of
the definition of a loop formula for a disjunctive program.
Proposition 13 Let  be a program with quantifiers, F the FOL-representation of ,
and Y a finite set of atoms. Under the assumption , formula QES  (Y ) is equivalent to
NES F (Y ). If  is a disjunctive program in normal form, then QES  (Y ) is also equivalent
to ES  (Y ) under the assumption .
Note that the size of (41) for each Y is polynomial to the size of the given program.
This is not the case when we apply (12) to the FOL-representation of the program, due to
the expansion of NES for nested implications. On the other hand, the syntactic condition
imposed on the rule with quantifiers avoids such an exponential blow up, as the following
lemma tells us.
Lemma 4 Let F be a formula such that every occurrence of an implication in F belongs
to a negative formula and let Y be a set of atoms. NES F (Y ) is equivalent to FY .
Proof. By induction on F .



Example 2 (continued) First-Order Loop Formula when  is understood as an
extended program (Using QES  (Y )) : Under the assumption ,
 LF  (Y1 ) is equivalent to the universal closure of
p(u)  (x(q(x)  (p(x)  x 6= u))  z(r(z)  (p(z)  z 6= u))).
151

fiLee & Meng

 LF  (Y2 ) is equivalent to the universal closure of
q(u)  y(p(y)  (q(y)  y 6= u)).
 LF  (Y3 ) is equivalent to the universal closure of
r(u)  .
 LF  (Y4 ) is equivalent to the universal closure of
(p(u)  q(u))  (x((q(x)  x 6= u)  (p(x)  x 6= u))
 y((p(y)  y 6= u)  (q(y)  y 6= u))
 z(r(z)  (p(z)  z 6= u))).

A finite set  of sentences entails a sentence F under the stable model semantics (symbolically,  |=SM F ), if every stable model of  satisfies F .
If SM[F ] can be reduced to a first-order sentence, as described in Theorem 5 and Theorem 6, then
 |=SM F iff    |= F,
where  is the set of first-order loop formulas required (and possibly including SPP c(F )
when Theorem 6 is applied). This fact allows us to use first-order theorem provers to reason
about query entailment under the stable model semantics.
Example 9 Consider program 1 in Section 1, which has the following finite complete
set of loops: {Man(u)}, {Spouse(u, v)}, {HasWife(u)}, {Married (u)}, {Accident(u, v)},
{Discount(u, v)}, and {HasWife(u), Married (u)}. Their loop formulas for 1  2  3 are
equivalent to the universal closure of

Man(u)   Man(John)  John 6= u ;

Spouse(u, v)   y Spouse(John, y)  (John, y) 6= (u, v) ;

HasWife(u)  x y Spouse(x, y)  (HasWife(x)  x 6= u)

 x Man(x)  Married (x)  (HasWife(x)  x 6= u) ;

Married (u)  x Man(x)  HasWife(x)  (Married (x)  x 6= u) ;
Accident(u, v)  ;
Discount(u, v) 

x Married (x)  z Accident(x, z)  (w(Discount(x, w)  (x, w) 6= (u, v))) ;
Married (u)  HasWife(u) 

x y Spouse(x, y)  (HasWife(x)  (x 6= u))

 x Man(x)  Married (x)  x 6= u  (HasWife(x)  x 6= u)
 x Man(x)  HasWife(x)  x 6= u  (Married (x)  x 6= u) .
152

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

These loop formulas, conjoined with the FOL-representation of 1  2  3 , entail under
first-order logic each of x Married (x) and xy(Discount(x, y)  x = John). We verified
the answers using a first-order theorem prover Vampire 14 .

7. Extension to Allow Extensional Predicates
The definition of a stable model in the journal paper by Ferraris et al. (2011), reviewed in
Section 2, is more general than the definition in their conference paper (Ferraris et al., 2007)
in that it allows us to distinguish between intensional and non-intensional (a.k.a. extensional) predicates. Similar to Datalog, intensional (output) predicates are characterized in
terms of extensional (input) predicates. For instance, consider Example 9 again, but now assume that Man and Spouse are non-intensional. 1  2  3 still entails xyDiscount(x, y)
but no longer entails xy(Discount(x, y)  x = John) because there may be a person other
than John who has a spouse.
The results in the earlier sections can be extended to this general semantics in view of
Proposition 14 below, which characterizes SM[F ; p] in terms of SM[F ]. By pr (F ) we denote
the list of all predicate constants occurring in F ; by Choice(p) we denote the conjunction
of choice formulas x(p(x)  p(x)) for all predicate constants p in p, where x is a list of
distinct object variables; by False(p) we denote the conjunction of xp(x) for all predicate
constants p in p. We sometimes identify a list with the corresponding set when there is no
confusion.
Proposition 14 For any list p of predicate constants, formula SM[F ; p] is equivalent to
SM[F  Choice(pr (F )\p)  False(p\pr (F ))]

(42)

SM[F   Choice(pr (F )\p)  False(p\pr (F ))],

(43)

and to
where F  is obtained from F by replacing every atom of the form q(t) in F such that q
does not belong to p by q(t).
This proposition allows us to extend the results established for SM[F ] to SM[F ; p]. For
instance, Theorem 3 can be extended to SM[F ; p] by first rewriting it into the form SM[G],
where G is
F   Choice(pr (F )\p)  False(p\pr (F )).
(44)
In the next three corollaries,  is a signature, F is a rectified sentence of  (possibly
containing function constants of positive arity), p is any finite list of predicate constants
from , and G is (44).
The first corollary follows from Theorem 2 and Proposition 14.
Corollary 8 For any interpretation I of  that satisfies F , if G is bounded w.r.t. I, then
the following conditions are equivalent to each other:
(a) I |= SM[F ; p];
14. http://www.vampire.fm .

153

fiLee & Meng

(b) for every nonempty finite set Y of atoms formed from predicate constants in p and
object names for |I|, I satisfies LF F (Y );
(c) for every finite loop Y of G w.r.t. I whose predicate constants are contained in p, I
satisfies LF F (Y ).
The next corollary follows from Theorem 4 and Proposition 14.
Corollary 9 If G is bounded, then, for any interpretation I of  that satisfies F and CET ,
the following conditions are equivalent to each other:
(a) I |= SM[F ; p];
(b) for every nonempty finite set Y of atoms of (G) whose predicate constants are contained in p, I satisfies LF F (Y );
(c) for every finite first-order loop Y of G whose predicate constants are contained in p,
I satisfies LF F (Y ).
The last corollary follows from Theorem 5 and Proposition 14.
Corollary 10 If G is in normal form and is bounded, then the following formulas are
equivalent to each other:
(a) SM[F ; p];
(b) {F }  {LF F (Y ) | Y is a nonempty finite set of atoms of (G) whose predicate
constants are contained in p};
(c) {F }  {LF F (Y ) | Y is a finite first-order loop of G whose predicate constants are
contained in p}.
Example 10 Consider Example 9 again, assuming that Man and Spouse are extensional.
Let F be the FOL-presentation of 1  2  3 and let G be the formula (44). The loops
of G are the same as the loops of F . The loop formulas remain the same as before except
for the following loop formulas of Man(u) and Spouse(u, v):


Man(u)   Man(John)  John 6= u  x  (Man(x)  x 6= u)  Man(x) ;

Spouse(u, v)   y Spouse(John, y)  (John, y) 6= (u, v) 

xy  (Spouse(x, y)  (x, y) 6= (u, v))  Spouse(x, y) .
These two formulas are tautologies. As a result, the loop formulas of all loops, conjoined
with G, entail xyDiscount(x, y), but no longer entail xy (Discount(x, y)  x = John).
In general, there are no loops of G that contain both intensional and extensional predicates. Also every loop of G that contains an extensional predicate is a singleton, and the
loop formula of such a loop is a tautology.
154

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

Corollary 3 is extended to allow extensional predicates as in the following. By SLF[F ; p],
we denote
{LF F ({p(x)}) | p is a predicate constant in p, and x is a list
of distinct object variables whose length is the same as the arity of p}.
We say that a formula F is p-atomic-tight w.r.t. I if every infinite path in the dependency
graph of F w.r.t. I whose vertices are satisfied by I contains an atom whose predicate
constant is not in p.
Corollary 11 Let F be a rectified sentence (possibly containing function constants of positive arity), and let I be a model of F . If F is p-atomic-tight w.r.t. I, then I satisfies
SM[F ; p] iff I satisfies SLF[F ; p].
The definition of semi-safety is extended to distinguish between intensional and nonintensional predicates as follows. Let F be a formula that has no function constants of
positive arity. To every first-order formula F we assign a set RVp (F ) of restricted variables
relative to p as follows.
 For an atomic formula F (including equality and ),
 if F is an equality between two variables, or is an atom whose predicate constant
is not in p, then RVp (F ) = ;
 otherwise, RVp (F ) is the set of all variables occurring in F ;
 RVp (G  H) = RVp (G)  RVp (H);
 RVp (G  H) = RVp (G)  RVp (H);
 RVp (G  H) = .
 RVp (QvG) = RVp (G) \ {v} where Q  {, }.
We say that a variable x is p-restricted in F if x  RVp (F ). A rectified formula F is
semi-safe relative to p if every strictly positive occurrence of every variable x belongs to a
subformula G  H, where x is p-restricted in G.
The small predicate property is generalized as follows. Formula SPP pc is the conjunction
of the sentences


^
v1 , . . . , vn p(v1 , . . . , vn ) 
inc (vi )
i=1,...,n

for all predicate constants p in p, where v1 , . . . , vn are distinct variables.
Proposition 15 (Lee et al., 2009) For any semi-safe sentence F relative to p, formula
SM[F ; p] entails SPP pc(F ) .
The following proposition tells us that for a semi-safe sentence F , formula SM[F ; p] can
be equivalently rewritten as a first-order sentence.
155

fiLee & Meng

Theorem 7 Let F be a rectified sentence that has no function constants of positive arity.
If F is semi-safe relative to p, then SM[F ; p] is equivalent to the conjunction of F , SPP pc(F )
and a finite number of first-order loop formulas.
Proof. Let F be a sentence of the signature . If F is semi-safe relative to p, then SM[F ; p]
entails SPP pc(F ) , so it is sufficient to prove that under the assumption SPP pc(F ) , SM[F ; p]
is equivalent to the conjunction of F and a finite number of first-order loop formulas. By
Proposition 14, SM[F ; p] is equivalent to SM[G], where G is (44). Consider any interpretation I of  that satisfies G and SPP pc(F ) . Note that the dependency graph of G w.r.t. I
contains no outgoing edges from a vertex whose predicate constant does not belong to p.
Together with the fact that I |= SPP pc(F ) , we conclude that each path in the dependency
graph whose vertices are satisfied by I visits only finitely many vertices. Consequently, G
is bounded w.r.t. I. Since every finite loop of G w.r.t. I can be represented by a finite set
of atoms whose terms are object variables, it follows from Theorem 2 that I satisfies SM[G]
iff I satisfies the loop formulas of those sets.


8. Related Work
The notion of a bounded program is related to the notion of a finitely recursive program
studied by Bonatti (2004), where a different definition of a dependency graph was considered. The atom dependency graph of a nondisjunctive ground program defined by Bonatti
is a directed graph such that the vertices are the set of ground atoms, and the edges go
from the atom in the head to atoms in the body of every rule, including those in the negative body. A program is called finitely recursive if, for every atom, there are only finitely
many atoms reachable from it in the atom dependency graph. It is clear that every finitely
recursive program is bounded, but the converse does not hold. For instance, the program
p(x)  not p(f (x))
is bounded, but is not finitely recursive because there are infinite paths that involve negative
edges. Also the program
p(a)  q(f (x))
is bounded, but is not finitely recursive because infinitely many atoms q(f (a)), q(f (f (a))), . . .
can be reached from p(a) in the atom dependency graph. Like bounded programs, checking
finitely recursive programs is undecidable in the presence of function constants of positive
arity.
Lin and Wang (2008) extended answer set semantics with functions by extending the
definition of a reduct, and also provided loop formulas for such programs. We can provide
an alternative account of their results by considering the notions there as special cases of
the definitions presented in this paper. For simplicity, we assume non-sorted languages.15
Essentially, they restricted attention to a special case of non-Herbrand interpretations such
that object constants form the universe, and ground terms other than object constants are
mapped to object constants. According to Lin and Wang, an LW-program P consists of
15. Lin and Wang (2008) consider essentially many-sorted languages. The result of this section can be
extended to that case by considering many-sorted SM (Kim, Lee, & Palla, 2009).

156

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

type definitions and a set of rules. Type definitions introduce the domains for a many-sorted
signature consisting of some object constants, and includes the evaluation of each function
symbol of positive arity that maps a list of object constants to an object constant. Since we
assume non-sorted languages, we consider only a single domain (universe). We say that an
interpretation I is a P -interpretation if the universe is the set of object constants specified
by P , object constants are evaluated to itself, and ground terms other than object constants
are evaluated conforming to the type definitions of P .
Proposition 16 Let P be an LW-program and let F be the FOL-representation of the set
of rules in P . The following conditions are equivalent to each other:
(a) I is an answer set of P according to Lin and Wang (2008);
(b) I is a P -interpretation that satisfies SM[F ];
(c) I is a P -interpretation that satisfies F and the loop formulas of Y for all loops Y of
F w.r.t. I.
The equivalence between (b) and (c) follows from Proposition 2 since the universe is
finite. The equivalence between (a) and (c) follows from the fact that LW answer sets can
be characterized by loop formulas that are defined by Lin and Wang (2008) and that these
loop formulas are essentially the same as the loop formulas in (c).
Since the proposal of the first-order stable model semantics, there have been some papers
about first-order definability of SM[F ]. Zhang and Zhou (2010) show that, for a nondisjunctive program  that has no function constants of positive arity, its first-order stable
model semantics can be reformulated by a progression based semantics. They also showed
that the programs whose answer sets can be found by a finite progression are exactly those
that can be represented by first-order formulas. Some researchers have paid special attention to first-order definability of SM[F ] on finite structures. Chen, Zhang, and Zhou (2010)
show a game-theoretic characterization for the first-order indefinability of first-order answer
set programs on finite structures. Asuncion, Lin, Zhang, and Zhou (2010) show first-order
definability on finite structures by turning programs into modified completion using new
predicates to record levels. Chen, Lin, Zhang, and Zhou (2011) present a condition called
loop-separable, which is more refined than finite complete set of loops under which the
finite answer sets of a program can be captured by first-order sentences. However, like the
condition of finite complete set of loops, this condition is disjoint with semi-safety. The
following program is semi-safe but not loop-separable:
p(x)  p(y), q(x, y).
However, all this work is limited to nondisjunctive programs that contain no function constants of positive arity. Our work is not limited to finite structures, and considers function
constants of positive arity as well. Nonetheless the above papers on first-order definability
are closely related to our work and more insights would be gained from the relationship
between them.
The use of first-order theorem provers for the stable model semantics was already investigated by Sabuncu and Alpaslan (2007), but their results are limited in several ways. They
157

fiLee & Meng

considered nondisjunctive logic programs with trivial loops only, in which case the stable
model semantics is equivalent to the completion semantics. They also restricted attention
to Herbrand models.

9. Conclusion
This paper puts first-order loop formulas in the context of first-order reasoning and studies
how they are related to first-order stable model semantics. The similarities and mismatches
found in this paper provide useful insights into first-order reasoning with stable models.
Future work is to find further restrictions that make first-order stable model reasoning
decidable and computable in an efficient manner, like the conditions imposed in finitary
programs (Bonatti, 2004). Recently, the first-order stable model semantics was shown to
be used as a unifying nonmonotonic logic for integrating rules and ontologies (de Bruijn,
Pearce, Polleres, & Valverde, 2010; Lee & Palla, 2011), in which ontology predicates are
identified with extensional predicates. Based on the studied relationship between first-order
stable model semantics and first-order loop formulas, one may find further restrictions that
are tailored to the hybrid knowledge bases for efficient computation.

Acknowledgments
We are grateful to Joseph Babb, Michael Bartholomew, Piero Bonatti, Vladimir Lifschitz,
and Ravi Palla for useful discussions, and to the anonymous referees for their useful comments. The authors were partially supported by the National Science Foundation under
Grant IIS-0916116 and by the IARPA SCIL program.

Appendix A. Proofs
The proofs are presented in the order of dependencies. Theorem 3 is the main theorem.
The proof of Theorem 2 uses Theorem 3. The proofs of Theorems 4 and 5 follow from
Theorem 2. The proof of Lemma 1 follows from Proposition 13.
In the following, unless otherwise noted, F is a rectified first-order sentence, p is the list
of distinct predicate constants p1 , . . . , pn occurring in F , symbols u, v are lists of distinct
predicate variables of the same length as p, and symbols q, r are lists of distinct predicate
names of the same length as p.
A.1 Proof of Theorem 3
Theorem 3
other:

For any rectified sentence F , the following sentences are equivalent to each

(a) SM[F ];
(b) F  u((u  p)  Nonempty(u)  NSES F (u));
(c) F  u((u  p)  Ext-Loop F (u)  NSES F (u)).
158

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

The notation that we use in the proof involves predicate expressions (Lifschitz, 1994,
Section 3.1) of the form
xF (x),
(45)
where F (x) is a formula. If e is (45) and G(p) is a formula containing a predicate constant
p of the same arity as the length of x then G(e) stands for the result of replacing each
atomic part of the form p(t) in G(p) with F (t), after renaming the bound variables in G(p)
in the usual way, if necessary. For instance, if G(p) is p(a)  p(b) then G(y(x = y)) is
x = a  x = b. Substituting a tuple e of predicate expressions for a tuple p of predicate
constants is defined in a similar way.
Lemma 5 Let v be the list of yi (pi (yi )  ui (yi )). The following formulas are logically
valid:
 u  p  (F  (u)  NSES F (v));
 u  p  (F  (v)  NSES F (u)).
Proof. By induction.
A.1.1 Proof of Equivalence between (a) and (b) of Theorem 3
It is sufficient to show that
u(u < p  F  (u))
is equivalent to
v(v  p  Nonempty(v)  NSES F (v)).
From left to right: Take u such that u < pF  (u). Let v be the list of yi (pi (yi )  ui (yi )).
 Clearly, v  p holds.
 From
u < p, it follows that there are x and i such that pi (x)  ui (x), from which
W
i
i
i x vi (x ) follows, so that Nonempty(v) follows.
 By Lemma 5, NSES F (v) follows from u < p and F  (u).
From right to left: Take v such that v  p  Nonempty(v)  NSES F (v). Let u be the list
of yi (pi (yi )  vi (yi )).
 Clearly, u  p holds. Moreover (u = p) holds. Indeed, if u = p, then xi vi (xi )
follows, which contradicts the assumption Nonempty(v). Consequently, u < p follows.
 By Lemma 5, F  (u) follows from v  p and NSES F (v).

159

fiLee & Meng

A.1.2 Proof of Proposition 3
Lemma 6 Let I be an interpretation of  that contains (F ), and let q, r be lists of
predicate names corresponding to p. Let Z and Y be sets of atoms in the dependency graph
of F w.r.t. I such that
pi (  )  Y iff I |= qi (  )
and
pi (  )  Z iff I |= ri (  ),
where   is a list of object names. Then
I |= r  q  EF (r, q)
iff Z is a subset of Y and there is an edge from an atom in Z to an atom in Y \ Z in the
dependency graph of F w.r.t. I.
Proof. From left to right: Assume I |= r  q  EF (r, q). The fact that Z is a subset of
Y follows from the assumption that I |= r  q and the construction of Z and Y . Since
_
I |=
z(ri (t)  qj (t0 )  rj (t0 )),
(pi (t),pj (t0 )) : pi (t) depends on pj (t0 )
in a rule of F

where z is the list of all object variables in t and t0 , there is a substitution  that maps
object variables in t and t0 to object names such that
_
I |=
ri (t)  qj (t0 )  rj (t0 ).
(pi (t),pj (t0 )) : pi (t) depends on pj (t0 )
in a rule of F

Consequently, there are atoms pi (t), pj (t0 ) such that pi (t) depends on pj (t0 ) in a rule of F
and I |= ri (t)qj (t0 )rj (t0 ). From I |= ri (t) and the construction of Z, it follows that
pi (((t)I ) ) belongs to Z. Also from I |= qj (t0 )  rj (t0 ), it follows that that pj (((t0 )I ) )
belongs to Y \ Z. Therefore, there is an edge from an atom in Z to an atom in Y \ Z in the
dependency graph of F w.r.t. I.
From right to left: Assume that Z is a subset of Y and there is an edge from an atom
pi (  ) in Z to an atom pj (  ) in Y \ Z in the dependency graph of F w.r.t. I. Clearly,
I |= r  q.
From the assumption that pi (  )  Z, pj (  )  Y \ Z and the construction of Y and Z,
it follows that I |= ri (  )  qj (  )  rj (  ). From the definition of the dependency graph
w.r.t. I, it follows that there are pi (t), pj (t0 ) such that pi (t) depends on pj (t0 ) in a rule of
F with a substitution  that maps object variables in t and t0 to object names such that
(t)I =  and (t0 )I = .
Consequently,
_
I |=
ri (t)  qj (t0 )  rj (t0 ),
(pi (t),pj (t0 )) : pi (t) depends on pj (t0 )
in a rule of F

160

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

which is equivalent to saying that
_
I |=

z(ri (t)  qj (t0 )  rj (t0 )),

(pi (t),pj (t0 )) : pi (t) depends on pj (t0 )
in a rule of F

where z is the list of all variables in t and t0 .



Lemma 7 A graph (V, E) is strongly connected iff, for any nonempty proper subset U of
V , there is an edge from U to V \ U .
Proof. Follows from the definition of a strongly connected graph.



Proposition 3 Let q be a list of predicate names corresponding to p, and let Y be a set
of atoms in the dependency graph of F w.r.t. I such that
pi (  )  Y iff I |= qi (  ),
where   is a list of object names. Then I |= Loop F (q) iff Y is a loop of F w.r.t. I.
Proof. From left to right: Assume that I |= Loop F (q). From I |= Nonempty(q), it
follows that Y is nonempty.
Take any nonempty proper subset Z of Y . Let r be the list of predicate names such
that
I |= ri (  ) iff pi (  )  Z.
It is clear that
I |= Nonempty(r)  r < q.
Consequently, from I |= Loop F (q), it follows that I |= E F (r, q). By Lemma 6, there is an
edge from an atom in Z to an atom in Y \ Z. Consequently, by Lemma 7, Y induces a
strongly connected subgraph and thus a loop of F w.r.t. I.
From right to left: Let Y be loop of F w.r.t. I and q a list of predicate names such that
I |= qi (  ) iff pi (  )  Y.
Since Y is nonempty, I |= Nonempty(q).
Consider any list of predicate names r such that
I |= Nonempty(r)  r < q.
Let Z be a set of vertices in the dependency graph of F w.r.t. I such that
pi (  )  Z iff I |= ri (  ).
Clearly, Z is a nonempty proper subset of Y . Since Y induces a strongly connected subgraph,
by Lemma 7, there is an edge from an atom in Z to an atom in Y \ Z. Consequently by
Lemma 6, I |= EF (r, q).


161

fiLee & Meng

A.1.3 Proof of Proposition 4
Proposition 4 Let q be a list of predicate names corresponding to p, and let Y be a set
of atoms in the dependency graph of F w.r.t. I such that
pi (  )  Y iff I |= qi (  ),
where   is a list of object names. Then
I |= Nonempty(q)  v((v  q)  Loop F (v)  EF (v, q))
iff Y is an unbounded set of F w.r.t. I.
Proof. From left to right: Assume
I |= Nonempty(q)  v(v  q  Loop F (v)  EF (v, q)).

(46)

Since I |= Nonempty(q), it is clear that Y is nonempty.
Take any subset Z of Y that is a loop of F w.r.t. I. Let r be a list of predicate names
such that
I |= ri (  ) iff pi (  )  Z.
Since Z is a subset of Y , it is clear that I |= r  q. Since Z is a loop of F w.r.t. I, by
Proposition 3, I |= Loop F (r). Consequently, from (46) it follows that I |= E F (r, q). By
Lemma 6, there is an edge from an atom in Z to an atom in Y \ Z. Therefore, Y is an
unbounded set of F w.r.t. I.
From right to left: Let Y be an unbounded set of F w.r.t. I. Since Y is nonempty, it is
clear that I |= Nonempty(q).
Take any list of predicate names r such that I |= r  q  Loop F (r). Let Z be a set of
vertices in the dependency graph of F w.r.t. I such that
pi (  )  Z iff I |= ri (  ).
By Proposition 3, Z is a loop of F w.r.t. I. It is clear that Z is a subset of Y . Since Y is an
unbounded set of F w.r.t. I, there is an edge from Z to Y \ Z. Consequently by Lemma 6,
I |= EF (r, q).

A.1.4 Proof of Proposition 5
Proposition 5 For any negative formula F , formula
NSES F (u)  F
is logically valid.
Proof. The proof follows immediately from the following two lemmas, which can be proved
by induction.


162

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

Lemma 8 For any formula F ,
NSES F (u)  F
is logically valid.
Lemma 9 Let F be a formula, and let SF be the set of pi (t) that has a strictly positive
occurrence in F . Formula
^
F
zvi (t)  NSES F (v)
(47)
pi (t)SF

is logically valid, where z is the tuple of variables in t that are not free in F .
A.1.5 Proof of Equivalence between (b) and (c) of Theorem 3
Lemma 10 Let F be a rectified formula, let SF+ be the set of all atoms pi (t) that have a
positive occurrence in F that does not belong to a negative formula, and let SF be the set
of all atoms pi (t) that have a negative occurrence in F that does not belong to a negative
formula.16 The following formulas are logically valid, where z is the list of all variables in
t that are not free in F .
V
(a) (v  u)  pi (t)S + z(ui (t)  vi (t))  NSES F (v)  NSES F (u);
F
V
(b) (v  u)  pi (t)S  z(ui (t)  vi (t))  NSES F (u)  NSES F (v).
F

Proof. Both parts are proved simultaneously by induction on F .
Case 1: F is an atom pi (t).
Part (a): NSES F (v) entails NSES F (u) under the assumption
^
z(ui (t)  vi (t)).
+
pi (t)SF

Part (b): NSES F (u) entails NSES F (v) under the assumption v  u.
Case 2: F is  or an equality. It is clear since NSES F (v) and NSES F (u) are the same as
F.
Case 3: F is G  H or G  H. Follows from I.H.
Case 4: F is G  H.
Part (a): Assume
(v  u) 

^

z(ui (t)  vi (t)).

(48)

+
pi (t)SF

We need to show that
(NSES G (v)  NSES H (v))  (G  H)
16. Note that we distinguish between formula being negative and an occurrence being negative. See at the
end of Section 2.

163

fiLee & Meng

entails
(NSES G (u)  NSES H (u))  (G  H).
Note that
^

z(ui (t)  vi (t))


pi (t)SG

and
^

z(ui (t)  vi (t))

+
pi (t)SH

are entailed by formula (48). By I.H., NSES G (u) entails NSES G (v) and NSES H (v) entails
NSES H (u).
Part (b): Similar to Part (a).
Case 5: F is x G
Part (a): Assume
^

(v  u) 

z(ui (t)  vi (t))  xNSES G (v).

+
pi (t)SF

From the assumption NSES G (v), G follows by Lemma 8. Also
^
z0 (ui (t)  vi (t))
+
pi (t)SG

follows, where z0 is the list of all variables in t that are not free in G, so that by I.H. on G,
NSES G (u) holds from the assumption. Since x is not free in the assumption, xNSES G (u)
holds as well.
Part (b): Similar to Part (a).
Case 6: F is x G.
Part (a): Assume
^

(v  u) 

z(ui (t)  vi (t))  xNSES G (v).

(49)

z(ui (t)  vi (t))  NSES G (v).

(50)

+
pi (t)SF

Take x such that
(v  u) 

^
+
pi (t)SF

From NSES G (v), by Lemma 8, G follows. Also
^
z0 (ui (t)  vi (t))
+
pi (t)SG

follows, where z0 is the list of all variables in t that are not free in G. By I.H. on G,
NSES G (u) holds under the assumption (50). Consequently, xNSES G (u) holds from the
164

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

same assumption. Since x is not free in (49), we conclude that xNSES G (u) holds from
the assumption (49).
Part (b): Similar to Part (a).



Lemma 11 For any rectified formula F ,
(v  u)  EF (v, u)  NSES F (u)  NSES F (v)
is logically valid.
Proof. By induction on F .
Case 1: F is an atom pi (t). NSES F (u) entails NSES F (v) under the assumption v  u.
Case 2: F is  or equality. It is clear since NSES F (v) and NSES F (u) are the same as F .
Case 3: F is G  H or G  H. Follows from I.H.
Case 4: F is G  H. Assume
(v  u)  EF (v, u)  NSES F (u)
and NSES G (v). From NSES F (u), by Lemma 8, we conclude G  H. From NSES G (v),
by Lemma 8, G follows, and consequently H.
Assume NSES H (v) for the sake of contradiction. By Lemma 9, from H and NSES H (v),
it follows that
_
xvi (t)
(51)
pi (t) : pi (t) occurs strictly positively in H

, where x is the list of variables in t that are not free in H.
Since F is rectified, the variables in F can be partitioned into three sets: the list of
variables x that are not free in H, the list of variables y that are not free in G, and the
rest. Note that EF (v, u) entails


^
xvi (t)  y(uj (t0 )  vj (t0 )) ,
(52)
(pi (t),pj (t0 )) : pi (t) depends on pj (t0 ) in a rule GH in F
pi (t) occurs in H,pj (t0 ) occurs in G

where x is the list of all variables in t that are not free in H, and y is the list of all variables
in t0 that are not free in G. From (51) and (52), we conclude
^
y(uj (t0 )  vj (t0 )).
pj (t0 ) : pj (t0 ) occurs positively and not in a negative subformula of G

From this, together with the assumption (v  u) and NSES G (v), by Lemma 10 (a),
NSES G (u) follows. Thus NSES H (u) follows from NSES F (u) and NSES G (u). Since E F (v, u)
entails E H (v, u), by I.H. on H, NSES H (v) follows, which contradicts the assumption.
Case 5: F is xG or xG. Follows from I.H.
165



fiLee & Meng

Lemma 12
Nonempty(u)  v(v  u  Ext-Loop F (v)  EF (v, u))
is logically valid.
Proof. Take any list q of predicate names, and any interpretation I that satisfies Nonempty(q).
Let Y be a set of vertices in the dependency graph of F w.r.t. I such that
pi (  )  Y iff I |= qi (  ).
Consider the subgraph G of the dependency graph of F w.r.t. I that is induced by Y . If Y
is an unbounded set w.r.t. I, by Proposition 4, I |= Ext-Loop F (q). So
I |= q  q  Ext-Loop F (q)  EF (q, q).
Otherwise, consider the graph G0 that is obtained from G by collapsing strongly connected
components of G, i.e., the vertices of G0 are the strongly connected components of G and
G0 has an edge from V to V 0 if G has an edge from a vertex in V to a vertex in V 0 . Since
we assumed that Y is not an unbounded set w.r.t. I, there exists a vertex Z in G0 that has
no outgoing edges. Consider the list of predicate names r such that
I |= ri (  ) iff pi (  )  Z.
It is clear that I |= r  q. By Proposition 3, I |= Loop F (r) thus I |= Ext-Loop F (r). Since
there is no edge from Z to Y \ Z, by Lemma 6, I |= EF (r, q). Consequently, the claim
follows.

Proof of Equivalence Between (b) and (c) of Theorem 3
From (b) to (c):
valid.

Clear from that the formula Ext-Loop F (u)  Nonempty(u) is logically

From (c) to (b): Assume
F  v(v  p  Ext-Loop F (v)  NSES F (v)).
Take any u such that u  p  Nonempty(u). By Lemma 12, it follows from Nonempty(u)
that there exists v such that v  u  Ext-Loop F (v)  EF (v, u). It is clear that v  p
follows from v  u and u  p. It follows from the assumption that NSES F (v). Then by
Lemma 11, NSES F (u) follows from v  u and EF (v, u).

A.2 Proof of Theorem 2
Lemma 3 Let F be a rectified sentence of a signature  (possibly containing function
constants of positive arity), and let I be an interpretation of  that satisfies F . If F is
bounded w.r.t. I,
I |= u(u  p  Ext-Loop F (u)  NSES F (u))
166

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

iff there is a finite loop Y of F w.r.t. I such that
^

I |=
Y  NES F (Y ) .
Proof. From left to right: Assume
I |= q  p  Ext-Loop F (q)  NSES F (q)
for some list of predicate names q. Consider Y to be the set of vertices in the dependency
graph of F w.r.t. I such that
pi (  )  Y iff I |= qi (  ).
Since I |= Ext-Loop F (q), by Proposition 3 and Proposition 4, it follows that Y is an
extended loop of F w.r.t. I. Since I |= qi (  ) for all pi (  )  Y and I |= q  p, it follows
that I satisfies every atom in Y . Together with the assumption that F is bounded w.r.t. I,
this implies that set Y is finite. Since I |= NSES F (q) and Y is finite, by Lemma 2, it
follows that I |= NES F (Y ).
From right to left: Consider any finite loop Y of F w.r.t. I. Assume
^
I |=
Y  NES F (Y ).
Let q be a list of predicate names such that
I |= qi (  ) iff pi (  )  Y.
 I |= q  p follows from the construction of q and I |=

V

Y.

 Since Y is a loop of F w.r.t. I, by Proposition 3, I |= Loop F (q), and consequently,
I |= Ext-Loop F (q).
 From I |= NES F (Y ), by Lemma 2, I |= NSES F (q).
Consequently, I |= u(u  p  Ext-Loop F (u)  NSES F (u)).



Theorem 2 Let F be a rectified sentence of a signature  (possibly containing function
constants of positive arity), and let I be an interpretation of  that satisfies F . If F is
bounded w.r.t. I, then the following conditions are equivalent to each other:
(a) I satisfies SM[F ];
(b) for every nonempty finite set Y of atoms formed from predicate constants in (F ) and
object names for |I|, I satisfies LF F (Y );
(c) for every finite loop Y of F w.r.t. I, I satisfies LF F (Y ).
Proof. Between (a) and (c): By Theorem 3 and Lemma 3.
Between (b) and (c):
167

fiLee & Meng

 From (b) to (c): Clear.
 From (c) to (b): Assume that I satisfies LF F (L) for every finite loop L of F w.r.t I.
Consider any nonempty finite set V
Y of atoms formed from predicate constants in (F )
and object names such that I |= Y . Let q be a list of predicate names such that
I |= qi (  ) iff pi (  )  Y.
Since Y is nonempty, it is clear that Nonempty(q) follows. In view of Lemma 12,
there is a list of predicate names r such that
I |= r  q  Ext-Loop F (r)  EF (r, q).

(53)

Consider Z to be the set of vertices in the dependency graph of F w.r.t. I such that
pi (  )  Z iff I |= ri (  ).
Since I |= Ext-Loop F (r), by Proposition
3 and PropositionV4, Z is an extended loop
V
of F w.r.t. I. Clearly, I |= Z since Z  Y and I |= Y . Since F is bounded
w.r.t. I, and Z is satisfied by I, it follows that Z is a finite loop of F w.r.t. I.
Since I |= r  q  EF (r, q), Z is a subset of Y and, by Lemma 6, there is no edge
from Z to Y \ Z in the dependency graph of F w.r.t. I. Since I |= LF F (Z), we
conclude that I |= NES F (Z), and by Lemma 2, I |= NSES F (r). From (53) and
that I |= NSES F (r), by Lemma 11, we have I |= NSES F (q). By Lemma 2 again,
I |= NES F (Y ). Consequently, I |= LF F (Y ).

A.3 Proof of Proposition 6
Proposition 6
If a rectified formula F of a signature  is bounded, then F is bounded
w.r.t. any interpretation of  that satisfies CET .
Lemma 13 For any terms t1 and t2 of signature , any interpretation I that satisfies
CET , and any substitution  from object variables in t1 and t2 to object names such that
(t1 )I = (t2 )I , Robinsons unification algorithm (Robinson, 1965), when applied to t1 and
t2 , returns a most general unifier (mgu)  of t1 and t2 such that
(a) t1  = t2 , and
(b) for every variable x in t1 or t2 , (x)I = (x)I .
Proof. From the assumptions, by Lemma 5.1 from the work of Kunen (1987), t1 and t2
are unifiable, in which case Robinsons algorithm returns a mgu for t1 and t2 that maps
variables occurring in t1 and t2 into terms. Given this, part (b) can be proven by induction.

The proof of Proposition 6 follows from the following lemma.
168

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

Lemma 14 Let F be a rectified sentence of a signature , and let I be an interpretation of
 that satisfies CET . For any path
hp1 ( 1 ), p2 ( 2 ), . . . , pk ( k ), pk+1 ( k+1 )i

(54)

in the dependency graph of F w.r.t I, there is a path
hp1 (u1 ), p2 (u2 ), . . . , pk (uk ), pk+1 (uk+1 )i
in the first-order dependency graph of F with a substitution  that maps object variables in
ui to object names such that (ui )I =  i for all i.
Proof. Each edge (pi ( i ), pi+1 ( i+1 )) in (54) is obtained from a pair of atoms (pi (ti ), pi+1 (t0i ))
and a substitution i such that pi (ti ) depends on pi+1 (t0i ) in a rule of F , and
(t1 1 )I =  1 , (t0i i )I = (ti+1 i+1 )I =  i+1 (1  i < k), (t0k k )I =  k+1 .

(55)

For simplicity we assume that each pair (pi (ti ), pi+1 (t0i )) considered above has no common
variables with another pair by first renaming variables. This allows us to use one substitution  = 1 . . . k in place of individual i in the rest of the proof.
We will show by induction that, for each j where j  {1 . . . k}, there are substitutions
j
i (1  i  j) from variables in ti and t0i to terms such that
(a) hp1 (t1 )1j , p2 (t2 )2j , . . . , pj (tj )jj , pj+1 (t0j )jj i is a path in the first-order dependency
graph of F , and
(b) (ti ij )I =  i for all 1  i  j, and (t0j jj )I =  j+1 .
When j = 1, we take ij to be an identity substitution. Clearly, conditions (a) and (b)
are satisfied.
Otherwise, by I.H. we assume that, for some j in {1, . . . , k1}, there are substitutions
1j , . . . , jj such that conditions (a) and (b) are satisfied. We will prove that there are
substitutions ij+1 (1  i  j +1) from variables in ti and t0i to terms such that
j+1
j+1
(a) hp1 (t1 )1j+1 , p2 (t2 )2j+1 , . . . , pj+1 (tj+1 )j+1
, pj+2 (t0j+1 )j+1
i is a path in the first-order dependency graph of F , and
j+1 I
(b) (ti ij+1 )I =  i for all 1  i  j +1, and (t0j+1 j+1
) =  j+2 .

From I.H., we have (t0j jj )I =  j+1 and from (55) we have (tj+1 )I =  j+1 . By Lemma 13
there is a substitution  from variables in t0j jj or tj+1 to terms such that t0j jj  = tj+1  and
for any variable x in t0j jj or tj+1 ,
(x)I = (x)I .
We define ij+1 as
 ij  when 1  i  j and
169

(56)

fiLee & Meng

  when i = j +1.
It is easy to check that condition (a) is satisfied. To check that condition (b) is satisfied,
consider any variable x in the set
{t1 1j , t2 2j , . . . , tj jj , t0j jj , tj+1 , t0j+1 }.

(57)

If x is in t0j jj or tj+1 , by (56), (x)I = (x)I . Otherwise, since  does not change the
variables that are not in t0j jj or tj+1 , (x)I = (x)I . Consequently, for any variable x in
(57), we get (x)I = (x)I . It remains to check the following.
 For 1  i  j, (ti ij+1 )I = (ti ij )I = (ti ij )I . The last one is equal to  i by I.H.
j+1 I
 (tj+1 j+1
) = (tj+1 )I = (tj+1 )I . The last one is equal to  j+1 by (55).
j+1 I
 (t0j+1 j+1
) = (t0j+1 )I = (t0j+1 )I . The last one is equal to  j+2 by (55).


A.4 Proof of Proposition 7
Proposition 7 For any rectified sentence F of a signature  and for any interpretation
I of  that satisfies CET , I is a model of
{LF F (Y ) | Y is a finite first-order loop of F }
iff I is a model of
{LF F (Y ) | Y is a finite loop of F w.r.t. I}.

The proof follows immediately from the following fact and Lemma 15.
Fact 1 Let F be a rectified sentence of a signature , and let I be an interpretation of .
For any first-order loop Y of F and any substitution  that maps variables in Y to object
names, Y 0 = {pi (  ) | pi (t)  Y , tI = } is a loop of F w.r.t. I.
Lemma 15 Let F be a rectified sentence of a signature , and let I be an interpretation
of . If I satisfies CET , then, for any finite loop Y 0 of F w.r.t. I, there is a finite
loop Y of F with a substitution  that maps variables in Y to object names such that
Y 0 = {pi (  ) | pi (t)  Y, (t)I = }.
Proof. Without loss of generality, consider a path
hp1 ( 1 ), p2 ( 2 ), . . . , pk ( k ), p1 ( 1 )i
(k  1) in the dependency graph of F w.r.t. I that consists of the vertices in Y 0 . Since
I |= CET , by Lemma 14, there is a path
hp1 (u1 ), p2 (u2 ), . . . , pk (uk ), p1 (uk+1 )i
170

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

in the first-order dependency graph of F with a substitution  that maps variables in ui
to object names such that (ui )I =  i for all 1  i  k, and (uk+1 )I =  1 . Since
(uk+1 )I = (u1 )I , by Lemma 13, there is a unifier  for uk+1 and u1 such that, for any
variable x in uk+1 or u1 , (x)I = (x)I . Consequently,
{p1 (u1 ), p2 (u2 ), . . . , pk (uk )}
induces a finite strongly connected subgraph such that (ui )I = (ui )I =  i .



A.5 Proof of Proposition 8
Proposition 8 If a rectified formula F in normal form is bounded, then F is bounded
w.r.t. any interpretation.
The proof follows from the following lemma.
Lemma 16 Let F be a rectified sentence of a signature  in normal form, and let I be an
interpretation of . For any path
hp1 ( 1 ), p2 ( 2 ), . . . , pk ( k ), pk+1 ( k+1 )i
in the dependency graph of F w.r.t I, there exists a path
hp1 (u1 ), p2 (u2 ), . . . , pk (uk ), pk+1 (uk+1 )i
in the first-order dependency graph of F with a substitution  that maps object variables in
ui to object names such that (ui )I =  i for all i, and u1 is a list of object variables.
Proof. The proof is similar to the proof of Lemma 14 except that we do not require that
I satisfy CET . Instead, the existence of a unifier  for t0j jj and tj+1 is ensured by the
assumption on normal form that tj+1 is a list of variables and the assumption that t0j jj
contains none of those variables (due to variable renaming).
A.6 Proof of Proposition 9
Proposition 9 If a rectified sentence F in normal form is bounded, then for any interpretation I, I is a model of
{LF F (Y ) | Y is a finite first-order loop of F }
iff I is a model of
{LF F (Y ) | Y is a finite loop of F w.r.t. I}.

The proof follows from Fact 1 and the following lemma.
Lemma 17 If a rectified sentence F in normal form is bounded, then for any finite loop
Y 0 of F w.r.t. I, there is a finite loop Y of F with a substitution  that maps variables in
Y to object names such that Y 0 = {pi (  ) | pi (t)  Y, (t)I = }.
171

fiLee & Meng

Proof. Let Y 0 be a finite loop of F w.r.t. I. Without loss of generality, there is a path
hp1 ( 1 ), p2 ( 2 ), . . . , pk ( k ), p1 ( 1 )i
(k  1) in the dependency graph of F w.r.t. I that consists of the vertices in Y 0 . Since F
is in normal form, by Lemma 16, there are a path
hp1 (u1 ), p2 (u2 ), . . . , pk (uk ), p1 (uk+1 )i

(58)

in the first-order dependency graph of F , where u1 consists of object variables only, and
a substitution  that maps variables in ui to object names such that (ui )I =  i for all
1  i  k, and (uk+1 )I =  1 . There are two cases to consider.
 Case 1: There is a unifier  for u1 and uk+1 that maps variables in u1 to terms in uk+1
so that u1  = uk+1 . It follows that, for any variable x in uk+1 or u1 , (x)I = (x)I .
Consequently,
{p1 (u1 ), p2 (u2 ), . . . , pk (uk )}
induces a finite strongly connected subgraph such that (ui )I = (ui )I =  i .
 Case 2: There is no such unifier .
Consider another path
hp1 (v1 ), p2 (v2 ), . . . , pk (vk ), p1 (vk+1 )i
that is obtained similar to (58) except that the variables in the path are disjoint from
the variables in (58). Clearly, there is a unifier  0 for uk+1 and v1 that maps the
variables v1 to terms, so that
hp1 (u1 ), p2 (u2 ), . . . , pk (uk ), p1 (v1  0 ), p2 (v2  0 ), . . . , pk (vk  0 )i
is another path in the first-order dependency graph of F . It is clear that using the
same construction repeatedly, we can form an infinite path that visits infinitely many
vertices in the first-order dependency graph. But this contradicts the assumption that
F is bounded.

A.7 Proof of Proposition 11
We will use the following lemma in this section and the next section, which extends Theorem 2 in the work of Chen et al. (2006) that provides a few equivalent conditions for a
program to have a finite complete set of loops to a disjunctive program and a sentence.
Lemma 18 (Chen et al., 2006, Thm. 2) For any formula F that contains no function
constants of positive arity, the following conditions are equivalent:
(a) F has a finite complete set of loops.
172

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

(b) There is a nonnegative integer N such that for every loop L of F , the number of
variables in L is bounded by N .
(c) For any loop L of F and any atom A1 and A2 in L, the variables occurring in A1 are
identical to the variables occurring in A2 .
(d) For any loop L of Ground (F ){c1 ,c2 } (F ) where c1 , c2 are two new object constants,
there are no two atoms A1 and A2 in L such that A1 mentions c1 but A2 does not or
A1 mentions c2 but A2 does not.
Proposition 11 For any rectified formula F that contains no function constants of positive
arity, F is bounded iff F has a finite complete set of loops.
Proof. From left to right: Assume that F is bounded. Then every loop of F is finite. It
follows that there exists a nonnegative integer N such that the number of variables in any
loop is bounded by N . By Lemma 18 (b), F has a finite complete set of loops.
From right to left: Assume that F has a finite complete set of loops and, for the sake of
contradiction, assume that it is not bounded. Without loss of generality, there is an infinite
path
hp1 (t1 )1 , p2 (t2 )2 , . . .i
(59)
in the first-order dependency graph of F that visits infinitely many vertices, where pi (ti )
are atoms occurring in F and i are substitutions.
Since F is a finite string, it contains finitely many atoms. It follows that there is an
atom pi (ti ) occurring in F with arbitrarily many substitutions  such that atoms pi (ti )
are contained in (59). Without loss of generality, consider the path
hpi (ti )i , pi+1 (ti+1 )i+1 , . . . , pi (ti )k i
that is contained in (59), where k and i agree on substituting object constants for variables
in ti . Since ti i and ti k contain no function constant, there exists a substitution 0 that
maps variables in ti k to terms in ti i so that ti k 0 = ti i . Consequently,
{pi (xi )i 0 , pi+1 (xi+1 )i+1 0 , . . . , pi (xi )k 0 }
is a loop of F . Since the length of the path is arbitrarily large, there are arbitrarily many
variables occurring in the loop. By Lemma 18 (b), it follows that F has no finite complete
set of loops.

A.8 Proof of Proposition 10
Proposition 10
arity),

For any rectified sentence F (allowing function constants of positive

(a) checking whether F is bounded is not decidable;
(b) checking whether F is atomic-tight is not decidable.
If F contains no function constants of positive arity,
(c) checking whether F is bounded is decidable;
(d) checking whether F is atomic-tight is decidable.
173

fiLee & Meng

A.8.1 Proof of Part (a) and (b)
We show the proof of Part (a) first. The proof repeats, with minor modifications, the
argument from the proof of Theorem 26 from the work of Bonatti (2004), which considers
the following program M to simulate deterministic Turing machines M.
t(s, L, v, [V | R], C)  t(s0 , [v 0 | L], V, R, C +1)
t(s, L, v, [ ], C)  t(s0 , [v 0 | L], b, [ ], C +1)
t(s, [V | L], v, R, C)  t(s0 , L, V, [v 0 | R], C +1)
t(s, [ ], v, R, C)  t(s0 , [ ], b, [v 0 | R], C +1)
t(s, L, v, R, C)

for
for
for
for
for

all
all
all
all
all

instr.hs, v, v 0 , s0 , righti
instr.hs, v, v 0 , s0 , righti
instr.hs, v, v 0 , s0 , lefti
instr.hs, v, v 0 , s0 , lefti
final states s.

The Halting problem can be reduced to the problem of checking bounded formulas. More
precisely, we show that M is bounded iff M terminates from every configuration.
We first establish the following facts:
(i) for every non-terminating computation of M on input x, there is a corresponding
infinite path in the first-order dependency graph of M that visits infinitely many
vertices;
(ii) if there is an infinite path in the first-order dependency graph of M , then there
is an infinite path starting with a legal encoding of an input and corresponds to a
non-terminating computation of M.
Fact (i) is immediate from the definition of M : Note that the step counter (the last
argument of t) ensures that the dependency graph is acyclic. Then, whenever M falls into
a cycle, the dependency graph contains an infinite acyclic path that visits infinitely many
vertices and hence the program is not bounded.
Fact (ii) can be proven as follows. Assume that there is an infinite path in the dependency graph. We observe that the first argument of every vertex in the path must be a legal
state and the third argument of every vertex must be a legal tape value. Otherwise, there
is no outgoing edge from the vertices in the dependency graph of M . So only the second,
fourth and fifth arguments can contain variables or illegal values which were obtained from
substitutions from the variables L, R, V and C. In this case, we can easily find substitutions
from these variables or illegal values to legal values and apply them uniformly along the
path, so that we obtain another infinite path starting from the vertex that correctly encodes
a configuration of M and thus M has a corresponding non-terminating computation.
The claim follows immediately from the two facts: if M does not terminate on some
computation, then by (i), M is unbounded. If M is unbounded, then by (ii), M does
not terminate.
The same proof works for Part (b) as well. This is because the step counter (the last
argument of t) ensures that the dependency graph is acyclic. Consequently, every infinite
path in the dependency graph visits infinitely many vertices, so that M is atomic-tight iff
M is bounded.

A.8.2 Proof of Part (c)
In view of the equivalence between (a) and (d) in Lemma 18, checking whether a formula
F containing no function constants of positive arity has a finite complete set of loops can
174

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

be done by examining a finite number of loops from a finite dependency graph, which is
decidable. By Proposition 11, it follows that checking whether F is bounded is decidable.

A.8.3 Proof of Part (d)
For any sentence F that has no function constants of positive arity and any finite set c of object constants, Ground c (F ) is defined recursively. If F is an atomic formula thenGround c (F )
is F . The function Ground c commutes with all propositional connectives; quantifiers turn
into finite conjunctions and disjunctions over all object constants occurring in c.
Lemma 19 Let c be the set consisting of all object constants occurring in F , and possibly a new object constant if F contains no object constants. F has a non-trivial loop iff
Ground c (F ) has a non-trivial loop.
In order to check whether F is atomic-tight, we first check whether F is bounded,
which is decidable. If F is not bounded, then F is not atomic-tight. Otherwise, in view of
Lemma 19, checking whether F is atomic-tight is the same as checking whether Ground c (F )
is atomic-tight. Since F contains no function constants of positive arity, the dependency
graph of Ground c (F ) is finite. So it is decidable to check whether the dependency graph of
Ground c (F ) has a non-trivial loop.

A.9 Proof of Proposition 13
Lemma 20 Let F be a formula and Y a set of atoms. If no predicate constant occurring
in Y occurs strictly positively in F , then NES F (Y ) is equivalent to F .
Proof. By induction.



Proposition 13 Let  be a program with quantifiers, F the FOL-representation of ,
and Y a finite set of atoms. Under the assumption , formula QES  (Y ) is equivalent to
NES F (Y ). If  is a disjunctive program in normal form, then QES  (Y ) is also equivalent
to ES  (Y ) under the assumption .
Proof. Between QES  (Y ) and NES F (Y ): NES F (Y ) is
^

x[(G  H)  (NES G (Y )  NES H (Y ))].

(60)

HG

Under the assumption F , formula (60) is equivalent to
_
x(NES G (Y )  NES H (Y )).

(61)

HG

In view of Lemma 20, if H does not contain any strictly positive occurrence of a predicate
constant that belongs to Y , NES H (Y ) is equivalent to H. Also, it follows from Lemma 2
and Lemma 8 that NES G (Y ) implies G. So NES G (Y )NES H (Y ) conflicts the assumption
175

fiLee & Meng

G  H when H does not contain any strictly positive occurrence of a predicate constant
that belongs to Y . As a result, under the assumption F , formula (61) is equivalent to the
disjunction of
x(NES G (Y )  NES H (Y ))
(62)
for all rules H  G, where H contains a strictly positive occurrence of a predicate constant
that belongs to Y . Note that G and H are formulas such that every occurrence of an
implication in G and H belongs to a negative formula. By Lemma 4, (62) is equivalent to
QES  (Y ).
Between QES  (Y ) and ES  (Y ): When  is a disjunctive program, QES  (Y ) is the disjunction of


^
_
^

z B  N 
(t 6= t0 )  
(p(t) 
t 6= t0 )
(63)
p(t)B
p(t0 )Y

p(t0 )Y

p(t)A

over all rules (10) such that A contains a predicate constant that occurs in Y , where z is a
list of variables in (10) but not in Y . On the other hand, ES  (Y ) is the disjunction of
0



z B  N  

^

_

0

(t 6= t )  

p(t)B
p(t0 )Y

^

(p(t) 

0

t 6= t )




(64)

p(t0 )Y

p(t)A

over all rules (10) such that A contains a predicate constant that occurs in Y and AY 6= ,
where z0 is a list of variables in A  B, N  but not in Y .
It is clear that (64) implies (63). To prove that (63) implies (64), assume
BN 

^

_

(t 6= t0 )  

p(t)B
p(t0 )Y

^

(p(t) 


t 6= t0 )

(65)

p(t0 )Y

p(t)A

and consider two cases.
V
If p(t0 )Y t 6= t0 for all p(t)  A, then (65) is equivalent to
^

BN 

(t 6= t0 )  

p(t)B
p(t0 )Y

_

p(t)

p(t)A

which contradicts the assumption .
Otherwise, there exists p(t)  A and p(t0 )  Y such that t = t0 . Since  is in normal form,
there exists  that maps t to t0 , so that A  Y 6= . Consequently, (65) is equivalent to
B  N  

^
p(t)B
p(t0 )Y

_

(t 6= t0 )  

p(t)A

Thus the claim follows.

(p(t) 

^


t 6= t0 ) .

p(t0 )Y


176

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

A.10 Proof of Proposition 16
Proposition 16 Let P be an LW-program and let F be the FOL-representation of the set
of rules in P . The following conditions are equivalent to each other:
(a) I is an answer set of P according to Lin and Wang (2008);
(b) I is a P -interpretation that satisfies SM[F ];
(c) I is a P -interpretation that satisfies F and the loop formulas of Y for all loops Y of
F w.r.t. I.
Given a program , Norm() is a normal form of  and Ground () is a ground program
obtained from  as described by Lin and Wang (2008). The proof of Proposition 16 follows
from the following lemma. We refer readers to the work of Lin and Wang for the definition
of ES (, , ) defined there.
Lemma
21 For any program  and any set Y of ground atoms, ES Norm() (Y ) is equivalent
W
to p(c)Y ES (p(c), Y, Ground ()).
Proof. By definition, ES Norm() (Y ) is

_
z B  N   x = t 

^


(t 6= t ) ,
0

(66)

q(t)B
q(t0 )Y

p(x)B,N,x=t is in Norm()
:p(x)Y

where x is a list of distinct object variables,  is a substitution that maps variables in x
to object constants occurring in Y , and z is the list of all variables that occur in the rule
p(x)  B, N , x = t. (66) is equivalent to


_
^
0
0
z B  N  t = c 
(t 6= t ) ,
(67)
p(t)B,N 
p(c)Y

q(t)B
q(t0 )Y

where z0 is the list of all variables that occur in the rule p(t)  B, N . In turn, (67) is
equivalent to


_
^
0
0
0
(tg 6= t ) .
(68)
B N d=c
p(d)B 0 ,N 0 Ground()
p(c)Y

q(tg )B 0
q(t0 )Y

Note that when d does not cover c, there exists di  d such that di mentions only constants
and pre-interpreted functions and di can not be evaluated to ci independent of interpretations. In that case, d = c is equivalent to . Thus (68) is equivalent to


_
_
^
0
0
0
(tg 6= t ) ,
(69)
B N d=c
p(c)Y

which is essentially

p(d)B 0 ,N 0 Ground()
p(d) can cover p(c)

W

p(c)Y

q(tg )B 0
q(t0 )Y

ES (p(c), Y, Ground ()).

177



fiLee & Meng

References
Asuncion, V., Lin, F., Zhang, Y., & Zhou, Y. (2010). Ordered completion for first-order
logic programs on finite structures. In AAAI, pp. 249254.
Bonatti, P. A. (2004). Reasoning with infinite stable models. Artificial Intelligence, 156 (1),
75111.
Chen, Y., Lin, F., Wang, Y., & Zhang, M. (2006). First-order loop formulas for normal
logic programs. In Proceedings of International Conference on Principles of Knowledge
Representation and Reasoning (KR), pp. 298307.
Chen, Y., Lin, F., Zhang, Y., & Zhou, Y. (2011). Loop-separable programs and their firstorder definability. Artificial Intelligence, 175 (3-4), 890913.
Chen, Y., Zhang, Y., & Zhou, Y. (2010). First-order indefinability of answer set programs
on finite structures. In AAAI, pp. 285290.
Clark, K. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and Data
Bases, pp. 293322. Plenum Press, New York.
de Bruijn, J., Pearce, D., Polleres, A., & Valverde, A. (2010). A semantical framework for
hybrid knowledge bases. Knowl. Inf. Syst., 25 (1), 81104.
Ferraris, P., Lee, J., & Lifschitz, V. (2006). A generalization of the Lin-Zhao theorem.
Annals of Mathematics and Artificial Intelligence, 47, 79101.
Ferraris, P., Lee, J., & Lifschitz, V. (2007). A new perspective on stable models. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), pp. 372379.
Ferraris, P., Lee, J., & Lifschitz, V. (2011). Stable models and circumscription. Artificial
Intelligence, 175, 236263.
Gebser, M., Lee, J., & Lierler, Y. (2006). Elementary sets for logic programs. In Proceedings
of National Conference on Artificial Intelligence (AAAI), pp. 244249.
Gebser, M., Lee, J., & Lierler, Y. (2011). On elementary loops of logic programs. Theory
and Practice of Logic Programming, To appear.
Gebser, M., & Schaub, T. (2005). Loops: Relevant or redundant?. In Proceedings of the
Eighth International Conference on Logic Programming and Nonmonotonic Reasoning
(LPNMR05), pp. 5365.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming. In
Kowalski, R., & Bowen, K. (Eds.), Proceedings of International Logic Programming
Conference and Symposium, pp. 10701080. MIT Press.
Kim, T.-W., Lee, J., & Palla, R. (2009). Circumscriptive event calculus as answer set programming. In Proceedings of International Joint Conference on Artificial Intelligence
(IJCAI), pp. 823829.
Kunen, K. (1987). Negation in logic programming. The Journal of Logic Programming,
4 (4), 289  308.
Lee, J. (2004). Nondefinite vs. definite causal theories. In Proceedings 7th Intl Conference
on Logic Programming and Nonmonotonic Reasoning, pp. 141153.
178

fiFirst-Order Stable Model Semantics and First-Order Loop Formulas

Lee, J. (2005). A model-theoretic counterpart of loop formulas. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), pp. 503508.
Lee, J., Lierler, Y., Lifschitz, V., & Yang, F. (2010). Representing synonymity in causal logic
and in logic programming. In Proceedings of International Workshop on Nonmonotonic Reasoning (NMR). http://peace.eas.asu.edu/joolee/papers/syn.pdf.
Lee, J., & Lifschitz, V. (2003). Loop formulas for disjunctive logic programs. In Proceedings
of International Conference on Logic Programming (ICLP), pp. 451465.
Lee, J., Lifschitz, V., & Palla, R. (2009). Safe formulas in the general theory of stable models.
Technical Report. http://peace.eas.asu.edu/joolee/papers/safety.pdf.
Lee, J., & Lin, F. (2006). Loop formulas for circumscription. Artificial Intelligence, 170 (2),
160185.
Lee, J., & Meng, Y. (2008). On loop formulas with variables. In Proceedings of the International Conference on Knowledge Representation and Reasoning (KR), pp. 444453.
Lee, J., & Palla, R. (2011). Integrating rules and ontologies in the first-order stable model
semantics (preliminary report). In Proceedings of International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR), pp. 248253.
Lifschitz, V. (1994). Circumscription. In Gabbay, D., Hogger, C., & Robinson, J. (Eds.),
Handbook of Logic in AI and Logic Programming, Vol. 3, pp. 298352. Oxford University Press.
Lifschitz, V., Morgenstern, L., & Plaisted, D. (2008). Knowledge representation and classical
logic. In van Harmelen, F., Lifschitz, V., & Porter, B. (Eds.), Handbook of Knowledge
Representation, pp. 388. Elsevier.
Lin, F., & Shoham, Y. (1992). A logic of knowledge and justified assumptions. Artificial
Intelligence, 57, 271289.
Lin, F., & Wang, Y. (2008). Answer set programming with functions. In Brewka, G., &
Lang, J. (Eds.), Proceedings of International Conference on Principles of Knowledge
Representation and Reasoning (KR), pp. 454465. AAAI Press.
Lin, F., & Zhao, Y. (2004). ASSAT: Computing answer sets of a logic program by SAT
solvers. Artificial Intelligence, 157, 115137.
Lin, F., & Zhou, Y. (2011). From answer set logic programming to circumscription via logic
of GK. Artificial Intelligence, 175, 264277.
Liu, L., & Truszczynski, M. (2006). Properties and applications of programs with monotone
and convex constraints. J. Artif. Intell. Res. (JAIR), 27, 299334.
McCarthy, J. (1980). Circumscriptiona form of non-monotonic reasoning. Artificial Intelligence, 13, 2739,171172.
McCarthy, J. (1986). Applications of circumscription to formalizing common sense knowledge. Artificial Intelligence, 26 (3), 89116.
Pearce, D., & Valverde, A. (2005). A first order nonmonotonic extension of constructive
logic. Studia Logica, 80, 323348.
179

fiLee & Meng

Robinson, J. A. (1965). A machine-oriented logic based on the resolution principle. J. ACM,
12, 2341.
Sabuncu, O., & Alpaslan, F. N. (2007). Computing answer sets using model generation
theorem provers. Unpublished Draft.
You, J.-H., & Liu, G. (2008). Loop formulas for logic programs with arbitrary constraint
atoms. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp.
584589.
Zhang, Y., & Zhou, Y. (2010). On the progression semantics and boundedness of answer
set programs. In Proceedings of International Conference on Principles of Knowledge
Representation and Reasoning (KR), pp. 518526.

180

fiJournal of Artificial Intelligence Research 42 (2011) 393-426

Submitted 04/11; published 11/11

Making Decisions Using Sets of Probabilities:
Updating, Time Consistency, and Calibration
Peter D. Grunwald

pdg@cwi.nl

CWI, P.O. Box 94079
1090 GB Amsterdam, The Netherlands

Joseph Y. Halpern

halpern@cs.cornell.edu

Computer Science Department
Cornell University
Ithaca, NY 14853, USA

Abstract
We consider how an agent should update her beliefs when her beliefs are represented by
a set P of probability distributions, given that the agent makes decisions using the minimax
criterion, perhaps the best-studied and most commonly-used criterion in the literature. We
adopt a game-theoretic framework, where the agent plays against a bookie, who chooses
some distribution from P. We consider two reasonable games that differ in what the
bookie knows when he makes his choice. Anomalies that have been observed before, like
time inconsistency, can be understood as arising because different games are being played,
against bookies with different information. We characterize the important special cases
in which the optimal decision rules according to the minimax criterion amount to either
conditioning or simply ignoring the information. Finally, we consider the relationship
between updating and calibration when uncertainty is described by sets of probabilities.
Our results emphasize the key role of the rectangularity condition of Epstein and Schneider.

1. Introduction
Suppose that an agent models her uncertainty about a domain using a set P of probability
distributions. How should the agent update P in light of observing that random variable
X takes on value x? Perhaps the standard answer is to condition each distribution in P
on X = x (more precisely, to condition those distributions in P that give X = x positive
probability on X = x), and adopt the resulting set of conditional distributions P | X = x
as her representation of uncertainty. In contrast to the case where P is a singleton, it
is often not clear whether conditioning is the right way to update a set P. It turns out
that in general, there is no single right way to update P. Different updating methods
satisfy different desirata, and for some sets P, not all of these desiderata can be satisfied at
the same time. In this paper, we determine to what extent conditioning and some related
update methods satisfy common decision-theoretic optimality properties. The main three
questions we pose are:
1. Is conditioning the right thing to do under a minimax criterion, that is, does it lead
to minimax-optimal decision rules?
2. Is the minimax criterion itself reasonable in the sense that it satisfies consistency
criteria such as time consistency (defined formally below)?
c
2011
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGrunwald & Halpern

3. Is conditioning the right thing do under a calibration criterion?
We show that the answer to the first two questions is yes if P satisfies a condition that
Epstein and Schneider (2003) call rectangularity, while the answer to the third question is
yes if P is convex and satisfies the rectangularity condition.1 Thus, the main contribution
of this paper is to show that, under the rectangularity condition, conditioning is the right
thing to do under a wide variety of criteria. Apart from this main conclusion, our analysis
provides new insights into the relation between minimax optimality, time consistency, and
variants of conditioning (such as ignoring the information that X = x altogether). We now
discuss our contributions in more detail.
1.1 The Minimax Criterion, Dilation, and Time Inconsistency How should an
agent make decisions based on a set P of distributions? Perhaps the best-studied and
most commonly-used approach in the literature is to use the minimax criterion (Wald,
1950; Gardenfors & Sahlin, 1982; Gilboa & Schmeidler, 1989). According to the minimax
criterion, action a1 is preferred to action a2 if the worst-case expected loss of a1 (with
respect to all the probability distributions in the set P under consideration) is better than
the worst-case expected loss of a2 . Thus, the action chosen is the one with the best worstcase outcome.
As has been pointed out by several authors, conditioning a set P on observation X = x
sometimes leads to a phenomenon called dilation (Augustin, 2003; Cozman & Walley, 2001;
Herron, Seidenfeld, & Wasserman, 1997; Seidenfeld & Wasserman, 1993): the agent may
have substantial knowledge about some other random variable Y before observing X = x,
but know significantly less after conditioning. Walley (1991, p. 299) gives a simple example
of dilation: suppose that a fair coin is tossed twice, where the second toss may depend
in an arbitrary way on the first. (In particular, the tosses might be guaranteed to be
identical, or guaranteed to be different.) If X represents the outcome of the first toss and
Y represents the outcome of the second toss, then before observing X, the agent believes
that the probability that Y is heads is 1/2, while after observing X, the agent believes that
the probability that Y is heads can be an arbitrary element of [0, 1].
While, as this example and others provided by Walley show, such dilation can be quite
reasonable, it interacts rather badly with the minimax criterion, leading to anomalous
behavior that has been called time inconsistency (Grunwald & Halpern, 2004; Seidenfeld,
2004): the minimax-optimal conditional decision rule before the value of X is observed
(which has the form If X = 0 then do a1 ; if X = 1 then do a2 ; . . . ) may be different from
the minimax-optimal decision rule after conditioning. For example, the minimax-optimal
conditional decision rule may say If X = 0 then do a1 , but the minimax-optimal decision
rule conditional on observing X = 0 may be a2 . (See Example 2.1.) If uncertainty is
modeled using a single distribution, such time inconsistency cannot arise.
1.2 The Two Games To understand this phenomenon better, we model the decision
problem as a game between the agent and a bookie (for a recent approach that is similar
in spirit but done independently, see Ozdenoren & Peck, 2008). It turns out that there is
more than one possible game that can be considered, depending on what information the
1. All these results are proved under the assumption that the domain of the probability measures in P is
finite and the set of actions that the decision maker is choosing among is finite.

394

fiMaking Decisions Using Sets of Probabilities

bookie has. We focus on two (closely related) games here. In the first game, the bookie
chooses a distribution from P before the agent moves. We show that the Nash equilibrium
of this game leads to a minimax decision rule. (Indeed, this can be viewed as a justification
of using the minimax criterion). However, in this game, conditioning on the information is
not always optimal.2 In the second game, the bookie gets to choose the distribution after
the value of X is observed. Again, in this game, the Nash equilibrium leads to the use of
minimax, but now conditioning is the right thing to do.
If P is a singleton, the two games coincide (since there is only one choice the bookie can
make, and the agent knows what it is). Not surprisingly, conditioning is the appropriate
thing to do in this case. The moral of this analysis is that, when uncertainty is characterized
by a set of distributions, if the agent is making decision using the minimax criterion, then
the right decision depends on the game being played. The agent must consider if she is
trying to protect herself against an adversary who knows the value of X = x when choosing
the distribution or one that does not know the value of X = x.
1.3 Rectangularity and Time Consistency In earlier work (Grunwald & Halpern,
2004) (GH from now on), we essentially considered the first game, and showed that, in this
game, conditioning was not always the right thing to do when using the minimax criterion.
Indeed, we showed there are sets P and games for which the minimax-optimal decision rule
is to simply ignore the information. Our analysis of the first game lets us go beyond GH here
in two ways. First, we provide a simple sufficient condition for when conditioning on the
information is minimax optimal (Theorem 4.4). Second, we provide a sufficient condition
for when it is minimax optimal to ignore information (Theorem 5.1).
Our sufficient condition guaranteeing that conditioning is minimax optimal can be
viewed as providing a sufficient condition for time consistency. Our condition is essentially
Epstein and Schneiders (2003) rectangularity condition, which they showed was sufficient
to guarantee what has been called in the decision theory community dynamic consistency.
Roughly speaking, dynamic consistency says that if, no matter what the agent learns, he
will prefer decision rule  to decision rule  0 , then he should prefer  to  0 before learning
anything. Dynamic consistency is closely related to Savages (1954) sure-thing principle.
Epstein and Schneider show that, if an agents uncertainty is represented using sets of
probability distributions, all observations are possible (in our setting, this means that all
probability distributions that the agent considers possible assign positive probability to all
basic events of the form X = x), and the set of distributions satisfies the rectangularity condition then, no matter what the agents loss function,3 if the agent prefers  to  0
after making an observation, then he will also prefer  to  0 before making the observation. Conversely, they show that if the agents preferences are dynamically consistent, then
the agents uncertainty can be represented by a set of probability measures that satisfies
the rectangularity condition, and the agent can be viewed as making decisions using the
minimax criterion.
Our results show that if all observations are possible and the rectangularity condition
holds, then, no matter what the loss function, time consistency holds. Time consistency
2. In some other senses of the words conditioning and optimal, conditioning on the information is
always optimal. This is discussed further in Section 7.
3. We work with loss functions in this paper rather than utility functions, since losses seem to be somewhat
more standard in this literature. However, we could trivially restate our results in terms of utility.

395

fiGrunwald & Halpern

holds if a decision is minimax optimal before making an observation iff it is optimal after
making the observation. Note that time consistency just considers just the optimal decision,
while dynamic consistency considers the whole preference order. However, time consistency
is an iff requirement: a decision is optimal before making the observation if and only
if that decision is optimal after making the observation. By way of contrast, dynamic
consistency is uni-directional: if a is preferred to a0 after making the observation, then it
must still be preferred before making the observation.
These results show that if uncertainty is represented by a rectangular set of measures, all
observations are possible, and the minimax criterion is used, then both dynamic consistency
and time consistency hold. On the other hand, as we show in Proposition 4.7, in general
dynamic consistency and time consistency are incomparable.
1.4 C-conditioning and Calibration As stated, we provide a general condition on P
under which conditioning is minimax optimal, as well as a general condition under which
ignoring the information is minimax optimal. Note that ignoring the information can also be
viewed as the result of conditioning; not conditioning on the information, but conditioning
on the whole space. This leads us to consider a generalization of conditioning. Let C be
a partition of the set of values of the random variable X, and let C(x) be the element of
the partition that contains x. Suppose that when we observe x, we condition on the event
X  C(x). We call this variant of conditioning C-conditioning; standard conditioning is just
the special case where each element of C is a singleton. Is C-conditioning always minimax
optimal in the first game? That is, is it always optimal to condition on something? As we
show by considering a variation of the Monty Hall Problem (Example 5.4), this is not the
case in general.
Nevertheless, it turns out that considering C-conditioning is useful; it underlies our
analysis of calibration. As pointed out by Dawid (1982), an agent updating her beliefs
and making decisions on the basis of these beliefs should also be concerned about being
calibrated. Calibration is usually defined in terms of empirical data. To explain what it
means and its connection to decision making, consider an agent that is a weather forecaster
on your local television station. Every night the forecaster makes a prediction about whether
or not it will rain the next day in the area where you live. She does this by asserting that
the probability of rain is p, where p  {0, 0.1, . . . , 0.9, 1}. How should we interpret these
probabilities? The usual interpretation is that, in the long run, on those days at which
the weather forecaster predict probability p, it will rain approximately 100p% of the time.
Thus, for example, among all days for which she predicted 0.1, the fraction of days with
rain was close to 0.1. A weather forecaster with this property is said to be calibrated. If
a weather forecaster is calibrated, and you make bets which, based on her probabilistic
predictions, seem favorable, then in the long run you cannot lose money. On the other
hand, if a weather forecaster is not calibrated, there exist bets that may seem favorable
but result in a loss. So clearly there is a close connection between calibration and decision
making.
Calibration is usually defined relative to empirical data or singleton distributions. We
first consider the obvious extension to sets of probabilities, but the obvious extension turns
out to be only a very weak requirement. We therefore define a stronger and arguably more
interesting variation that we call sharp calibration. We take an update rule  to map a set
396

fiMaking Decisions Using Sets of Probabilities

P and a value x to a new set (P, x) of probabilities. Intuitively, (P, x) is the result of
updating P given the observation X = x, according to update rule . A calibrated update
rule  is sharply calibrated for P if there is no other rule 0 that is also calibrated such
that, for all x, 0 (P, x)  (P, x), and for some x, the inclusion is strict. We first show
that if P is convex, then C-conditioning is sharply calibrated for some C; different choices
of P require different C. We then show that, if P also satisfies the rectangularity condition,
then standard conditioning is sharply calibrated.
1.5 Discussion Both the idea of representing uncertainty by a set P of distributions and
that of handling decisions in a worst-case optimal manner may, of course, be criticized.
While we do not claim that this is necessarily the right or the best approach, it is
worth pointing out that two of the most common criticisms are, to some extent, unjustified.
First, since it may be hard for an agent to determine the precise boundaries of the set P,
it has been argued that soft boundaries are more appropriate. These soft boundaries
may be thought of as inducing a single distribution on (X  Y), the set of probability
distributions on X  Y (with the density of Pr  (X  Y) proportional to the extent to
which Pr is included in the set P). With this single distribution, the setting becomes
equivalent to the setting of standard Bayesian decision theory. The problem with this
criticism is that in some cases, hard boundaries are in fact natural. For example, some
conditional probabilities may be known to be precisely 0, as is the case in the Monty Hall
game (Example 5.4). Similarly, the use of the minimax criterion is not as pessimistic as
is often thought. The minimax solution often coincides with the Bayes-optimal solution
under some maximum entropy prior (Grunwald & Dawid, 2004), which is not commonly
associated with being overly pessimistic. In fact, in the Monty Hall problem, the minimaxoptimal decision rule coincides with the solution usually advocated, which requires making
further assumptions about P to reduce it to a singleton.
The rest of this paper is organized as follows. In Section 2, we define the basic framework.
In Section 3, we formally define the two games described above and show that the minimaxoptimal decision rule gives a Nash equilibrium. In Section 4, we characterize the minimaxoptimal decision rule for the first game, in which the bookie chooses a distribution before
X is observed. In Section 5 we discuss C-conditioning and show that, in general, it is not
minimax optimal. In Section 6, we discuss calibration and C-conditioning. We conclude
with some discussion in Section 7. All proofs can be found in the appendix.

2. Notation and Definitions
In this paper, uncertainty is represented by a set P of probability distributions. For ease of
exposition, we assume throughout this paper that we are interested in two random variables,
X and Y , which can take values in spaces X and Y, respectively. P always denotes a set of
distributions on X  Y; that is, P  (X  Y), where (S) denotes the set of probability
distributions on S. For ease of exposition, we assume that P is a closed set; this is a standard
assumption in the literature that seems quite natural in our applications, and makes the
statement of our results simpler (otherwise we have to state our results using closures). If
Pr  (X  Y), let PrX and PrY denote the marginals of Pr on X and Y, respectively. Let
PY = {PrY : Pr  P}. If E  X  Y, then let P | E = {Pr | E : Pr  P, Pr(E) > 0}. Here
397

fiGrunwald & Halpern

Pr | E (often written Pr( | E)) is the distribution on X  Y obtained by conditioning on
E.
The represesentation of uncertainty using sets of probability distributions is closely
related to Walleys (1991) use of (lower and upper) previsions. A prevision is an expectation
function; that is, a lower prevision is a mapping random variables to the reals satisfying
certain properties. It is well known (Huber, 1981) that what Walley calls a coherent lower
prevision (a lower prevision satisfying some minimal properties) can be identified with
the lower expectation of a set of probability measures (that is, the function E such that
E(X) = inf PrP EPr (X)). Indeed, there is a one-to-one map between lower previsions
and closed convex sets of probability measures. The notion of conditioning we are using
corresponds to what Walley calls the regular extension of a lower prevision (see Walley,
1991, Appendix J).
2.1 Loss Functions As in GH, we are interested in an agent who must choose some
action from a set A, where the loss of the action depends only on the value of random
variable Y . We assume in this paper that X , Y, and A are finite, and that |A|  2, so that
there are always at least two possible choices. (If we allowed A to be a singleton, then some
of our results would not hold for trivial reasons.)
We assume that with each action a  A and value y  Y is associated some loss to the
agent. (The losses can be negative, which amounts to a gain.) Let L : Y  A  IR be the
loss function.
Such loss functions arise quite naturally. For example, in a medical setting, we can take
Y to consist of the possible diseases and X to consist of symptoms. The set A consists of
possible courses of treatment that a doctor can choose. The doctors loss function depends
only on the patients disease and the course of treatment, not on the symptoms. But, in
general, the doctors choice of treatment depends on the symptoms observed.
2.3 Decision Problems and Decision Settings For our purposes, a decision setting
is a tuple DS = (X , Y, A, P), where X , Y, A, and P are as above. A decision problem
is characterized by a tuple DP = (X , Y, A, P, L), where L is a loss function. That is, a
decision problem is a decision setting together with a loss function. We say that the decision
problem (X , Y, A, P, L) is based on the decision setting (X , Y, A, P).
2.4 Decision Rules Given a decision problem DP = (X , Y, A, P, L), suppose that the
agent observes the value of the variable X. After having observed X, she must perform an
act, the quality of which is judged according to loss function L. The agent must choose a
decision rule that determines what she does as a function of her observations. We allow
decision rules to be randomized. Thus, a decision rule is a function  : X  (A) that
chooses a distribution over actions based on the agents observations. Let D(X , A) be
the set of all decision rules. A special case is a deterministic decision rule, which assigns
probability 1 to a particular action. If  is deterministic, we sometimes abuse notation
and write (x) for the action that is assigned probability 1 by the distribution (x). Given
a decision rule  and a loss function L, let L be the random variable on X  Y such
P
that L (x, y) = aA (x)(a)L(y, a). Here (x)(a) stands for the probability of performing
action a according to the distribution (x) over actions that is adopted when x is observed.
Note that in the special case that  is a deterministic decision rule, L (x, y) = L(y, (x)).
398

fiMaking Decisions Using Sets of Probabilities

We also extend this notation to randomized actions: for   (A), we let L be the random
P
variable on Y such that L (y) = aA (a)L(y, a).
A decision rule  0 is a priori minimax optimal for the decision problem DP if
max EPr [L0 ] =

PrP

min

D(X ,A)

maxPrP EPr [L ].

That is,  0 is a priori minimax optimal if  0 gives the best worst-case expected loss with
respect to all the distributions in Pr. We can write max here instead of sup because of our
assumption that P is closed. This ensures that there is some Pr  P for which EPr [L0 ]
takes on its maximum value.
A decision rule  1 is a posteriori minimax optimal for DP if, for all x  X such that
Pr(X = x) > 0 for some Pr  P,
max

PrP|X=x

EPr [L1 ] =

min

max

D(X ,A) PrP|X=x

EPr [L ].

(1)

To get the a posteriori minimax-optimal decision rule we do the obvious thing: if x is
observed, we simply condition each probability distribution Pr  P on X = x, and choose
the action that gives the least expected loss (in the worst case) with respect to P | X = x.
Since all distributions Pr mentioned in (1) satisfy Pr(X = x) = 1, the minimum over
  D(X , A) does not depend on the values of (x0 ) for x0 6= x; the minimum is effectively
over randomized actions rather than decision rules.
As the following example, taken from GH, shows, a priori minimax-optimal decision
rules are in general different from a posteriori minimax-optimal decision rules.
Example 2.1: Suppose that X = Y = A = {0, 1} and P = {Pr  (X  Y) : PrY (Y =
1) = 2/3}. Thus, P consists of all distributions whose marginal on Y gives Y = 1 probability
2/3. We can think of the actions in A as predictions of the value of Y . The loss function is 0
if the right value is predicted and 1 otherwise; that is, L(i, j) = |i  j|. This is the so-called
0/1 or classification loss. It is easy to see that the optimal a priori decision rule is to choose
1 no matter what is observed (which has expected loss 1/3). Intuitively, observing the value
of X tells us nothing about the value of Y , so the best decision is to predict according to
the prior probability of Y = 1. However, all probabilities on Y = 1 are compatible with
observing either X = 0 or X = 1. That is, both (P | X = 0)Y and (P | X = 1)Y consist
of all distributions on Y. Thus, the minimax optimal a posteriori decision rule randomizes
(with equal probability) between Y = 0 and Y = 1.
To summarize, if you make decisions according to the minimax optimality criterion, then
before making an observation, you will predict Y = 1. However, no matter what observation you make, after making the observation, you will randomize (with equal probability)
between predicting Y = 0 and Y = 1. Moreover, you know even before making the observation that your opinion as to the best decision rule will change in this way. (Note that
this is an example of both time inconsistency and dynamic inconsistency.)
2.5 Time and Dynamic Consistency Formally, a decision problem DP is time consistent iff, for all decision rules ,  is a priori minimax optimal for DP iff  is a posteriori
minimax optimal. We say that DP is weakly time consistent if every a posteriori minimax
optimal rule for DP is also a priori minimax optimal for DP . A decision setting DS is
(weakly) time consistent if every decision problem based on DS is.
399

fiGrunwald & Halpern

Following Epstein and Schneider (2003), we say that a decision problem DP is dynamically consistent if for every pair ,  0 of decision rules, the following conditions both hold:
1. If, for all x such that Pr(X = x) > 0 for some Pr  P,
max

Pr(P|X=x)

EPr [L ] 

max

Pr(P|X=x)

EPr [L0 ],

(2)

then
max EPr [L ]  max EPr [L0 ].

PrP

PrP

(3)

2. If, for all x such that Pr(X = x) > 0 for some Pr  P, we have strict inequality in
(2), then (3) must hold with strict inequality as well.
Informally, dynamic consistency means that whenever  is preferred to  0 according to the
minimax criterion a posteriori, then  is also preferred to  0 according to the minimax
criterion a priori, and that whenever the a posteriori preference is strict for all possible
observations, then the a priori preference must be strict as well.
A decision setting DS is dynamically consistent if every decision problem based on DS
is.

3. Two Game-Theoretic Interpretations of P
What does it mean that an agents uncertainty is characterized by a set P of probability
distributions? How should we understand P? We give P a game-theoretic interpretation
here: namely, an adversary gets to choose a distribution from the set P.4 But this does not
completely specify the game. We must also specify when the adversary makes the choice.
We consider two times that the adversary can choose: the first is before the agents observes
the value of X , and the second is after. We formalize this as two different games, where we
take the adversary to be a bookie.
We call the first game the P-game. It is defined as follows:
1. The bookie chooses a distribution Pr  P.
2. The value x of X is chosen (by nature) according to PrX and observed by both bookie
and agent.
3. The agent chooses an action a  A.
4. The value y of Y is chosen according to Pr | X = x.
5. The agents loss is L(y, a); the bookies loss is L(y, a).
This is a zero-sum game; the agents loss is the bookies gain. In this game, the agents
strategy is a decision rule, that is, a function that gives a distribution over actions for each
observed value of X. The bookies strategy is a distribution over distributions in P.
We now consider a second interpretation of P, characterized by a different game that
gives the bookie more power. Rather than choosing the distribution before observing the
value of X, the bookie gets to choose the distribution after observing the value. We call
this the P-X-game. Formally, it is specified as follows:
4. This interpretation remains meaningful in several practical situations where there is no explicit adversary;
see the final paragraph of this section.

400

fiMaking Decisions Using Sets of Probabilities

1. The value x of X is chosen (by nature) according to some procedure that is guaranteed
to end up with a value of x for which Pr(X = x) > 0 for some Pr  P, and observed
by both the bookie and the agent.5
2. The bookie chooses a distribution Pr  P such that Pr(X = x) > 0.6
3. The agent chooses an action a  A.
4. The value y of Y is chosen according to Pr | X = x.
5. The agents loss is L(y, a); the bookies loss is L(y, a).
Recall that a pair of strategies (S1 , S2 ) is a Nash equilibrium if neither party can do
better by unilaterally changing strategies. If, as in our case, (S1 , S2 ) is a Nash equilibrium in
a zero-sum game, it is also known as a saddle point; S1 must be a minimax strategy, and
S2 must be a maximin strategy (Grunwald & Dawid, 2004). As the following results show,
an agent must be using an a priori minimax-optimal decision rule in a Nash equilibrium
of the P-game, and an a posteriori minimax-optimal decision rule is a Nash equilibrium of
the P-X-game. This can be viewed as a justification for using (a priori and a posteriori)
minimax-optimal decision rules.
Theorem 3.1: Fix X , Y, A, L, and P  (X  Y).
(a) The P-game has a Nash equilibrium (  ,   ), where   is a distribution over P with
finite support.
(b) If (  ,   ) is a Nash equilibrium of the P-game such that   has finite support, then
(i) for every distribution Pr0  P in the support of   , we have
EPr0 [L ] = maxPrP EPr [L ];
(ii) if Pr = PrP, (Pr)>0   (Pr) Pr (i.e., Pr is the convex combination of the
distributions in the support of   , weighted by their probability according to   ),
then
P

EPr [L ] =
=
=
=

minD(X ,A) EPr [L ]
maxPrP minD(X ,A) EPr [L ]
minD(X ,A) maxPrP EPr [L ]
maxPrP EPr [L ].

Once nature has chosen a value for X in the P-X-game, we can regard steps 25 of the
P-X-game as a game between the bookie and the agent, where the bookies strategy is characterized by a distribution in P | X = x and the agents is characterized by a distribution
over actions. We call this the P-x-game.
Theorem 3.2: Fix X , Y, A, L, P  (X  Y).
5. Because x is observed by both parties, and y is chosen after x is chosen, the procedure by which nature
chooses x is irrelevant. We could assume for definiteness that nature chooses uniformly at random among
the values x such that Pr(x) > 0 for some Pr  P, but any other choice would work equally well.
6. If we were to consider conditional probability distributions (de Finetti, 1936; Popper, 1968), for which
Pr(Y = y | X = x) is defined even if Pr(X = x) = 0, then we could drop the restriction that x is chosen
such that Pr(X = x) > 0 for some Pr  P.

401

fiGrunwald & Halpern

(a) The P-x-game has a Nash equilibrium (  ,   (x)), where   is a distribution over
P | X = x with finite support.
(b) If (  ,   (x)) is a Nash equilibrium of the P-x-game such that   has finite support,
then
(i) for all Pr0 in the support of   , we have EPr0 [L ] = maxPrP|X=x EPr [L ];
P
(ii) if Pr = PrP, (Pr)>0   (Pr) Pr, then
EPr [L ] =
=
=
=

minD(X ,A) EPr [L ]
maxPrP|X=x minD(X ,A) EPr [L ]
minD(X ,A) maxPrP|X=x EPr [L ]
maxPrP|X=x EPr [L ].

Since all distributions Pr in the expression minD(X ,A) maxPrP|X=x EPr [L ] in part (b)(ii)
are in P | X = x, as in (1), the minimum is effectively over randomized actions rather than
decision rules.
Theorems 3.1 and 3.2 can be viewed as although, according to the definition, there is
time inconsistency, when viewed properly, there is no real inconsistency here; rather, we
must just be careful about what game is being played. If the P-game is being played, the
right strategy is the a priori minimax-optimal strategy, both before and after the value
of X is observed; similarly, if the P-X-game is being played, the right strategy is the a
posteriori minimax-optimal strategy, both before and after the value of X is observed.
Indeed, thinking in terms of the games explains the apparent time inconsistency. In both
games, the agent gains information by observing X = x. But in the P-X game, so does
the bookie. The information may be of more use to the bookie than the agent, so, in this
game, the agent can be worse off by being given the opportunity to learn the value of X.
Of course, in most practical situations, agents (robots, statisticians,. . . ) are not really
confronted with a bookie who tries to make them suffer. Rather, the agents may have no
idea at all what distribution holds, except that it is in some set P. Because all they know
is P, they decide to prepare themselves for the worst-case and play the minimax strategy.
The fact that such a minimax strategy can be interpreted in terms of a Nash equilibrium
of a game helps to understand differences between different forms of minimax (such as a
priori and a posteriori minimax). From this point of view, it seems strange to have a bookie
choose between different distributions in P according to some distribution   . However, if
P is convex, we can replace the distribution   on P by a single distribution in P, which
consists of the convex combination of the distributions in the support of   ; this is just the
distribution Pr of Theorems 3.1 and 3.2. Thus, Theorems 3.1 and 3.2 hold with the bookie
restricted to a deterministic strategy.

4. Conditioning, Rectangularity, and Time Consistency
To get the a posteriori minimax-optimal decision rule we do the obvious thing: if x is
observed, we simply condition each probability distribution Pr  P on X = x, and choose
the action that gives the least expected loss (in the worst case) with respect to P | X = x.
We might expect that the a priori minimax-optimal decision rule should do the same
thing. That is, it should be the decision rule that says, if x is observed, then we choose
402

fiMaking Decisions Using Sets of Probabilities

the action that again gives the best result (in the worst case) with respect to P | X = x.
But Example 2.1 shows that this cannot be true in general, since in some cases the a priori
optimal decision rule is not to condition, but to ignore the observed value of X, and just
choose the action that gives the least expected loss (in the worst case) with respect to P, no
matter what value X has. We later show that there are cases in which the optimal a priori
rule is neither to condition nor to ignore (see Example 5.4). Our goal in this section is to
show that the rectangularity condition of Epstein and Schneider (2003) suffices to guarantee
that conditioning is optimal.
Definition 4.1: Let hPi, the hull of P, be the set
{Pr  (X  Y) : PrX  PX and, if Pr(X = x) 6= 0, then (Pr | X = x)  (P | X = x)} .

Thus, hPi consists of all distributions Pr whose marginal on X is the marginal on X of
some distribution in P and whose conditional on observing X = x is the conditional of
some distribution in P, for all x  X . Clearly P  hPi, but the converse is not necessarily
true, as the following example shows.
Example 4.2: Suppose that X = Y = {0, 1}, and Pr1 , Pr2 , Pr3  (X  Y) are defined as
follows:
 Pr1 (0, 0) = Pr1 (1, 0) = 1/3; Pr1 (0, 1) = Pr1 (1, 1) = 1/6;
 Pr2 (0, 0) = Pr2 (1, 0) = 1/6; Pr2 (0, 1) = Pr2 (1, 1) = 1/3;
 Pr3 (0, 0) = Pr3 (1, 1) = 1/3; Pr3 (0, 1) = Pr3 (1, 0) = 1/6.
Suppose that P = {Pr1 , Pr2 }. Then Pr3 6 P, but it is easy to see that Pr3  hPi. For
(Pr1 )X = (Pr2 )X = (Pr3 )X is the uniform distribution on X , Pr3 | (X = 0) = Pr1 | (X = 0),
and Pr3 | (X = 1) = Pr2 | (X = 1).
Note also that for the P in Example 2.1, we have hPi = (X  Y) 6= P. The notion
of the hull arises in a number of contexts. In the language of Walley (1991), the hull of
P is the natural extension of the marginals PX and the collection of sets of conditional
probabilities P | X = x for x  X . Thus, if P = hPi, then we can reconstruct the joint
probability distributions P from PX and the collection of sets of conditional probabilities.
The assumption that P = hPi is closely related to a set of probabilities being separately
specified, introduced by da Rocha and Cozman (2002). As da Rocha and Cozman point
out, this assumption makes it possible to apply ideas from Bayesian networks to uncertainty
represented by sets of probability distributions.
The condition P = hPi is an instance of the rectangularity condition which goes back at
least to the work of Sarin and Wakker (1998). It was introduced in its most general form by
Epstein and Schneider (2003). Epstein and Schneider define this condition for a sequence
of random variables X1 , . . . , Xt , where the support of each Xj is not necessarily finite. In
the special case that t = 2, and X := X1 and Y := X2 are restricted to have finite support,
the rectangularity condition is exactly equivalent to our condition that P = hPi.
403

fiGrunwald & Halpern

Considering hPi also gives some insight into the two games that we considered in Section 3. In the P-X -game, the bookie has more power than in the P-game, since he gets
to choose the distribution after the agent observes x in the P-X -game, and must choose it
before the agent observes x in the P-game. That means that the agent can draw inferences
about the distribution that the bookie chose in the P-game. Such inferences cannot be
drawn if P = hPi. More generally, in a precise sense, the agent has the same information
about Y in the P-X -game as in the hPi-game. Rather than making this formal (since it is
somewhat tangential to our main concerns), we give an example to show the intuition.
Example 4.3: Suppose that X = Y = {0, 1}, and P = {Pr1 , Pr2 }, where
 Pr1 (0, 0) = (1  ), Pr1 (0, 1) = (1  )2 , Pr1 (1, 0) = (1  ), and Pr1 (1, 1) = 2 ;
 Pr2 (0, 0) = (1  ), and Pr2 (0, 1) = 2 , Pr2 (1, 0) = (1  ), Pr2 (1, 1) = (1  )2 .
In the P-game, if the agent observes that X = 0, then he is almost certain that the
bookie chose Pr1 , and thus is almost certain that Y = 1. On the other hand, in the PX-game, when the agent observes x, he has no idea whether the bookie will choose Pr1
or Pr2 (since the bookie makes this choice after observing x), and has no idea whether Y
is 0 or 1. Note that P 6= hPi; in particular, there is a distribution Pr3  hPi such that
(Pr3 )X = (Pr1 )X and (Pr3 ) | (X = 0) = (Pr2 ) | (X = 0). For example, we can take Pr3
such that Pr3 (0, 0) = (1  )2 and Pr3 (0, 1) = (1  ) (the values of Pr3 (1, 0) and Pr3 (1, 1)
are irrelevant, as long as they sum to  and are nonnegative). Thus, after observing that
X = 0 in the hPi game, the agent would have no more of an idea of the value of Y than he
does in the P-X game.
The key point for us here is that when P = hPi, conditioning is optimal, as the following
theorem shows. We first need a definition. We call P conservative if for all Pr  P and all
x  X , Pr(X = x) > 0.7
Theorem 4.4: Given a decision setting DS = (X , Y, A, P) such that P = hPi, then for
all decision problems DP based on DS, there exists an a priori minimax-optimal rule that
is also a posteriori minimax optimal. Indeed, every a posteriori minimax-optimal rule is
also a priori minimax optimal, so DS and DP are weakly time consistent. Moreover, if P
is conservative, then for every decision problem DP based on DS, every a priori minimaxoptimal rule is also a posteriori minimax optimal, so DS and DP are time consistent.
This raises the question as to whether the qualification there exists in Theorem 4.4 is
necessary, and whether the converse of the theorem also holds. Example 4.5 shows that
the answer to the first question is yes; Example 4.6 shows that the answer to the second
question is no.
Example 4.5 : If for some x  X , there exist Pr, Pr0  P such that Pr(X = x) = 0
and Pr0 (X = x) > 0, then there may be an a priori minimax decision rule that is not a
posteriori minimax. For example, consider the decision problem DP = (X , Y, A, P, L) with
7. Our notion of conservative corresponds to what Epstein and Schneider (2003) call the full support condition.

404

fiMaking Decisions Using Sets of Probabilities

X = {0, 1}, A = Y = {0, 1, 2}, L the classification loss (Example 2.1) and P = {Pr1 , Pr2 }.
We first define Pr1 :
Pr1 (X = 1) = 1/2,
Pr1 (Y = 0 | X = 0) = Pr1 (Y = 1 | X = 0) = Pr1 (Y = 2 | X = 0) = 1/3, and
Pr1 (Y = 0 | X = 1) = 1/2,
Pr1 (Y = 1 | X = 1) = 2/5,
Pr1 (Y = 2 | X = 1) = 1/10.
Pr2 is defined as follows: Pr2 (X = 0) = 1, and for all j  Y, Pr2 (Y = j, X = 0) = Pr2 (Y =
j | X = 0) := Pr1 (Y = j | X = 0). It is easy to see that P = hPi, so the rectangularity
condition holds.
Note that (0), the decision taken when observing X = 0, does not affect the expected
loss; for both Pr1 | X = 0 and Pr2 | X = 0, Y is uniform, so the expected loss is 2/3, regardless of (0). This implies that every decision rule  with (1) a randomized combination
of {0, 1} is a priori optimal, and has worst-case expected loss 2/3, since EPr2 [L ] = 2/3 and
EPr1 [L ] < 2/3. But the minimax optimal rules with (1) = 1 are not a posteriori optimal,
since if the player observes X = 1, he knows that the distribution is Pr1 , and the minimax
loss relative to Pr1 is 1/2 for action 0 and 3/5 for action 1.
Both in this example and in Example 4.3, observing a particular value of X gives
information about which distribution in P the bookie has chosen. In Example 4.3, observing
X = 0 implies that the bookie almost certainly chose Pr1 in the P-game; in the present
example, observing X = 1 implies that the bookie certainly chose Pr1 in both the P-game
and the P  X game. We note, however, that observing X = x can give information about
the distribution chosen by the bookie in the P  X game only if there exist Pr and Pr0 in
the P-game such that Pr(X = x) = 0 and Pr0 (X = x) > 0. If no such Pr and Pr0 exists,
then the bookie is completely free to choose any Pr  P he likes after x has been observed,
so observing x gives no information about which Pr  P has been chosen.
There exist decision settings such that P is conservative and P 6= hPi, although we still
have weak time consistency. Hence, the converse of Theorem 4.4 does not hold in general.
We now give an example of such a P.
Example 4.6: Let X = A = Y = {0, 1} and P = {Pr0 , Pr1 } with Pr0 (X = 1) = Pr1 (X =
1) = 1/2 and for x  {0, 1}, Pr0 (Y = 0 | X = x) = 1 and Pr1 (Y = 1 | X = x) = 1. Clearly
P is conservative and P 6= hPi; for example, the distribution Pr3 such that Pr3 (X = 1) =
1/2, Pr3 (Y = 0 | X = 0) = 1, and Pr3 (Y = 0 | X = 1) = 0 is in hPi  P. Note that X and
Y are independent with respect to both Pr0 and Pr1 . Now take an arbitrary loss function
L. Since (Pr | X = x)Y contains two distributions, one with Pr(Y = 1) = 0 and one with
Pr(Y = 1) = 1, the minimax a posteriori act is to play (0) = (1) = (1   )  0 +   1
(i.e., the act that plays 0 with probability 1   and 1 with probability  ), where  is
chosen so as to minimize f () = max{(1  )L(0, 0) + L(0, 1), (1  )L(1, 0) + L(1, 1)}.
For simplicity, assume that there is a unique such  . (If not, then it must be the case that
all   [0, 1] minimize this expression, and it is easy to check L(0, 0) = L(0, 1) = L(1, 0) =
L(1, 1), so time consistency holds trivially.)
405

fiGrunwald & Halpern

We want to show that  is also a priori minimax. It is easy to check that
max

Pr{Pr0 ,Pr1 }

L = f ( ),

where f is as above. So it suffices to show that for any decision rule  0 , we must have
max

Pr{Pr0 ,Pr1 }

L0  f ( ),

Suppose that (x) = (1  x )  0 + x  1, for x  {0, 1}. Then
maxPr{Pr0 ,Pr1 } EP r [L0 ]
= max{ 21 ((1  0 )L(0, 0) + 0 L(0, 1) + (1  1 )L(0, 0) + 1 L(0, 1)),
1
2 ((1  0 )L(1, 0) + 0 L(1, 1) + (1  1 )L(1, 0) + 1 L(1, 1)}
= max{(1  )L(0, 0) + L(0, 1), (1  )L(1, 0) + L(1, 1)}, where  =
= f ()  f ( ).

0 +1
2

It is interesting to compare Theorem 4.4 with the results of Epstein and Schneider
(2003). For this, we first compare our notion of time consistency with their notion of
dynamic consistency. Both notions were formally defined at the end of Section 2. Our
results are summarized in Proposition 4.7. First we need two definitions: Let P be a set of
distributions on X  Y. A decision problem is based on P if it is of the form (X , Y, A, P, L)
for some arbitrary A and L. A decision problem satisfies strong dynamic consistency if it
satisfies condition (2) of the definition of dynamic consistency and satisfies the following
strengthening of (3):
 If, for all x such that Pr(X = x) > 0 for some Pr  P, (2) holds, and for some x such
that Pr(X = x) > 0, we have
max

Pr(P|X=x)

EPr [L ] <

max

Pr(P|X=x)

EPr [L0 ],

(4)

then (3) must hold with strict inequality.
Proposition 4.7:
(a) Every dynamically consistent decision problem is also weakly time consistent.
(b) Not every dynamically consistent decision problem is time consistent.
(c) Every strongly dynamically consistent decision problem is time consistent.
(d) There exist weakly time consistent decision problems that are not dynamically consistent.
(e) All decision problems based on P are dynamically consistent if and only if all decision
problems based on P are weakly time consistent.
406

fiMaking Decisions Using Sets of Probabilities

Proposition 4.7(c) shows that the comparison between time consistency and dynamic consistency is subtle: replacing for all x by for some x in the second half of the definition
of dynamic consistency, which leads to a perfectly reasonable requirement, suffices to force
time consistency. Proposition 4.7(e) leads us to suspect that a decision setting is weakly
time consistent if and only if it is dynamically consistent. We have, however, no proof of
this claim. The proof of part (e) involves two decision problems based on the same set P,
but with different sets of actions, so these decision problems are not based on the same
decision setting. It does not seem straightforward to extend the result to decision settings.
Epstein and Schneider show, among other things, that if P is closed, convex, conservative, and rectangular, then DS is is dynamically consistent, and hence weakly time
consistent. We remark that the convexity assumption is not needed for this result. It easy
to check that  is prefered to  0 with respect to P according to the minimax criterion iff
 is preferred to  0 with respect to the convex closure of P according to the minimax criterion. Proposition 4.7 shows that dynamic and time consistency are closely related. Yet,
while there is clear overlap in what we prove in Theorem 4.4 and the Epstein-Schneider
(ES from now on) result, in general the results are incomparable. For example, we can
already prove weak time consistency without assuming conservativeness; ES assume conservativeness throughout. On the other hand, ES also show that if dynamic consistency
holds, then the agents actions can be viewed as being the minimax optimal actions relative
to a rectangular convex conservative set; we have no analogous result for time consistency.
Moreover, in contrast to the ES result, our results hold only for the restricted setting with
just two time steps, one before and one after making a single observation.

5. Belief Updates and C-conditioning
In this section we define the notion of a belief update rule, when belief is represented by
sets of probabilities, and introduce a natural family of belief update rules which we call
C-conditioning.
To motivate these notions, recall that Example 2.1 shows that the minimax-optimal a
priori decision rule is not always the same as the minimax-optimal a posteriori decision
rule. In this example, the minimax-optimal a priori decision rule ignores the information
observed. Formally, a rule  ignores information if (x) = (x0 ) for all x, x0  X . If 
ignores information, define L0 to be the random variable on Y such that L0 (y) = L (x, y)
for some choice of x. This is well defined, since L (x, y) = L (x0 , y) for all x, x0  X .
The following theorem provides a general sufficient condition for ignoring information
to be optimal.

Theorem 5.1 : Fix X , Y, L, A, and P  (X  Y). If, for all PrY  PY , P contains a distribution Pr0 such that X and Y are independent under Pr0 , and Pr0Y = PrY ,
then there is an a priori minimax-optimal decision rule that ignores information. Under
these conditions, if  is an a priori minimax-optimal decision rule that ignores information,
then  essentially optimizes with respect to the marginal on Y ; that is, maxPrP EPr [L ] =
maxPrY PY EPrY [L0 ].
407

fiGrunwald & Halpern

GH focused on the case that PY is a singleton (i.e., the marginal probability on Y is the
same for all distributions in P) and for all x, PY  (P | X = x)Y . It is immediate from
Theorem 5.1 that ignoring information is a priori minimax optimal in this case.
Standard conditioning and ignoring information are both instances of C-conditioning,
which in turn is an instance of an update rule. We now define these notions formally.
Definition 5.2: A belief update rule (or just an update rule) is a function  : 2(X Y) X 
2(X Y)  {} mapping a set P of distributions and an observation x to a nonempty set
(P, x) of distributions; intuitively, (P, x) is the result of updating P with the observation
x.
In the case where P is a singleton {Pr}, then one update rule is conditioning; that is,
({Pr}, x) = {Pr( | X = x)}. But other update rules are possible, even for a single
distribution; for example, Lewis (1976) considered an approach to updating that he called
imaging. There is even more scope when considering sets of probabilities; for example,
both Walleys (1991) natural extension and regular extension provide update rules (as we
said, our notion of conditioning can be viewed as an instance of Walleys regular extension).
Simply ignoring information provides another update rule: (P, x) = P. As we said above,
ignoring information and standard conditioning are both instances of C-conditioning.
Definition 5.3: Let C = {X1 , . . . , Xk } be a partition of X ; that is, Xi 6=  for i = 1, . . . , k;
X1  . . .  Xk = X ; and Xi  Xj =  for i 6= j. If x  X , let C(x) be the cell containing x;
that is, the unique element Xi  C such that x  Xi . The C-conditioning belief update rule
is the function  defined by taking (P, x) = P | C(x) (if for all Pr  P, Pr(C(x)) = 0,
then (P, x) is undefined). A decision rule  is based on C-conditioning if it amounts to
first updating the set P to P | C(x), and then taking the minimax-optimal distribution over
actions relative to (P | C(x))Y . Formally,  is based on C-conditioning if, for all x  X with
Pr(X = x) > 0 for some Pr  P,
max

Pr(P|XC(x))Y

EPr [L(x) ] = min

max

(A) Pr(P|XC(x))Y

EPr [L ].

Standard conditioning is a special case of C-conditioning, where we take C to consist of
all singletons; ignoring information is also based on C-conditioning, where C = {X }. Our
earlier results suggest that perhaps an a priori minimax-optimal decision rule must be based
on C-conditioning for some C. The Monty Hall problem again shows that this conjecture is
false.
Example 5.4 : [Monty Hall] (Mosteller, 1965; vos Savant, 1990): We start with the
original Monty Hall problem, and then consider a variant of it. Suppose that youre on a
game show and given a choice of three doors. Behind one is a car; behind the others are
goats. You pick door 1. Before opening door 1, Monty Hall, the host (who knows what is
behind each door) opens one of the other two doors, say, door 3, which has a goat. He then
asks you if you still want to take whats behind door 1, or to take whats behind door 2
instead. Should you switch? You may assume that initially, the car was equally likely to
be behind each of the doors.
408

fiMaking Decisions Using Sets of Probabilities

We formalize this well-known problem as a P-game, as follows: Y = {1, 2, 3} represents
the door which the car is behind. X = {G2 , G3 }, where, for j  {2, 3}, Gj corresponds
to the quizmaster showing that there is a goat behind door j. A = {1, 2, 3}, where action
a  A corresponds to the door you finally choose, after Monty has opened door 2 or 3. The
loss function is once again the classification loss, L(i, j) = 1 if i 6= j, that is, if you choose
a door with a goat behind it, and L(i, j) = 0 if i = j, that is, if you choose a door with a
car. P is the set of all distributions Pr on X  Y satisfying
PrY (Y = 1) = PrY (Y = 2) = PrY (Y = 3) = 13
Pr(Y = 2 | X = G2 ) = Pr(Y = 3 | X = G3 ) = 0.
Note that P does not satisfy the rectangularity condition. For example, let Pr be the
distribution such that Pr (G2 , 1) = Pr (G2 , 3) = 1/3 and Pr (G3 , 1) = Pr (G3 , 2) = 1/6. It
is easy to see that Pr  hPi  P.
It is well known, and easy to show, that the a priori minimax-optimal strategy is always
to switch doors, no matter whether Monty opens door 2 or door 3. Formally, let S be the
decision rule such that S (G2 ) = 3 and S (G3 ) = 2. Then S is the unique a priori minimaxoptimal decision rule (and has expected loss 1/3). The rule S is also a posteriori minimax
optimal. But now we modify the problem so that there is a small cost, say  > 0, associated
with switching. The cost is associated both with switching to door 2 and with switching to
door 3. As long as  is sufficiently small, the action S of always switching is still uniquely a
priori minimax optimal. However, now S is not based on C-conditioning. There exist only
two partitions of X . The corresponding two update rules based on C-conditioning amount
to, respectively, (1) ignoring X, and (2) conditioning on X in the standard way. The decision
rule based on ignoring the information is to stick to door 1, because there is a cost associated
with switching. The decision rule based on conditioning is to switch doors with probability
1/(2 + ). To see this, consider the observation X = G2 , and let  be the randomized action
of switching to door 3 with probability q and sticking to door 1 with probability 1  q. Let
m(q) = maxPr(P|X=G2 )Y EP r [L ]. Thus, m(q) = maxp[0,1/2] (qp(1 + ) + (1  q)(1  p)).
Again, to compute m(q), we need to consider only what happens when is at the extremes of
the interval; that is, when p = 0 or p = 1/2, so m(q) = max(1  q, (1 + q)/2). Clearly m(q)
is minimized when 1  q = (1 + q)/2, that is, when q = 1/(2 + ). A similar analysis applies
when the observation X = G3 . Thus, neither of the decision rules based on conditioning is
minimax optimal.
Although C-conditioning does not guarantee minimax optimality, it turns out to be a useful
notion. As we show in the next section, it is quite relevant when we consider calibration.

6. Calibration
As we said in the introduction, Dawid (1982) pointed out that an agent who is updating his
beliefs should want to be calibrated. In this section, we consider the effect of requiring calibration. Up to now, calibration has been considered only when uncertainty is characterized
by a single distribution. Below we generalize the notion of calibration to our setting, where
uncertainty is characterized by a set of distributions. We then investigate the connection
409

fiGrunwald & Halpern

between calibration and some of the other conditions that we considered earlier, specifically
the conditions that P is convex and P = hPi.8
Calibration is typically defined with respect to empirical data. We view the set P
of distributions not as describing empirical data, but as defining an agents uncertainty
regarding the true distribution. We want to define calibration in such a setting. For the
case that P is a singleton, this has already been done, for example, by Vovk, Gammerman,
and Shafer (2005). 9 Below, we first define calibration for the case where P is a singleton,
and then extend the notion to general P.
Let  be an update rule such that ({Pr}, x) contains just a single distribution for
each x  X (for example,  could be ordinary conditioning). Given x  X and , define
[x],P = {x0 : ((P, x0 ))Y = (P, x)Y }. Thus, [x],P consists of all values x0 that, when
observed, lead to the same updated marginal distributions as x.
Definition 6.1 : The update rule  is calibrated relative to Pr if, for all x  X , if
Pr([x],{Pr} ) 6= 0, then Pr( | [x],{Pr} )Y = ({Pr}, x)Y .10
In words, this definition says that if Pr0 is the distribution on Y that results from updating
Pr after observing x according to  and then marginalizing to Y, then  is calibrated if Pr0 is
also the marginal distribution that results when conditioning Pr on the set of values x0 that,
when observed, result in Pr0 being the marginal distribution according to . Intuitively,
for each x that may be observed, an agent who uses  produces a distribution ({Pr}, x).
The agent may then make decisions or predictions about Y based on this distribution,
marginalized to Y. We consider the set P 0 of all distributions on Y that the agent may use
to predict Y after observing the value of X. That is, Pr0  P 0 iff with positive Pr-probability
the agent, after observing the value of X, uses Pr0 to predict Y . The set P 0 has at most
|X | elements. Definition 6.1 then says that, for each Pr0  P 0 , whenever the agent predicts
with Pr0 , the agent is correct in the sense that the distribution of Y given that the agent
uses Pr0 is indeed to Pr0 . Note that in Definition 6.1, as in all subsequent definitions in
this section, we marginalize on Y. We discuss this further at the end of this section. It is
straightforward to generalize Definition 6.1 to sets P of probability distributions that are
not singletons, and update rules  that map to sets of probabilities.
Definition 6.2: The update rule  is calibrated relative to P if, for all x  X , if Pr([x],P ) 6=
0 for some Pr  P, then (P | [x],P )Y = (P, x)Y .
We now want to relate calibration and C-conditioning. The following result is a first step in
that direction. It gives conditions under which standard conditioning is calibrated, and also
shows that, for convex P and arbitrary C, C-conditioning satisfies one of the two inclusions
required by Definition 6.2.
8. Recall that convexity is an innocuous assumption in the context of time and dynamic consistency.
However, as we show in this section, it is far from innocuous in the context of calibration.
9. Vovk et al.s setting is somewhat different from ours, because they are interested only in upper bounds
on, rather than precise values of, probabilities. As a result, their definition of validity (as they call
their notion of calibration) is somewhat different from Definition 6.1, but the underlying idea is the same.
We have found no definition in the literature that coincides with ours.
10. As usual, if A  X , then we identify P | A with P | (A  Y).

410

fiMaking Decisions Using Sets of Probabilities

Theorem 6.3:
(a) If  is C-conditioning for some partition C of X and P is convex then, for all x  X ,
we have that (P | [x],P )Y  (P, x)Y .
(b) If  is standard conditioning, P = hPi, and x  X , then (P, x)Y  (P | [x],P )Y .
Corollary 6.4 : If P is convex and P = hPi, then standard conditioning is calibrated
relative to P.
This corollary will be significantly strengthened in Theorem 6.12 below. In general, both
convexity and the P = hPi condition are necessary in Corollary 6.4, as the following two
examples show.
Example 6.5: Let X = Y = {0, 1}, let P = {Pr1 , Pr2 , Pr3 , Pr4 }, where Pr1 , . . . , Pr4 are
defined below as a sequence of four numbers (a, b, c, d), with Pri (0, 0) = a, Pri (0, 1) = b,
Pri (1, 0) = c, and Pri (1, 1) = d):
 Pr1 = (1/4, 1/4, 1/4, 1/4),
 Pr2 = (1/8, 3/8, 1/8, 3/8),
 Pr3 = (1/4, 1/4, 1/8, 3/8),
 Pr4 = (1/8, 3/8, 1/4, 1/4).
Clearly P is not convex. Note that Pr1 (Y = 0 | X = 0) = Pr1 (Y = 0 | X = 1) =
1/2, Pr2 (Y = 0 | X = 0) = Pr2 (Y = 0 | X = 1) = 1/4, and Pr3 (Y = 0 | X = 0) = 1/2,
Pr3 (Y = 0 | X = 1) = 1/4. Since, for all Pr  P, Pr(X = 0) = 1/2, and (P | X = 0)Y =
(P | X = 1)Y = {Pra , Prb } where Pra (Y = 0) = 1/2 and Prb (Y = 0) = 1/4, we have
P = hPi. We now show that standard conditioning is not calibrated relative to P. Let 
stand for standard conditioning. For x  {0, 1}, we have
(P, x)Y = (P | X = x)Y = {Pr01 , Pr02 },

(5)

where Pr01 (Y = 0) = 1/2 and Pr02 (Y = 0) = 1/4. It also follows that, for x  {0, 1},
[x],P = {0, 1} = X , so that
(P | [x],P )Y = PY .
(6)
Since PY contains a distribution Pr03 such that Pr03 (Y = 0) = 3/8, (5) and (6) together show
that  is not calibrated.
Example 6.6: Let X = Y = {0, 1}, and let P consist of all distributions on X  Y with
Pr(Y = 1) = 0.5. Clearly P is convex. However, P =
6 hPi. To see this, note that P contains
a distribution Pr with Pr(Y = 0 | X = 0) = 1 and a distribution Pr0 with Pr0 (X = 0) = 1,
but no distribution Pr00 with Pr00 (X = 0) = 1 and Pr00 (Y = 0 | X = 0) = 1. Let  stand for
standard conditioning. We now show that  is not calibrated. For x  {0, 1}, we have
(P, x)Y = (P | X = x)Y = (Y),
411

(7)

fiGrunwald & Halpern

that is, conditioning both on X = 0 and on X = 1 leads to the set of all distributions on
Y. It follows that, for x  {0, 1}, [x],P = {0, 1} = X , so that
(P | [x],P )Y = PY = {Pr  (Y) | Pr(Y = 1) = 0.5}.

(8)

Together, (7) and (8) show that  is not calibrated.
Corollary 6.4 gives conditions under which standard conditioning is calibrated. Theorem 6.3(a) gives general conditions under which C-conditioning satisfies one inclusion required for calibration; specifically, (P | [x],P )Y  (P, x)Y . Rather than trying to find
conditions under which the other inclusion holds, we consider a strengthening of calibration, which is arguably a more interesting notion. For, as the following example shows,
calibration it is arguably too weak a requirement.
Example 6.7: Let X = Y = {0, 1}, and let P = {Pr} consist of all distributions on X  Y
satisfying Pr(Y = X) = 1. Then the rule  that ignores X, that is, with (P, x) = P for
x  {0, 1}, is calibrated, even though (a) it outputs all distributions on Y, and (b) there
exists another calibrated rule (standard conditioning) that, upon observing X = x, outputs
only one distribution on Y.
Intuitively, the fewer distributions that there are in P, the more information P contains.
Thus, we want to restrict ourselves to sets P that are as small as possible, while still being
calibrated.
Definition 6.8: Update rule 0 is narrower than update rule  relative to P if, for all
x  X , 0 (P, x)Y  (P, x)Y . 0 is strictly narrower relative to P if the inclusion is strict
for some x.  is sharply calibrated if there exists no update rule 0 that is strictly narrower
than  and that is also calibrated.
We now show that if P is convex, then every sharply calibrated update rule must involve
C-conditioning. To make this precise, we need the following definition.
Definition 6.9:  is a generalized conditioning update rule if, for all convex P, there exists
a partition C (that may depend on P) such that for all x  X , (P, x) = P | C(x).
Note that, as long as P is convex, in a generalized conditioning rule, we condition on a
partition of X , but the partition may depend on the set P. For example, for some convex
P, the rule may ignore the value of x, whereas for other convex P, it may amount to
ordinary conditioning. Since we are only interested in generalized conditioning rules when
P is convex, their behavior on nonconvex P is irrelevant. Indeed, the next result shows that,
if we require only that P be convex (and do not require that P = hPi), then C-conditioning
is calibrated, indeed, sharply calibrated, for some C; moreover, every sharply calibrated
update rule must be a generalized conditioning rule.
Theorem 6.10: Suppose that P is convex.
(a) C-conditioning is sharply calibrated relative to P for some partition C.
412

fiMaking Decisions Using Sets of Probabilities

(b) If  is sharply calibrated relative to P, then there exists some C such that  is equivalent to C-conditioning on P (i.e., (P, x) =  | C(x) for all x  X ).
Corollary 6.11: There exists a generalized conditioning update rule that is sharply calibrated relative to all convex P. Moreover, every update rule that is sharply calibrated relative
to all convex P is a generalized conditioning update rule relative to the set of all convex P.
Theorem 6.10 establishes a connection between sharp calibration and C-conditioning. We
now show that the same conditions that make standard conditioning calibrated also make
it sharply calibrated.
Theorem 6.12: If P is convex and P = hPi, then standard conditioning is sharply calibrated relative to P.
This result shows that the P = hPi condition in Theorem 6.12 is not just relevant for
ensuring time consistency, but also for ensuring the well-behavedness of conditioning in
terms of calibration. Note, however, that the result says nothing about C-conditioning for
arbitrary partitions C. In general, C-conditioning may be sharply calibrated relative to
some convex P with P = hPi, but not relative to others. For example, if P is a singleton,
then it is convex, P = hPi, and the update rule that ignores x is sharply calibrated. In
Example 6.7, P is also convex and P = hPi, yet ignoring x is not sharply calibrated.
Remark All the results in this section were based on a definition of calibration in which
the updated set of distributions (P, x) is marginalized to Y. It is also possible to define
calibration without this marginalization. However, we found that this makes for a less
interesting notion. For example, without marginalizing on Y there no longer seems to be a
straightforward way of defining sharp calibration, and without sharpness, the notion is of
quite limited interest. Moreover, it does not seem possible to state and prove an analogue
of Theorem 6.3 (at least, we do not know how to do it).

7. Discussion and Related Work
We have examined how to update uncertainty represented by a set of probability distributions, where we motivate updating rules in terms of the minimax criterion. Our key
innovation has been to show how different approaches can be understood in terms of a
game between a bookie and an agent, where the bookie picks a distribution from the set
and the agent chooses an action after making an observation. Different approaches to updating arise depending on whether the bookies choice is made before or after the observation.
We believe that this game-theoretic approach should prove useful more generally in understanding different approaches to updating. In fact, after the publication of the conference
version of this paper, we learned that Ozdenoren and Peck (2008) use the same type of
approach for analyzing dynamic situations related to the Ellsberg (1961) paradox. Like us,
Ozdenoren and Peck resolve apparent time inconsistency by describing the decision problem
as a game between an agent and a bookie (called malevolent nature by them). Just as we
do, they point out that different games lead to different Nash equilibria, and hence different
minimax optimal strategies for the agent. In particular, although the precise definitions
413

fiGrunwald & Halpern

differ, their game 1 is similar in spirit to our P-game, and their game 3 is in the spirit of
our P-X-game.
We (as well as Ozdenoren and Peck, 2008) prove our results under the assumptions
that the set of possible values of X and Y is finite, as is the set of actions. It would be
of interest to extend this results to the case where these sets are infinite. The extension
seems completely straightforward in the case that the set of values and the set of actions
is countable, and we only consider bounded loss functions (i.e. supyY,aA |L(y, a)| < ).
Indeed, we believe that our results should go through without change in this case, although
we have not checked the details. However, once we allow an uncountable set of values, then
some subtleties arise. For example, in the P-X game, we required nature to choose a value
x that was given positive probability by some Pr  P. But there may not be such an x if
the set of possible values of X is the interval [0, 1]; all the measures in P may then assign
individual points probability 0.
We conclude this paper by giving an overview of the senses in which conditioning is
optimal and the senses in which it is not, when uncertainty is represented by a set of
distributions. We have established that conditioning the full set P on X = x is minimax
optimal in the P-x-game, but not in the P-game. The minimax-optimal decision rule in
the P-game is often an instance of C-conditioning, a generalization of conditioning. The
Monty Hall problem showed, however, that this is not always the case. On the other hand,
if instead of the minimax criterion, we insist that update rules are sharply calibrated, then
if P is convex, C-conditioning is always the right thing to do after all. While, in general,
C may depend on P (Theorem 6.10), if P = hPi, we can take C(x) = {x}, so standard
conditioning is the right thing to do (Theorem 6.12).
There are two more senses in which conditioning is the right thing to do. First, Walley
(1991) shows that, in a sense, conditioning is the only updating rule that is coherent,
according to his notion of coherence. He justifies coherence decision theoretically, but not
by using the minimax criterion. Note that the minimax criterion puts a total order on
decision rules. That is, we can say that  is at least as good as  0 if
max EPr [L ]  max EPr [L0 ].

PrP

PrP

By way of contrast, Walley (1991) puts a partial preorder11 on decision rules by taking 
to be at least as good as  0 if
max EPr [L  L0 ]  0.
PrP

Since both maxPrP EPr [L  L0 ] and maxPrP EPr [L0  L ] may be positive, this is indeed
a partial order. If we use this ordering to determine the optimal decision rule then, as
Walley shows, conditioning is the only right thing to do.
Second, in this paper, we interpreted conditioning as conditioning the full given set
of distributions P. Then conditioning is not always an a priori minimax optimal strategy
on the observation X = x. Alternatively, we could first somehow select a single Pr  P,
condition Pr on the observed X = x, and then take the optimal action relative to Pr | X = x.
It follows from Theorem 3.1 that the minimax-optimal decision rule   in a P-game can be
11. For a partial order  is reflexive, transitive, and anti-symmetric, so that if x  y and x  y, we must
have x = y. A partial preorder is just reflexive and transitive.

414

fiMaking Decisions Using Sets of Probabilities

understood this way. It defines the optimal response to the distribution Pr  (X  Y)
defined in Theorem 3.1(b)(ii). If P is convex, then Pr  P. In this sense, the minimaxoptimal decision rule can always be viewed as an instance of conditioning, but on a single
special Pr that depends on the loss function L rather than on the full set P.
It is worth noting that Grove and Halpern (1998) give an axiomatic characterization of
conditioning sets of probabilities, based on axioms given by van Fraassen (1987, 1985) that
characterize conditioning in the case that uncertainty is described by a single probability
distribution. As Grove and Halpern point out, their axioms are not as compelling as those
of van Fraassen. It would be interesting to know whether a similar axiomatization can be
used to characterize the update notions that we have considered here.

Acknowledgments
A preliminary version of this paper appears in Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference, 2008, with the title A Game-Theoretic Analysis
of Updating Sets of Probabilities. The present paper expands on the conference version
in several ways. Most importantly, the section on calibration has been entirely rewritten,
with a significant error corrected. We would like to thank Wouter Koolen, who pointed
out an error in a previous version of Definition 5.3, and the anonymous referees for their
thoughtful remarks. Peter Grunwald is also affiliated with Leiden University, Leiden, the
Netherlands. He was supported by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. Joseph Halpern was supported
in part by NSF under grants ITR-0325453, IIS-0534064, IIS-0812045, and IIS-0911036, by
AFOSR under grant FA9550-05-1-0055 and FA9550-08-1-0438, and by ARO under grant
W911NF-09-1-0281.

Appendix A. Proofs
To prove Theorems 3.1 and Theorem 3.2, we need two preliminary observations. The first
is a characterization of Nash equilibria. In the P-game, a Nash equilibrium or saddle point
amounts to a pair (  ,   ) where   is a distribution in P and   is a randomized decision
rule such that
E EPr [L ] = minD(X ,A) E [EPr [L ]]
(9)
= maxPrP EPr [L ],
where E [EPr [L ]] is just PrP, (Pr)>0   (Pr)EPr [L ]. In the P-x-game, a Nash equilibrium is a pair (  ,   ) where   is a distribution in P | X = x and   is a randomized
decision rule, such that (9) holds with P replaced by P | X = x.
The second observation we need is the following special case of Theorem 3.2 from the
work of Grunwald and Dawid (2004), itself an extension of Von Neumanns original minimax
theorem.
P

Theorem A.1: If Y 0 is a finite set, P 0 is a closed and convex subset of (Y 0 ), A0 a closed
and convex subset of IRk for some k  IN , and L0 : Y 0  A0  IR is a bounded function such
that, for each y  Y 0 , L(y, a) is a continuous function of a, then there exists some Pr  P 0
415

fiGrunwald & Halpern

and some   A0 such that,
EPr [L0 (Y 0 ,  )] = minA0 EPr [L0 (Y 0 , )]
= maxPrP 0 EPr [L0 (Y 0 ,  )].

(10)

With these observations, we are ready to prove Theorem 3.1:
Theorem 3.1: Fix X , Y, A, L, and P  (X  Y).
(a) The P-game has a Nash equilibrium (  ,   ), where   is a distribution over P with
finite support.
(b) If (  ,   ) is a Nash equilibrium of the P-game such that   has finite support, then
(i) for every distribution Pr0  P in the support of   , we have
EPr0 [L ] = maxPrP EPr [L ];
(ii) if Pr = PrP, (Pr)>0   (Pr) Pr (i.e., Pr is the convex combination of the
distributions in the support of   , weighted by their probability according to   ),
then
EPr [L ] = minD(X ,A) EPr [L ]
= maxPrP minD(X ,A) EPr [L ]
= minD(X ,A) maxPrP EPr [L ]
= maxPrP EPr [L ].
P

Proof: To prove part (a), we introduce a new loss function L0 that is essentially equivalent
to L, but is designed so that Theorem A.1 can be applied. Let Y 0 = X Y, let A0 = D(X , A),
and define the function L0 : Y 0  A0  IR as
L0 ((x, y), ) := L (x, y) =

X

(x)(a)L(y, a).

aA

Obviously L0 is equivalent to L in the sense that for all Pr  (X  Y), for all   D(X , A),
EPr [L ] = EPr [L0 ((X, Y ), )].
If we view A0 = D(X , A) as a convex subset of IR|X |(|A|1) , then L0 ((x, y), a) becomes
a continuous function of a  A0 . Let P 0 be the convex closure of P. Since X  Y is
finite, P 0 consists of all distributions Pr on (X , Y) of the form c1 Pr1 +    + ck Prk for
k = |X  Y|, where Pr1 , . . . , Prk  P and c1 , . . . , ck are nonnegative real coefficients such
that c1 +    + ck = 1. Applying Theorem A.1 to L0 and P 0 , it follows that (10) holds for
some Pr  P 0 and some    A0 = D(X , A) (that is, the  in (10) is   ). Thus, there must
P
be some distribution   on P with finite support such that Pr = PrP, (Pr)>0   (Pr) Pr.
It is easy to see that the two equalities in (10) are literally the two equalities in (9). Thus,
(  ,   ) is a Nash equilibrium. This proves part (a).
To prove part (b)(i), suppose first that (  ,   ) is a Nash equilibrium of the P-game
such that   has finite support. Let V = maxPrP EPr [L ]. By (9), we have that
X

  (Pr)EPr [L ] = V.

PrP,  (Pr)>0

416

(11)

fiMaking Decisions Using Sets of Probabilities

Trivially, for each Pr0  P, we must have EPr0 [L ]  maxPrP EPr [L ]. If this inequality
were strict for some Pr0  P in the support of   , then
X

  (Pr)EPr [L ] < V,

PrP,  (Pr)>0

contradicting (11). This proves part (b)(i).
To prove part (b)(ii), note that straightforward arguments show that
maxPrP EPr [L ]
 minD(X ,A) maxPrP EPr [L ]
 maxPrP minD(X ,A) EPr [L ]
 minD(X ,A) EPr [L ].
(The second inequality follows because, for all Pr0  P, minD(X ,A) maxPrP EPr [L ] 
minD(X ,A) EPr0 [L ].) Since (  ,   ) is a Nash equilibrium, part (b)(ii) is immediate, using
the equalities in (9).
Theorem 3.2: Fix X , Y, A, L, P  (X  Y).
(a) The P-x-game has a Nash equilibrium (  ,   (x)), where   is a distribution over
P | X = x with finite support.
(b) If (  ,   (x)) is a Nash equilibrium of the P-x-game such that   has finite support,
then
(i) for all Pr0 in the support of   , we have
EPr0 [L ] = maxPrP|X=x EPr [L ];
(ii) if Pr =

P

PrP,  (Pr)>0 

=
=
=
=

 (Pr) Pr,

then

EPr [L ]
minD(X ,A) EPr [L ]
maxPrP|X=x minD(X ,A) EPr [L ]
minD(X ,A) maxPrP|X=x EPr [L ]
maxPrP|X=x EPr [L ].

Proof: To prove part (a), we apply Theorem A.1, setting L0 = L, Y 0 = Y, A0 = (A), and
P 0 to the convex closure of P | X = x. Thus, (10) holds for some   A0 , which we denote
  (x). As in the proof of Theorem 3.1, there must be some distribution   on P | X = x
P
with finite support such that Pr = PrP|X=x, (Pr)>0   (Pr) Pr. The remainder of the
argument is identical to that in Theorem 3.1.
The proof of part (b) is completely analogous to the proof of part (b) of Theorem 3.1,
and is thus omitted.
Theorem 4.4: Given a decision setting DS = (X , Y, A, P) such that P = hPi, then for
all decision probems DP based on DS, there exists an a priori minimax-optimal rule that
417

fiGrunwald & Halpern

is also a posteriori minimax optimal. Indeed, every a posteriori minimax-optimal rule is
also an a priori minimax-optimal rule. If, for all Pr  P and all x  X , Pr(X = x) > 0,
then for every decision problem based on DS, every a priori minimax-optimal rule is also
a posteriori minimax optimal.
Proof: Let X + = {x  X : maxPrP Pr(X = x) > 0}. Let m be a random variable on
X defined by taking m (x) = 0 if x 
/ X + , and m (x) = maxPr0 P|X=x EPr0 [L ] if x  X + .
We first show that for every   D(X , A),
max EPr [L ] = max

PrP

PrP

X

PrX (X = x)m (x).

(12)

xX

Note that
EPr [L ] =
=
=

=
=

P
Pr((X, Y ) = (x, y))L (x, y)
P(x,y)X Y
P
PrX (X = x) yY Pr(Y = x | X = x)L (x, y)
{xX
:Pr
(x)>0}
X
P
Pr (X = x)EPr|X=x [L ]
P{xX :PrX (x)>0} X
Pr (X = x) maxPr0 P|X=x EPr0 [L ]
P{xX :PrX (x)>0} X
Pr (X = x)m (x)
P{xX :PrX (x)>0} X
xX

PrX (X = x)m (x).

Taking the max over all Pr  P, we get that
max EPr [L ]  max

PrP

PrP

X

PrX (X = x)m (x).

xX

It remains to show the reverse inequality in (12). Since P is closed, there exists Pr  P
such that
X
X
max
PrX (X = x)m (x) =
PrX (X = x)m (x).
PrP

xX

xX

Moreover, since P | X = x is closed, if x  X + , there exists Prx  P | X = x such that
m (x) = EPrx [L ]. Define Pr  (X  Y) by taking
(

Pr ((X, Y ) = (x, y)) =

0
if x 
/ X+

x
PrX (X = x) Pr (Y = y) if x  X + .

Clearly PrX = PrX and (Pr | X = x) = (Prx | X = x)  P | X = x if x  X + . Thus, by
definition, Pr  hPi. Since, by assumption, hPi = P, it follows that Pr  P. In addition,
it easily follows that
P
maxPrP xX PrX (X = x)m (x)
P
=
PrX (X = x)m (x)
PxX
P



=
xX + PrX (X = x)
= EPr [L ]
 maxPrP EPr [L ].

yY

Pr (Y = y | X = x)L (x, y)

This establishes (12).
Now let   be an a priori minimax decision rule. Since the P-game has a Nash equilibrium (Theorem 3.1), such a   must exist. Let X 0 be the set of all x0  X for which   is not
418

fiMaking Decisions Using Sets of Probabilities

minimax optimal in the Px0 -game, i.e., x0  X 0 iff x  X + and maxPr0 P|X=x0 EPr0 [L ] >
minD(X ,A) maxPr0 P|X=x0 EPr0 [L ]. Define  0 to be a decision rule that agrees with   on
X \ X 0 and is minimax optimal in the P | X = x0 game for all x0  X 0 ; that is,  0 (x) = (x)
for x 
/ X 0 and, for x  X 0 ,
(x)  argminD(X ,A)

max

Pr0 P|X=x0

EPr0 [L ].

By construction, m0 (x)  m (x) for all x  X and m0 (x) < m (x) for all x  X 0 . Thus,
using (12), we have
maxPrP EPr [L0 ]
P
= maxPrP xX Pr(X = x)m0 (x)
P
(13)
 maxPrP xX Pr(X = x)m (x)
= maxPrP EPr [L ].
Thus,  0 is also an a priori minimax-optimal decision rule. But, by construction,  0 is also
an a posteriori minimax-optimal decision rule, and it follows that there exists at least one
decision rule (namely,  0 ) that is both a priori and a posteriori minimax optimal. This
proves the first part of the theorem. To prove the last part, note that if Pr(X = x) > 0 for
all Pr  P and x  X , and X 0 6= , then the inequality in (13) is strict. It follows that X 0 is
empty in this case, for otherwise   would not be a priori minimax optimal, contradicting
our assumptions. But, if X 0 is empty, then   must also be a posteriori minimax optimal.
It remains to show that every a posteriori minimax-optimal rule is also a priori minimax
optimal. For all x  X , define mm(x) = 0 if x 6 X + , and mm(x) = min m (x) if x  X + .
Let  be the set of all a posteriori minimax-optimal rules. We have already shown that
 has at least one element, say 0 , that is also a priori minimax optimal. For all   
and all x  X , we must have m (x) = mm(x). By (12), it follows that for every    ,
P

maxPrP EPr [L ] = maxPrP xX PrX (X = x)m (x)
P
= maxPrP xX PrX (X = x)mm(x).
Hence,
max EPr [L ] = max EPr [L0 ].

PrP

PrP

Since 0 is a priori minimax optimal, this implies that all    are a priori minimax
optimal.
Proposition 4.7:
(a) Every dynamically consistent decision problem is also weakly time consistent.
(b) Not every dynamically consistent decision problem is time consistent.
(c) Every strongly dynamically consistent decision problem is time consistent.
(d) There exist weakly time consistent decision problems that are not dynamically consistent.
(e) All decision problems based on P are dynamically consistent if and only if all decision
problems based on P are weakly time consistent.
419

fiGrunwald & Halpern

Proof: Part (a) is immediate by part 1 of the definition of dynamic consistency. Part
(b) follows because the decision problem of Example 4.5 is dynamically consistent but
not time consistent. We already showed that it is not time consistent. To see that it is
dynamically consistent, note that every decision rule that can be defined on the domain in
the example is a priori minimax optimal, so part 1 of the definition of dynamic consistency
holds automatically. Part 2 also holds automatically, since for every two decision rules 
and  0 , (2) does not hold with strict inequality for X = 0.
For part (c), consider an arbitrary decision problem DP that is strongly dynamically
consistent. It is easy to construct an a posteriori minimax optimal decision rule; call it .
Since DP is strongly dynamically consistent,  must be a priori minimax optimal. Suppose,
by way of contradiction, that some decision rule  0 is a priori minimax optimal but not
a posteriori minimax. Since  is a posteriori minimax optimal, it must be the case that
(2) holds, and that the the inequality is strict for some x with Pr(X = x) > 0 for some
Pr  P. Thus, by strong dynamic consistency,  must be a priori preferred to  0 according
the minimax criterion, a contradiction to the assumption that  0 is a priori minimax optimal.
For part (d), consider Example 2.1 again, in which there was both time and dynamic
inconsistency. Randomizing with equal probabibility between 0 and 1, no matter what is
observed, is a posteriori preferred over all other randomized actions, but it was not the a
priori minimax optimal. Now we extend the example by adding an additional action 2 and
defining L(0, 2) = L(1, 2) = 1; L(y, a) remains unchanged for y  Y and a  {0, 1}. Now
both the a priori and the a posteriori minimax optimal act is to play 2, no matter what
value of X is observed, so time consistency holds. Yet dynamic consistency still does not
hold, because after observing both X = 0 and X = 1, randomizing with equal probabibility
between 0 and 1 is preferred over playing action 1, but before observing X, the decision
rule that plays action 1 no matter what is observed is strictly preferred over randomizing
between 0 and 1.
The only if direction of part (e) already follows from part (a). For the if direction,
suppose, by way of contradiction, that all decision problems based on P are weakly time
consistent, but some decision problem based on P is not dynamically consistent. This
decision problem has some loss function L, set A of actions, and two decision rules  and  0
such that  is preferred a posteriori over  0 but not a priori; thus, in the definition of dynamic
consistency, (2) holds and (3) does not. Let Lmax be the a posteriori minimax expected loss
of . Extend A and L with an additional act a0 such that for all y, L(y, a0 ) = Lmax . Now
we have a new decision problem with action set A  {a0 } in which  has become a minimax
optimal a posteriori rule (it is not the only one, but that does not matter). However, 
cannot be a priori minimax optimal, because (3) still does not hold for  and  0 :  0 is a
priori strictly better than . Hence, we do not have weak time consistency in this new
decision problem. Since it is still a decision problems based on P, we do not have weak
time consistency for all decision problems based on P, and we have arrived at the desired
contradiction.
Theorem 5.1: Fix X , Y, L, A, and P  (X  Y). If, for all PrY  PY , P contains a distribution Pr0 such that X and Y are independent under Pr0 , and Pr0Y = PrY ,
then there is an a priori minimax-optimal decision rule that ignores information. Under
these conditions, if  is an a priori minimax-optimal decision rule that ignores information,
420

fiMaking Decisions Using Sets of Probabilities

then  essentially optimizes with respect to the marginal on Y ; that is, maxPrP EPr [L ] =
maxPrY PY EPrY [L0 ].
Proof: Let P 0 be the subset of P of distributions under which X and Y are independent.
Let D(X , A)0 be the subset of D(X , A) of rules that ignore information. Let    D(X , A)0
be defined as the optimal decision rule that ignores information relative to P 0 , i.e.
max EPr [L ] =

PrP 0

min

max EPr [L ].

D(X ,A)0 PrP 0

We have
maxPrP EPr [L ] 

=
=

minD(X ,A) maxPrP EPr [L ]
minD(X ,A) maxPrP 0 EPr [L ]
minD(X ,A)0 maxPrP 0 EPr [L ] [see below]
maxPrP 0 EPr [L ].

(14)

To see that the equality between the third and fourth line in (14) holds, note that for
Pr  P 0 , we have
P

EPr [L ] =
Pr(x, y)L (x, y)
P(x,y)X Y
P
P
=
xX Pr(X = x)
yY Pr(Y = y)( aA (x)(a)L(y, a))
The decision rule that minimizes this expression is independent of x; it is the distribution
  over actions that minimizes
X

Pr(Y = y)(

X

  (a)L(y, a)).

aA

yY

This calculation also shows that, since   ignores information, for Pr  P 0 , we have that
max EPr [L ] = max EPrY [L0 ] = max0 EPr [L ].

PrP

PrY PY

PrP

(15)

This implies that the first and last line of (14) are equal to each other, and therefore also
equal to the second line of (14). It follows that   is a priori minimax optimal. Since every
a priori minimax optimal rule that ignores information must satisfy (15), the second result
follows. We next prove Theorem 6.3. We first need three preliminary results.
Lemma A.2: If P is convex and X0  X , then (P | X0 )Y is convex.
Proof: Without loss of generality, assume that (P | X0 )Y is nonempty. Given Pr00 , Pr01 
(P | X0 )Y , let Pr0 =  Pr01 +(1  ) Pr00 . We show that, for all   [0, 1], Pr0  (P | X0 )Y .
Choose Pr0 , Pr1  P with Pr0 (X0 ) > 0, Pr1 (X0 ) > 0, (Pr0 | X0 )Y = Pr00 , and (Pr1 | X0 )Y =
Pr01 . For c  [0, 1], let Prc = c Pr1 +(1  c) Pr0 . Then, for all y  Y,
Prc (Y = y | X0 ) =
=
=

Prc (XX0 ,Y =y)
Prc (XX0 )
c Pr1 (XX0 ) Pr1 (Y =y|XX0 )+(1c) Pr0 (XX0 ) Pr0 (Y =y|XX0 )
c Pr1 (XX0 )+(1c) Pr0 (XX0 )
c Pr01 (Y = y) + (1  c ) Pr00 (Y = y),

421

(16)

fiGrunwald & Halpern

where c = c Pr1 (X0 )/(c Pr1 (X0 ) + (1  c) Pr0 (X0 )). Clearly, c is a continuous increasing
function of c, with 0 = 0 and 1 = 1. Thus, there exists c such that c = . Since
c is independent of y, (16) holds for all y  Y (with the same choice of c ), That is,
(Prc | X0 )Y =  Pr00 +(1  ) Pr01  Pr0 . Thus, Pr0  (P | X0 )Y , as desired.
Lemma A.3: If U = {X1 , . . . , Xk } is a collection of nonoverlapping subsets of X (i.e., for
1  i < j  k, Xi  Xj = ), (P | X1 )Y is convex, (P | X1 )Y = (P | X2 )Y = . . . = (P | Xk )Y ,
S
and V = ki=1 Xi , then for all j  {1, . . . , k}, (P | V)Y  (P | Xj )Y .
Proof: The result is immediate if (P | V) is empty. So suppose that Pr  P and Pr(V) > 0.
Using Bayes Rule, we have that
(Pr | V)Y =

X

Pr(Xi | V)(Pr | Xi )Y .

{i:Pr(Xi |V)>0}

Now (P | X1 )Y = . . . = (P | Xk )Y by assumption. Thus, for all i such that Pr(Xi |
V) > 0, there must exist some Pri  P such that (Pr | Xi )Y = (Pri | X1 )Y . Thus,
P
(Pr | V)Y = {i:Pr(Xi |V)>0} Pr(Xi | V)(Pri | X1 )Y . Since P is convex by assumption, by
Lemma A.2, (P | X1 )Y is convex as well. Thus, we can write (Pr | V)Y as a convex
combination of elements of (P | X1 )Y , It follows that (Pr | V)Y  (P | X1 )Y . Since
(P | X1 )Y = . . . = (P | Xk )Y , it follows that (Pr | V)Y  (P | Xj )Y for all j = 1, . . . , k.
Lemma A.4: If P = hPi and U = {x1 , . . . , xk }, then

Tk

j=1 (P

| X = xj )Y  (P | U)Y .

Proof: Let Q  kj=1 (P | X = xj )Y . There must exist Pr1 , . . . , Prk  P such that, for
j = 1, . . . , k, (Prj | X = xj )Y = Q. Clearly Pr1 (x1 ) > 0. Since P = hPi, there also exists
Pr  P such that PrX = (Pr1 )X and for all j  {1, . . . , k} such that Pr1 (xj ) > 0, we have
(Pr | X = xj )Y = (Prj | X = xj )Y = Q. It follows that (Pr | U)Y = Q, so Q  (P | U)Y .
T

Theorem 6.3:
(a) If  is C-conditioning for some partition C of X and P is convex then, for all x  X ,
we have that (P | [x],P )Y  (P, x)Y .
(b) If  is standard conditioning, P = hPi, and x  X , then (P, x)Y  (P | [x],P )Y .
Proof: For part (a), since P is convex, by Lemma A.2, (P | X 0 )Y is convex for all X 0  X .
Let U = {C(x0 ) | x0  [x],P }. By the definition of [x],P , for all x0  [x],P , we have
(P, x0 ) = P | C(x0 ) = P | C(x) = (P, x).
Thus, by Lemma A.3, (P | V)Y  (P, x)Y , where V = U = [x],P C(x0 ). This proves
part (a).
For part (b), since  is standard conditioning, we have that (P | X = x)Y = (P | X =
x0 )Y for all x0  U. By assumption, P = hPi. Thus, it follows immediately from Lemma A.4
(taking U = [x],P ) that (P, x)Y  (P | [x],P )Y , as desired.
S

We next want to prove Theorem 6.10. We first need a definition and a preliminary
result.
422

fiMaking Decisions Using Sets of Probabilities

Definition A.5 : An update rule  is semi-calibrated relative to P if (P | [x],P )Y 
(P, x)Y .
Note that, by Theorem 6.3, if P is convex, then C-conditioning is semi-calibrated for all C.
Lemma A.6: If  is semi-calibrated relative to P and C = {[x],P | x  X }, then C is a
partition of X and
(a) C-conditioning is narrower than  relative to P.
(b) If C-conditioning is not strictly narrower than  relative to P, then  is equivalent
to C-conditioning on P, and is calibrated.
Proof: Clearly C is a partition of X . For part (a), if 0 is C-conditioning then, by definition,
0 (P, x) = P | C(x) = P | [x],P . Since  is semi-calibrated, (P | [x],P )Y  ((P, x))Y .
Thus, C-conditioning is narrower than  relative to P.
For part (b), if C-conditioning (i.e., 0 ) is not strictly narrower than  relative to P,
then we must have (P(, x))Y = (P 0 (, x))Y for all x  X , so (P | [x],P )Y = (P, x)Y ,
and  is claibrated relative to P.
Theorem 6.10:
(a) C-conditioning is sharply calibrated relative to P for some partition C.
(b) If  is sharply calibrated relative to P, then there exists some C such that  is equivalent to C-conditioning on P (i.e., (P, x) =  | C(x) for all x  X ).
Proof: We can place a partial order P on partitions C by taking C1 P C2 if C1 conditioning is narrower than C2 conditioning relative to P. Since X is finite, there are
only finitely many possible partitions of X . Thus, there must be some minimal elements
of P . We claim that each minimal element of P is sharply calibrated relative to P.
For suppose that C0 is minimal relative to P . Because P is convex, C0 -conditioning is
semi-calibrated (Theorem 6.3) and we can apply Lemma A.6 with  as C0 . Because C0
is minimal, the C defined in Lemma A.6 cannot be strictly narrower than C0 . It follows
by Lemma A.6(b) that C0 -conditioning is calibrated. We now show that C0 -conditioning is
in fact sharply calibrated, by showing that there exists no calibrated update rule that is
a strict narrowing of C0 -conditioning. For suppose, by way of contradiction, that  is an
update rule that is calibrated and that is strictly narrower than C0 relative to P. Then by
Lemma A.6(a) there exists a partition C such that C is narrower than  relative to P. But
then C <P C0 , contradicting the minimality of C0 . This proves part (a).
For part (b), suppose that  is sharply calibrated relative to P. By Lemma A.6(a),
there must be some partition C such that C-conditioning is narrower than , relative to P.
Let C0 be a minimal element of P such that C0 P C. Part (a) shows that C0 -conditioning
is sharply calibrated relative to P. Since C0 -conditioning is narrower than , and we  is
sharply calibrated relative to P, we must have that C0 -conditioning is not strictly narrower
than  relative to P, and hence  is equivalent to C0 -conditioning on P.
Theorem 6.12: If P is convex and P = hPi, then standard conditioning is sharply
calibrated relative to P.
423

fiGrunwald & Halpern

Proof: By Corollary 6.4, standard conditioning is calibrated relative to P under the stated
assumptions on P. To show that it is sharply calibrated, suppose that there exists some
update rule 0 that is narrower than standard conditioning, and that is sharply calibrated
relative to P. By Theorem 6.10, 0 is equivalent to C-conditioning for some C relative to
P. Thus, for all x  X and all x0  C(x), we have that
(P | C(x))Y  (P | x0 )Y ,
so
(P | C(x))Y 

\

(P | x0 )Y .

x0 C(x)

By Lemma A.4, it is immediate that
\

(P | x0 )Y  (P | C(x))Y .

x0 C(x)

Thus, we must have
\

(P | x0 )Y = (P | C(x))Y .

(17)

x0 C(x)

Now we want to show that, for all x0  C(x), we have that (P | C(x))Y = (P | x0 )Y . This
will show that C is equivalent to conditioning, and that conditioning is sharply calibrated.
Suppose not, and that Q  (P | x0 )Y  P | C(x)Y for some x0  C(x). Let Q0 be the
distribution in (P | C(x))Y that is closest to Q. The fact that there is such a distribution
Q0 follows from the fact that P is closed (recall that we assume that P is closed throughout
the paper). (In fact, it follows from convexity that Q0 is unique, but this is not necessary for
our argument.) Since Q0  (P | C(x))Y , it follows from (17) that, for each x00  C(x), there
must be some distribution Prx00  P such that Prx00 (x00 ) > 0 and (Prx00 | x00 )Y = Q. Since
P is convex, there is some distribution Pr  P such that Pr (x00 ) > 0 for all x00  C(x)
(indeed, Pr can be any convex combination of the distributions Prx00 for x00  C where all
the coefficients are positive). Since P = hPi, there must exist a distribution Pr  P such
that (Pr)X = (Pr )X (so that Pr is positive on all elements of C), (Pr | x00 )Y = Q0 for all
x00  C(x) other than x0 , and (Pr | x0 )Y = Q. Note that (Pr | (C(x)  {x0 }))Y = Q0 . Thus,
(Pr | C(x)Y = c(Pr | C(x)  x0 )Y + (1  c)(Pr | x0 )Y = cQ0 + (1  c)Q,
for some c such that 0 < c < 1. Clearly cQ0 + (1  c)Q is closer to Q than Q0 is. This gives
the desired contradiction.

References
Augustin, T. (2003). On the suboptimality of the generalized Bayes rule and robust Bayesian
procedures from the decision theoretic point of view: A cautionary note on updating
imprecise priors. In 3rd International Symposium on Imprecise Probabilities and Their
Applications, pp. 3145. Available at http://www.carleton-scientific.com/isipta/2003toc.html.
424

fiMaking Decisions Using Sets of Probabilities

Cozman, F. G., & Walley, P. (2001).
Graphoid properties of epistemic irrelevance and independence.
In 2nd International Symposium on Imprecise Probabilities and Their Applications, pp. 112121.
Available at
http://www.sipta.org/ isipta01/proceedings/index.html.
da Rocha, J. C. F., & Cozman, F. G. (2002). Inference with separately specified sets of
probabilities in credal networks. In Proc. Eighteenth Conference on Uncertainty in
Artificial Intelligence (UAI 2002), pp. 430437.
Dawid, A. P. (1982). The well-calibrated Bayesian. Journal of the American Statistical
Association, 77, 605611. Discussion: pages 611613.
de Finetti, B. (1936). Les probabilites nulles. Bulletins des Science Mathematiques (premiere
partie), 60, 275288.
Ellsberg, D. (1961). Risk, ambiguity, and the Savage axioms. Quarterly Journal of Economics, 75, 643649.
Epstein, L. G., & Schneider, M. (2003). Recursive multiple priors. Journal of Economic
Theory, 113 (1), 131.
Gardenfors, P., & Sahlin, N. (1982). Unreliable probabilities, risk taking, and decision
making. Synthese, 53, 361386.
Gilboa, I., & Schmeidler, D. (1989). Maxmin expected utility with a non-unique prior.
Journal of Mathematical Economics, 18, 141153.
Grove, A. J., & Halpern, J. Y. (1998). Updating sets of probabilities. In Proc. Fourteenth
Conference on Uncertainty in Artificial Intelligence (UAI 98), pp. 173182.
Grunwald, P. D., & Dawid, A. P. (2004). Game theory, maximum entropy, minimum
discrepancy, and robust Bayesian decision theory. The Annals of Statistics, 32 (4),
13671433.
Grunwald, P. D., & Halpern, J. Y. (2004). When ignorance is bliss. In Proc. Twentieth
Conference on Uncertainty in Artificial Intelligence (UAI 2004), pp. 226234.
Herron, T., Seidenfeld, T., & Wasserman, L. (1997). Divisive conditioning: Further results
on dilation. Philosophy of Science, 64, 411444.
Huber, P. J. (1981). Robust Statistics. Wiley, New York.
Hughes, R. I. G., & van Fraassen, B. C. (1985). Symmetry arguments in probability kinematics. In Kitcher, P., & Asquith, P. (Eds.), PSA 1984, Vol. 2, pp. 851869. Philosophy
of Science Association, East Lansing, Michigan.
Lewis, D. (1976). Probability of conditionals and conditional probabilities. Philosophical
Review, 83 (5), 297315.
Mosteller, F. (1965). Fifty Challenging Problems in Probability with Solutions. AddisonWesley, Reading, Mass.
425

fiGrunwald & Halpern

Ozdenoren, E., & Peck, J. (2008). Ambiguity aversion, games against nature, and dynamic
consistency. Games and Economic Behavior, 62 (1), 106115.
Popper, K. R. (1968). The Logic of Scientific Discovery (2nd edition). Hutchison, London.
The first version of this book appeared as Logik der Forschung, 1934.
Sarin, R., & Wakker, P. (1998). Dynamic choice and nonexpected utility. Journal of Risk
and Uncertainty, 17 (2), 87120.
Savage, L. J. (1954). Foundations of Statistics. Wiley, New York.
Seidenfeld, T. (2004). A contrast between two decision rules for use with (convex) sets of
probabilities: -maximin versus E-admissibility. Synthese, 140 (12), 6988.
Seidenfeld, T., & Wasserman, L. (1993). Dilation for convex sets of probabilities. Annals
of Statistics, 21, 11391154.
van Fraassen, B. C. (1987). Symmetries of personal probability kinematics. In Rescher, N.
(Ed.), Scientific Enquiry in Philosophical Perspective, pp. 183223. University Press
of America, Lanham, Md.
vos Savant, M. (Sept. 9, 1990). Ask Marilyn. Parade Magazine, 15. Follow-up articles
appeared in Parade Magazine on Dec. 2, 1990 (p. 25) and Feb. 17, 1991 (p. 12).
Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic Learning in a Random World.
Springer, New York.
Wald, A. (1950). Statistical Decision Functions. Wiley, New York.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities, Vol. 42 of Monographs
on Statistics and Applied Probability. Chapman and Hall, London.

426

fiJournal of Artificial Intelligence Research 42 (2011) 575605

Submitted 06/11; published 12/11

Computing Approximate Nash Equilibria
and Robust Best-Responses Using Sampling
Marc Ponsen
Steven de Jong

M . PONSEN @ MAASTRICHTUNIVERSITY. NL
STEVEN . DEJONG @ MAASTRICHTUNIVERSITY. NL

Department of Knowledge Engineering
Maastricht University, The Netherlands

Marc Lanctot

LANCTOT @ UALBERTA . CA

Department of Computer Science
University of Alberta, Canada

Abstract
This article discusses two contributions to decision-making in complex partially observable
stochastic games. First, we apply two state-of-the-art search techniques that use Monte-Carlo sampling to the task of approximating a Nash-Equilibrium (NE) in such games, namely Monte-Carlo
Tree Search (MCTS) and Monte-Carlo Counterfactual Regret Minimization (MCCFR). MCTS has
been proven to approximate a NE in perfect-information games. We show that the algorithm quickly
finds a reasonably strong strategy (but not a NE) in a complex imperfect information game, i.e.
Poker. MCCFR on the other hand has theoretical NE convergence guarantees in such a game. We
apply MCCFR for the first time in Poker. Based on our experiments, we may conclude that MCTS
is a valid approach if one wants to learn reasonably strong strategies fast, whereas MCCFR is the
better choice if the quality of the strategy is most important.
Our second contribution relates to the observation that a NE is not a best response against
players that are not playing a NE. We present Monte-Carlo Restricted Nash Response (MCRNR),
a sample-based algorithm for the computation of restricted Nash strategies. These are robust bestresponse strategies that (1) exploit non-NE opponents more than playing a NE and (2) are not
(overly) exploitable by other strategies. We combine the advantages of two state-of-the-art algorithms, i.e. MCCFR and Restricted Nash Response (RNR). MCRNR samples only relevant parts
of the game tree. We show that MCRNR learns quicker than standard RNR in smaller games. Also
we show in Poker that MCRNR learns robust best-response strategies fast, and that these strategies
exploit opponents more than playing a NE does.

1. Introduction
This article investigates decision-making in strategic, complex multi-player games. As our most
complex test-bed, we use the game of two-player Limit Texas Holdem Poker (henceforth abbreviated as Poker or full-scale Poker). In this introduction, we will first briefly outline why research
in such games is relevant. Then, we discuss the complexity factors involved in games. Finally, we
outline our approach and contributions.
1.1 Relevance of Games-Related Research
Games have attracted scientific attention for years now; the importance of research in the area of
game theory became apparent during the Second World War (Osborne & Rubinstein, 1994). Nowadays, examples of serious games can be found in many real-life endeavors, such as economics (e.g.,

c
2011
AI Access Foundation. All rights reserved.

fiP ONSEN , L ANCTOT & D E J ONG

buyers and sellers in the stock market have the goal to maximize profit) or politics (e.g., politicians
have the goal to collect sufficient political support for some cause). Games that serve as entertainment, such as puzzles, board-, sports- or modern video-games, are often abstracted, simpler variants
of serious games. An example is the card game of Poker. The objective in Poker is not very different
from the objective of investors in the stock market. Players may invest (or risk) money and speculate on future events that may or may not yield profit. Because strategies in abstract games (such as
Poker) can be more easily and rapidly evaluated than strategies in real-life endeavors (such as acting
in the stock market), abstract games are the perfect tool for assessing and improving the strategic
decision-making abilities of humans as well as computers. For this reason, various complex multiplayer games have attracted a great deal of attention from the artificial intelligence (AI) community
(Schaeffer, 2001).
1.2 Complexity Factors in Games
Games are characterized by several complexity factors. We briefly mention some factors that are
relevant for the work presented in this article.
 Number of players. Multi-player games are generally assumed to be more complex than
single-player games. Within the class of multi-player games, fully competitive games, also
known as zero-sum games, are those games where all players have conflicting goals and
therefore deliberately try to minimize the payoff of others.
 Size of the state space. The size of the state space (the number of different situations the
game may be in) varies from game to game, depending on the number of legal situations
and the number of players. Large state spaces produce more complex games because of the
computational requirements for traversing the entire state space.
 Uncertainty. Stochastic games, as opposed to deterministic games, are more complex because
there is uncertainty about the effects of actions, or occurrences of (future) events, for instance
because die rolls are involved.
 Imperfect information. Parts of the game state may be hidden to the players, e.g., opponent
cards in any card game, or the probability of a certain chance outcome. This is also known as
partial observability.
In the remainder of this article, we deal with partially observable stochastic games (Fudenberg &
Tirole, 1991), using a full-scale Poker game as our most complex test-bed. The game is a multiplayer, competitive, partially observable stochastic game. It is a daunting game for both human and
AI players to master.
1.3 Our Contributions
We investigate two different sampling-based approaches for decision-making, namely (1) a classical
game-theoretic approach and (2) a best-response approach.
For our first approach, we apply current state-of-the-art algorithms to the task of computing
an approximated Nash-Equilibrium strategy (NES) in the game of Poker. In a two-player, zerosum game, the expected value of a NES is a constant, regardless of the opponent strategy or the
specific NES. In fair games (i.e. players have equal chance of winning), such strategies cannot lose
576

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

in expectation and may win in the long run. For complex games such as Poker, a NES can only
be computed by introducing abstractions. Also, sampling algorithms may be used to (relatively)
quickly compute an approximate NES. We use both abstractions as well as sampling in this work.
We will look at two families of algorithms, both of which rely on Monte-Carlo sampling, namely
Monte-Carlo Tree Search (MCTS), including Upper Confidence Bounds applied to Trees (Kocsis
& Szepesvari, 2006; Chaslot, Saito, Bouzy, Uiterwijk, & van den Herik, 2006; Coulom, 2006), and
a regret-minimizing algorithm, called Monte-Carlo Counterfactual Regret Minimization (MCCFR)
(Lanctot, Waugh, Zinkevich, & Bowling, 2009). We are the first to offer a comparison between these
two algorithms in full-scale Poker.1
For our second approach, we begin with the observation that a NES is not necessarily most profitable against any strategy other than a NES. Above all, it is a safe strategy. If we have information on
the strategy of opponent players, we can adapt our strategy based on this, i.e., by learning so-called
best-response strategies. Rather than playing the safe NES (i.e., play not to lose), we want to learn
tailored counter-strategies based on an opponent model (i.e., play to win). For learning a good compromise between best response and equilibrium, we combine the general technique of Restricted
Nash Response (RNR) (Johanson, Zinkevich, & Bowling, 2008) with the Monte-Carlo Counterfactual Regret Minimization (MCCFR) algorithm, leading to a new algorithm, named Monte-Carlo
Restricted Nash Response (MCRNR).
1.4 Structure of this Article
The remainder of this article is structured as follows. Section 2 provides a brief overview of the
background knowledge required for this article, i.e., game-theoretic concepts focussed on extensiveform games, and a discussion on the games used in the article. Section 3 contains our work on a
comparison of Monte-Carlo Tree Search (MCTS) and Monte-Carlo Counterfactual Regret Minimization (MCCFR) in full-scale Poker. Section 4 introduces Monte-Carlo Restricted Nash Response
(MCRNR) and describes a set of experiments in smaller games as well as Poker. Finally, in Section
5, we conclude the article.

2. Background
In the current section we provide some background information. In Section 2.1 we discuss game
theory (GT) fundamentals, focussing on extensive-form games. We can represent partially observable stochastic games by means of such games (Fudenberg & Tirole, 1991). Our main test domain,
two-player limit Texas Holdem Poker, is introduced in Section 2.2, along with a number of smaller
games we also use to validate our new algorithm experimentally.
2.1 Game Theory and Extensive Form Games
Game theory (GT) studies strategic decision-making in games with two or more players. The basic assumptions that underlie the theory are that players are rational, i.e. they are self-interested
and able to optimally maximize their payoff, and that they take into account their knowledge or
1. We note that we do not aim to join the arms race to compute the closest Nash-Equilibrium (NE) approximation
for full-scale Poker. The aim of our contribution is a comparison of two recent promising algorithms. Poker is the
test-bed we chose to use because it is the most complex partially observable stochastic game both algorithms have
been applied to thus far, and because there exist reasonably strong benchmarks to test against.

577

fiP ONSEN , L ANCTOT & D E J ONG

expectations of other decision-makers behavior, i.e. they reason strategically (Fudenberg & Tirole,
1991; Osborne & Rubinstein, 1994). The field originated in economics to analyze behaviour in noncooperative settings, and was firmly established by von Neumann and Morgenstern (1944). Nash
(1951) introduced what is now known as the Nash-Equilibrium (NE). In the current section we
briefly discuss the fundamentals of GT and extensive-form games.
2.1.1 G AMES
Games are descriptions of strategic interaction between players. They specify a set of available
actions to players and a payoff for each combination of actions. Game-theoretic tools can be used
to formulate solutions for classes of games and examine their properties. Typically, a distinction is
made between two types of game representations, namely normal-form games and extensive-form
games. A normal-form game is usually represented by a matrix which shows the players, actions,
and payoffs. In normal-form games it is presumed that all players act simultaneously (i.e. not having
any information on the action choice of opponents). The second representation, the extensive-form
game, describes how games are played over time. Previous action sequences are stored in a socalled game-tree, and as such, information about the choices of other players can be observed. In
this article, we focus on extensive-form games.
2.1.2 E XTENSIVE -F ORM G AMES
An extensive-form game is a general model of sequential decision-making with imperfect information. As with perfect-information games (such as Chess or Checkers), extensive-form games consist
primarily of a game tree where nodes represent states of the game. Each non-terminal node has an
associated player (possibly chance) that makes the decision at that node, and each terminal node
(leaf) has associated utilities for the players. Additionally, game states are partitioned into information sets Ii . A player i cannot distinguish between states in the same information set. The player,
therefore, must choose actions with the same policy at each state in the same information set.
A strategy of player i, i , is a function that assigns a probability distribution over A(Ii ) to each
Ii  Ii , where Ii is an information set belonging to i, and A(Ii ) is the set of actions that can be
chosen at that information set. We denote i as the set of all strategies for player i, and i  i as
the players current strategy. A strategy profile, , consists of a strategy for each player, 1 , . . . , n .
We let i refer to the strategies in  excluding i .
Valid sequences of actions in the game are called histories, denoted h  H. A history is a
terminal history, h  Z where Z  H, if its sequences of actions lead from root to leaf. A prefix
history h v h0 is one where h0 can be obtained by taking a valid sequence of actions from h.
Given h, the current player to act is denoted P (h). Each information set contains one or more valid
histories. The standard assumption is perfect recall: information sets are defined by the information
that was revealed to each player over the course of a history, assuming infallible memory.
Let   (h) be the probability of history h occurring if all players choose actions according to
. We can decompose   (h) into each players contribution to this probability. Here, i (h) is
 (h) be the
the contribution to this probability from player i when playing according to . Let i
product of all players contribution (including chance) except that of player i. Finally, let   (h, z) =
 (h, z) be defined similarly. Using
  (z)/  (h) if h v z, and zero otherwise. Let i (h, z) and i
P
this notation, we can define the expected payoff for player i as ui () = hZ ui (h)  (h).

578

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Given a strategy profile, , we define a players best response as a strategy that maximizes
their expected payoff assuming all other players play according to . The best-response value for
player i is the value of that strategy, bi (i ) = maxi0 i ui (i0 , i ). An -Nash-Equilibrium (NE)
is an approximation of a best response against itself. Formally, an -Nash-Equilibrium (NE) it is a
strategy profile  that satisfies:

i  N

ui () +   max
ui (i0 , i )
0
i i

(1)

If  = 0 then  is a Nash-Equilibrium (NE): no player has any incentive to deviate as they are all
playing best responses. If a game is two-player and zero-sum, we can use exploitability as a metric
for determining how close  is to an equilibrium,  = b1 (2 ) + b2 (1 ).
It is well-known that in two-player, zero-sum games NE strategies are interchangeable. That is,
if (1 , 2 ) and (10 , 20 ) are different equilibrium profiles with i 6= i0 , then (1 , 20 ) and (10 , 2 ) are
also both NE profiles. This property makes equilibrium strategies for this class of games desireable
since worst-case guarantees are preserved regardless of how the opponent plays. The property is
easily extended to the case where  > 0, therefore playing an NE strategy will guarantee that a
player is only exploitable by . For more details, we refer the reader to the work of Fudenberg and
Tirole (1991) as well as Osborne and Rubinstein (1994).
Throughout this article, we will refer to a player that plays the NES as a rational player. A
player that plays rationally also assumes rationality on the part of its opponents. Experiments have
shown that assuming rationality is generally not correct (e.g. for experiments in Poker see Billings,
Burch, Davidson, Holte, Schaeffer, Schauenberg, & Szafron, 2003); even experienced human players in complex games at best play an approximated rational strategy, and frequently play dominated
actions (i.e., actions that should never have been chosen at all). Moreover, for complex games such
as Chess or Poker, even AI algorithms running on modern computers with a great deal of processor
speed and memory can not (yet) cope with the immense complexity required to compute a NES.
Thus, they are forced to either abstract the full game or do selective sampling to compute only an
approximated NE.
2.2 Test Domains
In the current section, we introduce the test domains used throughout this article. In particular we
describe the game of Poker, as well as some smaller games, some of which are similar to Poker.
Poker is a card game played between at least two players. In a nutshell, the objective of the game is to
win (money) by either having the best card combination at the end of the game (i.e. the showdown),
or by being the only active player. The game includes several betting rounds wherein players are
allowed to invest money. Players can remain active by at least matching the largest investment made
by any of the players, or they can choose to fold (stop investing money and forfeit the game). In
the case that only one active player remains, i.e. all other players chose to fold, the active player
automatically wins the game. The winner receives the money invested by all the players. There exist
many variants of the game. We will now specifically describe the ones used in this article.
Kuhn Poker is a two-player simple Poker game. There are only three cards (J - Jack, Q - Queen,
and K - King). There are two actions, bet and pass. In the event of a showdown, the player with the
higher card wins the pot (the King is highest and the Jack is lowest). After the deal, the first player
579

fiP ONSEN , L ANCTOT & D E J ONG

J/Q
pass
1-

pass



pass

1

-1

pass

1-

+1

1-



bet

1

-2

1

-2

pass



1

pass

1-

1

+1

1-

bet

pass



1-

pass


bet

bet

1-

pass

pass



1

+1
pass

pass

1-

bet

bet

1-

K/Q
bet

pass
1

1



K/J

pass

pass

bet

bet

bet

Q/K

Q/J

J/K
bet

+1



pass

1

+1

bet

1-

+1



+2

bet

bet


1
Player 1 Choice/Leaf Node

-1

-1

+2
2

-1

-2

+2

Player 2 Choice Node

Figure 1: Kuhn Poker game tree (taken from the work of Hoehn, Southey, & Holte, 2005).
has the opportunity to bet or pass. If the first player bets in round one, then in round two the second
player can either bet (and go to showdown) or pass (and forfeit the pot). If the first player passes
in round one, then in round two the second player can bet or pass. A bet leads to a third action
for the first player, namely bet (and go to showdown) or pass (and forfeit the pot), whereas with a
pass the game immediately proceeds to a showdown. Figure 1 shows the game tree with the first
players value for each outcome. The dominated actions have been removed from this tree. They
include actions such as betting with a Queen as a first action, or passing a King for the second player.
There are in total seven possible dominated actions to be made. The Nash-Equilibria of the game
can be summarized by three parameters (, , ) (each in [0, 1]). Kuhn determined that the set of
equilibrium strategies for the first player has the form (, , ) = (/3, (1 + )/3, ). Thus, there
is a continuum of Nash-Equilibrium strategies governed by a single parameter . There is only one
NES for the second player, namely  = 1/3 and  = 1/3. If either player plays a NES, then the first
player expects to lose at a rate of 1/18 bets (i.e., size of ante) per hand. Kuhn Poker therefore is
not a balanced or fair game, since the first player is expected to lose if both players play the rational
strategy.
One-Card Poker (abbreviated OCP(N )) (Gordon, 2005) is a generalization of Kuhn Poker. The
deck contains N cards; Kuhn Poker corresponds with N = 3. Each player must ante a single chip,
has one more chip to bet with, and is dealt one card.
Goofspiel (abbreviated Goof(N )) is a bidding card game where players have a hand of cards numbered 1 to N , and take turns secretly bidding on the top point-valued card in a point card stack,
using cards in their hands (Ross, 1971). Our version is less informational: players only find out the
result of each bid and not which cards were used to bid, and the player with the highest total points
wins. We also use a fixed point card stack that is strictly decreasing, e.g. (N, N  1, . . . , 1).
Bluff(1,1,N) also known as Liars Dice and Perudo2 , is a dice-bidding game. In our version, each
player rolls a single N -sided die and looks at their die without showing it to their opponent. Then
players, alternately, either increase the current bid on the outcome of all die rolls in play, or call
the other players bluff (claim that the bid does not hold). The highest value on the face of a die is
2. See e.g. http://www.perudo.com/perudo-history.html.

580

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

wild and can count as any face value. When a player calls bluff, they win if the opponents bid is
incorrect, otherwise they lose.
Texas Holdem Poker (Sklansky, 2005) is the most complex game under investigation here. The
game includes 4 betting rounds, respectively called the preflop, flop, turn and river phase. During
the first betting round, all players are dealt two private cards (i.e. only known to the specific player)
out of a full deck consisting of 52 cards. To encourage betting, two players are obliged to invest a
small amount the first round (the so-called small- and big-blind). One by one, the players can decide
whether or not they want to participate in this game. If they indeed want to participate, they have to
invest at least the current bet (i.e., the big-blind in the beginning of a betting round). This is known
as calling. Players may also decide to raise the bet. If they do not wish to participate, players fold,
resulting in loss of money they may have bet thus far. In the situation of no outstanding bet, players
may choose to check (i.e., not increase the stakes) or bet more money. The size of bets and raises
can either be predetermined (i.e., Limit Poker, as used in this paper), no larger than the size of the
pot (i.e., Pot-Limit Poker) or unrestrained (i.e., No-Limit Poker). During the remaining three betting
phases, the same procedure is followed. In every phase, community cards appear on the table (three
cards in the flop phase, and one card in the other phases). These cards apply to all the players and
are used to determine the card combinations (e.g., a pair or three-of-a-kind may be formed from the
players private cards and the community cards). During the showdown, if two or more players are
still active, cards are compared, thus ending the game.
Princess and Monster (abbreviated PAM(R, C, H)) (Isaacs, 1965) is not a Poker game; rather,
it is a variation of the pursuit-evasion game on a graph, neither player ever knowing the location
of the other nor discovering their moves (pursuit in a dark room). In our experiments we use
random starting positions on a 4-connected grid graph with R rows and C columns. Players take
turns alternately moving to an adjacent location. The game ends when the monster moves to the
same location as the princess, or H moves have been taken in total with no capture. The payoff to
the evader is the number of steps uncaptured.
In the next section, we will use Kuhn Poker (or OCP(3)) and two-player Limit Texas Holdem Poker.
In Section 4, we use all games mentioned above, except Kuhn Poker; instead of Kuhn Poker, we use
a larger game, OCP(500).

3. Computing Approximated Nash-Equilibrium Strategies
Our contribution in this section is to evaluate current and promising state-of-the-art search methods
for computing (approximated) Nash-Equilibrium strategies in complex domains, and more specifically in Poker. Given the size of the game tree, such methods should (1) incorporate appropriate
abstractions, and (2) be capable of analysing small subsets of the full game (i.e., do sampling).
We will look at two families of sampling algorithms, namely an Upper Confidence Bounds
applied to Trees (UCT) based algorithm, called Monte-Carlo Tree Search (MCTS) (Kocsis &
Szepesvari, 2006; Chaslot et al., 2006; Coulom, 2006), and a regret minimizing algorithm called
Monte-Carlo Counterfactual Regret Minimization (MCCFR) (Lanctot et al., 2009). MCTS has
achieved tremendous success in perfect information games, and in particular in the game of Go
(Lee, Wang, Chaslot, Hoock, Rimmel, Teytaud, Tsai, Hsu, & Hong, 2010). In games with imper-

581

fiP ONSEN , L ANCTOT & D E J ONG

fect information, it has no convergence guarantees for finding Nash-Equilibria.3 However, it has
been reported that it may nonetheless produce strong players in games with imperfect information
(Sturtevant, 2008). We will empirically evaluate the merits of MCTS in the imperfect information
game of Poker. MCCFR does have theoretical guarantees for convergence to a Nash-Equilibrium
(NE). It has only been applied to smaller games thus far; we are the first to evaluate it in the complex
domain of Poker.
This section is structured as follows. In Sections 3.1 and 3.2, we discuss existing work for
computing Nash-Equilibrium strategies in large extensive games and provide details on the two
sampling algorithms. Next, we will analyze the algorithms empirically, both in the domain of Kuhn
Poker and also in the larger domain of two-player Limit Texas Holdem Poker (Section 3.3).
3.1 Non-Sampling Algorithms for Computing Approximate Nash Equilibria
In the current subsection, we give an overview of existing non-sampling techniques for computing
NE approximations in extensive form games. A large body of such work exists in many domains,
but here we will focus specifically on work in the domain of Poker.
The conventional method for solving extensive form games (such as Poker), is to convert them
into a linear program, which is then solved by a linear programming solver. Billings et al. (2003)
solved an abstraction of the full game of two-player Poker using sequence-form linear programming. Their abstractions were three-fold. First, they learned models in separation for the different
phases of the game. Basically, phases are considered independent and are solved in isolation. However, they state that previous phases contain important contextual information that is critical for
making appropriate decisions. The probability distribution over cards for players strongly depends
on the path that led to that decision point. Therefore, they provide input (i.e., contextual information
such as pot size and the number of bets in the game) to the models learned for later phases. Second,
they shorten the game tree by allowing only three (instead of the regular four) bet actions per phase.
Third, they apply bucketing to the cards. The strategic strength of cards (i.e., private cards, possibly in combination with board cards) can be reflected by a numeric value. A higher value reflects
stronger cards. Billings (2006) computed the so-called Effective Hand Strength (EHS), a value in
the range from 0 to 1, that combines the Hand Strength (i.e., current winning probability against active opponents) and Hand Potential (i.e., the probability that currently losing/winning cards end up
winning/losing with the appearance of new board cards). The work of Billings (2006), in Sections
2.5.2.1 to 2.5.2.3, provides a more detailed discussion. Then they divide all values in buckets. For
example, in a 10-bucket discretization (equal width), which we employ in this paper, EHS values in
the range of 0 to 0.1 are grouped together in bucket 1, 0.1 to 0.2 in bucket 2, and so forth. A coarser
view implies more information loss. The solution to the linear program induces a distribution over
actions at each information set, which corresponds to a mixed behavioral strategy. A Poker-playing
program can then sample actions from this mixed strategy. The resulting Poker-playing program
was competitive with human experts.
The Counterfactual Regret Minimization (CFR) algorithm (Zinkevich, Johanson, Bowling, &
Piccione, 2008) may be used to compute an approximated NE in richer abstractions because it
requires less computational resources. In Poker, Zinkevich et al. (2008) only applied abstraction
on the cards and were capable to learn strategies using CFR that were strong enough to defeat
3. For example, Shafiei, Sturtevant, and Schaeffer (2009) provide an analysis in simultaneous imperfect-information
games, indicating that UCT finds suboptimal solutions in such games.

582

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

human experts. Although this was an important step in solving complex games, when complexity
is increased even more (as for example by increasing the number of buckets), learning time and
memory requirements will become impractical.
Another method by Hoda et. al. (2010) and Sandholm (2010) is to use the Excessive Gap Technique applied to a relaxed optimization problem from which the linear program described above
is derived. The optimization problem is smoothened to be made differentiable; the solution to the
new, relaxed problem is suboptimal in the original problem by some amount 0 . Parameters of the
optimization problem are then modified by following the gradient of the smooth approximations of
the objective functions. At iteration i+1 the modified parameters give a new solution with improved
suboptimality i+1 < i . This process is repeated until the desired value of  is reached.
3.2 Sampling Algorithms for Computing Approximate Nash Equilibria
Performing Monte-Carlo sampling can enable algorithms to deal with highly complex domains. In
this section we will discuss two algorithms that perform Monte-Carlo sampling, namely MonteCarlo Tree Search (MCTS) and Monte-Carlo Counterfactual Regret Minimization (MCCFR). Although the internal workings of both techniques are different, they share the general procedure of
sampling a simulated game, determining utilities at a leaf node (i.e., game state that ends the game),
and backpropagating the results. The underlying idea of both techniques is to improve the quality
of simulations progressively by taking into account the simulated games previously played. More
specifically, simulations are driven to that part of the game tree that is most relevant, assuming players take rational decisions (i.e., choose actions that optimize their reward). The result of each game
is backpropagated on the visited path. Progressively, the program concentrates its search on the best
actions, leading to a deeper look-ahead ability.
We will now discuss an example in the game of Poker, as illustrated in Figure 2. As a first
step (moving down), chance nodes and action nodes are sampled until a terminal node is reached.
Chance nodes in Poker represent the dealing of cards. Similarly to previous work in Poker, we also
apply bucketing on cards in our work. Cards are grouped together in buckets based on their strategic
strength. We use a 10-bucket discretization (equal width), where EHS values (see Section 3.1) in
the range of 0 to 0.1 are grouped together in bucket 1, 0.1 to 0.2 in bucket 2, and so forth. A higher
bucket indicates a stronger hand. In our example we sampled buckets 5 and 7 in the preflop phase for
respectively player one and two. Again, these buckets now reflect the strategic strength of players
private cards. With the appearance of new board cards in subsequent phases, we again encounter
chance nodes, and buckets may change.4
We assume imperfect recall. With imperfect recall previous action or chance nodes are forgotten, and as a consequence several information sets are grouped together. For example, in our
work we only take into account the current bucket assignment in information sets, and forget previous ones. This way we reduce the game complexity tremendously, which reduces both memory
requirements and convergence time. In the case of imperfect recall the CFR algorithm loses its convergence guarantees to the NE, but even though we lose theoretical guarantees, this application of
imperfect recall has been shown to be practical, specifically in Poker (Waugh, Zinkevich, Johanson,
Kan, Schnizlein, & Bowling, 2009).
4. In the preflop phase the first player had the weakest hand, but with the appearance of board cards the player could,
for example, have formed a pair and its bucket therefore has increased.

583

fiP ONSEN , L ANCTOT & D E J ONG

Select chance node:

p
5,7

dealing of private cards

Select a according to 
in information-set:
{bucket=5, hist=p}

Update a in information-set:

F1

C1

B1

F2

C2

B2

Select a according to 
in information-set:
{bucket=7, hist=p-C1}

bucket=5, hist=p

Update a in information-set:

bucket=7, hist=p-C1

Select chance node in:
{bucket=5,7, hist=p-C1-C2}

f
6,4

dealing of board cards at flop

Select a according to 
in information-set:
{bucket=4, hist=p-C1-C2-f}

Update a in information-set:

F2

C2

Continue to terminal node, and
determine utilities:
util=8,-8, bucket=9,2, hist=p-

B2

bucket=4, hist=p-C1-C2-f

Update a in information-set:

C1

C1-C2-f-B2-C1-t-C2-C1-r-B2-C1

bucket=9, hist=p-C1-C2-f-B2C1-t-C2-C1-r-B2-C1

Figure 2: Illustration of Monte-Carlo simulations in the game of Poker. On the left going down,
chancenodes (triangles) and action nodes (circles) are sampled based on some statistics
at the information set level. Utilities are then determined at the terminal nodes (squares),
having full information on both players cards (i.e., buckets). On the right going up, the
results of this simulated game are then backpropagated along all information sets that are
a prefix of the terminal node. Statistics are updated at information set level, effectively
altering the strategy.

Players select their actions a based on the current strategy  given the information available
to them (i.e., not knowing the bucket for the opponent player). The letters F, C and B, followed by the player index, respectively correspond to the fold, call/check or bet/raise actions in the
game of Poker. This process continues until a terminal leaf node is encountered, where utilities are
determined for this simulated game.
As a second step (moving up), the utilities are backpropagated along the sampled path, and
statistics are stored at the information set level. Different statistics are required for either MCCFR
or MCTS. We discuss both algorithms in detail in the forthcoming subsections.

584

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

3.2.1 M ONTE -C ARLO T REE S EARCH
MCTS is a game tree search algorithm based on Monte-Carlo simulations. MCTS converges to a
NE in perfect information games, whereas for the imperfect information case such guarantees are
not given. It has been applied successfully in several perfect-information games (Lee et al., 2010;
Chaslot et al., 2006; Bouzy & Chaslot, 2006; Coulom, 2006). It is therefore interesting to see if
MCTS can also be successfully applied to the imperfect-information game of Poker. Two statistics
of nodes are important for sampling actions in MCTS, i.e.:
1. the value, va of action node a. This is the average of the reward of all simulated games that
visited this node.
2. the visit count, na of action node a. This represents the number of simulations in which this
node was reached.
A crucial distinction with work that used MCTS in perfect-information games, is that now we assume imperfect information, as for example opponent cards in Poker. As a result, one has to reason
over information sets (see Section 2.1) instead of individual nodes. Therefore, part of the algorithm
is performed on information sets rather than individual nodes. The starting state of the game is represented by the root node, which is initially the only node in the tree. MCTS consists of repeating
the following four steps (illustrated in Figure 3), as long as there is time left.
1. Selection. Actions in set A are encoded as nodes in the game tree. They are chosen according
to the stored statistics in a way that balances between exploitation and exploration. When
exploiting, actions that lead to the highest expected value are selected. Less promising actions
still have to be explored due to the uncertainty of the evaluation (exploration). We use the
Upper Confidence Bound applied to Trees (UCT) rule to select actions (Kocsis & Szepesvari,
2006). In UCT, a  A is the set of nodes (possible actions) reachable from the parent node,
p. Using the following equation, UCT selects the child node a of parent node p which has
the highest value.
s
a  argmaxaA

va + C 

ln np
na

!
.

(2)

Here va is the expected value of the node a, na is the visit count of a, and np is the visit
count of p, the parent of node a. C is a coefficient that balances exploration and exploitation.
A higher value encourages longer exploration since nodes that have not been visited often
receive a higher value. This value is usually tweaked in preliminary experiments. Again, note
that in imperfect-information games, expected values and visit counts are stored and updated
per information set.
2. Expansion. When a leaf node is selected, one or several nodes are added to the tree. As such
the tree grows with each simulated game. Please note that the tree in memory deals with game
nodes (assuming full information), and not information sets (which are used in the selection
and backpropagating part of the algorithm).

585

fiP ONSEN , L ANCTOT & D E J ONG

Repeated X times

Selection
Selection

selection function
is applied
TheThe
selection
function
is
recursively
until a leaf
node is
applied
recursively
until
reached
a leaf node is reached

Expansion
Expension

Simulation
Simulation

or morenodes
nodes
OneOne
or more
created
are are
created

One simulated
One
simulated
game isisplayed
game
played

Backpropagation
Backpropagation

Theresult
result of
game
is is
The
ofthis
this
game
backpropagated in the tree
backpropagated
in the tree

Figure 3: Outline of Monte-Carlo Tree Search (Chaslot et al., 2008).

3. Simulation. From the newly expanded node, nodes are selected according to some simulation
policy until the end of the game. More realistic simulations will have a significant effect on
the computed expected values. Examples for simulation strategies in Poker are: (1) random
simulations, (2) roll-out simulations or (3) on-policy simulations. In the first simulation strategy, one samples nodes uniformly from the set of A. In roll-out simulations, one assumes that
all remaining active players call or check until the end of the game. In other words, all remaining active players stay active and compete for the pot, but the stakes are not raised. Finally,
the on-policy simulation uses current estimations of expected values or action probabilities to
select actions. In this paper, we employ the latter. More specifically we always take action a
according to Equation 2. This produces simulations that are closest to the actual strategy.
4. Backpropagation. After reaching a terminal node z (i.e., the end of the simulated game), we
establish the reward for all players (having full information on the buckets for all players).
Then we update each information set that contains a prefix of the terminal history. The visit
counts are increased and the expected values are modified according to the rewards obtained
at the terminal node.
After the MCTS algorithm is stopped, the (potentially mixed) action distribution is determined by
the visit counts, i.e., the actions in information sets with a high visit count have a larger probability
of being selected. In perfect-information games, those actions are selected with the highest expected
value, which in the end leads to an optimal payoff. For imperfect-information games, such as Poker,
potentially mixed strategy distributions are required for obtaining the optimal payoff (e.g., see the
NES for Kuhn poker described in Section 2.2). Therefore, the ratio between the actions selected
determines the probability distribution.
3.2.2 M ONTE -C ARLO C OUNTERFACTUAL R EGRET M INIMIZATION
Before we describe the Monte-Carlo Counterfactual Regret Minimization (MCCFR) algorithm, we
first describe the intuition behind the Counterfactual Regret Minimization (CFR) algorithm, since
MCCFR is based on it.
586

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

CFR employs a full game-tree traversal in self-play, and updates player strategies at information
sets at each iteration. Strategy updates are based upon regret minimization. Imagine the situation
where players are playing with strategy profile . Players may regret using their strategy i against
i to some extent. In particular, for some information set I they may regret not taking a particular
action a instead of following i . Let Ia be a strategy identical to  except a is taken at I. Let ZI
be the subset of all terminal histories where a prefix of the history is in the set I; for z  ZI let z[I]
be that prefix. The counterfactual value vi (, I) is defined as:
vi (, I) =

X


i
(z[I])  (z[I], z)ui (z).

(3)

zZI

The algorithm applies a no-regret learning policy at each information set over these counterfactual
values (Zinkevich et al., 2008). Each player starts with an initial strategy and accumulates a counterfactual regret for each action at each information set r(I, a) = v(Ia , I)  v(, I) through
self-play. Minimizing the regret of playing i at each information set also minimizes the overall
external regret, and so the average strategies approach a NE.
MCCFR (Lanctot et al., 2009) avoids traversing the entire game tree on each iteration while still
having the immediate counterfactual regrets be unchanged in expectation. Let Q = {Q1 , . . . , Qr }
be a set of subsets of Z, such that their union spans the set Z. These Qj are referred to as blocks of
terminal histories. MCCFR samples one of these blocks and only considers the terminal histories in
the sampled
P probability of considering block Qj for the current iteration
P block. Let qj > 0 be the
(where rj=1 qj = 1). Let q(z) = j:zQj qj , i.e., q(z) is the probability of considering terminal
history z on the current iteration. The sampled counterfactual value when updating block j is:
vi (, I|j) =

X
zQj ZI

1 
 (z[I])  (z[I], z)ui (z)
q(z) i

(4)

Selecting a set Q along with the sampling probabilities defines a complete sample-based CFR algorithm. For example, the algorithm that uses blocks that are composed of all terminal histories whose
chance node outcomes are equal is called chance-sampled CFR. Rather than doing full game-tree
traversals the algorithm samples one of these blocks, and then examines only the terminal histories
in that block.
Sampled counterfactual value matches counterfactual value in expectation (Lanctot et al., 2009).
That is, Ejqj [vi (, I|j)] = vi (, I). So, MCCFR samples a block and for each information set
that contains a prefix of a terminal history in the block, it computes the sampled counterfactual
t
regrets of each action, r(I, a) = vi ((Ia)
, I)  vi ( t , I). These sampled counterfactual regrets are
accumulated, and the players strategy on the next iteration is determined by applying the regretmatching rule to the accumulated regrets (Hart & Mas-Colell, 2000). This rule assigns a probability
to an action in an information set. Define rI+ [a] = max{0, rI [a]}. Then:

(I, a) =







if a  A(I) : rI [a]  0

1/|A(I)|
rI+ [a]
P
+
aA(I) rI [a]

0

if rI [a] > 0

(5)

otherwise.

Here rI [a] is the cumulative sampled counterfactual regret of taking action a at I. If there is at
least one positive regret, each action with positive regret is assigned a probability that is normalized
587

fiP ONSEN , L ANCTOT & D E J ONG

over all positive regrets and the actions with negative regret are assigned probability 0. If all the
regrets are negative, then Equation 5 yields (I, a) = 0 a in this information set. To repair this,
the strategy is then reset to a default uniform random strategy.
There are different ways to sample parts of the game tree. Here we will focus on the most
straightforward way, outcome sampling, which is described in Algorithm 1. In outcome-sampling
Q is chosen so that each block contains a single terminal history, i.e., Q  Q, |Q| = 1. On
each iteration one terminal history is sampled and only updated at each information set along that
history. The sampling probabilities, Pr(Qj ) must specify a distribution over terminal histories. We
0
specify this distribution using a sampling profile,  0 , so that Pr(z)=   (z). Note that any choice
of sampling policy will induce a particular distribution over the block probabilities q(z). As long as
i0 (a|I) > , then there exists a  > 0 such that q(z) > , thus ensuring Equation 4 is well-defined.
0
The algorithm works by sampling z using policy  0 , storing   (z). In particular, an -greedy
strategy is used to choose the successor history: with probability  choose uniformly randomly and
probability 1   choose based on the current strategy. The single history is then traversed forward (to compute each players probability of playing to reach each prefix of the history, i (h))
and backward (to compute each players probability of playing the remaining actions of the history, i (h, z)). During the backward traversal, the sampled counterfactual regrets at each visited
information set are computed (and added to the total regret). Here,

wI (  (z[I]a, z)    (z[I], z)) if z[I]a v z
r(I, a) =
wI   (z[I], z)
otherwise
where wI =

 (z[I])
ui (z)i
.
0
  (z)

(6)

The algorithm requires tables to be stored at each information set; each table has a number of entries
equal to the number of actions that can be taken at that information set. Therefore, if we denote |Ai |
as the maximum number of actions available to player i over all their information sets, then the
space requirement for MCCFR is O(|I1 ||A1 | + |I2 ||A2 |). The time required by MCCFR, using
outcome sampling, depends on the regret bounds and the desired . To reach a fixed -NE, with
probability 1  p the number of iterations required is


2 |A|M 2
O

p 2
2
where  is the smallest probability of sampling a terminal history over all histories, |A| is the maximum available actions over all information sets, and |M | is a balance factor depending on the
relative
number of decisions taken by each player throughout the entire game with the property that
p
|I|  M  |I|. In contrast, the full CFR algorithm requires O(|A|M 2 /2 ) iterations; however,
the iterations of MCCFR require only sampling a single history whereas an iteration of CFR requires
traversing the entire game tree in the worst case. In practice, one benefit of outcome sampling is that
information gained on previous iterations is quickly leveraged on successive iterations.
Algorithm 1 shows the pseudocode of the entire algorithm. We refer the interested reader to the
work of Lanctot et al. (2009), providing a more in-depth discussion, including convergence proofs.

588

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Data: root node
Data: Sampling scheme  greedy
Data: Initialize information set markers: I, cI  0
Data: Initialize regret tables: I, rI [a]  0
Data: Initialize cumulative strategy tables: I, sI [a]  0
Data: Initialize initial strategy: (I, a) = 1/|A(I)|
1 for t = 1, 2, 3 . . . do
2
current node  root node
3
S ELECT:
4
while (current node 6= terminal) do
5
P  R EGRET M ATCHING(rI ), a  I
6
P    G REEDY(P)
7
current node  Select(current node, P  )
8
end
9
current node  P arent(current node)
10
U PDATE :
11
while (current node 6= root node) do
12
foreach a  A[I] do
13
r = r(I, a) (sampled counterfactual regret)
14
rI [a]  rI [a] + r
15
sI [a]  sI [a] + (t  cI )i (z[I])i (I, a)
16
end
17
cI  t
18
  R EGRET M ATCHING(rI )
19
current node  P arent(current node)
20
end
21 end
Algorithm 1: Outcome-sampling with Monte-Carlo Counter-Factual Regret Minimization
3.2.3 M ONTE -C ARLO C OUNTERFACTUAL R EGRET E XAMPLE
We now provide an example of Algorithm 1 on Kuhn Poker, as shown in Figure 1. The algorithm
starts on the first iteration, during the selection phase. At the root node (a chance node), the chance
outcome K|Q is sampled with probability 16 . Following this node, the algorithm loads the information set belonging to the first player where they have received a King and no actions have been
taken; let us call this information set I1 . Since no regret has been collected for actions at I1 yet, P
is set to the uniform distribution U = ( 12 , 12 ), which represent probabilities for (pass, bet). Then,
the sampling distribution is obtained P  = (1  )P + U . Note that regardless of the value of
, on this first iteration each action is equally likely to be sampled. An action is sampled from P  ;
suppose it is a bet. Following this action, the algorithm loads the information set belonging to the
second player where they have received a queen and the action made by the first player was a bet
(I2 ). Similarly, the algorithm constructs P and P  as before  which have identical distributions
as at I1 ; suppose the pass action is sampled this time. Finally, this action is taken and the terminal

589

fiP ONSEN , L ANCTOT & D E J ONG

node is reached. Therefore, the terminal history that was sampled on the first iteration, z1 , is the
sequence: (K|Q, bet, pass).
In the update phase, the algorithm updates the information sets touched by the nodes that were
traversed in the sample in reverse order. Note that the notation z1 [I1 ] represents the subsequence
(K|Q), and z1 [I2 ] represents (K|Q, bet). The sampled counterfactual regret is computed for each
action at I2 . Note that u2 (z1 ) = u1 (z1 ) = 1. The opponents reaching probability is

i
(z1 [I2 ]) =

1 1
1
 = .
6 2
12

The probability of sampling z1 is
0

  (z1 ) =

1 1 1
1
  = .
6 2 2
24

Therefore, wI2 = 2. The two remaining things to compute are the tail probabilities   (z1 [I2 ], z) =
1

2 and for each  (z1 [I2 ]a, z) = 1, where a is the pass action. Finally, from equation 6 we get
1
r(I2 , pass) = 2  (1  ) = 1,
2
and

1
r(I2 , bet) = (2  ) = 1.
2
After the updates on line 13, the regret table rI2 = (1, +1). The average strategy table is then
updated. The reaching probability i (z1 [I2 ]) is the product of probabilities of the strategy choices
of player 2, equal to 1 here since player 2 has not acted yet. The players strategy is currently
2 (I2 , pass) = 2 (I2 , bet) = 21 . The current iteration t = 1 and cI2 = 0, therefore after the average
strategy updates the table sI2 = ( 12 , 21 ). Finally cI2 is set to 1 and (I2 ) = (0, 1) according to
equation 5.
The algorithm then proceeds to update tables kept at I1 , where
1
1
0

u1 (z1 ) = 1, i
(z1 [I1 ]) = , and   (z1 ) = , therefore wI1 = 4.
6
24

The tail reaching probabilities are   (z1 [I1 ], z) = 14 , and   (z1 [I1 ]a, z) =
action. This leads to sampled counterfactual values of

1
2,

where a is the bet

r(I1 , pass) = 1, and r(I1 , bet) = 1.
As before cI1 is incremented to 1. The average strategy table update is identical to the one that was
applied at I2 . Since there are no previous actions taken by the player, the reaching probability is
again 1, and the players current strategy was uniform, as at I2 . Finally t is incremented to 2 and the
entire process restarts.
3.3 Experiments
In the current section, we will empirically evaluate the chosen algorithms in the smaller domain of
Kuhn Poker and in full-scale Poker. We will present experimental results in Kuhn Poker in Section
3.3.1. Then we will give results in Poker in Section 3.3.2. Note that Kuhn and full-scale Poker are
explained in Section 2.2.
590

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Game Limit 2-p Poker

0
UCT MCCFR, E=0.6
-0.30821
-0.502385
-0.27291
-0.38137
-0.211665
-0.35938
-0.219625
-0.299085
-0.201485
-0.31999
-0.17621
-0.315805
-0.15521
-0.27771
-0.17901
-0.31643
-0.17799
-0.2864
-0.186585
-0.26078
-0.15607
-0.290925
-0.17974
-0.29087
-0.15528
-0.27865
-0.14181
-0.231025
-0.12594
-0.285545
-0.162695
-0.26514
-0.159415
-0.24142
-0.141695
-0.26353
-0.133745
-0.2314
-0.18484
-0.25223
-0.13882
-0.24581
-0.16769
-0.26161
-0.136985
-0.203455
-0.154215
-0.236695
-0.18157
-0.21148
-0.152525
-0.22165
-0.17569
-0.213335
-0.152255
-0.20121
-0.151065
-0.22558
-0.14731
-0.197915
-0.149325
-0.20694
-0.138365
-0.238875
-0.15505
-0.176595
-0.136595
-0.21927
-0.144725
-0.22413
-0.167625
-0.20264
-0.126125
-0.171715
-0.1177
-0.219025
-0.1466
-0.1911
-0.182245
-0.18643
-0.10686
-0.18665
-0.137965
-0.195105
-0.142795
-0.17549
-0.12844
-0.196955
-0.138275
-0.18438
-0.152245
-0.149855
-0.11488
-0.180895
-0.149785
-0.18449
-0.160885
-0.17541
-0.154335
-0.202345
-0.135555
-0.176805
-0.135585
-0.166755
-0.172165
-0.176525
-0.140035
-0.15277
-0.14462
-0.11999
-0.13888
-0.1564
-0.119515
-0.18582
-0.159195
-0.155025
-0.12965
-0.16708
-0.150855
-0.16455
-0.111345
-0.17555
-0.132165
-0.182095
-0.14703
-0.149315
-0.14222
-0.189185
-0.144715
-0.151055
-0.14483
-0.180865
-0.143565
-0.17185
-0.14643
-0.149485
-0.15015
-0.137985
-0.157835
-0.142525
-0.166905
-0.171325
-0.15989
-0.184765
-0.12237
-0.1488
-0.13732
-0.169455
-0.15739
-0.144705
-0.151765
-0.12328
-0.14768
-0.15946
-0.14448
-0.13819
-0.142815
-0.16202
-0.14195
-0.16011
-0.17219
-0.13438
-0.154565
-0.13179
-0.14677
-0.15241
-0.14296
-0.164015
-0.157345
-0.14014
-0.13648
-0.14606
-0.16621
-0.152785
-0.12989
-0.126545
-0.18669
-0.152755
-0.162215
-0.11338

-0.05
-0.1
-0.15

sb/h

-0.2
-0.25
-0.3
-0.35
-0.4
-0.45

MCTS, C=17
MCCFR, E=0.6

-0.5

+0
7

+0
7

+0
7

+0
8
1.
0E

9.
1E

8.
1E

+0
7

7.
1E

+0
7

+0
7

+0
7

+0
7

+0
7

6.
1E

5.
1E

4.
1E

3.
1E

2.
1E

1.
1E

+0
6

-0.55
1.
0E

#iteration
1000000
2000000
3000000
4000000
5000000
6000000
7000000
8000000
9000000
10000000
11000000
12000000
13000000
14000000
15000000
16000000
17000000
18000000
19000000
20000000
21000000
22000000
23000000
24000000
25000000
26000000
27000000
28000000
29000000
30000000
31000000
32000000
33000000
34000000
35000000
36000000
37000000
38000000
39000000
40000000
41000000
42000000
43000000
44000000
45000000
46000000
47000000
48000000
49000000
50000000
51000000
52000000
53000000
54000000
55000000
56000000
57000000
58000000
59000000
60000000
61000000
62000000
63000000
64000000
65000000
66000000
67000000
68000000
69000000
70000000
71000000
72000000
73000000
74000000
75000000
76000000
77000000
78000000
79000000
80000000
81000000
82000000
83000000
84000000
85000000
86000000
87000000
88000000
89000000
90000000

# iterations

Figure 4: Experimental results for Monte-Carlo Counter-Factual Regret Minimization (MCCFR)
and Monte-Carlo Tree Search (MCTS) in the game of Kuhn Poker (left) and Poker (right).
In both figures the x-axis denotes the number of iterations the algorithms ran. The y-axis
denotes the quality of the Nash-Equilibrium strategy learned so far. For our experiments
in Kuhn Poker this is represented by the squared error (SQR-E) and cumulative dominated
error (DOM-E), while in Poker we use the metric named small bets per hand (sb/h). For
all metrics applies that a value close to zero indicates a near Nash-Equilibrium strategy.
The opponent in the Poker experiments is an approximated Nash Equilibrium strategy,
computed by MCCFR.

3.3.1 K UHN P OKER

We ran both MCCFR and MCTS in the game of Kuhn Poker. The C-constant for MCTS was
tweaked in preliminary experiments and set to 2. It has been suggested by Balla & Fern (2009)
that the C-constant should be set in the same scale as the payoff range. Also, Auer, Cesa-Bianchi,
& Fischer (2002) discuss a modified version of the original Upper Confidence Bound (UCB) algorithm (on which the Upper Confidence Bound applied to Trees (UCT) algorithm is based) that tunes
the exploration term. In our experiments, we ran several runs of MCTS with varying values for the
parameter C, and then took the best experimental run. For MCCFR we used epsilon-greedy as its
sampling scheme. As suggested in earlier work (Lanctot et al., 2009),  was set to the relatively high
value of 0.6 to cover a large area of the search space.
After every 104 iterations we measured the performance of the current policy. Since the equilibria are known in Kuhn Poker (see Section 2.2) we can compare our strategy with the theoretically
correct one. Each evaluation we compute the squared error, which is simply the correct NE probability minus our current learned probability squared. We also compute the cumulative dominated
error, which denotes the summed probabilities for selecting dominated actions.
From Figure 4 (left) we can confirm earlier results from the work of Lanctot et al. (2009) and
Sturtevant (2008), namely that MCCFR learns a NE and MCTS learns a balanced situation that
is not (necessarily) a NE. The balanced situation that MCTS eventually resides in depends on the
parameter value C. Where MCCFR obtains squared errors and dominated errors close to zero,
MCTS has converged slightly off the NE and is playing dominated actions. However, we also see
that MCTS is still unlearning these dominated actions. Therefore, unlike MCCFR, MCTS does not

591

fiP ONSEN , L ANCTOT & D E J ONG

learn perfectly rational strategies, and as such is exploitable. However, it does a reasonable job at
avoiding dominated mistakes. As a consequence, it will not lose (much) to a NES.
3.3.2 L ARGER T EST D OMAIN : P OKER
We evaluated policies learned by MCTS and MCCFR in the game of Poker. To reduce the complexity of the task of finding a NES, we decreased the size of the game-tree by applying a bucket
discretization (Billings, 2006) on the cards, along with imperfect recall (i.e., buckets of previous
phases are forgotten). At each phase, the strategic strength of private cards, along with zero or more
board cards, determines the bucket.
Learning Against a Precomputed Nash Equilibrium
In this experiment we use a 10-bucket discretization for MCCFR and MCTS. MCCFR uses similar
parameter settings compared to the experiments in Kuhn Poker, but for MCTS we change the Cconstant to 17. Again, this value was determined in preliminary experiments. At every 106 iterations
we evaluate the current policy learned by MCTS and MCCFR. Policies are evaluated in small bets
per hand (sb/h), which describes the big blinds won per hand on average; this quantity can thus be
used to reflect a players playing skill. The small bets per hand are computed in 105 games against a
pre-learned Nash-Equilibrium strategy (using MCCFR with a 10-bucket discretization). The results
are shown in Figure 4 (right). The x-axis denotes the number of iterations, and the y-axis the small
bets per hand. A value close to zero indicates that the strategy is no longer being exploited by the
pre-computed NES, and arguably has converged to an (approximated) Nash Equilibrium itself.
We can see that MCTS again converges fast to a balanced situation, though not necessarily to
the NE. MCCFR on the other hand converges considerably slower, but unlike MCTS it continues to
learn better NE approximations. Given these results, one may conclude that MCTS learns reasonably
good strategies fast, but MCCFR will in the long run produce better NE approximations.
It is interesting to mention here that a single iteration of MCTS requires much less computation
time and memory than a single iteration of MCCFR. If the graph in Figure 4 (right) plotted computation time against performance, rather than the number of iterations required, we would see a
larger advantage for MCTS in the early phase of learning. Clearly, the end result would still be the
same; at a certain point, MCCFR surpasses MCTS.
Playing Against Benchmark Poker Bots
To get a good measure of the strength of the policies, we evaluated both policies against strong
opponent bots provided with the software tool Poker Academy Pro, namely P OKI and S PAR B OT.
For a more detailed explanation of P OKI and experiments with this bot, we refer the reader to
the work of Billings (2006). S PAR B OT is a bot that plays according to the NE strategy described
by Billings et al. (2003). It was designed solely for 2-player Poker, in contrast to P OKI, which was
designed for multi-player games. Since S PAR B OT specializes in two-player games, it is significantly
less exploitable in a two-player game than P OKI, a stochastic rule-based bot.
We ran a large number of offline iterations for MCCFR and MCTS, froze the policies, and
evaluated them. In addition to a 10-bucket discretization for MCTS and MCCFR, we also used
MCCFR with a 100-bucket discretization to examine the effect of a finer abstraction. For clarity,
we will mention the number of buckets used after the algorithm name; e.g., MCCFR100 refers to
MCCFR with 100 buckets. All results are shown in Table 1. We performed 104 evaluation games
in the software tool Poker Academy Pro for each policy, with an estimated standard deviation of
0.06sb/h. MCCFR10, after a great deal of iterations, wins by a small margin from P OKI, while
592

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Opponent

MCTS10

MCCFR10

MCCFR100

P OKI
S PAR B OT

0.077
-0.103

0.059
-0.091

0.191
0.046

Table 1: Experimental results for Monte-Carlo Counter-Factual Regret Minimization (MCCFR)
and Monte-Carlo Tree Search (MCTS) against two Poker bots. Outcomes are reported in
small bets per hand (sb/h). The numbers behind the algorithm names refer to the number
of buckets used in the card abstraction.
losing a small amount from S PAR B OT. This loss may be due to our chosen abstractions (10-bucket
imperfect recall), or due to our choice of using outcome sampling (Lanctot et al., 2009). Looking
at MCCFR100, we see that the abstraction level is the culprit; it wins convincingly from P OKI and
S PAR B OT. MCTS10 surprisingly, knowing it doesnt necessarily learn a NES, performs slightly
better than MCCFR10 against P OKI, although the difference is not significant. Against S PARBOT,
the MCTS10 strategy loses slightly more than MCCFR10, with no statistical significance.
To conclude, we evaluated two state-of-the-art algorithms in extensive-form games. We confirm
previous theoretical claims with results obtained in experiments in a smaller game (Kuhn Poker),
and for the first time apply both sampling techniques in the complex game of Poker. MCTS is a valid
approach if one wants to learn reasonably strong policies fast, but these are not necessarily a NE,
whereas MCCFR is the better choice if the quality of the strategy is more important than the learning
time and memory constraints. However, both techniques can produce strategies that are competitive
with strong Poker bots, especially if fine abstractions are used. Both sampling techniques show
promise for complex domains, not exclusively Poker.

4. Robust Best-Response Learning via Monte-Carlo Restricted Nash Response
It is well-known that a perfectly rational Nash-Equilibrium strategy (NES) is not necessarily a bestresponse strategy (i.e., best counter-strategy) against strategies other than the rational strategy itself
(Osborne & Rubinstein, 1994). If the opposition employs clearly inferior strategies, these can be
exploited best using tailored counter-strategies. The resulting best-response strategy would be more
profitable than the Nash-Equilibrium strategy (NES), because it is designed to win instead of designed not to lose. This stresses the importance of opponent modeling in games too complex to fully
analyze, since we can expect that our opposition is incapable of playing a perfectly rational strategy,
and we can profit from playing best-response strategies. However, we do not want to become too
exploitable ourselves by other strategies, because our opponent model may be inaccurate, or players
may switch strategies.
This section presents Monte-Carlo Restricted Nash Response (MCRNR), a sample-based algorithm for the offline computation of restricted Nash strategies in complex extensive-form games.
A restricted Nash strategy is essentially a robust best-response strategy, i.e., it exploits opponents
to some degree based on some opponent model, while preventing that the strategy itself becomes
to exploitable. The new algorithm described in this section combines a state-of-the-art algorithm
and a general technique, i.e., Monte-Carlo Counterfactual Regret Minimization (MCCFR) (Lanctot
et al., 2009) (see Section 3.2.2) and Restricted Nash Response (RNR) (Johanson et al., 2008; Jo-

593

fiP ONSEN , L ANCTOT & D E J ONG

hanson & Bowling, 2009). Given the promising results of applying sampling in Counterfactual Regret Minimization (CFR), we apply the original Restricted Nash Response (RNR) technique using
Monte-Carlo Counterfactual Regret Minimization (MCCFR) as the underlying equilibrium solver.
Our new algorithm, Monte-Carlo Restricted Nash Response (MCRNR), benefits from sampling only
relevant parts of the game tree. The algorithm is therefore able to converge very quickly to robust
best-response strategies given a model of the opponent(s).
The section is structured as follows. We first outline related work on computing bestresponse strategies. Then, we introduce our new algorithm, Monte-Carlo Restricted Nash Response
(MCRNR). Finally, we describe experiments that validate the new algorithm in a variety of games,
as discussed in Section 2.2.
4.1 Computing Best-Response Strategies
Poker is a perfect domain for investigating best-response strategies since the ability to anticipate
an opponents move highly influences the outcome of the game. We mention a few of the previous
approaches to perform opponent modeling in Poker. Then, we discuss work on Restricted Nash
Response (RNR).
4.1.1 G ENERAL O PPONENT M ODELING
One approach is the Adaptive Imperfect Information game-tree search algorithm (Billings, 2006),
which has an opponent model integrated in it. It keeps track of statistics for both the outcome of
the game and actions at opponent decision nodes for every possible betting sequence. The problem
with this approach is that it uses little generalization and hence the frequency counts are limited to a
small number of situations. A more general system for opponent modeling was obtained by training
a neural network. Davidson, Billings, Schaeffer, and Szafron (2000) use nineteen different parameters as input nodes and three output nodes representing the possible actions. The input parameters
include information about the players, information about the betting history and information on the
community cards. Southey et al. (2005) use prior distributions over the opponents strategy space
and compute a posterior using Bayes rule and observations of the opponents decisions. It also
investigates several ways to play an appropriate response to that distribution.
In this paper we also integrate an opponent model with a search technique in order to learn a
best-response strategy. Unlike Billings (2006) we learn a model that generalizes between states. Our
work also differs from the aforementioned studies in that we learn robust best-response strategies,
which prevents the strategy from becoming too exploitable itself.
4.1.2 R ESTRICTED NASH R ESPONSE
It is known that a best-response strategy, in other words a strategy that maximally exploits an opponent, is more profitable than a (pessimistic) NES, given that the model of the opponent is accurate.
Experiments by Hoehn et al. in the game of Kuhn Poker, validate this claim. Johanson et al. (2008)
argue that best-response strategies are not sufficiently robust; the best-response strategy may be exploited by strategies other than the strategy of the current opponent, or even by the opponent itself
if the opponent model is not accurate. The authors therefore introduce a general technique named
Restricted Nash Response (RNR) and put it to the test in Poker using chance-sampled Counterfactual Regret Minimization (CFR). Unlike Nash-Equilibrium strategies that are oblivious to opponent
play, and best-response strategies that are potentially exploitable, RNR strategies are robust best594

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

*

p
*

*

1

1
2

2

Game G

*

*
1

2

2

2

R

1p

2

Game G

Game G

Figure 5: Illustration of Restricted Nash Response. The RNR technique transforms a game; a
chance node with an outcome that is unrevealed (to the unrestricted player) is added
to the top of both trees. This new game, G0 , has two subtrees: the left subtree is GR and
the right subtree is G. GR is identical to G except that one player is restricted to play the
strategy f ix . Think of the initial coin flip as a moderator that either forces the restricted
player to use f ix or some other strategy i . Their opponent does not know which of
these two options was forced upon the restricted player. This causes the information sets
containing the same nodes in each game to be merged, because the unrestricted player
cannot tell them apart.

response strategies given a model of opponent policies (Johanson et al., 2008). It is shown that RNR
strategies are capable of exploiting opponents, with reasonable performance even when the model
is wrong. The RNR technique transforms an existing game into a modified game; an equilibrium
solver then solves this modified game using the previously mentioned CFR algorithm, wherein it is
assumed that the opponent plays according to a fixed strategy, as specified by the model, with a certain probability, denoted by the parameter p. Otherwise, the opponent plays according to a rational
regret-minimizing strategy (see Figure 5).
This p parameter is an indication of how confident we are the model is correct; it can be thought
of as a confidence value used to set the trade-off between exploitation and exploitability, where
respectively the first indicates the maximum amount we can win from a specific opponent strategy,
and the second indicates the maximum amount we risk losing by playing this strategy. Setting p = 0
leads to the unmodified game G (see Figure 5) being solved, which results in a game-theoretic
solution. The overall solution will not be exploitable (i.e., will at least break-even), but will also
not maximally exploit its opponents. The other extreme, namely setting p = 1, will result in a pure
best-response strategy against the fixed strategy denoted by the opponent model. In the game GR ,
all opponent nodes have their action probabilities drawn from the opponent model. Focusing on this

595

fiP ONSEN , L ANCTOT & D E J ONG

game results in a maximal exploitative strategy against the opponent model, potentially at the cost
of becoming exploitable by other strategies.5
Calculating the RNR response requires a model of the opponents strategy, denoted as f ix .6
Suppose in a 2-player game, the opponent (i.e., restricted) player is player 2, then f ix  2 .
p,
Define 2 f ix to be the set of mixed strategies of the form pf ix + (1  p)2 where 2 is an
arbitrary strategy in 2 . The set of restricted best responses to 1  1 is:
(u2 (1 , 2 ))
BRp,f ix (1 ) = argmax
p,
2 2

f ix

(7)

A (p, f ix ) RNR equilibrium is a pair of strategies (1 , 2 ) where 2  BRp,f ix (1 ) and 1 
BRp,f ix (2 ). In this pair, the strategy 1 is a p-restricted Nash response to f ix . These are counterstrategies for f ix , where p provides a balance between exploitation and exploitability. In fact, given
a particular value of p, solving a RNR-modified game assures the best possible trade-off between
best response and equilibrium is achieved.
4.2 Monte-Carlo Restricted Nash Response (MCRNR)
We extend the original RNR algorithm with sampling. The resulting new algorithm, MCRNR, benefits from sampling only relevant parts of the game tree. It is therefore able to converge very fast
to robust best-response strategies. The pseudo-code of the algorithm is provided in Algorithm 2.
The algorithm has identical inputs compared to the MCCFR algorithm (see Algorithm 1), with an
additional two components: (1) an opponent model that translates to a fixed strategy f ix for the
so-called restricted player pr , and (2) a confidence value, p, that we assign to this model. For learning the opponent models, we apply a standard supervised learning method, namely the J48 decision
tree learner in the Weka datamining tool. Similar to the work in the original RNR article (Johanson
et al., 2008), we here use a fixed value of p.7
We then sample a terminal history h  Z, either selecting actions based on a provided opponent
model, or based on the strategy obtained by regret-matching. The R EGRET M ATCHING routine
assigns a probability to an action in an information set (according to Equation 5). The -greedy
sampling routine S() samples action a with probability


1
+ (1  )i (I, a).
|A(I)|

(8)

After sampling the action, we recursively call the MCRNR routine given the extended history,
namely the history of h after applying a (see line 13). Once at a terminal node z, utilities are
determined and backpropagated through all z[I] @ z.
Regret and average strategy updates are applied when the algorithm returns from the recursive
call, in lines 14 to 18. On line 15 we add the sampled counterfactual regret (according to Equation 6; it takes as input the reaching probabilities and utility for this sampled terminal history) to
5. Note the generality of the RNR technique: since it simply modifies an extensive-form game, the underlying solution
technique used to solve the game is independent of the application of RNR. Nonetheless, from this point on we refer
to RNR as an algorithm to mean the original application of the RNR technique coupled with the CFR algorithm.
6. We refer the reader back to Section 2.1 for an overview of the notations used in this section.
7. When learning the model from data it makes sense to have different values per information set because the confidence
depends on how many observations are available per information set; counter-strategies using a model built from data
are called Data-Biased Responses (Johanson & Bowling, 2009).

596

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

initialize: Information set markers: I, cI  0
initialize: Regret tables: I, rI [a]  0
initialize: Strategy tables: I, sI [a]  0
initialize: Initial strategy: (I, a) = 1/|A(I)|
input : A starting history h
input : A sampling scheme S() (e.g., -greedy)
input : An opponent model or fixed strategy f ix for restricted player pr
input : Confidence value p
input : Current iteration t
MCRNR(h) =
if h  Z then
return (ui (h),   (h))
else
pi  P (h)
if at chance node then select chance node
else if pi = pr then
if h is a prefix of a terminal history in the restricted subtree then i (I)  f ix (I)
else i (I)  R EGRET M ATCHING(rIi )
else
i (I)  R EGRET M ATCHING(rIi )
Sample a from S(i (I))
(u, )  MCRNR(h+a)
foreach a  A(I) do
rI [a]  rI [a] + r(I, a)
sI [a]  sI [a] + (t  cI )i i (I, a)
end
cI  t
return (u, )

20

Algorithm 2: One iteration of the Monte-Carlo Restricted Nash Response algorithm.
the cumulative regret. On line 16 the average strategy is updated using optimistic averaging, which
assumes that nothing has changed since the last visit at this information set.8 Finally, the strategy
tables are updated before a new iteration starts. In the next iteration, another player becomes the
restricted player. This process is repeated a number of times until satisfactory; meanwhile, each
player has been assigned to be the restricted player. At any iteration, the average strategy (I, a)
can be obtained by normalizing sI . When pr = 2 then 1 =  1 . When pr = 1 then 2 =  2 . Over
time,   = (1 , 2 ) approaches an RNR equilibrium.9
8. For more information on update strategies, we refer to the work of Lanctot et al. (2009).
9. Note that the Data-Biased Response (DBR) variant (Johanson & Bowling, 2009) works in a slightly different way.
Instead of having the selection of restricting the player or not at the root as a chance node, it is done before each
information set and then hidden from the restricted player (Johanson & Bowling, 2009). The restricted player is
forced to used a mixed strategy based on the confidence value for the current information set pConf . Lines 8 and 9

597

fiP ONSEN , L ANCTOT & D E J ONG

4.3 Experiments
We performed experiments in smaller games and in Poker. In the smaller games we performed two
types of experiments; one to characterize the relationship between exploitation and exploitability
and the other for evaluating convergence rates of sampling versus non-sampling algorithms. In Poker
we evaluate the strength of learned strategies against benchmark opponent players.
4.3.1 S MALLER G AMES
We ran two separate sets of experiments for the games OCP (we use a deck of size N = 500),
Goofspiel, Bluff, and PAM. The first set of experiments aims to characterize the relationship between exploitation and exploitability for different values of p. The second set of experiments is a
comparison of the convergence rates of RNR and MCRNR. In both cases perfect opponent models
were taken from runs of MCCFR and  was set to 0.6. Results are shown in Figures 6 and 7.
Results from the first set of experiments may influence the choice of p. If exploitation is much
more important than exploitability then a value above 0.9 is suggested; on the other hand a noticeable boost in exploitation can be achieved for a small loss of exploitability for 0.5  p  0.8. In
every game except Bluff it seems that the region p  [0.97, 1] has high impact on to the magnitude
of this trade-off. Results from Figure 7 confirm the performance benefit of sampling since MCRNR
produces a better NE approximation in less time, especially in the early iterations. This is particularly important when attempting to learn online (i.e. during playing, rather than beforehand), when
time might be limited.
4.3.2 L ARGER T EST D OMAIN : P OKER
We evaluated the policies learned by MCRNR against two poker bots, namely P OKI and S PAR B OT.
Both opponents were also used in experiments with a game theoretic (or rational) player in Section
3. The experimental settings as well as chosen abstractions are identical to those in the experiments
in Section 3.3.2. We remind the reader that S PAR B OT is a bot designed to play according to a NES
in an abstracted game; it is therefore less exploitable than P OKI.
Our MCRNR implementation in Poker restricts the player at each information set, and in essence
represents a MCDBR algorithm (see Section 4.2). However, unlike the original DBR paper, in our
experiments pConf is a global constant and its value is not biased from data. The resulting algorithm therefore is a mix between the two algorithms. We will continue using the name MCRNR
here, but the reader should note that the original RNR algorithm is slightly different.10
For the opponent modeling, we deliberately chose a setup that was realistically difficult. We
observed only  20K games played against both P OKI and S PAR B OT, as opposed to, e.g., the 1
million games used by Johanson and Bowling (2009) for RNR. These games were used to gather
opponent data concerning the two bots. Learning an opponent model can be approached as a pattern
recognition task (Bishop, 2006), wherein a model is learned based on experience (in this case,
previous Poker games of the players that are modeled). The model is then used to estimate the
behavior of opponents in unseen situations. Since the opponent data we gathered is rather sparse
due to the 20K games, and since a frequency count cannot generalize, we chose to apply a standard
from Algorithm 2 are replaced by i (I)  pConf f ix (I) + (1  pConf )i (I), where pConf is a specific value
of p per information set.
10. Based on experimental results in the smaller games, we noticed that the difference between vanilla RNR and MCDBR
with a fixed pConf is very small  both algorithms are thus similar.

598

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Exploitation/Exploitability Trade-offs in Goofspiel(5)

Exploitation/Exploitability Trade-offs in Bluff(1,1,6)

0.12

0.08

Exploitation

Exploitation

0.1

0.06
0.04
0.02
0
0

0.3

0.6

0.9

1.2

1.5

1.8

0.11
0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0

2.1

0

0.3

0.6

Exploitability
Exploitation/Exploitability Trade-offs in OCP(500)

1.2

1.5

1.8

Exploitation/Exploitability Trade-offs in PAM(3,3,8)

0.1

0.12

0.09

0.1

0.08

Exploitation

Exploitation

0.9

Exploitability

0.07
0.06
0.05

0.08
0.06
0.04
0.02

0.04
0.03

0
0

0.03

0.06

0.09

0.12

0.15

0.18

0

Exploitability

1

2

3

4

5

6

7

Exploitability

Figure 6: The trade-off between exploitation and exploitability with Monte-Carlo Restricted Nash
Response. The exploitation value is the gain in payoff when using the Monte-Carlo
Restricted Nash Response equilibrium profile compared to a Nash equilibrium profile, summed over pr  {1, 2}. The exploitability is bi (i ) summed over i =
pr  {1, 2}. The value of p used, from bottom-left point to top-right point, was:
0, 0.5, 0.7, 0.8, 0.9, 0.93, 0.97, 1.

decision tree induction algorithm (i.e., the J48 algorithm in the Weka data-mining tool) to learn
an opponent model from the sparse data. We provided the J48 algorithm with five simple features,
namely (1) the starting seat relative to the button, (2) the sum of bets or raises during the game, (3)
the sum of bets or raises in the current phase, (4) the sum of bets or raises of the modeled player
in the game, and finally (5) the bucket of the modeled player (if it was observed). For each specific
phase, we learn a model that predicts the strategy of the modeled player. We set p to a fixed value of
0.75.11 We ran offline iterations of MCRNR, froze the policy, and evaluated it. All results are shown
in Table 2, including the results for MCCFR, which we already presented earlier. We performed 104
evaluation games for each player. Again, we provide mostly results with a 10-bucket abstraction
(labeled MCRNR10). A 100-bucket abstraction (MCRNR100) was used as well, against P OKI,
to demonstrate the effect of finer abstraction levels.
11. This value is not adapted based on the experience in a specific information set, as was done in the data-biased
approach (Johanson & Bowling, 2009).

599

fiP ONSEN , L ANCTOT & D E J ONG

Convergence Rates on Bluff(1,1,9)
1.8

RNR
MCRNR

1.4
1.2
1
0.8
0.6
0.4
0.2
0

0

50000

100000

150000

200000

0

Total time (seconds)

0.25

5000

10000

15000

20000

Total time (seconds)

Convergence Rates on OCP(500)

Convergence Rates on PAM(3,3,13)
18

RNR
MCRNR

RNR
MCRNR

16

0.2

BR Convergence

BR Convergence

RNR
MCRNR

1.6
BR Convergence

BR Convergence

Convergence Rates on Goof(7)
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

0.15
0.1
0.05

14
12
10
8
6
4
2

0

0
0

1000

2000

3000

4000

5000

0

Total time (seconds)

10000

20000

30000

Total time (seconds)

Figure 7: The convergence rates of Restricted Nash Response versus Monte-Carlo Restricted Nash
Response. Fixed strategy profiles f ix were generated using MCCFR and run until
f ix  0.1. Each data point in the graphs represent two separate runs with average profiles ( r1 ,  2 ) and ( 1 ,  r2 ), where a superscript of r represents the restricted player. The
profile of interest is then  = ( 1 ,  2 ). The value on the y-axis is  = b1 ( 2 ) + b2 ( 1 ).
The profile  is an RNR equilibrium when  is minimized. Note that  does not necessary approach 0 when p > 0; the strategies may be always be somewhat exploitable due
to opponent exploitation.

As expected, MCRNR exploits P OKI considerably more than MCCFR, namely with 0.369 sb/h
(using a 10-bucket abstraction) and 0.482 sb/h (100 buckets). Interestingly, as depicted in Figure 8,
even MCRNR10 has learned to exploit P OKI in only 20 million sampled iterations. We note that
in a sampled iteration, only very few nodes are touched (i.e., only information sets are updated along
the history of the sampled terminal node), while in a RNR (and CFR, for that matter) iteration all
information sets are updated. Consequently, 20 million sampled iterations with MCRNR (or MCCFR) map to far less full-backup iterations with RNR (or CFR), and require much less computation
time than 20 million full backups.
Against S PAR B OT, which plays a better NES approximation, an improvement in performance
is also observed, but here, the difference is not significant. This is not surprising, given that S PAR -

600

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Figure 8: An online evaluation of a MCRNR policy during learning against P OKI. While the bot
is playing 1, 000 online games, an approximated 6 million offline iterations with MonteCarlo Restricted Nash Response, using a 10-bucket abstraction, are run.

Opponent

MCCFR10

MCRNR10

MCCFR100

MCRNR100

P OKI
S PAR B OT

0.059
-0.091

0.369
-0.039

0.191
0.046

0.482
0.061

Table 2: Experimental results with MCRNR versus two bots. We repeat the results of Monte-Carlo
Counter-Factual Regret Minimization (MCCFR) as reported in Section 3. Outcomes are in
small bets per hand (sb/h). Numbers behind algorithm names refer to the abstraction level
(number of buckets).

B OT is not as (extremely) exploitable as Poki. Clearly, though, MCRNR performs similarly against
S PAR B OT as MCCFR, implying that the MCRNR policy plays very few dominated actions.
In conclusion, against both P OKI as well as S PAR B OT, MCRNR finds at least an equally good
policy as MCCFR. Against P OKI, an exploitable opponent, the benefit of the algorithm becomes apparent; the MCRNR policy earns much more than the MCCFR policy, even with a coarse abstraction
(compare 0.191 sb/h for MCCFR100 with 0.369 sb/h for MCRNR10; a coarser abstraction).
We also deliberately limited the amount of data available for the opponent model to illustrate
the performance of the new algorithm under difficult conditions. Under more ideal conditions, i.e.,
most prominently with more opponent data, the algorithm can be expected to further outperform
MCCFR in terms of (simulated) money won when playing against an exploitable opponent, and to
further outperform both RNR and MCCFR in terms of the time required to find a good policy.

5. Conclusion
This article highlights two of our contributions in the field of decision-making in complex partially
observable stochastic games. We first applied two existing recent search techniques that use Monte601

fiP ONSEN , L ANCTOT & D E J ONG

Carlo sampling to the task of approximating a Nash-Equilibrium strategy (NES), namely MonteCarlo Tree Search (MCTS) and Monte-Carlo Counterfactual Regret Minimization (MCCFR). These
algorithms have not been compared before in a game as complex as we use, namely two-player Limit
Texas HoldEm Poker. MCTS has been used predominantely in perfect-information games such as
Go. In such games, the algorithm is proven to compute a NES. In imperfect-information games, balanced situations are learned that are not necessarily a Nash-Equilibrium (NE) (Sturtevant, 2008). In
our experiments, we confirm this finding: the algorithm can indeed learn reasonably strong policies
in Poker, with the drawback that these strategies are not necessarily a NE. MCCFR on the other hand
has already been shown to converge to a NE in smaller imperfect information games, such as the
ones we outlined in Section 2.2 (Lanctot et al., 2009). We apply it for the first time in the complex
game of Poker, and show that it indeed does approximate a NES. The initial convergence of MCCFR
is slower than that of MCTS. This may be due to the fact that we used outcome sampling. Other
sampling schemes, e.g., external sampling (Lanctot et al., 2009), may lead to MCCFR converging
as fast as MCTS (where fast is measured in number of iterations required). It should be mentioned
that a typical iteration of MCCFR takes significantly longer (in actual time) than one of MCTS,
due to additional computational complexity involved in the backpropagation process. MCCFR also
takes up more memory because more statistics are required for computing strategy distributions.
Our second contribution relates to the observation that Nash-Equilibrium strategies are not necessarily best to deal with clearly irrational opposition (i.e., players not playing a NES). A tailored
best-response strategy can yield more profit. Pure best-response strategies however may be brittle
and exploitable against strategies other than the one they were trained against. We present MonteCarlo Restricted Nash Response (MCRNR), a sample-based algorithm for the computation of restricted Nash strategies, essentially robust best-response strategies that (1) exploit irrational opponents more compared to the NES and (2) are not (too) exploitable by other strategies. This algorithm
combines the advantages of two state-of-the-art existing algorithms, i.e., MCCFR and Restricted
Nash Response (RNR). MCRNR samples only relevant parts of the game tree. It is therefore able to
converge faster to robust best-response strategies than RNR. We evaluate our algorithm on a variety
of imperfect-information games that are small enough to solve yet large enough to be strategically
interesting. We empirically show that MCRNR learns much quicker than standard RNR in smaller
games. We also apply MCRNR in the large game of Poker, deliberately choosing hard settings, i.e.,
relatively few iterations to come up with a policy, and relatively little opponent data. Even with such
hard settings, MCRNR learns to exploit the strong (yet exploitable) opponent bot P OKI significantly
more than the NES learned by MCCFR, while performing similarly as MCCFR against the strong
(hardly exploitable) opponent bot S PAR B OT. MCRNR achieves this performance in a fraction of
the computation time required by previous algoritms.
The strong results obtained by MCCFR and MCRNR in the complex game of two-player Poker
point to many possible applications for (refinements of) these algorithms. Most prominently, we
see ample opportunity for continued work in Poker. One of the first applications of interest is twoplayer Poker with even finer abstractions. We show that a 100-bucket abstraction performs much
better than a 10-bucket one. Indeed, state-of-the-art two-player NE bots use strategies computed
with extremely fine-grained abstractions. Less abstractions would further improve performance considerably against the opponent bots used in this article. However, in such settings, we would require
more opponent-model data, and also need to allow more iterations to learn policies. Second, integrating the Data-Biased Response (DBR) approach may lead to increased performance. Third, it
would be highly interesting to evaluate the performance of the algorithms in Poker with more than
602

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

two players. In zero-sum games with more than two players, the NES is no longer guaranteed not
to lose; coalitions between players may be formed. Nonetheless, the first work on applying algorithms such as CFR to games with more than two players suggests that the NES still is a very strong
strategy (Risk & Szafron, 2010). It would be very interesting to determine whether MCCFR and
MCRNR are as beneficial (for the speed of convergence and the quality of the solution converged
to) in three-player games as in a two-player game.

Acknowledgments
The authors thank Michael Johanson for his extensive commentary on the article, and his valuable
input on the Restricted Nash Response algorithm and Poker research at the University of Alberta in
general. We also thank the anonymous reviewers and the editors for their valuable input.

References
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multi-armed bandit
problem. Machine Learning, 47(2-3), 235256.
Balla, R. K., & Fern, A. (2009). UCT for tactical assault planning in real-time strategy games. In
International Joint Conference on Artificial Intelligence (IJCAI-2009).
Billings, D. (2006). Algorithms and Assessment in Computer Poker. Ph.D. dissertation. University
of Alberta.
Billings, D., Burch, N., Davidson, A., Holte, R. C., Schaeffer, J., Schauenberg, T., & Szafron, D.
(2003). Approximating game-theoretic optimal strategies for full-scale poker. In Gottlob, G.,
& Walsh, T. (Eds.), Proceedings of the Eighteenth International Joint Conference on Artificial
Intelligence (IJCAI-03), pp. 661668. Morgan Kaufmann.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Bouzy, B., & Chaslot, G. (2006). Monte-carlo go reinforcement learning experiments. In IEEE
2006 Symposium on Computational Intelligence in Games, Reno, USA, pp. 187194.
Chaslot, G. M. J.-B., Saito, J.-T., Bouzy, B., Uiterwijk, J., & van den Herik, H. (2006). Montecarlo strategies for computer go. In Schobbens, P.-Y., Vanhoof, W., & Schwanen, G. (Eds.),
Proceedings of the 18th BeNeLux Conference on Artificial Intelligence, pp. 8390.
Chaslot, G. M. J.-B., Winands, M., Uiterwijk, J., van den Herik, H., & Bouzy, B. (2008). Progressive
strategies for monte-carlo tree search. New Mathematics and Natural Computation, 4(3),
343357.
Coulom, R. (2006). Efficient selectivity and backup operators in monte-carlo tree search. In van den
Herik, H., Ciancarini, P., & Donkers, H. (Eds.), Proceedings of the 5th International Conference on Computer and Games, Vol. 4630 of Lecture Notes in Computer Science (LNCS), pp.
7283. Springer-Verlag, Heidelberg, Germany.
Davidson, A., Billings, D., Schaeffer, J., & Szafron, D. (2000). Improved opponent modeling
in poker. In Proceedings of The 2000 International Conference on Artificial Intelligence
(ICAI2000), pp. 14671473.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press, Cambridge, MA.
603

fiP ONSEN , L ANCTOT & D E J ONG

Gordon, G. J. (2005). No-regret algorithms for structured prediction problems. Tech. rep. CMUCALD-05-112, Carnegie Mellon University.
Hart, S., & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5), 11271150.
Hoda, S., Gilpin, A., Pena, J., & Sandholm, T. (2010). Smoothing techniques for computing Nash
equilibria of sequential games. Mathematics of Operations Research, 35(2), 494512.
Hoehn, B., Southey, F., & Holte, R. C. (2005). Effective short-term opponent exploitation in simplified poker. In In Proceedings of the National Conference on Artificial Intelligence (AAAI),
pp. 783788. AAAI Press.
Isaacs, R. (1965). Differential Games: A Mathematical Theory with Applications to Warfare and
Pursuit, Control and Optimization. John Wiley & Sons. Research Problem 12.4.1.
Johanson, M., & Bowling, M. (2009). Data biased robust counter strategies. In Proceedings of
the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS), pp.
264271.
Johanson, M., Zinkevich, M., & Bowling, M. (2008). Computing robust counter-strategies. In
Advances in Neural Information Processing Systems 20 NIPS.
Kocsis, L., & Szepesvari, C. (2006). Bandit Based Monte-Carlo Planning. In Furnkranz, J., Scheffer,
T., & Spiliopoulou, M. (Eds.), Machine Learning: ECML 2006, Vol. 4212 of Lecture Notes
in Artificial Intelligence, pp. 282293.
Kuhn, H. W. (1950). Simplified two-person poker. Contributions to the Theory of Games, 1, 97103.
Lanctot, M., Waugh, K., Zinkevich, M., & Bowling, M. (2009). Monte carlo sampling for regret
minimization in extensive games. In Advances in Neural Information Processing Systems 22
(NIPS), pp. 10781086.
Lee, C.-S., Wang, M.-H., Chaslot, G.-B., Hoock, J.-B., Rimmel, A., Teytaud, O., Tsai, S.-R., Hsu,
S.-C., & Hong, T.-P. (2010). The computational intelligence of mogo revealed in taiwans
computer go tournaments. IEEE Transactions on Computational Intelligence and AI in
games, 1, 7389.
Nash, J. (1951). Non-cooperative games. The Annals of Mathematics, 54(2), 286295.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press.
Risk, N. A., & Szafron, D. (2010). Using counterfactual regret minimization to create competitive
multiplayer poker agents. In Proc. of 9th Int. Conf. on Autonomous Agents and Multiagent
Systems (AAMAS 2010), pp. 159166.
Ross, S. M. (1971). Goofspiel  the game of pure strategy. Journal of Applied Probability, 8(3),
621625.
Sandholm, T. (2010). The state of solving large incomplete-information games, and application to
poker. AI Magazine, 31(4), 1332.
Schaeffer, J. (2001). A gamut of games. AI Magazine, 22, 2946.
Shafiei, M., Sturtevant, N., & Schaeffer, J. (2009). Comparing UCT versus CFR in simultaneous
games. In GIGA Workshop on General Game Playing IJCAI.

604

fiC OMPUTING A PPROXIMATE NASH E QUILIBRIA AND ROBUST B EST-R ESPONSES U SING S AMPLING

Sklansky, D. (2005). The Theory of Poker (Fourth ed. edition). Las Vegas: Two plus two.
Southey, F., Bowling, M., Larson, B., Piccione, C., Burch, N., Billings, D., & Rayner, D. C. (2005).
Bayes bluff: Opponent modelling in poker. In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence (UAI 05), pp. 550558.
Sturtevant, N. R. (2008). An analysis of uct in multi-player games. In In Computers and Games.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior. Princeton
University Press.
Waugh, K., Zinkevich, M., Johanson, M., Kan, M., Schnizlein, D., & Bowling, M. (2009). A practical use of imperfect recall. In Proceedings of the 8th Symposium on Abstraction, Reformulation and Approximation (SARA).
Zinkevich, M., Johanson, M., Bowling, M., & Piccione, C. (2008). Regret minimization in games
with incomplete information. In Advances in Neural Information Processing Systems 20
NIPS.

605

fiJournal of Artificial Intelligence Research 42 (2011) 353-392

Submitted 5/11; published 11/11

Learning to Make Predictions In Partially Observable
Environments Without a Generative Model
Erik Talvitie

erik.talvitie@fandm.edu

Mathematics and Computer Science
Franklin and Marshall College
Lancaster, PA 17604-3003, USA

Satinder Singh

baveja@umich.edu

Computer Science and Engineering
University of Michigan
Ann Arbor, MI 48109-2121, USA

Abstract
When faced with the problem of learning a model of a high-dimensional environment, a
common approach is to limit the model to make only a restricted set of predictions, thereby
simplifying the learning problem. These partial models may be directly useful for making
decisions or may be combined together to form a more complete, structured model. However, in partially observable (non-Markov) environments, standard model-learning methods
learn generative models, i.e. models that provide a probability distribution over all possible futures (such as POMDPs). It is not straightforward to restrict such models to make
only certain predictions, and doing so does not always simplify the learning problem. In
this paper we present prediction profile models: non-generative partial models for partially
observable systems that make only a given set of predictions, and are therefore far simpler
than generative models in some cases. We formalize the problem of learning a prediction
profile model as a transformation of the original model-learning problem, and show empirically that one can learn prediction profile models that make a small set of important
predictions even in systems that are too complex for standard generative models.

1. Introduction
Learning a model of the dynamics of an environment through experience is a critical capability for an artificial agent. Agents that can learn to make predictions about future events
and anticipate the consequences of their own actions can use these predictions to plan and
make better decisions. When the agents environment is very complex, however, this learning problem can pose serious challenges. One common approach to dealing with complex
environments is to learn partial models, focusing the model-learning problem on making
a restricted set of particularly important predictions. Often when only a few predictions
need to be made, much of the complexity of the dynamics being modeled can be safely ignored. Sometimes a partial model can be directly useful for making decisions, for instance
if the model makes predictions about the agents future rewards (e.g., see McCallum, 1995;
Mahmud, 2010). In other cases, many partial models making restricted predictions are
combined to form a more complete model as in, for instance, factored MDPs (Boutilier,
Dean, & Hanks, 1999), factored PSRs (Wolfe, James, & Singh, 2008), or collections of
local models (Talvitie & Singh, 2009b).
c
2011
AI Access Foundation. All rights reserved.

fiTalvitie & Singh

The most common approach to learning a partial model is to apply an abstraction
(whether learned or supplied by a domain expert) that filters out detail from the training data that is irrelevant to making the important predictions. Model-learning methods
can then be applied to the abstract data, and typically the learning problem will be more
tractable as a result. However, especially in the case of partially observable systems, abstraction alone may not sufficiently simplify the learning problem, even (as we will see in
subsequent examples) when the model is being asked to make intuitively simple predictions.
The counter-intuitive complexity of learning a partial model in the partially observable case
is a direct result of the fact that standard model-learning approaches for partially observable systems learn generative models that attempt to make every possible prediction about
the future and cannot be straightforwardly restricted to making only a few particularly
important predictions.
In this paper we present an alternative approach that learns non-generative models that
make only the specified predictions, conditioned on history. In the following illustrative
example, we will see that sometimes a small set of predictions is all that is necessary for
good control performance but that learning to make these predictions in a high-dimensional
environment using standard generative models can pose serious challenges. By contrast we
will see that there exists a simple, non-generative model that can make and maintain these
predictions and this will form the learning target of our method.
1.1 An Example
Consider the simple game of Three Card Monte. The dealer, perhaps on a crowded street,
has three cards, one of which is an ace. The dealer shows the location of the ace, flips over
the cards, and then mixes them up by swapping two cards at every time step. A player of
the game must keep track of location of the ace. Eventually the dealer stops mixing up the
cards and asks for a guess. If a player correctly guesses where the ace is, they win some
money. If they guess wrong, they lose some money.
Consider an artificial agent attempting to learn a model of the dynamics of this game
from experience. It takes a sequence of actions and perceives a sequence of observations.
The raw data received by the agent includes a rich, high-dimensional scene including the
activities of the crowd, the movement of cars, the weather, as well as the game itself (the
dealer swapping cards). Clearly, learning a model that encompasses all of these complex
phenomena is both infeasible and unnecessary. In order to win the game, the agent needs
only focus on making predictions about the cards, and need not anticipate the future behavior of the city scene around it. In particular, the agent need only make three predictions:
If I flip over card 1, will it be the ace? and the corresponding predictions for cards 2
and 3. One can safely ignore much of the detail in the agents experience and still make
these important predictions accurately. Once one filters out the irrelevant detail, the agents
experience might look like this:
bet pos2 watch swap1, 2 watch swap2, 3 . . . ,
where the agent takes the bet action, starting the game, and observes the dealer showing
the card in position 2. Then the agent takes the watch action, observes the dealer swapping
cards 1 and 2, takes the watch action again, observes the dealer swapping cards 2 and 3, and
354

fiLearning to Make Predictions Without a Generative Model

so on until the dealer prompts the agent for a guess (note that this is not an uncontrolled
system; watch is indeed an action that the agent must select over, say, reaching out and
flipping the cards itself, which in a real game of Three Card Monte would certainly result
in negative utility!) Now the data reflects only the movement of the cards. One could learn
a model using this new data set and the learning problem would be far simpler than before
since complex and irrelevant phenomena like the crowd and the weather have been ignored.
In the Markov case, the agent directly observes the entire state of the environment and
can therefore learn to make predictions as a direct function of state. Abstraction simplifies
the representation of state and thereby simplifies the learning problem. Note, however, that
the Three Card Monte problem is partially observable (non-Markov). The agent cannot
directly observe the state of the environment (the location of the ace and the state of the
dealers mind are both hidden to the agent). In the partially observable case, the agent
must learn to maintain a compact representation of state as well as learn the dynamics of
that state. The most common methods to achieve this, such as expectation-maximization
(EM) for learning POMDPs (Baum, Petrie, Soules, & Weiss, 1970), learn generative models
which provide a probability distribution over all possible futures.
In Three Card Monte, even when all irrelevant details have been ignored and the data
contains only information about the cards movement, a generative model will still be intractably complex! A generative model makes predictions about all future events. This
includes the predictions the model is meant to make (such as whether flipping over card 1
in the next time-step will reveal the ace) but also many irrelevant predictions. A generative
model, will also predict, for instance, whether flipping over card 1 in 10 time-steps will
reveal the ace or whether cards 1 and 2 will be swapped in the next time-step. To make
these predictions, the model must capture not only the dynamics of the cards but also of
the dealers decision-making process. If the dealer decides which cards to swap using some
complex process (as a human dealer might) then the problem of learning a generative model
of this abstract system will be correspondingly complex.
Of course, in Three Card Monte, predicting the dealers future behavior is entirely
unnecessary to win. All that is required is to maintain the aces current location over time.
As such, learning a model that devotes most of its complexity to anticipating the dealers
decisions is counter-intuitive at best. A far more reasonable model can be seen in Figure 1.
Here the states of the model are labeled with predictions about the aces location. The
transitions are labeled with observations of the dealers behavior. As an agent plays the
game, it could use such a model to maintain its predictions about the location of the ace
over time, taking the dealers behavior into account, but not predicting the dealers future
behavior. Note that this is a non-generative model. It does not provide a distribution
over all possible futures and it cannot be used to simulate the world because it does not
predict the dealers next move. It only provides a limited set of conditional predictions
about the future, given the history of past actions and observations. On the other hand,
it is far simpler than a generative model would be. Because it does not model the dealers
decision-making process, this model has only 3 states, regardless of the underlying process
used by the dealer.
The model in Figure 1 is an example of what we term a prediction profile model. This
paper will formalize prediction profile models and present an algorithm for learning them
from data, under some assumptions (to be specified once we have established some necessary
355

fiTalvitie & Singh

Figure 1: Maintaining predictions about the location of the ace in Three Card Monte. Transitions are labeled with the dealers swaps. States are labeled with the predicted
position of the special card.

terminology). We will empirically demonstrate that in some partially observable systems
that prove too complex for standard generative model-learning methods, it is possible to
learn a prediction profile model that makes a small set of important predictions that allow
the agent to make good decisions. The next sections will formally describe the setting
and establish some notation and terminology and formalize the general learning problem
being addressed. Subsequent sections will formally present prediction profile models and
an algorithm for learning them, as well as several relevant theoretical and empirical results.
1.2 Discrete Dynamical Systems
We focus on discrete dynamical systems. The agent has a finite set A of actions that it can
take and the environment has a finite set O of observations that it can produce. At every
time step i, the agent chooses an action ai  A and the environment stochastically emits
an observation oi  O.
Definition 1. At time step i, the sequence of past actions and observations since the
beginning of time hi = a1 o1 a2 o2 . . . ai oi is called the history at time i.
The history at time zero, before the agent has taken any actions or seen any observations
h0 , is called the null history.
1.2.1 Predictions
An agent uses its model to make conditional predictions about future events, given the history of actions and observations and given its own future behavior. Because the environment
is assumed to be stochastic, predictions are probabilities of future events. The primitive
building block used to describe future events is called a test (after Rivest & Schapire, 1994;
Littman, Sutton, & Singh, 2002). A test t is simply a sequence of actions and observations
356

fiLearning to Make Predictions Without a Generative Model

that could possibly occur, t = a1 o1 . . . ak ok . If the agent actually takes the action sequence
in t and observes the observation sequence in t, we say that test t succeeded. A prediction
p(t | h) is the probability that test t succeeds after history h, assuming the agent takes the
actions in the test. Essentially, the prediction of a test is the answer to the question If
I were to take this particular sequence of actions, with what probability would I see this
particular sequence of observations, given the history so far? Formally,
def

p(t | h) = Pr(o1 | h, a1 )Pr(o2 | ha1 o1 , a2 ) . . . Pr(ok | ha1 o1 a2 o2 . . . ak1 ok1 , ak ).

(1)

Let T be the set of all tests (that is, the set of all possible action-observation sequences
of all lengths). Then the set of all possible histories H is the set of all action-observation
sequences that could possibly occur starting from the null history, and the null history itself:
def
H = {t  T | p(t | h0 ) > 0}  {h0 }.
A model that can make a prediction p(t | h) for all t  T and h  H can make any
conditional prediction about the future (Littman et al., 2002). Because it represents a
probability distribution over all futures, such a model can be used to sample from that
distribution in order to simulate the world, or sample possible future trajectories. As
such, we call a model that makes all predictions a generative model.
Note that the use of the word generative here is closely related to its broader sense
in general density estimation. If one is attempting to represent the conditional probability
distribution Pr(A | B), the generative approach would be to represent the full joint distribution Pr(A, B) from which the conditional probabilities can be computed as Pr(A,B)
Pr(B) . That is
to say, a generative model in this sense makes predictions even about variables we only wish
to condition on. The non-generative or, in some settings, discriminitive approach would
instead directly represent the conditional distribution, taking the value of B as un-modeled
input. The non-generative approach can sometimes result in significant savings if Pr(B) is
very difficult to represent/learn, but Pr(A | B) is relatively simple (so long as one is truly
disinterested in modeling the joint distribution).
In our particular setting, a generative model is one that provides a probability distribution over all futures (given the agents actions). As such, one would use a generative model
0)
. In fact, from Equation 1 one
to compute p(t | h) for some particular t and h as p(ht|h
p(h|h0 )
can see that the prediction for any multi-step test can be computed from the predictions of
one-step tests:
p(a1 o1 a2 o2 . . . ak ok | h) = p(a1 o1 | h)p(a2 o2 | ha1 o1 ) . . . p(ak ok | ha1 o1 a2 o2 . . . ak ok ).
This leads to a simple definition of a generative model:
Definition 2. Any model that can provide the predictions p(ao | h) for all actions a  A,
observations o  O and histories h  H is a generative model.
A non-generative model, then, would not make all one-step predictions in all histories
and, consequently, would have to directly represent the prediction p(t | h) with the history h
as an un-modeled input. It would condition on a given history, but not necessarily be capable
of computing the probability of that history sequence. As we saw in the Three Card Monte
example, this can be beneficial if making and maintaining predictions for t is substantially
simpler than making predictions for every possible action-observation sequence.
357

fiTalvitie & Singh

Note that a test describes a very specific future event (a sequence of specific actions
and observations). In many cases one might wish to make predictions about more abstract
events. This can be achieved by composing the predictions of many tests. For instance set
tests (Wingate, Soni, Wolfe, & Singh, 2007) are a sequence of actions and a set of observation
sequences. A set test succeeds when the agent takes the specified action sequence and sees
any observation sequence contained within the set occur. While traditional tests allow an
agent, for instance to express the question If I go outside, what is the probability I will see
this exact sequence of images? a set test can express the far more useful, abstract question
If I go outside, what is the probability that it will be sunny? by grouping together all
observations of a sunny day. Even more generally, option tests (Wolfe & Singh, 2006; Soni
& Singh, 2007) express future events where the agents behavior is described abstractly as
well as the resulting observations. These types of abstract predictions can be computed as
the linear combination of a set of concrete predictions.
1.2.2 System Dynamics Matrix and Linear Dimension
It is sometimes useful to describe a dynamical system using a conceptual object called the
system dynamics matrix (Singh, James, & Rudary, 2004). The system dynamics matrix
contains the values of all possible predictions, and therefore fully encodes the dynamics of
the system. Specifically,
Definition 3. The system dynamics matrix of a dynamical system is an infinity-by-infinity
matrix. There is a column corresponding to every test t  T . There is a row corresponding
to every history h  H. The ijth entry of the system dynamics matrix is the prediction
p(tj | hi ) of the test corresponding to column j at the history corresponding to row i and
there is an entry for every history-test pair.
Though the system dynamics matrix has infinitely many entries, in many cases it has
finite rank. The rank of the system dynamics matrix can be thought of as a measure of the
complexity of the system (Singh et al., 2004).
Definition 4. The linear dimension of a dynamical system is the rank of the corresponding
system dynamics matrix.
For some popular modeling representations, the linear dimension is a major factor in the
complexity of representing and learning a generative model of the system. For instance, in
POMDPs, the number of hidden states required to represent the system is lower-bounded
by the linear dimension. In this work we adopt linear dimension as our measure of the
complexity of a dynamical system. When we say a system is simpler than another, we
mean it has a lower linear dimension.
1.2.3 The Markov Property
A dynamical system is Markov if all that one needs to know about history in order to make
predictions about future events is the most recent observation.
Definition 5. A system is Markov if for any two histories h and h (that may be the null
history), any two actions a and a , any observation o, and any test t, p(t | hao) = p(t | h a o).
358

fiLearning to Make Predictions Without a Generative Model

In the Markov case we will use the notational shorthand p(t | o) to indicate the prediction
of t at any history that ends in observation o. In the Markov case, because observations
contain all the information needed to make any prediction about the future, they are often
called state (because they describe the state of the world). When a system is not Markov,
it is partially observable. In partially observable systems predictions can depend arbitrarily
on the entire history. We focus on the partially observable case.

2. Learning to Make Predictions
In this work we assume that, as in Three Card Monte, though the agent may live in a
complex environment, it has only a small set of important predictions to make. These
predictions could have been identified as important by a designer, or by some other learning
process. We do not address the problem of identifying which predictions should be made,
but rather focus on the problem of learning to make predictions, once they are identified.
In general, we imagine that we are given some finite set T I = {t1 , t2 , . . . , tm } of tests of
interest for which we would like our model to make accurate predictions. Here the term
test should be construed broadly, possibly including abstract tests in addition to raw
sequences of actions and observations. The tests of interest are the future events the model
should predict. For instance, in the Three Card Monte problem, in order to perform well
the agent must predict whether it will see the ace when it flips over each card. So it will
have three one-step tests of interest: f lip1 ace, f lip2 ace, and f lip3 ace (representing the
future events where the agent flips over card 1, 2, and 3, respectively, and sees the ace). If
the agent can learn to maintain the probability of these events over time, it can win the
game.
As such, the general problem is to learn a function  : H  [0, 1]m where
def

(h) = hp(t1 | h), p(t2 | h), . . . , p(tm | h)i,

(2)

that is, a function from histories to the predictions for the test of interest (which we will refer
to as the predictions of interest) at that history. Note that the output of  is not necessarily
a probability distribution. The tests of interest may be selected arbitrarily and therefore
need not represent mutually exclusive or exhaustive events. We will call a particular vector
of predictions for the tests of interest a prediction profile.
Definition 6. We call (h) the prediction profile at history h.
We now describe two existing general approaches to learning : learning a direct function from history to predictions (most common in the Markov case), and learning a fully
generative model that maintains a finite-dimensional summary of history (common in the
partially observable case). Both have strengths and weaknesses as approaches to learning
. Section 2.3 will contrast these with our approach, which combines some of the strengths
of both approaches.
2.1 Direct Function Approximation
When the system is Markov, learning  is conceptually straightforward; essentially it is
a problem of learning a function from observation (state) to predictions. Rather than
359

fiTalvitie & Singh

learning  which takes histories as input, one can instead learn a function M arkov : O 
[0, 1]m , which maps an observation to the predictions for the tests of interest resulting from
all histories that end in that observation. Note that, as an immediate consequence, in
discrete Markov systems there is a finite number of distinct prediction profiles. In fact,
there can be no more distinct prediction profiles than there are observations.
When the number of observations and the number of tests of interest are small enough,
M arkov can be represented as a |O|  |T I | look-up table, and the entries estimated using
sample averages1 :
p(ti | o) =

# times t succeeds from histories ending in o
.
# times acts(t) taken from histories ending in o

(3)

The main challenge of learning Markov models arises when the number of observations is
very large. Then it becomes necessary to generalize across observations, using data gathered
about one observation to learn about many others. Specifically, one may be able to exploit
the fact that some observations will be associated with very similar (or identical) prediction
profiles (that is, the same predictions for the tests of interest) and share data amongst them.
Restricting a models attention to only a few predictions can afford more generalization,
which is why learning a partial model can be beneficial in the Markov setting.
Even when the system is partially observable, one can still attempt to learn  directly,
typically by performing some sort of regression over a set of features of entire histories. For
instance, U-Tree (McCallum, 1995) takes a set of history features and learns a decision tree
that attempts to distinguish histories that result in different expected asymptotic return
under optimal behavior. Wolfe and Barto (2006) apply a U-Tree-like algorithm but rather
than restricting the model to predicting future rewards, they learn to make predictions
about some pre-selected set of features of the next observation (a special case of the more
general concept of tests of interest). Dinculescu and Precup (2010) learn the expected value
of a given feature of the future as a direct function of a given real-valued feature of history
by clustering futures and histories that have similar associated values.
Because they directly approximate  these types of models only make predictions for T I
and are therefore non-generative (and therefore able, for instance, to avoid falling into the
trap of predicting the dealers decisions in Three Card Monte). Though this approach has
demonstrated promise, it also faces a clear pragmatic challenge, especially in the partially
observable setting: feature selection. Because  is a function of history, an ever-expanding
sequence of actions and observations, finding a reasonable set of compactly represented features that collectively capture all of the history information needed to make the predictions
of interest is a significant challenge. In a sense, even in the partially observable setting,
this type of approach takes only a small step away from the Markov case. It still requires
a good idea a priori of what information should be extracted from history (in the form of
features) in order to make the predictions of interest.
1. Bowling, McCracken, James, Neufeld, and Wilkinson (2006) showed that this estimator is unbiased only
in the case where the data is collected using a blind policy, in which action selection does not depend on
the history of observations and provided an alternative estimator that is unbiased for all policies. For
simplicitys sake, however, we will assume throughout that the data gathering policy is blind.

360

fiLearning to Make Predictions Without a Generative Model

2.2 Generative Models
If one does not have a good idea a priori of what features should be extracted from history
to make accurate predictions, one faces the additional challenge of learning to summarize
the relevant information from history in a compact sufficient statistic.
There exist methods that learn from training data to maintain a finite-dimensional
statistic of history from which any prediction can be computed. In analogy to the Markov
case, this statistic is called the state vector. Clearly any model that can maintain state
can be used to compute  (since it can make all predictions). We briefly mention two
examples of this approach that are particularly relevant to the development and analysis of
our method.
POMDPs By far the most popular representation for models of partially observable
systems is the partially observable Markov decision process (POMDP) (Monahan, 1982).
A POMDP posits an underlying MDP (Puterman, 1994) with a set S of hidden states that
the agent never observes. At any given time-step i, the system is in some particular hidden
state si1  S (unknown to the agent). The agent takes some action ai  A and the system
transitions to the next state si according to the transition probability Pr(si | si1 , ai ). An
observation oi  O is then emitted according to a probability distribution that in general
may depend upon si1 , ai , and si : Pr(oi | si1 , ai , si ).
Because the agent does not observe the hidden states, it cannot know which hidden
state the system is in at any given moment. The agent can however maintain a probability
distribution that represents the agents current beliefs about the hidden state. This probability distribution is called the belief state. If the belief state associated with history h is
known, then it is straightforward to compute the prediction of any test t:
X
p(t | h) =
Pr(s | h)Pr(t | s),
sS

where Pr(t | s) can be computed using the transition and observation emission probabilities.
The belief state is a finite summary of history from which any prediction about the
future can be computed. So, the belief state is the state vector for a POMDP. Given the
transition probabilities and the observation emission probabilities, it is possible to maintain
the belief state over time using Bayes rule. If at the current history h one knows Pr(s | h)
for all hidden states s and the agent takes action a and observes observation o, then one
can compute the probability of any hidden state s at the new history:
P



s S Pr(s | h)Pr(s | s , ai )Pr(oi | s , ai , s)
P
Pr(s | hao) = P
.
(4)





s S
s S Pr(s | h)Pr(s | s , ai )Pr(oi | s , ai , s )
The parameters of a POMDP that must be learned in order to be able to maintain
state are the transition probabilities and the observation emission probabilities. Given these
parameters, the belief state corresponding to any given history can be recursively computed
and the model can thereby make any prediction at any history. POMDP parameters are
typically learned using the Expectation Maximization (EM) algorithm (Baum et al., 1970).
Given some training data and the number of actions, observations, and hidden states as
input, EM essentially performs gradient ascent to find transition and emission distributions
that (locally) maximize the likelihood of the provided data.
361

fiTalvitie & Singh

PSRs Another more recently introduced modeling representation is the predictive state
representation (PSR) (Littman et al., 2002). Instead of hidden states, PSRs are defined more
directly in terms of the system dynamics matrix (described in Section 1.2.2). Specifically,
PSRs find a set of core tests Q whose corresponding columns in the system dynamics matrix
form a basis. Recall that the system dynamics matrix often has finite rank (for instance, the
matrix associated with any POMDP with finite hidden states has finite linear dimension)
and thus Q is finite for many systems of interest. Since the predictions of Q are a basis,
the prediction for any other test at some history can be computed as a linear combination
of the predictions of Q at that history.
The vector of predictions for Q is called the predictive state. While the belief state was
the state vector for POMDPs, the predictive state is the state vector for PSRs. It can also
be maintained by application of Bayes rule. Specifically, if at some history h, p(q | h)
is known for all core tests q and the agent takes some action a  A and observes some
observation o  O, then one can compute the prediction of any core test q at the new
history:
P


p(aoq | h)
q  Q p(q | h)maoq (q )
P
p(q | hao) =
=
,
(5)


p(ao | h)
q  Q p(q | h)mao (q )
where maoq (q  ) is the coefficient of p(q  | h) in the linear combination that computes the
prediction p(aoq | h).
So, given a set of core tests, the parameters of a PSR that must be learned in order
to maintain state are the coefficients mao for every action a and observation o and the
coefficients maoq for every action a, observation o, and core tests q. Given these parameters
the predictive state at any given history can be recursively computed and used to make any
prediction about the future. PSRs are learned by directly estimating the system dynamics matrix (James & Singh, 2004; Wolfe, James, & Singh, 2005) or, more recently, some
sub-matrix or derived matrix thereof (Boots, Siddiqi, & Gordon, 2010, 2011) using sample
averages in the training data. The estimated matrix is used to find a set of core tests and
the parameters are then estimated using linear regression.
Note that both of these types of models are inherently generative. They both rely
upon the maintenance of the state vector in order to make predictions and, as can be
seen in Equations 4 and 5, the state update equations of these models rely upon access to
one-step predictions to perform the Bayesian update. As such, unlike the direct function
approximation approach, one cannot simply choose a set of predictions for the model to
make. These models by necessity make all predictions.
There are many reasons to desire a complete, generative model. Because it makes all
possible predictions, such a model can be used to sample possible future trajectories which is
a useful capability for planning. A generative model is also, by definition, very flexible about
what predictions it can be used to make. On the other hand, in many cases a complete,
generative model may be difficult to obtain. Both PSR and POMDP training methods scale
very poorly with the linear dimension of the system being learned. The linear dimension
lower-bounds the number of hidden states needed to represent a system as a POMDP and
is precisely the number of core tests needed to represent it as a PSR. The learning methods
for POMDPs and PSRs are rarely successfully applied to systems with a linear dimension of
362

fiLearning to Make Predictions Without a Generative Model

Figure 2: Size 10 1D Ball Bounce
more than a few hundred (though the work of Boots et al. is pushing these limits further).
Most systems of interest will have several orders of magnitude higher linear dimension.
Furthermore, a complete, generative model is overkill for the problem at hand. Recall
that we do not seek to make all predictions; we are focused on making some particularly
important predictions T I . Even in problems where learning to make all predictions might
be intractable, it should still be possible to make some simple but important predictions.
2.2.1 Abstract Generative Models
As discussed earlier, when there is a restricted set of tests of interest, the learning problem
can often be simplified by ignoring irrelevant details through abstraction. Of course, having
an abstraction does not solve the problem of partial observability. What is typically done
is to apply the abstraction to the training data, discarding the irrelevant details (as we
did in the Three Card Monte example) and then to apply model learning methods like the
ones described above to the abstract data set. Just as in the Markov setting, in some cases
observation abstraction can greatly simplify the learning problem (certainly learning about
only the cards in Three Card Monte is easier than learning about the cards and the crowd
and weather and so on).
Ignoring details irrelevant to making the predictions of interest is intuitive and can
significantly simplify the learning problem. On the other hand, because they are generative
models, an abstract POMDP or PSR will still make all abstract predictions. This typically
includes predictions other than those that are directly of interest. If these extra predictions
require a complex model, even an abstract generative model can be intractible to learn. This
is true of the Three Card Monte example (where a generative model ends up modeling the
dealer as well as the cards). The following is another simple example of this phenomenon.
Example. Consider the uncontrolled system pictured in Figure 2, called the 1D Ball
Bounce system. The agent observes a strip of pixels that can be black or white. The
black pixel represents the position of a ball that moves around on the strip. The ball has
a current direction and every time-step it moves one pixel in that direction. Whenever it
reaches an edge pixel, its current direction changes to move away from the edge. In Figure
3(a) a complete POMDP model of a 10 pixel version of this system is pictured. If there
are k pixels, the POMDP has 2k  2 hidden states (because the ball can have one of 2
possible directions in one of k possible positions, except the two ends, where there is only
one possible direction).
Now say the agent wishes only to predict whether the ball will be in the position marked
with the x in the next time step. Clearly this prediction can be made by only paying
attention to the immediate neighborhood of the x. The details of what happens to the
ball while it is far away do not matter for making these predictions. So, one could apply
363

fiTalvitie & Singh

(a)

(b)

Figure 3: POMDP model of the size 10 1D Ball Bounce system (a) and of the abstracted
1D Ball Bounce system (b).

an abstraction that lumps together all observations in which the neighborhood about x
looks the same. The problem is that an abstract generative model of this system makes
predictions not only about x, but also about the pixels surrounding x. Specifically, the
model still makes predictions about whether the ball will enter the neighborhood in the near
future. This of course depends on how long it has been since the ball left the neighborhood.
So, the POMDP model of the abstract system (pictured in Figure 3(b)) has exactly the same
state diagram as the original system, though its observations have changed to reflect the
abstraction. The abstract system and the primitive system have the same linear dimension.
In order to make predictions about x, one must condition on information about the
pixels surrounding x. Consequently, a generative model also makes predictions about these
pixels. Counterintuitively, the abstract models complexity is mainly devoted to making
predictions other than the predictions of interest. In general, while learning an abstract
model can drastically simplify the learning problem by ignoring irrelevant details, an abstract generative model still learns to make predictions about any details that are relevant,
even if they are not directly of interest.

2.3 Prediction Profile Models
The contribution of this paper, prediction profile models, seek to combine the main strengths
of the two model-learning approaches discussed above. As with a direct approximation of
, a prediction profile model will only make the predictions of interest, and no others.
As such, it can be far simpler than a generative model, which will typically make many
extraneous predictions. However, the learning method for prediction profile models will
not require a set of history features to be given a priori. By leveraging existing generative
model learning methods, prediction profile models learn to maintain the state information
necessary for making the predictions of interest.
364

fiLearning to Make Predictions Without a Generative Model

Figure 4: Prediction profile model for the 1D Ball Bounce system
A typical model learns to make predictions about future observations emitted by the
system. The main idea behind prediction profile models is to instead model the values of
the predictions themselves as they change over time, conditioned on both the actions chosen
by the agent and the observations emitted by the system.
We have already seen an example of this in Three Card Monte. The prediction profile
model (shown in Figure 1) takes observations of the dealers behavior as input and outputs
predictions for the tests of interest. It does not predict the dealers behavior, but it takes it
into account when updating the predictions of interest. Recall that, though the Three Card
Monte system can be arbitrarily complicated (depending on the dealer), this prediction
profile system has three states, regardless of the dealers decision making process.
Another example is shown in Figure 4. This is the prediction profile system for the 1D
Ball Bounce system (Figure 2), where the model must predict whether the ball will enter
position x in the next time-step. Each state of the prediction profile model is labeled with a
prediction for pixel x (white or black). The transitions are labeled with observations of the
3-pixel neighborhood centered on position x. In this case the transitions capture the ball
entering the neighborhood, moving to position x, leaving the neighborhood, staying away
for some undetermined amount of time, and returning again. Recall that a POMDP model
of this system has 2k  2 hidden states, where k is the number of pixels, even after ignoring
all pixels irrelevant to making predictions about pixel x. By contrast, the prediction profile
model always has three states, regardless of the number of pixels.
The next section will formally describe prediction profile models as models of a dynamical system that results from a transformation of the original system. Subsequent sections
will discuss how to learn prediction profile models from data (by converting data from the
original system into data from the transformed system and learning a model from the converted data set) and present results that help to characterize the conditions under which
prediction profile models are best applied.

3. The Prediction Profile System
We now formally describe a theoretical dynamical system, defined in terms of both the
dynamics of the original system and the given tests of interest. We call this constructed
system the prediction profile system. A prediction profile model, which it is our goal to
365

fiTalvitie & Singh

construct, is a model of the prediction profile system (that is, the system is an ideal,
theoretical construct, a model may be imperfect, approximate, etc.). As such, our analysis of
the problem of learning a prediction profile model will depend a great deal on understanding
properties of the prediction profile system.
In this paper we make the restrictive assumption that, as in the Markov case, there is
a finite number of distinct prediction profiles (that is, the predictions of interest take on
only a finite number of distinct values). This is certainly not true of all partially observable
systems and all sets of tests of interest, though it is true in many interesting examples.
Formally, this assumption requires that  map histories to a finite set of prediction profiles:
Assumption 7. Assume there exists a finite set of prediction profiles P = {1 , 2 , . . . , k } 
[0, 1]m such that for every history h, (h)  P .
This assumption allows the definition of the prediction profile system (or P P for short)
as a discrete dynamical system that captures the sequence of prediction profiles over time,
given an action observation sequence. The prediction profile systems actions, observations,
and dynamics are defined in terms of quantities associated with the original system:
Definition 8. The prediction profile system is defined by a set of observations, a set of
actions, and a rule governing its dynamics.
1. Observations: The set of prediction profile observations, OP P , is defined to be the set
def
of distinct prediction profiles. That is, OP P = P = {1 , . . . , k }.
2. Actions: The set of prediction profile actions, AP P , is defined to be the set of actiondef
observation pairs in the original system. That is, AP P = A  O.
3. Dynamics: The dynamics of the prediction profile system are deterministically governed by . At any prediction profile history, ha1 , o1 i1 ha2 , o2 i2 . . . haj , oj ij , and
for any next P P -action, haj+1 , oj+1 i, the prediction profile system deterministically
emits the P P -observation (a1 o1 a2 o2 . . . aj oj aj+1 oj+1 ).
We now present some key facts about the prediction profile system. Specifically, it
will be noted that the prediction profile system is always deterministic. Also, though the
prediction profile system may be Markov (as it is in the Three Card Monte example), in
general it is partially observable.
Proposition 9. Even if the original system is stochastic, the prediction profile system is
always deterministic.
Proof. This follows immediately from the definition: every history corresponds to exactly
one prediction profile. So a P P -history (action-observation-profile sequence) and a P P action (action-observation pair) fully determine the next P P -observation (prediction profile). The stochastic observations in the original system have been folded into the unmodeled actions of the prediction profile system.
Proposition 10. If the original system is Markov, the prediction profile system is Markov.
366

fiLearning to Make Predictions Without a Generative Model

Proof. By definition, if the original system is Markov the prediction profile at any time
step depends only on the most recent observation. So, if at time step t, the current profile
is t , the agent takes action at+1 and observes observation ot+1 , the next profile is simply
t+1 = M arkov (ot+1 ). So, in fact, when the original system is Markov, the prediction profile
system satisfies an even stronger condition: the next P P -observation is fully determined
by the P P -action and has no dependence on history whatsoever (including the most recent
P P -observation).
Proposition 11. Even if the original system is partially observable, the prediction profile
system may be Markov.
Proof. Consider the Three Card Monte example. The original system is clearly non-Markov
(the most recent observation, that is the dealers most recent swap, tells one very little about
the location of the ace). However, the prediction profile system for the tests of interest
regarding the location of the special card (pictured in Figure 1) is Markov. The next profile
is fully determined by the current profile and the P P -action.
In general, however, the P P system may be partially observable. Though in the Three
Card Monte example the current prediction profile and the next action-observation pair
together fully determine the next prediction profile, in general the next prediction profile is
determined by the history of action-observation pairs (and prediction profiles).
Proposition 12. The prediction profile system may be partially observable.
Proof. Recall the 1D Ball Bounce example. The corresponding prediction profile system is
shown in Figure 4. Note that two distinct states in the update graph are associated with
the same prediction profile (pixel x will be white). Given only the current prediction profile
(pixel x will be white) and the P P -action (observe the ball in a neighboring pixel on the left
or right), one cannot determine whether the ball is entering or leaving the neighborhood,
and thus cannot uniquely determine the next profile. This prediction profile system is
partially observable.
So, in general, the prediction profile system is a deterministic, partially-observable dynamical system. A model of the prediction profile system can only be used to make the
predictions of interest. As such, if one wishes to use a prediction profile model as a generative
model, one must select the tests of interest carefully. For instance:
Proposition 13. If the tests of interest include the set of one-step primitive tests, that is
if {ao | a  A, o  O}  T I , then a model of the prediction profile system can be used as a
generative model of the original system.
Proof. This follows immediately from the definition of generative model.
While in this special case a prediction profile model can be a complete, generative
model of the system, it will be shown in Section 5 that if one desires a generative model,
it is essentially never preferable to learn a prediction profile model over a more traditional
representation. A prediction profile model is best applied when it is relatively simple to make
and maintain the predictions of interest in comparison to making all predictions. In general,
367

fiTalvitie & Singh

Figure 5: Flow of the algorithm.
a prediction profile model conditions on the observations, but it does not necessarily predict
the next observation. As such, a model of the prediction profile system cannot typically be
used for the purposes of model-based planning/control like a generative model could. The
experiments in Section 6 will demonstrate that the output of prediction profile models can,
however, be useful for model-free control methods.

4. Learning a Prediction Profile Model
The definition of the prediction profile system straightforwardly suggests a method for
learning prediction profile models (estimate the prediction profiles, and learn a model of
their dynamics using a standard model-learning technique). This section will present such
a learning algorithm, discussing some of the main practical challenges that arise.
Let S be a training data set of trajectories of experience with the original system (actionobservation sequences) and let T I = {t1 , t2 , . . . , tk } be the set of tests of interest. The
algorithm presented in this section will learn a model of the prediction profile system from
the data S. The algorithm has three main steps (pictured in Figure 5). First the training
data is used to estimate the prediction profiles (both the number of unique profiles and their
values). Next, the learned set of prediction profiles is used to translate the training data into
trajectories of experience with the prediction profile system. Finally, any applicable model
learning method can be trained on the transformed data to learn a model of the prediction
profile system. Ultimately, in our experiments, the learned prediction profile models will be
evaluated by how useful their predictions are as features for control.
4.1 Estimating the Prediction Profiles
Given S and T I , the first step of learning a prediction profile model is to determine how
many distinct prediction profiles there are, as well as their values. The estimated prediction
for a test of interest t at a history h is:
p(t | h) =

# times t succeeds from h
.
# times acts(t) taken from h
def

(6)

One could, at this point, directly estimate  by letting (h) = hp(t1 | h), p(t2 | h), . . . , p(tk |
h)i. Of course, due to sampling error, it is unlikely that any of these estimated profiles
will be exactly the same, even if the true underlying prediction profiles are identical. So,
368

fiLearning to Make Predictions Without a Generative Model

to estimate the number of distinct underlying profiles, statistical tests will be used to find
histories that have significantly different prediction profiles.
To compare the profiles of two histories, a likelihood-ratio test of homogeneity is performed on the counts for each test of interest in the two histories. If the statistical test
associated with any test of interest rejects the null hypothesis that the prediction is the
same in both histories, then the two histories have different prediction profiles.
In order to find the set of distinct prediction profiles, we greedily cluster the estimated
prediction profiles. Specifically, an initially empty set of exemplar histories is maintained.
The algorithm searches over all histories in the agents experience, comparing each historys
estimated profile to the exemplar histories estimated profiles. If the candidate historys
profile is significantly different from the profiles of all exemplar histories, the candidate is
added as a new exemplar. In the end, the estimated profiles corresponding to the exemplar
histories are used as the set of prediction profiles. In order to obtain the best estimates
possible, the search is ordered so as to prioritize histories with lots of associated data.
The prediction profile estimation procedure has two main sources of complexity. The
first is the sample complexity of estimating the prediction profiles. It can take a great
deal of exploration to see each history enough times to obtain good statistics, especially if
the number of actions and observations is large. This issue could be addressed by adding
generalization to the estimation procedure, so that data from one sample trajectory could
improve the estimates of many similar histories. In one of the experiments in Section 6,
observation abstraction will be employed as a simple form of generalization. The second
bottleneck is the computational complexity of searching for prediction profiles, as this involves exhaustively enumerating all histories in the agents experience. It would be valuable
to develop heuristics to identify the histories most likely to provide new profiles, in order
to avoid searching over all histories. In the experiments in Section 6, a simple heuristic
of limiting the search to short histories is employed. Long histories will tend to have less
associated data, and will therefore be less likely to provide distinguishably new profiles.
4.2 Generating Prediction Profile Trajectories
Having generated a finite set of distinct prediction profiles, the next step is to translate the
agents experience into sequences of action-observation pairs and prediction profiles. These
trajectories will be used to train a model of the prediction profile system.
The process of translating an action-observation sequence s into a prediction profile trajectory s is straightforward and, apart from a few practical concerns, follows directly from
Definition 8. Recall that, for an action-observation sequence s = a1 o1 a2 o2 . . . ak ok , the corresponding P P -action sequence is ha1 , o1 iha2 , o2 i . . . hak , ok i. The corresponding sequence of
profiles is (a1 o1 )(a1 o1 a2 o2 ) . . . (a1 o1 . . . ak ok ). Thus, in principle, every primitive actionobservation sequence can be translated into an action-observation-profile sequence.
Of course  is not available to generate the sequence of prediction profiles. So, it is
necessary to use an approximation , generated from the training data. Specifically, the
estimated predictions for the tests of interest at each history h (computed using Equation
6) are compared, using statistical tests, to the set of distinct estimated prediction profiles
from Section 4.1. If there is only one estimated profile  that is not statistically significantly
different from the estimated predictions at h, then let (h) = .
369

fiTalvitie & Singh

Given sufficient data, the statistical tests will uniquely identify the correct match with
high probability. In practice, however, some histories will not have very much associated
data. It is possible in such a case for the test of homogeneity to fail to reject the null
hypothesis for two or more profiles. This indicates that there is not enough data to distinguish between multiple possible matches. In the experiments in Section 6, two different
heuristic strategies for handling this situation are employed. The first strategy lets (h)
be the matching profile that has the smallest empirical KL-Divergence from the estimated
predictions (summed over all tests of interest). This is a heuristic choice that may lead to
noise in the prediction profile labeling, which could in turn affect the accuracy of the learned
model. The second strategy is to simply cut off any trajectory at the point where multiple
matches occur, rather than risk assigning an incorrect labeling. This ensures that labels
only appear in the prediction profile trajectories if there is a reasonable level of confidence
in their correctness. However, it is wasteful to throw out training data in this way.
4.3 Learning a Prediction Profile Model
The translation step produces a set S  of trajectories of interaction with the prediction
profile system. Recall that the prediction profile system is a deterministic, partially observable, discrete dynamical system and these trajectories can be used to train a model of the
prediction profile system using, in principle, any applicable model-learning method.
There is an issue faced by models of the prediction profile system that is not present in
the usual discrete dynamical systems modeling setting. While the prediction profile labels
are present in the training data, when actually using the model they are not available. Say
the current history is h, and an action a1 is taken and an observation o1 is emitted. Together,
this action-observation pair constitutes a P P -action. Being a model of the prediction profile
system, a prediction profile model can identify the next profile, . This profile can be used
to compute predictions p(t | ha1 o1 ) for the tests of interest at the history ha1 o1 . Now
another action a2 and observation o2 occur. It is now necessary to update the PP-models
state in order to obtain the next prediction profile. A typical dynamical systems model
makes predictions about the next observation, but is then able to update its state with the
actual observation that occurred. A prediction profile models observations are prediction
profiles themselves, which are not observable when interacting with the world. As such,
the prediction profile model will update its state with prediction profile it itself predicted
(). Once updated, the prediction profile model can obtain the profile that follows ha2 , o2 i
which gives the predictions for the tests of interest at the new history ha1 o1 a2 o2 .
If the prediction profile model is a perfect model of the prediction profile system, this
poses no problems. Because the prediction profile system is deterministic, there is no need
to observe the true prediction profile label; it is fully determined by the history. In practice,
of course, the model will be imperfect and different modeling representations will require
different considerations when performing the two functions of providing predictions for the
tests of interest, and providing a profile for the sake of updating the model.
4.3.1 PP-POMDPs
Since the prediction profile system is partially observable it is natural to model it using a POMDP. Unfortunately, even when the training data is from a deterministic sys370

fiLearning to Make Predictions Without a Generative Model

tem, POMDP training using the EM algorithm will generally not provide a deterministic
POMDP. Thus, at any given history, a learned POMDP model of the prediction profile
system (PP-POMDP) will provide a distribution over prediction profiles instead of deterministically providing the one profile associated with that history. The implementation
used in Section 6 simply takes the most likely profile from the distribution to be the profile
associated with the history and uses it to make predictions for the tests of interest, as well
as to update the POMDP model.
4.3.2 PP-LPSTs
Another natural choice of representation for a prediction profile model is a looping predictive
suffix tree (LPST) (Holmes & Isbell, 2006). LPSTs are specialized to deterministic, partially
observable systems. As such, they could not be used to model the original system (which
is assumed to be stochastic in general), but they do apply to the prediction profile system
(and they do not have to be determinized like a POMDP).
Briefly, an LPST captures the parts of recent history relevant to predicting the next
observation. Every node in the tree corresponds to an action-observation pair. A node
may be a leaf, may have children, or it may loop to one of its ancestors. Every leaf of the
tree corresponds to a history suffix that has a deterministic prediction of an observation
for every action. In order to predict the next observation from a particular history, one
reads the history in reverse order, following the corresponding links on the tree until a leaf
is reached, which gives the prediction. Holmes and Isbell provide a learning algorithm that,
under certain conditions on the training data, is guaranteed to produce an optimal tree.
The reader is referred to the work of Holmes and Isbell (2006) for details.
One weakness of LPSTs, however, is that they fail to make a prediction for the next
observation if the current history does not lead to a leaf node in the tree (or if the leaf
node reached does not have a prediction for the action being queried). This typically occurs
when some history suffixes do not occur in the training data but do occur while using the
model. For a PP-LPST, this can mean that in some histories the model cannot uniquely
determine the corresponding prediction profile. When this happens the implementation
used in Section 6 simply finds the longest suffix of the current history that does occur in the
data. This suffix will be associated with multiple prediction profiles (otherwise the LPST
would have provided a prediction). To make predictions for the tests of interest, the model
provides the average prediction over this set of profiles. The profile used to update the
model is picked out of the set uniformly randomly.
4.3.3 PP-PSRs
Applying PSR learning algorithms to prediction profile data poses a practical concern.
Specifically, methods that attempt to estimate the system dynamics matrix (James & Singh,
2004; Wolfe et al., 2005) implicitly presume that every action sequence could in principle
be taken from every history. If some action sequences can be taken from some histories but
not from others, then the matrix will have undefined entries. This poses challenges to rank
estimation (and, indeed, the very definition of the model representation). Unfortunately,
this can be the case for the prediction profile system since P P -actions (action-observation
pairs) are not completely under the agents control; they are partly selected by the environ371

fiTalvitie & Singh

ment itself. The recent spectral learning algorithms presented by Boots et al. (2010) may
be able to side-step this issue, as they have more flexibility in selecting which predictions
are estimated for use in the model-learning process, though we have not investigated this
possibility in this work.
Note that, though our method for learning a prediction profile model involves standard
model-learning methods for partially observable environments, the result is not a generative
model of the original system. A prediction profile model is a generative model of the
prediction profile system and, as such, cannot be used to make any predictions about the
original system, other than the predictions of interest.

5. Complexity of the Prediction Profile System
The learning algorithm we have presented will be evaluated empirically in Section 6. First,
however, we analyze the complexity of the prediction profile system in relation to the complexity of the original system. This will give some indication of how difficult it is to learn a
prediction profile model and provide insight into when it is appropriate to learn a prediction
profile model over a more typical generative model approach.
There are many factors that affect the complexity of learning a model. This section
will largely focus on linear dimension as the measure of complexity, taking the view that,
generally speaking, systems with lower linear dimension are easier to learn than systems with
larger linear dimension. As discussed in Section 1.2.2, this is generally true for POMDPs,
where the linear dimension lower-bounds the number of hidden states. So comparing the
linear dimension of the prediction profile system to that of the original system can give
some idea of whether it would be easier to learn a PP-POMDP or just to learn a standard
POMDP of the original system. Of course, there are other model-learning methods for
which other complexity measures would be more appropriate (for instance it is not known
precisely how LPSTs interact with linear dimension). Extending some of these results to
other measures of complexity may be an interesting topic of future investigation.
5.1 Linear Dimension Comparison
This section will discuss how the linear dimension of the prediction profile system relates
to that of the original system. The first result is a proof of concept that simply states
that there exist problems in which the prediction profile system is vastly more simple than
the original system. In fact, such a problem has already been presented.
Proposition 14. The prediction profile system can have linear dimension that is arbitrarily
lower than that of the original system.
Proof. Recall the Three Card Monte example. Thus far the domain has been described
without describing the dealers behavior. However, note that the prediction profile system
for the tests of interest relating to the location of the special card (pictured in Figure 1) has
a linear dimension of 3, regardless of how the dealers swaps are chosen. If a very complex
dealer is chosen, the original system will have high linear dimension, but the prediction
profile systems linear dimension will remain constant. For instance, in the experiments in
Section 6, the dealer chooses which cards to swap stochastically, but is more likely to choose
372

fiLearning to Make Predictions Without a Generative Model

the swap that has been selected the least often so far. Thus, in order to predict the dealers
next decision, one must count how many times each swap has been chosen in history and
as a result the system effectively has infinite linear dimension.
On the other hand, prediction profile models are not a panacea. The following results
indicate that there are problems for which learning a prediction profile model would not
be advisable over learning a standard generative model, in that the linear dimension of the
prediction profile system can be far greater than that of the original system. Later in the
section some special cases will be characterized where prediction profile models are likely to
be useful. The next result shows that the linear dimension of the prediction profile model
can be infinite when the original system has finite linear dimension, via a lower bound on
linear dimension that is true of all deterministic dynamical systems.
Proposition 15. For any deterministic dynamical system with actions A, and observations
O, the linear dimension, n  log(|A|1)+log(|O|+1)
.
log |A|
Proof. See Appendix A.1.
Because Proposition 15 applies to all deterministic dynamical systems, it certainly applies to the prediction profile system. Though it is a very loose bound, the basic implication
is that as the number of prediction profiles (the observations of P P ) increases in comparison to the number of action-observation pairs (the actions of P P ), the linear dimension of
the prediction profile system necessarily increases. This bound also clearly illustrates the
importance of the assumption that there is a finite number of distinct prediction profiles.
Corollary 16. If there are infinitely many distinct prediction profiles, the prediction profile
system has infinite linear dimension.
Proof. Clearly |AP P | = |A  O| is finite so long as there are finitely many actions and
observations. So, from the last result it follows immediately that as the number of distinct
prediction profiles |OP P | approaches infinity, then so must the linear dimension of the
prediction profile system.
Hence, so long as prediction profile models are represented using methods that rely
on a finite linear dimension, it is critical that there be finitely many prediction profiles.
Note that this is not a fundamental barrier, but a side effect of the representational choice.
Model learning methods that are not as sensitive to linear dimension (such as those designed
to model continuous dynamical systems) may be able to effectively capture systems with
infinitely many prediction profiles.
One conclusion to be drawn from the last few results is that knowing the linear dimension
of the original system does not, in itself, necessarily say much about the complexity of the
prediction profile system. The prediction profile system may be far simpler or far more
complex than the original system. Thus it may be more informative to turn to other factors
when trying to characterize the complexity of the prediction profile system.
373

fiTalvitie & Singh

5.2 Bounding the Complexity of The Prediction Profile System
The results in the previous section do not take into account an obviously important aspect
of the prediction profile system: the predictions it is asked to make. Some predictions
of interest can be made very simply by keeping track of very little information. Other
predictions will rely on a great deal of history information and will therefore require a more
complex model. The next result identifies the worst case set of tests of interest for any
system: the tests of interest whose corresponding prediction profile model has the highest
linear dimension. Ultimately this section will present some (non-exhaustive) conditions
under which the prediction profile system is likely to be simpler than the original system.
Proposition 17. For a given system and set of tests of interest, the linear dimension of
the corresponding prediction profile system is no greater than that of the prediction profile
system associated with any set of core tests for the system (as described in Section 2.2).
Proof. See Appendix A.2.
With this worst case identified, one can immediately obtain bounds on how complex
any prediction profile system can possibly be.
Corollary 18. For any system and any set of tests of interest, the corresponding prediction
profile system has linear dimension no greater than the number of distinct predictive states
for the original system.
Proof. The prediction profile system for a set of core tests Q is a deterministic MDP where
the observations are prediction profiles for Q (that is, predictive states). That is, each state
is associated with a unique prediction profile. The linear dimension of an MDP is never
greater than the number of observations (Singh et al., 2004). Therefore, by the previous
result the prediction profile system for any set of tests of interest can have linear dimension
no greater than the number of predictive states.
Corollary 19. If the original system is a POMDP, the prediction profile system for any set
of tests of interest has linear dimension no greater than the number of distinct belief states.
Proof. This follows immediately from the previous result and the fact that the number of
distinct predictive states is no greater than the number of distinct belief states (Littman
et al., 2002).
The bounds presented so far help explain why the prediction profile system can be more
complex than the original system. However, because they are focused on the worst possible
choice of tests of interest, they do little to illuminate when the opposite is true. A prediction
profile model is at its most complex when it is asked to perform the same task as a generative
model: keep track of as much information from history as is necessary to make all possible
predictions (or equivalently, the predictive state or the belief state). These results indicate
that, generally speaking, if one desires a generative model, standard approaches would be
preferable to learning a prediction profile model.
On the other hand, our stated goal is not to learn a generative model, but instead to
focus on some particular predictions that will hopefully be far simpler to make than all
predictions. The examples we have seen make it clear that in some cases, some predictions
374

fiLearning to Make Predictions Without a Generative Model

can be made by a prediction profile model far simpler than a generative model of the original
system. In general one might expect the prediction profile model to be simple when the
predictions of interest rely on only a small amount of the state information required to
maintain a generative model. The next bound aligns with this intuitive reasoning.
Essentially what this result points out is that often much of the hidden state information
in a POMDP will be irrelevant to the predictions of interest. The linear dimension of the
prediction profile system is bounded only by the number of distinct beliefs over the relevant
parts of the hidden state, rather than the number of distinct beliefs states overall. The idea
of the result is that if one can impose an abstraction over the hidden states of a POMDP
(not the observations) that still allows the predictions of interest to be made accurately
and that allows abstract belief states to be computed accurately, then the prediction profile
systems linear dimension is bounded by the number of abstract belief states.
Proposition 20. Consider a POMDP with hidden states S, actions A, and observations
O. Let T I be the set of tests of interest. Let ai be the action taken at time-step i, si be the
hidden state reached after taking action ai , and oi be the observation emitted by si . Now,
consider any surjection  : S  S  mapping hidden states to a set of abstract states with
the following properties:
1. For any pair of primitive states s1 , s2  S, if (s1 ) = (s2 ), then for any time-step i
and any test of interest t  T I , p(t | si = s1 ) = p(t | si = s2 ).
2. For any pair of primitive states s1 , s2  S, if (s1 ) = (s2 ), then for any time-step i,
abstract state S  S  , observation o  O, and action a  A,
Pr((si+1 ) = S | si = s1 ,ai+1 = a, oi+1 = o) =
Pr((si+1 ) = S | si = s2 , ai+1 = a, oi+1 = o).
For any such , the prediction profile system for T I has linear dimension no greater than
the number of distinct beliefs over abstract states, S  .
Proof. See Appendix A.3
There are a few things to note about this result. First, a surjection  always exists that
def
has properties 1 and 2. One can always define  : S  S with (s) = s. This degenerate
case trivially satisfies the requirements of Proposition 20 and recovers the bound given in
Corollary 19. However, Proposition 20 applies to all surjections that satisfy the conditions.
There must be a surjection that satisfies the conditions and results in the smallest number of
beliefs over abstract states. Essentially, this is the one that ignores as much state information
as possible while still allowing the predictions of interest to be made accurately and it is this
surjection that most tightly bounds the complexity of the prediction profile system (even if
 is not known).
Of course, there may still be a large or even infinite number of distinct beliefs, even over
abstract states, so other factors must come into play to ensure a simple prediction profile
system. Furthermore, this result does not characterize all settings in which the prediction
profile system will be simple. That said, this result does support the intuition that the
375

fiTalvitie & Singh

prediction profile system will tend to be simple when the predictions it is asked to make
depend on small amounts of state information.
In order to build intuition about how this result relates to earlier examples, recall the
Three Card Monte problem. In Three Card Monte there are two sources of hidden state:
the aces unobserved position and whatever hidden mechanism the dealer uses to make its
decisions. Clearly the agents predictions of interest depend only on the first part of the
hidden state. So, in this case one can satisfy Property 1 with a surjection  that maps
two hidden states to the same abstract state if the ace is in the same position, regardless
of the dealers state. Under this  there are only 3 abstract states (one for each possible
position), even though there might be infinitely many true hidden states. Now, different
states corresponding to the same ace position will have different distributions over the aces
next position; this distribution does, after all, depend upon the dealers state. However,
Property 2 is a statement about the distribution over the next abstract state given the
observation that is emitted after entering the abstract state. If one knows the current
abstract state and observes what the dealer does, the next abstract state is fully determined.
So Property 2 holds as well. In fact, since the aces position is known at the beginning of the
game, this means the current abstract state is always known with absolute certainty, even
though beliefs about the dealers state will in general be uncertain. Hence, there are only 3
distinct beliefs about the abstract states (one for each state). As such, the prediction profile
models linear dimension is upper-bounded by 3, regardless of the dealers complexity (and
in this case the bound is met).
5.3 Bounding the Number of Prediction Profiles
The previous section describes some conditions under which the prediction profile system
may have a lower linear dimension than the original system. Also of concern is the number
of prediction profiles, and whether that number is finite. This section will briefly discuss
some (non-exhaustive) cases in which the number of prediction profiles is bounded.
One case that has already been discussed is when the original system is Markov. In that
case the number of prediction profiles is bounded by the number of observations (states).
Of course, when the original system is Markov, there is little need to use prediction profile
models. Another, similar case is when the system is partially observable, but completely
deterministic (that is, the next observation is completely determined by history and the
selected action). If the system is a deterministic POMDP then at any given history the
current hidden state is known. As such, the number of belief states is bounded by the
number of hidden states. Since there cannot be more prediction profiles than belief states,
the number of prediction profiles are bounded as well.
One can move away from determinism in a few different ways. First, note that the key
property of a deterministic POMDP is that the hidden state is fully determined by history.
It is possible to satisfy this property even in stochastic systems, as long as one can uniquely
determine the hidden state, given the observation that was emitted when arriving there. In
that case, observations can be emitted stochastically, but the number of belief states (and
the number of prediction profiles) is still bounded by the number of hidden states.
Another step away from determinism is a class of systems, introduced by Littman (1996),
called Det-POMDPs. A Det-POMDP is a POMDP where the transition function and
376

fiLearning to Make Predictions Without a Generative Model

the observation function are both deterministic, but the initial state distribution may be
stochastic. A Det-POMDP is not a deterministic dynamical system, as there is uncertainty
about the hidden state. Because of this uncertainty, the system appears to emit observations
stochastically. It is only the underlying dynamics that are deterministic. Littman showed
that a Det-POMDP with n hidden states and an initial state distribution with m states in
its support has at most (n + 1)m  1 distinct belief states. So, this bounds the number of
prediction profiles as well.
Finally, and most importantly, if the hidden state can be abstracted as in Proposition 20,
then these properties only really need to hold for abstract beliefs. That is, the environment
itself may be complex and stochastic in arbitrary ways, but if the abstract hidden state
described in Proposition 20 is fully determined by history, then the number of prediction
profiles is bounded by the number of abstract states (as was the case in Three Card Monte).
Similarly, Det-POMDP-like properties can be imagined for abstract hidden states as well.
These cases by no means cover all situations where the number of prediction profiles
can be bounded, but they do seem to indicate that the class of problems where the number
of prediction profiles is finite is quite broad, and may contain many interesting examples.

6. Experiments
This section will empirically evaluate the prediction profile model learning procedure developed in Section 4. In each experiment an agent faces an environment for which a generative
model would be a challenge to learn due to its high linear dimension. However, in each
problem the agent could make good decisions if it could only have the predictions to a small
number of important tests. A prediction profile model is learned for these important tests
and the accuracy of the learned predictions is evaluated.
These experiments also demonstrate one possible use of prediction profile models (and
partial models in general) for control. Because they are not generative, prediction profile
models cannot typically be used directly by offline, model-based planning methods. However, their output may be useful for model-free methods of control. Specifically, in these
experiments, the predictions made by the learned prediction profile models are provided as
features to a policy gradient algorithm.
6.1 Predictive Features for Policy Gradient
Policy gradient methods (e.g., Williams, 1992; Baxter & Bartlett, 2000; Peters & Schaal,
2008) have been very successful as viable options for model-free control in partially observable domains. Though there are differences between various algorithms, the common
thread is that they assume a parametric form for the agents policy and then attempt to
alter those parameters in the direction of the gradient with respect to expected average reward. These experiments will make use of Online GPOMDP with Average Reward Baseline
(Weaver & Tao, 2001), or OLGARB (readers are referred to the original paper for details).
OLGARB assumes there is some set of features of history, and that the agents policy takes
the parametric form:
Pr(a | h; w)
~ =P

e

P

a

377

wi,a fi (h)
P
i wi,a fi (h)

i

e

fiTalvitie & Singh

where fi (h) is the ith feature and each parameter wi,a is a weight specific to the feature
and the action being considered.
Typically the features used in policy gradient are features that can be directly read
from history (e.g., features of the most recent few observations or the presence/absence of
some event in history). It can be difficult to know a priori which historical features will be
important for making good control decisions. In contrast, the idea in these experiments is
to provide the values of some predictions as features. These predictive features have direct
consequences for control, as they provide information about the effects of possible behaviors
the agent might engage in. As such, it may be easier to select a set of predictive features
that are likely to be informative about the optimal action to take (e.g., Will the agent
reach the goal state when it takes this action? or Will taking this action damage the
agent?). Furthermore, information may be expressed compactly in terms of a prediction
that would be complex to specify purely in terms of past observations. As seen in the
discussion of PSRs in Section 2.2, an arbitrary-length history can be fully captured by
a finite set of short-term predictions. For these reasons it seems reasonable to speculate
that predictive features, as maintained by a prediction profile model, may be particularly
valuable to model-free control methods like policy gradient.
6.2 Experimental Setup
The learning algorithm will be applied to two example problems. In each problem prediction
profile models are learned with various amounts of training data (using both LPSTs and
POMDPs as the representation and using both strategies for dealing with multiple matches,
as described in Section 4.3). The prediction accuracy of the models is evaluated, as well as
how useful their predictions are as features for control. The training data is generated by
executing a uniform random policy in the environment.
The free parameter of the learning algorithm is the significance value of the statistical
tests, . Given the large number of contingency tests that will be performed on the same
data set, which can compound the probability of a false negative,  should be set fairly
low. In these experiments we use  = 0.00001, though several reasonable values were tried
with similar results. As discussed in Section 4, there will also be a maximum length of
histories to consider during the search for prediction profiles. This cutoff allows the search
to avoid considering long histories, as there are many long histories to search over and they
are unlikely to provide new prediction profiles.
After a prediction profile model is learned, its predictions are evaluated as features for
the policy gradient algorithm OLGARB. Specifically, for each test of interest t the unit
interval is split up into 10 equally-sized bins b and a binary feature ft,b is provided that is
1 if the prediction of t lies in bin b, and 0 otherwise. Also provided are binary features fo ,
for each possible observation o. The feature fo = 1 if o is the most recent observation and
0, otherwise. The parameters of OLGARB, the learning rate and discount factor, are set
to 0.01 and 0.95, respectively in all experiments.
To evaluate a prediction profile model OLGARB is run for 1,000,000 steps. The average
reward obtained and the root mean squared error (RMSE) of the predictions for the tests
of interest accrued by the model along the way are reported. Prediction performance is
compared to that obtained by learning a POMDP on the training data and using it to
378

fiLearning to Make Predictions Without a Generative Model

Prediction Performance

Control Performance
0.1

Avg. Reward (20 trials)

Avg. RMSE (20 Trials)

1
0.8
0.6
Flat POMDP

0.4
PPLPST(KLD)
PPLPST(cut)

0.2
0
0

PPPOMDP(KLD)
PPPOMDP(cut)

2

4

# Training Trajectories

True

0.06
0.04

PPPOMDP(KLD)
PPPOMDP(cut)

0.02

PPLPST(KLD)
PPLPST(cut)

0
Flat POMDP

0.02
0.04
0.06
0

6

Expert

0.08

SOM

2

4

6

# Training Trajectories x 104

4

x 10

Figure 6: Results in the Three Card Monte domain.
make the predictions of interest. Because these problems are too complex to feasibly train
a POMDP with the correct number of underlying states, 30-state POMDPs were used
(stopping EM after a maximum of 50 iterations)2 . Control performance is compared to
that obtained by OLGARB using the predictions provided by a learned POMDP model as
features, as well as OLGARB using the true predictions as features (the best the prediction
profile model could hope to do), OLGARB using second-order Markov features (the two
most recent observations, as well as the action between them) but no predictive features at
all, and a hand-coded expert policy.
6.3 Three Card Monte
The first domain is the Three Card Monte example. The agent is presented with three
cards. Initially, the card in the middle (card 2) is the ace. The agent has four actions
available to it: watch, f lip1, f lip2, and f lip3. If the agent chooses a flip action, it observes
whether the card it flipped over is the special card. If the agent chooses the watch action,
the dealer can swap the positions of two cards, in which case the agent observes which two
cards were swapped, or the dealer can ask for a guess. If the dealer has not asked for a
guess, then watch results in 0 reward and any flip action results in -1 reward. If the dealer
asks for a guess and the agent flips over the special card, the agent gets reward of 1. If the
agent flips over one of the other two cards, or doesnt flip a card (by selecting watch), it
gets reward of -1. The agent has three tests of interest, and they take the form f lipX ace,
for each card X (that is, If I flip card X, will I see the ace?).
As discussed previously, the complexity of this system is directly related to the complexity of the dealers decision-making process. In this experiment, when the agent chooses
watch the dealer swaps the pair of cards it has swapped the least so far with probability
0.5; with probability 0.4 it chooses uniformly amongst the other pairs of cards; otherwise
it asks for a guess. Since the dealer is keeping a count of how many times each swap was
made, the process governing its dynamics effectively has an infinite linear dimension. The
2. Similar results were obtained with 5, 10, 15, 20, and 25 states.

379

fiTalvitie & Singh

prediction profile system, on the other hand, has only 3 states, regardless of the dealers
complexity (see Figure 1).
Training trajectories were of length 10. Figure 6 shows the results for various amounts of
training data, averaged over 20 trials. Both PP-POMDPs and PP-LPSTs learned to make
accurate predictions for the tests of interest, eventually achieving zero prediction error. In
this case, PP-POMDPs did so using less data. This is likely because a POMDP model is
more readily able to take advantage of the fact that the prediction profile system for Three
Card Monte is Markov. As expected, the standard POMDP model was unable to accurately
predict the tests of interest.
Also compared are the two different strategies for dealing with multiple matches discussed in Section 4.3. Recall that the first one (marked KLD in the graph) picks the
matching profile with the smallest empirical KL-Divergence from the estimated predictions.
The second (marked cut in the graph) simply cuts off the trajectory at the point of
a multiple match to avoid any incorrect labels. In this problem these two strategies result in almost exactly the same performance. This is likely because the profiles in Three
Card Monte are deterministic, and are therefore quite easy to distinguish (making multiple
matches unlikely). The next experiment will have stochastic profiles.
The predictive features provided by the prediction profile models are clearly useful for
control, as the control performance of OLGARB using their predictions approaches, and
eventually exactly matches that of OLGARB using the true predictions (marked True).
The inaccurate predictions provided by the POMDP were not very useful for control; OLGARB using the POMDP provided predictions does not even break even, meaning it loses
the game more often than it wins. The POMDP features did, however, seem to contain
some useful information beyond that provided by the second-order Markov features (marked
SOM) which, as one might expect, performed very poorly.
6.4 Shooting Gallery
The second example is called the Shooting Gallery, pictured in Figure 7(a). The agent
has a gun aimed at a fixed position on an 88 grid (marked by the X) . A target moves
diagonally, bouncing off of the boundaries of the image and 22 obstacles (an example
trajectory is pictured). The agents task is to shoot the target. The agent has two actions:
watch and shoot. When the agent chooses watch, it gets 0 reward. If the agent chooses
shoot and the target is in the crosshairs in the step after the agent shoots, the agent gets
reward of 10, otherwise it gets a reward of -5. Whenever the agent hits the target, the
shooting range resets: the agent receives a special reset observation, each 2  2 square on
the range is made an obstacle with probability 0.1, and the target is placed in a random
position. There is also a 0.01 probability that the range will reset at every time step. The
difficulty is that the target is sticky. Every time step with probability 0.7 it moves in its
current direction, but with probability 0.3 it sticks in place. Thus, looking only at recent
history, the agent may not be able to determine the targets current direction. The agent
needs to know the probability that the target will be in its sights in the next step, so clearly
the single test of interest is: watch target (that is If I choose the watch action, will the
target enter the crosshairs?). When the target is far from the crosshairs, the prediction of
this test will be 0. When it target is in the crosshairs, it will be 0.3. When the target is
380

fiLearning to Make Predictions Without a Generative Model

(a)

(b)

Figure 7: The Shooting Gallery domain. (a) A possible arrangement of obstacles and trajectory for the target (lighter is further back in time). In this case the target will
definitely not enter the agents crosshairs, since it will bounce off of the obstacle.
(b) The abstraction applied to the most recent observation.

near the crosshairs, the model must determine whether the prediction is 0.7 or 0, based on
the targets previous behavior (its direction) and the configuration of nearby obstacles.
This problem has stochastic prediction profiles, so it is expected that more data will
be required to differentiate them. Also, due to the number of possible configurations of
obstacles and positions of the target, this system has roughly 4,000,000 observations and
even more latent states. This results in a large number of possible histories, each with only
a small probability of occurring. As discussed in Section 4, this can lead to a large sample
complexity for obtaining good estimates of prediction profiles. Here this is addressed with
a simple form of generalization: observation abstraction. Two observations are treated as
the same if the target is in the same position and if the configuration of obstacles in the
immediate vicinity of the target is the same. In other words, each abstract observation
contains information only about the targets position and the obstacles surrounding the
target, and not the placement of obstacles far away from the target (see Figure 7(b)) for an
example. Under this abstraction, the abstract observations still provide enough detail to
make accurate predictions. That is, two histories do indeed have the same prediction profile
if they have the same action sequence and their observation sequences correspond to the
same sequence of aggregate observations. This enables one sample trajectory to improve
the estimates for several histories, though, even with this abstraction, there are still over
2000 action-observation pairs. The same observation abstraction was applied when training
the POMDP model.
Training trajectories were length 4 and the search for profiles was restricted to length
3 histories. Results are shown in Figure 8. Perhaps the most eye-catching feature of
the results is the upward trending curve in the prediction error graph, corresponding to
the PP-POMDP with the KL-Divergence based matching (labeled PP-POMDP(KLD)).
Recall that the danger of the KL-divergence based matching strategy is that it may produce
incorrect labels in the training data. Apparently these errors were severe enough in this
problem to drastically mislead the POMDP model. With a small amount of data it obtained
381

fiTalvitie & Singh

Prediction Performance

Control Performance
0.025

Avg. Reward (20 Trials)

Avg. RMSE (20 Trials)

0.25
0.2
PPPOMDP(cut)

0.15
Flat POMDP

0.1

PPPOMDP(KLD)

0.05

PPLPST(cut)
PPLPST(KLD)

0
0

2

4

6

8

# Training Trajectories

0.02
0.015
0.01

PPLPST(KLD)
PPPOMDP(KLD)

PPLPST(cut)

0.005
PPPOMDP(cut)

0
0.005
0

10
5
x 10

Expert
True

SOM
Flat POMDP

2

4

6

8

10

# Training Trajectories x 105

Figure 8: Results in the Shooting Gallery domain.
very good prediction error, but with more data came more misleading labelings, and the
performance suffered. The PP-POMDP trained with the other matching method (PPPOMDP(cut)) displays a more typical learning curve (more data results in better error),
though it takes a great deal of data before it begins to make reasonable predictions. This
is because cutting off trajectories that have multiple matches throws away data that might
have been informative to the model. The PP-LPSTs generally outperform the PP-POMDPs
in this problem. With the trajectory cutting method, the PP-LPST (PP-LPST(cut))
quickly outperforms the flat POMDP and, with enough data, outperforms both versions of
PP-POMDP. The PP-LPST with the KL-divergence based matching (PP-LPST(KLD))
is by far the best performer, quickly achieving small prediction error. Clearly the incorrect
labels in the training data did not have as dramatic an effect on the LPST learning, possibly
because, as a suffix tree, an LPST mostly makes its predictions based on recent history,
limiting the effects of labeling errors to a few time-steps.
Control performance essentially mirrors prediction performance, with some interesting
exceptions. Note that even though PP-POMDP(KLD) obtains roughly the same prediction
error as the flat POMDP at 1,000,000 training trajectories, the predictive features it provides
still result in substantially better control performance. This indicates that, even though the
PP-POMDP is making errors in the exact values of the predictions, it has still captured more
of the important dynamics of the predictions than the flat POMDP has. The flat POMDP
itself provides features that are roughly as useful as second-order Markov features, which
do not result in good performance. Again, OLGARB using these features does not break
even, meaning it is wasting bullets when the target is not likely to enter the crosshairs. The
best-performing prediction profile model, PP-LPST(KLD) approaches the performance of
OLGARB using the true predictions with sufficient data.

7. Related Work
The idea of modeling only some aspects of the observations of a dynamical system has
certainly been raised before. For instance, in a recent example Rudary (2008) learned linear382

fiLearning to Make Predictions Without a Generative Model

Gaussian models of continuous partially observable environments where some dimensions of
the observation were treated as unmodeled exogenous input. These inputs were assumed
to have a linear effect on state transition. Along somewhat similar lines, but in the context of
model minimization (taking a given, complete model and deriving a simpler, abstract model
that preserves the value function) Wolfe (2010) constructed both an abstract model and a
shadow model that predicts observation details that are ignored by the abstraction. The
shadow model takes the abstract observations of the abstract model as unmodeled input.
Splitting the observation into modeled and un-modeled components and then learning a
generative model is certainly related to our approach. In that case, a model would make all
conditional predictions about the modeled portion of the observation, given the exogenous
inputs (as well as actual actions and the history). Prediction profile models take this to an
extreme, by treating the entire observation as input. Instead of predicting future sequences
of some piece of the next observation conditioned on another piece, prediction profile models
predict the values of an arbitrary set of predictions of interest at the next time step, given
the entire action and observation. This allows significantly more freedom in choosing which
predictions the model will make (and, more importantly, will not make).
One modeling method closely related to prediction profiles is Causal State Splitting
Reconstruction (CSSR) (Shalizi & Klinker, 2004). CSSR is an algorithm for learning generative models of discrete, partially observable, uncontrolled dynamical systems. The basic
idea is to define an equivalence relation over histories where two histories are considered
equivalent if they are associated with identical distributions over possible futures. The
equivalence classes under this relation are called causal states. The CSSR algorithm learns
the number of causal states, the distribution over next observations associated with each
causal state, and the transitions from one causal state to the next, given an observation.
It is straightforward to see that there is a one-to-one correspondance between causal states
and the predictive states of a PSR. As such, a causal state model is precisely the prediction
profile model where the set of tests of interest is Q, some set of core tests. With this correspondance in hand, the results in Section 5.2 show that in many cases the number of causal
states will greatly exceed the linear dimension of the original system and that therefore
CSSR may be inadvisable in many problems, in comparison to more standard modeling
approaches. It is possible that the CSSR algorithm could be adapted to the more general
setting of arbitrary sets of tests of interest, however the algorithm does rely heavily on the
fact that a prediction profile model with Q as the tests of interest is Markov, which is not
generally the case for other sets of tests of interest.
As mentioned in Section 2, McCallum (1995) presented UTree, a suffix-tree-based algorithm for learning value functions in partially observable environments. Because UTree
learns only the value function (a prediction about future rewards), and does not make any
predictions about observations, UTree does learn a non-generative partial model. Wolfe
and Barto (2006) extend UTree to make one-step predictions about particular observation
features rather than limiting predictions to the value function. Because it learns a suffix
tree, UTree is able to operate on non-episodic domains (whereas our method requires seeing
histories multiple times) and is not required to explicitly search for distinct prediction profiles. UTree also directly incorporates abstraction learning, learning simultaneously which
observation features are important, and where in the history suffix to attend to them. That
said, the main drawback of the suffix tree approach is that the tree only takes into account
383

fiTalvitie & Singh

information from relatively recent history (a suffix of the history). It cannot remember
important information for an arbitrary number of steps as a recurrent state-based model
can. In the Three Card Monte example, for instance, having access to a depth-limited suffix
of history would be of little help. In order to track the ace, one must take into account
every move the dealer has made since the beginning of the game. UTree would essentially
forget where the card was if the games length surpassed the depth of its memory.
McCallum (1993) and Mahmud (2010) both provide methods for learning state machines
that predict the immediate reward resulting from any given action-observation pair in partially observable control tasks (and thus do not suffer from the issue of finite-depth memory
that suffix trees do). Thus, their learning problem is a special case of ours, where they
restrict their models to make one-step predictions about the immediate reward. In both
cases, a simple model is incrementally and greedily elaborated by proposing states to be split
and evaluating the results (via statistical tests in the case of McCallum and via likelihood
hill-climbing in the case of Mahmud). McCallum expressed concern that his approach had
difficulty extracting long-range dependencies (for instance, learning to attend to an event
that does not appear to affect the distribution of rewards until many steps later); it is not
clear the extent to which Mahmuds approach addresses this issue. These methods have
some of the advantages of UTree, most notably that they can be applied to non-episodic
domains. That said, our approach has advantages as well. By re-casting the problem of
learning a non-generative model as a standard generative model-learning problem, we have
been able to gain deeper understanding of the complexity and applicability of prediction
profile models compared to more standard generative models. Furthermore, this has allowed us to incorporate standard, well-studied generative model-learning methods into our
learning algorithm, thereby leveraging their strengths in the non-generative setting. Most
specifically, this has resulting in a principled (albeit heuristic) learning algorithm, that does
not rely on guess-and-check or stochastic local search.
The prediction profile system is also similar in spirit to finite state controllers for
POMDPs. Sondik (1978) noted that in some cases, it is possible to represent the optimal policy for a POMDP as a finite state machine. These finite state controllers are very
much like prediction profile models in that they take action-observation pairs as inputs,
but instead of outputting predictions associated with the current history, they output the
optimal action to take. Multiple authors (e.g., Hansen, 1998; Poupart & Boutilier, 2003)
provide techniques for learning finite state controllers. However, these algorithms typically
require access to a complete POMDP model of the world to begin with which, in our setting,
is assumed to be impractical.

8. Conclusions and Future Directions
The most standard methods for learning models in partially observable environments learn
generative models. If one has only a small set of predictions of interest to make (and
therefore does not require the full power of a generative model), one can ignore irrelevant
detail via abstraction to simplify the learning problem. Even so, a generative model will
necessarily make predictions about any relevant details, even if they are not directly of
interest. We have seen by example that the resulting model can be counter-intuitively
complex, even if the predictions the model is being asked to make are quite simple.
384

fiLearning to Make Predictions Without a Generative Model

We presented prediction profile models, which are non-generative models for partially
observable systems that make only the predictions of interest and no others. The main idea
of prediction profile models is to learn a model of the dynamics of the predictions themselves
as they change over time, rather than a model of the dynamics of the system. The learning
method for prediction profile models learns a transformation of the training data and then
applies standard methods to the transformed data (assuming that the predictions of interest
take on only a finite number of distinct values). As a result, it retains advantages of methods
like EM for POMDPs that learn what information from history must be maintained in order
to make predictions (rather than requiring a set of history features a priori). We showed
that a prediction profile model can be far simpler than a generative model, though it can
also be far more complex, depending on what predictions it is asked to make. However, if
the predictions of interest depend on relatively little state information, prediction profile
models can provide substantial savings over standard modeling methods such as POMDPs.
While the experiments in Section 6 demonstrate that it is possible to learn prediction
profile models in contrived systems too complex for POMDPs, the specific learning algorithm presented here is not likely to scale to more natural domains without modification.
The most critical scaling issues for prediction profile models are the sample complexity of
estimating the prediction profiles, and the computational complexity of searching for prediction profiles and translating the data. In both cases, the critical source of complexity is
essentially how many distinct histories there are in the training data (more distinct histories
means the data is spread thin amongst them and there are more estimated profiles to search
through). As such, generalization of prediction estimates across many histories would be
a key step toward applying these ideas to more realistic domains. We are currently developing learning algorithms that combine the ideas behind prediction profile models with
methods for learning abstractions that allow many essentially equivalent histories to be
lumped together for the purposes of estimating the predictions of interest.
Another limitation of the prediction profile model learning method presented here is its
reliance on the assumption of a finite number of prediction profiles. While this assumption
does hold in many cases, an ideal method would be able to deal gracefully with a very large
or infinite number of prediction profiles. One possibility is to simply cluster the predictions
in other ways. For instance, one may only desire a certain level of prediction accuracy and
may therefore be willing to lump some distinct prediction profiles together in exchange for a
simpler prediction profile system. Another idea would be to learn a prediction profile model
using continuous-valued representations such as Kalman filters (Kalman, 1960) or PLGs
(Rudary, Singh, & Wingate, 2005) (or their nonlinear variants, e.g., Julier & Uhlmann,
1997; Wingate, 2008). These representations and learning algorithms explicitly deal with
systems with an infinite number of observations (prediction profiles in this case). Even
when there are finitely many prediction profiles, methods for learning non-linear continuous
models may still be able to (approximately) capture the discrete dynamics.
Additionally, though our results have focused on discrete systems, the main motivation
behind prediction profile models also has purchase in the continuous setting. Typical methods for learning models of partially observable systems in continuous systems, much like
their discrete valued counterparts, learn generative models. As such, the non-generative
approach of prediction profile models may provide similar benefits in the continuous setting
if not all predictions need be made. In this setting, prediction profiles might be represented
385

fiTalvitie & Singh

in a parametric form (for instance, the mean and variance of a Gaussian). The main idea of
prediction profile models (though not the specific method presented here) could still then
be applied: learn a model of the dynamics of these distribution parameters, rather than the
dynamics of the system itself.
Finally, we have not discussed in this work how the tests of interest should be determined, only how to predict them once they are selected. Automatically selecting interesting/important predictive features as targets for partial models would certainly be an
interesting research challenge. Of course, this would depend on what the predictions will
be used for. If the predictions will be used as features for control, as we have done in
our experiments, then it would certainly seem intuitive to start with predictive features
regarding the reward signal, and perhaps observation features that strongly correlate with
reward (as we have intuitively done by hand in our experiments). It may also be useful to
consider making predictions about those predictions in the style of TD Networks (Sutton
& Tanner, 2005). For instance, one could imagine learning models that make predictions
about which profile another model will emit. In this way models could be chained together
to make predictions about more extant rewards, rather than focusing solely on predicting
the immediate reward signal (which is not always a particularly good feature for temporal
decision problems). Another common use of partial models is to decompose a large modeling
problem into many small ones, as in, for instance, factored MDPs (Boutilier et al., 1999),
factored PSRs (Wolfe et al., 2008), or collections of local models (Talvitie & Singh, 2009b).
In this setting, choosing tests of interest would be an example of the structure learning
problem: decomposing one-step predictions into relatively independent components and
then assigning them to different models.

Acknowledgments
Erik Talvitie was supported under the NSF GRFP. Satinder Singh was supported by NSF
grant IIS-0905146. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of the NSF.
The work presented in this paper is an extension of work presented at IJCAI (Talvitie &
Singh, 2009a). We are grateful to the anonymous reviewers whose helpful comments have
improved the presentation of this work.

Appendix A.
A.1 Proof of Proposition 15
This result will follow straightforwardly from a general fact about dynamical systems. Let
h[i...j] be the sequence of actions and observations from h starting with the ith time-step in
the sequence and ending with the jth time-step in the sequence. For conveniences sake, if
i > j let h[i...j] = h0 , the null sequence. The following two results will show that if some
test t ever has positive probability, then it must have positive probability at some history
with length less than the linear dimension of the system.

386

fiLearning to Make Predictions Without a Generative Model

Figure 9: The matrix constructed in Lemma 21 is full rank (a contradiction).
Lemma 21. If the linear dimension of a dynamical system is n, then for any test t and
history h with length(h) = k  n and p(t | h) > 0, i, j with 0  i < j  1  k such that
p(t | h[1...i] h[j...k] ) > 0.
Proof. Note that because p(t | h) > 0, p(h[(i+1)...k] t | h[1...i] ) = p(t | h)p(h[(i+1)...k] | h[1...i] ) >
0 for all 0  i  k. Now assume for all i, j with 0  i < j  1  k that p(h[j...k] t | h[1...i] ) =
p(t | h[1...i] h[j...k] )p(h[j...k] | h[1...i] ) = 0 and seek a contradiction. Consider a submatrix of
the system dynamics matrix. The rows of this submatrix correspond to prefixes of h: h[1...i]
for all 0  i  k. The columns correspond to suffixes of h pre-pended to the test t: h[j...k] t
for all 1  j  k + 1. This is a k + 1  k + 1 matrix. Under the above assumption, this
matrix is triangular with positive entries along the diagonal (Figure 9 shows this matrix
when k = 4). As such, this matrix is full rank (rank k + 1). This is a contradiction since
k  n and a submatrix can never have higher rank than the matrix that contains it.
The next result follows immediately from Lemma 21.
Corollary 22. If the system has linear dimension n and for some test t and history h
p(t | h) > 0, then there exists a (possibly non-consecutive) subsequence h of h such that
length(h ) < n with p(t | h ) > 0.
Proof. By Lemma 21, every history h with length k  n such that p(t | h) > 0 must have
a subsequence h1 with length k1 < k such that p(t | h) > 0. If k1  n, then h1 must have a
subsequence h2 with length k2 < k1 . This argument can be repeated until the subsequence
has length less than n.
The consequence of Corollary 22 is that every test that ever has positive probability,
must have positive probability following some history of length less than n. With this fact
in hand, Proposition 15 can now be proven.
Proposition 15. For any deterministic dynamical system with actions A, and observa.
tions O, the linear dimension, n  log(|A|1)+log(|O|+1)
log |A|
387

fiTalvitie & Singh

Proof. Since the system is deterministic, each history and action correspond to exactly one
resulting observation. A history is a sequence of actions and observations. However, since
the sequence of observations is fully determined by the sequence of actions in a deterministic
system, the number of distinct histories of length k is simply |A|k . At each history there
are |A| action choices that could each result in a different observation. So, the number
of observations that could possibly occur after histories of length k is simply |A|k+1 . By
Corollary 22, if the linear dimension is n, all observations must occur after some history h
with length(h)  n  1. Thus, the number of observations that can possibly follow histories
of length less than n is:
|O| 

n1
X

|A|i+1 =

i=0

|A|n+1  1
 1.
|A|  1

Solving for n yields the bound on linear dimension in terms of the number of actions and
the number of observations.
A.2 Proof of Proposition 17
Proposition 17. For a given system and set of tests of interest, the linear dimension of
the corresponding prediction profile system is no greater than that of the prediction profile
system associated with any set of core tests for the system (as described in Section 2.2).
Proof. Recall from the discussion of PSRs in Section 2.2 that a set of core tests, Q, is
a set of tests whose corresponding columns in the system dynamics matrix constitute a
basis. The predictions for the core tests at a given history form the predictive state at that
history. So, the predictive state is precisely the prediction profile for the core tests Q. The
prediction for any other test can be computed as a linear function of the prediction profile
for Q. Note that the prediction profile system for Q is itself an MDP. It was shown in
Section 2.2 how to compute the next predictive state given the current predictive state and
an action-observation pair.
Now consider some other set of tests of interest T I . Because the predictions for Q
can be used to compute the prediction for any other test, it must be that there is some
function  that maps the prediction profiles for Q to the prediction profiles for T I . In
general, multiple predictive states may map to the same prediction profile for T I so  is a
surjection. Now it is easy to see that the prediction profile system for T I is the result of
applying the observation abstraction  to the prediction profile system for Q. Performing
observation abstraction on an MDP generally produces a POMDP, but never increases the
linear dimension (Talvitie, 2010). Hence, the prediction profile system for any set of tests
of interest T I has linear dimension no greater than that of the prediction profile system for
any set of core tests, Q.
A.3 Proof of Proposition 20
Proposition 20. Consider a POMDP with hidden states S, actions A, and observations
O. Let T I be the set of tests of interest. Let ai be the action taken at time-step i, si be the
hidden state reached after taking action ai , and oi be the observation emitted by si . Now,
388

fiLearning to Make Predictions Without a Generative Model

consider any surjection  : S  S  mapping hidden states to a set of abstract states with
the following properties:
1. For any pair of primitive states s1 , s2  S, if (s1 ) = (s2 ), then for any time-step i
and any test of interest t  T I , p(t | si = s1 ) = p(t | si = s2 ).
2. For any pair of primitive states s1 , s2  S, if (s1 ) = (s2 ), then for any time-step i,
abstract state S  S  , observation o  O, and action a  A,
Pr((si+1 ) = S | si = s1 ,ai+1 = a, oi+1 = o) =
Pr((si+1 ) = S | si = s2 , ai+1 = a, oi+1 = o).
If such a  exists, then the prediction profile system for T I has linear dimension no greater
than the number of distinct beliefs over abstract states, S  .
Proof. The proof follows similar reasoning to the proof of Proposition 17. Note that, because
of Property 1 the belief over abstract states at a given history is sufficient to compute the
prediction profile. For any history h and any test of interest t  T I :
X
X X
p(t | h) =
Pr(s | h)p(t | s) =
Pr(s | h)p(t | s)
sS

=

X

SS 

p(t | S)

X

SS  sS

Pr(s | h) =

X

p(t | S)Pr(S | h),

SS 

sS

where the third equality follows from property 1: for any S  S  , all hidden states s  S
have the same associated probabilities for the tests of interest.
Now, consider the dynamical system with beliefs over abstract states as observations
and action-observation pairs as actions. Call this the abstract belief system. Just as
with the predictive state, because it is possible to compute the prediction profile from
the abstract beliefs, the prediction profile model for T I can be seen as the result of an
observation aggregation of the abstract belief system. As a result, the prediction profile
system has linear dimension no greater than that of the abstract belief system.
The rest of the proof shows that, because of Property 2, the abstract belief system is
an MDP, and therefore has linear dimension no greater than the number of distinct beliefs
over abstract states.
Given the probability distribution over abstract states at a given history h, and the agent
takes an action a and observes and observation o, it is possible to compute the probability
of an abstract state S  S  at the new history:
X
X X
Pr(S | hao) =
Pr(s | h)Pr(S | s, a, o) =
Pr(s | h)Pr(S | s, a, o)
sS

=

X

S  S 

Pr(S | S  , a, o)

X

S  S  sS 

Pr(s | h) =

X

Pr(S | S  , a, o)Pr(S  | h),

S  S 

sS 

where the third equality follows from Property 2: for any S  S  , all hidden states s  S
have the same associated conditional distribution over next abstract states, given the action
and observation.
389

fiTalvitie & Singh

So, because one can compute the next abstract beliefs from the previous abstract beliefs,
the abstract belief system is an MDP, and therefore has linear dimension no greater than
the number of observations (the number of distinct abstract beliefs). Because one can
compute the prediction profile from the abstract beliefs, the prediction profile system can
be constructed by applying an observation abstraction to the abstract belief system. Thus,
the prediction profile system has linear dimension no greater than the number of distinct
abstract beliefs.

References
Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). A maximization technique occuring
in the statistical analysis of probabilistic functions of markov chains. The Annals of
Mathematical Statistics, 41 (1), 164171.
Baxter, J., & Bartlett, P. L. (2000). Reinforcement learning in POMDPs via direct gradient ascent. In Proceedings of the Eighteenth International Conference on Machine
Learning (ICML), pp. 4148.
Boots, B., Siddiqi, S., & Gordon, G. (2010). Closing the learning-planning loop with predictive state representations. In Proceedings of Robotics: Science and Systems, Zaragoza,
Spain.
Boots, B., Siddiqi, S., & Gordon, G. (2011). An online spectral learning algorithm for
partially observable nonlinear dynamical systems. In Proceedings of the Twenty-Fifth
National Conference on Artificial Intelligence (AAAI).
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 194.
Bowling, M., McCracken, P., James, M., Neufeld, J., & Wilkinson, D. (2006). Learning
predictive state representations using non-blind policies. In Proceedings of the TwentyThird International Conference on Machine Learning (ICML), pp. 129136.
Dinculescu, M., & Precup, D. (2010). Approximate predictive representations of partially
observable systems. In Proceedings of the Twenty-Seventh International Conference
on Machine Learning (ICML), pp. 895902.
Hansen, E. (1998). Finite-Memory Control of Partially Observable Systems. Ph.D. thesis,
University of Massachussetts, Amherst, MA.
Holmes, M., & Isbell, C. (2006). Looping suffix tree-based inference of partially observable hidden state. In Proceedings of the Twenty-Third International Conference on
Machine Learning (ICML), pp. 409416.
James, M., & Singh, S. (2004). Learning and discovery of predictive state representations
in dynamical systems with reset. In Proceedings of the Twenty-First International
Conference on Machine Learning (ICML), pp. 417424.
Julier, S. J., & Uhlmann, J. K. (1997). A new extension of the kalman filter to nonlinear
systems. In Proceedings of AeroSense: The Eleventh International Symposium on
Aerospace/Defense Sensing, Simulation and Controls, pp. 182193.
390

fiLearning to Make Predictions Without a Generative Model

Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Transactions of the ASME  Journal of Basic Engineering, 82, 3545.
Littman, M., Sutton, R., & Singh, S. (2002). Predictive representations of state. In Advances
in Neural Information Processing Systems 14 (NIPS), pp. 15551561.
Littman, M. L. (1996). Algorithms for Sequential Decision Making. Ph.D. thesis, Brown
University, Providence, RI.
Mahmud, M. M. H. (2010). Constructing states for reinforcement learning. In Proceedings
of the Twenty-Seventh International Conference on Machine Learning (ICML), pp.
727734.
McCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden
State. Ph.D. thesis, Rutgers University.
McCallum, R. A. (1993). Overcoming incomplete perception with utile distinction memory.
In Proceedings of the Tenth International Conference on Machine Learning (ICML),
pp. 190196.
Monahan, G. E. (1982). A survey of partially observable markov decisions processes: Theory,
models, and algorithms. Management Science, 28 (1), 116.
Peters, J., & Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71, 11801190.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. In Advances in Neural
Information Processing Systems 16 (NIPS).
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley and Sons, New York, NY.
Rivest, R. L., & Schapire, R. E. (1994). Diversity-based inference of finite automata. Journal
of the Association for Computing Machinery, 41 (3), 555589.
Rudary, M. (2008). On Predictive Linear Gaussian Models. Ph.D. thesis, University of
Michigan.
Rudary, M., Singh, S., & Wingate, D. (2005). Predictive linear-gaussian models of stochastic dynamical systems. In Uncertainty in Artificial Intelligence: Proceedings of the
Twenty-First Conference (UAI), pp. 501508.
Shalizi, C. R., & Klinker, K. L. (2004). Blind construction of optimal nonlinear recursive
predictors for discrete sequences. In Proceedings of the Twentieth Conference on
Uncertainty in Artificial Intelligence (UAI), pp. 504511.
Singh, S., James, M. R., & Rudary, M. R. (2004). Predictive state representations: A
new theory for modeling dynamical systems. In Uncertainty in Artificial Intelligence:
Proceedings of the Twentieth Conference (UAI), pp. 512519.
Sondik, E. J. (1978). The optimal control of partially observable markov processes over the
infinite horizon: Discounted costs. Operations Research, 26, 282304.
Soni, V., & Singh, S. (2007). Abstraction in predictive state representations. In Proceedings
of the Twenty-Second National Conference on Artificial Intelligence (AAAI), pp. 639
644.
391

fiTalvitie & Singh

Sutton, R. S., & Tanner, B. (2005). Temporal-difference networks. In Advances in Neural
Information Processing Systems 17 (NIPS), pp. 13771384.
Talvitie, E. (2010). Simple Partial Models for Complex Dynamical Systems. Ph.D. thesis,
University of Michigan, Ann Arbor, MI.
Talvitie, E., & Singh, S. (2009a). Maintaining predictions over time without a model. In
Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI), pp. 12491254.
Talvitie, E., & Singh, S. (2009b). Simple local models for complex dynamical systems. In
Advances in Neural Information Processing Systems 21 (NIPS), pp. 16171624.
Weaver, L., & Tao, N. (2001). The optimal reward baseline for gradient-based reinforcement learning. In Uncertainty in Artificial Intelligence: Proceedings of the Seventeenth
Conference (UAI), pp. 538545.
Williams, R. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8, 229256.
Wingate, D. (2008). Exponential Family Predictive Representations of State. Ph.D. thesis,
University of Michigan.
Wingate, D., Soni, V., Wolfe, B., & Singh, S. (2007). Relational knowledge with predictive
state representations. In Proceedings of the Twentieth International Joint Conference
on Artificial Intelligence (IJCAI), pp. 20352040.
Wolfe, A. P. (2010). Paying Attention to What Matters: Observation Abstraction in Partially
Observable Environments. Ph.D. thesis, University of Massachussetts, Amherst, MA.
Wolfe, A. P., & Barto, A. G. (2006). Decision tree methods for finding reusable MDP
homomorphisms. In Proceedings of the Twenty-First National Conference on Artificial
Intelligence (AAAI).
Wolfe, B., James, M., & Singh, S. (2008). Approximate predictive state representations. In
Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems
(AAMAS).
Wolfe, B., James, M. R., & Singh, S. (2005). Learning predictive state representations in
dynamical systems without reset. In Proceedings of the Twenty-Second International
Conference on Machine Learning (ICML), pp. 985992.
Wolfe, B., & Singh, S. (2006). Predictive state representations with options. In Proceedings of the Twenty-Third International Conference on Machine Learning (ICML), pp.
10251032.

392

fiJournal of Artificial Intelligence Research 42 (2011) 211-274

Submitted 03/11; published 10/11

Representing and Reasoning with Qualitative Preferences for
Compositional Systems
Ganesh Ram Santhanam
Samik Basu
Vasant Honavar

gsanthan@cs.iastate.edu
sbasu@cs.iastate.edu
honavar@cs.iastate.edu

Department of Computer Science
Iowa State University
Ames, IA 50011, USA

Abstract
Many applications, e.g., Web service composition, complex system design, team formation, etc., rely on methods for identifying collections of objects or entities satisfying some
functional requirement. Among the collections that satisfy the functional requirement,
it is often necessary to identify one or more collections that are optimal with respect to
user preferences over a set of attributes that describe the non-functional properties of the
collection.
We develop a formalism that lets users express the relative importance among attributes
and qualitative preferences over the valuations of each attribute. We define a dominance
relation that allows us to compare collections of objects in terms of preferences over attributes of the objects that make up the collection. We establish some key properties of the
dominance relation. In particular, we show that the dominance relation is a strict partial
order when the intra-attribute preference relations are strict partial orders and the relative
importance preference relation is an interval order.
We provide algorithms that use this dominance relation to identify the set of most
preferred collections. We show that under certain conditions, the algorithms are guaranteed
to return only (sound), all (complete), or at least one (weakly complete) of the most
preferred collections. We present results of simulation experiments comparing the proposed
algorithms with respect to (a) the quality of solutions (number of most preferred solutions)
produced by the algorithms, and (b) their performance and efficiency. We also explore
some interesting conjectures suggested by the results of our experiments that relate the
properties of the user preferences, the dominance relation, and the algorithms.

1. Introduction
Many applications call for techniques for representing and reasoning about preferences over
a set of alternatives. In such settings, preferences over the alternatives are expressed with
respect to a set of attributes that describe the alternatives. Such preferences can be either
qualitative or quantitative. A great deal of work on multi-attribute decision theory has
focused on reasoning with quantitative preferences (Fishburn, 1970a; Keeney & Raiffa,
1993). However, in many settings it is more natural to express preferences in qualitative
terms (Doyle & Thomason, 1999) and hence, there is a growing interest on formalisms for
representing and reasoning with qualitative preferences (Brafman & Domshlak, 2009) in AI.
An important problem in this context has to do with representing qualitative preferences
over multiple attributes and reasoning with them to find the most preferred among a set of
c
2011
AI Access Foundation. All rights reserved.

fiSanthanam, Basu & Honavar

alternatives. Brafman, Domshlak and Shimonys seminal work (2006) attempts to address
this problem by introducing preference networks that capture: (a) intra-variable or intraattribute preferences specifying preferences over the domains of attributes; (b) the relative
importance among the attributes. Preference networks use a graphical representation to
compactly encode the above types of preferences from the user, and employ the ceteris
paribus 1 semantics to reason about the most preferred alternatives. In this model, each
alternative is completely described by the values assigned to a set of attributes.
In many AI applications such as planning and scheduling, the alternatives have a composite structure, i.e., an alternative represents a collection or a composition of objects rather
than simple objects. In such settings, typically there are a set of user specified functional requirements that compositions are required to satisfy2 . Among all the possible compositions
that do satisfy the functional requirements, there is often a need to choose compositions
that are most preferred with respect to a set of user preferences over a set of non-functional
attributes of the objects that make up the composition. We illustrate the above problem
using the following example.
1.1 Illustrative Example
Consider the task of designing a program of study (POS) for a Masters student in the
Computer Science department. The POS consists of a collection of courses chosen from a
given repository of available courses spanning different areas of focus in computer science.
Apart from the area of focus, each course also has an assigned instructor and a number of
credit hours. A repository of available courses, their areas of focus, their instructors and
the number of credit hours are specified in Table 1.
Course
CS501
CS502
CS503
CS504
CS505
CS506
CS507
CS508
CS509
CS510

Area
Formal Methods (FM)
Artificial Intelligence (AI)
Formal Methods (FM)
Artificial Intelligence (AI)
Databases (DB)
Networks (NW)
Computer Architecture (CA)
Software Engineering (SE)
Theory (TH)
Theory (TH)

Instructor
Tom
Gopal
Harry
White
Bob
Bob
White
Tom
Jane
Tom

Credits
4
3
2
3
4
2
3
2
3
3

Table 1: List of courses the student can choose from
In this example, each POS can be viewed as a composition of courses. The requirements
for an acceptable Masters POS (i.e., a feasible composition) are as follows.
F 1. The POS should include at least 15 credits
1. A Latin term for all else being equal
2. For example, in planning, a valid plan is a collection of actions that satisfies the goal; and in scheduling,
a valid schedule is a collection of task-to-resource assignments that respects the precedence constraints.

212

fiRepresenting and Reasoning with Qualitative Preferences

AI
FM

TH

DB

NW

SE

Gopal

Bob
Jane

T om

W hite

CA
(a) A

Harry
(b) I

Figure 1: Intra-attribute preferences for Area (A ) and Instructor (I ).
F 2. The POS should include the two core courses CS509 and CS510
F 3. There should be courses covering at least two breadth areas of study (apart from the
area of Theory (TH))
Given the repository of courses (see Table 1; there may be one or more acceptable
programs of study, i.e., feasible compositions). For example:
 P1 = CS501  CS502  CS503  CS504  CS509  CS510
 P2 = CS501  CS502  CS505  CS506  CS509  CS510
 P3 = CS503  CS504  CS507  CS508  CS509  CS510
Suppose that in addition to the above requirements, a student has some preferences
over the course attributes such as the area of focus, the choice of instructors and difficulty
level in terms of credit hours. Among several acceptable programs of study, the student
may be interested in those programs of study that: (a) satisfy the minimum requirements
(see above) for an acceptable POS, and (b) those that are most preferred with respect to
his/her preferences specified above. The preferences of a student with respect to the course
attributes Area (A) and Instructor (I) are illustrated in Figure 1 (arrows are directed
toward the preferred area/instructor in the figure, e.g., AI is preferred to F M and Bob is
preferred to T om). In addition let us say that the student prefers the POS that have lesser
total number of credits (this specifies C ). Further, let the relative importance among the
attributes A, I and C be I  A  C, i.e., I is relatively more important than A, which is in
turn relatively more important than C.
1.2 Problem Statement for the Illustrative Example
The problems that we try to address in this paper for the above example are:
 Given two programs of study, namely Pi and Pj , determine whether Pi dominates
(i.e., is preferred to) Pj or vice versa with respect to the students preferences;
 Given a repository of courses and an algorithm for computing a set of acceptable
programs of study, find the most preferred, acceptable programs of study with respect
to the above dominance relation.
213

fiSanthanam, Basu & Honavar

In the example given in Section 1.1, the functional requirements correspond to the three
conditions F 1 to F 3, all of which must be satisfied for a collection of courses to be an
acceptable POS. Area (A), instructor (I) and number of credits (C) constitute the nonfunctional attributes, and the user preferences over these attributes are given by {A , I
, C } and I  A  C. One can envision similar problems in several other applications,
ranging from assembling hardware and software components in an embedded system (such
as designing a pacemaker or anti-lock braking system) to putting together a complex piece
of legislation (such as the one for reforming health care).
In general, we are interested in the problem of (a) reasoning about preferences over compositions of objects, given the preferences over a set of non-functional attributes describing
the objects; and (b) identifying compositions that satisfy the functional requirements of the
compositional system, and at the same time are optimal with respect to the stated preferences over the non-functional attributes. Against this background, we present a preference
formalism and a set of algorithms to address this problem in compositional systems.
1.3 Contributions
We adopt the preference network representation introduced by Brafman et al. (2006) for
the specification of qualitative preferences3 over valuations of each attribute as well as the
relative importance among the attributes. We extend reasoning about preferences over
single objects to deal with preferences over collections of objects. The main contributions
of this paper are as follows.
1. We develop a preference formalism that allows users to specify preferences in terms
of intra-attribute and relative importance preferences over a set of attributes, and
includes mechanisms for:
a) Computing the valuation of a composition: With respect to each attribute, we
define a generic aggregation function to compute the valuation of a composition
as a function of the valuations of its components. We also present a strict partial
order preference relation for comparing two compositions with respect to their
aggregated valuations of each attribute.
b) Comparing the valuations of compositions: We introduce a dominance relation
that compares compositions (in terms of their aggregated valuations) with respect
to the stated preferences, and establish some of its key properties. In particular,
we show that this relation is a strict partial order whenever the intra-attribute
preferences are strict partial orders and relative importance preference is an
interval order.
2. We develop a suite of algorithms that identify the set, or subset of the most preferred
composition(s) with respect to the user preferences. In particular, we show that
under certain conditions, the algorithms are guaranteed to return only (sound), all
(complete), or at least one (weakly complete) of the most preferred compositions. The
algorithms we develop fall into two classes:
3. We do not deal with conditional preferences in this work.

214

fiRepresenting and Reasoning with Qualitative Preferences

a) those that first compute the set of all feasible compositions using a functional
composition algorithm as a black box, and then proceed to find the most preferred
among them using the preference relations developed in (1); and
b) an algorithm that interleaves at each step the execution of a functional composition algorithm and the ordering of partial solutions with respect to user
preferences. It requires the functional composition algorithm to be able to construct a composition satisfying the functional requirement incrementally, i.e., by
iteratively extending partial compositions with additional components.
We analyze some key properties of the algorithms that yield specific conditions on the
structure of preferences, under which the algorithms produce only/at least one/all of
the most preferred solutions.
3. We present results of experiments that compare performance of the above algorithms
for computing the most preferred compositions on a set of simulated composition
problem instances. The results demonstrate the feasibility of our approach in practice, and compare our algorithms with respect to the quality of (number of good or
most preferred) solutions produced by the algorithms and their performance (running time) and efficiency (the number of times they invoke the functional composition
algorithm). Based on analysis of the experimental results, we also establish some
previously unknown key theoretical properties of the dominance relation directly as a
function of the user preferences.
Our formalism is generic in the sense that one can use any aggregation function that
appropriately represents the valuation of the composition as a function of the valuations
of its constituents. In particular, we show examples of aggregation functions that compute
the summation (numeric), the minimum/maximum valuation (totally ordered), or the set
of worst valuations (partially ordered) of the constituents of a composition. Our formalism
also provides flexibility in choosing the preference relation that compares sets of valuations
of two compositions, so that any strict partial order preference relation can be used.
All our algorithms are completely independent of various aspects of the preference formalism, namely, the choice of aggregation functions, the preference relation used to compare
aggregated valuations over a single attribute, and the dominance relation used to compare
compositions over all attributes, except that the preference relations are strict partial orders. The theoretical and experimental results provide precise conditions under which the
algorithms produce only/at least one/all of the most preferred solutions. This enables the
user to choose an algorithm of his/her choice for particular problem instance, depending
on the quality of solutions that is needed. In addition, our analysis also allows the user to
trade off the quality of solutions produced against performance and efficiency.
1.4 Related Work
The closest work related to our paper is a paper by Binshtok, Brafman, Domshlak, and
Shimony (2009), where preferences are expressed over collections based on the number of
objects in the collection that satisfy a desired property (e.g., having at least two political
and two sports articles in choosing articles for a newspaper publication). In contrast, we
215

fiSanthanam, Basu & Honavar

develop a formalism that considers the desirability of the collection as a whole based on
the attributes of the objects that make up the collection, and algorithms to identify the
most preferred collection(s) among those that satisfy the requirement. We further show
how the problems solved using the formalism due to Binshtok et al. can also be solved in
our formalism (see Section 7.3.2).
In the recent years, there has been a lot of work in the database community on the
evaluation of preference queries (e.g., skyline queries) to find the most preferred subset of
tuples from a result set. The problem of finding the most preferred set of tuples is analogous
to finding the most preferred set of alternatives, where each alternative is a simple object,
i.e., a tuple described by a set of attributes. Our problem then corresponds to finding the
most preferred set of alternatives, where each alternative is in turn a set of tuples that satisfy
some requirement (e.g., the set of tuples that satisfy a set of integrity constraints). Moreover,
the algorithms found in the database literature mostly address totally or weakly ordered
preferences over the values of attributes, while we address partially ordered preferences
as well. In addition, most of them rely on the maintenance of database indexes over the
attributes of the tuples because they typically cater to large scale, static data which is not
typical in our setting. We however note the relevance and possible utility of techniques
developed in the databases community for our problem in specific scenarios.
We refer the reader to Section 7.3 for a more detailed discussion of related work.
1.5 Organization
The rest of the paper is organized as follows. In Section 2, we define a compositional system,
discuss the types of preferences that we will consider, and specify the problem in formal
terms. In Section 3, we present our preference formalism including the dominance relation
and analyze its properties. In Section 4, we present four algorithms for identifying the most
preferred compositions and discuss their properties. The proofs of the results in this section
are given in Appendix A. In Section 5, we discuss the complexity of our algorithms.In
Section 6, we present results of experiments that we performed to compare our algorithms
in terms of the quality of solutions produced, performance and efficiency. In Section 7, we
summarize our contributions and discuss the related and future work in this area.

2. Preliminaries
We recall some basic properties and definitions concerning binary relations that we will use
in the rest of the paper (see Fishburn, 1985, for a comprehensive treatment of the same).
2.1 Properties of Binary Relations
Let  be a binary relation on a set S, i.e.,  S  S. We say that  is an equivalence (eq),
a (strict) partial order (po), an interval order (io), a weak order (wo) or a total order (to),
as defined in Table 2.
A total order is also a weak order; a weak order is also an interval order; and an interval
order is also a strict partial order.
216

fiRepresenting and Reasoning with Qualitative Preferences

#
1.
2.
3.
4.
5.
6.
7.
8.

Property of relation
reflexive
irreflexive
symmetric
asymmetric
transitive
total or complete
negatively transitive
ferrers

Definition
x  S : x  x
x  S : x 6 x
x, y  S : x  y  y  x
x, y  S : x  y  y 6 x
x, y, z  S : x  y  y  z  x  z
x, y  S : x 6= y  x  y  y  x
x, y, z  S : x  y  x  z  z  y
x, y, z, w  S : (x  y  z  w) 
(x  w  z  y)

eq
X

po

io

wo

to

X

X

X

X

X
X

X
X

X
X

X

X
X

X
X
X
X
X

X
X

Table 2: Properties of binary relations
2.2 Compositional System
A compositional system consists of a repository of pre-existing components from which we
are interested in assembling compositions that satisfy a pre-specified functionality. Formally,
a compositional system is a tuple hR, , |=i where:
 R = {W1 , W2 . . . Wr } is a set of available components,
  denotes a composition operator that functionally aggregates components and encodes all the functional details of the composition.  is a binary operation on components Wi , Wj in the repository that produces a composition Wi  Wj .
 |= is a satisfaction relation that evaluates to true when a composition satisfies some
pre-specified functional properties.
Definition 1 (Compositions, Feasible Compositions and Extensions). Given a compositional system hR, , |=i, and a functionality , a composition C = Wi1  Wi2  . . . Win is
an arbitrary collection of components Wi1 , Wi2 , . . . , Win s.t. j  [1, n] : Wij  R.
i. C is a feasible composition whenever C |= ;
ii. C is a partial feasible composition whenever Wj1 . . . Wjm  R : C  Wj1  . . .  Wjm
is a feasible composition; and
iii. C  Wi is a feasible extension of a partial feasible composition C whenever C  Wi is
a feasible or a partial feasible composition.
Given a compositional system hR, , |=i and a functionality , an algorithm that produces a set of feasible compositions (satisfying ) is called a functional composition algorithm. The most general class of functional composition algorithms we consider can be
treated as black boxes, simply returning a set of feasible compositions satisfying  in a single step. Some other functional composition algorithms proceed by computing the set of
feasible extensions of partial feasible compositions incrementally.
Definition 2 (Incremental Functional Composition Algorithm). A functional composition
algorithm is said to be incremental if, given an initial partial feasible composition C and the
desired functionality , the algorithm computes the set of feasible extensions to C.
217

fiSanthanam, Basu & Honavar

An incremental functional composition algorithm can be used to compute the feasible
compositions by recursively invoking the algorithm on the partial feasible compositions it
produces starting with the empty composition (), and culminating with a set of feasible
compositions satisfying . In this sense, incremental functional composition algorithms are
similar to their black box counterparts. However, (as we later show in Section 4.5) in
contrast to their black box counterparts, incremental functional composition algorithms
can be exploited in the search for the most preferred feasible compositions, by interleaving
each step of the functional composition algorithm with the optimization of the valuations of
non-functional attributes (with respect to the user preferences). This allows us to develop
algorithms that can eliminate partial feasible compositions that will lead to less preferred
feasible compositions from further consideration early in the search.
Different approaches to functional composition, (e.g., Traverso & Pistore, 2004; Lago,
Pistore, & Traverso, 2002; Baier, Fritz, Bienvenu, & McIlraith, 2008; Passerone, de Alfaro,
Henzinger, & Sangiovanni-Vincentelli, 2002) differ in terms of (a) the languages used to
represent the desired functionality  and the compositions, and (b) the algorithms used to
verify whether a composition C satisfies , i.e., C |= . We have intentionally abstracted the
details of how functionality  is represented (e.g., transition systems, logic formulas, plans,
etc.) and how a composition is tested for satisfiability (|=) against , as the primary focus
of our work is orthogonal to details of the specific methods used for functional composition.
2.3 Preferences over Non-functional Attributes
We now turn to the non-functional aspects of compositional systems. In addition to obtaining functionally feasible compositions, users are often concerned about the non-functional
aspects of the compositions, e.g., the reliability of a composite Web service. In such cases,
users seek the most preferred compositions among those that are functionally feasible, with
respect to a set of non-functional attributes describing the components. In order to compute
the most preferred compositions, it is necessary for the user to specify his/her preferences
over a set of non-functional attributes X .
2.3.1 Notation
In general, for any relation P , we use the same notation, i.e., P to denote the transitive
closure of the relation as well, and 6P or  P to denote its complement. The list of
notations used in this paper are given in Table 3.
We focus only on strict partial order preference relations, i.e., relations that are both
irreflexive and transitive, because transitivity is a natural property of any rational preference
relation (von Neumann & Morgenstern, 1944; French, 1986; Mas-Colell, Whinston, & Green,
1995), and irreflexivity ensures that the preferences are strict.
With respect to any strict partial order preference relation P , we say that two elements
u and v are indifferent, denoted u P v, whenever u 6P v and v 6P u. For preference
relations i , i ,  and d , we denote the corresponding indifference relation by i , i , 
and d respectively. We will drop the subscripts whenever they are understood from the
context.
Proposition 1. For any strict partial order preference relation P , the corresponding indifference relation P is reflexive and symmetric.
218

fiRepresenting and Reasoning with Qualitative Preferences

Notation
P(S)
R = {W1    Wr }

C, U, V, Z
C
X = {X1    Xm }
D = {D1    Dm }
ui , vi , ai , bi     Di
VWi
VCi
VWi (Xj )
VCi (Xj )
i , X

i
F (Xi )
i
d
 (S)


Meaning
Power set of the set S
Set of components in the repository
Operation that composes components from R
Composition or collectionn4 of a set of components from R
A set {Ci } of compositions
Set of non-functional attributes
Set of possible valuations (domains) of attributes in X respectively
Valuations of an attribute with domain Di
Overall valuation of the component Wi with respect to all attributes
X
Overall valuation of the composition Ci with respect to all attributes
X
Valuation of the component Wi with respect to the attribute Xj
Valuation of the composition Ci with respect to the attribute Xj
Intra-attribute preference over valuations of Xi or X respectively
(user input)
Relative importance among attributes (user input)
Aggregation function that computes the valuation of a composition
with respect to Xi as a function of the valuation of its components
Range of the aggregation function i for attribute Xi
Derived preference relation on the aggregated valuations with respect
to Xi
Dominance relation that compares two compositions in terms of their
aggregated valuations over all attributes
The non-dominated set of elements in S with respect to 
User specified functionality to be satisfied by a feasible composition
Table 3: Notation

Proof. Follows from a well-known property of strict partial orders due to Fishburn (1970b).

It is important to note that indifference with respect to a strict partial order is not
necessarily transitive. For instance, X = {(b, c)} is a strict partial order on the set {a, b, c}
with b X a, a X c but b X c.
2.3.2 Representing Multi-Attribute Preferences
Following the representation scheme introduced by Boutilier et al. (2004) and Brafman et
al. (2006), we model the users preferences with respect to multiple attributes in two forms:
(a) intra-attribute preferences with respect to each non-functional attribute in X , and (b)
relative importance over all attributes.
4. We will use the terms composition and collection; and component and object interchangeably.

219

fiSanthanam, Basu & Honavar

Definition 3 (Intra-attribute Preference). The intra-attribute preference relation, denoted
by i is a strict partial order (irreflexive and transitive) over the possible valuations of an
attribute Xi  X . u, v  Di : u i v iff u is preferred to v with respect to Xi .
Definition 4 (Relative Importance). The relative importance preference relation, denoted
by  is a strict partial order (irreflexive and transitive) over the set of all attributes X .
Xi , Xj  X : Xi  Xj iff Xi is relatively more important than Xj .
Given a set X of attributes, the intra-attribute preference relations {i } over their
respective domains, and the relative importance preference relation  on X , we address the
following problems.
 Given two compositions Cj and Ck , determine whether VCj d VCk or vice versa;
 Given a compositional system hR, , |=i, and an algorithm for computing a set of
feasible compositions {Cf : Cf |= }, find the most preferred feasible compositions
with respect to the above dominance relation.

3. Preference Formalism
Given a compositional system with a repository of components described by attributes X
and preferences ({i }, ) over them, we are interested in reasoning about preferences over
different compositions. Note that based on preferences {i } and , one can make use of
existing formalisms such as TCP-nets (Brafman et al., 2006) to select the most preferred
components. However, the problem of comparing compositions (as opposed to comparing
components) with respect to the attribute preferences is complicated by the fact that the
valuation of a composition is a function of the valuations of its components. Our approach
to developing the preference formalism is as follows.
First, given a composition and the valuations of its components with respect to the
attributes, we obtain the aggregated valuation of the composition with respect to each
attribute as a function of the valuations of its components. Next, we define preference
relations to compare the aggregated valuations of two compositions with respect to each
attribute. Finally, we build a dominance preference relation d that qualitatively compares
any two compositions with respect to their aggregated valuations across all attributes.
3.1 Aggregating Attribute Valuations across Components
In order to reason about preferences over compositions, it is necessary to obtain the valuation of a composition with respect to each attribute Xi in terms of its components, using
some aggregation function i . There are several ways to aggregate the preference valuations
attribute-wise across components in a composition. The aggregation function i defines the
valuation of a composition with respect to an attribute Xi as a function of the valuations
of its components.
Remark. In the compositional systems considered here, we assume that the valuation
of a composition with respect to its attributes is a function of only the valuations of its
components. In other words, if C = W1  W2  . . .  Wn , then VC is a function of only
{VW1 , VW2 , . . . , VWn }. However, in the most general setting, the aggregation functions i
220

fiRepresenting and Reasoning with Qualitative Preferences

need to take into account, in addition to the valuations of the components themselves, the
structural or functional details of a composition encoded by  (e.g., the reliability of a Web
service composition depends on whether the service components are composed in a series
or parallel structure).
Definition 5 (Aggregation Function). The aggregation function on a multiset5 of possible
valuations (Di ) of attribute Xi is
i : M(Di )  F (Xi )
where F (Xi ) denotes the range of the aggregation function.
Aggregation with respect to an attribute Xi amounts to devising an appropriate aggregation function i that computes the valuation of a composition in terms of the valuations
of its components for Xi . The range F (Xi ) of i depends on the choice of aggregation
function. Some examples of aggregation functions are given below.
1. Summation. This is applicable in cases where an attribute is real-valued and represents some kind of cost. For example, the cost of a shopping cart is the sum of the
costs of the individual items it includes. In our running example, the total number
of credits in a POS consisting of a set of courses is the sum of the credits of all the
courses it includes. That is, if S is the set of credit hours (valuations of the courses
with respect to the attribute C) of courses in a POS, then
C (S) := {sS s}
2. Minimum/Maximum. Here, the valuation of a composition with respect to an attribute is the worst, i.e., the minimum among the valuations of its components. This
type of aggregation is a natural one to consider while composing embedded systems
or Web services. For example, when putting together several components in an embedded system, the system is only as secure (or safe) as its least secure (or safe)
component.
i (S) := {minsS s}
Analogously, one could choose as the valuation of the composition the maximum (best)
among the valuations of its components. Such an aggregation function may be useful
in applications such as parallel job scheduling, where the maximum response time is
used to measure the quality of a schedule.
3. Best/Worst Frontier. In some settings, it is possible that the intra-attribute preference over the values of an attribute is a partial order (not necessarily a ranking or a
total order). Hence, it may not be possible to compute the valuation of a composition as the best or worst among the valuations of its components because a unique
maximum or minimum may not exist. For example, it may be useful to compute the
5. A multiset is a generalization of a set that allows for multiple copies of its elements.

221

fiSanthanam, Basu & Honavar

valuation of a composition as the minimal set of valuations among the valuations of
its components, which we call the worst frontier. The worst frontier represents the
worst possible valuations of an attribute Xi with respect to i , i.e., the minimal set6
among the set of valuations of the components in a composition.
Definition 6 (Aggregation using Worst Frontier). Given a set S of valuations of an attribute Xi , the worst frontier aggregation function is defined by
S  Di : i (S) := {v : v  S  u  S : v i u}
In our running example (see Section 1.1), the user would like to avoid courses not in his
interest area and professors whom he is not comfortable with. That is, a program of study
is considered only as good as the least interesting areas of study it covers, and the set of
professors he is least comfortable with. Hence, the worst frontier aggregation function is
chosen for the breadth area and instructor attributes.
Example. The worst possible valuations of the attributes A and I for the program of
study (composition) P1 with respect to A and I are {F M, T H} and {W hite, Harry}
respectively. Similarly, for P2 the valuations of the attributes A and I are {DB, N W }
and {Jane, T om} respectively; and for P3 the valuations of the attributes A and I are
{CA, SE} and {Harry, W hite} respectively. These sets correspond to the worst frontiers of the respective attributes. The different areas of focus covered in the POS P2 are
{F M, AI, DB, N W, T H}, and the worst frontier of this set is A ({F M, AI, DB, N W, T H}) =
{DB, N W } because AI A DB, F M A DB, T H A N W . Similarly the set of instructors in P2 are {T om, Gopal, Bob, Jane}, and hence we have I ({T om, Gopal, Bob, Jane}) =
{Jane, T om} because Bob I Jane and Gopal I T om. For attribute C, the aggregation
function evaluates the sum of credits of the constituent courses in a POS. Therefore, for P2
we have C ({4, 3, 4, 2, 3, 3}) = 4 + 3 + 4 + 2 + 3 + 3 = 19.

We note that other choices of the aggregation function can be accommodated in our
framework (such as average or a combination of best and worst frontier sets), and that the
above is only a representative list of choices.
Proposition 2 (Indifference of Frontier Elements). Consider an attribute Xi , whose valuations are aggregated using the best or worst frontier aggregation function. Let A  F (Xi ).
Then u i v for all u, v  A.
Proof. Follows from Definition 6 (or the analogous definition of a best frontier) and a wellknown result due to the work of Fishburn (1985).
Definition 7 (Valuation of a Composition for Attributes Aggregated using Best/Worst
Frontier). Consider an attribute Xi , whose valuations are aggregated using the best or worst
frontier aggregation function. The valuation of a component W with respect to an attribute
Xi is denoted as VW (Xi )  Di . The valuation of a composition of two components W1 and
W2 with respect to an attribute Xi , each with valuation VW1 (Xi ) and VW2 (Xi ) respectively,
is given by
6. Note that if i is a total order, then worst frontier represents the minimum or lowest element in the set
with respect to the total order.

222

fiRepresenting and Reasoning with Qualitative Preferences

VW1 W2 (Xi ) := i (VW1 (Xi )  VW2 (Xi ))
Example. Consider P2 = CS501  CS502  CS505  CS506  CS509  CS510 in our
running example (see Section 1.1).
VP2 (I) = I (VCS501 (I)  VCS502 (I)  VCS505 (I)  VCS506 (I)  VCS509 (I)  VCS510 (I))
= I ({T om}  {Gopal}  {Bob}  {Bob}  {Jane}  {T om})
= I ({T om, Gopal, Bob, Jane})
= {T om, Jane}

It must be noted that VW1 W2 (Xi ) = VW2 W1 (Xi ) according to the above definition,
because the valuations of compositions are subsets of the union of individual component
valuations.
3.2 Comparing Aggregated Valuations
Having obtained an aggregated valuation with respect to each attribute, we next proceed
to discuss how to compare aggregated valuations attribute-wise. We denote the preference
relation used to compare the aggregated valuations for an attribute Xi by i . In the simple
case when an aggregation function i with respect to an attribute Xi returns a value in
Di (F (Xi ) = Di ), the intra-attribute preference i can be (re)used to compare aggregated
valuations, i.e., i =i . Other choices of i can be considered as long as i is a partial
order. In order to obtain a strict preference relation, we require irreflexivity, and to obtain
a rational preference relation, we require transitivity7 .
For worst frontier-based aggregation (Definition 6), we present a preference relation that
uses the following idea: Given two compositions with different aggregated valuations (worst
frontiers) A, B with respect to an attribute Xi , we say that A is preferred to B if for every
valuation of Xi in B, there is some valuation in A that is strictly preferred.
Definition 8 (Preference over Worst Frontiers). Let A, B  F (Xi ) be two worst frontiers
with respect to attribute Xi . We say that valuation A is preferred to B with respect to
Xi , denoted by A i B, if for each element in B, there exists an element in A that is more
preferred.
A, B  F (Xi ) : A i B  b  B, a  A : a i b
Example. In our running example (see Section 1.1), we have {F M, T H} A {DB, N W }
because F M A DB and T H A N W .

Given a preference relation over a set of elements, there are several ways of obtaining a
preference relation over subsets of elements from the set (see Barbera, Bossert, & Pattanaik,
2004, for a survey on preferences over sets). Definition 8 is one simple way to achieve
this. In some settings, in contrast to Definition 8, it might be useful to compare only
7. Any preference relation, including the one that compares only the uncommon elements of two sets can
be used, provided it is irreflexive and transitive.

223

fiSanthanam, Basu & Honavar

elements in the two sets that are not common. In such settings, a suitable irreflexive
and transitive preference relation can be used, such as the asymmetric part of preference
relations developed by Brewka et al. (2010) and Bouveret et al. (2009). In the absence of
transitivity, the transitive closure of the relation may be used to compare sets of elements,
as done by Brewka et al.
We now discuss some properties of the specific relation i as introduced in Definition 8.
Proposition 3 (Irreflexivity of i ). A  F (Xi )  A 6 i A.
Proof. a, b  A, a i b

(follows from P roposition 2)

Proposition 4 (Transitivity of i ). If A, B, C  F (Xi ), then A i B  B i C  A i C.
Proof. Immediate from Definition 8.
Definition 9. Let A, B  F (Xi ). We say that valuation A is at least as preferred as B
with respect to Xi , denoted i iff
A i B  A = B  A i B
Proposition 5. i is reflexive and transitive.
Proof. Follows from the facts that = is reflexive and transitive, and i is irreflexive and
transitive.
Definition 10 (Complete Valuation). The complete valuation or outcome or assignment of
a composition C is defined as a tuple VC := hVC (X1 ), . . . VC (Xm )i, where VC (Xi )  F (Xi ).
m
Y
F (Xi ).
The set of all possible valuations or outcomes is denoted as
i=1

Example. In case of our example in Section 1.1:
VP 1

= hA ({F M, AI, T H}), I ({T om, Gopal, Harry, W hite, Jane}), C ({4, 3, 2, 3, 3, 3})i
= h{F M, T H}, {W hite, Harry}, {18}i

VP 2

= hA ({F M, AI, DB, N W, T H}), I ({T om, Gopal, Bob, Jane}), C ({4, 3, 4, 2, 3, 3})i
= h{DB, N W }, {T om, Jane}, {19}i

VP 3

= hA ({F M, AI, CA, SE, T H}), I ({Harry, W hite, T om, Jane}), C ({2, 3, 3, 2, 3, 3})i
= h{CA, SE}, {Harry, W hite}, {16}i


3.3 Dominance: Preference over Compositions
In the previous sections, we have discussed how to evaluate and compare a composition
with respect to the attributes as a function of its components. In order to identify preferred
compositions, we need to compare compositions with respect to their aggregated valuations
over all attributes, based on the originally specified intra-attribute and relative importance
preferences. We present a specific dominance relation for performing such a comparison.
224

fiRepresenting and Reasoning with Qualitative Preferences

U d V  Xi  X : U(Xi ) i V(Xi ) 
Xk  X : (Xk  Xi  Xk  Xi )  U(Xk ) k V(Xk )

Layer 2
(Dominance)

Compare
compositions

d 

m
Y

F (Di ) 

i=1

F (Di )

i=1

m  F (Dm )  F (Dm )

Compare
1  F (D1 )  F (D1 ) 2  F (D2 )  F (D2 )
aggregated valuations
F (D1 )
F (D2 )

Layer 1
(Aggregation)

Compute
aggregated valuations

1

2

P(D1 )

User
Input

m
Y

X X
X = {X1 , X2 , . . . Xm }

F (Dm )
...

P(D2 )

m
P(Dm )

1  D1  D1

2  D2  D2

...

m  Dm  Dm

D1 = {a1 , a2 . . .}

D2 = {b1 , b2 . . .}

...

Dm = {u1 , u2 . . .}

Relative Importance

Intra-attribute preferences

Figure 2: Dominance: Preference over compositions
Definition 11 (Dominance). Dominance d is a binary relation defined as follows: for
m
Y
F (Xi )
all U 8 , V 
i=1

U d V 

Xi : U(Xi ) i V(Xi ) 
Xk : (Xk  Xi  Xk  Xi )  U(Xk ) k V(Xk )

In Definition 11, we call the attribute Xi as the witness of the relation. The dominance
relation d is derived from and respects both the intra-attribute preferences (i ) as well as
the relative importance preferences () asserted by the user. Figure 2 graphically illustrates
how dominance is derived from user-specified preferences. First, to start with we have user
specified preferences, namely intra-attribute (i ) and relative importance () preferences.
Next, from i preferences, the valuations of compositions with respect to attributes are
computed using the aggregation function (i ). Then the intra-attribute preference relation
to compare the aggregated valuations ( i ) is derived from i . Finally, the global dominance
( d ) is defined in terms of i and .
The definition of dominance states that a composition U dominates V iff we can find
a witness attribute Xi such that with respect to the intra-attribute preference i , the
valuation of U dominates V in terms of i , and for all attributes Xk which the user considers
more important than () or indifferent to () Xi , the valuation of Xk in U is at least as
preferred ( i ) as the valuation of Xk in V.
8. To avoid excessively cluttering the notation, for a given composition C, we will slightly abuse notation
by using C interchangeably with VC .

225

fiSanthanam, Basu & Honavar

Example. In our running example (see Section 1.1), we have VP2 d VP1 with I as witness
and VP1 d VP3 with A as witness. If I A, I C but A  C then VP2 d VP1 and VP2 d VP3
with I as witness, but VP1 6 d VP3 and VP3 6 d VP1 . This is because P1 is preferred to P3
with respect to A ({F M, T H} A {CA, SE}); but P3 is preferred to P1 with respect to C
({16} C {18}), and neither A nor C is relatively more important than the other.

3.4 Properties of d
We now proceed to analyze some properties of d with respect to the worst-frontier aggregation function. First, we show that a partial feasible composition is not dominated
with respect to d by any of its extensions. This property will be useful in establishing
the soundness of algorithms that compute the most preferred compositions (see Section 4).
Next, we observe that d is irreflexive (follows from the irreflexivity of i ), and proceed to
identify the conditions under which d is transitive. We focus on transitive preferences because many studies have considered transitivity to be a key property of preference relations
(von Neumann & Morgenstern, 1944; French, 1986; Mas-Colell et al., 1995)9 .
Proposition 6. Whenever preferences are aggregated using the worst-frontier based aggregation function, for any partial feasible composition C, there is no feasible extension C  W
that dominates it, i.e., VCW 6 d VC .
Proof. The proof proceeds by showing that with respect to each attribute Xi , VCW (Xi ) 6 i
VC (Xi ), thereby ruling out the existence of a witness for VCW d VC . Suppose that by
contradiction, C  W is a feasible extension of C such that VCW d VC . By Definition 11,
VCW d VC requires the existence of a witness attribute Xi  X such that VCW (Xi ) i VC (Xi ),
i.e.,
b  VC (Xi ) a  VCW (Xi ) : a i b
(1)
By Definition 7, we have VCW (Xi ) = i (VC (Xi )  VW (Xi )). However, by Definition 6
a  i (VC (Xi )  VW (Xi ))  b  VC (Xi )  VW (Xi ) : a i b, which contradicts Equation (1).
This rules out the existence of a witness for VCW d VC . Hence, VCW 6 d VC .
We next proceed to show that d is not necessarily transitive when intra-attribute and
relative importance preference relations are both arbitrary strict partial orders.
Proposition 7. When intra-attribute preferences i as well as relative importance among
attributes  are arbitrary partial orders, U d V  V d Z ; U d Z
Proof. We show a counter example of a compositional system with partially ordered {i }, 
and compositions U, V, Z such that U d V, V d Z but U 6 d Z.
Consider a system with a set of attributes X = {X1 , X2 , X3 , X4 }, each with domains
D1 = {a1 , b1 }, . . . D4 = {a4 , b4 }. Let the relative importance relation  on X and the intraattribute preferences 1 . . . 4 be given by  = {(X1 , X3 ), (X2 , X4 )} and i = {(ai , bi )}, i =
1, 2, 3, 4 respectively (Figure 3). The valuations of U, V, Z with respect to the attributes X
are given in Table 4.
9. While some studies of human decision making have argued that human preferences are not necessarily
transitive (Tversky, 1969), others have offered evidence to the contrary (Regenwetter, Dana, & DavisStober, 2011).

226

fiRepresenting and Reasoning with Qualitative Preferences

Intra-variable preferences

Relative Importance ()

a1 1 b1

X2

X1

a2 2 b2
a3 3 b3
a4 4 b4

X4

X3

U = h{a1 }, {a2}, {b3}, {b4}i

V = h{b1 }, {a2}, {a3}, {b4}i

Z = h{b1 }, {b2}, {a3}, {a4 }i
Figure 3: Counter example
Comp. (C)

VC (X1 )

VC (X2 )

VC (X3 )

VC (X4 )

U
V
Z

a1
b1
b1

a2
a2
b2

b3
a3
a3

b4
b4
a4

Table 4: Valuations of U, V, Z

Clearly U d V with X1 as the witness, and V d Z with X2 as the witness. In addition,
note that:
Z(X3 ) 3 U(X3 )

(2)

Z(X4 ) 4 U(X4 )

(3)

However, we observe that U 6 d Z:
a. X1 is not a witness due to X4  X1 and Equation (3).
b. X2 is not a witness due to X3  X2 and Equation (2).
c. X3 is not a witness due to Equation (2).
d. X4 is not a witness due to Equation (3).
The above proposition shows that the dominance relation d is not transitive when
i and  are arbitrary partial orders, when considering worst-frontier based aggregation.
Because transitivity of preference is a necessary condition for rational choice (von Neumann
227

fiSanthanam, Basu & Honavar

& Morgenstern, 1944; French, 1986; Mas-Colell et al., 1995), we proceed to investigate the
possibility of obtaining such a dominance relation by restricting . We later prove that
such a restriction is necessary and sufficient for the transitivity of d .
Definition 12 (Relative Importance as an Interval Order). A relative importance relation
 is a binary relation which is reflexive and satisfies the following axiom.
Xi , Xj , Xk , Xl  X : (Xi  Xj  Xk  Xl )  (Xi  Xl  Xk  Xj )

(4)

We say that Xi is relatively more important than Xj if Xi  Xj .
Proposition 8 (Transitivity of  see Fishburn, 1985).  is transitive.

Remarks.
1. Definition 12 imposes an additional restriction on the structure of the relative importance relation , over a strict partial order. A strict partial order is just irreflexive
and transitive; however, the relative importance relation in Definition 12 should in
addition satisfy Equation (4), thereby yielding an interval order (Fishburn, 1985).
2. The indifference relation with respect to , namely  is not transitive. For example,
if there are three attributes X = {X1 , X2 , X3 }, and  = {(X1 , X2 )}.  satisfies the
condition for an interval order, and we have X1  X3 and X3  X2 , but X1 6 X2
because X1  X2 .
Propositions 9-12 establish the properties of the dominance relation d in the case
where the relative importance relation  is an interval order. In particular, we prove that
d is irreflexive (Proposition 9) and transitive (Proposition 12), making d a strict partial
order (Theorem 1).
Proposition 9 (Irreflexivity of d ). U 

m
Y

F (Xi )  U 6 d U.

i=1

Proof. Suppose that U d U by contradiction. Then Xi , s.t. U(Xi ) i U(Xi ) by definition.
But this contradicts Proposition 3.
The above proposition ensures that the dominance relation d is strict over compositions. In other words, no composition is preferred over itself. Next, we proceed to establish
the other important property of rational preference relations: transitivity of d . We make
use of two intermediate propositions 10 and 11 that are needed for the task.
In Proposition 10, we prove that if an attribute Xi is relatively more important than
Xj , then Xi is not more important than a third attribute Xk implies that Xj is also not
more important than Xk . This will help us prove the transitivity of the dominance relation.
Figure 4 illustrates the cases that arise.
Proposition
 10. Xi , Xj , Xk :

Xi  Xj  (Xk  Xi  Xk  Xi )  (Xk  Xj  Xk  Xj )
228

fiRepresenting and Reasoning with Qualitative Preferences

Xk  Xi

Xk  Xi
Xk

Xi

Xi

Xj

Xk

Xk

Xi
Xj

Xj
(a)

(b)

(c)

Figure 4: Xi  Xj  (Xk  Xi  Xk  Xi )
The proof follows from the fact that  is a partial order.
Proof.
1. Xi  Xj

(Hyp.)

2. Xk  Xi  Xk  Xi

(Hyp.) Show Xk  Xj  Xk  Xj

2.1. Xk  Xi  Xk  Xj By transitivity of  and (1.); see Figure 4(a)
2.2. Xk  Xi  Xk  Xj  Xk  Xj
i. Xk  Xi (Hyp.)
ii. (Xk  Xj )  (Xj  Xk )  (Xk  Xj ) Always; see Figure 4(b,c)
iii. Xj  Xk  Xi  Xk (1.) Contradiction!
iv. Xk  Xj  Xk  Xj (2.2.ii., iii.)


3. Xi  Xj  (Xk  Xi  Xk  Xi )  (Xk  Xj  Xk  Xj )
(1., 2.1, 2.2)

Proposition 11 states that if attributes Xi , Xj are such that Xi  Xj then at least
one of them, Xu is such that with respect to the other, Xv , there is no attribute Xk that
is less important while at the same time Xk  Xu . This result is needed to establish the
transitivity of the dominance relation.
Proposition 11. Xi , Xj , u 6= v, Xi  Xj  Xu , Xv  {Xi , Xj }, Xk : (Xu  Xk 
Xv  Xk )
The proof makes use of the fact that relative importance is an interval order relation.
Proof. Let Xi  Xj , and Xi and Xj be attributes that are less important than Xi and Xj
respectively (if any). Figure 5 illustrates all the cases. Figure 5(a, b, c, d, e) illustrates the
cases when at most one of Xi and Xj exists, and in each case the claim holds trivially. For
example, in the cases of Figure 5(a, b, c), both Xu = Xi ; Xv = Xj and Xu = Xj ; Xv = Xi
satisfy the implication, and in the cases of Figure 5(d, e), the corresponding satisfactory
assignments to Xu and Xv are shown in the figure. The case of Figure 5(f) never arises
because  is an interval order (see Definition 12). Hence, the proposition holds in all
cases.
229

fiSanthanam, Basu & Honavar

Xi

Xj

Xi

(a)
Xi

Xj

Xi

Xj

Xi

(b)

Xj

Xi

Xj

Xi

Xj

(c)

Xj

Xu = Xj
Xv = Xi

Xu = Xi
Xv = Xj

(d)

(e)

Xi

Xj

Xi

Xj

Contradiction!
( is an interval order )
(f)

Figure 5: Xi  Xj
The above proposition reflects the interval order property of the  relation, and it
complements the result of Proposition 7, where d was shown to be intransitive when 
is not an interval order. In fact, if relative importance was defined as a strict partial order
instead, the above proof does not hold. Given that U d V with witness Xi and V d Z with
witness Xj , the above proposition guarantees that one among Xi and Xj can be chosen as
a potential witness for U d Z so that the conditions demonstrated in the counter example
of Proposition 7 are avoided. Using the propositions 10 and 11, we are now in a position to
prove transitivity of d in Proposition 12.
Proposition 12 (Transitivity of d ).  U, V, Z 

m
Y

F (Xi ),

i=1

U d V  V d Z  U d Z.

The proof proceeds by considering all possible relationships between Xi , Xj , the respective attributes that are witnesses of the dominance of U over V and V over Z. Lines 5, 6, 7 in
the proof establish the dominance of U over Z in the cases Xi  Xj , Xj  Xi and Xi  Xj
respectively. In the first two cases, the more important attribute among Xi and Xj is shown
to be the witness for U d Z with the help of Proposition 10; and in the last case we make
use of Proposition 11 to show that at least one of Xi , Xj is a witness for U d Z.
Proof.
1. U d V

(Hyp.)

2. V d Z

(Hyp.)

3. Xi : U(Xi ) i V(Xi ) (1.)
4. Xj : V(Xj ) j Z(Xj ) (2.)
Three cases arise: Xi  Xj (5.), Xj  Xi (6.) and Xi  Xj (7.).
230

fiRepresenting and Reasoning with Qualitative Preferences

5. Xi  Xj  U d Z
5.1.
5.2.
5.3.
5.4.

Xi  Xj (Hyp.)
V(Xi ) i Z(Xi ) (2., 5.1.)
U(Xi ) i Z(Xi ) (3., 5.2.)
Xk : (Xk  Xi  Xk  Xi )  U(Xk ) k Z(Xk )
i. Let Xk  Xi  Xk  Xi (Hyp.)
ii. U(Xk ) k V(Xk ) (1., 5.4.i.)
iii. Xk  Xj  Xk  Xj (5.4.i., P roposition 10)
iv. V(Xk ) k Z(Xk ) (2., 5.4.iii.)
v. U(Xk ) k Z(Xk ) (5.4.ii., 5.4.iv.)
5.5. Xi  Xj  U d Z (5.1., 5.3., 5.4.)
6. Xj  Xi  U d Z
6.1. This is true by symmetry of Xi , Xj in the proof of (5.); in this case, it can easily be
shown that U(Xj ) i Z(Xj ) and Xk : (Xk Xj Xk  Xj )  U(Xk ) k Z(Xk ).
7. Xi  Xj  U d Z
Xi  Xj (Hyp.)
Xu , Xv  {Xi , Xj } : Xu 6= Xv Xk : (Xu  Xk Xv Xk ) (7.1., P roposition 11)
Without loss of generality, suppose that Xu = Xi , Xv = Xj (Hyp.).
V(Xi ) i Z(Xi ) (2., 7.1.)
U(Xi ) i Z(Xi ) (3., 7.4.)
Xk : Xk  Xi  U(Xk ) k Z(Xk ).
i. Xk  Xi (Hyp.)
ii. U(Xk ) k V(Xk ) (1., 7.6.i.)
iii. Xk  Xj  Xk  Xj Because Xj  Xk Contradicts (7.1., 7.6.i.)!
iv. V(Xk ) k Z(Xk ) (2., 7.6.iii.)
v. U(Xk ) k Z(Xk ) (7.6.ii., 7.6.iv.)
7.7. Xk : Xk  Xi  U(Xk ) k Z(Xk )
i. Xk  Xi (Hyp.)
ii. U(Xk ) k V(Xk ) (1., 7.7.i.)
iii. Xk  Xj  Xk  Xj Because Xj  Xk Contradicts (7.2., 7.3.)!
iv. V(Xk ) k Z(Xk ) (2., 7.7.iii.)
v. U(Xk ) k Z(Xk ) (7.7.ii., 7.7.iv.)
7.8. Xk : Xk  Xi  Xk  Xi  U(Xk ) k Z(Xk ) (7.6., 7.7.)
7.9. Xi  Xj  U d Z (7.5., 7.8.)
7.1.
7.2.
7.3.
7.4.
7.5.
7.6.

8. (Xi  Xj  Xj  Xi  Xi  Xj )  U d Z
9. U d V  V d Z  U d Z

(5., 6., 7.)

(1., 2., 8.)

Theorem 1. If the intra-attribute preferences i are arbitrary strict partial orders and
relative importance  is an interval order, then d is a strict partial order.
Proof. Follows immediately from Propositions 9 and 12.
231

fiSanthanam, Basu & Honavar

Xi

Xj

Xi

Xj

Figure 6: A 2  2 substructure, not an Interval Order
3.5 Role of Interval Order Restriction on  in the Transitivity of d
Theorem 1 establishes that given partially ordered intra-attribute preferences i , if the
relative importance relation () is an interval order (Definition 12), then d is transitive.
In addition, we have also seen a counter example in Proposition 7, which shows that the
transitivity of d does not necessarily hold when  is an arbitrary partial order.
Is there a condition weaker than the interval order restriction that still makes d
transitive when retain intra-attribute preferences as arbitrary partial orders and dominance
as in Definition 11? The answer turns out to be no, which we prove next.
Before we proceed to prove the necessity of an interval ordered relative importance
relation  for a transitive dominance relation d , we will examine interval orders more
closely. Recall from Definition 12 that every interval order  on X is a partial order, and
it additionally satisfies Ferrers axiom for all X1 , X2 , X3 , X4  X :
(X1  X2  X3  X4 )  (X1  X4  X3  X2 )
We borrow a characterization of the above axiom by Fishburn (1970a, 1985) that the
relation  is an interval order if and only if 2  2 * , where 2  2 is a relational structure
shown in Figure 6. In other words, a partial order is an interval order if and only if it has
no restriction of itself that is isomorphic to the partial order structure shown in Figure 6.
Theorem 2 (Necessity of Interval Order). For partially ordered intra-attribute preferences
and dominance relation in Definition 11, d is transitive only if relative importance  is
an interval order.
Proof. Assume that  is not an interval order. This is true if and only if 2  2  .
However, we showed in Proposition 7 that in this case, d is not transitive using a counter
example (see Figure 3). Hence, d is transitive only if relative importance  is an interval
order.
3.6 Additional Properties of d with Respect to the Properties of {i } and 
We now present some additional properties10 of d that hold when certain restrictions are
imposed on the intra-attribute and relative importance preference relations.
Proposition 13. If  is a total order and Xi is the most important attribute in X with
respect to , then i  d .
10. The results in this section essentially prove conjectures that arose out of analysis of the results of our
experiments (see Section 6).

232

fiRepresenting and Reasoning with Qualitative Preferences

Proof. Let Xi be the (unique) most important attribute in X . Suppose that U(Xi ) i V(Xi ),
thereby making Xi a potential witness for U d V. Since Xi is the most important attribute,
Xk  X : Xi  Xk , the second clause in the definition of U d V trivially holds. Hence, Xi
is a witness for U d V (see Definition 11).
Note that the proof of the above proposition only made use of the fact that Xk 
X : Xi  Xk , which is a weaker condition than  being a total order. Hence, we have the
following more general result.
Proposition 14. If  is such that there is a unique most important attribute Xi , i.e.,
Xi  X : Xk  X \ {Xi } : Xi  Xk , then i  d .
We proceed to prove an important result that gives conditions under which d is a
weak order.
Theorem 3. When the aggregation function i is defined as in Definition 8, if  as well
as {i } are total orders, then d is a weak order.
Proof. d is a weak order if and only if it is a strict partial order and negatively transitive.
We have already shown that d is a strict partial order in Theorem 1, and hence we are
only left with proving that d is negatively transitive, i.e., U 6 d V  V 6 d Z  U 6 d Z.
First, we note that since i is a total order, i is also a total order (see Definition 8).
U 6 d V  (Xi : U(Xi ) i V(Xi )  Xk : (Xk  Xi  U(Xk ) 6  k V(Xk ))) (Xk  Xi
is not possible because  is a total order).
(1)
Let Xi and Xj be the most important attributes s.t. U(Xi ) i V(Xi ) and V(Xj ) j Z(Xj )
respectively.
(2)

Let Xp and Xq be the most important attributes s.t. Xp  Xi  U(Xp ) 6  p V(Xp ) and
Xq  Xj  V(Xq ) 6  q Z(Xq ) respectively (such Xp and Xq must exist by (1)).
(3)
Case 1 Both Xi and Xj as defined in (2) exist (cases when such Xi and/or Xj dont exist
will be dealt with separately).
Three sub-cases arise: Xp  Xq , Xq  Xp and Xp = Xq .
Case 1a: Suppose that Xp  Xq (see Figure 7).
(4)
 From (3) we know that Xp  Xi  U(Xp ) 6  p V(Xp ), i.e., V(Xp ) p U(Xp ).

(5)

 From (3) and (4) we know that V(Xp ) p Z(Xp ), because Xq is the most important
attribute that is also more important than Xj and V(Xq ) 6  q Z(Xq ), and Xp is more
important than Xq (and hence Xj as well).
(6)
 But because Xj is the most important attribute with V(Xj ) j Z(Xj ), and Xp  Xj
(since Xq  Xj and Xp  Xq ), we have V(Xp ) 6  p Z(Xp ) (as Xj is the most important
attribute with V(Xj ) j Z(Xj ), using (2)). Along with (6), this means that V(Xp ) =
Z(Xp ).
(7)
 From (5) and (7), Z(Xp ) p U(Xp ).

(8)

 Also, Xk : Xk  Xp  U(Xk ) = V(Xk )  V(Xk ) = Z(Xk ) (because Xk is more
important than Xi , Xj and Xp , Xq ).
(9)
233

fiSanthanam, Basu & Honavar

U (Xk ) = V(Xk )

V(Xk ) = Z(Xk )
Xp

U (Xi) i V(Xi )
Xq
V(Xj ) j Z(Xj )

Figure 7: The case when Xp  Xq
 From (8) and (9), Z d U with Xp as witness. Hence, U 6 d Z.
Case 1b: Suppose that Xq  Xp . The claim holds by symmetry.
Case 1c: Suppose that Xp = Xq .
 From (3) we know that Xp  Xi  U(Xp ) 6  p V(Xp ), i.e., V(Xp ) p U(Xp ).
 Similarly, Z(Xp ) p V(Xp ).
 Hence, Z(Xp ) p U(Xp ). Moreover, Xk : Xk  Xp  U(Xk ) = V(Xk )  V(Xk ) =
Z(Xk ) (because Xk is more important than Xi , Xj and Xp , Xq ).
 Therefore, Z d U with Xp as witness. Hence, U 6 d Z.
Case 2 : If Xi (say) does not exist, then Xi : U(Xi ) 6  i V(Xi ). Let Xp be the most
important attribute s.t. V(Xp ) p U(Xp ) (if Xp does not exist, then trivially U 6 d Z
because U = V).
(10)
Case 2a: Suppose Xp  Xq . Then Xk : Xk  Xp  V(Xk ) = Z(Xk ) (because Xk  Xq
as well). Moreover, Xp  Xq  V(Xp ) = Z(Xp ). Hence, Z d U with Xp as witness and
therefore U 6 d Z.
Case 2b: Suppose Xq  Xp . Then Xk : Xk  Xq  U(Xk ) = V(Xk ) (because Xk  Xp
as well). Moreover, Xq  Xp  U(Xq ) = V(Xq ). Hence, Z d U with Xq as the witness and
therefore U 6 d Z.
Case 2c: Suppose Xp = Xq . Then Xk : Xk Xp  V(Xk ) = Z(Xk ) (because Xk Xq as
well) and similarly Xk : Xk  Xq  U(Xk ) = V(Xk ) (because Xk  Xp as well). Moreover,
since V(Xq ) 6  q Z(Xq ) (by (3)), V(Xp ) p U(Xp ) (using (10)) we have Z(Xp ) p U(Xp ).
Hence, Z d U with Xp as the witness and therefore U 6 d Z.
Case 3

: If Xj (say) does not exist, the proof is symmetric to Case 2.

Case 4 : Suppose that both Xi and Xj do not exist. Then, for any attribute Xi ,
V(Xi ) i U(Xi ) and Z(Xi ) i V(Xi ), i.e., Xi : Z(Xi ) i U(Xi ). Hence, there is no witness
for U d Z, or U 6 d Z.
Cases 1 - 4 are exhaustive, and in each case U 6 d Z. This completes the proof.
234

fiRepresenting and Reasoning with Qualitative Preferences

We further conjecture that d is a weak order when {i } are total orders and  is an
arbitrary interval order (i.e., under conditions that are more general than the conditions of
Theorem 3). We leave this as an open problem.
Conjecture 1. If {i } are total orders and  is an arbitrary interval order, then d is a
weak order.
Remark.
As stated, Conjecture 1 and Theorem 3 apply whenever {i } are totally ordered, and when
using our method of comparing two aggregated valuations ( i ) (see Definition 8). More
generally, we note that they hold whenever { i } are total orders, regardless of the chosen
method of comparing two aggregated valuations, and regardless of the properties of the
input intra-attribute preferences {i }. For example, suppose that {i } are ranked weak
orders (i.e., not total orders). As such, Conjecture 1 and Theorem 3 do not apply. However,
for each attribute Xi if we define i (S) to be the rank number corresponding to the worst
frontier of S, and i as the natural total order over the ranks in the weak order, then the
consequences of Conjecture 1 and Theorem 3 hold.
We summarize the theoretical results relating the properties of the dominance relation
and the properties of the preference relations  and { i } in Table 5.


i

d

Remarks

io
io
to

po
to
to

po
wo
wo

Theorem 1
Conjecture 1
Theorem 3

Table 5: Summary of results and conjectures relating to the properties of d with respect
to the properties of  and { i }.

3.7 Choosing the Most Preferred Solutions
Given a set C = {Ci } of compositions and a preference relation  (e.g., d ) that allows
us to compare any pair of compositions, the problem is to find the most preferred composition(s). When the preference relations are totally ordered (e.g., a ranking) over a set of
alternative solutions, rationality of choice suggests ordering the alternatives with respect
to the complete preference and choosing the best alternative, i.e., the one that ranks the
highest. However, when the preference relation is a strict partial order, e.g., in the case of
d , not every pair of solutions (compositions) may be comparable. Therefore, a solution
that is the most preferred with respect to the preference relation may not exist. Hence, we
use the notion of the non-dominated set of solutions defined as follows.
Definition 13 (Non-dominated Set). The non-dominated set of elements (alternatives or
solutions or compositions) of a set C with respect to a (partially ordered) preference relation
 (e.g., d ), denoted  (C), is a subset of C such that none of the elements in S are
preferred to any element in  (C).
235

fiSanthanam, Basu & Honavar

 (C) = {Ci  C|Cj  C : Cj  Ci }
Note that as per this definition,  (C) is the maximal set of elements in C with respect to
the relation . It is also easy to observe that C 6=    (C) 6= .

4. Algorithms for Computing the Most Preferred Compositions
We now turn to the problem of identifying from a set of feasible compositions (that satisfy
a pre-specified functionality ()), the most preferred subset, i.e., the non-dominated set.
4.1 Computing the Maximal/Minimal Subset with Respect to a Partial Order
The straightforward way of computing the maximal (non-dominated) elements in a set
S of n elements with respect to any preference relation  is the following algorithm: For
each element si  S, check if sj  S : sj  si , and if not, si is in the non-dominated
set. This simple compare all pairs and delete dominated approach involves computing
dominance with respect to  O(n2 ) times.
Recently Daskalakis, Karp, Mossel, Riesenfeld and Verbin (2009) provided an algorithm
that performs at most O(wn) pairwise comparisons to compute the maximal elements of a
set S with respect to a partial order , where n = |S| and w is the width of the partial
order  on S (the size of the maximal set of pairwise incomparable elements in S with
respect to ). The algorithm presented by Daskalakis et al. finds the minimal elements;
the corresponding algorithm for finding the maximal elements is as follows.
Let T0 = . Let the elements of the set S be x1 , x2 ,    xn . At step t( 1):
 Compare xt to all elements in Tt1 .
 If there exists some a  Tt1 such that a  xt , do nothing.
 Otherwise, remove from Tt1 all elements a such that xt  a and put xt into Tt .
On termination, the set Tn contains all the maximal elements in S, i.e., non-dominated
subset of S with respect to . We make use of the above algorithm to compute the nondominated (maximal) subsets (namely,  ()), and the original version of the algorithm
given in by Daskalakis (2009) to compute the worst-frontiers (minimal subsets).
4.2 Algorithms for Finding the Most Preferred Feasible Compositions
We proceed to develop algorithms for finding the most preferred feasible compositions, given
a compositional system hR, , |=i consisting of a repository R of pre-existing components,
a user specified functionality , user preferences {i } and  and a functional composition
algorithm f . We analyze the properties of the algorithms with respect to the worst-frontier
based aggregation (see Definition 6).
Definition 14 (Soundness and Completeness). An algorithm A that, given a set C of
feasible compositions, computes a set of feasible compositions SA   d (C) is said to be
sound with respect to C. Such an algorithm is complete with respect to C if SA   d (C).
236

fiRepresenting and Reasoning with Qualitative Preferences

Algorithm 1 ComposeAndFilter(, f, )
1. Find the set C of feasible compositions w.r.t.  using f
2. return  (C)
Given a compositional system hR, , |=i consisting of a repository R of pre-existing components, and a user specified functionality , the most straightforward approach to finding
the most preferred feasible compositions involves: (a) computing the set C of functionally
feasible compositions using a functional composition algorithm f , and (b) choosing the
non-dominated set according to preferences over non-functional attributes.
Algorithm 1 follows this simple approach to produce the set  d (C) of all non-dominated
feasible compositions, when invoked with the preference relation d , the functional composition algorithm f and the desired functionality .  d (C) can be computed using the
procedure described in Section 4.1. Algorithm 1 is both sound and complete with respect
to C.
4.3 A Sound and Weakly Complete Algorithm
Note that in the worst case, Algorithm 1 evaluates the dominance relation d between
all possible pairs of feasible compositions C. However, this can be avoided if we settle for
a non-empty subset of  d (C). Note that every solution in such a subset is guaranteed
to be optimal with respect to user preferences d . We introduce the notion of weak
completeness to describe an algorithm that computes a set of feasible compositions, at least
one of which is non-dominated with respect to d .
Definition 15 (Weak Completeness). An algorithm A that, given a set C of feasible compositions, computes a set SA of feasible compositions is said to be weakly complete with respect
to C if  d (C) 6=   SA   d (C) 6= .
We now proceed to describe a sound and weakly complete algorithm, i.e., one that computes a non-empty subset of  d (C). The algorithm is based on the following observation:
Solutions that are non-dominated with respect to each of the relatively most-important
attributes are guaranteed to include some solutions that are non-dominated overall with
respect to d as well. Hence, the solutions that are most preferred with respect to each
such attribute can be used to compute a non-empty subset of  d (C). We proceed by
considering solutions that are most preferred with respect to an attribute Xi .
Definition 16 (Non-dominated solutions w.r.t. attributes). The set  i (C) of solutions
that are non-dominated with respect to an attribute Xi is defined as
 i (C) = {U | U  C  V  C : V(Xi ) i U(Xi )}.
Let I  X be the set of most important attributes with respect to , i.e., I = (X ) =
{Xi |Xj  X : Xj  Xi }. Clearly, I 6=  because there always exists a non-empty maximal
set of elements in the partial order . The following proposition states that for every Xi  I,
at least one of the solutions in  i (C) is also contained in  d (C).
Proposition 15. Xi  I :  d (C) 6=    i (C)   d (C) 6=  (See Appendix A for a
proof ).
237

fiSanthanam, Basu & Honavar

Algorithm 2 constructs a subset of  d (C), using the sets { i (C) | Xi  I}. First, the
algorithm computes the set I of most important attributes in X with respect to  (Line 2).
The algorithm iteratively computes  i (C) for each Xi  I (Lines 3, 4), identifies the subset
of solutions that are non-dominated with respect to d in each case, and combines them
to obtain    d (C).
Algorithm 2 WeaklyCompleteCompose({i | Xi  X }, , f, )
1.   
2. I  (X ) = {Xi | Xj : Xj  Xi }
3. for all Xi  I do
4.
 i (C)  ComposeAndFilter( i , f, )
5.
     d ( i (C))
6. end for
7. return 
Theorem 4 (Soundness and Weak Completeness of Algorithm 2). Given a set of attributes
X , preference relations  and i , Algorithm 2 generates a set  of feasible compositions
such that    d (C) and  d (C) 6=    6=  (See Appendix A for a proof ).
In general, Algorithm 2 is not guaranteed to yield a complete set of solutions, i.e.,  6=
 d (C). The following example illustrates such a case.
Example. Consider a compositional system with two attributes X = {X1 , X2 }, with
domains {a1 , a2 , a3 } and {b1 , b2 , b3 } respectively. Let their intra-attribute preferences be
total orders: a1 1 a2 1 a3 and b1 2 b2 2 b3 respectively, and let both attributes be
equally important ( = ). Suppose the user-specified goal  is satisfied by three feasible
compositions C1 , C2 , C3 with valuations VC1 = h{a1 }, {b3 }i, VC2 = h{a3 }, {b1 }i and VC3 =
h{a2 }, {b2 }i respectively. Given the above preferences,  1 (C) = {C1 } and  2 (C) = {C2 }.
Thus,  = {C1 , C2 } However,  d (C) = {C1 , C2 , C3 } =
6 .

The above example shows that the most preferred valuation for one attribute (e.g., X1 )
can result in poor valuations for one or more other attributes (e.g., X2 ). Algorithm 2 may
thus leave out solutions like C3 that are not most preferred with respect to any one i ,
but nevertheless may correspond to a good compromise when we consider multiple most
important attributes. It is a natural question to ask what are the minimal conditions
under which Algorithm 2 is complete. A related question is whether Algorithm 2 can be
guaranteed to produce a certain minimum number of non-dominated solutions (||) under
some specific set of conditions. Note that in general, the cardinality of  depends not only
on the user preferences i , , but also on the user specified functionality  which together
with the repository R determines the set C of feasible compositions. However, in the special
case when  specifies a single attribute Xt that is relatively more important than all other
attributes, we can show that Algorithm 2 is complete.
Proposition 16. If I = {Xt }  Xk 6= Xt  X : Xt  Xk , then  d (C)  , i.e.,
Algorithm 2 is complete (See Appendix A for a proof ).
It remains to be seen what are all the necessary and sufficient conditions for ensuring
the completeness of Algorithm 2, and we plan to address this problem in the future.
238

fiRepresenting and Reasoning with Qualitative Preferences

4.4 Optimizing with Respect to One of the Most Important Attributes
As we will see in Section 5, Algorithm 2 has a high worst case complexity, especially if
the set I of most important attributes is large. This is due to the fact that for each
most important attribute Xi  I, the algorithm computes the non-dominated set over
the feasible compositions with respect to i first, and then with respect to d , i.e.,  
 d ( i (C)) (Line 4). The computation of the non-dominated set with respect to d ,
although expensive, is crucial to ensuring the soundness of Algorithm 2.
While soundness is a desirable property, there may be settings requiring faster computation of feasible compositions, where it may be acceptable to obtain a set S of feasible
compositions that contains at least one (whenever there exists one) of the most preferred
feasible compositions (one that is non-dominated by any other feasible composition with
respect to d ). In such a case, it might be useful to have an algorithm with lower complexity that finds a set of feasible compositions of which at least one is most preferred (i.e.,
weakly complete), as opposed to one with a higher complexity that finds a set of feasible
compositions all of which are most preferred (i.e., sound).
Algorithm 3 AttWeaklyCompleteCompose({i | Xi  X }, , f, )
1. I  (X ) = {Xi | Xj : Xj  Xi }
2. for some Xi  I
3.    i (C) = ComposeAndFilter( i , f, )
4. return 
We consider one such modification of Algorithm 2, namely Algorithm 3, that arbitrarily
picks one of the most important attributes Xi  I (as opposed to the entire set I as in
Algorithm 2) and finds the set of all feasible compositions that are non-dominated with
respect to i , i.e.,  =  i (C) for Algorithm 3.
The weak completeness of Algorithm 3 follows directly from Proposition 15. In the
following example, however, we show that some of the feasible compositions produced by
Algorithm 3 may be dominated by some other feasible composition with respect to d ,
i.e., Algorithm 3 is not sound.
Example. Consider a compositional system with two attributes X = {X1 , X2 }, with
domains {a1 , a2 } and {b1 , b2 } respectively. Let their intra-attribute preferences be: a1 1 a2
and b1 2 b2 respectively, and let both attributes by equally important ( = ; I =
{X1 , X2 }). Suppose the user-specified goal  is satisfied by three feasible compositions
C1 , C2 , C3 with valuations VC1 = h{a1 }, {b1 }i, VC2 = h{a2 }, {b1 }i and VC3 = h{a1 }, {b2 }i
respectively. Given the above preferences, if we choose to maximize the preference with
respect to attribute X1  I, then  =  1 (C) = {C1 , C3 }. If we choose X2  I instead, we
get  =  2 (C) = {C1 , C2 }. However, in any case  d (C) = {C1 } =
6 .

The following proposition gives a condition under which Algorithm 3 is complete.
Proposition 17. If |I| = 1, i.e., there is a unique most important attribute with respect to
, then Algorithm 3 is complete (See Appendix A for a proof ).
239

fiSanthanam, Basu & Honavar

Algorithm 4 InterleaveCompose(L, , f, )
1. if L =  then
2.
return 
3. end if
4.  =  (L)
5.   = 
6. for all C   do
7.
if C 6|=  then
8.
  =    f (C)
9.
else
10.
  =    {C}
11.
end if
12. end for
13. if   =  then
14.
return 
15. else
16.
InterleaveCompose((L \ )    , , f, )
17. end if
4.5 Interleaving Functional Composition with Preferential Optimization
Algorithms 1, 2 and 3 identify the most preferred feasible compositions using the two
step approach: (a) find the feasible compositions C; and (b) compute a subset of C that is
preferred with respect to the user preferences. We now develop an algorithm that eliminates
some of the intermediate partial feasible compositions from consideration based on the user
preferences. This is particularly useful in settings (such as when |C| is large relative to
| d (C)|), where it might be more efficient to compute only a subset of C that are likely
(based on i and ) to be in  d (C).
Algorithm 4 requires that the functional composition algorithm f is incremental (see
Definition 2), i.e., that it produces a set f (C) of functionally feasible extensions given any
existing partial feasible composition C. At each step, Algorithm 4 chooses a subset of
the feasible extensions produced by applying f on all the non-dominated partial feasible
compositions, based on the user preferences. Algorithm 4 computes the non-dominated
set of feasible compositions by interleaving the execution of the incremental functional
composition algorithm f with the ordering of partial solutions with respect to preferences
over non-functional attributes.
Algorithm 4 is initially invoked using the parameters L = () 11 , d , the functional
composition algorithm f and . The algorithm maintains at each step a list L of partial
feasible compositions under consideration. If L is empty at any step, i.e., there are no
more partial feasible compositions to be explored, then the algorithm terminates with no
11. It is not necessary to invoke the algorithm with L = () (i.e., only  in the list L) initially. There
may be functional composition algorithms that begin with an non-empty composition C and proceed to
obtain a feasible composition by iteratively altering C. For instance, one could think of randomized or
evolutionary algorithms that begin with a random, non-empty composition which is somehow repeatedly
improved during the course of composition.

240

fiRepresenting and Reasoning with Qualitative Preferences

solution (Lines 1  3); otherwise it selects from L, the subset  that is non-dominated with
respect to some preference relation  (Line 4). If all the partial feasible compositions in
 are also feasible compositions, then the algorithm outputs  and terminates (Lines 13 
14). Otherwise, it replaces the partial feasible compositions in  that are not feasible
compositions, with their one-step extensions (Lines 7  8). The algorithm continues to
recurse (Line 16), at each iteration updating the dominated set by replacing  with   until
there are no changes in the dominated set i.e.,  =   . Note that it is not possible to
eliminate the dominated compositions (L \ ) at this stage because some of their extensions
(in a later iteration) could result in non-dominated compositions.
Proposition 18 (Termination of Algorithm 4). Given a finite repository of components,
Algorithm 4 terminates in a finite number of steps (See Appendix A for a proof ).
We next investigate the soundness, weak-completeness and completeness properties of Algorithm 4. Proposition 19 states that the algorithm is in general not sound with respect to
C, i.e., it is not guaranteed to produce feasible compositions that are non-dominated with
respect to d . However, this does not discount the usefulness of the algorithm, as we will
show that it is sound under some other assumptions (see Theorem 5).
Proposition 19 (Unsoundness of Algorithm 4). Given a functional composition algorithm
f and user preferences i and  over a set of attributes X , Algorithm 4 is not guaranteed
to generate a set of feasible compositions  such that    d (C) (See Appendix A for a
proof ).
This result implies that in general, not all feasible compositions returned by Algorithm 4
() are in  d (C). The example shown in Figure 8 illustrates this problem. At the time
of termination, there may exist some partial feasible composition B in the list L that is
dominated by some feasible composition E in ; however, it may be possible to extend B to
a feasible composition B  W that dominates one of the compositions F in  (as illustrated
by the counter example in the proof, see Appendix A). In other words, VE d VB , VF d VE ,
VF d VB and VBW d VF .

... E F ...

...

B

...



B W

Figure 8: The case when Algorithm 4 is not sound

Although Algorithm 4 is not sound in general, we show that it is sound when the d
relation is an interval order (as opposed to an arbitrary partial order).
Theorem 5 (Soundness of Algorithm 4). If d is an interval order, then given a functional composition algorithm f and user preferences { i },  over a set of attributes X ,
241

fiSanthanam, Basu & Honavar

Algorithm 4 generates a set  of feasible compositions such that    d (C) (See Appendix
A for a proof ).
Because Theorem 5 requires d to be an interval order, an important question arises:
What are the conditions under which d an interval order? Theorem 3 (see Section 3.6)
gives us one such condition when d is a weak order (i.e., also an interval order). The next
two theorems give conditions under which Algorithm 4 is weakly complete and complete
respectively.
Theorem 6 (Weak Completeness of Algorithm 4). If d is an interval order, then given a
functional composition algorithm f and user preferences { i },  over a set of attributes X ,
Algorithm 4 produces a set  of feasible compositions such that  d (C) 6=    d (C) 6=
 (See Appendix A for a proof ).
Theorem 7 (Completeness of Algorithm 4). If d is a weak order, then given a functional composition algorithm f and user preferences { i },  over a set of attributes X ,
Algorithm 4 generates a set  of feasible compositions such that  d (C)   (See Appendix
A for a proof ).

Remark. The above algorithm does not explore feasible compositions that can be generated by extending other feasible compositions (by the condition in Line 7). Proposition
6 shows that when worst-frontier based aggregation is used, extending a feasible composition cannot yield a more preferred feasible composition. This guarantees the soundness
of Algorithm 4 (Theorem 5). However, when other aggregation schemes are used, it might
be the case that a feasible extension of a feasible composition is more preferred, in which
case, in order to ensure the soundness of Algorithm 4, Line 10 will have to be changed to
  =    {C}  f (C).
A summary of the conditions (in terms of the properties of the relative importance or
dominance preference) under which the algorithms are sound, complete and weak complete
are given in Table 6.
Algorithm

Sound

Weakly Complete

Complete

A1
A2
A3
A4

po
po

io

po
po
po
io

po
|I| = 1
|I| = 1
wo

Table 6: Properties of d or  for which the algorithms are sound, weakly complete and
complete. po stands for d being a partial order; io stands for d being an
interval order; wo stands for d being a weak order; and |I| = 1 is when  is such
that there is a unique most important attribute.  indicates that condition(s)
under which A3 is sound remains as an open problem.

242

fiRepresenting and Reasoning with Qualitative Preferences

5. Complexity
In this section, we study the complexity of dominance testing (evaluating d , see Section 3.3) as well as the complexity of the algorithms for computing the non-dominated set
of feasible compositions (see Section 4). We express the worst case time complexity of
dominance testing in terms of the size of user specified intra-attribute, relative importance
preference relations and the attribute domains (see Table 7).
Relation / Set

Symbol

Cardinality

Remarks

Attributes
Domain of Attributes
Intra-attribute preferences
Intra-attribute preferences
Relative Importance
Relative Importance
Most Important Attributes
Repository
Feasible Compositions
Dominance Relation

X
Di
i
i


I
R
C
d

m
n
wint
kint
wrel
krel
mI
r
c
wdom

Number of attributes
Number of possible valuations of Xi
Width of the partial order i
Size of the relation i
Width of the partial order 
Size of the relation 
Number of most important attributes
Number of components in R
Number of feasible compositions
Width of the dominance relation

Table 7: Cardinalities of sets and relations
5.1 Computing the Maximal(Non-dominated)/Minimal Subset.
Let  be a partial order on the set S, with a width of w (size of the maximal set of elements
which are pairwise incomparable) and n = |S|. The algorithm due to Daskalakis et al.
discussed in Section 4.1 finds the maximal or minimal subset of S with respect to  within
O(wn) pairwise comparisons. Note that the maximum width of any partial order is w = n,
when = . Hence, in the worst case O(n2 ) comparisons are needed.
5.2 Complexity of Dominance Testing
Computing Worst Frontiers (i ). Let S  Di . Recall from Definition 6 that the worst
frontier of a set S with respect to an attribute Xi is i (S) := {v : v  S, u  S s.t. v i u},
i.e., the minimal set of elements in S with respect to the preference relation i . Using the
algorithm due to Daskalakis et al. to find the minimal set with respect to a partial order
(see above), the complexity of computing i (S) is O(nwint ).
Comparing Worst Frontiers ( i ). Let A, B  F (Xi ). As per Definition 8, A i B 
b  B, a  A : a i b. In the worst case, computing A i B would involve checking
whether a i b for each pair a, b, which would cost O(kint ). Hence, the complexity of
comparing the worst frontiers A and B is O(n2 kint ).
Dominance Testing ( d ).
U d V 

Recall from Definition 11 the definition of dominance:

Xi : U(Xi ) i V(Xi ) 
Xk , (Xk  Xi  Xk  Xi )  U(Xk ) k V(Xk )
243

fiSanthanam, Basu & Honavar

The complexity of dominance testing is the complexity of finding a witness attribute
in X for U d V. For each attribute Xi , the complexity of computing the first clause
in the conjunction of the definition of U d V is O(n2 kint ); and that of computing the
second clause is O m(n2 kint + krel ) , where O(krel ) and O(n2 kint ) are the complexities of
evaluating the left and right hand sides of the implication (respectively) for each
 Xk  X .
2k
2k
Hence, the complexity
of
dominance
testing
is
O
m
n
+
m(n
+
k
)
, or simply
int
int
rel

O m2 (n2 kint + krel ) . We will use the shorthand d to denote m2 (n2 kint + krel ).
5.3 Complexity of Algorithms

Each of the algorithms for computing the non-dominated feasible compositions (presented in
Section 4) makes use of a functional composition algorithm f to find feasible compositions.
Hence, the complexity analysis of the algorithms needs to incorporate of the complexity of
the functional composition algorithm as well.
Recall that Algorithms 1, 2 and 3 begin by computing the set of all feasible compositions
in a single shot using a functional composition algorithm as a black box, and then proceed
to find the most preferred among them. Algorithm 4 instead makes use of a functional
composition algorithm that produces the set of feasible compositions by iteratively extending partial feasible compositions. Specifically, it interleaves the execution of the functional
composition algorithm with the ordering of partial solutions with respect to preferences
over non-functional attributes.
We denote by O(fe ) and O(fg ) respectively, the complexity of computing the set of
feasible extensions of a partial feasible composition with respect to  and the complexity
of computing the set of all feasible compositions with respect to .
5.4 Complexity of Algorithm 1
The overall complexity for finding the set of all non-dominated feasible compositions is
O(fg +cwdom d), where O(d) is the complexity of evaluating d for any pair of compositions.
The first term fg accounts for Line 1 of the algorithm which computes the set of all feasible
compositions, and the term cwdom d corresponds to the computation of  d (C) as per the
algorithm given in Section 4.1.
5.5 Complexity of Algorithm 2
The complexity of identifying the most important attributes I with respect to  (Line 1)
is O(mwrel krel ). For each most important attribute Xi  I, Algorithm 2 (a) invokes Algorithm 1 using the derived intra-attribute preference i to compute  i (C); (b) identifies
the subset of  i (C) that is non-dominated with respect to d ; and (c) adds them to the set
of solutions. Hence,
the complexity of Algorithm 2 is O mwrel krel + mI (fg + cwdom n2 kint )+

2
mI | i (C)| d .
Since the feasible compositions with respect to any given  are fixed, by computing
the feasible compositions only once (during the first invocation of Algorithm 1 and storing them), the complexity of Algorithm 2 can be further reduced to O(fg + mwrel krel +
mI cwdom n2 kint + mI | i (C)|2 d).
244

fiRepresenting and Reasoning with Qualitative Preferences

5.6 Complexity of Algorithm 3
The complexity of identifying the most important attributes I with respect to  (Line 1) is
O(mwrel krel ). In contrast to Algorithm 2, Algorithm 3 invokes Algorithm 1 using the derived intra-attribute preference i to compute  i (C) for exactly one of the most important
attributes, Xi  I. Hence, the complexity of Algorithm 3 is O fg +mwrel krel +cwdom n2 kint ).
5.7 Complexity of Algorithm 4
We consider the worst case wherein the space of partial feasible compositions explored by
Algorithm 4 is a tree rooted at ; let b be its maximum branching factor (corresponding to
the maximum number of extensions produced by the functional composition algorithm), and
h its height (corresponding to the maximum number of components used in a composition
that satisfies ). In the worst case, in each iteration of Algorithm 4, every element of L,
the list of current partial feasible compositions, ends up in the non-dominated set .
Each level in the tree corresponds to one iteration of Algorithm 4, and at the lth iterl
ation, in the worst
 case there are b nodes in L. Hence, the complexity of the lth iteration
l
2
l
is O (b ) d + b fe , where the first term corresponds to computing the non-dominated set
among the current set of partial feasible compositions, and the second term corresponds to
computing the feasible extensions
partial feasible composition. Hence, the overall
Phof each
2l d + bl f )  O(b2h d + bh f ).
complexity of Algorithm 4 is O
(b
e
e
l=0
We further conducted experiments on the algorithms using simulated problem instances
to study how the algorithms perform in practice, which we describe next.

6. Experiments, Results & Analysis
We now describe the design and results of our experiments aimed at comparing the algorithms described in Section 4 with respect to the following attributes.
a) Quality of solutions produced by the algorithms. We measure the quality
of the solutions produced by the algorithms as follows. First, among all the most
preferred solutions that exist to the composition problem, we measure the fraction
that is produced by the algorithm. Second, among all the solutions produced by
the algorithm, we measure the fraction of solutions that are most preferred for the
composition problem.
b) Performance and efficiency of the algorithms. The performance of an algorithm
is measured in terms of response time (time taken to return the set of solutions), and
the efficiency is measured in terms of the number of times an algorithm invokes the
functional composition algorithm.
6.1 Experimental Setup
We now describe the data structure used to model the search space of compositions and the
simulation parameters used to generate the compositions in our experiments.
245

fiSanthanam, Basu & Honavar

6.1.1 Modeling the Search Space of Compositions using Recursive Trees
The uniform recursive tree (Smythe & Mahmoud, 1995) serves as a good choice to model
the search space of partial compositions and their feasible extensions. A tree with n vertices
labeled by 1, 2, . . . n is a recursive tree if the node labeled 1 is distinguished as the root,
and k : 2  k  n, the labels of the nodes in the unique path from the root to the node
labeled with k form an increasing sequence. A uniform recursive tree of n nodes (denoted
U RT ree(n)) is one that is chosen with equal probability from the space of all such trees.
A simple growth rule can be used to generate a uniform random recursive tree of n
nodes, given such a tree of n  1 nodes: Given U RT ree(n  1), choose uniformly at random
a node in U RT ree(n  1), and add a node labeled n with the randomly chosen node as
parent to obtain U RT ree(n). The properties of this class of uniform random recursive trees
are well studied in the literature of random data structures (see Smythe & Mahmoud, 1995,
for a survey).
The rationale behind choosing the uniform recursive tree data structure to model the
search space of our problem is that the growth rule that generates the recursive tree is similar
in intuition to the process of searching for a feasible composition. Recall that the search
space of partial compositions is generated by the recursive application of the functional
composition algorithm f . The nodes in the recursive tree correspond to components in
the repository of the composition problem. The tree is built starting with the root node 
the search for feasible compositions correspondingly begins with . The recursive tree is
further grown by attaching new nodes to any of the existing nodes  this corresponds to
extending feasible partial compositions by adding (composing) new components to any of
the existing feasible partial compositions. Finally, the leaves of a recursive tree at depth
d from the root correspond to a (possibly feasible) composition of d components from the
repository in the composition problem.
We now show the precise correspondence between a recursive tree data structure and a
search space of partial compositions.
 Each node in the tree corresponds to a composition.
 The root node corresponds to the empty composition ,
 Each node at level 1 corresponds to the composition of  with a component W in the
repository, i.e.,   W = W, W  R,
 Each node at level i corresponds to the composition of a component W in the repository with the composition associated with the parent of this node,
 A leaf node is called a feasible node if the composition associated with this node
satisfies .
For the purpose of experimentally evaluating our algorithms for finding the most preferred compositions and to compare them, we generate random recursive trees with varying
number of nodes (or |R|, the number of components in the repository). In the generated
random recursive tree, a certain fraction (f eas) of leaves are picked uniformly randomly
and are labeled to be feasible compositions. For each node in the generated and labeled
246

fiRepresenting and Reasoning with Qualitative Preferences

random recursive tree, the valuation of attributes X = {Xi } (corresponding to the partial
composition it represents) is randomly generated based on the respective domains ({Di })12 .
6.1.2 User Preferences
We generate user preferences by generating random partial/total orders for each i and
random interval/total order for  for varying number of attributes m = |X | and domain
size of attributes n = |Di |.
A summary of the simulation parameters is given in Table 8.
Parameter

Meaning

Range

f eas

Fraction of leaves in the search tree that are feasible compositions
Domain size of preference attributes
Number of preference attributes
Number of components in the repository (nodes in
the search tree)
Overhead (in milliseconds) per invocation of the
step-by-step functional composition algorithm f
Intra-attribute preference over the values of Xi
Relative importance preference over X

{0.25, 0.5, 0.75, 1.0}

|Di |
|X |
|R|
f delay
i


{2, 4, 6, 8, 10}
{2, 4, 6, . . . 20}
{10, 20, . . . 200}
{1, 10, 100, 1000}
{po, to}
{io, to}

Table 8: Simulation parameters and their ranges

6.1.3 Implementation of Algorithms
Computing Dominance
In order to check if one valuation dominates another with respect to the user preferences
{i } and , we iterate through all attributes X and check if there exists a witness for the
dominance to hold (see Definition 11).
Computing the most preferred solutions
We implemented algorithms A1, A2, A3 and A4 in Java. Preliminary experiments with A2
showed that the algorithm did not scale up for large problem instances. In particular, when
the number of attributes is large and dominance testing is computationally intensive, A2
timed out due to the computation of the non-dominated set multiple times for each of the
most important attributes. Hence we did not proceed to run experiments on the samples
with A2. However, we were able to run experiments with algorithm A3 that arbitrarily picks
one of the most important attributes and finds the most preferred solutions with respect
to the intra-attribute preferences of that attribute.
In algorithms A1 and A3 we first compute all solutions using the functional composition
algorithm (simulated by f ), whereas in A4, we interleave calls to f with choosing preferred
12. Note that in the setup described here, the valuations for attributes is generated randomly for each node.
In real applications, the valuations of the nodes may depend on the valuations of their parents.

247

fiSanthanam, Basu & Honavar

compositions (partial solutions) at each step. At each step, A4 chooses a subset of the
feasible extensions of the current compositions for further exploration. Table 9 gives a brief
description of the implemented algorithms.
Alg.

Name of Algorithm

Remarks

A1

ComposeAndF ilter

A2

W eaklyCompleteCompose

A3

AttW eaklyCompleteCompose

A4

InterleaveCompose

First identifies functionally feasible compositions; then finds the non-dominated
set of feasible compositions with respect
to d
First identifies functionally feasible compositions; then finds the non-dominated
set of feasible compositions with respect
to i for the most important attributes
{Xi }
First identifies functionally feasible compositions; then picks an arbitrary most important attribute Xi and finds the nondominated set of feasible compositions
with respect to i
Identifies the non-dominated set of feasible extensions with respect to d at each
step; and recursively identifies their feasible extensions until all the non-dominated
feasible extensions are feasible compositions

Table 9: Implemented Algorithms
Table 10 shows the attributes that are recorded during the execution of each of the
algorithms A1, A3 and A4 for each composition problem.
6.2 Results
We compare the algorithms A1, A3, A4 with respect to:
1. Quality of solutions produced by the algorithms, in terms of SP/P F and SP/S
2. Performance and efficiency in terms of running time and number of calls to the functional composition algorithm f
6.2.1 Quality of Solutions
We compare the quality of solutions produced by the algorithms in terms of the following
measures.
248

fiRepresenting and Reasoning with Qualitative Preferences

Attribute

Meaning

Remarks

F

Set of solutions (feasible compositions) in a sample
problem instance
Set of most preferred solutions in a sample problem
instance with respect to the user preferences and
the dominance relation
Set of solutions produced by the composition algorithm
Set of solutions produced by the composition algorithm that are also most preferred solutions with
respect to the user preferences and the dominance
relation
Running time of the composition algorithm (ms)
Number of times the algorithm invokes the stepby-step functional composition algorithm f

F = C

PF

S
SP

T
f count

P F =  d (C  )  F

SP = P F  S  S

Table 10: Attributes observed during the execution of each algorithm
 SP/P F 13 : Proportion of most preferred solutions produced by the algorithm (fraction
of all optimal solutions produced by the algorithm). If the algorithm is complete, then
SP/P F = 1.
 SP/S: Proportion of solutions produced by the algorithm that are most preferred
(fraction of solutions produced by the algorithm, that are optimal). If the algorithm
is sound, then SP/S = 1.
The algorithm A1 exhaustively searches the entire space of compositions to identify all
the feasible compositions F , and then finds the most preferred among them with respect to
the user preferences  and {i }. Because it computes the set  d (F ), we observed that
for A1, SP = P F = S, i.e., it is both sound (finds only the most preferred solutions) and
complete (finds all the most preferred solutions).
We next compare the algorithms A3 and A4 with respect to SP/P F and SP/S for
various types of ordering restrictions on the user preferences {i } and . Table 11 reports
results for the following combinations: (i)  is an interval order, {i } are partial orders;
(ii)  is an interval order, {i } are total orders; (iii)  is a total order, {i } are partial
orders; and (iv)  is a total order, {i } are total orders.
Comparison of SP/P F
 In general, most of the most preferred solutions were found by both the algorithms
(see Table 11).
13. For the sake of readability, we use the notation used to denote the set to denote its cardinality as well,
e.g., SP is used to denote both the set and its cardinality (|SP |).

249

fiSanthanam, Basu & Honavar



i

A3

A4

io
io
to
to

po
to
po
to

77.50
71.00
100.00
100.00

83.95
100.00
85.88
100.00

Table 11: Comparison of SP/P F for algorithms A3 and A4 with respect to various ordering
restrictions on {i }, . The percent of problem instances for which SP/P F = 1
is shown in each row with respect to the corresponding ordering restrictions on
the preference relations  and {i }. The parameters used for simulating the
problem instances and their ranges are given in Table 8.

 We observe that when relative importance () is a total order and {i } are arbitrary
partial orders, 100% of the most preferred solutions are produced by A3. Propositions 13 and 14 (see Section 3.6) were obtained based on this insight.



i

A3

A4

io
io
to
to

po
to
po
to

41.78
30.78
33.90
27.30

98.45
100.00
96.98
100.00

Table 12: Comparison of SP/S for algorithms A3 and A4 with respect to various ordering
restrictions on {i }, . The percent of problem instances for which SP/S = 1
is shown in each row with respect to the corresponding ordering restrictions on
the preference relations  and {i }. The parameters used for simulating the
problem instances and their ranges are given in Table 8.

Comparison of SP/S
 In general, most of the solutions that were found by the interleaved algorithm A4
were the most preferred solutions (see Table 12). On the other hand, algorithm A3
produced many solutions that were not the most preferred.
 The second (and fourth) row(s) of Tables 12 and 11 suggests that when intra-attribute
preferences ({i }) are total orders and  is an arbitrary interval order, the interleaved
algorithm A4 is sound and complete, i.e., it produces exactly the non-dominated set
of solutions with respect to d . Conjecture 1 and Theorem 3 in Section 3.6 were
obtained based on this insight.
250

fiRepresenting and Reasoning with Qualitative Preferences

6.2.2 Performance and Efficiency
We compare the performance and efficiency of A3, A4 in terms of the number of times the
functional composition algorithm f is invoked, and running time (in milliseconds) for the
algorithms to compute their solutions.
Number of calls to functional composition f
The plots in Figures 9 and 10 show the results of our experiments performed on problem
instances where relative importance preferences are interval/total orders and intra-attribute
preferences are partial/total orders, and they yield the following observations.
 In general, our experiments show that the interleaved algorithm A4 makes fewer calls
to f compared to A3. This can be seen in Figures 9 and 10, where all the data points
corresponding to the number of calls to f made by A4 (colored red) lie below those
that correspond to A3 (colored green) in plots (a) and (b). This is because A4 explores
only the most preferred subset of the available feasible extensions at each step in the
search. On the other hand, A3 exhaustively explores all feasible extensions at each
step.
 When the intra-attribute preferences {i } are total orders, the difference in the number of calls to f made by A3 and A4 is more pronounced. This can be observed in
Figures 9 and 10, where the data points corresponding to the number of calls to f
made by A4 (colored red) lie much closer to the axis corresponding to the number of
feasible compositions, in comparison to A3 (colored green). This can be explained by
the fact that in this case the dominance relation is larger, due to which the number
of incomparable pairs of compositions is smaller. Therefore, at each interleaving step
the non-dominated set computed for extension is smaller.
 For both A3 and A4, the number of calls to f decreases as the fraction of feasible
compositions (f eas) increases. Figures 9 and 10 show that as the number of feasible
compositions increases, the data points corresponding to the number of calls to f
(for all algorithms) gets closer to the axis corresponding to the number of feasible
compositions.
Running time
We observed that the running times of both algorithms A3, A4 depend on two key factors:
 f delay, the time taken per execution of the functional composition at each step
 Complexity of dominance testing which is in turn a function of |Di |, |X | and the
properties of {i } and . In particular, the complexity of dominance testing depends
on the size of the preference relations {i } and  (see Section 5.2).
In order to understand the effect of f delay on the running times of the algorithms, we
ran experiments with f delay = 10ms and f delay = 1000ms on problem instances where
relative importance preferences are interval/total orders and intra-attribute preferences are
partial/total orders (see Table 8 for the other parameters used and their ranges). The
respective results are shown in Figures 11  14. The results yield the following observations.
251

fiSanthanam, Basu & Honavar

 !"#$ %&'()"!*+ 3 %*")$! .)/)0 %*" )!,!"" )#12" 3 4!)"#! .)/)




ff


	

























fi







   
 !"#$ %&'()"!*+,, - ("! .)/)0 %*" )!,!"" )#12" 3 4!)"#! .)/)






ff	




















fi











   

Figure 9: A comparison of the algorithms A1, A3 and A4 with respect to the number of
times they invoke the step-by-step functional composition algorithm during their
execution. The plots (a) and (b) correspond to results of running the algorithms
on simulated problem instances, where the intra-attribute preference (i ) is a
partial order, and the relative importance preference () is an interval or total
order. The four distinct bands seen in the plots correspond to various fractions
of leaves in the search tree of the problem instance that are feasible compositions:
f eas = 0.25, 0.5, 0.75, 1.0.

252

fiRepresenting and Reasoning with Qualitative Preferences

EFGHIJKF LMNOPI H]^F _ L] IFPKHG `PaFPb L] I PHcHII PJdeIF _ fOI HG `PaFPP
655
:95
:85
:75
@
??>
=
<
;

:65

g:

:55

gh

95

g7

85
75
65
5
5

65

75

ABC

85

95

:55

:65

Q RSBTUDVS WX Y ZXTU[UX \T

EFGHIJKF LMNOPI H]^F _ fOIHG `PaFPb L] I PHcHII PJdeIF _ fOI HG `PaFP
655
:95
:85
:75
@
??>
=
<
;

:65

g:

:55

gh

95

g7

85
75
65
5
5
ADC

65

75

85

95

:55

:65

Q RSBTUDVS WX Y ZXTU[UX \T

Figure 10: A comparison of the algorithms A1, A3 and A4 with respect to the number
of times they invoke the step-by-step functional composition algorithm during
their execution. The plots (a) and (b) correspond to results of running the
algorithms on simulated problem instances, where the intra-attribute preference
(i ) is a total order, and the relative importance preference () is an interval
or total order. The four distinct bands seen in the plots correspond to various
fractions of leaves in the search tree of the problem instance that are feasible
compositions: f eas = 0.25, 0.5, 0.75, 1.0.

253

fiSanthanam, Basu & Honavar

 In general, in comparison to the running time of the algorithm A4 when the intraattribute preferences (i ) are partial orders, A4 is faster when i are total orders. This
trend is observed in plots (a) and (b) of Figure 12 (where intra-attribute preferences
are total orders), as the data points corresponding to the running time of A4 (colored
red) are much closer to the axis corresponding to the number of feasible compositions,
in comparison to the plots (a) and (b). A similar trend is also observed in Figures 13
and 14.
 The algorithm A3 almost always outperforms the blind search algorithm A1 in terms
of running time. This is because A3 computes the non-dominated set in the last step
with respect to the intra-attribute preference over the valuations of one attribute i
(in place of the dominance relation d used by A1).
 The interleaved algorithm A4 is more sensitive to the complexity of dominance than
A1 and A3, because at each step A4 computes the non-dominated subset of extensions
to explore. On the other hand, A1 and A3 involve computation of dominance only
in the last step. A3 is faster than A1, more than A4, because it computes the nondominated set with respect to the intra-attribute preference over the valuations of one
attribute i (in place of the dominance relation d used by A1 and A4).
 Algorithms A1 and A3 are more sensitive to f delay than the interleaved algorithm
A4. This is because at each step A1 and A3 explore all feasible extensions, but A4
only explores the preferred subset of the feasible extensions at each step.
 The overall running times of A1, A3 and A4 depend on the relative trade-offs among
|Di |, |X |, the properties of {i },  (those that influence the complexity of dominance
testing) on the one hand and f delay on the other.

7. Summary and Discussion
We now summarize our contributions in this paper.
7.1 Summary
Many applications, e.g., planning, Web service composition, embedded system design, etc.,
rely on methods for identifying collections (compositions) of objects (components) satisfying some functional specification. Among the compositions that satisfy the functional
specification (feasible compositions), it is often necessary to identify one or more compositions that are most preferred with respect to user preferences over non-functional attributes.
Of particular interest are settings where user preferences over attributes are expressed in
qualitative rather than quantitative terms (Doyle & Thomason, 1999).
In this paper, we have proposed a framework for representing and reasoning with qualitative preferences over compositions in terms of the qualitative preferences over attributes of
their components; and developed a suite of algorithms to compute the most preferred feasible compositions, given an algorithm that computes the functionally feasible compositions.
Specifically,
254

fiRepresenting and Reasoning with Qualitative Preferences

          
oiiii
niiii
{z
w
y
x
w
tv
u
s
ts
s
qr

miiii
j
liiii

l

kiiii

m

jiiii
i
i

ki

oi

mi

|}~

pi

jii

jki

 }  
          
mniii
miiii
lniii

{z
w
y
x
w
tv
u
s
ts
s
qr

liiii
j

kniii
kiiii

l

jniii

m

jiiii
niii
i
i
|~

ki

oi

mi

pi

jii

jki

 }  

Figure 11: A comparison of the algorithms A1, A3 and A4 with respect to their running
times as a function of the number of feasible compositions, when each invocation
step in the step-by-step functional composition algorithm has a overhead of
10 milliseconds. The plots (a) and (b) correspond to results of running the
algorithms on simulated problem instances, where the intra-attribute preference
(i ) is a partial order, and the relative importance preference () is an interval
or total order. The four distinct bands seen in the plots correspond to various
fractions of leaves in the search tree of the problem instance that are feasible
compositions: f eas = 0.25, 0.5, 0.75, 1.0.

255

fiSanthanam, Basu & Honavar

          









































   

          




 



































   

Figure 12: A comparison of the algorithms A1, A3 and A4 with respect to their running
times as a function of the number of feasible compositions, when each invocation
step in the step-by-step functional composition algorithm has a overhead of
10 milliseconds. The plots (a) and (b) correspond to results of running the
algorithms on simulated problem instances, where the intra-attribute preference
(i ) is a total order, and the relative importance preference () is an interval
or total order. The four distinct bands seen in the plots correspond to various
fractions of leaves in the search tree of the problem instance that are feasible
compositions: f eas = 0.25, 0.5, 0.75, 1.0.

256

fiRepresenting and Reasoning with Qualitative Preferences



 	
fffi  fi fifi  fi fi  fi fifi
















































   

 	
fffi  ff fifi  fi fi  fi fifi
















































   

Figure 13: A comparison of the algorithms A1, A3 and A4 with respect to their running
times as a function of the number of feasible compositions, when each invocation
step in the step-by-step functional composition algorithm has a overhead of
1000 milliseconds. The plots (a) and (b) correspond to results of running the
algorithms on simulated problem instances, where the intra-attribute preference
(i ) is a partial order, and the relative importance preference () is an interval
or total order. The four distinct bands seen in the plots correspond to various
fractions of leaves in the search tree of the problem instance that are feasible
compositions: f eas = 0.25, 0.5, 0.75, 1.0.

257

fiSanthanam, Basu & Honavar

;<=>?@A< BCDEF?>GH< I BG?<FA>= JFK<FL BG? F>M>?? F@NO?< I PE?>= JFK<F



('
$
&
%
$
!#
"
!





9



9:



9













)*+







, -.*/012. 34564/07 048/

;<=>?@A< BCDEF?>GH< I PE?>= JFK<FL BG? F>M>?? F@NO?< I PE?>= JFK<F



('
$
&
%
$
!#
"
!





9



9:



9







)1+













, -.*/012. 34564/07 048/

Figure 14: A comparison of the algorithms A1, A3 and A4 with respect to their running
times as a function of the number of feasible compositions, when each invocation
step in the step-by-step functional composition algorithm has a overhead of
1000 milliseconds. The plots (a) and (b) correspond to results of running the
algorithms on simulated problem instances, where the intra-attribute preference
(i ) is a total order, and the relative importance preference () is an interval
or total order. The four distinct bands seen in the plots correspond to various
fractions of leaves in the search tree of the problem instance that are feasible
compositions: f eas = 0.25, 0.5, 0.75, 1.0.

258

fiRepresenting and Reasoning with Qualitative Preferences

a) We have defined a generic aggregation function to compute the valuation of a composition as a function of the valuations of its components. We have also presented a
strict partial order preference relation for comparing two compositions with respect
to their aggregated valuations of each attribute;
b) We have introduced a dominance relation for comparing compositions based on user
specified preferences and established some of its key properties. In particular, we
have shown that this dominance relation is a strict partial order when intra-attribute
preferences are strict partial orders and relative importance preferences are interval
orders.
c) We have developed four algorithms for identifying the most preferred composition(s)
with respect to the user preferences. The first three algorithms first compute the
set of all feasible compositions (solutions) using a functional composition algorithm
as a black box, and then proceed to find the most preferred among them (1) based
on the dominance relation (ComposeAndFilter ); and (2) based on the preferred valuations with respect to the most important attribute(s) (WeaklyCompleteCompose
and AttWeaklyCompleteCompose). The fourth algorithm interleaves the execution of
a functional composition algorithm that produces the set of solutions by iteratively
extending partial solutions and the ordering of partial solutions with respect to user
preferences (InterleaveCompose).
d) We have established some key properties of the above algorithms. ComposeAndFilter
is guaranteed to return the set of all non-dominated solutions; WeaklyCompleteCompose is guaranteed to return a non-empty subset of non-dominated solutions; AttWeaklyCompleteCompose is guaranteed to return at least one of the non-dominated
solutions; and InterleaveCompose is guaranteed to return (i) a non-empty subset of
non-dominated solutions when the dominance relation is an interval order; and (ii) the
entire set of non-dominated solutions when the dominance relation is a weak order.
e) We have performed simulation experiments to compare the algorithms with respect to
(i) the ratio of most preferred solutions produced to the actual set of most preferred
solutions, and the ratio of the most preferred solutions produced to the entire set
of solutions produced by the algorithm; (ii) their running times as a function of the
search space and the overhead in each call to the functional composition algorithm;
and (iii) the number of calls each algorithm makes to the functional composition
algorithm during the course of its execution. The results showed the feasibility of our
algorithms for composition problems that involve up to 200 components.
f) We have analyzed the results of experiments to obtain additional theoretical properties of the dominance relation as a function of the properties of the underlying
intra-attribute preference relations and relative importance preference relation. In
particular, we obtained non-trivial results as a consequence of our analysis of experimental results, which were not known apriori, including conditions under which the
dominance relation is a weak order. These conjectures/results are significant because
they give the properties of the dominance relation directly as a function of the input
259

fiSanthanam, Basu & Honavar

user preferences. In turn, they also throw light on the soundness, weak-completeness
and/or completeness properties of the algorithms.
The proposed techniques for reasoning with preferences over non-functional attributes
are independent of the language used to express the desired functionality  of the composition, and the method used to check whether a composition C satisfies the desired functionality, i.e., C |= . Our formalism and algorithms may be applicable to a broad range of
domains including Web service composition (see Dustdar & Schreiner, 2005; Pathak, Basu,
& Honavar, 2008, for surveys), planning (see Hendler, Tate, & Drummond, 1990; Baier &
McIlraith, 2008a), team formation (see Lappas, Liu, & Terzi, 2009; Donsbach, Tannenbaum,
Alliger, Mathieu, Salas, Goodwin, & Metcalf, 2009) and indeed any setting that calls for
choosing the most preferred solutions from a set of candidate solutions, where each solution
is made up of multiple components.
7.2 Discussion
In the following, we discuss some of the alternate choices that one could make in applying
our formalism for specific applications.
Aggregation Functions. In our previous work (Santhanam, Basu, & Honavar, 2008), we
had proposed the use of TCP-net representation with ceteris paribus semantics (Brafman
et al., 2006) for reasoning with preferences in addressing the problem of Web service composition. We had assumed that the intra-attribute preferences are total orders; however,
this assumption does not hold in many practical settings involving qualitative preferences
over non-functional attributes. In this paper, we have relaxed this requirement, allowing
intra-attribute preferences that are strict partial orders.
In this paper we demonstrated the use of the summation (e.g., number of credits in
a POS) and worst frontier (e.g., areas of study and instructors) aggregation functions. In
some scenarios, it might be necessary to consider other ways of aggregating the valuations of
the components, for example, using the best frontier denoting the best possible valuations of
the components (i.e., the maximal valuations for each attribute Xi with respect to i ). Any
aggregation function can be used in our formalism, provided that the preference relation
over the aggregated valuations is a strict partial order. Otherwise, the choice of aggregation
function and the preference relation to compare aggregated valuations may impact the
properties of the resulting dominance relation, and as a result, may also affect the soundness
and completeness properties of some of the proposed algorithms.
The aggregation functions demonstrated in this paper are independent of the how the
components interact or are assembled, i.e., the structure of a composition. However, in
general, it may be necessary for the aggregation function to take into account the structure and/or other interactions between the valuations of components in a composition. For
example, in evaluating the reliability of a composition, one needs to consider the precise
structure of the composition. The reliability of a composition Ci is the product of the relin
Y
abilities of the components ( VWi (Reliability)) when the components are arranged in a
i=1

series configuration (Rausand & Hyland, 2003). On the other hand, when the same set of
components {Wi } are arranged in a parallel configuration, the reliability of Ci is computed
260

fiRepresenting and Reasoning with Qualitative Preferences

n
Y
(1  VWi (Reliability))). In general, it might be necessary to introduce aggregaas (1 
i=1

tion functions that take into consideration a variety of factors including the structure, the
function, as well as the non-functional attributes of the composition.
Comparing Sets of Aggregated Valuations. In this paper, we presented a preference relation
( i ) to compare sets of valuations computed using the worst frontier aggregation function
(Definition 8). This preference relation requires that given two sets of valuations, every
element in the dominated set is preferred to at least one of the elements in the dominating
set of valuations. Other choices of i can be used as well, but care should be taken because
the properties of the chosen preference relation may affect the properties of the dominance
( d ) relation and the properties of the algorithms. However, as long as i is a strict partial
order (irreflexive and transitive), the dominance relation continues to remain a strict partial
order (subject to  being an interval order), and hence the properties of the algorithms
hold. This provides the user with a wide range of preference relations for comparing sets of
valuations to choose from (see Barbera et al., 2004, for a survey of preferences over sets).
Note that Definition 8 does not ignore common elements when comparing two sets of
elements. However, some settings may require a preference relation that compares only
elements in the two sets that are not common. In such settings, a suitable irreflexive
and transitive preference relation can be used, such as the asymmetric part of preference
relations developed by Brewka et al. (2010) and Bouveret et al. (2009). In the absence of
transitivity, the transitive closure of the relation may be used to compare sets of elements,
as done by Brewka et al.
Dominance and its Properties. The dominance relation ( d ) adopted in this paper is a
strict partial order when the intra-attribute preferences are arbitrary strict partial orders
and the relative importance is an interval order. It would be interesting to explore alternative notions of dominance that preserve the rationality of choice, by requiring a different set
of properties (e.g., those that satisfy negative-transitivity instead of transitivity). It would
also be of interest to examine the relationships between d and alternative dominance relations. Some results comparing d with the dominance relations proposed by other authors
(Brafman et al., 2006; Wilson, 2004b, 2004a) have been presented elsewhere (Santhanam,
Basu, & Honavar, 2010b, 2009).
Implementation. The current implementation of dominance testing with respect to d is
based on iteratively searching all the attributes to find a witness. It would be interesting to
compare this with other methods for dominance testing such as the one proposed in one of
our earlier works (Santhanam, Basu, & Honavar, 2010a) that uses efficient model checking
techniques. We would also like to use other multi-attribute preference formalisms that
include conditional preferences in our framework for compositional systems and compare
the performance of the resulting implementation with the current implementation.
7.3 Related Work
Techniques for representing and reasoning with user preferences over a set of alternatives
have been studied extensively in the areas of decision theory, microeconomics, psychol261

fiSanthanam, Basu & Honavar

ogy, operations research, etc. The seminal work by von Neumann and Morgenstern (1944)
models user preferences using utility functions that map the set of possible alternatives
to numeric values. More recently, models for representing and reasoning with quantitative preferences over multiple attributes have been developed (Fishburn, 1970a; Keeney
& Raiffa, 1993; Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman, 2001). Such
models have been used to address problems such as identifying the most preferred tuples
resulting from database queries (Agrawal & Wimmers, 2000; Hristidis & Papakonstantinou, 2004; Borzsonyi, Kossmann, & Stocker, 2001), assembling preferred composite Web
services (Zeng, Benatallah, Dumas, Kalagnanam, & Sheng, 2003; Zeng, Benatallah, Ngu,
Dumas, Kalagnanam, & Chang, 2004; Yu & Lin, 2005; Berbner, Spahn, Repp, Heckmann,
& Steinmetz, 2006), and in other composition problems.
However, in many applications it is more natural for users to express preferences in
qualitative terms (Doyle & McGeachie, 2003; Doyle & Thomason, 1999; Dubois, Fargier,
Prade, & Perny, 2002) and hence, there is a growing interest in AI on formalisms for
representing and reasoning with qualitative preferences (Brafman & Domshlak, 2009). We
now proceed to place our work in the context of some of the recent work on representing
and reasoning with qualitative preferences.
7.3.1 TCP-nets
Notable among qualitative frameworks for preferences are preference networks (Boutilier
et al., 2004; Brafman et al., 2006) that deal with qualitative and conditional preferences.
A class of preference networks, namely Tradeoff-enhanced Conditional Preference networks
(TCP-nets) (Brafman et al., 2006) are closely related to our work, and we now proceed to
discuss where our framework departs from and adds to the existing TCP-net framework.
TCP-nets provide a very elegant and compact graphical model to represent qualitative
intra-attribute and relative importance preferences over a set of attributes. In addition,
TCP-nets can also model conditional preferences using dependencies among attributes.
While TCP-nets allow us to represent and reason about preferences in general over simple
objects (each of which is described by a set of attributes), the focus of our work is to reason
about such preferences over compositions of simple objects (i.e., a collection of objects
satisfying certain functional properties). For example, in the domain of Web services, the
problem of identifying the most preferred Web services from a repository of available ones
based on their non-functional attributes, namely Web service selection can be solved using
the TCP-net formalism. On the other hand, in addition to Web service selection, our
formalism can also address the more complicated problem of identifying the most preferred
composite Web services that collectively satisfy a certain functional requirement, namely
Web service composition.
Our formalism is based on the intra-attribute and relative importance preferences over a
set of attributes describing the objects. As a result, the graphical representation scheme of
TCP-nets can still be used to compactly encode the intra-attribute and relative importance
preferences of the users within our formalism 14 .
14. In our setting, we do not consider conditional preferences that correspond to edges denoting conditional
dependencies in the TCP-nets.

262

fiRepresenting and Reasoning with Qualitative Preferences

We have extended reasoning about preferences over single objects to enable reasoning
about preferences over collections of objects. We have: (a) provided an aggregation function for computing the valuation of a composition as a function of the valuations of its
components; (b) defined a dominance relation for comparing the valuations of compositions
and established some of its properties; and (c) developed algorithms for identifying a subset
or the set of most preferred composition(s) with respect to this dominance relation.
Our formalism departs from TCP-nets in the interpretation of the intra-attribute and
relative importance preferences over objects: the dominance relation in a TCP-net is defined
as any partial order relation that is consistent with the given preferences over attributes
of the objects, based on the ceteris-paribus semantics. We introduce a dominance relation
(see Definition 11) that allows us to reason about preferences over collections of objects in
terms of sets of valuations of the attributes of objects that make up the collection. For
instance, our worst frontier aggregation function returns the set of worst possible attribute
valuations among all the components.
When our dominance relation is applied in the simpler setting where each collection
consists of a single object, the aggregation function for each attribute reduces to the identity function, and the preference relation i over sets of valuations of each attribute Xi
reduces to the intra-attribute preference i . We have recently shown in our earlier works
(Santhanam et al., 2010b, 2009) that in general, when TCP-nets are restricted to unconditional preferences, our dominance relation (when each collection consists of a single object)
and the dominance relation used in TCP-nets are incomparable; when relative importance
is restricted to be an interval order, our dominance relation is more general than the dominance relation used in TCP-nets with only unconditional preferences. In the latter case,
our dominance relation is computable in polynomial time, whereas there are no known
polynomial time algorithms for computing TCP-net dominance (Santhanam et al., 2010b,
2009).
7.3.2 Preferences over Collections of Objects
Several authors have considered ways to extend user preferences to obtain a ranking of
collections of objects (see Barbera et al., 2004, for a survey). In all these works, preferences
are specified over individual objects in a set as opposed to preferences over valuations of
the attributes of the objects. The preferences over objects are in turn used to reason
about preferences over collections of those objects. This scenario can be simulated by our
framework, by introducing a single attribute whose valuations correspond to objects in the
domain.
DesJardins et al. (2005) have considered the problem of finding subsets that are optimal
with respect to user specified quantitative preferences over a set of attributes in terms of the
desired depth, feature weight and diversity for each attribute. In contrast, our framework
focuses on qualitative preferences. In our setting, depth preferences that map attribute
valuations to their relative desirability can be mapped to qualitative intra-attribute preferences and feature weights can be mapped to relative importance. Diversity preferences over
attributes refer to the spread (e.g., variance, range, etc.) of component valuations with respect to the corresponding attributes. It would be interesting to explore whether a suitable
263

fiSanthanam, Basu & Honavar

Property

Denoted by

New Attribute

Attribute Domain

Party Affiliation
Views
Experience

P
V
E

XP
XV
XE

{Re, De}
{Li, Co, U l}
{Ex, In}

Table 13: Properties/Attributes describing the senators
dominance relation can be defined so as to simultaneously capture in our framework the
user preferences with respect to the depth, diversity and feature weights.
More recently, Binshtok et al. (2009) have presented a language for specification of
preferences over sets of objects. This framework, in addition to intra-attribute and relative
importance preferences over attributes, allows users to express preferences over the number
(||) of elements in a set that satisfy a desired property . The preference language in this
case allows statements such as Si : || REL n (number of elements in the preferred set
with property  should be REL n), Sj : || REL || (number of elements in the preferred
set with property  should be REL number of elements in the preferred set with property
), etc., where REL is one of the arithmetic operators >, <, =, ,  and n is an integer.
In addition, there can be relative importance between the various preference statements
such as Si is more important than Sj  as well as external cardinality constraints such as
a bound on the number of elements in the preferred set.
Our formalism can accommodate such preference statements, by representing each preference statement Si as a new binary valued attribute in the compositional system. For
example, preference statements Si : ||  n and Sj : ||  || can be represented in
our formalism by creating new binary attributes Xi and Xj with intra-attribute preferences
1 i 0 and 1 j 0 respectively. The relative importance statements such as Si is more
important than Sj  can then be directly mapped to Xi  Xj . Any external cardinality
constraints on the size of the preferred set can be encoded in our setting by functional requirements, so as to restrict the feasible solutions to only those that satisfy the cardinality
constraints.
Consider the example discussed by Binshtok et al. (2009), with preferences over senate
members described by attributes: Party affiliation (Republican, Democrat ), Views (liberal, conservative, ultra conservative), and Experience (experienced, inexperienced). The
attributes and their domains are listed in Table 13. The set preferences are given by:
 S1 : h|P = Re  V = Co|  2i
 S2 : h|E = Ex|  2i
 S3 : h|V = Li|  1i
Note that the senate members (i.e., the individual objects) are described by three attributes XP , XV , XE representing the party affiliation, views and experience respectively.
The valuation function for these attributes is defined in the obvious manner, e.g., if a senator
Wj is a republican, then VWj (XP ) = Re. We introduce three additional boolean attributes
264

fiRepresenting and Reasoning with Qualitative Preferences

X1 , X2 , X3 corresponding to the preference statements S1 , S2 , S3 respectively. The valuation
function for each new attribute of a senator Wi can then be defined as follows.
(
1 , if Wi |= S1 i.e., VWi (XP ) = Re or VWi (XV ) = Co
 VWi (X1 ) =
0 , otherwise
(
1 , if Wi |= S2 i.e., VWi (XE ) = Ex
 VWi (X2 ) =
0 , otherwise
(
1 , if Wi |= S3 i.e., VWi (XV ) = Li
 VWi (X3 ) =
0 , otherwise
The valuation of the collection of senators W1  W2  . . .  Wn for i  {1, 2, 3} is:
VW1 W2 ...Wn (Xi ) = i (VW1 , VW2 , . . . VWn ) = VW1 (Xi ) + VW2 (Xi ) +    + VWn (Xi )
Note that the aggregation function i defined above differs from the worst-frontier
based aggregation function adopted in Definition 6. The preference relation for comparing
groups of senators with respect to each new attribute Xi can then be defined based on
the preference statement Si . For example, in the case of X1 we define 1 such that any
value  2 is preferred to any value < 2, etc. Having defined the above aggregation function
and comparison relation for each new attribute, any dominance relation can be adopted
to compare compositions (arbitrary subsets) with respect to all attributes including the
dominance relation used by Binshtok et al. (2009).
In contrast to the framework of Binshtok et al., (2009) our formalism focuses on collections of objects that satisfy some desired criteria, rather than arbitrary subsets. We provide
algorithms for finding the most preferred compositions that satisfy the desired criteria.
7.3.3 Database Preference Queries
Several authors (Borzsonyi et al., 2001; Chomicki, 2003; Kiessling & Kostler, 2002; Kiessling,
2002) have explored techniques for incorporating user specified preferences over the result
sets of relational database queries. For instance, Chomickis framework (2003) allows user
preferences over each of the attributes of a relation to be expressed as first order logic formulas. Suppose Sq is the set of tuples that match a query q. For each attribute Xi , from Sq ,
a subset Sqi of tuples that have the most preferred value(s) for Xi is identified. The result
set for the query q is then given by i Sqi . A similar framework for expressing and combining user preferences is presented by Kiessling (2002) and Kiessling and Kostler (2002).
Brafman and Domshlak (2004) have pointed out some of the semantic difficulties associated
with above approaches, and considered an alternative approach to identifying the preferred
result set based on the CP-net (Boutilier et al., 2004) dominance relation. Because of the
high computational complexity of dominance testing for CP-nets, Boutilier et al. proposed
an efficient alternative based on an ordering operator that orders the tuples in the result
set in a way that is consistent with the user preferences. Our formalism can be used in the
database setting, similar in spirit to that of Brafman and Domshlak, by considering each
265

fiSanthanam, Basu & Honavar

tuple in Sq as a collection with a single object. The differences in the semantics of the
CP-net dominance and our dominance relation is discussed in Section 7.3.1.
A host of algorithms have also been proposed for computing the non-dominated result
set in response to preference queries, especially for the efficient evaluation of skyline queries
(Borzsonyi et al., 2001; Chomicki, 2003). A skyline query yields the non-dominated result
set from a database, where dominance is evaluated based on the notion of pareto dominance
that considers all attributes to be equally important. Most of the proposed algorithms
for computing the skylines (see Jain, 2009, for a survey) are applicable only when intraattribute preferences are totally or weakly ordered. Some other algorithms that can handle
partially ordered attribute domains (Chan, Eng, & Tan, 2005; Sacharidis, Papadopoulos, &
Papadias, 2009; Jung, Han, Yeom, & Kang, 2010) rely on creating and maintaining indexes
over the attributes in the database, and on data structures specifically designed to identify
the skyline with respect to pareto dominance. These algorithms may be considered if a
particular problem instance involves such a large set of components are already stored in
a database and indexed. However, it is not obvious that they generalize to an arbitrary
notion of dominance such as the one presented here. On the other hand, our algorithms
for finding the non-dominated set are applicable to any notion of dominance, provided the
user preferences are such that the dominance relation is a partial order.
7.3.4 Planning with Preferences
The classical planning problem consists of finding a sequence of actions that take a system
from an initial state to one of the states that satisfies the user specified goal. Preference
based planning refers to the problem of finding plans that are most preferred with respect to
a set of user preferences over the plans. Such preferences are usually compactly expressed in
terms of the preferences over the properties satisfied by the plans in the goal or intermediate
states, or over actions, or over action sequences (i.e., temporal properties of the plans). We
refer the interested reader to surveys by Baier et al. (2008b) and Bienvenu et al. (2011) for
an overview of qualitative and quantitative preference languages used in preference based
AI planning, and different algorithms for computing the most preferred plans.
Preference based planning can be viewed as a problem of finding the most preferred
composition in a compositional system, where the components correspond to the actions,
and the feasible compositions correspond to the states of the plans that satisfy the goal
in the planning problem. The allowed set of actions that can be performed from a given
state in the planning problem can be encoded in the compositional system in terms of a
set of functional requirements (or constraints on the functionality). The preferences over
the various actions that can be taken at any given state in a plan can be captured by
preferences over the components with which a composition can be extended in terms of
their properties or attribute valuations. The properties satisfied by a state of a plan in the
planning problem can be captured by the valuations of the attributes of the corresponding
composition in the compositional system. Based on the mapping of actions performed in
a given state to the properties of the resulting state in the planning problem, aggregation
functions can be suitably defined in the compositional system. The addition of an action to
a partial plan in the planning problem can be represented in the compositional system by
the extension of a partial composition by a new component, and the properties satisfied by
266

fiRepresenting and Reasoning with Qualitative Preferences

the resulting state in the planning problem correspond to the valuations of the attributes
of the extended composition as determined by the aggregation functions. Finding the most
preferred plans then involves finding the most preferred feasible compositions.
The algorithms presented in this paper can be used to find the most preferred plans with
respect to the user specified preferences over actions in terms of the properties satisfied by
their resulting states, or over the properties satisfied by the plans in the goal state. However,
planning problems that involve preferences over the orderings of states and actions in a plan,
e.g., preferences over the properties that hold over the entire sequence of states of a plan
(Baier, Bacchus, & McIlraith, 2009; Bienvenu et al., 2011) cannot be handled within our
framework.

8. Acknowledgments
Aspects of this work were supported in part by NSF grants CNS0709217, CCF0702758,
IIS0711356 and CCF1143734. The work of Vasant Honavar was supported by the National
Science Foundation, while working at the Foundation. Any opinion, finding, and conclusions
contained in this article are those of the authors and do not necessarily reflect the views of
the National Science Foundation.
We are grateful to anonymous reviewers for a thorough review and Dr. Ronen Brafman
for many useful suggestions that have helped improve the manuscript.

Appendix A. Proofs of Propositions and Theorems in Section 4
Proposition 15 Xi  I :  d (C) 6=    i (C)   d (C) 6= .
Proof. Let Xi  I and U   i (C). There are two possibilities: U   d (C) and U 
/
 d (C). If U   d (C), then there is nothing left to prove.
Suppose that U 
/  d (C). Then we show that V =
6 U such that V   i (C) d (C).
/  d (C)  V   d (C) : V d U.
U   i (C)  U 
By Definitions 11 and 16, it follows that V   d (C) : V(Xi ) i U(Xi ). Hence, Xi
cannot be a witness for V d U. Now there are two cases to consider.
Case 1: U(Xi ) i V(Xi ).
Let attribute Xj 6= Xi be a witness for V d U. Since Xi  I, (Xi  Xj )  (Xi 
Xj ). It therefore follows that V(Xi ) i U(Xi ), which contradicts our assumption that
U(Xi ) i V(Xi ). Hence, U(Xi ) 6 i V(Xi ).
Case 2: U(Xi ) i V(Xi ).
Let attribute Xj 6= Xi be a witness for V d U. Since Xi  I, (Xi  Xj )  (Xi 
Xj ). From Definition 11, V d U only if V(Xi ) i U(Xi ). Because of our assumption that
U(Xi ) i V(Xi ), it must be the case that V(Xi ) = U(Xi ), i.e., V   i (C). Thus, we have:
U   i (C) \  d (C)  V   i (C)   d (C) : V d U
This completes the proof.
267

(5)

fiSanthanam, Basu & Honavar

Theorem 4 [Soundness and Weak Completeness of Algorithm 2] Given a set of attributes
X , preference relations  and i , Algorithm 2 generates a set  of feasible compositions
such that    d (C) and  d (C) 6=    6= .
Proof.
Soundness: The proof proceeds by contradiction. Suppose that the algorithm returns a
solution U   such that U 
/  d (C). Because U  , it is necessary (by Line 5) that
Xi  I : U   i (C) \  d (C). Then, from Equation (5) in the proof of Proposition 15,
V   i (C)   d (C) : V d U, which means that U 
/  d ( i (C)). However, this
contradicts Line 5 of the algorithm. Hence,    d (C), i.e., Algorithm 2 is sound.
Weak Completeness: Because I 6= , Line 5 is executed by the algorithm at least once for
some Xi  I. By Definition 13, we have C 6=    i (C) 6=    d ( i (C)) 6=    6=
. Hence, Algorithm 2 is weakly complete by Definition 15.
Proposition 16 If I = {Xt }Xk 6= Xt  X : Xt Xk , then  d (C)  , i.e., Algorithm 2
is complete.
Proof. The proof proceeds by contradiction. Let I = {Xt } and Xk 6= Xt  X : Xt  Xk ,
and suppose that V   d (C) \  t (C). Since V 
/  t (C), by Definition 13 it must be

the case that U   t (C) : U(Xt ) t V(Xt ). However, then U d V by Definition 11 thus
contradicting our assumption that V   d (C).
Proposition 17 If |I| = 1, i.e., there is a unique most important attribute with respect
to , then Algorithm 3 is complete.
Proof. Let I = {Xi }. We know from Proposition 14 that i  d . It follows that
 d (S)   i (S) for any set S. Hence,  d (C)   i (C) = , i.e., Algorithm 3 is
complete.
Proposition 18 [Termination of Algorithm 4] Given a finite repository of components,
Algorithm 4 terminates in a finite number of steps.
Proof. Given a finite repository R of components, and an algorithm f that computes feasible
extensions of partial feasible compositions15 , and due to the fact that Algorithm 4 does not
re-visit any partial feasible composition, the number of recursive calls is finite.
Proposition 19 [Unsoundness of Algorithm 4] Given a functional composition algorithm
f and user preferences i and  over a set of attributes X , Algorithm 4 is not guaranteed
to generate a set of feasible compositions  such that    d (C).
Proof. We provide an example wherein Algorithm 4 returns a feasible composition that is
dominated by some other feasible composition. Consider a compositional system with a
single attribute X = {X1 }, with a domain of {a1 , a2 , a3 , a4 }. Let the intra-attribute preference of the user over those values be the partial order: a4 1 a1 and a2 1 a3 (Figure 15).
Let R = {W1 , W2 , W3 , W4 } be the repository of components in the compositional system
such that VWi (X1 ) = {ai }.
15. An f that terminates with a set of feasible extensions is guaranteed by the decidability of .

268

fiRepresenting and Reasoning with Qualitative Preferences

a4

a2

a1

a3

Figure 15: Intra-attribute preference 1 for attribute X1
Suppose that there are three feasible compositions in C satisfying the user specified
functionality , namely C1 = W1 , C2 = W2 , C3 = W3  W4 . Their respective valuations
are: VC1 = h{a1 }i, VC2 = h{a2 }i and VC3 = h{a3 , a4 }i. Clearly,  d (C) = {C2 , C3 }, because
VC3 d VC1 (due to the fact that {a3 , a4 } 1 {a1 }).


Iteration 0

Iteration 1

Iteration 2

W1

W2

W3

W3  W4

Algorithm terminates with
W1 and W2 as solutions
as W2 dominates W3 and
W1 in incomparable to W3
dominates W1 !

Figure 16: Execution of Algorithm 4
Now suppose that there exists a functional composition algorithm f that produces
the following sequence of partial feasible compositions (Figure 16): {}, {W1 , W2 , W3 },
{W1 , W2 , W3  W4 }. According to Line 13 of Algorithm 4, the algorithm will terminate after the first invocation of f , i.e., when the set {W1 , W2 , W3 } of partial feasible compositions
is produced by f . This is because after the first iteration,  = {W1 , W2 }, with VW2 d VW3 ,
and both W1 and W2 are feasible compositions. This results in  = {C1 , C2 } 6  d (C).
Theorem 5 [Soundness of Algorithm 4] If d is an interval order, then given a functional composition algorithm f and user preferences { i },  over a set of attributes X ,
Algorithm 4 generates a set  of feasible compositions such that    d (C).
Proof. Suppose that by contradiction, F   and there is a feasible composition C 
/  such
that VC d VF . If C is present in the list L upon termination of the algorithm, then C should
have been in , because the algorithm terminates only when all compositions in  d (L)
are feasible. This implies that the algorithm did not terminate with an L containing C.
The algorithm keeps track of all partial feasible compositions that can be extended from
 in L, without discarding any of them before termination. Therefore, the existence of
any such feasible composition C that is not in L at the time of termination must imply the
existence of some partial feasible composition B in the list (at the time of termination) that
can be extended to produce the feasible composition C, i.e., B  W1  W2  . . .  Wn = C
such that B 6|=  and C |= .
B 6|=   B 
/  at the time of termination, and therefore E   : VE d VB . Because
d is transitive (by Proposition 12), since VC 6 d VB (by Proposition 6), it follows that
VC 6 d VE (otherwise, VC d VE  VE d VB  VC d VB , a contradiction). Hence, C must
269

fiSanthanam, Basu & Honavar

dominate some composition other than E, say F   at the time of termination, i.e.,
VC d VF . Because E, F  , it follows that VF d VE , which in turn implies that VE 6 d VC .
Therefore, F   : VC d VF , VF d VE and VE d VC (see Figure 17).
VD

VE

VF

VB

Figure 17: Dominance relationships that violate the interval order restriction on d
From VE d VB , VC d VF , VF d VE and VC 6 d VB , it follows that VC d VB (because
VB d VC would otherwise imply VE d VF , a contradiction). Finally, it must be the case
that: VB 6 d VF , since otherwise it would contradict VF d VE ; and VF 6 d VB , since
otherwise it would contradict VC d VB . Therefore, VB d VF . Thus, the only possible
dominance relationships among the compositions B, C, E, F are as follows (see Figure 17):
 VE d VB
 VC  d VF
However, this scenario is ruled out by the fact that d is an interval order. Hence
F  , C  C \  : VC 6 d VF , i.e.,    d (C).
Theorem 6 [Weak Completeness of Algorithm 4] If d is an interval order, then given a
functional composition algorithm f and user preferences { i },  over a set of attributes X ,
Algorithm 4 produces a set  of feasible compositions such that  d (C) 6=    d (C) 6=
.
Proof. From Theorem 5, we have    d (C) when d is an interval order. It suffices to
show that  d (C) 6=    6= . The algorithm terminates with the non-dominated set
of compositions in the current list L, i.e., the maximal elements of L with respect to d .
The set of maximal elements of any partial order on the set of elements in L is not empty
whenever L is not empty, and the set of elements in L is in turn not empty whenever C is
not empty. Therefore,  d (C) 6=   C 6=   L =
6    6=  as required.
Theorem 7 [Completeness of Algorithm 4] If d is a weak order, then given a functional composition algorithm f and user preferences { i },  over a set of attributes X ,
Algorithm 4 generates a set  of feasible compositions such that  d (C)  .
Proof. It suffices to show that there is no feasible composition C   d (C) \ .
Suppose by contradiction that C   d (C), and C 
/ . This means that C was not
present in the list L upon the termination of the algorithm (because otherwise C   as per
Lines 4, 6, 13 in Algorithm 4). Hence, C must be a feasible extension of some partial feasible
composition B that is present in L at the time of termination such that B  W1  W2 
. . .  Wk = C.
From Proposition 6, we have VC 6 d VB . Because d is a weak order, (a) E 
 : VE d VB ; and (b) VC 6 d VB  VE d VB  VE d VC . However, this contradicts our
assumption that C   d (C).
270

fiRepresenting and Reasoning with Qualitative Preferences

References
Agrawal, R., & Wimmers, E. L. (2000). A framework for expressing and combining preferences. SIGMOD Rec., 29 (2), 297306.
Bacchus, F., & Grove, A. J. (1995). Graphical models for preference and utility. In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence
(UAI-1995), pp. 310.
Baier, J. A., Bacchus, F., & McIlraith, S. A. (2009). A heuristic search approach to planning
with temporally extended preferences. Artificial Intelligence, 173 (5-6), 593  618.
Baier, J. A., Fritz, C., Bienvenu, M., & McIlraith, S. (2008). Beyond classical planning:
Procedural control knowledge and preferences in state-of-the-art planners. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI), Nectar Track,
pp. 15091512, Chicago, Illinois, USA.
Baier, J. A., & McIlraith, S. A. (2008a). Planning with preferences. AI Magazine, 29 (4),
2536.
Baier, J. A., & McIlraith, S. A. (2008b). Planning with preferences. AI Magazine, 29 (4),
2536.
Barbera, S., Bossert, W., & Pattanaik, P. K. (2004). Ranking sets of objects. In Handbook
of Utility Theory. Volume II Extensions, chap. 17, pp. 893977. Kluwer Academic
Publishers.
Berbner, R., Spahn, M., Repp, N., Heckmann, O., & Steinmetz, R. (2006). Heuristics for qosaware web service composition. In Proceedings of the IEEE International Conference
on Web Services, pp. 7282.
Bienvenu, M., Fritz, C., & McIlraith, S. A. (2011). Specifying and computing preferred
plans. Artificial Intelligence, 175 (7-8), 1308  1345.
Binshtok, M., Brafman, R. I., Domshlak, C., & Shimony, S. E. (2009). Generic preferences
over subsets of structured objects. Journal of Artificial Intelligence Research, 34,
133164.
Borzsonyi, S., Kossmann, D., & Stocker, K. (2001). The skyline operator. In Proceedings
of the 17th International Conference on Data Engineering, pp. 421430, Washington,
DC, USA. IEEE Computer Society.
Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). CP-nets: A tool
for representing and reasoning with conditional ceteris paribus preference statements.
Journal of Artificial Intelligence Research, 21, 135191.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: A directed graphical representation of conditional utilities. In Proceedings of the 17th Conference in
Uncertainty in Artificial Intelligence (UAI-2001), pp. 5664.
Bouveret, S., Endriss, U., & Lang, J. (2009). Conditional importance networks: A graphical
language for representing ordinal, monotonic preferences over sets of goods. In IJCAI,
pp. 6772.
271

fiSanthanam, Basu & Honavar

Brafman, R. I., Domshlak, C., & Shimony, S. E. (2006). On graphical modeling of preference
and importance. Journal of Artificial Intelligence Research, 25, 389424.
Brafman, R. I., & Domshlak, C. (2004). Database preference queries revisited. Tech. rep.
1934, Department of Computing and Information Science, Cornell University.
Brafman, R. I., & Domshlak, C. (2009). Preference handling - an introductory tutorial. AI
magazine, 30 (1).
Brewka, G., Truszczynski, M., & Woltran, S. (2010). Representing preferences among sets.
In AAAI. AAAI Press.
Chan, C.-Y., Eng, P.-K., & Tan, K.-L. (2005). Stratified computation of skylines with
partially-ordered domains. In SIGMOD 05: Proceedings of the 2005 ACM SIGMOD
international conference on Management of data, pp. 203214, New York, NY, USA.
ACM.
Chomicki, J. (2003). Preference formulas in relational queries. ACM Trans. Database Syst.,
28 (4), 427466.
Daskalakis, C., Karp, R. M., Mossel, E., Riesenfeld, S., & Verbin, E. (2009). Sorting and
selection in posets. In SODA, pp. 392401.
desJardins, M., & Wagstaff, K. (2005). DD-PREF: A language for expressing preferences
over sets. In AAAI, pp. 620626.
Donsbach, J. S., Tannenbaum, S. I., Alliger, G. M., Mathieu, J. E., Salas, E., Goodwin,
G. F., & Metcalf, K. A. (2009). Team composition optimization: The team optimal
profile system (tops). Tech. rep. ARI TR 1249, U.S. Army Research Institute for the
Behavioral and Social Sciences.
Doyle, J., & McGeachie, M. (2003). Exercising qualitative control in autonomous adaptive
survivable systems. In Self-Adaptive Software: Applications, chap. 8. Springer Berlin
Heidelberg.
Doyle, J., & Thomason, R. H. (1999). Background to qualitative decision theory. AI
magazine, 20, 5568.
Dubois, D., Fargier, H., Prade, H., & Perny, P. (2002). Qualitative decision theory: from
savages axioms to nonmonotonic reasoning. Journal of the ACM, 49 (4), 455495.
Dustdar, S., & Schreiner, W. (2005). A survey on web services composition. International
Journal on Web and Grid Services, 1 (1), 120.
Fishburn, P. (1970a). Utility Theory for Decision Making. John Wiley and Sons.
Fishburn, P. (1970b). Utility theory with inexact preferences and degrees of preference.
Synthese, 21, 204221. 10.1007/BF00413546.
Fishburn, P. (1985). Interval Orders and Interval Graphs. J. Wiley, New York.
French, S. (1986). Decision theory: An introduction to the mathematics of rationality..
Hendler, J., Tate, A., & Drummond, M. (1990). AI planning: systems and techniques. AI
Mag., 11 (2), 6177.
Hristidis, V., & Papakonstantinou, Y. (2004). Algorithms and applications for answering
ranked queries using ranked views. The VLDB Journal, 13 (1), 4970.
272

fiRepresenting and Reasoning with Qualitative Preferences

Jain, R. (2009). Handling worst case in skyline. Masters thesis, York University, Department
of Computer Science and Engineering.
Jung, H., Han, H., Yeom, H. Y., & Kang, S. (2010). A fast and progressive algorithm for
skyline queries with totally- and partially-ordered domains. Journal of Systems and
Software, 83 (3), 429  445.
Keeney, R. L., & Raiffa, H. (1993). Decisions with multiple objectives: Preferences and
value trade-offs..
Kiessling, W. (2002). Foundations of preferences in database systems. In VLDB 02:
Proceedings of the 28th international conference on Very Large Data Bases, pp. 311
322. VLDB Endowment.
Kiessling, W., & Kostler, G. (2002). Preference sql: design, implementation, experiences.
In VLDB 02: Proceedings of the 28th international conference on Very Large Data
Bases, pp. 9901001. VLDB Endowment.
Lago, U. D., Pistore, M., & Traverso, P. (2002). Planning with a language for extended
goals. In Eighteenth national conference on Artificial intelligence, pp. 447454, Menlo
Park, CA, USA. American Association for Artificial Intelligence.
Lappas, T., Liu, K., & Terzi, E. (2009). Finding a team of experts in social networks.
In Proceedings of the 15th ACM SIGKDD international conference on Knowledge
discovery and data mining (KDD), pp. 467476, New York, NY, USA. ACM.
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford
University Press.
Passerone, R., de Alfaro, L., Henzinger, T. A., & Sangiovanni-Vincentelli, A. L. (2002). Convertibility verification and converter synthesis: two faces of the same coin. In ICCAD
02: Proceedings of the 2002 IEEE/ACM international conference on Computer-aided
design, pp. 132139, New York, NY, USA. ACM.
Pathak, J., Basu, S., & Honavar, V. (2008). Assembling composite web services from autonomous components. In Emerging Artificial Intelligence Applications in Computer
Engineering, Maglogiannis, I., Karpouzis, K., and Soldatos, J. (ed). IOS Press. In
press.
Rausand, M., & Hyland, A. (2003). System Reliability Theory: Models, Statistical Methods
and Applications Second Edition. Wiley-Interscience.
Regenwetter, M., Dana, J., & Davis-Stober, C. P. (2011). Transitivity of preferences. Psychological Review, 118 (1), 42  56.
Sacharidis, D., Papadopoulos, S., & Papadias, D. (2009). Topologically sorted skylines for
partially ordered domains. In ICDE 09: Proceedings of the 2009 IEEE International
Conference on Data Engineering, pp. 10721083, Washington, DC, USA. IEEE Computer Society.
Santhanam, G. R., Basu, S., & Honavar, V. (2008). TCP-compose - a TCP-net based
algorithm for efficient composition of web services using qualitative preferences. In
Bouguettaya, A., Krger, I., & Margaria, T. (Eds.), Procceedings of the Sixth International Conference on Service-Oriented Computing, Vol. 5364 of Lecture Notes in
Computer Science, pp. 453467.
273

fiSanthanam, Basu & Honavar

Santhanam, G. R., Basu, S., & Honavar, V. (2009). A dominance relation for unconditional
multi-attribute preferences. Tech. rep. 09-24, Department of Computer Science, Iowa
State University.
Santhanam, G. R., Basu, S., & Honavar, V. (2010a). Dominance testing via model checking. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence
(AAAI), pp. 357362. AAAI Press.
Santhanam, G. R., Basu, S., & Honavar, V. (2010b). Efficient dominance testing for unconditional preferences. In Proceedings of the Twelfth International Conference on
the Principles of Knowledge Representation and Reasoning (KR), pp. 590592. AAAI
Press.
Smythe, R. T., & Mahmoud, H. M. (1995). A survey of recursive trees. Theor Prob Math
Stat, pp. 127.
Traverso, P., & Pistore, M. (2004). Automated composition of semantic web services into
executable processes. In Proceedings of ISWC 2004, pp. 380394. Springer-Verlag.
Tversky, A. (1969). Intransitivity of preferences. Psychological Review, 76, 3148.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior.
Princeton University Press.
Wilson, N. (2004a). Consistency and constrained optimisation for conditional preferences.
In ECAI, pp. 888894.
Wilson, N. (2004b). Extending CP-nets with stronger conditional preference statements.
In AAAI, pp. 735741.
Yu, T., & Lin, K. J. (2005). Service selection algorithms for composing complex services
with multiple qos constraints. In Service-Oriented Computing - ICSOC 2005, pp.
130143. Springer Berlin / Heidelberg.
Zeng, L., Benatallah, B., Dumas, M., Kalagnanam, J., & Sheng, Q. Z. (2003). Quality
driven web services composition. In Proceedings of the 12th International Conference
on World Wide Web, pp. 411421. ACM.
Zeng, L., Benatallah, B., Ngu, A. H. H., Dumas, M., Kalagnanam, J., & Chang, H. (2004).
Qos-aware middleware for web services composition. IEEE Transactions on Software
Engineering, 30 (5), 311327.

274

fiJournal of Artificial Intelligence Research 42 (2011) 487-527

Submitted 7/11; published 11/11

Unfounded Sets and Well-Founded Semantics
of Answer Set Programs with Aggregates
Mario Alviano
Francesco Calimeri
Wolfgang Faber
Nicola Leone
Simona Perri

alviano@mat.unical.it
calimeri@mat.unical.it
faber@mat.unical.it
leone@mat.unical.it
perri@mat.unical.it

Department of Mathematics
University of Calabria
I-87030 Rende (CS), Italy

Abstract
Logic programs with aggregates (LPA ) are one of the major linguistic extensions to
Logic Programming (LP). In this work, we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone
aggregates (LPA
m,a programs). In particular, we present a new notion of unfounded set
A
for LPm,a programs, which is a sound generalization of the original definition for standard
(aggregate-free) LP. On this basis, we define a well-founded operator for LPA
m,a programs,
the fixpoint of which is called well-founded model (or well-founded semantics) for LPA
m,a
programs. The most important properties of unfounded sets and the well-founded semantics for standard LP are retained by this generalization, notably existence and uniqueness
of the well-founded model, together with a strong relationship to the answer set semantics for LPA
m,a programs. We show that one of the D-well-founded semantics, defined by
Pelov, Denecker, and Bruynooghe for a broader class of aggregates using approximating
operators, coincides with the well-founded model as defined in this work on LPA
m,a programs. We also discuss some complexity issues, most importantly we give a formal proof of
tractable computation of the well-founded model for LPA
m,a programs. Moreover, we prove
A
that for general LP programs, which may contain aggregates that are neither monotone
nor antimonotone, deciding satisfaction of aggregate expressions with respect to partial
interpretations is coNP-complete. As a consequence, a well-founded semantics for general
LPA programs that allows for tractable computation is unlikely to exist, which justifies the
restriction on LPA
m,a programs. Finally, we present a prototype system extending DLV,
which supports the well-founded semantics for LPA
m,a programs, at the time of writing the
only implemented system that does so. Experiments with this prototype show significant
computational advantages of aggregate constructs over equivalent aggregate-free encodings.

1. Introduction
The use of logical formulas as a basis for a knowledge representation language was proposed about 50 years ago in some seminal works of McCarthy (1959), and McCarthy and
Hayes (1969). However, it was soon realized that the monotonic nature of classical logic
(the addition of new knowledge may only increase the set of consequences of a theory in
classical logic) is not always suited to model commonsense reasoning, which sometimes is
intrinsically nonmonotonic (Minsky, 1975). As an alternative, it was suggested to represent
c
2011
AI Access Foundation. All rights reserved.

fiAlviano, Calimeri, Faber, Leone, & Perri

commonsense reasoning using logical languages with nonmonotonic consequence relations,
which can better simulate some forms of human reasoning, allowing new knowledge to invalidate some of the previous conclusions. This observation opened a new and important
research field, called nonmonotonic reasoning, and led to the definition and investigation of
new logical formalisms, called nonmonotonic logics. The most popular nonmonotonic logics
are circumscription (McCarthy, 1980, 1986), default logic (Reiter, 1980), and nonmonotonic
modal logics (McDermott & Doyle, 1980; McDermott, 1982; Moore, 1985). Later on, from
cross fertilizations between the field of nonmonotonic logics and that of logic programming,
another nonmonotonic language, called Declarative Logic Programming (LP) has emerged,
incorporating a nonmonotonic negation operator denoted by not. Declarative Logic Programming has gained popularity in the last years, and today it is a widely used formalism for
knowledge representation and reasoning, with applications in various scientific disciplines
and even in industry (Ricca, Alviano, Dimasi, Grasso, Ielpa, Iiritano, Manna, & Leone,
2010; Ricca, Grasso, Alviano, Manna, Lio, Iiritano, & Leone, 2011; Manna, Ricca, & Terracina, 2011; Manna, Ruffolo, Oro, Alviano, & Leone, 2011). In LP problems are solved by
means of declarative specifications of requirements to be achieved. No ad-hoc algorithms
are required.
Several semantics for LP have been proposed in the literature, which have to take care
about the inherent non-monotonicity of the not operator in programs. The well-founded
semantics (Van Gelder, Ross, & Schlipf, 1991) is one of the most prominent among them. It
associates a three-valued model, the well-founded model, to every logic program. Originally,
the well-founded semantics has been defined for normal logic programs, that is, standard
logic programs with nonmonotonic negation. A distinguishing property of the well-founded
semantics is that existence and uniqueness of the well-founded model is guaranteed for all
logic programs. Moreover, the well-founded semantics is computable in polynomial time
with respect to the input program in the propositional case.
Even if LP is a declarative programming language, standard LP does not allow for
representing properties over sets of data in a natural way, a relevant aspect in many application domains. For addressing this insufficiency, several extensions of LP have been
proposed, the most relevant of which is the introduction of aggregate functions (LPA ;
Kemp & Stuckey, 1991; Denecker, Pelov, & Bruynooghe, 2001; Dix & Osorio, 1997; Gelfond, 2002; Simons, Niemela, & Soininen, 2002; DellArmi, Faber, Ielpa, Leone, & Pfeifer,
2003; Pelov & Truszczynski, 2004; Pelov, Denecker, & Bruynooghe, 2004). Among them,
recursive definitions involving aggregate functions (i.e., aggregation in which aggregated
data depend on the evaluation of the aggregate itself) are particularly interesting, as the
definition of their semantics is not straightforward (Pelov, 2004; Faber, Leone, & Pfeifer,
2004; Son & Pontelli, 2007; Liu, Pontelli, Son, & Truszczynski, 2010). Note that a similar
construct, referred to as abstract constraint, has been introduced in the literature (Marek
& Truszczynski, 2004; Liu & Truszczynski, 2006; Son, Pontelli, & Tu, 2007; Truszczynski,
2010; Brewka, 1996). All of the results in this paper carry over also to LP with abstract
constraints, for which well-founded semantics to our knowledge has not been defined so far.
In this paper we focus on the fragment of LPA allowing for monotone and antimonotone
A
aggregate expressions (LPA
m,a ; Calimeri, Faber, Leone, & Perri, 2005). LPm,a programs
have many interesting properties. Among them, we highlight similarities between monotone
aggregate expressions and positive standard literals, and between antimonotone aggregate
488

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

expressions and negative standard literals. In particular, we take advantage of this aspect
for defining unfounded sets and, based on this definition, a well-founded semantics for the
A
LPA
m,a fragment. The well-founded semantics for LPm,a programs obtained in this way
retains many desirable properties of the original well-founded semantics for LP, which it
extends: For each LPA
m,a program a unique well-founded model exists, which is polynomialtime computable, approximates the programs answer sets, and coincides with the answer
set on stratified LPA
m,a programs.
Actually it turns out that the well-founded semantics thus obtained coincides (on LPA
m,a
programs) with a well-founded semantics proposed by Pelov, Denecker, and Bruynooghe
(2007). Pelov et al. define several semantics of logic programs with aggregates using various approximating immediate consequence operators. The notion of logic program adopted
by Pelov et al. is more general than the one considered in the present work, allowing
for arbitrary first-order formulas in bodies, unrestricted aggregates, and non-Herbrand interpretations. Because of the equivalence of the two semantics, some properties proved by
Pelov et al. carry over to this work as well. This applies to the results that the well-founded
model is total on stratified programs (Theorem 9), that the well-founded model is contained
in each answer set (Theorem 16), and that the well-founded model is computable in polynomial time (Theorem 21). However, the framework introduced in this article is considerably
different from the one developed by Pelov et al., which allows for giving alternative proofs
to these result. Vice versa, this article contains many new results, which carry over to the
framework of Pelov et al. on LPA
m,a programs. In particular, it provides an alternative
definition of the well-founded semantics, a characterization of answer sets by means of unfounded sets, and an implemented system computing the well-founded semantics, at the
time of writing the only one of its kind.
We would like to point out that for most extensions of LPA
m,a programs that come to
mind, the definition of unfounded sets would have to be considerably changed (see for instance the definition provided in Faber, 2005), and moreover the main desired properties
of the well-founded semantics would no longer be guaranteed. For instance, the most obvious extension, including aggregate expressions that are neither monotone nor antimonotone
would most likely not be computable in polynomial time: In fact, while the evaluation of
aggregate expressions with respect to partial interpretations is tractable for monotone and
antimonotone aggregates, the same task is coNP-complete for general aggregate expressions. Also, for instance allowing aggregates in rule heads would necessarily complicate the
definition of unfounded sets, would not guarantee the existence of a well-founded model for
every program, and would most likely not guarantee polynomial-time computability.
The concepts defined in this paper directly give rise to a computation method for the
well-founded semantics on LPA
m,a programs. We have implemented this method, which is
to the best of our knowledgethe first of its kind. We have conducted experiments with
this system on LPA
m,a encodings of a particular problem domain, and compared it with
encodings not using aggregates. The latter encodings were tested with the system from
which our prototype was derived and with XSB, a state-of-the-art system for computing
the well-founded model. The experiments show a clear advantage of the LPA
m,a encodings
run on our prototype system.
Summarizing, the main contributions of the paper are as follows.
489

fiAlviano, Calimeri, Faber, Leone, & Perri

 We define a new notion of unfounded set for logic programs with monotone and
antimonotone aggregates (LPA
m,a programs). This notion is a sound generalization of
the concept of unfounded set previously given for standard logic programs. We show
that our definition coincides with the original definition of unfounded sets (Van Gelder
et al., 1991) on the class of normal (aggregate-free) programs, and that it shares its
distinguishing properties (such as the existence of the greatest unfounded set).
 We define a well-founded operator WP for logic programs with aggregates, which extends the classical well-founded operator (Van Gelder et al., 1991). The total fixpoints
of WP are exactly the answer sets of P, and its least fixpoint WP () is contained in
the intersection of all answer sets. We also show that the operator is equivalent to an
operator defined by Pelov et al. (2007).
 We provide a declarative characterization of answer sets in terms of unfounded sets.
In particular, we prove that the answer sets of an LPA
m,a program are precisely the
unfounded-free models.
 We show that reasoning with aggregates without restrictions may easily increase the
complexity of the computation. In particular, we prove that deciding the truth
or falsity of an aggregate expression with respect to a partial interpretation is a
coNP-complete problem. However, while the problem is intractable in general, it
is polynomial-time solvable for monotone and antimonotone aggregates.
 We analyze the complexity of the well-founded semantics, confirming and extending
results in the work of Pelov et al. (2007). Importantly, it turns out that WP ()
is polynomial-time computable for propositional LPA
m,a programs. For non-ground
programs, the data-complexity remains polynomial, while the program complexity
rises from P to EXPTIME, as for aggregate-free programs.
 We present a prototype system supporting the well-founded semantics defined in this
article. The prototype, obtained by extending DLV, is the first system implementing
a well-founded semantics for (unrestricted) LPA
m,a programs.
 We report on experimental results on the implemented prototype. More specifically,
we define the Attacks problem, a problem inspired by the classic Win-Lose problem
often considered in the context of the well-founded semantics for standard logic programs. We compare the execution times of our prototype with an LPA
m,a encoding and
with equivalent LP encodings. In particular, one of the tested LP encodings is obtained by means of a compilation of aggregates into standard LP, which is also briefly
presented in this paper. The obtained results evidence computational advantages for
the problem encoding using aggregate expressions over those without them.
The presentation is organized as follows. In Section 2 we present the basics of the
LPA language and, in particular, we introduce the LPA
m,a fragment. For this fragment,
we define unfounded sets and well-founded semantics in Section 3. Relationships between
well-founded semantics and answer set semantics are discussed in Section 4. A complexity
analysis of the well-founded semantics for LPA
m,a programs is reported in Section 5. In
490

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Section 6 we discuss the implemented prototype system and the experimentation. Finally,
related work is discussed in Section 7, and in Section 8 we draw our conclusions.

2. The LPA Language
Syntax, instantiation, interpretations and models of LPA programs are introduced in this
section. Moreover, we introduce the LPA
m,a fragment of the language, for which we define a
well-founded semantics in Section 3. For additional background on standard LP, we refer
to the literature (Gelfond & Lifschitz, 1991; Baral, 2003).
2.1 Syntax
We assume sets of variables, constants, and predicates to be given. Similar to Prolog, we
assume variables to be strings starting with uppercase letters and constants to be nonnegative integers or strings starting with lowercase letters. Predicates are strings starting
with lowercase letters. An arity (non-negative integer) is associated with each predicate.
Moreover, the language allows for using built-in predicates (i.e., predicates with a fixed
meaning) for the common arithmetic operations over positive integers (i.e., =, , , +, ,
etc.; written in infix notation), which are interpreted in the standard mathematical way.
2.1.1 Standard Atom
A term is either a variable or a constant. A standard atom is an expression p(t1 , . . . , tn ),
where p is a predicate of arity n and t1 , . . . , tn are terms. An atom p(t1 , . . . , tn ) is ground if
t1 , . . . , tn are constants.
2.1.2 Set Term
A set term is either a symbolic set or a ground set. A symbolic set is a pair {Terms : Conj },
where Terms is a list of terms (variables or constants) and Conj is a conjunction of standard
atoms, that is, Conj is of the form a1 , . . . , ak and each ai (1  i  k) is a standard
atom. Intuitively, a set term {X : a(X, c), p(X)} stands for the set of X-values making the
conjunction a(X, c), p(X) true, i.e., {X | a(X, c) and p(X) are true}. A ground set is a set of
pairs of the form hconsts : conj i, where consts is a list of constants and conj is a conjunction
of ground standard atoms.
2.1.3 Aggregate Function
An aggregate function is of the form f (S), where S is a set term, and f is an aggregate
function symbol. Intuitively, an aggregate function can be thought of as a (possibly partial)
function mapping multisets of constants to a constant. Throughout the remainder of the
paper, we will adopt the notation of the DLV system (Leone, Pfeifer, Faber, Eiter, Gottlob,
Perri, & Scarcello, 2006) for representing aggregates.
Example 1 The most common aggregate functions are listed below:
 #min, minimal term, undefined for the empty set;
 #max, maximal term, undefined for the empty set;
491

fiAlviano, Calimeri, Faber, Leone, & Perri

 #count, number of terms;
 #sum, sum of integers;
 #times, product of integers;
 #avg, average of integers, undefined for the empty set.
2.1.4 Aggregate Atom
An aggregate atom is a structure of the form f (S)  T , where f (S) is an aggregate function,
  {<, , >, } is a comparison operator, and T is a term (variable or constant). An
aggregate atom f (S)  T is ground if T is a constant and S is a ground set.
Example 2 The following are aggregate atoms in DLV notation:
#max{Z : r(Z), a(Z, V )} > Y
#max{h2 : r(2), a(2, m)i, h2 : r(2), a(2, n)i} > 1

2.1.5 Literal
A literal is either (i) a standard atom, or (ii) a standard atom preceded by the negation as
failure symbol not, or (iii) an aggregate atom. Two standard literals are complementary if
they are of the form a and not a, for some standard atom a. For a standard literal , we
denote by . the complement of . Abusing of notation, if L is a set of standard literals,
then .L denotes the set {. |   L}.
2.1.6 Program
A rule r is a construct of the form
a : 1 , . . . , m .

where a is a standard atom, 1 , . . . , m are literals, and m  0. The atom a is referred
to as the head of r, and the conjunction 1 , . . . , m as the body of r. If the body is empty
(m = 0), then the rule is called fact. We denote the head atom by H(r) = a, and the set of
body literals by B(r) = {1 , . . . , m }. Moreover, the set of positive standard body literals
is denoted by B + (r), the set of negative standard body literals by B  (r), and the set of
aggregate body literals by B A (r). A rule r is ground if H(r) and all the literals in B(r) are
ground. A program is a set of rules. A program is ground if all its rules are ground.
2.1.7 Safety
A local variable of a rule r is a variable appearing solely in sets terms of r; a variable of r
which is not local is global. A rule r is safe if both the following conditions hold: (i) for each
global variable X of r there is a positive standard literal   B + (r) such that X appears in
; (ii) each local variable of r appearing in a symbolic set {Terms : Conj } also appears in
Conj . Note that condition (i) is the standard safety condition adopted in LP to guarantee
that the variables are range restricted (Ullman, 1989), while condition (ii) is specific for
aggregates. A program is safe if all its rules are safe.
492

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Example 3 Consider the following rules:
p(X) : q(X, Y, V ), #max{Z : r(Z), a(Z, V )} > Y.
p(X) : q(X, Y, V ), #sum{Z : r(X), a(X, S)} > Y.
p(X) : q(X, Y, V ), #min{Z : r(Z), a(Z, V )} > T.

The first rule is safe, while the second is not because the local variable Z violates condition
(ii). Also the third rule is not safe, since the global variable T violates condition (i).
2.2 Program Instantiation, Interpretations and Models
In Section 3 we define a well-founded semantics for a relevant class of LPA programs.
The well-founded semantics is defined for ground programs, while programs with variables
are associated with equivalent ground programs. In this section we introduce preliminary
notions such as program instantiation, interpretations and models.
2.2.1 Universe and Base
Given an LPA program P, the universe of P, denoted by UP , is the set of constants
appearing in P. The base of P, denoted by BP , is the set of standard atoms constructible
from predicates of P with constants in UP .
2.2.2 Instantiation
A substitution is a mapping from a set of variables to UP . Given a substitution  and
an LPA object obj (rule, set, etc.), we denote by obj  the object obtained by replacing
each variable X in obj by (X). A substitution from the set of global variables of a rule
r (to UP ) is a global substitution for r; a substitution from the set of local variables of a
set term S (to UP ) is a local substitution for S. Given a set term without global variables
S = {Terms : Conj }, the instantiation of S is the following ground set:
inst(S) = {hTerms  : Conj i |  is a local substitution for S}.
A ground instance of a rule r is obtained in two steps: First, a global substitution  for r
is applied, and then every set term S in r is replaced by its instantiation inst(S). The
instantiation Ground(P) of a program P is the set of instances of all the rules in P.
Example 4 Consider the following program P1 :
q(1) : not p(2, 2).
p(2, 2) : not q(1).

q(2) : not p(2, 1).
p(2, 1) : not q(2).

t(X) : q(X), #sum{Y : p(X, Y )} > 1.

The instantiation Ground(P1 ) of P1 is the following program:
q(1) : not p(2, 2).
p(2, 2) : not q(1).

q(2) : not p(2, 1).
p(2, 1) : not q(2).

t(1) : q(1), #sum{h1 : p(1, 1)i, h2 : p(1, 2)i} > 1.
t(2) : q(2), #sum{h1 : p(2, 1)i, h2 : p(2, 2)i} > 1.

2.2.3 Aggregate Function Domain
X

Given a set X, let 2 denote the set of all multisets over elements from X. The domain of
an aggregate function is the set of multisets on which the function is defined. Without loss
of generality, we assume that aggregate functions map to Z (the set of integers).
493

fiAlviano, Calimeri, Faber, Leone, & Perri

Example 5 Let us look at common domains for the aggregate functions of Example 1:
U
Z
#count is defined over 2 ,P #sum and #times over 2 , #min, #max and #avg over
Z
2 \ {}.
2.2.4 Interpretation
An interpretation I for an LPA program P is a consistent set of standard ground literals,
that is, I  BP  .BP and I  .I = . We denote by I + and I  the set of standard
positive and negative literals occurring in I, respectively. An interpretation I is total if
I +  .I  = BP , otherwise I is partial. The set of all the interpretations of P is denoted
by IP . Given an interpretation I and a standard literal , the evaluation of  with respect
to I is defined as follows: (i) if   I, then  is true with respect to I; (ii) if .  I, then
 is false with respect to I; (iii) otherwise, if  6 I and . 6 I, then  is undefined with
respect to I. An interpretation also provides a meaning to set terms, aggregate functions
and aggregate literals, namely a multiset, a value, and a truth value, respectively. We
first consider a total interpretation I. The evaluation I(S) of a set term S with respect
to I is the multiset I(S) defined as follows: Let S I = {ht1 , ..., tn i | ht1 , ..., tn : Conj i 
S and all the atoms in Conj are true with respect to I}; I(S) is the multiset obtained as
the projection of the tuples of SI on their first constant, that is, I(S) = [t1 | ht1 , ..., tn i  S I ].
The evaluation I(f (S)) of an aggregate function f (S) with respect to I is the result of the
application of f on I(S).1 If the multiset I(S) is not in the domain of f , then I(f (S)) = 
(where  is a fixed symbol not occurring in P). A ground aggregate atom  = f (S)  k is
true with respect to I if both I(f (S)) 6=  and I(f (S))  k hold; otherwise,  is false.
Example 6 Let I1 be a total interpretation having I1+ = {f (1), g(1, 2), g(1, 3), g(1, 4), g(2, 4),
h(2), h(3), h(4)}. Assuming that all variables are local, we can check that:
 #count{X : g(X, Y )} > 2 is false; indeed, if S1 is the corresponding ground set, then
S1I1 = {h1i, h2i}, I1 (S1 ) = [1, 2] and #count([1, 2]) = 2.
 #count{X, Y : g(X, Y )} > 2 is true; indeed, if S2 is the corresponding ground set,
then S2I1 = {h1, 2i, h1, 3i, h1, 4i, h2, 4i}, I1 (S2 ) = [1, 1, 1, 2] and #count([1, 1, 1, 2]) = 4.
 #times{Y : f (X), g(X, Y )} <= 24 is true; indeed, if S3 is the corresponding ground
set, then S3I1 = {h2i, h3i, h4i}, I1 (S3 ) = [2, 3, 4] and #times([2, 3, 4]) = 24.
 #sum{X : g(X, Y ), h(Y )} <= 3 is true; indeed, if S4 is the corresponding ground set,
then S4I1 = {h1i, h2i}, I1 (S4 ) = [1, 2] and #sum([1, 2]) = 3.
 #sum{X, Y : g(X, Y ), h(Y )} <= 3 is false; indeed, if S5 is the corresponding ground
set, then S5I1 = {h1, 2i, h1, 3i, h1, 4i, h2, 4i}, I1 (S5 ) = [1, 1, 1, 2] and #sum([1, 1, 1, 2]) =
5.;
 #min{X : f (X), h(X)} >= 2 is false; indeed, if S6 is the corresponding ground set,
then S6I1 = , I1 (S6 ) = , and I1 (#min()) =  (we recall that  is not in the domain
of #min).
1. In this paper, we only consider aggregate functions value of which is polynomial-time computable with
respect to the input multiset.

494

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

We now consider a partial interpretation I and refer to an interpretation J such that
I  J as an extension of I. If a ground aggregate atom  is true (resp. false) with respect
to each total interpretation J extending I, then  is true (resp. false) with respect to I;
otherwise,  is undefined.
Example 7 Let S7 be the ground set in the literal 1 = #sum{h1 : p(2, 1)i, h2 : p(2, 2)i} >
1, and consider a partial interpretation I2 = {p(2, 2)}. Since each total interpretation
extending I2 contains either p(2, 1) or not p(2, 1), we have either I2 (S7 ) = [2] or I2 (S7 ) =
[1, 2]. Thus, the application of #sum yields either 2 > 1 or 3 > 1, and thus 1 is true with
respect to I2 .
Remark 1 Observe that our definitions of interpretation and truth values preserve knowledge monotonicity: If an interpretation J extends I (i.e., I  J), each literal which is true
with respect to I is true with respect to J, and each literal which is false with respect to I
is false with respect to J as well.
2.2.5 Model
Given an interpretation I, a rule r is satisfied with respect to I if at least one of the following
conditions is satisfied: (i) H(r) is true with respect to I; (ii) some literal in B(r) is false
with respect to I; (iii) H(r) and some literal in B(r) are undefined with respect to I. An
interpretation M is a model of an LPA program P if all the rules r in Ground(P) are
satisfied with respect to M .
Example 8 Consider again the program P1 of Example 4. Let I3 be a total interpretation
for P1 such that I3+ = {q(2), p(2, 2), t(2)}. Then I3 is a minimal model of P1 .
2.3 The LPA
m,a Language
A
The definition of LPA
m,a programs, the fragment of LP analyzed in this paper, is based on
the following notion of monotonicity of literals.

2.3.1 Monotonicity
Given two interpretations I and J, we say that I  J if I +  J + and I   J  . A
ground literal  is monotone if, for all interpretations I, J such that I  J, we have that:
(i)  true with respect to I implies  true with respect to J, and (ii)  false with respect
to J implies  false with respect to I. A ground literal  is antimonotone if the opposite
happens, that is, for all interpretations I, J such that I  J, we have that: (i)  false with
respect to I implies  false with respect to J, and (ii)  true with respect to J implies
 true with respect to I. A ground literal  is nonmonotone if  is neither monotone nor
antimonotone. Note that positive standard literals are monotone, whereas negative standard
literals are antimonotone. Aggregate literals, instead, may be monotone, antimonotone or
nonmonotone. Some examples are shown below and the complete picture for the most
common aggregate functions is summarized in Table 1.
Example 9 Let us assume a universe in which all numerical constants are non-negative
integers. All ground instances of the following aggregate literals are thus monotone:
495

fiAlviano, Calimeri, Faber, Leone, & Perri

Table 1: Character of the most common aggregate literals.
Function Domain Operator
Character
#count
any
>, 
monotone
<, 
antimonotone
#sum
N
>, 
monotone
<, 
antimonotone
Z
<, , >,  nonmonotone
#times
N+
>, 
monotone
<, 
antimonotone
N, Z
<, , >,  nonmonotone
#min
any
>, 
nonmonotone
<, 
monotone
#max
any
>, 
monotone
<, 
nonmonotone
#avg
N, Z
<, , >,  nonmonotone


Antimonotone if the context guarantees that the set term of the aggregate never becomes empty.

#sum{Z : r(Z)}  10.

#count{Z : r(Z)} > 1;

Ground instances of the following literals are instead antimonotone:
#sum{Z : r(Z)}  10.

#count{Z : r(Z)} < 1;

2.3.2 LPA
m,a Programs
A
Let LPA
m,a denote the fragment of LP allowing monotone and antimonotone literals. For
an LPA
m,a rule r, the set of its monotone and antimonotone body literals are denoted by
m
B (r) and B a (r), respectively. An LPA
m,a program P is stratified if there exists a function
||  ||, called level mapping, from the set of predicates of P to ordinals, such that for each
pair a, b of predicates, occurring in the head and body of a rule r  P, respectively: (i) if b
appears in an antimonotone literal, then ||b|| < ||a||, (ii) otherwise ||b||  ||a||. Intuitively,
stratification forbids recursion through antimonotone literals (for aggregate-free programs
this definition coincides with the common notion of stratification with respect to negation).

Example 10 Consider an LPA
m,a program consisting of the following rules:
q(X) : p(X), #count{Y : a(Y, X), b(X)}  2.
p(X) : q(X), b(X).

and assume that the predicates a and b are defined by facts, which we do not include
explicitly. The program is stratified, as the level mapping ||a|| = ||b|| = 1, ||p|| = ||q|| = 2
satisfies the required conditions. If we add the rule b(X) : p(X), then no such levelmapping exists, and the program becomes unstratified.
We would like to note that the definition of LPA
m,a could be enlarged, as in the form
given above classifies literals independently of the context (that is, the program) in which
496

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

they occur. Some aggregates that are nonmonotone by the definition given above, might
not manifest their nonmonotone effects in a given context: If one limits the interpretations
to be considered to those that do not violate the program in which the literal occurs,
some interpretation pairs that violate monotonicity and antimonotonicity may no longer
be present. In fact, one could refine the definition in this way (considering only pairs of
non-violating interpretations of a given context program). The modified definition would
enlarge the class of LPA
m,a programs, while retaining all of the results in this paper, but for
simplicity of exposition we refrain from doing it formally. As an example, any aggregate
atom involving #max with a < operator is formally not in LPA
m,a , but when one considers
occurrences in a program that has no non-violating interpretation I such that I(S) = 
(where S the set term of the aggregate), then the aggregate behaves in an antimonotone
way in that particular program. We have noted these cases by a footnote in Table 1.

3. Unfounded Sets and Well-Founded Semantics
In this section we introduce a new notion of unfounded set for LPA
m,a programs, which
extends the original definition for aggregate-free programs introduced by Van Gelder et al.
(1991). Unfounded sets are then used for extending the well-founded semantics, originally
defined for aggregate-free programs by Van Gelder et al., to LPA
m,a programs. We also
highlight a number of desirable properties of this semantics. In the following we deal with
ground programs, so we will usually denote by P a ground program. We will also use the
notation L  .L for the set (L \ L )  .L , where L and L are sets of standard ground
literals.
Definition 1 (Unfounded Set) A set X  BP of ground atoms is an unfounded set for
an LPA
m,a program P with respect to a (partial) interpretation I if and only if, for each rule
r  P having H(r)  X , at least one of the following conditions holds:
(1) some (antimonotone) literal in B a (r) is false with respect to I, or
(2) some (monotone) literal in B m (r) is false with respect to I  .X .
Intuitively, each rule with its head atom belonging to an unfounded set X is already
satisfied with respect to I (in case condition (1) holds), or it is satisfiable by taking as false
all the atoms in the unfounded set (in case condition (2) holds). Note that, according to
the definition above, the empty set is an unfounded set with respect to every program and
interpretation.
Example 11 Consider an interpretation I4 = {a(1), a(2), a(3)} for the following program
P2 :
r1 :
r2 :
r3 :

a(1) : #count{h1 : a(1)i, h2 : a(2)i, h3 : a(3)i} > 2.
a(2).
a(3) : #count{h1 : a(1)i, h2 : a(2)i, h3 : a(3)i} > 2.

Then X1 = {a(1)} is an unfounded set for P2 with respect to I4 , since condition (2) of
Definition 1 holds for r1 (the only rule with head a(1)). Indeed, the (monotone) literal
appearing in B m (r1 ) is false with respect to I4  .X1 = {not a(1), a(2), a(3)}. Similarly,
{a(3)} and {a(1), a(3)} are unfounded sets for P2 with respect to I4 . Clearly, also  is an
unfounded set. All other sets of atoms are not unfounded for P2 with respect to I4 .
497

fiAlviano, Calimeri, Faber, Leone, & Perri

As formalized below, Definition 1 generalizes the one given by Van Gelder et al. (1991)
for aggregate-free programs: A set of standard atoms X  BP is an unfounded set for a
program P with respect to an interpretation I if and only if, for each rule r  P such that
H(r)  X , either (i) B(r)  .I 6= , or (ii) B + (r)  X =
6 .
Theorem 1 For an aggregate-free program P, Definition 1 is equivalent to the one introduced in the work of Van Gelder et al. (1991).
Proof. For an aggregate-free program P, conditions (1) and (2) of Definition 1 are equivalent to (a) B  (r)  .I 6=  and (b) B + (r)  .(I  .X ) 6= , respectively. Condition (b)
is equivalent to B + (r)  (.(I \ X )  ..X ) 6= , which holds if and only if either (b.1)
B + (r)  .(I \ X ) 6= , or (b.2) B + (r)  X =
6 . Condition (b.2) is exactly condition (ii)
in the work of Van Gelder et al. Concerning condition (b.1), since B + (r) contains only
positive literals, we can ignore the negative literals in .(I \ X ), that is, the positive literals
in I \ X . By noting that the negative literals in I \ X are precisely the negative literals in
I, we can then conclude that (b.1) is equivalent to B + (r)  .I 6= . Finally, by combining
the previous statement with condition (a) above, we obtain condition (i) in the work of Van
Gelder et al.

Thus, Definition 1 is an alternative characterization of unfounded sets for aggregate-free
programs. In fact, while condition (1) of Definition 1 does not exactly cover the first one in
Van Gelder et al., condition (2) catches all cases of the second in the work of Van Gelder
et al. and those missed by condition (1).
Theorem 2 If X and X  are unfounded sets for an LPA
m,a program P with respect to an

interpretation I, then X  X is an unfounded set for P with respect to I.
Proof. Let r  P be such that H(r)  X  X  . We want to show that either (1) some
(antimonotone) literal in B a (r) is false with respect to I, or (2) some (monotone) literal
in B m (r) is false with respect to J = I  .(X  X  ). By symmetry, we can assume that
H(r) belongs to X . Since X is an unfounded set with respect to I by hypothesis, either
(a) some (antimonotone) literal in B a (r) is false with respect to I, or (b) some (monotone)
literal in B m (r) is false with respect to K = I  .X . Case (a) is equals to (1). Thus, it
remains to prove that case (b) implies (2). Indeed, we have that J  K because J +  K +
and J   K  . Therefore, by definition of monotonicity, each monotone literal  which is
false with respect to K is false with respect to J as well, and so we are done.

As a corollary of Theorem 2, the union of all the unfounded sets is an unfounded set as
well.
Corollary 3 The union of all the unfounded sets for an LPA
m,a program P with respect to
an interpretation I is an unfounded set for P with respect to I as well. We refer to this set
as the greatest unfounded set of P with respect to I, denoted by GU SP (I).
Below is an important monotonicity property of the greatest unfounded set.
Proposition 4 Let I and J be interpretations for an LPA
m,a program P. If I  J, then
GU SP (I)  GU SP (J).
498

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Proof. Since GU SP (J) is the union of all the unfounded sets for P with respect to J by
definition, it is enough to show that X = GU SP (I) is an unfounded set for P with respect
to J. Thus, we want to show that, for each rule r  P such that H(r)  X , either (1) some
(antimonotone) literal in B a (r) is false with respect to J, or (2) some (monotone) literal in
B m (r) is false with respect to J  .X . We already know that X is an unfounded set for P
with respect to I by Corollary 3. Therefore, either (a) some (antimonotone) literal in B a (r)
is false with respect to I, or (b) some (monotone) literal in B m (r) is false with respect to
I  .X . Since I  J, we have that J and J  .X are extensions of the interpretations I
and I  .X , respectively. Hence, by Remark 1, (a) implies (1) and (b) implies (2), and so
we are done.

We are now ready for extending the well-founded operator defined by Van Gelder et al.
(1991) to the case of LPA
m,a programs.
Definition 2 Let P be an LPA
m,a program. The immediate logical consequence operator
B
P
TP : IP  2
and the well-founded operator WP : IP  2BP .BP are defined as follows:
TP (I) = {  BP | r  P such that H(r) = 
and all the literals in B(r) are true with respect to I}
WP (I) = TP (I)  .GU SP (I).
Intuitively, given an interpretation I for a program P, WP derives as true a set of
atoms belonging to every model extending I (by means of the TP operator). Moreover,
WP derives as false all the atoms belonging to some unfounded set for P with respect to
I (by means of the GU SP operator). Note that TP (I) and GU SP (I) are set of atoms, so
WP (I)+ = TP (I) and WP (I) = .GU SP (I). The following proposition formalizes the
intuition that Definition 2 extends the WP operator defined by Van Gelder et al. (1991) for
standard programs to LPA
m,a programs.
Proposition 5 Let P be an aggregate-free program. The WP operator of Definition 2
coincides with the WP operator defined by Van Gelder et al. (1991).
Proof. Since WP is equal to the union of TP and .GU SP in both cases, we have just to
show that our definitions of TP and GU SP coincide with those introduced by Van Gelder
et al. (1991) for aggregate-free programs.
 The two immediate logical consequence operators (TP ) coincide for an aggregate-free
program P. Indeed, for each rule r  P, B(r) has only standard literals.
 Our definition of GU SP (I) coincides with the one of Van Gelder et al. (1991) for an
aggregate-free program P and an interpretation I. Indeed, in both cases GU SP (I) is
defined as the union of all the unfounded sets for P with respect to I, and our notion
of unfounded set coincides with the one in the work of Van Gelder et al. for standard
programs by Theorem 1.

We next show that a fixpoint of the well-founded operator WP is a (possibly partial)
model.
499

fiAlviano, Calimeri, Faber, Leone, & Perri

{a, b}

{a, not b}

{a}

{b}

{not a, b} {not a, not b}

{not b}

{not a}


Figure 1: A meet semilattice
Theorem 6 Let P be an LPA
m,a program and M a (partial) interpretation. If M is a
fixpoint of WP , then M is a (partial) model of P.
Proof. Let us assume that WP (M ) = M holds. Thus, TP (M )  M and .GU SP (M )  M
hold. Consider now a rule r  P. If all the literals in B(r) are true with respect to M ,
then H(r)  TP (M )  M . If H(r) is false with respect to M , then H(r)  GU SP (M ).
Since GU SP (M ) is an unfounded set for P with respect to M by Corollary 3, either some
literal in B a (r) is false with respect to M , or some literal in B m (r) is false with respect to
M  .GU SP (M ) = M . We can then conclude that r is satisfied by M .

The theorem below states that WP is a monotone operator in the meet semilattice induced on IP by the subset-containment relationship. We recall here that a meet semilattice
is a partially ordered set which has a meet (or greatest lower bound) for any nonempty finite
subset. An example of such a meet semilattice for a program with base {a, b} is reported
in Figure 1.
Theorem 7 Let P be an LPA
m,a program. The well-founded operator WP is a monotone
operator in the meet semilattice hIP , i.
Proof. Since WP is equal to the union of TP and .GU SP by Definition 2, we have just to
prove the monotonicity of the operators TP and GU SP .
 We first show that TP is a monotone operator, that is, for each pair of interpretations
I, J for P such that I  J, it holds that TP (I)  TP (J). Consider an atom   TP (I).
By Definition 2, there is a rule r  P such that H(r) =  and all the literals in B(r)
are true with respect to I. Since I  J, we can conclude that all the literals in B(r)
are true with respect to J as well (see Remark 1), and so H(r) =  belongs to TP (J)
by Definition 2.
 We already know that GU SP is a monotone operator from Proposition 4: For each
pair of interpretations I, J for P such that I  J, it holds that GU SP (I)  GU SP (J).

We can now prove that the sequence W0 = , Wn+1 = WP (Wn ) is well-defined, that is,
each element of the sequence is an interpretation.
Theorem 8 Let P be an LPA
m,a program. The sequence W0 = , Wn+1 = WP (Wn ) is
well-defined.
500

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Proof. We use strong induction. The base case is trivial, since W0 = . In order to prove
the consistency of Wn+1 = TP (Wn ).GU SP (Wn ), we assume the consistency of every Wm
such that m  n. Since WP is a monotone operator by Theorem 7, it is enough to show
that GU SP (Wn )  Wn+1 = . To this end, we next show that any set X of atoms such that
X  Wn+1 6=  is not an unfounded set for P with respect to Wn (and so is not contained in
GU SP (Wn )). Let Wm+1 be the first element of the sequence such that X  Wm+1 6=  (note
that m  n). Consider any atom   X  Wm+1 . By definition of TP , there is a rule r  P
having H(r) =  and such that all the literals in B(r) are true with respect to Wm . Note
that no atom in Wm can belong to X (for the way in which Wm+1 has been chosen). Thus,
by Remark 1, all the literals in B(r) are true with respect to both Wn and Wn  .X (we
recall that Wn  Wm because WP is monotone). This ends the proof, as neither condition
(1) nor (2) of Definition 1 hold for .

Theorem 8 and Theorem 7 imply that WP admits a least fixpoint (Tarski, 1955), which
is referred to as the well-founded model of P. The well-founded semantics of an LPA
m,a
program P is given by this model. We can now state a first important property of the
well-founded semantics of LPA
m,a programs.
Property 1 For every LPA
m,a program, the well-founded model always exists and is unique.
Another important property of well-founded semantics easily follows from Proposition 5.
Property 2 On aggregate-free programs, the well founded semantics as defined in this
paper coincides with the classical well-founded semantics of Van Gelder et al. (1991).
Although the well-founded model, in general, might leave some atoms as undefined,
there are cases where WP () is a total interpretation.
Example 12 Consider the following program P3 :
a(1) : #sum{h1 : a(1)i, h2 : a(2)i} > 2.
a(2) : b.
b : not c.

The iterated application of WP yields the following sets:
1.
2.
3.

WP () = {not a(1), not c};
WP ({not a(1), not c}) = {not a(1), not c, b};
WP ({not a(1), not c, b}) = {not a(1), not c, b, a(2)} = WP ().

In this case, the well-founded model is total. Indeed, each atom in BP is either true or false
with respect to WP ().
The totality of the well-founded model of the program above is due to its stratification,
as formalized by the next theorem. Given Corollary 25, an equivalent result has been stated
already by Pelov et al. (2007) as Theorem 7.2 and its Corollary 7.1. However, its proof is
labelled as sketch by Pelov et al., which moreover relies on rather different formalisms than
our proof.
501

fiAlviano, Calimeri, Faber, Leone, & Perri

Theorem 9 On stratified LPA
m,a programs, the well-founded model is total.

Proof. Let P be a stratified LPA
m,a program. In order to prove that WP () is total, we
show that each (standard) atom in BP \WP () is false with respect to WP (). By definition
of stratification, there is a level mapping ||  || of the (standard) predicates of P such that,
for each pair a, b of standard predicates occurring in the head and body of a rule r  P,
respectively, the following conditions are satisfied: (i) if b appears in an antimonotone literal,
then ||b|| < ||a|| holds; (ii) otherwise, if b appears in a monotone literal, then ||b||  ||a||
holds. We are then in order to define a non-decreasing sequence of subsets of BP as follows:

L0 = 
Li+1 = Li  {  BP | the predicate of  is p and ||p|| = i},

i  N.

Our aim is then to show that, for each i  N, the set Li+1 \WP () is contained in .WP () .
We use induction on i. The base case is trivial because L0 =  holds by definition. Now
suppose that all the atoms in Li \ WP () are false with respect to WP () in order to show
that all the atoms in Li+1 \ WP () are false with respect to WP () as well. To this end,
we prove that Li+1 \ WP () is an unfounded set for P with respect to WP (). Consider
a rule r  Ground(P) with H(r)  Li+1 \ WP (). We want to show that either (1) some
(antimonotone) literal in B a (r) is false with respect to WP (), or (2) some (monotone)
literal in B m (r) is false with respect to WP ()  .(Li+1 \ WP ()). Since H(r)  Li+1 , by
definition of stratification the following propositions hold:
(a) each literal in B a (r) is either a negated standard atom belonging to Li , or an aggregate
literal depending only on atoms in Li ;
(b) each literal in B m (r) is either a standard atom belonging to Li+1 , or an aggregate
literal depending only on atoms in Li+1 .
Since H(r) 6 WP () (that is, H(r) 6 TP (WP ())), there is a literal  in B(r) such that 
is not true with respect to WP () (by definition of TP ). If  is an antimonotone literal, we
apply (a) and the induction hypothesis and conclude that (1) holds ( cannot be undefined
with respect to WP (), so  must be false). If  is a monotone literal, we apply (b) and the
induction hypothesis and conclude that (2) holds ( cannot be undefined with respect to
WP ()  .(Li+1 \ WP ()) and WP ()  .(Li+1 \ WP ())  WP () holds, so  must be
false).


4. Answer Set Characterization via Unfounded Sets
The well-founded semantics is a three-valued semantics, that is, each program is associated with a model in which atoms are either true, false or undefined. Other semantics in
the literature associate programs with two-valued models (i.e., models without undefined
atoms). A commonly accepted two-value semantics in LP is the answer set semantics. In
this section we present a number of results concerning unfounded sets and answer sets of
LPA
m,a programs. We first recall the definition of answer sets provided by Faber, Leone,
and Pfeifer (2011).
502

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Definition 3 (Minimal Model) A total model M for P is (subset-)minimal if no total
model N for P exists such that N +  M + . Note that, under these definitions, the words
interpretation and model refer to possibly partial interpretations, while a minimal model is
always a total interpretation.
We next provide the transformation by which the reduct of a ground program with
respect to a total interpretation is formed. Note that this definition is a generalization (Faber
et al., 2004) of the Gelfond-Lifschitz transformation (1991) for standard logic programs.
Definition 4 (Program Reduct) Given an LPA program P and a total interpretation
I, let Ground(P)I denote the transformed program obtained from Ground(P) by deleting
rules in which a body literal is false with respect to I, i.e.:
Ground(P)I = {r  Ground(P) | all the literals in B(r) are true with respect to I}.
We are now ready for introducing the notion of answer set for LPA programs.
Definition 5 (Answer Set for LPA Programs) Given an LPA program P, a total interpretation M of P is an answer set of P if and only if M is a minimal model of
Ground(P)M .
Example 13 Consider two total interpretations I5 = {p(0)} and I6 = {not p(0)} for the
following two programs:
P4 = {p(0) : #count{X : p(X)} > 0.}
P5 = {p(0) : #count{X : p(X)}  0.}

We then obtain the following transformed programs:
Ground(P4 )I5
Ground(P4 )I6
Ground(P5 )I5
Ground(P5 )I6

= Ground(P4 ) = {p(0) : #count{h0 : p(0)i} > 0.}
=
=
= Ground(P5 ) = {p(0) : #count{h0 : p(0)i}  0.}

Hence, I6 is the only answer set of P4 . Indeed, I5 is not a minimal model of Ground(P4 )I5 .
Moreover, P5 has no answer sets. Indeed, I5 is not a minimal model of Ground(P5 )I5 , and
I6 is not a model of Ground(P5 )I6 = Ground(P5 ).
Note that any answer set M of P is also a total model of P because Ground(P)M 
Ground(P), and the rules in Ground(P) \ Ground(P)M are satisfied with respect to M
(by Definition 4, each of these rules must have at least one body literal which is false with
respect to M ).
On the language LPA
m,a considered in this work, answer sets as defined in Definition 5
coincide with stable models as defined by Pelov, Denecker, and Bruynooghe (2003) and
hence also those defined by Pelov et al. (2007) and Son et al. (2007). This equivalence
follows from Propositions 3.7 and 3.8 of Ferraris (2011), which respectively state that stable
models of Pelov et al. (2003) on LPA
m,a coincide with a semantics defined by Ferraris (2011),
which in turn coincides with Definition 5 on a larger class of programs. This means that all
our results involving answer sets also hold for these other semantics on LPA
m,a . On the other
hand, this also implies that some of the results (for example Theorem 16) are consequences
of results in the work of Pelov et al. (2007) by virtue of Theorem 24 in Section 7.
In the remainder of this section we highlight relevant relationships between answer sets
and unfounded sets. Before introducing our results, let us provide an additional definition.
503

fiAlviano, Calimeri, Faber, Leone, & Perri

Definition 6 (Unfounded-free Interpretation) An interpretation I for an LPA
m,a program P is unfounded-free if and only if I  X =  holds for each unfounded set X for P
with respect to I.
For total interpretations, an equivalent characterization of the unfounded-free property
is given below.
Lemma 10 A total interpretation I for an LPA
m,a program P is unfounded-free if and only
+
if the empty set is the only subset of I which is an unfounded set for P with respect to I.
Proof. () Straightforward: By Definition 6, I is disjoint from all the unfounded set for
P with respect to I.
() We prove the contrapositive: If I is not unfounded-free, then there exists a non-empty
subset of I + which is an unfounded set for P with respect to I. From Definition 6, if I is
not unfounded-free, then there exists an unfounded set X for P with respect to I such that
I X =
6 . We next show that I  X is an unfounded set for P with respect to I, i.e., for
each rule r  P such that H(r)  I  X , either (1) some (antimonotone) literal in B a (r)
is false with respect to I, or (2) some (monotone) literal in B m (r) is false with respect to
I  .(I  X ). Since X is an unfounded set, by Definition 1, either (a) some (antimonotone)
literal in B a (r) is false with respect to I, or (b) some (monotone) literal in B m (r) is false with
respect to I  .X . Thus, we can end the proof by showing that I  .X = I  .(I  X ).
To this end, observe that (i) .X = .(X \ I)  .(I  X ). Moreover, since I is total,
.(BP \ I + ) = I  , and thus (ii) .(X \ I) = .(X \ I + )  I   I \ X . By using (i) in
I  .X = (I \ X )  .X and simplifying with (ii) we obtain I  .X = (I \ X )  .(I  X ).
We conclude by observing that I \ X = I \ (I  X ), and thus I  .X = I  .(I  X )
holds.

Now we give another interesting characterization of total models for LPA
m,a programs.
Lemma 11 A total interpretation M is a (total) model for an LPA
m,a program P if and
only if .M  is an unfounded set for P with respect to M .
Proof. We start by observing that each rule r  P such that H(r)  M + is satisfied by
M . Thus, we have to show that, for each rule r  P with H(r)  .M  , some literal in
B(r) is false with respect to M if and only if either (1) some (antimonotone) literal in B a (r)
is false with respect to M , or (2) some (monotone) literal in B m (r) is false with respect
to M  .(.M  ). To this end, it is enough to prove that M  .(.M  ) = M holds.
By definition, () M  .(.M  ) = (M \ .M  )  ..M  . From the consistency of M
we have that M and .M  are disjoint. Moreover, ..M  = M  is a subset of M . By
simplifying () with the last two sentences, we obtain M  .(.M  ) = M .

Now we give further characterizations of answer sets for LPA
m,a programs.
Theorem 12 A total model M is an answer set of an LPA
m,a program P if and only if M
is unfounded-free.
504

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Proof. () We prove the contrapositive: If a total model M of an LPA
m,a program P is
not unfounded-free, then M is not an answer set of P. By Lemma 10, since M is a total
interpretation and it is not unfounded-free, there exists an unfounded set X for P with
respect to M such that X  M + and X =
6 . Therefore, to prove that M is not an answer

set of P, we next show that M  .X is a model of P M such that M  .X  M . To this
end, consider a rule r  P M . By Definition 4 of reduct, all the literals in B(r) are true with
respect to M , and so H(r)  M + because M is a model of P and P M  P. We now have
to consider two cases:
1. H(r) 6 X . In this case, H(r)  M  .X as well.
2. H(r)  X . In this case, since X is an unfounded set for P with respect to M , either
(1) some literal in B a (r) is false with respect to M , or (2) some literal in B m (r) is false
with respect to M  .X . By previous considerations, since r  P M , (1) cannot hold,
and so we can conclude that some literal in B(r) is false with respect to M  .X .
Hence, we have that r is satisfied by M  .X either by head (in case H(r) 6 X ), or by
body (in case H(r)  X ), and so we are done.
() We prove the contrapositive: If a total model M of an LPA
m,a program P is not an
answer set of P, then M is not unfounded-free. Since M is a model of P  P M but not an
answer set of P, there exists a total model N of P M such that N +  M + . We next show
that M + \ N + is an unfounded set for P with respect to M , that is, for each rule r  P such
that H(r)  M + \ N + , either (1) some (antimonotone) literal in B a (r) is false with respect
to M , or (2) some (monotone) literal in B m (r) is false with respect to M  .(M + \ N + ).
We start by showing that M  .(M + \ N + ) = N . By definition, (a) M  .(M + \ N + ) =
(M \ (M + \ N + ))  .(M + \ N + ). From N +  M + we have (b) M \ (M + \ N + ) =
N +  M  . Moreover, since N and M are total interpretations and N +  M + , we have (c)
N   M  and (d) .(M + \ N + ) = N  \ M  . Thus, by using (b) and (d) in (a) we obtain
M  .(M + \ N + ) = N + M  (N  \M  ), and by observing that M  (N  \M  ) = N 
holds because of (c), we conclude (e) M  .(M + \ N + ) = N +  N  = N .
Consider now a rule r  P such that H(r)  M + \ N + . We have to deal with two cases:
1. r  P \ P M . In this case, by Definition 4, there must be a literal   B(r) such that 
is false with respect to M . If  is an antimonotone literal, then (1) holds. Otherwise,
 is a monotone literal and so  is false with respect to N as well, since N  M ; thus,
(2) holds because of (e).
2. r  P M . In this case, since N is a model of P M and H(r) is false with respect to N
(because H(r)  M + \ N + by assumption), there must be a literal   B(r) such that
 is false with respect to N . If  is an antimonotone literal, then  is false with respect
to M as well, since N  M , and so (1) holds. Otherwise,  is a monotone literal and
(2) holds because of (e).

We are then ready to state an important connection between answer sets and unfounded
sets.
Theorem 13 A total interpretation M for an LPA
m,a program P is an answer set of P if
and only if GU SP (M ) = .M  .
505

fiAlviano, Calimeri, Faber, Leone, & Perri

Proof. () Let M be an answer set of P. By Lemma 11, .M  is an unfounded set for P
with respect to M , and hence GU SP (M )  .M  . By Theorem 12, M is unfounded-free,
and hence GU SP (M )  .M  because M is total. In sum, GU SP (M ) = .M  .
() Let M be a total interpretation such that GU SP (M ) = .M  . Then M and GU SP (M ) =
.M  are disjoint, and so M is unfounded-free. Moreover, by Corollary 3, GU SP (M ) = M 
is an unfounded set for P with respect to M and so, by applying Lemma 11, we conclude
that M is a model of P. We are then in order to apply Theorem 12 (M is an unfounded-free
model of P) and conclude that M is an answer set of P.

The following theorem shows that answer sets of LPA
m,a programs are exactly the total
fixpoints of the well-founded operator defined in Section 3.
Theorem 14 Let M be a total interpretation for an LPA
m,a program P. Then M is an
answer set for P if and only if M is a fixpoint of the well-founded operator WP .
Proof. () Let M be an answer set of P. We want to show that M is a fixpoint of WP , that
is, WP (M ) = M . Our aim is then to show that TP (M ) = M + and .GU SP (M ) = M  .
Since M is an answer set, by applying Theorem 13, we obtain GU SP (M ) = .M  , which
is equivalent to .GU SP (M ) = M  . Therefore, it remains to prove that TP (M ) = M + :
() Consider an atom   TP (M ). By Definition 2, there is a rule r  P such that
H(r) =  and all the literals in B(r) are true with respect to M . Thus,   M + holds
because M is a model of P.
() Consider an atom   M + . Since M is an answer set of P, we can apply Theorem 12
and conclude that M is unfounded-free. Hence, the (singleton) set {}  M + is not
an unfounded set for P with respect to M . Thus, by Definition 1, there is a rule
r  P such that H(r) =  and neither (1) some (antimonotone) literal in B a (r) is
false with respect to M , nor (2) some (monotone) literal in B m (r) is false with respect
to M  .{}. Since M is a total interpretation, neither (1) nor (2) is equivalent to
both (i) all the (antimonotone) literals in B a (r) are true with respect to M , and (ii)
all the (monotone) literals in B m (r) are true with respect to M  .{}. By observing
that M  .{}  M , we can state that (ii) implies that all the (monotone) literals in
B m (r) are true with respect to M as well. By combining the latter statement with (i)
we obtain that all the literals in B(r) are true with respect to M , and so   TP (M )
by Definition 2.
() Let M be a total fixpoint of WP , i.e., WP (M ) = M . Thus, M  = .GU SP (M ) by
Definition 2, and so M is an answer set for P because of Theorem 13.

Observe that Theorem 14 is a generalization of Theorem 5.4 of Van Gelder et al. (1991)
to the class of LPA
m,a programs. It is also worth noting that WP (I) extends I preserving its
correctness: If I is contained in an answer set M , then WP (I) may add to I some literals
of M , but never introduces any literal which would be inconsistent with M .
Proposition 15 Let I be an interpretation for an LPA
m,a program P, and let M be an
answer set for P. If I  M , then WP (I)  M .
506

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Proof. This is a trivial consequence of the monotonicity of the operator WP (Theorems 7)
and Theorem 14. Indeed, by Theorems 7, WP is I  M implies WP (I)  WP (M ), and
WP (M ) = M by Theorem 14.

We next show that the well-founded model of an LPA
m,a program is contained in all the
answer sets (if any) of P. We would like to point out that due to Theorem 24 in Section 7
(showing the equivalence of the well-founded operators defined in this work and the one
defined in Pelov et al., 2007) and Propositions 3.77 and 3.8 of Ferraris (2011; showing the
equivalence of answer sets in Faber et al., 2011 and stable models in Pelov et al., 2007),
the following results also hold by virtue of the definitions of the well-founded and stable
semantics in the work of Pelov et al., in particular due to Proposition 7.3 of that paper. We
nevertheless also provide a proof using the concepts defined earlier.

Theorem 16 Let P be an LPA
m,a program. For each answer set M of P, WP ()  M .

Proof. Let M be an answer set of P. Note that WP () is the limit of the sequence W0 = ,
Wn = WP (Wn1 ). We show that Wn  M by induction on n. The base case is trivially
true since W0 =  by definition. Now assume Wn  M in order to show that Wn+1  M .
Since Wn+1 = WP (Wn ) by definition and Wn  M by induction hypothesis, we apply
Proposition 15 and conclude that Wn+1  M .

The theorem above suggests another property of well-founded semantics for LPA
m,a programs.
Property 3 The well-founded semantics for LPA
m,a programs approximates the answer set
semantics: The well-founded model is contained in the intersection of all answer sets (if
any).
By combining Theorem 14 and Theorem 16, we obtain the following claim.

Corollary 17 Let P be an LPA
m,a program. If WP () is a total interpretation, then it is
the unique answer set of P.

Therefore, by combining Theorem 9 and the corollary above, we obtain another property
of well-founded semantics for LPA
m,a programs.
Property 4 On stratified LPA
m,a programs, the well-founded model coincides with the
unique answer set.

5. The Complexity of the Well-Founded Semantics
For the complexity analysis carried out in this section, we consider ground programs and
polynomial-time computable aggregate functions (note that all example aggregate functions
appearing in this paper fall into this class). However, we eventually provide a discussion on
how results change when considering non-ground programs. We start with an important
property of monotone and antimonotone aggregate literals.
507

fiAlviano, Calimeri, Faber, Leone, & Perri

Lemma 18 Let I be a partial interpretation for a ground LPA
m,a program P. We define
two total interpretations for P as follows: Imin = I  .(BP \ I) and Imax = I  (BP \ .I).
For each (ground) aggregate literal A occurring in P, the following statements hold:
1. If A is a monotone literal, then A is true (resp. false) with respect to I if and only if
A is true with respect to Imin (resp. false with respect to Imax ).
2. If A is an antimonotone literal, then A is true (resp. false) with respect to I if and
only if A is true with respect to Imax (resp. false with respect to Imin ).
Proof. We start by noting that Imin (resp. Imax ) is a total interpretation extending I
and such that all the standard atoms which are undefined with respect to I are false with
respect to Imin (resp. true with respect to Imax ). Thus, we have () Imin  I  Imax . If A
is monotone and true with respect to Imin (resp. false with respect to Imax ), then A is true
(resp. false) with respect to I because of (). If A is antimonotone and true with respect
to Imax (resp. false with respect to Imin ), then A is true (resp. false) with respect to I
because of (). We end the proof by observing that if A is true (resp. false) with respect
to I, then A is true with respect to Imin and Imax by definition.

We are now ready to analyze the computational complexity of the well-founded semantics
for LPA
m,a programs. Our analysis will lead to prove the following fundamental property.
Property 5 The well-founded model for a ground LPA
m,a program is efficiently (polynomialtime) computable.
Given Corollary 25, this property also follows from Theorem 7.4 in the work of Pelov
et al. (2007). In the following, we will provide an alternative proof based on the concepts
defined earlier in this paper, which also leads to several interesting intermediate results.
Property 5 is not trivial because aggregates may easily increase the complexity of the
evaluation. Indeed, even deciding the truth of an aggregate with respect to a partial interpretation is intractable in general; a similar observation has already been made by Pelov
(2004). However, this task is polynomial-time computable for the aggregate literals occurring in LPA
m,a programs.
Proposition 19 Deciding whether a ground aggregate literal A is true (resp. false) with
respect to a partial interpretation I is:
(a) co-NP-complete in general;
(b) polynomial-time computable if A is either a monotone or an antimonotone literal.
Proof. (a) As for the membership, we consider the complementary problem, that is,
deciding whether a ground aggregate literal A is not true (resp. not false) with respect
to a partial interpretation I, and prove that it belongs to NP. In order to show that A
is not true (resp. not false) with respect to I it is enough to find a total interpretation J
extending I (that is, J  I) such that A is false (resp. true) with respect to J. Thus, we
can guess such a J and check the falsity (resp. truth) of A with respect to J in polynomial
508

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

time (if the aggregate function can be computed in polynomial time with respect to the size
of the input multiset, as we are assuming).
As for the hardness, we first consider the problem of checking the truth of an aggregate and provide a polynomial-time reduction from TAUTOLOGY. The TAUTOLOGY
problem is co-NP-complete and can be stated as follow: Given a proposition formula 
on variables X1 , . . . , Xn , does each truth assignment v for the variables X1 , . . . , Xn satisfy
the formula ? Without loss of generality, we assume that  is a 3-DNF formula of the form
 = D1      Dm ,

where each disjunct Di is a conjunction 1i  2i  3i , and each ji is a positive or negative
literal (note that, in the context of TAUTOLOGY, the term literal denotes a variable
Xk or a variable preceded by the negation symbol ). For a given , we then consider a
partial interpretation I = {} and construct an aggregate literal A = #sum{S}  1, where
S contains two groups of elements. The elements in the first group represent disjuncts of 
and are
h1 : (1i ), (2i ), (3i )i,
i = 1, . . . , m ,
where, for each i = 1, . . . , m and j = 1, . . . , 3, the propositional atom (ji ) is defined as
follows:
 t
j
 xk if i is a positive literal Xk , for some k  {1, . . . , n}.
j
(i ) =
 f
xk if ji is a negative literal Xk , for some k  {1, . . . , n}.
The elements in the second group represent variables of  and are as follows:

h 1,


 h1,
h1,



h 1,

xk :  i
xk : xtk i
,
xk : xfk i
xk : xtk , xfk i

k = 1, . . . , n ,

where xk and xk are constants associated with the variable Xk . Note that, for each variable
Xk of , there are two atoms in A, xtk and xfk . Thus, for each interpretation J, four cases
are possible:
(1) {not xtk , not xfk }  J: In this case, only h1, xk : i contribute to the evaluation of
A, and its contribution is 1;
(2) {xtk , xfk }  J: In this case, all the four elements contribute to the evaluation of A,
and thus their contribution is 1  1 + 1 = 1 (note that h1, xk : xtk i and h1, xk : xfk i
give a total contribution of 1 because of our pure set approach);
(3) {xtk , not xfk }  J: In this case, only h1, xk : i and h1, xk : xtk i contribute, giving
1  1 = 0;
(4) {not xtk , xfk }  J: In this case, only h1, xk : i and h1, xk : xfk i contribute, giving
1  1 = 0.
509

fiAlviano, Calimeri, Faber, Leone, & Perri

Thus, for each k  {1, . . . , n}, the total contribution of the four elements of S associated with
the variable Xk is either 0 or 1. Note that also the total contribution of the other elements
of S (i.e., those in the first group) is either 0 or 1. Therefore, if there is k  {1, . . . , n} such
that either case (1) or (2) occurs, the interpretation J trivially satisfies A. Otherwise, J
is such that, for each variable k  {1, . . . , n}, either (3) or (4) occurs. In this case, we say
that J is a good interpretation.
We next define a one-to-one mapping between the set of assignments for  and the set of
good interpretations. Let v be an assignment for . The good interpretation Iv associated
with v is such that   Iv and

f
 {xtk , not xk }  Iv if v(Xk ) = 1
,
k = 1, . . . , n .

f
t
{not xk , xk }  Iv if v(Xk ) = 0
We want to show that v satisfies  if and only if A is true with respect to Iv . Since Iv is
a good interpretation, the elements of S in the second group give a total contribution of 0,
and so we have just to consider the elements of S in the first group. These elements give a
contribution of 1 if and only if {(1i ), (2i ), (3i )}  I holds for at least one i  {1, . . . , n},
and this holds if and only v(Di ) = 1 holds for the disjunct Di . We can then conclude that
A is true with respect to Iv if and only v() = 1.
Concerning the check of falsity of an aggregate, we can start from a 3-DNF formula 
and construct an aggregate literal A = #sum{S} < 1, where S is obtained as described
above. Then  is a tautology if and only if A is false with respect to I = {}.
(b) Let I be a partial interpretation for an LPA
m,a program P and A an aggregate literal
occurring in P. We want to show that deciding whether A is true (resp. false) with respect
to I can be done in polynomial-time in the size of BP . By Lemma 18, it is enough to evaluate
the aggregate with respect to either Imin = I  .(BP \ I) or Imax = I  (BP \ .I). We
then end the proof by observing that the interpretations Imin and Imax can be constructed
in polynomial time, and that the value of the aggregate function in A can be computed in
polynomial time with respect to the size of the input multiset by assumption.

In order to prove the tractability of the well-founded semantics we need an efficient
method for computing the greatest unfounded set, which is part of the well-founded operator
WP . Hence, we next give a polynomial-time construction of the set BP \GU SP (I) by means
of a monotone operator.
Definition 7 Let I be an interpretation for an LPA
m,a program P. The operator I :
B
B
P
P
is defined as follows:
2 2
I (Y ) = {  BP |  r  P with H(r) =  such that
no (antimonotone) literal in B a (r) is false with respect to I, and
all the (monotone) literals in B m (r) are true with respect to Y \ .I  }
The least fixpoint of I coincides with the greatest unfounded set of P with respect to
I.
Theorem 20 Let P be an LPA
m,a program and I an interpretation for P. Then:
510

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

1. The I operator has a least fixpoint I ();
2. GU SP (I) = BP \ I ().
Proof. The I operator is a monotonically increasing operator in the meet semilattice
hBP , i, and it therefore admits a least fixpoint I () (Tarski, 1955). We next prove that
GU SP (I) = BP \ I () in two steps:
() We first observe that I () can be computed iteratively, starting from the empty set,
as the limit of the sequence F0 = , Fi+1 = I (Fi ). Thus, we prove by induction on i
that GU SP (I)  BP \Fi holds. The base case is trivial, since F0 =  by definition and
GU SP (I) is a subset of BP by Definition 1. We then assume GU SP (I)  BP \ Fi in
order to prove that GU SP (I)  BP \ Fi+1 . Since GU SP (I) is an unfounded set for P
with respect to I by Theorem 2, by Definition 1 we have that, for each   GU SP (I)
and for each rule r  P with H(r) = , either (1) some (antimonotone) literal in
B a (r) is false with respect to I, or (2) some (monotone) literal in B m (r) is false with
respect to I  .GU SP (I). We want to show that such a  does not belong to Fi+1 ,
that is, each rule r as above is such that either (i) some (antimonotone) literal in
B a (r) is false with respect to I, or (ii) some (monotone) literal in B m (r) is not true
with respect to Fi \ .I  (recall that Fi+1 = I (Fi ) by definition). Since (1) and (i)
are equals, we have to show that (2) implies (ii). To this end, assume that there is
a (monotone) literal   B m (r) which is false with respect to I  .GU SP (I). Our
aim is to show that  is false with respect to J = (Fi \ .I  )  .(BP \ (Fi \ .I  )),
since in this case  would be not true with respect to Fi \ .I  (see Lemma 18).
We start by proving that (I  .GU SP (I)) = I   .GU SP (I) is a subset of J  .
Observe that J  = .(BP \ (Fi \ .I  )) = I   .(BP \ Fi ) because .I  is a
subset of BP . Thus, since GU SP (I)  BP \ Fi by induction hypothesis, we obtain
(I  .GU SP (I)) = I   .GU SP (I)  I   .(BP \ Fi ) = J  . Since J is total,
(I  .GU SP (I))  J  implies that there is an extension K of I  .GU SP (I)
such that K   J  and K +  J  (for example, the one containing as true all the
standard positive literals which are undefined with respect to I  .GU SP (I)). Since
 is false with respect to I  .GU SP (I) by assumption and K is an extension of
I  .GU SP (I),  is false with respect to K by Remark 1. Thus, since J  K and
 is monotone, the latter implies that  is false with respect to J as well.
() We prove that BP \ I () is an unfounded set for P with respect to I, that is, for
each r  P with H(r)  BP \ I (), either (1) some (antimonotone) literal in B a (r) is
false with respect to I, or (2) some (monotone) literal in B m (r) is false with respect
to I  .(BP \ I ()). By Definition 7, H(r) 6 I () implies either that (i) some
(antimonotone) literal in B a (r) is false with respect to I, or that (ii) some (monotone)
literal in B m (r) is not true with respect to I () \ .I  . Since (i) and (1) are equals,
we have to show that (ii) implies (2). To this end, assume that there is a (monotone)
literal   B m (r) which is not true with respect to I () \ .I  . Thus, there is an
extension of I () \ .I  for which  is false, and in particular  must be false with
respect to J = (I () \ .I  )  .(BP \ (I () \ .I  )) because of Lemma 18. Now
observe that (I  .(BP \ I ())) = I  .(BP \I ()) = .(BP \(I ()\.I  )) =
511

fiAlviano, Calimeri, Faber, Leone, & Perri

J  holds (because .I   BP ), and so (I  .(BP \ I ()))+  J + because J is total.
By combining the last two sentences we obtain I  .(BP \ I ())  J. Therefore,
since  is a monotone literal which is false with respect to J, the latter implies that 
is false with respect to I  .(BP \ I ()) as well, and so (2) holds.

Eventually, Property 5 is a consequence of the following theorem. As mentioned earlier,
this theorem also follows from Theorem 7.4 in the work of Pelov et al. (2007) because of
Corollary 25, but the proof provided here differs considerably from the one of Theorem 7.4
in the work of Pelov et al.
Theorem 21 Given an LPA
m,a program P:
1. The greatest unfounded set GU SP (I) of P with respect to a given interpretation I is
polynomial-time computable;
2. WP () is polynomial-time computable.
Proof. (1.) From Theorem 20, GU SP (I) = BP \ I (). We next show that I () is
efficiently computable. The fixpoint I () is the limit  of the sequence 0 = , k =
I (k1 ). This limit is reached in a polynomial number of applications of I because each
new element of the sequence k must add at least a new atom (otherwise the limit has been
already reached), that is,   |BP |. If we show that each application of I is feasible in
polynomial time, we can conclude that  is computable in polynomial time. Each step
processes at most all the rules once, and for each rule checks the truth-value of at most
all body literals once. The check of the truth valuation is clearly tractable for all standard
(i.e., non-aggregates) literals; the tractability of the check for aggregate literals stems from
Proposition 19, as we deal with monotone and antimonotone aggregate atoms only. In
conclusion,  is computable in polynomial time, and GU SP (I) is tractable as well since it
is obtainable as BP \ I ().
(2.) By the argumentation carried out for I (), we can show that WP () is computed
in a number of steps which is polynomial (actually linear) in |BP |. Indeed, each step is
polynomial-time computable: We have just proved the tractability of GU SP (I), and TP is
polynomial-time computable as well.

This result has a positive impact also for the computation of the answer set semantics
of logic programs with aggregates. Indeed, as stated in Theorem 16, WP () approximates
the intersection of all answer sets (if any) from the bottom, and can be therefore used to
efficiently prune the search space. It is worthwhile noting that the computation of the
well-founded semantics is also hard for polynomial-time. In particular, deciding whether
a (ground) atom is true with respect to the well-founded semantics is P-complete, as this
task is P-hard even for the standard well-founded semantics of aggregate-free programs (and,
from Proposition 5, our semantics coincides with the standard well-founded on aggregatefree programs).
We end this section by briefly addressing the complexity of non-ground programs. When
considering data-complexity (i.e., an LPA
m,a program P is fixed and the input only consists
of facts), the results are as for propositional programs: Deciding whether a (ground) atom
is true with respect to the well-founded semantics of a non-ground program is P-complete,
512

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

under data-complexity (Van Gelder et al., 1991). However, if program complexity (i.e., an
LPA
m,a program P is given as input) is considered, complexity of reasoning rises exponentially. Indeed, a non-ground program P can be reduced, by naive instantiation, to a ground
instance of the problem, and in general the size of Ground(P) is single exponential in the
size of P. The complexity of reasoning increases accordingly by one exponential, from P
to EXPTIME, and the result can be derived using complexity upgrading techniques (Eiter,
Gottlob, & Mannila, 1997; Gottlob, Leone, & Veith, 1999).

6. Compilation into Standard LP, Implementation and Experimental
Results
The well-founded semantics for LPA
m,a programs has been implemented by extending the
DLV system (Leone et al., 2006). In this section we briefly describe the implemented
prototype and report on the results of our experiments aimed at assessing its efficiency.
Note that, even if LPA
m,a programs can be replaced by equivalent LP programs (for a
rewriting strategy see Section 6.1 below), our experimental results highlight a significant
performance advantage of LPA
m,a encodings.
6.1 Compilation into Standard Logic Programming
In this section we briefly present a strategy for representing #count, #sum and #times
with standard constructs.2 The compilation is in the spirit of the one introduced for #min
and #max by Alviano, Faber, and Leone (2008) and defines a subprogram computing the
value of a (possibly recursive) aggregate. The compilation takes into account specific properties of monotone and antimonotone aggregate functions, and is therefore referred to as
monotone/antimonotone encoding (mae).
The monotone/antimonotone encoding of an LPA
m,a program P is obtained by replacing
each aggregate literal A = f (S)  T by a new predicate symbol f . Predicate f is defined
by means of a subprogram (i.e., a set of rules) that can be thought of as a compilation of A
into standard LP. The compilation uses a total order < of the elements of UP  {}, where
 is a symbol not occurring in P and such that  < u for each u  UP . We further assume
the presence of a built-in relation Y < Y  , where Y = Y1 , . . . , Yn and Y  = Y1 , . . . , Yn
are lists of terms. This built-in relation has y < y  if and only if y precedes y  in the
lexicographical order induced by <. Moreover, we will use a built-in relation Y  Y  , where
y  y  is true if and only if either y < y  or y = y  . For simplicity, let us assume that A is
of the form f ({Y : p(Y , Z)})  k, where Y and Z are lists of local variables and k is an
integer constant. For such an aggregate, we introduce a new predicate symbol faux of arity
|Y | + 1 and rules for modeling that an atom faux (y, s) must be true whenever the value of
f ({Y : p(Y , Z), Y  y}) is at least s. Thus, we use a fact for representing the value of
the aggregate function for the empty set, and a rule for increasing this value for larger sets.
The lexicographical order induced by < is used to guarantee that all elements in the set are
2. Since we are considering only monotone and antimonotone aggregate literals, the domains of #sum and
#times are assumed to be N and N+ , respectively.

513

fiAlviano, Calimeri, Faber, Leone, & Perri

User Interface

Diagnosis
Frontend

Inheritance
Frontend

SQL3
Frontend

Planning
Frontend

DLV core
Ground
Program

Intelligent
Grounding

Model
Checker

Model
Generator

File
System

Relational
Database

Filtering

Output

Figure 2: Prototype system architecture.
considered at most once. In particular, the following rules are introduced:

faux (, ).
  = 0,  = S + 1 if f = #count;
 = 0,  = S + Y1 if f = #sum;
where
faux (Y  , X) : faux (Y , S), p(Y  , Z),

 = 1,  = S  Y1 if f = #times.
Y < Y  , X = .
If  {, >}, truth of an aggregate f ({Y : p(Y , Z)})  k must be inferred if and only
if some atom faux (y, s) such that s  k is true. This aspect is modeled by means of the
following rules:
fk : faux (Y , S), S  k.

f>k : faux (Y , S), S > k.

If  is , instead, truth of an aggregate f ({Y : p(Y , Z)})  k must be inferred if and only
if all atoms faux (y, s) such that s > k are false (and similar if  is <). These aspects are
modeled by means of the following rules:
fk : not f>k .

f<k : not fk .

Extending the technique to aggregate literals with global variables is quite simple:
Global variables are added to the arguments of all the atoms used in the compilation,
and a new predicate fgroupby is used for collecting their possible substitutions.
6.2 System Architecture and Usage
We have extended DLV by implementing the well-founded operator and the well-founded
semantics for LPA
m,a programs described in this paper. The architecture of the prototype is
514

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

reported in Figure 2. In detail, we modified two modules of DLV, the Intelligent Grounding
module and the Model Generator module. In our prototype, the well-founded semantics
is adopted if one of -wf or --well-founded is specified on the command-line. Otherwise,
the stable model semantics is adopted as usual. The well-founded operator WP introduced
in Section 3 is used for both semantics. In particular, for the stable model semantics, the
well-founded model is profitably used for pruning the search space. For the well-founded
semantics, the well-founded model is printed after the computation of the least fixpoint of
the well-founded operator. In this case the output of the system consists of two sets, for
representing true and undefined standard atoms in the well-founded model. A binary of the
prototype is available at http://www.dlvsystem.com/dlvRecAggr/.
6.3 Experimental Results
To our knowledge, the implemented prototype is currently the only system supporting a
well-founded semantics for logic programs with recursive aggregates. For certain special
cases, such as when the well-founded model is total, the well-founded model coincides with
other semantics such as answer sets (see Corollary 17) and in theses cases systems supporting
those semantics such as IDP (Wittocx, Marien, & Denecker, 2008), Smodels (Simons et al.,
2002), or clasp (Gebser, Kaufmann, Neumann, & Schaub, 2007), can be used to compute
the well-founded model.
We are however interested in systems that are able to compute the well-founded model
for all input programs. One of the major systems supporting the well-founded semantics,
XSB (Swift & Warren, 2010), has some support for aggregates, but (apart from #min and
#max) XSB does not support recursive aggregates (i.e., aggregates occurring in recursive
definitions). Therefore, our experiments have been designed for investigating the computational behavior of aggregate constructs with respect to equivalent encodings without
aggregates.
More specifically, we introduce the Attacks problem, which is inspired by the classic
Win-Lose problem often used in the context of the well-founded semantics for standard
logic programs, and study performance on it.
Definition 8 (Attacks Problem) In the Attacks problem, a set of p players and a positive integer m are given. Each player attacks n other players. A player wins if no more
than m winners attack it. This kind of problem is frequently present in turn-based strategy
games.
Note that the definition of winner is recursive and, in particular, a recursive aggregate
is the natural way of encoding this problem.
Example 14 An instance of the Attacks problem in which p = 6, n = 2 and m = 1 could
be the following:
 player a attacks players b and c;

 player d attacks players b and f ;

 player b attacks players a and c;

 player e attacks players c and f ;

 player c attacks players a and b;

 player f attacks players d and e.

515

fiAlviano, Calimeri, Faber, Leone, & Perri

b

d

a

f

e

c

Figure 3: An instance of the Attacks problem with 6 players, each one attacking 2 other
players.

A graphical representation of this instance is shown in Figure 3. Since d is only attacked by
f , we can conclude that d is a winner. Similarly for e. Therefore, f is not a winner because
f is attacked by d and e, which are winners. For the other players, namely a, b and c, we
cannot determine who is a winner or not.
In our experiments, instances of Attacks are encoded by means of the predicates max,
player and attacks for representing the parameter m, the set of players and the attacks of
the players, respectively. We consider three equivalent encodings for the Attacks problem.
6.3.1 Aggregate-Based Encoding
This encoding is a natural representation of the Attacks problem in LP A
m,a . The complete
encoding consists of a single rule, reported below:
win(X) : max(M ), player(X), #count{Y : attacks(Y, X), win(Y )}  M.

6.3.2 Join-Based Encoding
An equivalent encoding can be obtained by computing a number of joins proportional to
m. The tested encoding is reported below:
win(X) : player(X), not lose(X).
lose(X) : max(1), attacks(Y1 , X), win(Y1 ),
attacks(Y2 , X), win(Y2 ), Y1 < Y2 .
lose(X) : max(2), attacks(Y1 , X), win(Y1 ),
attacks(Y2 , X), win(Y2 ), Y1 < Y2 ,
attacks(Y3 , X), win(Y3 ), Y1 < Y3 , Y2 < Y3 .
lose(X) : max(3), . . .

Note that in the encoding above there is a rule for each possible value of parameter m.
However, only one of these rules is considered by our solver during program instantiation.
In fact, only the rule is instantiated, which contains the instance of atom max(m) for which
a fact is present. All the other rules are satisfied because of a false body literal.
516

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

30

300

DLV-join
A
DLV

25

250

20

200

15

150

10

100

5
01

2

3

4

5

6

x

7

8

9 1

2

3

4

5

6

7

8

9

10

50
01

y

2

3

4

6

x

(a) 100 players

7

8

9 1

2

3

4

5

6

8

9

10

y

(b) 200 players

600

600

500

500

400

400

300

300

200

200

100
01

5

7

2

3

4

5

x

6

7

8

9 1

2

3

4

5

6

7

8

9

10

100
01

y

2

3

4
x

(c) 400 players

5

6

7

8

9 1

2

3

4

5

6

7

8

9

10

y

(d) 800 players

Figure 4: Attacks: Average execution time of DLV running the aggregate-based encoding
and DLV running the join-based encoding.

6.3.3 Mae-Based Encoding
This encoding has been obtained by applying the compilation presented in Section 6.1 with
some minor simplifications. The full encoding is reported below:
win(X) : player(X), not lose(X).
lose(X) : count(X, Y, S), max(M ), S > M.
count(X, Y, 1) : aux(X, Y ).
count(X, Y  , S  ) : count(X, Y, S), aux(X, Y  ), Y < Y  , S  = S + 1.
aux(X, Y ) : attacks(Y, X), win(Y ).

Intuitively, an atom count(x, y, s) stands for there are at least s constants y  such that
y   y and attacks(y  , x), win(y  ) is true. Note that the rules defining predicate count use
the natural order of integers to guarantee that each y  is counted at most once.
Example 15 The instance shown in Figure 3 is represented by means of the following facts:
player(a).
attacks(a, b).
attacks(a, c).
max(1).

player(b).
attacks(b, a).
attacks(b, c).

player(c).
attacks(c, a).
attacks(c, b).

player(d).
attacks(d, b).
attacks(d, f ).

517

player(e).
attacks(e, c).
attacks(e, f ).

player(f ).
attacks(f, d).
attacks(f, e).

fiAlviano, Calimeri, Faber, Leone, & Perri

8

10

XSB-join
A
DLV

7
6

9
8
7

5

6

4

5

3

4
3

2
1
01

2

3

4

5

6

x

7

8

9 1

2

3

4

5

6

7

8

9

10

2
1
01

y

2

3

4

6

x

(e) 100 players

7

8

9 1

2

3

4

5

6

8

9

10

y

(f) 200 players

9

12

8

10

7
6

8

5

6

4
3

4

2
1
01

5

7

2

3

4

5

6

x

7

8

9 1

2

3

4

5

6

7

8

9

10

2
01

y

2

3

4
x

(g) 400 players

5

6

7

8

9 1

2

3

4

5

6

7

8

9

10

y

(h) 800 players

Figure 5: Attacks: Average execution time of DLV running the aggregate-based encoding
and XSB running the join-based encoding.

For all the encodings, the well-founded model restricted to the win predicate is {win(d),
win(e), not win(f )}. Note that win(a), win(b) and win(c) are neither true nor false, and so
they are undefined.
6.3.4 Discussion
We performed an intensive experimentation for this benchmark by varying the parameters
p, m and n. For each combination of these parameters, we measured the average execution
time of DLV and XSB (version 3.2) on 3 randomly generated instances. The experiments
R
R
have been performed on a 3GHz Intel
Xeon
processor system with 4GB RAM under the
Debian 4.0 operating system with GNU/Linux 2.6.23 kernel. The DLV prototype used has
been compiled with GCC 4.4.1. For every instance, we have allowed a maximum running
time of 600 seconds (10 minutes) and a maximum memory usage of 3GB.
The results of our experimentation are reported in Figures 47. In the graphs, DLV A
is the implemented prototype with the aggregate-based encoding, DLV-join and DLV-mae
the implemented prototype with the aggregate-free encodings, XSB-join and XSB-mae the
XSB system with the aggregate-free encodings (as mentioned earlier, XSB does not support
518

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

DLV-mae
A
DLV
6

12

5

10

4

8

3

6

2

4

1
01

2

3

4

5

6

x

7

8

9 1

2

3

4

5

6

7

8

9

10

2
01

y

2

3

4

5

6

x

(i) 1600 players

7

8

9 1

2

3

4

5

6

7

8

9

10

y

(j) 3200 players

25

60
50

20

40
15
30
10
20
5
01

2

3

4
x

5

6

7

8

9 1

2

3

4

5

6

7

8

9

10

10
01

y

2

3

4
x

(k) 6400 players

5

6

7

8

9 1

2

3

4

5

6

7

8

9

10

y

(l) 12800 players

Figure 6: Attacks: Average execution time of DLV running the aggregate-based encoding
and DLV running the mae-based encoding.

recursive aggregates). For the XSB system, we explicitly set indices and tabled predicates
for optimizing its computation.
For each graph, the number of players is fixed, while parameters m (x-axis) and n
(y-axis) vary. Therefore, the size of the instances grows moving from left to right along
the y-axis, while it is invariant with respect to the x-axis. However, the number of joins
required by the join-based encoding depends on the parameter m. As a matter of fact, we
can observe in the graphs in Figures 45 that the average execution time of the join-based
encoding increases along both the x- and y-axis (for both DLV and XSB). Instead, for
the encoding using aggregates, and for the mae-based encoding, the average execution time
only depends on instance sizes, as shown in the graphs in Figures 67.
For the join-based encoding, XSB is generally faster than DLV, but consumes much
more memory. Indeed, in Figure 5, we can observe that XSB terminates its computation in
a few seconds for the smallest instances, but rapidly runs out of memory on slightly larger
instances. Considering the mae-based encoding, we can observe significant performance
gains for both DLV and XSB (see Figures 67). Indeed, both systems complete their computation in the allowed time and memory on larger instances. Computational advantages
of the mae-based encoding with respect to the join-based encoding are particularly evident
519

fiAlviano, Calimeri, Faber, Leone, & Perri

XSB-mae
A
DLV
3.5

9
8

3

7
2.5

6

2

5

1.5

4
3

1
0.5
01

2

3

4

5

6

x

7

8

9 1

2

3

4

5

6

7

8

9

2

10

1
01

y

2

3

4

5

6

x

(m) 6400 players

7

8

9 1

2

3

4

5

6

7

8

9

10

y

(n) 12800 players

60

400
350

50

300
40

250

30

200
150

20
10
01

2

3

4
x

5

6

7

8

9 1

2

3

4

5

6

7

8

9

100

10

50
01

y

2

3

4
x

(o) 25600 players

5

6

7

8

9 1

2

3

4

5

6

7

8

9

10

y

(p) 51200 players

Figure 7: Attacks: Average execution time of DLV running the aggregate-based encoding
and XSB running the mae-based encoding.

for XSB, which solved all tested instances with this encoding. However, also XSB with the
mae-based encoding is outperformed by DLV with native support for aggregate constructs
(see Figure 7).
In sum, the experimental results highlight that the presence of aggregate constructs can
significantly speed-up the computation. Indeed, the encoding using recursive aggregates
outperforms the aggregate-free encodings in all tested instances.

7. Related Work
Defining a well-founded semantics for logic programs with aggregates has been a challenge
of major interest in the last years. The first attempts, not relying on a notion of unfounded
set, have been defined on a restricted language. Some of these are discussed by Kemp and
Stuckey (1991). Another semantics falling in this class is the one introduced by Van Gelder
(1992), subsequently generalized by Osorio and Jayaraman (1999). The main problem of
these semantics is that they often leave too many undefined literals, as shown by Ross and
Sagiv (1997).
520

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

A first attempt to define a well-founded semantics for unrestricted LPA has been done by
Kemp and Stuckey (1991). This semantics is based on a notion of unfounded sets. According
to Kemp and Stuckey, a set X of standard atoms is an unfounded set for a (ground) program
P with respect to an interpretation I if, for each rule r  P with H(r)  X , either (a) some
literal in B(r) is false with respect to I, or (b) B(r)  X =
6 . Note that only standard
literals are considered by condition (b), and aggregates are not covered by it. We point
out that this definition of unfounded set makes the semantics inadequate for programs with
recursive aggregates, even if only monotone aggregates are considered. For example, for the
program {a(1):#count{X : a(X)} > 0.}, the well-founded model in the work of Kemp and
Stuckey is , while a reasonable well-founded semantics should identify a(1) as false.
Pelov et al. (2007) defined a well-founded semantics based on approximating operators,
namely D-well-founded semantics, which extends the standard well-founded semantics; indeed, they coincide for aggregate-free programs. More in detail, in that work aggregates
are evaluated in one of three possible ways. Therefore, a family of semantics is defined by
Pelov et al., which can be ordered by precision: More precise three-valued aggregates lead
to more precise semantics. In general, higher precision comes at the price of a higher computational complexity. The authors discuss the following three-valued aggregate relations
for the evaluation of aggregate literals: trivial, bound and ultimate approximating aggregates, where the first is the less precise, and the last is the most precise. Semantics relying
on trivial approximating aggregates is very imprecise, but it is still suitable for the class
of stratified aggregate programs. Both trivial and bound approximations have polynomial
complexity, while ultimate has been shown to be intractable for nonmonotone aggregate
functions (Pelov, 2004). A detailed comparison with our results is presented in Section 7.1.
Ferraris (2005) showed that the semantics of Smodels programs with positive weight
constraints is equal to answer sets as defined by Faber et al. (2004) on the respective
fragment. Since by Theorem 16 WP () approximates answer sets as defined by Faber et al.,
WP () can be used also as an approximating operator for the respective Smodels programs.
Indeed, it can be shown that the AtMost pruning operator of Smodels (Simons et al., 2002)
is a special case of the I operator (defined in the proof of Theorem 21).
Other works attempted to define stronger notions of well-founded semantics (also for
programs with aggregates), like the Ultimate Well-Founded Semantics (Denecker et al.,
2001), or WFS1 and WFS2 (Dix & Osorio, 1997). Whether a characterization of these
semantics in terms of unfounded sets can exist for these semantics is unclear and left for
future research.
Concerning compilations of LP A programs into standard LP, a transformation was provided by Van Gelder (1992). The compilation that we presented in Section 6.1 differs from
the one introduced by Van Gelder in several respects. Our approach uses a total order
of the universe of the input program and takes advantage of the character of monotonicity/antimonotonicity of the aggregate literals in the input program, while the transformation
defined by Van Gelder uses uninterpreted function symbols for representing ground sets,
and recursive negation for checking truth of aggregate literals. We briefly discuss these aspects in the following. Roughly, for an aggregate f (S)  k, uninterpreted function symbols
are used by the transformation in the work of Van Gelder for determining all pairs S  , k 
such that S  is a ground set associated with S and k  = f (S  ). After that, the transformation defined by Van Gelder checks whether there exists a pair S  , k  satisfying the following
521

fiAlviano, Calimeri, Faber, Leone, & Perri

conditions: (i) for every element hconsts : conji in S  , conj is true; (ii) k   k holds. We
point out that Condition (i) requires recursive negation in order to be checked. Indeed,
it is equivalent to there is no element hconsts : conji in S  such that conj is not true.
This aspect of the transformation has an undesirable side effect: Stratified LPA
m,a programs
may have partial well-founded models, that is, Theorem 9 does not hold for programs compiled with the transformation introduced by Van Gelder. An example of this side effect is
given by Van Gelder, where it is shown that this transformation possibly leads to partial
well-founded models for instances of Company Controls, a well-known problem that can be
modeled by using monotone recursive aggregates.
7.1 Comparison with the work of Pelov et al. (2007)
In this section we report a detailed comparison of the well-founded semantics as defined
in this paper with the one of Pelov et al. (2007). We recall that Pelov et al. defines wellfounded and stable semantics as the least and total fixpoints of the three-valued stable
model operator extended to aggregate programs.
We start by observing that the evaluation of ultimate approximating aggregates coincides
with the evaluation of aggregates defined in this article; also the evaluation of bound approximating aggregates coincides for monotone and antimonotone aggregates (as a consequence
of Lemma 18 in this paper and Proposition 7.16 in the work of Pelov et al., 2007).
Let us now introduce a translation of an aggregate literal into a formula of standard
literals. For a (partial) interpretation I, let conj(I) denote the conjunction of all the literals
in I. The translation trm(A) of a ground aggregate literal A is defined as follows:
W
trm(A) = {conj(I) | I is a subset-minimal interpretation
such that A is true with respect to I}
Note that, for each (partial) interpretation J, the evaluation of A with respect to J coincides
with the evaluation of trm(A) with respect to J (Proposition 2 and Proposition 3 in the
work of Pelov et al., 2003). Moreover, for a monotone (resp. antimonotone) aggregate literal
A, only positive (resp. negative) literals appear in trm(A).
For a rule r in a ground LPA
m,a program P and an aggregate literal A  B(r), the
translation trm(P, r, A) of A in r is the program obtained from P by removing r and
by adding a rule r such that H(r ) = H(r) and B(r ) = B(r) \ {A}  conj, for each
conj  trm(A). Therefore, the full translation trm(P) of P is defined as the recursive
application of trm(P, r, A) (note that the order in which rules and aggregates are processed
is not relevant). We next show that P and trm(P) have the same unfounded sets.
Lemma 22 A set of atoms X is an unfounded set for a program P with respect to an
interpretation I if and only if X is an unfounded set for trm(P) with respect to I.
Proof. We use induction on the number of aggregate literals in P. If P has no aggregate
literals, then P = trm(P). Now consider a program P and a rule r  P with an aggregate
literal A in B(r). We want to show that a set X of atoms is an unfounded set for P with
respect to I if and only if X is an unfounded set for trm(P, r, A) with respect to I, since
in this case we might apply the induction hypothesis and prove the claim. Thus, we can
end the proof by means of the following observations: (i) A is false with respect to an
522

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

interpretation J if and only if trm(A) is false with respect to J, that is, if and only if for
each conjunction conj  trm(A) there is a literal   conj such that  is false with respect
to J; (ii) such an  is a positive (resp. negative) standard literal if and only if A is monotone
(resp. antimonotone).

We can then prove that the well-founded operators of P and trm(P) coincide.
Lemma 23 Let P be an LPA
m,a program and I an interpretation for P. Then WP (I) =
Wtrm(P) (I).
Proof. We have to show that (1) TP (I) = Ttrm(P) (I) and (2) GU SP (I) = GU Strm(P) (I).
We note that (2) immediately follows from Lemma 22. In order to prove (1), we consider
an aggregate literal A occurring in P. By previous considerations, we have that A is true
with respect to I if and only if there is a conjunct in trm(A) which is true with respect to
I. Thus, (1) holds.

We are now ready to relate our well-founded operator with the one provided by Pelov
et al. (2007).
Theorem 24 For the class of LPA
m,a programs, the well-founded operator herein defined
coincides with the one of Pelov et al. (2007; for both the ultimate and bound approximating
aggregate semantics).3
Proof. By Lemma 23, we already know that WP (I) = Wtrm(P) (I). We also have that
Wtrm(P) (I) coincides with the one in the work of Van Gelder et al. (1991) by Theorem 1
(since trm(P) is a standard logic program). On the other hand, for both the ultimate and
bound approximating aggregate semantics, the well-founded operators (as defined in Pelov
et al., 2007) of P and trm(P) coincide: This is a consequence of Theorem 1 in the work of
Pelov et al. (2003), because the three-valued immediate consequence operators in the work
of Pelov et al. (2003) and Pelov et al. (2007) coincide (see Definition 7 in Pelov et al., 2003
and Definition 7.5 in Pelov et al., 2007). Moreover, the well-founded operator of Pelov et al.
(2007) coincides with the one in the work of Van Gelder et al. for standard logic programs,
thereby obtaining the equality of the operators.

The correspondence of the two well-founded semantics immediately follows from the
theorem above. Indeed, the two well-founded models are defined as the fixpoints of the
respective well-founded operators.
Corollary 25 The well-founded model herein defined and the one of Pelov et al. (2007;
for both the ultimate and bound approximating aggregate semantics) coincide for LPA
m,a
programs.
As mentioned also earlier, by virtue of the above theorem and corollary, some of the
results presented in this paper also follow from earlier results in the literature. In particular,
Theorem 9, Theorem 16 and some of our complexity results follow from definitions and
results of Pelov (2004) and Pelov et al. (2007).
3. Note that this operator is referred to as stable revision operator by Pelov et al. (2007).

523

fiAlviano, Calimeri, Faber, Leone, & Perri

8. Conclusion
In this paper we introduced a new notion of unfounded set for LPA
m,a programs and analyzed
a well-founded semantics for this language based on this notion. This semantics generalizes
the traditional well-founded semantics for aggregate-free programs and also coincides with
well-founded semantics for aggregate programs as defined by Pelov et al. (2007; the latter
not being defined by means of a notion of unfounded set). We could also show that this
semantics and its main operator WP have close ties with answer sets as defined by Faber
et al. (2004, 2011), and can hence serve as approximations.
We proved that computing this semantics is a tractable problem. Indeed, the semantics
is given by the least fixpoint of the well-founded operator WP . The fixpoint is reached after a
polynomial number of applications of the operator WP (with respect to the size of the input
program), each of them requiring polynomial time. For showing that an application of WP
is polynomial-time feasible, we have proved that evaluating monotone and antimonotone
aggregate literals remains polynomial-time computable also for partial interpretations, since
in this case only one of the possibly exponential extensions must be checked. For a monotone
aggregate literal, this extension is obtained by falsifying each undefined literal, while for an
antimonotone aggregate literal, each undefined literal is taken as true in the extension.
Motivated by these positive theoretical results, we have implemented the first system
supporting a well-founded semantics for unrestricted LPA
m,a . Allowing for using monotone
and antimonotone aggregate literals, the implemented prototype is ready for experimenting
with the LPA
m,a framework. The experiments conducted on the Attacks benchmark highlight
the computational gains of a native implementation of aggregate constructs with respect to
equivalent encodings in standard LP.

Acknowledgments
Partly supported by Regione Calabria and EU under POR Calabria FESR 2007-2013 within
the PIA project of DLVSYSTEM s.r.l., and by MIUR under the PRIN project LoDeN
and under the PON project FRAME proposed by Atos Italia S.p.a.; we also thank the
anonymous reviewers for their valuable comments.

References
Alviano, M., Faber, W., & Leone, N. (2008). Compiling minimum and maximum aggregates
into standard ASP. In Formisano, A. (Ed.), Proceedings of the 23rd Italian Conference
on Computational Logic (CILC 2008).
Baral, C. (2003). Knowledge Representation, Reasoning and Declarative Problem Solving.
Cambridge University Press.
Brewka, G. (1996). Well-Founded Semantics for Extended Logic Programs with Dynamic
Preferences. Journal of Artificial Intelligence Research, 4, 1936.
Calimeri, F., Faber, W., Leone, N., & Perri, S. (2005). Declarative and Computational
Properties of Logic Programs with Aggregates. In Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI-05), pp. 406411.
524

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

DellArmi, T., Faber, W., Ielpa, G., Leone, N., & Pfeifer, G. (2003). Aggregate Functions
in DLV. In de Vos, M., & Provetti, A. (Eds.), Proceedings ASP03 - Answer Set
Programming: Advances in Theory and Implementation, pp. 274288, Messina, Italy.
Online at http://CEUR-WS.org/Vol-78/.
Denecker, M., Pelov, N., & Bruynooghe, M. (2001). Ultimate Well-Founded and Stable
Model Semantics for Logic Programs with Aggregates. In Codognet, P. (Ed.), Proceedings of the 17th International Conference on Logic Programming, pp. 212226.
Springer Verlag.
Dix, J., & Osorio, M. (1997). On Well-Behaved Semantics Suitable for Aggregation. In
Proceedings of the International Logic Programming Symposium (ILPS 97), Port Jefferson, N.Y.
Eiter, T., Gottlob, G., & Mannila, H. (1997). Disjunctive Datalog. ACM Transactions on
Database Systems, 22 (3), 364418.
Faber, W. (2005). Unfounded Sets for Disjunctive Logic Programs with Arbitrary Aggregates. In Baral, C., Greco, G., Leone, N., & Terracina, G. (Eds.), Logic Programming and Nonmonotonic Reasoning  8th International Conference, LPNMR05,
Diamante, Italy, September 2005, Proceedings, Vol. 3662 of Lecture Notes in Computer Science, pp. 4052. Springer Verlag.
Faber, W., Leone, N., & Pfeifer, G. (2004). Recursive aggregates in disjunctive logic programs: Semantics and complexity. In Alferes, J. J., & Leite, J. (Eds.), Proceedings
of the 9th European Conference on Artificial Intelligence (JELIA 2004), Vol. 3229 of
Lecture Notes in AI (LNAI), pp. 200212. Springer Verlag.
Faber, W., Leone, N., & Pfeifer, G. (2011). Semantics and complexity of recursive aggregates
in answer set programming. Artificial Intelligence, 175 (1), 278298. Special Issue:
John McCarthys Legacy.
Ferraris, P. (2005). Answer Sets for Propositional Theories. In Baral, C., Greco, G., Leone,
N., & Terracina, G. (Eds.), Logic Programming and Nonmonotonic Reasoning  8th
International Conference, LPNMR05, Diamante, Italy, September 2005, Proceedings,
Vol. 3662 of Lecture Notes in Computer Science, pp. 119131. Springer Verlag.
Ferraris, P. (2011). Logic programs with propositional connectives and aggregates. ACM
Transactions on Computational Logic, 12 (4). In press.
Gebser, M., Kaufmann, B., Neumann, A., & Schaub, T. (2007). Conflict-driven answer
set solving. In Twentieth International Joint Conference on Artificial Intelligence
(IJCAI-07), pp. 386392. Morgan Kaufmann Publishers.
Gelfond, M. (2002). Representing Knowledge in A-Prolog. In Kakas, A. C., & Sadri, F.
(Eds.), Computational Logic. Logic Programming and Beyond, Vol. 2408 of LNCS, pp.
413451. Springer.
Gelfond, M., & Lifschitz, V. (1991). Classical Negation in Logic Programs and Disjunctive
Databases. New Generation Computing, 9, 365385.
Gottlob, G., Leone, N., & Veith, H. (1999). Succinctness as a Source of Expression Complexity. Annals of Pure and Applied Logic, 97 (13), 231260.
Kemp, D. B., & Stuckey, P. J. (1991). Semantics of Logic Programs with Aggregates. In
Saraswat, V. A., & Ueda, K. (Eds.), Proceedings of the International Symposium on
Logic Programming (ISLP91), pp. 387401. MIT Press.
525

fiAlviano, Calimeri, Faber, Leone, & Perri

Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
The DLV System for Knowledge Representation and Reasoning. ACM Transactions
on Computational Logic, 7 (3), 499562.
Liu, L., Pontelli, E., Son, T. C., & Truszczynski, M. (2010). Logic programs with abstract
constraint atoms: The role of computations. Artificial Intelligence, 174 (34), 295315.
Liu, L., & Truszczynski, M. (2006). Properties and applications of programs with monotone
and convex constraints. Journal of Artificial Intelligence Research, 27, 299334.
Manna, M., Ruffolo, M., Oro, E., Alviano, M., & Leone, N. (2011). The HiLeX System for
Semantic Information Extraction. Transactions on Large-Scale Data and KnowledgeCentered Systems. Springer Berlin/Heidelberg, To appear.
Manna, M., Ricca, F., & Terracina, G. (2011). Consistent Query Answering via ASP from
Different Perspectives: Theory and Practice. Theory and Practice of Logic Programming, To appear.
Marek, V. W., & Truszczynski, M. (2004). Logic programs with abstract constraint atoms.
In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI
2004), pp. 8691. AAAI Press / The MIT Press.
McCarthy, J. (1959). Programs with Common Sense. In Proceedings of the Teddington
Conference on the Mechanization of Thought Processes, pp. 7591. Her Majestys
Stationery Office.
McCarthy, J. (1980). Circumscription  a Form of Non-Monotonic Reasoning. Artificial
Intelligence, 13 (12), 2739.
McCarthy, J. (1986). Applications of Circumscription to Formalizing Common-Sense
Knowledge. Artificial Intelligence, 28 (1), 89116.
McCarthy, J. (1990). Formalization of Common Sense, papers by John McCarthy edited by
V. Lifschitz. Ablex.
McCarthy, J., & Hayes, P. J. (1969). Some Philosophical Problems from the Standpoint
of Artificial Intelligence. In Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4,
pp. 463502. Edinburgh University Press. reprinted in (McCarthy, 1990).
McDermott, D. V. (1982). Non-Monotonic Logic II: Nonmonotonic Modal Theories. Journal
of the ACM, 29 (1), 3357.
McDermott, D. V., & Doyle, J. (1980). Non-Monotonic Logic I. Artificial Intelligence,
13 (12), 4172.
Minsky, M. (1975). A Framework for Representing Knowledge. In Winston, P. H. (Ed.),
The Psychology of Computer Vision, pp. 211277. McGraw-Hill.
Moore, R. C. (1985). Semantical Considerations on Nonmonotonic Logic. Artificial Intelligence, 25 (1), 7594.
Osorio, M., & Jayaraman, B. (1999). Aggregation and Negation-As-Failure. New Generation
Computing, 17 (3), 255284.
Pelov, N. (2004). Semantics of Logic Programs with Aggregates. Ph.D. thesis, Katholieke
Universiteit Leuven, Leuven, Belgium.
Pelov, N., Denecker, M., & Bruynooghe, M. (2003). Translation of Aggregate Programs to
Normal Logic Programs. In de Vos, M., & Provetti, A. (Eds.), Proceedings ASP03
- Answer Set Programming: Advances in Theory and Implementation, pp. 2942,
Messina, Italy. Online at http://CEUR-WS.org/Vol-78/.
526

fiUnfounded Sets and Well-Founded Semantics of ASP Programs with Aggregates

Pelov, N., Denecker, M., & Bruynooghe, M. (2004). Partial stable models for logic programs with aggregates. In Proceedings of the 7th International Conference on Logic
Programming and Non-Monotonic Reasoning (LPNMR-7), Vol. 2923 of Lecture Notes
in AI (LNAI), pp. 207219. Springer.
Pelov, N., Denecker, M., & Bruynooghe, M. (2007). Well-founded and Stable Semantics of
Logic Programs with Aggregates. Theory and Practice of Logic Programming, 7 (3),
301353.
Pelov, N., & Truszczynski, M. (2004). Semantics of disjunctive programs with monotone
aggregates - an operator-based approach. In Proceedings of the 10th International
Workshop on Non-monotonic Reasoning (NMR 2004), Whistler, BC, Canada, pp.
327334.
Reiter, R. (1980). A Logic for Default Reasoning. Artificial Intelligence, 13 (12), 81132.
Ricca, F., Alviano, M., Dimasi, A., Grasso, G., Ielpa, S. M., Iiritano, S., Manna, M., &
Leone, N. (2010). A Logic-Based System for e-Tourism. Fundamenta Informaticae.
IOS Press, 105 (12), 3555.
Ricca, F., Grasso, G., Alviano, M., Manna, M., Lio, V., Iiritano, S., & Leone, N. (2011).
Team-building with Answer Set Programming in the Gioia-Tauro Seaport. Theory
and Practice of Logic Programming. Cambridge University Press, To appear.
Ross, K. A., & Sagiv, Y. (1997). Monotonic Aggregation in Deductive Databases. Journal
of Computer and System Sciences, 54 (1), 7997.
Simons, P., Niemela, I., & Soininen, T. (2002). Extending and Implementing the Stable
Model Semantics. Artificial Intelligence, 138, 181234.
Son, T. C., & Pontelli, E. (2007). A Constructive semantic characterization of aggregates
in answer set programming. Theory and Practice of Logic Programming, 7, 355375.
Son, T. C., Pontelli, E., & Tu, P. H. (2007). Answer Sets for Logic Programs with Arbitrary
Abstract Constraint Atoms. Journal of Artificial Intelligence Research, 29, 353389.
Swift, T., & Warren, D. S. (2010). XSB: Extending prolog with tabled logic programming.
Computing Research Repository (CoRR), abs/1012.5123.
Tarski, A. (1955). A lattice-theoretical fixpoint theorem and its applications. Pacific J.
Math, 5, 285309.
Truszczynski, M. (2010). Reducts of propositional theories, satisfiability relations, and
generalizations of semantics of logic programs. Artificial Intelligence, 174, 12851306.
Ullman, J. D. (1989). Principles of Database and Knowledge Base Systems. Computer
Science Press.
Van Gelder, A. (1992). The Well-Founded Semantics of Aggregation. In Proceedings of
the Eleventh Symposium on Principles of Database Systems (PODS92), pp. 127138.
ACM Press.
Van Gelder, A., Ross, K. A., & Schlipf, J. S. (1991). The Well-Founded Semantics for
General Logic Programs. Journal of the ACM, 38 (3), 620650.
Wittocx, J., Marien, M., & Denecker, M. (2008). The IDP system: A model expansion
system for an extension of classical logic. In Denecker, M. (Ed.), Proceedings of
the 2nd Workshop on Logic and Search, Computation of Structures from Declarative
Descriptions (LaSh08), pp. 153165.

527

fiJournal of Artificial Intelligence Research 42 (2011) 1-29

Submitted 11/10; published 09/11

Where Are the Hard Manipulation Problems?
Toby Walsh

toby.walsh@nicta.com.au

NICTA and UNSW
Sydney
Australia

Abstract
Voting is a simple mechanism to combine together the preferences of multiple agents. Unfortunately, agents may try to manipulate the result by mis-reporting their preferences. One barrier
that might exist to such manipulation is computational complexity. In particular, it has been
shown that it is NP-hard to compute how to manipulate a number of different voting rules. However, NP-hardness only bounds the worst-case complexity. Recent theoretical results suggest that
manipulation may often be easy in practice. In this paper, we show that empirical studies are
useful in improving our understanding of this issue. We consider two settings which represent
the two types of complexity results that have been identified in this area: manipulation with unweighted votes by a single agent, and manipulation with weighted votes by a coalition of agents.
In the first case, we consider Single Transferable Voting (STV), and in the second case, we consider
veto voting. STV is one of the few voting rules used in practice where it is NP-hard to compute
how a single agent can manipulate the result when votes are unweighted. It also appears one of
the harder voting rules to manipulate since it involves multiple rounds. On the other hand, veto
voting is one of the simplest representatives of voting rules where it is NP-hard to compute how a
coalition of weighted agents can manipulate the result. In our experiments, we sample a number
of distributions of votes including uniform, correlated and real world elections. In many of the
elections in our experiments, it was easy to compute how to manipulate the result or to prove that
manipulation was impossible. Even when we were able to identify a situation in which manipulation was hard to compute (e.g. when votes are highly correlated and the election is hung), we
found that the computational difficulty of computing manipulations was somewhat precarious (e.g.
with such hung elections, even a single uncorrelated voter was enough to make manipulation
easy to compute).

1. Introduction
The Gibbard-Satterthwaite theorem proves that, under some weak assumptions like three or more
candidates and the absence of a dictator, voting rules are manipulable (Gibbard, 1973; Satterthwaite, 1975). That is, it may pay for agents not to report their preferences truthfully. One appealing
escape from this result was proposed by Bartholdi, Tovey and Trick (1989). Whilst a manipulation
may exist, perhaps it is computationally too difficult to find. To illustrate this idea, they demonstrated that the second order Copeland rule is NP-hard to manipulate. Shortly after, Bartholdi and
Orlin (1991) proved that the more well known Single Transferable Voting (STV) rule is NP-hard
to manipulate. A whole subfield of social choice has since grown from this proposal, studying the
computational complexity of manipulation and of the control of voting rules. Two good surveys have
recently appeared that provide many references into the literature (Faliszewski, Hemaspaandra, &
Hemaspaandra, 2010; Faliszewski & Procaccia, 2010). Computational complexity results about the
manipulation of voting rules typically vary along five different dimensions.
Weighted or unweighted votes: Are the votes weighted or unweighted? Although many elections involve unweighted votes, weighted votes are used in a number of real-world settings like
shareholder meetings, and elected assemblies. Weights are also useful in multi-agent systems
where we have different types of agents. Weights are interesting from a computational perspective for at least two reasons. First, weights can increase computational complexity. For
c
2011
AI Access Foundation. All rights reserved.

fiWalsh

example, computing how to manipulate the veto rule is polynomial with unweighted votes but
NP-hard with weighted votes (Conitzer, Sandholm, & Lang, 2007). Second, the weighted case
informs us about the unweighted case when we have probabilistic information about the votes.
For instance, if it is NP-hard to compute if an election can be manipulated with weighted
votes, then it is NP-hard to compute the probability of a candidate winning when there is
uncertainty about how the unweighted votes have been cast (Conitzer & Sandholm, 2002a).
Bounded or unbounded number of candidates: Do we have a (small) fixed number of candidates? Or is the number of candidates allowed to grow? For example, with unweighted votes,
computing a manipulation of the STV rule is polynomial if we bound the number of candidates and only NP-hard when the number of candidates is allowed to grow with problem size
(Bartholdi & Orlin, 1991). Indeed, with unweighted votes and a bounded number of candidates, it is polynomial to compute how to manipulate most voting rules (Conitzer et al., 2007).
On the other hand, with weighted votes, it is NP-hard to compute how to manipulate many
voting rules with a bounded number of candidates. For example, it is NP-hard to compute a
manipulation with the veto rule and 3 or more candidates (Conitzer et al., 2007).
One manipulator or a coalition of manipulators: Is a single agent trying to manipulate the
results or is a coalition of agents acting together? A single agent is unlikely to be able to change
the outcome of many elections. A coalition, on the other hand, may be able to manipulate
the result. With some rules, like STV, it is NP-hard to compute how a single agent needs to
vote to manipulate the result or to prove that manipulation by this single agent is impossible
(Bartholdi & Orlin, 1991). With other rules like Borda, it may require a coalition of two
agents for manipulation to be NP-hard to compute (Davies, Katsirelos, Narodytska, & Walsh,
2011; Betzler, Niedermeier, & Woeginger, 2011). With other rules like veto, we may require
a coalition of manipulating agents whose size is unbounded (and allowed to grow with the
problem size) for manipulation to be NP-hard to compute (Conitzer et al., 2007).
Complete or incomplete information: Many results assume that the manipulator(s) have complete information about the other agents votes. Of course, we may not know precisely how
other agents will vote in practice. However, there are several reasons why the case of complete information is interesting. First, if we can show that it is computationally intractable to
compute how to manipulate the election with complete information then it is also intractable
when we have incomplete information. Second, the complete information case informs the case
when we have uncertainty. For instance, if it is computationally intractable for a coalition to
compute how to manipulate an election with complete information then it is also intractable
for an individual to compute how to manipulate an election when we have only probabilistic
information about the votes (Conitzer et al., 2007).
Constructive or destructive manipulation: Is the manipulator trying to make one particular
candidate win (constructive manipulation) or prevent one particular candidate from winning
(destructive manipulation)? Destructive manipulation is easier to compute than constructive
manipulation. For instance, constructive manipulation of the veto rule by a coalition of agents
with weighted votes is NP-hard but destructive manipulation is polynomial (Conitzer et al.,
2007). However, there are also rules where both destructive and constructive manipulation are
in the same complexity class. For example, both constructive and destructive manipulation of
plurality are polynomial to compute, whilst both constructive and destructive manipulation
of plurality with runoff for weighted votes are NP-hard (Conitzer et al., 2007).
In Figure 1, we give a representative selection of results about the complexity of manipulating
voting. References to many of these results can be found in the work of Conitzer et al. (2007).
In this paper, we will focus on two cases that cover the main types of computational complexity
results that have been identified: manipulation of STV with unweighted votes by a single agent
2

fiWhere Are the Hard Manipulation Problems?

number of candidates
plurality
Borda
veto
STV
plurality with runoff
Copeland

unweighted votes,
constructive
manipulation
P
NP-c
P
NP-c
P
NP-c

2
P
P
P
P
P
P

weighted votes,
constructive
destructive
3
4
2
3
P
P
P
P
NP-c NP-c P
P
NP-c NP-c P
P
NP-c NP-c P
NP-c
NP-c NP-c P
NP-c
P
NP-c P
P

Figure 1: Computational complexity of deciding if various voting rules can be manipulated by an
agent (unweighted votes) or a coalition of agents (weighted votes). P means that the
problem is polynomial, and NP-c that the problem is NP-complete. For example, constructive manipulation of the veto rule is polynomial for unweighted votes or for weighted
votes with 2 candidates, NP-hard for 3 or more candidates. On the other hand, destructive manipulation of the veto rule is polynomial for weighted votes with a coalition of
manipulating agents and 2 or more candidates.

and an unbounded number of candidates in the election, and manipulation of veto voting with
weighted votes by a coalition of agents and just 3 candidates in the election. In both cases, we
assume complete information about the votes of the other agents. The two cases thus cover cases
in which computational complexity is associated: unweighted votes, a small (bounded) number of
manipulators and a large (unbounded) number of candidates; weighted votes, a large (unbounded)
number of manipulators, and a small (bounded) number of candidates.
STV is a very obvious and interesting case to consider when we study the computational complexity of manipulation. STV is one of the few voting rules used in practice where manipulation is
NP-hard to compute when votes are unweighted. Bartholdi and Orlin argued that STV is one of
the most promising voting rules to consider in this respect:
STV is apparently unique among voting schemes in actual use today in that it is computationally resistant to manipulation., (Bartholdi & Orlin, 1991, p. 341).
STV also appears more difficult to manipulate than many other rules. For example, Chamberlain
(1985) studied four different measures of the manipulability of a voting rule: the probability that
manipulation is possible, the number of candidates who can be made to win, the coalition size
necessary to manipulate, and the margin-of-error which still results in a successful manipulation.
Compared to other commonly used rules like plurality and Borda, his results showed that STV was
the most difficult to manipulate by a substantial margin. He concluded that:
[this] superior performance . . . combined with the rather complex and implausible nature
of the strategies to manipulate it, suggest that it [the STV rule] may be quite resistant to
manipulation, (Chamberlin, 1985, p. 203).
The second case considered in this paper (manipulation of the veto rule by a coalition of manipulators with three candidates) is interesting to study for several reasons. First, the veto rule is
a simple representative of voting rules where manipulation by a coalition of agents with weighted
votes and a small number of candidates is NP-hard to compute. Second, the veto rule is very easy
to reason about. Unlike STV, there are not multiple rounds and the elimination of candidates to
worry about. In fact, as we show, manipulation of the veto rule is equivalent to a simple number
partitioning problem. We can therefore use efficient number partitioning algorithms to compute manipulations. Third, the veto rule is on the borderline of tractability since constructive manipulation
3

fiWalsh

of the rule by a coalition of weighted agents is NP-hard but destructive manipulation is polynomial
(since the best way to ensure a candidate does not win is simply to veto this candidate) (Conitzer
et al., 2007).
Our empirical study considers the computational difficulty of computing manipulations in practice. NP-hardness results only describe the worst-case. There is increasing concern that computational complexity results like these may not reflect the actual difficulty of computing manipulations
in practice. For instance, a number of recent theoretical results suggest that manipulation may
often be computationally easy (Conitzer & Sandholm, 2006; Procaccia & Rosenschein, 2007b; Xia &
Conitzer, 2008a; Friedgut, Kalai, & Nisan, 2008; Xia & Conitzer, 2008b). Our results demonstrate
that we can profitably study this issue empirically. There are several reasons why empirical analysis
like that undertaken here is useful. First, theoretical analysis is often asymptotic so does not show
the size of hidden constants. In addition, elections are typically bounded in size. Can we be sure
that asymptotic behaviour is relevant for the finite sized electorates met in practice? For instance,
our results suggest that we can easily compute manipulations for almost any type of STV election
with up to 100 candidates. Second, theoretical analysis is often restricted to particular distributions
(e.g. independent and identically distributed votes). Manipulation may be very different in practice
due to correlations between votes. For instance, if all preferences are single-peaked then a voting
rule which selects the median candidate is not manipulable. With the median voting rule, it is
in the best interests of all agents to state their true preferences. It is thus clear that correlations
between votes can have an impact on the manipulability of an election. Indeed, a number of recent
results have studied whether manipulation remains computationally hard when votes are limited to
be single-peaked (Walsh, 2007; Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009; Brandt,
Brill, Hemaspaandra, & Hemaspaandra, 2010). Our experiments will therefore look at elections
in which there are correlations between votes. Third, many of the theoretical results about the
computational complexity of manipulation have been hard won and are limited in their scope. For
instance, a long standing open result was recently settled, proving that computing a manipulation of
the Borda rule by a coalition of manipulators is NP-hard (Davies et al., 2011; Betzler et al., 2011).
However, both proofs require precisely 2 manipulators. It remains open if computing a manipulation of the Borda rule by a larger coalition is NP-hard. An empirical study may quickly suggest
whether the result extends to larger coalitions. Finally, empirical studies may suggest new avenues
for theoretical study. For example, the experiments reported here suggest a simple and universal
form for the probability that a coalition of agents in a veto election can elect a desired candidate.
It would be interesting to try to derive this form theoretically.

2. Background
We give some notation and background that will be used throughout the rest of the paper. Let m
be the number of candidates in the election. A vote is a linear order (a transitive, antisymmetric,
and total relation) over the set of m candidates. Let n the the number of agents voting. A profile
is a n-tuple of votes. We let N (i, j) be the number of agents preferring i to j in a profile. A voting
rule is a function that maps any profile to an unique winning alternative. In this paper, we consider
a number of common voting rules:
Scoring rules: (w1 , . . . , wm ) is a vector of weights, the ith candidate in a total order scores wi , and
the winner is the candidate with highest total score. The plurality rule has the weight vector
(1, 0, . . . , 0), the veto rule has the vector (1, 1, . . . , 1, 0), whilst the Borda rule has the vector
(m  1, m  2, . . . , 0). With the veto rule, each voter effectively vetoes one candidate and the
candidate with the fewest vetoes wins.
Single transferable vote (STV): STV proceeds in a number of rounds. Unless one candidate
has a majority of first place votes, we eliminate the candidate with the least number of first

4

fiWhere Are the Hard Manipulation Problems?

place votes. Any ballots placing the eliminated candidate in first place are re-assigned to the
second place candidate. We then repeat until one candidate has a majority.
Copeland (aka tournament):
P The candidate with the highest Copeland score wins. The Copeland
score of candidate i is i6=j (N (i, j) > n2 )  (N (i, j) < n2 ). The Copeland winner is the candidate that wins the most pairwise elections. In the second order Copeland rule, if there is a
tie, the winner is the candidate whose defeated competitors have the largest sum of Copeland
scores.
Maximin (aka Simpson): The candidate with the highest maximin score wins. The maximin
score of candidate i is mini6=j N (i, j). The Simpson winner is the candidate whose worst
performance in pairwise elections is best.
All these rules can be easily modified to work with weighted votes. A vote of integer weight w can be
viewed as w agents who vote identically. All these voting rules are anonymous as the order of votes in
the profile is unimportant. A profile can therefore be thought of as a multi-set of n votes. To ensure
the winner is unique, we will sometimes need to break ties (e.g. when two candidates have the same
number of vetoes, or when two candidates have the same number of first place votes). In the UK,
for example, when an election is tied, the returning officer will choose between the candidates using
a random method like lots or a coin toss. A typical assumption made in the literature (and in this
paper) is that ties are broken in favour of the manipulator. More precisely, given a choice of several
candidates, we tie-break in favour of the candidate most preferred by the manipulator. Suppose the
manipulator can make their preferred candidate win assuming ties are broken in their favour but
ties are in fact broken at random. Then we can conclude that the manipulator can increase the
chance of getting their preferred result. It would be interesting to consider other tie-breaking rules.
Indeed, tie-breaking can even introduce computational complexity into manipulation. For example,
computing how to manipulate the Copeland rule with weighted votes is polynomial if ties are scored
with 1 but NP-hard if they are scored with 0 (Faliszewski, Hemaspaandra, & Schnoor, 2008).
We will consider one agent or a coalition of k agents trying to manipulate the result of the election.
Manipulation is the situation where the manipulators vote differently to their true preferences in
order to improve the outcome from their perspective. As is common in the literature, we assume
that the manipulators have complete knowledge of the other votes (and, where appropriate, complete
knowledge of all the weights associated with all the votes). Whilst it may be unrealistic in practice
to assume we have complete knowledge about the other votes, there are several reasons why this case
is interesting to consider. First, complete information is likely to be a special case of any uncertainty
model. Hence, any computational hardness results for complete information directly imply hardness
for the corresponding uncertainty model. Second, results about the hardness of manipulation by
a coalition with weighted votes and complete information imply hardness of manipulation by an
individual agent with unweighted votes and incomplete information (Conitzer et al., 2007). Third,
by assuming complete information, we factor out any complexity coming from the uncertainty model
and focus instead on computing just the manipulation.
In addition to some of the standard uniform and random models of votes, we will consider
two restricted types of votes: single-peaked and single-troughed votes. With single-peaked votes,
candidates can be placed on a line, and an agents preference for a candidate decreases with distance
from their single most preferred candidate. Single-peaked preferences are interesting from several
perspectives. First, single-peaked preferences are likely to occur in a number of domains. For
example, if you are buying a house, you might have an optimal price in mind and your preference
for a house decreases as the distance from this price increases. Second, single-peaked preferences
are easy to elicit. Conitzer (2007, 2009) gives a strategy for eliciting any single-peaked preference
ordering with a linear number of pairwise ranking questions. Third, single-peaked preferences prevent
certain problematic situations arising when aggregating preferences. In particular, they prevent the
existence of Condorcet cycles. In fact, the median candidate of a single-peaked profile beats all

5

fiWalsh

others in pairwise comparisons (that is, the median candidate is the Condorcet winner) (Black,
1948). With single-troughed votes, on the other hand, candidates can be placed on a line, and an
agents preference for a candidate increases with distance from their single least preferred candidate.
For example, if the candidates are locations to build a new incinerator, you might have a least
preferred location (your own neighbourhood), and your preference increases the further away the
incinerator is from this. Single-troughed votes have similar nice properties to single-peaked votes
(Barbera, Berga, & Moreno, 2009).

3. Single Transferable Voting
We begin with an empirical study of manipulation in STV elections. STV is used in a wide variety
of real-world settings including the election of the Irish presidency, the Australian House of Representatives, the Academy awards, and many organisations including the American Political Science
Association, the International Olympic Committee, and the British Labour Party. Interestingly it
is NP-hard to compute if a single agent can manipulate the STV rule. Indeed, it is one of the
few voting rules used in practice where manipulation is NP-hard to compute in this setting. More
precisely, STV is NP-hard to manipulate by a single agent if the number of candidates is unbounded
and votes are unweighted (Bartholdi & Orlin, 1991), or by a coalition of agents if there are 3 or
more candidates and votes are weighted (Conitzer et al., 2007). Coleman and Teague give an enumerative method for a coalition of k unweighted agents to compute a manipulation of the STV rule
which runs in O(m!(n + mk)) time where n is the number of agents voting and m is the number
of candidates (Coleman & Teague, 2007). For a single manipulator, Conitzer, Sandholm and Lang
give an O(n1.62m ) time algorithm (called CSL from now on) to compute the set of candidates that
can win an STV election (Conitzer et al., 2007).
In Figure 2, we give a modified version of the CSL algorithm for computing a manipulation of
the STV rule. This uses a similar recursion as CSL but changes the original algorithm in several
ways to take advantage of the fact that we only want to compute if one distinguished candidate can
win and do not need to know all of the other candidates that can possibly win. There are two main
changes to CSL. First, we can ignore elections in which the chosen candidate is eliminated. Second,
we can terminate search as soon as a manipulation is found in which the chosen candidate wins. In
particular, we do not need to explore the left branch of the search tree when the right branch gives
a successful manipulation.
We tested the modified algorithm with the simplest possible scenario: elections in which each
vote is equally likely. We have one agent trying to manipulate an election of m candidates where
n other agents vote. Votes are drawn uniformly at random from all m! possible votes. This is the
Impartial Culture (IC) model. To show the benefits of our modifications to the CSL algorithm, we
ran a simple experiment in which m = n. The experiment was run in CLISP 2.42 on a 3.2 GHz
Pentium 4 with 3GB of memory running Ubuntu 8.04.3. Table 1 gives the mean nodes explored
and runtime needed to compute a manipulation or prove none exists. Median and other percentiles
display similar behaviour. We see that our modified method can be considerably faster than the
original CSL algorithm. In addition, as problems get larger, the improvement increases. At n = 32,
our method is nearly 10 times faster than CSL. This increases to roughly 40 times faster at n = 128.
These improvements reduce the time to find a manipulation on the largest problems from several
hours to a couple of minutes.
3.1 Varying Number of Agents
We next performed some detailed experiments looking for phase transition behaviour and hard manipulation problems. In many other NP-hard problem domains, the computationally hard instances
have been shown to be often associated with the region of the parameter space in which there is
a rapid transition in the probability that a solution exists (Cheeseman, Kanefsky, & Taylor, 1991;

6

fiWhere Are the Hard Manipulation Problems?

Manipulate(c, R, (s1 , . . . , sm ), f )
1 if |R| = 1
; Is there one candidate left?
2
then return (R = {c})
; Is it the chosen candidate?
3 if f = 0
; Is the top of the manipulators vote currently free?
4
then
5
d  arg minjR (sj )
; Who will currently be eliminated?
6
sd  sd + w
; Suppose the manipulator votes for them
7
e  arg minjR (sj )
8
if d = e
; Does this not change the result?
9
then return
10
(c 6= d) and Manipulate(c, R  {d}, Transfer ((s1 , . . . , sm ), d, R), 0)
11
else return
12
((c 6= d) and Manipulate(c, R  {d}, Transfer ((s1 , . . . , sm ), d, R), 0)) or
13
((c 6= e) and Manipulate(c, R  {e}, Transfer ((s1 , . . . , sm ), e, R), d))
14
else
; The top of the manipulators vote is fixed
15
d  arg minjR (sj )
; Who will be eliminated?
16
if c = d
; Is this the chosen candidate?
17
then return f alse
18
if d = f
; Is the manipulator free again to change the result?
19
then return Manipulate(c, R  {d}, Transfer ((s1 , . . . , sm ), d, R), 0)
20
else return Manipulate(c, R  {d}, Transfer ((s1 , . . . , sm ), d, R), f )

Figure 2: Our modified algorithm to compute if an agent can manipulate an STV election.
We use integers from 1 to m for the candidates, integers from 1 to n for the agents (with n being the
manipulator), c for the candidate who the manipulator wants to win, R for the set of un-eliminated
candidates, sj for the weight of agents ranking candidate j first amongst R, w for the weight of the
manipulator, and f for the candidate most highly ranked by the manipulator amongst R (or 0 if
there is currently no constraint on who is most highly ranked). The function Transfer computes the
a vector of the new weights of agents ranking candidate j first amongst R after a given candidate is
eliminated. The algorithm is initially called with R set to every candidate, and f to 0.
Mitchell, Selman, & Levesque, 1992). The phase transition between a satisfiable and an unsatisfiable phase resembles that seen in statistical physics in Ising magnets and similar systems. There
are several good surveys of this area (Dubois, Monasson, Selman, & Zecchina, 2001; Hartmann &
Weigt, 2005; Gomes & Walsh, 2006).
Our first experiment varied the number of agents voting. In Figures 3 and 4, we plot the
probability that a manipulator can make a random agent win, and the cost to compute if this is
possible when we fix the number of candidates but vary the number of agents in the STV election. In
this and subsequent experiments, we tested 1000 problems at each point. The number of candidates
and of agents voting are varied in powers of 2, typically from 20 (= 1) to 27 (= 128) unless otherwise
indicated.
The ability of an agent to manipulate the election decreases as the number of agents increases.
Only if there are few votes and few candidates is there a significant chance that the manipulator will
be able to change the result. Phase transition behaviour has been observed in many NP-complete
problem domains including propositional satisfiability (Mitchell et al., 1992; Gent & Walsh, 1994,
1996b), constraint satisfaction (Prosser, 1994; Smith, 1994; Gent, MacIntyre, Prosser, & Walsh, 1995;
Gent, MacIntyre, Prosser, Smith, & Walsh, 2001), graph colouring (Walsh, 1998, 1999, 2001), number
partitioning (Gent & Walsh, 1996a, 1998; Mertens, 2001) and the travelling salesperson problem
(Zhang & Korf, 1996; Gent & Walsh, 1996c). Unlike these domains, the probability curve observed
7

fiWalsh

CSL algorithm
nodes
time/s
1.46
0.00
3.28
0.00
11.80
0.00
59.05
0.03
570.11
0.63
14,676.17
33.22
8,429,800.00 6,538.13

n
2
4
8
16
32
64
128

Modified algorithm
nodes
time/s
1.24
0.00
1.59
0.00
3.70
0.00
12.62
0.01
55.20
0.09
963.39
3.00
159,221.10 176.68

Table 1: Comparison between the original CSL algorithm and the modified version which computes
a constructive manipulation of an STV election. The table gives the mean nodes explored
and runtime needed to compute a manipulation or prove none exists. Median and other
percentiles display similar behaviour. Each election has n agents voting uniformly at random over n different candidates. Best results in each row are in bold.

1

m=4
m=8
m=16
m=32
m=64
m=128

0.9

prob(manipulable)

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

10
total number of agents voting, n+1

100

Figure 3: Manipulability of an STV election containing random uniform votes. The number of
candidates is fixed and we vary the number of agents voting. The vertical axis measures
the probability that a single manipulating agent can make a random candidate win. The
horizontal axis measures the total number of agents voting. Note that n is the number of
agents voting besides the manipulator so that a log scale can be used on the horizontal
axis.

8

fiWhere Are the Hard Manipulation Problems?

here does not appear to sharpen to a step function around a fixed point. The probability curve
resembles the smooth phase transitions seen in polynomial problems like 2-colouring (Achlioptas,
1999) and 1-in-2 satisfiability (Walsh, 2002). As indicated before, we assume that ties are broken in
favour of the manipulator. For this reason, the probability that an election is manipulable is greater
1
. Finding a manipulation or proving none is possible is easy unless we have both a large
than m
1e+06
m=128
m=64
m=32
m=16
m=8
m=4

100000

mean nodes

10000

1000

100

10

1
1

10

100

agents, n

Figure 4: Search cost to compute if an agent can manipulate an STV election containing random
uniform votes. The number of candidates, m is fixed and we vary the number of agents.
The vertical axis measures the mean nodes explored to compute if a single manipulating
agent can make a random candidate win. The horizontal axis measures the number of
agents voting besides the manipulator. Median and other percentiles are similar.
number of agents and a large number of candidates. However, in this situation, the chance that the
manipulator can change the result is very small.
3.2 Varying Number of Candidates
Our next experiment slices the parameter space in an orthogonal direction, varying the number of
candidates in the election. In Figure 5, we plot the cost to compute if the manipulator can make
a random agent win an STV election when we fix the number of agents but vary the number of
candidates. The probability curve that the manipulator can make a random agent win resembles
Figure 3. Whilst the cost of computing a manipulation appears to increase exponentially with the
number of candidates m, the observed scaling is much better than the 1.62m worst case scaling of
the original CSL algorithm. We can easily compute manipulations for up to 128 candidates. Note
that 1.62m is over 1026 for m = 128. Thus, we appear to be far from the worst case. We fitted the
observed data to the function cdm and found a fit with d = 1.008 and a coefficient of determination
R2 = 0.95 indicating a good fit.
3.3 Correlated Votes
In many real life situations, votes are not completely uniform and uncorrelated with each other.
What happens if we introduce correlation between votes? Here we consider random votes drawn
from the Polya-Eggenberger urn model (Berg, 1985). In this model, we have an urn containing all
m! possible votes. We draw votes out of the urn at random, and put them back into the urn with
9

fiWalsh

1.62**m
n=128
n=64
n=32
n=16
n=8
n=4

1e+14
1e+12

mean nodes

1e+10
1e+08
1e+06
10000
100
1
20

40

60

80

100

120

candidates, m

Figure 5: Search cost to compute if an agent can manipulate an STV election containing random
uniform votes. The number of agents, n is fixed and we vary the number of candidates.
The vertical axis measures the mean number of nodes explored to compute a manipulation
or prove that none exists. The horizontal axis measures m, the number of candidates in
the election. Median and other percentiles are similar.

a additional votes of the same type (where a is a parameter). As a increases, there is increasing
correlation between the votes. This generalises both the Impartial Culture model (a = 0) in which
all votes are equally likely and the Impartial Anonymous Culture (a = 1) model in which all profiles
are equally likely (McCabe-Dansted & Slinko, 2006). To give a parameter independent of problem
a
size, we consider b = m!
. For instance, with b = 1, there is a 50% chance that the second vote is the
same as the first.
In Figures 6 and 7, we plot the probability that a manipulator can make a random agent win
an STV election, and the cost to compute if this is possible as we vary the number of candidates in
an election where votes are drawn from the Polya-Eggenberger urn model. As before, the ability of
an agent to manipulate the election decreases as the number of candidates, m increases. The search
cost to compute a manipulation appears to increase exponentially with the number of candidates
m. However, we can easily compute manipulations for up to 128 candidates. We fitted the observed
data to the function cdm and found a fit with d = 1.001 and a coefficient of determination R2 = 0.99
indicating a good fit.
In Figure 8, we plot the cost to compute a manipulation when we fix the number of candidates
but vary the number of agents. As before, the ability of an agent to manipulate the election decreases
as the number of agents increases. Only if there are few votes and few candidates is there any chance
that the manipulator will succeed. As in previous experiments, finding a manipulation or proving
none exists is easy even if we have many agents and candidates. We also observed results which are
almost indistinguishable when votes were correlated by being single-peaked (or single-troughed) and
were drawn either uniformly at random or from an urn model.

10

fiWhere Are the Hard Manipulation Problems?

1

n=64
n=32
n=16
n=8
n=4

0.9

prob(manipulable)

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

10

100

candidates, m

Figure 6: Manipulability of an STV election containing correlated votes. The number of agents is
fixed and we vary the number of candidates, m. The n fixed votes are drawn from the
Polya-Eggenberger urn model with b = 1. The vertical axis measures the probability that
the manipulator can make a random candidate win. The horizontal axis measures the
number of candidates, m in the election.

1.62**m
n=64
n=32
n=16
n=8
n=4

1e+14
1e+12

mean nodes

1e+10
1e+08
1e+06
10000
100
1
20

40

60
80
candidates, m

100

120

Figure 7: Search cost to compute if an agent can manipulate an STV election containing correlated
votes. The number of agents, n is fixed and we vary the number of candidates, m. The
n fixed votes are drawn using the Polya-Eggenberger urn model with b = 1. The vertical
axis measures the mean number of search nodes explored to compute a manipulation or
prove that none exists. The horizontal axis measures the number of candidates, m in the
election. The curves for different n fit closely on top of each other. Median and other
percentiles are similar.

11

fiWalsh

1e+06

m=64
m=32
m=16
m=8
m=4

100000

mean nodes

10000

1000

100

10

1
20

40

60
80
agents, n

100

120

Figure 8: Search cost to compute if an agent can manipulate an STV election with correlated votes.
The number of candidates, m is fixed and we vary the number of agents, n. The n fixed
votes are drawn using the Polya-Eggenberger urn model with b = 1. The vertical axis
measures the mean number of search nodes explored to compute a manipulation or prove
that none exists. The horizontal axis measures the number of agents, n. Median and other
percentiles are similar.

4. Coalition Manipulation
Our algorithm for computing manipulation of an STV election by a single agent can also be used
to compute if a coalition can manipulate an STV election when the members of coalition vote in
unison. This ignores more complex manipulations where the members of the coalition need to vote
in different ways. Insisting that the members of the coalition vote in unison might be reasonable if
we wish manipulation to have both a low computational and communication cost (Slinko & White,
2008). In Figures 9 and 10, we plot the probability that a coalition voting in unison can make a
random agent win an STV election, and the cost to compute if this is possible as we vary the size of
the coalition. Theoretical results due to Procaccia and Rosenschein (2007a) and Xia and Conitzer

(2008a) suggest that the critical size of a coalition
that can manipulate an election grows as n.

We therefore normalize the coalition size by n.
The ability of the coalition to manipulate
the election increases as the size of the coalition

increases. When the coalition is about n in size, the probability that the coalition can manipulate
the election so that a candidate chosen at random wins is around 21 . The cost to compute a
manipulation (or prove that none exists) decreases as we increase the size of the coalition. It is
easier for a larger coalition to manipulate an election than for a smaller one.
These experiments again suggest different behaviour occurs here than in other combinatorial
problems like propositional satisfiability and graph colouring. For instance, we do not see a rapid
transition that sharpens around a fixed point as in 3-satisfiability. When we vary the coalition size,
we see
 a transition in the probability of being able to manipulate the result around a coalition size
k = n. However, this transition appears smooth and does not seem to sharpen
 towards a step
function as n increases. In addition, hard instances do not occur around k = n. Indeed, the
hardest instances are when the coalition is smaller than this and has only a small chance of being
able to manipulate the result.

12

fiWhere Are the Hard Manipulation Problems?

1
n=4
n=8
n=16
n=32
n=64

prob(manipulable)

0.8

0.6

0.4

0.2

0
0

0.5

1

1.5

2

2.5

3

3.5

4

normalized coalition size, k/sqrt(n)

Figure 9: Manipulability of an STV election as we vary the size of the manipulating coalition. The
number of candidates is the same as the number of non-manipulating agents. The n fixed
votes are uniformly drawn at random from the n! possible votes. The vertical axis measures
the probability that the coalition can make arandom candidate win. The horizontal axis
measures the coalition size, k normalized by n.

1e+06

n=64
n=32
n=16
n=8
n=4

100000

mean nodes

10000

1000

100

10

1
0

0.5

1
1.5
2
2.5
3
normalized coalition size, k/sqrt(n)

3.5

4

Figure 10: Search cost to compute if a coalition can manipulate an STV election as we vary coalition
size. The vertical axis measures the mean number of search nodes explored to compute a
manipulation or prove
that none exists. The horizontal axis measures the coalition size,

k normalized by n. Median and other percentiles are similar.

13

fiWalsh

5. Sampling Real Elections
Elections met in practice may differ from those sampled so far. There might, for instance, be some
votes which are never cast. On the other hand, with the models studied so far every possible vote
has a non-zero probability of being seen. We therefore sampled some real voting records. We have
previously studied phase transition behaviour in other real world problems using similar sampling
techniques (Gent & Walsh, 1995; Gent, Hoos, Prosser, & Walsh, 1999).
Our first experiment uses the votes cast by 10 teams of scientists to select one of 32 different
trajectories for NASAs Mariner spacecraft (Dyer & Miles, 1976). Each team ranked the different
trajectories based on their scientific value. We sampled these votes. For elections with 10 or fewer
agents voting, we simply took a random subset of the 10 votes. For elections with more than 10
agents voting, we simply sampled from the 10 votes with uniform frequency. For elections with 32
or fewer candidates, we simply took a random subset of the 32 candidates. Finally for elections
with more than 32 candidates, we duplicated each candidate and assigned them the same ranking.
Since STV works on total orders, we then forced each agent to break any ties randomly. Each
agent broke ties independently of any other agent. New candidates introduced in this way are
clones of the existing candidates. It would be interesting to consider other, perhaps more random
methods for introducing new candidates. Nevertheless, we note that clones are a feature of a
number of real world elections. Indeed, one way to manipulate an election is to introduce clone
candidates for the opposition, and thereby to divide their vote. For example, as motivation for
studying clones, Tideman (1987) describes how he successfully won the vote for class treasurer as
a somewhat precocious 12 year old by nominating the best friend of his main rival. We therefore
believe it may be of interest to consider elections like those generated here in which clones can be
present.
In Figures 11 to 12, we plot the cost to compute if a manipulator can make a random agent
win an STV election as we vary the number of candidates and agents. Votes are sampled from the
NASA experiment as explained earlier. The probability that the manipulator can manipulate the
election resembles the probability curve for uniform random votes. The search needed to compute a
manipulation again appears to increase exponentially with the number of candidates m. However,
the observed scaling is much better than the 1.62m worst-case scaling of the original CSL algorithm.
We can easily compute manipulations for up to 128 candidates.
In our second experiment, we used votes from a faculty hiring committee at the University of
California at Irvine (Dobra, 1983). This dataset had 10 votes for 3 different candidates. We sampled
from this data set in the same ways as from the NASA dataset and observed very similar results.
Results are reported in Figures 13 and 14. As in the previous experiments, it was easy to find a
manipulation or prove that none exists. The observed scaling was again much better than the 1.62m
worst-case scaling of the original CSL algorithm.

6. Veto Rule
We now turn to the manipulation of elections where there is a small, bounded number of candidates,
the votes are weighted and there is a coalition of agents trying to manipulate the result. For this
part of the empirical study, we consider the veto rule. We recall that veto is a scoring rule in
which each agent gets to cast a veto against one candidate. The candidate with the fewest vetoes
wins. As the next theorem shows, simple number partitioning algorithms can be used to compute a
successful manipulation of the veto rule. More precisely, as the following theorem demonstrates, the
manipulation of an election with 3 candidates and weighted votes by a coalition (which is NP-hard
to compute) can be directly reduced to 2-way number partitioning problem. We therefore compute
manipulations in our experiments using an efficient number partitioning algorithm like that proposed
by Korf (1995).

14

fiWhere Are the Hard Manipulation Problems?

1.62**m
n=128
n=64
n=32
n=16
n=8
n=4

1e+14
1e+12

mean nodes

1e+10
1e+08
1e+06
10000
100
1
20

40

60

80

100

120

candidates, m

Figure 11: Search cost to compute if an agent can manipulate an STV election with votes sampled
from the NASA experiment. The number of agents, n is fixed and we vary the number
of candidates, m. The vertical axis measures the mean number of search nodes explored
to compute a manipulation or prove that none exists. The horizontal axis measures the
number of candidates, m. Median and other percentiles are similar.

1e+06

m=128
m=64
m=32
m=16
m=8
m=4

100000

mean nodes

10000

1000

100

10

1
20

40

60
80
agents, n

100

120

Figure 12: Search cost to compute if an agent can manipulate an STV election with votes sampled
from the NASA experiment. The number of candidates, m is fixed and we vary the
number of agents, n. The vertical axis measures the mean number of search nodes
explored to compute a manipulation or prove that none exists. The horizontal axis
measures the number of agents, n. Median and other percentiles are similar.

15

fiWalsh

1.62**m
n=64
n=32
n=16
n=8
n=4

1e+14
1e+12

mean nodes

1e+10
1e+08
1e+06
10000
100
1
20

40

60

80

100

120

candidates, m

Figure 13: Search cost to compute if an agent can manipulate an STV election with votes sampled
from a faculty hiring committee. The number of agents voting, n is fixed and we vary
the number of candidates, m. The vertical axis measures the mean number of search
nodes explored to compute a manipulation or prove that none exists. The horizontal
axis measures the number of candidates, m. Median and other percentiles are similar.

1e+06

m=48
m=24
m=12
m=6
m=3

100000

mean nodes

10000

1000

100

10

1
20

40

60
80
agents, n

100

120

Figure 14: Search cost to compute if an agent can manipulate an STV election with votes sampled
from a faculty hiring committee. The number of candidates, m is fixed and we vary
the number of agents voting, n. The vertical axis measures the mean number of search
nodes explored to compute a manipulation or prove that none exists. The horizontal axis
measures the number of agents, n. Median and other percentiles are similar.

16

fiWhere Are the Hard Manipulation Problems?

Theorem 1 There exists a successful manipulation of an election with 3 candidates by a weighted
coalition using the veto rule if and only if there exists a partitioning of W  {|a P
b|} into two bags
such that the difference between their two sums is less than or equal to a + b  2c + iW i, where W
is the multiset of weights of the manipulating coalition, a, b and c are the weights of vetoes assigned
to the three candidates by the non-manipulators and the manipulators wish the candidate with weight
c to win.
Proof: It never helps a coalition manipulating the veto rule to veto the candidate that they
wish to win. The coalition does, however, need to decide how to divide their vetoes between the
candidates that they wish to lose. P
Consider the case a  b. Suppose the partition has weights
w  /2 and w + /2 where 2w = iW {|ab|} i and  is the difference between the two sums.
The same partition of vetoes is a successful manipulation if and only if the winning candidate
has no more vetoes than the nextPbest candidate. That is,Pc  b + (w  /2). Hence  
2w + 2b  2c = (a P
b) + 2b  2c + iW i = (a + b P
 2c) + 2 iW i. In the other case, a < b and
  (b + a  2c) + iW i. Thus   a + b  2c + iW i. 2
As with the STV rule, we start our analysis with uniform votes. We first consider the case that
the n agents veto uniformly at random one of the 3 possible candidates, and vetoes carry weights
drawn uniformly from (0, w]. When the coalition is small in size, it has too little weight to be able
to change the result. On the other hand, when the coalition is large in size, it is sure to be able to
make a favoured candidate win. There is thus a transition in the manipulability of the problem as
the coalition size increases (see Figure 15).
1

prob(elect chosen candidate)

0.9

n=14^2
n=12^2
n=10^2
n=8^2
n=6^2

0.8
0.7
0.6
0.5
0.4
0.3
0

10

20
30
manipulators, k

40

50

Figure 15: Manipulability of a veto election. The vertical axis measures the probability that a
coalition of k agents elect a chosen candidate in a veto election where n agents have
already voted. The horizontal axis measures the number of manipulators, k. Vetoes are
weighted and weights are uniformly drawn from (0, 28 ]. At k = 0, there is a 1/3rd chance
that the non-manipulators have already elected this candidate.
Based on the work of Procaccia and Rosenschien
(2007a) and of Xia and Conitzer (2008a), we

expect the critical coalition size to increase as n. In Figure16, we see that the phase transition
displays a simple and universal form when plotted against k/ n. The phase transition appears to
be smooth, with the probability varying slowly and not
 approaching a step function as problem
size increases. We obtained a good fit with 1  32 ek/ n . Other smooth phase transitions have
been seen with 2-colouring (Achlioptas, 1999), 1-in-2 satisfiability and Not-All-Equal 2-satisfiability
17

fiWalsh

1

prob(elect chosen candidate)

0.9

n=14^2
n=12^2
n=10^2
n=8^2
n=6^2

0.8
0.7
0.6
0.5
0.4
0.3
0

1

2

3

4

5

k/sqrt(n)

Figure 16: Manipulability of a veto election with rescaled axes. The vertical axis measures the probability that a coalition of k agents can elected a chosen candidate in a veto election where
n agents have already voted. The horizontal axis measures the number of manipulators,
k divided by the square root of the number of agents who have already voted. Vetoes are
weighted andweights are uniformly drawn from (0, 28 ]. Note that the horizontal axis is
scaled by 1/ n compared to the previous figure.

(Achlioptas, Chtcherba, Istrate, & Moore, 2001; Walsh, 2002). It is interesting to note that all these
decision problems are polynomial.
The theoretical results mentioned earlier leave open how hard it is to compute whether a manipulation is possible when the coalition size is critical. Figure 17 displays the computational cost
to find a manipulation (or prove none exists) using Korfs efficient number partitioning algorithm.
Even in the critical region where problems may or may not be manipulable, it is easy to compute
whether the problem is manipulable. All problems are solved in a few branches. This contrasts
with phase transition behaviour in NP-complete problems like propositional satisfiability or in other
complexity classes (Gent & Walsh, 1999; Bailey, Dalmau, & Kolaitis, 2001; Slaney & Walsh, 2002)
where the hardest problems tend to occur around the phase transition.

7. Why Hard Veto Problems Are Rare
Based on our reduction of manipulation problems to number partitioning, we give aheuristic argument why hard manipulation problems become vanishing rare as n ;  and k = ( n). The basic
idea is simple: by the time the coalition is large enough to be able to change the result, the variance
in scores between the candidates is likely to be so large that computing a successful manipulation
or proving none is possible will be easy. Our argument is approximate. For example, we replace discrete sums with continuous integrals, and call upon limiting results like the Central Limit Theorem.
Nevertheless, it provides insight into why manipulations are typically easy to compute.
Suppose that n vetoes voted by the non-manipulators carry weights drawn uniformly from [0, w].
Suppose also that the k manipulators also have weights drawn uniformly from [0, w], that they want
candidates A and B to lose so that C wins, and that they have cast vetoes of weight a, b and c
for A, B and C respectively. Without loss of generality we suppose that a  b. There are three
cases to consider. In the first case, a  c and b  c. It is then easy for the manipulators to make

18

fiWhere Are the Hard Manipulation Problems?

1.05

average branches

1.04

n=14^2
n=12^2
n=10^2
n=8^2
n=6^2

1.03

1.02

1.01

1
0

1

2

3

4

5

k/sqrt(n)

Figure 17: Computational cost for Korfs number partitioning algorithm to decide if a coalition of
k agents can manipulate a veto election where n agents have already voted. Vetoes are
weighted and weights are uniformly drawn from (0, 2k ]. The vertical axis measures the
mean number of branches used by the algorithm to find a manipulation or prove none
exists. As in the previous figure, the horizontal axis measures the number of manipulators, k divided by the square root of the number of agents who have already voted. We
note that all problems are solved with little search. Most took a single branch to solve.
Only a few took 2 or more branches.

C win since C wins whether they veto A or B. In the second case, a  c > b. Again, it is easy
for the manipulators to decide if they can make C win. They all veto B. There is a successful
manipulation if and only if C now wins. In the third case, a < c and b < c. The manipulators
must partition their k vetoes between A and B so that the total vetoes received by A and B exceeds
those for C. Let d be the deficit in weight between A and C and between B and C. That is,
d = (c  a) + (c  b) = 2c  a  b. We can approximate d by the sum of n random variables drawn
uniformly with probability 1/3 from [0, 2w] and with probability 2/3 from [w, 0]. These variables
have mean 0 and variance 2w2 /3. By the Central Limit Theorem, d tends to a normal distribution
with mean 0, and variance t2 = 2nw2 /3. For a manipulation to be possible, d must be less than
s, the sum of the weights of the vetoes of the manipulators. By the Central Limit Theorem, s also
tends to a normal distribution with mean  = kw/2, and variance  2 = 2kw2 /3.
A simple heuristic argument due to (Karmarkar, Karp, Lueker, & Odlyzko, 1986) and also based
on the Central
 Limit Theorem upper bounds the optimal partition difference of k numbers from
[0, w] by O(w k/2k ). In addition, based on the phase transition in number partitioning (Gent &
Walsh, 1998), we expect partitioning problems to be easy unless log2 (w) = (k).
 Combining these
two observations, we expect hard manipulation problems when 0  s  d   k for some constant
. The probability of this occurring is:
Z 
Z x
(x)2
1  y22
1

e 22
e 2t dy dx
 
2
2t
x k
0
By substituting for t,  and , we get:
Z 
Z x
2
(xkw/2)2
1
1

 y2
4nw /3 dy dx
p
p
e 4kw2 /3
e

4kw2 /3
4nw2 /3
0
x k
19

fiWalsh

For n ; , this tends to:
Z 
0

1
p

4kw2 /3

e




 k

(xkw/2)2
4kw2 /3

p

4nw2 /3

e



x2
4nw2 /3

dx

As ez  1 for z > 0, this is upper bounded by:

Z 
(xkw/2)2
 k
1

p
p
e 4kw2 /3 dx
4nw2 /3 0
4kw2 /3

Since the integral is bounded by 1, k = ( n) and log2 (w) = (k), this upper bound varies as:
O( 

1
)
k2k

Thus, we expect hard instances of manipulation problems to be exponentially rare. Since even a
brute force manipulation algorithm takes O(2k ) time in the worst-case, we do not expect the hard
instances to have a significant impact on the average-case as n (and thus k) grows. We stress this
is only a heuristic argument. It makes many assumptions about the complexity of manipulation

problems (in particular that hard instances should lie within the narrow interval 0  s  d   k).
These assumptions are currently only supported by empirical observation and informal argument.
However, the experimental results reported in Figure 17 support these conclusions.

8. Other Distributions of Vetoes
The theoretical analyses of manipulation due to Procaccia and Rosenschein (2007a) and Xia and
Conitzer (2008a) suggest that the probability of an election being manipulable is largely independent
of w, the size of the weights attached to the vetoes. Figure 18 demonstrates that this indeed appears
to be the case in practice. When weights are varied in size from 28 to 216 , the probability does not
appear to change. In fact, the probability curve fits the same simple and universal form plotted
in Figure 16. We also observed that the cost of computing a manipulation or proving that none is
possible did not change as the weights were varied in size.
Similarly, theoretical results typically place few assumptions about the distribution of votes.
For example, the results of Procaccia and Rosenschein
 (2007a) and Xia and Conitzer (2008a) that
there is a critical coalition size that increases as ( n) hold for any independent and identically
distributed random votes. Similarly, our heuristic argument about why hard manipulation problems
are vanishingly rare depends on application of the Central Limit Theorem. It therefore works with
other types of independent and identically distributed random votes.
We considered therefore another type of independent and identically distributed vote. In particular, we study an election in which weights are independently drawn from a normal distribution.
Figure 19 shows that there is again a smooth phase transition in manipulability. We also plotted
Figure 19 on top of Figures 16 and 18. All curves appear to fit the same simple and universal form.
As with uniform weights, the computational cost of deciding if an election is manipulable was small
even when the coalition size was critical. Finally, we varied the parameters of the normal distribution. The probability of electing a chosen candidate as well as the cost of computing a manipulation
did not appear to depend on the mean or variance of the distribution. We do not reproduce the
figures as they look identical to the previous figures.

9. Correlated Vetoes
We conjecture that one place to find hard manipulation problems for veto voting is when votes are
highly correlated. For example, consider a hung election where all n agents veto the candidate
20

fiWhere Are the Hard Manipulation Problems?

1

prob(elect chosen candidate)

0.9

log2(w)=16
log2(w)=14
log2(w)=12
log2(w)=10
log2(w)=8

0.8
0.7
0.6
0.5
0.4
0.3
0

1

2

3

4

5

k/sqrt(n)

Figure 18: Independence of the size of the weights and the manipulability of a veto election. The
vertical axis measures the probability that a coalition of k agents can elect a chosen
candidate where n agents have already voted. As in the previous figure, the horizontal
axis measures the number of manipulators, k divided by the square root of the number
of agents who have already voted. Vetoes are weighted and weights are uniformly drawn
from (0, w].

1

prob(elect chosen candidate)

0.9

n=14^2
n=12^2
n=10^2
n=8^2
n=6^2

0.8
0.7
0.6
0.5
0.4
0.3

0

1

2

3

4

5

k/sqrt(n)

Figure 19: Manipulability of a veto election with weighted votes taken from a normal distribution.
The vertical axis measure the probability that a coalition of k agents can elect a chosen
candidate in a veto election where n agents have already voted. As in the previous figure,
the horizontal axis measures the number of manipulators, k divided by the square root
of the number of agents who have already voted. Vetoes are weighted and drawn from a
normal distribution with mean 28 and standard deviation 27 .

21

fiWalsh

prob(elect chosen candidate)

1
m=24
m=18
m=12
m=6

0.8

0.6

0.4

0.2

0
0

0.5

1
log2(w)/k

1.5

2

Figure 20: Manipulability of a veto election where votes are highly correlated and the result is
hung. Vetoes of the manipulators are weighted and weights are uniformly drawn from
(0, w], the other agents have all vetoed the candidate that the manipulators wish to win,
and the sum of the weights of the manipulators is twice that of the non-manipulators.
The vertical axis measures the probability that a coalition of k agents can elect a chosen
candidate. The horizontal axis measures log2 (w)/k.

100000
m=24
m=18
m=12
m=6

average branches

10000

1000

100

10

1

0

0.5

1

1.5

2

log2(w)/k

Figure 21: The search cost to decide if a hung veto election can be manipulated. Vetoes of
the manipulators are weighted and weights are uniformly drawn from (0, w], the other
agents have all vetoed the candidate that the manipulators wish to win, and the sum
of the weights of the manipulators is twice that of the non-manipulators. The vertical
axis measures the mean number of branches explored by Korfs algorithm to decide if
a coalition of k agents can manipulate a veto election. The horizontal axis measures
log2 (w)/k.

22

fiWhere Are the Hard Manipulation Problems?

that the manipulators wish to win, but the k manipulators have exactly twice the weight of vetoes
of the n agents. This election is finely balanced. The preferred candidate of the manipulators wins
if and only if the manipulators perfectly partition their vetoes between the two candidates that
they wish to lose. Note that this is precisely the trick used in reducing number partitioning to the
manipulation problem by Conitzer et al. (2007). In Figure 20, we plot the probability that the k
manipulators can make their preferred candidate win in such a hung election as we vary the size
of their weights w. Similar to number partitioning (Gent & Walsh, 1998), we see a rapid transition
in manipulability around log2 (w)/k  1. In Figure 21, we observe that there is a rapid increase in
the computationally complexity to compute a manipulation around this point.
What happens when the votes are less correlated? We consider an election which is perfectly
hung as before except for one agent who vetoes at random one of the three candidates. In Figure 22,
we plot the cost of computing a manipulation as the weight of this single random veto increases.
Even one uncorrelated vote is enough to make manipulation easy if it has the same magnitude in
weight as the vetoes of the manipulators. This suggests that we will find hard manipulation problems
in veto elections only when votes are highly correlated.
100000
m=24
m=18
m=12
m=6

average branches

10000

1000

100

10

1

0

0.2

0.4
0.6
log2(w)/log2(w)

0.8

1

Figure 22: The impact of one random agent on the manipulability of a hung veto election. Vetoes
of the manipulators are weighted and weights are uniformly drawn from (0, w], the nonmanipulating agents have all vetoed the candidate that the manipulators wish to win,
and the sum of the weights of the manipulators is twice that of the non-manipulators
except for one random non-manipulating agent whose weight is uniformly drawn from
(0, w0 ]. The vertical axis measures the mean number of search branches explored by
Korfs algorithm to decide if a coalition of k agents can manipulate a veto election. The
horizontal axis measures log2 (w0 )/ log2 (w). When the veto of the one random agent has
the same weight as the other agents, it is computationally easy to decide if the election
can be manipulated.

10. Related Work
As indicated earlier, a number of theoretical results suggest elections are easy to manipulate in
practice despite worst case NP-hardness results. For example, Procaccia and Rosenschein proved
that for most scoring rules and a wide variety of distributions over votes, when the size of the

23

fiWalsh


coalition
is o( n), the probability that they can change the result tends to 0, and when it is

( n), the probability that they can manipulate the result tends to 1 (Procaccia & Rosenschein,
2007a). They also gave a simple greedy procedure that will find a manipulation of a scoring rule
in polynomial time with a probability of failure that is an inverse polynomial in n (Procaccia &
Rosenschein, 2007b). However, we should treat this result with caution as the junta distributions
used in this work may be of limited usefulness (Erdelyi, Hemaspaandra, Rothe, & Spakowski, 2009).
As a second example, Xia and Conitzer have shown that for a large class of voting rules including
STV, as the number of agents grows, either the probability that a coalition can manipulate the result
is very small (as the coalition is too small), or the probability that they can easily manipulate the
result to make any alternative win is very large (Xia & Conitzer, 2008a). They left open only a
small interval in the size for the coalition for which the coalition is large enough to manipulate but
not obviously large enough to manipulate the result easily.
Friedgut, Kalai and Nisan proved that if the voting rule is neutral and far from dictatorial and
there are 3 candidates then there exists an agent for whom a random manipulation succeeds with
probability ( n1 ) (Friedgut et al., 2008). They were, however, unable to extend their proof to four
(or more) candidates. More recently, Isaksson, Kindler and Mossel proved a similar result for 4
or more candidates using geometric arguments (Isaksson, Kindler, & Mossel, 2010). Starting from
different assumptions again, Xia and Conitzer showed that a random manipulation would succeed
with probability ( n1 ) for 3 or more candidates for STV, for 4 or more candidates for any scoring
rule and for 5 or more candidates for Copeland (Xia & Conitzer, 2008b).
As discussed earlier, Coleman and Teague proposed some algorithms to compute manipulations
for the STV rule (Coleman & Teague, 2007). They also conducted an empirical study which demonstrated that only relatively small coalitions are needed to change the elimination order of the STV
rule. They observed that most uniform and random elections are not trivially manipulable using a
simple greedy heuristic. On the other hand, our results suggest that, for manipulation by a single
agent, often only a small amount of backtracking is needed to find a manipulation or prove that
none exists.

11. Conclusions
We have studied empirically whether computational complexity is a barrier to the manipulation for
the STV and veto rules when the manipulating agents have complete information about the other
votes. We have looked at a number of different distributions of votes including uniform random
votes, correlated votes drawn from an urn model, and votes sampled from some real world elections.
We have looked at manipulation by a single unweighted agent in the case of STV, and by a coalition
of weighted agents in the case of veto voting. In many of the elections in our experiments, it was easy
to compute a manipulation or to prove no manipulation was possible. The situations we identified
where manipulations were computationally difficult to find depended either on the election having
hundreds of candidates or on the election being very tightly hung. These results increase the concern
that computational complexity may not be a significant barrier to manipulation in practice.
What other lessons can be learnt from this study? First, whilst we have focused on the STV and
veto rules, similar behavior is likely with other voting rules. It would, for instance, be interesting to
study the Borda rule as this is one of the few rules used in practice where computing a manipulation
is NP-hard with unweighted votes (Davies et al., 2011; Betzler et al., 2011). It would also be
interesting to study voting rules like Copeland, maximin and ranked pairs. These rules are members
of the small set of voting rules which are NP-hard to manipulate without weights on the votes
(Xia, Zuckerman, Procaccia, Conitzer, & Rosenschein, 2009). Second, there may be a connection
between the smoothness of the phase transition and problem hardness. Sharp phase transitions
like that for propositional satisfiability are associated with hard decision problems, whilst smooth
transitions are associated with easy instances of NP-hard problems and with polynomial problems
like 2-colourability. The phase transitions observed here appear to be smooth. Third, given the
24

fiWhere Are the Hard Manipulation Problems?

insights provided by empirical studies, it would be interesting to consider similar studies for related
problems. For example, is computational complexity an issue in preference elicitation (Conitzer &
Sandholm, 2002b; Walsh, 2008; Pini, Rossi, Venable, & Walsh, 2008)? Fourth, we have assumed that
the manipulators have complete information about the votes of the other agents. It is an interesting
future direction to determine if uncertainty in how agents have voted adds to the computational
complexity of manipulation in practice (Conitzer & Sandholm, 2002a; Walsh, 2007; Lang, Pini,
Rossi, Venable, & Walsh, 2007).

Acknowledgments
NICTA is funded by the Australian Government through the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre
of Excellence program. Some of the results in this paper appeared in two earlier conference papers
(Walsh, 2009, 2010).

References
Achlioptas, D. (1999). Threshold phenomena in random graph colouring and satisfiability. Ph.D.
thesis, Department of Computer Science, University of Toronto.
Achlioptas, D., Chtcherba, A., Istrate, G., & Moore, C. (2001). The phase transition in 1-in-k
SAT and NAE SAT. In Proceedings of the 12th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA01), pp. 719720. Society for Industrial and Applied Mathematics.
Bailey, D., Dalmau, V., & Kolaitis, P. (2001). Phase transitions of PP-complete satisfiability problems. In Nebel, B. (Ed.), Proceedings of 17th International Joint Conference on Artificial
Intelligence (IJCAI 2001), pp. 183189. International Joint Conference on Artificial Intelligence, Morgan Kaufmann.
Barbera, S., Berga, D., & Moreno, B. (2009). Single-dipped preferences. UFAE and IAE working papers 801.09, Unitat de Fonaments de lAnlisi Econmica (UAB) and Institut dAnlisi Econmica
(CSIC).
Bartholdi, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social Choice and
Welfare, 8 (4), 341354.
Bartholdi, J., Tovey, C., & Trick, M. (1989). The computational difficulty of manipulating an
election. Social Choice and Welfare, 6 (3), 227241.
Berg, S. (1985). Paradox of voting under an urn model: the effect of homogeneity. Public Choice,
47, 377387.
Betzler, N., Niedermeier, R., & Woeginger, G. (2011). Unweighted coalitional manipulation under
the Borda rule is NP-hard. In Walsh, T. (Ed.), Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI 2011). International Joint Conference on Artificial
Intelligence.
Black, D. (1948). On the rationale of group decision-making. Journal of Political Economy, 56 (1),
2334.
Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. (2010). Bypassing combinatorial
protections: Polynomial-time algorithms for single-peaked electorates. In Fox, M., & Poole,
D. (Eds.), Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI 2010).
AAAI Press.
Chamberlin, J. (1985). An investigation into the relative manipulability of four voting systems.
Behavioral Science, 30, 195203.
25

fiWalsh

Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are. In Mylopoulos, J., & Reiter, R. (Eds.), Proceedings of the 12th International Joint Conference on
Artificial Intelligence (IJCAI 1991), pp. 331337. International Joint Conference on Artificial
Intelligence.
Coleman, T., & Teague, V. (2007). On the complexity of manipulating elections. In Gudmundsson,
J., & Jay, B. (Eds.), Proceedings of the 13th Australasian Symposium on Theory of Computing
(CATS 07), pp. 2533. Australian Computer Society, Inc.
Conitzer, V. (2007). Eliciting single-peaked preferences using comparison queries. In Durfee, E.,
Yokoo, M., Huhns, M., & Shehory, O. (Eds.), Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2007), pp. 408415. IFAAMAS.
Conitzer, V. (2009). Eliciting single-peaked preferences using comparison queries. Journal of Artificial Intelligence Research, 35, 161191.
Conitzer, V., & Sandholm, T. (2002a). Complexity of manipulating elections with few candidates.
In Dechter, R., Kearns, M., & Sutton, R. (Eds.), Proceedings of the 18th National Conference
on Artificial Intelligence (AAAI 2002), pp. 314319. Association for Advancement of Artificial
Intelligence.
Conitzer, V., & Sandholm, T. (2002b). Vote elicitation: Complexity and strategy-proofness. In
Dechter, R., Kearns, M., & Sutton, R. (Eds.), Proceedings of the 18th National Conference on
Artificial Intelligence (AAAI 2002), pp. 392397. Association for Advancement of Artificial
Intelligence.
Conitzer, V., & Sandholm, T. (2006). Nonexistence of voting rules that are usually hard to manipulate. In Gil, Y., & Mooney, R. (Eds.), Proceedings of the 21st National Conference on Artifical
Intelligence (AAAI 2006), pp. 627634. Association for Advancement of Artificial Intelligence.
Conitzer, V., Sandholm, T., & Lang, J. (2007). When are elections with few candidates hard to
manipulate?. Journal of the Association for Computing Machinery, 54 (3). Article 14 (33
pages).
Davies, J., Katsirelos, G., Narodytska, N., & Walsh, T. (2011). Complexity of and algorithms for
Borda manipulation. In Burgard, W., & Roth, D. (Eds.), Proceedings of the Twenty-Fifth
AAAI Conference on Artificial Intelligence (AAAI 2011). AAAI Press.
Dobra, J. (1983). An approach to empirical studies of voting paradoxes: An update and extension..
Public Choice, 41, 241250.
Dubois, O., Monasson, R., Selman, B., & Zecchina, R. (2001). Special issue: Phase transitions in
combinatorial problems. Theoretical Computer Science, 265 (12), 1306.
Dyer, J., & Miles, R. (1976). An actual application of collective choice theory to the selection of
trajectories for the Mariner Jupiter/Saturn 1977 project. Operations Research, 24 (2), 220244.
Erdelyi, G., Hemaspaandra, L., Rothe, J., & Spakowski, H. (2009). Generalized Juntas and NP-hard
sets. Theoretical Computer Science, 410 (38-40), 39954000.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity to protect elections. Communications of the ACM, 53 (11), 7482.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). The shield that never
was: societies with single-peaked preferences are more open to manipulation and control. In
Heifetz, A. (Ed.), Proceedings of the 12th Conference on Theoretical Aspects of Rationality and
Knowledge (TARK-2009), pp. 118127.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: ties matter. In Padgham,
L., Parkes, D., Muller, J., & Parsons, S. (Eds.), 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2008), pp. 983990.

26

fiWhere Are the Hard Manipulation Problems?

Faliszewski, P., & Procaccia, A. (2010). AIs war on manipulation: Are we winning?. AI Magazine,
31 (4), 5364.
Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections can be manipulated often. In Proceedings
of the 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2008), pp.
243249. IEEE Computer Society Press.
Gent, I., Hoos, H., Prosser, P., & Walsh, T. (1999). Morphing: Combining structure and randomness.
In Hendler, J., & Subramanian, D. (Eds.), Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI 1999), pp. 654660. Association for Advancement of Artificial
Intelligence.
Gent, I., MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (2001). Random constraint satisfaction:
Flaws and structure. Constraints, 6 (4), 345372.
Gent, I., MacIntyre, E., Prosser, P., & Walsh, T. (1995). Scaling effects in the CSP phase transition. In Montanari, U., & Rossi, F. (Eds.), Proceedings of the 1st International Conference
on Principles and Practices of Constraint Programming (CP-95), Vol. 976 of Lecture Notes in
Computer Science, pp. 7087. Springer-Verlag.
Gent, I., & Walsh, T. (1994). The SAT phase transition. In Cohn, A. (Ed.), Proceedings of the 11th
European Conference on Artificial Intelligence (ECAI-94), pp. 105109. John Wiley & Sons.
Gent, I., & Walsh, T. (1995). Phase transitions from real computational problems. In Proceedings
of the 8th International Symposium on Artificial Intelligence: Intelligent Systems Applications
in Industry and Business, pp. 356364.
Gent, I., & Walsh, T. (1996a). Phase transitions and annealed theories: Number partitioning as
a case study. In Wahlster, W. (Ed.), Proc. of the 12th European Conference on Artificial
Intelligence (ECAI-96), pp. 170174. John Wiley and Sons, Chichester.
Gent, I., & Walsh, T. (1996b). The satisfiability constraint gap. Artificial Intelligence, 81 (12),
5980.
Gent, I., & Walsh, T. (1996c). The TSP phase transition. Artificial Intelligence, 88, 349358.
Gent, I., & Walsh, T. (1998). Analysis of heuristics for number partitioning. Computational Intelligence, 14 (3), 430451.
Gent, I., & Walsh, T. (1999). Beyond NP: the QSAT phase transition. In Hendler, J., & Subramanian,
D. (Eds.), Proceedings of the 16th National Conference on AI, pp. 648653. Association for
Advancement of Artificial Intelligence.
Gibbard, A. (1973). Manipulation of voting schemes: A general result. Econometrica, 41, 587601.
Gomes, G., & Walsh, T. (2006). Randomness and structure. In Rossi, F., van Beek, P., & Walsh,
T. (Eds.), Handbook of Constraint Programming, Foundations of Artificial Intelligence, pp.
639664. Elsevier.
Hartmann, A., & Weigt, M. (2005). Phase Transitions in Combinatorial Optimization Problems:
Basics, Algorithms and Statistical Mechanics. Wiley-VCH, Weinheim.
Isaksson, M., Kindler, G., & Mossel, E. (2010). The geometry of manipulation: A quantitative proof
of the Gibbard-Satterthwaite theorem. In 51th Annual IEEE Symposium on Foundations of
Computer Science (FOCS 2010), pp. 319328. IEEE Computer Society.
Karmarkar, N., Karp, R., Lueker, J., & Odlyzko, A. (1986). Probabilistic analysis of optimum
partitioning. Journal of Applied Probability, 23, 626645.
Korf, R. (1995). From approximate to optimal solutions: A case study of number partitioning. In
Mellish, C. S. (Ed.), Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI 1995), pp. 266272. International Joint Conference on Artificial Intelligence.

27

fiWalsh

Lang, J., Pini, M., Rossi, F., Venable, B., & Walsh, T. (2007). Winner determination in sequential
majority voting. In Veloso, M. M. (Ed.), Proceedings of the 20th International Joint Conference on Artificial Itelligence (IJCAI-2007), pp. 13721377. International Joint Conference on
Artificial Intelligence.
McCabe-Dansted, J., & Slinko, A. (2006). Exploratory analysis of similarities between social choice
rules. Group Decision and Negotiation, 15, 77107.
Mertens, S. (2001). A physicists approach to number partitioning. Theoretical Computer Science,
265 (1-2), 79108.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and Easy Distributions of SAT Problems. In
Proceedings of the 10th National Conference on AI, pp. 459465. Association for Advancement
of Artificial Intelligence.
Pini, M., Rossi, F., Venable, K., & Walsh, T. (2008). Dealing with incomplete agents preferences
and an uncertain agenda in group decision making via sequential majority voting. In Brewka,
G., & Lang, J. (Eds.), Principles of Knowledge Representation and Reasoning: Proceedings of
the Eleventh International Conference (KR 2008), pp. 571578. AAAI Press.
Procaccia, A. D., & Rosenschein, J. S. (2007a). Average-case tractability of manipulation in voting
via the fraction of manipulators. In Durfee, E. H., Yokoo, M., Huhns, M. N., & Shehory, O.
(Eds.), Proceedings of 6th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS-07), pp. 718720. IFAAMAS.
Procaccia, A. D., & Rosenschein, J. S. (2007b). Junta distributions and the average-case complexity
of manipulating elections. Journal of Artificial Intelligence Research, 28, 157181.
Prosser, P. (1994). Binary constraint satisfaction problems: Some are harder than others. In Cohn,
A. G. (Ed.), Proceedings of the 11th European Conference on Artificial Intelligence, pp. 9599.
European Conference on Artificial Intelligence, John Wiley and Sons.
Satterthwaite, M. (1975). Strategy-proofness and Arrows conditions: Existence and correspondence
theorems for voting procedures and social welfare functions. Journal of Economic Theory, 10,
187216.
Slaney, J., & Walsh, T. (2002). Phase transition behavior: from decision to optimization. In Proceedings of the 5th International Symposium on the Theory and Applications of Satisfiability
Testing, SAT 2002.
Slinko, A., & White, S. (2008). Non- dictatorial social choice rules are safely manipulable. In
Goldberg, U. E. . P. W. (Ed.), Proceedings of 2nd International Workshop on Computational
Social Choice (COMSOC08), pp. 403413.
Smith, B. (1994). The phase transition in constraint satisfaction problems: A closer look at the
mushy region. In Cohn, A. G. (Ed.), Proceedings of the 11th European Conference on Artificial
Intelligence, pp. 100104. European Conference on Artificial Intelligence, John Wiley and Sons.
Tideman, T. (1987). Independence of clones as a criterion for voting rules. Social Choice and
Welfare, 4, 185206.
Walsh, T. (1998). The constrainedness knife-edge. In Mostow, J., & Rich, C. (Eds.), Proceedings of
the 15th National Conference on AI, pp. 406411. Association for Advancement of Artificial
Intelligence.
Walsh, T. (1999). Search in a small world. In Dean, T. (Ed.), Proceedings of the 16th International
Joint Conference on Artificial Itelligence (IJCAI-99), pp. 11721177. International Joint Conference on Artificial Intelligence, Morgan Kaufmann.
Walsh, T. (2001). Search on high degree graphs. In Nebel, B. (Ed.), Proceedings of the 17th International Joint Conference on Artificial Itelligence (IJCAI-2001), pp. 266274. International
Joint Conference on Artificial Intelligence, Morgan Kaufmann.
28

fiWhere Are the Hard Manipulation Problems?

Walsh, T. (2002). From P to NP: COL, XOR, NAE, 1-in-k, and Horn SAT. In Dechter, R., Kearns,
M., & Sutton, R. (Eds.), Proceedings of the 17th National Conference on AI (AAAI 2002), pp.
695700. Association for Advancement of Artificial Intelligence.
Walsh, T. (2007). Uncertainty in preference elicitation and aggregation. In Proceedings of the 22nd
National Conference on AI, pp. 38. Association for Advancement of Artificial Intelligence.
Walsh, T. (2008). Complexity of terminating preference elicitation. In Padgham, L., Parkes, D. C.,
Muller, J. P., & Parsons, S. (Eds.), 7th International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS 2008), pp. 967974. IFAAMAS.
Walsh, T. (2009). Where are the really hard manipulation problems? The phase transition in
manipulating the veto rule. In Boutilier, C. (Ed.), Proceedings of the 21st International Joint
Conference on Artificial Itelligence (IJCAI-2009), pp. 324329. International Joint Conference
on Artificial Intelligence.
Walsh, T. (2010). An empirical study of the manipulability of single transferable voting. In Coelho,
H., Studer, R., & Wooldridge, M. (Eds.), Proc. of the 19th European Conference on Artificial
Intelligence (ECAI-2010), Vol. 215 of Frontiers in Artificial Intelligence and Applications, pp.
257262. IOS Press.
Xia, L., & Conitzer, V. (2008a). Generalized scoring rules and the frequency of coalitional manipulability. In Fortnow, L., Riedl, J., & Sandholm, T. (Eds.), EC 08: Proceedings of the 9th ACM
conference on Electronic Commerce, pp. 109118. ACM.
Xia, L., & Conitzer, V. (2008b). A sufficient condition for voting rules to be frequently manipulable.
In Fortnow, L., Riedl, J., & Sandholm, T. (Eds.), Proceedings of the 9th ACM conference on
Electronic Commerce (EC 08), pp. 99108. ACM.
Xia, L., Zuckerman, M., Procaccia, A., Conitzer, V., & Rosenschein, J. (2009). Complexity of
unweighted coalitional manipulation under some common voting rules. In Boutilier, C. (Ed.),
Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009),
pp. 348353. International Joint Conference on Artificial Intelligence.
Zhang, W., & Korf, R. (1996). A study of complexity transitions on the asymmetic traveling salesman
problem. Artificial Intelligence, 81 (1-2), 223239.

29

fiJournal of Artificial Intelligence Research 42 (2011) 719-764

Submitted 03/11; published 12/11

Defeasible Inclusions in Low-Complexity DLs
Piero A. Bonatti
Marco Faella
Luigi Sauro

BONATTI @ NA . INFN . IT
MFAELLA @ NA . INFN . IT
SAURO @ NA . INFN . IT

Dipartimento di Scienze Fisiche,
Universita di Napoli Federico II

Abstract
Some of the applications of OWL and RDF (e.g. biomedical knowledge representation and
semantic policy formulation) call for extensions of these languages with nonmonotonic constructs
such as inheritance with overriding. Nonmonotonic description logics have been studied for many
years, however no practical such knowledge representation languages exist, due to a combination of
semantic difficulties and high computational complexity. Independently, low-complexity description logics such as DL-lite and EL have been introduced and incorporated in the OWL standard.
Therefore, it is interesting to see whether the syntactic restrictions characterizing DL-lite and EL
bring computational benefits to their nonmonotonic versions, too. In this paper we extensively investigate the computational complexity of Circumscription when knowledge bases are formulated
in DL-liteR , EL, and fragments thereof. We identify fragments whose complexity ranges from P
to the second level of the polynomial hierarchy, as well as fragments whose complexity raises to
PSPACE and beyond.

1. Introduction
The ontologies at the core of the semantic web  as well as ontology languages such as RDF, OWL,
and related Description Logics (DLs)  are founded on fragments of first-order logic and inherit
strengths and weaknesses of this well-established formalism. Limitations include monotonicity, and
the consequent inability to design knowledge bases (KBs) by describing prototypes whose general
properties can be later refined with suitable exceptions. This natural, iterative approach is commonly used by biologists and calls for an extension of DLs with defeasible inheritance with overriding (a mechanism normally supported by object-oriented languages). Some workarounds have been
devised for particular cases; however, no general solutions are currently available (Rector, 2004;
Stevens, Aranguren, Wolstencroft, Sattler, Drummond, Horridge, & Rector, 2007). Another motivation for nonmonotonic DLs stems from the recent development of policy languages based on DLs
(Uszok, Bradshaw, Jeffers, Suri, Hayes, Breedy, Bunch, Johnson, Kulkarni, & Lott, 2003; Finin,
Joshi, Kagal, Niu, Sandhu, Winsborough, & Thuraisingham, 2008; Zhang, Artale, Giunchiglia, &
Crispo, 2009; Kolovski, Hendler, & Parsia, 2007). DLs nicely capture role-based policies and facilitate the integration of semantic web policy enforcement with reasoning about semantic metadata
(which is typically necessary in order to check policy conditions). However, in order to formulate
standard default policies such as open and closed policies,1 and support common policy language
features such as authorization inheritance with exceptions (which is meant to facilitate incremental
1. If no explicit authorization has been specified for a given access request, then an open policy permits the access while
a closed policy denies it.
c
2011
AI Access Foundation. All rights reserved.

fiB ONATTI , FAELLA , & S AURO

policy formulation), it is necessary to adopt a nonmonotonic semantics; Bonatti and Samarati (2003)
provide further details on the matter.
Given the increasing size of semantic web ontologies and RDF bases, the complexity of reasoning is an influential factor that may either foster or prevent the adoption of a knowledge representation language. Accordingly, OWL2 introduces profiles that adopt syntactic restrictions (compatible
with application requirements) in order to make reasoning tractable. Two of such profiles are based
on the following families of DLs: DL-lite (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati,
2005), that formalizes RDFS, and EL (Baader, 2003; Baader, Brandt, & Lutz, 2005), that extensively covers important biomedical ontologies such as GALEN and SNOMED. Unfortunately, in
general, nonmonotonic DL reasoning can be highly complex and reach NExpTimeNP and even 3ExpTime (Donini, Nardi, & Rosati, 1997, 2002; Bonatti, Lutz, & Wolter, 2009). A natural question,
in this context, is whether restrictions such as those adopted by DL-lite and EL help in reducing the
complexity of nonmonotonic DL reasoning, too.
Answering this question is the main goal of this paper. We extensively investigate the complexity of reasoning in DL-lite and EL. The nonmonotonic semantics adopted is Circumscription
(McCarthy, 1980), whose main appealing properties (discriminating Circumscription from other
nonmonotonic DL semantics proposed in the literature) are summarized below:
1. Circumscription is compatible with all the interpretation domains supported by classical DLs;
there is no need for adopting a fixed domain of standard names;
2. In circumscribed DLs, nonmonotonic inferences apply to all individuals, including those that
are not denoted by any constants and are implicitly asserted by existential quantifiers;
3. Circumscription naturally supports priorities among conflicting nonmonotonic axioms and
can easily simulate specificity-based overriding.
As an attempt to simplify the usage of circumscribed DLs and simultaneously remove potential
sources of computational complexity, we do not support the usage of abnormality predicates (McCarthy, 1986) in their full generality; we rather hide them within defeasible inclusions (Bonatti,
Faella, & Sauro, 2009). Defeasible inclusions are expressions C vn D whose intuitive meaning
is: an instance of C is normally an instance of D. Such inclusions can be prioritized to resolve
conflicts. Priorities can be either explicit or automatically determined by the inclusions specificity,
i.e. a defeasible inclusion C1 vn D1 may override C2 vn D2 if C1 is classically subsumed by
C2 . In this framework, we prove that restricting the syntax to DL-lite inclusions sufficesin almost
all casesto reduce complexity to the second level of the polynomial hierarchy. On the contrary,
circumscribed EL is still ExpTime-hard and further restrictions are needed to confine complexity within the second level of the polynomial hierarchy. Syntactic restrictions will be analyzed
in conjunction with other semantic parameters, such as the kind of priorities adopted (explicit or
specificity-based), and which predicates may or may not be affected by Circumscription (i.e., fixed
and variable predicates, in Circumscriptions jargon).
The paper is organized as follows: First, the basics of low-complexity description logics and
their extension based on circumscription are recalled in Section 2 and Section 3, respectively. Then,
some reductions that can be used to eliminate language features and work on simpler frameworks
are illustrated in Section 4. After an undecidability result caused by fixed roles (Section 5), the paper
focuses on variable roles: The complexity of circumscribed DL-liteR and EL/EL are investigated
720

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Name

Syntax

inverse role

R

nominal
negation
conjunction
existential
restriction
top
bottom

{a}
C
C uD
R.C
>


Semantics
I

(R ) = {(d, e) | (e, d)  RI }
{aI }
I \ C I
C I  DI
{d  I | (d, e)  RI : e  C I }
>I = I
I = 

Figure 1: Syntax and semantics of some DL constructs.

in Section 6 and Section 7, respectively. A section on related work and a final discussion conclude
the paper.

2. Preliminaries
In DLs, concepts are inductively defined with a set of constructors, starting with a set NC of concept
names, a set NR of role names, and (possibly) a set NI of individual names (all countably infinite).
We use the term predicates to refer to elements of NC  NR . Hereafter, letters A and B will range
over NC , P will range over NR , and a, b, c will range over NI . The concepts of the DLs dealt with in
this paper are formed using the constructors shown in Figure 1. There, the inverse role constructor
is the only role constructor, whereas the remaining constructors are concept constructors. Letters
C, D will range over concepts and letters R, S over (possibly inverse) roles.
The semantics of the above concepts is defined in terms of interpretations I = (I , I ). The
domain I is a non-empty set of individuals and the interpretation function I maps each concept
name A  NC to a set AI  I , each role name P  NR to a binary relation P I on I , and each
individual name a  NI to an individual aI  I . The extension of I to inverse roles and arbitrary
concepts is inductively defined as shown in the third column of Figure 1. An interpretation I is
called a model of a concept C if C I 6= . If I is a model of C, we also say that C is satisfied by I.
A (strong) knowledge base is a finite set of (i) concept inclusions (CIs) C v D where C and
D are concepts, (ii) concept assertions A(a) and role assertions P (a, b), where a, b are individual
names, P  NR , and A  NC , (iii) role inclusions (RIs) R v R0 . An interpretation I satisfies (i) a CI
C v D if C I  DI , (ii) an assertion C(a) if aI  C I , (iii) an assertion P (a, b) if (aI , bI )  P I ,
and (iv) a RI R v R0 iff RI  R0 I . Then, I is a model of a strong knowledge base S iff I satisfies
all the elements of S. We write C vS D iff for all models I of S, I satisfies C v D.
Terminologies are particular strong knowledge bases consisting of definitions, i.e. axioms such
as A  C, that abbreviate the inclusions A v C and C v A. If a terminology T contains the above
definition, then we say that A is defined in T and that C is the definition of A. Each A defined in
T must have a unique definition. A concept name A directly depends on B (in T ) if B occurs in
As definition; moreover, A depends on B (in T ) if there is a chain of such direct dependencies
leading from A to B. A terminology T is acyclic if no A depends on itself in T . Terminologies
are conservative extensions, and the concept names defined in an acyclic terminology T can be
721

fiB ONATTI , FAELLA , & S AURO

eliminated by unfolding them w.r.t. T , that is, by exhaustively replacing the concepts defined in T
with their definition. For all expressions (i.e., concepts or inclusions) E, we denote with unf(E, T )
the unfolding of E w.r.t. T .
The logic DL-liteR (Calvanese et al., 2005) restricts concept inclusions to expressions CL v
CR , where
CL ::= A | R

R ::= P | P 

CR ::= CL | CL

(as usual, R abbreviates R.>).
The logic EL (Baader, 2003; Baader et al., 2005) restricts knowledge bases to assertions and
concept inclusions built from the following constructs:
C ::= A | > | C1 u C2 | P.C
(note that inverse roles are not supported). The extension of EL with , role hierarchies, and
nominals (respectively) are denoted by EL , ELH, and ELO. Combinations are allowed: for
example ELHO denotes the extension of EL supporting role hierarchies and nominals. Finally,
ELA denotes the extension where negation can be applied to concept names.

3. Defeasible Knowledge
A general defeasible inclusion (GDI) is an expression C vn D whose intended meaning is: Cs
elements are normally in D.
Example 3.1 (Bonatti et al., 2009) The sentences: in humans, the heart is usually located on the
left-hand side of the body; in humans with situs inversus, the heart is located on the right-hand side
of the body (Rector, 2004; Stevens et al., 2007) can be formalized with the EL axioms and GDIs:
Human vn has heart.has position.Left ;
Situs Inversus v has heart.has position.Right ;
has heart.has position.Left u
has heart.has position.Right v  .

2

A defeasible knowledge base (DKB) in a logic DL is a pair hK, i, where K = KS  KD , KS is
a strong DL KB, KD is a set of GDIs C vn D such that C v D is a DL inclusion, and  is a strict
partial order (a priority relation) over KD . In the following, by C v[n] D we denote an inclusion that
is either classical or defeasible. Moreover, for a DKB KB = hK  T , i, where T is a (classical)
acyclic terminology, we denote by unf(KB) = hK0 , 0 i the DKB where K0 is the unfolding of all
inclusions in K w.r.t. T , and, for all DIs ,  0 in K, the relation unf(, T ) 0 unf( 0 , T ) holds if and
only if    0 .
As priority relation we shall often adopt the specificity relation K which is determined by
classically valid inclusions. Formally, for all GDIs 1 = (C1 vn D1 ) and 2 = (C2 vn D2 ), let
1 K 2 iff C1 vKS C2 and C2 6vKS C1 .
722

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Example 3.2 The access control policy: Normally users cannot read project files; staff can read
project files; blacklisted staff is not granted any access can be encoded with:
Staff v User
Blacklisted v Staff
UserRequest v subj.User u target.Proj u op.Read
StaffRequest v subj.Staff u target.Proj u op.Read
UserRequest vn decision.{Deny}
StaffRequest vn decision.{Grant}
subj.Blacklisted v decision.{Deny}
decision.{Grant} u decision.{Deny} v  .
Staff members cannot simultaneously satisfy the two defeasible inclusions (due to the last inclusion
above). With specificity, the second defeasible inclusion overrides the first one and yields the intuitive inference that non-blacklisted staff members are indeed allowed to access project files. More
formally, the subsumption
subj.(Staff u Blacklisted) u target.Proj u op.Read v decision.{Grant}
holds in all the models of the above knowledge base (as defined below).

2

Intuitively, a model of hK, i is a model of KS that maximizes the set of individuals satisfying
the defeasible inclusions in KD , resolving conflicts by means of the priority relation  whenever
possible. In formalizing the notion of model, one should specify how to deal with the predicates
occurring in the knowledge base: is their extension allowed to vary in order to satisfy defeasible
inclusions? A discussion of the effects of letting predicates vary vs. fixing their extension can be
found in the work of Bonatti, Lutz and Wolter (2006); they conclude that the appropriate choice is
application dependent. So, in general, the set of predicates NC NR can be arbitrarily partitioned into
two sets F and V containing fixed and varying predicates, respectively; we denote this semantics
by CircF .
However, in Section 5 it is shown that fixed roles cause undecidability issues, so most of our
results concern a specialized framework in which all role names are varying predicates, that is,
F  NC . We use the notation CircF (rather than CircF ) to indicate that F  NC .
The set F , the GDIs KD , and the priority relation  induce a strict partial order over interpretations. As we move down the ordering we find interpretations that are more and more normal w.r.t.
KD . For all  = (C vn D) and all interpretations I let the set of individuals satisfying  be:
satI () = {x  I | x 6 C I or x  DI } .
Definition 3.3 Let KB = hK, i be a DKB. For all interpretations I and J , and all F  NC  NR ,
let I <KB,F J iff:
1. I = J ;
2. aI = aJ , for all a  NI ;
3. AI = AJ , for all A  F  NC , and P I = P J , for all P  F  NR ;
723

fiB ONATTI , FAELLA , & S AURO

4. for all   KD , if satI () 6 satJ () then there exists  0  KD such that  0   and
satI ( 0 )  satJ ( 0 ) ;
5. there exists a   KD such that satI ()  satJ ().
The subscript KB will be omitted when clear from context. Now a model of a DKB can be defined
as a maximally preferred model of its strong (i.e. classical) part.
Definition 3.4 Let KB = hK, i and F  NC  NR . An interpretation I is a model of CircF (KB)
iff I is a (classical) model of KS and for all models J of KS , J 6<F I.
Remark 3.5 This semantics is a special case of the circumscribed DLs introduced by Bonatti et
al. (2006). The correspondence can be seen by (i) introducing for each GDI C vn D a fresh atomic
concept Ab, playing the role of an abnormality predicate; (ii) replacing C vn D with C uAb v D;
(iii) minimizing all the predicates Ab introduced above.
In order to enhance readability, we will use the following notation for the special cases in which
all concept names are varying and the case in which they are all fixed: <var and Circvar stand for
< and Circ , respectively; <fix and Circfix stand respectively for <NC and CircNC . For a DKB
KB = hKS  KD , i, we say that an interpretation I is a classical model of KB in case I is a model
of KS .
In this paper, we consider the following standard reasoning tasks over defeasible DLs:
Knowledge base consistency Given a DKB KB, decide whether CircF (KB) has a model.
Concept consistency Given a concept C and a DKB KB, check whether C is satisfiable w.r.t. KB,
that is, whether a model I of CircF (KB) exists such that C I 6= .
Subsumption Given two concepts C, D and a DKB KB, check whether CircF (KB) |= C v D,
that is, whether for all models I of CircF (KB), C I  DI .
Instance checking Given a  NI , a concept C, and a DKB KB, check whether CircF (KB) |=
C(a), that is, whether for all models I of CircF (KB), aI  C I .
The following example illustrates most of the above tasks as well as the main difference between
Circvar and Circfix .
Example 3.6 Consider the following simplification of Example 3.2:
User vn decision.{Grant}
Staff v User
Staff vn decision.{Grant}
BlacklistedStaff v Staff u decision.{Grant} .
Extend the knowledge base with the assertion Staff(John), and let the priority relation be K (i.e.,
priorities are determined by specificity). Denote the resulting knowledge base with KB. Due to the
inclusion Staff v User, the GDI for Staff (third line) has higher priority than the GDI for User
724

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

(first line). Therefore, in all models I of Circvar (KB), John belongs to decision.{Grant} and
hence the following entailments hold:
Circvar (KB) |= {John} v decision.{Grant}
Circvar (KB) |= decision.{Grant}(John)

(subsumption)

(instance checking)

(1)
(2)

Interestingly, John does not belong to BlacklistedStaff, because this is the only way of satisfying the top-priority GDI for Staff. Analogously, in all models I of Circvar (KB), John is the only
member of Staff because this setting maximizes the number of individuals satisfying both GDIs
(as all the individuals in Staff vacuously satisfy the GDI for Staff for all values of decision).
More generally, as a side effect of the maximization of all satI (), Circvar induces a sort of closedworld assumption over all concepts with exceptional properties (w.r.t. some larger concept). Consequently, BlacklistedStaff is not satisfiable w.r.t. KB, and the following subsumption holds:
Circvar (KB) |= Staff v {John} .

(3)

On the contrary, under Circfix , User and Staff may contain any number of individuals (other than
zero) because Circfix is not allowed to change the extension of any atomic concept, even if this
would satisfy more GDIs. Similarly, there exist models of Circfix (KB) where John does not belong
to decision.{Grant} because John belongs to BlacklistedStaff and Circfix does not allow
to change its extension to satisfy more GDIs. As a consequence, it can be easily verified that
BlacklistedStaff is satisfiable w.r.t. Circfix (KB), and that (1), (2), and (3) do not hold if Circvar
is replaced by Circfix . We only have inferences such as:
Circfix (KB) |= User u Staff v decision.{Grant} ,

(4)

Circfix (KB) |= Staff u BlacklistedStaff v decision.{Grant} .

(5)
2

Note that in Circvar one can obtain nominals (cf. Staff in (3)) without using nominals explicitly in
the knowledge base. If other axioms do not interfere, then an assertion A(a) and a GDI A vn 
suffice to make A a singleton. This idea will be used in some reductions later on.
The next example deals with multiple inheritance, and in particular with parent concepts with
conflicting properties.
Example 3.7 Let KB consist of the axioms:
Whale

v

Mammal u SeaAnimal

Mammal vn has organ.Lungs
SeaAnimal vn has organ.Gills
has organ.Lungs

u

has organ.Gills v  ,

where the priority relation is specificity. Note that mammals and sea animals have conflicting default
properties. In all models of Circvar (KB) Whale is empty, because this is the only way of having
both GDIs satisfied by all individuals. Now let KB 0 = KB  {Whale(Moby)}. In each model of
725

fiB ONATTI , FAELLA , & S AURO

Circvar (KB 0 ), Moby satisfies as many GDIs as possible, that is, exactly one of the two GDIs of KB 0 .
As a consequence, we have the reasonable inference:2
Circvar (KB 0 ) |= {Moby} v has organ.Lungs t has organ.Gills .
The conflict between the two default properties inherited by Moby can be settled by adding a simple
axiom like Whale v has organ.Lungs, that overrides the property of sea animals. In this specific
example a strong axiom is appropriate; note, however, that the corresponding GDI would have the
same effect under K ; for instance, we have:
Circvar (KB 0  {Whale vn has organ.Lungs}) |= {Moby} v has organ.Lungs .
Consider Circfix now. The expected subsumptions Mammal v has organ.Lungs and SeaAnimal v
has organ.Gills are not entailed by Circfix (KB), because Lungs and Gills are empty in some
models (as Circfix cannot change their extension to satisfy more GDIs). The two GDIs could be
enabled by forcing Lungs and Gills to be nonempty. This can be done in several ways, e.g. via
assertions such as Lungs(L) or inclusions such as > v aux.Lungs (where aux is a new auxiliary
role). Let KB 00 denote such an extension. Then
Circfix (KB 00 ) |= {Moby} v has organ.Lungs t has organ.Gills ,
(similarly, the aforementioned expected consequences are entailed by Circfix (KB 00 )). The conflict
between the properties inherited from Mammal and SeaAnimal can be settled as discussed above.
2
The impossibility of forcing existentials with GDIs in Circfix , illustrated by the above example, can
be exploited to check whether a concept is always nonempty. It suffices to introduce a fresh role
aux (in order to prevent interference with the other axioms of the knowledge base) and a GDI
> vn aux.C. Clearly, the subsumption > v aux.C is entailed iff C is nonempty in all models
of Circfix (KB). Similar ideas will be used in the rest of the paper.
The next example is artificial. It is a convenient way of illustrating the interplay of specificity
and multiple inheritance.
Example 3.8 Let KB be the following set of axioms:
A1 v A01

(6)

A1 vn R1

(12)

A02

(7)

A2 vn R2

(13)

A01
A02

(14)

A2 v
B

v A1 u A2

(8)

R1

u

v

(9)

R2

u

R10
R20

v

(10)

R1

u R2 v 

(11)

vn
vn

R10
R20

(15)

For all sets of concept names F , and for all models I of CircF (KB), each member x of B I (if any)
satisfies exactly one of the top priority GDIs (12) and (13), that are conflicting due to (11). If x does
2. The symbol t is description logics equivalent of disjunction. Formally, (C t D)I = C I  DI .

726

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

not satisfy (12) then it can satisfy the conflicting GDI (14); symmetrically, if x does not satisfy (13)
then x can satisfy (15). Consequently, we have:
Circfix (KB) |= B v (R1 u R20 ) t (R2 u R10 ).
2
The last two examples show that GDIs and disjointness constraints together can express disjoint
unions. Similar techniques will be used later on to simulate the law of excluded middle, negation,
and 3-valued logic.
Subsumption, instance checking, and the complement of concept satisfiability can be reduced
to each other, as in the classical setting:
Theorem 3.9 Let DL range over DL-liteR and EL ; let X = var, fix, F . In CircX (DL), subsumption, instance checking, and concept unsatisfiability can be reduced to each other in constant
time.
The proof is not completely standard, due to the limited expressiveness of DL-liteR and EL , as
well as the peculiarities of nonmonotonic reasoning.3
Proof. First we focus on Circvar and CircF , where F consists of concept names occurring in a given
KB.
From unsatisfiability to subsumption. Checking unsatisfiability of a concept C can be reduced
to checking the subsumption C v . DL-liteR does not support  explicitly, however an equivalent
concept A can be easily defined with the inclusion A v A .
From subsumption to unsatisfiability. Conversely, a subsumption C v D can be reduced to the
unsatisfiability of C u D. In DL-liteR conjunction is not supported, so the subsumption must be
reduced to the unsatisfiability of a fresh variable concept A axiomatized by A v C and A v D. In
EL conjunction is supported while negation is not; therefore the given subsumption can be reduced
to the unsatisfiability of C u D where D is a fresh variable concept axiomatized with D u D v .
From instance checking to subsumption. An instance checking query C(a) can be reduced to
subsumption as follows: Introduce a fresh variable concept A and assert A(a); then minimize A
with A vn ; now in all models I of Circvar , AI = {aI }. It follows that C(a) holds iff the
subsumption A v C holds.
From unsatisfiability to instance checking. Finally, the unsatisfiability of a concept C is equivalent to the validity of the instance checking problem C(a), where a is a fresh individual constant. In EL , C must be suitably axiomatized with a fresh concept name C and the inclusions
C u C v , > vn C, and > vn C (these three axioms entail the subsumption > v C t C, thereby
enforcing the law of the excluded middle). In order to preserve the semantics of the knowledge
base, > vn C and > vn C must be given a priority strictly smaller than the priority of all the other
defeasible inclusion in the KB. This ensures that the new GDIs cannot block the application of any
of the original GDIs. Clearly, the two new GDIs must have the same priority.
3. For example, in classical logic a subsumption C v D is a logical consequence of KB iff for any fresh individual a,
D(a) is a logical consequence of KB  {C(a)}. This approach is not correct for Circumscription. The models of
CircF (KB) can be quite different from the models of CircF (KB  {C(a)}); for instance, consider the example in
which nonmonotonic reasoning makes Whale empty and the assertion Whale(Moby) overrides this inference.

727

fiB ONATTI , FAELLA , & S AURO

This completes the proof for Circvar and CircF . The proof for Circfix can be obtained by replacing the fresh variable concept names A, C, and D with a corresponding (variable) concept R,
where R is a fresh role.
2
Note that the above reductions still apply if priorities are specificity-based (K ), with the exception
of the reduction of concept unsatisfiability to instance checking in EL . For this case, one can use
Theorem 4.6 below to eliminate general priorities, and get a reduction for Circfix .

4. Complexity Preserving Features
In some cases, nonmonotonic inferences and language featurese.g. variable predicates and explicit prioritiesdo not affect complexity. In this section several such results (and related lemmata)
are collected; the reader is warned that, in general, they may not apply to all reasoning tasks and
all language fragments. We start by observing that the logics we deal with enjoy the finite model
property.
Lemma 4.1 Let KB = hK, i be a DKB in DL-liteR or ELHO, . For all F  NC , CircF (KB)
has a model only if CircF (KB) has a finite model whose size is exponential in the size of KB.
Proof. A simple adaptation of a result for ALCIO (Bonatti et al., 2006), taking role hierarchies
into account.
2
As a consequence, these logics preserve classical consistency (because all descending chains of
models originating from a finite model must be finite):
Theorem 4.2 Let KB = hKD  KS , i be a DKB in DL-liteR or ELHO, . For all F  NC , KS
is (classically) consistent iff CircF (KB) has a model.
Remark 4.3 Obviously, a similar property holds for all circumscribed DLs with the finite model
property, including ALCIO and ALCQO.
Since knowledge base consistency is equivalent to its classical version, it will not be discussed in
this paper any further.
Next, we prove that under mild assumptions, CircF is not more expressive than Circfix (which is
a special case of the former), that is, variable concept names do not increase the expressiveness of
the logic and can be eliminated.4
Theorem 4.4 If DL is a description logic fully supporting unqualified existential restrictions
( R),5 then, for all F  NC , concept consistency, subsumption, and instance checking in CircF (DL)
can be reduced in linear time to concept consistency, subsumption, and instance checking (respectively) in Circfix (DL).
4. The standard techniques for eliminating variable predicates (Cadoli, Eiter, & Gottlob, 1992) use connectives that are
not fully supported in DL-liteR and EL, therefore an ad-hoc proof is needed.
5. We say that DL fully supports unqualified restrictions if they can occur wherever a concept name could.

728

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Proof. Let KB be any given DKB in the language DL. Introduce a new role name RA for each
(variable) concept name A 6 F . Then, replace each occurrence of A in KB with RA and call
KB 0 the resulting KB. Recall that in Circfix (DL) all concept names are fixed and all roles are variable. Hence, the newly added roles RA behave in Circfix (KB 0 ) exactly in the same way as concepts
A 6 F do in CircF (KB). Formally, there is a bijection between the models of CircF (KB) and the
models of Circfix (KB 0 ), which preserves the interpretation of all role and concept names, except
that the extension of concept names A 6 F in a model of CircF (KB) coincides with the domain of
the corresponding role RA in the corresponding model of Circfix (KB 0 ). As a consequence, the consistency of a concept C w.r.t. CircF (KB) is equivalent to the consistency of C 0 w.r.t. Circfix (KB 0 ),
where C 0 is obtained from C by replacing each occurrence of A 6 F with the corresponding RA .
Similarly for subsumption and instance checking.
2
Symmetrically, the next theorem proves that in EL fixed predicates can be eliminated using general
priorities. The reduction adapts the classical encoding of fixed predicates to the limited expressiveness of EL .
Theorem 4.5 For all F  NC , concept consistency, subsumption, and instance checking in
CircF (EL ) can be reduced in linear time to concept consistency, subsumption, and instance checking (respectively) in Circvar (EL ) with general priorities.
Proof. Let K = hKS  KD , i be a given EL DKB. Fixed predicates are removed through
the following transformation. For each concept name A  F introduce a new concept name A
(representing A). Let KS be the set of all disjointness axioms A u A v , for each A  F . Let
 be the set of all defeasible inclusions > v A and > v A, for each A  F . Finally, let 0 be
KD
n
n
 and all   K ,   0 . Define
the minimal extension of  such that for all    KD
D

K0 = hKS  KS  KD  KD
, 0 i .

Claim 1. Let J and J 0 be two classical models of KS  KS such that J 0 <K0 ,var J and for all
 , satJ 0 () = satJ () and (ii) J and J 0 agree on all
A  F , AJ = AJ . Then (i) for all   KD
A  F.
 have maximal priority, therefore, for
Proof of Claim 1: By definition of 0 , the members of KD
0
 , satJ ()  satJ (). If there exists A  F such that satJ 0 (> v A)  satJ (> v A),
all   KD
n
n
0
0
0
then AJ  AJ , and hence AJ  AJ ; consequently satJ (> vn A)  satJ (> vn A) (a
0
contradiction). Symmetrically, the assumption that satJ (> vn A)  satJ (> vn A) leads to a
contradiction. This proves (i); (ii) is a straightforward consequence of (i).
Claim 2. Every model I of CircF (K) can be extended to a model J of Circvar (K0 ).
To prove this claim, extend I to J by setting AJ = AI , for all concept names A  F .
Suppose that J is not a model of Circvar (K0 ). Since J satisfies KS  KS by construction, there must
be a J 0 that satisfies KS  KS and such that J 0 <K0 ,var J . By Claim 1.(ii), J and J 0 agree on all
A  F ; by Claim 1.(i), the improvement of J 0 over J concerns the GDIs in KD . It follows that
I 0 <K,F I, where I 0 is the restriction of J 0 to the language of K. This contradicts the assumption
that I is a model of CircF (K).
Claim 3. For all models J of Circvar (K0 ), the restriction of J to the language of K is a model
of CircF (K).
Let I be the restriction of J to the language of K. Clearly I satisfies KS . Now suppose that I is
not a model of CircF (K), which means that there exists I 0 satisfying KS and such that I 0 <K,F I.
729

fiB ONATTI , FAELLA , & S AURO

0

0

Extend I 0 to J 0 by setting AJ = AI , for all concept names A  F . Note that I and I 0 must
,
agree on all A  F , therefore J and J 0 agree on them, too. Consequently, for all   KD
0
satJ () = satJ (). Moreover, I 0 improves I over the GDIs of KD , therefore J 0 improves J over
KD , too. It follows that J 0 <K0 ,var J (a contradiction).
The Theorem is now a straightforward consequence of Claims 2 and 3.
2
Now consider priority relations and GDIs. We are going to prove that if the language fragment is
sufficiently rich, then they can be simulated with the specificity-based relation K and normalized
defeasible inclusions A vn C (whose left-hand side is a concept name), respectively.
Let KB = hK, i be any given DKB in EL . First we need to define a new fixed concept Dom
that encodes the domain without being equivalent to >. This requires the following transformation:
A = Dom u A

(R.C) = Dom u R.(C  )

> = Dom
 = 

(C u D) = C  u D
(C v[n] D) = C  v[n] D

Obtain K from K by transforming all inclusions in K and by adding a nonemptiness axiom > v
aux .Dom (aux a fresh role) plus an assertion Dom(a) for each a  NI occurring in K. It is not
hard to see that the restrictions to Dom of the classical models of K correspond to the classical
models of K. More precisely, let I  be a classical model of K , we obtain a classical model I of





K by setting AI = AI  Dom I and RI = RI  (Dom I  Dom I ), for each concept name

A and role name R occurring in K. Notice that it is necessary for Dom I to be non-empty for
this to work. Conversely, given a classical model I of K, it is sufficient to set Dom I = I and
aux I = I  I to make I a classical model of K .
Now we have to remove general priorities and GDIs. For all GDIs  = (C vn D)  K , add
two fresh predicates A , P and replace  with the following axiom schemata:
Dom v A
A vn P

A 0 v A

for all  0  

P u C v D

(16)
(17)

Call the new DKB KB 0 = hK0 , K0 i. By (16), the specificity-based relation K0 prioritizes the new
GDIs according to the original priorities. Moreover, by (17), a defeasible inclusion A vn P
is satisfied by an individual if and only if the same individual satisfies the corresponding GDI .
Then it is not difficult to verify that all the reasoning tasks such that none of the new predicates A
and P occur in the query yield the same answer in hK ,  i and KB 0 . As a consequence of the
above discussion, by combining the transformation  with (16) and (17), and by observing that the
reduction makes use of EL constructs only, we have:
Theorem 4.6 Reasoning in Circfix (EL ) with explicit priorities and GDIs can be reduced in polynomial time to reasoning in Circfix (EL ) with only specificity-based priority and defeasible inclusions of the form A vn P .
Finally, by Theorem 4.4, the above result can be extended to all of CircF :
Corollary 4.7 Reasoning in CircF (EL ) with explicit priorities and GDIs can be reduced in polynomial time to reasoning in Circfix (EL ) with only specificity-based priority and defeasible inclusions of the form A vn P .
730

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

5. Undecidability of EL with Fixed Roles
In Circ both concept names and roles can be fixed; however, as we show in this section, fixed roles
in general make reasoning tasks undecidable. To this end, the model conservative extension problem
defined by Lutz and Wolter (2010) is reduced to the subsumption problem.6
Some preliminary definitions are needed; given a signature   NC  NR and two interpretations I and J , we say that I and J coincide on  if and only if I = J and for all predicates
X  , X I = X J . Then, let T1  T2 be classical EL TBoxes, T2 is a model conservative extension of T1 if and only if for every model I of T1 , there exists a model J of T2 such that I and J
coincide on the signature of T1 .
Lutz and Wolter (2010) prove (see Lemma 40) that there exists a class C of EL TBoxes such that
the problem of checking whether a TBox in C is a model conservative extension of another TBox in
C is undecidable. Moreover, the following property holds.
Lemma 5.1 Given two TBoxes T1  T2 in C, if T2 is not a model conservative extension of T1
then there exists a model I of T1 and an interpretation J of T2 such that I and J coincide on the
signature of T1 and the set of individuals in J that violate at least one concept inclusion from T2 is
finite.
For a DKB KB = hKS  KD , i and an interpretation I of KB, we denote by ab KB (I) (for
abnormal) the total number of individuals x  I such that x 6 satI () for some   KD .
Lemma 5.2 Let KB = hKS  KD , i be a DKB and I be a classical model of KB s.t. ab KB (I)
is finite. For all F  NC  NR , either I is a model of CircF (KB) or there exists a model J of
CircF (KB) such that J <KB,F I.
Proof. It suffices to show that each <KB,F -chain descending from I is finite. Since DIs are incomparable with each other, each step in the <KB,F -chain must improve at least one DI and leave all
other DIs unchanged. Formally, if I 0 <KB,F I then there exists at least a DI   KD such that
satI ()  satI 0 () and for all  0  KD it holds satI ( 0 )  satI 0 ( 0 ). Hence, ab KB (I 0 ) < ab KB (I)
and the thesis follows.
2
Assume that T1 , T2  C are given, where T1  T2 , and let  be the signature of T1 . Let F = 
and KB = hK, i where K = T1  {C vn D | C v D  T2 \ T1 }.
Lemma 5.3 For all T1 , T2  C, T2 is a model conservative extension of T1 iff CircF (KB) |= C v
D, for all C v D  T2 .
Proof. [if ] Suppose by contradiction that T2 is not a model conservative extension of T1 and
CircF (KB) |= C v D, for all C v D  T2 . By Lemma 5.1, we can consider a model I of
T1 and an extension J of I on the signature of T2 , such that the set of individuals that violate in J
at least one inclusion of T2 is finite. Since J is a classical model of KB and ab KB (J ) is finite, by
Lemma 5.2, there exists a model J 0 of CircF (KB) such that either J 0 = J or J 0 <KB,F J . Since
CircF (KB) entails T2 and F = , J 0 is a (classical) model of T2 that coincides with J and I on 
(absurdum).
6. The sketch of this proof has been kindly provided by Frank Wolter in a personal communication. Any imprecision in
the proof is due to the authors.

731

fiB ONATTI , FAELLA , & S AURO

[only if ] Suppose by contradiction that T2 is a model conservative extension of T1 , and for some
C v D  T2 , it holds CircF (KB) 6|= C v D. Since CircF (KB) 6|= C v D, there exists a model
I of CircF (KB) such that satI (C vn D)  I . Since I is a model of T1 and T2 is a model
conservative extension of T1 , there exists an interpretation J that (i) coincides with I on  and
(ii) is a model of T2 , i.e., for all defeasible inclusions C vn D in KB, satJ (C vn D) = J .
Therefore, J <KB,F I (absurdum).
2
Since checking whether a TBox is a model conservative extension of another one has been proved
to be undecidable for C  EL (Lutz & Wolter, 2010), it immediately follows that subsumption in
CircF (EL) is undecidable. Moreover, since subsumption can be reduced to concept unsatisfiability
or instance checking (Theorem 3.9), the latter reasoning tasks are undecidable as well.
Theorem 5.4 In CircF (EL), subsumption, concept consistency and instance checking are undecidable.

6. Complexity of Circumscribed DL-liteR
In this section we focus on DL-liteR DKBs. We first prove that in Circvar (DL-liteR ) the reasoning
tasks are complete for the second level of the polynomial hierarchy. From this, according to Theorem 4.4, we immediately obtain an hardness result for Circfix (DL-liteR ) too. Then, the membership
for Circfix (DL-liteR ) to second level of the polynomial hierarchy is shown for the fragment of DKBs
with left-fixed defeasible inclusions, i.e. defeasible inclusions of type A vn C.
6.1 Complexity of Circvar (DL-liteR )
In this section we prove that Circvar (DL-liteR ) subsumption, concept unsatisfiability (co-sat) and
instance checking are complete for p2 .
Our membership results rely on the possibility of extracting a small (polynomial) model from
any model of a circumscribed DKB.
Lemma 6.1 Let KB be a DL-liteR DKB. For all models I of Circvar (KB) and all x  I there
exists a model J of Circvar (KB) such that (i) J  I , (ii) x  J , (iii) for all DL-liteR concepts
C, x  C I iff x  C J (iv) |J | is polynomial in the size of KB.
Proof. Assume that KB = hKS  KD , i, I is a model of Circvar (KB), and x  I . Let cl(KB)
be the set of all concepts occurring in KB. Choose a minimal set   I containing: (i) x, (ii) all
aI such that a  NI  cl(KB), (iii) for each concept R in cl(KB) satisfied in I, a node yR such
that for some z  RI , (z, yR )  RI .
Now define J as follows: (i) J = , (ii) aJ = aI (for a  NI  cl(KB)), (iii) AJ = AI  
(A  NC  cl(KB)), and (iv) P J = {(z, yP ) | z   and z  P I }  {(yP  , z) | z   and z 
I
P  } (P  NR ).
Note that by construction, for all z  J and for all C  cl(KB), z  C J iff z  C I ;
consequently, J is a classical model of S. Moreover, the cardinality of J is linear in the size of
KB (by construction). So we are only left to show that J is a <KD ,var -minimal model of KB.
0
0
Suppose not, and consider any J 0 <KD ,var J . Define I 0 as follows: (i) I = I , (ii) aI =
0
0
0
0
0
aI , (iii) AI = AJ , (iv) P I = P J . Note that the elements in I \J satisfy no left-hand side of
any DL-liteR inclusion (be it classical or defeasible), therefore all inclusions are vacuously satisfied.
732

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

0

Moreover, the restriction of I 0 to J is <KD ,var -smaller than the corresponding restriction of I
in the interpretation ordering. It follows that I 0 <KD ,var I, and hence I cannot be a model of
Circvar (KB) (a contradiction).
2

2

x

Q

x
1

C

S, T

1
Q

Q

Q
4

3

C

Q

6

S
T

T

Q

4
S

S
T

6
T

S

C v Q
Q v S
S vn T

5

7

8

(b) A model I of K.

(a) A DKB K.

7

8

(c) A small model extracted from
the model in Figure 2(b). Dashed
arrows denote edges that are not
present in I.

Figure 2: Illustrating Lemma 6.1.
To illustrate Lemma 6.1, consider the DKB KB in Figure 2(a) and the model I in Figure 2(b).
Note that all individuals in I satisfy the defeasible inclusion in KB. The small model J , depicted
in Figure 2(c), is obtained as follows. First, it contains the designated individual x; then, for each
concept R that occurs in KB and is satisfied in I (where R is a possibly inverse role), it contains a
representative yR that receives role R in I. In our case, assume that the chosen representatives are:
yQ = 6, yQ = 4, yS = 8, and yT = 7. Hence, J = {x, 6, 4, 8, 7}. The roles in J are obtained
by connecting each individual z that satisfies a concept P in I to the chosen representative yP .
For instance, since 4 satisfies S in I, we have the edge (4, 8)  S J . Moreover, the representative
for an inverse role P  is connected to all nodes that satisfy the concept P  in I. In our case, since
4 is the representative for Q and 6 satisfies Q , we have the edge (4, 6)  QJ . Besides, since 4
itself satisfies Q , we also have (4, 4)  QJ . It can be verified by inspection that J is a model of
Circvar (KB), as its individuals satisfy all classical and defeasible inclusions in KB.
Theorem 6.2 Concept consistency over Circvar (DL-liteR ) DKBs is in p2 . Subsumption and instance checking are in p2 .
Proof. By Lemma 6.1, it suffices to guess a polynomial size model that provides an answer to the
given reasoning problem. Then, with an NP oracle, it is possible to check that the model is minimal
w.r.t. <var .
2
The complexity upper bounds proved by Theorem 6.2 are in fact tight, as stated by Theorem 6.6.
The proof of hardness is based on the reduction of the minimal-entailment problem of positive
disjunctive logic programs  which has been proved to be p2 -hard (Eiter & Gottlob, 1995).
A clause is a formula l1      lh , where the li are literals over a set of propositional variables
P V = {p1 , . . . , pn }. A positive disjunctive logic program (PDLP for short) is a set of clauses
733

fiB ONATTI , FAELLA , & S AURO

S = {c1 , . . . , cm } where each cj contains at least one positive literal. A truth valuation for S is a
set I  P V , containing the propositional variables which are true. A truth valuation is a model of S
if it satisfies all clauses in S. For a literal l, we write S |=min l if and only if every minimal7 model
of S satisfies l. The minimal-entailment problem can be then defined as follows: given a PDLP S
and a literal l, determine whether S |=min l.
For each propositional variable pi , 1  i  n, we introduce two concept names Pi and Pi , where
the latter encodes pi . We denote by Lj , 1  j  2n, a generic Pi or Pi . For each clause cj  S
we introduce the concept name Cj . Then, two other concept names True and False represent the
set of true and false literals, respectively. We employ roles RLi , RTrueCj , RFalse Pi , RTrue Pi ,
and TLi .
In the following, defeasible inclusions  are assigned a numerical priority h(), with the intended meaning that 1  2 iff h(1 ) < h(2 ).
The first step consists in reifying all the propositional literals, i.e., we want each of them to
correspond to an individual. Therefore, we introduce the axioms:

NonEmpty(a)

(18)

NonEmpty v Li

(1  i  2n)

(19)

NonEmpty v RLi

(1  i  2n)

(20)

(1  i  2n)

(21)

Li v Lj

(1  i < j  2n)

(22)

Li vn Li

(1  i  2n)

(23)

RL
i

v Li

[priority: 0]

Axioms (18-21) force the literal encodings Li to be non empty. Axioms (22) make literal encodings
pairwise disjoint. Finally, defeasible inclusions (23) are used to reduce the Li to singletons.
Then, we represent the set of clauses S by adding for each clause cj = lj1      ljk , 1  j  m,
the following axioms.

Lji v Cj

(1  i  k)

(24)

Cj vn Cj

[priority: 0]

(25)

NonEmpty v RTrueCj

(26)

RTrueCj

(27)

v TrueCj

TrueCj v True

(28)

TrueCj v Cj

(29)

Axioms (2425) ensure that each (encoding of a) clause Cj is the union of its literals Lji . Axioms
(2629) assure that each clause contains at least one true literal. In order to model the concepts

7. With respect to set inclusion.

734

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

True and False and the correct meaning of complementary literals we add the following axioms.
True v False

(30)

TruePi v RFalse Pi

(1  i  n)

(31)

(1  i  n)

(32)

TruePi v Pi

(1  i  n)

(33)

TruePi v True

(1  i  n)

(34)

False Pi v False

(1  i  n)

(35)

False Pi v Pi

(1  i  n)

(36)

RFalse Pi

v False Pi

The previous schemata regard TruePi only; analogous schemata are defined for FalsePi . The
following inclusions ensure that the truth of a given literal is locally visible in the individual a,
through the auxiliary roles TLi .
TrueLi v TL
i

(1  i  2n)

(37)

TL
i

(1  i  2n)

(38)

(1  i  2n)

(39)

TrueLi w

TLi v NonEmpty

The axioms defined so far encode the classical semantics of S. To represent only minimal models
we add the following axioms.
Pi vn FalsePi

(1  i  n)

[priority: 1]

(40)

Pi vn TruePi

(1  i  n)

[priority: 2]

(41)

Given a PDLP S, we call the KB defined above KB S .
Given a truth assignment I  P V and a domain  = {a, d1 , . . . , d2n }, we define a corresponding interpretation, denoted by model (S, I, ), whose structure mirrors I. Formally, model (S, I, )
is the interpretation I = (, I ) such that:
I. aI = a;
II. NonEmpty I = {a};
III. RLIi = {(a, di )} and T LIi = {(a, di ) | I |= li };
IV. for each 1  i  2n, LIi = {di };
V. for each 1  j  m, CjI = {LIj1 , . . . , LIjh } where cj = lj1      ljh ;
VI. for each 1  i  2n, di  TrueLIi (resp. di  FalseLIi ) iff I |= li (resp. I 6 |=li );
VII. (X Y )I = X I  Y I , where X Y is a concept name obtained by concatenating two other
concept names X and Y (for instance, concept name TrueCj is obtained by concatenating
concept names True and Cj ); in other words, juxtaposition represents conjunction;
VIII. (R X Y )I = I  (X Y )I .
The following lemma, proved in the Appendix, states the relationship between I and model (S, I, ).
735

fiB ONATTI , FAELLA , & S AURO

Lemma 6.3 Given a PDLP S over P V = {p1 , . . . , pn } and a truth assignment I  P V , I is
a minimal model of S iff the interpretation model (S, I, ) is a model of Circvar (KB S ), for all
domains  with || = 2n + 1.
The following result, also proved in the Appendix, shows that any model of Circvar (KB S ) in fact
corresponds to a minimal model of S.
Lemma 6.4 If I is a model of Circvar (KB S ), then there exist a minimal model I of S such that
pi  I iff PiI  True I iff PiI  False I , for all i = 1, . . . , n.
Lemma 6.5 Given a PDLP S and a literal l represented by concept name L, the following are
equivalent:
(minimal entailment) S |=min l;
(subsumption) Circvar (KB S ) |= L v True;
(co-sat) FalseL is not satisfiable w.r.t Circvar (KB S );
(instance checking) Circvar (KB S ) |= (T L)(a).
Proof. The three inference problems on KB S represent the fact that for all models I of Circvar (KB S )
we have that LI True I is not empty  co-sat in particular relies on the fact that in all Circvar (KB S )
models True and False are a partition of the individuals belonging to the literal concepts. Therefore, it suffices to prove that l is true in all minimal models of S iff LI  True I 6=  in all models
of Circvar (KB S ).
Lemma 6.3 establishes a bijection between minimal models I of S and certain models I =
model (S, I, ) of Circvar (KB S ), such that the truth of a literal l in I corresponds to the inclusion of
LI into True I in I (see rule VI in the definition of model ). Therefore, the right-to-left direction is
immediately satisfied. For the left-to-right direction, assume that l is true in all minimal models of
S and let I be a model of Circvar (KB S ). By Lemma 6.4, there is a minimal model I of S such that
pi  I iff PiI  True I . If l = pi , we conclude LI  True I and the thesis. Similarly, for l = pi .
2
The following theorem provides complexity lower-bounds for the main decision problems of
both Circvar (DL-liteR ) and Circfix (DL-liteR ). The result for Circvar (DL-liteR ) follows immediately
from Lemma 6.5, and it extends to Circfix (DL-liteR ) due to Theorem 4.4.
Theorem 6.6 Subsumption, co-sat and instance checking over circumscribed DL-liteR DKBs with
general priorities are p2 -hard.
6.2 Upper Bound of Circfix (DL-liteR ) with Restrictions
We develop the same argument used for Circvar (DL-liteR ) to prove similar upper bounds for
Circfix (DL-liteR ) DKBs with left-fixed DIs (i.e., their left-hand side is fixed orequivalently
a concept name) or empty priority relations. Whether the same upper bounds apply to
Circfix (DL-liteR ) without any restriction is left as an open question.
736

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Lemma 6.7 Let KB be a DL-liteR knowledge base whose DIs are left-fixed. For all models I of
Circfix (KB) and all x  I there exists a model J of Circfix (KB) such that (i) J  I , (ii)
x  J , (iii) for all DL-liteR concepts C, x  C I iff x  C J , and (iv) |J | is polynomial in the
size of KB.
Proof. Assume that I is a model of Circfix (KB), with KB = hKS  KD , i, and x  I . Let
cl(KB) be the set of all concepts occurring in KB. Choose a minimal set   I containing: (i)
x, (ii) all aI such that a  NI  cl(KB), (iii) for each concept R in cl(KB) satisfied in I, a node
yR such that yR  (R )I (where P  is considered equivalent to P ), and finally (iv) for all
inclusions C v[n] R in KB such that (C u R)I 6= , a node z  (C u R)I .
Now define J as follows: (i) J = , (ii) aJ = aI (for a  NI  cl(KB)), (iii) AJ = AI  
(A  NC  cl(KB)), and (iv) P J = {(z, yP ) | z   and z  P I }  {(yP , z) | z   and z 
I
P  } (P  NR ).
Note that by construction, for all z  J and for all C  cl(KB), z  C J iff z  C I ;
consequently, J is a classical model of KB. Moreover, the cardinality of J is linear in the size of
KB (by construction). So we are only left to show that J is a <fix -minimal model of KB.
0
0
Suppose not, and consider any J 0 <fix J . Define I 0 as follows: (a) I = I , (b) aI = aI ,
0
0
0
0
(c) AI = AI , (d) each RI is a minimal set such that (d1) RI  RJ , (d2) for all z  I \ J ,
0
and for all inclusions C v R or C vn R in KB such that z  (C u R)I , if RJ contains a pair
0
0
(v, w), then (z, w)  RI ; finally, (d3) each P I is closed under the role inclusion axioms of KB.
Note that, by construction,
0

(*) for all z  I \ J , z  RI only if z  RI ;
0

0

0

(**) for all z  I \ J , z  RI only if there exists v  J such that v  RJ .
Now we prove that I 0 is a classical model of KB. By construction, the edges (z, w) introduced
in (d2) do not change the set of existential restrictions satisfied by the members of J ; as a consequence  and since J 0 is a model of KB the members of J satisfy all the classical CIs of
KB.
Now consider an arbitrary element z  I \J and any CI  of KS . If  is an inclusion without
existential quantifiers, then I and I 0 give the same interpretation to  by definition, therefore z
satisfies . If  is R v A, R v A, R v S, or A v R (and considering that I satisfies
0
) z fails to satisfy  only if for some R0  {R, S}, z 6 (R0 )I and z  (R0 )I ; this is impossible
0
0
by (*). Next, suppose  is R v S. If z  (R)I , then by (**) there exists a v  J satisfying
0
0
0
(R)J and hence (S)J (as J 0 is a model of KS ), therefore z  (S)I (by d2). We are only left
0
0
to consider  = A v R: If z  AI = AI , then there exists wA  AJ (by construction of ) and
0
0
wA  (R)J because J 0 is a model of KB. Then z  (R)I by (d2). Therefore, in all possible
cases, z satisfies .
This proves that I 0 satisfies all the strong CIs of KB. It is not hard to verify that I 0 satisfies
also all role inclusions of KB. Therefore, in order to derive a contradiction, we are left to show that
I 0 <fix I (which implies that I is not a model of Circfix (KB)). Since by assumption J 0 <fix J ,
it suffices to prove the following claim: if satJ ()  satJ 0 () (resp. satJ ()  satJ 0 ()), then
satI ()  satI 0 () (resp. satI ()  satI 0 ()).
In J , I and J (resp. I 0 and J 0 ) satisfy the same concepts, therefore we only need to show
that for all z  I \ J , if z  satI () then z  satI 0 (). In all cases but those in which the
737

fiB ONATTI , FAELLA , & S AURO

right-hand side of  is R, the proof is similar to the proof for strong CIs (it exploits (*) and the fact
that concept names are fixed).
Let  be A vn R and consider an arbitrary z  I \ J such that z  satI (). Since concept
names are fixed, the only interesting case is that z actively satisfies , i.e. z  (A u R)I . By
construction,  contains an individual v  (A u R)J . Since by hypothesis satJ ()  satJ 0 (),
0
0
v  (A u R)J and hence, by (d2), z  (R)I , that is z  satI 0 ().
2
To prove the same lemma under the assumption that the priority relation is empty we need
some preliminary notions. Given a KB KB = hKS  KD , i, an interpretation I and an individual
z  I , we denote with KB [z] the classical knowledge base:

	
KB [z] = KS  C v D | (C vn D)  KD and z  satI (C vn D)
Then, the support of a concept C in I, supp I (C), is the set of individuals z  I such that, for
some A, z  AI and A vKB[z] C. If z  supp I (C) we say that z supports C in I.
Lemma 6.8 Let KB = hK, i be a DL-liteR knowledge base. For all models I of Circfix (KB) and
all x  I there exists a model J of Circfix (KB) such that (i) J  I , (ii) x  J , (iii) for all
DL-liteR concepts C, x  C I iff x  C J , and (iv) |J | is polynomial in the size of KB.
Proof. Assume that I is a model of Circfix (KB) and let   I be defined as in the above proof
of Lemma 6.7, except for case (iv), which is replaced by: (iv) for all inclusions C v[n] R in KB
such that supp I (R) 6= , a node wR  supp I (R). That is, for each inclusion whose RHS is
variable, we pick a witness that is in the support of the RHS, if such a witness exists.
Next, define J as in the proof of Lemma 6.7. As before, J is a classical model of KB and
the cardinality of J is linear in the size of KB. We are left to show that J is <fix -minimal.
Suppose not, and consider any J 0 <fix J . Since the priority relation is empty, for all DIs  in KB,
satJ ()  satJ 0 (). Hence, for all concepts C it holds supp J (C)  supp J 0 (C). Define I 0 as
in the proof of Lemma 6.7, except for case (d2), which is replaced by: (d2) for all z  I \ J ,
0
and for all inclusions C v[n] R in KB such that z  supp I (R), if RJ contains a pair (v, w),
0
then (z, w)  RI . We prove that I 0 <fix I, contradicting the hypothesis that I is a model of
0
0
Circfix (KB). The only non-trivial case consists in proving that the individuals in I \ J satisfy
in I 0 the same inclusions of type C v[n] S that they satisfy in I.
0
0
Assume that z  I \ J satisfies C v[n] S in I; we distinguish two cases. First, if
0
z  supp I (S), we have that J contains a witness wS s.t. wS  supp J (S)  supp J 0 (S).
0
0
Therefore, there exists a pair (wS , y)  S J and, by (d2), (z, y)  S I . Second, assume that
z 6 supp I (S). Since z  satI (C v[n] D), we have that z 6 supp I (C). Therefore, if C = A,
0
0
then z 6 AI = AI , whereas if C = R, then by (d2) z 6 (R)I . In both cases, z vacuously
satisfies  in I 0 .
2
Theorem 6.9 Let KB be a DKB with left-fixed DIs or an empty priority relation. Concept consistency in Circfix (KB) is in p2 . Subsumption and instance checking are in p2 .
2

Proof. Similar to the proof of Theorem 6.2.
738

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

7. Complexity of Circumscribed EL and EL
Recall that reasoning in circumscribed EL is undecidable when roles can be fixed. Here we analyze
the other cases, were F  NC .
In EL, that cannot express any contradictions, defeasible inclusions cannot be possibly blocked
under Circvar , and circumscription collapses to classical reasoning:
Theorem 7.1 Let KB = hKS  KD , i be an EL DKB. Then I is a model of Circvar (KB) iff I is a
model of KS  KD , where KD = {A v C | (A vn C)  KD }.
Proof. Let I be any model of Circvar (KB), and let J be the interpretation such that (i) J = I ,
(ii) for all a  NI , aJ = aI , (iii) for all A  NC , AJ = J , and (iv) for all P  NR , P J =
J  J . It can be easily verified by structural induction that for all EL concepts C, C J = J
and hence each domain element of J satisfies all EL inclusions (strong and defeasible). Then,
clearly, J is a model of Circvar (KB). Consequently, for all   KD , satI () = I , otherwise
J <var I (a contradiction). It follows that I is a classical model of KS  KD .
2
By the results of the work of Baader et al. (2005), it follows that in Circvar (EL), concept satisfiability
is trivial, subsumption and instance checking are in P.
Remark 7.2 Clearly, the same argument and the same result apply to Circvar (ELHO).
If we make EL more interesting by adding  as a source of inconsistency, then complexity
increases significantly.
Theorem 7.3 In Circvar (EL ), concept satisfiability, instance checking, and subsumption are
ExpTime-hard. These results still hold if knowledge bases contain no assertion.8
Proof. Let ELA be the extension of EL where atomic concepts can be negated. We first reduce
TBox satisfiability in ELA (which is known to be ExpTime-hard, see Baader et al., 2005) to the
complement of subsumption in Circvar (EL ). Let T be a TBox (i.e., a set of CIs) in ELA . First
introduce for each concept name A occurring in T a fresh concept name A whose intended meaning
is A. Obtain T 0 from T by replacing each literal A with A. Let KB = hK, K i be the DKB
obtained by extending T 0 with the following inclusions, where U and UA  for all A occurring in
T  are fresh concept names (representing undefined truth values), and R is a fresh role name:
A u A v 

(42)

> vn A

(46)

A u UA v 

(43)

> vn A

(47)

A u UA v 

(44)

> vn UA

(48)

UA v U

(45)

> vn R.UA

(49)

We prove that T is satisfiable iff in some model of Circvar (KB) all UA s are empty, which holds
iff Circvar (KB) 6|= > v R.U . Consequently, subsumption in Circvar (EL ) is ExpTime-hard. Assume that T is satisfiable and I is a model of T with domain I . From I we define an interpretation
8. Equivalently, in DLs terminology: ABoxes are empty.

739

fiB ONATTI , FAELLA , & S AURO

J that is a model of Circvar (KB) such that U J = , thus proving that Circvar (KB) 6|= > v R.U .
J has the same domain as I, and all concepts and roles occurring in T have the same interpretation
as in I; we only need to define the interpretation of the newly introduced concepts A, UA , and U ,
and of the role R. We set AJ = I \ AI and UAJ = U J = RJ = .
By construction J is a model of the classical inclusions in KB, in particular CIs (42)(45). It
remains to prove that J is minimal w.r.t. <var , i.e., it is not possible to improve any DI  without
violating another DI that is either incomparable with , or has a higher priority than . Notice that
defeasible inclusions (46) (resp., (47)) are violated by all individuals not in AJ (resp., all individuals
in AJ ). DIs (48) and (49) are violated by all individuals. Moreover, notice that DIs (46)(49) are
mutually incomparable according to specificity.
Each DI of type (46) or (47) can only be improved at the expenses of the corresponding DI
of the other type. Moreover, improving DIs (48) or (49) requires setting UAJ 6= , which, due to
rules (43) and (44) would damage the incomparable DIs (46) and/or (47). This proves that J is a
model of Circvar (KB), and hence Circvar (KB) 6|= > v R.U .
Conversely, assume that Circvar (KB) 6|= > v R.U , and let I be a model of Circvar (KB) with
an individual x  I such that x 6 (R.U )I . Due to rule (45), x 6 (R.UA )I for all atomic
concepts A. Hence, x violates all DIs of type (49). If there exists a concept UA such that (UA )I is
not empty, then the model obtained from I by adding an R-edge from x to an individual in (UA )I
is smaller than I according to <var , which is a contradiction. Therefore, all concepts UA are empty
in I.
Next, we show that for all atomic concepts A and all individuals y  I , either y  AI or
y  AI . Assume the contrary, i.e., there exists an individual y which belongs neither to AI nor to
AI . Then, y violates all DIs (46)(49). Consider the interpretation I 0 , obtained from I by setting
0
0
(UA )I = U I = {y}. By construction I 0 satisfies all CIs in KB. Compared to I, the status of
the DIs is the same, except that in I 0 the individual y satisfies (48). Hence, I 0 <var I, which is a
contradiction. Since each individual belongs to either AI or AI , we can convert I into a classical
model of T , thus showing that T is satisfiable.
Similarly, for any given a  NI , T is satisfiable iff there exists a model I of Circvar (KB) such
that aI 6 (R.U )I . Therefore, instance checking in Circvar (EL ) is ExpTime-hard as well.
Finally, add a fresh concept name B and all the inclusions B u R.UA v ; call the new
DKB KB 0 . Note that T is satisfiable iff in some model of Circvar (KB) all UA s are empty, which
holds iff B is satisfiable w.r.t. Circvar (KB 0 ). Consequently, concept satisfiability in Circvar (EL ) is
ExpTime-hard.
2
Since Circvar is a special case of CircF , and by Theorem 4.4, the above theorem applies to CircF
and Circfix , too:
Corollary 7.4 For X = F, fix, concept satisfiability checking, instance checking, and subsumption
in CircX (EL ) are ExpTime-hard. These results still hold if ABoxes are empty (i.e. assertions are
not allowed).
Fixed concept names can play a role similar to , so that the above proof can be adapted to
CircF (EL).
Theorem 7.5 Instance checking and subsumption are ExpTime-hard both in CircF (EL) and in
Circfix (EL). The same holds in the restriction of EL not supporting >.
740

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Proof. We reduce satisfiability of an ELA TBox T to the complement of subsumption in CircF (EL).
First we have to introduce a new concept name D representing > and translate each concept C in
ELA into a corresponding C  in EL, as follows:
 C  = C if C is a concept name;
 C  = A if C is A (for all A, A is a new concept name);
 C  = D u R.(C1 u D) if C is R.C1 ;
 C  = C1 u C2 if C is C1 u C2 .
Each C1 v C2 in T is translated into C1 v C2 . Then we extend the translated TBox with the
following inclusions, where Bot (representing ), all UA s, and Bad are new concept names and R
is a new role name:
A
A
UA

v
v
v

D
D
D

(50)

D vn UA

(58)

(51)

D

0

(59)

(52)

D

0

vn R.UA (60)

0

vn R.Bot (61)

v

D

A u A

v

Bot (53)

D

A u UA

v

Bot (54)

R.UA

v

Bad

(62)

A u UA

v

Bot (55)

R.Bot

v

Bad

(63)

D vn A

(56)

D vn A

(57)

D(a) (ABox assertion) (64)

Let KB = hK, K i be the resulting DKB, and set F = {D, Bot}. We prove that the following three
properties are equivalent: (i) T is satisfiable, (ii) CircF (KB) 6|= D v Bad , and (iii) CircF (KB) 6|=
Bad (a).
Let us prove that (ii) implies (i). Let I be a model of CircF (KB) with an individual x s.t.
x  DI and x 6 Bad I . By (62)(63), x 6 (R.UA )I for all A, and x 6 (R.Bad )I . By (59),
x  (D0 )I . Hence, x violates DIs (60) and (61). Assume that at least one concept UA is not empty
in I (resp., Bot is not empty in I). Then, I can be improved (according to <F ) by connecting with
an R-edge the individual x with the non-empty concept UA (resp., Bot), and then adding x to Bad
(notice that Bad is a variable concept). This being a contradiction, we conclude that Bot and all
UA s are empty in I. Then, we prove that the restriction of I to the domain DI is a model of T .
Inclusions (50)(51) ensure that all individuals satisfying either A or A are in DI . DIs (56)
(57), together with the fact that all UA s are empty, guarantee that each individual in DI satisfies
either A or A, for all concept names A. Rules (53), together with the fact that Bot is empty,
guarantee that no individual satisfies both A and A. The translation from C to C  completes our
claim.
Next, we show that (i) implies (ii). Let I be a model of T . We extend I to become a model
J of CircF (KB) such that DJ 6 Bad J , because DJ 6=  and Bad J = . For each A, set
AJ = I \ AI and UAJ = . Then, set DJ = (D0 )J = J = I and Bot J = Bad J = . It is
easy to verify that J satisfies all CIs of KB. It remains to prove that J is minimal w.r.t. <F .
741

fiB ONATTI , FAELLA , & S AURO

Name

Restrictions

full left local (LLf )
almost left local (aLL)

no qualified existentials on the LHS of inclusions
union of an LLf KB and a classical acyclic terminology, s.t. unfolding
the former w.r.t. the latter produces a LLf KB
A v[n] P.B A1 u A2 v B
only the following schemata:
P v B
P1 v P2 .B
(no nesting; no conflicts between DIs in Circfix )
A v[n] P.B P1 u P2 v P3 .B
only the following schemata:
P v B
(no nesting; potential conflicts between DIs even in Circfix )

left local (LL)

LL2

Figure 3: Fragments of EL considered in Section 7.

Since Bot is a fixed concept, inclusions (53)(55) ensure that A, A and UA are mutually exclusive, for all concepts A. Hence, each DI of type (56)(58) can only be improved at the expenses
of another DI of incomparable priority, which does not count as an improvement according to <F .
DI (61) cannot be improved because Bot is empty and fixed. Finally, suppose one tries to improve
one of the DIs of type (60). To do so, at least one individual x must enter the concept UA . Due
to the mutual exclusion property described earlier, x needs to exit from A (resp. A), thus violating
DI (56) (resp., (57)), which has a higher priority than (60) due to (59).
The equivalence between (i) and (iii) can be proved along similar lines. Just notice that the
fact (64) makes Bad (a) equivalent to the inclusion D v Bad . The thesis for Circfix is obtained as
a consequence of Theorem 4.4.
2
Concept consistency is simpler, instead. Call an interpretation I complete iff for all A  NC ,
AI = I , and for all P  NR , P I = I  I . It is not hard to verify that all EL concepts and all
EL inclusions (both classical and defeasible) are satisfied by all x  I , therefore complete models
are always models of CircF (KB), for all DKBs KB and all F  NC . As a consequence we have
that concept consistency is trivial:
Theorem 7.6 For all EL concepts C, DKBs KB, and F  NC , C is satisfied by some model of
CircF (KB).
7.1 Left Local EL and Circvar
In this subsection and in the next one, we prove that qualified existentials in the left-hand side of
inclusions are responsible for the higher complexity of EL w.r.t. DL-liteR . In particular, qualified
existentials in the left-hand side make the proof strategy of Lemma 6.7 fail: when the target of an
edge which starts in x is redirected, the individual x may satisfy a qualified existential restriction
that it did not satisfy before. If so, the truth value of inclusions may be affected. By limiting the
occurrences of qualified existential restrictions in the left-hand side of inclusions, it is possible to
reduce significantly the complexity of instance checking and subsumption in circumscribed EL .
Figure 3 summarizes the syntactic fragments of EL that we consider. We start with the following
class of knowledge bases:
Definition 7.7 A defeasible knowledge base hK, i is in the full left local (LLf ) fragment of EL
iff the left-hand sides of the inclusions of K contain no qualified existential restrictions.
742

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Note that this restriction rules out all the acyclic terminologies containing a qualified existential
restriction, and hence most of the existing ontologies of practical interest. Therefore, we introduce
the following relaxation of LLf EL :
Definition 7.8 An EL knowledge base KB = hK, i is almost LL (aLL for short) iff (i) K =
KLL  Ka , (ii) KLL is in LLf , (iii) Ka is a classical acyclic terminology, and (iv) if a concept name
A defined in Ka occurs in the left-hand side of an inclusion in KLL , then A does not depend (in
Ka ) on any qualified existential restriction.
In other words, by unfolding KLL with respect to Ka , we obtain a LLf knowledge base.
Example 7.9 Example 3.1 can be reformulated in aLL EL :
Human vn has lhs heart ;
has lhs heart u has rhs heart v  ;
Situs Inversus  Human u has rhs heart .
Here KLL consists of the first two axioms and Ka consists of the third axiom. Note that, in general,
a concept name A occurring in a terminology T can be extended with default properties by means of
an inclusion A vn C in the following cases: A can be a primitive concept (with no definition in T ),
or a concept partially defined by a one-way inclusion (e.g. Human v Mammal), or even a concept with
a complete definition A  D in T , provided that A does not depend on any qualified existentials.
Accordingly, in this example, we could add a defeasible inclusion like Situs Inversus vn C,
that would not be permitted if the definition of situs inversus depended on qualified existential
restrictions as in
Situs Inversus  Human u has heart. has position. Right .

2

A small model property similar to Lemma 6.7 can be proved for Circvar (aLL EL ) provided
that the right-hand side of subsumption queries has bounded quantifier depth. It is convenient to
split the proof into a proof for LLf EL and later extend it to aLL EL .
Since in LLf EL the RHS of an inclusion may have nested qualified existential restrictions, it
is difficult to prove the small model property when considering the entire language. For this reason,
we prove it indirectly: first we show how to transform a knowledge base KB into another KB  that
yields the following properties: (i) no nested formulas occur, (ii) defeasible inclusions are only of
type A vn B, (iii) every model I  Circvar (KB) can be extended to a model of Circvar (KB  ) on
the same domain and (iv) every model of Circvar (KB  ) is a model of Circvar (KB). Then, we prove
a small model property for the fragment with no nesting and, thanks to properties (iii) and (iv), we
recover the small model property for the entire language.
Each LLf EL inclusion C v[n] D is transformed in three steps. Note that Cs shape is:
A1 u . . . u An u R1 u . . . u Rm .
In the first step, C is replaced by a fresh concept name F0 (for convenience, we later refer to F0
also as FC ) and the following axioms are added:
Fi v Ai+1 u . . . u An u R1 u . . . u Rm
Ai+1 u Fi+1 v Fi
743

(0  i  n  1)

(65)

(0  i  n  2)

(66)

fiB ONATTI , FAELLA , & S AURO

if m = 0, i.e., there are no existentials in C, add the inclusion:
An v Fn1

(67)

Otherwise, add the following inclusions:
An u G0 v Fn1

(68)

Gj v Rj+1 u . . . u Rm (0  j  m  1)
Bj+1 u Gj+1 v Gj

(0  j  m  2)

Bm v Gm1

(69)
(70)
(71)

Bj v Rj
Rj v Bj

(1  j  m)

(72)

(1  j  m)

(73)

where the Fi , Gj and Bj are fresh concept names.
At this point, the initial inclusion C v[n] D can be replaced by FC v[n] D. To eliminate the
nesting in D, in the second step we replace it with a fresh concept name FD and add the inclusion
FD v D , where  is recursively defined as
A = A


(74)


(C u D) = C u D


(R.H) = R.FH



(75)


and add FH v H , where FH is fresh.

(76)

Finally, in the third step, all inclusions of type A v D1 u . . . u Dh are split into A v Di ,
1  i  h. The resulting knowledge base, that we denote with KB  , consists of instances of the
following axiom schemata:
A v[n] B

A v P.B

A1 u A2 v B

P v B

Now we prove the properties (iii) and (iv) mentioned above.
Lemma 7.10 Every model of Circvar (KB) can be extended to a model of Circvar (KB  ) on the same
domain.
Proof. First, note that inclusions (65)(73) are definitorial, that is every interpretation I of KB has
exactly one extension that satisfies them. This extension, for simplicity we continue to call it I, is
obtained by setting FiI = (Ai+1 u . . . u An u R1 u . . . u Rm )I , GIj = (Rj+1 u . . . u Rm )I
and BjI = (Rj )I .
Then, we can extend I by recursively setting FHI = H I , for each fresh concept FH introduced
in step 2. It is straightforward to see by structural induction on H that (H  )I = H I , and hence
all inclusions in step 2 and 3 are satisfied. Thus, if I is a classical model of KB, then it is also a
classical model of KB  .
Assume now that I is a model of Circvar (KB), we have to show that I is minimal also with
respect to KB  . Suppose not, and let J be a classical model of KB  such that J <KB ,var I. By
structural induction it is straightforward to see that for all C v[n] D in KB, (D )J  DJ . Since
FCJ = C J holds for all fresh concept names FC occurring in the LHS of a rule, we have that for
each inclusion C v[n] D in KB, satJ (FC v[n] FD )  satJ (C v[n] D)  which implies that J
744

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

is a classical model of KB. Concerning I, for every C v[n] D in KB, we have (FC )I = C I and
(FD )I = DI by construction, that is satI (C v[n] D) = satI (FC v[n] FD ).
The previous arguments entail that if satI (FC v[n] FD )  satJ (FC v[n] FD ) (respectively
satI (FC v[n] FD )  satJ (FC v[n] FD )), then satI (C v[n] D)  satJ (C v[n] D) (resp.
satI (C v[n] D)  satJ (C v[n] D)). Therefore, it would follow that J <KB,var I, which
contradicts the hypothesis.
2
Lemma 7.11 All models of Circvar (KB  ) are models of Circvar (KB).
Proof. The proof is similar to Lemma 7.10, in particular we already know that (i) if an individual
satisfies an inclusion FC v[n] FD in KB  , then it satisfies C v[n] D, and (ii) a classical model J of
KB can be extended in such a way that satJ (C v[n] D) = satJ (FC v[n] FD ), for each (possibly
defeasible) inclusion in KB. From this, we have that every classical model of KB  is a classical
model of KB and, by assuming that some classical model J of KB improves I (i.e. J <KB,var I),
it follows that J can be extended into a classical model of KB  such that J <KB ,var I.
2
The following result, whose proof can be found in the Appendix, represents a small model
property for LLf EL , which uses the above transformation of KB into KB  .
Lemma 7.12 Let KB = hK, K i be an LLf EL knowledge base, and C, D be EL concepts. For
all models I  Circvar (KB) and for all x  C I \ DI there exists a model J  Circvar (KB) such
that (i) J  I , (ii) x  C J \DJ , and (iii) |J | is O((|KB|2 +|C|)d ), where d = depth(D)+1.
Now we have to extend the above result to aLL EL . First, we show under which conditions
the concept names defined in Ka can be removed by unfolding them, using the unf operator defined
in Section 2. The proofs of the following two propositions can be found in the Appendix.
Proposition 7.13 Let KB = hKLL  Ka , i be an aLL EL knowledge base. Every model of
CircF (unf(KB)) can be extended to a model of CircF (KB).
The converse holds only if defined predicates are variable. The reason is that by adding a
definition like A  P where A is fixed, one fixes the expression P , too, thereby changing its
semantics.
Proposition 7.14 Let KB = hKLL  Ka , i be an aLL EL knowledge base and suppose that all
the concept names defined in Ka are variable. Then, for all models I of CircF (KB), the restriction
of I to primitive predicates is a model of CircF (unf(KB)).
With these lemmata we can prove:
Lemma 7.15 Let KB = hK, K i be an aLL EL knowledge base (where K = Ka  KLL ) and
let C, D be EL concepts. For all models I  Circvar (KB) and for all x  C I \ DI there
exists a model J  Circvar (KB) such that (i) J  I , (ii) x  C J \ DJ , and (iii) |J | is
O((|KB|2 + |C|)d ), where d = depth(D) + 1 + |Ka |2 .
745

fiB ONATTI , FAELLA , & S AURO

Proof. Let I  Circvar (KB) and x  C I \ DI . Let KB 0 = unf(KB), C 0 = unf(C, Ka ), and D0 =
unf(D, Ka ). Notice that KB 0 is a LLf knowledge base. By Proposition 7.14, the restriction I 0 of I
0
0
to primitive predicates is a model of Circvar (KB 0 ). In particular, it holds that x  (C 0 )I \(D0 )I . By
applying Lemma 7.12 to KB 0 , C 0 , D0 , and I 0 , we obtain that there exists a model J of Circvar (KB 0 )
0
such that x  (C 0 )J \(D0 )J and |J | is O((|KB 0 |2 +|C 0 |)d ), where d0 = depth(D0 )+1. We have
|KB 0 |2 + |C 0 |  |KB|2 + |C|, since replacing defined terms with their definitions
can only decrease
P
the total number of subformulas. Finally, depth(D0 )  depth(D) + (AE)Ka depth(E) 
depth(D) + |Ka |2 , hence the thesis.
2
Consequently we have that:
Theorem 7.16 In Circvar (aLL EL ) concept satisfiability is in p2 . Moreover, deciding EL subsumptions C v D or instance checking problems D(a) with a constant bound on the quantifier
depth of Ds unfolding w.r.t. the given DKB is in p2 .
2

Proof. Similar to the proof of Theorem 6.2.

Currently, we do not know whether the bound on quantifier nesting is necessary to the above upper
complexity bounds.
Next we prove that the p2 and p2 upper bounds for Circvar are tight. Actually, a much simpler
fragment suffices to reach that complexity:
Definition 7.17 An EL knowledge base is left local (LL) if its concept inclusions are instances of
the following schemata:
A v[n] P.B

A1 u A2 v B

P v B

P1 v P2 .B ,

where A and B are either concept names or . An LL EL concept is any concept that can occur
in the above inclusions.
Schema A v[n] B can be emulated in LL EL by the inclusions A v[n] R, R v B and
B v R, for a fresh role R. Note the similarity of LL schemata with the normal form of EL
inclusions (Baader et al., 2005) that, however, would allow the more general inclusions P.A v B
and P1 .A v P2 .B (that are forbidden by left locality).
Now we prove that reasoning in Circvar (LL EL ) is hard (and hence complete) for p2 and p2 .
For this purpose, we provide a reduction of minimal entailment over positive, propositional
disjunctive logic programs (PDLP), defined in Section 6.1. For each propositional variable pi ,
1  i  n, introduce two concept names Pi and Pi  where the latter encodes pi . In the following
we will denote by Lj , 1  j  2n, a generic Pi or Pi . For each clause cj  S introduce a concept
name Cj . Then, two other concept names True and False represent the set of true and false literals
respectively. Finally, the concept names Lit and Min are used to model minimal propositional
assignments; we need also an auxiliary role R.
First, literals are reified, i.e. modeled as individuals, with the axioms:
> v R.Li
Li u Lj v 
Li vn 
746

(1  i  2n)

(77)

(1  i < j  2n)

(78)

(1  i  2n)

(79)

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

The first axiom makes all Li nonempty. Axioms (78) make them pairwise disjoint. Finally, axioms
(79) minimize the concepts Li and make them singletons. Then, we represent S by adding for each
clause cj = lj1      ljh , 1  j  m, the axioms
Lji v Cj

(1  j  m and 1  i  h)

(80)

Cj vn 

(1  j  m)

(81)

> v R.(Cj u True) (1  j  m)

(82)

By axioms (80) and (81), Cj equals the set of (encodings of) literals in cj . Axioms (82) make sure
that each clause holds.
In order to model the concepts True and False and the correct meaning of complementary
literals we add the axioms
True u False v 

(83)

Pi u True v R.(Pi u False) (1  i  n)

(84)

Pi u False v R.(Pi u True) (1  i  n)

(85)

Pi u True v R.(Pi u False) (1  i  n)

(86)

Pi u False v R.(Pi u True) (1  i  n)

(87)

The axioms defined so far encode the classical semantics of S. To minimize models, add the following axioms:
Min u Pi v False (1  i  n)

(88)

Min u Pi v True

(1  i  n)

(89)

Li v Lit

(1  i  2n)

(90)

Cj v Lit

(1  j  m)

(91)

Lit vn Min

(92)

By (88) and (89), Min collects false positive literals and true negative literals. By (90) and (91),
Lit contains all the (representations of) literals and clauses. The purpose of these axioms is giving
defeasible inclusions (79) and (81) higher (specificity-based) priority than (92), so that model minimization cannot cause any Li to be larger than a singleton, nor any Cj to be different from the set
of literals of cj . Now (92) prefers those models where as many Pi as possible are in False.
In the following, given a PDLP S, let KB S be the Tbox defined above.
Lemma 7.18 Given a PDLP S, a literal l in Ss language, and the encoding L of l, the following
are equivalent:
(minimal entailment) S |=min l;
(subsumption) Circvar (KB S ) |= > v R.(True u L);
(co-sat) False u L is not satisfiable w.r.t Circvar (KB S );
(instance checking) Circvar (KB S ) |= (R.(True u L))(a).

This lemma can be proved by analogy with the proof of Lemma 6.5; the details are left to the reader.
The conjunctions (u) nested in  can be easily replaced with a new atom A by adding the
equivalence A  True u L, that can itself be encoded in LL EL , so we have:
747

fiB ONATTI , FAELLA , & S AURO

Theorem 7.19 Subsumption and instance checking over Circvar (LL EL ) are p2 -hard; concept
satisfiability is p2 -hard. These results hold even if the three reasoning tasks are restricted to LL
EL concepts, and priorities are specificity-based.
7.2 Left Local EL and Circfix
By a reduction of the validity problem for quantified Boolean formula, we can show that
Circfix (aLL EL ) is more complex than Circvar (aLL EL ), unless the polynomial hierarchy collapses. Computing the truth of a quantified Boolean formula
 = Q1 p1 . . . Qn pn .
(where the Qi s are quantifiers) can be reduced in polynomial time to subsumption checking in
Circfix (aLL EL ) as follows. Introduce concept names A0 , . . . , An , Ti and Fi for i = 1 . . . n, and
concept names Eij for 1  i < j  n. Introduce role names R, bad , good , and Ui for i = 1 . . . n.
We define a aLLEL knowledge base hK, K i, where K = KLL  Ka . The left-local part KLL
consists of the following groups of axioms. Notice that, in the following description, i is always an
arbitrary index in {1, . . . , n}. First, we encode the negation normal form  of . Let Bpi = Ti
and Bpi = Fi . For all subformulas F  G of  introduce a new concept name BF G and add the
inclusion BF u BG v BF G . For all subformulas F  G of  introduce a new concept name BF G
and add the inclusions BF v BF G and BG v BF G .
The second group of axioms of KLL constrains Ti and Fi to avoid inconsistencies. Intuitively
Ui means pi is undefined:
Ti u Fi v 

(93)

Ti u Ui v 

(94)

Fi u Ui v 

(95)

The third group of axioms of KLL defines a tree that encodes the truth assignments needed to
evaluate the QBF:
for all i 6= j

Ai u Aj v 

(96)

for all i s.t. Qi = 

Ai1 v R.(Ti u Ai ) u R.(Fi u Ai )

(97)

for all i s.t. Qi = 

Ai1 v R.Ai

(98)

The fourth group of axioms of KLL detects misrepresentations by forcing role bad to point
to the nodes of the evaluation tree where something is going wrong (i.e. the truth assignment is
incomplete, or some predicate changes value along a branch, or  is false in a leaf).

> vn bad .Ui

(99)

bad .Eij
bad .Eij

(101)

> vn bad .(B u An )

(102)

> vn
> vn

(Eij and Eij are defined in Ka below). Finally, good captures the absence of bad :
748

(100)

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

bad u good v  .

(103)

The acyclic terminology Ka has the only purpose of detecting whether some propositional symbol
changes its value along a path:
Eij

 Aj1 u Ti u R.(Aj u Fi )

(104)

Eij

 Aj1 u Fi u R.(Aj u Ti ) .

(105)

Lemma 7.20 Let I be a model of Circfix (KB) that satisfies A0 u good . For all i = 1 . . . n, all the
individuals in I are contained in either TiI or FiI .
Proof. First, by contradiction, assume that for some i = 1 . . . n and x  Ii , x is neither in FiI nor
in TiI . Since axioms (93)-(95) do not prevent x from satisfying Ui and I must be minimal w.r.t.
axiom (99), then the entire domain I satisfies bad .Ui . However, by axiom (103), this means
that A0 u good is unsatisfiable against the hypothesis.
2
Lemma 7.21 Let I be a model of Circfix (KB) that satisfies A0 u good . If for some i = 1 . . . n,
xi  (Ai u Ti )I (respectively, x  (Ai u Fi )I ), then all paths {xi , xi+1 , . . . , xn } such that xj  Aj
and (xj1 , xj )  RI , where i < j  n, are contained in TiI (resp., y  FiI ).
Proof. Assume that xi  (Ai uTi )I and {xi , xi+1 , . . . , xn } 6 TiI . This means that xi 6= xn and for
some i < j  n, xj1  (Aj1 u Ti )I and xj  (Aj1 u Fi )I . Then, by axiom (104), xj1  Eij
and since I must be minimal w.r.t. axiom (100) the entire domain I satisfies bad .Eij . However,
by axiom (103), this means that A0 u good is unsatisfiable against the hypothesis.
2
Theorem 7.22 Concept satisfiability, subsumption checking, and instance checking are PSPACEhard in Circfix (aLL EL ). The result still holds if the nesting level of existential restrictions is
bounded by a constant, and the priority relation is empty.
Proof. In order to prove the theorem it suffices to show that the QBF  is true iff A0 u good is
satisfiable w.r.t. the above KB.
[if ] Let I be a model of Circfix (KB) that satisfies A0 u good . Due to axioms (96)-(98), I must
contain a DAG that starts with x (which is in (A0 u good )I ) and, following the R-edges, proceeds
through the concepts Ai of increasing index, up to An . In this DAG, for all i = 1 . . . n such that
I and the other
Qi = , individuals belonging to a AIi have two successors: one in AIi+1  Ti+1
I . Individuals in AI , where Q = , have only one successor, in AI . Due to
in AIi+1  Fi+1
i
i
i+1
I or F I .
Lemma 7.20, such a successor is either in Ti+1
i+1
Now, consider any truth assignment v to the universally quantified variables of . In the DAG,
follow the unique path from x to a leaf z  AIn , that for each level i corresponding to a Qi = 
I or AI  F I in accordance with v. By Lemma 7.20, for all i = 1 . . . n
proceeds with AIi+1  Ti+1
i+1
i+1
z is in either Ti or Fi , moreover, by Lemma 7.21, membership of z in Ti or Fi is consistent with v.
Therefore, z represents a full truth assignment of the variables in  which extends v.
Now, since I minimizes the set of abnormal individuals w.r.t. the defeasible inclusion (99)
and in all models good and bad are disjoint, x  good I implies that z 6 BI . But then, it is
straightforward to conclude that this truth assignment satisfies .
749

fiB ONATTI , FAELLA , & S AURO

[only if ]. Assume that  is true. Assume w.l.o.g. that odd quantifiers Q1 , Q3 , . . . , Qn1 are universal and even quantifiers are existential. For each existential quantifier Qi , let fi : {T, F }i/2 
{T, F } be the function such that for all values v1 , v3 , . . . , vn1 of the universally quantified variables, (v1 , f2 (v1 ), v3 , f4 (v1 , v3 ), . . . , fn (v1 , v3 , . . . , vn1 )) is true.
We define a tree-like model I of KB that satisfies A0 u good . We start with a root individual
x, such that x |=I A0 u good . We proceed inductively as follows. For even i (including 0), each
individual y  AIi has two R-successors y 0 , y 00  AIi+1 , such that y 0  TiI and y 00  FiI (see
axioms (97)). For odd i, each individual y  AIi has one R-successor y 0  AIi+1 (see axioms (98)),
such that y 0 satisfies either Ti or Fi , according to the value of fi when applied to the truth values
that can be read along the path from the root to y. Along each R  path x0 . . . xi . . . xn the same
concept Ti or Fi assigned to xi is assigned to all xj , with i < j  n, and indifferently either Ti or Fi
is assigned to the xh with 1  h  i. The model is completed by assigning to xn (i) BF G , for all
subformulas F  G of , such that F and G are assigned to xn , and (ii) BF G , for all subformulas
F  G of , such that F or G are assigned to xn .
We leave to the reader the proof that the structure just defined satisfies the classical part of
KB. Regarding minimality w.r.t. the defeasible inclusions in KB, we remark the following. All
the individuals violate inclusions (99). However, due to rules (94) and (95), the situation cannot
be improved by simply modifying the roles. Similarly, all the individuals violate inclusions (100)(101). However, since the Eij are all empty these defeasible inclusions cannot be improved.
Finally, since each leaf z  An represents a truth assignment that satisfies , then B is empty
and hence our model is also minimal w.r.t. the inclusion (102).
2
The LL fragment of EL , unlike LLf , does not fully support unqualified existential. Consequently, Theorem 4.4 cannot by used to transfer the hardness results of Theorem 7.19 from var to
fix.9 The above hardness results hold only for the more general framework Circvar (LLf EL ) and
hence, by Theorem 4.4,for Circfix (LLf EL ):
Proposition 7.23 Subsumption and instance checking over Circfix (LLf EL ) are p2 -hard; concept satisfiability is p2 -hard. These results hold even if queries contain only LL EL concepts, and
priorities are specificity-based.
The following result, whose proof can be found in the Appendix, shows a context in which the
above lower bounds are tight: namely, the case in which the priority relation is empty (i.e., DIs are
mutually incomparable) and, for subsumption queries C v D or instance checking queries D(a),
the quantifier depth of D is bounded by a constant.
Lemma 7.24 Let KB = hKS KD , i be an LLf EL knowledge base, and C, D be EL concepts.
For all models I  Circfix (KB) and for all x  C I \ DI there exists a model J  Circfix (KB) such
that (i) J  I , (ii) x  C J \ DJ (iii) |J | is O((|KB| + |C|)d ) where d = depth(D).
Going back to the LL fragment, in the following we prove that Circfix is less complex than Circvar
(unless the polynomial hierarchy collapses). In particular, we show that Circfix (LLEL ) is tractable.
Algorithm 1 takes as input a knowledgedbase KB and two concepts C and D (we may assume
without loss of generality that C = AC u ni=1 Pi .Bi ) and checks whether Circfix (KB) |= C v D.
9. We will prove below that in LL EL , Circfix is actually less complex than Circvar .

750

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Algorithm 1:
dn
Data: C = AC u 1 Pi .Bi , D, KB = hK, i.
A := {A vn P.B | C |= A};
X := C;
while A 6=  do
remove from A a defeasible inclusion A vn P.B;
if SupCls(P.B)  SupCls(C) and NonEmpty(P.B, KS )  NonEmpty(C, KS ) then
X := X u P.B;
return X vKS D;

With SupCls(H) we mean the set of superclasses of a concept H, i.e., the set of B  NC  {}
such that H vKS B.
Given a concept H, the operator NonEmpty(H, KS ) represents the set of concepts that are
forced to be non empty whenever H is. Note that this set includes some concepts that are forced to
be non empty by the ABox in KB, independently from H. We write H ; A iff H vKS R.A for
+
some R, and we denote by ; the transitive closure of ;. Then, NonEmpty(H, KS ) is formally
defined as follows:
S
S
NE Kernel = {H}  aNI {A | KS |= A(a)}  aNI ,RNR {A | KS |= (R.A)(a)}
S
+
NonEmpty(H, KS ) = ANE Kernel {A0 | A ; A0 }.
Roughly speaking, the algorithm accumulates the RHS of defeasible inclusions actively satisfied
by a witness of C. Then, it tries to derive D. In particular, a defeasible inclusion A vn R.B is
actively satisfied just in the case (i) does not entail locally  or a concept name not subsumed by
C, and (ii) does not entail globally the non-emptiness of a concept name that should be empty. The
rationale is that concept names are fixed and circumscription cannot change their extension as the
application of A vn R.B could instead require.
Lemma 7.25 Circfix (KB) |= C v D holds iff Algorithm 1 returns true.
Proof. [if ] It suffices to show that for all models of Circfix (KB), X subsumes C, where X is the
formula obtained after the while statement. Assume per absurdum that for some model I and an
individual x  I , x  C I \ XI . This means that for some defeasible inclusion A n P.B (i) x 6
satI (A n P.B) and (ii) from line 1, SupCls(P.B)  SupCls(C) and NonEmpty(P.B, KS ) 
NonEmpty(C, KS ).
Note that, since NonEmpty(P.B, KS )  NonEmpty(C, KS ), whenever R.B vKS S.B
there exists an individual yB  B I . Let I 0 be the interpretation obtained from I by adding all such
(x, yB ). Clearly, by adding new arcs the set of individuals that satisfied a defeasible inclusion 
0
cannot decrease, therefore for all   KD , satI ()  satI 0 (). Moreover, since x  (R.B)I ,
satI (A n P.B)  satI 0 (A n P.B) and hence I 0 <fix I.
From condition (ii) and the fact that defeasible inclusions do not conflict with each other, it is
easy to verify that I 0 is also a classical model of KB, but this would mean that I is not a model of
Circfix (KB) against the hypothesis.
[only if ] Assume that Algorithm 1 returns false. Let I be the following interpretation:
 I = {xC }  {xA | A  NonEmpty(C, KS )}  {xa | a  NI };
751

fiB ONATTI , FAELLA , & S AURO

 for all B  NC , B I is the union of: (i) {xC } if B  S1 , (ii) the set of xA , with A  S2 , such
that A |=KS B, and (iii) the set of xa , with a  NI , such that KS |= B(a);
 for all R  NR , RI is the union of the pairs (i) (xA , xB ) where A, B  S2 and A vKS R.B,
(ii) (xa , xb ) where a, b  NI and KS |= R(a, b), (iii) (xa , xB ) where a  NI , B  S2 and
KS |= R.B(a), (iv) (xC , xB ) where B  S2 and X vKS R.B; other arcs are not relevant.
By construction I is a (classical) model of KS and xC  C I \ DI , hence in order to prove I 
Circfix (KB) it remains to show that I is minimal. Note that, since defeasible inclusions do not
contain nested roles on the right side, the set of defeasible inclusions satisfied by an individual does
not affect the set of defeasible inclusions satisfied by another individual. Therefore, an interpretation
can be improved point-wise and we can assume w.l.o.g. that all the individuals in I, except xC ,
cannot be further improved. Assume now that there exists an interpretation J that improves I in
xC , this means in particular that for some  = A vn P.B, xC  satJ () \ satI ().
The assumption xC 6 satI () means that P.B does not satisfy the condition in line 1 and,
since concept names are fixed,  cannot be satisfied in J .
2
Theorem 7.26 In Circfix (LL EL ) DKBs, LL EL subsumption, instance checking, and concept
consistency are in P.
Proof. Since SupCls(H) and NonEmpty(H, KS ) are based on classical reasoning, they can be
performed in polynomial time. Moreover, the number of iterations in Algorithm 1 is bounded by
the number of defeasible inclusions. Therefore, due to Lemma 7.25, the subsumption problem is
tractable. By Theorem 3.9, instance checking and concept inconsistency can be reduced to subsumption.
2
Complexity is low under Circfix because in this context LL axioms are not general enough to simulate quantifier nesting nor conjunctions of existential restrictions. In Circvar these features can be
simulated by abbreviating compound concepts C with concept names A using equivalences A  C
such that C does not depend on qualified existentials (hence the LL restriction is preserved). With
Circfix , such equivalences change the semantics of C whenever C is (or contains) an existential
restriction, because A is fixed and prevents C from varying freely. As we reintroduce the missing
features, complexity increases again.
Let LL2 EL support the schemata:
A v[n] P.B

P1 u P2 v P3 .B

P v B

One may easily verify that LL2 EL is equivalent to LL EL plus schema P1 u P2 v P3 .B.
The missing axioms can be reformulated using fresh roles R and suitable equivalences R  C
(that preserve Cs semantics because R is a varying predicate)10 .
With these additional schemata, one can create conflicts between variable concepts, as in P1 u
P2 v . Then different defeasible inclusions may block each other, thereby creating a potentially
exponential search space.
Theorem 7.27 Subsumption and instance checking over Circfix (LL2 EL ) are coNP-hard; concept satisfiability is NP-hard. These results hold even if the three reasoning tasks are restricted to
LL2 EL concepts, and priorities are specificity-based.
10. In particular, schema A1 uA2 v B can be emulated by the inclusions A1 v R1 , A2 v R2 , B v R3 , R1 v A1 ,
R2 v A2 , R3 v B, and R1 u R2 v R3 .

752

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Proof. By reduction of SAT. For each propositional variable pi introduce the concept names Ai , Ai ,
and role Ui , representing pi s truth value (resp. true, false, and undefined). These alternatives are
made mutually inconsistent with:
Ai u Ai v 

Ai u Ui v 

Ai u Ui v 

For each given clause cj = lj,1      lj,n , introduce a concept name Cj representing cj s falsity,
and for each Lj,k representing the complement of lj,k add Lj,1 u    u Lj,n v Cj .
Define a concept name F representing the falsity of the given set of clauses, and a disjoint
concept F with:
Cj v F

(for all input clauses cj )

F u F v  .

Now, with a defeasible inclusion, Ui is forced to be true for all individuals that satisfy neither Ai
nor Ai ; moreover, a role U detects undefined literals:
> vn Ui

Ui v U .

Let K be the above set of inclusions and KB = hK, K i. It can be proved that the given set of
clauses S is unsatisfiable iff Circfix (KB) |= F v U , therefore subsumption checking is coNPhard.
Similarly, it can be proved that S is unsatisfiable iff Circfix (KB 0 ) |= (U )(a), where KB 0 =
hK0 , K0 i and K0 = K  {F (a)}; therefore instance checking is coNP-hard.
Finally, it can be proved that S is satisfiable iff F uOK is satisfiable w.r.t. Circfix (KB 00 ), where
KB 00 = hK00 , K00 i and K00 = K  {U u OK v }; therefore satisfiability checking is NP-hard.
We are only left to remark that K can be easily encoded in LL2 EL .
2
We prove that this bound is tight using Algorithm 2. The algorithm non-deterministically looks for
an individual x (in some model) that satisfies C and not D. S1 guesses any additional fixed concept
names satisfied by x; S2 guesses the concept names that are satisfied somewhere in the model (not
necessarily by x) and finally 0 guesses a total extension of  that determines the application order
of GDIs.
Similarly to Algorithm 1, Algorithm 2 selects the defeasible inclusions that are active in x and
accumulates in the formula X the RHS of those that are not blocked, i.e. do not require to change the
interpretation of the concept names neither locally nor globally. The major differences are that (i)
defeasible inclusions are extracted according to 0 and (ii) in line 2 the entire accumulated formula
X u P.B is used to check that a defeasible inclusion is not blocked.
d
Finally, note that the variable part of C (i.e. ni=1 Pi .Bi ) is introduced in X only in line 8, after
all defeasible inclusions have been applied, because defeasible inclusions can influence the variable
part (e.g. by forcing it to be empty).
Lemma 7.28 Circfix (KB) |= C v D holds iff all the runs of Algorithm 2 return true.
Proof. [if ] Assume per absurdum that there exists an interpretation I  Circfix (KB) and an individual x  I such that x  C I \ DI . Let S1 and S2 be the set of concept names in NC that I
satisfies respectively locally in x and globally  i.e., for some individual. Let 0 be a linearization of
 compatible with I, i.e. for all ,  0  KD (i) either  0  0 or  0 0 , (ii)    0 implies  0  0 ,
(iii) if  and  0 are not comparable according to  ( 6  0 and  0 6 ) and x  satI () \ satI ( 0 ),
then  0  0 .
753

fiB ONATTI , FAELLA , & S AURO

Algorithm 2:
dn
Data: C = AC u 1 Pi .Bi , D, KB = hK, i.
Guess S1 , S2  NC , where uS1 |= AC and S1  S2 , and a linearization 0 of ;
A := {A
d vn P.B | uS1 |= A};
X := S1 ;
while A 6=  do
remove from A the 0 -minimal inclusion A vn P.B;
if SupCls(X u P.B)  S1 and NonEmpty(X u P.B, KS )  S2 then
X := X u P.B;
dn
X := X u 1 Pi .Bi ;
return SupCls(X) 6 S1 or NonEmpty(X, KS ) 6 S2 or X vKS D;

Let X be the result of running Algorithm 2 on guesses S1 , S2 , and 0 . It is straightforward to
see that for all  = A vn P.B  KD such that P.B occurs in X, x  satI (). This, together
with the fact that x  C I , implies that 1) SupCls(X)  S1 ; 2) NonEmpty(X, KS )  S2 and, since
x 6 DI , 3) X 6vKS D. But this means that on this run Algorithm 2 should return false.
[only if ] Assume that for some guess of S1 , S2 and 0 Algorithm 2 returns false. Let I be
defined as in Lemma 7.25. In a similar way it can be proved that I is a classical model of KS and
xC  C I \ DI . Assume now that I is improved by an interpretation J , w.l.o.g. we can also assume
that (i) for some  = A vn P.B, xC  satJ () \ satI () and (ii) for all  0 with higher priority
than  or not comparable with it, we have xC  satJ ( 0 ) iff xC  satI ( 0 ).
If X0 is the value of X when  is extracted on line 2 of Algorithm 2, since  0   implies  0 0 ,
all the  0 already extracted have higher priority or are not comparable with . Since (ii) holds and by
construction xC  X0I , xC  X0J . However, the assumption xC 6 satI () means that X0 u P.B
does not satisfy the condition in line 2 and since concept names are fixed  cannot be satisfied in J .
2
Theorem 7.29 LL2 EL subsumption and instance checking over Circfix (LL2 EL ) are in coNP;
LL2 EL concept satisfiability is in NP.
Proof. It is analogous to Theorem 7.26 and left to the reader.

2

It can be verified that the LL2 fragment does not support quantifier nesting. With quantifier nesting,
one would obtain LLf EL (i.e. full LL).

8. Related Work
DLs have been extended with nonmonotonic constructs such as default rules (Straccia, 1993; Baader
& Hollunder, 1995a, 1995b), autoepistemic operators (Donini et al., 1997, 2002), and circumscription (Cadoli, Donini, & Schaerf, 1990; Bonatti et al., 2009, 2009; Bonatti, Faella, & Sauro, 2010).
An articulated comparison of these approaches can be found in the work of Bonatti, Lutz, and
Wolter (2009).
Most of these approaches concern logics whose reasoning tasks complexity lies beyond PH
(unless the hierarchy collapses). For example, the logics considered by Donini et al. (1997, 2002)
range from PSPACE to 3-ExpTime. The circumscribed DLs studied by Cadoli et al. (1990) as well
754

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Table 1: Main complexity results. The corresponding decision problems for the classical versions
of the considered logics are solvable in polynomial time.

DL-liteR
var fix(\)
concept
satisfiability

p2

 p2

EL

EL
var

fix

var
LL  aLL ()

LL

LL2

fix
LLf

aLL

p2

P

NP

 p2 (#)

PSPACE

trivial

subsumption
& instance
p2  p2
P ExpTime
p2 ()
P co-NP
 p2 (#)
checking
() with specificity-based priorities; general priorities make var at least as complex as fix
() if quantifier nesting is bounded in the r.h.s. of subsumptions and in instance checking problems
(\) if DIs are left-fixed or the priority relation is empty
(#) membership holds if the priority relation is empty and condition () holds

PSPACE

as Bonatti et al. (2009) range from NPNExp to NExpNP . Some logics are undecidable (Baader &
Hollunder, 1995a; Bonatti et al., 2009).
A pioneering approach to low-complexity, circumscribed description logics was presented by
Cadoli et al. (1990). That approach applies non-prioritized circumscription to a fragment of the
description logic ALE. Decidability of reasoning is shown by a reduction to propositional reasoning under the Extended Closed World Assumption (ECWA), which is in p2 . To the best of our
knowledge, that was the first effective reasoning method for a nonmonotonic description logic.
A hybrid of Circfix (EL ) and closed world assumption has been proved to be in PTIME (Bonatti
et al., 2010). On the one hand, that approach imposes less restrictions on the structure of inclusions;
on the other hand, it cannot be fully extended to variable predicates without affecting tractability.
A recent approach that is similar in spirit to circumscription has been taken by Giordano et
al. (2008). They extend ALC with a modal operator T representing typicality, and maximize T s
extension to achieve nonmonotonic inferences. Decidability is proved via a tableau algorithm that
also establishes a co-NExpTimeNP upper bound for subsumption. No matching lower bounds are
given; it is proved that reasoning in the underlying monotonic logic is NP-hard.
Finally, an approach based on rational closures and ALC can be found in the work of Casini and
Straccia (2010). An appealing feature of this approach is that reasoning can be reduced to classical
inference. Complexity is not increased by nonmonotonic reasoning: it ranges from PSPACE to
ExpTime.

9. Discussion and Future Work
The main complexity results of this paper are summarized in Table 1. By restricting circumscribed
KBs to Circvar (DL-liteR ), complexity decreases significantly (from (co)-NExpTimeNP to the second level of PH). The same complexity upper bounds hold in Circfix (DL-liteR ) whenever the priority
755

fiB ONATTI , FAELLA , & S AURO

relation is empty or the defeasible inclusions admit only concept names on the LHS. However, the
general case is still an open question.
On the contrary, restricting the language to EL or EL does not in general suffice to bring complexity within PH. In particular, it can be proved that reasoning tasks are undecidable in CircF (EL)
(i.e., when roles can be fixed) and that reasoning in Circfix (EL) and Circvar (EL ) is in general
ExpTime-hard.
The main source of the higher complexity of the EL family (w.r.t. DL-liteR ) has been identified
by introducing a further restriction called full left locality (LLf ) that suffices to confine complexity within the second level of PH under Circvar with specificity-based priorities, provided that the
quantifier nesting level in subsumption queries and instance checking queries is suitably bounded
(no restrictions are needed on concept satisfiability).
Since the left locality restriction rules out acyclic terminologies (which are commonly used in
ontologies), it has been relaxed to almost left local (aLL) knowledge bases, that support acyclic
terminologies with the restriction that unfolding (i.e., the process of replacing the atoms defined
in the acyclic terminology with their definition) should yield a LLf knowledge base. Reasoning
becomes PSPACE-hard, in general; however in the aLL fragment of Circvar (and under the same
assumptions needed for LLf ), reasoning remains complete for the second level of PH. In particular,
the assumption that the priorities are determined by specificity is essential: By Theorem 4.5, general
priorities make Circvar at least as complex as Circfix , that is, PSPACE-hard.
We have also analyzed the complexity of several fragments lying between LL and aLL under
Circfix . These results provide some further information about the complexity sources in circumscribed DLs. For example, quantifier nesting in the KB is partially responsible for complexity
(presumably because it enables conflicts between the default properties of different individuals): in
particular, by removing quantifier nesting (i.e., by restricting KBs to the LL2 fragment) complexity drops to the first level of PH. The other source of complexity, of course, is due to the conflicts
between defeasible inclusions concerning each individual in isolation; in Circfix (LLEL ) a defeasible inclusion can never block another inclusion (because fixed predicates prevent this) andas a
consequencecomplexity drops within PTIME.
We have also proved that in all fragments that fully support unqualified existential restrictions,
variable concept names can be eliminated. Moreover, in EL and its various left local fragments,
compound concepts can be replaced with concept names in the left-hand side of defeasible inclusions, without affecting expressiveness. In the same fragments, general priorities can be simulated
using only specificity-based priorities.
We have to leave several interesting questions open: First, it is not clear whether general priorities are necessary to the hardness results for DL-liteR ; in particular, it would be interesting to establish the exact complexity of DL-liteR with specificity-based priorities. Other gaps in the complexity
of circumscribed DL-liteR concern the complexity of Circfix with unrestricted GDIs or nonempty
priority relations, and the complexity of reasoning with fixed roles. The next interesting question is
whether the bound on quantifier nesting in the queries is actually needed to confine complexity of
circumscribed EL within the second level of PH. Finally, there is no exact charcterization of the
complexity of Circfix (LLf EL ) and of the fragments whose complexity lies beyond PH.
For the fragments that do belong to the second level of PH, we see an interesting opportunity
of encoding reasoning in ASP and use some well-engineered engine such as DLV (Eiter, Leone,
Mateis, Pfeifer, & Scarcello, 1997) to test scalability. In order to evaluate implementations experimentally, it is necessary to set up suitable benchmarks that, in a first stage, must necessarily be
756

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

synthetic problems, since nonmonotonic KBs have not been supported so far. Of course, identifying
meaningful criteria for problem generation is a nontrivial issue. Therefore, systematic experimental
evaluations still require a significant body of work.

Acknowledgments
The authors wish to thank Frank Wolter for granting the permission to publish his undecidability
proof. Moreover, they are grateful to Frank Wolter and Carsten Lutz for many stimulating discussions and feedback. This work has been carried out in the framework of project LoDeN, that has
been (very) partially supported by the Italian Ministry for Research.

Appendix A. Additional Lemmas and Proofs
A.1 Proofs for Section 6
Lemma 6.3. Given a PDLP S over P V = {p1 , . . . , pn } and a truth assignment I  P V , I
is a minimal model of S iff the interpretation model (S, I, ) is a model of Circvar (KB S ), for all
domains  with || = 2n + 1.
Proof. [only if ] Let I = model (S, I, ), we first show that I is a model of the classical part of
KB S . Since I-V, the Abox (18) and all axioms (20-24) are satisfied. Whereas, axioms (33-36)
follow directly from VII.
Since I is an interpretation, VI assures that if TrueP I 6= , then False P I 6= . Together with
VIII, axioms (31-32) are satisfied, whereas together with VII, True and False reflect in I the truth
values of I; therefore True I  False I =  and hence axiom (30) is satisfied. Moreover, as I is a
model of S, for each c  S there exists at least one literal li occurring in c such that I |= li . Due
to V-VII, CjI  True I 6=  and, due to VII and VIII (where X = True and Y = Cj ), I satisfies
axioms (26-29).
It remains to prove that there exists no interpretation J such that J <var I. As I is finite,
we can assume w.l.o.g. that J is a model of KB S . Assume that for some 1  i  2n, satI (Li vn
I
I
Li )  satJ (Li vn Li ), this is equivalent to saying that LJ
i  Li , but since Li is a singleton,
J
Li would be empty contradicting axioms (18-23). Similarly, satI (Cj vn Cj )  satJ (Cj vn
Cj ) iff CjJ  CjI . Thus, due to V some axiom Lji v Cj would not be satisfied in J . Therefore
the defeasible inclusions with highest priority cannot be improved.
J
I
I
Now assume that for each literal and clause concept it holds LJ
i = Li = {di } and Cj = Cj .
Since I reflects the truth values of I, all the di s that are not included in FalseLIi are included in
TrueLIi . Thus, if for all 1  i  2n, satI (Pi vn FalsePi ) were equal to satJ (Pi vn FalsePi )
then there would be no way for J to improve a defeasible inclusion Pi vn TruePi . Therefore, the
only possibility so far is that J improves some instance of (40).
S
Note that True J and False J are a partition of i PiJ . Otherwise, we could set a PiJ without
truth value (i.e., di 6 TruePiJ  FalsePiJ ) to FalsePi  since no classical inclusion is jeopardized
we would obtain an improvement of J according to (40), against the hypothesis that J is a model.
Due to (31-36), True J and False J are a partition of all {d1 , . . . , d2n }.
Thus, we can consider the propositional assignment J such that pi  J iff PiJ  True J .
First, for all clauses cj , since J satisfies axioms (26-29), for some 1  i  2n we have
J
J
J
J
I
I
LJ
i  Cj  True . As Li = Li and Cj = Cj , ljl occurs in cj . But this means that J |= li , and
757

fiB ONATTI , FAELLA , & S AURO

hence
J is a model of S. Finally, as said before,
the intersection of
S I J |= cj . Thus,
Sif JJ <var I, then
I is strictly contained in the intersection of
J . This implies that
P
and
False
P
and
False
i i
i i
J  I, against the hypothesis that I is a minimal model.
[if ] Assume that I = model (S, I, ) is a model of Circvar (KB S ). First we show that I is a
model of S, i.e., for all cj  S, I |= cj . As I satisfies axioms (26-29), for some 1  i  2n it holds
di  CjI  True I . Due to IV and V, di belongs to some LIi such that the corresponding literal li
occurs in cj . According to VI, this implies that I |= li , and hence I |= cj .
It remains to show that I is a minimal model. Assume that there exists a model J of S such that
J  I, without loss of generality we can assume that J is minimal model. Let J = model (S, J, ),
from the above arguments J is a model of KB S . Furthermore, satJ () = satI () for each  of
J
J
type
S ILi vn LI i orSCj Jvn Cj . JFinally, as True and False reflect the truth values of J,
i Pi  F alse 
i Pi  F alse and hence J <var I due to the improvement of DIs (40). 2
Lemma 6.4. If I is a model of Circvar (KB S ), then there exist a minimal model I of S such that
pi  I iff PiI  True I iff PiI  False I , for all i = 1, . . . , n.
Proof. Let I be a model of Circvar (KB S ). First, we show that LIi is a singleton, for all 1  i  2n.
Assume the contrary. Clearly, to satisfy (1821), each LIi has to be nonempty. Therefore, for some
1  k  n, LIk contains at least two individuals. We will show that there exists an interpretation I 0
that improves I.
For all 1  i  2n, let di be an arbitrary element of LIi , and let  = {d1 , . . . , d2n }  {aI }. As
the LIi are disjoint with each other (see axioms (22)) and NonEmpty I is disjoint with any LIi , we
have || = 2n + 1.
 ). Let I 0 be
All PDLP are satisfiable, thus there exists a model I of S. Let Ib = model (S, I,
0
0
I
I
I
I
I
an interpretation such that: (i)  =  , (ii) for all roles R, R =    , (iii) I 0 coincides
with Ib on  with respect to all concept names, and (iv) all the other individuals d  I \  do not
belong to any concept name. It is straightforward to see that I 0 satisfies the classical part of KB S .
0
0
Furthermore, by construction, (a) for all 1  i  2n, LIi  LIi ; (b) for all 1  j  m, CjI  CjI ;
0
(c) for some 1  l  2n, LIl  LIl . Thus, I 0 <var I, due to the improvement of DI (23).
By the above argument, we have LIi = {di }. Define the truth valuation I = {pi | di  True I }.
It remains to prove that I is a minimal model of S. The fact that I is a model of S is ensured by
axioms (2429). Then, assume by contradiction that there exists a model J of S that is smaller
than I (i.e., J  I), and let J = model (S, J, ). From J we can build an interpretation J 0 with
0
J = I and such that J 0 is a classical model of KB S and J 0 <var I, thus contradicting the
hypothesis that I is a model of Circvar (KB S ). We define J 0 by copying from J all the properties
(concepts and roles) of the individuals in J = , and by leaving all the individuals in I \  out
of concept and role extensions.
2
A.2 Proofs for Section 7
Given a KB K, an interpretation I, and an individual z, recall the definition of KB [z] from Section 6.2. Redefine the notion of support as follows: supp I (C) is the set of individuals z  I
such that > vKB[z] C holds.
Lemma 7.12. Let KB = hK, K i be an LLf EL knowledge base, and C, D be EL concepts. For
all models I  Circvar (KB) and for all x  C I \ DI there exists a model J  Circvar (KB) such
that (i) J  I , (ii) x  C J \DJ , and (iii) |J | is O((|KB|2 +|C|)d ), where d = depth(D)+1.
758

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Proof. Given two individuals x and y in I , the distance d(x, y) is the minimal length of role-paths
from x to y. Let KB  be the knowledge base obtained from KB by applying the transformation
presented in Section 7. Notice that |KB  |  |KB|2 .
By Lemma 7.10, I can be extended into a model of Circvar (KB  ), which we continue to call I
for convenience. We define a small model J of Circvar (KB  ) such that x  C J \ DJ . Then, we
obtain the thesis by Lemma 7.11.
We start from an initial domain J that contains (i) x; (ii) all aI , where a  NI occurs in KB  ;
(iii) for all concepts H  cl(KB  )  cl(C) such that H I 6= , a witness yH  H I ; (iv) for all
concepts H  cl(KB  )  cl(C) such that supp I (H) 6= , a witness wH  supp I (H).
We expand J by exhaustively applying the following rule (where P is a special case of P.H
with H = >):
Let y  J and P.H  cl(KB  )  cl(C) be such that y  (P.H)I and y 6 (P.H)J . If
d(x, y) < d, then add z to J and (y, z) to P J , where z is such that (y, z)  P I and z  H I .
Otherwise, add (y, yH ) to P J .
Finally, for each concept name A, set AJ = J  AI .
With respect to the cardinality of J , note that initially the number of individuals in J is
O(|KB  | + |C|). During the expansion, for each individual whose distance from x is less than d, at
most O(|KB  | + |C|) new individuals are added. This means that |J | = O((|KB  | + |C|)d ) =
O((|KB|2 + |C|)d ).
By construction for each individual y  J and H  cl(KB  )cl(C) if y  H I , then y  H J .
In particular, in case H = P , also the inverse holds, if y  P J , then y  P I . From the previous
two facts it immediately follows that J is a classical model of KB  and x  C J . Moreover, since
up to a distance d from x, P J is contained in P I , for all P  NR , it is easy to see that x 6 DJ .
It remains to show that J is minimal. Assume by contradiction that for some classical model
0
J of KB  , it holds J 0 <var J , we show that there exists a classical model I 0 of KB  such that
I 0 <var I  against the hypothesis that I  Circvar (KB  ).
We distinguish two cases: in the first cas, all individuals wH introduced in clause (iv) still satisfy
the corresponding concept H in J 0 ; in the second case, at least one wH does not satisfy its concept
H. We define I 0 as follows. In both cases, individual names are interpreted as in I and concept
names for individuals in J are interpreted as in J 0 .
0
In the first case, an individual z  I \ J satisfies a concept name A, that is z  AI , if and
0
only if z  supp I (A). Moreover, for each P  NR , P I is the minimal set such that:
0

0

1. P J  P I ;
0

0

2. if z  I \ J and z  supp I (P.H), and y  H J then (z, y)  P I .
We prove that I 0 is a classical model of KB  . Since I 0 is a copy of J 0 over J and J 0 is
a classical model, we only need to show that the individuals z  I \ J satisfy all the strong
inclusions in KS . Note that if z satisfies in I 0 the LHS H of a strong inclusion, then z supports H
0
in I. By definition, z supports also the RHS in I. If the RHS is a concept name B, then z  B I
0
by construction. Otherwise, i.e., if the RHS is P.H, by step 2 above, it suffices to show that H J
is not empty. However, by assumption, the witness of P.H introduced in clause (iv) still satisfies
P.H in J 0 . Therefore, there exists an individual y satisfying H in J 0 .
Next, we prove that I 0 <var I. Since J 0 <var J , it suffices to show that an individual z 
I
 \J satisfies in I 0 all the defeasible inclusions it satisfies in I. Assume that a DI  = (A vn B)
759

fiB ONATTI , FAELLA , & S AURO

0

is satisfied by z in I. If z  AI , then by construction z  supp I (A). Clearly, if z  supp I (A)
0
and z  satI (A vn B), then z  supp I (B). Therefore, z  B I .
We are left to prove the theorem for the second case, i.e.: at least one wH does not satisfy its
concept H. Clearly, wH does not support H in J 0 anymore. In particular, there must be a DI  such
that wH  satJ () \ satJ 0 (). From J 0 <var J , it follows that there must be a DI  0 such that
 0 K  and satJ ( 0 )  satJ 0 ( 0 ). Now, in I 0 we can safely violate all DIs whose priority is lower
than  0 , and in particular all DIs whose LHS classically subsumes >. Then, complete the definition
of I 0 as follows. Each basic concept A holds in an individual z  I \ J if and only if > vKS A.
0
For each P  NR , P I is the minimal set such that
0

0

1. P J  P I ;
0

0

2. if z  I \ J , > vKS P.H, and y  H J then (z, y)  P I .
It is easy to verify that I 0 is a classical model of KB  . In order to prove that I 0 <var I, note that the
following two facts hold.
First, an individual z  I \J satisfies all the DIs whose priority is not minimal. Assume that
1 K 2 , for some DIs 1 and 2 , this means that the LHS of 2 subsumes the LHS of 1 but not
the vice versa. Then, the LHS of 1 does not subsume > and hence, by construction, z vacuously
satisfies 1 .
Second, if z violates a DI  00 = (A vn B), then  0 K  00 . As before, since  0 K , its LHS
0
does not subsume >. However, since z violates  00 , z  AI and hence A subsumes >. Therefore,
 0 K  00 .
From the first fact it immediately follows that satI ( 0 )  satI 0 ( 0 ). Assume now, that for some
 00 , satI ( 00 ) 6 satI 0 ( 00 ). If there exists an individual z  I \ J such that z  satI ( 00 ) \
satI 0 ( 00 ), then by the second fact  0 K  00 . Otherwise, there must exist an individual w  J
such that w  satI ( 00 ) \ satI 0 ( 00 ). However, in J the set of DIs that an individual satisfies in I
(resp. I 0 ) is the same set of DIs it satisfies in J (resp. J 0 ). This means that there exists a defeasible
inclusion  000 such that  000 K  00 and satJ ( 000 )  satJ 0 ( 000 ). Due to the first fact,  000 is satisfied
in all I \ J , and hence satI ( 000 )  satI 0 ( 000 ).
2
Proposition 7.13. Let KB = hKLL  Ka , i be an aLL EL knowledge base. Every model of
CircF (unf(KB)) can be extended to a model of CircF (KB).
Proof. Let Kunf = unf(KB) = hK0 , 0 i. Let I be any model of CircF (Kunf ). Extend it to a
classical model J of K0  Ka by setting AJ = DJ for all definitions A  D in Ka . Note that
K0  Ka is classically equivalent to KLL  Ka . Now suppose that J is not a model of CircF (KB).
Since by construction J is a classical model of Ka and the strong axioms of KLL , there must be a
classical model J 0 of the same axioms such that J 0 <F J . By restricting J 0 to primitive predicates
(i.e., predicates that are not defined in Ka ), we obtain a classical model I 0 of K0 . Note that for all
defeasible inclusions   KLL , it holds satJ 0 () = satJ 0 (unf(, Ka )) = satI 0 (unf(, Ka )) and
satJ () = satJ (unf(, Ka )) = satI (unf(, Ka )). It follows that I 0 <F I, a contradiction.
2
Proposition 7.14. Let KB = hKLL  Ka , i be an aLL EL knowledge base and suppose that all
the concept names defined in Ka are variable. Then, for all models I of CircF (KB), the restriction
of I to primitive predicates is a model of CircF (unf(KB)).
760

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Proof. Let Kunf = unf(KB) = hK0 , 0 i. Let J be the restriction of I to primitive predicates.
In other words, J is obtained from I by dropping the interpretation of all concept names defined
in Ka , which are all variable. It can be easily verified that J is a classical model of K0 . Suppose
by contradiction that J is not a model of CircF (Kunf ); then there exists a classical model J 0 of
0
0
K0 such that J 0 <F J . Now extend J 0 to a model I 0 of KLL  Ka by setting AI = DI for
all definitions A  D in Ka . Since the predicates defined in Ka are variable, all fixed predicates
preserve their extensions across I, J , J 0 , and I 0 . Moreover, for all defeasible inclusions   KLL ,
we have satI 0 () = satI 0 (unf(, Ka )) = satJ 0 (unf(, Ka )) and satI () = satI (unf(, Ka )) =
satJ (unf(, Ka )). It follows that I 0 <F I, a contradiction.
2
Given a knowledge base KB, an interpretation I and a concept D, again we have to override
the notion of support; supp I (D) is the set of z  I such that
 l 
A vKB[z] D .
zAI

Clearly, if I is a classical model of KB and z  supp I (D), then z  DI .
Lemma 7.24. Let KB = hKS KD , i be an LLf EL knowledge base, and C, D be EL concepts.
For all models I  Circfix (KB) and for all x  C I \ DI there exists a model J  Circfix (KB) such
that (i) J  I , (ii) x  C J \ DJ (iii) |J | is O((|KB| + |C|)d ) where d = depth(D).
Proof. Define the small model J as in the proof of Lemma 7.12, using the new definition of
support. Regarding the size of J and the fact that it is a classical model of KB, the same arguments
as in the proof of Lemma 7.12 apply. In particular, it holds |J | = O((|KB| + |C|)d ).
It remains to show that J is <fix -minimal. Assume by contradiction that for some interpretation
0
J , it holds J 0 <fix J ; as usual, we show that there exists I 0 , such that I 0 <fix I. Let I 0 be defined
0
0
0
0
as follows: I = I ; aI = aI , for all a  NI ; AI = AI , for all A  NC ; P I is the minimal set
such that:
0

0

 P J  P I , and
0

0

0

 for all z  I \ J , for all y  J and for all P.H  cl(KB) such that z 
0
0
supp I (P.H), if y  H I , then (z, y)  P I (P can be seen as a special case where
H = >).
First, we prove that I 0 is a classical model of KB. In particular, it suffices to show that classical
0
0
inclusions are satisfied in I \ J . Given a classical inclusion C1 v D1 of KB, assume that
0
z  C1I , and recall that C1 is of the type
A1 u . . . u An u R1 u . . . u Rm .
0

It suffices to show that there exists an individual w  J that satisfies C1 as well. By construction,
for all R occurring in C1 , z  supp I (R), therefore z  supp I (C1 ). This means that there
exists a witness w  supp I (C1 ) in J . Since for each concept E, w  E I implies w  E J ,
it follows that w  supp J (C1 ). Since the priority relation is empty, for each DI  in KD it holds
satJ ()  satJ 0 (). As a consequence, for each concept E it holds supp J (E)  supp J 0 (E). In
761

fiB ONATTI , FAELLA , & S AURO

0

0

particular, w  supp J 0 (C1 ) and hence w  C1J and w  D1J . By construction, we obtain that
0
z  D1I .
It remains to prove that I 0 improves I according to <fix . Let  = (C1 vn D1 ) be a defeasible
0
0
inclusion in KB. Since J 0 <fix J , it suffices to prove that for all z  I \ J , if z  satI ()
then z  satI 0 ().
Suppose first that z vacuously satisfies  in I (i.e., it violates C1 ). Atomic concepts have the
same extension in I and I 0 , and z satisfies an unqualified existential in I 0 only if it satisfies the
same existential in I. Hence, z vacuously satisfies  in I 0 as well.
0
Suppose instead that z actively satisfies  in I. If z 6 supp I (C1 ), then z 6 C1I , and so z
vacuously satisfies  in I 0 . Otherwise, z  supp I (C1 ) and z  supp I (D1 ). By construction, there
0
0
is a witness w  J such that w  supp J (D1 )  supp J 0 (D1 ). This implies that w  D1J and,
0
0
considering the construction of P I , z  D1J . Therefore, z  satJ 0 () and we obtain the thesis. 2

References
Baader, F. (2003). The instance problem and the most specific concept in the description logic EL
w.r.t. terminological cycles with descriptive semantics. In Proc. of the 26th Annual German
Conf. on AI, KI 2003, Vol. 2821 of LNCS, pp. 6478. Springer.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proc. of the 19th Int. Joint
Conf. on Artificial Intelligence, IJCAI-05, pp. 364369. Professional Book Center.
Baader, F., & Hollunder, B. (1995a). Embedding defaults into terminological knowledge representation formalisms.. J. Autom. Reasoning, 14(1), 149180.
Baader, F., & Hollunder, B. (1995b). Priorities on defaults with prerequisites, and their application
in treating specificity in terminological default logic.. J. Autom. Reasoning, 15(1), 4168.
Bonatti, P. A., Faella, M., & Sauro, L. (2009). Defeasible inclusions in low-complexity DLs: Preliminary notes. In Boutilier, C. (Ed.), IJCAI, pp. 696701.
Bonatti, P. A., Faella, M., & Sauro, L. (2010). EL with default attributes and overriding. In Int.
Semantic Web Conf. (ISWC 2010), Vol. 6496 of LNCS, pp. 6479. Springer.
Bonatti, P. A., Lutz, C., & Wolter, F. (2006). Description logics with circumscription. In Doherty,
P., Mylopoulos, J., & Welty, C. A. (Eds.), KR, pp. 400410. AAAI Press.
Bonatti, P. A., Lutz, C., & Wolter, F. (2009). The complexity of circumscription in DLs. J. Artif.
Intell. Res. (JAIR), 35, 717773.
Bonatti, P. A., & Samarati, P. (2003). Logics for authorization and security. In Logics for Emerging
Applications of Databases, pp. 277323. Springer.
Cadoli, M., Donini, F., & Schaerf, M. (1990). Closed world reasoning in hybrid systems. In Proc.
of ISMIS90, pp. 474481. Elsevier.
Cadoli, M., Eiter, T., & Gottlob, G. (1992). An efficient method for eliminating varying predicates
from a circumscription. Artif. Intell., 54(2), 397410.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite: Tractable
description logics for ontologies. In Proc. of AAAI 2005, pp. 602607.
762

fiD EFEASIBLE I NCLUSIONS IN L OW-C OMPLEXITY DL S

Casini, G., & Straccia, U. (2010). Rational closure for defeasible description logics. In Janhunen,
T., & Niemela, I. (Eds.), JELIA, Vol. 6341 of Lecture Notes in Computer Science, pp. 7790.
Springer.
Donini, F. M., Nardi, D., & Rosati, R. (1997). Autoepistemic description logics. In IJCAI, pp.
136141.
Donini, F. M., Nardi, D., & Rosati, R. (2002). Description logics of minimal knowledge and negation as failure. ACM Trans. Comput. Log., 3(2), 177225.
Eiter, T., Leone, N., Mateis, C., Pfeifer, G., & Scarcello, F. (1997). A deductive system for nonmonotonic reasoning. In Logic Programming and Nonmonotonic Reasoning, 4th International Conference, LPNMR97, Proceedings, Vol. 1265 of LNCS, pp. 364375. Springer.
Eiter, T., & Gottlob, G. (1995). On the computational cost of disjunctive logic programming: Propositional case. Ann. Math. Artif. Intell., 15(3-4), 289323.
Finin, T. W., Joshi, A., Kagal, L., Niu, J., Sandhu, R. S., Winsborough, W. H., & Thuraisingham,
B. M. (2008). ROWLBAC: representing role based access control in OWL. In Ray, I., & Li,
N. (Eds.), SACMAT, pp. 7382. ACM.
Giordano, L., Gliozzi, V., Olivetti, N., & Pozzato, G. (2008). Reasoning about typicality in preferential description logics. In Proc. of Logics in Artificial Intelligence, 11th European Conference,
JELIA 2008, Vol. 5293 of Lecture Notes in Computer Science. Springer.
Kolovski, V., Hendler, J. A., & Parsia, B. (2007). Analyzing web access control policies. In
Williamson, C. L., Zurko, M. E., Patel-Schneider, P. F., & Shenoy, P. J. (Eds.), WWW, pp.
677686. ACM.
Lutz, C., & Wolter, F. (2010). Deciding inseparability and conservative extensions in the description
logic el. J. Symb. Comput., 45(2), 194228.
McCarthy, J. (1980). Circumscription - a form of non-monotonic reasoning. Artif. Intell., 13(1-2),
2739.
McCarthy, J. (1986). Applications of circumscription to formalizing common-sense knowledge.
Artif. Intell., 28(1), 89116.
Rector, A. L. (2004). Defaults, context, and knowledge: Alternatives for OWL-indexed knowledge
bases. In Pacific Symposium on Biocomputing, pp. 226237. World Scientific.
Stevens, R., Aranguren, M. E., Wolstencroft, K., Sattler, U., Drummond, N., Horridge, M., & Rector,
A. L. (2007). Using OWL to model biological knowledge. International Journal of ManMachine Studies, 65(7), 583594.
Straccia, U. (1993). Default inheritance reasoning in hybrid KL-ONE-style logics.. In IJCAI, pp.
676681.
Uszok, A., Bradshaw, J. M., Jeffers, R., Suri, N., Hayes, P. J., Breedy, M. R., Bunch, L., Johnson, M.,
Kulkarni, S., & Lott, J. (2003). KAoS policy and domain services: Towards a descriptionlogic approach to policy representation, deconfliction, and enforcement.. In 4th IEEE Int.
Workshop on Policies for Distributed Systems and Networks (POLICY), pp. 9396. IEEE
Computer Soc.
763

fiB ONATTI , FAELLA , & S AURO

Zhang, R., Artale, A., Giunchiglia, F., & Crispo, B. (2009). Using description logics in relation
based access control. In Grau, B. C., Horrocks, I., Motik, B., & Sattler, U. (Eds.), Description
Logics, Vol. 477 of CEUR Workshop Proceedings. CEUR-WS.org.

764

fiJournal of Artificial Intelligence Research 42 (2011) 607-659

Submitted 09/11; published 12/11

Drake: An Efficient Executive for Temporal
Plans with Choice
Patrick R. Conrad
Brian C. Williams

prconrad@mit.edu
williams@mit.edu

Room 32-227
32 Vassar St
Cambridge, MA 02139 USA

Abstract
This work presents Drake, a dynamic executive for temporal plans with choice. Dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding
events, improving the robustness of the agent. Prior work developed methods for dynamically dispatching Simple Temporal Networks, and further research enriched the expressiveness of the plans executives could handle, including discrete choices, which are the focus of
this work. However, in some approaches to date, these additional choices induce significant
storage or latency requirements to make flexible execution possible.
Drake is designed to leverage the low latency made possible by a preprocessing step
called compilation, while avoiding high memory costs through a compact representation.
We leverage the concepts of labels and environments, taken from prior work in Assumptionbased Truth Maintenance Systems (ATMS), to concisely record the implications of the
discrete choices, exploiting the structure of the plan to avoid redundant reasoning or storage. Our labeling and maintenance scheme, called the Labeled Value Set Maintenance
System, is distinguished by its focus on properties fundamental to temporal problems, and,
more generally, weighted graph algorithms. In particular, the maintenance system focuses
on maintaining a minimal representation of non-dominated constraints. We benchmark
Drakes performance on random structured problems, and find that Drake reduces the size
of the compiled representation by a factor of over 500 for large problems, while incurring
only a modest increase in run-time latency, compared to prior work in compiled executives
for temporal plans with discrete choices.

1. Introduction
Model-based executives elevate commanding of autonomous systems to the level of goal
states while providing guarantees of correctness (Williams, Ingham, Chung, & Elliott, 2003).
Using a model-based executive, a user can provide a specification of the goal behavior of
the robot and leave it to a program, the executive, to determine an appropriate course of
action that meets those goals. Temporal plan executives are designed to work with plans
including timing requirements.
Typically, for an executive to be robust to disturbances, it must be able to react to the
outcomes of events on the fly, otherwise, even seemingly inconsequential variations in the
outcomes of events may cause a failure. Thus, it can be helpful to follow a strategy of least
commitment and delay each decision until it is actually time to act on that decision, allowing
the executive to act with as much information as possible. In the case of temporal plans,
an executive following this strategy is said to dynamically dispatch the plan (Muscettola,
c
2011
AI Access Foundation. All rights reserved.

fiConrad & Williams

Morris, & Tsamardinos, 1998). Such an executive is responsible for determining when to
schedule events as late as possible while guaranteeing that a consistent schedule exists for
all the remaining events. If an external disturbance causes some timing requirement to be
violated, then the executive should discover the failure and signal it as soon as possible.
Making such decisions on the fly requires some care, as on-line temporal reasoning can
introduce latency that is unacceptable for a real-time system. Therefore, Muscettola et al.
(1998) developed a low-latency executive for Simple Temporal Networks (STNs), where an
STN is comprised of a set of events and difference constraints on the time of execution of
the events. To achieve low latency, the executive is broken into two parts, a compiler and
a dispatcher. The compiler is run in advance to discover and explicitly record all temporal
constraints that cannot be quickly inferred on-line, thereby computing the dispatchable
form of the plan. The dispatcher uses this form to make real-time decisions using a greedy
strategy and local, low-latency inferences.
While dynamic scheduling has proven effective, robustness can further be improved by
making additional decision dynamically, such as the assignment of an activity to a particular resource. Encoding these decisions requires a more expressive formalism than STNs.
Consequently, subsequent research has developed efficient executives for more expressive
frameworks, many of which are variants of the STN. Examples of added features include
explicit modeling of uncertainty (Morris, Muscettola, & Vidal, 2001; Rossi, Venable, &
Yorke-Smith, 2006; Shah & Williams, 2008), discrete choices (Kim, Williams, & Abramson,
2001; Tsamardinos, Pollack, & Ganchev, 2001; Combi & Posenato, 2009; Shah & Williams,
2008), preferences (Hiatt, Zimmerman, Smith, & Simmons, 2009; Khatib, Morris, Morris,
& Rossi, 2001; Kim et al., 2001), discrete observations (Tsamardinos, Vidal, & Pollack,
2003), and combinations thereof.
This work focuses on enriching the executive to simultaneously schedule events and
make discrete choices as the execution unfolds. The ability to make discrete choices greatly
enriches an executive by offering it the ability to dynamically allocate resources, order activities, and choose between alternate methods (sub-plans) for achieving goals. Although prior
works have developed executives for this type of plan, they have trade-offs in performance.
For example, Tsamardinos, Pollack, and Ganchev (2001) presented an executive for Disjunctive Temporal Networks (DTN), a variant of STNs that include discrete choices. Their
executive extends the compilation strategy for STNs by breaking the DTN into its complete
exponential set of component STNs and then compiling and dispatching each in parallel.
Their strategy offers low latency, but incurs a high storage cost for the dispatchable plan.
Another example, Kirk, is an executive for Temporal Plan Networks (TPNs), which extends
STNs by including a hierarchical choice between sub-plans, developed by Kim, Williams,
and Abramson (2001). Kirk selects a set of choices and performs incremental re-planning
whenever a disturbance invalidates that choice, leaving a small memory footprint, but potentially inducing high latency when it selects new choices. Chaski is an executive presented
by Shah and Williams (2007) for temporal plans with resource allocation, whose expressiveness is between that of STNs and DTNs. Chaski takes an approach which is a hybrid
of the incremental strategy of Kirk and the compiled approach of Tsamardinos et al.: its
compiled representation is a base plan and a set of incremental differences, which provides
the benefits of compiled execution while improving efficiency by exploiting structure in the
plan .
608

fiDrake: An Efficient Executive for Temporal Plans with Choice

We develop Drake, a novel executive for temporal plans with choice encoded using
the expressive representation of DTNs (DTNs dominate TPNs, TCNs, and STNs). Drake
achieves low run-time latency through compilation, yet requires less storage than the fully
exponential expansion approach taken by Tsamardinos et al. (2001). In order to accomplish this, Drake works on a compact representation of the temporal constraints and discrete
choices. To develop the compact representation, we begin with the idea, taken from truth
maintenance, of labeling consequences of inferences with the minimal set of choices that imply the consequence; this minimal set is called an environment (McDermott, 1983; de Kleer,
1986). By monotonicity of inference, this consequence also holds for all sets of choices that
are a superset of the minimal environment, thus the environment is a compact encoding of
the decision contexts in which that consequence holds.
These ideas are directly applicable to temporal reasoning problems; Drake extends them
by leveraging properties fundamental to temporal reasoning problems, and weighted graph
problems in general. More specifically, temporal reasoning is non-monotonic, in the sense
that it does not need to explicitly represent all derivable constraints, only the tightest possible ones, referred to as the non-dominated constraints. Drake uses this property throughout
to reduce the computations and storage required. For example, in the inequality A  4  8,
where A is a temporal event, there is no need to store the constraint A  8, as the tighter
inequality makes it unnecessary, or dominates it. The focus on non-dominated values or
constraints is central to a range of inference problems, including temporal inference, interval
reasoning, and inference over weighted graphs.
Dechter, Meiri, and Pearl (1991) proved that STN inference problems are reducible to
a widely used set of inference methods on weighted graphs, such as Single Source Shortest
Path and All-Pairs Shortest Path Problems. Our approach is to develop labeled analogues
to the weighted graph structures that support these shortest path algorithms, providing a
compact representation for Drake. In this paper, we first present a new formalism for plans
with choice, the Labeled Simple Temporal Network (Labeled STN), which has the same
expressiveness as previous formalisms, but which shares ideas with the rest of our techniques.
Second, we explain a system for maintaining and deriving compact representations of values
that vary with choices, called the Labeled Value Set Maintenance System. Then, we use
Labeled Value Sets to construct Labeled Distance Graphs, which are distance graphs where
the edge weights may vary depending on the discrete choices. Finally, Drakes compilation
and dispatching algorithms are built around these techniques. While the focus of this paper
is on dispatchable execution, the techniques surrounding labeled distance graphs hold the
promise of extending a wide range of reasoning methods involving graph algorithms to
include choice.
In practical terms, Drakes compact encoding provides a reduction in the size of the plan
used by the dispatcher by over two orders of magnitude for problems with around 2,000
component STNs, as compared to Tsamardinos et al.s work (2001). This size reduction
comes at a modest increase in the run-time latency, making Drake a useful addition to the
available executives.
609

fiConrad & Williams

1.1 Overview of the Problem
Drake takes as its input a Labeled STN, which is a temporal constraint representation
with choices; in Section 3 we discuss how Labeled STNs can encode choices between subplans, temporal constraints, and resource assignment, and mappings to related frameworks.
Drakes output is a dynamic execution of the plan, where it determines in real-time when to
execute the events, such that at the end of the plan the execution times are consistent with
every temporal constraint implied by at least one complete set of choices, barring unforeseen
disturbances. If outside disturbances make every possible execution inconsistent, then Drake
signals the failure as soon as all possible solutions are rendered inconsistent.
Section 3 provides a formal definition of Labeled STNs; essentially, it is a collection of
events to schedule and the constraints the executive must follow. The events may be constrained with simple temporal constraints, which limit the difference in the scheduled times
of two events. Furthermore, the Labeled STN specifies discrete choices, where assignments
to the choices may imply additional simple temporal constraints.
Throughout this paper, we use the following simple example, which includes a choice
between sub-plans.
Example 1.1 A rover has 100 minutes to work before a scheduled contact with its operators. Before contact, the rover must drive to the next landmark, taking between 30 and
70 minutes. To fill any remaining time, the rover has two options: collect some samples
or charge its batteries. Collecting samples consistently takes 50 to 60 minutes, whereas
charging the batteries can be usefully done for any duration up to 50 minutes.


Always: [0, 100]

If collecting: Collect
Samples, [50, 60]

C

If collecting: [0, 0]

Always:
Drive, [30, 70]
A

Always: [0, 0]
B

If charging:
Charge [0, 50]

E

D

F

If charging: [0, 0]

Figure 1.1: This informal Labeled STN depicts Example 1.1. The rover needs to drive, then
either collect samples or charge its batteries within a certain time limit.

610

fiDrake: An Efficient Executive for Temporal Plans with Choice

Figure 1.1 shows an informal representation of the Labeled-STN corresponding to this
plan. The notation [l, u] on an edge from vertex X to Y means that the difference in
the execution times of the events lies in the given interval, which expresses the constraint
l  Y  X  u. In this figure the text explains which temporal constraints are implied by
which choice; we develop precise notation later. There are two types of constraints drawn,
those that are always required, and those only required if the rover is either collecting
samples or charging.
Consider the following correct output execution sequence for the rover problem. In this
example, we focus on the form of the executives output, deferring the presentation of the
decision-making strategy until later.
Example 1.2 Drake starts executing the plan, arbitrarily denoting the starting time as
t = 0. At that time, it instructs the system to begin the drive activity, indicating that
the drive should take 40 minutes. The executive then waits until the system responds that
that the drive has completed, at time t = 45. Then Drake selects the sample collection
option, which had not been determined before, and initiates the activity with a duration of
50 minutes. At t = 95, the sample collection completes, finishing the plan within the time
limit of 100 minutes.

1.2 Approach: Exploiting Shared Structure through Labeling
Drakes strategy during compilation is to begin with the Labeled STN, a concise statement
of the temporal constraints and the choices in the plan. Then, Drake constructs the Labeled Distance Graph associated with the Labeled STN, yielding a single graph structure
representing all the possible choices and constraints. Next, Drakes compiler computes the
dispatchable form of the problem, which is also a Labeled Distance Graph. This compilation is performed in a unified process that is able to exploit any similarities between the
choices to make the representation compact. In contrast, the prior work of Tsamardinos et
al. (2001), breaks the input plan into independent STNs, hence their compilation strategy
cannot exploit any similarities or shared structures between the choices. There are pathological cases, where every choice is completely unrelated, where there are no similarities
for Drake to exploit. However, we expect that nearly every real-world or human designed
plan has some degree of shared structure, because a plan usually has some unifying idea
which the choices are designed to accomplish. Indeed, we expect that most real plans have
significant similarities, allowing Drake to perform well. This section gives an overview of
the intuition behind the representation Drake uses and the similarities that Drake exploit.
Before we continue discussing the rover example, consider Figure 1.2, which depicts
a small STN, its associated distance graph, and the dispatchable distance graph that is
the result of compilation. The set of events in the STN are represented as vertices in the
distance graph. Upper bounds in the STN induce edges in the forward direction, weighted
with the upper bound, and lower bounds induce edges in the reverse direction weighted
with the negative of the lower bound. The distance graph in Figure 1.2b is compiled, and
the compiler outputs a new distance graph that contains representations of the constraints
needed by the dispatcher. The dispatchable form in Figure 1.2c is used by a dispatcher;
execution times are propagated through the edges to determine when other events may be
executed.
611

fiConrad & Williams

[5, 10]
A

C

[3, 3]

10

A

3

[2, 5]

C

-5
-3

2

5

B

B

(a) Input STN

(b) Associated distance graph

8

A

-5
3

C

-3

B
(c) Dispatchable graph

Figure 1.2: A simple example of reformulating an STN into its associated distance graph
and its dispatchable form.

The rover example has a single binary choice, hence for this problem Tsamardinos et al.s
(2001) algorithm separates the two possible STNs then compute their associated distance
graphs, which are shown in Figure 1.3. Note the repetition of certain edges in both graphs,
for example, the edge A  F , which is present throughout their compilation and dispatch
process. Plans with more choices can have an exponential number of repetitions, which can
be costly, and which Drake is designed to eliminate.
An informal version of the Labeled Distance Graph associated with the rover example is
shown in Figure 1.4. The differing constraints that result from the two possible assignments
to the choice are distinguished by annotations called labels. For example, edge (B, C) has
weight S : 60, which indicates whenever sampling is chosen, the edge has a weight of at
most 60; the value 60 is labeled with the discrete choice, S, that implies it. We gather
all the possible values under all the choices into Labeled Value Sets, which are placed on
edges. In this example, each edge has a Labeled Value Set with exactly one labeled value,
although this is not true in general. A Labeled Distance Graph is essentially a distance
graph where the numeric weights are replaced with Labeled Value Sets. Although we develop
more precise notation later on in this article, this version shows the intuition behind the
approach. Drake capitalizes on this improvement by using the compact representation
throughout compilation and dispatch, and this work develops the necessary machinery.
This paper is organized as follows. Section 2 discusses related work on temporal executives and provides background on truth maintenance. Section 3 defines Labeled STNs and
their correct dynamic execution, specifying the problem Drake solves. Section 4 recalls the
612

fiDrake: An Efficient Executive for Temporal Plans with Choice

100
0

C
A

70
-30

60
-50

0
0

B

E

0
0

F

0
0

F

D
(a) Distance graph if collecting samples

100
0

C
A

70
-30

B

E
0

50
0

0
D

(b) Distance graph if charging

Figure 1.3: The Tsamardinos et al. (2001) style distance graphs associated with the
Labeled-STN in Figure 1.1.

link between STNs and distance graphs, and provides the labeled version of distance graphs,
which Drake uses for reasoning. Section 5 presents the Labeled Value Set Maintenance System, completing the foundation of labeled techniques. Section 6 details the dispatcher and
Section 7 develops Drakes compilation algorithm. Finally, Section 8 provides some theoretical and experimental performance results, and Section 9 gives some concluding remarks.

2. Related Work
Before developing Drake, we give an overview of some relevant literature in the two major
areas Drake draws from: scheduling frameworks and truth maintenance.
613

fiConrad & Williams

A: 100
A: 0

C
S: 60
S: 50
A

A: 70
A:30

S: 0
S: 0

B

E

A: 0
A: 0

F

C: 0
C: 0

C: 50
C: 0
D

Figure 1.4: An informal labeled distance graph for the rover example. A:, S: and C:,
correspond to weights that hold always, with sampling, and with charging,
respectively.

2.1 Scheduling Frameworks and Executives
As stated in the introduction, to achieve robustness, we need executives that make decisions
dynamically and with low latency over expressive temporal representations. There are
known methods for manipulating and reasoning over Simple Temporal Networks efficiently,
which have been used as the foundation for most work in temporal executives. Furthermore,
numerous efforts have formulated and developed extensions of STNs to include other useful
properties, including uncertainty, preferences, and discrete choices. We briefly review some
of these efforts. Since our work focuses on discrete choices, we discuss several efforts to build
dynamic executives for these plans in more detail. These executives typically use one of
two approaches: they reason on all plans in parallel, or switch between plans incrementally.
However, these approaches, while promising, are typically either too memory intensive or
may have high latency.
Temporal Constraint Networks (TCNs), formalized by Dechter, Meiri and Pearl (1991),
capture many of the qualitative and metric temporal representations introduced by the AI
community. A restricted type of TCN, the Simple Temporal Network, is used throughout
recent work in temporal planning, temporal reasoning, and scheduling. Muscettola, Morris,
and Tsamardinos (1998) proposed framework for low-latency dynamic execution: a preprocessing step called compilation and a run-time component called dispatch. Tsamardinos,
Muscettola, and Morris (1998) later provided a faster compilation algorithm. Further work
has also developed more efficient methods for testing consistency of STNs (Xu & Choueiry,
2003; Planken, de Weerdt, & van der Krogt, 2008).
614

fiDrake: An Efficient Executive for Temporal Plans with Choice

Dechter et al. (1991) also proposed Temporal Constraint Satisfaction Problems, which
include discrete choices that alter the simple interval constraint between particular pairs of
events; each pair may have a choice of interval constraints, but choices for each pair of events
must be independent. Stergiou and Koubarakis (2000) loosened this structural restriction,
developing the Disjunctive Temporal Network (DTN). Tsamardinos, Pollack, and Ganchev
(2001) presented the first dynamic executive for DTNs, which functions by generating the
component STNs implied by all combinations of the discrete choices and compiling them
independently, creating an exponential growth in memory use with respect to the number
of choices.
Another important line of extension to STNs is the Simple Temporal Network with Uncertainty (STNU). Morris, Muscettola, and Vidal (2001) proved that an executive can test
for consistency of an STNU and compile it into dispatchable form in polynomial time. Morris (2006) described a more efficient algorithm for testing dynamic controllability. Hunsberg
(2009, 2010) corrected a flaw in the previous definitions and described an execution strategy
using the more efficient dynamic controllability algorithm. Venable and Yorke-Smith (2005)
added temporal uncertainty to DTNs. Tsamardinos (2002) introduced a probabilistic formulation of uncertainty in STNs. Conrad (2010) presents an extension of Drake to DTNs
with uncertainty.
Tsamardinos, Vidal, and Pollack (2003) introduced Conditional Temporal Problems
(CTP), adding uncontrollable discrete choices. The executive cannot control, but may
only observe the values of some discrete choices at designated parts of the plan. Some of
their notation is quite similar to that used here for Drake, but there are two important
differences. First, a CTP is a strictly harder problem, since Drake is not concerned with
uncontrollable choices, meaning that their algorithm does more work than is necessary for
the simpler case. Second, their algorithm does not use a compact representation; their
algorithm for consistency checking requires enumerating the possible scenarios. An open
problem for future research is to adapt their more general algorithms to take advantage of
the compactness of the Labeled Distance Graph.
Another useful feature added to STNs is preferences. Khatib, Morris, Morris, and Rossi
(2001) introduced a formulation including preferences for event execution times within the
simple interval bounds allowed by an STN, adding a notion of quality to the existing notion
of consistency. Rossi, Venable, and Yorke-Smith (2006) discuss simultaneous handling of
uncertainty and preferences.
Kim, Williams, and Abramson (2001) present Temporal Plan Networks, a representation
that provides simple temporal constraints over durations combined in series, parallel, and
with choice, where each choice has specified costs. Effinger (2006) expands this to a simple
preference model, in which choices and activities have associated, fixed costs. Kirk is a
dynamic executive for TPNs. Kirk performs optimal method selection just before run-time,
assigning the discrete choices and then dispatches the resulting component STN. If some
disturbance invalidates the STN that Kirk chose, then Kirk selects a new STN consistent
with the execution thus far. Further research developed incremental techniques to allow
Kirk to re-plan with lower latency (Shu, Effinger, & Williams, 2005; Block, Wehowsky, &
Williams, 2006).
Shah and Williams (2008) present Chaski, an executive that dynamically dispatches
plans with task assignment over heterogeneous, cooperative agents, represented by a TCN,
615

fiConrad & Williams

by removing some redundant data structures and computations performed by Tsamardinos
et al.s (2001) algorithm. Shah and Williams point out that the component STNs of realworld TCNs often differ by only a few constraints, allowing a compact representation. They
record all the component STNs by storing a single relaxed STN and maintaining a list of
modifications to the relaxed STN that recover each of the original component STNs. By
avoiding redundant records of shared constraints, their results show dramatic improvements
in performance. Our work is inspired by the observation that this technique, although
distinct, bears some resemblance to the environment labeling scheme that is employed by
an Assumption Based Truth Maintenance System (ATMS). We specifically adapted ATMS
ideas to work with a more general problem formulation than Chaski, expecting to see similar
performance improvements.
Combi and Posenato (2009, 2010) discuss applications of dynamic executives to business
work flows, which include flexibility over time of execution, hierarchical choice over execution
paths, and temporal uncertainty. Their formalism for plans, Workflow Schemata, is closely
related to DTN, STNU, and TPN frameworks, and they discuss variants of compilation
and dispatching algorithms specialized to their representations. Their work describes an
intriguing notion they call history-dependent controllability. Under this model, if event
X starts one of two sub-plans, the executive may not invalidate either sub-plan until it
executes X and begins one of them. Drake does not impose a similar requirement, but the
requirement is certainly useful for preserving the executives flexibility over future choices
as the execution unfolds. Their algorithms for testing controllability enumerate the possible
choices, and therefore suffers from memory growth.
Smith, Gallagher, and Zimmerman (2007) describe a distributed dynamic executive for
real world plans in the C TAEMS language. That representation uses STN temporal semantics and includes other features intended to represent cooperative multi-agent plans.
Their language features a rich and practical notion of activity failure not present in STNs,
including the potential for interruption of activities. The executive is given discrete choices
over method selection and resource allocation, and attempts to maximize the utility of the
overall plan. Their preference model accounts for partial or total method failures and supports different functions for accumulating reward, for example, summing or maxing. Their
executive uses a re-planning strategy, similar to Kirk, which is enhanced by Hiatt, Zimmerman, Smith, and Simmons (2009) with a type of compile-time analysis called strengthening.
This analysis performs a type of local repair that attempts to make the plan more robust
to uncertainties or activity failures.
There are two central approaches to dynamic executives that include discrete choices.
First, Tsamardinos et al.s (2003) CTPs, Tsamardinos et al.s (2001) DTN dispatcher, and
Combi et al. (2010) use a compile-time analysis to compute the implied constraints of every
possible plan and explicitly reason over them at run-time. Second, Kim et al. (2001), and
Smith et al. (2007) focus on a single, potentially optimal, assignment to the choices, and if
that becomes infeasible, they incrementally re-plan and extract a new plan. Both methods
have shortcomings, since explicit compilation is memory intensive and re-planning steps can
be computationally intensive, especially if the executive is forced to re-plan often. Drake,
like Chaski, provides a middle ground by working with a compilation strategy that has a
reduced memory footprint.
616

fiDrake: An Efficient Executive for Temporal Plans with Choice

2.2 Background on ATMSs
Stallman and Sussman (1977) introduced the profoundly useful idea of tracing the dependency of deductions in a computerized aid to circuit diagnosis, in order to focus the search
for a consistent component mode assignment during transistor circuit analysis. The dependencies it computes from solving the equations allow it to rapidly find those choices that
might be responsible for the detected failure. They generalize this approach to combinatorial search, introducing the dependency-directed backtracking algorithm, which ensures
that when a conflict is found that the search backs up far enough to ensure that the newly
found inconsistency is actually removed.
Doyle (1979) introduced Truth Maintenance Systems (TMSs) as a domain independent
method for supporting dependency-directed backtracking. The TMS represents data, their
justifications, and provides the ability to revise beliefs when assumptions change or contradictions arise. For example, consider a problem solver designed to search for a solution
to a constraint satisfaction problem. When determining whether a particular solution is
consistent, the problem solver will perform a chain of inferences, providing the TMS with
the justification for each step. If an inconsistency is found, the problem solver selects a new
candidate solution, and the TMS uses the justifications to determine which of the inferences
still hold under the new candidate and which must be recomputed to account for new circumstances. The TMS continually determines whether a particular datum, a general term
for any fact that arises in problem solving, is in or out, that is, currently believed true, or
not currently believed.
Later work relaxes the goal of maintaining a single, consistent assignment to all data
of in or out, and instead tracks the contexts in which particular facts hold, even if those
contexts may be mutually exclusive. McDermott (1983) uses beads to state a context, which
is a particular set of choices or assumptions on which the reasoning might rely, and provides
data pools that specify all the facts that hold in that context. De Kleer (1986) develops the
Assumption-based Truth Maintenance System, which uses a similar idea, but changes the
terminology to use environments and labels to specify contexts. The ATMS maintains a set
of minimal inconsistent environments, called conflicts or no-goods. These conflicts help the
system to avoid performing inferences for contexts that are already known to be inconsistent,
and the minimality of the conflict set makes the procedure tractable. The ATMS is designed
to simultaneously find the logical consequences of all possible combinations of assumptions,
in contrast to the TMS, which focuses on finding any one set of assumptions that solve the
problem of interest. Hence, the ATMS is well suited as the foundation for an executive
that is intended to consider all possible choices simultaneously without incurring latency
for switching between choices. Finally, there is some development of the idea that when
working with inequalities, the ATMS only needs to keep tightest bounds on the inequalities,
which we use extensively; this concept was described by Goldstone (1991) as hibernation.
We leave our review of the details of the ATMS to later sections, as we develop Drakes
machinery in depth.

3. Dynamic Execution of STNs and Labeled STNs
Now we are prepared to present the formal representation of the temporal plans that Drake
uses. Plans are composed of actions that need to be performed at feasible times, where we
617

fiConrad & Williams

define feasible times through constraints on the start and end times of the plan activities.
Our work builds upon Simple Temporal Networks, and we begin by explaining how STNs
constrain the events of a plan and their feasible execution. We extend these definitions to
include discrete choices, thereby constructing Labeled STNs.
3.1 Simple Temporal Networks
Simple Temporal Networks provide a framework for efficiently reasoning about a limited
form of temporal constraints. A simple temporal network is defined as a set of events related
by binary interval constraints, called simple interval constraints (Dechter et al., 1991).
Definition 3.1 (Event) An event is a real-valued variable, whose value is the execution
time of the event.

Definition 3.2 (Simple Interval Constraint) A simple interval constraint hA, B, l, ui
between two events A and B requires that l  B  A  u, denoted, [l, u].

By convention, u is non-negative. The lower bound, l may be positive if there is a strict
ordering of the events, or negative if there is not a strict ordering. Positive or negative
infinities may be used in the bounds to represent an unconstrained relationship.
Definition 3.3 (Simple Temporal Network) A Simple Temporal Network hV, Ci is
comprised of a set of events V and a set of simple interval constraints C. A schedule
for an STN is an assignment of a real number to each event in V , representing the time
to schedule each event. The Simple Temporal Problem (STP) is, given an STN hV, Ci,
return a consistent schedule if possible, else return false. A consistent schedule is a schedule
that satisfies every constraint in C. If and only if at least one solution exists, the STN is
consistent.

Definition 3.4 (Dynamic Execution) A dynamic execution of an STN is the construction of a consistent schedule for the STN in real-time. The executive decides at time t  
whether to execute any events at time t, for some suitably small . If at some time, there
are no longer any remaining consistent schedules, return false immediately. The executive
may arbitrarily select any consistent schedule.

3.2 Adding Choice: Labeled STNs
This section defines our key representational concept, Labeled STNs, a variant of STNs that
is designed to include discrete choices between temporal constraints. Although equivalent
in expressiveness to Disjunctive Temporal Networks, Labeled STNs provide more consistent
terminology for Drake, and their corresponding labeled distance graphs make it easier to
extend standard STN and weighted graph algorithms to include choice. We show the precise
connection between DTNs and Labeled STNs at the end of this section. These definitions
are used throughout the compilation and dispatching algorithms presented later in this
work.
The input problem needs a succinct way to state what the choices of the input plan are
and what the possible options are that the executive may select between. We accomplish
this through a set of finite domain variables.
618

fiDrake: An Efficient Executive for Temporal Plans with Choice

Definition 3.5 (Choice Variables) Each choice is associated with a finite domain variable xi . Each of these variables has a domain with size equal to the number of options
of that choice. X is the set of all the variables for a particular problem. An assignment
is a selection of a single option for a choice, represented as an assignment to the choices
associated choice variable.

Example 3.6 In the rover example, there is a single choice with two options, leading to a
single variable x  {collect, drive}. We might assign x = collect to represent the choice of
collecting samples.

In general, Drake will reason about the implications of combinations of assignments.
To specify these assignments to the choice variables, Drake uses environments. Following the ATMS, Drake annotates, or labels interval constraints and edge weights with
environments specifying when they are entailed.
Definition 3.7 (Environment) An environment is a partial assignment of the choice
variables in X, written e = {xi = dij , ...}. An environment may have at most one assignment
to each variable to be consistent. A complete environment contains an assignment to every
choice variable in X. An empty environment provides no assignments and is written {}.
We denote the set of possible environments as E and the set of complete environments as
Ec . The length of an environment is the number of assigned variables, denoted |e|.

Example 3.8 In a problem with two choice variables x, y  {1, 2}, some possible environments are {}, {x = 1}, {y = 2}, and {x = 1, y = 2}.

In a Labeled STN, different assignments to the choice variables entail different temporal
constraints, which we represent with labeled simple interval constraints.
Definition 3.9 (Labeled Simple Interval Constraint) A labeled simple interval constraint is a tuple hA, B, l, u, ei for a pair of events A and B, real valued weights l and u
and environment e  E. This constraint states that, if the assignments in e hold, then the
simple interval constraint hA, B, l, ui is entailed.

Then a Labeled STN is defined analogously to an STN, but extended to include choices
and labeled constraints.
Definition 3.10 (Labeled Simple Temporal Network) A Labeled STN hV, X, Ci is a
set of events V , a set of choice variables X, and a set of labeled simple interval constraints
C. As with STNs, a schedule for a Labeled STN is an assignment of real numbers to each
event, indicating the time to execute the event. This schedule is consistent if there is a
full assignment to the choice variables so that the schedule satisfies every simple interval
constraint entailed by a labeled simple interval constraint.

Example 3.11 (Rover Problem as a Labeled STN) The rover problem of Example
1.1 has a single choice, with two options, collecting samples or charging the batteries.
Use a single choice variable x  {collect, charge} to represent the choice. The network
has events {A, B, C, D, E, F }. The labeled simple interval constraints below are written
e : l  B  A  u.
619

fiConrad & Williams

{} : 0  F  A  100

(1)

{} : 30  B  A  70

(2)

{} : 0  F  E  0

(3)

{x = collect} : 50  C  B  60

(4)

{x = collect} : 0  E  C  0

(5)

{x = charge} : 0  D  B  50

(6)

{x = charge} : 0  E  D  0

(7)

This notation states that the inequalities on lines 1-3 must always hold, lines 4-5 must
hold if the executive decides to collect samples, and lines 6-7 must hold if the executive
decides to charge the batteries. The schedule given in Example 1.2, A = 0, B = 45, C =
95, D = 45, E = 95, F = 95, is consistent with the full assignment x = collect. The
constraints on lines 1-3 hold for any choice, so necessarily must hold here. Lines 4-5 give
constraints with environments whose assignments are all given in the full assignment, so
their simple interval constraints must hold, and do. Lines 6-7 give constraints with an
environment that include different assignments than our full assignment, so the constraints
do not need to hold for the schedule to be consistent.

Drake provides a dynamic execution of a Labeled STN, making decisions at run-time,
as late as possible.
Definition 3.12 (Dynamic execution of a Labeled STN) A dynamic execution of a
Labeled STN is the simultaneous real-time construction of a full assignment and a corresponding consistent schedule. The executive decides at time t whether it will execute any
events at time t, for some suitably small . Possible assignments to choices are only eliminated from consideration as necessary to schedule events. The executive may arbitrarily
select any consistent schedule.

Example 1.2 gave a dynamic execution of the rover problem because the dispatcher selected choices and scheduling times dynamically, only choosing the option to collect samples
immediately before the start of the sample collection activity.
We conclude this section by discussing the equivalence of DTNs and Labeled STNs,
to allow for easier comparison to prior work. Recall the definition of DTNs (Stergiou &
Koubarakis, 2000).
Definition 3.13 (Disjunctive Temporal Network) A Disjunctive Temporal Network
hV, Ci is a set of events V and a set of disjunctive constraints C. Each disjunctive constraint
Ci  C is of the form
ci1  ci2  ...  cin ,
(8)
for positive integer n and each cij is a simple interval constraint. As before, a schedule is
an assignment of a time to each event. The schedule is consistent if and only if at least
one simple interval constraint cij is satisfied for every disjunctive constraint Ci . A DTN is
consistent if and only if at least one consistent schedule exists.

620

fiDrake: An Efficient Executive for Temporal Plans with Choice

The DTN and Labeled STN definitions are analogous, except for the difference in how
choices are specified. We can construct a Labeled STN equivalent to any DTN by creating
a choice variable for each disjunctive constraint, with one value for each disjunct. Thus,
xi = 1...n. Each disjunctive constraint in the DTN, cij is labeled with environment xi = j.
Non-disjunctive constraints are labeled with {}.
Example 3.14 Consider a DTN with three events, A, B, C. Assume there are two disjunctive constraints, hA, B, 3, 5i, and hB, C, 0, 6i  hA, C, 4, 4i. Then the corresponding Labeled STN would have one binary choice, represented by x  {1, 2}. It would
have three labeled simple interval constraints: hA, B, 3, 5, {}i, hB, C, 0, 6, {x = 1}i, and
hA, C, 4, 4, {x = 2}i.

To see the reverse construction, note that the DTN specifies a set of simple interval
constraint in conjunctive normal form. Labeled STNs allow the specification of somewhat
more complex boolean expressions, but all boolean expressions are reducible to conjunctive
normal form, so Labeled STNs are not more expressive. The mapping from DTNs to
Labeled STNs is straightforward because we construct a Labeled STN that directly uses a
conjunctive normal form expression.
Example 3.15 Consider the rover problem, given as a Labeled STN in Example 3.11. If
we represent the constraint 0  F A  100 as CF,A , then we can identify that this problem
has the following boolean form
CF,A  CB,A  CF,E  ((CC,B  CE,C )  (CD,B  CE,D ))

(9)

We could expand this to conjunctive normal form
CF,A  CB,A  CF,E  (CC,B  CD,B )  (CC,B  CE,D )  (CD,B  CE,C )
From this form, we can construct a DTN directly.

(10)


Thus, like the DTN, Labeled STNs provide a rich notion of choice. Given the definition
of the problem that Drake solves in this section and the introduction of environments, the
next two sections develop the labeled machinery Drake uses to efficiently perform temporal
reasoning.

4. Distance Graphs and Temporal Reasoning
Dechter et al. (1991) showed that STN reasoning can be reformulated as a shortest path
problem on an associated weighted distance graph. This connection is important because
weighted graphs are easy to manipulate and have well developed theory and efficient algorithms, hence most practical algorithms for STNs are based on this connection. Drake
follows the prior literature, in that it frames the temporal reasoning as a labeled version
of shortest path problems. This section begins to develop the formalism for Labeled Value
Sets and Labeled Distance Graphs, which allow us to compactly represent the shortest path
problems and algorithms. We begin by reviewing the transformation for STNs.
621

fiConrad & Williams

Definition 4.1 (Distance Graph associated with an STN) A distance graph associated with an STN is a pair hV, W i of vertices V and edge weights W . Each event is
associated with a vertex in V . The vertices exactly correspond to the events of the STN.
The weights are a function V  V  R. The simple temporal constraint l  B  A  u is
represented by the edge weights W (B, A) = u and W (A, B) = l.

Figure 1.2 shows an example of the conversion from an STN to a distance graph. Recall that an STN is consistent if and only if its associated distance graph does not have
any negative cycles (Dechter et al., 1991). The compilation algorithm for an STN takes
the associated distance graph as an input and outputs another distance graph that is the
dispatchable form.
Definition 4.2 (Dispatchable form of a distance graph) A weighted distance graph
is the dispatchable form of an STN if an executive may dynamically execute the STN in
a greedy fashion to construct a consistent schedule using only local propagations. The
dispatchable form is minimal if all the edges are actually needed by the dispatcher for
correct execution.

The all-pairs shortest path (APSP) graph of the distance graph associated with an STN
is a dispatchable form because it explicitly contains all possible constraints of the STN
(Muscettola et al., 1998). The minimal form is computable by performing pruning on the
APSP graph.
We begin building the labeling formalism by defining labeled value pairs. Just as we
labeled simple temporal constraints with environments, labeled values associate values with
environments.
Definition 4.3 (Labeled Value Pair) A Labeled Value Pair is a pair (a, e), where a is a
value and e  E is an environment. The value a is entailed (usually as an assignment or an
inequality bound) under all environments where the assignments in e hold.

During compilation and dispatch, Drake uses labeled value pairs to track real valued
bounds, so a  R. During compilation, when performing shortest path computations,
Drake tracks predecessor vertices, so the value may be a pair (b, v)  R  V . Having
these two types of values complicates our discussion somewhat, but is necessary for the
compilation algorithms, and allows an elegant implementation of the relaxation algorithm
for directed, weighted graphs. The ATMS strategy of associating environments with values
is well founded for any arbitrary type of value, so both choices are sound. Labeled value
pairs always use minimal environments, that is, the environment specifies the smallest set
of assignments possible for the implication to be true. This minimality is critical for the
labeling system to be efficient.
Example 4.4 If there is a choice variable x  {1, 2}, then (3, {x = 1}) is a real valued
labeled value pair. Similarly, if A is an event, ((2, A), {x = 2}) is a possible predecessor
graph labeled value pair.

Now we have arrived at a crucial contribution of this paper: extending domination into
the labeled space. If we have the inequality B A  4  5, then it is clear that we only need
622

fiDrake: An Efficient Executive for Temporal Plans with Choice

to keep the dominant value, the four, and may discard the five. Shortest path algorithms
widely use the concept of dominance to propose many possible paths and keep only the
tightest one. When applied to labeled value pairs, dominance involves an ordering on two
parts, the value and the environment. In Drake, values are always ordered using real valued
inequalities, either  or . This paper mostly uses , except when reasoning on lower
bounds during dispatch, where explicitly noted. When necessary, a direct replacement of
the inequality direction suffices to extend the definitions. Environments are ordered through
the concept of subsumption.
Definition 4.5 (Subsumption of Environments) An environment e subsumes e0 if for
every assignment xi = dij  e, the same assignment exists in e0 , denoted xi = dij  e0 . 
Example 4.6 An environment {x = 1, y = 2, z = 1} is subsumed by {x = 1, z = 1}
because the assignments in the later environment are all included in the former.

Then domination of labeled value pairs applies to both orderings simultaneously.
Definition 4.7 (Dominated Labeled Value Pair) Let (a, ea ), (b, eb ) be two labeled
value pairs where the values are ordered with . Then (a, ea ) dominates (b, eb ) if a  b and
ea subsumes eb .

Example 4.8 If B  A  4 under {} and also B  A  4 under {x = 1}, then the
first inequality is non-dominated because its environment is less restrictive. Thus (4, {})
dominates (4, {x = 1}). Likewise, if B  A  2 under {x = 1}, this constraint dominates
the constraint B  A  4 under {x = 1}, because the first inequality is tighter and holds
within every environment where the second inequality holds, hence (2, {x = 1}) dominates
(4, {x = 1}).

To represent the various values a bound may have, depending on the choices the executive makes, Drake collects the non-dominated labeled value pairs into a Labeled Value
Set.
Definition 4.9 (Labeled Value Set) A Labeled Value Set, L is a set of non-dominated labeled value pairs, that is, a set of pairs (ai , ei ). Thus, we may write L = {(a1 , e1 ), ...(an , en )}.
No labeled value pair in the set is dominant over another pair in the set.

In any particular labeled value set, the values are all of the same type, either real values
or value/vertex pairs.
Example 4.10 Assume there is a single choice variable, x  {1, 2}. Some real valued
variable, A  R has value 3 if x = 1 and 5 if x = 2. Then A is represented by the labeled
value set, A = {(3, {x = 1}), (5, {x = 2})}.

Finally, we can modify distance graphs to use Labeled Value Sets instead of real values
for the weights, leading to the definition of labeled distance graphs.
623

fiConrad & Williams

Definition 4.11 (Labeled Distance Graph) A labeled distance graph G is a tuple
hV, E, W, Xi. V is a set of vertices and E is a set of directed edges between the vertices, represented as ordered pairs (i, j)  V  V . W is a weight function from edges to real
valued labeled value sets, where each edge (i, j)  E is associated with a labeled value set
W (i, j). X is the description of the choices variables, defining the set of environments that
may appear in the labeled value sets contained in W .

Example 4.12 Consider a simple graph with three vertices, V = {A, B, C}. This graph
contains edges E = {(A, B), (A, C), (B, C)}. There is one choice variable x  {1, 2}. Edge
(A, B) has weight 1 regardless of the choice. Edge (A, C) is 3 if x = 1 and 7 if x = 2.
Finally, edge (B, C) has weight 4 if x = 2. This labeled distance graph is shown in Figure
4.1.

(1, {})
A

B

{(3, {x = 1}),
(7, {x = 2})}

{(4, {x = 2})}
C

Figure 4.1: A simple Labeled Distance Graph with one choice variable, x  {1, 2}
The association between Labeled STNs and labeled distance graphs closely parallels the
association between STNs and distance graphs.
Definition 4.13 (Labeled Distance Graph Associated with a Labeled STN)
The labeled distance graph associated with a Labeled STN has a vertex associated with
every event in the Labeled STN. For each labeled inequality, denoted e : Y  X  w, where
e  E, X, Y  V , w  R, and the edge (X, Y )  E. The labeled value set W (X, Y ) includes
the labeled value pair (w, e) if the pair is not dominated by another (w0 , e0 )  W (X, Y ). 
Example 4.14 Figure 1.4 is the essentially the labeled distance graph for the rover problem
described in Example 1.1, except that we replace the informal notation with equivalent
labeled value sets. For example, the edge (B, C) currently has weight S : 60, which we
replace with the labeled value set {(60, {x = collect})}.

Drake compiles the input Labeled STN by creating its associated labeled distance graph
representation and then constructing a new labeled distance graph through a transformation
of the input graph.
Definition 4.15 (Dispatchable Form of a Labeled STN) A labeled distance graph is
the dispatchable form of a Labeled STN if it represents all the constraints necessary to
accurately perform dispatch with a greedy strategy, using only local propagations.

624

fiDrake: An Efficient Executive for Temporal Plans with Choice

This section constructed the labeling structures Drakes uses, beginning with labeled
value pairs and then building labeled value sets from minimal sets of labeled pairs. Labeled
value sets are used to construct labeled distance graphs, and we defined how to construct the
graph associated with a Labeled STN, on which Drakes compilation algorithms operate.

5. Labeled Value Set Maintenance System
The previous section defines the representation for labeled distance graphs and labeled value
sets; this section provides tools for manipulating them. The primary focus of labeled value
sets is to maintain only non-dominated labeled value pairs, which makes the labeled value
sets compact and efficient. This section introduces three concepts. First, we describe how
to extract values from a labeled value set, called a query, allowing us to find the dominant
value implied by an environment. Second, how to handle assignments to choices that are
inconsistent, which are called conflicts. Third, how to apply functions to labeled value sets,
which allows us to perform computations on them directly.
First we define the query, which lets us extract the precise dominant value(s) that are
guaranteed to hold under a particular environment.
Definition 5.1 (Labeled Value Set Query) The query operator A(e) is defined for labeled value set A and e  E. The query returns a set of values including ai from the pair
(ai , ei )  A, where ei subsumes e, if and only if there is no other pair (aj , ej )  A such that
ej subsumes e and aj < ai . If no environment ei subsumes e, then A(e) returns .

Here we introduce a convention: when using pairs of real values and vertices for the
values of labeled value pairs, we consider inequalities over the pairs to be defined entirely
by the real value. Thus, (5, A) < (6, E) and (5, A)  (5, E).
Example 5.2 Assume that there are two choice variables x, y  {1, 2}, and a real valued
variable A represented by a labeled value set, where A = {(3, {x = 1}), (5, {})}. A({x =
1, y = 1}) = 3 because the input environment is subsumed by both environments in the
labeled value set, but three is dominant. A({x = 2, y = 2}) = 5 because only the empty
environment in the labeled value set subsumes the input environment.

Example 5.3 Let A be a labeled value set with values that are pairs of real values and
vertices, A = {((3, A), {x = 1}), ((5, A), {y = 1}), ((5, B), {})}. Then A({x = 1, y = 2}) =
{(3, A)} because all the environments are subsumed, but (3, A) < (5, A) and (3, A) < (5, B).
A({x = 2, y = 1}) = {(5, A), (5, B)} because the tighter value is not applicable and neither
value with real part five is less than the other.

The query operator defines the expansion from the compact form to an explicit listing of
the values for each environment. Although Drake never performs this expansion, it is useful
for determining the correct behavior of a Labeled Value Set when designing algorithms.
Second, we consider conflicts, an important function of an ATMS that allows it to track
inconsistent environments. In our case, an inconsistent environment signals an inconsistent
component STN. The standard strategy in an ATMS is to keep a list of minimal conflicts,
also referred to as no-goods (de Kleer, 1986).
625

fiConrad & Williams

Definition 5.4 (Conflict) A conflict is an environment e such that if e subsumes e0 , then
e0 is inconsistent. The conflict e is minimal if there is no e00  E such that e00  e, such that
e00 is a conflict (Williams & Ragno, 2007).

Example 5.5 For example, the compilation process might determine that x = 1 and y = 1
are contradictory choices, and cannot be selected together during any execution. Then,
{x = 1, y = 1} is a conflict.

Often, reasoning algorithms keep a cache of conflicts to avoid performing work on environments that were previously discovered to contain an inconsistency. In practice, the set of
conflicts can become large and unwieldy, leading some practical systems to keep only a subset of conflicts, using principles such as temporal locality to maintain a small, useful cache.
Since the cache of conflicts is incomplete, the cache can miss, requiring the problem solver to
re-derive an inconsistency, but missing the cache would never lead to incorrectly accepting
an inconsistent solution. A good example of this is conflict learning in the widely studied
SAT-solver MiniSAT (En & Srensson, 2004). During real-time execution, an incomplete
cache of conflicts would require Drake to perform non-local propagation to re-test for inconsistency in the case of a cache miss. This extra step violates the principles of dispatchable
execution. Therefore, Drake maintains a complete cache of all known conflicts, allowing
Drake to verify that an environment is not known to be inconsistent with a single check
of the cache. Furthermore, Drake can quickly test whether any complete environments are
valid, as the inconsistencies are all readily available.
If there is a known conflict, Drake sometimes needs to determine how to avoid a conflict,
that is, what minimal environments ensure the conflict is not possible.
Definition 5.6 (Constituent Kernels, Williams et al., 2007) A conflict ec has an associated set of constituent kernels, each of which is an environment that specifies a single
assignment that takes some variable assigned in the conflict and assigns it a different value.
Hence, if ek is a constituent kernel, then any other environment e such that ek subsumes e
implies that ec does not subsume e, and hence is not subject to that conflict. Thus, we say
e avoids the conflict.

Example 5.7 If there are three variables, X, Y, Z  {1, 2}, assume {x = 1, y = 1} is a conflict. Then the constituent kernels are {x = 2} and {y = 2}, as any complete environment
that does not contain the conflict must assign either x or y not to be one.

The final tool needed is the ability to perform temporal and graph reasoning on labeled
value sets, which is principally accomplished through computing path lengths or propagating
inequality bounds. We can construct the approach by denoting the inference rule on values
as a function f . Then, we can build the method for applying f to labeled value sets from
the rule for applying it to labeled value pairs. We begin by defining the union operation
on environments, which is the fundamental operation on environments during temporal
reasoning, as in the ATMS literature (de Kleer, 1986).
Definition 5.8 (Union of Environments) The union of environments, denoted e  e0 is
the union of all the assignments of both environments. If e and e0 assign different values
to some variable xi , then there is no valid union and e  e0 = , where  is the symbol for
626

fiDrake: An Efficient Executive for Temporal Plans with Choice

false. If e  e0 is subsumed by a conflict, then e  e0 = . This value signifies that there is
no consistent environment where both e and e0 hold simultaneously.

Example 5.9 Most commonly, unions are used to compute the dependence of new derived
values. If A = 2 when {x = 1} and B = 3 when {y = 2}, then C = A + B = 5 when
{x = 1}  {y = 2} = {x = 1, y = 2}.

Using this notation, then an inference on labeled value pairs involves performing that
inference on the values to produce a new value, and unioning the environments to produce
a new environment.
Lemma 5.10 Consider labeled value pairs (a, ea ), (b, eb ). Applying some function f to the
pair yields (f (a, b), ea  eb ).

Proof For any environment e, if ea subsumes e then a is entailed, and if eb subsumes e then
b is entailed, by the definition of labeled value pairs. Additionally, ea  eb subsumes e if and
only if ea subsumes e and eb subsumes e. Therefore, ea  eb subsumes e entails both values
a, b, and hence allows us to compute f (a, b). Thus, (f (a, b), ea  eb ) is well defined.

Note that in this lemma, ea  eb may produce , which would indicate that the labeled
value pair never holds in a consistent environment and may be discarded.
Example 5.11 Consider computing the quantity (3, {x = 1}) + (6, {y = 1}) = (3 + 6, {x =
1}  {y = 1}) = (9, {x = 1, y = 1}).

If the function respects the dominance of values, then we may apply it to entire labeled
value sets.
Definition 5.12 A function f (a, b) is consistent with  dominance if for values a, b, c and
d, a  c, b  d = f (a, b)  f (a, d), f (a, b)  f (c, b), f (a, b)  f (c, d).

Lemma 5.13 If variables A, B are represented by labeled value sets and f is consistent with
the domination ordering used for f , then the result C = f (A, B) is represented by a labeled
value set containing a non-dominated subset of the labeled value pairs (f (ai , bj ), ei  ej )
constructed from every pair of labeled value pairs (ai , ei )  A and (bj , ej )  B.

Proof The lemma follows from the argument that the cross product of applying f to all
the labeled value pairs in A and B produces all possible labeled value pairs. Since we must
ensure that the labeled value pairs include all the dominant labeled value pairs for C, we
require f to be consistent with the ordering so that the dominant values of A and B, which
are all we have available, are sufficient to derive the dominant labeled value pairs of C. 
We conclude this section with an example of relaxation of a labeled distance graph,
which is a core rule of inference for most weighted graph algorithms that uses the path
A  B  C to compute a possible path weight A  C. The derived path length replaces
the old path if the newly derived path length is shorter.
627

fiConrad & Williams

Example 5.14 Consider the labeled distance graph in Figure 4.1. We compute W (A, B)+
W (B, C) = {(1, {})}+{(4, {x = 2})} = {(5, {x = 2})}. Compare this to the existing known
weights W (A, C) = {(3, {x = 1}), (7, {x = 2})}, and we determine that the new value
replaces the old value of 7. After the update, W (A, C) = {(3, {x = 1}), (5, {x = 2})}.

There are two fundamental inferences performed with labeled value sets: relaxation of
weighted edges during compilation, and sums and differences of edge weights to compute
bounds on execution times during dispatch. Both follow the framework outlined here, and
are explained in more detail in the compilation and dispatch sections, respectively. These
operations complete the definition of labeled value sets and the following sections use them
to construct compact compilation and dispatching algorithms.

6. Dispatching Plans with Choice
Given this foundation in Labeled STNs, labeled value sets, and labeled distance graphs, we
turn to the central focus of this article - dynamic execution of Labeled STNs. Recall that
the dispatcher uses a local, greedy algorithm to make decisions at run-time with low latency,
and that the accuracy of this approach is guaranteed by the compilation step. We begin
with the dispatcher because low latency execution is the fundamental goal of this work and
because, as in prior work, the compiler is designed to produce an output appropriate for
this dispatcher.
We adapt the STN dispatcher developed by Muscettola et al. (1998) to work with
dispatchable labeled distance graphs. Essentially, Drakes algorithms substitute real number
bounds on execution times, as in the STN dispatcher, with labeled value sets. Additionally,
we adapt Tsamardinos et al.s (2001) approach to reasoning over multiple possible options,
that is, allowing the dispatcher to accept a proposed execution time for an event if at least
one full assignment to the choices is consistent with that schedule. We present Drakes
dispatching algorithms by first reviewing standard STN dispatching, and then adapt these
techniques to handle labels.
6.1 STN Dispatching
Muscettola et al. (1998) showed that given a dispatchable form of the STN, a simple greedy
dispatcher can correctly execute the network with updates performed only on neighboring
events in the dispatchable form of the STN. The dispatcher loops over the non-executed
events at each time step, selecting an event to execute if possible, or else waiting until the
next time step. This process continues until either all the events are executed or a failure
is detected.
Determining whether an event is executable relies on two tests. First, the dispatcher tests
whether the ordering constraints of an event have been satisfied, which is called testing for
enablement. A simple temporal constraint may imply a strict ordering between two events,
which the dispatcher must explicitly test to ensure that an event is not scheduled before an
event that must precede it. Second, the dispatcher efficiently tracks the consequences of the
simple temporal constraints between the event and its neighbors by computing execution
windows for each event. Execution windows are the tightest upper and lower bounds derived
for each event through one-step propagations of execution times. If the current time is in
628

fiDrake: An Efficient Executive for Temporal Plans with Choice

an events execution window and it is enabled, the event may be scheduled at the current
time.
Now we briefly recall the derivation of these two rules for executing events. Recall that
each weighted edge in the distance graph corresponds to an inequality
B  A  wAB

(11)

where A and B are execution times of events and l is some real number bound. If we
select execution time tA for event A and B is not yet scheduled, then we can rearrange the
inequality as
B  wAB + tA

(12)

This produces a new upper bound for the execution time of B. Likewise, if A is not yet
scheduled and we select tB as the execution time of B, then we can rearrange the inequality
as
A  tB  wAB .

(13)

Thus, we derive a new lower bound for A. If, in this form, wAB < 0, then B < A, so event
B must precede A, implying an enablement constraint.
We can recast these rules in terms of propagations on the distance graph. If an event
A is scheduled at time t, then propagate it through all outbound edges (A, B) to derive
upper bounds B  wAB + t, and through all inbound edges (B, A) to derive lower bounds
B  t  wBA . Event B has event A as a predecessor if there is a negative weight edge
(B, A). As usual, dispatching is only affected by the dominant upper and lower bounds, so
the dispatcher only stores the dominant constraints.
Example 6.1 Consider the dispatchable distance graph fragment in Figure 6.1. The execution windows begin without constraint,   A   and   B  . If we begin
execution at t = 0, B is not executable yet because it is not enabled; the negative weighted
edge (B, A) implies that A must be executed first. A has no predecessor constraints and
zero lies within its execution bound, so may be executed at this time. Propagating the time
A = 0 allows us to derive the bounds 2  B  8. Then B may be executed at any time
between 2 and 8. If the dispatcher reached time 9 without having executed B, then it must
signal a failure.


A

8
-2

B

Figure 6.1: A dispatchable distance graph fragment.
The final consideration for the dispatcher is zero-related events. If two events are constrained to be executed at precisely the same time, then the prior work requires that the
events are collapsed into a single vertex in the dispatchable graph. Otherwise, zero-related
vertices may cause the dispatcher to make mistakes because they must occur together, yet
629

fiConrad & Williams

they appear independently schedulable. Equivalently, the dispatcher may simulate the collapse by always executing zero-related vertices as a set. For example, if A and B are known
to be zero-related, the dispatcher may schedule them together, scheduling the events at time
t if the execution windows and enablement constraints of both A and B are satisfied. Note
that since they are zero-related, it is impossible to have an enablement constraint between
them.
6.2 Labeled STN Dispatching
Drake relies on these same fundamental structures and rules as the STN dispatcher, but
modifies each step to consider labels. Since edge weights are labeled value sets, the execution
windows are also labeled value sets, implying that the upper and lower bounds of execution
times for events may vary depending on the assignments to choices and may vary separately.
For each possible bound or enablement constraint, Drakes dispatcher must either enforce
the constraint, or discard the constraint and decide not to select the associated environment.
Broadly, this is the same strategy as Tsamardinos et al. (2001), where the component STNs
are dispatched in parallel, and proposed scheduling decisions may be accepted if they are
consistent with at least one STN.
We begin by considering how to update the propagation rules to derive execution windows. The STN propagations involve adding edge weights (or negative weights) to the
execution time of an event. The upper bound of every event is initially {(, {})} and
every lower bound is initially {(, {})}. Upper bounds are dominated by low values,
or the  inequality, and lower bounds are dominated by large values, or the  inequality.
The following theorem describes the propagation of execution bounds, which is the labeled
implementation of Equations 12 and 13.
Theorem 6.2 If event A is executed at time t, then consider some other event B. For
every (wAB , eAB )  W (A, B), (wAB + t, eAB ) is a valid upper bound for execution times
of B and for every (wBA , eBA )  W (B, A), (t  wBA , eBA ) is a lower bound for execution
times of B.

Proof These rules are a direct extension of the STN propagation to the labeled case using
labeled operations as in Definition 5.12. Since the execution actually occurred, it holds
under all possible environments, thus we give it the empty environment {}. Then we apply
Definition 5.12, substituting the labeled version of addition.

At dispatch, these new bounds are added to the labeled value sets Bu and Bl , which
maintain the non-dominated bounds.
{(5, {x = 1}), (2, {})}
A

B
{(7, {x = 1}), (8, {})}

Figure 6.2: A dispatchable labeled distance graph fragment.

630

fiDrake: An Efficient Executive for Temporal Plans with Choice

Example 6.3 Consider the labeled distance graph fragment in Figure 6.2. If event A
is executed at t = 2, then we derive bounds {(7, {x = 1}), (4, {})}  B  {(9, {x =
1}), (10, {})}

Since the bounds may vary between choices, Drake cannot generally expect to obey all
the possible constraints, but instead is only required to enforce all the constraints implied
by at least one complete environment.
Example 6.4 Assume an event has lower bounds represented by the labeled value set
A  {(2, {x = 1}), (0, {})}. This implies that under any set of choices where x = 1, A  2,
and otherwise, A  0 is sufficient. If the dispatcher executes A at t = 0, then it may not
select x = 1. Thus, if the dispatcher can restrict its choices to those that do not include
x = 1, while leaving other consistent options, then it may execute A at t = 0. If all the
remaining consistent full assignments to the choices require that x = 1, then the dispatcher
must wait until t = 2 before executing A.

Drake performs this reasoning by collecting the environments for the bounds that a
particular execution would violate, and by determining whether those environments can
be made conflicts without making every complete environment inconsistent. If complete
environments would remain consistent, the execution is performed and then environments
are made conflicts; otherwise the execution is not possible, and discarded.
Finally, Drake simulates the collapse of zero-related events. At each time, Drake attempts to execute each event individually and also attempts to execute the sets of zerorelated vertices recorded by the compiler. As noted before, zero-related sets may exist under
some environments and not in others. If the zero-related set is enforced, then all of its member events must be executed together, or not at all. Therefore, if the dispatcher considers
executing a set of events that is a strict subset of a particular zero-related set, leaving out at
least one other member of the zero-related set, then it must discard the environments where
the zero-related set is implied. Additionally, while there cannot be enablement constraints
between zero-related events under any complete environments where the zero-related group
holds, other environments may imply strict orderings between the events, which the dispatcher discards when executing the zero-group simultaneously. Hence, the dispatcher must
discard any associated environments.
We summarize the rules as follows, which are further illustrated in Example 6.7.
Theorem 6.5 If S is set with one event or a set of zero-related events in a Labeled STN,
then the set may be executed at time t if and only if there is at least one consistent complete
environment where:
1. For every Al that is the lower bound labeled value set of some A  S, such that Al  A,
for every (l, e)  Al such that l > t, e is a conflict.
2. For every Au that is the upper bound labeled value set of some A  S, such that
Au  A, for every (u, e)  Au such that u < t, e is a conflict.
3. For every pair of events, A  S, B 
/ S, such that B is not yet scheduled, for every
(w, e)  W (A, B) such that w < 0, e is a conflict.
631

fiConrad & Williams

4. For every zero-related set of events Z with environment e, if S  Z, e is a conflict.
5. For every A1 , A2  S, for every (w, e)  W (A1 , A2 ) such that w < 0, e is a conflict.
Proof If there is a consistent full environment after creating these conflicts, then it is
necessarily not subsumed by the environments of any of the constraints that imply the
events cannot be executed at time t. Thus, the consistent full environment corresponds to a
component STN where every constraint holds, so the events may be executed. If there is no
consistent environment, then at least one of the above constraints prohibiting the proposed
execution exists in each remaining component STN, so the execution is not valid.

Similarly to the STN case, the dispatcher must check for missed upper bounds during
every time step. In an STN, a missed upper bound implies the execution has failed. In a
Labeled STN, a missed upper bounds implies that any complete environments subsumed
by the environment of the missed upper bound are no longer valid.
Theorem 6.6 If event A is not executed at time t, has upper bound labeled value set Au ,
and (u, e)  Au such that u < t, then e is a conflict.

Proof The theorem follows from noting that the upper bound exists in every component
STN corresponding to a complete environment subsumed by e, thus every environment
subsumed by e is inconsistent, which by definition, makes e a conflict.

It is possible that missed upper bounds could invalidate all remaining complete environments, in which case dispatch has failed and the dispatcher should signal an error
immediately.
Example 6.7 Consider the dispatchable labeled distance graph in Figure 6.3, with events
A, B, C, D and choice variables x, y  {1, 2}. There is a zero-related set {A, C} under
environment {x = 1}. Assume that the current time is t = 0 and no events have been
executed yet. All the possible complete environments are initially consistent. Consider
some possible executions and their consequences.
 Event C has no predecessors, and no restrictions from its execution window. However,
it is part of the zero-related set, so it may only be executed if we can make {x = 1}
a conflict, in order to remove the zero-related set from all possible executions. This
is feasible, so we may execute C at t = 0 and create the conflict. If we do this,
then the ordering and inequality implied by edge (A, D) is necessary for a consistent
execution, as its environment cannot be made a conflict without making all complete
environments inconsistent.
 Event A has D as a predecessor under {x = 2}, C as a predecessor under {y = 1},
and is part of the zero-related set under {x = 1}. To execute A at t = 0, we would
need to make all three environments conflicts, but that would invalidate all possible
choices, so we may not execute A at t = 0.
632

fiDrake: An Efficient Executive for Temporal Plans with Choice

 The zero-related set {A, C} has no execution window restrictions from either A or
C, and has a predecessor D under {x = 2} from the edge (A, D). Additionally, edge
(A, C) has a negative weight 1 with environment {y = 2}, so that environment is
also made a conflict. Thus, we can create the conflicts {x = 2} and {y = 2} and
execute both A and B at t = 0.
 Event B has A as a predecessor under {y = 1} and A is not yet executed, so we may
make that environment a conflict and execute B. Assume that A and C are executed
at t = 0, then the enablement constraint is satisfied, but any execution of B before
t = 3 requires making {y = 1} a conflict. Additionally, if the current time grows to
t = 7 and B has not yet been executed, then the upper bound has been violated and
{y = 1} is then a conflict.
 Event D has predecessor C under environment {}, but C has not yet been executed.
Since making {} a conflict makes all complete environments inconsistent, D cannot
be executed.

{(3, {y = 1})}
A

B
{(6, {y = 1})}

{(2, {x = 2})}
{(0, {x = 1}),
(1, {y = 1})}

{(0, {x = 1})}

{(1, {})}
C

D
{(6, {y = 2})}

Figure 6.3: A dispatchable labeled distance graph.
This completes our presentation of Drakes dispatch algorithms. Essentially, it makes
two adaptations to the STN dispatcher: (1) maintain labeled value sets for the execution
windows of events and (2) allow the dispatcher to select between the choices by creating conflicts. The next section describes the compilation algorithm, which computes the
dispatchable form of input Labeled STNs, ensuring that the local reasoning steps in the
dispatching algorithm will satisfy the requirements of the plan.
633

fiConrad & Williams

7. Compiling Labeled Distance Graphs
We complete the description of Drake with the compiler, which reformulates the input Labeled STN into a form the dispatcher is guaranteed to execute correctly. The compiler
leverages all of the labeling concepts we presented to efficiently compute a compact dispatchable form of input plans. An STN compiler takes a distance graph as its input and
outputs another distance graph, the minimal dispatchable form of the input problem. Similarly, Drakes compiler takes a labeled distance graph as its input and outputs a labeled
distance graph that is the minimal dispatchable form of the input.
Muscettola et al. (1998) introduced an initial compilation algorithm for STNs that
operates in two steps. First, it computes the All-Pairs Shortest Path (APSP) graph associated with the STN, which is itself a dispatchable form. Second, the compiler prunes any
edges that it can prove are redundant to other non-pruned edges, and which the dispatcher
therefore does not need to make correct decisions. The pruned edges are dominated, and
their removal significantly reduces the size of the dispatchable graph. The compiler tests
for dominance by applying the following rule on every triangle in the APSP graph.
Theorem 7.1 (Triangle Rule, Muscettola et al., 1998) Consider a consistent STN
where the associated distance graph satisfies the triangle inequality; a directed graph satisfies the triangle inequality if every triple of vertices (A, B, C) satisfies the inequality
W (A, B) + W (B, C)  W (A, C).
(1) A non-negative edge (A, C) is upper-dominated by another non-negative edge (B, C)
if and only if W (A, B) + W (B, C) = W (A, C).
(2) A negative edge (A, C) is lower-dominated by another negative edge (A, B) if and
only if W (A, B) + W (B, C) = W (A, C).

Although the basic concept of domination is unchanged, in that dominated constraints
are not needed by the executive, the specifics are quite different here. Domination within a
labeled value sets refers to labeled values that make other labeled values within that same
set unnecessary. In the context of edge pruning, one edge could dominate another if the
two edges share a start or end vertex, but not both. When we develop the labeled version
of edge pruning, the labeled value from one edge weight labeled value set can dominate a
value from a different edge weight labeled value set, where the corresponding edges share a
start or end vertex.
Example 7.2 Consider the two weighted graph fragments in Figure 7.1. In each case, edge
(A, C) is dominated, as shown by the theorem.

The compiler searches for all dominated edges and then removes them all together. Some
care is necessary to ensure that a pair of edges AC and BC do not provide justification
for pruning each other; these edges are said to be mutually
dominant. Typically, pruned

graphs have a number of edges closer to O N log N , than N 2 , a significant savings. As a
first prototype of Drake, Conrad, Shah, and Williams (2009) introduced a labeled extension
of this algorithm. This extension was used to perform task execution on the ATHLETE
Rover within the Mars Yard at NASA JPL.
Although simple and effective, this algorithm does not scale gracefully to large problems
because it uses the entire APSP graph an intermediate representation, which is much larger
634

fiDrake: An Efficient Executive for Temporal Plans with Choice

A

-5

-4

1

B

C

A

2

5

3

B

C

(a)
Lower
domination
weighted graph

(b)
Upper
domination
weighted graph

Figure 7.1: Examples of upper and lower dominated edges.
than the final result. To resolve this, Tsamardinos, Muscettola, and Morris (1998) presented
a fast compilation algorithm that interleaves the APSP step and the pruning step, and avoids
ever storing the entire APSP graph. Their algorithm is derived from Johnsons algorithm for
computing the APSP, which incrementally builds the APSP by repeated SSSP computations
(Cormen, Leiserson, Rivest, & Stein, 2001). Johnsons algorithm uses Dijkstras algorithm
as an inner loop, providing it with faster performance than Floyd-Warshall for sparse graphs.
Essentially, the fast compilation algorithm loops over the events in the graph, computes the
SSSP for that event, and adds all non-dominated edges with the event as the source to the
dispatchable form. Thus, this algorithm only needs to store the final dispatchable graph
and one SSSP, avoiding the bloat in the intermediate representation. Thus, this algorithm
has better space and time complexity than the APSP step followed by pruning.
This paper introduces a novel extension to the Drake system that adapts the fast compilation algorithm to labeled graphs in order to avoid unnecessary storage growth during
compilation. The fast algorithm requires a number of steps; this section describes each
component of the original fast algorithm and its adaptation to the labeled setting. First,
we recall Johnsons strategy to compute the APSP through iterated SSSP, and present
the labeled adaptation of the SSSP algorithm. Second, we discuss the predecessor graphs
that result from the SSSP and how to traverse them. Third, we describe how to interleave
pruning with the repeated SSSP computations. Finally, we discuss the issues arising from
mutual dominance, and the preprocessing step used to resolve them.
7.1 Johnsons Algorithm and the Structure of the Fast Algorithm
Many shortest path algorithms on weighted distance graphs essentially perform repeated
relaxation over the graph. Floyd-Warshall loops repeatedly over the entire graph, relaxing
the edges and computing the entire APSP in a single computation. Johnsons algorithm
computes the APSP one source vertex at a time, using Dijkstras SSSP algorithm as an
inner loop to perform relaxations more efficiently. Since Dijkstras algorithm only works
for positively weighted graphs, Johnsons algorithm re-weights the graph before Dijkstra
calls and un-weights it afterward. Unfortunately, adapting Dijkstras algorithm to labeled
distance graphs is inefficient because re-weighting both adds and subtracts labeled value
sets, and both are not compatible with a single ordering. Therefore, we use BellmanFords SSSP algorithm instead; we sacrifice the improved run-time of the fast algorithm,
635

fiConrad & Williams

but preserve the lower space overhead. The fast STN compilation algorithm copies the
essential structure of Johnsons algorithm. This section illustrates the overall algorithm in
the unlabeled case with a partial example.
Example 7.3 Consider one step of compilation shown in Figure 7.2. The input distance
graph is shown in Figure 7.2a. When we compute the SSSP of the input for source vertex
B, the output is the predecessor graph shown in 7.2b, which depicts the shortest distances
and paths from B. The weights on the vertices indicate that the complete APSP graph
should contain an edge from B to A with weight 4, B to C with weight 1, and B to D
with weight 0. Furthermore, in the original graph, the shortest path from B to A uses the
edge B  A. The shortest path from B to C is either B  C or B  A  C; both have
equal length. The shortest path from B to D is B  D.
However, the dispatchable form of this problem does not actually need all three of
these edges, which the compiler can deduce from the predecessor graph and SSSP values.
Roughly, since A has a lower distance from B than C, and is along a shortest path to C, the
edge BC is not necessary. Therefore, the other two edges are inserted in the dispatchable
graph, depicted in Figure 7.2c, and the edge BC is discarded. This process is repeated for
the other three vertices.


4
A

-4
8

A

B

B

A

0 1

3 -1

B

0

5

-1

C
C

-4

3

D

(a) Input Distance Graph

D

1

1

(b) Predecessor Graph for Source
B

C

D

(c) Partial dispatchable
graph with edges that have
source vertex B

Figure 7.2: A single step of the fast reformulation algorithm for STNs

7.2 Labeled Bellman-Ford Algorithm
Drake uses the Bellman-Ford algorithm as a central building block for the variant of the fast
STN algorithm Drake uses; Bellman-Ford derives the tightest possible edge weights, while
simultaneously deriving the predecessor graph. This graph provides enough information
to prune the dominated edges. To begin, we provide Algorithm 7.1, taken directly (with
slightly altered notation) from the work of Cormen et al. (2001). The algorithm loops over
the edges and performs relaxations, then tests for negative cycles to ensure the result is
valid. The value d[v] is the distance of the vertex v from the input source vertex s. The
value [v] is the vertex that is the (not necessarily unique) predecessor of v when forming
636

fiDrake: An Efficient Executive for Temporal Plans with Choice

a shortest path from s to v. Relating to Figure 7.2b, d corresponds to the annotations next
to the vertices, and  specifies the directed edges.
Algorithm 7.1 Bellman-Ford Algorithm
1: procedure BellmanFord(V, E, W, s)
2:
InitializeSingleSource(V, W, s)
3:
for i  {1...|V |  1} do
4:
for each edge (u, v)  E do
5:
Relax(u, v, W )
6:
end for
7:
end for
8:
for each edge (u, v)  E do
9:
if d[v] > d[u] + W (u, v) then
10:
return false
11:
end if
12:
end for
13:
return true
14: end procedure
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:

. Loop for relaxation

. Check for negative cycles
. Fail if a negative cycle is found

procedure InitializeSingleSource(V, s)
for each vertex v  V do
d[v]  
[v]  nil
end for
d[s]  0
end procedure
procedure Relax(u, v, W )
if d[v] > d[u] + W (u, v) then
d[v]  d[u] + W (u, v)
[v]  u
end if
end procedure

As we will see in the next few sections, the fast compilation algorithm for STNs requires
that we have access to the entire predecessor graph, which encodes all shortest paths in the
graph, rather than any single path. To compute the entire graph, Drake makes each [v] a
set, and if Relax determines that d[v] = d[u] + W (u, v), it then pushes u onto d[v]. When
the strict inequality holds and d[v] is updated, [v] is set to the single element set {u}.
Now we discuss the changes necessary to adapt the Bellman-Ford algorithm to Labeled
Distance Graphs. The relaxation procedure computes the new bound on the path length
from s to v, and if it dominates the old value, replaces the old value. If the vertex u allows
us to derive the current dominant value d[v], then it is stored in [v]. Drakes use of labeled
value sets that store value pairs (d, )  R  V allow it to perform both tasks at once, with
one data structure.
637

fiConrad & Williams

To see how this works, consider an example:
Example 7.4 In Figure 7.3a, edge (A, B) forms a snippet of a labeled distance graph;
consider computing the SSSP with source vertex A. We know that the initialization routine
under every choice is to set d[u] = 0 for the source vertex and d[u] =  for all others, and
to set [u] = nil for every vertex. In the combined notation, the initial value for the weight
of the vertices is (0, nil) or (, nil), labeled with the empty environment {}. These are
the initial weights shown on the vertices.
The relaxation of this edge allows us to derive that B is distance 8 from A under environment {x = 1}, and has predecessor A. Hence, we add the labeled value pair ((8, A), {x = 1})
to the labeled value set for the weight of B. This new value has a tighter value for d, and
hence is kept, but does not dominate the old value, thus both are kept. This single insertion
step updates both the distance of B from A and the predecessor of B along the shortest
path. The result is depicted in Figure 7.3b.


It should now be clear why we elected to keep non-identical values with the same numeric
value: the labeled value sets can hold all the predecessors for the target vertices, and not just
a single predecessor. Algorithm 7.2 shows the generalization of this example. Initialization
is exactly as in the example, it assigns the initial values from the standard algorithm with
empty environments. During relaxation, following Lemma 5.13, the potential new path
lengths are the cross product addition of the weight of u added to the weight of the edge
from u to v, labeled with the union of the environments, if the union is valid and not
subsumed by any known conflicts. Naturally, when relaxing u through an edge to v, u
is the predecessor. When adding this value to the labeled value set d[v], the domination
criterion is used to determine if this new path length is non-dominated, and to prune nondominant path lengths and predecessors as appropriate.
The final modification of the original algorithm is in the test for negative cycles; a
negative cycle is detected if another relaxation of any edge would further decrease the
distance from the source to any other vertex. In the labeled case, some choices may have
negative cycles and others may not. We begin the computation by computing the relaxation
d[u] + W (u, v) with the labeled addition operator, and discard the predecessor vertices from
d[u], thus producing a real valued labeled value set. If there is any environment where
d[v] > d[u] + W (u, v) holds, that environment is a conflict of the Labeled STN.
Finding the minimal conflicts in practice is somewhat convoluted. Consider each pair
of labeled values (dv , ev )  d[v], (duw , euw )  d[u] + W (u, v) in turn. If dv > duw then there
might be a conflict, however, there may be some environments where a smaller, dominant
value of d[v] takes precedence over dv , thus preventing the inequality from being satisfied.
Hence, if it exists, the conflict we deduce is ev  euw after modifying the union to avoid any
other environments e0v such that (d0v , e0v )  d[v] and d0v  duw . It is possible that conflicts
could invalidate all possible choices, which the executive tests for, before determining that
the plan is dispatchable.

638

fiDrake: An Efficient Executive for Temporal Plans with Choice

Example 7.5 Consider the following values for the inequality test, given two binary choice
variables:
d[v] = {(2, {y = 1}), (4, {})}

(14)

d[u] + W (u, v) = {(3, {x = 1}), (5, {})}

(15)
(16)

We must find and make conflicts for any minimal environments that imply d[v] > d[u] +
W (u, v) holds. Each side of the inequality has two possible values, so we can consider
four pairs of values that could satisfy the inequality. Three of the pairs do not satisfy the
inequality, and thus can cause no conflict: (2, {y = 1}) < (3, {x = 1}), (2, {y = 1}) < (5, {})
and (4, {}) < (5, {}). The last pair, (4, {}) > (3, {x = 1}), satisfies the inequality, so
this pair could imply that the union of corresponding environments, {}  {x = 1}, is a
conflict. However, to reach (4, {}) for the left hand side, we skipped over a smaller value,
(2, {y = 1})  d[v]. To correctly address this ordering, the conflict must also avoid the
environment for the smaller value. Taking {}  {x = 1} and avoiding {y = 1} produces one
environment, {x = 1, y = 2}, which is a valid environment, and hence it is a conflict.


{((0 , nil) ,{})}

{(( , nil) ,{})}

A

B

{(8, {x = 1})}
(a) Before Relaxation

{((0, nil), {})}
A

{((8, A), {x = 1}), ((, nil), {})}
{(8, {x = 1})}

B

(b) After Relaxation

Figure 7.3: A single labeled relaxation step.

7.3 Traversals of Labeled Predecessor Graphs
The fast compilation algorithm for STNs performs traversals of the predecessor graphs
produced by the SSSP analysis, and checks each edge along the traversal for dominance.
This is used to reduce the graph to minimal dispatchable form. In the unlabeled case, the
[u] values specify a directed graph such as in Figure 7.2b, where we may use depth-first
exploration to enumerate all possible paths beginning with the source vertex the SSSP is
computed for. However, in the labeled case, more care is necessary.
The simplest way to understand which paths are valid under complete environments is
to consider the projection of the predecessor graph under a particular complete environment
e. Take every labeled value set d[u] and query it under e. The result gives the shortest path
distance and any predecessors, producing a standard unlabeled predecessor graph.

639

fiConrad & Williams

Algorithm 7.2 Labeled Bellman-Ford Algorithm
1: procedure LabeledBellmanFord(V, E, W, S, s)
2:
LabeledInitializeSingleSource(V, W, s)
3:
for i  {1...|V |  1} do
. Loop over relaxations
4:
for each edge (u, v)  E do
5:
LabeledRelax(u, v, W )
6:
end for
7:
end for
8:
for each edge (u, v)  E do
. Test for negative cycles
9:
for each labeled value pair (dv , ev )  d[v] do
10:
for each labeled value pair (duw , euw )  d[u] + W (u, v) do
11:
if dv > duw then
12:
AddConflict(ev  euw , split to avoid all e0v such that
13:
(d0v , e0v )  d[v] and d0v  duw )
14:
end if
15:
end for
16:
end for
17:
end for
18:
IsSomeCompleteEnvironmentConsistent?()
19: end procedure
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:

procedure LabeledInitializeSingleSource(V, s)
for each vertex v  V \ s do
d[v]  {((, nil), {})}
end for
d[s]  {((0, nil), {})}
end procedure
procedure LabeledRelax(u, v, W )
for labeled value pair ((du , u ), eu )  d[u] do
for labeled value pair ((dw , w ), ew )  W (u, v) do
AddToLVS(d[v], ((du + dw , u), eu  ew ))
end for
end for
end procedure

640

fiDrake: An Efficient Executive for Temporal Plans with Choice

Example 7.6 Consider the labeled graph fragment in Figure 7.4a, where the vertices are
labeled with their SSSP weights for source vertex A. Further consider the predecessor
graph and path lengths implied under environment e = {x = 1, y = 1}: d[B](e) = (2, A)
and d[C](e) = (1, A). Hence, both vertices B and C have A as their only predecessor, and
B is distance 2 from A, and C is distance 1 from A, as shown in Figure 7.4b. Vertex D is
distance 6 from A, with predecessor C. An important feature of this example is that for
this environment, C is not a predecessor of B, even though the pair (7, C) is labeled with
{y = 1}, which subsumes e. Predecessors are only provided by the dominant path length.

{((2, A), {x = 1, y = 1}),
((7, C), {y = 1}),
((, nil), {})}

{((0, nil), {})}

{(2, {x = 1, y = 1})}
A

B
{(4, {y = 1})}

{(1, {x = 1}), (3, {})}

{(5, {y = 1})}
C

D
{((6, C), {x = 1, y = 1}),
((8, C), {y = 1}),
((, nil), {})}

{((1, A), {x = 1}),
((3, A), {})}

(a) Relaxed Graph

0

2

A

B

C

D

1

6

(b) Predecessor graph for
e = {x = 1, y = 1}

Figure 7.4: Extracting predecessor graphs from labeled SSSP
Although it is instructive to consider the projection of the SSSP onto an environment for
intuitive understanding, an implementation based on this approach is not efficient in space
or time. As in the rest of this work, Drake directly performs the traversals on the labeled
representation. Since paths in the predecessor graphs may exist under some environments
and not others, we take the natural step and use minimal environments to specify when a
641

fiConrad & Williams

particular path exists. Intuitively, we expect that the environment of a path is simply the
union of the environments of the labeled value pairs that specified the predecessor edges
used. This is true, but we require an extra step to test the validity of the path for that
environment.
Consider a partial path from the source vertex S, which passes through some subset of
vertices X1  . . . Xn , constructed from labeled value pairs ((di , Xi ), ei )  d[Xi+1 ], which
represent some path through the predecessor graph. Then the path is valid if, for every
vertex along the path, (di , Xi )  d[Xi+1 ](e1  . . . en ), and the path environment is e =
e1  . . . en . Essentially, this tests that labeled value pairs used to construct the path are
actually still entailed by the union of all of their respective environments. If a path is
invalid, all possible extensions are also invalid, because the same inconsistency will exist in
all extensions.
Example 7.7 We can apply this criteria to determine all the paths given by the SSSP in
Figure 7.4a. Begin with the source vertex, A, which clearly exists under all environments,
so we assign this partial path the empty environment. B contains the labeled value pair
((2, A), {x = 1, y = 1}), and thus is a candidate to extend our path. The union of the two
environments is {}  {x = 1, y = 1} = {x = 1, y = 1}. When queried with this environment,
d[B] returns the labeled value pair that we used to propose the extension, so the path
A  B has environment {x = 1, y = 1}. B has no other possible extensions.
Returning to A, it has a possible extension to C because of the labeled value pairs
((1, A), {x = 1}) and ((3, A), {}). First, consider ((1, A), {x = 1}), and the query of d[C]
under {x = 1} shows that the path exists, so we have the partial path A  C with
environment {x = 1}. This path has a potential extension to B because of the labeled
value pair ((8, C), {y = 1}). However, the query d[B]({x = 1}  {y = 1}) = (2, A) 6= (7, C),
so the extension is not valid, as expected from our earlier discussion. We may instead extend
this path to D, giving the path A  C  D the environment {x = 1, y = 1}. The queries
on both d[C] and d[D] give the values used to generate the path, and hence the path is
valid.
Returning to A, we consider the other path to B, using the labeled value pair ((3, A), {}).
Now the extension to B passes the query test, so A  C  B provides path length 7 if
{x = 1} but not {y = 1}. We label this path with the label {x = 1}, but must take care in
later algorithms not to use it to imply that 7 is the shortest path length for {x = 1, y = 1}.
Finally, we can extend the path to D, giving path A  B  D with length 8 under {y = 1},
with the same caveat.

Given this rule specifying the correct extensions to partial paths, we can construct a
depth-first search to enumerate all possible paths in the acyclic predecessor graph, re-testing
at each step to ensure the path is valid.
7.4 Pruning Dominated Edges with the SSSP
With the strategy for finding labeled paths through the labeled predecessor graphs, the
extension of the pruning algorithm from the fast STN compilation algorithm is straightforward. Tsamardinos, et al. (1998) provide two theorems relating dominance of edges to
paths in the predecessor graph, adjusted slightly for our notation. In the following, A is
assumed to be the source vertex of the SSSP, and B and C are other vertices.
642

fiDrake: An Efficient Executive for Temporal Plans with Choice

Theorem 7.8 A negative edge (A, C) is lower-dominated by a negative edge (A, B) if and
only if there is a path from B to C in the predecessor graph for A.

Theorem 7.9 A non-negative edge (A, C) is upper-dominated if and only if there is a
vertex B, distinct from A and C, such that d[B]  d[C] and there is a path from B to C in
the predecessor graph for A.

Example 7.10 Figure 7.5 shows two simple examples on which we can apply these two
theorems. First, Figure 7.5a is a weighted distance graph and Figure 7.5c shows its predecessor graph for source vertex A, on which we apply Theorem 7.8. Edge (A, C) is lower
dominated because the weight 5 on B implies an edge (A, B) with weight 5, which is
negative as the theorem requires. Furthermore, the predecessor graph has a path from B
to C in the predecessor graph for A. Therefore, edge (A, C) is dominated and needed in
the dispatchable form.
Figures 7.5a and 7.5c similarly exhibit Theorem 7.9. There is a path A  B  C in
the graph, and d[B] = 2  d[C] = 5, so edge (A, C) is upper-dominated. Thus, edge (A, C)
is not needed in the dispatchable form.
In both these examples, this step derives every possible edge weight for edges in the
dispatchable form with A as their source, namely edges (A, B) and (A, C), and determines
that only (A, B) is actually needed.

In the labeled case, particular labeled edge weights are dominated if the above conditions
hold under all the environments the weight holds in.
Theorem 7.11 A negative labeled edge weight (dC , eC ) in d[C] is lower-dominated by a
negative labeled edge weight (dB , eB ) in d[B] if and only if eB = eC and there is a path from
B to C with environment eP in the predecessor graph for A such that eP = eC .

Theorem 7.12 A non-negative labeled edge weight (dC , eC ) in edge d[C] is upper-dominated
if and only if there is a labeled edge weight (dB , eB ) in d[B], such that B is distinct from A
and C, eB subsumes eC , dB  d, C, and there is a path from B to C with environment eP
in the predecessor graph for A such that eP = eC .

In practice, Drake searches over every path in the labeled predecessor graph with the
source vertex as its start, and applies these theorems to find dominated edges. Specifically,
during the traversal, it records the smallest vertex weight of any vertices along the path,
not counting the source. That value is compared to other vertex weights of extensions of
the path to apply the domination theorems. Every time a vertex weight is found to be
dominated with some path, it is recorded in a list. After all traversals are done, every
labeled value in the vertex weights not present in the list of dominated values is converted
into an edge in the output dispatchable graph.
These two theorems require that eP = eC because the path must hold under all environments where the value dC does, but we also do not want eP to be tighter. Recall that
the vertex weights we might prune also specify the paths. If eC is tighter than eP , then it
must have a lower path length than the one implied by eP , or else it would not be in the
labeled value set d[C]. Thus, we cannot guarantee that our path is the shortest path from
the source to C, so this path is not suitable to prune it.
643

fiConrad & Williams

A

-5

-4

1

B

C

A

2

5

3

B

C

(a)
Lower
domination
weighted graph

(b)
Upper
domination
weighted graph

0

-5

0

2

A

B

A

B

C

C

-4

5

(c) Lower domination predecessor graph

(d) Upper domination predecessor graph

Figure 7.5: Simple edge domination examples.
Example 7.13 To demonstrate the application of these ideas, reconsider Figure 7.4a. Example 7.6 gave the possible paths in the graph. The first path is A  B with path
environment {x = 1, y = 1}. After reaching B, the minimal vertex weight is 2, but there
are no extension of this path, so nothing can be pruned. In general, the first step from the
source vertex cannot be pruned.
The next step of the traversal reaches C with environment {x = 1}, and the minimal
path length is 1. C cannot be pruned. This path cannot be extended to B, but it does
have an extension to D, using the vertex weight ((6, C), {x = 1, y = 1}). This path length
is strictly longer than the path length of 1, and the environment of the value is equal to the
environment of the path, so we add it to the pruned list.
Next consider path A  C under environment {} and path length 3. The extension
to B using ((7, C), {y = 1}) is also prunable. Note that this path could never be used to
prune the shorter path length ((2, A), {x = 1, y = 1}) because {x = 1, y = 1} =
6 {y = 1}.
Likewise, the extension to D prunes ((8, C), {y = 1}).
Collecting the non-pruned edges means that the algorithm adds edge (A, B) to the
output dispatchable graph with weight W (A, B) = {(2, {x = 1, y = 1})}, and adds edge
(A, C) with weight W (A, C) = {(1, {x = 1}), (3, {})}. We drop infinite weights, allowing
them to be implicitly specified, and both finite weights of D were pruned, so it does not
add the edge (A, D) at all.

644

fiDrake: An Efficient Executive for Temporal Plans with Choice

Essentially, the pruning algorithm has the same structure as the unlabeled fast compilation algorithm. The major difference is that the values to prune have environments and
that paths in the predecessor graph only exist under particular environments. Thus, the
pruning step must satisfy the pruning requirement with the identical environment to prune
a labeled value.
7.5 Mutual Dominance and Rigid Components
Tsamardinos et al. (1998) showed that rigid components in the distance graph of an STN
create mutual dominance, where two edges (A, B) and (A, C) are each used as evidence
to prune the other, and both are incorrectly removed when the algorithm of the previous
section is applied. The correct solution is to perform either pruning, but not both. Unfortunately, it is difficult to test for mutual dominance during the SSSP pruning step. Instead,
they present a pre-processing step that identifies the rigid components, updates the output
dispatchable graph accordingly, and alters the input weighted distance graph to remove the
rigid components entirely, before beginning the process of computing edges in the dispatchable form through repeated SSSPs. This leaves a problem with no mutually dominated
edges, and thus the pruning step can prune all edges found to be dominated without any
risk of encountering this problem. This section adapts Tsamardinos et al.s approach by
identifying labeled rigid components and uses a labeled analogue of the alteration process.
Example 7.14 Consider the predecessor graph fragment in Figure 7.6. There is a path
A  B  C, with d[B] = 1 and d[C] = 1, so that d[B]  d[C]. Thus, the edge (A, C)
can be pruned. Likewise, there is a path A  C  B that allows us to prune (A, B) from
the dispatchable graph. Then the result is that A is not constrained to the rest of the
events, which would allow the dispatcher to select an inconsistent schedule. These edges
are mutually dominant. Our algorithm is able to prune both edges because there is a path
B  C and C  B, if either did not exist, only one pruning would take place.


0

1

A

B

C
1
Figure 7.6: A predecessor graph, (A, B) and (A, C) are mutually dominant.
Tsamardinos et al. (1998) show that mutual dominance occurs if and only if there are
rigid components in the problem, which are collections of events that must be scheduled
with a fixed difference in their execution times. In Figure 7.6, B and C are constrained to
occur at the same time, so they are rigidly connected. Rigidly connected components in a
645

fiConrad & Williams

consistent distance graph coincide with strongly-connected components in the predecessor
graph for a single source shortest path for arbitrary connected vertices. Thus, they show
that we can identify all instances of possible mutual dominance by finding all the stronglyconnected components of the predecessor graphs, and then remove them.
Intuitively, we might understand the issue as the input problem having less degrees of
freedom than it appears to have. If two events are actually constrained to occur at the same
instant, then the dispatcher has one decision, not two. When compiling, each decision looks
redundant to the other, leading to incorrect prunings, as in Example 7.14. The solution
is to remove the redundancy by replacing the entire rigid component with a single vertex,
called the leader. The prior work actually removes all events other than the leader with a
preprocessing step, contracting the rigid component to a single vertex. Since we are working
with labeled distance graphs, where rigid components may exist only in some environments,
we must replicate this effect indirectly, in two steps: provide information in the compiled
form so the dispatcher can respect the rigid component, and alter the input graph so that
under conditions where the rigid component exists, the compilation algorithm only reasons
over the leader.
The first step of preprocessing is to identify the rigid components. Arbitrary graphs
might not be connected, so an SSSP from any arbitrary vertex may not reach all the
vertices, and thus cannot find rigid components in the disconnected subgraph. A standard
strategy, as in the Johnsons algorithm preprocessing step, is to introduce an extra vertex
for the preprocessing step, connected to every vertex with a zero weight edge. This extra
vertex is connected to every vertex, which makes it an ideal source to search from, even for
disconnected graphs. Furthermore, the extra vertex does not alter the rigid components in
the predecessor graph. In the labeled case, the equivalent approach is to introduce a new
vertex connected to every other vertex with an edge with weight (0, {}), so that the zero
weight edge applies under every environment. Then, the SSSP is computed with the added
vertex as the source. Finally, the algorithm searches the predecessor graph for cycles. The
simplest, if not necessarily the most efficient, method in the labeled case is simply to use
our knowledge about finding paths in the predecessor graph to search for loops.
As we might expect, rigid components in Labeled STNs are given labels, since they may
only exist under some environments. We need to find the maximal rigid components with
minimal environments. Thus, if there is a rigid component {A, B, C}, we do not separately
identify {A, B} as a rigid component. For every vertex in V , run a depth first search for a
path back to the vertex. If found, the vertices on the path are a rigid component, and the
path environment is the environment of the rigid component. Note that a valid path may
visit any vertex up to twice, as in the example below. As the search finds rigid components,
it maintains a list of the maximally sized rigid components with minimal environments.
Example 7.15 Figure 7.7 shows a small labeled distance graph with vertices {A, B, C}.
Recall that having an opposing pair of edges, one with the negative weight of the other,
implies a rigid component, so this problem obviously has some.
To apply our technique, the algorithm first adds another vertex X, and connects it
to each vertex with an edge with weight (0, {}). Figure 7.7b also shows the predecessor
graph computed after this has been done. Second, the algorithm searches for loops in
the graph. It begins with A: the path A  B  A with environment {x = 1} is a
loop, as is A  B  C  B  A with environment {x = 1, y = 1}. These paths
646

fiDrake: An Efficient Executive for Temporal Plans with Choice

imply rigid components {A, B} with environment {x = 1} and {A, B, C} with environment
{x = 1, y = 1}, respectively. Note that the second path visits B twice. From B, it find
paths B  A  B with environment {x = 1}, which is equivalent to the one found from
A, and B  C  B with environment {y = 1}, which is a new rigid component. The
paths from C re-derive the same rigid components. Thus, there are three maximal rigid
components, {A, B} with environment {x = 1}, {B, C} with environment {y = 1}, and
{A, B, C} with environment {x = 1, y = 1}.
If the two edges between B and C had the environment {x = 1} instead of {y = 1},
then there would only be a single maximal rigid component {A, B, C} with environment
{x = 1}. The smaller components, such as {A, B} with {x = 1}, would be non-maximal
and not needed.

{(5, {y = 1})}

{(3, {x = 1})}
A

B
{(3, {x = 1})}

C
{(5, {y = 1})}

(a) Labeled distance graph

{((, nil), {})}
X

A

B

{((3, B), {x = 1}),
((0, X), {})}

{((5, C), {y = 1}),
((0, A), {x = 1}),
((0, X), {})}

C
{((0, B), {y = 1}),
((, nil), {})}

(b) Predecessor graph of the added vertex X.

Figure 7.7: Identifying rigid components in labeled distance graphs.
After completing the search for rigid components, we must process them. The first
processing step is to identify a leader, the first vertex to occur, which we will use to represent
the entire rigid component. The leader is the vertex with the lowest distance from the
added vertex, queried under the environment of the rigid component. Ties may be broken
arbitrarily.
The second processing step is to update the dispatchable form accordingly. Within the
rigid component, the events are rigidly bound together and the leader is executed before
any other event of the rigid component. Thus, in the dispatchable form, we constrain the
non-leader events to occur the correct, fixed amount of time after the leader. Thus, if A
is the leader of the rigid component, and B is some other vertex of the rigid component
with environment e, the the output graph should have the labeled edge weight (d[B](e) 
d[A](e), e) on edge (A, B) and (d[A](e)  d[B](e), e) on edge (B, A). If any events are
rigidly connected in a way such that they will be executed at the same time, the dispatcher
647

fiConrad & Williams

needs them listed explicitly, because the execution window and enablement tests alone do
not guarantee correct execution of such plans. During this step we may identify maximal
groups of events that are constrained to occur at the same time as the leader, or the same
fixed duration after the leader. For example, if A is the leader and both B and C follow A
by exactly three time units, then B and C are constrained to occur at the same time. As
usual, these zero-related vertices are recorded with a label, which is the same environment
as the rigid component.
Third, we need to alter the labeled distance graph so that the rigid component no longer
appears to exist, but is instead totally represented by the leader vertex. The algorithm
begins with edges that are interior to the rigid component. If (A, B) are both vertices in a
rigid component with environment e, then the algorithm prunes W (A, B) and W (B, A) of
any environments subsumed by e. If (d, ed )  W (A, B), then if e subsumes ed , the algorithm
removes the value pair. If e does not subsume ed , then replace (d, ed ) with labeled values
(d, e0d ) where e0d avoids e, which may require multiple new values. Repeat for W (B, A).
Finally, we need to adjust edges that enter or leave the rigid component to ensure the
input to the compiler has no cycles in any predecessor graphs. The idea is to move these
edges to the leader under the environments of the rigid component, and to remove them
from the non-leader. Assume A is the leader of a rigid component with environment e, B
is another vertex of the rigid component, and C is a vertex not in the rigid component. For
every labeled value (d, ed )  w(B, C) for some vertex, the algorithm puts (d + d[B](e) 
d[C](e), ed  e) in W (A, C). Next, it replaces (d, ed )  W (B, C) with values (d, e0d ) for each
possible e0d that is a union of ed and the constituent kernels of e. The algorithm repeats
for W (C, B), putting (d + d[C](e)  d[B](e), ed  e) in W (C, A), and replacing (d, ed ) with
(d0 , e0d ) avoiding e. Unfortunately, this strategy leads to duplication that grows linearly with
the domain size of the choice variables. In practice, though, many problems do not have
any rigid components, or they are limited and the penalty is minor. Our results show some
outliers that may be associated with this growth, but most problems are not affected.
Example 7.16 As usual, let x, y  {1, 2} be choice variables. Figure 7.8a depicts an input
labeled distance graph fragment. From the opposing edge weights 1 and 1, we can easily
identify that {A, B} is a rigid component under environment {x = 1}. We need to follow
the above procedure to process it.
First, A is always scheduled before B, and is therefore the leader. If we ran the SSSP on
an added vertex, we would find that d[A]({x = 1}) = 1 and d[B]({x = 1}) = 0, proving
this assertion. Since d[B]({x = 1})  d[A]({x = 1}) = 1, event B follows A by one time
unit. Therefore, edges are inserted into the output dispatchable form shown in Figure 7.8c
that enforce this delay.
Next, we alter the input graph to remove the rigid component, beginning with edges
between the rigid components vertices. Edge (B, A) has one weight on it, which is subsumed
by the environment of the rigid component, so it only exists when the rigid component does,
and thus has already been handled, and is pruned. Likewise, the weight (1, {})  W (A, B)
is pruned. On the other hand, the weight (2, {x = 2})  W (A, B) already avoids the
environment of the rigid component, so is left unchanged.
Continue with edges that enter or leave the rigid component. (A, C) uses the leader
vertex, so does not directly require any modification. Edge (B, C) needs to be moved,
giving a new weight (4 + 1, {x = 1}) on W (A, C), which is already present, and requires
648

fiDrake: An Efficient Executive for Temporal Plans with Choice

no modification. Since its environment is subsumed by the rigid component environment,
(4, {x = 1}) is removed from w(B, C). Edge (C, B) also needs to be moved. It leads to a
new value (2  1, {y = 1}  {x = 1}) = (1, {x = 1, y = 1})  W (C, A). Since {y = 1} is not
subsumed by the rigid components environment, we must modify the value on (C, B) so
that it avoids the environment. Avoiding {x = 1} means that {x = 2}, so we modify the
value (2, {y = 1})  (2, {x = 2, y = 1}). These modifications completely remove the rigid
component and provide the dispatcher with sufficient information to correctly execute the
rigid components we have removed.

After these modifications the labeled distance graph is guaranteed not to have any cycles
in the predecessor graphs, and therefore has no rigid components. Thus the rest of the fast
algorithm given in this section correctly compiles it to dispatchable form. We adjusted
the unlabeled algorithm to search for labeled rigid components. Additionally, the unlabeled
algorithm removes the non-leader vertices of rigid components, moving and modifying edges
as necessary enforce the original input constraints. Since those vertices may be needed under
environments where the rigid component does not exist, Drake instead modifies the process
of moving edges to replicate the effect of removing the non-leader vertices.
Drakes compilation algorithm is designed to use labeling concepts to compute a compact
version of the dispatchable form of plans with choice. The structure of this algorithm is
similar to the unlabeled version, but a number of modifications are made within each step to
reason about environments. This section completes the presentation of Drakes algorithms.

8. Results
Finally, we explore Drakes performance, both from a theoretical standpoint and experimentally. Our analysis gives some justification for why we expect Drakes representation to
be compact, and our experimental results give evidence that Drake performs as intended.
8.1 Theoretical Results
We give a brief characterization of the analytical worst case performance of Drakes algorithms. The direct enumeration of STNs, as in Tsamardinos et al.s (2001) work, uses one
STN for each consistent STN. If there are n choices with d options each, and v vertices,
and we assume that
 the compiled sparse graphs are of size O v log v , then the compiled
size is O nd v log v . In contrast, Drake does not store the component
STNs independently,

but only stores the distinct values, so the size is O kv log v , where k  nd . In the worst
case, where every single component STN is completely different, there are no similarities
between choices to exploit, hence Drakes compiled representation is the same size, but is
never worse, up to a constant.
There is a strong parallel to the existing theory about the tree width of general constraint
satisfaction problems. Dechter and Mateescu (2007) explain that while a general constraint
satisfaction problem with n variables of domain size d can in general only be solved in O nd
steps, many problems have more structure. The tree width, n  n of the problem represents
the number
of variables that effectively interact, so that the search can be completed in


d
O (n ) time. Similarly, in Labeled STNs, the choice variables may not interact fully, and
thus there is an effective number of choice variables in the problem that is often smaller than
649

fiConrad & Williams

{(1, {x = 1}), (2, {x = 2})}
A

B
{(1, {x = 1})}
{(4, {x = 1})}

{(5, {x = 1})}

{(2, {y = 1})}
C
(a) Input labeled distance graph

{(2, {x = 2})}
A

B

{(1, {x = 1, y = 1})}

{(5, {x = 1})}

{(2, {x = 2, y = 1})}
C
(b) Contracted labeled distance graph

{(1, {x = 1})}
A

B
{(1, {x = 1})}

C
(c) Dispatchable form of rigid component

Figure 7.8: An example of processing a rigid component from a Labeled Distance Graph.


the total number. In this notation, Drakes compiled problems have size O (n )d n log v .
The smaller base of the exponent can lead to significant savings.
650

fiDrake: An Efficient Executive for Temporal Plans with Choice

The compile time and run-time latency are more difficult to characterize, however,
because the overhead of the labeling operations also grows with n. Therefore, we do not
attempt an analytic analysis.
8.2 Experimental Results
This section presents an experimental validation of Drakes compilation and dispatch algorithms on randomly generated, structured problems. First, we develop a suite of random
structured Labeled STNs, derived from Stedls (2004) problem generator. Then we compile
and dispatch the suites of problems twice, once with Drake and once by explicitly enumerating all component STNs, following techniques developed by Tsamardinos et al. (2001).
Finally, we compare the compiled size of the problems, the compilation time, and the execution latency. Throughout this section, our plots use the number of consistent component
STNs as the horizontal axis, because it appears to correlate well with the effective difficulty
of the problem. For each of the metrics, we provide the results from the two methods side
by side in one plot, and show the ratio of performance in another one, to allow point-wise
comparison of the difference in performance between identical problems. The problems are
constructed from 2-11 binary choices or 2-7 ternary choices. There are 100 problems at
each of these sizes; the problems range from 4 consistent component STNs up to about
2000. The number of events ranges from 4 to about 22 as the number of component STNs
increase to keep a consistent ratio of constraints to events.
This comparison is performed with a Lisp implementation, run on a 2.66 GHz machine
with 4 Gb of memory. There are some performance related implementation details we have
omitted in prior sections. For example, labeled value sets are stored as ordered lists to
reduce insertion and query time. Additionally, we found that memoization of subsumption
and union operations dramatically improved performance. The implementation aggressively
prunes values with inconsistent environments to avoid any unnecessary reasoning. The STN
compiler and dispatcher exercise the same code as Drake to support a fair comparison, and
pays a small overhead in execution speed.
The first metric of comparison is the size of the dispatchable form of the random problems. We computed this by serializing the graph representations to strings. Since Drakes
compilation algorithm is derived from the fast STN compiler, the maximum memory footprint for both compilation and dispatch is no more than about double these numbers.
Since we designed Drake with this metric in mind, we expect the improvement to be clear
and significant, and this is what Figure 8.1 shows. In the ratio plot, the value is the multiple of improvement of Drake over STN enumeration. Except for small problems, Drakes
memory performance is superior, and the improvement ranges up to around a 700 times
smaller memory footprint for the largest problems. The STN enumeration Tsamardinos et
al. (2001) develops uses up to around 2 MB of storage, and although that is insignificant
for a modern desktop, it is often significant for embedded hardware, especially if the system
must store a library of compiled plans. In contrast, Drakes memory footprint is 1-10 kB for
most cases, which is trivial, even in large numbers, for any hardware. More than half the
worse performing examples, those that do not fit in the main band of results, correspond
to ternary choices. We believe this likely corresponds to the growth caused by avoiding
environments when handling complex or overlapping rigid components in these problems.
651

fiConrad & Williams

Compiled Size (kB)
4

10

Drake
STN Enumeration

3

10

2

10

1

10

0

10

1

10

0

10

1

2

3

10
10
10
Number of Consistent Component STNs

4

10

(a) The size of the dispatchable labeled distance graphs.

Compiled Size Ratio STN/Labeled STN
3

10

2

10

1

10

0

10

1

10

0

10

1

2

3

10
10
10
Number of Consistent Component STNs

4

10

(b) The ratio of the size of the compiled STN enumeration size to the size of Drakes labeled distance graph.

Figure 8.1: The size of the dispatchable form of the random problems as a function of the
number of component STNs.

652

fiDrake: An Efficient Executive for Temporal Plans with Choice

The second metric is the time required to compile the random problems, shown in Figure
8.2. Often, Drakes compilation times are much better, but they are highly variable. For
a number of the largest problems, Drake is up to about 1000 times faster. However, some
problems exhibit little improvement, and a few are up to 100 times slower for the worst
cases. Most problems take less than about ten minutes for both methods, but Drake takes
a few hours on five of the largest problems. We expect higher variability for the run-time
because the compilation algorithms loop repeatedly over the labeled graph, exacerbating
the variability shown in the compiled size. Thus, it is not surprising that on problems whose
dispatchable form is not compact, the run-time suffers.
The final metric is the run-time latency incurred by the algorithms, shown in Figure
8.3. Drakes reported latency is the maximum latency for a single decision making period
during a single execution of the entire problem. The STN enumeration latency reported
is the time required to identify, execute, and propagate the first event in each consistent,
component STN. While these metrics are not identical, they are quite comparable. The
values reported at 103 seconds were reported as zero by Lisps timing features, and we
inflate them to fit on a log scale.
Although differences in compilation time are interesting, increases in the run-time latency are far more critical to Drakes applicability in the real world. Fortunately, the
situation looks favorable. Both systems execute all but the largest problems in under a
tenth of a second, and most small and moderate sized problems in around 10 milliseconds.
Although Drake is slower on a reasonable fraction of the problems, the margin is fairly low;
note that the cluster of values at a ratio of just less than 101 corresponds to the jump from
effectively zero to about 10 milliseconds. We do not know the real ratio, but these points
create a visible cluster at 101 which may or may not be misleading. Instead, we focus on
larger problems where both methods are measurable, and where Drake generally performs
quite well. Again, a handful of problems are outliers, taking much more time, up to tens
of seconds, which would not be acceptable for most applications. We conclude that Drake
will perform well on embedded systems for many real world problems, in terms of memory
usage and latency.
Overall, these results are what Drake was designed to achieve. Using a compact representation provides a smaller memory footprint. Sometimes, exploiting the similarity between
choices makes reasoning fast, and other times it imposes an extra computational burden to
tease out the similarities, or lack thereof. Enumerating the STNs directly has quite predictable costs, both in time and space, and Drake is far more variable, depending on the
precise nature of the problem. While we find these results quite promising, we must caution
that applications to particular problems could be better or worse, depending on factors we
do not know how to easily characterize. This is well illustrated by the few problems that
were outliers in all three metrics. It may be useful in future work to further investigate the
sources of variability in Drakes performance.
As a practical note, we tightly regulated the number of events to focus our investigation
on the scaling with the number of choices. However, both algorithms should scale gracefully,
and with similar increases in space and time costs, to plans with more events. Overall,
Drake appears to provide a noticeably lower memory footprint for dispatching problems
with discrete choices than the direct enumeration strategy of Tsamardinos et al. (2001),
while only suffering from a mild increase in run-time latency.
653

fiConrad & Williams

Compile Time (sec)
6
10
Drake
STN Enumeration
4

10

2

10

0

10

2

10

0

10

1

2

3

10
10
10
Number of Consistent Component STNs

4

10

(a) The compile time for random problems.

Compile Time Ratio, STN/Labeled STN
4
10

2

10

0

10

2

10

0

10

1

2

3

10
10
10
Number of Consistent Component STNs

4

10

(b) The ratio of the compile time of STN enumeration to Drakes compile time.

Figure 8.2: The compile time of random problems as a function of the number of component
STNs.

654

fiDrake: An Efficient Executive for Temporal Plans with Choice

Execution Latency (sec)
1

10

Drake
STN Enumeration
0

10

1

10

2

10

3

10

0

10

1

2

3

10
10
10
Number of Consistent Component STNs

4

10

(a) The execution latency.

Execution Latency Ratio STN/Labeled STN
2

10

1

10

0

10

1

10

2

10

0

10

1

2

3

10
10
10
Number of Consistent Component STNs

4

10

(b) The ratio of the execution latency for STN enumeration to Drakes execution latency.

Figure 8.3: The execution latency of random problems as a function of the number of component STNs.

655

fiConrad & Williams

9. Summary
This work presents Drake, a compact, flexible executive for plans with choice. Drake takes
input plans with temporal flexibility and discrete choices, such as Labeled STNs or DTNs,
and selects the execution times and makes discrete decisions at run-time (Dechter et al.,
1991). Choices substantially improve the expressiveness of the tasks that executives can
perform, and improve the robustness of the resulting executions. Prior execution approaches
typically impose significant memory requirements or introduce substantial latency during
execution. Our goal in developing Drake is to develop a dispatching executive with a lower
memory footprint.
Building upon the concept of labels employed by the ATMS to compactly encode all
the consequences of a set of alternative choices, Drake introduces a new compact encoding,
called labeled distance graphs, to encode and efficiently reason over discrete choices, and
we introduce a corresponding maintenance system (de Kleer, 1986). Our adaptation of the
ATMS labeling scheme focuses on only maintaining non-dominated constraints, which allows
Drake to exploit the structure of temporal reasoning, cast as a shortest path problem on a
distance graph, to provide a compact representation. Furthermore, modifying the existing
unlabeled algorithms to account for labels does not change the overall structure of the
algorithms.
Drakes compilation algorithm successfully compresses the dispatchable solution by over
two orders of magnitude relative to Tsamardinos, Pollack, and Ganchevs (2001) prior work,
often reducing the compilation time, and typically introducing only a modest increase in
execution latency. Thus, we believe that Drake successfully realizes our initial goals. Within
our experiments, compilation typically takes less than ten minutes, but on occasion takes
hours. Although time consuming in the later case, this is still acceptable, since compilation
can be performed off-line, when a task is first defined. To summarize, Drakes Labeled
STNs and labeled distance graphs enable an executive that strikes a useful balance between
latency and memory consumed, which is appropriate for real world applications. Drakes
labeling scheme also provides the opportunity to extend a wide range of graph algorithms
to reason about and represent choice efficiently.

Acknowledgments
The authors would like to thank Julie Shah and David Wang for many helpful ideas and
discussions, and the reviewers for their insightful comments. Patrick Conrad was funded
during this work by a Department of Defense NDSEG Fellowship.

References
Block, S., Wehowsky, A., & Williams, B. (2006). Robust execution of contingent, temporally
flexible plans. In Proceedings of the 21st National Conference on Artificial Intelligence,
pp. 802808.
Combi, C., & Posenato, R. (2009). Controllability in temporal conceptual workflow
schemata. In Dayal, U., Eder, J., Koehler, J., & Reijers, H. (Eds.), Business Process
656

fiDrake: An Efficient Executive for Temporal Plans with Choice

Management, Vol. 5701 of Lecture Notes in Computer Science, pp. 6479. Springer
Berlin / Heidelberg.
Combi, C., & Posenato, R. (2010). Towards temporal controllabilities for workflow
schemata. In Proceedings of the 17th International Symposium on Temporal Representation and Reasoning, pp. 129136. IEEE.
Conrad, P. R. (2010). Flexible execution of plans with choice and uncertainty. Masters
thesis, Massachusetts Institute of Technology.
Conrad, P. R., Shah, J. A., & Williams, B. C. (2009). Flexible execution of plans with choice.
In Proceedings of the Nineteenth International Conference on Automated Planning and
Scheduling (ICAPS-09). AAAI Press.
Cormen, T., Leiserson, C., Rivest, R., & Stein, C. (2001). Introduction to algorithms (Second
edition). The MIT Press.
de Kleer, J. (1986). An assumption-based TMS. Artificial intelligence, 28 (2), 127162.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artificial
Intelligence, 171 (2-3), 73106.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49 (1-3), 61  95.
Doyle, J. (1979). A truth maintenance system* 1. Artificial Intelligence, 12 (3), 231272.
Effinger, R. (2006). Optimal Temporal Planning at Reactive Time Scales via Dynamic
Backtracking Branch and Bound. Masters thesis, Massachusetts Institute of Technology.
En, N., & Srensson, N. (2004). An extensible sat-solver. In Giunchiglia, E., & Tacchella, A.
(Eds.), Theory and Applications of Satisfiability Testing, Vol. 2919 of Lecture Notes
in Computer Science, pp. 333336. Springer Berlin / Heidelberg.
Goldstone, D. (1991). Controlling inequality reasoning in a TMS-based analog diagnosis
system. In Proceedings of the Ninth National Conference on Artificial Intelligence,
pp. 512517.
Hiatt, L., Zimmerman, T., Smith, S., & Simmons, R. (2009). Strengthening schedules
through uncertainty analysis. In Proceedings of the International Joint Conference on
Artificial Intelligence, Vol. 2, pp. 53.
Hunsberger, L. (2009). Fixing the semantics for dynamic controllability and providing a
more practical characterization of dynamic execution strategies. In Proceedings of
the 16th International Symposium on Temporal Representation and Reasoning, pp.
155162. IEEE.
Hunsberger, L. (2010). A Fast Incremental Algorithm for Managing the Execution of Dynamically Controllable Temporal Networks. In Proceedings of the 17th International
Symposium on Temporal Representation and Reasoning, pp. 121128. IEEE.
Khatib, L., Morris, P., Morris, R., & Rossi, F. (2001). Temporal constraint reasoning
with preferences. In Proceedings of the International Joint Conference on Artificial
Intelligence, Vol. 1, pp. 322327.
657

fiConrad & Williams

Kim, P., Williams, B. C., & Abramson, M. (2001). Executing reactive, model-based programs through graph-based temporal planning. In Proceedings of the International
Joint Conference on Artificial Intelligence, Vol. 17, pp. 487493.
McDermott, D. (1983). Contexts and data dependencies: A synthesis. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, PAMI-5 (3), 237246.
Morris, P. (2006). A structural characterization of temporal dynamic controllability. Principles and Practice of Constraint Programming, 4204, 375389.
Morris, P., Muscettola, N., & Vidal, T. (2001). Dynamic control of plans with temporal
uncertainty. In Proceedings of the International Joint Conference on Artificial Intelligence, Vol. 17, pp. 494502.
Muscettola, N., Morris, P., & Tsamardinos, I. (1998). Reformulating temporal plans for
efficient execution. In Proceedings of the Principles of Knowledge Representation and
Reasoning-International Conference, pp. 444452.
Planken, L., de Weerdt, M., & van der Krogt, R. (2008). P 3 C: A New Algorithm for the
Simple Temporal Problem. In Proceedings of the Eighteenth International Conference
on Automated Planning and Scheduling (ICAPS-08), pp. 256263. AAAI Press.
Rossi, F., Venable, K., & Yorke-Smith, N. (2006). Uncertainty in soft temporal constraint
problems: a general framework and controllability algorithms for the fuzzy case. Journal of Artificial Intelligence Research, 27 (1), 617674.
Shah, J., Stedl, J., Williams, B., & Robertson, P. (2007). A fast incremental algorithm for
maintaining dispatchability of partially controllable Plans. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS2007). AAAI Press.
Shah, J. A., & Williams, B. C. (2008). Fast Dynamic Scheduling of Disjunctive Temporal Constraint Networks through Incremental Compilation. In Proceedings of the
Nineteenth International Conference on Automated Planning and Scheduling (ICAPS2008). AAAI Press.
Shu, I.-h., Effinger, R., & Williams, B. C. (2005). Enabling Fast Flexible Planning Through
Incremental Temporal Reasoning with Conflict Extraction. In Proceedings of the
Fifteenth International Conference on Automated Planning and Scheduling (ICAPS05), pp. 252261. AAAI Press.
Smith, S., Gallagher, A., & Zimmerman, T. (2007). Distributed management of flexible
times schedules. In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 18. ACM.
Stallman, R., & Sussman, G. (1977). Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis* 1. Artificial Intelligence,
9 (2), 135196.
Stedl, J. (2004). Managing temporal uncertainty under limited communication: a formal
model of tight and loose team coordination. Masters thesis, Massachusetts Institute
of Technology.
658

fiDrake: An Efficient Executive for Temporal Plans with Choice

Stergiou, K., & Koubarakis, M. (2000). Backtracking algorithms for disjunctions of temporal
constraints. Artificial Intelligence, 120 (1), 81117.
Tsamardinos, I. (2002). A probabilistic approach to robust execution of temporal plans
with uncertainty. Methods and Applications of Artificial Intelligence, 2308, 751751.
Tsamardinos, I., Muscettola, N., & Morris, P. (1998). Fast transformation of temporal
plans for efficient execution. In Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pp. 254261.
Tsamardinos, I., Pollack, M., & Ganchev, P. (2001). Flexible dispatch of disjunctive plans.
In 6th European Conference on Planning, pp. 417422.
Tsamardinos, I., Vidal, T., & Pollack, M. (2003). Ctp: A new constraint-based formalism
for conditional, temporal planning. Constraints, 8 (4), 365388.
Venable, K., & Yorke-Smith, N. (2005). Disjunctive temporal planning with uncertainty.
In Proceedings of the International Joint Conference on Artificial Intelligence, pp.
172122. Citeseer.
Williams, B., & Ragno, R. (2007). Conflict-directed A* and its role in model-based embedded systems. Discrete Applied Mathematics, 155 (12), 15621595.
Williams, B. C., Ingham, M. D., Chung, S. H., & Elliott, P. H. (2003). Model-based programming of intelligent embedded systems and robotic space explorers. Proceedings
of the IEEE: Special Issue on Modeling and Design of Embedded Software, 91 (1),
212237.
Xu, L., & Choueiry, B. (2003). A new effcient algorithm for solving the simple temporal
problem. In Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic, pp.
210220.

659

fiJournal of Artificial Intelligence Research 42 (2011) 887916

Submitted 04/11; published 12/11

Multi-Robot Adversarial Patrolling:
Facing a Full-Knowledge Opponent
Noa Agmon

agmon@cs.utexas.edu

Department of Computer Science
University of Texas at Austin
TX, USA

Gal A Kaminka
Sarit Kraus

galk@cs.biu.ac.il
sarit@cs.biu.ac.il

Computer Science Department
Bar Ilan University
Israel

Abstract
The problem of adversarial multi-robot patrol has gained interest in recent years, mainly
due to its immediate relevance to various security applications. In this problem, robots
are required to repeatedly visit a target area in a way that maximizes their chances of
detecting an adversary trying to penetrate through the patrol path. When facing a strong
adversary that knows the patrol strategy of the robots, if the robots use a deterministic
patrol algorithm, then in many cases it is easy for the adversary to penetrate undetected
(in fact, in some of those cases the adversary can guarantee penetration). Therefore this
paper presents a non-deterministic patrol framework for the robots. Assuming that the
strong adversary will take advantage of its knowledge and try to penetrate through the
patrols weakest spot, hence an optimal algorithm is one that maximizes the chances of
detection in that point. We therefore present a polynomial-time algorithm for determining
an optimal patrol under the Markovian strategy assumption for the robots, such that the
probability of detecting the adversary in the patrols weakest spot is maximized. We build
upon this framework and describe an optimal patrol strategy for several robotic models
based on their movement abilities (directed or undirected) and sensing abilities (perfect or
imperfect), and in dierent environment models - either patrol around a perimeter (closed
polygon) or an open fence (open polyline).

1. Introduction
The problem of multi-robot patrol has gained interest in recent years (e.g., Ahmadi & Stone,
2006; Chevaleyre, 2004; Elmaliach, Agmon, & Kaminka, 2007; Paruchuri, Tambe, Ordonez,
& Kraus, 2007; Amigoni, Gatti, & Ippedico, 2008), mainly due to its immediate relevance
to various security applications. In the multi-robot patrol problem, robots are required to
repeatedly visit a target area in order to monitor it. Many researchers have focused on a
frequency-based approach, guaranteeing that some point-visit frequency criteria are met by
the patrol algorithm, for example maximizing the minimal frequency or guaranteeing uniform frequency (e.g., refer to Elmaliach et al., 2007; Chevaleyre, 2004; Almeida, Ramalho,
Santana, Tedesco, Menezes, Corruble, & Chevaleyr, 2004).
In contrast, we advocate an approach in which the robots patrol in adversarial settings,
where their goal is to patrol in a way that maximizes their chances of detecting an adversary
c
2011
AI Access Foundation. All rights reserved.

fiAgmon, Kaminka & Kraus

trying to penetrate through the patrol path. Thus the decisions of the adversary must be
taken into account. Our objective is, therefore, to develop patrol paths for the robots, such
that following these paths the robots will maximize the chance of adversarial detection. The
problem of adversarial planning and specically adversarial patrolling is a wide problem,
where generally no computational tractable results exist. This paper presents the problem
in a restrictive environment of perimeter patrol by a set of homogenous robots, providing a
computational tractable optimal result.
As opposed to frequency-driven approaches, in adversarial settings the point-visit frequency criteria becomes less relevant. Consider the following scenario. We are given a cyclic
fence of a length of 100 meters and 4 robots must patrol around the fence while moving at a
velocity of 1m/sec. Clearly, the optimal possible frequency at each point around the fence,
in terms of maximizing the minimal frequency, is 1/25, i.e., each location is visited once
every 25 seconds. This optimal frequency is achieved if the robots are placed uniformly
along the fence (facing the same direction) and move forward without turning around. Assume that it takes an adversary 20 seconds to penetrate the area through the fence. As
the robots move in a deterministic path, an adversary knowing the patrol algorithm can
guarantee penetration if it simply enters through a position that was recently visited by
a patrolling robot. On the other hand, if the robots move non-deterministically, i.e., they
turn around from time to time with some probability greater than 0, then the choice of
penetration position becomes less trivial. Moreover, if we assume that an adversary may
penetrate at any time, it motivates the use of nondeterministic patrol behavior indenitely.
We rst consider the problem of patrolling around a closed polygon, i.e., a perimeter.
We introduce a non-deterministic framework for patrol under a rst order Markovian assumption for the robots strategy, in which the robots choose their next move at random
with some probability p. This probability value p characterizes the patrol algorithm. We
model the system as a Markov chain (e.g., Stewart, 1994), and using this model we calculate in polynomial time the probability of penetration detection at each point along the
perimeter as a function of p, i.e., it depends on the choice of patrol algorithm.
Based on the functions dening the probability of penetration detection, we nd an optimal patrol algorithm for the robots in the presence of a strong adversary, i.e., an adversary
having full knowledge on the patrolling robotstheir algorithm and current placement. In
this case, the adversary uses this knowledge in order to maximize its chances of penetrating
without being detected. It is therefore assumed that the adversary will penetrate through
the weakest spot of the path, hence the goal of the robots is to maximize the probability
of penetration detection in that weakest spot. We provide a polynomial time algorithm
(polynomial in the input size, depending on the number of robots and the characteristics
of the environment) for nding an optimal patrol for the robots facing this full knowledge
adversary. We show that a non-deterministic patrol algorithm is advantageous, and guarantees some lower bound criteria on the performance of the robots, i.e., on their ability to
block the adversary.
We then use the patrol framework to consider additional environment and robotic models. Specically, we consider the case in which the robots are required to patrol along an
open polyline (fence). We show that although this case is inherently dierent from patrolling
along a perimeter, the basic framework can still be used (with some changes) in order to nd
an optimal patrol algorithm for the robots. We investigate also dierent movement models
888

fiMulti-Robot Adversarial Patrolling

of robots, namely the robots can have directionality associated with their movement (and
turning around could cost the system time), or they can be omnidirectional. In addition,
we model various types of sensing capabilities of the robots, specically, their sensing capabilities can be perfect or imperfect, local or long-range. In all these cases we show how the
basic framework can be extended to include the various models.
This paper is organized as follows. In Section 2 we discuss previous work, related to
our research. Section 3 describes the basic robot and environment model. We introduce in
Section 4 a framework for the patrolling robots, and describe a polynomial-time algorithm
for determining the probability of penetration detection at every point along the patrol
path (Section 4.2). We then show in Section 4.3 an algorithm for dening an optimal patrol
algorithm for the robots, assuming they face a full-knowledge adversary, and in Section 4.4
provides some interesting results from an implementation of the algorithms. In Section 5
we show how the basic framework can be used in order to consider various robotic sensing
and movement models and in a dierent environment (open fences). Section 6 concludes.

2. Related Work
Systems comprising multiple robots that cooperate to patrol in some designated area have
been studied in various contexts (e.g., Chevaleyre, 2004; Elmaliach, Agmon, & Kaminka,
2009). Theoretical (e.g., Chevaleyre, 2004; Elmaliach et al., 2009; Amigoni et al., 2008)
and empirical (e.g., see Sak, Wainer, & Goldenstein, 2008; Almeida et al., 2004) solutions
have been proposed in order to assure quality patrol. The denition of quality depends on
the context. Most studies concentrate on the frequency-based patrolling, which optimizes
frequency of visits throughout the designated area (e.g. refer to Elmaliach et al., 2009;
Almeida et al., 2004; Chevaleyre, 2004). Ecient patrol, in this case, is a patrol guaranteeing a high frequency of visits in all parts of the area. In contrast, adversarial patrolling
(addressed in this paper) deals with the detection of moving adversaries who are attempting
to avoid detection. Here, an ecient patrol is one that deals eciently with intruders (e.g.,
see Sak et al., 2008; Basilico, Gatti, & Amigoni, 2009b; Amigoni et al., 2008).
The rst theoretical analysis of the frequency-based multi-robot patrol problem that
concentrated on frequency optimization was presented by Chevaleyre (2004). He introduced
the notion of idleness, which is the duration each point in the patrolled area is not visited.
In his work, he analyzed two types of multi-robot patrol schemes on graphs with respect
to the idleness criteria: partitioning the area into subsections, where each section is visited
continuously by one robot; and the cyclic scheme in which a patrol path is provided along
the entire area and all robots visit all parts of the area, consecutively. He proved that
in the latter approach, the frequency of visiting points in the area is considerably higher.
Almeida et al. (2004) oered an empirical comparison between dierent approaches towards
patrolling with regards to the idleness criteria, and shows great advantage of the cycle based
approach.
Elmaliach et al. (2007, 2009) oered new frequency optimization criteria for evaluating
patrol algorithms. They provide an algorithm for multi-robot patrol in continuous areas
that is proven to have maximal minimal frequency as well as uniform frequency, i.e., each
point in the area is visited with the same highest-possible frequency. Their work is based on

889

fiAgmon, Kaminka & Kraus

creating one patrol cycle that visits all points in the area in minimal time, and the robots
simply travel equidistantly along this patrol path.
Sak et al. (2008) considered the case of multi-agent adversarial patrol in general graphs
(rather than perimeters, as in our work). They concentrated on an empirical evaluation
(using a simulation) of several non-deterministic patrol algorithms that can be roughly
divided into two: Those that divide the graph between the patrolling agents, and those that
allow all agents to visit all parts of the graph. They considered three types of adversaries:
random adversary, an adversary that always chooses to penetrate through a recently-visited
node and an adversary that uses statistical methods to predict the chances that a node will
be visited soon. They concluded that there is no patrol method that outperformed the
others in all the domains they have checked, but the optimality depends on the graph
structure. In contrast to this investigation, we provide theoretical proofs of optimality for
dierent settings.
The work of adversarial multi-robot patrol was examined also by using game-theoretic
approaches (e.g., see Basilico et al., 2009b; Basilico, Gatti, & Amigoni, 2009a; Pita, Jain,
Ordonez, Tambe, Kraus, & Magorii-Cohen, 2009; Paruchuri, Tambe et al., 2007). Note
that the work described herein can be modeled as a game theoretic problem: Given two
players, the robots and the adversary, with a possible set of actions by each side, determine
the optimal policy of the robots that will maximize their utility gained from adversarial
detection. This is a zero-sum game. Since we assume a strong (full knowledge) adversarial
model, we adopt the minmax approach, namely, minimizing the maximal utility of the
opponent (or in this case: equivalent to maximizing the minimal probability of detection
of the robots). However, in our work we do not use game theoretic tools for nding the
equilibrium strategy, but use tailored ad-hoc solution that nds the optimal policy for the
robots in polynomial time, taking into account the robots possible sensing and movement
capabilities.
The most closely related work by Amigoni et al. (2008) and Basilico et al. (2009b, 2009a)
used a game-theoretic approach using leader-follower games for determining the optimal
strategy for a single patrolling agent. They considered an environment in which a patrolling
robot can move between any two nodes in a graph, as opposed to the perimeter model we
use. Their solution is suitable for one robot in heterogenous environments, i.e., the utility
of the agent and the adversary changes along the vertices of the graph. They formulate
the problem as a mathematical programming problem (either multilinear programming or
mixed integer linear programming). Consequently, the computation of the optimal strategy
is exponential, yet using optimization tools they manage to get good approximation to the
optimal solution.
Paruchuri, Tambe et al. (2007) considered the problem of placing stationary security
checkpoints in adversarial environments. Similar to our assumptions, they assume that
their agents work in an adversarial environment in which the adversary can exploit any predictable behavior of the agents, and that the adversary has full knowledge of the patrolling
agents. They model their system using Stackelberg games, which uses policy randomization
in the agents behavior in order to maximize their rewards. The problem is formulated
as a linear program for a single agent, yielding an optimal solution for that case. Using
this single agent policy, they present a heuristic solution for multiple agents, in which the
optimal solution is intractable. Paruchuri, Pearce et al. (2007) further study this problem
890

fiMulti-Robot Adversarial Patrolling

in cases where the adversarial model is unknown to the agents, although the adversary still
has full knowledge of the patrol scheme. They again provide heuristic algorithms for optimal strategy selection by the agents. Pita et al. (2009) continued this research to consider
the case in which the adversaries make their choices based on their bounded rationality or
uncertainty, rather than make the optimal game-theoretic choice. They considered three
dierent types of uncertainty over the adversarys choices, and provided new mixed-integer
linear programs for Stackelberg games that deal with these types of uncertainties.
As opposed to all these works that are based on using game-theoretic approaches and
provide approximate or heuristic solutions to intractable optimal solutions, in our work we
focus on specic characteristics of the robots and the environment, and provide optimal
polynomial-time algorithms for nding an optimal patrol strategy for the multi-robot team
using the minmax approach.
Theoretical work based on stochastic processes that is related to our work is the cat and
mouse problem (Coppersmith, Doyle, Raghavan, & Snir, 1993), also known as the predatorprey (Haynes & Sen, 1995) or pursuit evasion problem (Vidal, Shakernia, Kim, Shim, &
Sastry, 2002). In this problem, a cat attempts to catch a mouse in a graph where both
are mobile. The cat has no knowledge about the mouses movement, therefore as far as
the cat is concerned, the mouse travels similarly to a simple random walk on the graph.
We, on the other hand, have worst case assumptions about the adversary. We consider
a robotic model, in which the movement of the cat is correlated with the movement of a
robot, with possible directionality of movement, possible cost of changing directions and
possible sensorial abilities. Moreover, in our model the robots travel around a perimeter or
a fence, rather than in a general graph. Thus in a sense, our research is concerned with
pursuit-evasion on a polyline - open or closed.
Other theoretical work by Shieh and Calvert (1992), based on computational geometry
solutions, attempts to nd optimal viewpoints for patrolling robots. They try to maximize the view of the robots in the area, show that the problem is N P-Hard, and nd
approximation algorithms for the problem.

3. Robot and Environment Model
In the following section, we provide a description of the robotic model, environment model
and the adversarial model. We describe the basic model of patrolling around a perimeter
(closed polygon). Further environments and robotic models are discussed in Section 5.
3.1 The Environment
We consider a patrol in a circular path around a closed polygon P . The path around P
is divided into N segments of a length of uniform time distance, i.e., each robot travels
through one segment per cycle while sensing it (its velocity is 1 segment / 1 time cycle).
This division into segments makes it possible to consider patrols in heterogeneous paths. In
such areas, the diculty of passing through terrains varies from one terrain to another, for
example driving in muddy tracks vs. driving on a road. In addition, riding around corners
requires a vehicle to slow down. Figure 1 demonstrates a transition from a given area to a
discrete cycle. The area, on the left, is given along with its velocity constraints. The path
is then divided into segments such that a robot travels through one segment per time cycle
891

fiAgmon, Kaminka & Kraus

while monitoring it, i.e., the length of each segment is determined by both the velocity of
the robot (corresponding to the time it takes it to travel through the specic segment) and
the sensorial capabilities of the robot. After the path is divided into segments with uniform
travel time, it is equivalent to considering a simple cycle as appears in the right of Figure
1.
Note that the distance between the robots is calculated with respect to the number of
segments between them, i.e., the distance is in travel time. For example, if we say that
the distance between R1 and R2 is 7, then there are 7 segments between them, and if R1
had remained still, then it would have taken R2 7 time cycles to reach R1 (assuming R2 is
headed towards the right direction).

equivalent
Figure 1: An example for creating discrete segments from a circular path with the property that the
robots travel through one segment per cycle. The dierent line structures along the perimeter on the left
correlate to dierent velocity constraints, which are converted (in the middle gure) to N segments in
which the robots travel during one time cycle. This gure is equivalent to the gure in the right, which
is a simple cycle divided into N uni-time segments.

3.2 Patrolling Robotic Model
We consider a system of k > 1 homogenous mobile robots R1 , . . . , Rk , that are required to
patrol around a closed polygon. The robots operate in cycles, where each cycle consists of
two stages.
1. Compute: Execute the given algorithm, resulting in a goal point, denoted by pG , to
which the robot should travel.
2. Move: Move towards point pG .
This model is synchronous, i.e. all robots execute each cycle simultaneously. We concentrate our attention to the Compute stage, i.e., how to compute the next goal point.
We assume the robots movement model is directed such that if pG is behind the robot,
it has to physically turn around. Turning around is a costly operation, and we model this
cost in time, i.e., if the robot turns around it resides in its segment for  time units. The
case in which the movement model is not directed is discussed in Section 5.1. Throughout
the paper we assume for simplicity that  = 1, unless stated otherwise.
A key result of this research (Section 4) is that optimal patrolling necessitates robots
to be placed at a uniform distance d = N/k from one another along the perimeter. Consequently, we require the robots to be coordinated in the sense that all robots move in the
same direction, and if decided to turn around they do it simultaneously. This requirement
guarantees that the uniform distance of d is maintained throughout the execution of the
892

fiMulti-Robot Adversarial Patrolling

patrol algorithm. Note that this tightly-coordinated behavior is achievable in centralized
systems, or in systems where communication exists between all team members. Other practical implementations may exist (for example uniformly seeding a pseudorandom number
generator for all the robots), but they all require coordination inside the team. Distributed
systems that cannot assume reliable communication are left for future work.
3.3 Adversarial Model
Our basic assumption is that the system consists of an adversary that tries to penetrate
once through the patrolling robots path without being detected. The adversary decides,
at any unknown time, through which segment to penetrate. Its penetration time is not
instantaneous, and lasts t time units, during which it stays at the same segment.
Denition 1. Let si be a discrete segment of a perimeter P which is patrolled by one robot
or more. Then the Probability of Penetration Detection in si , ppdi , is the probability that a
penetrator going through si during t time units will be detected by some robot going through
si during that period of time.
In other words, ppdi is the probability that a patrol path of some robot will pass through
segment si during the time that a penetration is attempted through that segment, hence it
is calculated for each segment with respect to the current location of the robots at a given
time (since the robots maintain uniform distance between them throughout the execution,
this relative location remains the same at all times). We use the general acronym ppd when
referring to the general term of probability of penetration detection (without reference to a
certain segment).
Recall that the time distance between every two consecutive robots around the perimeter
is d = N/k. Therefore we consider t values between the boundaries d+
2  t < d. The reason
for this is that if it takes the robot  time units to turn, then the robot adjacent to s0 will
have probability > 0 of arriving at every segment si , 0  i  t, while the robot adjacent
to sd has probability > 0 of arriving at segments si , d  (t   )  i leqd. Hence segment
st+1 has probability > 0 of being visited only if d  (t   )  t + 1  d+2+1  t, otherwise
there is at least one segment, st+1 , that has probability 0 of being visited during t time
units. Therefore an adversary having full knowledge on the patrol will always manage to
successfully penetrate regardless of the actions taken by the patrolling robots. Note that 
appears in this equation since it inuences the number of segments reachable by the robot
located in segment sd+1 if turning around (sd , sd1 , . . . , sd/2+ ). On the other hand, if t  d
then all segments si can have ppdi = 1 simply by using a deterministic algorithm.
We dene the patrol scheme of the robots as the
1. Number of robots, the distance between them and their current position.
2. The movement model of the robots and any characterization of their movement.
3. The robots patrol algorithm.
The patrol scheme reects the knowledge obtained by the adversary on the patrolling robots
at any given time (hence is not necessarily time dependent).

893

fiAgmon, Kaminka & Kraus

We consider a strong adversarial model in which the adversary has full knowledge of
the patrolling robots. Therefore the full knowledge adversary knows the patrol scheme,
and will take advantage of this knowledge in order to choose its penetration spot as the
weakest spot of the patrol, i.e., the segment with minimal ppd. The solution concept adopted
here (as stated in Section 2) is similar to the game-theoretic minmax strategy, yielding a
strategy that is in equilibrium (none of the playersrobots or adversaryhas any initiative
to diverge from their strategy). The adversary can learn the patrol scheme by observing the
behavior of the robots for a sucient amount of time. Note that in security applications,
such strong adversaries exist. In other applications, the adversary models the behavior of
the system in the worst case scenario from the patrolling robots point of view (similar to
the classical Byzantine fault model in distributed systems, see Lynch, 1996).
In our environment, the robots are responsible only for detecting penetrations and not
handling the penetration (which requires task-allocation methods). Therefore the case in
which the adversary issues multiple penetrations is similar to handling a single penetration,
as the robots detect, report and continue to monitor the rest of the path, according to their
algorithm.

4. A Framework for Adversarial Patrolling of Perimeters
The environment we consider is a linear environment, in which at each step the robots can
decide to either go straight or turn around. The framework we suggest is nondeterministic
in the sense that at each time step the decision it done independently, at random, with
some probability p. Formally,
{
p
Go straight
Probability of next move =
1  p Turn around
Since the dierent patrol algorithms we consider vary in the probability p of the next
step, we assert that the probability p characterizes the patrol algorithm.
Assume a robot is currently located in segment si . Therefore if the robot is facing
segment si+1 , then with a probability of p it will go straight to it and with a probability of
1  p it will turn around and face segment si1 . Similarly, if it is facing segment si1 , then
with a probability of p it will reach segment si1 and with a probability of 1  p it will face
segment si+1 .
Note that the probability of penetration detection in each segment si , 1  i  d, is
determined by probability p characterizing the patrol algorithm, therefore ppdi is a function
of p, i.e., ppdi (p). However, whenever possible we will use the abbreviation ppdi . By the
denition of ppdi , we need to nd the probability that si will be visited during t time units
by some robot. Assuming perfect detection capabilities of the robots, ppdi is determined
only by the first visit of some robot to si , since once the intruder is detected the detection
mission is successful (specically, once the segment is visited, the game is over). Note
that ppdi is calculated regardless of the actions of the adversary.
As stated previously, in order to guarantee optimality of the patrol algorithm, the robots
should be uniformly distributed along the perimeter with a distance of d = N/k between
every two consecutive robots, and that they are coordinated in the sense that if they are

894

fiMulti-Robot Adversarial Patrolling

supposed to turn around, they do so simultaneously. In the following theorem and supporting lemmas we prove optimality of these assumptions in a full-knowledge adversarial
environment.
Lemma 1 follows directly from the fact that the movement of the robots is continuous,
thus a robot Rl cannot move from segment si to segment si+j , j > 0, without visiting
segments si+1 , . . . , sj1 in between. Note that since k > 1 it follows that the number of
segments unvisited by Rl is greater than 2t (otherwise a simple deterministic algorithm
would suce to detect the adversary with probability 1). Therefore during t time units
Rl residing initially in segment s0 cannot visit segment si , i < t, arriving from the other
direction of the perimeter without visiting the segments closer to its current location (s0 )
rst (this argument holds for segments to the left and to the right of s0 ).
Lemma 1. For a given p, the function ppdli : N  [0, 1] for constant t and Rl residing in
segment s0 is a monotonic decreasing function, i.e., as the distance between a robot and a
segment increases, the probability of reaching it during t time units decreases.
Lemma 2. As the distance between two consecutive robots along a cyclic patrol path is
smaller, the ppd in each segment is higher and vice versa.
Proof. Consider a sequence S1 of segments s1 , . . . , sw between two adjacent robots, Rl and
Rr , where s1 is adjacent to the current location of Rl and sw is adjacent to the current
location of Rr . Let S2 be a similar sequence, but with w  1 segments, i.e., the distance
between Rl and Rr decreases by one segment. Assume that other robots are at a distance
greater than or equal to w  1 from Rl and Rr , and that w  1 < t. Since a robot may
inuence the ppd in segments that are up to a distance t from it (as it has a probability of
0 of arriving at any segment at a greater distance within t time units), the probability of
penetration detection, ppd, in these sequences is inuenced only by the possible visits of Rl
and Rr .
Denote the probability of penetration detection in segment si  Sj by ppdi (j), 1  i  w,
j  {1, 2}, and the probability that the penetrator will be detected by robot Rx by ppdxi (j),
x  {l, r}. Therefore, for any segment si  Sj , ppdi (j) = ppdli (j) + ppdri (j)  ppdli (j)ppdri (j)
(either Rl or Rr will detect the adversary, not both). Note that either ppdli (j), ppdri (j) or
both can be equal to 0. We need to show that ppdi (2)  ppdi (1), for all 1  i  w, and
for at least one segment sm , ppdm (2) > ppdm (1). Specically, it is sucient to show that
ppdli (2) + ppdri (2)  ppdli (2)ppdri (2)  {ppdli (1) + ppdri (1)  ppdli (2)ppdri (2)}  0, and for some
i this inequality is strict.
For every segment si , ppdli (1) = ppdli (2) (there is no change in its relative location),
hence we need to prove that ppdri (2)  ppdri (1)  ppdli (2){ppdri (2)  ppdri (1)}. Since 0 
ppdli (2)  1, in order for the inequality to hold, it is left to show that ppdri (2)  ppdri (1)  0.
From Lemma 1 we know that ppdri (j) is monotonically decreasing, therefore for each i,
ppdri (2)  ppdri (1), which completes the proof of this inequality.
It is left to show that for some i = m, ppdrm (2)ppdrm (1) > ppdlm (2){ppdrm (2)ppdrm (1)},
i.e., for some m in which ppdlm (2) = 1, ppdrm (2) > ppdrm (1). Robot Rr may inuence the
ppd on both of his sides - segments located to the left and to the right of its current
position. Denote the number of inuenced segments to its right by y (y may be equal to
0). If y > 0, then ppdrwy+1 (2) > ppdrwy (1). In other words, Rr has a probability of 0
895

fiAgmon, Kaminka & Kraus

of reaching the segment with a distance of t + 1 from it in S1 , but in S2 it is y segments
away from it, therefore Rr has a probability greater than 0 to reach it. If y = 0, then
ppdrw (2) = 1 > ppdrw (1), as Rr lies exactly in segment sw of S2 , and ppdrw (1) = 0.
Theorem 3. A team of k mobile robots engaged in a patrol mission maximizes minimal ppd
if the following conditions are satisfied. a. The time distance between every two consecutive
robots is equal b. The robots move in the same direction and speed.
Note that condition b means that all robots move together in the same direction, i.e.,
if they change direction, then all k robots change their direction simultaneously.
Proof. Following Lemma 2, it is sucient to show that the combination of conditions a
and b yield the minimal distance between two consecutive
robots along the cyclic path.
(N )
Since we have N segments and k robots, there are k possibilities of initial placement of
robots along the cycle (robots are homogenous, so this is regardless of their order). If the
robots are positioned uniformly along the cycle, then the time distance between each pair
of consecutive robots is N/k. This is the minimal value that can be reached. Therefore,
clearly, condition a guarantees this minimality.
If the robots are not coordinated, then it is possible that two consecutive robots along
the cycle, Ri and Ri+1 , will move in opposite directions. Therefore the distance between
them will increase from Nk to Nk + 2, and by Lemma 2 the ppd in the segments between
them will be smaller. If Ri and Ri+1 move towards one another, then the distance between
them will be Nk  2 and the ppd in the segments between them will become higher. On the
other hand, some pair Rj and Rj+1 exists such that the distance between them increases,
as the total sum of distances between consecutive robots remains N , hence the minimal ppd
around the cycle will become smaller.
Therefore the only way of achieving the minimal distance (maximal ppd) is by assuring
that condition a is satised, and maintaining it is achieved by satisfying condition b.
Since when facing a full-knowledge adversary, the goal of the robots is to maximize the
minimal ppd along the perimeter, the following corollary follows.
Corollary 4. In the full-knowledge adversarial model, an optimal patrol algorithm must
guarantee that the robots are positioned uniformly along the perimeter throughout the execution of the patrol.
4.1 The Penetration Detection Problem
The general denition of the problem is as follows.
Penetration detection (PD) problem: Given a circular fence (perimeter) that is divided into N segments, k robots uniformly distributed around this perimeter with a distance
of d = N/k (in time) between every two consecutive robots, assume that it takes t time
units for the adversary to penetrate, and the adversary is known to have full-knowledge
of the patrol scheme. Let p be the probability characterizing the patrol algorithm of the
robots, and let ppdi (p), 1  i  d be a description of ppdi as a function of p. Find the

896

fiMulti-Robot Adversarial Patrolling

optimal value p, popt , such that the minimal ppd throughout the perimeter is maximized.
Formally,
popt = argmax{ min ppdi (p)}
0p1

1id

To summarize the model and the Theorems presented above, an optimal algorithm for
multi-robot perimeter patrol under the Markovian strategy assumption for the robots has
the following characteristics.

 The robots are placed uniformly around the perimeter with d segments between every
two consecutive robots.
 The robots are coordinated in the sense that if they decide to turn around, then they
do it simultaneously.
 At each time step, the robots continue straight with a probability of p or turn around
with a probability of 1  p, and if they turn around they stay in the same segment for
 time units.

Note that under the above framework (i.e. the framework for homogenous robots), the
division of the perimeter into sections of d segments creates an equivalent symmetric environment in the sense that in order to calculate the optimal patrol algorithm it is sucient
to consider only one section of d segments, and not the entire perimeter of N segments.
This is due to the fact that each section is completely equivalent to the other, and remains
so throughout the execution.
We divide the goal of solving the PD problem, i.e., nding an optimal patrol algorithm
into two stages.
1. Calculating the d ppdi functions for each 1  i  d. This is determined according to
the robotic movement model (directed or undirected), environment model (perimeter/fence) and sensorial model (perfect/imperfect, local/extended).
2. Given the d ppdi functions, nd the solution to the PD problem, i.e., maximize the
ppd in the segment(s) with minimal ppd.
These two steps are independent in the sense that incorporating various dierent robotic
models will not change the process of determining the solution to the PD problem, as long
as the result of the procedure are d functions representing the ppd values in each segment.
On the other hand, if we would like to consider dierent goal functions other than
maximizing the minimal ppd (for example maximizing the expected ppd), it can be done
without any change in the rst stage, i.e., determining the ppd functions. The important
result is that this framework can be applied to both dierent environment and robotic
models (for example fence patrol), and dierent goal functions (corresponding to dierent
adversarial settings).
897

fiAgmon, Kaminka & Kraus

The rst stage for a the basic model (perimeter patrol, directed movement model of the
robots, robots with perfect local sensing) is described in Section 4.2, and the second stage
is described in Section 4.3. Extensions of the rst stage to dierent robot motion models
and sensing models are described in Section 5.
4.2 Determining the Probability of Penetration Detection
In order to nd an optimal patrol algorithm, it is necessary to rst determine the probability
of penetration detection at each segment si (ppdi ), which is a function of p (the probability
characterizing the patrol algorithm, as shown in Section 4.1). In this section we present a
polynomial time algorithm that determines this probability.
As stated previously, based on the symmetric nature of the system, we need to consider
only one section of d segments that lie between two consecutive robots, without loss of
generality, R1 and R2 . We use a Markov chain in order to model the possible states and
transition between states in the system.
In order to calculate the probability of detection in each segment along t time cycles,
we use the graphic model G illustrated in Figure 2. For each segment si in the original
path, 1  i  d, we create two states in G: One for moving in a clockwise direction (scw
i ),
and the other for moving in a counterclockwise direction (scc
).
If
R
or
R
reach
one
1
2
i
of the si segments within t time units, then the adversary is discovered, i.e., it does not
matter if the segment is visited more than once during these t time units. Therefore we
would like to calculate only the probability of the first arrival to each segment, and this is
done by dening the state sdt (corresponding to s0 and s0 ) as absorbing states, i.e., once
a robot passes through si once, its additional visits to this segment in this path will not
be considered. The edges of G are as follows. One outgoing edge from scw
to scc
i
i exists
cw
with a probability of 1  p for turning around, and one outgoing edge to si1 exists with
a probability of p for continuing straightforward. Similarly, one outgoing edge from scc
i to
cw
cc
si exists with a probability 1  p for turning around, and one outgoing edge to si+1 exists
with a probability of p for continuing straightforward.
cc

S0

S1

S2

S3

S4

S0

cc

0

1p p

cw

1p 0

S1

R2
S

S dt

cw

p

S

4

1

cw

p

S

p

cw

cc

3

p

S

2

1p

S

4

S

3

1p
cc

p

1

1p

S

cc

cc

1

1

p

cw

cc

cw

S3

S4

S4

S dt

0

0

0

0

0

0

0

0

0

0

0

0

p

1p p

0

0

0

0

0

0

0

0

p

1p 0

0

0

0

0

0

cc

0

0

0

0

0

1p p

0

0

cw

0

0

0

p

1p 0

0

0

0

cc

0

0

0

0

0

0

0

1p p

S4

cw

0

0

0

0

0

p

1p 0

0

S dt

0

0

0

0

0

0

0

0

1

S3

1p

S

2

S dt

cc

S3

cw

S2
p

cw

S2

S2

cc

S2

cw

cc

S1

S1

R1

cw

S1

S3
S4

Figure 2: Conversion of the initial segments and robot locations into a graphical model, and the
respective stochastic matrix M . Each segment corresponds to two states: one going clockwise and one
going counterclockwise. ppdi are all paths starting from scw
and ending at sdt .
i

898

fiMulti-Robot Adversarial Patrolling

In the following theorem, we prove that the probability of detecting the adversary by
some robot in segment si (i.e., the probability of arriving at a segment during t time units)
is equivalent to nding all paths of size at most t to the absorbing state starting at state
scc
i . Therefore it is possible to use the Markov chain representation for determining ppdi ,
as shown in Algorithm FindFunc.
Theorem 5. Determining the probability of penetration detection at segment si , ppdi , is
equivalent to finding all paths of length at most t that start at scw
and end in sdt in the
i
Markov chain described above.
Proof. For simplicity reasons, in this proof we distinguish between sldt and srdt , which are
the absorbing state to the left and to the right of the Markov chain (respectively), although
practically they are represented by the same state sdt .
Clearly, due to the d and t values considered, ppdi is determined only by the visits of
the two robots surrounding the section of d segments s1 , . . . , sd , denoted by Rl and Rr .
Recall that the probability of penetration detection in segment si is dened as ppdi =
ppdli + ppdri  ppdli ppdri , where ppdri (ppdli ) is the probability that the adversary, penetrating
through si , is detected by Rr (Rl ). We claim that ppdli is equivalent to computing the
r
r
l
paths starting from scw
i and ending at the absorbing state sdt (similarly ppdi by state sdt ).
r
l
Clearly, under this claim, since a path of length at most t cannot reach both sdt and sdt , it
follows that ppdli ppdri = 0, and the theorem will follow. We prove the claim for ppdli , where
ppdri follows directly.
ppdli is the probability that Rl will reach si at least once during t time units. Therefore,
we must construct all paths starting from the current location of Rl that passes through si ,
but take into account only the rst visit to the segment (everything beyond the rst visit
results anyway in probability of detection = 1). At each step Rl continues straight with
probability p or turns around with probability 1  p. This is equivalent to keeping Rl in
place, and moving the segments towards Rl with probability p and switch the segments
direction with probability 1  p. Hence, every path starting at state scw
i (without loss of
generality; computing paths starting at scc
is
equivalent,
but
requires
switching
the locations
i
of Rl and Rr in the representation) reaching srdt is equivalent to a path started by Rl and
passing through si . Since srdt is set to be an absorbing state, every path passing through it
will not be considered again, i.e., only the rst visit of Rl to si is considered, as required.
Using the Markov chain, we can dene the stochastic matrix M which describes the
state transitions of the system. Figure 2 illustrates the Markov chain and its corresponding
stochastic matrix M used for computing the ppd functions. The probability of arrival at
segment si during t time units, hence the probability penetration detection in that segment,
t
cw
is the scc
2d+1 + s2d+1 entry of the result of Vi  M , where Vi is a vector of 0s, except for a
1 on the 2i  1th location. The formal description of the algorithm is given by Algorithm
1. Note that the algorithm makes a symbolic calculation, hence the result is a set of d
functions of p. The time complexity of Algorithm FindFunc depends on the calculation time
of M t , which is generally t  (2d)3 . However, since M is sparse, methods for multiplying
such matrices eciently exist (e.g., see Gustavson, 1978), reducing the time complexity to
t(2d)2 , i.e. O(td2 ). Since t is bounded by d  1, the time complexity is O(d3 ).

899

fiAgmon, Kaminka & Kraus

Algorithm 1 Algorithm FindFunc(d, t)
1: Create matrix M of size (2d + 1)(2d + 1), initialized with 0s
2: Fill out all entries in M as follows:
3: M [2d + 1, 2d + 1] = 1
4: for i  1 to 2d do
5:
M [i, max{i + 1, 2d + 1}] = p
6:
M [i, min{1, i  2}] = 1  p
7: Compute M T = M t
8: Res = vector of size d initialized with 0s
9: for 1  loc  d do
V = vector of size 2d + 1 initialized with 0s.
10:
11:
V [2loc]  1
Res[loc] = V  M T [2d + 1]
12:
13: Return Res
4.2.1 Handling Higher Values of 
Algorithm FindFunc and Figure 2 demonstrate the case in which  = 1, i.e., if the robot
turns around (with probability 1  p) it remains in its current position for one time step.
In the general case, when the robot turns around, the cost of turningmodeled in time
can be higher. In such cases, the Markov chain is modied to represent the value of  .
ccw
Specically, for each segment si , instead of having two corresponding states (scw
i and si ),
cw
ccw
we have 2( ) states: si and si , and one set of   1 states for turning around to each
direction (from cw to ccw and vice versa). The probabilities assigned to each of the edges
ccw and
is 1  p for the rst outgoing edge from scc
i to the rst intermediate state towards si
ccw
cc
1 for each edge on that direction, and similarly on the path from si to si . See Figure
3 for an illustration. The matrix M is lled out according to the new chain, and the time
complexity of creating this matrix grows in a factor of  from (2d + 1)t to (2 d + 1)t .
However, as long as  is a constant, the total time complexity does not change.
S0

S1

S2

S3

S4

S0

R1

R2
S

p

cw

S

4

p

cw

S

3

p

cw

S

2

1p

1 1p

1 1p

1 1p

1

1

1

1

cw

1

p

S dt

1
1

1

S dt

1

1
1

S

cc

4

p

1

1p

S

1

1

1p
p

1

1
cc

3

p

1p

S

cc

2

p

1p

S

cc

1

Figure 3: Illustration of the Markov chain when  > 1, and specically, here  = 3.

900

fiMulti-Robot Adversarial Patrolling

4.3 An Optimal Adversarial Patrol Algorithm for Full-Knowledge Adversaries
In cases in which the robots face a full knowledge adversary, it is assumed that the adversary
will take advantage of this knowledge to nd the weakest spot of the patrol, i.e., the segment
with minimal probability of penetration detection. Therefore an optimal patrol algorithm
to handle such an adversary is the one that maximizes the minimal ppd throughout the
perimeter. Hence we need to nd an optimal p, popt , such that the minimal ppd throughout
the perimeter is maximized.
Also here, since our environment is symmetric, we do not need to consider the entire
patrol path, but only a section of d segments between two consecutive robots. The input in
this procedure is the set of d ppdi (p) functions that were calculated in the previous section
(Section 4.2).
After establishing d equations representing the probability of detection in each segment,
we must nd the p value that maximizes the minimal possible value in each segment, where
p is continuous in the range p  [0, 1]. Denote these equations by ppdi (p), 1  i  d. The
maximal minimal value that we are looking is the p value yielding the maximal value inside
the intersection of all integrals of ppdi (p). The intersection of all integrals is also known as
the lower envelope of the functions (Sharir & Agarwal, 1996).
Observing the problem geometrically, consider a vertical sweep line that sweeps the
section [0, 1] and intersects with all d curves. It seeks the point p in which the minimal
intersection point between the sweep line and the curves, denoted by ppd (p), is maximal.
This p is the maximin point. Since the segment [0, 1] and the functions ppd1 , . . . , ppdd are
continuous, this sweep line solution cannot be implemented. We prove in the following
lemma that this point is either an intersection point of two curves, or a local maxima of
one curve (see Figure 4). See Algorithm 2 for the formal description of Algorithm FindP.

Figure 4: An illustration of two possible maximin points (marked by a full circle). The curves represent
d ppdi (p) functions in p  [0, 1]. On the left, the maximin point is created by the intersection of two
curves. On in the right, the maximin point it is the local maxima of the lowest curve.

In the following, we prove that Algorithm FindP nds point p such that the maximin
property is satised.
Lemma 6. A point p yields a maximin value ppd (p) if the following two properties are
satisfied.
a. ppd (p)  ppdi (p) 1  i  d.

901

fiAgmon, Kaminka & Kraus

b. One of the two following conditions holds: ppd (p) is an intersection of two curves (or
more), ppdi (p) and ppdj (p) or a local maxima of curve ppdk (p).
Proof. Property a. is derived from the denition of a maximin point. Therefore we are
looking for the maximal point that satises property a. We must still show that this point,
ppd (p), is obtained by either an intersection of two or more curves or is a local maxima.
Clearly, a maximal point of an integral is found on the border of the integral (the curve
itself). The area which is in the intersections of all curves lies beneath parts of curves,
ppdi1 , . . . , ppdim , such that ppdij is the minimal curve in the section between two points

j
j j
[lj , rj ] and m
j=1 [l , r ] = [0, 1]. By nding the maximal point in each section ppdmax =
max{f (x), x  [lj , rj ]}, and choosing the maximal between them, i.e., max{ppdjmax , 1  j 
m}, we obtain ppd (p). In each section [lj , rj ] the maximal point can be either inside the
section or on the borders of the section. The former case is precisely a local maxima of
ppdij . The latter is the intersection point of two curves ppdij1 , ppdij or ppdij , ppdij+1 .
Lemma 7. A point p exists yielding a maximin value ppd (p) > 0.
Proof. In order to prove the lemma, we need to show that the intersection of all integrals
ppd1 , . . . , ppdd in the x section [0, 1], and the y section (0, 1] is not empty. It suces to
show that for every ppdi , ppdi (x) > 0, 0 < x < 1.
Each function ppdi , 1  i  d represents the ppd in a segment si between two robots.
From our requirement that t  d2 + 1 (for  = 1), it follows that in all models we consider,
for 0 < p < 1 the ppd = 0. Note that if p = 0 or p = 1, then ppd is either 0 or 1, but this
does not contradict the fact that we have a point guaranteeing ppd (p) > 0.
Algorithm FindP nds this point by scanning all possible points satisfying the conditions
given in Lemma 6, and reporting the x-value (corresponding to the p value) with a y-value
dominated by all ppdi . The input to the algorithm is a vector of functions ppdi , 1  i  d
and the value t. Computing the intersections between every pair of functions costs d2 t2 :
d2 for all pair computation, t2 for nding the root of the polynomial using, for example, the
Lindsey-Fox method presented by Sitton, Burrus, Fox, and Treitel (2003). Computing the
dominance of the resulting points with respect to all other curves is d2 t as well. Therefore
the time complexity of Algorithm FindP is the complexity of Algorithm FindFunc, O(( Nk )3 ),
with additional cost of O(t2 d2 ) = O(( Nk )4 ) (the algorithm itself), i.e., jointly O(( Nk )4 ).
Theorem 8. Algorithm FindP(F, t) finds point p yielding the maximin value of ppd.
Proof. Algorithm FindP checks all intersection points between the pair of curves, and the
points of local maxima of the curves. It then checks the dominance of these points, i.e.,
whether in the location these points have a lower value compared to all other curves, and
picks the maximal of them. Therefore, if such a point is found, by Lemma 6, this point is
precisely the maximin point. Moreover, by Lemma 7 this point exists.
4.4 Examples
We have fully implemented Algorithm FindP in order to nd the optimal maximin p for pairs
of ds and ts. We use the following examples to illustrate how the relation between t and d
is reected in the ppd values. Recall that when running a deterministic patrol algorithm in
902

fiMulti-Robot Adversarial Patrolling

Algorithm 2 Algorithm FindP(d, t)
1: F  Algorithm FindFunc(d, t).
2: Set popt  0.
3: for Fpivot  F1,...,d do
4:
Compute local maxima (pmax , Fpivot (pmax )) of Fpivot in
5:
for each Fi , 1  i  d do
6:
Compute intersection point pi of Fi and Fpivot in the
7:
if Fpivot (pi ) > Fpivot (pmax ) and Fpivot (pi )  Fk (pi )k
8:
popt  pi .
9:
if Fpivot (pmax ) > Fpivot (pi ) and Fpivot (pi )  Fk (pi )k
popt  pmax .
10:
11: Return (pmax , Fpivot (pmax )).

the range (0, 1).
range (0, 1).
then
then

all scenarios we handle, the minimal ppd is 0. We assume the robots are initially heading
to the clockwise direction.
First of all, we have seen that the minimal ppd achieved after running FindP was always
more than 0. As t/d  1, i.e., t increases, then the value of the maximin ppd increases, and
vice versa, i.e., as t/d  1/2, then the value of the maximin ppd decreases. This can be seen
clearly in Figure 5. In this case, we have xed the value of t to 8 and checked the maximin
ppd for 9  d  15. When t/d is close to 1 (d = 9, t = 8) the maximin ppd = 0.423, and
the value decreases to 0.05 when t/d is close to 1/2 (d = 15, t = 8). Similar results are seen
if we x the value of d and check for dierent values of t.

Figure 5: On the left, results of maximin ppd for xed t = 8 and dierent values of d: the possible
maximin ppd decreases as d increases. On the right, results of maximin ppd for xed d = 16 and
dierent values of t: the possible maximin ppd increases as t increases.

In Figure 6, we present the values of the ppd in all 16 segments, for all dierent possible
values of t (9  d  15). It is seen clearly, that the value of ppd usually decreases as
the distance from the left robot increases, until it reaches the segment with maximin ppd,
then the value rises again until reaching the current location of the robot to the right. The
reason lies in the fact that the segments to the left of the segment with the maximin ppd
are inuenced mostly by the robot on the left, and the segments to the right of that point
are mostly inuenced by the robot to the right. Since the ps yielding the maximin point
in this example have value of greater than 0.8 for all ts, the segment having the maximin
value is to the right of the midpoint.
903

fiAgmon, Kaminka & Kraus

Figure 6: ppd values in all 16 segments for all t values (9 to 15)

5. Accounting for Movement Constraints and Sensing Uncertainty
In this section we describe various ways in which the basic framework of multi-robot patrol
can be used to solve the problem of nding an optimal patrol algorithm in various other
settings. First, we describe the case in which the movement model of the robots is not
necessarily directed. We then discuss various sensing capabilities of the robots in perimeter
patrol: imperfect local sensing, perfect long-range sensing and imperfect long-range sensing.
Finally, we describe the case in which the robots should travel along an open polyline (fence)
rather than a perimeter.
5.1 Dierent Movement Models
A basic assumption of the robotic framework is that the robots movement model is directed
in the sense that if a robot has to go back to visit a point behind it, it has to physically
turn around. This directed movement model is suitable for various robotic types, for example dierential drive robots commonly used in robotic labs. However, in some cases the
robots movement is undirected, for example if the robot travels along train tracks.We will
demonstrate in this section how the basic framework can be used also in the latter case,
i.e., if the robot movement is undirected.
We examine the dierence in the Markov chain and the resulting ppd in three dierent
cases:
1. Bidirectional Movement model, denoted by BMP. Here, the robots movement pattern
is similar to movement on tracks or a camera going back and forth along a xed course
(omnidirectional robots). In this model, the robots have no movement directionality
in the sense that switching directionsright to left and vice versadoes not require
physically changing the direction of the robot (turning around).

904

fiMulti-Robot Adversarial Patrolling

2. Directional Costly-Turn model, denoted by DCP, the basic framework discussed to
far for   1. The robots movement is directed, and turning around is a special
operation that has an attached cost in time. Specically, we show the results here for
 = 1.
3. Directional Zero-Cost model, denoted by DNCP, which is a special case of the DCP
model with  = 0. The robots movement is directed, yet turning around does not
take extra time. This is coherently dierent from BMP, as in each step the robot does
not go either right or left, but straight or back (where each could be either to the
right or to the left, depending on the current heading of the robot).
The basic framework can be used for handling all three models simply by adapting the
Markov chain to the current model. This changes only lines 5  6 in Algorithm FindFunc. A
description of the Markov chains are described in Figure 7. In the BMP model, it moves one
step to the right (segment i + 1) with a probability of p and one step to the left (segment
i1) with a probability of 1p. This model is similar to a random walk. The corresponding
Markov chain is simple: edges exist from si to si+1 with a probability of p and from si to
si1 with a probability of 1  p (with no related direction). In both the DNCP and DCP
models, we assume directionality of movement, hence the robot continues its movement in
its current direction with a probability of p, and turns around (rewinds) with a probability
of 1  p. In DCP, if the robot turns around it will remain in segment i (as described in
Figure 2). In the DNCP model, the chain is similar to the one above, however edges will
cc
cc
cw
exist from scw
i to si+1 and from si to si1 with a probability of 1  p. See Figure 7 for an
illustration of DNCP, DCP and BMP as a Markov chain.
S0

S1

S2

S3

S4

S0

R1

R2
S

S dt

cw

p

S

4

DCP

cw

1p
p

S

cc

cc

cw

DNCP

cw

S

cc

S

4

S dt

S

S

cc

S

3

p

p

p

p

1

p

cc

1

cw

p

S

2

cw

1

p

S

S

4

1p

3

S dt

1p

cc

S

2

cc

1

1p

p
p
p

1p

S dt

1p

S

2

1p

cc

cw

p

p

3

1p
p

BMP

S

p

2

p

p

1p S 4

cw

1p

S

3

p

S dt

S
1p

S

4

p

3

S
1p

S

2

S dt

1

1p

Figure 7: Conversion of the initial segments and robot locations into a graphical model in all three
movement models.

905

fiAgmon, Kaminka & Kraus

We examined the dierence between the resulting ppd values in the three models in a
case where d = 16, t = 12 (Figure 8). It is clearly noticeable that the DCP model yields
less or equal values of ppd compared to DNCP model throughout the segments. The reason
is because when turning around, in the DCP model, the operation costs an extra cycle,
therefore the probability of arriving at a segment decreases, compared to the case in which
turning around is not costly. Another interesting phenomena is that the ppd values of the
BMP are considerably higher (and close to 1) than the values obtained by other models
for segments closer to the location of the righthand side robot. The value then decreases
dramatically around the value of t and then increases back again. Recall that here there is
no directionality of movement, therefore the probability of going right is 0.707 and going
left is 1  0.707 = 0.293, which explains this phenomena. One might have expected to have
p = 0.5 in the random walk model (BMP), however by choosing an equal probability for
going right and left, the robots will necessarily neglect the segments further away from them
(the mid segments between two consecutive robots), resulting in a lower minimal ppd.

Figure 8: Results of maximin ppd values for d = 16 and t = 12 for all three models: DNCP, DCP
and BMP. The maximin ppd values are circled.

5.2 Perimeter Patrol with Imperfect Penetration Detection
Uncertainty in the perception of the robots should be taken into consideration in practical
multi-robot problems. Therefore we consider the realistic case in which the robots have
imperfect sensorial capabilities. In other words, even if the adversary passes through the
sensorial range of the robot, it still does not necessarily detect it.
We introduce the ImpDetect model, in which a robot travels through one segment per
time cycle along the perimeter while monitoring it, and has imperfect sensing. Denote the

906

fiMulti-Robot Adversarial Patrolling

probability that an adversary penetrating through a segment si while it is monitored by
some robot R and R will actually detect it by pd  1.
Note that if pd < 1, revisiting a segment by a robot could be worthwhileit could
increase the probability of detecting the adversary. Therefore the probability of detection
in a segment si (ppdi ) is not equivalent to the probability of first arriving at si (as illustrated
in Section 4.2), but the probability of detecting the adversary during some visit y to si ,
0  y  t. Denote the probability of the yth visit of some robot to segment si by wiy .
Therefore ppdi is dened as follows.
ppdi = wi1 pd + wi1 (1  pd )  {wi2 pd + wi2 (1  pd )  {. . . {wit  pd }}}

(1)

In other words, the probability of detecting the penetration is the probability that it
will be detected in the rst visit (wi1  pd ) plus the probability that it will not be detected
then, but during later stages. This again is the probability that it will be detected during
the second visit (wi2  pd ) or at later stages, and so on.
Note that after t time units, wit = 0 for all currently unoccupied segments si , and if a
robot resides in si , then wit is precisely (1  pd )t .
One of the building blocks upon which the optimal patrol algorithms are based, is the
assumption that the probability of detection decreases or remains the same as the distance
from a robot increases, i.e., it is a monotonic decreasing function. This fact was used in
Section 4 in proving that in order to maintain an optimal ppd, the robots must be placed
uniformly around the perimeter (with a uniform time distance), and maintain this distance
by being coordinated. In order to show this here as well, we rst prove that the probability
of detection monotonically decreases with the distance from the location of the robot.
Lemma 9. Let S = {st+ , . . . , s1 , s0 , s1 , . . . , st } be a sequence of 2t segments, where robot
Ra resides in s0 at time 0. Then i  0, ppdi  ppdi+1 , and i  0, ppdi  ppdi1 .
Proof. First, assume that i > 0 (positive indexes). By Equation 1, we need to compare
1 p + w 1 (1 
between wi1 pd + wi1 (1  pd )  {wi2 pd + wi2 (1  pd )  {. . . {wit  pd }}} and wi+1
d
i+1
2 p + w 2 (1  p )  {. . . {w t
pd )  {wi+1
d
d
i+1
i+1  pd }}}. It is therefore sucient to show that
m , for all 1  m  t. We prove this by induction on m. As the base case, consider
wim  wi+1
1 . This is accurately proven in Lemma 1, based
m = 1, i.e., we need to show that wi1  wi+1
on the fact that the movement of the robots is continuous, therefore in order to get to a
segment you must visit the segments in between (the formal proof also uses the conditional
probability law).
m . Denote the
We now assume correctness for m < m, and prove that wim  wi+1
probability that a robot placed at segment si will return to si within r time units by xi (r).
In our symmetric environment, for every i 
and j, xi (r) = xj (r). Moreover, r, xi (r) 
m =
xi (r  1). Therefore wim can be described as r+ut wim1 (u)  xi (r), and similarly wi+1

m1
m1
m1
 wi+1
, and since xi (r) =
r+ut wi+1 (u)  xi+1 (r). By the induction assumption, wi
m
m
xi+1 (r), it follows that wi  wi+1 , proving the lemma for positive indexes.
The negative indexes are a reective image of the positive indexes, but with t   time
units. Since the induction was proven for all t values, the proof for the negative indexes
directly follows.

907

fiAgmon, Kaminka & Kraus

The following Theorem follows directly from Lemma 9. The idea behind this is that
since the probability of penetration detection decreases as the distance from the robots
grow, both minimal ppd and average ppd are maximized if the distance between the robots
is as small as possible. Since the patrol path is cyclic, this is achieved only if the distance
between every two consecutive robots is uniform, and remains uniform. Note that Theorem
10 below is a generalization of Theorem 3 for imperfect sensing (based on the fact that that
the general structure of the ppd function remains the same even if the robots might benet
from revisiting a segment, and by that increasing the ppd in that segment).
Theorem 10. In the full knowledge adversarial model, a patrol algorithm in the ImpDetect
model is optimal only if it satisfies two conditions: a. The robots are placed uniformly
around the perimeter. b. The robots are coordinated in the sense that if they turn around,
they do it simultaneously. By assuring these two conditions, the robots preserve a uniform
distance between themselves throughout the execution.
Algorithm for nding ppdi with imperfect sensorial detection:
Find the probability of penetration detection with pd  1 results in a dierent Markov
chain, hence a dierent stochastic matrix M . Figure 9 demonstrates the new graphical
model and the new resulting stochastic matrix M (compared to Figure 2, in which pd = 1).
cc
The dierence in the algorithm is in the division of s0 to two states, scw
0 and s0 , the
addition of the absorbing state sdt that represents the detected state and the transitions
between these states. The ppdi is therefore obtained after t + 1 steps (compared to t steps)
in the sdt s location in the result vector.
The time complexity of the algorithm remains O(d4 ).
S0

S1

S2

S3

S4

S0
cc

R1

R2

S

cw

S

4

1p

cw

S

4

p

S

3

1p
cc

p

p

cw

S

2

1p

S

cc

3

p

1

S

2

p

S

cw

0

pd

(1p)(1pd )

1p
cc

p

S

cc

1

p(1pd )

S

cc

0

pd

cc

cw

cc

cw

S4

S4

S0

S0

S dt

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

cc

0

0

0

1p p

0

0

0

0

0

0

cw

0

p

1p 0

0

0

0

0

0

0

0

cc

0

0

0

0

0

1p p

0

0

0

0

cw

0

0

0

p

1p 0

0

0

0

0

0

cc

0

0

0

0

0

0

1p

0

0

S3
S4

cw

S4

cc

p

cw

S3

p

S3
1

cc

S3

1p 0

S2

S dt

cw

S2

0

S1

cw

1p p

S2

cc

S2
p

cc

S1

cw

S1
p(1pd )

cw

S1

S0

0
p(1pd )

0

p

0

0

0

0

p

1p 0

0

0

0

0

0

0

0

0 (1p)(1p
d

0

0

0
)

pd

S0

cw

0

0

0

0

0

0

0 p(1pd ) (1p)(1p
d )0

pd

S dt

0

0

0

0

0

0

0

1

0

0

0

Figure 9: Conversion of the initial segments and robot locations into a graphical model, and the
respective stochastic matrix M for the imperfect sensing model.

908

fiMulti-Robot Adversarial Patrolling

5.3 Improving Sensing Capabilities in Perimeter Patrol
In this section we present further enhancements by considering various sensing capabilities
of the robots. Specically, we rst consider the case in which a robot can sense beyond
its currently visited segment. We then oer a solution to the case in which the robot can
sense beyond its current position, yet its sensing capabilities are not perfect, and change as
a function of the distance from its current position.
5.3.1 Extending (Perfect) Sensing Range
In this section we consider the LRange model, in which the sensorial range of a robot exceeds
the section which it currently resides in. Use L to denote the number of segments the robot
senses beyond the segment it currently occupies. If L > 0, we refer to the L segments as
shaded segments. Note that the location of the shaded segments depends on the direction
of the robot shading them, and they are always in the direction the robot is facing.
A trivial solution to dealing with such a situation is to enlarge the size of the segment,
and thus enlarge the length of the time unit used as base for the system, such that it will
force L to be 0. However, in this case we lose accuracy of the analysis of the system, as the
length of the time cycle should be as small as possible to also suit the velocity of the robots
and the value of t.
In general, the values of t that can be handled by the system are bounded by its relation
to d (the distance between every two robots along the path) - see Section 4. If L > 0, this
changes. Specically, if L = 0, then the possible values of t considered are d+
2  t  d  1.
However, if L > 0, then it is possible to handle even smaller values of t, i.e., even if the
penetration time of the adversary is short. Formally, the possible values of t are given in
the following equation.
d+
LtdL1
2
If t is smaller than d+
2  L, then an adversary with full knowledge will manage to
penetrate with a probability of 1, i.e., there is a segment (sL+1 ) which is unreachable within
t time units. On the other hand, if t is greater than d  L  1, then a simple deterministic
patrol algorithm will detect all penetrations with a probability of 1. We assume that during
the  time units the robot turns around, it can sense only its current segment.
This change in the sensing model of the robot is reected in the Markov chain, as seen in
cw
Figure 10. The change is that we add 2L arrows to the absorbing state sdt , from scw
1 , . . . , sL
cc
cc
and sd , . . . , sdL+1 . The stochastic matrix M changes accordingly, and the probability of
penetration detection in segment si becomes the result of the vector multiplication M t+1 Vi ,
where Vi is a vector of size 2d + 1 with all entries 0 except for entry corresponding to the
location of scw
i , which holds a value of 1, similar to the process described in Algorithm
FindFunc (1).
5.3.2 Extending the Sensorial Range Along With Imperfect Detection
In many cases, the actual sensorial capabilities of the robot are composed of the two characteristics described in the previous sections, i.e., the robot can sense beyond its current
segment, however the sensing ability is imperfect. Therefore in this section we introduce
909

fiAgmon, Kaminka & Kraus

L=1
cc

S0

S1

S2

S3

S4

S0

S1

cc

0

cw

cc
cw

S

cw

p

S

4

cw

1p

S

S

cw

p

S

cc

3

p

p

S

2

1p

cc

4

p

3

1

cc

1p

S

cc

1

2

p

1p

S

cc

1

S dt

cw

cc

cw

S4

S dt

1p p

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

0

0

0

1p p

0

0

0

0

0

p

1p 0

0

0

0

0

0

0

0

0

0

0

1p p

0

0

0

0

0

p

1p 0

0

0

0

cc

0

0

0

0

0

0

1

0

0

S4

cw

0

0

0

0

0

p

1p 0

0

S dt

0

0

0

0

0

0

0

0

1

S3
1

cw

S4

S2
1

cc

S3

S2

cw

cw

S3

S1

R2

cc

S2

S1

R1

cw

S1

S3
S4

S2

Figure 10: An illustration of L segments shaded by robot R. In this case R is facing right, therefore
the shaded segments are to its right. The Markov chain changes accordingly, therefore also the stochastic
matrix M .

the ImpDetLRange sensorial model, which is a combination of the LRange and the ImpDetect
models. Here the robot can sense L segments beyond its current segment, yet the pd in
each segment varies and is not necessarily 1. We therefore describe how to compute ppdi in
this case, which deals with the most realistic form of sensorial capabilities (Duarte & Hu,
2004): imperfect, long range sensing.
The information regarding the sensorial capabilities of the robots includes two parameters. The rst describes the quantity of the sensing ability, i.e., the number of segments that
exceeds the current segment in which robot resides, for which it has some sensing abilities,
denoted by L. The second parameter describes the quality of sensing in all segments the
robot can sense. This is given in the form of a vector VS = {v0 , v1 , . . . , vL }, where vi is the
probability that the robot residing in s0 will detect a penetration that occurs in segment si .
We assume that the values in VS decrease monotonically, i.e., as i increases, vi decreases or
remains the same.
The Markov chain in this model, as illustrated in Figure 11, changes in order to reect
the imperfect sensing along with the long range sensing. The absorbing state sdt exist
in addition to the states scw
0 and s0 cc. The transition probabilities are added from 2L
segments: i, j 0  i  L; d  L + 1  j  d, a transition from scw
i to s0 with probability vi
cc
and from sj to s0 with probability vdj+1 . In addition, the transition from scw
i to si cc is
cw with probability (1  p)(1  v
with probability (1  p)(1  vi ), from scc
to
s
dj+1 ), hence
j
j
cw
cw
cc
cc
the transition probability from si to si1 is p(1  vi ) and from sj to sj+1 is p(1  vdj+1 ).
The probability of penetration detection in segment si is the result of M t+1 multiplied
by Vi in location s0 of the result vector. Note that also here, similar to the solution described
in Section 5.2, since we added a new absorbing state (which takes an extra step to reach),
ppdi is the result in the product of the stochastic matrix and Vi in location s0 after t + 1
time steps (not t).

910

fiMulti-Robot Adversarial Patrolling

v0<1 v1<1
S0

S1

S2

S3

S4

S0
cc

R1

S1

R2

cc

S1
p(1v0 )

cw

S1

v1

S

p

S

4

1p

cw

S

4

p

S

3

1p
cc

p

cw

2

1p

S

cc

3

p

S

p

1

p(1v1 )

S

cw

0

(1p)(1v
1 )

(1p)(1v0 )

cc

cc

cc

2

S

cw

S

p(1v1 )

1

p(1v0 )

S

0

v0

v0

cw

cc

cw

S4

S0

S0

S dt

0

v1

) 0

0

0

0

0

0

0

0

0

0

0

0 p(1v1 ) v1

0

0

1p p

0

0

0

0

0

0

p

1p 0

0

0

0

0

0

0

0

cc

0

0

0

0

0

1p p

0

0

0

0

cw

0

0

0

p

1p 0

0

0

0

0

0

cc

0

0

0

0

0

0

0

1p

cw

0

0

0

0

0

p

1p 0

0

p(1v0 ) 0

0

0

0

0

0

0 (1p)(1v
0

S3
S4
S4

cc

v1

cc

S4

0

S0

p

cw

S3

0

S3
1

cc

S3

0

(1p)(1v
)
1
0 p(1v

(1p)(1v
0 )0

cw

S2

cw

S2

S dt

cc

S2

cc

S2
cw

0

cw

S1

0

p

0

0

0

0
)

v0

cw

S0

0

0

0

0

0

0

0 p(1v0 ) (1p)(1v
0 )0

v0

S dt

0

0

0

0

0

0

0

1

0

0

0

Figure 11: An illustration of L segments shaded by robot R, where the probability of detection is not
necessarily 1. In this case R is facing right, therefore the shaded segments are to its right. The Markov
chain and the stochastic matrix M changes accordingly.

5.4 Multi-Robot Adversarial Patrolling Along Fences
In our general work, and specically in previous sections, we assumed the robots travel
around a closed, circular, area. In this section we discuss patrolling along an open polyline,
also known as fence patrol. First, we will discuss how this patrol is dierent from perimeter
patrol. We will then describe an algorithm for determining ppdi in fence patrol assuming
the robots have perfect sensing capabilities, and nally we will provide an algorithm for
robots with imperfect sensing.
5.4.1 Patrolling Along a Closed Polyline vs. an Open Polyline
In the following, we describe why patrolling along an open polyline is more challenging than
patrolling in cyclic environments (closed polyline).
The rst reason lies in the fact that the robots are required to go back and forth along
a part (or parts) of the open polyline. As a result, the elapsed time between two visits of
a robot at each point along this line can be almost twice as long as the elapsed time in a
circular setting. In Figure 12, we are given two environments: a closed polyline (circle) (a)
and an open polyline (b). Note that open polylines b. and c. are equivalent in the sense
that each robot travels through one segment per time step, regardless of the shape of the
section. Both lines a. and b. are of the same total length l and with the same number of
robots (4). In the circular environment, if it takes an adversary more than l/4 time units to
penetrate - it will never be able to penetrate even if the robots simply continuously travel
with uniform distance between them. However, if the robots travel along an open polyline
(b), the maximal time duration between two visits of the roboteven in the best case, is
2l/4  2 (Elmaliach, Shiloni, & Kaminka, 2008). Therefore a weaker adversary that has a

911

fiAgmon, Kaminka & Kraus

penetration time which is almost twice as long as in the circular fence might still be able to
penetrate.

a.

b.

c.

Figure 12: Illustration of the dierence between patrolling along a line and patrolling along a circle, for
dierent polylines

Another reason for the added complication in analyzing the probability of penetration
detection in open polyline environments lies in the asymmetric nature of traveling in the
segments along time. In a circular environment, if the robots are coordinated and switch
directions in unison, then the placement of the robots is symmetric in each time unit. Therefore all segments in the same distance from some robot (with respect to its direction) have
the same probability of penetration detection. Hence in order to calculate an optimal way
of movement (in our case the probability p of turning around), it is sucient to consider
only one section of d segments, and the resulted p is equivalent throughout the execution.
In an open polyline environment this is not the case. The probability of penetration detection diers with respect to the current location and direction of the robot. Therefore
the algorithm that nds the ppd for each segment, needs to calculate the ppd as a function
of p for each segment si for each possible initial location of the robot inside the section.
Therefore this results in a matrix of size d  d of the ppd functions (as opposed to a vector
of d functions in the circular fence).
5.4.2 Determining ppdi in an Open Polyline (Fence)
Following the framework for multi-robot patrol along an open line proposed by Elmaliach
et al. (2008), we assume each robot is assigned to patrol back and forth along one section of d
segments. Given this framework, we would like to compute the optimal patrol algorithm for
the robots along the section. Similar to the perimeter patrol case (Section 4.2), we describe
the system as a Markov chain (see Figure 13), with its relative stochastic matrix M . Since
the robots have directionality associated with their movement, we create two states for each
segment: the rst for traveling in a segment in the clockwise direction, and the second for
traveling in the counterclockwise direction. The probability of turning around at the end of
each section is 1, otherwise the robot will continue straight with probability of p, and will
turn around with probability of 1  p.
Note that the main dierence from the perimeter patrol calculation of ppdi lies in the
number of resulting ppdi functions. In perimeter patrol, due to its symmetric nature, there is
one ppdi function for each segment between the current location of each robot, representing
the probability of a robot arriving there during t time units. Here, however, ppdi depends
on the current location of the robot, hence for each location of the robot we have d functions
of probability of penetration detection, therefore a total of d2 such functions (compared to
d in perimeter patrol).
912

fiMulti-Robot Adversarial Patrolling

Denote the probability of penetration detection in segment si given that the robot is
currently at segment sj by ppdji . In order to calculate the d ppdji function for all 1  i, j  d,
we create d dierent matrices: M1 , . . . , Md . Each matrix Mi corresponds to calculating ppdji ,
i.e., the probability of penetration detection in segment si , and from that we calculate ppdji
from every current location sj of the robot (similar to what is done in perimeter patrol).
Figure 13 demonstrates the matrix M2 with which ppd2i is calculated. The gure describes
the general case of pd  1, i.e., the robot might have imperfect sensing.
cc

S1

S2

S3

S4

S1

cc

0

cw

S1

cw

1

S

p

S

4

cw

3

p

S

cw

4

p

S

(1p)(1p
d )

1p

cc

p(1pd )

2

S

cc

3

S

cc

2

p

1

1

S

p(1pd )

S dt
1p

cc

1

1

cw

cc

cw

S dt

S4

S4

1p p

0

0

0

0

0

0

1

0

0

0

0

0

0

0

0

cc

0

0

0

)0

0

0

pd

cw

0 p(1pd )(1p)(1p
d ) 0

0

0

0

0

pd

cc

0

0

0

0

0

1p p

0

0

cw

0

0

0

p

1p 0

0

0

0

cc

0

0

0

0

0

0

0

1p

cw

0

0

0

0

0

p

1p 0

0

0

0

0

0

0

0

1

S3
S3
S4
S4

pd

cc

S3

S2

cw

cw

S3

S2

S

cc

S2

S1
pd

cw

S1

S dt

p

S2

(1p)(1p
d )p(1pd

0

p

Figure 13: Description of the system as a Markov chain, along with its stochastic matrix M for
calculating the ppd in segment s2 .

5.4.3 Optimal Algorithm for Fence Patrol
In the case of fence patrolling, the ppd value depends on the current location of the robot.
Consequently, the optimal p value characterizing the patrol of the robots is dierent for
each segment si , where 1  i  d. Therefore there could be dierent optimal p values with
respect to both location and orientation of the robot (2d values). However, it is sucient
to calculate the ppd values only d times (and not 2d times)only for one direction, as the
other direction is a reective image of the rst.
In order to nd the maximin point for the fence patrolling case, we use algorithm
MaximinFence, which nds the value p such that the minimal ppd is maximized, using
Algorithm FindP that computes this point by nding the maximal point in the integral
intersection of all curves (ppdi ). The complete description of the algorithm is shown in
Algorithm 3.
Algorithm 3 Procedure MaximinFence(d, t)
1: M  FindFencePPD(d, t)
2: for i  1 to d do
3:
OpP [i]  FindP(d, t) with additional given input M [i] as a vector of ppd functions.
4: Return OpP

913

fiAgmon, Kaminka & Kraus

6. Summary
This paper presents the problem of multi-robot patrolling in strong, full-knowledge, adversarial environments. In this problem a team of robots is required to repeatedly visit
some path, in our basic case a perimeter, and detect penetrations that are controlled by an
adversary. We assume the robots act in a strong adversarial model, in which the adversary
has full knowledge of the patrolling robots and uses this knowledge in order to penetrate
through the weakest spot of the patrol. We describe a framework for the basic case of multirobot patrol around a closed polygon, and use this framework for developing, in polynomial
time, an optimal patrol algorithm, i.e., an algorithm that strengthens the weakest spot of
the patrol. This framework is then extended in order to solve the problem also in an open
fence environment and in various movement and sensing models of the robots.
The paper makes several assumptions allowing the computation of an optimal strategy
for the patrolling robots. One such assumption is the rst order Markovian strategy of the
patrolling robots. Although proving or disproving the optimality of using rst order Markovian strategy is hard, it could be interesting to examine the case of higher order Markovian
strategies and compare their time complexity and performance to the solution discussed
here. Another direction for future work involves non-uniform environments, in which the
utility obtained from detecting penetrations on one hand or succeeding in penetration on
the other is not uniform throughout the environment. Other challenges left for future work
include handling heterogenous robots and non linear environments.

7. Acknowledgments
Preliminary results appeared in Proceedings of the IEEE International Conference on
Robotics and Automation (2008), in Proceedings of the Tenth Conference on Intelligent
Autonomous Systems (2008) and in Proceedings of IJCAI Workshop on Quantitative Risk
Analysis for Security Applications (QRASA) (2009). This research was supported in part by
ISF grant #1357/07 and #1685/07, and MOST grant #3  6797. We thank the anonymous
reviewers for constructive comments and helpful suggestions, and as always, thanks to K.
Ushi.

References
Agmon, N., Kaminka, G. A., & Kraus, S. (2008). Multi-robot fence patrol in adversarial
domains. In Proceedings of the Tenth Conference on Intelligent Autonomous Systems
(IAS-10), pp. 193201. IOS Press.
Agmon, N., Kraus, S., & Kaminka, G. A. (2008). Multi-robot perimeter patrol in adversarial settings. In Proceedings of the IEEE International Conference on Robotics and
Automation (ICRA).
Agmon, N., Kraus, S., & Kaminka, G. A. (2009). Uncertainties in adversarial patrol. In Proc.
of the IJCAI 2009 workshop on Quantitative Risk Analysis for Security Applications
(QRASA).

914

fiMulti-Robot Adversarial Patrolling

Ahmadi, M., & Stone, P. (2006). A multi-robot system for continuous area sweeping tasks.
In Proceedings of the IEEE International Conference on Robotics and Automation
(ICRA).
Almeida, A., Ramalho, G., Santana, H., Tedesco, P., Menezes, T., Corruble, V., & Chevaleyr, Y. (2004). Recent advances on multi-agent patrolling. Lecture Notes in Computer
Science, 3171, 474483.
Amigoni, F., Gatti, N., & Ippedico, A. (2008). Multiagent technology solutions for planning
in ambient intelligence. In Proceedings of Agent Intelligent Technologies (IAT-08).
Basilico, N., Gatti, N., & Amigoni, F. (2009a). Extending algorithms for mobile robot
patrolling in the presence of adversaries to more realistic settings. In Proceedings
of the IEEE/WIC/ACM International Conference on Intelligent Agent Technology
(IAT), pp. 565572.
Basilico, N., Gatti, N., & Amigoni, F. (2009b). Leader-follower strategies for robotic patrolling in environments with arbitrary topologies. In AAMAS, pp. 5764.
Chevaleyre, Y. (2004). Theoretical analysis of the multi-agent patrolling problem. In Proceedings of Agent Intelligent Technologies (IAT-04).
Coppersmith, D., Doyle, P., Raghavan, P., & Snir, M. (1993). Random walks on weighted
graphs and applications to on-line algorithms. Journal of the ACM, 40 (3).
Duarte, M. F., & Hu, Y. H. (2004). Distance-based decision fusion in a distributed wireless
sensor network. Telecommunication Systems, 26 (2-4), 339350.
Elmaliach, Y., Agmon, N., & Kaminka, G. A. (2007). Multi-robot area patrol under frequency constraints. In Proceedings of the IEEE International Conference on Robotics
and Automation (ICRA).
Elmaliach, Y., Agmon, N., & Kaminka, G. A. (2009). Multi-robot area patrol under frequency constraints. Annals of Math and Artificial Intelligence, 57 (3-4), 293320.
Elmaliach, Y., Shiloni, A., & Kaminka, G. A. (2008). A realistic model of frequencybased multi-robot fence patrolling. In Proceedings of the Seventh International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS-08).
Gustavson, F. G. (1978). Two fast algorithms for sparse matrices: Multiplication and
permuted transposition. ACM Trans. Math. Softw., 4, 250269.
Haynes, T., & Sen, S. (1995). Evolving behavioral strategies predators and prey. In IJCAI95 Workshop on Adaptation and Learning in Multiagent Systems, pp. 3237.
Lynch, N. A. (1996). Distributed Algorithms. Morgan Kaufmann.
Paruchuri, P., Pearce, J. P., Tambe, M., Ordonez, F., & Kraus, S. (2007). An ecient
heuristic approach for security against multiple adversaries. In Proceedings of the
Sixth International Joint Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS-08).
Paruchuri, P., Tambe, M., Ordonez, F., & Kraus, S. (2007). Security in multiagent systems
by policy randomization. In Proceedings of the Sixth International Joint Conference
on Autonomous Agents and Multi-Agent Systems (AAMAS-07).
915

fiAgmon, Kaminka & Kraus

Pita, J., Jain, M., Ordonez, F., Tambe, M., Kraus, S., & Magorii-Cohen, R. (2009). Eective solutions for real-world stackelberg games: When agents must deal with human
uncertainties. In Proceedings of the Eighth International Conference on Autonomous
Agents and Multiagent Systems (AAMAS-09).
Sak, T., Wainer, J., & Goldenstein, S. K. (2008). Probabilistic multiagent patrolling. In
Proc. of the 19th Brazilian Symposium on Artificial Intelligence (SBIA-08), pp. 124
133.
Sharir, M., & Agarwal, P. K. (1996). Davenport-Schinzel sequences and their geometric
applications. Cambridge University Press.
Shieh, J. S., & Calvert, T. W. (1992). View and route planning for patrol and exploring
robots. Advanced Robotics, 6 (4), 399430.
Sitton, G., Burrus, C., Fox, J., & Treitel, S. (2003). Factoring very-high-degree polynomials.
Signal Processing Magazine, IEEE, 20 (6), 27  42.
Stewart, W. J. (1994). Introduction to the Numerical Solution of Markov Chains. Princeton
University Press.
Vidal, R., Shakernia, O., Kim, H. J., Shim, D. H., & Sastry, S. (2002). Probabilistic pursuitevasion games - theory, implementation, and experimental evaluation. Robotics and
Automation, IEEE Transactions on, 18 (5), 662669.

916

fiJournal of Artificial Intelligence Research 42 (2011) 765-813

Submitted 08/11; published 12/11

Theoretical and Practical Foundations of Large-Scale
Agent-Based Micro-Storage in the Smart Grid
Perukrishnen Vytelingum
Thomas D. Voice
Sarvapali D. Ramchurn
Alex Rogers
Nicholas R. Jennings

pv@ecs.soton.ac.uk
tdv@ecs.soton.ac.uk
sdr@ecs.soton.ac.uk
acr@ecs.soton.ac.uk
nrj@ecs.soton.ac.uk

Agents, Interaction and Complexity Group
School of Electronics and Computer Science
University of Southampton
Southampton, SO17 1BJ, UK.

Abstract
In this paper, we present a novel decentralised management technique that allows electricity micro-storage devices, deployed within individual homes as part of a smart electricity
grid, to converge to profitable and efficient behaviours. Specifically, we propose the use of
software agents, residing on the users smart meters, to automate and optimise the charging
cycle of micro-storage devices in the home to minimise its costs, and we present a study
of both the theoretical underpinnings and the implications of a practical solution, of using software agents for such micro-storage management. First, by formalising the strategic
choice each agent makes in deciding when to charge its battery, we develop a game-theoretic
framework within which we can analyse the competitive equilibria of an electricity grid populated by such agents and hence predict the best consumption profile for that population
given their battery properties and individual load profiles. Our framework also allows us to
compute theoretical bounds on the amount of storage that will be adopted by the population. Second, to analyse the practical implications of micro-storage deployments in the grid,
we present a novel algorithm that each agent can use to optimise its battery storage profile
in order to minimise its owners costs. This algorithm uses a learning strategy that allows
it to adapt as the price of electricity changes in real-time, and we show that the adoption
of these strategies results in the system converging to the theoretical equilibria. Finally,
we empirically evaluate the adoption of our micro-storage management technique within a
complex setting, based on the UK electricity market, where agents may have widely varying
load profiles, battery types, and learning rates. In this case, our approach yields savings of
up to 14% in energy cost for an average consumer using a storage device with a capacity of
less than 4.5 kWh and up to a 7% reduction in carbon emissions resulting from electricity
generation (with only domestic consumers adopting micro-storage and, commercial and
industrial consumers not changing their demand). Moreover, corroborating our theoretical
bound, an equilibrium is shown to exist where no more than 48% of households would wish
to own storage devices and where social welfare would also be improved (yielding overall
annual savings of nearly 1.5B).

1. Introduction
The vision of an intelligent electricity delivery network, commonly called the smart grid,
has been advocated as one of the main solutions to ensuring sustainable energy provision
c
!2011
AI Access Foundation. All rights reserved.

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

(US Department Of Energy, 2003; Galvin & Yeager, 2008; DECC, 2009). Such grids aim to
reduce inefficiencies in energy usage, minimise carbon emissions and reduce costs to generate
electricity. A key element of this vision is that consumers should be able to respond to the
current condition of the grid, by shifting or reducing their use of electricity, through a twoway communication channel between them and the other actors in the system (i.e., suppliers,
other consumers, and grid operators). By so doing, it is expected that peaks in demand
can be reduced, which, in turn, would reduce the need for expensive and carbon-intensive
peaking plants (i.e., spinning reserve) that rely on fossil fuels.
Two of the key technical enablers of the smart grid are increased degrees of automation
and the ability to store energy. In the former case, consumers need to be able to delegate
the complex and time-consuming reasoning about shifting or reducing demand, subject
to their individual preference, to software agents that will act on their behalf (Ramchurn,
Vytelingum, Rogers, & Jennings, 2011c). To this end, smart meters have been developed to
provide consumers and their agents with real-time information about a homes consumption
and the state of the grid, and to provide suppliers and grid operators with consumption
data from homes. Moreover, smart meter roll-outs have now been mandated in a number
of countries, including France (by 2016), Spain (by 2018) and the UK (by 2020). Through
such information feeds, consumers should be able to improve their management of energy
(e.g., switching off devices they do not need or rescheduling power-hungry devices to other
times). In the latter case, the agents can use the information to decide to store electricity
at times when overall demand from the grid is low (and generally cheaper) and re-use this
electricity when the grid is operating close to its limits (i.e., when generators are operating
near capacity or transmission lines are close to overload and electricity is generally more
expensive). Furthermore, storage devices can also be used to compensate for the variability
of many forms of renewable electricity generation (e.g., wind, wave, solar) that is likely to
be an increasingly prominent component of the future grid (Bathurst & Strbac, 2003).
To date, most research on storage technologies has focused on designing new low-cost
large-scale storage devices (with capacities of the order of 10-100MWh) that are able to
efficiently store electricity for long periods of time and allow a sufficient number of charging/discharging cycles without significant degradation in performance (Bathurst & Strbac,
2003). However, with the development of a large variety of micro-storage devices (i.e., with
capacities of the order of kWh) that can be installed in homes1 and vehicles2 , a future
where large numbers of individual consumers can store small amounts of electricity in order
to accommodate peaks in demand and variability in supply will soon be possible. There
are, however, a number of potential challenges in this setting. First, if all consumers decide
to charge their batteries at the same time, because the price is low, a significant peak in
demand could well ensue. This would, in turn, result in higher electricity generation costs
and greater carbon emissions and could overload a system that is already operating close
to its maximum capacity (resulting in a brown-out or, in the worst case, a black-out). Indeed, such unintended population-wide synchronisation has already been seen in real-world
1. See batteries recently developed by GS Yuasa (http://lithiumenergy.jp/en/products/) or Power Yiile
(http://eliiypower.co.jp/english/lithium-ion/).
2. Vehicle to grid (V2G) technologies enable energy to be stored in the batteries of electric vehicles (EVs)
(e.g., Mini-E or Nissan Leaf) or plug-in hybrid electric vehicles (PHEVs) (e.g., Toyota Plug-in Prius or
Chevrolet Volt) (Sovacool & Hirsh, 2009; Lund & Kempton, 2008).

766

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

demand-response trials where consumers manually react to critical pricing periods (Hammerstrom et al., 2008). Second, if consumers were to simply charge their batteries to ensure
they have sufficient energy to cover their whole demand across a day, they may end up
paying more than they need to. This is because it may be cheaper to use the grid-supplied
electricity at certain times, rather than the energy already stored in the battery. Finally, if
most homes in the system start using storage devices and manage to reduce peak demand
(by optimising their battery charging and usage costs), electricity prices may become lower
than the price of stored electricity (including the cost of the battery), thus voiding the need
for micro-storage and breaking down the market for such devices.
Addressing the aforementioned issues through the formulation of conventional closedform solutions (see Section 2 for more details) is challenged by the fact that the system
is composed of large numbers of distinct stakeholders (typically millions of consumers and
tens of market makers and network managers) operating in a completely decentralised fashion where they individually act to satisfy their own particular objectives and constraints
(to supply or use energy) which may conflict (e.g., the network managers aim to minimise
peaks in demand while consumers aim to minimise their costs and use devices at the most
convenient time of the day). Against this background, the agent-based approach can be
used both as a framework to analyse the properties of such systems and also as an implementation technology (Exarchakos, Leach, & Exarchakos, 2009; Houwing, Negenborn,
Heijnen, Schutter, & Hellendoorn, 2007; van Dam, Houwing, & Bouwmans, 2008; Rogers &
Jennings, 2010). In particular, game theory can be used to determine the properties of the
system as the multiple self-interested parties interact and software agents can be installed
on the smart meters to optimise the usage and storage profile of the house using information
from a variety of sources (e.g., weather data to predict heating needs and costs or price plan
data from suppliers). Now, most of the existing approaches to applying intelligent agents
study how individual homes could optimise the way they store energy or how storage devices could coordinate with renewable energy generation facilities to maximise energy used
from such sources (see Section 2 for more details). However, in so doing, they ignore the
individual selfish preferences of each consumer and do not model well the real impact of
agents learning to adapt to the constraints that they themselves impose on the system. To
remedy this, it is crucial to devise an approach that focuses on the system dynamics where
all agents in the system are given the freedom to buy electricity whenever they see fit.
In this paper, we take just such an approach and provide both an analytical and a
practical solution to the decentralised control of micro-storage devices in the grid. Specifically, we develop a game-theoretic framework for modelling storage devices in large-scale
systems where each device is controlled by a self-interested agent that aims to maximise its
monetary profits when a real-time price for electricity is provided by the grid. Using this
framework, under certain reasonable assumptions, we are able to predict the equilibria of
the system given that each agent behaves rationally (i.e., always adopts a storage profile
that minimises its costs) and only reacts to a price signal. Given this, we go on to devise
intelligent agent-based storage strategies that can learn the best storage profile given market prices that are themselves a result of the aggregate storage and consumption profile of
all the agents in the system. Crucially, we show that agents using such strategies achieve
our predicted equilibria and, building upon this, we simulate large populations of agents in
order to predict the system behaviour under various conditions.
767

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

In somewhat more detail, this work advances the state of the art in the following ways:
1. We provide a novel analytical game-theoretic framework that captures the synchronous
behaviours of consumers with micro-storage devices within the smart grid. We use
this to study electricity storage strategies that agents might adopt in a smart grid
with real-time pricing. Given the typical electricity usage profiles of consumers, we
are able to compute the competitive equilibria which describe when each individual
agent is going to charge its device, use its stored electricity, or use electricity from the
grid. We then use this analytical solution to benchmark our decentralised solution.
2. We provide a theoretical bound on the storage capacity that will be needed by the
population, as well as a bound on the portion of the population that will adopt storage.
3. We provide new micro-storage strategies that enable agents to learn the best storage
profile to adopt, even taking into account the probable heterogeneity of the other
micro-storage devices adopted by consumers across the grid (e.g., these devices will
vary in capacity and in how fast they can be charged or discharged). Our practical
strategies are shown to converge to the same competitive equilibria as those predicted
by our analytical framework and come with system-wide benefits that include reduced
carbon emissions, as well as cost savings.
4. We show how agents, having learnt their best storage profile, can also learn to buy
the most profitable storage capacity. Given this, using evolutionary game theoretic
analysis, we are able to predict the portion of the population that would actually
acquire such storage capacity to maximise their savings. We show that this is not the
entire population. Rather, it is just under half of them, confirming the theoretical
bounds that our analytical framework predicts.
In short, this is the first attempt at modelling, predicting equilibria and building intelligent strategies for the problem of electricity storage on a large scale. Our approach also
justifies and provides the basis for the implementation of real-time electricity pricing for
domestic electricity distribution.
The rest of this paper is structured as follows. In Section 2, we discuss related work in
the area of electricity storage and electricity markets. Section 3 discusses the key features of
such markets and lays down the general assumptions upon which we build our framework.
Section 4 presents our game-theoretic framework and shows how the competitive equilibria
of the system can be computed and, based on such equilibria, determines how many users
are likely to adopt micro-storage. Building on this, Section 5 describes the dynamics of a
market where agents are given the ability to learn their best storage profile and Section 6
empirically studies this system through simulations. Then, Section 7 expands on the costbenefit analysis of storage in a system given a heterogenous population of consumers with
different usage profiles. Finally, Section 8 concludes.

2. Background
In this section, we first review current storage technologies to illustrate how micro-storage
has evolved to date and is likely to be a potent energy management technology in the
768

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

future. We follow with a discussion of existing approaches in the power systems literature
to using storage, including agent-based approaches that have been proposed to manage the
grid in general and micro-storage in particular. Furthermore, we motivate the need to have
real-time pricing mechanisms in the grid in order to enable more responsive demand by
considering the response of consumers to fixed prices and time-of-use tariffs.
2.1 Energy Storage Technologies
The large scale storage of electricity within an electricity grid is not a new concept. Many
countries operate pumped-storage power plants which store electricity by pumping water
to a high reservoir when prices are low, and then release this water and use it to generate
electricity when prices are high or supply is short. For example, Dinorwig Power Station in
the UK can store approximately 10GWh of electricity, and can supply this stored energy
over six hours, while providing up to 1.8GW of power (Williams, 1984).
However, the increased need for storage capacity as electricity grids increasingly seek to
incorporate intermittent renewable sources, such as wind generation or solar power, coupled
with the high capital costs of such pumped storage plants and the limited number of suitable sites for their construction, mean that much recent research has focused on alternative
smaller storage solutions which typically store 10-100MWh of electricity. Examples of technologies that have already been demonstrated commercially include the use of underground
caverns to store electricity by compressing and releasing air (Swider, 2007) and its storage
in large chemical flow batteries (Shibata & Sato, 1999).
Most recently, attention has turned to micro-storage3 of up to 20kWh of electricity
which might be installed within homes as part of a smart grid roll-out. This interest has
been largely driven by the rapidly decreasing cost of efficient batteries, such as lithium-ion
cells or the nickel-zinc alternative, as they are developed for use within electric vehicles
(den Bossche, Vergels, Mierlo, Matheys, & Autenboer, 2006). Indeed, the energy storage
capacity required for a viable electric vehicle is close to the daily consumption of a home
(e.g., the Chevrolet Volt battery has a capacity of 16 kWh and the Nissan Leaf can store
up to 24 kWh). Thus, it is now possible to envisage that micro-storage devices will be
widely used in the short to medium term, either as dedicated home storage batteries, or as
an additional capability of electric vehicles.
2.2 Agent-Based Systems for the Grid
The use of software agents, residing on smart meters, was first envisaged by Schweppe,
Tabors, Kirtley, Outhred, Pickel, and Cox (1980) who proposed mechanisms for agents to
manage the use of electricity within a single home and to buy electricity in real-time on
behalf of their owner. Since then, a number of agent-based approaches have been developed
where multiple autonomous agents represent the interests of different actors on the grid.
For example, Ygge, Akkermans, Andersson, Krejic, and Boertjes (1999) initiated work on
abstracting electricity markets as multi-commodity markets and showed how agents trad3. Note that the average cost of a micro-storage battery today varies from around 300 to up to 1000
per kWh with an average start-up cost of 200. Note that these prices are only indicative and given
the innovative nature of the area and the investment in electric vehicle battery technology, the cost of
micro-storage is gradually decreasing (see http://www.pikeresearch.com).

769

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

ing energy for different times of the day, could generate efficient allocations . Moreover,
Jennings, Corera, Laresgoiti, Mamdani, Perriolat, Skarek, and Varga (1996) developed coordination mechanisms for different actors on the grid to manage the allocation of transmission
capacity and, more recently, Sun and Tesfatsion (2007) and Li and Tesfatsion (2009) developed agent-based electricity market simulations which incorporate transmission constraints
and different types of buyers and sellers in the grid. Kok and Venekamp (2010) take a similar approach and implement an agent-based architecture to run the electricity market where
the individual actors represent either generators or devices in the home. Note, however,
that none of these approaches consider the daily consumption profile of consumers, nor how
an agent might optimise its consumption and storage of electricity to maximise its owners
benefit. We believe this is crucial because agents have to be profitable to their users, each
with her own specific needs and lifestyle, in order to be commercially viable. Specifically,
agents should be able to provide personalised home energy management if they are to be
deployed in the real world.
In this context, we note the early work of Daryanian, Bohn, and Tabors (1989) that
illustrated how individual agents could optimise, through iterative algorithms, the load
profile of a house using an electricity storage device. Their approach was, however, limited
to considering very basic battery properties and did not consider wider issues for the grid
such as the level of adoption of batteries in the population and the optimum storage capacity
required for maximum savings. More recently, Houwing et al. (2007) provided algorithms for
agents to optimise storage using small domestic combined heat and power (CHP) plants, but
they ignored how populations of such agents would impact on the grid. On the other hand,
Exarchakos et al. (2009) and van Dam et al. (2008) have studied the application of storage
devices on a wider scale. They showed that using demand-side management techniques,
where the storage profile of a number of homes is controlled centrally, can increase savings
made in the system. Unfortunately, such a centralised approach automatically introduces a
single point of failure in the system and does not take into account the individual preferences
of each user to buy, use, or turn off her storage device. On the other hand, Ramchurn,
Vytelingum, Rogers, and Jennings (2011b) consider a supplier retailing to a large number
of agents that continuously optimise their storage, but they assume that the effect of such
large-scale optimisation does not influence the wholesale price of the market which is usually
not the case when considering a large enough proportion of the population. Thus, in
this paper, we take a market-based approach similar to Ygge et al. (1999) and Ygge and
Akkermans (1999), that is informationally decentralised (i.e., no centre with complete and
perfect information is required), and therefore more robust, where each agent buys electricity
in real-time markets and individually controls the storage profile of its associated home,
based on real-time prices that reflect the demand (and supply) of the market.
2.3 Electricity Pricing Mechanisms
Most of the above approaches assume the existence of a control signal that dictates when
the home must store and when it is best to use the stored electricity, thereby reducing the
load on the grid at peak times (see Figure 1 for an example of the average UK demand
that result in peaks) and making a saving when such reductions come with monetary rewards. To this end, Schweppe et al. (1980) proposed the use of real-time pricing (RTP)
770

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

or spot pricing of electricity as a better way to manage demand as conditions on the grid
are accurately reflected by the price (as set by a single utility or an electricity market).
Thus, contrary to the pricing signals that are traditionally used in the grid, that are either
based on a fixed price or a time-of-use (TOU) price, whereby a premium is charged during
times of anticipated peak demand, a real-time price reflects the current and continuously
changing balance of supply capacity and demand and, even in some cases, the congestion on
the network (e.g., locational marginal pricing approaches to pricing electricity at different
points in the grid  see Harris, 2005). Unfortunately, Schweppe et al.s solution was never
fully implemented on a large scale at the time it was proposed. This was for a number of
reasons. First, the properties of their solution were mainly proven analytically, under the
general assumption that most agents will behave in a similar fashion and did not attempt
to model the strategic choices that agents may make in charging their batteries (e.g., always
charging at times they predict will be cheaper and always using their battery when they
predict prices will be higher or charging their battery early for a whole days consumption
to avoid price peaks later). Instead, in this paper, we develop a game-theoretic framework
that fully captures the agents strategic behaviour within the context of the smart grid and
we complement this approach with simulations in order to evaluate the performance of the
system with a highly heterogeneous population of agents. Second, Schweppe et al.s design
also came up against problems associated with high communication costs and a lack of
autonomous storage management technology. However, recent advances in computational
power that make the deployment of autonomous agents entirely feasible, and new information and communication technologies such as wireless broadband internet and home energy
management systems (e.g., AlertMe4 or Intels Home energy dashboard5 ) mean that realtime pricing for the domestic sector looks closer to being realised. Moreover, the financial
commitment of countries such as the UK (8.6M invested in smart metering infrastructure
 see DECC, 2009) and the US (with 57.9 million smart meters planned for installation6 ) to
the implementation of smart metering infrastructure, provides unprecedented support for
the implementation of RTP.
Real-world trials, such as those of the GridWise alliance in the US (Hammerstrom et al.,
2008) or the Energy Demand Research Project in the UK (Smith, 2010), and theoretical
work such as those by Ramchurn, Vytelingum, Rogers, and Jennings (2011a) show that
the more accurate RTP signals (i.e., representing real costs as opposed to the TOU pricing
scheme) allow consumers to reduce their peak demand (and the duration of such peaks)
by reacting more frequently to a more accurate 30-min-tariff pricing model (rather than
over the peak and off-peak prices of TOU). By reducing such peaks, consumers under
RTP can make significant savings compared to those under the TOU pricing. However, it is
generally the case that more (short-duration) peaks exist in the RTP than the Fixed-Pricing
model (see Ramchurn et al., 2011, where two long-duration peaks exist in the morning and
evening), requiring the usage of expensive peaking plants only for short periods. This is
because if the agents predict (based on previous days) a low price for the next day at a
4. http://www.alertme.com/.
5. http://edc.intel.com/Applications/Energy-Solutions/Home-Energy-Management/.
6. http://www.pikeresearch.com/newsroom/
57-9-million-smart-meters-currently-planned-for-installation-in-the-united-states

771

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

0.6

Power (kW)

0.5

Morning Peak
Evening Peak

0.4
0.3
0.2
0.1
0:00 3:00

6:00

9:00

12:00 15:00 18:00 21:00 24:00
Time of Day

Figure 1: Average load profile (weekday in summer) in UK over half-hourly periods based
on a fixed pricing model.

certain time, they will all turn on their devices, which then results in a peak in demand at
that time. When such a mechanism is rolled out on a large scale, such reactive behaviours
can cause unpredictable and significant peaks (compared to two predictable ones for TOU)
in demand and prices, which, in turn, result in higher costs for individuals and greater stress
on grid resources (transmission lines reaching their thermal limit and generators reaching
their capacity).
Thus, if not properly managed, storage systems can be unprofitable for consumers and
adversely impact the whole system (Holland, 2009; Williams & Wright, 1991; Bathurst &
Strbac, 2003). Hence, in the setting we consider, it is important to know whether microstorage can be individually beneficial and what strategies maximise the consumers savings.
It is also important to understand the system-wide effects of such strategies; in particular,
quantifying the limits on the usefulness of small scale storage from a grid efficiency point of
view and determining how different types of storage (with different costs and efficiencies)
will be integrated into the system. These are the key open questions that are addressed by
this paper (see Sections 4 and 5 respectively).

3. Model Description
As already noted, in this paper we seek to analyse the behaviour and impact of microstorage devices from a large scale multi-agent systems point of view. That is, we consider
the situation where large numbers of autonomous agents each control energy storage for a
home, and interact with electricity suppliers within a market, with the aim of minimising
772

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

costs for their individual owners. Such a setup allows us to make a number of modelling
assumptions, which we will detail in this section. Our analysis considers fixed time intervals
consisting of single days, each separated into T = 48 settlement periods of half an hour. Each
day, agents consume electricity which is bought from suppliers through an electricity market.
This market operates for each time interval in the day, so that variations in demand over
time can be met. The agents autonomously control the charging and discharging behaviour
of their storage device in order to maximise their users profit. We describe this process
more thoroughly in Section 3.1. In our analysis, the market itself is modelled abstractly,
following a macro-model which we give details of in Section 3.2. In order to measure the
impact of the behaviour of energy storage agents in a wider context, we employ a number
of metrics for grid efficiency that are described and explained in Section 3.3. For a table of
notation definitions used throughout this paper, see Table 1.
3.1 Agents
We consider a set of N consumers, A, which we define as self-interested agents that always
aim to minimise their individual costs. Each agent a  A has a load profile7 !ai i  I =
{1, . . . , T }, such that !ai is the amount of electricity required by agent a for!time interval
a
i during each day. The aggregate load profile of the system is given by
aA !i = !i .
We consider this load profile to be fixed over different days (although in practice there are
seasonal variations in demand, there is a high degree of consistency from day to day). Each
agent a  A may also have some storage available to it, with capacity a , efficiency  a
and running costs a . Here, the cost a represents the ongoing storage costs (for example,
some battery devices expend energy through heating while they are in use or lose efficiency
through the depletion of chemicals used in them). We do not incorporate any fixed capital
investment by a at this stage, as these costs are fixed, and only the running costs can
have any effect on which storage profile is most profitable. However, such fixed capital
investments are important when users decide whether or not to purchase a battery, and we
include them in the cost-benefit analysis in Section 7. The storage efficiency  a and running
cost a are modelled to be such that if c amount of energy is stored, then only  a c may be
discharged and the storage cost is a c.
In order to minimise costs, agent a can attempt to strategise over its storage profile,
a
bi i  I, where for all i  I, bai = cai  dai , where cai = (bai )+ is the charging profile and
dai = (bai ) , is the discharging profile. Here, and throughout the paper, we use the notation
()+ to denote positive part, that is, y = (x)+ means y = x if x > 0, y = 0 otherwise.
Likewise we use (x) to denote (x)+ . These definitions implicitly assume that a user
will not both charge and discharge her storage device over a single time interval. However,
in our model, prices are fixed over each time interval, so we discount such behaviour as
it can never be profitable. For each agent a, a feasible storage profile bai i  I must
satisfy Da  bai  C a , where Da is the maximum discharging rate of the storage device,
7. We assume that the consumers load is inelastic and, thus, insensitive to price changes. In reality, we
would expect that the consumers load shows slight elasticity, i.e., the consumer will reduce her demand
if price increases and likewise increase her demand if price decreases. However, because demand elasticity
of domestic consumers is generally small, we believe that the results presented in this paper still provide
a good guide to the behaviour of real markets.

773

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

Notation
N
A
T
I
bai
cai
dai
!ai
qia
a
a
a
ca0
Ca
Da
Oia
bi
ci
di
!i
qi



C
D
Oi
s
pi
p+
p
i (), i ()
()+
()
LU
HU
r
rU
uU (, , )

Definition
Number of agents in the system
Set of all agents in the system
Number of time intervals in a day
Set of time intervals
Net charge/discharge from storage device of agent a during time period i
Amount charged by agent a during time period i
Amount discharged by agent a during time period i
Electricity used by agent a during time period i
Electricity purchased by agent a during time period i
Efficiency of storage device for agent a
Capacity of storage device for agent a
Storage cost for agent a
Energy agent a has stored at the start of the day
Maximum (per interval) charge for agent a
Maximum (per interval) discharge for agent a
Maximum usable discharge for agent a during i
Net charge/discharge by all agents during i
Amount charged by all agents during i
Amount discharged by all agents during i
Total consumer load by all agents during i
Total electricity purchased by all agents for i
Total storage capacity of the population
Homogeneous storage device efficiency
Homogeneous storage costs
Maximum total per interval charge
Maximum total per interval discharge
Maximum total useful discharge during i
Supply function for wholesale market
Price of electricity during i
Charging price point
Discharging price point
Equilibrium price functions for time interval i (see Definition 1)
Positive part
Negative part
Low-end energy users
High-end energy users
Agents strategy to adopt or not storage
Probability of an agent of type U  {LU, HU } adopting strategy r
Payoff of an agent of type U  {LU, HU }
Table 1: Notation definitions.

774

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

UK Wholesale Balancing Market Buy Price (GBP/MWh)
250
Price against Demand
Movingaverage

Unit Price (GBP/MWh)

200

150

100

50

0
2

2.5

3
3.5
Total Demand (MW)

4

4.5
4

x 10

Figure 2: Half-hourly UK wholesale real-time buy prices (in the balancing market) plotted
against total demand for August and September 2009 illustrating the correlation
between price and demand. The dotted line represents the average of the points.

and C a , the maximum charging rate. Since we are attempting to model the effect of the
widespread adoption of micro-storage devices, we can assume that !ai , C a , and Da are small
in comparison to !i (since we are considering an electricity grid
of a large number
!consisting
a , and the net storage
of consumers). We denote the total
storage
capacity
as

=

aA
!
profile as bi i  I where bi = aA bai . For each i  I and a  A we use qia = bai + !ai , to
denote the net quantity of electricity purchased by a during i, and we use qi = bi + !i to
denote the net quantity of electricity purchased by all users during
aggregate
! interval i. The!
maximum charging and discharging rates are defined as C = aA C a and D = aA Da .
We also define the maximum usable storage output Oi to be the maximum amount of stored
electricity that the whole population of agents is able to discharge and make use of during
time interval i. If agents !
can only use stored electricity to satisfy their own energy needs,
then we have that Oi = aA min(Da , !ai ) for all i  I, as each agent has no reason to
discharge more energy than its current load. This is the situation that we mainly focus on
in this paper. However, the analysis we present can also be applied to the case where agents
can sell stored energy back to the grid, using a feed-in tariff. If we assume that agents can
sell stored energy at the current grid price, then our model remains the same, except for
the values of Oi which should be set equal to D, for all i  I. To satisfy its load profile
and energy charging needs, each agent must purchase electricity from the available market,
which we describe in the next subsection.
775

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

3.2 A Macro-Model of the Electricity Market
In order for widespread home energy storage to effectively help reduce peak loads and improve the overall efficiency of the electricity market, there must be some incentive for users
to charge their storage devices when demand is low (or when renewable energy generators
are running cheaply) and discharge them when demand is high (or when expensive peaking
plants are used). As discussed in Section 2.3, both TOU and RTP pricing plans aim to incentivise users to optimise their demand by shifting to low demand periods. In particular,
by sending up to date pricing information, RTP allows electricity suppliers to provide consumers with prices that accurately reflect current levels of demand and costs of generating
that electricity. Hence, in our analysis, we focus on the use of RTP, and note that competition between suppliers that provide RTP pricing schemes should allow future consumers to
buy electricity at or close to the current market price (Schweppe, 1988). Although the use of
RTP makes the settlement process more complex than in the case of Fixed or TOU pricing
(as these are independent of the reaction of the agents), and requires more interactive and
intelligent energy management in the home, recent advances in smart metering technology
have meant that RTP has become a plausible and attractive possibility for future smart
electricity grids (as discussed in Section 2.2). Accordingly, we assume agents buy electricity
under an RTP scheme where the price of electricity accurately represents the market price
for power at that particular settlement period.
To this end, we consider a macro-model of the electricity market that abstracts the
actual market mechanism and trading that determines the real-time price of electricity.
Such an approach to modelling the supply side and the transmission system is common in
the power system economics literature as it does not significantly affect the general trends
observed when analysing the demand side (Kirschen & Strbac, 2004; Schweppe, 1988). Our
model is based on the observation that as the aggregate demand for electricity increases,
the unit price of that electricity will also increase, since more costly means of generation
must be used to satisfy this additional demand. For example, Figure 2 shows the half-hour
UK real-time wholesale buy prices (in the balancing market) during August and September
2009 plotted against aggregate demand.8 While there are numerous anomalies due to power
station outages that cause short term price increases, it is clear that over a large range of
demand, there is an increasing trend in the relationship between the unit cost of electricity
and the total aggregate demand, with prices rising rapidly for very high demand. An
analysis of the effect of these price fluctuations on a system with micro-storage is left for
future work.
In several countries around the world, including the UK and the US, electricity is traded
in wholesale for residential, commercial and industrial purposes in forward markets (up to
several months in advance) and in the balancing market (in real-time). Within this setup,
the role of suppliers is to buy electricity from the generators on the wholesale market (e.g.,
the National Grid runs the wholesale market in the UK and PJM runs the one on the east
coast of the US) and sell retail to its consumers. In the UK, the retail market is dominated
by six large suppliers that retail to 26 million residential consumers (representing around
30% of all of UK energy demand). Now, in this work, we focus exclusively on these domestic
consumers since they constitute of a significant portion of the overall energy consumption
8. The data is available at www.bmreports.com.

776

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

in the UK. Yet, our approach could readily be extended to the other commercial and
industrial consumers. This is in line with our main aim to show that the deployment of
micro-storage within a large heterogeneous population can have significant benefits. Thus,
demonstrating that we can flatten the demand of the heterogeneous domestic population
implies that a similar behaviour should be achievable across the remaining 70% of the
population (with even larger benefits). To this end, we model the domestic consumer
market with a pricing function that reflects the expected cost that a supplier (retailing
electricity to the consumers) would pay from the wholesale market. There are several
advantages for a supplier to having an aggregate demand with fewer peaks from its domestic
consumers. Specifically, the suppliers exposure to peak (and typically volatile) wholesale
prices is reduced and it can benefit more from long-term baseload (i.e., long-term flat load)
forward contracts (Voice, Vytelingum, Ramchurn, Rogers, & Jennings, 2011). Furthermore,
the peak load on transmission and distribution networks may be curtailed, reducing the need
for infrastructure reinforcements, and most significantly, expensive peaking plant capacity
(which may only be used for a few hours each day) can also be downsized.
Against this background, in our model, the real-time price of electricity per kWh
(/kWh) is specified by a price function s(q) = 0.04 + 0.20(q/0.6N ) where qkWh is the
total energy required by the N = |A| consumers during a half-hour time slot and  is set
to 4 to model the typical trend (i.e., monotonically and rapidly increasing) of the wholesale
demand prices at which the suppliers purchase their electricity. Note that as long as the
modelled demand curve is monotonically increasing to reflect the increasing unit cost of
electricity with demand (which incentivises demand response), the actual demand curve is
not critical to our work.
Given this function, the retail price depends on the total amount of electricity consumed
by the agents, !i , and the net discharging and charging of the agents storage devices, bi , such
that the total amount of electricity bought from suppliers at time interval i is qi = !i + bi ,
and the market price is given by pi = s(qi ). Hence, each agent a pays pi  (!ai + bai ) and the
total cost for all agents is pi qi .
3.3 Grid Performance Metrics
A key aim of this paper is to study the effect of storage on the overall system and whether
the global performance of the system improves as agents adopt it. In more detail, we
measure performance by considering the following standard metrics of an electricity market
(Harris, 2005):
 The load factor (LF) is the average power divided by peak power, over a period of
time:
!
iI qi
LF =
|I|  maxiI qi

where I  I is a selected period of time (e.g., a day). Ideally, the LF should be at
its maximum of 1 which means the aggregate load profile is completely flat. When
LF < 1 it suggests variations in demand through each daily period. Hence, if storage
strategies are effective, the LF should converge to 1 when agents are able to utilise
storage to shift consumption at peak time to periods of low demand. The LF on its
own only measures the aggregate load profile and does not indicate how individual
777

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

agents contribute to peaks in the system (e.g., if all agents are consuming at the same
time or if only a few agents cause large peaks). To better capture such behaviours we
rely on the following measure, the diversity factor.
 The diversity factor (DF) is the ratio of the sum of the individual maximum demands
of various consumers of the system to the maximum demand of the complete system:
!
maxiI qia
DF = aA
maxiI qi
The DF is always greater than or equal to 1 and the higher its value, the less correlated
the peak demands of consumers are. Less correlated peaks result in a flat aggregate
profile (as peaks are interposed), and a high DF implies that users are well diversified
and so will not cause large aggregate peaks (by all consuming at the same time) in
the system. On the other hand, a low DF indicates that agents have more correlated
load profiles which could result in peaks in demand. Now, while the LF describes the
aggregate demand, the DF describes how the individual demands compare with each
other and, thus, the DF can give insights into how the LF is achieved (i.e., how the
individual profiles are contributing to the aggregate profiles). For example, for the
same high LF, a high DF suggests well diversified profiles with individual consumer
peaks spread across the day while a low DF suggests flatter individual profiles. The
DF is generally useful when designing decentralised control mechanisms because it
identifies the individual (rather than the aggregate) behaviours of the agents and,
thus, potentially provides insights on how the individual behaviour of an agent should
be modified.
 The grid carbon intensity is the amount of carbon dioxide emitted in order to deliver
one unit of electricity to the consumer. It is expressed as grams of CO2 per kWh and,
ideally should be as low as possible (in order to minimise greenhouse gas emission).
The carbon intensity of the UK grid for half-hourly periods for August and September
2009 is shown in Figure 3. As with the market price of the electricity, there is again
a clear correlation between the carbon intensity of the electricity from the grid, and
the total demand on the grid. In the UK, this is due to the use of coal-fired power
stations to satisfy increasing demand, and thus, as well as reducing the total cost of
electricity to the consumer, we can expect the use of micro-storage to reduce total
carbon emissions by reducing the overall carbon intensity of the grid.
We note here that our pricing model depends on the aggregate demand in the domestic
sector. Thus, when we report load factor and diversity factor results, we only do so for
this domestic sector. However, in reality, the domestic sector is only one contributor to
the total electricity demand; in the UK, it represents, as previously mentioned, approximately 30% of the total demand, with the remaining 70% being commercial and industrial
consumption which remains unchanged within our model. To demonstrate the potential of
domestic micro-storage to reduce UK carbon emissions from electricity generation, when
we present our results regarding carbon emissions reductions, we describe the reduction of
carbon emissions (as a result of micro-storage in the domestic sector) as a percentage of the
778

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

UK Grid Carbon Intensity (gCO /kWh)
2

550

450

2

Carbon Intensity (gCO /kWh)

500

Carbon Content
BestFit (linear trend)

400
350
300
250
200
2

2.5

3
3.5
Total Demand (MW)

4

4.5
4

x 10

Figure 3: Half-hour UK grid carbon intensity plotted against total demand for August and
September 2009 illustrating the correlation between carbon intensity and demand.

current carbon emission from all sources (i.e., domestic, commercial and industrial), assuming micro-storage in the domestic sector and no change in the remaining 70% of commercial
and industrial daily profiles.

4. A Game-Theoretic Analysis of Micro-Storage
Having set the stage for the application of micro-storage in the smart grid, we now explore
the theoretical underpinnings of such a system. To this end, we apply a game-theoretic
framework to the models given above and characterise the resulting equilibria. These equilibria are important because they represent stable states of the system under which each
agent is unable to increase its profitability by unilaterally changing its strategy. In particular, we can expect selfish agents to maintain battery usage strategies that are maximally
profitable for them, and thus, these are the states that we predict will arise naturally if the
market prices stabilise. We address the question of how to endow individual agents with
the intelligence to reach a stable equilibrium while maximising their owners savings later
in the paper (see Section 5). For now, to ensure tractability we assume that agents have
homogeneous efficiency and running costs, that is  a =  and a =  for all a  A for some
 and . While such assumptions allow us to obtain closed-formed solutions to computing
the equilibria in the system, they abstract from real-world systems where agents are typically heterogeneous. However, as one of the key achievements of this work, in Section 6 we
show that such assumptions do not result in a significant loss of generality and that, in fact,
779

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

our theoretical results closely approximate the empirical results we obtain in more complex
environments with heterogeneous agents.
Formally, the game we consider has players which coincide with our agents, a  A, and
the game describes the outcome of a single 24 hour interval. The pay-off an agent receives
is !
equal to minus the total costs that it experiences when purchasing electricity that day,
 iI (pi (!ai + bai ) + cai ). The strategy space available to each agent is the set of feasible
storage profiles, bai i  I. We approximate the space of feasible aggregate storage profiles
to be the set of aggregate profiles which lie between the charging and discharging limits such
that the total amount of energy charged is less than or equal to the total storage capacity
available to the agents and exactly equal to the total amount of energy discharged divided
by the efficiency. More formally
the set of
! we consider
!
!aggregate storage profiles bi i  I
such that Oi  bi  C,
iI di =
iI ci and
iI ci  , where  is the storage
capacity (see Table 1 for notation).
Requiring that the total amount charged is less than the total storage capacity is a
stricter constraint than simply requiring that the capacity is never exceeded at any time.
However, it is a reasonable model of storage capacity limitations for a day-long time period
(given the daily-cyclic nature of demand), where demand typically goes through a single
cycle of low to high to low, implying that storage devices would go through a corresponding cycle of charging to discharging to charging. Indeed, in practice, we find that at the
equilibria of our simulations, there is indeed a single charging and discharging cycle during
which prices cycle from low to high. Furthermore, the equilibria reached in our experiments
closely agree with the equilibria predicted in this section (see Subsection 6.3).
We are also making a further approximation in considering all aggregate profiles which
satisfy the aggregate capacity, charging and discharging constraints. Even if the aggregate
constraints are satisfied, this does not necessarily mean that there exist feasible strategies
for individual agents that give this aggregate profile. To give an example, this would be
a poor model if all agents had the same values for C a , Da and !ai but there was a single
agent who was in possession of the majority of the storage capacity. In considering this
set of aggregate profiles we are therefore assuming that storage capacity is distributed
evenly amongst agents, roughly in proportion to their loads and charging and discharging
capacities. This is not unreasonable given the context of the situation we are modelling.
We now proceed to characterise the competitive equilibria for this game.
4.1 Competitive Equilibria as Global Optimisers
The set of competitive equilibria for the system corresponds to the set of Nash equilibria
under the assumption that each individual has negligible market power. That is, we assume
that each agents electricity consumption has a negligible effect on the price of electricity,
and we seek situations where no agent has an incentive to change its storage profile to
reduce its cost. We choose to analyse these equilibria as they capture the steady state of a
real system consisting of domestic users each owning micro-storage (optimised by an agent)
and where each users consumption has minimal effect on electricity prices (in the UK, each
user represents just one home out of 26 million).
Now, suppose agents have chosen some strategy profiles, and let us consider the effect
of a feasible change in strategy for one agent. That is, some agent a  A considers a change
780

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

for each i  I, changing cai to cai + cai and dai to dai + dai , giving a net change from bai to
bai + bai where bai = cai  dai . The change in payoff for agent a would be:
"#
$
#
$
s(qi )  s(qi + bai ) (!ai + bai ) + s(qi + bai )bai + cai .
(1)
iI

As noted in the previous section, since we are examining widespread micro-storage, we
can assume that for all i  I and a  A, !ai , C a , and Da are small in comparison to !i and
qi . This is equivalent to assuming each agent has negligible market power. Thus, s(qi +bai )
will be very close to s(qi ) and the first term in the above will be small. So, the change in
payoff for agent a would be approximately:
"#
$
s(qi )bai + cai .
(2)
iI

This is equal to the dot product of the gradient times the vector of changes (c, d), for
the following function, f (), 9 which we define as,
%
# a a
$ "# qi
$
s(x)dx + ci .
f {ci , di }aA,iI =
(3)
iI

0

Thus, the condition that each agent has no incentive to change their strategy is approximately equivalent to saying that for all a  A, the directional derivative of f () is positive in
any direction of change that leads to a feasible storage profile. This is equivalent to saying
that a vector of storage profiles is a local minimum for f () over the set of all feasible storage profiles. Since s() is increasing, f () must be convex, and since the feasible domain is
closed and convex, all local minimums are also global minimums for that domain. Thus, the
deterministic competitive equilibria for this game correspond to vectors of strategy profiles
which minimise f () over the set of feasible strategy profiles.
These approximations are typically well suited (i.e., do not result in a significant loss
of accuracy) to the large systems we consider (with millions of agents) and they have the
effect of greatly reducing our search for the competitive equilibrium of a complex multiplayer game to a relatively straightforward constrained optimisation problem  that of
minimising f (). We now proceed to find solutions to this optimisation problem.
4.2 Characterisation of the Competitive Equilibria
We can now characterise the aggregate storage profiles which form optimal solutions to the
constrained optimisation problem given above. This characterisation is given in our main
result, Theorem 1. However, this theorem may seem somewhat unintuitive at first. We can
think of the equilibrium as being characterised by two prices, p+ and p (defined below).
The equilibrium strategy is then to always charge as much as possible when energy is cheap,
right up to the point where the price reaches p+ , and to always discharge when energy prices
are high, right up until they fall to p . Thus, for any interval i, if at equilibrium, pi < p+ ,
then all agents must be at their maximum charge rates and if pi > p , then all agents must
9. There is no real world counterpart to f (), and so little intuition for its definition  it is not necessarily
equal to the total revenue, for example. It is simply a tool to help characterise global equilibrium.

781

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

be at their maximum output rates. If, at equilibrium, pi lies strictly between p+ and p ,
then the energy is too expensive to be worth charging, but too cheap to make discharging
profitable, and so no storage activity occurs for that time interval.
In order to formally state and prove this result, we must begin with a definition:
Definition 1. For a storage system as described, for each interval i, let us define the
functions i () and i () to be,
$
#
i (p) = min C, (s1 (p)  !i )+ ,
#
$
i (p) = min Oi , (s1 (p)  !i ) .
That is, i (p) is the amount of electricity that would have to be charged during interval
i, if there were no discharging, in order for the resulting price to be as close to p as possible.
Similarly, i (p) is the amount of electricity that would have to be discharged during interval
i, if there were no charging, in order for the resulting price to be as close to p as possible.
Oi is the maximum useful discharge during i and C, the maximum daily total charge.
We define the discharging price point, p , to be the maximum of the union of the
solutions to:
"
"
i (p ) = 
i (p  ),
iI

and the solutions to:

iI

"

i (p ) = ,

iI

if such exist. This maximum exists because s() is continuous and strictly increasing.
If p is well defined, then we also define the charging price point, p+ , to be p   if
"
i (p ) < ,
iI

or equal to the minimal solution to:
"

i (p+ ) = ,

iI

if such exist, otherwise.
!

+
+


well defined then
iI i (p ) =
!Note, if +p and
! p are 
! either+p = p   and
 iI i (p ), or iI i (p ) = ,
i (p !
) = . In the latter case, since p+ is
!and iI
strictly greater than all solutions to iI i (p ) =  iI i (p  ), and since the i ()
functions are increasing and the i () functions are decreasing, we must have that:
"
"
"

i (p+ ) =  =
i (p ) < 
i (p  ),
iI

iI

iI

and so p+ < p  . Under our intuitive understanding of the equilibrium given above,
this means that, p and p+ are chosen so that the total amount discharged is equal to the
total amount charged times . Furthermore, under this restriction, either p and p+ are
chosen to maximise the total amount charged or else the total amount charged is equal to
.
782

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

!
!
Lemma 1. There always exists a solution to iI i (p) =  iI
!i (p). Furthermore,
+ = p   unless
p
and
p
if p is the maximal solution then p =
iI i (p) > , in which
!


+
case, p is the maximal solution to iI i (p ) = , and p is the minimal solution to
!
+
iI i (p ) = .

Proof. We have that s() is a continuous, strictly increasing function, and !i > 0 is inside
its range for each time interval i. Thus, if p is sufficiently small, then for all i  I we
will have s1 (p) < !i and hence i (p) will be strictly positive and, since p   < p,
i (p  ) will be zero. Likewise if p is sufficiently large then for all i  I we will have
s1 (p) > !i and so i (p) will be strictly positive and i ((p + )/) will be zero. Since the
functions i () are
!
! decreasing for all i and i () are increasing for all i, we can conclude that

(p)


i
iI
iI i (p  ) is a continuous decreasing function in p which is negative
for sufficiently small p and!positive for sufficiently
large p. This implies the existence of
!
some solution p such that iI i (p) =  iI i (p  ). Let p be the maximal solution
to this.
!
Now if iI !
i (p)   then, since the i () functions are decreasing, there can be no
$ > p such that
$ = . Thus, p = p and, so, p+ = p   = p  . If
p
iI i (p ) !
!
reaches
iI i (p) >  then, since
iI i () is a decreasing function that eventually
!

zero, it must cross .!Thus, there will be a maximal value for p such that iI i (p ) =
.
since iI i (p) > , there must exist a minimal value for p+ such that
! Similarly,
+
iI i (p ) = , as required.
!
p so that it is the maximum solution to iI i (p ) =
!
!Although we have specified
 < , it is worth noting that for any two values p
 iI i (p  ),!if
iI i (p )!
!
!
and p$ that satisfy iI i (p) =  iI i (p  ) and iI i (p$ ) =  iI i (p$ 
), by the monotonicity on both sides of the equation,
we must
for all i  I,
!
! have $that !
$
$
i (p) = i (p ) and i (p) = i (p ). Likewise, if iI i (p) = iI i (p ) or iI i (p) =
!
$
$
$
iI i (p ) then for all i  I, i (p) = i (p ) or i (p) = i (p ) respectively. Indeed, with
these observations in mind, the specifications that maximal solutions be chosen makes no
difference to our main result, and is done simply so that p+ and p are well defined.
We can now state and prove the main result of this analysis.
Theorem 1. For a storage system as described, if  < 1 or  > 0 then the set of competitive
equilibria for the system is precisely the set of feasible agent strategies where, for all i  I,
ci = i (p+ ) and di = i (p ). In the case where storage devices are perfectly efficient and
costless, ( = 1 and  = 0), the set of competitive equilibria for the system is precisely the
set of feasible agent strategies where, for all i  I, bi = i (p+ )  i (p ).
Proof. We seek to find an aggregate storage profile, which minimises f () where:
#

$

f {ci , di }iI =

"#%
iI

"i +ci di
0

$
s(x)dx + ci .

Since the set of feasible aggregate storage profiles is closed and bounded, there must be at
least one minimum of f () over this domain. However, since this domain is convex, and
f () is a convex function, the only local minima will be a convex set of global minima.
To find these optimal allocations, we seek feasible aggregate storage profiles for which the
783

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

derivative of f () is non-negative in every direction that leads to another feasible allocation.
We can calculate that for all i  I, f /ci = pi +  and f /di = pi . Thus it remains to
characterise all feasible profiles such that:
"
(pi + )ci + pi di  0,
iI

for every vector of small changes that lead to another feasible aggregate storage profile, that
is, all c, d such that {ci + ci , di + di }iI is feasible.
Now suppose we have some storage profile, (c, d), which locally minimises f (). If there
are time intervals i, j with C > ci > 0 and C > cj > 0, then it would be feasible to increase
ci and decrease cj by an equal quantity, (or vice versa), hence we must have pi = pj . From
this we can deduce that if for some i, j, pi < pj , ci > 0 and cj > 0, then we must have
ci = C. So, if we let p+ be the maximum of pj for time intervals j with cj > 0, we get that
for all intervals i such that ci > 0, pi  p+ , and if pi < p+ , then ci must equal C. Similarly,
we can show that if p is the minimum of pj for time intervals j such that dj > 0 then for
all intervals i that di > 0, pi  p , and if pi > p then di = Oi .
Furthermore, there cannot be i, j such that pi +  > pj and ci > 0 and dj > 0, for
then it would be feasible and profitable to decrease ci by some ci and increase dj by
dj = ci . Hence, we must have p+ +    p . In particular, p+  p , with this
inequality being strict if  > 0 or  < 1, in which case, for all intervals i  I at most one
of ci and di can be non-zero.
For each interval i, if pi > p , then di = Oi and ci = 0, so s1 (pi )  !i = Oi and
thus, s1 (p )  !i < Oi as s1 () is an increasing function. This means that bi = Oi =
i (p ). On the other hand, if pi = p then bi = s1 (p )  !i = i (p ). Likewise,
for each interval i, if pi < p+ , then ci = C and di = 0, and so s1 (pi )  !i = C and so
(p+ ) = C = bi , and if pi = p+ then bi = i (p+ ), directly. If pi lies strictly between p
and p+ then both ci and di must equal 0, and so s1 (pi )  !i = 0. Thus, s1 (p )  !i  0
and s1 (p+ )  !i  0, and so i (p+ ) = i (p ) = 0. These results show that for all prices,
at equilibrium we must have bi = i (p+ )  i (p ). As stated above, if  < 1 or  > 0, then
for each time interval i, at most one of ci and di can be non-zero, and so, in this case we
+ ) and d =  (p ).
can specify,
i
! ci = i (p
! i
!

+ ) and p+ +   p , we must have that

Since

(p
)
=


(p
i
i
iI
iI
iI i (p ) 
!
!
!
 iI i ( p  ) and so, any solution to iI i (p) =  iI i (p  ) must satisfy
monotonicity !
of both sides of this equation.
any such p,
p  p , by the !
! Furthermore, for!
+
we would have iI i (p)  iI i (p ), meaning that iI

(p

)

i
iI i (p )
!
!

since
which implies that p  p+ . It would also
!mean that,
! iI di= iI i (p )  ,

by the capacity!constraint, we !
must have iI i (p )  iI i (p ). Similarly, we must
+
+
also have that iI !
i (p ) 
i (p ).
iI !
!
+
that iI i (p ) < iI i (p+ ). This would also imply that iI i (p ) <
! Now, suppose

1 +
iI i (p ). Hence, there would have to exist some intervals i, j with s (p )  !i <
s1 (p+ ) and s1 (p )  !j > s1 (p ). However, that would imply that, in our equilibrium
state, ci = di = 0, dj = cj = 0. Hence we would have pi < p+ and pj > p , meaning that
pi +  < pj and that it would be profitable to increase ci by some ci and increase dj by
dj = ci . This implies a contradiction, since for a small enough change, this would lead
to a feasible storage profile.
784

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

!
!
!
!
Hence we must have that iI i (p+ ) = iI i (p+ ) and iI i (p ) = iI i (p ).
However, since these functions are monotonic, we must have that for all i, i (p+ ) = i (p+ )
and i (p ) = i (p ). Thus, our equilibrium must satisfy the conditions in the statement
of this theorem.
Thus, there exists at least one minimiser of f () over the feasible domain, and any
minimiser of f () must satisfy the conditions given in the statement of the theorem. Furthermore, the conditions given in the statement of the theorem are sufficiently strict as
to specify f () precisely. Thus, any feasible storage profile which satisfies the conditions
given in the statement of the theorem must minimise f () over the feasible domain, as
required.
A direct consequence of this theorem and the prior observations is that no matter what
the storage capacity is, the aggregate amount charged during the day is bounded above by:
"
i (p  ),
iI

for p equal to any solution to:
"
iI

i (p) = 

"
iI

i (p  ).

If the storage capacity is greater than this amount, then some portion of it will not be
used at equilibrium. Moreover, since we characterise the competitive equilibria as global
minimisers of aggregate costs, and agents have negligible market power, the addition of
more storage capacity is profitable if and only if the total amount of storage is less than this
maximal value. This key result leads us to predict that for a given aggregate load profile,
either not all consumers will need to buy storage or the optimal battery capacity to buy
for each consumer will be bounded. In particular, this bound is further supported by our
empirical evaluation of the UK electricity market where only a subsection of the population
is required to adopt storage to minimise their costs (see Section 6).
4.3 Idealised Scenarios
Having determined the existence and characterisation of charging and discharging price
points, we now investigate how these prices will be set in the context of two idealised
scenarios where micro-storage devices have been deployed in the grid on a large scale. This
aims to identify the properties of the system as different parameters tend to particular
limits (and understand the broad system behaviour). Our intuitions on their impact are
expressed through the following corollaries. We consider the situation where agents can sell
electricity back to the grid in order to simplify the results we obtain and thus make clearer
the intuition we are trying to provide. Similar results would hold in the case where agents
cannot sell, but have similar-shaped load profiles, and storage capacities in proportion to
their daily load.
Corollary 1. If agents are allowed to sell electricity back to the grid at the current price,
and if, for all agents a  A, C a and Da are sufficiently large, then for all i, p+  pi  p .
Furthermore, if for any i  I, p+ < pi < p , then bi = 0.
785

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

Proof. If, for all a  A we let C a and Da be equal to a , then this does not break our
smallness assumption, and, furthermore, for all i  I, well have i (p ) < Oi and i (p+ ) <
C. For all i, either bi < 0, in which case 0 < i (p ) < D and so pi = p , or else bi > 0,
in which case 0 < i (p+ ) < C and so pi = p+ , or, lastly, we could have bi = 0, which can
only occur if i (p+ ) = i (p ), and so either p+ = pi = p or i (p+ ) = i (p ) = 0 and
p+ < pi < p . This covers all possible cases, as required.
Thus, if the charge and discharge rates are sufficiently high, then we could expect prices
to always lie within p+ and p .
Corollary 2. If agents are allowed to sell electricity back to the grid at the current price,
and if, for all agents a, C a and Da are sufficiently large, capacity  is sufficiently high,
 = 0 and  = 1, then for all i, p+ = pi = p .
Proof. If  is sufficiently high, then we must have an equilibrium where p+ = p  , or
p+ = p . The result then follows directly from the previous corollary.
Hence, in the above scenario, with perfectly efficient, cost free, and high capacity storage,
we would expect the market prices over time to flatten to a single value. This is because
perfect storage capability would allow agents to transport energy from any time interval to
any other time interval free of charge. Thus, different suppliers in different time intervals
would have to compete with each other, resulting in convergence to a single market price.
Even if storage devices are not perfectly efficient, they still have a price flattening effect,
though this is mitigated by the fact that agents would have to buy more energy than they
discharge, meaning they would always require some price difference between charging and
discharging periods. Since prices are a direct function of demand, we can infer that having
large amounts of storage should effectively bound the maximum and minimum levels of
demand for each time period. Thus, we should expect the addition of large amounts of
storage to have a significant effect on the grid load factor, with the size of this effect
directly related to the efficiency of the storage device. Hence, in Section 6, we study the
impact of storage adoption on the grid (i.e., in terms of grid efficiency and cost savings
for the population) and determine the impact of micro-storage devices for various levels
of saturation of micro-storage across the population. Before doing so, however, in the
next section, we provide an analysis of the proportion of the population expected to adopt
storage devices (i.e., based on an individuals profits from micro-storage). This is important
because, having shown that the real-time price of electricity should flatten, we aim to show
how such prices will impact on the profitability of buying micro-storage. Such an analysis
can then be helpful in determining the viability of micro-storage deployment projects.
4.4 Storage Adoption
Our model shows that as available storage capacity and charging (and discharging) rates
increase, there can be a significant effect on prices. Moreover, it shows there is a limit on
how much storage capacity is required, such that it is profitable to increase capacity if and
only if the available storage is below this limit. Finding such limits is a useful application
of this analysis, as it gives an indication of what level of adoption is likely to occur given
the cost, efficiency and charging and discharging rates of the leading storage technology.
786

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

In reality, however, it is not practical for home users to incrementally increase their
storage capacity over time to find the optimal level. The more likely scenario is that when
a home user buys a storage device, they will buy enough to cover their usage requirements.
Aggregate storage capacity will increase over time as more and more homes install such
devices. Predicting the number of potential buyers of micro-storage devices will be key to
understanding (for market makers and producers) whether demand will be large enough
to take advantage of the economies of scale for the production of micro-storage devices (of
high efficiency or high capacity). In this respect, it is crucial to study the maximal level
of storage adoption for such batteries. That is, the percentage of homes that can install
electricity storage devices before it is no longer profitable for more devices to be installed.
We will consider the case where home users cannot sell electricity back to the grid (or
to neighbours). If users can sell stored electricity, then it would be possible that some
users could purchase extra capacity or multiple storage devices as a way to make money.
However, the devices themselves are likely to be manufactured with the energy needs of a
single home in mind, and so this analysis still gives a useful guide to how many such devices
can be profitably deployed throughout the populace.
Corollary 3. Suppose we model a population of agents A such that a subset of agents
A$  A have homogeneous storage devices, where |A$ | = |A|, for some  < 1. Suppose
further that the population of agents in A$ is homogeneous, so that the aggregate load profile
of agents in A$ is !i and that storage devices have sufficiently large maximal charging
and discharging rates and capacities so that they do not restrict their storage profiles at
equilibrium. Then, at equilibrium, it is individually profitable and increases aggregate benefit
for an agent in A \ A$ to install a storage device if and only if:
 < max
iI

where p is the maximal solution to:

$
1 # 1
s (p)  !i ,
!i

& #
"#
$+ "
$ '
min !i , s1 (p)  !i
.
s1 (p  )  !i =
iI

iI

Proof. We can model this scenario by setting:
Da = C a = max !ai ,
iI

and
a =

"

!ai ,

iI

A$

for all a 
and
=
=
= 0 for all a  A \ A$ . For the agents in A$ , these values
for maximal charging and discharging rates and capacities are sufficiently large that the
corresponding constraints cannot be tight at equilibrium.
From Theorem 1, the aggregate storage profile at equilibrium will be:
&
# 1 
$+
$ '
#
s (p  )  !i  min !i , s1 (p )  !i
Ca

Ca

a

787

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

where p is the maximal solution to:
&
"#
$+ "
$ '
#
s1 (p  )  !i =
.
min !i , s1 (p )  !i
iI

(4)

iI

It is profitable for an agent in A \ A$ to get a storage device if and only if the addition
of that storage device will increase the total amount of energy charged and discharged at
equilibrium. For if the addition of the storage device does lead to an increase in the total
amount of energy charged, it means that it is more profitable for the agent to use its storage
device than not to use it. Thus, the agent must obtain some profit by having the device.
If the addition of the storage device would not lead to an increase in the total amount of
energy charged, then, since the amount charged is:
&
"
$ '
#
,
min !i , s1 (p )  !i
iI

This means the addition of the storage device can have no effect on p . Hence, in this
circumstance, the addition of a storage device would have no effect on the aggregate storage
profile, and so the collection of storage profiles that agents have in equilibrium would still
remain an equilibrium if a new device was added  with the maximally profitable behaviour
of the agent with the new device being to simply not use it.
If for any i  I,
$
#
!i < s1 (p )  !i ,

then, since the addition of a storage device will strictly increase , the previous value of p
will no longer satisfy (4) and so p , along with the total amount of energy charged, will
increase. Otherwise, increasing  has no effect on (4), meaning p will not change, and
neither will the aggregate amount of energy charged.
Let p be the maximal solution to:
& #
"#
$+ "
$ '
s1 (p  )  !i =
.
min !i , s1 (p)  !i
iI

iI

If

#
$
!i < s1 (p )  !i

for some i  I then, since, by inspection p  p , we must have:
#
$
!i < s1 (p)  !i .
If

for all i, then p satisfies:
"#
iI

#
$
!i  s1 (p )  !i

s1 (p  )  !i

$+

=

"#
$
s1 (p )  !i ,
iI

and, since !i > !i , p satisfies:
& #
"#
$+ "
$ '
1

1 
,
min !i , s (p )  !i
s (p  )  !i =
iI

iI

788

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

and so, p = p and:

#
$
!i  s1 (p)  !i .

Thus, the condition for an additional storage device to be profitable is that for some
i  I,
$
#
!i < s1 (p)  !i
as required.

This corollary can be used to give an indication of the level of adoption in a population
required to see maximal aggregate cost savings from the use of energy storage. Later, in
Sections 6.6 and 7, we complement this result with an empirical study of the system-wide
benefits of micro-storage adoption in the UK market (where agents use our novel storage
strategies) which points to a similar bound on the level of storage adoption.
4.5 Rationality Assumption
So far, our main results (i.e., Theorem 1 and Corollaries 1, 2, and 3) give the aggregate
storage behaviour when our game is in a deterministic competitive equilibrium and predict
the extent and nature of the adoption of storage devices in a population. We can use these
results to specify the limits of the grid performance benefits and market conditions (i.e.,
levels of adoption and equilibrium price for electricity) that can result from adopting microstorage. If the actions of such selfish and profit-motivated agents are to result in stable
aggregate behaviour, then we can do no better than the outcomes described above.
However, in using game theory, we have made some implicit assumptions, specifically
that agents are rational and have complete information about the market throughout the
time period and have the ability to compute an optimal strategy given that information.
In reality, information available to those owning storage devices will not be perfect and
the agents will have different computational capabilities. Furthermore, even with perfect
information, it might not be apparent to an automated agent which strategies are preferable.
Instead, the agents themselves must adapt over time, to become aware of any changes in
market prices, and learn which storage strategies are preferable. This is a difficult problem
and it is not guaranteed that selfish learning behaviour can converge. In particular, if agents
over-react to perceived opportunities in the market, cycles of price fluctuations could develop
with no stable outcome (as seen in Subsection 6.2).
Having looked at an analytical approach which required complete information, in the
next section, we provide a practical and (informationally) decentralised approach that addresses this lack of complete information. Specifically, we describe a novel adaptive storage
strategy that dynamically adapts to changes in market prices, allowing the selfish, profitmotivated agents to individually maximise their savings using only their private information
and information about observed market prices. Under this scheme, agents learn to change
their storage profiles each day to be closer to their perceived optimal strategy. In Section 6,
we show that provided the adaptation is not too fast, our mechanism converges to an equilibrium predicted by Theorem 1. Moreover, our empirical results confirm the bounds on
storage capacity and adoption of storage we have predicted so far. Altogether, these results show that the assumptions we have made in our theoretical framework are reasonable
789

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

enough to model heterogeneous populations of agents and, therefore, our framework can be
generally applied to large-scale micro-storage analysis.

5. An Adaptive Storage Strategy
In this section, we present a novel adaptive strategy that an agent can use to decide when
to store energy and when to use the energy it has stored. Now, for the system to converge
towards an equilibrium, we need to avoid having too many agents charging their batteries
at the same time, in turn resulting in higher costs for everyone. Any strategy that achieves
this would be a good candidate as long as it is shown to converge to the equilibrium. One
possible candidate would allow the agents to adapt their storage profile solely using a target
profile (which would be the equilibrium profile in this case) provided by the supplier (similar
to existing TOU pricing schemes which, rather poorly, incentivise charging during off-peak
hours and discharging at peak time). However, such a strategy would require optimisation
by a centre (i.e., the supplier) and the solution would depend on how accurately the supplier
can estimate the combined charging and discharging rate limits and storage capacities of
all agents with storage devices across the grid and how often it needs to do so. Thus, we
prefer a strategy that does not require grid-wide knowledge and that can adapt based only
on the agents private information (i.e., information about their own micro-storage devices)
and the observed market information (i.e., real-time retail prices that continuously change
as a result of changing demand due to consumers using storage devices). Indeed, we design
such a decentralised strategy which we now describe in more detail.
Our strategy is based on a day-ahead best-response storage.10 Because the market
prices are unknown a priori, we can only calculate the storage profile on a day-ahead basis,
as a best-response to the predicted market prices (which are only observed a posteriori
once the aggregate demand of the market is known). Now, if all agents were to adopt
their best-response, the resulting effect would simply be peaks moving from periods of high
demand to those with previously low demand as we empirically demonstrate in Subsection
6.2. With the peaks in demand moved to previously low-demand periods (as are peaks
in market prices), the agents end up paying higher prices when they charge their battery.
Thus, an agent that plays its best-response is exposed to these changing peaks. To mitigate
its exposure to these changes, we need to ensure that each agent gradually adapts its storage
towards the best-response storage instead of reacting to prices and, by so doing, avoid all
agents herding to consume at the lowest predicted price point. In this section, we first
describe how we calculate the day-ahead best response storage profile and, second, we
describe our learning mechanism, that is how the agent adapts its storage.

10. Our strategy is unaffected by the use of different time-scales (other than day-ahead). We perform the
day-ahead optimisation in our case as there exists a natural cycle of consumption over similar days that
we can exploit to generate load profile distributions in our simulations and reduce the number of times
the optimisation algorithms have to be run. In a real-world deployment, a finer grained optimisation (at
the level of half or quarter hours) would probably be more appropriate as the agent re-optimises based
on the up-to-date intra-day half-hourly consumption and market prices as well as the current amount of
stored energy available.

790

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

5.1 The Day-Ahead Best-Response Storage
The objective of agent a is to minimise its costs by storing energy when prices are low and
using that energy when prices are high.
because market prices are unknown until
! Now,
a
the aggregated load of all consumers aA !i = !i is known, the agent needs to decide
on its storage profiles based on a prediction of the market prices. Note that in our work,
we assume that market prices do not move significantly over similar days (e.g., during the
same season, weekdays tend to be similar to each other but different to week ends) and use
a weighted moving average to predict these future market prices.11
We compute the storage profile, bai = cai  dai at every time-slot during the day as the
solution to the optimisation problem (expressed as a linear program) where we minimise the
following cost function given the decision variables cai , dai , and a (representing the storage
capacity):12
(
)
"
(5)
arg min
pi (cai  dai + !ai ) + a cai
a
b

iI

subject to the following constraints:

Constraint 1: Energy conservation
"

dai =  a

iI

"

cai

iI

Constraint 2: Within charging and discharging rate limits
dai  Da and cai  C a i  I
Constraint 3: Energy that can be stored or used at a time-slot
&
&
''
!
a  da
dai   a ca0 + i1
c
i  I
j
j
j=1
&
''
&
!
a
a
i  I
cai  a  ca0 + i1
j=1 cj  dj

Constraint 4: No reselling allowed

!ai  dai  0 i  I
The last constraint can be removed in a system where consumers are allowed to sell power
to the grid. Note that when the capacity of the storage a is known, a is constrained
to a and when we need to find a , a is left unconstrained in the optimisation and a
is then &calculated as '
the maximum energy stored (in the optimised storage profile), i.e.,
!i
a
a
maxiI c0 + j=1 bj .

11. As we will demonstrate later on, this is not central to our work as the price movements are generally
small. However, a number of more sophisticated prediction algorithms, such as regression or Gaussian
processes, could be used instead for better predictions if price movements were significant as a result of
a very volatile market.
12. We used IBM ILOG CPLEX 12 to implement and solve the linear program.

791

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

As described earlier, a is the cost of using storage which we set to be very small as
we wish to find the best response regardless of the external factor which is the cost of
storage.  a is the efficiency of the storage device, C a is the maximum charging rate, Da the
maximum discharging rate and ca0 is the amount of energy stored at the beginning of the
day which is equal to the stored energy at the end of the day. We next consider how the
agent adapts its storage based on its best-response.
5.2 Learning in the Market
Because market prices move each day, the agent needs to continuously adapt its storage
profile to reflect these changes. One may expect that if micro-storage is incrementally rolled
out, the agents would be able to gradually adapt and stabilise market prices. However, as
we will empirically demonstrate in Subsection 6.2, the system becomes unstable when too
many agents attempt to optimise at the same time, even if they use their best response and
incrementally acquire micro-storage devices. Now, because of the relatively high cost of
storage (compared to the savings in energy cost  see Section 7) and assuming the effect of
micro-storage in the system will change market prices significantly (such that the optimal
capacity the agent requires will be changing  as predicted by the results in Section 4.2),
it is necessary for the agent to gradually change how much of its absolute storage capacity
that it actually uses. To this end, based on intuitions drawn from our analytical results that
point to a bound on the capacity required and the adaptation of charging profiles to prices
at different times of the day, we develop a learning mechanism based on the Widrow-Hoff
learning rule (i.e., a gradient descent approach)13 that adapts both the storage profile and
capacity of the micro-storage device that is used with respect to changes in the market
prices.
Our learning mechanism is based on a two-pass approach to adapt the storage capacity
and profile. Initially, the agent computes the optimal storage capacity  a required to
minimise its costs. More precisely,  a is the cost-minimising capacity by setting a as an
unconstrained variable in the optimisation (see Equation (5)). Now,  a constitutes a desired
capacity towards which the agent adapts its utilised storage capacity. That is, it changes
its storage capacity that it uses progressively to follow the changing trend of market prices.
The actual storage capacity used by the agent is defined by Equation (6) as a (t) that
follows the desired storage capacity  a such that:
a (t + 1) = a (t) + 1 ( a  a (t))

(6)

where a (0) = 0 by default and 1 is the learning rate of the storage capacity of agent
a.14 Given its storage capacity, the agent then computes its optimal storage profile for the
following day by fixing a at a (t + 1) in Equation (5).
On the second pass, given its current storage profile, the agent adapts its storage profile
as follows:
13. We used the Widrow-Hoff rule as it can be directly engineered into our optimisation algorithm, but with
relatively minor changes, other learning rules (such as reinforcement learning or Bayesian learning) could
be used.
14. As we will empirically demonstrate in Section 6, the choice of the learning rates determines the evolutionary stability of the system and has to be reasonably small to ensure convergence.

792

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

bai (t + 1) = bai (t) + 2 (ia  bai (t)) i  I

(7)

where  a is the desired storage profile given as the optimal storage profile subject to a fixed
storage capacity of a (t + 1) and 2  (0, 1] is the learning rate of the strategy. Note that
we analyse in more detail the sensitivity of the learning parameters 1 and 2 as part of the
empirical study of the system in the next section.

6. An Empirical Analysis of Micro-Storage in the UK Market
Based on the adaptive storage strategies defined in the previous section, the analysis of
micro-storage we present in this section aims to complement the theoretical part of this
paper which assumed a largely homogeneous population of agents (see Section 4). In particular, here we evaluate the emergent properties of a large populations of 1000 agents15
owning micro-storage of different charging and discharging rates and sizes and using our
learning strategy to adapt their storage profile during 500 simulation days over 200 runs.
At the beginning of each simulation day, an agent makes a prediction of its load profile
and the market prices (using historical data) across the 48 time-slots to compute its best
response on a day-ahead basis. At the end of the simulation day, the actual market prices
are computed from the total domestic market demand (i.e., the aggregate load and storage
profiles) based on the market macro-model described in Subsection 3.2 and published to all
the agents a posteriori. To frame our results within a real-world context, our simulations
focus on the UK electricity grid.16
Thus, given our macro-model of the UK electricity retail market (see Section 3.2), we
initialise individual consumers with typical UK load profiles.17 Moreover, the learning rates
of the agents (presented in Sections 5.1 and 5.2), as well as their charging and discharging
rates, are normally distributed around means drawn from charging (and discharging) rates of
current technologies (see Section 2).18 This is done to represent heterogeneous consumers
with different types of storage devices and address a more realistic scenario than in our
game-theoretic analysis.
Given this setup, for benchmarking purposes we first compute the competitive equilibrium predicted by our game-theoretic framework (which assumes complete information
15. This simulation could be readily extended to hundreds of thousands of agents given the distributed
nature of the computation. Now, given that, on average, agents tend to have similar load profiles, we
assume here that there are 1000 different clusters of load profiles among the domestic consumers such
that a simulation with only 1000 agents (rather than millions) is valid. We have observed from real data
that such an assumption about the heterogeneity of the UK domestic population is reasonable.
16. Note that we choose UK as a typical deregulated market. Our approach is nonetheless general enough
that our framework can be applied for other markets, including industrial or commercial markets (as
opposed to the residential case we consider) as well as micro-grids or the national grid of other countries
based on a macro-model of their electricity market. Thus, the results and insights presented in this paper
broadly generalise.
17. The profile of an agent is based on a normal distribution with mean !mean which is taken as the UK
, ), !ai  0 i  I where  = 0.2 to approximate
average profile. It is formally defined as !ai  N (!mean
i
the typical spiky daily profiles of consumers.
18. In all experiments except for when we analyse the effect of learning rates, we draw values for the learning
rates from a normal distribution N (0.05, 0.02). Through experimentation, this was found to result in
good system-wide performance (see Section 6.5) and is not too small that the system is slow to converge.

793

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

about all agents load profiles and battery capacities) for the UK electricity grid. Second,
we study how the system breaks down (in terms of the average individual costs and grid
performance metrics defined in Section 3.3) if agents were to gradually adopt storage with
no adaptive mechanism. This comparison motivates the need for our adaptive mechanism,
and, against these results, we empirically demonstrate that the market does indeed converge
to the competitive equilibrium when the population of agents adopt our adaptive storage
mechanism. Third, we perform a sensitivity analysis of the convergence properties of our
adaptive mechanism with respect to the learning rates to understand the impact of different
learning rates on cost savings and grid performance and how these parameters should be
set. Fourth, we evaluate the robustness of our approach when agents with micro-storage
do not adopt our learning mechanism. Finally, we evaluate the impact of different degrees
of saturation of micro-storage (using our agent-based micro-storage management) on the
efficiency of the grid and, in so doing, study the performance of the grid (see Subsection
3.3) as more and more consumers adopt the technology.
6.1 The Game-Theoretic Solution
Given the game-theoretic framework outlined in Section 4, we first calculate the competitive
equilibrium based on the average domestic consumption profile of a consumer (from the UK)
using the procedure described in Subsection 4.2 (i.e., where devices are perfectly efficient
and costless). The resulting storage profile is shown in Figure 4. It is clear that the
equilibrium behaviour for a consumer is to charge at off-peak hours (at night) and use the
stored energy during peak hours (after working hours) when the consumers load is highest.
Furthermore, as observed in Figure 4, at the equilibrium, the optimised load profile (i.e., the
sum of the aggregate unoptimised load profile and storage profile) is flattened completely
with a load factor of 1. This implies that agent-based micro-storage management can
theoretically reduce peaks completely and be completely efficient. Furthermore, the storage
capacity required to achieve this equilibrium is 2.3 kWh, computed as the maximum of the
cumulative sum of the storage profile (as the micro-storage device charges and discharges
over each time-slot). In the next subsection, we first demonstrate how, in practice, the
system breaks down completely without an adaptive mechanism to motivate our need for
an adaptive mechanism and, we then go on to show that the system indeed converges to
the competitive equilibrium when our decentralised adaptive mechanism is adopted.
6.2 Market without the Adaptive Mechanism
To analyse how the market operates without the adaptive mechanism, we set up a population
of agents playing their best-response storage profile every day (i.e., using the optimisation
algorithm defined in Section 5.1). Moreover, we simulate the gradual adoption of storage
devices by the consumers, with a rate of adoption, r (i.e., a probability of r that an agent
will adopt storage and keep a storage device; r = 1 simulates a system where all agents
have storage capabilities at the beginning). Once an agent has storage capability, it will
optimise daily and use its best-response storage profile. For this setting, Figure 5(a) shows
the deviation from the competitive equilibrium while Figure 5(b) shows the load factor of
the grid for different values of r. When r = 1 (i.e., all consumers adopt micro-storage
794

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

0.6
0.5

Power (KW)

0.4
0.3
Storage profile at equilibrium
Average UK load profile
Load profile at equilibrium

0.2
0.1
0
0.1
0.2
0:00 2:30

5:00

7:30 10:00 12:30 15:00 17:30 20:00 22:30 24:00
Time of day

Figure 4: Storage profile and load profile at the competitive equilibrium.

devices at the same time), the system clearly deviates from the equilibrium with the load
factor jumping immediately from 0.66 (without micro-storage) to 0.4 (with the immediate
adoption of microstorage), suggesting larger peaks in the system. For smaller values of r,
the system converges at the beginning (as the demand slowly decreases at peak time and
increases at off-peak time since only a small proportion of agents can change their demand),
only to break down after a number of simulation days and ends up with larger peaks. The
smaller r is, the longer the system takes to break down, though it inevitably does so. The
intuition behind this is that there invariably reaches a point when there are too many agents
that have adopted micro-storage devices and are using their best-response storage profile.
With too many agents re-optimising their storage profiles at the same time, the peaks
in market demand are moved around such that aggregate demand profile is not flattened
(inferred from the non-increasing load factor on a long-term). In the next subsection, we
show that our adaptive strategy helps remedy this and results in desirable system-wide
performance.
6.3 Market with the Adaptive Mechanism
Here, we study the convergence properties of our adaptive mechanism and also show how the
results corroborate the theoretical bound on storage capacity suggested in Section 4 (given
the worst case scenario in Subsection 6.2 when r = 1). In more detail, given a population
of agents using the adaptive mechanism with 1  N (0.05, 0.02) and 2  N (0.05, 0.02)
(we show how the performance varies with different learning rates in the next section), the
average storage profile is found to converge rapidly to the competitive equilibrium of our
795

fiMeansquared deviation from equilibrium (logscale)

Vytelingum, Voice, Ramchurn, Rogers & Jennings

0

10

1

10

r=1.0
50

100

150

r=0.01

r=0.005

200 250 300
Simulation Days

r=0.001
350

400

450

500

(a) Deviation of the population without adaptive storage from competitive equilibrium.

r=1.0

r=0.01

r=0.005

r=0.001

0.8
0.75

Load Factor

0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35

50

100

150

200 250 300
Simulation Days

350

400

450

500

(b) Load factor (LF) in a market with no adaptive mechanism.

Figure 5: Convergence properties of a system with no adaptive storage.

796

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

game-theoretic analysis within less than 20 simulation days (see Figure 6).19 As expected,
the convergence results from the agents gradually adapting their storage profiles such that
the aggregate market demand is shifted from peak to off-peak.

Meansquared deviation from equilibrium (logscale)

0

10

1

10

2

10

3

10

100

200
300
Simulation days

400

500

Figure 6: Convergence of the average storage profile to the competitive equilibrium when
all agents adopt the adaptive mechanism.

As the system converges to the competitive equilibrium, we also observe how the grid
efficiency (as measured by the LF and DF) improves and gradually converges (see Figure
7) as agents adapt their storage profiles and the system converges to the competitive equilibrium. In more detail, from Figure 7, we observe that the system LF increases from 0.68
and converges to around 0.93, suggesting considerably fewer peaks in the grid when microstorage is adopted, and indeed, a flattened demand.20 This is coupled with a DF that is
close to 1 which indicates (as discussed in Section 3.3) that, even though agents have closely
correlated load profiles, overall, these profiles tend to be reasonably flat.
Furthermore, from Figure 8, we can see that the average storage capacity required
converges to around 2.3 kWh (which equals the storage capacity prescribed by our analytical
solution  see Section 6.1) after several simulation days. This implies that while the average
19. Given that weekdays are homogeneous (as opposed to Saturdays and Sundays), the agent can learn
across weekdays such that the system would converge within a couple of weeks.
20. The results for each simulation day were averaged over the number of simulations. Furthermore, a
simulation size of 200 was statistically significant, with results in the figures given with error bars at the
95% confidence interval.

797

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

Load Factor and Diversity Factor

1.1

1

0.9

0.8

0.7
Load factor
Diversity factor
0.6

50

100

150

200
250
300
Simulation Days

350

400

450

500

Figure 7: Efficiency of the grid as system converges to competitive equilibrium when all
agents adopt the adaptive mechanism.

consumer may buy a storage device of capacity 3kWh (see the maximum storage capacity
in Figure 8), the agent would actually only need 2.3kWh of this capacity to minimise costs.
6.4 Sensitivity of the Adaptive Mechanism
One of the assumptions of our approach is that agents are expected to adopt the adaptive
mechanism. While this can be imposed as a feature of the smart meter controlling the
micro-storage device, it can exceptionally be the case that the smart meter is tampered
with or that the user programs it to ignore the learning mechanism and only use its best
response, i.e., the agent always executes its optimal behaviour. Thus, in this subsection,
we study the effect on the system if part of the population were not to adopt our proposed
adaptive mechanism, assuming that the whole population has storage capability.
From Figure 9, we can observe that the system is particularly robust and only starts
degrading when more than 60% of the population do not adopt the adaptive mechanism
and deliberately execute their best response. As the proportion of the population playing
their best response increases, the load factor slightly increases to 0.94 until the proportion
of population reaches around 60% after which the load factor rapidly decreases to 0.4
(suggesting large peaks in the system) when all agents adopt their best response, i.e., there
is no adaptive behaviour in the system. While some agents are using their best response,
other agents are gradually adapting their storage profiles (implicitly adapting to the impact
of the former agents best-response profile). The system eventually breaks down when too
798

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

3.5

Storage Capacity (kWh)

3

2.5

2

1.5

1

0.5

0

50

100

150

200
250
300
Simulation Days

350

400

450

500

Figure 8: Average storage capacity required for a typical consumer as system converges to
the competitive equilibrium.

many agents are using their best response and too few their adaptive mechanism. We also
notice a small increase in load factor as a result of the increased diversity among agents, an
emergent behaviour which we again observe when analysing the load factor of a population
with different proportions having storage capability (see Subsection 6.6 for a discussion).
Note that while the system remains robust upto 60% population saturation of micro-storage
even without a learning mechanism, our mechanism ensures that the system does not break
down for any proportion of the population, as we observe in Subsection 6.6.
6.5 Sensitivity of the Convergence Properties
To guarantee the consistency of our results given different parameter settings, in this section
we explore how the values of the learning rates21 1 and 2 affect our convergence results.22
In so doing, we aim to determine how fast an agent should ideally adapt its storage profile
to maximise its savings while, if possible, helping to improve the efficiency of the grid.
Figure 10(a) shows that the smaller the learning rate, the more efficient the system (with
a higher load factor). The intuition behind this is that a small learning rate allows the
market prices to change gradually. A higher learning rate, on the other hand, would result
21. Because the learning rates are intrinsic to the adaptive mechanism, we assume that 1 and 2 share the
same value without loss of generality as we are interested in how fast our mechanism adapts rather than
specifically in the two-part adaptation.
22. The mean load factors and savings between Day 400 and Day 500 (by which time the system generally
has converged) were recorded and averaged over 200 runs for different learning rates.

799

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

1

0.9

Load Factor

0.8

0.7

0.6

0.5

0.4

0.3
0

0.2

0.4

0.6

0.8

1

Proportion of population with storage switching to bestresponse

Figure 9: Grid efficiency when consumer switch to their best-response.

in agents adopting their optimal storage profile too quickly rather than gradually, which
clearly results in poor savings and poor system efficiency with peaks cycling in the system.
As the learning rate increases, the load factor quickly decreases to 0.59 when all agents
adopt their best-response immediately. From an individual perspective, the agent would
typically set its learning parameter based on its savings. From Figure 10(b), we can see
that, likewise, the smaller the learning parameter is, the better the average savings23 of the
individual agent.
Now, because an infinitely small learning rate is infeasible as it implies an infinitely
long time to reach the equilibrium, a trade-off is required. Specifically, because the learning
parameters are not very sensitive when they are small, a value of 0.05 is reasonable given
that the decrease in savings would be negligible. As mentioned in Footnote 18, we use this
value for all our experiments.
Given these results, we can claim that our adaptive strategy sets the benchmark for
any learning strategy in this system (i.e., the base requirement for any such strategy would
be convergence). While these results mainly hold for a whole population owning storage,
it is important to see how the system performs as storage is gradually introduced in the
population. This should enable us to identify the optimal level of adoption of storage that
is required in order to maximise grid efficiency. In so doing, we also aim to complement our
analytical results (see Section 4.4) with empirical evidence showing the level of adoption
that is still profitable for individual users to acquire storage in the UK market.
23. The average saving of a consumer is computed as the difference in her average costs (after the system
has converged) in a system with micro-storage (i.e., after the system has converged) and in one without.

800

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

1
0.95
0.9

Load Factor

0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0

0.2

0.4
0.6
Learning rate

0.8

1

0.8

1

(a)
0.1

Savings for agents with storage

0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0

0.2

0.4
0.6
Learning Rate

(b)

Figure 10: The sensitivity of the learning rate, 1 = 2 , on the system in terms of (a) the
load factor and (b) savings for agents with storage.

801

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

6.6 Grid Efficiency with Incremental Micro-Storage Adoption
In the following set of experiments, we investigate the grid performance metrics and carbon
emission reductions (which is one of the main aims of this work) achieved by our mechanism
as micro-storage is incrementally adopted.24 From Figure 11, we can observe that the grid
efficiency peaks at only 32% of the population (rather than at the aggregate storage capacity
of the population at 100%). This suggests that only 32% of the population is required to
have storage to maximise the grid efficiency. As that proportion increases from 0, more
agents have storage capability and thus storage profiles that are being adapted to flatten
the demand. Their aggregate storage profile gradually flattens the aggregate load profile
such that the load factor increases as does the diversity factor (since more agents now have
a storage profile and, thus, an adapted demand profile). Eventually, the load factor peaks
at 32%, at which point, the system is flattened as much as it could be. As more agents
acquire storage devices, the diversity factor decreases to 1 as more agents use storage, and
finally settles at 1, where, on average, they have the same (flatter) load profile and storage
profile. Furthermore, with more agents optimising at the same time, the load factor also
decreases slightly as too many agents are now trying to optimise and adapt their storage
profiles at the same time. Specifically, agents are optimising a surplus of storage capacity
that is not required to flatten the demand of the grid. As can be seen, the diversity factor
decreases which suggests that optimising the surplus of storage increases correlation among
individual demand profiles. The increased correlation further suggests that any small peak
in the load profiles has more impact which, on average, decreases the load factor.
From Figure 12, we also observe that a significant benefit of storage at a macro-level (i.e.,
ignoring the individual benefits of the agents  which we study in the next section) is that
if a sufficient proportion (32%) of the population does adopt storage, the carbon emissions
of the electricity market would decrease appreciably as peak demands are reduced. Indeed,
the carbon emission in the UK can be reduced by up to 7% (from 63 to 58.3 kilotonnes
CO2 per day),25 reaching a minimum when the domestic load factor is maximised (since
reducing the peaks in demand has the effect of reducing the carbon intensity of the supplied
electricity, which in turn, reduces the total carbon emissions).

7. Cost-Benefit Analysis of Micro-Storage
So far, we have studied the efficiency of the grid achieved by the population as storage is
gradually adopted. We now turn to the individual consumer who is principally driven by
how much profit she can achieve by adopting micro-storage (at a certain cost). In particular,
24. Note that the results we describe here (i.e., the grid efficiency after the system converges) are similar to
our game-theoretic solution given that a system where only a proportion of the population has storage
translates to a model in which there are smaller aggregate charging and discharging limits  see Section
4.
25. We calculate the carbon emissions by considering the reduction in carbon intensity of the electricity
supply when the domestic load factor reduces (see Figure 3). We consider a total population of 26M UK
households and scale these results to take account of the fact that these domestic consumers represent
30% of the total UK demand (as discussed in Section 3) and the remaining 70% consisting of commercial
and industrial profiles that remain unchanged.

802

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

1.8

Load factor
Diversity factor

Load Factor and Diversity Factor

1.6

1.4

1.2

1

0.8

32% of population
0.6
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Proportion of population with storage

0.8

0.9

1

Figure 11: Grid efficiency (i.e., the load factor and the diversity factor) for different proportions of the population using storage.

Carbon Emissions (kilotonnes CO2 per day)

64

63

62

61

60

59

58
0

7% CO2 reduct ion

0.2

0.4
0.6
0.8
Proportion of population with storage

1

Figure 12: Total daily carbon emissions from the UK domestic sector for different proportions of the population using storage.

803

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

the question we wish to address is that: at what level of adoption is storage still profitable
to agents in the system?26 Hence, in our experiments, we first assume no cost of storage
and only vary the proportion of the population with storage and record the cost-savings
achieved by the parts of the population that have and have not adopted micro-storage. From
our results (see Figure 13), we first observe that when very few agents have micro-storage
(i.e, close to 0%), the potential average savings to each agent is close to 14%. As more
consumers adopt storage, the average saving gradually decreases to slightly less than 8%
(as market prices also flatten and agents can no longer benefit from the difference between
low off-peak prices and high peak prices). Interestingly, we also observe that consumers
without micro-storage also benefit from its use by other consumers. This is because, as
empirically demonstrated in Section 6, the adoption of micro-storage flattens the peaks
in demand in the system and, thus, market prices. This means cheaper electricity for the
domestic market and, indeed, as more and more consumers adopt micro-storage, the savings
of those consumers that do not adopt it, also increase. There reaches a point when 48% of
consumers adopt micro-storage and the savings for those consumers with and without the
technology are equal. As the percentage increases past 48%, the savings for those consumers
who do not adopt micro-storage exceed those of the consumers who do adopt it. This is
because the consumers with micro-storage are trying to optimise a surplus of storage as
argued in Section 6.6. The implication of this dynamic is that consumers are incentivised
to adopt micro-storage until the 48% mark is reached. At this point, there is no incentive
for the consumers to deviate from their chosen behaviour (i.e., use micro-storage or not).27
Thus, over a long term, the system will converge to an equilibrium where only 48% of
the population adopt micro-storage and at that percentage, consumers can expect a saving
of 9% on their individual electricity bills (which equates to an annual saving of 60 per
household  based on an average annual electricity bill of 675). This result also points
to a slight misalignment between the optimal level of storage for the grid (in terms of grid
efficiency) and that for the consumers (in terms of savings). In particular, given the results
in the previous section (see Figure 11), we note that the 48% level of adoption equates to a
domestic load factor of 0.91, while the maximum load factor achievable (assuming control
over the proportion of population adoption storage) is 0.94 which occurs at 32% adoption.
This suggests that at the proportion of adoption that the system eventually settle at, the
system is only slightly suboptimal.
We next consider this dynamic within a more realistic setting when there is a cost to
storage, typically a startup cost from hardware installation (e.g., wiring and converter) and
the cost of the actual battery. Based on how much storage capacity the average consumer
would require to maximise her savings for different proportions of the population, we calculate the savings minus the cost of storage (based on a typical battery costing 200 per
kWh and 600 per kWh respectively with a lifetime of 10 years and a fixed startup cost
of 200 for both) and assuming an average cost of electricity of 675 for a consumer per

26. Note that, with respect to an average consumers savings, this set of experiments complement those in
Section 6.5 where we studied the cost-savings of the users as the learning rate is varied.
27. Note that the consumer is aware of its savings with and without micro-storage which can be computed
based on its initial load profile and the demand profile with the storage profile.

804

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

0.14
Savings without storage

Savings with storage (no cost of storage)

0.12

Savings with storage (Battery cost = 200/kWh and startup cost = 200)
Savings with storage (Battery cost = 600/kWh and startup cost = 200)

Normalised Savings

0.1
0.08
0.06
0.04
0.02
10%

0
0

0.1

48%

23%

0.2

0.3

0.4
0.5
0.6
0.7
Proportion of population with storage

0.8

0.9

1

Figure 13: Savings without storage and with storage (for the different costs of storage).

year.28 Note that savings without storage do not change as they are independent of the
cost of micro-storage. Given this setup, our results (see Figure 13) show that there is a
clear first-mover advantage. Thus, a maximum saving can be achieved when the proportion of the population with micro-storage is close to zero (i.e., only a few agents in the
population own storage). However, the storage capacity these agents require is relatively
high at 4.5kWh, decreasing rapidly to 2.3kWh when all consumers in the population adopt
micro-storage (see Figure 14). Moreover, based on the savings with cost of storage and the
savings without storage, we observe that the equilibrium moves to 23% for a cost of storage
device of 200 per kWh and to 10% for a cost of 600 per kWh (given a startup cost of
200). Hence, by combining the latter results with those from Figure 11, we can infer that
the grid efficiency quickly drops as the cost of storage increases (as the level of adoption
decreases). Thus, the cost of storage can significantly affect the benefits derived by the
users and by the grid as a whole.
The above results, however, consider populations of agents drawn uniformly from the
UK average load profile. It could therefore be argued that such results may not apply in
circumstances where the population is not uniformly distributed and, in particular, if users
consume differing amounts of energy  making savings from storage more viable for those
consuming more (as they are able to shift more energy across the day and recover the high
startup cost of such a system) than others consuming less. Given this, we next expand our
28. We compare the cost of storage with the daily cost of electricity by calculating the daily cost of owning
and running a storage device, i.e., the cost of the storage device and the startup costs that we assume,
are spread over the lifetime of the storage device which, in this case, is 10 years. A battery costing
200 per kWh and 600 per kWh, both with a startup costs of 200 equals 1p and 3p per kWh per
day respectively, while, on average, the daily cost of electricity is 180p for an 8kWh daily electricity
consumption (see Subsection 2.1).

805

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

5

Storage Capacity (kWh)

4.5

4

3.5

3

2.5

2
0

0.2

0.4
0.6
0.8
Proportion of population with storage

1

Figure 14: Average storage capacity required for different proportions of the population
with storage.

cost-benefit analysis to consider a fundamental distinction between users, namely the lowend users (typically with a yearly consumption29 of 1650 kWh) and high-end users (typically
with a yearly consumption of 4950 kWh). In particular, we want to analyse the dynamics of
the proportion of low-end and high-end users that will adopt storage (i.e., given a number
of agents of each type, what proportion of each type will adopt storage). The aim here is
to investigate which type of consumers can benefit the most from our system and, indeed,
whether our system can be more efficient. To this end, we adopt an evolutionary game
theoretic approach that is suitable to analyse such dynamics, and determine whether the
system eventually settles to a stable equilibrium where the behaviours (whether or not to
adopt storage) do not change. We next describe our evolutionary game-theoretic framework
and, thereon, provide a cost-benefit analysis for low-end and high-end users.
7.1 The Evolutionary Game-Theoretic Model
Here, we formulate the problem as a game where all low-end users adopt the same mixed
strategy30 rLU  (0, 1) (i.e., the probability that the low-end users have storage capability)
and are only motivated by financial gains, while all high-end users adopt a mixed strategy
29. This data is drawn from typical consumption data published by British Gas UK at
www.britishgas.co.uk. Furthermore, we assume that both types of consumers have the same normalised daily average load profiles.
30. We assume that agents of the same type share the same mixed strategy given that, on average, they
have the same load and storage profiles and thus, the same expected savings.

806

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

rHU  (0, 1). By analysing how rLU and rHU evolve as the payoffs change for different
proportions of the population of low-end and high-end users with storage, we want to study
how the proportion of the population using storage evolves. To this end, we use the classical
evolutionary game-theory (EGT) (Weibull, 1995) in which we first compute the heuristic
average payoffs of the low-end and high-end (based on simulations) (whether or not using
and not using storage) for different mixed strategies rHU and rLU , given respectively by:
"
uLU (r,  LU ,  HU )rLU
uLU ( LU ,  HU ) =
rS

uHU (

LU

,

HU

)=

"

uHU (r,  LU ,  HU )rHU

rS

where uLU (r,  LU ,  HU ) is the payoff of low-end users adopting the pure strategy r given
the low-end users mixed strategy  LU and the high-end users mixed strategy  LU and
uHU (r,  LU ,  HU ) the corresponding the payoff for high-end users.
We then use these results to calculate the replicator dynamics,  LU and  HU , that
describe the dynamics of the population (i.e., how the proportions of low-end and high-end
users are evolving), and are given by:
rLU = [uLU (r,  LU ,  HU )  uLU ( LU ,  HU )]rLU r  S

kHU = [uHU (k,  LU ,  HU )  uHU ( LU ,  HU )]kHU k  S
LU ,  HU ), points where
Finally, we test whether it converges to any Nash equilibria (nash
nash
there are no incentives for either low-end or high-end consumers to deviate from.
LU
HU
(nash
, nash
) = arg

min

 LU , HU

+

"&

kS

"&
rS

max[uLU (r,  LU ,  HU )  uLU ( LU ,  HU ), 0]

max[uHU (k,  LU ,  HU )  uHU ( LU ,  HU ), 0]

'2

'2

In the next subsection we provide the results of our EGT analysis.
7.2 EGT Analysis of Micro-Storage Adoption with Low- and High-end Users
The results of the EGT analysis are given in Figure 15 for different costs of the storage device
(i.e., 0, 200, 400, 500, 1000 per kWh) assuming a typical lifetime of 10 years for a
battery and a startup cost of 200. Thus, we can observe that when storage is completely
subsidised (i.e., cost of storage is 0), we have a range of Nash equilibria (along the straight
line from ( LU = 0.4,  HU = 1.0) to ( LU = 0.7,  HU = 0)). Furthermore, from Figure 16,
we can observe that at these Nash equilibria, the grid efficiency of the system is very high
(the load factor is higher than 0.9).
Now, when startup cost increases to 200 per kWh, we now have a single Nash equiLU = 0.07,  HU = 1) with a lower load factor of 0.82 (see Figure 16). This
librium at (nash
nash
implies that the system eventually converges to a Nash equilibrium where all high-end consumers adopt storage while only 7% of the low-end users do so. This is because the high-end
users overall make more savings (than low-end users) that cover the daily cost of storage and
807

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

the high startup costs. However, as the cost of the storage device increases to just 200 per
kWh, storage becomes too expensive for all low-end users, with the Nash equilibrium now
LU = 0,  HU = 1), but is still economically beneficial for all high-end users. With
at (nash
nash
increasing cost, storage gradually becomes too expensive even for the high-end users, as
LU ,  HU ) from (0,0.94) to (0,0.55) when
seen from the change of the Nash equilibrium (nash
nash
the cost of the storage device is 400 per kWh and to (0,0.37) when the cost of the storage
device is 500 per kWh. Eventually, the cost of the storage device is too high even for the
LU = 0,  HU = 0).
high-end users at 1000 per kWh, with the Nash equilibrium now at (nash
nash
By considering the change in the Nash equilibria as the cost of the storage device increases,
we also observe from Figure 16 that the domestic load factor decreases gradually to 0.68
when micro-storage has no impact on the grid, being too expensive to be adopted. From
this analysis, we gather that to improve grid efficiency and to maximise the impact of microstorage on the grid, the cost of storage has to be sufficiently small, and subsidising storage
would help improve the efficiency of the grid.

8. Conclusions
In this paper, we set out to explore the theoretical and practical foundations of agentbased micro-storage implementation in the smart grid. To achieve these objectives, we
first developed a game-theoretic framework to analyse the strategic choices that agents
make in using micro-storage devices in the grid. Our framework allows one to predict the
competitive equilibrium of the system, and in particular, specify theoretical bounds on the
level of micro-storage adoption and the capacity of micro-storage that will be adopted by a
largely homogeneous population.
Building upon the intuitions generated by our theoretical results, we then went on to
devise a novel micro-storage strategy that allows an agent to optimise both its storage profile
and storage capacity in order to maximise its owners savings. Furthermore, we provided an
adaptive mechanism based on predicted market prices that allowed the agent to change its
strategy in response to changing market prices. Our empirical evaluation of this mechanism
on the UK electricity grid was then shown to cause the average storage profile to converge to
the theoretical competitive equilibrium. At that point, peak demands are reduced, reducing
the requirements for more costly and carbon-intensive generation plants. Moreover, in our
analysis of the grid efficiency at this equilibrium we show that, while being stable, it results
in reduced costs and carbon emissions. This also shows that the objective of buying storage
to save on electricity bills is generally aligned with maximising the grid efficiency (i.e.,
flattening the peaks in the demand). In particular, we show that, without the burden of
cost (e.g., if storage were completely subsidised), the population would adopt storage until
an equilibrium with 48% of the population adopting storage is reached and this is achieved
with a high level of domestic load factor (i.e. 0.91). Given this, we analysed the system
in a more realistic setting and empirically demonstrated that if costs of storage are not
sufficiently low, the system will not converge to an equilibrium with a high grid efficiency,
and if costs are simply too high, there will be no incentives even for the high-end electricity
users to adopt micro-storage.
808

fiBattery cost = 0/kWh and startup cost = 200

1

0.8

0.8

0.6

0.6

p

HU

Battery cost = 0/kWh and startup cost = 0

1

p

HU

Theoretical and Practical Foundations of Agent-Based Micro-Storage

0.4

0.4

0.2

0.2

0
0

0.5
p

0
0

1

LU

Battery cost = 400/kWh and startup cost = 200

1

0.8

0.8

0.6

0.6

p

HU

1

p

HU

1

LU

Battery cost = 200/kWh and startup cost = 200

0.4

0.4

0.2

0.2

0
0

0.5
pLU

0
0

1

Battery cost = 500/kWh and startup cost = 200

0.5
pLU

1

Battery cost = 1000/kWh and startup cost = 200

1

0.8

0.8

0.6

0.6

p

HU

1

p

HU

0.5
p

0.4

0.4

0.2

0.2

0
0

0.5
p

0
0

1

LU

0.5
p

1

LU

Figure 15: Evolutionary game-theoretic analysis for different costs of storage. Lines are
trajectories representing the evolution of the proportion of low-end and highend users adopting storage. The black dots are the Nash equilibria.

809

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

1
0.9
0.9
0.8
0.7

0.85

p

HU

0.6
0.5

0.8

0.4
0.75

0.3
0.2

0.7

0.1
0
0

0.2

0.4

0.6

0.8

1

pLU

Figure 16: Load factor for different proportions of low-end and high-end users adopting
storage.
In general, our theoretical and practical results provide fertile ground for research into
agent-based techniques that might be applied to manage demand in the smart grid. In
particular, demand-side management technologies (Hammerstrom et al., 2008), which involve loads (e.g., washing machine or dishwasher) in a users home being automatically
scheduled to run at certain times, present similar properties to micro-storage in that they
allow energy to be moved around from peak to off-peak times in order to flatten demand
and reduce costs. Hence, applying a similar framework and strategies to ours, we could
expect analogous theoretical results and efficiency gains being predicted for deployments of
such technologies. Moreover, our techniques could be used to predict how demand would
generally vary across the day once real-time pricing is rolled out and in different regions
(populated by different proportions of low-end or high-end users), and this could help better
prepare assets (e.g., spinning reserve or strengthening transmission lines).
To generalise our techniques further, we intend to integrate more sophisticated models of
the electricity market mechanism into our work in order to better capture the price fluctuations that can occur in real markets. For the theoretical analysis, we will turn to stochastic
processes, which are commonly used to model volatility in financial markets. Further, our
experiments can be extended by generating prices by drawing samples from suitable distributions, or existing data points, or even by developing our simulations to include market
clearing with strategic bidding by energy suppliers and consumers. Accordingly, we also
intend to employ better forecasting models of demand and supply (e.g., using Gaussian processes or other regression techniques) to predict prices in our optimisation model. Indeed,
so far we have assumed that agents predict prices simply from the previous days prices for
individual settlement periods. As shown by Wellman, Reeves, Lochner, and Vorobeychik
810

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

(2004), agents can perform significantly differently if they adopt different approaches to
price prediction and therefore, to improve our empirical analysis, it will be interesting to
see how the grid performance and individual agents profits are affected as different agents
adopt different forecasting models. In particular, it will be important to determine how the
widespread adoption of micro-storage devices should affect the volatility of the wholesale
electricity market.
Furthermore, we intend to explore mechanisms that can ensure convergence towards the
equilibrium. In particular, our point of departure will be the theory of strategic behaviour
found in the Minority Game (Challet & Zhang, 1997), which shares similarities with our
problem, or using a more complex pricing mechanism where the consumers always play
their best response (Voice et al., 2011) such that a learning mechanism for the consumer
agent would not be required. As part of this work, we also intend to investigate the market
efficiency for different proportions of the population adopting micro-storage devices, and
whether the decrease in efficiency observed when the market is saturated was a by-product
of our adaptive mechanism which could be avoided.
Finally, we intend to consider the grid distribution network. Because peaks are different
across different nodes of the electricity network, more storage capacity might be required in
some areas than in others. Thus, we will investigate whether our agent-based micro-storage
management approach can be used to flatten peaks locally within a node of the electricity
network rather than flattening the aggregate demand profile of the grid.

Acknowledgments
This paper extends our previous work (Vytelingum, Voice, Ramchurn, Rogers, & Jennings,
2010). It extends the game-theoretic framework to consider levels of micro-storage adoption
and expands the empirical evaluation to consider the sensitivity of our convergence properties to the learning rate and more complex agent populations. This work was funded by
the iDEaS project (http://www.ideasproject.info).

References
Bathurst, G. N., & Strbac, G. (2003). Value of combining energy storage and wind in
short-term energy and balancing markets. Electric Power Systems Research, 67 (1),
18.
Challet, D., & Zhang, Y. C. (1997). Emergence of cooperation and organization in an
evolutionary game. In Physica A, Vol. 246(3-4), pp. 407418.
Daryanian, B., Bohn, R., & Tabors, R. (1989). Optimal demand-side response to electricity
spot prices for storage-type customers. IEEE Trans. on Power Systems, 4 (3), 897903.
DECC (2009). Smarter grids: The opportunity. Tech. rep., Department of Energy and
Climate Change (DECC). http://www.decc.gov.uk.
den Bossche, P. V., Vergels, F., Mierlo, J. V., Matheys, J., & Autenboer, W. V. (2006).
Subat: An assessment of sustainable battery technology. Journal of Power Sources,
162 (2), 913  919.
811

fiVytelingum, Voice, Ramchurn, Rogers & Jennings

Exarchakos, L., Leach, M., & Exarchakos, G. (2009). Modelling electricity storage systems
management under the influence of demand-side management programmes. International Journal of Energy Research, 33 (1), 6276.
Galvin, R., & Yeager, K. (2008). Perfect Power: How the MicroGrid Revolution Will Unleash
Cleaner, Greener, More Abundant Energy. McGraw-Hill Professional.
Hammerstrom, D., et al. (2008). Pacific Northwest GridWise Testbed Demonstration
Projects; Part I. Olympic Peninsula Project. Tech. rep., PNNL-17167, Pacific Northwest National Laboratory (PNNL), Richland, WA (US).
Harris, C. (2005). Electricity Markets: Pricing, Structures, and Economics. Wiley.
Holland, A. (2009). Welfare losses in commodity storage games. In Proceedings of The
8th International Conference on Autonomous Agents and Multiagent Systems, pp.
12531254, Budapest.
Houwing, M., Negenborn, R. R., Heijnen, P. W., Schutter, B. D., & Hellendoorn, H. (2007).
Least-cost model predictive control of residential energy resources when applying
chp. In Power Tech, pp. 425430, London, UK.
Jennings, N. R., Corera, J., Laresgoiti, I., Mamdani, E. H., Perriolat, F., Skarek, P., & Varga,
L. Z. (1996). Using ARCHON to develop real-world DAI applications for electricity
transportation management and particle accelerator control. IEEE Expert Systems,
11 (6), 6088.
Kirschen, D., & Strbac, G. (2004). Fundamentals of power system economics. Wiley.
Kok, K., & Venekamp, G. (2010). Market-based control in decentralized power systems.
In Proceedings of the First International Workshop on Agent Technology for Energy
Systems.
Li, H., & Tesfatsion, L. (2009). Development of open source software for power market
research: The AMES test bed. Journal of Energy Markets, 2 (2), 111128.
Lund, H., & Kempton, W. (2008). Integration of renewable energy into the transport and
electricity sectors through V2G. Energy Policy, 36 (9), 35783587.
Ramchurn, S., Vytelingum, P., Rogers, A., & Jennings, N. R. (2011a). Agent-based control
for decentralised demand side management in the smart grid. In Proceedings of the
Tenth International Conference on Autonomous Agents And Multi-Agent Systems, pp.
512.
Ramchurn, S., Vytelingum, P., Rogers, A., & Jennings, N. R. (2011b). Agent-based homeostatic control for green energy in the smart grid. ACM Trans. on Intelligent Systems
and Technology, 2 (4).
Ramchurn, S., Vytelingum, P., Rogers, A., & Jennings, N. R. (2011c). Putting the smarts
into the smart grid:a grand challenge for artificial intelligence. In Communication of
the ACM (to appear).
Rogers, A., & Jennings, N. R. (2010). Intelligent agents for the smart grid. PerAda Magazine, 10.2417/2201005.003002.
Schweppe, F., Tabors, R., Kirtley, J., Outhred, H., Pickel, F., & Cox, A. (1980). Homeostatic
utility control. IEEE Trans. on Power Apparatus and Systems, 99 (3), 1151  1163.
812

fiTheoretical and Practical Foundations of Agent-Based Micro-Storage

Schweppe, F. (1988). Spot pricing of electricity. Kluwer academic publishers.
Shibata, A., & Sato, K. (1999). Development of vanadium redox flow battery for electricity
storage. Power Engineering Journal, 13 (3), 130 135.
Smith, K. (2010). Energy Demand Research Project - Fifth Progress Report. Tech. rep.,
OFGEM, UK. http://www.ofgem.gov.uk/Pages/MoreInformation.aspx?docid\
=19\&refer=sustainability/edrp.
Sovacool, B. K., & Hirsh, R. F. (2009). Beyond batteries: An examination of the benefits
and barriers to plug-in hybrid electric vehicles (phevs) and a vehicle-to-grid (v2g)
transition. Energy Policy, 37 (3), 1095  1103.
Sun, J., & Tesfatsion, L. (2007). DC optimal power flow formulation and solution using
QuadProgJ. In ISU Economics Working Paper No. 06014.
Swider, D. (2007). Compressed air energy storage in an electricity system with significant
wind power generation. IEEE trans. on energy conversion, 22 (1), 95.
US Department Of Energy (2003). Grid 2030: A National Vision For Electricitys Second
100 Years.. http://www.oe.energy.gov/smartgrid.htm.
van Dam, K. H., Houwing, M., & Bouwmans, I. (2008). Agent-based control of distributed
electricity generation with micro combined heat and powercross-sectoral learning for
process and infrastructure engineers. Computers & Chemical Engineering, 32 (1-2),
205  217.
Voice, T. D., Vytelingum, P., Ramchurn, S., Rogers, A., & Jennings, N. R. (2011). Decentralised control of micro-storage in the smart grid. In Proceedings of the 25th
International Conference on AI (AAAI).
Vytelingum, P., Voice, T. D., Ramchurn, S., Rogers, A., & Jennings, N. R. (2010). Agentbased micro-storage management for the smart grid. In Proceedings of the Ninth
International Conference on Autonomous Agents And Multi-Agent Systems, pp. 39
46.
Weibull, J. W. (1995). Evolutionary Game Theory. MIT Press, Cambridge, MA.
Wellman, M., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2004). Price Prediction in
a Trading Agent Competition. Journal of Artificial Intelligence Research, 21, 1936.
Williams, E. (1984). DINORWIG: The Electric Mountain. Central Electricity Generating
Board.
Williams, J., & Wright, B. (1991). Storage and Commodity Markets. UIT, Cambridge.
Ygge, F., Akkermans, J. M., Andersson, A., Krejic, M., & Boertjes, E. (1999). The HOMEBOTS System and Field Test: A Multi-Commodity Market for Predictive Power Load
Management. In Proceedings Fourth International Conference on the Practical Application of Intelligent Agents and Multi-Agent Technology, Vol. 1, pp. 363382.
Ygge, F., & Akkermans, H. (1999). Decentralized markets versus central control: A comparative study. Journal of Artificial Intelligence Research, 11, 301333.

813

fiJournal of Artificial Intelligence Research 42 (2011) 689-718

Submitted 06/11; published 12/11

Combining Evaluation Metrics via the Unanimous
Improvement Ratio and its Application to Clustering Tasks
Enrique Amigo
Julio Gonzalo

enrique@lsi.uned.es
julio@lsi.uned.es

UNED NLP & IR Group, Juan del Rosal 16
Madrid 28040, Spain

Javier Artiles

javier.artiles@qc.cuny.edu

Blender Lab, Queens College (CUNY),
65-30 Kissena Blvd, NY 11367, USA

Felisa Verdejo

felisa@lsi.uned.es

UNED NLP & IR Group, Juan del Rosal 16
Madrid 28040, Spain

Abstract
Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion
and some sort of weighted combination is needed to provide system rankings. A problem of
weighted combination measures is that slight changes in the relative weights may produce
substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria
(such as van Rijsbergens F-measure) and indicates how robust the measured differences
are to changes in the relative weights of the individual metrics. UIR is meant to elucidate
whether a perceived difference between two systems is an artifact of how individual metrics
are weighted.
Besides discussing the theoretical foundations of UIR, this paper presents empirical
results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are
particularly sensitive to the weighting scheme used to combine them. Remarkably, our
experiments show that UIR can be used as a predictor of how well differences between
systems measured on a given test bed will also hold in a different test bed.

1. Introduction
Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion, and
some sort of weighted combination is needed to provide system rankings. Many problems,
for instance, require considering both Precision (P) and Recall (R) to compare systems
performance. Perhaps the most common combining function is the F-measure (van Rijsbergen, 1974), which includes a parameter  that sets the relative weight of metrics; when
 = 0.5, both metrics have the same relative weight and F computes their harmonic mean.
A problem of weighted combination measures is that relative weights are established
intuitively for a given task, but at the same time a slight change in the relative weights may
produce substantial changes in the system rankings. The reason for this behavior is that
an overall improvement in F often derives from an improvement in one of the individual
c
2011
AI Access Foundation. All rights reserved.

fiAmigo, Gonzalo, Artiles & Verdejo

metrics at the expense of a decrement in the other. For instance, if a system A improves a
system B in precision with a loss in recall, F may say that A is better than B or viceversa,
depending on the relative weight of precision and recall (i.e. the  value).
This situation is more common than one might expect. Table 1 shows evaluation results
for different tasks extracted from the ACL 2009 (Su et al., 2009) conference proceedings,
in which P and R are combined using the F-measure. For each paper we have considered
three evaluation results: the one that maximizes F, which is presented as the best result
in the paper, the baseline, and an alternative method that is also considered. Note that in
all cases, the top ranked system improves the baseline according to the F-measure, but at
the cost of decreasing one of the metrics. For instance, in the case of the paper on Word
Alignment, the average R grows from 54.82 to 72.49, while P decreases from 72.76 to 69.19.
In the paper on Sentiment Analysis, P increases in four points but R decreases in five points.
How reasonable is to assume that the contrastive system is indeed improving the baseline?
The evaluation results for the alternative approach are also controversial: in all cases,
the alternative approach improves the best system according to one metric, and it is improved according to the other. Therefore, depending on the relative metric weighting, the
alternative approach could be considered better or worse than the best scored system.
The conclusion is that the  parameter is crucial when comparing real systems. In
practice, however, most authors set  = 0.5 (equal weights for precision and recall) as a
standard, agnostic choice that requires no further justification. Thus, without a notion of
how much a perceived difference between systems depends on the relative weights between
metrics, the interpretation of results with F  or any other combination scheme  can be
misleading.
Our goal is, therefore, to find a way of estimating to what extent a perceived difference
using a metric combination scheme  F or any other  is robust to changes in the relative
weights assigned to each individual metric.
In this paper we propose a novel measure, the Unanimity Improvement Ratio (UIR),
which relies on a simple observation: when a system A improves other system B according to
all individual metrics (the improvement is unanimous), A is better than B for any weighting
scheme. Given a test collection with n test cases, the more test cases where improvements
are unanimous, the more robust the perceived difference (average difference in F or any
other combination scheme) will be.
In other words, as well as statistical significance tests provide information about the
robustness of the evaluation across test cases (Is the perceived difference between two systems
an artifact of the set of test cases used in the test collection? ), UIR is meant to provide
information about the robustness of the evaluation across variations of the relative metric
weightings (Is the perceived difference between two systems an artifact of the relative metric
weighting chosen in the evaluation metric? ).
Our experiments on clustering test collections show that UIR contributes to the analysis
of evaluation results in two ways:
 It allows to detect system improvements that are biased by the metric weighting
scheme. In such cases, experimenters should carefully justify a particular choice of
relative weights and check whether results are swapped in their vicinity.
690

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

Systems
Precision Recall
F
Task: Word alignment (Huang 2009)
Baseline
BM
72.76
54.82 62.53
Max. F
Link-Select
69.19
72.49 70.81
Alternative
ME
72.66
66.17 69.26
Task: Opinion Question Answering (Li et al. 2009)
Baseline
System 3
10.9
21.6
17.2
Max. F
OpHit
10.2
25.6
20.5
Alternative
OpPageRank
10.9
24.2
20
Task: Sentiment Analysis (Kim et al 2009)
Baseline
BASELINE
30.5
86.6
45.1
Max. F
VS-LSA-DTP
34.9
71.9
47
Alternative
VS PMI
31.1
83.3
45.3
Task: Lexical Reference Rule Extraction (Shnarch et al 2009)
Baseline
No expansion
54
19
28
Max. F
Wordnet+Wiki
47
35
40
Alternative All rules + Dice filter
49
31
38
Table 1: A few three-way system comparisons taken from ACL 2009 Conference Proceedings (Su et al., 2009)

 It increases substantially the consistency of evaluation results across datasets: a result
that is supported by a high Unanimous Improvement Ratio is much more likely to hold
in a different test collection. This is, perhaps, the most relevant practical application
of UIR: as a predictor of how much a result can be replicable across test collections.
Although most of the work presented in this paper applies to other research areas, here
we will focus on the clustering task as one of the most relevant examples because clustering
tasks are specially sensitive to the metric relative weightings. Our research goals are:
1. To investigate empirically whether clustering evaluation can be biased by precision
and recall relative weights in F. We will use one of the most recent test collections
focused on a text clustering problem (Artiles, Gonzalo, & Sekine, 2009).
2. To introduce a measure that quantifies the robustness of evaluation results across
metric combining criteria, which leads us to propose the UIR measure, which is derived
from the Conjoint Measurement Theory (Luce & Tukey, 1964).
3. To analyze empirically how UIR and F-measure complement each other.
4. To illustrate the application of UIR when comparing systems in the context of a shared
task, and measure how UIR serves as a predictor of the consistency of evaluation
results across different test collections.

691

fiAmigo, Gonzalo, Artiles & Verdejo

Figure 1: Evaluation results for semantic labeling in CoNLL 2004

2. Combining Functions for Evaluation Metrics
In this section we briefly review different metrics combination criteria. We present the
rationale behind each metric weighting approach as well as its effects on the systems ranking.
2.1 F-measure
The most frequent way of combining two evaluation metrics is the F-measure (van Rijsbergen, 1974). It was originally proposed for the evaluation of Information Retrieval systems
(IR), but its use has expanded to many other tasks. Given two metrics P and R (e.g.
precision and recall, Purity and Inverse Purity, etc.), van Rijsbergens F-measure combines
them into a single measure of efficiency as follows:
F (R, P ) =

( P1 )

1
+ (1  )( R1 )

F assumes that the  value is set for a particular evaluation scenario. This parameter
represents the relative weight of metrics. In some cases the  value is not crucial; in
particular, when metrics are correlated. For instance, Figure 1 shows the precision and
recall levels obtained at the CoNLL-2004 shared task for evaluating Semantic Role Labeling
systems (Carreras & Marquez, 2004). Except for one system, every substantial improvement
in precision involves also an increase in recall. In this case, the relative metric weighting
does not substantially modify the system ranking.
In cases where the metrics are not completely correlated, the Decreasing Marginal Effectiveness property (van Rijsbergen, 1974) ensures a certain robustness across  values. F
satisfies this property, which states that a large decrease in one metric cannot be compensated by a large increase in the other metric. Therefore, systems with very low precision
or recall will obtain low F-values for any  value. This is discussed in more detail in Section 5.1. However, as we will show in Section 3.4, in other cases the Decreasing Marginal
692

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

Effectiveness property does not prevent the F-measure from being overly sensitive to small
changes in the  value.

Figure 2: An example of two systems evaluated by break-even point

2.2 Precision and Recall Break-even Point
Another way of combining metrics consists of evaluating the system at the point where one
metric equals the other (Tao Li & Zhu, 2006). This method is applicable when each system
is represented by a trade-off between both metrics, for instance, a precision/recall curve.
This method relies on the idea that increasing both metrics implies necessarily a overall
quality increase. For instance, it assumes that obtaining a 0.4 of precision at the recall
point 0.4 is better than obtaining a 0.3 of precision at the recall point 0.3.
Actually, the break-even point assumes the same relevance for both metrics. It considers
the precision/recall point where the system distributes its efforts equitably between both
metrics. Indeed, we could change the relative relevance of metrics when computing the
break-even point.
Figure 2 illustrates this idea. The continuous curve represents the trade-off between
precision and recall for system S1. The straight diagonal represents the points where both
metrics return the same score. The quality of the system corresponds therefore with the
intersection between this diagonal and the precision/recall curve. On the other hand, the
discontinuous curve represents another system S2 which achieves an increase of precision
at low recall levels at the cost of decreasing precision at high recall levels. According to the
break-even points, the second system is superior than the first one.
However, we could give more relevance to recall identifying the point where recall doubles
precision. In that case, we would obtain the intersection points Q01 and Q02 shown in the
figure, which reverses the quality order between systems. In conclusion, the break-even
point also assumes an arbitrary relative relevance for the combined metrics.
2.3 Area Under the Precision/Recall Curve
Some approaches average scores over every potential parameterization of the metric combining function. For instance, Mean Average Precision (MAP) is oriented to IR systems,
693

fiAmigo, Gonzalo, Artiles & Verdejo

and computes the average precision across a number of recall levels. Another example is
the Receiver Operating Characteristic (ROC) function used to evaluate binary classifiers
(Cormack & Lynam, 2005). ROC computes the probability that a positive sample receives
a confidence score higher than a negative sample, independently from the threshold used to
classify the samples. Both functions are related with the area AUC that exists under the
precision/recall curve (Cormack & Lynam, 2005).
In both MAP and ROC the low and high recall regions have the same relative relevance
when computing this area. Again, we could change the measures in order to assign different
weights to high and low recall levels. Indeed in (Weng & Poon, 2008) a weighted Area Under
the Curve is proposed. Something similar would happen if we average F across different 
values.
Note that these measures can only be applied in certain kinds of problem, such as binary
classification or document retrieval, where the system output can be seen as a ranking, and
different cutoff points in the ranking give different precision/recall values. They are not
directly applicable, in particular, to the clustering problem which is the focus of our work
here.

3. Combining Metrics in Clustering Tasks
In this section we present metric combination experiments on a specific clustering task. Our
results corroborate the importance of quantifying the robustness of systems across different
weighting schemes.
3.1 The Clustering Task
Clustering (grouping similar items) has applications in a wide range of Artificial Intelligence
problems. In particular, in the context of textual information access, clustering algorithms
are employed for Information Retrieval (clustering text documents according to their content
similarity), document summarization (grouping pieces of text in order to detect redundant
information), topic tracking, opinion mining (e.g. grouping opinions about a specific topic),
etc.
In such scenarios, clustering distributions produced by systems are usually evaluated
according to their similarity to a manually produced gold standard (extrinsic evaluation).
There is a wide set of metrics that measure this similarity (Amigo, Gonzalo, Artiles, &
Verdejo, 2008), but all of them rely on two quality dimensions: (i) to what extent items
in the same cluster also belong to the same group in the gold standard; and (ii) to what
extent items in different clusters also belong to different groups in the gold standard. A
wide set of extrinsic metrics has been proposed: Entropy and Class Entropy (Steinbach,
Karypis, & Kumar, 2000; Ghosh, 2003), Purity and Inverse Purity (Zhao & Karypis, 2001),
precision and recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based on counting
pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.1

1. See the work of Amigo et al. (2008) for a detailed overview.

694

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

3.2 Dataset
WePS (Web People Search) campaigns are focused on the task of disambiguating person
names in Web search results. The input for systems is a ranked list of web pages retrieved
from a Web search engine using a person name as a query (e.g. John Smith). The
challenge is to correctly estimate the number of different people sharing the name in the
search results and group documents referring to the same individual. For every person
name, WePS datasets provide around 100 web pages from the top search results, using the
quoted person name as query. In order to provide different ambiguity scenarios, person
names were sampled from the US Census, Wikipedia, and listings of Program Committee
members of Computer Science Conferences.
Systems are evaluated comparing their output with a gold standard: a manual grouping
of documents produced by two human judges in two rounds (first they annotated the corpus
independently and then they discussed the disagreements together). Note that a single
document can be assigned to more than one cluster: an Amazon search results list, for
instance, may refer to books written by different authors with the same name. The WePS
task is, therefore, an overlapping clustering problem, a more general case of clustering where
items are not restricted to belong to one single cluster. Both the WePS datasets and the
official evaluation metrics reflect this fact.
For our experiments we have focused on the evaluation results obtained in the WePS-1
(Artiles, Gonzalo, & Sekine, 2007) and WePS-2 (Artiles et al., 2009) evaluation campaigns.
The WePS-1 corpus also includes data from the Web03 test bed (Mann, 2006), which was
used for trial purposes and follows similar annotation guidelines, although the number of
document per ambiguous name is more variable. We will refer to these corpora as WePS-1a
(trial), WePS-1b and WePS-2 2 .
3.3 Thresholds and Stopping Criteria
The clustering task involves three main aspects that determine the systems output quality.
The first one is the method used for measuring similarity between documents; the second is
the clustering algorithm (k-neighbors, Hierarchical Agglomerative Clustering, etc.); and the
third aspect to be considered usually consists of a couple of related variables to be fixed: a
similarity threshold  above which two pages will be considered as related  and a stopping
criterion which determines when the clustering process stops and, consequently, the number
of clusters produced by the system.
Figure 3 shows how Purity and Inverse Purity values change for different clustering
stopping points, for one of the systems evaluated on the WePS-1b corpus 3 . Purity focuses
on the frequency of the most common category into each cluster (Amigo et al., 2008). Being
C the set of clusters to be evaluated, L the set of categories (reference distribution) and
2. The WePS datasets were selected for our experiments because (i) they address a relevant and well-defined
clustering task; (ii) its use is widespread: WePS datasets have been used in hundreds of experiments
since the first WePS evaluation in 2007; (iii) runs submitted by participants to WePS-1 and WePS-2 were
available to us, which was essential to experiment with different evaluation measures. WePS datasets
are freely available from http://nlp.uned.es/weps.
3. This system is based on bag of words, TF/IDF word weighting, stopword removal, cosine distance and
a Hierarchical Agglomerative Clustering algorithm.

695

fiAmigo, Gonzalo, Artiles & Verdejo

N the number of clustered items, Purity is computed by taking the weighted average of
maximal precision values:
Purity =

X |Ci |
i

N

maxj Precision(Ci , Lj )

where the precision of a cluster Ci for a given category Lj is defined as:
|Ci Lj |
Precision(Ci , Lj ) =
|Ci |
T

Purity penalizes the noise in a cluster, but it does not reward grouping items from
the same category together; if we simply make one cluster per item, we reach trivially a
maximum purity value. Inverse Purity focuses on the cluster with maximum recall for each
category. Inverse Purity is defined as:
Inverse Purity =

X |Li |
i

N

maxj Precision(Li , Cj )

Inverse Purity rewards grouping items together, but it does not penalize mixing items from
different categories; we can reach a maximum value for Inverse purity by making a single
cluster with all items.
Any change in the stopping point implies an increase in Purity at the cost of a decrease in
Inverse Purity, or viceversa. Therefore, each possible  value in F rewards different stopping
points. This phenomenon produces a high dependency between clustering evaluation results
and the metric combining function.

Figure 3: An example of the trade-off between Purity and Inverse Purity when optimizing
the grouping threshold

696

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

3.4 Robustness Across  Values
Determining the appropriate  value for a given scenario is not trivial. For instance, from a
users point of view in the WePS task, it is easier to discard a few irrelevant documents from
the good cluster (because its precision is not perfect but it has a high recall) than having
to check for additional relevant documents in all clusters (because its precision is high but
its recall is not). Therefore, it seems that Inverse Purity should have priority over Purity,
i.e., the value of  should be below 0.5. From the point of view of a company providing a
web people search service, however, the situation is quite different: their priority is having
a very high precision, because mixing the profiles of, say, a criminal and a doctor may result
in the company being sued. From their perspective,  should receive a high value. The
WePS campaign decided to be agnostic and set a neutral  = 0.5 value.
Table 2 shows the resulting system ranking in WePS-1b according to F with  set at 0.5
and 0.2. This ranking includes two baseline systems: B1 consists of grouping each document
in a separate cluster, and B100 consists of grouping all documents into one single cluster.
B1 maximizes Purity, and B100 maximizes Inverse Purity.
B1 and B100 may obtain a high or low F-measure depending on the  value. As the
table shows, for  = 0.5 B1 outperforms B100 and also a considerable number of systems.
The reason for this result is that, in the WePS-1b test set, there are many singleton clusters
(people which are referred to in only one web page). This means that a default strategy
of making one cluster per document will not only achieve maximal Purity, but also an
acceptable Inverse Purity (0.45). However, if  is fixed at 0.2, B1 goes down to the bottom
of the ranking and it is outperformed by all systems, including the other baseline B100 .
Note that outperforming a trivial baseline system such as B1 is crucial to optimize
systems, given that the optimization cycle could otherwise lead to a baseline approach like
B1 . The drawback of B1 is that it is not informative (the output does not depend on the
input) and, crucially, it is very sensitive to variations in . In other words, its performance
is not robust to changes in the metric combination criterion. Remarkably, the top scoring
system, S1 , is the best for both  values. Our primary motivation in this article is to
quantify the robustness across  values in order to complement the information given by
traditional system ranking.
3.5 Robustness Across Test Beds
The average size of the clusters in the gold standard may change from one test bed to
another. As this affects the Purity and Inverse Purity trade-off, the same clustering system
may obtain a different balance between both metrics in different corpora; and this may
produce contradictory evaluation results when comparing systems across different corpora,
even for the same  value.
For instance, in the WePS-1b test bed (Artiles et al., 2007), B1 substantially outperforms
B100 (0.58 vs. 0.49 using F=0.5 ). In the WePS-2 data set (Artiles et al., 2009), however,
B100 outperforms B1 (0.53 versus 0.34). The reason is that singletons are less common in
WePS-2. In other words, the comparison between B100 and B1 depends both on the  value
and of the particular distribution of reference cluster sizes in the test bed.
Our point is that system improvements that are robust across  values (which is not the
case of B1 and B100 ) should not be affected by this phenomenon. Therefore, estimating the
697

fiAmigo, Gonzalo, Artiles & Verdejo

F=0.5
Ranked systems F result
S1
0.78
S2
0.75
S3
0.75
S4
0.67
S5
0.66
S6
0.65
S7
0.62
B1
0.61
S8
0.61
S9
0.58
S10
0.58
S11
0.57
S12
0.53
S13
0.49
S14
0.49
S15
0.48
B100
0.4
S16
0.4

F0.2
Ranked systems
S1
S3
S2
S6
S5
S8
S11
S7
S14
S15
S12
S9
S13
S4
S10
B100
S16
B1

F result
0.83
0.78
0.77
0.76
0.73
0.73
0.71
0.67
0.66
0.66
0.65
0.64
0.63
0.62
0.6
0.58
0.56
0.49

Table 2: WePS-1b system ranking according to F=0.5 vs F=0.2 using Purity and Inverse
Purity

robustness of system improvements to changes in  should prevent reaching contradictory
results for different test beds. Indeed, evidence for this is presented in Section 7.

4. Proposal
Our primary motivation in this article is to quantify the robustness across  values in
order to complement the information given by traditional system rankings. To this end we
introduce in this section the Unanimous Improvement Ratio.
4.1 Unanimous Improvements
The problem of combining evaluation metrics is closely related with the theory of conjoint
measurement (see Section 5.1 for a detailed discussion). Van Rijsbergen (1974) argued that
it is not possible to determine empirically which metric combining function is the most
adequate in the context of Information Retrieval evaluation. However, starting from the
measurement theory principles, van Rijsbergen described the set of properties that a metric
combining function should satisfy. This set includes the Independence axiom (also called
Single Cancellation), from which the Monotonicity property derives. The Monotonicity
property states that the quality of a system that surpasses or equals another one according
to all metrics is necessarily equal or better than the other. In other words, one system is
698

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

better than the other with no dependence whatsoever on how the relative importance of
each metric is set.
We will define a combination procedure for metrics, Unanimous Improvement, which is
based on this property:
QX (a)  QX (b) if and only if x  X.Qx (a)  Qx (b)
where QX (a) is the quality of a according to a set of metrics X.
This relationship has no dependence on how metrics are scaled or weighted, or on their
degree of correlation in the metric set. Equality (= ) can be derived directly from  : The
unanimous equality implies that both systems obtain the same score for all metrics:
QX (a) = QX (b)  (QX (a)  QX (b))  (QX (b)  QX (a))

The strict unanimous improvement implies that one system improves the other strictly
at least for one metric, and it is not improved according to any metric:
QX (a) > QX (b)  (QX (a)  QX (b))  (QX (a) = QX (b)) 
(QX (a)  QX (b))  (QX (b)  QX (a))

Non comparability k can also derived from here: it occurs when some metrics favor one
system and some other metrics favor the other. We refer to this cases as metric-biased
improvements.
QX (a)k QX (b)  (QX (a)  QX (b))  (QX (b)  QX (a))

The theoretical properties of the Unanimous Improvement are described in depth in
Section 5.2. The most important property is that the Unanimous Improvement is the only
relational structure that does not depend on relative metric weightings, while satisfying
the Independence (Monotonicity) axiom. In other words, we can claim that: A system improvement according to a metric combining function does not depend whatsoever on metric
weightings if and only if there is no quality decrease according to any individual metric. The
theoretical justification of this assertion is developed in Section 5.2.1.
4.2 Unanimous Improvement Ratio
According to the Unanimous Improvement, our unique observable over each test case is a
three-valued function (unanimous improvement, equality or biased improvement). We need,
however, a way of quantitatively comparing systems.
Given two systems, a and b, and the Unanimous Improvement relationship over a set of
test cases T , we have samples where a improves b (QX (a)  QX (b)), samples where b improves a (QX (b)  QX (a)) and also samples with biased improvements (QX (a)k QX (b)).
We will refer to these sets as Ta b , Tb a and Tak b , respectively. The Unanimous Improvement Ratio (UIR) is defined according to three formal restrictions:
699

fiAmigo, Gonzalo, Artiles & Verdejo

Test cases  T
1
2
3
4
5
6
7
8
9
10

Precision
System A System B
0.5
0.5
0.5
0.5
0.5
0.4
0.6
0.6
0.7
0.6
0.3
0.1
0.4
0.5
0.4
0.6
0.3
0.1
0.2
0.4

Recall
System A System B
0.5
0.5
0.2
0.2
0.2
0.2
0.4
0.3
0.5
0.4
0.5
0.4
0.5
0.6
0.5
0.6
0.5
0.6
0.5
0.3

A  B
YES
YES
YES
YES
YES
YES
NO
NO
NO
NO

B  A
YES
YES
NO
NO
NO
NO
YES
YES
NO
NO

Table 3: Example of experiment input to compute UIR
1. UIR(a, b) should decrease with the number of biased improvements (Tak b ). In the
boundary condition where all samples are biased improvements (Tak b = T ), then
UIR(a, b) should be 0.
2. If a improves b as much as b improves a (Ta b = Tb a ) then UIR(a, b) = 0.
3. Given a fixed number of biased improvements (Tak b ), UIR(a, b) should be proportional
to Ta b and inversely proportional to Tb a .
Given these restrictions, we propose the following UIR definition:
UIRX,T (a, b) =

|Ta b |  |Tb a |
=
|T |

|t  T /QX (a)  QX (b)|  |t  T /QX (b)  QX (a)|
|T |
which can be alternatively formulated as:
UIRX,T (a, b) = P(a  b)  P(b  a)
where these probabilities are estimated in a frequentist manner.
UIR range is [1, 1] and is not symmetric: UIRX,T (a, b) = UIRX,T (b, a). As an
illustration of how UIR is computed, consider the experiment outcome in Table 3. Systems
A and B are compared in terms of precision and recall for 10 test cases. For test case 5, for
instance, A has an unanimous improvement over B: it is better both in terms of precision
(0.7 > 0.6) and recall (0.5 > 0.4). From the table, UIR value is:
UIRX,T (A, B) =

|TA B |  |TB A |
64
=
= 0.2 = UIRX,T (B, A)
|T |
10

UIR has two formal limitations. First, it is not transitive (see Section 5.2). Therefore,
it is not possible to define a linear system ranking based on UIR. This is, however, not
700

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

necessary: UIR is not meant to provide a ranking, but to complement the ranking provided
by the F-measure (or other metric combining function), indicating how robust results are to
changes in . Section 6.4 illustrates how UIR can be integrated with the insights provided
by a system ranking.
The second limitation is that UIR does not consider improvement ranges; therefore,
it is less sensitive than the F-measure. Our empirical results, however, show that UIR is
sensitive enough to discriminate robust improvements versus metric-biased improvements;
and in Section 8 we make an empirical comparison of our non-parametric definition of UIR
with a parametric version, with results that make the non-parametric definition preferable.

5. Theoretical Foundations
In this section we discuss the theoretical foundations of the Unanimous Improvement Ratio
in the framework of the Conjoint Measurement Theory. Then we proceed to describe the
formal properties of UIR and their implications from the point of view of the evaluation
methodology. Readers interested solely in the practical implications of using UIR may
proceed directly to Section 6.
5.1 Conjoint Measurement Theory
The problem of combining evaluation metrics is closely related with the Conjoint Measurement Theory, which was independently discovered by the economist Debreu (1959) and the
mathematical psychologist R. Duncan Luce and statistician John Tukey (Luce & Tukey,
1964). The Theory of Measurement defines the necessary conditions to state an homomorphism between an empirical relational structure (e.g. John is bigger than Bill) and a
numeric relational structure (Johns height is 1.79 meters and Bills height is 1.56 meters).
In the case of the Conjoint Measurement Theory, the relational structure is factored into
two (or more) ordered substructures (e.g. height and weight).
In our context, the numerical structures are given by the evaluation metric scores (e.g.
Purity and Inverse Purity). However, we do not have an empirical quality ordering for
clustering systems. Different human assessors could assign more relevance to Purity than
to Inverse Purity or viceversa. Nevertheless, the Conjoint Measurement Theory does provide
mechanisms that state what kind of numerical structures can produce an homomorphism
assuming that the empirical structure satisfies certain axioms. Van Rijsbergen (1974) used
this idea to analyze the problem of combining evaluation metrics. These axioms shape an
additive conjoint structure. Being (R, P ) the quality of a system according to two evaluation
metrics R and P , these axioms are:
Connectedness: All systems should be comparable to each other. Formally: (R, P ) 
(R0 , P 0 ) or (R0 , P 0 )  (R, P ).
Transitivity: (R, P )  (R0 , P 0 ) and (R0 , P 0 )  (R00 , P 00 ) implies that (R, P )  (R00 , P 00 ).
The axioms Transitivity and Connectedness shape a weak order.
Thomsen condition: (R1 , P3 )  (R3 , P2 ) and (R3 , P1 )  (R2 , P3 ) imply that (R1 , P1 ) 
(R2 , P2 ) (where  indicates equal effectiveness).
701

fiAmigo, Gonzalo, Artiles & Verdejo

Independence: The two components contribute their effects independently to the effectiveness. Formally, (R1 , P )  (R2 , P ) implies that (R1 , P 0 )  (R2 , P 0 ) for all P 0 , and
(R, P1 )  (R, P2 ) implies that (R0 , P2 )  (R0 , P2 ) for all R0 . This property implies
Monotonicity (Narens & Luce, 1986) which states that an improvement in both metrics necessarily produces an improvement according to the metric combining function.
Restricted Solvability: A property which is ... concerned with the continuity of each
component. It makes precise what intuitively we would expect when considering the
existence of intermediate levels. Formally: whenever (R1 , P 0 )  (R, P )  (R2 , P 0 )
then exists R such that (R0 , P 0 ) = (R, P ).
Essential Components: Variation in one while leaving the other constant gives a variation in effectiveness. There exists R, R0 and P such that it is not the case that
(R, P ) = (R0 , P ); and there exists P , P 0 and R such that it is not the case that
(R, P ) = (R, P 0 ).
Archimedean Property: which merely ensures that the intervals on a component are
comparable.
The F-measure proposed by van Rijsbergen (1974) and the arithmetic mean of P,R
satisfy all these axioms. According to these restrictions, indeed, an unlimited set of acceptable combining functions for evaluation metrics can be defined. The F relational structure,
however, satisfies another property which is not satisfied by other functions such as the
arithmetic mean. This property is the Decreasing Marginal Effectiveness. The basic idea
is that increasing one unit in one metric and decreasing one unit in the other metric can
improve the overall quality (i.e. if the first metric has more weight in the combining function), but this does not imply that a great loss in one metric can be compensated by a great
increase in the other. It can be defined as:
R, P > 0, n > 0 such that ((P + n, R  n) < (R, P ))
According to this, high values in both metrics are required to obtain a high overall
improvement. This makes measures observing this property - such as F - more robust to
arbitrary metric weightings.
5.2 Formal Properties of the Unanimous Improvement
The Unanimous Improvement x trivially satisfies most of the desirable properties proposed by van Rijsbergen (1974) for metric combining functions: transitivity, independence,
Thomsen condition, Restricted Solvability, Essential Components and Decreasing Marginal
Effectiveness; the exception being the connectedness property4 . Given that the non comparability k (biased improvements, see Section 4.1) is derived from the Unanimous Improvement, it is possible to find system pairs where neither QX (a)  QX (b) nor QX (b)  QX (a)
hold. Therefore, Connectedness is not satisfied.
Formally, the limitation of the Unanimous Improvement is that it does not represent a
weak order, because it cannot satisfy Transitivity and Connectedness simultaneously. Let
us elaborate on this issue.
4. For the sake of simplicity, we consider here the combination of two metrics (R, P ).

702

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

Systems
A
B
C

Metric x1
0.5
0.6
0.45

Metric x2
0.5
0.4
0.45

Table 4: A counter sample for Transitivity in Unanimous Improvement
We could satisfy Connectedness by considering that biased improvements represent
equivalent system pairs (= ). But in this case, Transitivity would not be satisfied. See, for
instance, Table 4. According to the table:
QX (B)k QX (A) and QX (C)k QX (B)
Therefore, considering that k represents equivalence, we have:
QX (B)  QX (A) and QX (C)  QX (B)
but not
QX (C)  QX (A)
In summary, we can choose to satisfy transitivity or connectedness, but not both: the
Unanimous Improvement can not derive a weak order.
5.2.1 Uniqueness of the Unanimous Improvement
The Unanimous Improvement has the interesting property that is does not contradict any
evaluation result given by the F-measure, regardless of the  value used in F:
QX (a)  QX (b)  F (a)  F (b)
This is due to the fact that the F-measure (for any  value) satisfies the monotonicity
axiom, in which the Unanimous Improvement is grounded. This property is essential for
the purpose of checking the robustness of system improvements across  values. And
crucially, the Unanimous Improvement is the only function that satisfies this property. More
precisely, the Unanimous Improvement is the only relational structure that, while satisfying
monotonicity, does not contradict any Additive Conjoint Structure (see Section 5.1).
In order to prove this assertion, we need to define the concept of compatibility with any
additive conjoint structure. Let add be any additive conjoint structure and let R be any
relational structure. We will say that R is compatible with any conjoint structure if and
only if:
ha, b, add i.(QX (a) R QX (b))  (QX (a) add QX (b))
In other words: if R holds, then any other additive conjoint holds. We want to prove
that the unanimous improvement is the only relation that satisfies this property; therefore,
we have to prove that if R is a monotonic and compatible relational structure, then it
necessarily matches the unanimous improvement definition:
703

fiAmigo, Gonzalo, Artiles & Verdejo

R is monotonic and compatible = (QX (a) R QX (b)  xi (a)  xi (b)xi  X)
which can be split in:
(1) R monotonic and compatible = (QX (a) R QX (b)  xi (a)  xi (b)xi  X)
(2) R monotonic and compatible = (QX (a) R QX (b)  xi (a)  xi (b)xi  X)
Proving (1) is immediate, since the rightmost component corresponds with the monotonicity property definition. Let us prove (2) by reductio ad absurdum, assuming that there
exists a relational structure o such that:
(o monotonic and compatible)  (QX (a) o QX (b))  (xi  X.xi (a) < xi (b))
In this case, we could define an additive conjoint structure over the combined measure
Q0X (a) = 1 x1 (a)+..i xi (a)..+n xn (a) with i big enough such that Q0X (a) < Q0X (b). The
Q0 additive conjoint structure would contradict o . Therefore, o would not be compatible
(contradiction). In conclusion, predicate (2) is true and the Unanimous Improvement X
is the only monotonic and compatible relational structure.
An interesting corollary can be derived from this analysis. If the Unanimous Improvement is the only compatible relational structure, then we can formally conclude that the
measurement of system improvements without dependence on metric weighting schemes can
not derive a weak order (i.e. one that satisfies both transitivity and connectedness). This
corollary has practical implications: it is not possible to establish a system ranking which is
independent on metric weighting schemes.
A natural way to proceed is, therefore, to use the unanimous improvement as an addition
to the standard F-measure (for a suitable  value) which provides additional information
about the robustness of system improvements across  values.

6. F versus UIR: Empirical Study
In this Section we perform a number of empirical studies on the WePS corpora in order to
find out how UIR behaves in practice. First, we focus on a number of empirical results that
show how UIR rewards robustness across  values, and how this information is complementary to the information provided by F. Second, we examine to what extent  and why  F
and UIR are correlated.
6.1 UIR: Rewarding Robustness
Figure 4 shows three examples of system comparisons in WePS-1b corpus using the metrics
Purity and Inverse Purity. Each curve represents the F value obtained for one system
according to different  values. System S6 (black curves) is compared with S10, S9 and
S11 (grey curves) in each of the three graphs. In all cases there is a similar quality increase
according to F=0.5 ; UIR, however, ranges between 0.32 and 0.42, depending on how robust
the difference is to changes in . The highest difference in UIR is for the (S6,S11) system
pair (rightmost graph), because these systems do not swap their F values for any  value.
704

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

| 4 F=0.5 |
|UIR|

Improvements
for all 
(28 system pairs)
0.12
0.53

Other cases
(125 system pairs)
0.13
0.14

Table 5: UIR and F=0.5 increase when F increases for all  values

Figure 4: F-measure vs. UIR: rewarding robustness
The smallest UIR value is for (S6,S10), where S6 is better than S10 for  values below
0.8, and worse when  is larger. This comparison illustrates how UIR captures, for similar
increments in F, which ones are less dependent of the relative weighting scheme between
precision and recall.
Let us now consider all two-system combinations in the WePS-1b corpus, dividing them
in two sets: (i) system pairs for which F increases for all  values (i.e. both Purity and
Inverse Purity increases), and (ii) pairs for which the relative systems performance swaps
at some  value; i.e. F increases for some  values and decreases for the rest.
One would expect that the average increase in F should be larger for those system pairs
where one beats the other for every  value. Surprisingly, this is not true: Table 5 shows the
average increments for UIR and F=0.5 for both sets. UIR behaves as expected: its average
value is substantially larger for the set where different  do not lead to contradictory results
(0.53 vs. 0.14). But the average relative increase of F=0.5 , however, is very similar in both
sets (0.12 vs. 0.13).
The conclusion is that a certain F=0.5 improvement range does not say anything about
whether both Purity and Inverse Purity are being simultaneously improved or not. In other
words: no matter how large is a measured improvement in F is, it can still be extremely
dependent on how we are weighting the individual metrics in that measurement.
This conclusion can be corroborated by considering independently both metrics (Purity and Inverse Purity). According to the statistical significance of the improvements for
independent metrics, we can distinguish three cases:
1. Opposite significant improvements: One of the metrics (Purity or Inverse Purity)
increases and the other decreases, and both changes are statistically significant.
705

fiAmigo, Gonzalo, Artiles & Verdejo

| 4 F=0.5 |
|UIR|

Significant
concordant
improvements
53 pairs
0.11
0.42

Significant
opposite
improvements
89 pairs
0.15
0.08

Non
significant
improvements
11 pairs
0.05
0.027

Table 6: UIR and F=0.5 increases vs. statistical significance tests
2. Concordant significant improvements: Both metrics improve significantly or at least
one improves significantly and the other does not decrease significantly.
3. Non-significant improvements: There is no statistically significant differences between
both systems for any metric.
We use the Wilcoxon test with p < 0.05 to detect statistical significance. Table 6
shows the average UIR and | 4 F=0.5 | values in each of the three cases. Remarkably, the
F=0.5 average increase is even larger for the opposite improvements set (0.15) than for
the concordant improvements set (0.11). According to these results, it would seem that
F=0.5 rewards individual metric improvements which are obtained at the cost of (smaller)
decreases in the other metric. UIR, on the other hand, has a sharply different behavior,
strongly rewarding the concordant improvements set (0.42 versus 0.08).
All these results confirm that UIR provides essential information about the experimental
outcome of two-system comparisons, which is not provided by the main evaluation metric
F .
6.2 Correlation Between F and UIR
The fact that UIR and F offer different information about the outcome of an experiment
does not imply that UIR and F are orthogonal; in fact, there is some correlation between
both values.
Figure 5 represents F=0.5 differences and UIR values for each possible system pair in the
WePS-1 test bed. The general trends are (i) high UIR values imply a positive difference in
F (ii) high |4F0,.5 | values do not imply anything on UIR values; (iii) low UIR do not seem
to imply anything on |4F0,.5 | values. Overall, the figure suggest a triangle relationship,
which gives a Pearson correlation of 0.58.
6.2.1 Reflecting improvement ranges
When there is a consistent difference between two systems for most  values, UIR rewards
larger improvement ranges. Let us illustrate this behavior considering three sample system
pairs taken from the WePS-1 test bed.
Figure 6 represents the F[0,1] values for three system pairs. In all cases, one system
improves the other for all  values. However, UIR assigns higher values to larger improvements in F (larger distance between the black and the grey curves). The reason is that a
706

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

Figure 5: |4F0,.5 | vs UIR

Figure 6: F vs. UIR: reflecting improvement ranges
larger average improvement over test cases makes less likely the cases where individual test
cases (which are the ones that UIR considers) contradict the average result.
Another interesting finding is that, when both metrics are improved, the metric that
has the weakest improvement determines the behavior of UIR. Figure 7 illustrates this
relationship for the ten system pairs with a largest improvement; the Pearson correlation in
this graph is 0.94. In other words, when both individual metrics improve, UIR is sensitive
to the weakest improvement.
6.2.2 Analysis of boundary cases
In order to have a better understanding of the relationship between UIR and F, we will now
examine in detail two cases of system improvements in which UIR and F produce drastically
different results. These two cases are marked as A and B in Figure 5.
The point marked as case A in the Figure corresponds with the comparison of systems
S1 and S15 . There exists a substantial (and statistically significant) difference between both
systems according to F=0.5 . However, UIR has a low value, i.e., the improvement is not
robust to changes in  according to UIR.
707

fiAmigo, Gonzalo, Artiles & Verdejo

Figure 7: Correlation between UIR and the weakest single metric improvement.

Figure 8: Purity and Inverse Purity per test case, systems S1 and S1 5
A visual explanation of these results can be seen in Figure 8. It shows the Purity and
Inverse Purity results of systems S1 , S15 for every test case. In most test cases, S1 has
an important advantage in Purity at the cost of a slight  but consistent  loss in Inverse
Purity. Given that F=0.5 compares Purity and Inverse Purity ranges, it states that there
exists an important and statistically significant improvement from S15 to S1 . However, the
slight but consistent decrease in Inverse Purity affects UIR, which decreases because in most
test cases the improvements in F are metric biased (k in our notation).
Case B (see Figure 9) is the opposite example: there is a small difference between systems
S8 and S12 according to F=0.5 , because differences in both Purity and Inverse Purity are
also small. S8, however, gives small but consistent improvements both for Purity and Inverse
Purity (all test cases to the right of the vertical line in the figure); these are unanimous
improvements. Therefore, UIR considers that there exists a robust overall improvement in
this case.
708

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

Figure 9: Purity and Inverse Purity per test case, systems S1 2 and S8
Again, both cases show how UIR gives additional valuable information on the comparative behavior of systems.
6.3 A Significance Threshold for UIR
We mentioned earlier that UIR has a parallelism with statistical significance tests, which
are typically used in Information Retrieval to estimate the probability p that an observed
difference between two systems is obtained by chance, i.e., the difference is an artifact of
the test collection rather than a true difference between the systems. When computing
statistical significance, it is useful to establish a threshold that allows for a binary decision;
for instance, a result is often said to be statistically significant if p < 0.05, and not significant
otherwise. Choosing the level of significance is arbitrary, but it nevertheless helps reporting
and summarizing significance tests. Stricter thresholds increase confidence of the test, but
run an increased risk of failing to detect a significant result.
The same situation applies to UIR: we would like to establish an UIR threshold that
decides whether an observed difference is reasonably robust to changes in . How can we set
such a threshold? We could be very restrictive and decide, for instance, that an improvement
is significantly robust when UIR  0.75. This condition, however, is so hard that it would
never be satisfied in practice, and therefore the UIR test would not be informative. On
the other hand, if we set a very permissive threshold it will be satisfied by most system
pairs and, again, it will not be informative. The question now is whether there exists a
threshold for UIR values such that obtaining a UIR above the threshold guarantees that an
improvement is robust, and, at the same time, is not too strong to be satisfied in practice.
Given the set of two-system combinations for which UIR surpasses a certain candidate
threshold, we can think of some desirable features:
1. It must be able to differentiate between two types of improvements (robust vs. nonrobust); in other words, if one of the two types is usually empty or almost empty, the
threshold is not informative.
709

fiAmigo, Gonzalo, Artiles & Verdejo

2. The robust set should contain a high ratio of two-system combinations such that the
average F increases for all  values (F (a) > F (b)).
3. The robust set should contain a high ratio of significant concordant improvements and
a low ratio of significant opposite improvements (see Section 6.1).
4. The robust set should contain a low ratio of cases where F contradicts UIR (the dots
in Figure 5 in the region |4F0,.5 | < 0).

Figure 10: Improvement detected across UIR thresholds
Figure 10 shows how these conditions are met for every threshold in the range [0, 0.8].
A UIR threshold of 0.25 accepts around 30% of all system pairs, with a low (4%) ratio of
significant opposite improvements and a high (80%) ratio of significant concordant improvements. At this threshold, in half of the robust cases F increases for all  values, and in
most cases (94%) F=0.5 increases. It seems, therefore, that UIR  0.25 can be a reasonable
threshold, at least for this clustering task. Note, however, that this is a rough rule of thumb
that should be revised/adjusted when dealing with clustering tasks other than WePS.
6.4 UIR and System Rankings
All results presented so far are focused on pairwise system comparisons, according to the
nature of UIR. We now turn to the question of how can we use UIR as a component in the
analysis of the results of an evaluation campaign.
In order to answer this question we have applied UIR to the results of the WePS-2
evaluation campaign (Artiles et al., 2009). In this campaign, the best runs for each system
were ranked according to Bcubed precision and recall metrics, combined with F=0.5 . In
addition to all participant systems, three baseline approaches were included in the ranking:
710

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

all documents in one cluster (B100 ), each document in one cluster (B1 ) and the union of
both (BCOMB )5 .
Table 7 shows the results of applying UIR to the WePS-2 participant systems. -robust
improvements are represented in the third column (improved systems): for every system,
it displays the set of systems that it improves with UIR  0.25. The fourth column is the
reference system, which is defined as follows: given a system a, its reference system is the
one that improves a with maximal UIR:
Sref (a) = ArgmaxS (UIR(S, a))
In other words, Sref (a) represents the system with which a should be replaced in order
to robustly improve results across different  values. Finally, the last column (UIR for the
reference system) displays the UIR between the system and its reference (UIR(Sref , Si )).
Note that UIR adds new insights into the evaluation process. Let us highlight two
interesting facts:
 Although the three top-scoring systems (S1, S2, S3) have a similar performance in
terms of F (0.82, 0.81 and 0.81), S1 is consistently the best system according to UIR,
because it is the reference for 10 other systems (S2, S4, S6, S8, S12, S13, S14, S15,
S16 and the baseline B1 ). In contrast, S2 is reference for S7 only, and S3 is reference
for S11 only. Therefore, F and UIR together strongly point towards S1 as the best
system, while F alone was only able to discern a set of three top-scoring systems.
 Although the non-informative baseline B100 (all documents in one cluster) is better
than five systems according to F, this improvement is not robust according to UIR.
Note that UIR will signal near-baseline behaviors in participant systems with a low
value, while they can receive a large F depending on the nature of the test collection:
when the average cluster is large or small, systems that tend to cluster everything or
nothing can be artificially rewarded. This is, in our opinion, a substantial improvement
over using F alone.

7. UIR as Predictor of the Stability of Results across Test Collections
A common issue when evaluating systems that deal with Natural Language is that results on
different test collections are often contradictory. In the particular case of Text Clustering,
a factor that contributes to this problem is that the average size of clusters can vary across
different test beds, and this variability modifies the optimal balance between precision and
recall. A system which tends to favor precision, creating small clusters, may have good
results in a dataset with a small average cluster size and worse results in a test collection
with a larger average cluster size.
Therefore, if we only apply F to combine single metrics, we can reach contradictory
results over different test beds. As UIR does not depend on metric weighting criteria, our
hypothesis is that a high UIR value ensures robustness of evaluation results across test beds.
5. See the work of Artiles et al. (2009) for an extended explanation.

711

fiAmigo, Gonzalo, Artiles & Verdejo

System

F0.5

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12
B100
S13

0,82
0,81
0,81
0,72
0,71
0,71
0,70
0,70
0,63
0,63
0,57
0,53
0,53
0,52
0,52
0,42
0,41
0,39
0,34
0,33

BCOMB
S14
S15
S16
B1
S17

Improved systems
(UIR > 0.25)
S2 S4 S6 S7 S8 S11..S17 B1
S4 S6 S7 S8 S11..S17 B1
S2 S4 S7 S8 S11..S17 B1
S11 S13..S17
S12..S16
S4 S7 S11 S13..S17 B1
S11 S13..S17
S11..S17
S4 S12 S14 S16
S12..S16
S14..S17
S14 S16
BCOMB
S15 S16
S16
S17
-

Reference
system
S1
S1
S1
S2
S1
S3
S1
S1
B100
S1
S1
S1
S1
S6

UIR for the
reference system
0,26
0,58
0,35
0,65
0,74
0,68
0,71
0,9
0,65
0,9
0,97
1,00
0,29
0,84

Table 7: WePS-2 results with Bcubed precision and recall, F and UIR measures
In other words: given a particular test bed, a high UIR value should be a good predictor that
an observed difference between two systems will still hold in other test beds.
The following experiment is designed to verify our hypothesis. We have implemented
four different systems for the WePS problem, all based on an agglomerative clustering algorithm (HAC) which was used by the best systems in WePS-2. Each system employs a
certain cluster linkage technique (complete link or single link) and a certain feature extraction criterion (word bigrams or unigrams). For each system we have experimented with 20
stopping criteria. Therefore, we have used 20x4 system variants overall. We have evaluated
these systems over WePS-1a, WePS-1b and WePS-2 corpora6 .
The first observation is that, given all system pairs, F=0.5 only gives consistent results
for all three test beds in 18% of the cases. For all other system pairs, the best system
is different depending of the test collection. A robust evaluation criterion should predict,
given a single test collection, whether results will still hold in other collections.
We now consider two alternative ways of predicting that an observed difference (system
A is better than system B) in one test-bed will still hold in all three test beds:
 The first is using F (A)  F (B): the larger this value is on the reference test bed, the
more likely that F (A)  F (B) will still be positive in a different test collection.
6. WEPS-1a was originally used for training in the first WePS campaign, and WePS-1b was used for testing.

712

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

 The second is using U IR(A, B) instead of F: the larger UIR is, the more likely that
F (A)  F (B) is also positive in a different test bed.
In summary, we want to compare F and UIR as predictors of how robust is a result to
a change of test collection. This is how we tested it:
1. We select a reference corpus out of WePS-1a, WePS-1b and WePS-2 test beds.
Cref  {WePS-1a,WePS-1b,WePS-2}
2. For each system pair in the reference corpus, we compute the improvement of one
system with respect to the other according to F and UIR. We take those system pairs
such that one improves the other over a certain threshold t. Being UIRC (s1 , s2 ) the
UIR results for systems s1 and s2 in the test-bed C, and being FC (s) the results of F
for the system s in the test-bed C:
SU IR,t (C) = {(s1 , s2 )|UIRC (s1 , s2 ) > t}
SF,t (C) = {s1 , s2 |(FC (s1 )  FC (s2 )) > t)}
For every threshold t, SU IR,t and SF,t represent the set of robust improvements as
predicted by UIR and F, respectively.
3. Then, we consider the system pairs such that one improves the other according to F
for all the three test collections simultaneously.
T = {s1 , s2 |FC (s1 ) > FC (s2 )C}
T is the gold standard to be compared with predictions SU IR,t and SF,t .
4. For every threshold t, we can compute precision and recall of UIR and F predictions
(SU IR,t (C) and SF,t (C)) versus the actual set of robust results across all collections
(T ).
P recision(SU IR,t (C)) =

|SU IR,t (C)  T |
|SU IR,t |

P recision(SF,t (C)) =

|SF,t (C)  T |
|SF,t (C)|

Recall(SU IR,t (C)) =

Recall(SF,t (C)) =

|SU IR,t (C)  T |
|T |

|SF,t (C)  T |
|T |

We can now trace the precision/recall curve for each of the predictors F, UIR and
compare their results. Figures 11, 12 and 13, show precision/recall values for F (triangles)
and UIR (rhombi); each figure displays results for one of the reference test-beds: WEPS1a,WEPS-1b and WePS-27 .
Altogether, the figures show how UIR is much more effective than F as a predictor.
Note that F suffers a sudden drop in performance for low recall levels, which suggests that
7. The curve parametric UIR refers to an alternative definition of UIR which is explained in Section 8

713

fiAmigo, Gonzalo, Artiles & Verdejo

Figure 11: Predictive power of UIR and F from WePS-1a

Figure 12: Predictive power of UIR and F from WePS-1b

714

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

Figure 13: Predictive power of UIR and F from WePS-2
big improvements in F tend to be due to the peculiarities of the test collection rather than
to a real superiority of one system versus the other.
This is, in our opinion, a remarkable result: differences in UIR are better indicators of
the reliability of a measured difference in F than the amount of the measured difference.
Therefore, UIR is not only useful to know how stable are results to changes in , but also
to changes in the test collection, i.e., it is an indicator of how reliable a perceived difference
is.
Note that we have not explicitly tested the dependency (and reliability) of UIR results
with the number of test cases in the reference collection. However, as working with a
collection of less than 30 test cases is unlikely, in practical terms the usability of UIR is
granted for most test collections, at least with respect of the number of test cases.

8. Parametric versus Non-Parametric UIR
According to our analysis (see Section 5.2), given two measures P and R, the only relational
structure over pairs hPi , Ri i that does not depend on weighting criteria is the unanimous
improvement:
a  b  Pa  Pb  Ra  Rb
When comparing systems, our UIR measure counts the unanimous improvement results
across test cases:
UIRX,T (a, b) =

|Ta b |  |Tb a |
|T |

Alternatively, this formulation can be expressed in terms of probabilities:
715

fiAmigo, Gonzalo, Artiles & Verdejo

UIRX,T (a, b) = Prob(a  b)  Prob(b  a)
where these probabilities are estimated in a frequentist manner.
As we said, the main drawback of the unanimous improvement is that it is a threevalued function which does not consider metric ranges; UIR inherits this drawback. As a
consequence, UIR is less sensitive than other combining schemes such as the F measure. In
order to solve this drawback, we could estimate UIR parametrically. However, the results
in this section seem to indicate that this is not the best option.
One way of estimating P rob(a  b) and P rob(b  a) consists of assuming that the
metric differences (P, R) between two systems across test cases follow a normal bivariate
distribution. We can then estimate this distribution from the case samples provided in each
test bed. After estimating the density function P rob(P, R), we can estimate P rob(a 
b) as8 :
P rob(a  b) = P rob(P  0  R  0) =

Z P =1,R=1

P rob(P, R) dP dR
P =0,R=0

This expression can be used to compute UIRX,T (a, b) = Prob(a  b)  Prob(b  a),
and leads to a parametric version of UIR.
In order to compare the effectiveness of the parametric UIR versus the original UIR, we
repeated the experiment described in Section 7, adding UIRparam to the precision/recall
curves in Figures 11, 12 and 13. The squares in that figures represent the results for the
parametric version of UIR. Note that its behavior lies somewhere between F and the nonparametric UIR: for low levels of recall, it behaves like the original UIR; for intermediate
levels, it is in general worse than the original definition but better than F; and in the
recall high-end, it overlaps with the results of F. This is probably due to the fact that the
parametric UIR estimation considers ranges, and becomes sensitive to the unreliability of
high improvements in F.

9. Conclusions
Our work has addressed the practical problem of the strong dependency (and usually some
degree of arbitrariness) on the relative weights assigned to metrics when applying metric
combination criteria, such as F .
Based on the theory of measurement, we have established some relevant theoretical results: the most fundamental is that there is only one monotonic relational structure that
does not contradict any Additive Conjoint Structure, and that this unique relationship is
not transitive. This implies that it is not possible to establish a ranking (a complete ordering) of systems without assuming some arbitrary relative metric weighting. A transitive
relationship, however, is not necessary to ensure the robustness of specific pairwise system
comparisons.
Based on this theoretical analysis, we have introduced the Unanimous Improvement Ratio (UIR), which estimates the robustness of measured system improvements across potential
metric combining schemes. UIR is a measure complementary to any metric combination
8. For this computation we have employed the Matlab tool

716

fiCombining Evaluation Metrics via the Unanimous Improvement Ratio

scheme and it works similarly to a statistical relevance test, indicating if a perceived difference between two systems is reliable or biased by the particular weighting scheme used to
evaluate the overall performance of systems.
Our empirical results on the text clustering task, which is particularly sensitive to this
problem, confirm that UIR is indeed useful as an analysis tool for pairwise system comparisons: (i) For similar increments in F, UIR captures which ones are less dependent of the
relative weighting scheme between precision and recall; (ii) unlike F, UIR rewards system
improvements that are corroborated by statistical significance tests over each single measure; (iii) in practice, a high UIR tends to imply a large F increase, while a large increase
in F does not imply a high UIR; in other words, a large increase in F can be completely
biased by the weighting scheme, and therefore UIR is an essential information to add to F.
When looking at results of an evaluation campaign, UIR has proved useful to (i) discern
which is the best system among a set of systems with similar performance according to F ;
(ii) penalize trivial baseline strategies and systems with a baseline-like behavior.
Perhaps the most relevant result is a side effect on how our proposed measure is defined:
UIR is a good estimator of how robust a result is to changes in the test collection. In other
words, given a measured increase in F in a test collection, a high UIR value makes more
likely that an increase will also be observed in other test collections. Remarkably, UIR
estimates cross-collection robustness of F increases much better than the absolute value of
the F increase.
A limitation of our present study is that we have only tested UIR on the text clustering
problem. While its usefulness for clustering problems already makes UIR a useful analysis
tool, its potential goes well beyond this particular problem. Most Natural Language problems  and, in general, many problems in Artificial Intelligence  are evaluated in terms of
many individual measures which are not trivial to combine. UIR should be a powerful tool
in many of those scenarios.
An UIR evaluation package is available for download at http://nlp.uned.es.

Acknowledgments
This research has been partially supported by the Spanish Government (grant Holopedia,
TIN2010-21128-C02) and the Regional Government of Madrid under the Research Network
MA2VICMR (S2009/TIC-1542).

References
Amigo, E., Gonzalo, J., Artiles, J., & Verdejo, F. (2008). A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Information Retrieval, 12 (4), 461486.
Artiles, J., Gonzalo, J., & Sekine, S. (2009). WePS-2 Evaluation Campaign: Overview of the
Web People Search Clustering Task. In Proceedings Of The 2nd Web People Search
Evaluation Workshop (WePS 2009).
Artiles, J., Gonzalo, J., & Sekine, S. (2007). The SemEval-2007 WePS evaluation: Establishing a Benchmark for the Web People Search Task. In Proceedings of the 4th
International Workshop on Semantic Evaluations, SemEval 07, pp. 6469 Stroudsburg, PA, USA. Association for Computational Linguistics.
717

fiAmigo, Gonzalo, Artiles & Verdejo

Bagga, A., & Baldwin, B. (1998). Entity-Based Cross-Document Coreferencing Using the
Vector Space Model. In Proceedings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International Conference on Computational
Linguistics (COLING-ACL98), pp. 7985.
Carreras, X., & Marquez, L. (2004). Introduction to the CoNLL-2004 Shared Task: Semantic
Role Labeling. In Ng, H. T., & Riloff, E. (Eds.), HLT-NAACL 2004 Workshop: Eighth
Conference on Computational Natural Language Learning (CoNLL-2004), pp. 8997
Boston, Massachusetts, USA. Association for Computational Linguistics.
Cormack, G. V., & Lynam, T. R. (2005). TREC 2005 Spam Track Overview. In Proceedings
of the fourteenth Text REtrieval Conference (TREC-2005).
Debreu, G. (1959). Topological methods in cardinal utility theory. Mathematical Methods
in the Social Sciences, Stanford University Press, 1 (76), 1626.
Ghosh, J. (2003). Scalable clustering methods for data mining. In Ye, N. (Ed.), Handbook
of Data Mining. Lawrence Erlbaum.
Halkidi, M., Batistakis, Y., & Vazirgiannis, M. (2001). On Clustering Validation Techniques.
Journal of Intelligent Information Systems, 17 (2-3), 107145.
Luce, R., & Tukey, J. (1964). Simultaneous conjoint measurement: a new scale type of
fundamental measurement. Journal of Mathematical Psychology, 1 (1).
Mann, G. S. (2006). Multi-Document Statistical Fact Extraction and Fusion. Ph.D. thesis,
Johns Hopkins University.
Meila, M. (2003). Comparing clusterings. In Proceedings of COLT 03.
Narens, L., & Luce, R. D. (1986). Measurement: The theory of numerical assignments. Psychological Bulletin, 99.
Steinbach, M., Karypis, G., & Kumar, V. (2000). A comparison of document clustering
techniques. In KDD Workshop on Text Mining,2000.
Su, K.-Y., Su, J., Wiebe, J., & Li, H. (Eds.). (2009). Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Association for Computational Linguistics, Suntec, Singapore.
Tao Li, C. Z., & Zhu, S. (2006). Empirical Studies on Multilabel Classification. In Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence
(ICTAI 2006).
van Rijsbergen, C. J. (1974). Foundation of evaluation. Journal of Documentation, 30 (4),
365373.
Weng, C. G., & Poon, J. (2008). A New Evaluation Measure for Imbalanced Datasets. In
Roddick, J. F., Li, J., Christen, P., & Kennedy, P. J. (Eds.), Seventh Australasian
Data Mining Conference (AusDM 2008), Vol. 87 of CRPIT, pp. 2732 Glenelg, South
Australia. ACS.
Zhao, Y., & Karypis, G. (2001). Criterion functions for document clustering: Experiments
and analysis. Technical Report TR 0140, Department of Computer Science, University of Minnesota, Minneapolis, MN.
718

fiJournal of Artificial Intelligence Research 42 (2011) 851-886

Submitted 07/11; published 12/11

Dr.Fill: Crosswords and an Implemented
Solver for Singly Weighted CSPs
Matthew L. Ginsberg
On Time Systems, Inc.
355 Goodpasture Island Road, Suite 200
Eugene, Oregon 97401

Abstract
We describe Dr.Fill, a program that solves American-style crossword puzzles.
From a technical perspective, Dr.Fill works by converting crosswords to weighted
csps, and then using a variety of novel techniques to find a solution. These techniques
include generally applicable heuristics for variable and value selection, a variant of
limited discrepancy search, and postprocessing and partitioning ideas. Branch and
bound is not used, as it was incompatible with postprocessing and was determined
experimentally to be of little practical value. Dr.Fills performance on crosswords
from the American Crossword Puzzle Tournament suggests that it ranks among the top
fifty or so crossword solvers in the world.

1. Introduction
In recent years, there has been interest in solving constraint-satisfaction problems, or csps,
where some of the constraints are soft in that while their satisfaction is desirable, it is
not strictly required in a solution. As an example, if a construction problem is modeled as
a csp, it may be possible to overutilize a particular labor resource by paying the associated
workers overtime. While not the cheapest way to construct the artifact in question, the
corresponding solution is certainly viable in practice.
Soft constraints can be modeled by assigning a cost to violating any such constraint,
and then looking for that solution to the original csp for which the accumulated cost is
minimized.
By and large, work on these systems has been primarily theoretical as various techniques for solving these weighted csps (wcsps) are considered and evaluated without the
experimental support of an underlying implementation on a real-world problem. Theoretical complexity results have been obtained, and the general consensus appears to be that
some sort of branch-and-bound method should be used by the solver, where the cost of one
potential solution is used to bound and thereby restrict the subsequent search for possible
improvements.
Our goal in this paper is to evaluate possible wcsp algorithms in a more practical
setting, to wit, the development of a program (Dr.Fill) designed to solve American-style
crossword puzzles. Based on the search engine underlying Dr.Fill, our basic conclusions
are as follows:

c
2011
AI Access Foundation. All rights reserved.

fiGinsberg

1. We present specific variable- and value-selection heuristics that improve the effectiveness of the search enormously.
2. The most effective search technique appears to be a modification of limited discrepancy
search (lds) (Harvey & Ginsberg, 1995).
3. Branch-and-bound appears not to be a terribly effective solution technique for at least
some problems of this sort.
4. Postprocessing complete candidate solutions improves the effectiveness of the search.
A more complete description of the crossword domain can be found in Section 2.2;
example crosswords appear in Figures 1 and 2. The overall view we will take is that, given
both a specific crossword clue c and possible solution word or fill f , there is an associated
score p(f |c) that gives the probability that the fill is correct, given the clue. Assuming that
these probabilities are independent for different clues, the probability that a collection of
fills solves a puzzle correctly is then simply
Y
p(fi |ci )
(1)
i

where fi is the fill entered in response to clue ci . Dr.Fills goal is to find a set of fills
that is legal (in that intersecting words share a letter at the square of intersection) while
maximizing (1).
For human solvers, p(f |c) will in general be zero except for a handful of candidate fills
that conform to full domain knowledge. Thus a 1973 nonfiction best seller about a woman
with multiple personalities must be Sybil; a 3-letter Black Halloween animal might
be bat or cat, and so on. For Dr.Fill, complete domain knowledge is impractical
and much greater use is made of the crossing words, as the csp solver exploits the hard
constraints in the problem to restrict the set of candidate solutions.
Dr.Fills performance as a solver is comparable to (but significantly faster than) all
but the very best human solvers. In solving New York Times crosswords (which increase in
difficulty from Monday to Saturday, with the large Sunday puzzles comparable to Thursdays
in difficulty), Dr.Fill generally solves Monday to Wednesday puzzles fairly easily, does well
on Friday and Saturday puzzles, but often struggles with Thursday and Sunday puzzles.
These puzzles frequently involve some sort of a gimmick where the clues or fill have been
modified in some nonstandard way in order to make the puzzle more challenging. When run
on puzzles from the American Crossword Puzzle Tournament, the annual national gathering
of top solvers in the New York City area, Dr.Fills performance puts it in the top fifty or
so of the approximately six hundred solvers who typically attend.
The outline of this paper is as follows. Preliminaries are contained in the next section,
both formal preliminaries regarding csps in Section 2.1 and a discussion of crosswords in
Section 2.2. Section 2.3 discusses crosswords as csps specifically, including a description of
the variety of ways in which crosswords differ from the problems typically considered by
the constraint satisfaction community.
The heuristics used by Dr.Fill are described in Section 3, with value-selection heuristics
the topic of Section 3.1 and variable-selection heuristics the topic of Section 3.2. The
852

fiDr.Fill: A Crossword Solver

techniques for both value and variable selection can be applied to wcsps generally, although
it is not clear how dependent their usefulness is on the crossword-specific features described
in Section 2.3.
Our modification of lds is described in Section 4, and is followed in Section 5 with our
first discussion of the experimental performance of our methods. Algorithmic extensions
involving postprocessing are discussed in Section 6, which also discusses the reasons that
branch-and-bound techniques are not likely to work well in this domain. Branch-andbound and postprocessing are not compatible but the arguments against branch-and-bound
are deeper than that. Section 7 describes the utility of splitting a crossword into smaller
problems when the associated constraint graph disconnects, an idea dating back to work of
Freuder and Quinn (1985) but somewhat different in the setting provided by lds.
Section 8 concludes by describing related and future work, including the earlier crossword
solvers Proverb (Littman, Keim, & Shzaeer, 2002) and WebCrow (Ernandes, Angelini,
& Gori, 2005), and the Jeopardy-playing program Watson (Ferrucci, Brown, Chu-Carroll,
Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, & Welty, 2010).

2. Preliminaries
In this section, we give a brief overview of constraint satisfaction, crossword puzzles, and
the relationship between the two.
2.1 Constraint Satisfaction
In a conventional constraint-satisfaction problem, or csp, the goal is to assign values to
variables while satisfying a set of constraints. The constraints indicate that certain values
for one variable, say v1 , are inconsistent with other specific values for a different variable v2 .
Map coloring is a typical example. If a particular country is colored red, then neighboring
countries are not permitted to be the same color.
We will formulate crossword solving by associating a variable to each word in the crossword, with the value of the variable being the associated fill. The fact that the first letter
of the word at 1-Across has to match the first letter of the word at 1-Down corresponds to
a constraint between the two variables in question.
A basic csp, then, consists of a set V of variables, a set D of domains, one for each
variable, from which the variables values are to be taken, and a set of constraints.
Definition 2.1 Given a set of domains D and set V of variables, an n-ary constraint  is
a pair (T, U ) where T  V is of size n and U is a subset of the allowed sets of values for
the variables in V . An assignment S is a mapping from each variable v  V to an element
of vs domain. For T  V , the restriction of S to T , to be denoted S|T , is the restriction of
the mapping S to the set T . We will say that S satisfies the constraint (T, U ) if S|T  U .
The constraint simply specifies the sets of values that are allowed for the various variables
involved.
As an example, imagine coloring a map of Europe using the four colors red, green, blue
and yellow. Now T might be the set {France, Spain} (which share a border), D would assign
the domain {red, green, blue, yellow} to each variable, and U would be the twelve ordered
853

fiGinsberg

pairs of distinct colors from the four colors available. The associated constraint indicates
that France and Spain cannot be colored the same color.
As we have remarked, we will take the view that the variables in a crossword correspond
to the various slots into which words must be entered, and the values to be all of the
words in some putative dictionary from which the fills are taken (but see the comments in
Section 2.3). If two words intersect (e.g., 1-Across and 1-Down, typically), there is a binary
constraint excluding all pairs of words for which shared letters differ.
Definition 2.2 A constraint-satisfaction problem, or csp, is a triple (V, D, ) where V is
a set of variables, D gives a domain for each variable in V , and  is a set of constraints.
|V | will be called the size of the csp. If every constraint in  is either unary or binary, the
csp is called a binary csp.
For a csp C, we will denote the set of variables in C by VC , the domains by DC , and
the constraints by C .
A solution to a csp is an assignment that satisfies every constraint in .
Both map coloring and crossword solving as described above are binary csps.
There is an extensive literature on csps, describing both their applicability to a wide
range of problems and various techniques that are effective in solving them. It is not
practical for me to repeat that literature here, but there are two points that are particularly
salient.
First, csps are generally solved using some sort of backtracking technique. Values are
assigned to variables; when a conflict is discovered, a backtrack occurs that is designed to
correct the source of the problem and allow the search to proceed. As with other chronologically based backtracking schemes, there are well-known heuristics for selecting the variables
to be valued and the values to be used, and the most effective backtracking techniques use
some kind of nogood reasoning (Doyle, 1979; Ginsberg, Frank, Halpin, & Torrance, 1990,
and many others) to ensure that the backtrack will be able to make progress.
We will need to formalize this slightly.
Definition 2.3 Let C be a csp, and suppose that v is a variable in VC and x a value in
the associated domain in DC . By C|v=x we will denote the csp obtained by setting v to x.
In other words, C|v=x = (VC  v, DC , ) where  consists of all constraints  such that for
some constraint (T, U )  C

(T, U ),
if v 6 T ;
=
(2)
(T  v, {u  U |u(v) = x}|T v ), if v  T .
C|v=x will be called the restriction of C to v = x.
The notation may be intimidating but the idea is simple: Values permitted by constraints
in the new problem are just those permitted in the old problem, given that we have decided
to set v to x. So if an original constraint doesnt mention v (the top line in (2)), the new
constraint is unchanged. If v is mentioned, we see which values for the other variables are
allowed, given that v itself is known to take the value x.

854

fiDr.Fill: A Crossword Solver

Definition 2.4 Let S be a partial solution to a csp C, in that S maps some of the variables
in VC to elements of DC . The restriction of C to S, to be denoted C|S , is the csp obtained
by successively restricting C to each of the assignments in S as in Definition 2.3.
This definition is well-defined because we obviously have:
Lemma 2.5 The restriction defined in Definition 2.4 is independent of the order in which
the individual restrictions are taken.
2
Backtracking csp solvers work by selecting variables, trying various values for the variables so selected, and then recursively solving the restricted problems generated by setting
the variable in question to the value chosen.
The second general point we would like to make regarding csp solvers is that most
implementations use some kind of forward checking to help maintain consistency as the
search proceeds. As an example, suppose that we are about to assign a value x to a
variable v, but that if we do this, then every possible value for some other variable v 0 will
be eliminated by the constraints. In this case, we can obviously eliminate x as a value for v.
It is worth formalizing this a bit, although we do so only in the most general of terms.
Definition 2.6 A propagation mechanism  is a mapping from csps to csps that does not
change the variables, so that (V, D, ) = (V, D0 , 0 ) for any (V, D, ). We also require
that for any variable in V , the associated domain in D0 is a subset of the associated domain
in D, and if  = (T, U )  , there must be a 0 = (T 0 , U 0 )  0 with U 0  U . We will say
that  is sound if, for any csp C and solution S to C, S is also a solution to (C).
The propagation mechanism strengthens the constraints in the problem, and may reduce
some of the variable domains as well. It is sound if it never discards a solution to the original
csp.
A wide range of propagation mechanisms has been discussed in the literature. Simplest,
of course, is to simply eliminate variable values that can be shown to violate one of the
constraints in the problem. Iterating this idea recursively until quiescence (Mackworth,
1977) leads to the well known AC-3 algorithm, which preserves arc consistency as the csp
is solved.
Returning to the map of Europe, Germany borders France, Holland, Poland, and Austria
(among other countries). If Holland is red, Poland is blue, and Austria is yellow, this is
sufficient to cause Germanys live set to be just green (assuming that these are the four
colors available), which will in turn cause Frances live set to exclude green, even though
France does not share a direct constraint with Holland, Poland or Austria.
Alternatively, consider the crossword showed in Figure 1; this is the New York Times
crossword (with solution) from March 10, 2011. Once we decide to put READING at 1Across [Poets performance], the live set for words at 1-Down consists only of six-letter words
beginning with R. If we had also entered ASAMI at 21-Across [Me, too] and REDONDO
at 27-Across, then the live set for 1-Down would be words of the form R...AR.
Weighted CSPs Sometimes it is desirable for a csp to include soft constraints. The
notion here is that while a soft constraint indicates a set of variable values that are preferred
in a solution, the requirement is like the pirates code, more what youd call guidelines
855

fiGinsberg

NY Times, Thu, Mar 10, 2011 Matt Ginsberg / Will Shortz
1
2
3
4
5
6
7
ACROSS

R

E A D

I

N G

8

C

9

E

1. *Poet's
14
15
performance
A L B A N I A
S O M
8. Frequent flooding 16
17
site
W O R D S P R O N O U
14. Country with
18
19
which the U.S.
B I O
S Y B I L
goes to war in
21
22
23
24
25
A S A M I
I T E M
"Wag the Dog"
27
28
29
30
15. Who "saved my
R E D O N D O
R A
life tonight" in a
32
33
34
35
1975 Elton John
B A I L E Y
N
hit
36
37
16. With 36- and 58D I F F E R E N T
Across, what the 38 39
40
answers to the
S P E L L
A N G E L
starred clues are 41
42
43
18. Jacket material,
C A M E A S
G O E
for short?
47
48
49
50
51
R D A
T O R I
S
19. 1973 nonfiction
52
53
54
55
56
best seller about
A T R A
M I S S M
a woman with
58
59
60
multiple
W H E N C A P I T A L
personalities
62
63
20. Lady of the
L A S T O N E
I M A
knight?
64
65
21. "Me, too"
S I T S B Y
R A I
24. Line ___
 2011, The New York Times
26. "The Thin Man"
actress
58. See 16-Across
11. Neighborhood
27. ___ Beach, Calif.
62. "I'm done after
12. Flower that
30. Plunder
this"
shares its name
32. Big name in
63. "Somehow
with a tentacled
circuses
everything gets
sea creature
35. B, A, D, G and E,
done"
13. They might depart
e.g.
64. Does nothing
at midnight
36. See 16-Across
65. *Like Seattle vis15. Huff
38. Say "B-A-D-G-E,"
-vis Phoenix
17. Japanese band
e.g.
22. *Not fixed
40. Figures on the
23. Like Elgar's
DOWN
ceiling of la
Symphony No. 1
1. Seafood lover's
Cappella Sistina
25.
Cloaks
hangout
41. Impersonated at a
28. "What's the ___?"
2. Nancy Drew's
costume party
29. Pharmaceutical
aunt
43. Spoils
oils
3. One way to travel
47. Nutritional amt.
31.
*Shine
or study
48. Doughnuts, but
33. Old World eagle
4. Pop
not danishes
34. Burglar in
5. Connections
51. Piece of the
detective stories
6. Cheese ___
action
36. William who
7. Player of golf
52. Gillette offering
played Uncle
8. Clink
54. Bette's "Divine"
Charley on "My
9. Prey of wild dogs
stage persona
Three Sons"
and crocodiles
57. Actress Vardalos
10. Furnish

10

L

11

L

12

13

A R

E O N
N C

E D

20

D A M
26

E

L

O Y

I

N

31

P

O T
L

E

E

E S

Y

I
S

44

45

46

B A D

H A R
57
61

I

E

N

I

A

Z

E D

N A G E
N

I

E R

37. Prefix with
paganism
38. Many signatures
39. Noodle dish
42. Lots and lots of
44. Battle cry
45. French
department in the
Pyrenees
46. Less lively
49. Opportune
50. "Whatever it ___
don't care!"
53. Drones, maybe
55. Excitement
56. ___ Bear
59. Inner ear?
60. Medieval French
love poem
61. What a keeper
may keep

c
Figure 1: A Thursday New York Times crossword. 2011
The New York Times. Reprinted
with permission.
856

fiDr.Fill: A Crossword Solver

than actual rules. If no solution can be found without violating one of the soft constraints,
it is acceptable to return a solution that does violate a soft constraint. The hard constraints are required to be satisfied in any case.
There are a variety of ways to formalize this. One of the simplest is to simply associate
a cost with each soft constraint and to then search for the overall assignment of values to
variables for which the total cost is minimized. We will take the view that the total cost
of an assignment is the sum of the costs of the soft constraints that have been violated,
although other accumulation functions (e.g., maximum) are certainly possible (Bistarelli,
Montanari, Rossi, Schiex, Verfaillie, & Fargier, 1999).
A soft k-ary constraint thus consists of a mapping c : Dk  IR giving the cost associated
with various selections for the variables being valued. The cost of a complete assignment of
values to variables is the sum of the costs incurred for each soft constraint. A csp including
costs of this form is called a weighted constraint satisfaction problem, or wcsp (Larrosa &
Schiex, 2004, and many others).
Definition 2.7 A weighted csp, or wcsp, is a quadruple C = (V, D, , W ) where (V, D, )
is a csp and W is a set of pairs (U, c) where U  V is a set of variables and c is a cost
function assigning a cost to each assignment of the variables in U . Each element w  W
will be called a weighted constraint. Where no ambiguity can arise, we will abuse notation
and also denote by w the associated cost function c.
Given a partial solution S, the associated cost of the weighted constraint w = (U, c), to
be denoted by c(S, w), is the minimum cost associated by c to any valuation for the variables
in U that extends the partial solution S. The cost of the partial solution is defined to be
X
c(S) =
c(S, w)
(3)
wW

Informally, c(S, w) is the minimum cost that will be charged by w to any solution to C that
is an extension of S. We therefore have:
Lemma 2.8 Given a wcsp C and partial solution S, every solution to C that extends S
has cost at least c(S).
2
Note that our Definition 2.7 is slightly nonstandard in that we explicitly split the hard
constraints in  from the soft constraints in W . We do this because in the crossword
domain, there is a further condition that is met: The soft constraints are always unary
(although the hard constraints are not). There is simply a cost associated with setting the
variable v to some specific value x. We will refer to such problems as singly weighted csps,
or swcsps. While the algorithmic ideas that we will present in this paper can be applied
reasonably easily to wcsps that are not in fact swcsps, the experimental work underlying
Dr.Fill clearly reflects performance on swcsps specifically.1
As it is possible to use propagation to reduce the sizes of the domains in any particular
csp, it is also possible to use a variety of polynomial time algorithms to compute lower
1. And in fact, Larrosa and Dechter (2000) have shown that all weighted csps can be recast similarly, into
a form with only hard binary constraints and soft unary constraints.

857

fiGinsberg

bounds on c(S) for any partial solution S. The techniques used here have become increasingly sophisticated in recent years, ranging from algorithms that move costs around the
constraint graph to better compute the minimum (Givry & Zytnicki, 2005; Zytnicki, Gaspin,
de Givry, & Schiex, 2009) to more sophisticated approaches that solve linear programming
problems to compute more accurate bounds (Cooper, de Givry, Sanchez, Schiex, Zytnicki,
& Werner, 2010).
Finally, we note in passing the every csp has a dual version where the roles of the
variables and constraints are exchanged. We view crosswords as csps where the variables
are word slots and the values are the words that fill them, with constraints that require
that the letters match where the words cross. But we could also view crosswords as csps
where the variables are individual letters, the values are the usual A through Z, and the
constraints indicate that every collection of letters needs to make up a legal word. We
will discuss the likely relative merits of these two approaches in Section 2.3, after we have
described crosswords themselves.
2.2 Crosswords
Since the introduction of the first word cross in the Sunday New York World almost a
century ago (December 21, 1913), crosswords have become one of the worlds most popular
mental pastimes. Will Shortz, editor of the crossword for the New York Times, estimates
that some five million people solve the puzzle each day, including syndication.2
2.2.1 Features of Crosswords
A typical New York Times crossword appears in Figure 1. We assume that the reader is
familiar with the basic format, but there are many specific features that are worth mentioning.
Crosswords are symmetric. The black squares, or blocks, are preserved under a 180
rotation.3 In addition, crosswords are almost always square in shape, with the Times daily
puzzles being of size 15  15 and the Sundays 21  21.4
Multiple words are permitted as fill. In the puzzle of Figure 1, we have [Seafood
lovers hangout] cluing RAW BAR at 1-Down and [Somehow everything gets done] cluing
I MANAGE at 63-Across. There is no indication in the clue that a multiword answer is
expected.
Without the clues, crossword solutions are not unique. There are many ways to
fit words into any particular crossword grid; it is the clues that determine which legal fill
is the puzzles solution. This is what makes solving so challenging from a computational
perspective: Failing to understand the clues (at least at some level) leaves the problem
underconstrained.
2. Personal communication.
3. In rare cases, horizontal symmetry is present instead of rotational symmetry. In rarer cases still, the
symmetry requirement is not honored.
4. The Sunday puzzles used to be 23  23 on occasion, but a reduction in the size of the Times printed
magazine section made these larger puzzles impractical.

858

fiDr.Fill: A Crossword Solver

Puzzles can be themed or themeless. A themeless puzzle contains a collection of
generally unrelated words and clues. A themed puzzle has some shared element that connects many of the answers; when this happens, the shared answers are generally located
symmetrically in the grid.
The puzzle in Figure 1 is themed. The (symmetric) entries at 1-Across, 65-Across,
22-Down and 31-Down are marked with asterisks and are all words that are pronounced
differently when capitalized. This description also appears in the puzzle itself using the
(also symmetric) entries at 16-Across, 36-Across and 58-Across.5
The presence of a theme has a profound impact on the solving experience. In this particular example (which is relatively straightforward), there are two entries (WORDSPRONOUNCED and WHENCAPITALIZED) that surely do not appear in any dictionary that
the solver is using to fill the grid. There are also complex relationships among many of the
entries  the phrase in 16-Across, 36-Across and 58-Across, but also the relationship of that
phrase to the entries marked with asterisks.
Other themes present other challenges. Some of the more popular themes are quip
puzzles, where a famous saying is split into symmetric pieces and inserted into the grid,
and rebus puzzles, where more than one letter must be put in a single square. Arguably
the most famous themed Times puzzle appeared on election day in 1996. The clue for
39-Across was [Lead story in tomorrows newspaper (!), with 43-Across]. 43-Across was
ELECTED and, depending on the choices for the down words, 39-Across could be either
CLINTON or BOBDOLE. For example, 39-Down, a three-letter [Black Halloween animal]
could be CAT (with the C in CLINTON) or BAT (with the B in BOBDOLE). So here, not
only are there multiple ways to insert words legally into the grid, there are multiple ways
for those words to match the clues provided. (Until the winner of the election was decided,
of course.) The two legal solutions were identical except for the CLINTON/BOBDOLE
ambiguity and the associated crossing words; it is not known whether there is a puzzle that
admits multiple solutions without shared words, while conforming to the usual restrictions
on symmetry, number of black squares, and so on.
A more extreme example of a themed crossword appears in Figure 2. Each clue has a
particular letter replaced with asterisks (so in 1-A, for example, the e in [Twinkle] has been
replaced). The letter that has been replaced is dropped in the fill, but the result is still
a word. So 1-A, which would normally be GLEAM, becomes the David Bowie rock genre
GLAM.
Every word in this puzzle is missing letters in this fashion. A computer (or human)
solver will be unable to get a foothold of any kind if it fails to understand the gimmick,
and Dr.Fill fails spectacularly on this puzzle.
Puzzles have structural restrictions. For the Times, a daily unthemed puzzle will
have at most 72 words; a themed puzzle will generally have at most 78. For a Sunday,
140 words is the limit. At most 61 of the squares in the grid can be black squares.6 This
information is potentially of value to a solver because the word count can often be used to
5. When submitted, this puzzle also contained an asterisk for the entry at 36-Across, which did not break
the symmetry of the pronounced-differently-when-capitalized entries. Shortz decided that too many
solvers wouldnt get the joke, though, and the asterisk was removed when the puzzle was published.
6. The Times puzzle with the fewest words (52) appeared on January 21, 2005 and was by Frank Longo.
The puzzle with the fewest blocks (18) appeared on August 22, 2008 and was by Kevin Der, a feat

859

fiGinsberg

NY Times, Sun, May 17, 2009 TAKEAWAY CROSSWORD (see Notepad)Matt Ginsberg / Will Shortz
1
2
3
4
5
6
7
8
9
10
11
12
ACROSS
1. Twinkl*
13
14
5. Ou*look
9. "Dani*l Boon*"
16
17
actor
19
20
13. Gung-*o [Lat.]
14. Sp*tted cats
23
15. Male chauvini*t,
*ay
26
27
28
16. Playin* the slots,
31
32
e.*.
17. Minia*ure
38
39
40
desser*s
18. Admonis* [suffix]
43
44
19. *iding, as a sword
48
49
21. Neither young nor
ol*
52
53
54
23. Diat*ibe delive*e*s
25. *hief
57
26. Sergea*t of old TV
63
64
29. Denta* devices
31. Still feelin* sleepy
67
68
32. *over up
34. Mentalist inspired
70
71
by "Mandra*e the
 2009, The New York Times
Magician"
38. Struc* with the
68. Lessons fro*
foot
fables
40. Mail origi*ator
69. 1960* prote*t [2
42. Absolutely ama*e
wds.]
43. *emoves pencil
70. S*ut up
ma*ks
71. Places *o pick up
45. Big pi*kles?
chicks?
47. Cat*' warning*
72. Likel* to rise?
48. Apo*tle known a*
"the Zealot"
DOWN
50. Dise*se
1. Brib*, informally
tr*nsmitted by
[Fr.]
cont*min*ted
2. Bit of *ire
w*ter
3. Asla*'s realm
52. Italian po*t?
4. Lite*ally, "sac*ed
55. Cerea* protein
utte*ances"
[Ger.]
5. Nothing speci*l
57. P*blic sales
6. Opposite of *on't
59. Im*roved one's lot
7. M*nhole
[hyph.]
em*n*tion
63. Can*ne restra*nt
8. The *eatles' Paul
64. Sou*hwes*
McCartney, e.g.
German ci*y
9. Made smalle*
66. Rea*y to be ri**en
10. *ost
67. Roun*e* up
co*prehensive

15
18
21
24
29

22
25

30

33

34
41

45

35

55
58

61

62

47
51

56
59

65

37

42
46

50

36

60
66
69
72

11. *mall amount,
briefly
12. Device that
re*oves stalks fro*
fruit
14. Chin*s* cuisin*
20. *atchy *ony
22. *oting booth
feature
24. Big name in ba**s
26. Pulit*er, e.g. [Fr.]
27. *egis*ative routine
[Sp.]
28. B*ont family
30. Speaks gi**erish
33. First mo*th, to
Jua*
35. Lesley of "60
Minu*es"
36. Waiti*g o*e's tur*
37. Di*dainful
expre**ion
39. Left the
Met*oline*
41. Core cont*iners

44. National park in
sout*west
Tennessee
46. Turt*es and
bu**ets have
them
49. *mpos*ng house
51. Biase*
52. Qi*g Dy*asty
people
53. A**ow shoote*s
54. O*erdo the diet
56. Street art, *aybe
58. Stron* *rowth
60. Not homo*eneous
[Lat.]
61. "Lo*e Me Tender"
star
62. B*rth cert., for one
65. Rank*es

c
Figure 2: A difficult themed crossword. 2009
The New York Times. Reprinted with
permission.

860

fiDr.Fill: A Crossword Solver

determine whether or not a puzzle has a theme. (The themed puzzle of Figure 1 has only
70 words, however, so all that can really be concluded is that a 15  15 puzzle with more
than 72 words is likely themed in some way.)
There are other restrictions as well. Two-letter words are not permitted, and every
square must have two words passing through it (alternatively, one-letter words are not
permitted, either). The puzzle must be connected (in that the underlying csp graph is as
well). For a puzzle to be singly connected, in that converting a single empty square to a
block disconnects it, is viewed as a flaw but an acceptable one.
Fill words may not be repeated elsewhere in the puzzle. If the fill BAT appears
in one location in the puzzle, then it cannot be used elsewhere (including in multiword fill).
In addition, BAT cannot be used in a clue. This means that words appearing in clues do
not (or at least, should not) appear in the fill as well.
Crossword clues must pass the substitution test. This is arguably the most
important requirement from the solvers perspective. It must be possible to construct a
sentence in which the clue appears, and so that the meaning of the sentence is essentially
unchanged if the clue is replaced with the fill. So for the puzzle in Figure 1, one might
say, Ive seen a video of e.e. cummings giving a READING, or the equivalent (although
stilted) Ive seen a video of e.e. cummings giving a [poets performance]. One might say,
We dont keep food in our houses CELLAR because we dont want it to get wet, or the
equivalent We dont keep food in our houses [frequent flooding site] because we dont want
it to get wet.
The fact that the clues and associated fill must pass the substitution test means that
it is generally possible to determine the part of speech, number (singular vs. plural) and
tense (present, past, future, etc.) of the fill from the clue. So in Figure 1, the fill for 1-A
is a singular noun, and so on. This restricts the number of possible values for each word
considerably.
There are other conventions regarding cluing. If a clue contains an abbreviation,
then the answer is an abbreviation as well. The puzzle in Figure 1 has no abbreviations
clued in this way (a rarity), but 62-D in Figure 2 is [B*rth cert., for one]. The solution is
IDENT, which is an abbreviation for identification. Of course, the I gets dropped, so it is
DENT that is entered into the grid. Abbreviations can also be indicated by a phrase like
for short in the clue.
Clues ending in a ? generally indicate that some sort of wordplay is involved. In Figure 1,
for example, we have 18-A: [Jacket material, for short?] The solution is BIO because jacket
in the clue refers to a books jacket, not clothing.
The wordplay often exploits the fact that the first letter of the clue is capitalized. So
in Figure 1, 7-D is [Player of golf], referring to GARY Player. If the clue had been [Golfs
Player], the capitalization (not to mention the phrasing) would have made it obvious that
a proper name was involved. As the clue was written, the solver might easily be misled.
Crossword features and Dr.Fill Many of the features that we have described (e.g.,
symmetry) do not bear directly on the solving experience, and Dr.Fill is therefore unaware
repeated on August 7, 2010 by Joe Krozel. It is not known whether a 17-block puzzle of reasonable
quality exists.

861

fiGinsberg

of them. The program does look for multiple-word fill and has a module that is designed to
identify rebus puzzles. It does not check to see if fill words are repeated elsewhere, since this
is so rare as to offer little value in the search. It uses fairly straightforward part-of-speech
analysis to help with the substitution test, and checks clues for abbreviations. Dr.Fill has
no knowledge of puns.
2.3 Crossword Puzzles as SWCSPs
Given all of this, how are we to cast crossword solving as a csp?
The view we will take, roughly speaking, is that we start with some large dictionary of
possible fills, and the goal is to enter words into the grid that cross consistently and so that
each word entered is a match for the associated clue. If D is our dictionary, we will define Dn
to be the subset of D consisting of words of exactly n letters, so that BAT is in D3 , and so
on. We also assume that we have a scoring function p that scores a particular word relative
to a given clue, which we will interpret probabilistically. Given a clue like [Black Halloween
animal]3 for a 3-letter word and potential fill BAT, p(BAT|[Black Halloween animal]3 ) is the
probability that BAT is the correct answer for the word in question. Our goal is to now find
the overall fill with maximum probability of being correct.
In other words, if ci is the ith clue and fi is the value entered into the grid, we want to
find fi that satisfy the constraints of the problem (crossing words must agree on the letter
filled in the crossing square) and for which
Y
p(fi |ci )
(4)
i

is maximized. As mentioned in the introduction, this assumes that the probabilities of
various words being correct are uncorrelated, which is probably reasonably accurate but
not completely correct in a themed puzzle.
If we define (fi , ci ) =  log p(fi |ci ), maximizing (4) is equivalent to minimizing
X
(fi , ci )
(5)
i

This is exactly the swcsp framework that we have described.
The dictionary Di must include not just words of length i, but also all sequences of words
that are collectively of length i. (In other words, D15 needs to include WHENCAPITALIZED.)
In actuality, however, even this is not enough. There are many instances of crossword fill
that are not even word sequences.
This may be because a word does not appear in any particular dictionary. A puzzle
in the 2010 American Crossword Puzzle Tournament (acpt) clued MMYY as [Credit card
exp. date format] although MMYY itself is not a word in any normal sense. A further
example is the appearance of SNOISSIWNOOW in the Times puzzle of 11/11/10, clued as
Apollo 11 and 12 [180 degrees]. Rotating the entire puzzle by 180 degrees and reading
SNOISSIWNOOW upside down produces MOONMISSIONS.
Given these examples and similar ones, virtually any letter sequence can, in fact, appear
in any particular puzzle. So the domain D should in fact consist of all strings of the
appropriate length, with the cost function used to encourage use of letter strings that are
862

fiDr.Fill: A Crossword Solver

dictionary words (or sequences of words) where possible. This means that the variable
domains are so large that both they and the associated  functions must be represented
functionally; computing either the dictionaries or the scores in their entirety is simply
impractical.
This is the fundamental difference between the problem being solved by Dr.Fill and
the problems generally considered in the AI literature. The number of variables is modest,
on the order of 100, but the domain size for each variable is immense, 265 or approximately
12 million for a word of length five, and some 1.7  1018 for a word of length fifteen.
One immediate consequence of this is that Dr.Fill can do only a limited amount of
forward propagation when it solves a particular puzzle. When a letter is entered into a
particular square of the puzzle, it is effective to see the way in which that letter constrains
the choices for the crossing words. But it appears not to be effective to propagate the
restriction further. So if, in the puzzle in Figure 1, we restrict 1-Down to be a word of
the form R...AR and the only word of that form in the dictionary is RAW BAR, we could
conceivably then propagate forward from the B in BAR to see the impact on 18-Across. In
actuality, however, the possibility that 1-Down be non-dictionary fill causes propagation
beyond a simple one-level lookahead to be of negative practical value. The sophisticated
propagation techniques mentioned in Section 2.1 appear not to be suitable in this domain.
A second consequence of the unrestricted domain sizes is that it is always possible to
extend a partial solution in a way that honors the hard constraints in the problem. We can
do this by simply entering random letters into each square of the puzzle (but only one letter
per square, so that the horizontal and vertical choices agree). Each such random string is
legal, and may even be correct. The reason such fills are in general avoided is that random
strings are assigned very high cost by the soft constraints in our formulation.
The fact that partial solutions can always be extended to satisfy the hard constraints is a
difference between the problem being solved by Dr.Fill and those considered elsewhere in
the csp literature. Here, however, there is an exception. Much of the work on probabilistic
analysis using Markov random fields focuses on a probabilistic maximization similar to ours,
once again in an environment where solving the hard constraints is easy but maximizing
the score of the result is hard.
A popular inference technique in the Markov setting is dual decomposition, where the
roles of the variables and values are switched, with Lagrange multipliers introduced corresponding to the variable values and their values then optimized to bound the quality of
the solution of the original problem (Sontag, Globerson, & Jaakkola, 2011, for example).
This is similar to the csp notion of duality, where the roles of variables and values are also
exchanged.
It is not clear how to apply this idea in our setting. In the probabilistic case, the variable
values are probabilities selected from a continuous set of real numbers. In the crossword
case, the domain is still impracticably large but there does not appear to be any natural
ordering or notion of continuity between one string value and the next.
There is one further difference between the crossword domain and more standard ones
that is also important to understand. Consider the themed crossword called Heads of
State from the 2010 acpt. The theme entries in this puzzle were common phrases with
two-letter state abbreviations appended to the beginning. Thus [Film about boastful jerks?]
clues VAIN GLOURIOUS BASTERDS, which is the movie title INGLOURIOUS BASTERDS
863

fiGinsberg

together with the two-letter state abbreviation VA. [Origami?] clues PAPER FORMING,
which is PA adjoined to PERFORMING, and so on.
These multiword fills that do not appear explicitly in the dictionary score fairly badly.
In most conventional csps, it is reasonable to respond to this by filling the associated words
earlier in the search. This allows better values to be assigned to these apparently difficult
variables. This general idea underlies Joslin and Clements (1999) squeaky wheel optimization and virtually every more recent variable selection heuristic, such as Boussemart
et. als (2004) notion of constraint weighting, and the dom/wdeg heuristic (Lecoutre, Sas,
Tabary, & Vidal, 2009, and others).
In the crossword domain, however, words that score badly in this way should arguably
be filled later in the search, as opposed to earlier. There is obviously no way for a program
such as Dr.Fill, with extremely limited domain knowledge. to figure out that a 21-letter
word for a [Film about boastful jerks?] should be VAIN GLOURIOUS BASTERDS. The
hints suggested by the crossing words are essential (as they are for humans as well). So none
of the classic variable selection heuristics can be applied here, and something else entirely
is needed. The heuristics that we use are presented in Section 3.
Before moving on, there are two final points that we should make. First, our goal is
to find fill for which (4) is maximized; in other words, to maximize our chances of solving
the entire puzzle correctly. This is potentially distinct from the goal of entering as many
correct words as possible, in keeping with the scoring metric of the acpt as described in
Section 5. The best human solvers generally solve the acpt puzzles perfectly, however, so
if the goal is to win the acpt, maximizing the chances of solving the puzzles perfectly is
appropriate.
Second, we designed Dr.Fill so that it could truly exploit the power of its underlying
search algorithm. In constructing the  function of (5), for example, we dont require that
the correct solution for a clue ci be the specific fill fi for which (5) is minimized, but only
hope that the correct fi be vaguely near the top of the list. The intention is that the hard
constraints corresponding to the requirement that the filling words mesh do the work
for us. As with so many other automated game players (Campbell, Hoane, & Hsu, 2002;
Ginsberg, 2001; Schaeffer, Treloar, Lu, & Lake, 1993), we will rely on search to replace
understanding.
2.4 Data Resources Used by Dr.Fill
One of the most important resources available to Dr.Fill is its access to a variety of
databases constructed from online information. We briefly describe each of the data sources
here; a summary is in Table 1. The table also includes information on the size of the
analogous data source used by Littmans crossword solving program Proverb (Littman
et al., 2002).
2.4.1 Puzzles
Dr.Fill has access to a library of over 47,000 published crosswords. These include virtually
all of the major published sources, including the New York Times, the (now defunct) New
York Sun, the Los Angeles Times, USA Today, the Washington Post, and many others.
Most puzzles from the early 1990s on are included.

864

fiDr.Fill: A Crossword Solver

data type
puzzles
clues
unique clues
small dictionary
big dictionary
word roots
synonyms
Wikipedia titles
Wikipedia pairs

source
various
http://www.otsys.com/clue
http://www.otsys.com/clue
http://www.crossword-compiler.com
various
http://wordnet.princeton.edu
WordNet and other online
http://www.wikipedia.org
http://www.wikipedia.org

quantity
47,693
3,819,799
1,891,699
8,452
6,063,664
154,036
1,231,910
8,472,583
76,886,514

quantity (Proverb)
5,142
350,000
250,000
655,000
2,100,000
154,036 (?)
unknown



Table 1: Data used by Dr.Fill
Collectively, these puzzles provide a database of just over 3.8 million clues, of which
approximately half are unique. This is to be contrasted with the corresponding database in
Proverb, which contains some 5,000 puzzles and 250,000 unique clues.
The clue database is available from http://www.otsys.com/clue, a public-domain clue
database used by many crossword constructors. The underlying data is compressed, but
the source code is available as well and should enable interested parties to decompress the
data in question.
2.4.2 Dictionaries
As with Proverb, Dr.Fill uses two dictionaries. A small dictionary is intended to contain
common words, and a larger one is intended to contain everything. The larger dictionary is an amalgamation from many sources, including Moby7 and other online dictionaries,
Wikipedia titles, all words that have ever been used in crosswords, and so on.
The small dictionary is the basic English dictionary that is supplied with Crossword
Compiler, an automated tool that can be used to assist in the construction of crosswords.
The large dictionary is much more extensive. Every entry in the large dictionary is
also marked with a score that is intended to reflect its crossword merit. Some words are
generally viewed as good fill, while others are bad. As an example, BUZZ LIGHTYEAR is
excellent fill. It is lively and has positive connotations. The letters are interesting (high
Scrabble score, basically), and the combination ZZL is unusual. TERN is acceptable fill;
the letters are mundane and the word is overused in crosswords, but the word itself is at
least well known. ELIS (Yale graduates) is poor fill. The letters are common, the word
is obscure, and its an awkward plural to boot. Crossword merit for the large dictionary
was evaluated by hand scoring approximately 50,000 words (100 volunteers, all crossword
constructors, scored 500 words each). The words were then evaluated against many criteria
(length, Scrabble score, number of Google hits,8 appearances in online corpora, etc.) and a
linear model was built that best matched the 50,000 hand-scored entries. This model was
used to score the remaining words.
7. http://icon.shef.ac.uk/Moby/mwords.html
8. I would like to thank Google in general and Mark Lucovsky in particular for allowing me to run the
approximately three million Google queries involved here.

865

fiGinsberg

Note that the scores here reflect the crossword value of the words in isolation, ignoring
the clues. Thus we cannot use the dictionaries alone to solve crosswords; indeed, for any
particular crossword, there will be many legal fills and the actual solution is unlikely to be
anywhere near the best fill in terms of word merit alone.
2.4.3 Grammatical and Synonym Information
Grammatical information is collected from the data provided as part of the WordNet
project (Fellbaum, 1998; Miller, 1995). This includes a list of 154,000 words along with
their parts of speech and roots (e.g., WALKED has WALK as a root). Proverb also cites
WordNet as a source. In addition, a list of 1.2 million synonyms was constructed from an
online thesaurus.
2.4.4 Wikipedia
Finally, a limited amount of information was collected from Wikipedia specifically. Dr.Fill
uses a list of all of the titles of Wikipedia entries as a source of useful names and phrases, and
uses a list of every pair of consecutive words in Wikipedia to help with phrase development
and fill-in-the-blank type clues. There are approximately 8.5 million Wikipedia titles, and
Wikipedia itself contains 77 million distinct word pairs.

3. Heuristics
At a high level, most csps are solved using some sort of depth-first search. Values are
assigned to variables and the procedure is then called recursively. In pseudocode, we might
have:
Procedure 3.1 To compute solve(C, S), a solution to a csp C that extends a partial
solution S:
1
2
3
4
5
6
7
8
9

if S assigns a value to every variable in VC , return S
v  a variable in VC unassigned by S
for each d  Dv (C|S )
do S 0  S  (v = d)
C 0  propagate(C|S 0 )
if C 0 6= 
then Q  solve(C 0 , S 0 )
if Q 6= , return Q
return 

We select an unassigned variable, and try each possible value. For each value, we set
the variable to the given value and propagate in some unspecified way. We assume that
this propagation returns the empty set as a failure marker if a contradiction is discovered,
in which case we try the next value for v. If the propagation succeeds, we try to solve the
residual problem and, if we manage to do so, we return the result.

866

fiDr.Fill: A Crossword Solver

Proposition 3.2 Let C be a csp of size n. Then if the propagate function is sound, the
value solve(C, ) computed by Procedure 3.1 is  if C has no solutions, and a solution to
C otherwise.
Proof. The proof is by induction on n. For a csp of size 1, each live domain value is tried
for the variable in question; when one survives the propagate construction, the solution is
returned by the recursive call in line 1 and then from line 8 as well.
For larger n, if the csp is not solvable, then every recursive call will fail as well so that
we eventually return  on line 9. If the csp is solvable, we will eventually set any particular
variable v to the right value d  D so that the recursive call succeeds and a solution is
returned.
2
For weighted csps, the algorithmic situation is more complex because we want to return
the best solution, as opposed to any solution. We can augment Procedure 3.1 to also accept
an additional argument that, if nonempty, is the currently best known solution B. We need
the following easy lemma:
Lemma 3.3 In a wcsp where the costs are non-negative, if S1  S2 , then c(S1 )  c(S2 ).
Proof. This is immediate; more costs will be incurred by the larger set of assignments.
Note that this is true for wcsps generally, not just singly weighted csps.
2
For convenience, we introduce an inconsistent assignment  and assume that c() is
infinite. Now we can modify Procedure 3.1 as follows:
Procedure 3.4 To compute solve(C, S,B), the best solution to a wcsp C that extends a
partial solution S given a currently best solution B:
1
2
3
4
5
6
7
8

if c(S)  c(B), return B
if S assigns a value to every variable in VC , return S
v  a variable in VC unassigned by S
for each d  Dv (C|S )
do S 0  S  (v = d)
C 0  propagate(C|S 0 )
if C 0 6= , B  solve(C 0 , S 0 , B)
return B

We use the B notation at the beginning of the procedure to indicate that B is passed
by reference, so that when B is changed on line 7, the value of B used in other recursive
calls is changed as well.
In the loop through variable values, we can no longer return a solution as soon as we
find one; instead, all we can do is update the best known solution if appropriate. This
will, of course, dramatically increase the number of nodes that are expanded by the search.
There is some offsetting saving in the comparison on line 1; if the cost of a partial solution
is higher than the total cost of the best known solution, Lemma 3.3 ensures that we need
not expand this partial solution further. Note that the conditions of the lemma are satisfied
in Dr.Fill, since the costs are negated logarithms of probabilities, and the probabilities
can be assumed not to exceed one.
867

fiGinsberg

Proposition 3.5 Let C be a csp. Then the value solve(C, , ) computed by Procedure 3.4 is  if C has no solutions, and the least cost solution to C otherwise.
Proof. Suppose first that we drop line 1 and replace line 7 with
if C 0 6=   c(solve(C 0 , S 0 , B)) < c(B) then B  solve(C 0 , S 0 , B)

(6)

Now the result follows easily by an inductive argument similar to the proof of Proposition 3.1. Every possible solution will be considered, and we will gradually find the least
cost one to return.
Consider now Procedure 3.4 as written. If we return a set S on line 2, we must have
c(S) < c(B) by virtue of the test on line 1. Thus the new requirement in (6), namely that
c(solve(C 0 , S 0 , B)) < c(B), will always be satisfied and the proof will be complete if we can
show simply that the test on line 1 will never discard the best solution. In other words,
we need to show that for any solution S 0 discarded as a result of the test on line 1, we
will have c(S 0 )  c(B). But this follows directly from Lemma 3.3, since c(S 0 )  c(S) and
c(S)  c(B).
2
Procedure 3.4 is the historical method of choice on wcsps. It is generally referred to as
branch and bound because the cost of the best solution B found in one branch is used to
bound the searches in other branches.
To implement the procedure, we need to specify mechanisms by which variables are
selected on line 3 and the domain is ordered on line 4. We discuss value selection first and
then variable selection. As described in Section 2.3, the propagation mechanism most useful
in crossword solving considers only the direct impact of a word selection on the crossing
words.
3.1 Value Selection
The performance of Procedure 3.4 depends critically on the order in which values are selected
from the domain D. The sooner we find good solutions, the earlier we can use the test in
line 1 to prune the subsequent search. This kind of argument will remain valid even after we
replace Procedure 3.4 with other algorithms that are more effective in practice; it is always
advantageous to order the search so that the final solution is found earlier, as opposed to
later.
There are a variety of elements to this. First, note that all we really need for line 4
of Procedure 3.4 is a function fill(v, n) that returns the nth element of vs live domain
Dv . On each pass through the loop, we call fill while gradually increasing the value
of n. Faltings and Macho-Gonzalez (2005) take a similar approach in their work on open
constraint programming.
As in the work on open constraint programming, this observation allows us to deal with
the fact that not all crossword fills appear explicitly in the dictionary. Our scoring function
allows non-dictionary words, but assumes that an apparently unrelated string of words (or
of letters) is less likely to be correct than a word or phrase that actually appears in the
dictionary. This means that the fill function can evaluate all of the dictionary possibilities
before generating any multiwords. The multiwords are generated only as needed; by the

868

fiDr.Fill: A Crossword Solver

time they are needed, most of the letters in the word are generally filled. This narrows the
search for possible multiwords substantially.9
The implementation begins by scoring every word of the appropriate length and storing
the results with the domain for word n. When multiwords are needed and generated, they
are added to the end of the domain sets as appropriate.
This approach reduces the value selection problem to two subproblems. First, we need
the scoring function (fi , ci ) that evaluates fill fi given clue ci . Second, we need to use this
scoring function to produce an actual ordering on possible words to enter; the lowest cost
word may or may not be the one we wish to try first.
We will not spend a great deal of time describing our scoring function; the details are
predictably fairly intricate but the ideas are simple. Fundamentally, we will take the view
that has proven so successful elsewhere in computer game players: It is more important
that the system be able to search effectively than that it actually have a terribly good idea
what it is doing. The power is always more in the search than in the heuristics. This
overall search-based approach underlies virtually all of the best computer game players
(Campbell et al., 2002; Ginsberg, 2001; Schaeffer et al., 1993) and search-based algorithms
have easily outperformed their knowledge-based counterparts (Smith, Nau, & Throop, 1996,
for example) in games where direct comparisons can be made.
We implement this idea with a scoring system that is in principle quite simplistic. Words
are analyzed based on essentially five criteria:10
1. A match for the clue itself. If a clue has been used before, the associated answer is
preferred. If a new clue shares a word or subphrase with an existing one, that answer
scores well also.
2. Part of speech analysis. If it is possible to parse the clue to determine the likely part
of speech of the answer, fill matching the desired part of speech is preferred. The part
of speech analysis is based on the WordNet dictionary (Fellbaum, 1998; Miller, 1995),
which is then used to search for parse patterns in the clue database. No external
syntax or other grammatical mechanisms are used.
3. Crossword merit as discussed in Section 2.4.2.
4. Abbreviation. Abbreviations in the dictionary are identified by assuming that words
that are generally clued using abbreviations are themselves abbreviations, as described
previously. This information is then used in scoring a possible answer to a new
clue. What exactly constitutes an abbreviation clue is determined by recursively
analyzing the clue database.
5. Fill-in-the-blank. Some clues are fill in the blanks. These generally refer to a
common phrase with a word missing, as in 24-A in Figure 1, where [Line ] clues
9. And, as remarked earlier, means that we need to value badly scoring variables late in the search as
opposed to early.
10. Proverb has some thirty individual scoring modules (Littman et al., 2002), although Littman has
suggested (personal communication) that most of the value comes from modules that are analogous to
those used by Dr.Fill. Proverb does not analyze the clues to determine the part of speech of the
desired fill.

869

fiGinsberg

ITEM. These clues are analyzed by looking for phrases that appear in the body of
Wikipedia.
These five criteria are then combined linearly. To determine the weights for the various
criteria, a specific set of weights (w1 , . . . , wn ) is selected and then used to solve each of a
fixed testbed of puzzles (the first 100 New York Times puzzles from 2010). For each puzzle
in the testbed, we count the number of words entered by the search procedure before a
mistake is made in that the heuristically chosen word is not the one that appears in the
known solution to the puzzle. The average number of words entered correctly is then the
score of (w1 , . . . , wn ) and the weights are varied to maximize the score.
Given the scoring function , how are we to order the values for any particular word?
We dont necessarily want to put the best values first, since the value that is best on this
word may force us to use extremely suboptimal choices for all of the crossing words.
More precisely, suppose that we assign value d to variable v, and that propagation now
reduces the variable domains to new values Di (C|S{v=d} ). An argument similar to that
underlying Lemma 2.8 now produces:
Proposition 3.6 Let C be an swcsp and S a partial solution, so that Du ((C|S{v=f } ))
is the domain for u after v is set to f and the result propagated. Then the minimum cost
of a solution to C that extends S  {v = f } is at least
X
min
(x, u).
2
(7)
u

xDu ((C|S{v=f } ))

We order the variable values in order of increasing total cost as measured by (7), preferring
choices that not only work well for the word slot in question, but also minimally increase
the cost of the associated crossing words.
This notion is fairly general. In any wcsp, whenever we choose a value for a variable,
the choice damages the solution to the problem at large; the amount of damage can
be determined by propagating the choice made using whatever mechanism is desired (a
simplistic approach such as ours, full arc consistency, Coopers linear relaxation, etc). Cost
is incurred not only by the choice just made, but as implied on other variables by the
propagation mechanism. (7) says that we want to choose as value for the variable v that
value for which the total global cost is minimized, not just the local cost for the variable
being valued.
In the crossword domain, this heuristic appears to be reasonably effective in practice.
Combined with the variable selection heuristic to be described in the next section, Dr.Fill
inserts an average of almost 60 words into a Times puzzle before making its first mistake.
3.2 Variable Selection
As argued in the previous section, the heuristic we use in valuing a possible fill f for a word
slot s in our puzzle is
X
X
h(f, v) =
min
(x, u) 
min (x, u)
(8)
u

xDu ((C|S{v=f } ))

870

u

xDu (C|S )

fiDr.Fill: A Crossword Solver

Because the domain for variable u before setting v to f is Du (C|S ), the term on the right
in (8) gives a lower bound on the best possible score of a complete solution before v is set
to f (and this expression is thus independent of f ).
The value of the term on the left is a lower bound on the best possible score after v is
set to f because the domain for u after setting v to f and propagating is Du ((C|S{v=f } )).
The heuristic value of setting v to f is the difference between these two numbers, the total
damage caused by the commitment to use fill f for variable v. Given (8), which variable
should we select for valuation at any point in the search?
It might seem that we should choose to value that variable for which h(f, v) is minimized.
This would cause us to fill words that could be filled without having a significant impact
on the projected final score of the entire puzzle. So we could define the heuristic value of a
slot v, which we will denote by H(v), to be
H(v) = min h(f, v)
f

(9)

This apparently attractive idea worked out poorly in practice, and a bit of investigation
revealed the reason. Especially early on, when there remains a great deal of flexibility in
the choices for all of the variables, there may be multiple candidate fills for a particular
clue, all of which appear attractive in that h(f, v) is small. In such a situation, there is
really no strong reason to prefer one of these attractive fills to the others, but using (9) as
the variable selection heuristic will force us to value such a variable and therefore commit
to such a choice.
The solution to this problem is to choose to value not that variable for which h(f, v)
is minimized, but the variable for which the difference between the minimum value and
second-best value is maximal. It is this difference that indicates how confident we truly
are that we will fill the slot correctly once we decide to branch on it. If we define min2(S)
to be the second-smallest element of a set S, then the variable selection heuristic we are
proposing is
H(s) = min2 h(f, v)  min h(f, v)
(10)
f

f

where large values are to be preferred over smaller ones.
As mentioned previously, a combination of (10) and (8) allows Dr.Fill to enter, on
average, an initial 59.4 words into a Times puzzle before it makes its first error.
It is important to realize that this metric  59.4 words inserted correctly on average  is
not because the scoring function accurately places the correct word first a large fraction
of the time. Instead, our methods are benefiting even at this point from anticipation of how
the search is likely to develop; the heuristics themselves are based as much on a glimpse of
the future search as they are on the word values in isolation. Indeed, if we use the variable
selection described here but switch our value selection heuristic to simply prefer the best fill
for the word in question (without considering the impact on subsequent search), the average
number of words filled correctly at the outset of the search drops to 25.3, well under half
of its previous value.

871

fiGinsberg

r
@
@







r
HH

HH


H
HH

H
HH
HHe
r
@
@
@
@

@
r
A
 A

A

A

A
Ae
r
r

@
@
@re
A
 A

A

A

A
Ae
r
h
r

r
A
 A

A

A

A
Ah
re
r

@
@h
re
A
 A

A

A

A
Ar
r

0

1

1

2

1

2

2

3

Figure 3: Limited discrepancy search

4. Limited Discrepancy Search
Given that Dr.Fill can enter nearly sixty correct words in a crossword before making an
error, one would expect it to be a strong solver when combined with the branch-and-bound
solving procedure 3.4. Unfortunately, this is not the case.
The reason is that this solving procedure suffers from what Harvey (1995) has called
the early mistakes problem. Once a mistake is made, it impacts the subsequent search
substantially and the mistake is never retracted until the entire associated subspace is
examined. An initial mistake at depth (say) sixty seems impressive but the quality of the
solution below this point is likely to be quite poor, and there is unlikely to be sufficient time
to retract the original error that led to the problem.
One way around this problem in csps with binary domains is to use limited discrepancy
search, or lds (Harvey & Ginsberg, 1995). The idea is that if a heuristic is present, we
define the discrepancy count of a partial solution S to be the number of times that S
violates the heuristic. In Figure 3, we have shown a simple binary search tree of depth
three; assuming that the heuristic choice is always to the left, we have labeled each fringe
node with the number of times that the heuristic is violated in reaching it.
Lds is an iterative search method that expands the tree using depth-first search and
in order of increasing discrepancy count. On the first iteration, only nodes without discrepancies are examined, so the search is pruned at each node in the figure with a single
bullseye. On the second iteration, a single discrepancy is permitted and the nodes with
double bullseyes are pruned. It is not hard to see that iteration n expands O(dn ) nodes,
as the discrepancy limit forms a barrier against a full search of the tree. Each iteration
also uses only O(d) memory, since the expansion on any individual iteration is depth first.
There is some work repeated from iteration to iteration, but since the bulk of the work in
iteration n involves nodes that were not expanded in iteration n  1, this rework has little

872

fiDr.Fill: A Crossword Solver

impact on performance. Korf (1996) presents an algorithmic improvement that addresses
this issue to some extent.
The point of lds is that it allows early mistakes to be avoided without searching large
portions of the space. In the figure, for example, if the heuristic is wrong at the root of the
tree, the node labeled 1 will be explored at the second iteration (with discrepancy limit 1),
without the need to expand the left half of the search space in its entirety.
While it is clear that the basic intuition underling lds is a good match for the search
difficulties encountered by Dr.Fill, it is not clear how the idea itself can be applied. One
natural approach would be to order the values for any particular word slot, and to then say
that using the second value (as opposed to the first) incurred one discrepancy, using the
third value incurred two discrepancies, and so on.
This doesnt work. Assuming that the first word in the list is wrong, subsequent words
may all score quite similarly. Just because we believed strongly (and wrongly, apparently)
that the first word was the best fill does not mean that we have a strong opinion about
what to use as a backup choice. The net result of this is that the best solution often uses
words quite late in the ordered list; these correspond to a very high discrepancy count and
are therefore unlikely to be discovered using this sort of an algorithmic approach.
An alternative idea is to say that a discrepancy is incurred when a variable is selected for
branching other than the variable suggested by the variable-selection heuristic (10). This
avoids the problem described in the previous paragraph, since we now will pick a fill for a
completely different word slot. Unfortunately, it suffers from two other difficulties.
The first (and the less important) is that in some cases, we wont want to change the
variable order after all. Perhaps there was a clear first choice and, once that choice is
eliminated, there is a clear second choice among the remaining candidate values. In such
an instance, we would want the single discrepancy search choice to try the second fill
instead of the first.
More important is the fact that the bad choice is likely to come back on the very next
node expansion, when we once again consider the variable in question. The word that
looked good when the discrepancy was incurred may well still look good, and we will wind
up having used the discrepancy but not really having changed the area of the search space
that we are considering.
The algorithm that we actually use combines ideas from both of these approaches. As
the search proceeds, we maintain a list P of value choices that have been discarded, or
pitched. Each element of P is a pair (v, x) indicating that the value x should not be
proposed for variable v. The pitched choices remain in the live set, but are not considered
as branch values for v until they are forced in that vs live set becomes a singleton. In
evaluating the heuristic expressions (8) and (10), pitched values are not considered.
We now incur a discrepancy by pitching the variable and value suggested by the heuristics. Assuming that we then completely recompute both the variable chosen for branching
and the value being used, the problems mentioned in the previous paragraphs are neatly
sidestepped. We continue to make choices in which we have confidence, and since a pitched
value remains pitched as the search proceeds, we do not repeat an apparent mistake later
in the search process.

873

fiGinsberg

Formally, we have:
Procedure 4.1 Let C be a wcsp. Let n be a fixed discrepancy limit and suppose that S is
a partial solution, B is the best solution known thus far, and P is the set of values pitched
in the search. To compute solve(C, S, B, n, P ), the best solution extending S with at most
n discrepancies:
1
2
3
4
5
6
7
8
9

if c(S)  c(B), return B
if S assigns a value to every variable in VC , return S
v  a variable in VC unassigned by S
d  an element of Dv (C|S ) such that (v, d) 6 P
S 0  S  (v = d)
C 0  propagate(C|S 0 )
if C 0 6= , B  solve(C 0 , S 0 , B, n, P )
if |P | < n, B  solve(C, S, B, n, P  (v, d))
return B

Proposition 4.2 Let C be a csp of size k. Then the value solve(C, , , n, ) computed
by Procedure 4.1 is  if C has no solutions. If C has a solution, there is some n0  k(|D|1)
such that for any n  n0 , solve(C, , , n, ) is the least cost solution to C.
Proof. There are essentially three separate claims in the proposition, which we address
individually.
1. If C has no solutions, then the test in line 2 will never succeed, so B will be  throughout
and the procedure will therefore return .
2. It is clear that the space explored with a larger n is a superset of the space explored
with a smaller n because the test in line 8 will succeed more often. Thus if there is any n for
which the best solution is returned, the best solution will also be returned for any larger n.
3. We claim that for n = k(|D|  1), every solution is considered, and prove this by
induction on k.
For k = 1, we have n = |D|  1. If we are interested in a particular choice x for the
unique variable in the problem, then after |D|  1 iterations through line 8, we will either
have selected x on line 4 or we will have pitched every other value in which case x will be
selected on the last iteration.
The argument in the inductive case is similar. For the variable v selected on line 3, we
will use up at most |D|  1 discrepancies before setting the v to the desired value, leaving
at least (n  1)(|D|  1) discrepancies to handle the search in the subproblem after v is
set.
2
Proposition 4.3 Let C be a csp of size k. Then for any fixed n, the number of node
expansions in computing solve(C, , , n, ) is at most (k + 1)n+1 .
Proof. Consider Figure 4, which shows the top of the lds search tree and labels the nodes
with the number of unvalued variables and number of unused discrepancies at each point.
874

fiDr.Fill: A Crossword Solver



s (k, n)
HH

HH


H
HH


H
HH (k, n  1)
Hs
%%e
e
%
e
%
e
%
ees
s%

(k  1, n)s
%%e
%
%
s
%

%

(k  2, n)

e
e
e
ees

(k  1, n  1)

(k  1, n  1)

(k, n  2)

Figure 4: Limited discrepancy search
At the root, therefore, there are k variables left to value and n discrepancies available. If
we branch left, we assign a value to some variable. If we branch right, we pitch that choice
so that there are still k variables left to value but only n  1 discrepancies available.
It follows that if we denote by f (d, m) the size of the search tree under the point with
d variables and m discrepancies, we have

f (d, m) = 1 + f (d, m  1) + f (d  1, m)

(11)

= 1 + f (d, m  1) + 1 + f (d  1, m  1) + f (d  2, m)

(12)

= 2 + f (d, m  1) + f (d  1, m  1) + f (d  2, m)
..
.
d
X
= k+
f (i, m  1) + f (d  k, m)

(13)

i=dk+1

= d+

d
X

f (i, m  1) + f (0, m)

(14)

i=1

= d+1+

d
X

f (i, m  1)

(15)

i=1

 d + 1 + (d  1)[f (d, m  1)  1] + f (d, m  1)

(16)

= 2 + df (d, m  1)]
(11) follows from counting the nodes as in the figure. (12) is the result of expanding the
last term in (11), corresponding to expanding the node labeled (k  1, n) in the figure.
(13) continues to expand the corresponding term a total of k times, and (14) is just (13)
with k = d. But f (0, m) = 1 because there are no variables left to value, producing (15).
(16) follows because f (i, m  1)  f (d, m  1)  1 for all 0 < i < d (in the figure, every step
down the left side is at least one node smaller).

875

fiGinsberg

Given f (d, m)  2 + df (d, m  1), we have
f (d, m)
2

+d1+d
f (d, m  1)
f (d, m  1)
Now f (d, 0) = d+1 because the search must progress directly to the fringe if no discrepancies
remain. Thus f (d, m)  (1 + d)1+m . Taking m = n and d = k at the root of the tree now
produces the desired result.
2

5. Dr.Fill as a Crossword Solver
At this point, we have described enough of Dr.Fills underlying architecture that it makes
sense to report on the performance of the system as described thus far.11
Our overall experimental approach is as follows.
First, we tune the word scoring function . Although there are only five basic contributions to the value of  for any particular clue and fill, there are currently twenty-four
tuning parameters that impact both the five contributions themselves and the way in which
they are combined to get an overall value for . As described in Section 3, the goal is to
maximize the average number of words entered correctly when beginning to solve any of
the first 100 Times puzzles of 2010.
This tuning process is time consuming; Dr.Fill spends approximately one cpu minute
analyzing the clues in any given puzzle to determine the value of  for words in its dictionary.
This analysis often needs to be repeated if the tuning parameters are changed; it follows
that a single run through the testbed of 100 puzzles takes about an hour. The clue analysis
is multithreaded and the work is done on an 8-processor machine (two 2.8GHz quad-core
Xeons), which reduces wall clock time considerably, but it remains impractical to sample
the space of parameter values with other than coarse granularity, and the parameters must
in general be tuned independently of one another even though a variety of cross effects
undoubtedly exist.
After the tuning is complete, Dr.Fill is evaluated on the puzzles from the 2010 acpt
(American Crossword Puzzle Tournament). This is a set of only seven puzzles, but algorithmic and heuristic progress appear to translate quite well into progress on the acpt sample.
The puzzles are scored according to the acpt rules, and Dr.Fills total score is examined
to determine where it would have ranked had it been a competitor.
The acpt scoring on any particular puzzle is as follows:
1. 10 points for each correct word in the grid,
2. 25 bonus points for each full minute of time remaining when the puzzle was completed.
This bonus is reduced by 25 points for each incorrect letter, but can never be negative.
3. 150 bonus points if the puzzle is solved correctly.
11. Dr.Fill is written in C++ and currently runs under MacOS 10.6. It needs approximately 3.5 GB
of free memory to run, and is multithreaded. The multithreading uses posix threads and the GUI
is written using wxWidgets (www.wxwidgets.org). The code and underlying data can be obtained (for
noncommercial use only) by contacting the author. The code can be expected to run virtually unchanged
under Linux; Windows will be more of a challenge because Windows has no native support for posix
threads.

876

fiDr.Fill: A Crossword Solver

version
lds
postprocess
and/or
best human

1
1280
1280
1280
1230

2
925
1185
1185
1615

3
1765
1790
1815
1930

4
1140
1165
1165
1355

5
1690
1690
1690
1565

6
2070
2070
2095
1995

7
1920
2030
2080
2515

total
10790
11210
11310
12205

rank
89 (tied)
43 (tied)
38
1

Table 2: Results from the 2010 acpt
Since the puzzles are timed, Dr.Fill needs some sort of termination condition. It stops
work and declares its puzzle complete if any of the following conditions occur:
1. A full minute goes by with no improvement in the cost of the puzzle as currently filled,
2. A full lds iteration goes by with no improvement in the cost of the puzzle as currently
filled, or
3. The acpt time limit for the puzzle is reached.
Results for this and other versions of Dr.Fill appear in Table 2, with scores by puzzle,
total score for the tournament, and ranking had Dr.Fill competed. We also give scores for
the human (Dan Feyer) who won the event.12 The first and fourth puzzles are generally the
easiest, and the second and fifth puzzles are the hardest. The lds-based Dr.Fill scored a
total of 10,790, good enough for 89th place.
After the evaluation is complete, an attempt is generally made to improve Dr.Fills
performance. We examine puzzles from the Times testbed (not the acpt puzzles, which we
try to keep as clean as possible) and try to understand why mistakes were made. These
mistakes can generally be classified as one of three types:
1. Heuristic errors, in that the words entered scored better than the correct ones even
though they were not the correct fill,
2. Search errors, where the words entered scored worse than the correct ones but Dr.Fill
did not find a better fill because the discrepancy limit was reached, and
3. Efficiency errors, where points were lost because the search took a long time to
complete.
Heuristic errors generally lead to a change in the scoring algorithms in some way, although generally not to the introduction of new scoring modules. Perhaps a different
thesaurus is used, or the understanding of theme entries changes. Search errors may lead
to modifications of the underlying search algorithm itself, as in Sections 6 and 7. Dr.Fill
has a graphical user interface that allows the user to watch the search proceed, and this is
often invaluable in understanding why the program performed as it did. Efficiency issues
can also (sometimes) be corrected by allowing the visual search to suggest algorithmic modifications; this convinced us that it was worthwhile to treat the overall csp as an and/or
tree as discussed in Section 7.
12. Feyer went on to win in 2011 as well; Tyler Hinman was the acpt champion from 20052009.

877

fiGinsberg

6. Postprocessing
An examination of Dr.Fills completed puzzles based on the algorithms presented thus
far reveals many cases where a single letter is wrong, and the problem is with the search
instead of the heuristics. In other words, replacing the given letter with the right one
decreases the total cost of the puzzles fill. This would presumably have been found with a
larger discrepancy limit, but was not discovered in practice.
This suggests that Dr.Fill would benefit from some sort of postprocessing. The simplest approach is to simply remove each word from the fill, and replace it with the best word
for the slot in question. If this produces a change, the process is repeated until quiescence.
6.1 Formalization and Algorithmic Integration
We can formalize this process easily as follows:
Procedure 6.1 Given a csp C and a best solution B, to compute post(C, B), the result
of attempting to improve B with postprocessing:
1 change  true
2 while change
3
do change  false
4
for each v  CV
5
do B 0  B
6
unset the value of v in B 0
7
B 0  solve(C, B 0 , B 0 )
8
if c(B 0 ) < c(B)
9
then B  B 0
10
change  true
11 return B
We work through the puzzle, erasing each word in line 6. We then re-solve the puzzle
(line 7), so that if there is a better choice for that word in isolation, it will be found. If this
leads to an improvement, we set a flag on line 10 and repeat the entire process. Note that
we only erase one word at a time, since we always begin with the currently best solution in
line 5.
As with AC-3, Procedure 6.1 can be improved somewhat by realizing that on any particular iteration, we need only examine variables that share a constraint with a variable
changed on the previous iteration. In practice, so little of Dr.Fills time is spent postprocessing that efficiency here is not a concern.
Lemma 6.2 For any csp C and solution B, c(post(C, B))  c(B).

2

How are we to combine Procedure 6.1 with the basic search procedure 4.1 used by
Dr.Fill itself? We can obviously postprocess the result computed by Procedure 4.1 before
returning it as our final answer, but if postprocessing works effectively, we should surely
postprocess all of the candidate solutions considered. That produces:
878

fiDr.Fill: A Crossword Solver

Procedure 6.3 Let C be a wcsp. Let n be a fixed discrepancy limit and suppose that S is
a partial solution, B is the best solution known thus far, and P is the set of values pitched
in the search. To compute solve(C, S,B, n, P ), the best solution extending S with at most
n discrepancies:
1
2
3
4
5
6
7
8
9

if c(S)  c(B), return B
if S assigns a value to every variable in VC , return post(C, S)
v  a variable in VC unassigned by S
d  an element of Dv (C|S ) such that (v, d) 6 P
S 0  S  (v = d)
C 0  propagate(C|S 0 )
if C 0 6= , B  solve(C 0 , S 0 , B, n, P )
if |P | < n, B  solve(C, S, B, n, P  (v, d))
return B

The only difference between this and Procedure 4.1 is on line 2, where we postprocess the
solution before returning it.
6.2 Interaction With Branch and Bound
Further thought reveals a potential problem with this approach. Suppose that our original
procedure 4.1 first produces a solution B1 and subsequently produces an improvement B2 ,
with c(B2 ) < c(B1 ). Suppose also that postprocessing improves both solutions comparably,
so that c(post(B2 )) < c(post(B1 )). And finally, suppose that postprocessing improves the
solutions considerably, so much so, in fact, that c(post(B1 )) < c(B2 ).
We are now in danger of missing B2 , since it will be pruned by the test on line 1 of
Procedure 6.3. B2 will allow us to find a better solution, but only after postprocessing. If
we prune B2 early, we will never postprocess it, and the improvement will not be found
until a larger discrepancy limit is used.
This suggests that we return to the earlier possibility of postprocessing only the final
answer returned by Procedure 4.1, but that may not work, either. Perhaps B1 is improved
by postprocessing and B2 is not; once again, the best solution may be lost.
The problem is that branch-and-bound and postprocessing are fundamentally inconsistent; it is impossible to use both effectively. The very idea of branch-and-bound is that
a solution can be pruned before it is complete if its cost gets too large. The very idea
of postprocessing is that the final cost of a solution cannot really be evaluated until the
solution is complete and the postprocess has been run.
Our solution to this is to remove branch and bound from Dr.Fills search algorithm,
producing:

879

fiGinsberg

Procedure 6.4 Let C be a wcsp. Let n be a fixed discrepancy limit and suppose that S is
a partial solution, B is the best solution known thus far, and P is the set of values pitched
in the search. To compute solve(C, S,B, n, P ), the best solution extending S with at most
n discrepancies:
1
2
3
4
5
6
7
8
9

if S assigns a value to every variable in VC ,
return whichever of B and post(C, S) has lower cost
v  a variable in VC unassigned by S
d  an element of Dv (C|S ) such that (v, d) 6 P
S 0  S  (v = d)
C 0  propagate(C|S 0 )
if C 0 6= , B  solve(C 0 , S 0 , B, n, P )
if |P | < n, B  solve(C, S, B, n, P  (v, d))
return B

Since the test on line 2 ensures that we only change the best solution B when an
improvement is found, all of our previous results continue to hold. But is it really practical
to abandon branch and bound as a mechanism for controlling the size of the search?
It is. One reason is that the size of the search is now being controlled by lds via
Proposition 4.3. For any fixed discrepancy limit n, this guarantees that the number of
nodes expanded is polynomial in the size of the problem being solved.
More important, however, is that experimentation showed that branch-and-bound was
ineffective in controlling Dr.Fills search. The reason is the effectiveness of the (searchanticipating) heuristics used in Dr.Fill itself. These heuristics are designed to ensure that
the words inserted early in the search both incur little cost themselves and allow crossing
words to incur low cost as well. What happens in practice is that the costs incurred early
are extremely modest. Even when a mistake is made, attention typically changes to a
different part of the puzzle because filling an additional word w near the mistake begins
to have consequences on the expected cost of the words crossing w. Eventually, the rest
of the puzzle is complete and the algorithm finally begrudgingly returns to w and the cost
increases.
Thinking about this, what happens is that while the cost does eventually increase when
an error is made, the increase is deferred until the very bottom of the search tree, or nearly
so. With so much of the cost almost invariably accumulating at the bottom the search tree,
branch and bound is simply an ineffective pruning tool in this domain. The nature of the
argument suggests that in other wcsps that are derived from real-world problems, good
heuristics may exist and branch and bound may provide little value in practical problem
solving.13
6.3 Results
The results of Procedure 6.4 appear in Table 2. Dr.Fills score improves to 11,210, which
would have earned it a tie for 43rd place in the 2010 tournament.
13. That said, there are certainly real-world problems where branch-and-bound is useful, such as the use of
MendelSoft to solve cattle pedigree problems (Sanchez, de Givry, & Schiex, 2008).

880

fiDr.Fill: A Crossword Solver

7. AND/OR Search
There is one further algorithmic improvement that is part of Dr.Fill as the system is
currently implemented.
As we watched Dr.Fill complete puzzles, there were many cases where it would fill
enough of the puzzle that the residual problem would split into two disjoint subproblems.
The search would then frequently oscillate between these two subproblems, which could
clearly introduce inefficiencies.
This general observation has been made by many others, and probably originates with
Freuder and Quinn (1985), who called the variables in independent subproblems stable
sets. McAllester (1993) calls a solution technique a polynomial space aggressive backtracking
procedure if it solves disjoint subproblems in time that is the sum of the times needed for
the subproblems independently. Most recently, Marinescu and Dechter (2009) explore this
notion in the context of constraint propagation specifically, exploiting the structure of the
associated search spaces as and/or graphs.
None of this work is directly applicable to Dr.Fill because it needs to be integrated
appropriately with lds. But the integration itself is straightforward:
Definition 7.1 Let C be a csp or wcsp. We will say that C splits if there are nonempty
V1 , V2  VC such that V1  V2 = , V1  V2 = VC , and no constraint or weighted constraint
in C mentions variables in both V1 and V2 . We will denote this as C = C|V1 + C|V2 .
Proposition 7.2 Suppose that C is a csp that splits into V1 and V2 . Then if S1 is a
solution to C|V1 and S2 is a solution to C|V2 , S1  S2 is a solution to C, and all solutions
to C can be constructed in this fashion.
In addition, if C is a wcsp, then the least cost solution to C is the union of the least
cost solutions to C|V1 and C|V2 .
2
Note also that we can check to see if C splits in low order polynomial time by checking
to see if the constraint graph associated with C is connected. If so, C does not split. If the
constraint graph is disconnected, C splits.
Procedure 7.3 Let C be a wcsp. Let n be a fixed discrepancy limit and suppose that S is
a partial solution, B is the best solution known thus far, and P is the set of values pitched
in the search. To compute solve(C, S, B, n, P ), the best solution extending S with at most
n discrepancies:
1
2
3
4
5
6
7
8
9
10
11

if S assigns a value to every variable in VC ,
return whichever of B and post(C, S) has lower cost
if C splits into V and W ,
return solve(C|V , S|V , B|V , n, P )  solve(C|W , S|W , B|W , n, P )
v  a variable in VC unassigned by S
d  an element of Dv (C|S ) such that (v, d) 6 P
S 0  S  (v = d)
C 0  propagate(C|S 0 )
if C 0 6= , B  solve(C 0 , S 0 , B, n, P )
if |P | < n, B  solve(C, S, B, n, P  (v, d))
return B
881

fiGinsberg

Puzzle
1
2
3
4
5
6
7
total

Words
78
94
118
78
94
122
144
643

Letters
185
237
301
187
245
289
373
1817

Words wrong
0
8
4
4
0
0
11
27

Dr.Fill
Letters wrong
0
11
2
2
0
0
13
28

Time
1
2
2
2
1
1
2
11

Score
1280
1185
1815
1165
1690
2095
2080
11310

Feyer
Time Score
3
1230
4
1615
6
1930
3
1355
6
1565
5
1995
8
2515
35
12205

Table 3: Results from the 2010 acpt
Note that in line 4, we solve each of the split subproblems with a discrepancy limit of n.
So if (for example) we currently have n = 3 with one discrepancy having been used at the
point that the split occurs, we will be allowed two additional discrepancies in solving each
subproblem, perhaps allowing five discrepancies in total.
In spite of this, the node count will be reduced. If there are d variables remaining when
the split is encountered, solving the unsplit problem with m remaining discrepancies might
expand (1 + d)1+m nodes (Proposition 4.1), while solving the split problems will expand at
most
(1 + d1 )1+m + (1 + d  d1 )1+m
(17)
nodes. A small amount of calculus and algebra14 shows that (1 +d1 )1+m + (1 + d  d1 )1+m 
(1 + d)1+m for m  1, so that the split search will be faster even though more total
discrepancies are permitted.
The change embodied in Procedure 7.3 significantly improves performance on later
lds iterations, and it is arguable that we should exploit this improvement by modifying
Dr.Fills current strategy of terminating the search when an increase in the lds limit
does not produce an improved solution. Even without such modification, the increased
speed of solution improves Dr.Fills acpt score by 100 points (one minute faster on puzzles 3 and 6, and two minutes faster on puzzle 7), moving it up to a notional 38th place in
the 2010 event.
Detailed performance of this final version on the 2010 puzzles is shown in Table 3. For
each puzzle, we give the number of words and letters to be filled, and the number of errors
made by Dr.Fill in each area. We also give the time required by Dr.Fill to solve the
program (in minutes taken), along with the time taken by Dan Feyer, the human winner of
the contest. (Feyer made no errors on any of the seven puzzles.) As can be seen, Dr.Fill
had 27 incorrect words (out of 643, 95.8% correct) and 28 incorrect letters (out of 1817,
98.5% correct) over the course of the event.
14. Differentiating (17) shows that the worst case for the split is d1 = 1, so we have to compare 21+m + d1+m
and (d + 1)1+m . Multiplying out (d + 1)1+m produces d1+m + (1 + m)dm +   , and 21+m < (1 + m)dm
if d  2 and m  1.

882

fiDr.Fill: A Crossword Solver

8. Related and Future Work
There is a wide variety of work on wcsps in the academic literature, and we will not repeat
any particular element of that work here. What distinguishes our contribution is the fact
that we have been driven by results on a naturally occurring problem: that of solving
crossword puzzles. This has led us to the following specific innovations relative to earlier
work:
 The development of a value selection heuristic based on the projected cost of assigning
a value both to the currently selected variable and to all variables with which this
variable shares a constraint,
 The development of a variable selection heuristic that compares the difference between
the projected cost impacts of the best and second-best values, and branches on the
variable for which this difference is maximized,
 A modification of limited discrepancy search that appears to work well for weighted
csps with large domain sizes,
 The recognition that branch-and-bound may not be an effective search technique in
wcsps for which reasonably accurate heuristics exist, and
 The development and inclusion of an effective postprocessing algorithm for wcsps,
and the recognition that such postprocessing is inconsistent with branch-and-bound
pruning.
We do not know the extent to which these observations are general, and the extent to
which they are a consequence of the properties of the crossword csp itself. As discussed
previously, crossword csps have a relatively small number of variables but almost unlimited
domain sizes, and variables whose valuations incur significant cost should in general be filled
late as opposed to early.
The two existing projects that most closely relate to Dr.Fill are Proverb (Littman
et al., 2002), the crossword solver developed by Littman et. al in 1999, and Watson (Ferrucci et al., 2010), the Jeopardy-playing robot developed by ibm in 2011. All three systems
(Watson, Proverb, and Dr.Fill) respond to natural language queries in a game-like
setting. In all three cases, the programs seem to have very little idea what they are doing,
primarily combining candidate answers from a variety of data sources and attempting to
determine which answer is the best match for the query under consideration. This appears
to mesh well with the generally accepted view (Manning & Schuetze, 1999) that natural
language processing is far better accomplished using statistical methods than by a more
classical parse-and-understand approach.
The domain differences between Jeopardy and crosswords make the problems challenging
in different ways. In one sense, crosswords are more difficult because in Jeopardy, one is
always welcome to simply decline to answer any particular question. In crosswords, the
entire grid must be filled. On the other hand, the crossing words in a crossword restrict
the answer in a way that is obviously unavailable to Jeopardy contestants. Search plays
a key role in Dr.Fills performance in a way that Watson cannot exploit. As a result,
Dr.Fill can get by with relatively limited database and computational resources. The
883

fiGinsberg

program runs on a 2-core notebook with 8 GB of memory and uses a database that is just
over 300 MBytes when compressed. Watson needs much more: 2880 cores and 16 TB of
memory. Watson, like Dr.Fill, stores all of its knowledge in memory to improve access
speeds  but Watson relies on much more extensive knowledge than does Dr.Fill.
The programs are probably comparably good at their respective cognitive tasks. Dr.Fill
outperforms all but the very best humans in crossword filling, both in terms of speed (where
it is easily the fastest solver in the world) and in terms of accuracy. Watson, too, outperforms humans easily in terms of speed; its much-ballyhooed victory against human Jeopardy
competitors was probably due far more to Watsons mastery of button pushing than to its
question-answering ability. In terms of the underlying cognitive task, Watson appears to
not yet be a match for the best Jeopardy players, who are in general capable of answering
virtually all of the questions without error.
Dr.Fill itself remains a work in progress. Until this point, we have found heuristic and
search errors relatively easily by examining the performance of the program on a handful of
crosswords and simply seeing what went wrong. As Dr.Fills performance has improved,
this has become more difficult. We have therefore developed automated tools that examine
the errors made on a collection of puzzles, identify them as heuristic or search issues, and
report the nature of the errors that caused mistakes in the largest sections of fill. The results
of these tools will, we hope, guide us in improving Dr.Fills performance still further.
Acknowledgments
I would like to thank my On Time Systems coworkers for useful technical advice and assistance, and would also like to thank the crossword solving and constructing communities,
especially Will Shortz, for their warm support over the years. Daphne Koller, Rich Korf,
Michael Littman, Thomas Schiex, Bart Selman, and this papers anonymous reviewers provided me with invaluable comments on earlier drafts, making the paper itself substantially
stronger as a result. The work described in this paper relates to certain pending and issued
US patent applications, and the publication of these ideas is not intended to convey a license to use any patented information or processes. On Time Systems will in general grant
royalty-free licenses for non-commercial purposes.

References
Bistarelli, S., Montanari, U., Rossi, F., Schiex, T., Verfaillie, G., & Fargier, H. (1999).
Semiring-based CSPs and valued CSPs: Frameworks, properties, and comparison.
Constraints, 4.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004). Boosting systematic search by
weighting constraints. In Proceedings of ECAI-2004, pp. 146150.
Campbell, M., Hoane, A. J., & Hsu, F. (2002). Deep Blue. Artificial Intelligence, 134,
5783.
Cooper, M., de Givry, S., Sanchez, M., Schiex, T., Zytnicki, M., & Werner, T. (2010). Soft
arc consistency revisited. Artificial Intelligence, 174, 449478.
Doyle, J. (1979). A truth maintenance system. Artificial Intelligence, 12, 231272.

884

fiDr.Fill: A Crossword Solver

Ernandes, M., Angelini, G., & Gori, M. (2005). WebCrow: a WEB-based system for CROssWord solving. In Proceedings of the Twentieth National Conference on Artificial Intelligence, pp. 14121417.
Faltings, B., & Macho-Gonzalez, S. (2005). Open constraint programming. Artificial Intelligence, 161, 181208.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.
Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., Lally, A.,
Murdock, J. W., Nyberg, E., Prager, J., Schlaefer, N., & Welty, C. (2010). Building
Watson: An overview of the DeepQA poject. AI Magazine, 31 (3), 5979.
Freuder, E. C., & Quinn, M. J. (1985). Taking advantage of stable sets of variables in
constraint satisfaction problems. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, pp. 224229.
Ginsberg, M. L. (2001). GIB: Steps toward an expert-level bridge-playing program. Journal
of Artificial Intelligence Research, 14, 313368.
Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learned
from crossword puzzles. In Proceedings of the Eighth National Conference on Artificial
Intelligence, pp. 210215.
Givry, S. D., & Zytnicki, M. (2005). Existential arc consistency: Getting closer to full arc
consistency in weighted CSPs. In Proceedings of the Nineteenth International Joint
Conference on Artificial Intelligence, pp. 8489.
Harvey, W. D. (1995). Nonsystematic Backtracking Search. Ph.D. thesis, Stanford University, Stanford, CA.
Harvey, W. D., & Ginsberg, M. L. (1995). Limited discrepancy search. In Proceedings of
the Fourteenth International Joint Conference on Artificial Intelligence, pp. 607613.
Joslin, D. E., & Clements, D. P. (1999). Squeaky wheel optimization. Journal of Artificial
Intelligence Research, 10, 353373.
Korf, R. E. (1996). Improved limited discrepancy search. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 286291.
Larrosa, J., & Dechter, R. (2000). On the dual representation of non-binary semiring-based
CSPs. In Proceedings SOFT-2000.
Larrosa, J., & Schiex, T. (2004). Solving weighted CSP by maintaining arc consistency.
Artificial Intelligence, 159, 126.
Lecoutre, C., Sas, L., Tabary, S., & Vidal, V. (2009). Reasoning from last conflict(s) in
constraint programming. Artificial Intelligence, 173, 15921614.
Littman, M. L., Keim, G. A., & Shzaeer, N. (2002). A probabilistic approach to solving
crossword puzzles. Artificial Intelligence, 134, 2355.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8,
99118.

885

fiGinsberg

Manning, C. D., & Schuetze, H. (1999). Foundations of Statistical Natural Language Processing. MIT Press.
Marinescu, R., & Dechter, R. (2009). AND/OR branch-and-bound search for combinatorial
optimization in graphical models. Artificial Intelligence, 173, 14571491.
McAllester, D. A. (1993). Partial order backtracking. Unpublished technical report, MIT.
Miller, G. A. (1995). WordNet: A lexical database for English. Communications of the
ACM, 38 (11), 3941.
Sanchez, M., de Givry, S., & Schiex, T. (2008). Mendelian error detection in complex
pedigrees using weighted constraint satisfaction techniques. Constraints, 13.
Schaeffer, J., Treloar, N., Lu, P., & Lake, R. (1993). Man versus machine for the world
checkers championship. AI Magazine, 14 (2), 2835.
Smith, S. J., Nau, D. S., & Throop, T. (1996). Total-order multi-agent task-network planning for contract bridge. In Proceedings of the Thirteenth National Conference on
Artificial Intelligence, Stanford, California.
Sontag, D., Globerson, A., & Jaakkola, T. (2011). Introduction to dual decomposition for
inference. In Sra, S., Nowozin, S., & Wright, S. J. (Eds.), Optimization for Machine
Learning, pp. 219254. MIT Press.
Zytnicki, M., Gaspin, C., de Givry, S., & Schiex, T. (2009). Bounds arc consistency for
weighted CSPs. Journal of Artificial Intelligence Research, 35, 593621.

886

fiJournal of Artificial Intelligence Research 42 (2011) 815-850

Submitted 07/11; published 12/11

Stochastic Enforced Hill-Climbing
Jia-Hong Wu

JW @ ALUMNI . PURDUE . EDU

Institute of Statistical Science,
Academia Sinica, Taipei 115, Taiwan ROC

Rajesh Kalyanam
Robert Givan

RKALYANA @ PURDUE . EDU
GIVAN @ PURDUE . EDU

Electrical and Computer Engineering,
Purdue University, W. Lafayette, IN 47907, USA

Abstract
Enforced hill-climbing is an effective deterministic hill-climbing technique that deals with local optima using breadth-first search (a process called basin flooding). We propose and evaluate
a stochastic generalization of enforced hill-climbing for online use in goal-oriented probabilistic planning problems. We assume a provided heuristic function estimating expected cost to the
goal with flaws such as local optima and plateaus that thwart straightforward greedy action choice.
While breadth-first search is effective in exploring basins around local optima in deterministic problems, for stochastic problems we dynamically build and solve a heuristic-based Markov decision
process (MDP) model of the basin in order to find a good escape policy exiting the local optimum.
We note that building this model involves integrating the heuristic into the MDP problem because
the local goal is to improve the heuristic.
We evaluate our proposal in twenty-four recent probabilistic planning-competition benchmark
domains and twelve probabilistically interesting problems from recent literature. For evaluation,
we show that stochastic enforced hill-climbing (SEH) produces better policies than greedy heuristic
following for value/cost functions derived in two very different ways: one type derived by using
deterministic heuristics on a deterministic relaxation and a second type derived by automatic learning of Bellman-error features from domain-specific experience. Using the first type of heuristic,
SEH is shown to generally outperform all planners from the first three international probabilistic
planning competitions.

1. Introduction
Heuristic estimates of distance-to-the-goal have long been used in deterministic search and deterministic planning. Such estimates typically have flaws such as local extrema and plateaus that limit
their utility. Methods such as simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983; Cerny,
1985) and A* (Nilsson, 1980) search have been developed for handling flaws in heuristics. More
recently, excellent practical results have been obtained by flooding local optima using breadth-first
search. This method is called enforced hill-climbing (Hoffmann & Nebel, 2001).
Deterministic enforced hill-climbing (DEH) is proposed in the work of Hoffmann and Nebel
(2001) as a core element of the successful deterministic planner Fast-Forward (FF). DEH is an
extension of the basic hill-climbing approach of simply selecting actions greedily by looking
ahead one action step, and terminating when reaching a local optimum. DEH extends basic hillclimbing by replacing termination at local optima with breadth-first search to find a successor state
with strictly better heuristic value. The planner then moves to that descendant and repeats this
c
2011
AI Access Foundation. All rights reserved.

fiW U , K ALYANAM , & G IVAN

process. DEH is guaranteed to find a path to the goal if the problem itself is deadend-free (so that
every state has such a path). While that relatively weak guarantee applies independent of the quality
of the heuristic function, the intent of DEH is to remediate flaws in a generally accurate heuristic
in order to leverage that heuristic in finding short paths to the goal. In domains where the basin
size (search depth needed to escape any optimum) is bounded, DEH can provide a polynomial-time
solution method (Hoffmann, 2005).
Enforced hill-climbing is not defined for probabilistic problems, due to the stochastic outcomes
of actions. In the presence of stochastic outcomes, finding descendants of better values no longer
implies the existence of a policy that reaches those descendants with high probability. One may argue that FF-Replan (Yoon, Fern, & Givan, 2007)a top performer for recent probabilistic planning
benchmarksuses enforced hill-climbing during its call to FF. However, the enforced hill-climbing
process is used on a determinized problem, and FF-Replan does not use any form of hill climbing
directly in the stochastic problem. In fact, FF-Replan does not consider the outcome probabilities
at all.
One problem to consider in generalizing enforced hill-climbing to stochastic domains is that the
solution to a deterministic problem is typically concise, a sequential plan. In contrast, the solution
to a stochastic problem is a policy (action choice) for all possibly reached states. The essential
motivation for hill-climbing is to avoid storing exponential information during search, and even the
explicit solution to a stochastic problem cannot be directly stored while respecting this motivation.
For this reason, we limit consideration to the online setting, where the solution to the problem is a
local policy around the current state. After this local policy is committed to and executed until the
local region is exited, the planner then has a new online problem to solve (possibly retaining some
information from the previous solution). Our approach generalizes directly to the construction of
offline policies in situations where space to store such policies is available. Note that, in contrast,
deterministic enforced hill-climbing is easily implemented as an offline solution technique.
We propose a novel tool for stochastic planning by generalizing enforced hill-climbing to goalbased stochastic domains. Rather than seeking a sequence of actions deterministically leading to a
better state, our method uses a finite-horizon MDP analysis around the current state to seek a policy
that expects to improve on the heuristic value of the current state. Critical to this process is the direct
incorporation of both the probabilistic model and the heuristic function in finding the desired policy.
Therefore, for the finite-horizon analysis, the heuristic function is integrated into the MDP problem
in order to represent the temporary, greedy goal of improving on the current heuristic value. This
integration is done by building a novel heuristic-based MDP in which any state has a new exit
action available that terminates execution with cost equal to the heuristic estimate for that state, and
all other action costs are removed1 . In a heuristic-based MDP, finite-horizon policies are restricted
by a requirement that at horizon one, the exit action must be selected (but can also be selected at
other horizons). In this heuristic-based MDP, the cost of any policy  at a state s is the expected
value of the heuristic upon exit (or horizon) if  is executed from s.
Thus, we find the desired local policy using value iteration on the heuristic-based MDP around
the current state, with deepening horizon, until a policy is found with cost improving on the heuristic
estimate at the current state. The restriction of selecting the exit action at horizon one corresponds
to initializing value iteration with the provided heuristic function. When such a policy is found, the
1. The motivation for the removal of action costs in the heuristic-based MDP is discussed in Section 3.2.

816

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

method executes the policy until an exiting action is indicated (or to the horizon used in computing
the policy).
The resulting method, stochastic enforced hill-climbing (SEH), simply generalizes depth-k
breadth-first search for a state with improved heuristic value (from DEH) to a k-horizon value iteration computation seeking a policy that expects improvement in heuristic value. Note that although
stochastic enforced hill-climbing is an explicit statespace technique, it can be suitable for use in astronomically large statespaces if the heuristic used is informative enough to limit the effective size
of the horizon k needed to find expected heuristic improvement. Our empirical results in this work
demonstrate this behavior successfully.
1.1 Applicability and Limitations
Stochastic enforced hill-climbing (SEH) can be applied to any heuristic function. However, the
applicability (and likewise the limitations) of SEH greatly depends on the characteristics of the
heuristic function. SEH is appropriate in any goal-oriented problem given a strong enough heuristic
function, and we demonstrate empirically that SEH generally outperforms greedy following of the
same heuristic for a variety of heuristics in a variety of domains, even in presence of probabilistically interesting features (Little & Thiebaux, 2007) and deadends. SEH can rely upon the heuristic
function for identification of dead-ends and appropriate handling of probabilistically interesting features that require non-local analysisSEH simply provides local search that often can correct other
flaws in the heuristic function. SEH is thus intended as a possible improvement over stochastic
solution methods that construct a cost-to-go (cost) function and follow it greedily when using the
constructed cost function as a search heuristic. Many methods for constructing value/cost functions
have been proposed and evaluated in the literature, all of which can potentially be improved for
goal-based domains by using SEH in place of greedy following (Sutton, 1988; Fahlman & Lebiere,
1990; Bertsekas, 1995; Gordon, 1995; Mahadevan & Maggioni, 2007; Sanner & Boutilier, 2009)2 .
We prove the correctness of SEH in Section 3.4 by showing that in deadend-free domains, SEH
finds the goal with probability one (i.e. SEH does not get stuck in local optima).
While SEH is a search technique that leverages a heuristic estimate of distance to go, it must
be emphasized that, unlike many other such search techniques, SEH makes no promises about the
optimality of the solution path found. SEH is a greedy, local technique and can only promise to
repeatedly find a policy that reduces the heuristic value, and only when that is possible. As such,
SEH is an inappropriate technique for use when optimal solutions are required.
Stochastic enforced hill-climbing can be ineffective in the presence of huge plateaus or valleys
in the heuristic functions, due to extreme resource consumption in finding desired local policies.
Heuristic functions with huge plateaus result from methods that have failed to find any useful information about the problem in those state regions. SEH is inappropriate as the only tool for solving
a stochastic planning problemother tools are needed to construct a useful heuristic function that
manages deadends and avoids huge plateaus. This weakness mirrors the weakness of enforced hillclimbing in deterministic domains. SEH can also fail to find the goals when avoidable dead-ends
are present but not recognized early enough by the heuristic. In fact, effective dead-end detection is
a central goal in heuristic design when any greedy technique will be applied to the heuristic.
2. For applicability of SEH, a cost function must be non-negative and must identify goals by assigning zero to a state if
and only if it is a goal state; however, more general value/cost functions can be normalized to satisfy these requirements.

817

fiW U , K ALYANAM , & G IVAN

Further insight into the usefulness of SEH can be gained by comparison with recent determinizing replanners. As mentioned above, one way to exploit deterministic planning techniques such as
DEH for stochastic problems is to determinize the planning problem and use a deterministic planner to select an action sequence. Executing this action sequence in the problem is not guaranteed
to reach the goal due to the determinization approximation, so replanning is needed to augment
this technique. In this paper, we call stochastic planners that use this technique determinizing replanners. Determinizing replanners using a determinization (called all outcomes) that retains
all possible state transitions can be shown to reach the goal with probability one in the absence of
dead-end states.
In contrast to determinizing replanners, SEH at no point relies on any determinization of the
problem, but instead analyzes increasing-size local probabilistic approximations to the problem.
SEH conducts a full probabilistic analysis within the horizon, seeking the objective of reducing the
provided heuristic, using value iteration. In this way, SEH leverages the probabilistic parameters
that are ignored by determinizing replanners, as well as the provided heuristic function, which can
be based upon substantial probabilistic analysis. As a result, SEH successfully handles probabilistic
problem aspects that cause major problems for determinizing replanners. However, at this point,
we have no theoretical results characterizing its gains over determinizing replanners. Instead, we
have an extensive empirical evaluation showing advantages over FF-Replan (Yoon et al., 2007)
and RFF (Teichteil-Konigsbuch, Kuter, & Infantes, 2010) (two determinizing replanners), as well
as substantial gains compared to greedy following of the heuristic (which also uses the transition
probability parameters).
1.2 Evaluation
We test SEH on a broad range of domains from the first three international probabilistic planning
competitions (as well as the probabilistically interesting domains in Little & Thiebaux, 2007),
using two very different methods to generate heuristic functions. First, we test SEH on a heuristic function based on the ideas of the successful re-planner FF-Replan (Yoon et al., 2007). This
new controlled-randomness FF (CR-FF) heuristic is the deterministic FF heuristic (Hoffmann
& Nebel, 2001) computed on the simple determinization of the probabilistic problem that makes
available a deterministic transition wherever a probabilistic transition was possible. We note that
FF-Replan itself does not use this (or any) heuristic function in the stochastic problem. Instead, FFReplan relies on FF to construct a plan in the deterministic problem, and these calls to FF in turn use
deterministic enforced hill-climbing with exactly this heuristic. Here, we consider the performance
of this heuristic directly in the stochastic problem, comparing greedy heuristic-following with SEHbased search around the heuristic. The latter method using SEH constitutes a novel method for
combining determinization (that removes the probabilistic parameters) with probabilistic reasoning.
Our experiments show that this new method substantially outperforms FF-Replan across our broad
evaluation.
We have also performed a second evaluation of our technique on heuristic functions learned
from domain-specific experience by the relational feature-learning method presented in the work
of Wu and Givan (2007, 2010). These heuristic functions have already been shown to give good
performance when used to construct a simple greedy policy, and are further improved by SEH.
The SEH technique can be seen to perform well in a domain-by-domain analysis across the
broad set of competition planning domains, and full domain-by-domain results are available in an
818

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

online appendix. However, to compress and summarize the extensive per-problem results, we have
divided all the evaluation domains into experimenter-defined categories and have aggregated performance measurement within each problem category. While some categories are single domains,
more generally, multiple closely related domains may be aggregated within a single category. For
example, multiple domains from the competitions have been variants of the blocks world, and
problems in these domains are aggregated as a B LOCKSWORLD category.
In order to fairly compare SEH with FF-based planners (such as RFF, as described in TeichteilKonigsbuch et al., 2010, and FF-Replan) that exploit the blocksworld-targeted planning heuristics
added goal deletion and goal agenda, we have provided these heuristics as extensions to SEH.
The resulting planner is called SEH+ , described in detail in Section 3.6. Our results show that SEH+
performs nearly identically to SEH on non-blocksworld categories when using the CR-FF heuristic.
We employ these extensions when comparing SEH with the CR-FF heuristic to other planners.
Using experimenter-defined categories, we are able to show that SEH exploits the heuristic
functions more effectively than greedy following of the heuristic. SEH statistically significantly
outperforms greedy following in thirteen out of seventeen categories using the CR-FF heuristics
while losing in one category. SEH also outperforms greedy following in six out of seven categories using the learned heuristics. (In both cases, the other categories showed similar performance
between the compared planners.)
We show that SEH+ , when using the CR-FF heuristics, outperforms FF-Replan on ten out of
fifteen categories, with similar performance on two more categories, losing on only three categories.
Our aggregate results show that SEH+ (using the CR-FF heuristics) has a particularly strong performance advantage over FF-Replan in probabilistically interesting categories (Little & Thiebaux,
2007).
Finally, we compare the performance of SEH+ against that of RFF-BG (Teichteil-Konigsbuch
et al., 2010), one winner of the fully-observable track of the third international probabilistic planning competition. SEH+ outperforms RFF-BG on twelve out of fifteen categories, with similar
performance on one more category, losing on only two categories.
In summary, our empirical work demonstrates that SEH provides a novel automatic technique
for improving on a heuristic function using limited searches, and that simply applying SEH to
reasonable heuristic functions produces a state-of-the-art planner.

2. Technical Background: Markov Decision Processes
We give a brief review of Markov decision processes (MDPs) specialized to goal-region objectives.
For more detail on MDPs, see the work of Bertsekas (1995), Puterman (2005), and Sutton and Barto
(1998).
2.1 Goal-Oriented Markov Decision Processes
A Markov decision process (MDP) M is a tuple (S, A, C, T, sinit ). Here, S is a finite state space
containing initial state sinit , and A selects a non-empty finite available action set A(s) for each state
s in S. The action-cost function C assigns a non-negative real action-cost to each state-action-state
triple (s, a, s ) where action a is enabled in state s, i.e., a is in A(s). The transition probability
function T maps state-action pairs (s, a) to probability distributions over S, P(S), where a is in
A(s).
819

fiW U , K ALYANAM , & G IVAN

To represent the goal, we include in S a zero-cost absorbing state , i.e., such that C(, a, s) =
0 and T (, a, ) = 1 for all s  S and a  A(). Goal-oriented MDPs are MDPs where there is
a subset G  S of the statespace, containing , such that: (1) C(g, a, s ) is zero whenever g  G
and one otherwise, and (2) T (g, a, ) is one for all g  G and all a  A(g). The set G can thus be
taken to define the action-cost function C, as well as constrain the transition probabilities T .
A (stochastic) policy for an MDP  : S  N  P(A) specifies a distribution over actions for
each state at each finite horizon. The cost-to-go function J  (s, k) gives the expected cumulative
cost for k steps of execution starting at state s selecting actions according to () at each state
encountered. For any horizon k, there is at least one (deterministic) optimal policy   (, k) for

which J  (s, k), abbreviated J  (s, k), is no greater than J  (s, k) at every state s, for any other
policy . The following Q function evaluates an action a by using a provided cost-to-go function
J to estimate the value after action a is applied,
X
Q(s, a, J) =
T (s, a, s )[C(s, a, s ) + J(s )].
s S

Recursive Bellman equations use Q() to describe J  and J  as follows:
J  (s, k) = E [Q(s, (s, k), J  (, k  1))] and
J  (s, k) = min Q(s, a, J  (, k  1)),
aA(s)

taking the expectation over the random choice made by the possibly stochastic policy (s, k). In
both cases, the zero step cost-to-go function is zero everywhere, so that J  (s, 0) = J  (s, 0) = 0
for all s. Value iteration computes J  (s, k) for each k in increasing order starting at zero. Note that
when a policy or cost function does not depend on k, we may drop k from its argument list.
Also using Q(), we can select an action greedily relative to any cost function. The policy
Greedy(J) selects, at any state s and horizon k, a uniformly randomly selected action from
argminaA(s) Q(s, a, J(, k  1)).
While goal-based MDP problems can be directly specified as above, they may also be specified
exponentially more compactly using planning languages such as PPDDL (Younes, Littman, Weissman, & Asmuth, 2005), as used in our experiments. Our technique below avoids converting the
entire PPDDL problem explicitly into the above form, for resource reasons, but instead constructs a
sequence of smaller problems of explicit MDP form modeling heuristic flaws.
A dead-end state is a state for which every policy has zero probability of reaching the goal at any
horizon. We say that a policy reaches a region of states with probability one if following that policy
to horizon k has a probability of entering the region at some point that converges to one as k goes to
infinity. We say dead-ends are unavoidable in a problem whenever there is no policy from sinit that
reaches the goal region with probability one. (We then say a domain has unavoidable dead-ends if
any problem in that domain has unavoidable dead-ends.) We note that greedy techniques such as
hill-climbing can be expected to perform poorly in domains that have dead-end states with attractive
heuristic values. Application of SEH thus leaves the responsibility for detecting and avoiding deadend states in the design of the heuristic function.
A heuristic h : S  R may be provided, intended as an estimate of the cost function J for large
horizons, with h(s) = 0 for s  G, and h(s) > 0 otherwise. The heuristic may indicate dead-end
states by returning a large positive value V which we assume is selected by the experimenter to
exceed the expected steps to the goal from any state that can reach the goal. In our experiments, we
820

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

add trivial, incomplete dead-end detection (described in Section 5.2) to each heuristic function that
we evaluate.
We note that some domains evaluated in this paper do contain unavoidable deadends, so that
there may be no policy with success ratio one. The choice of the large value used for recognized
dead-end states effects a trade-off between optimizing success ratio and optimizing expected cost
incurred to the goal when successful.
2.2 Determinizing Stochastic Planning Problems
Some stochastic planners and heuristic computation techniques, including some used in our experiments, rely on computing deterministic approximations of stochastic problems. One such planner,
the all-outcomes FF-Replan (Yoon et al., 2007), determinizes a stochastic planning problem and
invokes the deterministic planner FF (Hoffmann & Nebel, 2001) on the determinized problem. The
determinization used in FF-Replan is constructed by creating a new deterministic action for each
possible outcome of a stochastic action while ignoring the probability of that outcome happening. This effectively allows the planner to control the randomness in executing actions, making
this determinization a kind of relaxation of the problem. In Section 5.2, we define a domainindependent heuristic function, the controlled-randomness FF heuristic (CR-FF), as the deterministic FF heuristic (Hoffmann & Nebel, 2001) computed on the all-outcomes FF-Replan determinization of the probabilistic problem3 . A variety of relaxations have previously been combined with a
variety of deterministic heuristics in order to apply deterministic planning techniques to stochastic problems (Bonet & Geffner, 2005). More generally, deterministic relaxations provide a general
technique for transferring techniques from deterministic planning for use in solution of stochastic
problems.

3. Stochastic Enforced Hill-Climbing
Deterministic enforced hill-climbing (DEH) (Hoffmann & Nebel, 2001) searches for a successor
state of strictly better heuristic value and returns a path from the current state to such a successor.
This path is an action sequence that guarantees reaching the desired successor. We illustrate the
behavior of DEH as compared to greedy policy using the example in Figure 1. In a stochastic
environment, there may be no single better descendant that can be reached with probability one,
since actions may have multiple stochastic outcomes. If we simply use breadth-first search as in
DEH to find a single better descendant and ignore the other possible outcomes, we might end up
selecting an action with very low probability of actually leading to any state of better heuristic value,
as illustrated in Figure 2. As shown in this figure, our algorithm, stochastic enforced hill-climbing
(SEH), accurately analyzes the probabilistic dynamics of the problem of improving the heuristic
value.
In this section, we give details of SEH. We note that in DEH, the local breadth-first search
gives a local policy in a state region surrounding the current state in a deterministic environment.
The value of following this policy is the heuristic value of the improved descendant found during
breadth-first search. In SEH, we implement these same ideas in a stochastic setting.
3. The deterministic FF heuristic, described in the work of Hoffmann and Nebel (2001), from FF planner version 2.3
available at http://www.loria.fr/hoffmanj/ff.html, efficiently computes a greedy plan length in a problem relaxation
where state facts are never deleted. The plan found in the relaxed problem is referred to as a relaxed plan for the
problem.

821

fiW U , K ALYANAM , & G IVAN

h=7

h=7

h=5

h=5

h=7
h=8

h=0

h=7

h=8

h=8

h=0

h=6
h=8

h=6

h = 10

h = 10

(a) Behavior of greedy policy.

(b) Behavior of DEH.

Figure 1: Comparison between the behavior of DEH and greedy policy when a local optimum is
encountered. The solid black circle represents the current state, and the shaded circle represents the
goal state (with heuristic value zero). In (a) the greedy policy keeps selecting actions indicated by
the wide arrow and cannot reach the goal state. On the other hand, DEH uses breadth-first search
and finds the goal state that is two steps away from the current state, as shown in (b).

h=7
h=5
h=7
h=8

h=7

p =0.2
p =0.8

h=6

h=5

h=2

h = 10

p =0.2

h=7

h=0

p =0.8
h=8

(a) Behavior of DEH in stochastic environments.

h=6

h=2

h=0

h = 10

(b) Behavior of SEH in stochastic environments.

Figure 2: Comparison between the behavior of SEH and DEH in a stochastic example. We assume
DEH first determinizes the problem, creating one deterministic action for each possible stochastic
outcome. The solid black circle represents the current state, and the shaded circle represents the
goal state (with heuristic value zero). In (a) DEH looks one step ahead and selects the action drawn
with double lines, as one of the outcomes leads to a state with h = 2, which is better than the current
state. However, this action choice has a higher probability of going to the state with h = 10 than
the one with h = 2. In (b) SEH first decides there is no policy with better value than 5 when the
horizon in the MDP only includes states reachable from the current state in one step. SEH then
extends the horizon to two so that all states are considered. It then selects the actions indicated in
the wide arrows that lead to the goal state.

822

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

Online Planning using a Local Planner
1. Repeat
2.
s  current state
3.
local  Find-Local-Policy(s,h)
4.
Follow local until a is selected
5. Until the goal is reached
Table 1: Pseudo-code for an online planning framework. The policy local may be non-stationary,
in which case the local planner also returns the initial horizon for execution of the policy and termination in line 4 can also happen by reaching that specified horizon.

We present SEH in two steps. First, we present a simple general framework for online planning that repeatedly calls a local planner that selects a policy around the current state. Second,
we present a local planner based on the enforced hill-climbing idea. When the online planning
framework is instantiated with this local planner, the resulting algorithm is SEH. The combination
of these two steps constitute the central algorithmic contribution of this paper. Finally, we present
some analytical properties of our algorithm.
3.1 A Simple Online Planning Framework
A familiar direct approach to online planning is to call the planner at the current state and have the
planner select an action. That action is then executed in the environment, resulting in a new current
state. This process can then be repeated.
Here, we present a simple generalization of this approach that allows the planner to select more
than one action during each call, before any action is executed. The idea is that the planner makes a
plan for the local context surrounding the current state, and then that plan is executed until the local
context is exited. When the local context is exited, we have a new current state and the process is
repeated.
More formally, we augment the action space with a new terminate action (called a ), indicating that the planned-for local context has been exited. We then define a local policy around a state
s to be a partial mapping from states to the augmented action space that is defined at s and at every
state reachable from s under the policy4 . An online planner can then be built by repeatedly seeking
and executing a local policy around the current state using a planning subroutine. The local policy
is executed until the terminate action is called (which has no effect on the state), at which point a
new local policy must be sought. These ideas are reflected in the pseudo-code shown in Table 1.
We note that the notion of local context in our discussion here is informal  the precise
notion is given by the use of the terminate action. A local policy is executed until it selects the
terminate action. The Find-Local-Policy routine is free to use any method to decide when a state
will be assigned the terminate action. Previously published envelope methods (Dean, Kaelbling,
Kirman, & Nicholson, 1995) provide one way to address this issue, so that termination will be
assigned to every state outside some envelope of states. However, this framework is more general
than envelope methods, and allows for local policies that are not selected based upon pre-existing
4. The local policy returned can be non-stationary and finite horizon, but must then select the terminate action at the
final stage, in all reachable states.

823

fiW U , K ALYANAM , & G IVAN

envelopes of states (though we can always, post-planning, interpret the set of reachable states as
an envelope). The general intuition is that selecting the action for the current states may involve
analysis that is sufficient to select actions for many surrounding states, so our framework allows the
Find-Local-Policy routine to return a policy specifying all such action selections.
Also, we note that this online planning framework includes recent re-planners such as FFReplan (Yoon et al., 2007) and RFF (Teichteil-Konigsbuch et al., 2010). However, replanning
because the current plan failed (e.g. because the determinization used to generate it was naive)
is quite different in character from SEH, which constructs plans to improve the heuristic value, and
replans each time such a plan terminates. Thus, SEH uses the heuristic function to define subgoals
and plan for the original goal incrementally.
It remains to present the local planner we combine with this online planning framework to
define stochastic enforced hill climbing. Our local planner analyzes the MDP problem around the
current state, but with the heuristic function integrated into the problem so as to embody the subgoal
of improving on the heuristic value of the current state. We describe this simple integration of the
heuristic function into the problem next, and then discuss the local planner based on this integration.
3.2 Heuristic-Based Markov Decision Processes
Our method relies on finite horizon analyses of a transformed MDP problem, with increasing horizons. We transform the MDP problem with a novel heuristic achievement transform before analysis
in order to represent the goal of finding and executing a policy that expects to improve on the initial
(current) states heuristic value.
The heuristic achievement transform is very straightforward, and applies to any goal-oriented
MDP problem. First, all action costs are removed from the problem. Second, the terminate action
a is assigned the action cost h(s) and transitions deterministically to the absorbing state . When
a policy is executed, the selection of the action a at any state will result in replanning, as discussed
in the online planning framework just presented. The actions a can be thought of as heuristic
achievement actions, allowing the immediate achievement of the value promised by the heuristic
function.
Analyzing the MDP transformed by the heuristic achievement transform at a finite horizon
around s0 represents the problem of finding a policy for improving on the heuristic value of s0
without regard to the cost of achieving such improvement in the heuristic. Allowing the heuristic
achievement action a to be selected at any point at any state reflects the greedy nature of this goal:
the planner is not forced to look further once an improvement is found, so long as there is a policy
from the initial state that expects to see improvement.
Formally, given MDP M = (S, A, C, T, s0 ) and non-negative heuristic function h : S  R, the
heuristic achievement transform of M under h, written Mh , is given by (S, A , C  , T  , s0 ), where
A , C  , and T  are as follows. Let s, s1 , and s2 be arbitrary states from S. We define A (s) to be
A(s)  {a }, and we take C  (s1 , a, s2 ) = 0 and T  (s1 , a, s2 ) = T (s1 , a, s2 ) for each a  A(s1 ).
Finally, we define T  (s, a , ) = 1 and C  (s, a , ) = h(s).
The transformed MDP will have zero-cost policies at all states, and as such is not of immediate use. However, policies that are required to select a as their final action (at horizon one)
represent policies that are seeking to get to regions with low heuristic value, whatever the cost.
An increasing-horizon search for such policies corresponds roughly to a breadth-first search for an
improved heuristic value in deterministic enforced hill-climbing. Formally, we define the class of
824

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

heuristic-achievement policies H as the class of policies (s, k) that satisfy (s, 1) = a for all s.
We define Jh (s, k) to be the value minH J  (s, k) in the heuristic transform MDP for h and h to
be a policy that achieves this value. We note that, due to the zeroing of non-terminal action costs,
Jh (s, k) represents the expected heuristic value achieved at the next execution of a , where a is
required at horizon k if not before. Formally, if we define the random variable s to be the state at
which h first executes a in a trajectory from s, we have Jh (s, k) = E[h(s )].
The rough motivation for setting action costs to zero for the analysis of the heuristic-based
MDP is that such actions are being considered by our method to remediate a flawed heuristic. The
cumulative action cost required to reach a state of improved heuristic value is a measure of the
magnitude of the flaw in the heuristic. Here, we remove this cost from the analysis in order to
directly express the subgoal of reaching a state with lower heuristic value. Including action costs
might, for example, lead to preferring cheap paths to higher heuristic values (i.e., to states worse
than s0 ) when expensive paths to lower heuristic values have been found. The basic motivation
for enforced hill climbing is to strongly seek improved heuristic values. Instead of diluting this
subgoal by adding in action costs, our methods seek the shortest path to a heuristic improvement
by analyzing the heuristic-based MDP with an iteratively deepened finite horizon, as discussed in
the next subsection. This approach is most reasonable in settings where each action has the same
cost, so that the finite-horizon value iteration is a stochastic-setting analogue to uniform cost search.
In settings with varying action cost, future work is needed to adapt SEH to usefully consider that
cost without excessively diluting the focus on improving the heuristic.
3.2.1 H EURISTIC ACHIEVEMENT VALUE I TERATION
Following the formalism of value iteration in Section 2.1, we compute Jh (s, k) with heuristic
achievement value iteration as follows:
Jh (s, 1) = h(s), and
Jh (s, k) = min Q(s, a, Jh (, k  1)) for k  2.
aA (s)

A non-stationary policy achieving the cost-to-go given by Jh (, k) can also be computed using the
following definition:
h (s, 1) = a , and
h (s, k) = argminaA (s) Q(s, a, Jh (, k  1)) for k  2.
Note that Q() is computed on the heuristic-achievement transformed MDP Mh in both equations.
For technical reasons that arise when zero-cost loops are present, we require that tie breaking in the
argmin for h (s, k) favors the action selected by h (s, k  1) whenever it is one of the options. This
is to prevent the selection of looping actions over shorter, more direct routes of the same value.
3.3 A Local Planner
We consider a method to be stochastic enforced hill-climbing if it uses an online planning framework, such as that presented in Table 1, together with a local policy selection method that solves
the heuristic-achievement MDP (exactly, approximately, or heuristically). Here, we describe one
straightforward method of local policy selection by defining SEH-find-local-policy using finitehorizon value iteration. This method generalizes the breadth-first search used in deterministic
825

fiW U , K ALYANAM , & G IVAN

enforced hill-climbing, and seeks an expected heuristic improvement rather than a deterministic
path to an improved heuristic value. More sophisticated and heuristic methods than finite-horizon
value iteration should be considered if the implementation presented here finds the local MDP problems intractable. Our analytical results in Section 3.4 apply to any method that exactly solves the
heuristic-achievement MDP, such as the method presented in Table 2; our experimental results are
conducted using the implementation in Table 2 as well.
We present pseudo-code for SEH-Find-Local-Policy in Table 2. A heuristic function h respects
goals if h(s) = 0 iff s  G. The algorithm assumes a non-negative heuristic function h : S  R
that respects goals, as input. SEH-Find-Local-Policy(s0 ,h) returns the policy h and a horizon k.
The policy is only computed for states and horizons needed in order to execute h from s0 using
horizon k until the policy terminates.
Thus, in lines 5 to 11 of Table 2 , heuristic-achievement value iteration is conducted for increasing horizons around s0 , seeking a policy improving h(s0 ). Note that for a given horizon k + 1, only
states reachable within k steps need to be included in the value iteration.
3.3.1 E ARLY T ERMINATION
The primary termination condition for repeated local policy construction is the discovery of a policy
improving on the heuristic estimate of the initial state. As discussed below in Proposition 1, in
domains without deadends, SEH-Find-Local-Policy will always find a policy improving on h(s0 ),
given sufficient resources.
However, for badly flawed heuristic functions the large enough horizons that are analyzed in
SEH-Find-Local-Policy may be unacceptably large given resource constraints. Moreover, in domains with unavoidable deadends, there may be no horizon, however large, with a policy improving
on the heuristic at the initial state. For these reasons, in practice, the algorithm stops enlarging the
horizon for heuristic-based MDP analysis when user-specified resource limits are exceeded.
When horizon-limited analysis of the heuristic-transform MDP construction does not yield the
desired results inexpensively, biased random walk is used to seek a new initial state. As an example, consider a problem in which the provided heuristic labels all states reachable in k steps
with cost-to-go estimates that are very similar, forming a very large plateau. If analysis of this large
plateau exceeds the resources available, biased random walk is indicated, for lack of useful heuristic
guidance.
So, once a horizon k is found for which Jh (s0 , k) < h(s0 ), the system executes h from s0
at horizon k until the terminate action a is selected. If the resource limit is exceeded without
finding such a horizon, the system executes a biased random walk of length , with the terminating
action a imposed in all states s reachable by such biased random walk with h(s) < h(s0 ). This
additional biased random walk allows our method to retain some of the beneficial properties of
random exploration in domains where heuristic flaws are too large for MDP analysis. The resource
consumption threshold at which random walk is triggered can be viewed as a parameter controlling
the blend of random walk and MDP-based search that is used in overcoming heuristic flaws. We
currently do not have a principled way of analyzing the tradeoff between resource consumption and
the cost of switching to biased random walk, or determining when to do such switching. Instead,
we use domain-independent resource limits as described in Section 5.1, which are determined after
some experimentation.
826

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

SEH-Find-Local-Policy(s0 ,h)
//
s0 is the current state.
//
h : S  {}  R the heuristic function, extended with h() = 0.
//
We assume global variable Mh is the heuristic-achievement
transform of the original problem M under h.
//
We seek a policy in the problem Mh achieving cost less than h(s0 ).
1. k = 1
2. Repeat
3.
k =k+1
4.
// Compute Jh (s0 , k) in Mh using value iteration
5.
Jh (, 1) = h(), h (, 1) = a , n = 1
6.
Repeat
7.
n=n+1
8.
For each s reachable from s0 in Mh using at most k  n steps
9.
Jh (s, n) = minaA (s) Q(s, a, Jh (, n  1))
10.
h (s, n) = argminaA (s) Q(s, a, Jh (, n  1))
11.
Until n = k
12. Until Jh (s0 , k) < h(s0 ) or resource consumption exceeds user-set limits
13. If Jh (s0 , k) < h(s0 )
14.
then
15.
Return h and horizon k
16.
else
17.
// Return a -step biased random walk policy 
18.
// Note: implementations should compute  lazily online
19.
For n = 1 to 
20.
For each state s
21.
If h(s) < h(s0 )
22.
then
23.
(s, n) selects a with probability one
24.
else
25.
(s, n) selects each action a  A(s) with probability
26.

P

eQ(s,a,h)
ai A(s) (e

Q(s,ai ,h) )

Return  and horizon 

Table 2: Pseudo-code for the local planner used to implement stochastic enforced hill-climbing.

827

fiW U , K ALYANAM , & G IVAN

Horizon-limited analysis of the heuristic-transform MDP may also terminate without finding
a horizon k such that Jh (s0 , k) < h(s0 ) when the entire reachable statespace has been explored,
in the presence of deadends. This may happen without exceeding the available resources, and in
this case we fall back to a fixed number of iterations of standard VI on the original MDP model
(including action costs and without the heuristic transform) for the reachable states.
3.4 Analytical Properties of Stochastic Enforced Hill-Climbing
In deterministic settings, given sufficient resources and no dead-ends, enforced hill-climbing can
guarantee finding a deterministic path to an improved heuristic value (if nothing else, a goal state
will suffice). Given the finite state space, this guarantee implies a guarantee that repeated enforced
hill-climbing will find the goal.
The situation is more subtle for stochastic settings. In a problem with no dead-ends, for every
state s the optimal policy reaches the goal with probability one. It follows that in such problems, for
any h assigning zero to every goal state, for every state s and real value  > 0, there is some horizon
k such that Jh (s, k) < . (Recall that Jh analyzes the heuristic transform MDP wherein action costs
are dropped except that h() must be realized at horizon one.) Because SEH-Find-Local-Policy(s,h)
considers each k in turn until Jh (s, k) < h(s) we then have:

Proposition 1. Given non-goal state s, no dead-ends, non-negative heuristic function
h : S  R respecting goals, and sufficient resources, the routine SEH-Find-Localpolicy(s,h) returns the policy h and a horizon k with expected return Jh (s, k) strictly
less than h(s).
However, unlike the deterministic setting, the policy found by the routine SEH-Find-Local-Policy
only expects some improvement in the heuristic value. So particular executions of the policy from
the current state may result in a degraded heuristic value.
Here, we prove that even in stochastic settings, in spite of this possibility of poor results from
one iteration, SEH will reach the goal region with probability one, in the absence of dead-end states
and with sufficient resources. In practice, the provision of sufficient resources is a serious hurdle,
and must be addressed by providing a base heuristic with modest-sized flaws.

Theorem 1. In dead-end free domains, with unbounded memory resources, SEH reaches the goal region with probability one.
Proof. Let x0 , x1 , x2 , . . . , xm , . . . be random variables representing the sequence of
states assigned to s in line 2 of Table 1 when we execute SEH on a planning problem,
with x0 being the initial state sinit . If the algorithm achieves x  G for some  , and
thus terminates, we take xj+1 = xj for all j   . (Note that as a result xj  G implies
xj+1  G, where G is the goal region of states.)
We show that for arbitrary m > 0 the probability that xm is in the goal region is
h(sinit )
at least 1  m
, for a real value  > 0 defined below. This expression goes to one
828

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

as m goes to infinity, and so we can conclude that SEH reaches the goal region with
probability one.
By Proposition 1, from any non-goal state s, absent dead-ends and with sufficient
resources, one iteration of SEH is guaranteed to return a policy for some finite horizon
ks with value Jh (s, ks ) improving on h(s). Let s = h(s)  Jh (s, ks ) > 0 be the value
of the improvement from s at horizon ks . Because there are finitely many non-goal
states, there exists  = minsSG s > 0 such that the improvement h(s)  Jh (s, ks )
is at least . Consider an arbitrary i such that xi 
/ G. Noting that Jh (xi , kxi ) =
E[h(xi+1 )] due to zero action costs in Mh , it follows immediately then that E[h(xi ) 
h(xi+1 )|xi 
/ G]  , where G is the goal region of states. Using that xi  G implies
both xi+1  G and h(xi ) = h(xi+1 ) = 0, we write this as
E[h(xi )  h(xi+1 )]
=E[h(xi )  h(xi+1 )|xi 
/ G]Qi
+ E[h(xi )  h(xi+1 )|xi  G](1  Qi )

(1)

Qi , for  > 0,
defining Qj to be the probability that xj 
/ G.
Now, we lower-bound the expected heuristic improvement E[h(x0 )  h(xm )] after m calls to SEH-Find-Local-Policy, for m > 0. We can decompose this expected
improvement over m calls to SEH-Find-Local-Policy as the sum of the expected improvements for the individual calls. Then, lower-bounding this sum using its smallest
term, we get
E[h(x0 )  h(xm )]
=


m1
X

i=0
m1
X

E[h(xi )  h(xi+1 )]
(2)
Qi  (from Inequality 1)

i=0

 mQm ,
as Qm is non-increasing, since xm1  G implies xm  G.
Next, we combine this lower bound with the natural upper bound h(sinit ), since h
is assumed to be non-negative (so E[h(x0 )  h(xm )]  h(sinit )) and x0 = sinit . Thus,
h(sinit )  Qm m.
h(s

)

init , converging to zero with
Therefore the probability Qm that xm 
/ G is at most m
large m and so SEH reaches the goal with probability one.

While the theorem above assumes the absence of dead-ends, problems with dead-ends are covered by this theorem as well if the dead-ends are both avoidable and identified by the heuristic.
Specifically, we may require that the heuristic function assigns  to a state if and only if there
is no policy to reach the goal from that state with probability one. In this case, the problem can
be converted to the form required by our theorem by simply removing all states assigned  from
consideration (either in pre-processing or during local MDP construction).
829

fiW U , K ALYANAM , & G IVAN

3.5 Variants and Extensions of SEH
SEH is based on finite-horizon analysis of the MDP transformed by the heuristic-achievement transform around the current state s0 . The particular heuristic-achievement transform we describe is of
course not the only option for incorporating the heuristic in a local search around s0 . While we
have already considered a number of related alternatives in arriving at the choice we describe, other
options can and should be considered in future research. One notable restriction in our transform
is the removal of action costs, which is discussed in Section 3.2. It is important for the method
to retain the actual heuristic value in the analysis so that it can trade off large, small, positive and
negative changes in heuristic value according to their probabilities of arising. For this reason, we
do not have the heuristic transform abstract away from the value and simply assign rewards of 1 or
0 according to whether the state improves on h(s0 ). Our choice to remove action costs during local
expansion can lead to poor performance in domains with flawed heuristics interacting badly with
high variations in action costs. This is a subject for future research on the method.
Also, the MDP models we describe in this paper are limited in some obvious ways. These
limitations include that the state space is discrete and finite, the problem setting lacks discounting,
and the objective is goal-oriented. We have yet to implement any extension to relax these limitations,
and leave consideration of the issues that arise to future work. We note that it would appear that
the method is fundamentally goal-oriented, given the goal of repeatedly reducing the heuristic value
of the current state. However, it is possible to contemplate infinite-horizon discounted non-goaloriented variants that seek policies that maintain the current heuristic estimate.
3.6 Incorporating FF Goal-Ordering Techniques in SEH
The planner FF contains heuristic elements inspired by ordering issues that arise in the blocksworld
problems (Hoffmann & Nebel, 2001). These heuristic elements improve performance on the blocksworld problems significantly. To assist in a fair comparison of SEH with FF-Replan, we have
implemented two of the heuristic elements, namely goal agenda and added goal deletion, in a variant
of SEH that we call SEH+ .
The implementation of SEH+ is as follows. The stochastic planning problem is first determinized using the all outcomes determinization described in Section 2.2. The goal-agenda technique of FF is then invoked on the determinized problem to extract a sequence of temporary goals
G1 , . . . , Gm , where each Gi is a set of goal facts and Gm is the original problem goal. SEH with a
stochastic version of added goal deletion, described next in this subsection, is then invoked repeatedly to compute a sequence of states s0 , . . . , sm , where s0 is the initial state and for i > 0 each si
is defined as the state reached by invoking SEH in state si1 with goal Gi (thus satisfying Gi ).
Added goal deletion is the idea of pruning the state search space by avoiding repetitive addition
and deletion of the same goal fact along searched paths. In FF, for a search state s, if a goal fact
is achieved by the action arriving at s, but is deleted by an action in the relaxed plan found from s,
then s is not expanded further (Hoffmann & Nebel, 2001).
For our stochastic adaptation of added goal deletion, we define the set of facts added by any
state transition (s, a, s ) to be those facts true in s but not in s and represent it as the set difference
s  s. Then, the set of added goal facts for the transition are those added facts which are also true in
the current temporary goal Gi , i.e., (s  s)  Gi . We prune any state transition (s, a, s ) whenever
the relaxed plan computed from s to the current temporary goal Gi contains an action that deletes
any of the added goal facts. A transition (s, a, s ) is pruned by modifying the Bellman update at
830

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

state s so that s contributes the dead-end state value (V ) to the Q-value for a at s, weighted by
the transition probability (instead of contributing the cost-to-go at s ). More formally, we define a
modified Q-function when using added goal deletion, Qagd (s, a, J) as follows:
(
1, if f  (s  s)  Gi is deleted by an action in relaxed plan(s ,Gi )5

I(s ) =
0, otherwise
X
Qagd (s, a, J) =
T (s, a, s )[I(s )V + (1  I(s ))J(s )]
s

Qagd () then replaces Q() in the definition of the cost-to-go function Jh () in Section 3.2. Also,
reachability in line 8 of Table 2 does not use pruned transitions.
In some problems, subsequent deletion of newly added goals is unavoidable for any valid plan.
Added goal deletion prunes all routes leading to the goal region for such problems even though
no actual deadend is present. Hence, this is an incomplete technique as discussed in the work
of Hoffmann and Nebel (2001). FF falls back to best-first search if DEH is not able to find a valid
plan due to pruning. Similarly, when unable to find an improved policy, SEH falls back to either
value iteration or biased random walk as described in Section 3.3.
Preliminary exploration of incorporating stochastic variants of FFs helpful action pruning
(Hoffmann & Nebel, 2001) into SEH did not improve performance, much like the effect of added
goal deletion on all domains except the blocksworld6 . As a result, we do not report on helpfulaction-pruning methods here.

4. Related Work
In this section we discuss planning techniques that are close to our work in one or more dimensions.
4.1 Fast-Foward (FF) Planner and Deterministic Enforced Hill-Climbing
For an introduction to deterministic enforced hill-climbing (DEH) and its relation to our technique,
please see Section 3. Here, we additionally note that there are several lines of work that directly
extend the FF planner to allow planning with numeric state-variables (Hoffmann, 2003) and planning with uncertainty (Hoffmann & Brafman, 2006, 2005; Domshlak & Hoffmann, 2007). Although
these techniques involve significant changes to the computation of the relaxed-plan heuristic and the
possible addition of the use of belief states to handle uncertainty, enforced hill-climbing is still the
primary search technique used in these lines of work. We note that although in the work of Domshlak and Hoffmann actions with probabilistic outcome are handled, the planner (Probabilistic-FF)
is designed for probabilistic planning with no observability, whereas our planner is designed for
probabilistic planning with full observability.
4.2 Envelope-Based Planning Techniques
Stochastic enforced hill-climbing dynamically constructs local MDPs to find a local policy leading
to heuristically better state regions. The concept of forming local MDPs, or envelopes, and using
5. relaxed plan(s ,Gi ) computes the relaxed plan between states s and Gi as defined in the work of Hoffmann and
Nebel (2001) using the all-outcomes problem determinization defined in Section 2.2.
6. We explored ideas based on defining the helpfulness of an action to be the expectation of the helpfulness of its
deterministic outcomes.

831

fiW U , K ALYANAM , & G IVAN

them to facilitate probabilistic planning has been used in previous research such as the work of Bonet
and Geffner (2006), and Dean et al. (1995), which we briefly review here.
The envelope-based methods in the work of Dean et al. (1995) and Gardiol and Kaelbling (2003)
start with a partial policy in a restricted area of the problem (the envelope), then iteratively improves the solution quality by extending the envelope and recomputing the partial policy. The
typical assumption when implementing this method is that the planner has an initial trajectory from
the starting state to the goal, generated by some stochastic planner, to use as the initial envelope.
Another line of work, including RTDP (Barto, Bradtke, & Singh, 1995), LAO* (Hansen &
Zilberstein, 2001), and LDFS (Bonet & Geffner, 2006), starts with an envelope containing only the
initial state, and then iteratively expands the envelope by expanding states. States are expanded
according to state values and dynamic programming methods are used to backup state values from
newly added states, until some convergence criterion is reached. Stochastic enforced hill-climbing
can be viewed as repeatedly deploying the envelope method with the goal, each time, of improving
on the heuristic estimate of distance-to-go. For a good h function, most invocations result in trivial
one-step envelopes. However, when local optima or plateaus are encountered, the envelope may
need to grow to locate a stochastically reachable set of exits.
All of the above referenced previous search methods have constructed envelopes seeking a high
quality policy to the goal rather than our far more limited and relatively inexpensive goal of basin
escape. Our results derive from online greedy exploitation of the heuristic rather than the more
expensive offline computation of converged values proving overall (near) optimality. LDFS, for
example, will compute/check values for at least all states reachable under the optimal policy (even
if given J  as input) and possibly vastly many others as well during the computation.
Some of these previous methods are able to exploit properties (such as admissibility) of the
heuristic function to guarantee avoiding state expansions in some regions of the state space. Clearly,
SEH exploits the heuristic function in a way that can avoid expanding regions of the statespace.
However, we have not at this point conducted any theoretical analysis of what regions can be guaranteed unexpanded for particular kinds of heuristic, and such analyses may be quite difficult.
4.3 Policy Rollout
The technique of policy rollout (Tesauro & Galperin, 1996; Bertsekas & Tsitsiklis, 1996) uses a
provided base policy  to make online decisions. The technique follows the policy Greedy(Vf ),
where Vf is computed online by sampling simulations of the policy .
The computation of the optimal heuristic-transform policy h in SEH has similarities to policy
rollout: in each case, online decisions are made by local probabilistic analysis that leverages provided information to manage longer-range aspects of the local choice. For SEH, a heuristic function
is provided while, for policy rollout, a base policy is provided. In this view, policy rollout does local
analysis under the assumption that non-local execution will use the base policy , whereas SEH
does local analysis under the assumption that non-local execution will achieve the base heuristic
cost estimate h.
In fact, for our goal-oriented setting, when the provided heuristic function h is stochastic (a simple generalization of what we describe in this paper), and equal to a sampled-simulation evaluation
of V  for some policy , then SEH executes the same policy as policy rollout, assuming uniform
action costs and sufficient sampling to correctly order the action choices. This claim follows because when h = V  there is always some action to yield an expected improvement in h in one step,
832

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

in our goal-oriented setting. The need for uniform action costs in this claim may be relaxed if a
variant of SEH is developed that retains action costs in the heuristic transform.
In policy rollout, only horizon-one greedy use of the sampled heuristic is needed, but the main
substance of SEH is to enable the repair and use of heuristic functions with flaws that cannot be
repaired at horizon one. Thus the central differences between the techniques are reflected in the
ability of SEH to leverage arbitrary heuristic functions and repair flaws in those functions at larger
horizons.
Policy rollout provides an elegant guarantee that the online policy selected improves on the base
policy, given sufficient sampling. This result follows intuitively because the computed policy is the
policy-iteration improvement of the base policy. Unfortunately, no similar guarantee is known to
apply for SEH for an arbitrary heuristic function. However, policy rollout cannot be used to improve
an arbitrary heuristic function either.
4.4 Local Search in Optimization
Stochastic enforced hill-climbing can be regarded as one of many local-search techniques designed
to improve on greedy one-step lookahead, the most naive form of local search optimization. Here
we briefly discuss connections to the method of simulated annealing, one of a large family of related
local search techniques. For more detail, please see the work of Aarts and Lenstra (1997).
Simulated annealing (Kirkpatrick et al., 1983; Cerny, 1985) allows the selection of actions with
inferior expected outcome with a probability that is monotone in the action q-value. The probability
that an inferior action will be selected often starts high and decreases over time according to a cooling schedule. The ability to select inferior actions leads to a non-zero probability of escaping local
optima. However, this method does not systematically search for a policy that does so. In contrast,
stochastic enforced hill-climbing analyzes a heuristic-based MDP at increasing horizons to systematically search for policies that give improved expected value (hence leaving the local extrema). In
our substantial preliminary experiments, we could not find successful parameter settings to control
simulated annealing for effective application to online action selection in goal-directed stochastic
planning. To our knowledge, simulated annealing has not otherwise been tested on direct forwardsearch action selection in planning, although variants have been applied with some success to other
planning-as-search settings (Selman, Kautz, & Cohen, 1993; Kautz & Selman, 1992; Gerevini &
Serina, 2003) such as planning via Boolean satisfiability search.

5. Setup for Empirical Evaluation
Here, we describe the parameters used in evaluating our method, the heuristics we will test the
method on, the problem categories in which the tests will be conducted, the random variables for
aggregated evaluation, and issues arising in interpreting the results and their statistical significance.
We run our experiments on Intel Xeon 2.8GHz machines with 533 MHz bus speed and 512KB
cache.
5.1 Implementation Details
If at any horizon increase no new states are reachable, our implementation of SEH simply switches
to an explicit statespace method to solve the MDP formed by the reachable states. More specifically,
833

fiW U , K ALYANAM , & G IVAN

if the increase in k at line 3 in Table 2 does not lead to new reachable states in line 8, we trigger
value iteration on the states reachable from s0 .
Throughout our experiments, the thresholds used to terminate local planning in line 12 of Table 2
are set at 1.5  105 states and one minute. We set the biased random walk length  to ten. This work
makes the assumption that the heuristic functions used assign large values to easily recognized deadends, as hill-climbing works very poorly in the presence of dead-end attractor states. We enforce
this requirement by doing very simple dead-end detection on the front-end of each heuristic function
(described next in Section 5.2 for each heuristic) and assigning the value 1.0  105 to recognized
dead-end states.
We denote this implementation running on heuristic h with SEH(h).
5.2 Heuristics Evaluated
We describe two different types of heuristic functions used in our evaluation and the associated
dead-end detection mechanisms.
5.2.1 T HE C ONTROLLED -R ANDOMNESS FF H EURISTIC
For use in our evaluations, we define a domain-independent heuristic function, the controlledrandomness FF heuristic (CR-FF). We define CR-FF on a state s to be the FF distance-to-goal
estimate (Hoffmann & Nebel, 2001) of s computed on the all-outcomes determinization as described
in Section 2.2. We denote the resulting heuristic function F . While computing the CR-FF heuristic,
we use the reachability analysis built into the FF planner for the detection of deadends.
5.2.2 L EARNED H EURISTICS
We also test stochastic enforced hill-climbing on automatically generated heuristic functions from
the work of Wu and Givan (2010), which on their own perform at the state-of-the-art when used to
construct a greedy policy. We shift these heuristic functions to fit the non-negative range requirement of h discussed previously. These learned heuristic functions are currently available for only
seven of our test categories, and so are only tested in those categories.
We note that these heuristics were learned for a discounted setting without action costs and
so are not a direct fit to the distance-to-go formalization adopted here. We are still able to get
significant improvements from applying our technique. We denote these heuristics L. Only states
with no valid action choice available are labeled as deadends when applying SEH to the learned
heuristics.
5.3 Goals of the Evaluation
Our primary empirical goal is to show that stochastic enforced hill-climbing generally improves
significantly upon greedy following of the same heuristic (using the policy Greedy(h) as described
in the technical background above). We will show that this is true for both of the heuristics defined in
Section 5.2. We show empirically the applicability and limitation of SEH discussed in Section 1.1,
in different types of problems including probabilistically interesting ones (Little & Thiebaux, 2007).
A secondary goal of our evaluation is to show that for some base heuristics the resulting performance is strong in comparison to the deterministic replanners FF-Replan (Yoon et al., 2007) and
RFF (Teichteil-Konigsbuch et al., 2010). While both FF-Replan and RFF use the Fast-Forward (FF)
834

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

base planner, RFF uses a most-probable-outcome determinization in contrast to the all-outcomes
determinization used by FF-Replan. The primary other difference between RFF and FF-Replan is
that before executing the plan, RFF grows policy trees to minimize the probability of having to
replan, while FF-Replan does not.
5.4 Adapting IPPC Domains for Our Experiments
We conduct our empirical evaluation using all problems from the first three international probabilistic planning competitions (IPPCs) as well as all twelve probabilistically interesting problems
from the work of Little and Thiebaux (2007). We omit some particular problems or domains from
particular comparisons for any of several practical reasons, detailed in an online appendix.
Because enforced hill-climbing is by nature a goal-oriented technique and SEH is formulated
for the goal-oriented setting, we ignore the reward structure (including action and goal rewards)
in any of the evaluated problems and assume an uniform action cost of one for those problems,
transforming any reward-oriented problem description into a goal-oriented one.
We provide detailed per-problem results in an online appendix for each planner evaluated in this
work. However, in support of our main conclusions, we limit our presentation here to aggregations
comparing pairs of planners over sets of related problems. For this purpose, we define seventeen
problem categories and aggregate within each problem category. While some categories are single
domains, more generally, multiple closely related domains may be aggregated within a single category. For example, the blocksworld category aggregates all blocksworld problems from the three
competitions, even though the action definitions are not exactly the same in every such problem. For
some paired comparisons, we have aggregated the results of all problems labeled as or constructed
to be probabilistically interesting by the IPPC3 organizers or by the work of Little and Thiebaux
(2007) into a combined category PI PROBLEMS.
In Table 3, we list all evaluated categories (including the combined category PI PROBLEMS),
as well as the planning competitions or literature the problems in each category are from. The
evaluated problems in each category are identified in an online appendix.
The reward-oriented S YSADMIN domain from IPPC3 was a stochastic longest-path problem
where best performance required avoiding the goal so as to continue accumulating reward as long
as possible (Bryce & Buffet, 2008). (Note that contrary to the organizers report, the domains goal
condition is all servers up rather than all servers down.) Our goal-oriented adaptation removes
the longest-path aspect of the domain, converting it to a domain where the goal is to get all the
servers up.
The B LOCKSWORLD problems from IPPC2 contain flawed definitions that may lead to a block
stacking on top of itself. Nevertheless, the goal of these problems is well defined and is achievable
using valid actions, hence the problems are included in the B LOCKSWORLD category.
We have discovered that the five rectangle-tireworld problems (p11 to p15 from IPPC3 2T IREWORLD) have an apparent bugno requirement to remain alive is included in the goal condition. The domain design provides a powerful teleport action to non-alive agents intended only to
increase branching factor (Buffet, 2011). However, lacking a requirement to be alive in the goal,
this domain is easily solved by deliberately becoming non-alive and then teleporting to the goal. We
have modified these problems to require the predicate alive in the goal region. We have merged
these modified rectangle-tireworld problems with triangle-tireworld problems from both IPPC3 and
835

fiW U , K ALYANAM , & G IVAN

Category

Problem Source(s)

B LOCKSWORLD

IPPC1, IPPC2, IPPC3

B OXWORLD

IPPC1, IPPC3

B USFARE

Little and Thiebaux (2007)

D RIVE

IPPC2

E LEVATOR

IPPC2

E XPLODING B LOCKSWORLD

IPPC1, IPPC2, IPPC3

F ILEWORLD

IPPC1

P ITCHCATCH

IPPC2

R ANDOM

IPPC2

R IVER

Little and Thiebaux (2007)

S CHEDULE

IPPC2, IPPC3

S EARCH AND R ESCUE

IPPC3

S YSADMIN

IPPC3

S YSTEMATIC - TIRE

Triangle-tireworld (IPPC3 2-Tireworld P1 to P10, Little and Thiebaux (2007)),
Rectangle-tireworld (IPPC3 2-Tireworld P11 to P15) with bug fixed

T IREWORLD

IPPC1, IPPC2

T OWERS OF H ANOI

IPPC1

Z ENOTRAVEL

IPPC1, IPPC2

PI PROBLEMS

B USFARE, D RIVE, E XPLODING B LOCKSWORLD
P ITCHCATCH, R IVER, S CHEDULE, S YSTEMATIC - TIRE, T IREWORLD

Table 3: List of categories and the planning competitions or literature from which the problems in
each category are taken.

836

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

the work of Little and Thiebaux (2007) into a category S YSTEMATIC - TIRE, as these problems have
been systematically constructed to emphasize PI features.
5.5 Aggregating Performance Measurements
For our experiments, we have designed repeatable aggregate measurements that we can then sample
many times in order to evaluate statistical significance. We now define the random variables representing these aggregate measurements and describe our sampling process, as well as our method for
evaluating statistical significance.
5.5.1 D EFINING AND S AMPLING AGGREGATE -M EASUREMENT R ANDOM VARIABLES
For each pair of compared planners, we define four random variables representing aggregate performance comparisons over the problems in each category. Each random variable is based upon a
sampling process that runs each planner five times on all problems in a category, and aggregates the
per-problem result by computing the mean. We use five-trial runs to reduce the incidence of lowsuccess planners failing to generate a plan length comparison. Each mean value from a five-trial run
is a sample value of the respective random variable.
First, the per-problem success ratio (SR) is the fraction of the five runs that succeed for each
problem. The success ratio random variable for each category and planner is then the mean SR
across all problems in the category.
Second, the per-problem successful plan length (SLen) is the mean plan length of all successful
runs among the five runs. In order to compare two planners on plan length, we then define the perproblem ratio of jointly successful plan lengths (JSLEN-RATIO) for the two compared planners as
follows. If both planners have positive SR among the five trials on the problem, JSLEN-RATIO is
the ratio of the SLen values for the two planners; otherwise, JSLEN-RATIO is undefined for that
problem. We use ratio of lengths to emphasize small plan length differences more in short solutions
than in long solutions, and to decrease sensitivity to the granularity of the action definitions.
The mean JSLEN-RATIO random variable for each category and pair of planners is then the
geometric mean of the JSLEN-RATIO across all problems in the category for which JSLEN-RATIO
is well defined. In this manner we ensure that the two planners are compared on exactly the same set
of problems. Note then that, unlike SR, JSLEN-RATIO depends on the pair of compared planners,
rather than being a measurement on any single planner; it is the ratio of successful plan length on
the jointly solved problems for the two planners.
Similarly, the per-problem ratio of jointly successful runtimes (JSTIME-RATIO) is defined in
the same manner used for comparing plan lengths. The mean JSTIME-RATIO is again computed
as the geometric mean of well-defined per-problem JSTIME-RATIO values.
Because JSLEN-RATIO and JSTIME-RATIO are ratios of two measurements, we use the geometric mean to aggregate per-problem results to generate a sample value, whereas we use arithmetic
mean for the SR variables. Note that geometric mean has the desired property that when the planners are tied overall (so that the geometric mean is one), the mean is insensitive to which planner is
given the denominator of the ratio.
Thus, to draw a single sample of all four aggregate random variables (SR for each planner,
JSLEN-RATIO, and JSTIME-RATIO) in comparing two planners, we run the two planners on each
problem five times, computing per-problem values for the four variables, and then take the (arith837

fiW U , K ALYANAM , & G IVAN

metic or geometric) means of the per-problem variables to get one sample of each aggregate variable. This process is used repeatedly to draw as many samples as needed to get significant results.
We use a plan-length cutoff of 2000 for each attempt. Each attempt is given a time limit of 30
minutes.
5.5.2 S IGNIFICANCE OF P ERFORMANCE D IFFERENCES B ETWEEN P LANNERS
Our general goal is to order pairs of planners in overall performance on each category of problem.
To do this, we must trade off success rate and plan length. We take the position that a significant
advantage in success rate is our primary goal, with plan length used only to determine preference
among planners when success rate differences are not found to be significant.
We determine significance for each of the three performance measurements (SR, JSLENRATIO, and JSTIME-RATIO) using t-tests, ascribing significance to the results when the p-value
is less than 0.05. The exact hypothesis tested and form of t-test used depends on the performance
measurement, as follows:
1. SR  We use a paired one-sided t-test on the hypothesis that the difference in true means is
larger than 0.02.
2. JSLEN-RATIO  We use a one-sample one-sided t-test on the hypothesis that the true geometric mean of JSLEN-RATIO exceeds 1.05 (log of the true mean of JSLEN-RATIO exceeds
log(1.05)).
3. JSTIME-RATIO  We use a one-sample one-sided t-test on the hypothesis that the true
geometric mean of JSTIME-RATIO exceeds 1.05 (log of the true mean of JSTIME-RATIO
exceeds log(1.05)).
We stop sampling the performance variables when we have achieved one of the following criteria, representing an SR winner is determined or SR appears tied:
1. Thirty samples have been drawn and the p-value for SR difference is below 0.05 or above 0.5.
2. Sixty samples have been drawn and the p-value for SR difference is below 0.05 or above 0.1.
3. One hundred and fifty samples have been drawn.
In all the experiments we present next, this stopping rule leads to only 30 samples being drawn
unless otherwise mentioned. Upon stopping, we conclude a ranking between the planners (naming a
winner) if either the SR difference or the JSLEN-RATIO has p-value below 0.05, with significant
SR differences being used first to determine the winner. If neither measure is significant upon
stopping, we deem the experiment inconclusive.
Combining categories For some of our evaluations, we aggregate results across multiple categories of problem, e.g., the combined category PI PROBLEMS. In such cases, we have effectively
defined one larger category, and all our techniques for defining performance measurements and determining statistical significance are the same as in Section 5.5. However, we do not actually re-run
planners for such combined-category measurements. Instead, we re-use the planner runs used for
the single-category experiments. Rather than use the stopping rule just described, we compute the
maximum number of runs available in all the combined categories and use that many samples of
838

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

the combined-category performance measurements. To avoid double counting problem results, we
treat combined categories separately when analyzing the results and counting wins and losses.

6. Empirical Results
We present the performance evaluation of stochastic enforced hill-climbing (SEH) in this section.
The experiments underlying the results presented here involve 169,850 planner runs in 17 categories.
6.1 Summary of Comparison
The results in Table 4 show that, for the CR-FF heuristic, SEH with the goal-ordering and addedgoal-deletion enhancements (SEH+ (F )) improves significantly over the baseline SEH technique
(SEH(F )) in the category B LOCKSWORLD, but does not show significant changes in the aggregated
performance for non-blocksworld problems7 . For the remainder of the experiments involving CRFF, we evaluate only SEH+ (F ), noting that both of our comparison planners (FF-Replan and RFF)
benefit from the goal-ordering and added-goal-deletion enhancements of their base planner, FFplan.
The results we present next for SEH+ (F ) show:
 SEH+ (F ) significantly outperforms Greedy(F ) in 13 categories, but is outperformed by
Greedy(F ) in S CHEDULE. There were three categories where the comparison was inconclusive (B USFARE, R IVER and T IREWORLD). See Table 5 for details.
 FF-Replan was inapplicable in two categories (IPPC3 S EARCH - AND -R ESCUE and IPPC3
S YSADMIN). SEH+ (F ) significantly outperforms FF-Replan in 10 categories, but is outperformed by FF-Replan in three categories (E XPLODING B LOCKSWORLD, P ITCHCATCH, and
Z ENOTRAVEL). There were two categories where the comparison was inconclusive (F ILE WORLD and R IVER ). SEH+ (F ) also significantly outperforms FF-Replan on the combined
category PI PROBLEMS, although the winner varied between the aggregated categories. See
Table 6 for details.
 RFF-BG was inapplicable in two categories (B USFARE and IPPC1 F ILEWORLD). SEH+ (F )
significantly outperforms RFF-BG in 12 categories, but is outperformed by RFF-BG in two
categories (E XPLODING B LOCKSWORLD and S YSTEMATIC - TIRE). There was one category
where the comparison was inconclusive (S YSADMIN). SEH+ (F ) also significantly outperforms RFF-BG on the combined category PI PROBLEMS, although the winner varied between
the aggregated categories. See Table 7 for details.
The learned heuristic from the work of Wu and Givan (2010) has been computed only in
a subset of the domains, hence only seven categories are applicable for the evaluation using the
learned heuristic (see an online appendix for details). The results we present next for SEH with the
learned heuristic, SEH(L), show:
 SEH(L) significantly outperforms Greedy(L) in six categories. There was one category
(T IREWORLD) where the comparison was inconclusive. See Table 8 for details.
7. We show p-values rounded to two decimal places. For example, we show p=0.00 when the value of p rounded to two
decimal places is 0.

839

fiW U , K ALYANAM , & G IVAN

SR of
SR of
SEH+ (F ) SEH(F )

Category

JSLENRATIO
(SEH/
SEH+ )

JSTIMERATIO
(SEH/
SEH+ )

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.72

1.58

2.85

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

N ON - BLOCKSWORLD

0.69

0.69

1.01

0.97

NO

(p=1.00)

NO

(p=1.00)

Inconclusive

Table 4: Aggregated comparison of SEH+ (F ) against SEH(F ).

 SEH(L) significantly outperforms FF-Replan in five categories, but is outperformed by FFReplan in two categories (E XPLODING B LOCKSWORLD and Z ENOTRAVEL). See Table 9 for
details.
6.2 Discussion
We now discuss the results for comparisons between pairs of planners, including SEH versus greedy
heuristic-following, SEH versus FF-Replan, and SEH versus RFF-BG.
6.2.1 SEH/SEH+ V ERSUS G REEDY
Our primary evaluation goal was to show that stochastic enforced hill-climbing generally improves
significantly upon greedy following of the same heuristic (using the policy Greedy(h) as described
in the technical background above). This was demonstrated by evaluating SEH with two different
heuristics in Tables 5 and 8, where SEH(h) significantly outperforms Greedy(h) in nineteen out of
twenty-four heuristic/category pairs, only losing in S CHEDULE for SEH+ (F ) against Greedy(F ).
We now discuss the only category where Greedy outperforms SEH techniques significantly.
In S CHEDULE, there are multiple classes of network packets with different arrival rates. Packets have deadlines, and if a packet is not served before its deadline, the agent encounters a classdependent risk of death as well as a delay while the packet is cleaned up. To reach the goal of
serving a packet from every class, the agent must minimize the dropping-related risk of dying while
waiting for an arrival in each low-arrival-rate class. The all-outcomes determinization underlying
the CR-FF heuristic gives a deterministic domain definition where dying is optional (never chosen)
and unlikely packet arrivals happen by choice, leading to a very optimistic heuristic value. When
using a very optimistic heuristic value, the basic local goal of SEH, which is to improve on the
current state heuristic, leads to building very large local MDPs for analysis. In the presence of
dead-ends (death, as above), even arbitrarily large local MDPs may not be able to achieve a local
improvement, and so in S CHEDULE, SEH+ will typically hit the resource limit for MDP size at
every action step.
In contrast, greedy local decision making is well suited to packet scheduling. Many well known
packet scheduling policies (e.g. earliest deadline first or static priority in the work of Liu &
Layland, 1973) make greedy local decisions and are practically quite effective. In our experiments,
the Greedy policy applied to CR-FF benefits from locally seeking to avoid the incidental delays of
dropped-packet cleanup: even though the heuristic sees no risk-of-dying cost to dropping, it still
recognizes the delay of cleaning up lost dropped packets. Thus, Greedy(F ) is a class-insensitive
840

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

JSLENSR of
RATIO
SR of
SEH+ (F ) Greedy(F ) (Greedy/
SEH+ )

Category

JSTIMERATIO
(Greedy/
SEH+ )

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.35

1.40

0.63

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.99

0.05

1.18

1.12

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B USFARE

1.00

0.99

0.85

0.86

NO

(p=0.97)

NO

(p=0.21)

Inconclusive

D RIVE

0.69

0.35

1.60

1.41

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

E LEVATOR

1.00

0.40

1.82

1.81

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

E XPLODING
B LOCKSWORLD

0.44

0.18

1.01

0.63

YES

(p=0.00)

NO

(p=0.93)

SEH+ (F )

F ILEWORLD

1.00

0.21

1.03

0.24

YES

(p=0.00)

NO

(p=1.00)

SEH+ (F )

P ITCHCATCH

0.45

0.00





YES

(p=0.00)

R ANDOM

0.99

0.94

1.76

0.59

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

R IVER

0.66

0.67

0.97

0.98

NO

(p=0.60)

NO

(p=0.75)

Inconclusive

S CHEDULE

0.54

0.60

1.18

0.32

YES

(p=0.00)

YES

(p=0.01)

Greedy(F )

S EARCH
AND R ESCUE

1.00

1.00

1.23

1.08

NO

(p=1.00)

YES

(p=0.00)

SEH+ (F )

S YSADMIN

0.27

0.27

1.21

1.23

NO

(p=1.00)

YES

(p=0.00)

SEH+ (F )

S YSTEMATIC
- TIRE

0.29

0.21

1.03

0.72

YES

(p=0.00)

NO

(p=0.86)

SEH+ (F )

T IREWORLD

0.91

0.90

0.96

0.79

NO

(p=0.93)

NO

(p=0.74)

Inconclusive

T OWERS
H ANOI

0.53

0.00





YES

(p=0.00)

0.90

0.20

1.31

0.74

YES

(p=0.00)

OF

Z ENOTRAVEL




YES

(p=0.00)

SEH+ (F )

SEH+ (F )
SEH+ (F )

Table 5: Aggregated comparison of SEH+ (F ) against Greedy(F ). The R IVER domain evaluation required extending sampling to 60 samples as per the experimental protocol described in Section 5.5.2. The values and p-values of JSLEN-RATIO and JSTIME-RATIO in P ITCHCATCH and
T OWERS OF H ANOI are not available due to the zero success ratio of Greedy(F ) in these categories.

841

fiW U , K ALYANAM , & G IVAN

JSLENSR of
SR of
RATIO
SEH+ (F ) FF-Replan (FFR/
SEH+ (F ))

Category

JSTIMERATIO
(FFR/
SEH+ (F ))

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.87

1.33

1.17

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.99

0.88

3.93

1.57

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B USFARE

1.00

0.01

0.00

0.00

YES

(p=0.00)

D RIVE

0.69

0.54

1.26

2.42

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

E LEVATOR

1.00

0.93

0.95

0.93

YES

(p=0.00)

NO

(p=0.36)

SEH+ (F )

E XPLODING
B LOCKSWORLD

0.44

0.44

0.85

0.56

NO

(p=0.96)

YES

(p=0.00)

FF-Replan

F ILEWORLD

1.00

1.00

0.97

0.57

NO

(p=1.00)

NO

(p=1.00)

Inconclusive

P ITCHCATCH

0.45

0.51

2.78

0.21

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

R ANDOM

0.99

0.96

1.37

0.19

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

R IVER

0.66

0.65

0.94

0.93

NO

(p=0.60)

NO

(p=0.33)

Inconclusive

S CHEDULE

0.54

0.48

1.04

0.10

YES

(p=0.00)

NO

(p=0.59)

SEH+ (F )

S YSTEMATIC
- TIRE

0.29

0.07

0.36

0.38

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

T IREWORLD

0.91

0.69

0.69

0.57

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

T OWERS
H ANOI

0.59

0.50

0.64

0.06

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

Z ENOTRAVEL

0.90

1.00

0.70

0.10

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

PI
P ROBLEMS

0.55

0.45

1.02

0.54

YES

(p=0.00)

NO

(p=1.00)

SEH+ (F )

OF



SEH+ (F )

Table 6: Aggregated comparison of SEH+ (F ) against FF-Replan (FFR). The R ANDOM and R IVER
domains required extending sampling to 60 samples and the T OWERS OF H ANOI domain required
extending sampling to 150 samples as per the experimental protocol described in Section 5.5.2.
The p-value of JSLEN-RATIO in B USFARE is not available because the extremely low success rate
of FFR leads to only one sample of JSLEN being gathered in 30 attempts, yielding no estimated
variance.

842

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

SR of
SR of
SEH+ (F ) RFF-BG

Category

JSLENRATIO
(RFF-BG/
SEH+ (F ))

JSTIMERATIO
(RFF-BG/
SEH+ (F ))

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

0.93

0.77

0.79

0.22

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.99

0.89

1.03

3.70

YES

(p=0.00)

NO

(p=1.00)

SEH+ (F )

D RIVE

0.69

0.61

1.07

1.24

YES

(p=0.00)

NO

(p=0.08)

SEH+ (F )

E LEVATOR

1.00

1.00

1.27

0.15

NO

(p=1.00)

YES

(p=0.00)

SEH+ (F )

E XPLODING
B LOCKSWORLD

0.44

0.43

0.84

0.56

NO

(p=0.92)

YES

(p=0.00)

RFF-BG

P ITCHCATCH

0.45

0.00





YES

(p=0.00)

R ANDOM

0.99

0.74

1.26

0.56

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

R IVER

0.66

0.51

0.77

0.21

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

S CHEDULE

0.54

0.43

1.06

0.08

YES

(p=0.00)

NO

(p=0.40)

SEH+ (F )

S EARCH
AND R ESCUE

1.00

0.01

2.99

0.86

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

S YSADMIN

0.27

0.27

1.10

9.31

NO

(p=1.00)

NO

(p=0.05)

Inconclusive

S YSTEMATIC
- TIRE

0.29

0.81

1.22

4.49

YES

(p=0.00)

YES

(p=0.00)

RFF-BG

T IREWORLD

0.91

0.71

0.68

0.21

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

T OWERS
H ANOI

0.58

0.48

0.64

0.01

YES

(p=0.03)

YES

(p=0.00)

SEH+ (F )

Z ENOTRAVEL

0.90

0.02

1.20

0.04

YES

(p=0.00)

NO

(p=0.27)

SEH+ (F )

PI
P ROBLEMS

0.55

0.51

0.91

0.50

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

OF



SEH+ (F )

Table 7: Aggregated comparison of SEH+ (F ) against RFF-BG. The R IVER and T OWERS OF
H ANOI domains required extending sampling to 60 samples as per the experimental protocol described in Section 5.5.2. The values and p-values of JSLEN-RATIO and JSTIME-RATIO in P ITCH CATCH are not available due to the zero success ratio of RFF-BG in this category.

843

fiW U , K ALYANAM , & G IVAN

SR of
SEH(L)

Category

JSLENSR of
RATIO
Greedy(L) (Greedy/
SEH)

JSTIMERATIO
(Greedy/
SEH)

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

1.00

1.00

7.00

3.69

NO

(p=1.00)

YES

(p=0.00)

SEH(L)

B OXWORLD

0.89

0.89

5.00

0.55

NO

(p=1.00)

YES

(p=0.00)

SEH(L)

E XPLODING
B LOCKSWORLD

0.10

0.02

1.09

1.00

YES

(p=0.00)

NO

(p=0.31)

SEH(L)

S YSTEMATIC
- TIRE

0.34

0.14

0.75

0.39

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

T IREWORLD

0.90

0.89

1.05

1.05

NO

(p=0.92)

NO

(p=0.60)

Inconclusive

T OWERS
H ANOI

0.60

0.00





YES

(p=0.00)

0.58

0.03

13.25

5.66

YES

(p=0.00)

OF

Z ENOTRAVEL


YES

(p=0.00)

SEH(L)
SEH(L)

Table 8: Aggregated comparison of SEH(L) against Greedy(L). The values of JSLEN-RATIO and
JSTIME-RATIO and p-value of JSLEN-RATIO in T OWERS OF H ANOI are not available due to the
zero success ratio of Greedy(L) in this category.

SR of
SEH(L)

Category

JSLENSR of
RATIO
FF-Replan (FFR/
SEH(L))

JSTIMERATIO
(FFR/
SEH(L))

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

Winner

B LOCKSWORLD

1.00

0.83

0.99

2.06

YES

(p=0.00)

NO

(p=1.00)

SEH(L)

B OXWORLD

0.89

0.88

3.61

0.54

NO

(p=0.97)

YES

(p=0.00)

SEH(L)

E XPLODING
B LOCKSWORLD

0.10

0.46

0.71

0.73

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

S YSTEMATIC
- TIRE

0.34

0.10

0.28

0.18

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

T IREWORLD

0.90

0.70

0.66

0.51

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

T OWERS
H ANOI

0.60

0.42

0.64

4.76

YES

(p=0.00)

YES

(p=0.00)

SEH(L)

0.58

1.00

0.58

0.03

YES

(p=0.00)

YES

(p=0.00)

FF-Replan

OF

Z ENOTRAVEL

Table 9: Aggregated comparison of SEH(L) against FF-Replan (FFR).

844

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

policy that greedily seeks to avoid dropping, similar to earliest deadline first. The problems
SEH encounters in our evaluation in S CHEDULE suggest future work in automatically recognizing
domains where large MDP construction is proving futile and automatically reducing MDP size
limits to adapt performance towards the behavior of a greedy policy. We note that across all tested
benchmark domains and both heuristics, there is only one domain/heuristic combination where this
phenomenon arose in practice.
6.2.2 SEH/SEH+ V ERSUS FF-R EPLAN

AND

RFF-BG

We have also demonstrated performance improvement of SEH+ (F ) over the best performing planners in the first three international probabilistic planning competitions, outperforming FF-Replan in
ten out of fifteen categories while losing in three (E XPLODING B LOCKSWORLD, P ITCHCATCH, and
Z ENOTRAVEL), and outperforming RFF-BG in 12 out of 15 categories while losing in E XPLODING
B LOCKSWORLD and S YSTEMATIC - TIRE. Additionally, SEH(L) outperforms FF-Replan in five out
of seven categories while losing in E XPLODING B LOCKSWORLD and Z ENOTRAVEL. In this section
we discuss the categories where SEH+ (F ) and SEH(L) lose to FF-Replan and RFF-BG.
Z ENOTRAVEL is a logistics domain where people are transported between cities via airplanes
and each load/unload/fly action has a non-zero probability of having no effect. As a result, it takes
an uncertain number of attempts to complete each task. In domains where the only probabilistic effect is a choice between change and no change, the all-outcome determinization leads to a
safe determinized plan for FF-Replanone in which no replanning is needed to reach the goal.
In such domains, including Z ENOTRAVEL, all-outcomes determinization can provide an effective
way to employ deterministic enforced hill-climbing on the problem. We note though though, that
determinization still ignores the probabilities on the action outcomes, which can lead to very bad
choices in some domains (not Z ENOTRAVEL). While both deterministic and stochastic enforced
hill-climbing must climb out of large basins in Z ENOTRAVEL, the substantial overhead of stochastic backup computations during basin expansion leads to at least a constant factor advantage for deterministic expansion. An extension to SEH that might address this problem successfully in future
research would detect domains where the only stochastic choice is between change and non-change,
and handle such domains with more emphasis on determinization.
E XPLODING B LOCKSWORLD is a variant of the blocks world with two new predicates detonated and destroyed. Each block can detonate once, during put-down, with some probability,
destroying the object it is being placed upon. The state resulting from the action depicted in Figure 3 has a delete-relaxed path to the goal, but no actual path, so this state is a dead-end attractor
for delete-relaxation heuristics such as CR-FF. FF-Replan or RFF-BG will never select this action
because there is no path to the goal including this action. SEH+ (F ) with the weak dead-end detection used in these experiments will select the dead action shown, resulting in poor performance
when this situation arises. It would be possible to use all-outcomes determinization as an improved
dead-end detector in conjunction with SEH+ (F ) in order to avoid selecting such actions. Any such
dead-end detection would have to be carefully implemented and managed to control the run-time
costs incurred as SEH relies critically on being able to expand sufficiently large local MDP regions
during online action selection.
In P ITCHCATCH, there are unavoidable dead-end states (used by the domain designers to simulate cost penalties). However, the CR-FF heuristic, because it is based on all-outcomes determinization, assigns optimistic values that correspond to assumed avoidance of the dead-end states. As a
845

fiW U , K ALYANAM , & G IVAN

Current
State

b3

b5

b4

b1

b2

b5

b3

b2

b4

Goal State

No Path
Destroyed Table

Pick up from table b3

b3

b5

b4

b1

b2

Destroyed Table

Figure 3: An illustration of a critical action choice of SEH+ (F ) in an E XPLODING B LOCKSWORLD
problem (IPPC2 P1). The middle state has no actual path to the goal but has a delete-relaxed path to
the goal. Due to the table having been exploded, no block can be placed onto the table, resulting in
the middle state being a dead-end state. The middle state is a dead-end with an attractive heuristic
value without regard to whether the blocks shown have remaining explosive charge or not, so this
state feature is not shown.
result, local search by SEH+ (F ) is unable to find any expected improvement on the CR-FF values,
and falls back to biased random walk in this domain. This domain suggests, as do the other domains where SEH+ (F ) performs weakly, that further work is needed on managing domains with
unavoidable deadend states.
The two categories where SEH(L) loses to FF-Replan (E XPLODING B LOCKSWORLD and
Z ENOTRAVEL) are also categories where SEH+ (F ) loses to FF-Replan. Greedily following the
learned heuristics in these two categories leads to lower success ratio than greedily following CRFF, suggesting more significant flaws in the learned heuristics than in CR-FF. Although SEH is able
to give at least a five-fold improvement over greedy following, in success ratio in these two categories, this improvement is not large enough for SEH(L) to match the performance of SEH+ (F ) or
FF-Replan, both based on the relaxed-plan heuristic of FF.
SEH+ loses to RFF in S YSTEMATIC - TIRE due to weak performance in Triangle Tireworld problems. Triangle Tireworld provides a map of connected locations arranged so that there is a single
safe path from the source to the destination, but exponentially many shorter unsafe paths8 .
Determinizing heuristics do not detect the risk in the unsafe paths and so greedy following of such
heuristics will lead planners (such as SEH+ ) to take unsafe paths, lowering their success rate. While
our results above show that SEH+ can often repair a flawed heuristic, in the Triangle Tireworld domain the heuristic attracts SEH+ to apparent improvements that are actually dead-ends.
In contrast, RFF is designed to increase robustness for determinized plans with a high probability of failure. RFF will continue planning to avoid such failure rather than relying on replanning
after failure. Because the initial determinized plan has a high probability of failure (relative to RFFs
8. The safe path can be drawn as following two sides of a triangular map, with many unsafe paths through the interior
of the triangle. Safety in this domain is represented by the presence of spare tires to repair a flat tire that has 50%
chance of occurring on every step.

846

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

JSLENRATIO
(FFR/
SEH+ )

JSTIMERATIO
(FFR/
SEH+ )

SR
Difference
Significant?
(p-value)

JSLENRATIO
Significant?
(p-value)

SR of
SEH+ (F )

SR of
FFReplan

B LOCKSWORLD

0.70

0.37

0.72

0.88

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

B OXWORLD

0.67

0.34

5.02

0.98

YES

(p=0.00)

YES

(p=0.00)

SEH+ (F )

Category

Winner

Table 10: Aggregated comparison of SEH+ (F ) against FF-Replan in scaled-up problems.

Category

SR of
SEH+ (F )

SR of
RFFBG

JSLENRATIO
(RFFBG/
SEH+ )

JSTIMERATIO
(RFFBG/
SEH+ )

SR
Difference
Significant?
(p-value)

B LOCKSWORLD

0.70

0.33

0.46

0.14

YES

(p=0.00)

B OXWORLD

0.67

0.00

0.88

10.81

YES

(p=0.00)

JSLENRATIO
Significant?
(p-value)

YES

(p=0.00)


Winner

SEH+ (F )
SEH+ (F )

Table 11: Aggregated comparison of SEH+ (F ) against RFF-BG in scaled-up problems.

threshold), RFF extends the plan before execution and can often detect the need to use the longer,
safe route.
6.2.3 P ERFORMANCE ON L ARGE P ROBLEMS
In order to demonstrate that the advantages of SEH are emphasized as problem size grows, we
present aggregated performance of SEH+ (F ) on additional large-sized problems we have generated using generators provided by the first IPPC. As such scaling experiments are computationally
very expensive, we have only run two domains that have been most widely evaluated in the planning literature: B LOCKSWORLD and B OXWORLD (which is a stochastic version of logistics). For
B LOCKSWORLD, we generated 15 problems each for 25- and 30-block problems. For B OXWORLD,
we generated 15 problems for the size of 20 cities and 20 boxes. (Only one problem across the three
competitions reached this size in B OXWORLD, and that problem was unsolved by the competition
winner, RFF.) The aggregated results against FF-Replan and RFF-BG are presented in Tables 10
and 11. The experiments for these scaled-up problems consumed 3,265 hours of CPU time and
show that SEH+ (F ) successfully completed a majority of the attempts while FF-Replan and RFF
succeeded substantially less often9 .
Note that although the FF heuristic is very good on B OXWORLD and other logistics domains, the
failure of all-outcomes determinization to take into account the probabilities on action outcomes is
quite damaging to FFR in B OXWORLD, leading the planner to often select an action hoping for its
9. Our statistical protocol requires 30 samples of a random variable averaging performance over 5 solution attempts, for
each planner for each problem. With 45 problems and 3 planners, this yields 30*5*45*3=20,250 solution attempts,
each taking approximately 10 CPU minutes on these large problems.

847

fiW U , K ALYANAM , & G IVAN

low-probability error outcome. We note that RFF uses a most-probable-outcome determinization
and will not suffer from the same issues as FFR in the boxworld. Given the high accuracy of the
FF heuristic in the boxworld, we believe that the ideas in RFF can likely be re-implemented and/or
tuned to achieve better scalability in the boxworld problems. We leave this possibility as a direction
for future work on understanding the scalability of RFF.

7. Summary
We have proposed and evaluated stochastic enforced hill-climbing, a novel generalization of the
deterministic enforced hill-climbing method used in the planner FF (Hoffmann & Nebel, 2001).
Generalizing deterministic search for a descendant that is strictly better than the current state in
heuristic value, we analyze a heuristic-based MDP around any local optimum or plateau reached at
increasing horizons to seek a policy that expects to exit this MDP with a better valued state. We
have demonstrated that this approach provides substantial improvement over greedy hill-climbing
for heuristics created using two different styles for heuristic definition. We have also demonstrated
that one resulting planner is a substantial improvement over FF-Replan (Yoon et al., 2007) and
RFF (Teichteil-Konigsbuch et al., 2010) in our experiments.
We find that the runtime of stochastic enforced hill-climbing can be a concern in some domains.
One reason for the long runtime is that the number and size of local optima basins or plateaus may
be large. Currently, long runtime is managed primarily by reducing to biased random walk when
resource consumption exceeds user-set thresholds. A possible future research direction regarding
this issue is how to prune the search space automatically by state or action pruning.

Acknowledgments
This material is based upon work supported in part by the National Science Foundation, United
States under Grant No. 0905372 and by the National Science Council, Republic of China (98-2811M-001-149 and 99-2811-M-001-067).

References
Aarts, E., & Lenstra, J. (Eds.). (1997). Local Search in Combinatorial Optimization. John Wiley &
Sons, Inc.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72, 81138.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bonet, B., & Geffner, H. (2005). mGPT: A probabilistic planner based on heuristic search. Journal
of Artificial Intelligence Research, 24, 933944.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: A unified approach to heuristic search
in deterministic and non-deterministic settings, and its application to MDPs. In Proceedings
of the Sixteenth International Conference on Automated Planning and Scheduling, pp. 142
151.
848

fiS TOCHASTIC E NFORCED H ILL -C LIMBING

Bryce, D., & Buffet, O. (2008). International planning competition uncertainty part: Benchmarks
and results.. http://ippc-2008.loria.fr/wiki/images/0/03/Results.pdf.
Buffet, O. (2011) Personal communication.
Cerny, V. (1985). Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm. J. Optim. Theory Appl., 45, 4151.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1995). Planning under time constraints in
stochastic domains. Artificial Intelligence, 76, 3574.
Domshlak, C., & Hoffmann, J. (2007). Probabilistic planning via heuristic forward search and
weighted model counting. Journal of Artificial Intelligence Research, 30, 565620.
Fahlman, S., & Lebiere, C. (1990). The cascade-correlation learning architecture. In Advances in
Neural Information Processing Systems 2, pp. 524  532.
Gardiol, N. H., & Kaelbling, L. P. (2003). Envelope-based planning in relational MDPs. In Proceedings of the Seventeenth Annual Conference on Advances in Neural Information Processing
Systems.
Gerevini, A., & Serina, I. (2003). Planning as propositional CSP: from Walksat to local search
techniques for action graphs. Constraints, 8(4), 389413.
Gordon, G. (1995). Stable function approximation in dynamic programming. In Proceedings of the
Twelfth International Conference on Machine Learning, pp. 261268.
Hansen, E., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds solutions with
loops. Artificial Intelligence, 129, 3562.
Hoffmann, J. (2003). The Metric-FF planning system: Translating ignoring delete lists to numeric
state variables. Journal of Artificial Intelligence Research, 20, 291341.
Hoffmann, J., & Brafman, R. (2005). Contingent planning via heuristic forward search with implicit
belief states. In Proceedings of the 15th International Conference on Automated Planning and
Scheduling.
Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search: A new
approach. Artificial Intelligence, 170(6-7), 507  541.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253302.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Proceedings of the Tenth European
Conference on Artificial Intelligence (ECAI92).
Kirkpatrick, S., Gelatt, Jr, C., & Vecchi, M. (1983). Optimization by simulated annealing. Science,
220, 671680.
Little, I., & Thiebaux, S. (2007). Probabilistic planning vs replanning. In Workshop on International
Planning Competition: Past, Present and Future (ICAPS).
Liu, C., & Layland, J. (1973). Scheduling algorithms for multiprogramming in a hard-real-time
environment. Journal of the Association for Computing Machinery, 20, 4661.
849

fiW U , K ALYANAM , & G IVAN

Mahadevan, S., & Maggioni, M. (2007). Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes. Journal of Machine Learning
Research, 8, 21692231.
Nilsson, N. (1980). Principles of Artificial Intelligence. Tioga Publishing, Palo Alto, CA.
Puterman, M. L. (2005). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques for first-order MDPs. Artificial
Intelligence, 173(5-6), 748788.
Selman, B., Kautz, H., & Cohen, B. (1993). Local search strategies for satisfiability testing. In
DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pp. 521532.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Incremental plan aggregation for generating policies in MDPs. In Proceedings of the Ninth International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2010), pp. 12311238.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using Monte-Carlo search. In
NIPS.
Wu, J., & Givan, R. (2007). Discovering relational domain features for probabilistic planning.
In Proceedings of the Seventeenth International Conference on Automated Planning and
Scheduling, pp. 344351.
Wu, J., & Givan, R. (2010). Automatic induction of Bellman-Error features for probabilistic planning. Journal of Artificial Intelligence Research, 38, 687755.
Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: A baseline for probabilistic planning. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling, pp. 352358.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). The first probabilistic track of the
international planning competition. Journal of Artificial Intelligence Research, 24, 851887.

850

fiJournal of Artificial Intelligence Research 42 (2011) 309-352

Submitted 02/11; published 11/11

Most Relevant Explanation in Bayesian Networks
Changhe Yuan

cyuan@cse.msstate.edu

Department of Computer Science and Engineering
Mississippi State University
Mississippi State, MS 39762

Heejin Lim

hlim@ai.kaist.ac.kr

Department of Computer Science
Korea Advanced Institute of Science and Technology
Daejeon, South Korea 305-701

Tsai-Ching Lu

tlu@hrl.com

HRL Laboratories LLC
Malibu, CA 90265

Abstract
A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. Existing methods for solving
this problem often generate explanations that are either too simple (underspecified) or too
complex (overspecified). In this paper, we introduce a method called Most Relevant Explanation (MRE) which finds a partial instantiation of the target variables that maximizes the
generalized Bayes factor (GBF) as the best explanation for the given evidence. Our study
shows that GBF has several theoretical properties that enable MRE to automatically identify the most relevant target variables in forming its explanation. In particular, conditional
Bayes factor (CBF), defined as the GBF of a new explanation conditioned on an existing
explanation, provides a soft measure on the degree of relevance of the variables in the new
explanation in explaining the evidence given the existing explanation. As a result, MRE is
able to automatically prune less relevant variables from its explanation. We also show that
CBF is able to capture well the explaining-away phenomenon that is often represented in
Bayesian networks. Moreover, we define two dominance relations between the candidate
solutions and use the relations to generalize MRE to find a set of top explanations that is
both diverse and representative. Case studies on several benchmark diagnostic Bayesian
networks show that MRE is often able to find explanatory hypotheses that are not only
precise but also concise.

1. Introduction
One essential quality of human experts is their ability to explain their reasoning to other
people. In comparison, computer expert systems still lack the capability in that regard.
Early medical decision-support systems such as MYCIN (Buchanan & Shortliffe, 1984) were
shown empirically to have comparable or even better diagnostic accuracies than domain
experts. However, physicians were still reluctant to use these systems in their daily clinical
settings. One major reason is that these expert systems lack the capability to clearly explain
their advice; physicians are uncomfortable in following a piece of advice that they do not
fully understand (Teach & Shortliffe, 1981). The capability of explanation is thus critical
for the success of a decision-support system.
c
2011
AI Access Foundation. All rights reserved.

fiYuan, Lim, & Lu

Bayesian networks (Pearl, 1988) offer compact and intuitive graphical representations of
the uncertain relations among the random variables in a domain and have become the basis
of many probabilistic expert systems (Heckerman, Mamdani, & Wellman, 1995b). Bayesian
networks provide principled approaches to finding explanations for given evidence, e.g.,
belief updating, Maximum a Posteriori assignment (MAP), and Most Probable Explanation
(MPE) (Pearl, 1988). However, these methods may generate explanations that are either too
simple (underspecified) or too complex (overspecified). Take a medical diagnostic system as
an example. Such a system may contain dozens or even hundreds of potentially dependent
diseases as target variables. Target variables are defined as the variables with diagnostic
or explanatory interest. Belief updating finds only singleton explanations by ignoring the
compound effect of multiple diseases. MAP and MPE consider such effect by finding a
full configuration of the target variables, but their explanations often contain too many
variables. Although a patient may have more than one disease, she almost never has more
than a few diseases at one time as long as she does not delay treatments for too long. It is
desirable to find explanations that only contain the most relevant diseases. Other diseases
should be excluded from further medical tests or treatments.
In this paper, we introduce a method called Most Relevant Explanation (MRE) which
finds a partial instantiation of the target variables that maximizes the generalized Bayes
factor (GBF) as the best explanation for the given evidence. Our study shows that GBF has
several theoretical properties that enable MRE to automatically identify the most relevant
target variables in forming its explanation. In particular, conditional Bayes factor (CBF),
defined as the GBF of a new explanation conditioned on an existing explanation, provides a
soft measure on the degree of relevance of the variables in the new explanation in explaining
the evidence given the existing explanation. As a result, MRE is able to automatically prune
less relevant variables from its explanation. We also show that CBF is able to capture well
the explaining-away phenomenon that is often represented in Bayesian networks. Moreover,
we define two dominance relations between the candidate solutions and use the relations to
generalize MRE to find a set of top explanations that is both diverse and representative. Our
case studies show that MRE performed well on the explanation tasks in a set of benchmark
Bayesian networks.
The remainder of the paper is structured as follows. Section 2 provides a brief overview
of the literature on explanation, including scientific explanation, explanation in artificial
intelligence, and the relation between causation and explanation. Section 3 provides an
introduction to explanation in Bayesian networks, especially methods for explaining evidence. Section 4 introduces the formulation of Most Relevant Explanation and discusses
its theoretical properties. This section also discusses how to generalize MRE to find a set
of top explanations. Section 5 presents the case studies of MRE on a set of benchmark
Bayesian networks. Finally, Section 6 concludes the paper.

2. Explanation
Explanation is a topic that is full of debate; in fact, there is no commonly accepted definition
of explanation yet. This section provides a very brief overview of the major developments
of explanation in both philosophy of science and artificial intelligence. A brief discussion
on the relation between causation and explanation is also included.
310

fiMost Relevant Explanation in Bayesian Networks

2.1 Scientific Explanation
Explanation has been the focal subject of philosophy of science for a long time. The goal
of explanation is not simply to describe the world as it is, but to develop a fundamental
and scientific understanding of the world (Woodward, 2003) and answer questions such as
why did this happen? The field is hence named scientific explanation.
One of the earliest model of scientific explanation is the Deductive-Nomological (D-N)
model (Hempel & Oppenheim, 1948). According to the D-N model, a scientific explanation
consists of an explanandum, the phenomenon to be explained, and an explanation (or
explanans), the facts used to explain the explanandum. The explanation can successfully
explain the explanandum if the explanation is true, and the explanation logically entails
the explanandum.
However, not every phenomenon can be expressed in terms of deterministic logic rules.
Many phenomena are inherently uncertain. Hempel (1965) modified his D-N model and
introduced the inductive-statistical (I-S) model. The I-S model is similar to the D-N model
except that it assumes there is a probability for each logic rule for capturing the uncertainty
in linking the initial conditions to the phenomenon to be explained.
Both the D-N and I-S models have the limitation of allowing the inclusion of irrelevant
facts in an explanation, because such facts do not affect the correctness of the rules (Suermondt, 1992). To address the shortcoming, Salmon (1970) introduced the statisticalrelevance (S-R) model. The S-R model requires that an explanation should only consist
of facts that are statistically relevant to the explanandum. A fact is statistically relevant
in explaining the explanandum if the posterior probability of the fact after observing the
explanandum is different from its prior probability. The intuition behind the S-R model
is that statistically irrelevant facts, even though they may have high probabilities, do not
constitute a good explanation.
Salmon (1984) introduced the Causal-Mechanical (C-M) model of explanation to take
into account causation. The basic idea behind the C-M model is that the process of explanation involves fitting an explanandum into a causal structure of a domain and tracing the
causes that may lead to the explanandum.
There are many other approaches as well. Ketcher (1989) believes that a scientific explanation should provide a unified account of the natural phenomena in the world. Van
Fraassen (1980) believes that an explanation should favor the explanandum, i.e., the explanation either increases the probability of the explanandum or decreases the probability of
the nearest competitor of the explanandum. Several mathematical theories of explanatory
power have also been proposed by Jeffreys (1935), Good (1977), and Gardenfors (1988).
2.2 Explanation in Artificial Intelligence
In comparison to scientific explanation, researchers in the area of artificial intelligence have
taken a much broader view of explanation. Early development of decision-support systems made it clear that decision-support systems should not be intended to replace human
experts, but rather provide advice or second opinion to the experts so that they can have
better performance. So it is necessary that the decision-support systems have the capability
to explain how their conclusions are made and why the conclusions are appropriate so that
the domain experts can understand and possibly follow the advice (Dannenberg, Shapiro,
311

fiYuan, Lim, & Lu

& Fries, 1979). In a decision-support system, any presentation that can help a users understanding of the conclusions of the system is regarded as an explanation (Suermondt, 1992).
For example, an explanation can be a trace of the reasoning process of a system in reaching
its conclusions (Suermondt, 1992), a canned textual explanation of an abstract reasoning
process (Bleich, 1972), a verbal translation of the inference rules (Buchanan & Shortliffe,
1984), or a visual display of certain elements of the system that helps a user to understand
the results (Lacave, Luque, & Diez, 2007).
Nevertheless, the most important form of explanation in artificial intelligence is the forming of explanatory hypotheses for observed facts, often called abductive inference (Peirce,
1948). Many issues need consideration in abductive inference. One issue is to define what
to explain. Not every observation needs explanation. According to Pierce (1948), a common trigger for abductive inference is that a surprising fact is observed, that is, the newly
observed information is in conflict with what is already known or what is currently believed.
In other words, what requires explanation is something that causes someones cognitive dissonance between the explanandum and the rest of her belief (Gardenfors, 1988). It is also
argued that some observations can serve as part of an explanation (Chajewska & Halpern,
1997; Nielsen, Pellet, & Elisseeff, 2008). Suppose we observe that the grass is wet, and it
rained. There is no need to explain why it rained. The fact that it rained is actually an
excellent explanation for why the grass is wet (Nielsen et al., 2008). Therefore, there is a
potential distinction between the explanandum, i.e., observations to be explained, and the
other observations. Deciding the explanandum may be a nontrivial task.
Once an explanandum is decided, the task reduces to finding an explanatory hypothesis
for the explanandum. The issue here is to define what a good explanation is. A good explanation should be able to provide some sort of cognitive relief, that is, the explanation should
decrease the surprise value caused by the observation of the explanandum. Intuitively,
the value of an explanation is the degree to which the explanation decreases the surprise
value. Many different criteria have been used in existing explanation methods, including
weight of evidence (Good, 1985), probability (Pearl, 1988), explanatory power (Gardenfors,
1988), likelihood of evidence (de Campos, Gamez, & Moral, 2001), and causal information
flow (Nielsen et al., 2008). One goal of this paper is to study and compare the properties
of some of these measures.
Moreover, the quality of an explanation is also highly goal-dependent. The resulting
explanations may vary in the level of specificity or scope depending on the objectives of the
explanation task (Leake, 1995). For example, when explaining the symptoms of a patient,
a doctor can either find an explanation that constitutes a diagnosis, i.e., explaining which
diseases the patient may have, or provide an explanation on which risk factors may have
caused the symptoms. Defining the goal of an explanation task is thus important.
The above is a general overview of explanation in artificial intelligence. In Section 3,
we will provide a more detailed review of one particular topic in this area: explanation in
Bayesian networks.
2.3 Causation and Explanation
It is agreed upon that causation plays an important role in explanation and helps to generate
intuitive explanations (Chajewska & Halpern, 1997; Halpern & Pearl, 2005; Nielsen et al.,
312

fiMost Relevant Explanation in Bayesian Networks

2008). However, there is considerable disagreement among researchers about whether all explanations are causal and what the distinction between causal and non-causal explanations
is. Those who believe in not completely subsuming explanation into causation typically
have a broader view of explanation. The following are some arguments for the belief that
there are explanation tasks that do not require causal meanings.
First of all, explanation has a much broader meaning in AI. Some explanation tasks in
AI do not require causal meanings. For example, verbal or visual explanations are often
used in decision-support systems to illustrate concepts, knowledge, or reasoning processes;
they do not seem to have causal meanings. If it is insisted that all explanations have to be
causal, all these explanation tasks may have to be disallowed.
Furthermore, there are many domains in which clear understandings of causal relations
have not yet been established. There is only statistical relevance information. Such relevance information is insufficient to fully capture causal relations. Again, explanation may
have to be disallowed altogether in these domains if we insist that all explanations have to
be causal.
Moreover, the situation is further complicated by the fact that there is still considerable
disagreement on the definition of causation. Causal claims themselves also vary drastically
in the extent to which they are explanatorily deep enough (Woodward, 2003). For some, it
is a sufficient causal explanation to say that the rock broke the window. For others, that is
merely a descriptive statement; it is necessary to resort to deeper Newtonian mechanics to
explain why the rock broke the window.
Finally, not all legitimate why-questions are causal or require causal explanations. For
example, there is a difference between the explanation of belief and the explanation of
fact (Chajewska & Halpern, 1997). When someone tells you that it rained last night and
you ask her why, she may give you as an explanation that the grass is wet. Another example
is that a variety of physical explanations are geometrical rather than causal because they
explain phenomena by using the structure of spacetime rather than using forces or energy
transfer (Nerlich, 1979).
It is not our goal in this paper to take on the tall task of settling the debate on the
relation between causation and explanation. The methods that we propose in this paper
are aimed to be general enough so that they are applicable to both causal and non-causal
settings.

3. Explanation in Bayesian Networks
A Bayesian network is a directed acyclic graph (DAG) in which the nodes denote random/chance variables, and the arcs or lack of them denote the qualitative relations among
the variables. The dependence relations between the variables are further quantified with
conditional probability distributions, one for each variable conditioned on its parents. Figure 1(b) shows an example of a Bayesian network. A Bayesian network essentially encodes
a joint probability distribution over the random variables of a domain and can serve as a
probabilistic expert system to answer various queries about the domain.
Unlike many machine learning methods that are mostly predictive methods, a Bayesian
network can be used for both prediction and explanation with a deep representation of a
domain. Explanation tasks in Bayesian networks can be classified into three categories;
313

fiYuan, Lim, & Lu

explanation of reasoning, explanation of model, and explanation of evidence (Lacave &
Diez, 2002). The goal of explanation of reasoning in Bayesian networks is to explain the
reasoning process used to produce the results so that the credibility of the results can
be established. Because reasoning in Bayesian networks follows a normative approach,
explanation of reasoning is more difficult than explanation in methods that try to imitate
human reasoning (Druzdzel, 1996; Lacave & Diez, 2002). The goal of explanation of model
is to present the knowledge encoded in a Bayesian network in easily understandable forms
such as visual aids so that experts or users can examine or even update the knowledge.
For example, graphical modeling tools such as GeNIe (Druzdzel, 1999), Elvira (Lacave
et al., 2007), and SamIam (AR Group, UCLA, 2010) have functionalities for visualizing
the strength of the probabilistic relations between the variables in a domain. The goal of
explanation of evidence is to explain why some observed variables are in their particular
states using the other variables in the domain. We focus on the explanation of evidence
in this research. Next we review the major methods for explaining evidence in Bayesian
networks and discuss their limitations.
3.1 Explanation of Evidence
Numerous methods have been developed to explain evidence in Bayesian networks. Some
of these methods make simplifying assumptions and focus on singleton explanations. For
example, it is often assumed that the fault variables are mutually exclusive and collectively
exhaustive, and there is conditional independence of evidence given any hypothesis (Heckerman, Breese, & Rommelse, 1995a; Jensen & Liang, 1994; Kalagnanam & Henrion, 1988).
However, singleton explanations may be underspecified and are unable to fully explain the
given evidence if the evidence is the compound effect of multiple causes.
For a domain with multiple dependent target variables, multivariate explanations are
often more appropriate for explaining the given evidence. Maximum a Posteriori assignment (MAP) finds a complete instantiation of a set of target variables that maximizes the
joint posterior probability given partial evidence on the other variables. Most Probable
Explanation (MPE) (Pearl, 1988) is similar to MAP except that MPE defines the target
variables to be all the unobserved variables. The common drawback of these methods is that
they often produce hypotheses that are overspecified and may contain irrelevant variables
in explaining the given evidence.
Everyday explanations are necessarily partial explanations (Leake, 1995). It is difficult
and also unnecessary to account for all the potential factors that may be related to the
occurrence of an event; it is desirable to find the most relevant contributing factors. Various
pruning techniques have been used to avoid overly complex explanations. These methods
can be grouped into two categories: pre-pruning and post-pruning. Pre-pruning methods
use the context-specific independence relations represented in Bayesian networks to prune
irrelevant variables (Pearl, 1988; Shimony, 1993; van der Gaag & Wessels, 1993, 1995) before
applying methods such as MAP to generate explanations. For example, Shimony (1993)
defines an explanation as the most probable independence-based assignment that is complete
and consistent with respect to the evidence nodes. Roughly speaking, an explanation is a
truth assignment to the variables relevant to the evidence nodes. Only the ancestors of the
evidence nodes can be relevant. An ancestor of a given node is irrelevant if it is independent
314

fiMost Relevant Explanation in Bayesian Networks

of that node given the values of other ancestors. However, these independence relations are
too strict and are unable to prune marginally or loosely relevant target variables.
In contrast, post-pruning methods first generate explanations using methods such as
MAP or MPE and then prune variables that are not important. An example is the method
proposed by de Campos et al. (2001). Their method first finds the K most probable explanations (K-MPE) for the given evidence, where K is a user-specified parameter, and then
simplify the explanations by removing unimportant variables one at a time. A variable is
regarded as unimportant if its removal does not reduce the likelihood of an explanation.
It is pointed out that Shimonys partial explanations are not necessarily concise (Chajewska & Halpern, 1997). His explanation must include an assignment to all the nodes in
at least one path from a variable to the root, since for each relevant node, at least one of its
parents must be relevant. The method by de Campos et al. can only prune conditionally
independent variables as well. The limitations of both methods were addressed by using a
thresholding method to allow the pruning of marginally relevant variables (Shimony, 1996;
de Campos et al., 2001). However, the modified methods involve the manual setting of some
tunable parameters, which can be rather arbitrary and are subject to human errors.
Several methods use the likelihood of evidence to measure the explanatory power of
an explanation (Gardenfors, 1988). Chajewska and Halpern (1997) extend the approach
further to use the value pair of <likelihood, prior probability> to order the explanations,
forcing users to make decisions if there is no clear order between two explanations. Since
the likelihood measure allows comparing explanations that contain different numbers of
variables, it can potentially find more concise explanations. But in practice these methods
often fail to prune irrelevant variables, because adding such variables typically does not
affect the likelihood.
Henrion and Druzdzel (1991) assume that a system has a set of pre-defined explanation
scenarios organized as a tree; they use the scenario with the highest posterior probability as
the explanation. This method also allows comparing explanations with different numbers
of variables but requires the explanation scenarios to be specified in advance.
Flores et al. (2005) propose to automatically create an explanation tree by greedily
branching on the most informative variable at each step while maintaining the probability
of each branch of the tree above a certain threshold. It is pointed out that Flores et al.s
approach adds variables in the order of how informative they are about the remaining target
variables, not how informative they are about the explanandum (Nielsen et al., 2008). The
results in this paper provide evidence for that drawback as well. Moreover, the criterion
that they use to choose the best explanation is the probability of the explanation given the
evidence, which makes the ranking of the explanations extremely sensitive to a user-specified
threshold for bounding the probabilities of the branches. Nielsen et al. developed another
method that uses the causal information flow (Ay & Polani, 2008) to select variables to
expand an explanation tree. However, it inherits the drawback that explanation tree-based
methods are in essence greedy search methods; even though they can identify important
individual variables, they may fail to recognize the compound effect of multiple variables.
Furthermore, the method also has tunable parameters that may be subject to human errors.
Finally, since each explanation in an explanation tree has to contain a full branch starting
from the root, such explanations may still contain redundant variables.
315

fiYuan, Lim, & Lu

Input

B

D

A

C

Output of B

A (0.016)
Output of D

Input

C (0.15)

Output of C

Output of A

Output

B (0.1)
Total Output

D (0.1)

(a)

(b)

Figure 1: (a) An electric circuit and (b) its corresponding diagnostic Bayesian network
3.2 Explanation in Two Benchmark Models
This section uses a couple of examples to illustrate how some of the methods reviewed in
the last section work in practice.
3.2.1 Circuit
We first consider the electric circuit in Figure 1(a) (Poole & Provan, 1991). Gates A, B, C,
and D are defective if they are closed. The prior probabilities that the gates are defective
are 0.016, 0.1, 0.15, and 0.1 respectively. We also assume that the connections between the
gates may fail with some small probabilities. The circuit can be modeled with the diagnostic
Bayesian network shown in Figure 1(b). Nodes A, B, C, and D correspond to the gates in
the circuit and have two states each: defective and ok. The others are input or output
nodes with two states each as well: current or noCurr. The probabilities that the
output nodes of A, B, C, and D are in the state current given their parent nodes are
parameterized as follows.
P (Output of B = current|B = def ective, Input = current) = 0.99;
P (Output of A = current|A = def ective, Input = current) = 0.999;
P (Output of C = current|C = def ective, Output of B = current) = 0.985;
P (Output of D = current|D = def ective, Output of B = current) = 0.995.
Otherwise if no parent is in the state current, the output nodes are in the state
noCurr with probability 1.0. Finally, the conditional probability table of Total Output
is a noisy-or gate (Pearl, 1988), which means each parent node that is in the state current
causes Total Output to be in the state current independently from the other parents.
If no parent node is in the state current, the probability that Total Output is in the
state current is 0.0. The individual effect of the parent nodes on Total Output is
parameterized as follows.
P (T otal Output = current|Output of A = current) = 0.9;
316

fiMost Relevant Explanation in Bayesian Networks

P (T otal Output = current|Output of C = current) = 0.99;
P (T otal Output = current|Output of D = current) = 0.995.
Suppose we observe that electric current flows through the circuit, which means that
nodes Input and T otal Output are both in the state current. Nodes A, B, C, and D are
the logical choices of target variables in this model. The task is to find the best explanatory
hypothesis to explain the observation of electric current in the circuit. Domain knowledge
suggests that there are three basic scenarios that most likely lead to the observation: (1) A
is defective; (2) B and C are defective; and (3) B and D are defective.
Given the observation of electric current, the posterior probabilities of A, B, C, and D
being defective are 0.391, 0.649, 0.446, and 0.301 respectively. Therefore, the explanation
(B) is the best single-fault explanation, where B means that B is defective. However,
B alone does not fully explain the evidence; C or D has to be involved. Actually, if we
do not restrict ourselves to the defective states, (D) is the best singleton explanation with
probability 0.699, but it is clearly not a useful explanation for the evidence.
MAP finds (A, B, C, D) as the best explanation. Given that B and C are defective, it
is arguable that A and D being okay is irrelevant in explaining the evidence. But MAP has
no intrinsic capability to indicate which part of its explanation is more important. Since
MPE assumes that all unobserved variables are the target variables, its explanation has
even more redundancy.
The pre-pruning techniques are unable to prune any target variable for this model because there is no context-specific independence between the target variables given the evidence. The method of simplifying K-MPE solutions requires the intermediate output nodes
to be included as explanatory variables. Since it is typically only necessary to consider
the target variables, we adapt their method slightly to simplify the top K MAP solutions
instead, which we refer to as the K-MAP simplification method hereafter. The best explanation found by this method is (B, D). It is a good explanation, although we will argue
later that (B, C) is a better explanation.
The methods based on the likelihood of evidence will overfit and choose (A, B, C, D)
as the best explanation, because the probability of the evidence given that all the target
variables are defective is almost 1.0.
Both explanation tree methods find (A) as the best explanation. (A) is a good
explanation, but again (B, C) is a better explanation as we argue later.
3.2.2 Vacation
Consider another example (Shimony, 1993). Mr. Smith is considering taking strenuous
hiking trips. His decision to go hiking or not depends on his health status. If he is healthy,
he will go hiking; otherwise, he would rather stay home. Mr. Smith is subject to different
risks of dying depending on his health status and on where he spends the vacation. The
relations between the variables are best represented using the Bayesian network in Figure 2.
The conditional probabilities of the model are parameterized as follows:
P (healthy) = 0.8;
P (home|healthy) = 0.8;
P (home|healthy) = 0.1;
317

fiYuan, Lim, & Lu

Healthy
Vacation
location

Alive

Figure 2: A Bayesian network for the vacation problem.

P (alive|healthy, V acation location = ) = 0.99;
P (alive|healthy, home) = 0.9;
P (alive|healthy, hiking) = 0.1,
where * means that the value does not matter. There are totally 100 similar hiking
trails for Mr. Smith to choose from. We can model the 100 hiking trips either as different
states of the variable Vacation location, or as one state named hiking. In case that the
hiking trails are modeled as different states, the conditional probability given the health
status is distributed evenly across these states. Shimony (1993) showed that the modeling
choice of one-state vs. multi-state significantly affects the best explanation of MAP. Given
that Mr. Smith is alive after his vacation, the best explanation on the target variable
set {Healthy, V acation location} changes from (healthy, hiking) for the one-state model
to (healthy, home) for the multi-state model. This is rather undesirable because the
explanation should not totally change simply because the model is refined.
We now examine how some other methods are affected by the modeling choice. The
explanation tree method finds (hiking) as the explanation for the one-state model and
(healthy, home) for the multi-state model. These explanations seem counterintuitive.
Both the causal explanation tree and K-MAP simplification methods find (healthy) as the
explanation for the two models.
What if Mr. Smith died afterwards? This is a more interesting explanation task because
the outcome is surprising given the low prior probability of dying in this problem. MAPs explanation changed from (healthy, hiking) for the one-state model to (healthy, home) for
the multi-state model. The explanation tree method finds (hiking) for the one-state model
and (home) for the multi-state model, again perplexing explanations. The causal explanation tree method finds (healthy, hiking) for the one-state model and (healthy, any trip)
for the multi-state model; so does the K-MAP simplification method. The causal explanation tree and K-MAP simplification methods are quite robust in the face of the refinement
of the model; their explanations also seem plausible. However, since the 100 hiking trips
are identical in the multi-state model, any hiking trip can be plugged into the explanation.
It means that there are 100 equally good explanations. It is debatable whether the specific
hiking trip is a necessary detail that needs to be included.
318

fiMost Relevant Explanation in Bayesian Networks

The above results show that the existing explanation methods in Bayesian networks
often generate explanations that are either underspecified or overspecified. They fail to find
just right explanations that only contain the most relevant target variables.

4. Most Relevant Explanation in Bayesian Networks
Users want accurate explanations but do not want to be burdened with unnecessary details.
A Bayesian network for a real-world domain may contain many target variables, but typically only a few of the target variables are most relevant in explaining any given evidence.
The goal of this research is to develop an explanation method that is able to automatically
identify the most relevant target variables in forming its explanation.
We first state several basic assumptions behind this research. First, an explanation
typically depends on an agents epistemic state (Gardenfors, 1988). We assume that the
knowledge encoded in a Bayesian network constitutes the complete epistemic state of the
explainer. Second, we assume that the explanandum is specified as the observed states
of a set of evidence variables in an explanation task. Third, we assume that the Bayesian
network is annotated such that a set of target variables is clearly defined. Under this setting,
we are able to focus on the most important issue of how to evaluate and select the best
explanation. We believe, however, the proposed methodologies can be easily generalized to
more general settings.
4.1 What is a Good Explanation?
We consider that a good explanation should have two basic properties: precise and concise.
Precise means that the explanation should decrease the surprise value of the explanandum
as much as possible. Many other concepts have been used to refer to the same property,
including confirmative (Carnap, 1948), sufficient (Khan, Poupart, & Black, 2009), and
relevant (Shimony, 1993). We also regard high explanatory power (Gardenfors, 1988) as
referring to the preciseness of an explanation. Concise means that the explanation should
only contain the most relevant variables in explaining the evidence. It is similar to another
concept called minimal (Khan et al., 2009).
There are attempts to capture both preciseness and conciseness into a single concept,
including consistent (Pearl, 1988) and coherent (Ng & Mooney, 1990). Pearl (1988) argues
that an explanation needs to be internally consistent, and that just taking sets of facts
that are likely to be true given the evidence may not produce reasonable results. However,
putting consistent facts together does not necessarily lead to either preciseness or conciseness. For example, two perfectly correlated facts are consistent, but they are not necessarily
relevant in explaining the evidence, and adding both of them to an explanation likely leads
to redundancy. Ng and Mooney (1990) define a coherence metric that measures how well
an explanation ties the various observations together. Their definition also seems likely to
lead to simple explanations. However, their definition is based on Horn-clause axioms and
cannot be generalized to probabilistic systems easily.
In addition, an explanation method for Bayesian networks should be able to capture the
explaining-away phenomenon often represented in Bayesian networks. The explaining-away
phenomenon refers to the situation in which an effect has multiple causes, and observing
the effect and one of the causes reduces the likelihood of the presence of the other causes.
319

fiYuan, Lim, & Lu

The explaining-away phenomenon can be represented using the collider structure (Pearl,
1988), i.e., the V structure of a single node with multiple parents. It is desirable to capture
this phenomenon in order to find explanations that are both precise and concise.
4.2 Definition of Explanation of Evidence
We note that the definition of an explanation as a full instantiation of the target variables
used in MAP or MPE is quite restrictive. It fundamentally limits the capability of these
methods to find concise explanations. In our approach we define explanation as follows.
Definition 1 Given a set of target variables M in a Bayesian network and partial evidence
e on the remaining variables, an explanation for the evidence is a joint instantiation x of
a non-empty subset X of the target variables, i.e.,   X  M.
The definition allows an explanation to be any partial instantiation of the target variables. Therefore, it provides an explanation method the freedom to choose which target
variables to include.
One key difference between our definition and those of many existing methods is that
the existing definitions often have a built-in relevance measure to be optimized, while our
definition treats any partial instantiation of the target variables as an explanation. We
believe that deciding a relevance measure is a separate issue from defining explanation.
The separation of the two issues allows us to not only compare the quality of different
explanations but also generalize our method to find multiple top explanations.
Note that we disallow disjunctives in our definition. We agree with Halpern and
Pearl (2005) that allowing disjunctive explanations causes both technical and philosophical problems. For example in an explaining-away situation, an effect may have multiple
potential causes. Let the number of causes be n. Each of the causes can be a good explanation itself. If we allow disjunctives of the causes, there are totally 2n disjunctives. It
is really difficult to consider all these disjunctives. Philosophically, if one of the causes is
present, any disjunctive that includes that cause as a part is true as well. It is unclear
which disjunctive we should choose as the explanation. Besides, a disjunctive of multiple
causes seems equivalent to claiming that each cause is a potential explanation. Separating
the causes into individual explanations allows comparing the explanations. It is unclear
what benefits it has to allow disjunctives.
4.3 Relevance Measures
We need a relevance measure to evaluate the quality of an explanation. The measure should
be able to not only evaluate the explanatory power of an explanation but also favor more
concise explanations so that only the most relevant variables are included in an explanation.
Since we are dealing with probabilistic expert systems, the measure should be based on
the probabilistic relations between the explanation and the explanandum (Chajewska &
Halpern, 1997). It is also desirable if the probabilistic relation can be summarized into a
single number (Suermondt, 1992). Next we discuss several popular relevance measures in
order to motivate our own choice.
One commonly used measure is the probability of an explanation given the evidence, as
used in MAP and MPE to find the most likely configuration of a set of target variables. By
320

fiMost Relevant Explanation in Bayesian Networks

relying on the posterior probability, however, an explanation may contain independent or
marginally relevant events that have a high probability. Methods that use probability as the
relevance measure do not have the intrinsic capability to prune the less relevant facts. Some
may argue that those variables should not be selected as target variables for explanation in
the first place. But that requires the important task of selecting the most relevant variables
to rest on the shoulders of users. For an explanation method to be effective, it should
perform for the user the task of extracting the most essential knowledge and be as simple
as possible (Druzdzel, 1996). Moreover, we have shown earlier that the probability measure
is quite sensitive to the modeling choices; simply refining a model can dramatically change
the best explanation.
Another commonly used measure is the likelihood of the evidence and its variations. The
likelihood measure has an undesirable property called irrelevant conjunction (Chajewska &
Halpern, 1997; Rosenkrantz, 1994), that is, adding an irrelevant fact to a valid explanation
does not change its likelihood. This property limits the capability of any method based on
this measure to find concise explanations. Another common drawback of the probability
and likelihood measures is that they focus on measuring the preciseness of an explanation;
they have no intrinsic mechanism to achieve conciseness in an explanation. A measure that
can achieve both preciseness and conciseness at the same time is highly desirable.
According to Salmon (1984), in order to construct a satisfactory statistical explanation,
it is necessary to factor in both prior and posterior probabilities of either the explanandum
or the explanation. An explanation should result from the comparison between the prior and
posterior probabilities. This comparative view of explanation generates a set of possibilities.
One form of comparison is the difference between the prior and posterior probabilities.
However, it is inappropriate as a relevance measure according to Good (1985). Consider an
increase of probability from 1/2 to 3/4, and from 3/4 to 1. In both cases the difference in
the probabilities is 1/4, but the degree of relevance is entirely different.
Another possibility is the belief update ratio, which we define as follows.
Definition 2 Assuming P (x) 6= 0, the belief update ratio of x given e, r(x; e), is defined
as
P (x|e)
.
(1)
r(x; e) 
P (x)
A trivial mathematical derivation shows that the following is also true.
r(x; e) =

P (e|x)
.
P (e)

(2)

Therefore, the belief update ratio is equivalent to the ratio of the posterior and prior
probabilities of the explanandum given the explanation. It is clear that both ratios are
proportional to the likelihood measure P (e|x) up to a constant. They therefore share the
same drawbacks as the likelihood measure.
Suermondt (1992) uses the cross entropy between the prior and posterior probability
distributions of a target variable to measure the influence of an evidence variable on the
target variable. Cross entropy assigns a large penalty for incorrect statements of certainty
or near-certainty. However, the measure was used to select the most influential evidence
variables, not to select specific states of the target variables as an explanation. Furthermore,
321

fiYuan, Lim, & Lu

it is pointed out that cross-entropy is not an ideal measure because it is not symmetric; it
is necessary to keep track of which distribution represents the origin for comparison at all
times.
In a 1935 paper (Jeffreys, 1935) and in the book Theory of Probability (Jeffreys, 1961), a
measure called Bayes factor was proposed for quantifying the evidence in favor of a scientific
theory. Bayes factor is the ratio between the likelihoods of a hypothesis and an alternative
hypothesis. If the alternative hypothesis is also a simple statistical hypothesis, the measure
is simply the likelihood ratio. In other cases when either hypothesis has unknown parameters,
the Bayes factor still has the form of a likelihood ratio, but the likelihoods are obtained
by integrating over the parameter space (Kass & Raftery, 1995). The logarithm of the
Bayes factor was defined as the weight of evidence independently by Good (1950) and
by Minsky and Selfridge (1961). Similar to cross-entropy, Bayes factor (or the weight of
evidence) also assigns large values to the probabilities close to certainty. The drawback of
Bayes factor, however, is that it is difficult to use this measure to compare more than two
hypotheses (Suermondt, 1992); it is necessary to do pairwise comparisons between multiple
hypotheses.
4.4 Generalized Bayes Factor
With a slight modification, we can generalize the Bayes factor to compare multiple hypotheses. We define the generalized Bayes factor (GBF) in the following.
Definition 3 The generalized Bayes factor (GBF) of an explanation x for given evidence
e is defined as
P (e|x)
,
(3)
GBF (x; e) 
P (e|x)
where x denotes the set of all alternative hypotheses of x.
There is only one hypothesis in x if X contains a single binary variable. Otherwise,
x catches all the alternative hypotheses of x. This catch-all form of Bayes factor was
previously introduced by Fitelson (2001). In the next few sections, we show that GBF has
several desirable theoretical properties that enable it to automatically identify the most
relevant variables in finding an explanation. These properties are not possessed by the
simple form of Bayes factor and prompt us to give GBF the new name to emphasize its
importance.
Note that we do not really need to compute P (e|x) directly in calculating GBF (x; e).
A trivial mathematical derivation shows that
P (x|e)(1  P (x))
.
(4)
GBF (x; e) =
P (x)(1  P (x|e))
Therefore, GBF is no longer a measure for comparing different hypotheses, but simply a
measure that compares the posterior and prior probabilities of a single hypothesis. GBF is
hence able to overcome the drawback of Bayes factor in having to do pairwise comparisons
between multiple hypotheses.
Using x in the definition of GBF to catch all the alternative hypotheses enables GBF
to capture a property that we call symmetry in explanatory power. Consider two complementary simple hypotheses H and H in the following two distinct cases. In the first case,
322

fiMost Relevant Explanation in Bayesian Networks

P (H) increases from 0.7 to 0.8. The increase goes hand in hand with the decrease of P (H)
from 0.3 to 0.2. In the second case, P (H) increases from 0.2 to 0.3, which also goes hand in
hand with the decrease of P (H) from 0.8 to 0.7. The two cases are completely symmetric
to each other, which indicates that the two Hs should have the same amount of explanatory
power. GBF assigns the same score to H in both cases. But many other relevance measures
such as probability and the likelihood measure assign them totally different scores.
Besides Equation 4, GBF can be expressed in other forms, one of which is in the following.
P (x|e)/P (x|e)
.
(5)
GBF (x; e) 
P (x)/P (x)
Essentially, GBF is equal to the ratio between the posterior odds ratio of x given e and
the prior odds ratio of x (Good, 1985). Therefore, GBF measures the degree of change in
the odds ratio of an explanation.
GBF can also be calculated as the ratio between the belief update ratios of x and
alternative explanations x given e, i.e.,
GBF (x; e) =

P (x|e)/P (x)
.
P (x|e)/P (x)

(6)

When there are multiple pieces of evidence, GBF can also be calculated using a chain
rule similar to that of a joint probability distribution of multiple variables. We first define
conditional Bayes factor as follows.
Definition 4 The conditional Bayes factor (CBF) of explanation y for given evidence e
conditioned on explanation x is defined as
GBF (y; e|x) 

P (e|y, x)
.
P (e|y, x)

(7)

Then, it is easy to show that the following chain rule for calculating the GBF of an
explanation given a set of evidence is true.
GBF (x; e1 , e2 , ..., en ) = GBF (x; e1 )

n
Y

GBF (x; ei |e1 , e2 , ..., ei1 ).

(8)

i=2

The chain rule is especially useful when the multiple pieces of evidence are obtained
incrementally.
4.4.1 Handling Extreme Values
GBF assigns much more weight to probabilities in ranges close to 0 and 1. Such weighting
is consistent with the decision-theoretic interpretation of a subjective probability of 0 and
1 (Suermondt, 1992). The belief that the probability of an event is equal to 1 means that
one is absolutely sure that the event will occur. One probably would bet anything, including
her own life, on the occurrence of the event. It is a rather strong statement. Therefore,
any increase in probability from less than one to one or decrease from non-zero to zero is
extremely significant, no matter how small the change is.
323

fiYuan, Lim, & Lu

 


 


  







fffi




	




  	     	  
  



 

 	 

Figure 3: The GBF as a function of the prior probability given a fixed increase in the
posterior probability from the prior. The different curves correspond to different
probability increases. For example, +0.01 means the difference between the
posterior and prior probabilities is 0.01.

The following are several special cases for computing GBF in the face of extreme probabilities.
If P (x) = 0.0, P (x|e) must be equal to zero as well, which yields a ratio between
two zeros. As is commonly done, we define the ratio of two zeros as zero. Intuitively, an
impossible explanation is never useful for explaining any evidence.
If P (x) = 1.0 and P (x|e) = 1.0, we again have a ratio between two zeros. Since
the explanation is true no matter what the evidence is, it is not useful for explaining the
evidence either. This case can actually be used as a counterexample for using probability as
a relevance measure, because the fact that an explanation has a high posterior probability
may simply be due to its high prior probability.
If P (x) < 1.0 and P (x|e) = 1.0, the GBF score is equal to infinity. The fact that an
explanation initially has uncertainty but becomes certainly true after observing the evidence
warrants the explanation to have a large GBF score.
4.4.2 Monotonicity of GBF
We study the monotonicity of GBF with regard to two other relevance measures. The
first measure is the difference between the posterior and prior probabilities. It is commonly
believed that the same amount of difference in probability in ranges close to zero or one is
much more significant than in other ranges. A prominent measure that is able to capture
the belief is the K-L divergence (Kullback & Leibler, 1951). A measure for explanatory
power should also capture this belief in ranking explanations. Figure 3 shows a plot of
GBF against the prior probability when the difference between the posterior and prior
probabilities is fixed. For example, +0.01 means the difference between the posterior and
324

fiMost Relevant Explanation in Bayesian Networks






fi


ff
	











         
 







  

Figure 4: The GBF as a function of the prior probability when the belief update ratio is
fixed. The different curves correspond to different belief update ratios.

prior probabilities is 0.01. The figure clearly shows that GBF assigns much higher scores
to probability changes close to zero and one.
We also study the monotonicity of GBF with regard to another measure, the belief
update ratio defined in Equation 1. Recall that the belief update ratio is proportional to
the likelihood measure up to a constant. The likelihood measure, typically together with a
penalty term for the complexity of a model, is a popular metric used in model selection. AIC
or BIC (Schwartz, 1979) are prominent examples. Next we show that GBF provides more
discriminant power than the belief update ratio (hence, also the likelihood measure and
the ratio between the posterior and prior probabilities of evidence). We have the following
theorem. All the proofs of the theorems and corollaries in this paper can be found in the
appendix.
Theorem 1 For an explanation x with a fixed belief update ratio r(x; e) greater than 1.0,
GBF (x; e) is monotonically increasing as the prior probability P (x) increases.
Figure 4 plots GBF as a function of the prior probability while fixing the belief update
ratio. The different curves correspond to different belief update ratios. GBF automatically
takes into account the relative magnitude of the probabilities in measuring the quality of an
explanation. As the prior probability of an explanation increases, the same ratio of probability increase becomes more and more significant. Therefore, explanations that cannot be
distinguished by the likelihood measure can be ranked using GBF. Since lower-dimensional
explanations typically have higher probabilities, GBF has the intrinsic capability to penalize more complex explanations. The value pair of <likelihood, prior probability> used by
Chajewska and Halpern (1997) is only able to produce partial orderings among the explanations. In some sense, GBF addresses its limitation by integrating the two values into a
single value to produce a complete ordering among the explanations. Users do not have the
burden to define a complete ordering anymore.
325

fiYuan, Lim, & Lu

4.4.3 Achieving Conciseness in Explanation
This section discusses several theoretical properties of GBF. The key property of GBF is
that it is able to weigh the relative importance of multiple variables and only include the
most relevant variables in explaining the given evidence. We have the following theorem.
Theorem 2 Let the conditional Bayes factor (CBF) of explanation y given explanation x
be less than or equal to the inverse of the belief update ratio of the alternative explanations
x, i.e.,
1
GBF (y; e|x) 
,
(9)
r(x; e)
then we have
GBF (x, y; e)  GBF (x; e).

(10)

The conditional independence relations in Bayesian networks only provide a hard measure on the relevance of an explanation y with regard to another explanation x; the answer
is either yes or no. In contrast, GBF (y; e|x) is able to provide a soft measure on the relevance of y in explaining e given x. GBF also encodes a decision boundary, the inverse
belief update ratio of the alternative explanations x given e. The ratio is used to decide how
important the remaining variables should be in order to be included in an explanation. If
1
, y is important enough to be included. Otherwise, y will
GBF (y; e|x) is greater than r(x;e)
be excluded from the explanation. Simply being dependent is not enough for a variable to
be included in an explanation.
Theorem 2 has several intuitive and desirable corollaries. The first corollary states that,
for any explanation x with a belief update ratio greater than 1.0, adding an independent
variable to the explanation will decrease its GBF score.
Corollary 1 Let x be an explanation with r(x; e) > 1.0, and Y  X, E, then for any state
y of Y , we have
GBF (x, y; e) < GBF (x; e).
(11)

Therefore, adding an irrelevant variable will dilute the explanatory power of an existing
explanation. GBF is able to automatically prune such variables from its explanation.
Note that we focus on explanations with a belief update ratio greater than 1.0. An
explanation whose probability does not change or even decreases given the evidence does
not seem to be able to relieve the cognitive dissonance between the explanandum and the
rest of our beliefs. According to Chajewska and Halpern (1997), the fact that a potential
explanation is equally or less likely a posteriori than a priori should cause some suspicion.
In the philosophical literature, it is also often required that the posterior probability of
the explanandum to be at least greater than its unconditional probability, so that learning
the explanation increases the probability of the explanandum (Gardenfors, 1988; Carnap,
1948).
Corollary 1 requires that variable Y to be independent from both X and E. The
assumption is rather strong. The following corollary shows that the same result holds if Y
is conditionally independent from E given x.
326

fiMost Relevant Explanation in Bayesian Networks

Corollary 2 Let x be an explanation with r(x; e) > 1.0, and Y  E|x, then for any state
y of Y , we have
GBF (x, y; e) < GBF (x; e).
(12)

Corollary 2 is a more general result than Corollary 1 and captures the intuition that
conditionally independent variables add no additional information to an explanation in explaining the evidence. Note that these properties are all relative to an existing explanation.
It is possible that a variable is independent from the evidence given one explanation, but becomes dependent on the evidence given another explanation. Therefore, selecting variables
one by one greedily does not guarantee to find the explanation with the highest GBF.
The above results can be further relaxed to accommodate cases in which the posterior
probability of y given e is smaller than its prior conditioned on x, i.e.,
Corollary 3 Let x be an explanation with r(x; e) > 1.0, and y be a state of a variable Y
such that P (y|x, e) < P (y|x), then we have
GBF (x, y; e) < GBF (x; e).

(13)

This is again an intuitive result; the state of a variable whose posterior probability decreases
given the evidence should not be part of an explanation for the evidence.
We applied GBF to rank the candidate explanations in the circuit example introduced
in Section 3. The following shows a partial ranking of the explanations with the highest
GBF scores.
GBF (B, C; e) > GBF (B, C, A; e), GBF (B, C, D; e) > GBF (B, C, A, D; e)
(B, C) not only has the highest GBF score but also is more concise than the other
explanations, which indicates that variables A and D are not very relevant in explaining
the evidence once (B, C) is observed. The results indicate that GBF has the intrinsic
capability to penalize higher-dimensional explanations and prune less relevant variables,
which match the theoretical properties well.
4.5 Most Relevant Explanation
The theoretical properties presented in the previous section show that GBF is a plausible
relevance measure for the explanatory power of an explanation. In particular, they show
that GBF is able to automatically identify the most relevant target variables in finding an
explanation. We hence propose a method called Most Relevant Explanation (MRE) which
relies on GBF in finding explanations for given evidence in Bayesian networks.
Definition 5 Let M be a set of target variables, and e be the partial evidence on the
remaining variables in a Bayesian network. Most Relevant Explanation is the problem
of finding an explanation x for e that has the maximum generalized Bayes factor score
GBF (x; e), i.e.,
M RE(M; e)  arg maxx,XM GBF (x; e) .
(14)
327

fiYuan, Lim, & Lu

Although MRE is general enough to be applied to any probabilistic distribution model,
MREs properties make it especially suitable for Bayesian networks. Bayesian networks were
invented to model the conditional independence relations between the random variables of
a domain so that we not only obtain a concise representation of the domain but also have
efficient algorithms for reasoning about the relations between the variables. The concise
representation of a Bayesian network is also beneficial to MRE in the following ways.
First, MRE can utilize the conditional independence relations modeled by a Bayesian
network to find explanations more efficiently. For example, Corollaries 1 and 2 can be used
to prune independent and conditionally independent target variables so that not all of the
partial instantiations of the target variables need to be considered during the search for
a solution. Such independence relations can be identified through the graphical structure
of a Bayesian network and may significantly improve the efficiency of the search for an
explanation.
Second, similar to the chain rule of Bayesian networks, the chain rule of GBF in Equation 7 can also be simplified using the conditional independence relations. When computing GBF (x; ei |e1 , e2 , ..., ei1 ), we may only need to condition on a subset of the evidence in
(e1 , e2 , ..., ei1 ). Conditioning on fewer evidence variables allows the reasoning algorithms to
involve a smaller part of a Bayesian network in computing the GBF score (Lin & Druzdzel,
1998). One extreme case is that, when all the evidence variables are independent given an
explanation, the GBF of the explanation given all the evidence is simply the product of the
individual GBFs given each individual piece of evidence.
GBF (x; e1 , e2 , ..., en ) =

n
Y

GBF (x; ei ).

(15)

i=1

Finally, MRE is able to capture well the unique explaining-away phenomenon that is
often represented in Bayesian networks. Wellman and Henrion (1993) characterized explaining away as the negative influence between two parents induced by an observation on
the child in a Bayesian network. Let A and B be predecessors of C in a graphical model
G. C is observed to be equal to c. Let x denote an assignment to Cs other predecessors,
and y denote an assignment to Bs predecessors. The negative influence between A and B
conditioned on C = c means
P (B|A, c, x, y)  P (B|c, x, y)  P (B|A, c, x, y).

(16)

MRE captures the explaining-away phenomenon well with CBF. CBF provides a measure on how relevant some new variables are in explaining the evidence conditioned on an
existing explanation. In the explaining-away situation, if one of the causes is already present
in an explanation, the other causes typically do not receive high CBFs. In fact, CBF can
capture Equation 16 in an equivalent way as shown in the following theorem.
Theorem 3 Let A and B be predecessors of C in a Bayesian network. C is observed to
be equal to c. Let x denote an assignment to Cs other predecessors, and y denote an
assignment to Bs predecessors, then we have
P (B|A, c, x, y)  P (B|c, x, y)  P (B|A, c, x, y)
 GBF (B; c|A, x, y)  GBF (B; c|x, y)  GBF (B; c|A, x, y) .
328

(17)
(18)

fiMost Relevant Explanation in Bayesian Networks

Bayes factor
<1
1 to 3
3 to 10
10 to 30
30 to 100
>100

Strength of evidence
Negative
Barely worth mentioning
Substantial
Strong
Very strong
Decisive

Table 1: Bayes factor.

Again consider the circuit example, (B, C) and (A) are both good explanations
for the evidence by themselves; the GBF scores of (B, C) and (A) given only e are
42.62 and 39.44 respectively. Jeffreys (1961) recommends using Table 1 as a guidance in
determining the significance of a Bayes factor value. If we use the same table to judge GBFs,
(B, C) and (A) are both very strong explanations. However, when (B, C) is already
observed, GBF (A; e|B, C) is only equal to 1.03, which is barely worth mentioning.
The results indicate that GBF is able to capture the explaining-away phenomenon for the
circuit example.
4.6 K-MRE
In many decision making problems, decision makers typically would like multiple competing
options to choose from. Outputting only the best solution is hence not the best practice.
This is especially true when there are multiple top solutions that are almost equally good.
In that case, we want to know not only which solution is the best, but also how much better
it is than the other solutions. If the difference between the first and second best solutions is
large, it gives decision makers more confidence in the quality of the best solution. Moreover,
finding multiple top solutions can be used as a sensitivity analysis method to provide insight
on how sensitive the best explanations are to the changes in the model parameters.
A naive approach to finding the top K MRE solutions is to select the explanations with
the highest GBF scores. However, this strategy may find explanations that are supersets
of other top explanations. Once again consider the circuit example. Table 2 lists the
explanations with the highest GBF scores. Simply relying on the scores will produce the
following rather similar top four explanations: (B, C), (A, B, C), (B, C, D), and
(A, B, C, D). All these explanations essentially cover the same basic scenario in which
both B and C are defective. We have to search further down the list before finding the
other two basic scenarios: A is defective, and B and D are both defective.
It is often critical to achieve diversity when the goal is to find multiple solutions. For
example, it is argued that diversity is important in recommender systems (Smyth & McClave, 2001). A rather similar set of recommendations will not give users a useful set of
alternatives to choose from. We believe that the same holds in finding explanations. In
order to find a set of top explanations that are more diverse and representative, we define
two dominance relations among the candidate solutions of MRE. The first relation is strong
dominance.
329

fiYuan, Lim, & Lu

Explanations
(B, C)
(A, B, C)
(B, C, D)
(A, B, C, D)
(A)
(A, B)
(A, C)
(B, D)

GBF
42.62
42.15
39.93
39.56
39.44
36.98
35.99
35.88

Table 2: The explanations with the highest GBF scores for the Circuit network. The explanations in boldface are the top minimal explanations.

Definition 6 An explanation x strongly dominates another explanation y if and only if
x  y and GBF (x)  GBF (y).
If x strongly dominates y, x is clearly a better explanation than y, because it not only
has a higher or equal explanatory score but also is more concise. We only need to include
x in the top explanation set. The second relation is weak dominance.
Definition 7 An explanation x weakly dominates another explanation y if and only if
x  y and GBF (x) > GBF (y).
In this case, x has a larger GBF score than y, but y is more concise. It is possible
that we can include them both and let the decision maker to decide whether she prefers
conciseness or a higher score. However, we believe that we only need to include x, because
its higher GBF score indicates that the extra variables in X but not in Y are important in
explaining the given evidence.
Based on the two kinds of dominance relations, we define the concept minimal.
Definition 8 An explanation is minimal if it is neither strongly nor weakly dominated by
any other explanation.
A new K-MRE approach can now be defined such that only the minimal explanations
are included in the top set. For the circuit network, since (A, B, C), (B, C, D), and
(A, B, C, D) are strongly dominated by (B, C), we only need to consider (B, C)
among them. Similarly, (A, B) and (A, C) are strongly dominated by (A), so only (A)
is included in the top set. Finally, we get the set of top explanations shown in boldface in
Table 2. It is clearly more diverse and representative than the original set that contains the
dominated explanations.
The dominance relations defined here are not restricted to GBF; they are also applicable
to other relevance measures. For example, they can potentially help the methods based on
the likelihood measure to find more concise explanations.
330

fiMost Relevant Explanation in Bayesian Networks

5. Case Studies
We tested MRE on the explanation tasks of a set of benchmark Bayesian networks from the
literature, including Circuit (Poole & Provan, 1991), Vacation (Shimony, 1993), Academe
(Flores et al., 2005), Asia (Lauritzen & Spiegelhalter, 1988), and Circuit2 (Darwiche, 2009).
These networks were annotated such that the variables are classified into three categories:
target, observation, and auxiliary. A target node, also named fault, usually represents
a diagnostic interest (e.g., the health status of an engine). An observation node usually
represents a symptom (e.g., observing excessive smoking in the engine exhaust), built-in
error message (e.g., the status of a power supply), or a test (e.g., measuring the voltage of a
battery). A node which is neither target nor observation is classified as an auxiliary node.
We compared K-MRE against several existing explanation methods, including K-MAP
(Pearl, 1988), explanation tree (ET) (Flores et al., 2005), causal explanation tree (CET)
(Nielsen et al., 2008), and K-MAP simplification (K-SIMP) (de Campos et al., 2001). Several of these methods have tunable parameters. The explanation tree (ET) method has two
parameters for controlling the growth of an explanation tree. One parameter is a threshold
value for deciding whether a target variable is significant enough to be used to expand a
branch of the explanation tree; the variable is used if the average mutual information between the target variable and the other unused target variables conditioned on the current
branch is larger than or equal to the threshold. The other parameter is a threshold value
on the probability of any branch of the explanation tree. A branch is not expanded further
if the probability of the branch is less than the threshold. We set these two parameters to
be 0.05 and 0 respectively. A branch in the explanation tree is marked with its posterior
probability.
The causal explanation tree (CET) method has only one parameter, a lower-bound
threshold on the causal information flow of a variable on the evidence conditioned on the
current branch. If the causal information flow is larger than or equal to the threshold,
the variable is used to expand the branch further. We set the threshold to be 0.01. Each
branch of a causal explanation tree is marked using the log ratio of the posterior and prior
probabilities of the evidence given the branch.
The K-MAP simplification (K-SIMP) method also has a threshold value which bounds
the reduction in the likelihood of evidence when simplifying an explanation. If deleting
a variable only reduces the likelihood of an explanation within a factor bounded by the
threshold, the simplification is allowed. We set the threshold value to be 0.05. We keep
track of the explanations with the highest likelihood values during the simplification.
All the parameters in these methods were set to allow as much expansion and simplification as possible. But even so, the ET and CET methods may fail to find any significant
variable to create even the root of an explanation tree. When that happens, we force these
two methods to generate at least the root node by ignoring the thresholds.
5.1 Circuit
For the circuit network introduced in Section 3, Table 3 lists the top explanations found
by K-MRE, K-MAP, and K-SIMP. We set K to be 3 throughout the case studies. Figure 5
shows the explanation trees found by the ET and CET methods.
331

fiYuan, Lim, & Lu

Circuit
K-MRE

K-MAP

K-SIMP

Explanations
(B, C)
(A)
(B, D)
(A, B, C, D)
(A, B, C, D)
(A, B, C, D)
(B, D)
(B, C)
(A)

Scores
42.62
39.45
35.88
0.0128
0.0099
0.0082
0.9818
0.9683
0.9014

Table 3: Top explanations found by K-MRE, K-MAP, and K-SIMP for the Circuit network
given the observation of electric current. Note the scores of the methods have
the following different meanings: GBF for MRE, probability for MAP, and the
likelihood of evidence for K-SIMP.

K-MRE was able to find intuitive explanations for the Circuit network. (B, C) is a
better explanation than both (A) and (B, D), because it has a larger posterior probability than the other two explanations (the posterior probabilities are 0.394, 0.391, and
0.266 respectively), while the prior probabilities of the explanations are 0.015, 0.016, and
0.01 respectively.
The explanations found by K-MAP are mostly consistent with what K-MRE found,
although the K-MAP solutions are all supersets of the K-MRE solutions. It is not surprising
because MAP has to find complete assignments to the target variables. MAP has no ability
to indicate which parts of the explanations are most important, which we believe is a
fundamental drawback of MAP. Users are now burdened by the task of identifying the most
important parts of the explanations.
The K-SIMP method first finds the top K MAP solutions and then simplifies them
by deleting variables that do not reduce the likelihood of evidence much, if any. The set
of top solutions found by K-SIMP is the same as that of K-MRE. The results indicate
that the simplification method helped to prune less relevant target variables for the Circuit
network. However, K-SIMPs ranking of the explanations is different. (B, D) is its best
explanation. The number of variables of an explanation was also considered as a ranking
criterion for the explanations (de Campos et al., 2001), but then (A) becomes the best
explanation.
The ET method selects node A as the root of the explanation tree. A is important
because whether A is closed or not significantly affects the likelihood of the evidence. However, the ET method selects variable D as the second most important variable, which does
not lead to any good explanation. Moreover, there is no easy way to extract the top explanations from the explanation tree. The ET method relies on probability in ranking the
explanations. It seems that we should not consider partial paths as the solutions. For
example, (A) has a higher probability than (A) but is clearly not a good explanation. If
we only consider the full paths from the root to the leaves, (A) is the best explanation.
332

fiMost Relevant Explanation in Bayesian Networks

A

A
ok
0.609

def

D

0.391

def
0.262

ok
-0.692

def

C

4.610
ok

def

0.347

1.391

(a)

ok
-1.913

(b)

Figure 5: (a) Explanation tree and (b) causal explanation tree for the Circuit network
given the observation of electric current.

Although (A, D) has a probability that is only slightly smaller than (A), it is not a good
explanation at all. Moreover, using probability to rank the explanations inevitably makes
the threshold value for bounding the probabilities of the branches have a significant effect
on the ranking, which we believe is a fundamental flaw of the ET method.
The CET method also selects node A as the root of its explanation tree. However, it
selects C as the second most important variable, which does not lead to any good explanation either. Since the branches of a causal explanation tree are marked by the log ratio
of the posterior and prior probabilities of the evidence, it makes sense to consider all the
partial branches as explanations. The top two explanations according to the CET method
are (A) and (A, C), but (A, C) is clearly not a good explanation.
Both explanation tree methods failed to find either (B, C) or (B, D) as a top
explanation. The main reason is that they are greedy search methods. They may be good
at identifying individual variables that are important, but they often fail to identify the
compound effect of multiple variables. Even though variable B itself may not have as large
an effect as A, it forms excellent explanations together with C or D. The explanation tree
methods both failed to find these variable pairs as they only consider one variable at a time.
5.2 Vacation
Two versions of the vacation network were introduced in Section 3. One version models all
the possible hiking trips as one state named hiking, while the other version models the
100 hiking trips as separate but identical states. For both networks, we first consider the
scenario in which Mr. Smith is alive after his vacation. Table 4 shows the top explanations
by K-MRE, K-MAP, and K-SIMP, and Figure 6 shows the explanation trees by the ET and
CET methods.
333

fiYuan, Lim, & Lu

Vacation
K-MRE
K-MAP

K-SIMP

One-state model
Explanations
Scores
(healthy)
1.3378
(home)
1.0078
(healthy, hiking) 0.6336
(healthy, home)
0.1584
(healthy, home) 0.1440
(healthy)
0.9900
(home)
0.9450

Multi-state model
Explanations
Scores
(healthy)
1.3378
(any trip)
1.0034
(healthy, home)
0.1440
(healthy, home)
0.0792
(healthy, any trip) 0.0071
(healthy)
0.9900
(home)
0.9300

Table 4: Top explanations found by K-MRE, K-MAP, and K-SIMP for the two Vacation
networks given that Mr. Smith is alive after his vacation.

K-MRE only found two top explanations for each model because the other explanations
all have GBFs less than or equal to 1.0. (healthy) is the best explanation for both models.
In fact, the explanation has the same score in both models. Vacation location is treated by
K-MRE as irrelevant because the probability of staying alive is the same regardless of where
Mr. Smith decides to spend the vacation. Although the second best explanation changed
from (home) to (any tip), we note that both explanations have GBF scores very close to 1.0
and are barely worth mentioning according to Table 1. Actually the best explanations also
have GBFs only slightly over 1.0 and are not interesting explanations either. The reason
is that Mr. Smiths being alive after his vacation is not a surprising event; there is no real
need for explaining the observation.
K-MAP is extremely sensitive to the modeling choice. Not only did the best explanation change from (healthy, hiking) to (healthy, home), but also the highest probability
decreased significantly from 0.6336 to 0.1440. (healthy, home) is ranked third in the onestate model, but became the best explanation in the multi-state model.
Although the K-SIMP method started by simplifying the top three explanations by
K-MAP, it ended up with only two explanations for both models. It is because the simplification method resulted in duplicate solutions. Otherwise, the simplification method is
quite robust in the face of the modeling choice; (healthy) is the best explanation in both
models.
The ET method produced similar trees for both models. However, it selects Vacation
location as the most important variable. This is counterintuitive because Vacation location
does not even affect the probability of Mr. Smith being alive when he is healthy. This result
indicates that the mutual information between the target variables is not a good indicator
of their relevance in explaining the evidence. Also, it is again unclear which explanations
we should extract from the tree. If we rely on probability in selecting full branches, we will
select (hiking) for the one-state model and (healthy, home) for the multi-state model.
Neither of them is a good explanation for Mr. Smiths being alive after his vacation.
The causal explanation tree method is also robust in the face of the modeling choice.
It selects Healthy as the most important variable. It also finds (healthy) as the best
explanation in both models.
334

fiMost Relevant Explanation in Bayesian Networks

Healthy
Vacation location

no
-0.345

home
hiking
0.322

0.075

Vacation location
0.678

Healthy
no

home hiking

yes

-0.063

0.169

0.153

yes

(a) ET for the one-state model

-3.233

(b) CET for the one-state model
Healthy

Vacation location

no
-0.345

home
any trip
0.237

no
0.153

Vacation location

0.008

Healthy

0.075

home any trip

yes

-0.063

0.084

(c) ET for the multi-state model

yes

-3.233

(d) CET for the multi-state model

Figure 6: Explanation trees (ET) and causal explanation trees (CET) for the Vacation
networks give that Mr. Smith is alive after his vacation.

Next we consider the scenario in which Mr. Smith died after his vacation. Table 5 and
Figure 7 show the explanations found by the various methods for this observation.
MRE finds (healthy, hiking) as the best explanation for the one-state model and
(healthy) for the multi-state model. It is a rather intuitive result. For the one-state
model, hiking is a likely event and significantly increases the chance of Mr. Smiths death
if he is unhealthy. For the multi-state model, however, the hiking trails are modeled as
individual states, each of which has rather low prior and posterior probabilities. As a result, MRE considered the individual hiking trips as unimportant details and excluded them
from the best explanation. Some may argue that each individual hiking trip has the same
effect as the hiking state in the one-state model. However, that reasoning ignores the prior
probabilities of the explanations and essentially supports the use of the likelihood measure
335

fiYuan, Lim, & Lu

Vacation
K-MRE
K-MAP

K-SIMP

One-state model
Explanations
Scores
(healthy, hiking)
36.00
(healthy, hiking)
(healthy, home)
(healthy, hiking)
(healthy, hiking)
(healthy)
(hiking)

0.0360
0.0160
0.0064
0.9000
0.2600
0.0624

Multi-state model
Explanations
Scores
(healthy)
26.0000
(home)
1.2310
(healthy, home)
0.0160
(healthy, home)
0.0008
(healthy, any trip)
0.0004
(healthy, any trip)
0.9000
(healthy)
0.2600
(home)
0.0700

Table 5: Top explanations found by K-MRE, K-MAP, and K-SIMP for the two Vacation
networks given that Mr. Smith died after his vacation.

to select explanations. As we already discussed earlier, the likelihood measure has some
significant drawbacks.
K-MAP is again shown to be sensitive to the modeling choice. The best explanation
changed from (healthy, hiking) for the one-state model to (healthy, home) for the multistate model. (healthy, home) is not a good explanation because staying home reduces the
likelihood of Mr. Smiths death after his vacation.
The K-SIMP method selects (healthy, hiking) for the one-state model and (healthy,
any trip) for the multi-state model. Therefore, there are 100 best explanations with exactly
the same score according to this method.
The ET method creates simple trees with only a root for both models. However, the
variable chosen by this method is again counterintuitive. Vacation location is not the most
important variable for explaining Mr. Smiths death.
The CET method made a good choice in selecting Healthy as the most important variable. Similar to K-SIMP, CET also included the detailed vacation locations as part of its
explanations, so it has 100 best explanations with the same score.
5.3 Academe
Figure 8 shows the Academe network introduced by Flores et al. (2005) to discuss the
explanation tree method. The prior probability distributions of the four target variables
Theory, Practice, Extra, and OtherFactors are parameterized as follows.
P (T heory) < good, average, bad > = < 0.4, 0.3, 0.3 >;
P (P ractice) < good, average, bad > = < 0.6, 0.25, 0.15 >;
P (Extra) < yes, no > = < 0.3, 0.7 >;
P (OtherF actors) < plus, minus > = < 0.8, 0.2 >;
The conditional probabilities of MarkTP, GlobalMark, and FinalMark given their parents are parameterized as follows.
P (M arkT P = pass|T heory = bad  P ractice = bad) = 0.0;
336

fiMost Relevant Explanation in Bayesian Networks

Healthy
no
2.115

home hiking

home hiking
0.293

0.737

0.707

(a) ET for the one-state model

-2.585

Vacation location

Vacation location

yes

3.907

(b) CET for the one-state model
Healthy
no
2.115
Vacation location

Vacation location

0.737

0.007

(c) ET for the multi-state model

-2.585

home any trip

home any trip
0.280

yes

3.907

(d) CET for the multi-state model

Figure 7: Explanation trees (ET) and causal explanation trees (CET) for the Vacation
networks given that Mr. Smith died after his vacation.

P (M arkT P = pass|T heory = good, P ractice = good) = 1.0;
P (M arkT P = pass|T heory = good, P ractice = average) = 0.85;
P (M arkT P = pass|T heory = average, P ractice = good) = 0.9;
P (M arkT P = pass|T heory = average, P ractice = average) = 0.2;
P (GlobalM ark = pass|M arkT P = pass, Extra = ) = 1.0;
P (GlobalM ark = pass|M arkT P = f ail, Extra = yes) = 0.25;
P (GlobalM ark = pass|M arkT P = f ail, Extra = no) = 0.0;
P (F inalM ark = pass|GlobalM ark = pass, OtherF actors = plus) = 1.0;
P (F inalM ark = pass|GlobalM ark = pass, OtherF actors = minus) = 0.7;
337

fiYuan, Lim, & Lu

Practice

Theory

Extra

MarkTP

OtherFactors

GlobalMark

FinalMark

Figure 8: The Academe network.
Academe
K-MRE

K-MAP

K-SIMP

Explanations
(bad theory)
(bad practice, no extra)
(good theory, bad practice, minus otherF actors)
(bad theory, good practice, no extra, plus otherF actors)
(bad theory, average practice, no extra, plus otherF actors)
(average theory, bad practice, no extra, plus otherF actors)
(bad theory, no extra)
(average theory, bad practice)

Scores
3.0205
2.2986
2.0209
0.0958
0.0399
0.0399
0.9600
0.7260

Table 6: Top explanations found by K-MRE, K-MAP, and K-SIMP for the Academe network given that the FinalMark is fail.

P (F inalM ark = pass|GlobalM ark = f ail, OtherF actors = plus) = 0.05;
P (F inalM ark = pass|GlobalM ark = f ail, OtherF actors = minus) = 0.0.
We consider the problem of finding explanations for the observation that FinalMark is
fail using the target variable set {T heory, P ractice, Extra, OtherF actors}. Table 6 and
Figure 9 show the explanations found by the various methods for this observation.
K-MRE selects (bad theory) as the best explanation for the failing final grade. There
are other good explanations that contain bad theory as a part, but (bad theory) dominates
all those explanations. (bad practice) itself is a good explanation, but the combination of
bad practice and no extra preparation turns out to be a better explanation.
The top three explanations found by K-MAP all have probabilities smaller than 0.1. In
general, it is highly domain-dependent as to which probabilities mean good explanations
and which mean bad explanations. In comparison, GBFs and the likelihood of evidence
are more consistent across different domains. Note that we are not claiming that the top
explanations ranked by probability are necessarily bad; we just believe that probability may
not be a good measure for the explanatory power of an explanation.
The K-SIMP method selects (bad theory, no extra) as the best explanation. This explanation is a good explanation itself. However, it is not really the explanation with the highest
338

fiMost Relevant Explanation in Bayesian Networks

Theory
good
-1.135
Practice
average
-1.360

good

average
-0.242
Practice
good
average
-1.728

bad

bad

0.911

bad

Practice
OtherFactors

-2.984

good
0.423
Theory

average
0.295

plus
-1.848
Extra

0.030

0.054

good average

0.339

0.039

0.613

0.911

plus
minus
-2.433

minus

0.282

Theory

good average bad

OtherFactors

0.911

bad

0.115

bad

yes

0.141

-2.151

(a)

Extra

-0.257
no
-1.736

-0.380

yes no
-2.736

-2.321

(b)

Figure 9: (a) Explanation tree and (b) causal explanation tree for the Academe network
when the FinalMark is fail.

likelihood. For example, the likelihood of a failing grade given (bad theory, bad practice, no
extra, minus otherF actors) is equal to 1.0, but K-SIMP is limited to the top solutions
found by K-MAP. Also, the method again ended up with only two explanations, because
the top two MAP solutions are simplified to the same explanation.
The ET method incorrectly selects Practice as the most important variable; Theory has
a higher impact on the final mark according to this model. Again, it is because the ET
method measures the importance of a target variable using its mutual information with
other target variables, not with the evidence variable. Recall that it made bad choices for
the Academe and Vacation networks as well.
The CET method made a more sensible choice by selecting Theory as the most important
variable. There are three equally good explanations according to this method: (bad theory),
(average theory, bad practice), and (good theory, bad practice). The reason that this
method cannot distinguish these explanations is that it marks the branches using the log
ratio between the posterior and prior probabilities of the evidence, which is proportional to
the likelihood measure.
5.4 Asia
The Asia network is first introduced by Lauritzen and Spiegelhalter (1988) and was used
by Nielsen et al. (2008) to discuss the CET method. The probabilities of this network are
parameterized as follows.
P (V isitT oAsia = yes) = 0.01;
P (Smoking = yes) = 0.5;
P (T uberculosis = yes|V isitT oAsia = yes) = 0.05;
339

fiYuan, Lim, & Lu

VisitToAsia

Tuberculosis

Lung_Cancer

Smoking

Dyspnea

Bronchitis

TborCa

X_Ray

Figure 10: The Asia network.
P (T uberculosis = yes|V isitT oAsia = no) = 0.01;
P (LungCancer = yes|Smoking = yes) = 0.1;
P (LungCancer = yes|Smoking = no) = 0.01;
P (Bronchitis = yes|Smoking = yes) = 0.6;
P (Bronchitis = yes|Smoking = no) = 0.3;
P (T borCa = yes|T uberculois = yes  LungCancer = yes) = 1.0;
P (T borCa = yes|T uberculois = no, LungCancer = no) = 0.0;
P (Dyspnea = yes|T borCa = yes, Bronchitis = yes) = 0.9;
P (Dyspnea = yes|T borCa = yes, Bronchitis = no) = 0.7;
P (Dyspnea = yes|T borCa = no, Bronchitis = yes) = 0.8;
P (Dyspnea = yes|T borCa = no, Bronchitis = no) = 0.1;
P (X ray = abnormal|T borCa = yes) = 0.98;
P (X ray = abnormal|T borCa = no) = 0.05;
We consider two different observations in the Asia network: Dyspnea is present, or X-ray
is abnormal. Table 7 and Figure 11 show the explanations found by the various methods for
these two observations using the target variable set {Bronchitis, LungCancer, T uberculosis}.
It is interesting that K-MRE obtained quite intuitive and concise top explanations for both
observations: (Bronchitis), (LungCancer), or (T uberculosis). Here we use a disease name
such as Bronchitis to denote the presence of the disease and a negation such as Bronchitis
to denote the absence of the disease. The ranking of the explanations, however, is different. For the first observation, (Bronchitis) is a better explanation for Dyspnea than both
(T uberculosis) and (LungCancer) because the conditional probabilities show that the presence of Bronchitis has a larger effect on Dyspnea. For the second observation, Tuberculosis
and LungCancer are the ancestors of X-ray, while Bronchitis has no direct effect on X-ray
and only receives a small GBF. The reason that (Bronchitis)s GBF is still greater than 1.0
is that Bronchitis increases the likelihood of Smoking, which in turn increases the likelihood
of abnormal X-ray.
K-MAP identified Bronchitis as part of the best explanation for the first observation,
but it included both Bronchitis and LungCancer as part of the best explanation for the
second observation, even though Bronchitis is not a direct contributor to the abnormal
340

fiMost Relevant Explanation in Bayesian Networks

Asia
K-MRE

K-MAP

K-SIMP

K-MRE

K-MAP

K-SIMP

Explanations

Scores

Dyspnea present
(Bronchitis)
(LungCancer)
(T uberculosis)
(LungCancer, T uberculosis, Bronchitis)
(LungCancer, T uberculosis, Bronchitis)
(LungCancer, T uberculosis, Bronchitis)
(LungCancer, Bronchitis)
(Bronchitis)
(T uberculosis)
Abnormal X-ray
(LungCancer)
(T uberculosis)
(Bronchitis)
(LungCancer, T uberculosis, Bronchitis)
(LungCancer, T uberculosis, Bronchitis)
(LungCancer, T uberculosis, Bronchitis)
(LungCancer)
(T uberculosis)

6.1391
1.9678
1.8276
0.3313
0.0521
0.0521
0.9000
0.8080
0.4323
16.4231
9.6886
1.2535
0.0305
0.0261
0.0228
0.9800
0.1012

Table 7: Top explanations found by K-MRE, K-MAP, and K-SIMP for the Asia network
given two different observations.

X-ray. The second best explanation for the first observation claims none of the diseases is
present, which is clearly not a good explanation.
The K-SIMP method found quite perplexing explanations for the observations. The best
explanation is (LungCancer, Bronchitis) for the first observation and (LungCancer)
for the second observation. The reason that K-SIMP did not find good explanations for
this network is that K-SIMP is restricted to the solutions found by K-MAP. The method
was still able to find (Bronchitis) as the second best explanation for the first observation,
but it missed altogether for the second observation.
The ET method was able to find the most important variables in explaining both observations, but it fell short in recognizing the importance of Tuberculosis in explaining the
second observation. It only expanded LungCancer in explaining the first observation.
The CET method was able to find intuitive explanations. The best explanations are
(Bronchitis) for Dyspnea and (LungCancer) for abnormal X-ray. However, one drawback
of its explanation tree is that the tree structure requires each explanation to start from the
root. For example, one of the explanations for explaining Dyspnea is (Bronchitis, LungCancer, T uberculosis). It is arguable that (T uberculosis) itself is a good explanation for
explaining Dyspnea; it is not really necessary to include Bronchitis and LungCancer
as part of the explanation. We believe that this is another common drawback of the two
explanation tree methods caused because of the tree representation that they use.
341

fiYuan, Lim, & Lu

Bronchitis
no
-1.650

yes

Bronchitis

0.887

LungCancer

no
0.166

yes

LungCancer

0.834

yes

no
-2.037

yes

0.683

Tuberculosis

no

yes

0.128

0.038

0.683

(a) ET for Dyspnea

no
-2.124

(b) CET for Dyspnea
LungCancer

yes

LungCancer
yes
0.489

3.151

no
-0.886
Tuberculosis

no

yes

0.511

(c) ET for abnormal X-ray

3.151

no
-1.141

(d) CET for abnormal X-ray

Figure 11: Explanation trees (ET) and causal explanation trees (CET) found for the Asia
network given two different observations.

5.5 Circuit2
Model-based diagnosis is an application of abductive inference in Horn-clause logic theories (Peirce, 1948; de Kleer & Williams, 1987), which tries to find a minimal set of assumptions that, together with background knowledge, logically entail the observations that need
explanation. However, methods for model-based diagnosis are developed based on logic
theories. Entailment is either true or false for logic systems. These methods cannot be
easily generalized to probabilistic expert systems such as Bayesian networks. In contrast,
342

fiMost Relevant Explanation in Bayesian Networks

A

B

1

2

C

A

B

OK_1

OK_2

OK_3

D
C

D

3
E

E

(a)

(b)

Figure 12: (a) A digital circuit from Darwiche (2009) and (b) its corresponding Bayesian
network.

Circuit in Figure 12
K-MRE
K-MAP

K-SIMP

Explanations
(OK3 )
(OK1 , OK2 )
(OK1 , OK2 , OK3 )
(OK1 , OK2 , OK3 )
(OK1 , OK2 , OK3 )
(OK3 )
(OK1 , OK2 )

Scores
4.0000
2.0000
0.1250
0.1250
0.1250
1.0000
1.0000

Table 8: Top explanations found by K-MRE, K-MAP, and K-SIMP for the Circuit2 network
given that the output is observed to be low.

MRE can be easily applied to model-based diagnostic systems in which the faulty behaviors
of the components are also specified.
We consider the digital circuit in Figure 12(a) used by Darwiche (2009) to discuss
methods for model-based diagnosis. Gates 1 and 2 are inverters, and gate 3 is an OR
gate. The prior probability that each of the gates is abnormal is 0.5. When an inverter
is abnormal, it outputs low when the input is low, and outputs high with probability 0.5
when the input is high. When an OR gate is abnormal, it always outputs low. This digital
circuit can be modeled as the Bayesian network in Figure 12(b). We consider the case when
output E is observed to be low. The two kernel model-based diagnoses are (OK1 , OK2 )
and (OK3 ). Table 8 and Figure 13 show the explanations found by the various methods
for the observation using the target variable set {OK1 , OK2 , OK3 }.
K-MRE was able to find the two kernel diagnoses as its top two explanations, with
(OK3 ) receives a higher GBF than (OK1 , OK2 ). The higher GBF score is due to
(OK3 )s higher prior and posterior probabilities than (OK1 , OK2 ). In comparison,
methods for model-based diagnosis typically treat the two explanations as equally good.
343

fiYuan, Lim, & Lu

OK_2
no
yes
0.600
OK_3

0.400

no
yes
0.400
OK_1
no
0.200

OK_2

0.200

no

yes

-0.322

0.263

0.200

(a)

yes

(b)

Figure 13: (a) Explanation tree and (b) causal explanation tree for the Circuit2 network
given that the output is observed to be low.

K-MAP was not able to single out the two kernel diagnoses. In fact, many MAP solutions
have the same posterior probability, including the explanation in which all the gates are
defective.
The K-SIMP method found the same two explanations as K-MRE. Therefore, the simplification method helped in simplifying the explanations in this network. However, K-SIMP
was not able to rank the two explanations either.
The two explanation tree methods completely misfired on this network. It is unclear
how to make sense of the explanation tree by the ET method in Figure 13(a). The causal
explanation tree in Figure 13(b) only expanded variable OK 2 and failed to find any of
the kernel diagnoses. Again, it is because the explanation tree methods are greedy search
methods and cannot recognize the compound effect of multiple variables.
5.6 Summary of the Case Studies
The case studies show that K-MRE was able to identify the most relevant target variables
and find more concise and intuitive explanations than the other methods. GBF seems to be
a plausible measure of explanatory power that can achieve both preciseness and conciseness
in explanations at the same time. Another advantage of GBF is that we can use Table 1 as
a general guidance for determining the significance of the explanations found by K-MRE.
In contrast, probability seems not to be a good measure of explanatory power. Methods
344

fiMost Relevant Explanation in Bayesian Networks

based on probability, such as K-MAP, are quite sensitive to modeling choices; they also
lack the capability to indicate the most important parts of their explanations. The K-MAP
simplification method was shown often to be able to simplify the solutions of K-MAP to
get more concise explanations. But its fundamental drawback is that it is restricted to
the solutions found by K-MAP and may not be able to find the best explanations. Also,
it may reduce multiple top MAP solutions to the same explanation. The ET method
uses the mutual information between the target variables as the criterion to select target
variables to explain the evidence. The criterion was shown not to be very effective. Also,
using probability to rank the explanations makes the ET method very sensitive to the
user-specified threshold value for bounding the probabilities of the branches. The CET
method is good at identifying individual target variables that are important, but it often
fails to recognize the significant compound effect of multiple variables. It is because the
CET method, as well as the ET method, is based on a greedy search that only considers
one variable at a time. Another common drawback of the explanation tree methods is that
the tree representation fundamentally limits the capability of these methods to find concise
explanations.

6. Concluding Remarks
In this paper, we introduced the Most Relevant Explanation (MRE) method for finding
explanations for given evidence in Bayesian networks. Our study shows that MRE has
several desirable theoretical properties that enable MRE to automatically identify the most
relevant target variables to find an explanation for the evidence. MRE is also able to
capture the unique explaining-away phenomenon often represented in Bayesian networks.
We defined two dominance relations among the MRE solutions and used them to develop a
K-MRE method to find a set of top MRE solutions that is both diverse and representative.
The results of the case studies on a set of benchmark Bayesian networks agree quite well
with our theoretical understandings of MRE. Another contribution of this research is that
it also made clear the properties and drawbacks of several existing relevance measures and
explanation methods.

Acknowledgments
This research was supported by the National Science Foundation grants IIS-0842480, IIS0953723, and EPS-0903787. Part of this research has previously been presented in DX07 (Yuan & Lu, 2007), AAAI-08 (Yuan & Lu, 2008), UAI-09 (Yuan, Liu, Lu, & Lim, 2009),
and ExaCt-09 (Yuan, 2009). We thank the editors and anonymous reviewers for their
constructive comments.

Appendix A. Proofs
The following are the proofs of the theorems and corollaries.
345

fiYuan, Lim, & Lu

A.1 Proof of Theorem 1
Proof: The likelihood measure can be expressed as
P (e|x) =

P (x|e)P (e)
= r(x; e)P (e).
P (x)

Therefore, a fixed likelihood P (e|x) indicates that the belief update ratio r(x; e) remains
constant while the prior and posterior probabilities may vary. Furthermore, GBF can be
expressed as follows.
GBF (x; e) = 1 +

r(x; e)  1
.
1  r(x; e)P (x)

Therefore, GBF is monotonically non-decreasing for a fixed belief update ratio r(x; e)
greater than or equal to 1.0 as the prior and posterior probabilities increase.
2
A.2 Proof of Theorem 2
Proof:
GBF (x, y; e) =
=
=

P (x, y|e)(1  P (x, y))
P (x, y)(1  P (x, y|e))
P (x|e)P (y|x, e)(1  P (y|x)P (x))
P (x)P (y|x)(1  P (y|x, e)P (x|e))
1
P (x|e) 1  P (x) + P (y|x)  1
P (x) 1  P (x|e) +

1
P (y|x,e)

1

The above equation is less than or equal to GBF (x; e) when



1
P (y|x)  1
1
P (y|x,e)  1



P (y|x, e)(1  P (y|x))
P (y|x)(1  P (y|x, e))



 CBF (y; e|x) 

1  P (x)
1  P (x|e)
P (x)
P (x|e)
1
.
r(x; e)
2

A.3 Proof of Corollary 1
Proof: The corollary follows from Theorem 2. Here we present another way to prove it.
GBF (x, y; e) =
=
=

P (x, y|e)(1  P (x, y))
P (x, y)(1  P (x, y|e))
P (x|e)P (y)(1  P (y)P (x))
P (x)P (y)(1  P (y)P (x|e))
P (x|e)(1  P (y)P (x))
.
P (x)(1  P (y)P (x|e))
346

fiMost Relevant Explanation in Bayesian Networks

Because P (x|e) > P (x), we have
GBF (x, y; e) =
=
<

=

P (x|e)(1  P (y)P (x))
P (x)(1  P (y)P (x|e))
P (x|e)(1  P (x) + (1  P (y))P (x))
P (x)(1  P (x|e) + (1  P (y))P (x|e))
P (x|e)(1  P (x) + (1  P (y))P (x))
P (x)(1  P (x|e) + (1  P (y))P (x))
P (x|e)(1  P (x))
P (x)(1  P (x|e))
GBF (x; e) .
2

A.4 Proof of Corollary 2
Proof: This corollary can be proved in a similar way as in Corollary 1.
GBF (x, y; e) =
=
=
=

P (x, y|e)(1  P (x, y))
P (x, y)(1  P (x, y|e))
P (x|e)P (y|x, e)(1  P (y|x)P (x))
P (x)P (y|x)(1  P (y|x, e)P (x|e))
P (x|e)P (y|x)(1  P (y|x)P (x))
P (x)P (y|x)(1  P (y|x)P (x|e))
P (x|e)(1  P (y|x)P (x))
.
P (x)(1  P (y|x)P (x|e))

Because P (x|e) > P (x), we have
GBF (x, y; e) =
=
<

=

P (x|e)(1  P (y|x)P (x))
P (x)(1  P (y|x)P (x|e))
P (x|e)(1  P (x) + (1  p(y|x))P (x))
P (x)(1  P (x|e) + (1  p(y|x))P (x|e))
P (x|e)(1  P (x) + (1  p(y|x))P (x))
P (x)(1  P (x|e) + (1  p(y|x))P (x))
P (x|e)(1  P (x))
P (x)(1  P (x|e))
GBF (x; e) .
2

A.5 Proof of Corollary 3
Proof:
GBF (x, y; e) =

P (x|e)P (y|x, e)(1  P (y|x)P (x))
P (x, y|e)(1  P (x, y))
=
.
P (x, y)(1  P (x, y|e))
P (x)P (y|x)(1  P (y|x, e)P (x|e))
347

fiYuan, Lim, & Lu

Because P (y|x, e)  P (y|x),
GBF (x, y; e) 

P (x|e)(1  P (y|x)P (x))
P (x|e)P (y|x)(1  P (y|x)P (x))
=
.
P (x)P (y|x)(1  P (y|x)P (x|e))
P (x)(1  P (y|x)P (x|e))

Because P (x|e) > P (x), we have
GBF (x, y; e) =
=
<

=

P (x|e)(1  P (y|x)P (x))
P (x)(1  P (y|x)P (x|e))
P (x|e)(1  P (x) + (1  p(y|x))P (x))
P (x)(1  P (x|e) + (1  p(y|x))P (x|e))
P (x|e)(1  P (x) + (1  p(y|x))P (x))
P (x)(1  P (x|e) + (1  p(y|x))P (x))
P (x|e)(1  P (x))
P (x)(1  P (x|e))
GBF (x; e) .
2

A.6 Proof of Theorem 3
Proof:

When C is not observed, we have the following equality.
P (B|A, x, y) = P (B|x, y) = P (B|A, x, y) = P (B|y).

(19)

Therefore, we have
P (B|A, c, x, y)  P (B|c, x, y)  P (B|A, c, x, y)


1  P (B|A, c, x, y)  1  P (B|c, x, y)  1  P (B|A, c, x, y)
P (B|A,c,x,y)
1P (B|A,c,x,y)




P (B|A,c,x,y) 1P (B|A,x,y)
1P (B|A,c,x,y) P (B|A,x,y)







P (B|c,x,y)
1P (B|c,x,y)



P (B|A,c,x,y)
1P (B|A,c,x,y)

P (B|c,x,y) 1P (B|x,y)
1P (B|c,x,y) P (B|x,y)



P (B|A,c,x,y) 1P (B|A,x,y)
1P (B|A,c,x,y) P (B|A,x,y)

GBF (B; c|A, x, y)  GBF (B; c|x, y)  GBF (B; c|A, x, y)

.
2

References
AR Group, UCLA (2010). Samiam: Sensitivity analysis, modeling, inference and more..
http://reasoning.cs.ucla.edu/samiam/index.php.
Ay, N., & Polani, D. (2008). Information flows in causal networks. Advances in Complex
Systems (ACS), 11 (01), 1741.
Bleich, H. L. (1972). Computer-based consultation: Electrolyte and acid-base disorders.
The American Journal of Medicine, 53 (3), 285  291.
348

fiMost Relevant Explanation in Bayesian Networks

Buchanan, B., & Shortliffe, E. (Eds.). (1984). Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project. Addison-Wesley, Reading,
MA.
Carnap, R. (1948). Logical Foundations of Probability. 2nd ed. University of Chicago Press,
Chicago, IL.
Chajewska, U., & Halpern, J. Y. (1997). Defining explanation in probabilistic systems. In
Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI97), pp. 6271, San Francisco, CA. Morgan Kaufmann Publishers.
Dannenberg, A., Shapiro, A., & Fries, J. (1979). Enhancement of clinical predictive ability
by computer consultation. Methods Inf Med, 18 (1), 1014.
Darwiche, P. A. (2009). Modeling and Reasoning with Bayesian Networks (1st edition).
Cambridge University Press, New York, NY, USA.
de Campos, L. M., Gamez, J. A., & Moral, S. (2001). Simplifying explanations in Bayesian
belief networks. International Journal of Uncertainty, Fuzziness Knowledge-Based
Systems, 9 (4), 461489.
de Kleer, J., & Williams, B. C. (1987). Diagnosing multiple faults. Artificial Intelligence,
32 (1), 97130.
Druzdzel, M. J. (1996). Explanation in probabilistic systems: Is it feasible? will it work?. In
Proceedings of the Fifth International Workshop on Intelligent Information Systems
(WIS-96), pp. 1224, Deblin, Poland.
Druzdzel, M. J. (1999). GeNIe: A development environment for graphical decision-analytic
models. In Proceedings of the 1999 Annual Symposium of the American Medical
Informatics Association (AMIA1999), p. 1206, Washington, D.C.
Fitelson, B. (2001). Studies in Bayesian Confirmation Theory. Ph.D. thesis, University of
Wisconsin, Madison, Philosophy Department.
Flores, J., Gamez, J. A., & Moral, S. (2005). Abductive inference in Bayesian networks:
finding a partition of the explanation space. In Eighth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, ECSQARU05, pp.
6375. Springer Verlag.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States.
MIT Press.
Good, I. J. (1950). Probability and the Weighing of Evidence. Griffin, London.
Good, I. J. (1977). Explicativity: A mathematical theory of explanation with statistical
applications. In Proceedings of Explicativity: A Mathematical Theory of Explanation
with Statistical Applications, Series A. Vol. 354, No. 1678, pp. 303330.
Good, I. J. (1985). Weight of evidence: A brief survey. Bayesian Statistics, 2, 249270.
349

fiYuan, Lim, & Lu

Halpern, J. Y., & Pearl, J. (2005). Causes and Explanations: A Structural-Model Approach.
Part II: Explanations. The British Journal for the Philosophy of Science, 56 (4), 889
911.
Heckerman, D., Breese, J., & Rommelse, K. (1995a). Decision-theoretic troubleshooting.
Communications of the ACM, 38, 4957.
Heckerman, D., Mamdani, E. H., & Wellman, M. P. (1995b). Real-world applications of
Bayesian networks - introduction. Commun. ACM, 38 (3), 2426.
Hempel, C. G. (1965). Aspects of Scientific Explanation and other Essays in the Philosophy
of Science. Free Press, New York, NY.
Hempel, C. G., & Oppenheim, P. (1948). Studies in the logic of explanation. Bobbs-Merrill,
Indianapolis, IN.
Henrion, M., & Druzdzel, M. J. (1991). Qualitative propagation and scenario-based schemes
for explaining probabilistic reasoning. In Bonissone, P., Henrion, M., Kanal, L., &
Lemmer, J. (Eds.), Uncertainty in Artificial Intelligence 6, pp. 1732. Elsevier Science
Publishing Company, Inc., New York, N. Y.
Jeffreys, H. (1935). Contribution to the discussion of Fisher. Journal of the Royal Statistical
Society, 98, 7072.
Jeffreys, H. (1961). Theory of Probability. Oxford University Press.
Jensen, F. V., & Liang, J. (1994). drHugin: A system for value of information in Bayesian
networks. In Proceedings of the 1994 Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, pp. 178183.
Kalagnanam, J., & Henrion, M. (1988). A comparison of decision analysis and expert rules
for sequential diagnosis. In Proceedings of the 4th Annual Conference on Uncertainty
in Artificial Intelligence (UAI-88), pp. 253270, New York, NY. Elsevier Science.
Kass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical
Association, pp. 773795.
Khan, O. Z., Poupart, P., & Black, J. P. (2009). Minimal sufficient explanations for factored
markov decision processes. In Proceedings of the Nineteenth International Conference
on Automated Planning and Scheduling, pp. 194200.
Kitcher, P., & Salmon, W. (1989). Explanatory unification and the causal structure of the
world, pp. 410505. University of Minnesota Press, Minneapolis, MN.
Kullback, S., & Leibler, R. (1951). On information and sufficiency. Annals of Mathematical
Statistics, 22(1), 7986.
Lacave, C., & Diez, F. (2002). A review of explanation methods for Bayesian networks. The
Knowledge Engineering Review, 17, 107127.
350

fiMost Relevant Explanation in Bayesian Networks

Lacave, C., Luque, M., & Diez, F. (2007). Explanation of Bayesian networks and influence
diagrams in elvira. IEEE Transactions on Systems, Man, and Cybernetics, Part B:
Cybernetics, 37 (4), 952  965.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations with probabilities on
graphical structures and their application to expert systems. Journal of the Royal
Statistical Society, Series B (Methodological), 50 (2), 157224.
Leake, D. B. (1995). Abduction, experience, and goals: A model of everyday abductive
explanation. The Journal of Experimental and Theoretical Artificial Intelligence, 7,
407428.
Lin, Y., & Druzdzel, M. J. (1998). Relevance-based sequential evidence processing in
Bayesian networks. In Proceedings of the Uncertain Reasoning in Artificial Intelligence
track of the Eleventh International Florida Artificial Intelligence Research Symposium
(FLAIRS98), pp. 446450, Menlo Park, CA. AAAI Press/The MIT Press.
Minsky, M., & Selfridge, O. (1961). Learning in random nets. Addison-Wesley, Butterworth,
London.
Nerlich, G. (1979). Time and the direction of conditionship. Australasian Journal of
Philosoph, 57 (1), 314.
Ng, H. T., & Mooney, R. J. (1990). On the role of coherence in abductive explanation. In
Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90),
pp. 337342, Boston, MA.
Nielsen, U., Pellet, J.-P., & Elisseeff, A. (2008). Explanation trees for causal Bayesian
networks. In Proceedings of the 24th Annual Conference on Uncertainty in Artificial
Intelligence (UAI-08), pp. 427434.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann Publishers, Inc., San Mateo, CA.
Peirce, C. (1948). The Philosophy of Peirce: Selected Writings, chap. Abduction and induction. Harcourt, Brace and Company, New York.
Poole, D., & Provan, G. M. (1991). What is the most likely diagnosis?. In Bonissone, P.,
Henrion, M., Kanal, L., & Lemmer, J. (Eds.), Uncertainty in Artificial Intelligence 6,
pp. 89105. Elsevier Science Publishing Company, Inc., New York, N. Y.
Rosenkrantz, R. D. (1994). Bayesian confirmation: Paradise regained. British Journal for
the Philosophy of Science, 45(2), 467476.
Salmon, W. (1970). Statistical explanation, pp. 2987. University of Pittsburgh Press,
Pittsburgh, PA.
Salmon, W. (1984). Explanation and the Causal Structure of the World. Princeton University Press, Princeton, NJ.
351

fiYuan, Lim, & Lu

Schwartz, G. (1979). Estimating the dimensions of a model. Ann. Stat., 6, 461464.
Shimony, S. (1993). The role of relevance in explanation I: Irrelevance as statistical independence. International Journal of Approximate Reasoning, 8 (4), 281324.
Shimony, S. (1996). The role of relevance in explanation II: Disjunctive assignments and
approximate independence. International Journal of Approximate Reasoning, 14 (1),
2554.
Smyth, B., & McClave, P. (2001). Similarity vs. diversity. In Proceedings of the International
Conference on Case-based Reasoning (ICCBR-01), pp. 347361.
Suermondt, H. (1992). Explanation in Bayesian Belief Networks. Ph.D. thesis, Stanford
University, Palo Alto, California.
Teach, R. L., & Shortliffe, E. H. (1981). An analysis of physician attitudes regarding
computer-based clinical consultation systems. Computers and biomedical research, an
international journal, 14 (6), 542558.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering for diagnostic belief
networks. AISB Quarterly, pp. 2334.
van der Gaag, L., & Wessels, M. (1995). Efficient multiple-disorder diagnosis by strategic
focusing, pp. 187204. UCL Press, London.
van Fraassen, B. (1980). The Scientific Image. Oxford University Press, Oxford.
Wellman, M., & Henrion, M. (1993). Explaining explaining away. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 15, 287291.
Woodward, J. (2003). Scientific explanation. In Zalta, E. N. (Ed.), The Stanford Encyclopedia of Philosophy (Winter 2003 Edition).
Yuan, C. (2009). Some properties of Most Relevant Explanation. In Proceedings of the 21st
International Joint Conference on Artificial Intelligence ExaCt Workshop (ExaCt-09),
pp. 118126, Pasadena, CA.
Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). Most Relevant Explanation: Properties,
algorithms, and evaluations. In Proceedings of 25th Conference on Uncertainty in
Artificial Intelligence (UAI-09), pp. 631638, Montreal, Canada.
Yuan, C., & Lu, T.-C. (2007). Finding explanations in Bayesian networks. In Proceedings
of the 18th International Workshop on Principles of Diagnosis (DX-07), pp. 414419.
Yuan, C., & Lu, T.-C. (2008). A general framework for generating multivariate explanations
in Bayesian networks. In Proceedings of the Twenty-Third National Conference on
Artificial Intelligence (AAAI-08), pp. 11191124.

352

fiJournal of Artificial Intelligence Research 42 (2011) 5590

Submitted 04/11; published 09/11

M APP: a Scalable Multi-Agent Path Planning Algorithm with
Tractability and Completeness Guarantees
Ko-Hsin Cindy Wang
Adi Botea

C INDY.WANG @ RSISE . ANU . EDU . AU
A DI .B OTEA @ NICTA . COM . AU

NICTA & The Australian National University,
Canberra, Australia

Abstract
Multi-agent path planning is a challenging problem with numerous real-life applications. Running a centralized search such as A* in the combined state space of all units is complete and
cost-optimal, but scales poorly, as the state space size is exponential in the number of mobile units.
Traditional decentralized approaches, such as FAR and W HCA *, are faster and more scalable, being
based on problem decomposition. However, such methods are incomplete and provide no guarantees with respect to the running time or the solution quality. They are not necessarily able to tell in
a reasonable time whether they would succeed in finding a solution to a given instance.
We introduce M APP, a tractable algorithm for multi-agent path planning on undirected graphs.
We present a basic version and several extensions. They have low-polynomial worst-case upper
bounds for the running time, the memory requirements, and the length of solutions. Even though
all algorithmic versions are incomplete in the general case, each provides formal guarantees on
problems it can solve. For each version, we discuss the algorithms completeness with respect to
clearly defined subclasses of instances.
Experiments were run on realistic game grid maps. M APP solved 99.86% of all mobile units,
which is 1822% better than the percentage of FAR and W HCA *. M APP marked 98.82% of all
units as provably solvable during the first stage of plan computation. Parts of M APPs computation
can be re-used across instances on the same map. Speed-wise, M APP is competitive or significantly
faster than W HCA *, depending on whether M APP performs all computations from scratch. When
data that M APP can re-use are preprocessed offline and readily available, M APP is slower than the
very fast FAR algorithm by a factor of 2.18 on average. M APPs solutions are on average 20%
longer than FARs solutions and 731% longer than W HCA *s solutions.

1. Introduction
Path planning is important in many real-life problems, including robotics, military operations, disaster rescue, logistics, and commercial games. Single-agent path planning, where the size of the state
space is bounded by the size of the map, can be tackled with a search algorithm such as A* (Hart,
Nilsson, & Raphael, 1968). However, when there are many units moving simultaneously inside a
shared space, the problem becomes much harder. A centralized search from an initial state to a goal
state is a difficult problem even inside a fully known, two-dimensional environment represented as
a weighted graph, where one node can be occupied by exactly one unit at a time. Assuming that
units have the same size, and each unit moves synchronously to an adjacent unoccupied node in one
time step, the problems state space grows exponentially in the number of mobile units. Existing
hardness results have shown that it is NP-complete to decide if a solution of at most k moves exists (Ratner & Warmuth, 1986), or to optimize the solution makespan (Surynek, 2010b). A version
of the problem with one robot only and movable obstacles at several nodes, where either the robot
c
2011
AI Access Foundation. All rights reserved.

fiWANG & B OTEA

or an obstacle can move to an adjacent vacant node per step, is also NP-complete (Papadimitriou,
Raghavan, Sudan, & Tamaki, 1994). Yet another version of the problem, determining if a solution exists for moving two-dimensional rectangles of different sizes inside a box, has been shown
to be PSPACE-hard, even without requiring optimality (Hopcroft, Schwartz, & Sharir, 1984). Despite its completeness and solution optimality guarantees, a centralized A* search has little practical
value in a multi-agent path planning problem, being intractable even for relatively small maps and
collections of mobile units.
Scalability to larger problems can be achieved with decentralized approaches, which decompose
the global search into a series of smaller searches to significantly reduce computation. However,
existing decentralized methods such as FAR (Wang & Botea, 2008) and W HCA * (Silver, 2006)
are incomplete, and provide no formal criteria to distinguish between problem instances that can be
successfully solved and other instances. Further, no guarantees are given with respect to the running
time and the quality of the computed solutions.
In this work we present an algorithm that combines the strengths of both worlds: working well
in practice and featuring theoretical tractability and partial completeness guarantees. We introduce
M APP, a tractable multi-agent path planning algorithm for undirected graphs. For each problem instance, M APP systematically identifies a set of units, which can contain all units in the instance, that
are guaranteed to be solved within low-polynomial time. For the sake of clarity we will distinguish
between a basic version and a few extended versions of M APP. M APP provides formal guarantees
for problems it can solve. The Basic M APP algorithm is complete for a class of problems, called
S LIDABLE, which we define in Section 3. Extended versions of the algorithm enlarge the completeness range, as discussed in Section 7, and improve solution length, as discussed in Section 8. We
will also evaluate a version that attempts to solve all units, not only the provably solvable ones.
Given a problem with m graph nodes and n mobile units, M APPs worst case performance for
the running time is O(m2 n2 ), or even smaller (e.g., O(max(mn2 , m2 log m))), depending on the
assumptions on the input instance. The worst-case memory requirements are within O(m2 n) or
even O(mn). An upper bound of the solution length, measured as the total number of moves, is in
the order of O(m2 n2 ) or even O(mn2 ). See Section 6 for a detailed discussion.
M APP keeps its running costs low by eliminating the need for replanning. A path (u) for
each unit u is computed at the beginning. No replanning is required at runtime. A blank travel
idea, inspired from the way the blank moves around in sliding tile puzzles, is at the center of the
u
algorithm. A unit u can progress from its current location liu to the next location li+1
on its path
u
(u) only if a blank is located there (i.e., li+1 is empty). Intuitively, if the next location is currently
occupied by another unit, M APP tries to bring a blank along an alternate path, outlined in bold in
u
u
and li+1
without passing through liu . When possible, the blank is
Figure 1, which connects li1
u
brought to li+1 by shifting units along the alternate path, just as the blank travels in a sliding tile
puzzle. The ability to bring a blank to the next location is key to guarantee a units progress. Formal
details are provided in Section 5.
We performed detailed experiments, evaluating different versions of M APP and comparing
M APP with fast but incomplete methods such as FAR and W HCA * on grid maps. The results are
presented in Section 9. The benchmark data (Wang & Botea, 2008) consist of 100 to 2000 mobile
units uniformly randomly generated on 10 game maps, with 10 scenario instances per number of
units of each map. We conclude that the extended M APP has significantly better success ratio and
scalability than state-of-the-art incomplete decentralized algorithms. In particular, M APP solves
a higher percentage of units even on crowded instances. Despite M APPs incompleteness in the
56

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

a

b

u

l

a
u

c

b
c

Figure 1: At the left, unit u is blocked by a. A blank is found at location l along the alternate
path, which is marked with a bold contour. At the right: by sliding b and then a along
u . For the sake of clarity and simplicity, we
the alternate path, the blank is brought to li+1
illustrate examples in a four-connected grid world.

general case, the algorithm marks 98.82% of all units as provably solvable during the first stage of
plan computation. When attempting to solve all units, not only the provably solvable ones, M APP
succeeds for 99.86% of all units. In comparison, FAR solved 81.87% of all units. W HCA * solved
77.84% (with diagonal moves allowed) and 80.87% (with no diagonal moves) of all units. Even in
challenging instances with 2,000 mobile units on the maps, 92% to 99.7% mobile units in our test
data fall within M APPs completeness range (i.e., they are provably solvable). In terms of the percentage of fully solved instances, a version of M APP that attempts to solve all units, not only those
that are provably solvable, is successful in 84.5% of all instances. This is significantly better than
FAR (70.6%), W HCA * with no diagonal moves (58.3%), and W HCA * with diagonals (71.25%).
Parts of M APPs computation can be re-used across instances on the same map. On instances
solved by all algorithms, M APP is competitive in speed or significantly faster than W HCA *, depending on whether M APP performs all computations from scratch. When such re-usable data are
available, M APP is slower than the very fast FAR algorithm by a factor of 2.18 on average. M APPs
solutions reported here are on average 20% longer than FARs solutions and 731% longer than
W HCA *s solutions.
Parts of this work have been reported in shorter conference papers as follows. A theoretical
description of Basic M APP, with no experiments, is provided in an earlier paper (Wang & Botea,
2009). A brief overview of M APP extensions and a brief summary of initial results are the topic of a
two-page paper (Wang & Botea, 2010). New material added in the current paper includes a detailed
algorithmic description of the enhancements to Basic M APP and formal proofs for the algorithms
properties. We also provide a comprehensive empirical analysis of enhanced M APP, with several
additional experiments.
The rest of this paper is structured as follows. Next we briefly overview related work. Then,
we state our problem definition in Section 3. Sections 46 focus on Basic M APP. Sections 7 and
8 cover enhancements to the Basic M APP algorithm, extending its completeness range (Section 7),
57

fiWANG & B OTEA

and improving the quality of plans and also the running time (Section 8). An empirical evaluation
is the topic of Section 9. The last part contains conclusions and future work ideas.

2. Related Work
Finding a shortest path that connects a single pair of start-target points on a known, finite map can
be optimally solved with the A* algorithm (Hart et al., 1968). The extension to path planning for
multiple simultaneously moving units, with distinct start and target positions, introduces potential
collisions due to the physical constraint that one location can only be occupied by one unit at a
time. Units have to interact and share information with other units in their path planning, making
the problem more complex.
In multi-agent path planning, a centralized A* performs a single global search in the combined
state space L1  L2      Ln for n units, where Li is the set of possible locations of unit i.
Centralized A* plans the paths for all units simultaneously, finding a joint plan containing all units
actions (waits as well as moves). It retains the optimality and completeness guarantees of A*, but
has a prohibitively large state space of O(mn ) states, for n units on a graph with m nodes. Moreover,
most of the search nodes generated are unpromising, taking some units farther from goal (Standley,
2010). This poses a strong limiting factor on problems that a centralized A* can solve in practice.
On the other hand, a purely decentralized method, Local Repair A* (L RA *) (Stout, 1996) first
plans each units path independently with A*. Then, during execution, L RA * replans by additional
independent A* searches every time a collision occurs. In a good case, L RA * can significantly
reduce computations to O(mn). However, it can also generate cycles between units, and is unable to
prevent bottlenecks. These problems have been discussed by Silver (2005), Bulitko, Sturtevant, Lu,
and Yau (2007), Pottinger (1999), and Zelinsky (1992). In such cases, L RA * exhibits a significant
increase in running time and may not terminate. Therefore, all of the straightforward extensions of
single-agent A* outlined above have strong limitations in practice.
Traditionally, multi-agent path planning took a centralised or a decentralised approach (Latombe,
1991; Choset et al., 2005). A centralized approach plans globally, sharing information centrally,
such as using a potential field (Barraquand, Langlois, & Latombe, 1991). By contrast, a decentralized approach decomposes the problem into a series of smaller subproblems, typically first computing the units paths individually, ignoring all other units, then handling the interactions online.
Examples in robotics include computing velocity profiles to avoid collisions with other units (Kant
& Zucker, 1986), or pre-assigning priorities to process robots one by one (Erdmann & LozanoPerez, 1986). Recent algorithms can also use a combination of the two approaches. For instance,
the Biased Cost Pathfinding (BCP) technique (Geramifard, Chubak, & Bulitko, 2006) generalised
the notion of centralized planning to a central decision maker that resolves collision points on paths
that were pre-computed independently per unit, by replanning colliding units around a highestpriority unit. To avoid excessively long (or even potentially unbounded) conflict resolutions, a limit
on planning time is set. BCP returns paths with the fewest collisions within that time. The algorithm
was shown to work well in small-scale gridworld scenarios, but it is not complete or optimal in the
general case. Standleys (2010) algorithm, on the other hand, improved the standard centralized
search whilst preserving both optimality and completeness. His new state space representation incorporates the next move assignments of every unit into each state, and decomposes a timestep from
advancing all units to advancing units one by one in a fixed ordering. Thus the branching factor is
reduced from 9n to 9, while increasing the depth of the search by a factor of n. This technique gen58

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

erates no more than 9nt state nodes with a perfect heuristic (t being the number of timesteps in the
optimal solution). In practice, this operator decomposition technique (OD) is still often intractable,
producing a lower exponential search space than the standard joint search space. Recognising that
it is much cheaper to perform several independent searches than one global search, Standley also
decoupled planning for non-interfering subgroups of units after an independence detection (ID).
Each group is then solved centrally such that optimality of the overall solution is still guaranteed.
The fully developed hybrid algorithm, OD+ID, uses operator decomposition to improve the centralized planning of non-independent subproblems. Nonetheless, the optimality requirement is costly
in practice. Planning time is still dominated by the largest subgroup of units. As the number of
units increases, they are less likely to be independent as their paths unavoidably overlap, so the
subgroups are expected to increase in size too. Standleys (2010) experiments showed that the incomplete algorithm HCA* (Silver, 2005) actually solved more instances. Furthermore, these are
relatively small problems compared to our experiments (Wang & Botea, 2008, 2010), having at
least 2 orders of magnitude fewer agents (between 260 units), and much smaller maps, with 1 to 2
orders of magnitude fewer tiles (approximately 819 tiles).
Therefore, methods for tackling larger problems take a decentralized approach, and are usually suboptimal in nature. In general, giving up optimality reduces computation significantly. Decentralized path planning is often much faster, and scales up to much larger problems, but yields
suboptimal solutions and provides no completeness guarantees. Recent work on grid maps include
W HCA * (Silver, 2006), which uses a 3-dimensional temporal-spatial reservation table and performs
a series of windowed forward searches on each unit, based on a true distance heuristic obtained
from an initial backward A* search from each target. In the FAR algorithm (Wang & Botea, 2008),
units follow a flow annotation on the map when planning and moving, repairing plans locally using
heuristic procedures to break deadlocks. Other flow related ideas include Jansen and Sturtevants
(2008) direction map for sharing information about units directions of travel, so later units can
follow the movement of earlier ones, with the improved coherence leading to reduced collisions.
Methods such as these scale up to instances with the number of units well beyond the capabilities of
centralized search. However, as mentioned earlier, these methods have no known formal characterizations of their running time, memory requirements, and the quality of their solutions in the worst
case. They lack the ability to answer in a reasonable bounded time whether a given problem would
be successfully solved, which is always important in the case of incomplete algorithms.
In practice, both traditional approaches to multi-agent pathfinding have serious drawbacks, with
the inherent trade-off between scalability, optimality and completeness. Recently, a body of work
has begun to bridge the gap between the two, by addressing both completeness and tractability issues
hand in hand, in a bounded suboptimal approach. Ryan (2008) introduced a complete method that
combines multi-agent path planning with hierarchical planning on search graphs with specific substructures such as stacks, halls, cliques and rings. For example, a stack is a narrow corridor with
only one entrance, which is placed at one end of the stack. Many maps, including the game maps
used in our experiments, seem not to allow an efficient decomposition into stacks, halls, cliques
and rings. B IBOX (Surynek, 2009b) solves problems with at least 2 unoccupied vertices on a biconnected graph. In the worst case, the number of steps is cubic in the number of nodes. B IBOX
was later extended to work with just 1 unoccupied vertex necessary (Surynek, 2009a). Because of
the densely populated problems that the algorithm was designed for, Surynek (2010a) has expressed
that B IBOX does not target computer game scenarios, where there are normally a lot fewer units
than locations on the map. B IBOX is suited for multi-robot scenarios such as automatic packages
59

fiWANG & B OTEA

inside a warehouse (Surynek, 2010c). Bibox- (Surynek, 2009a), that requires only 1 unoccupied
node, was shown to run significantly faster and have significantly shorter solutions than Kornhauser,
Miller, and Spirakiss (1984) algorithm for their related pebble coordination game. We performed
a quick evaluation of B IBOX using the code obtained from the author. We found that, on graphs at
an order of magnitude smaller than our game maps, B IBOX exhibits a fast-growing runtime (e.g.,
more than 10 minutes for a graph with 2500 locations) and long solutions, with millions of moves.
Part of the explanation is that B IBOX builds its instances to be very crowded. In our understanding,
B IBOX was designed to solve very crowded instances, not necessarily to efficiently solve instances
with significantly fewer units than locations.

3. Problem Statement
An instance is characterized by a graph representation of a map, and a non-empty collection of
mobile units U . Units are homogeneous in speed and size. Each unit u  U has an associated starttarget pair (su , tu ). All units have distinct starting and target positions. The objective is to navigate
all units from their start positions to the targets while avoiding all fixed and mobile obstacles. A state
contains the positions of all units at a given time. Our work assumes undirected weighted graphs
where each unit occupies exactly one node at a time, and can move to an unoccupied neighbour
node. The time is discretized and one or more units can move synchronously at each time step.
Travelling along an edge does not depend on or interfere with the rest of the problem, except for the
two nodes connected by that edge.
Several methods exist to abstract a problem map into a search graph, including navigation
meshes (Tozour, 2002), visibility points (Rabin, 2000), and quadtrees (Samet, 1988). However, a
graph abstraction that generates too few nodes, such as a visibility graph, may render a multi-agent
pathfinding problem unsolvable, even though it works for the single agent case. On the other hand,
a search graph obtained from imposing a regular grid contains more nodes, covering all locations
of the traversable space, and offers more path options to avoid collisions between units. Hence,
grid maps, besides being very popular and easy to implement, are more suitable to multi-agent
problems. For clarity and practicality, we focus on grid maps in our examples and experiments.
Nonetheless, the conditions and algorithmic definitions for M APP, which we introduce in the next
few sections, are not specific to regular grid maps. In our illustrated examples, we assume that only
straight moves in the four cardinal directions can be performed (4 connected grid). Restricting the
movements from 8 directions (cardinal + diagonals) to 4 cardinal directions has no negative impact
on completeness. Since the standard practice is to allow a diagonal move only if an equivalent (but
longer) two-move path exists, for every solution that allows diagonal moves, there is a solution with
only cardinal moves. Therefore, any problem with diagonal moves can be reduced to a problem with
only straight moves, at the price of possibly taking longer paths. Introducing diagonal moves could
reduce the path length, but has the potential drawback of blocking units more often than straight
moves on crowded maps. Whether there is enough clearance to make a diagonal move depends on
the other two adjacent nodes (i.e., the other two tiles sharing the common corner on the grid), since
it is physically impossible to squeeze through two units.

4. The S LIDABLE Class of Instances
We introduce a subclass of instances for which Basic M APP will be shown to be complete.

60

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

i
i+1

u

(u)
i-1

u and lu (denoted as i  1
Figure 2: An example of an alternate path, i , connecting locations li1
i+1
and i + 1 in the picture) that belong to the precomputed path (u) of unit u.

Definition 1 (S LIDABLE unit and S LIDABLE instance). A mobile unit u is S LIDABLE iff a path
u
u
(u) = (l0u , l1u , . . . , l|(u)|
) of nodes exists, where l0u = su , l|(u)|
= tu , such that all the following
conditions are met:
u , lu , lu on (u), except the
1. Alternate connectivity. For each three consecutive locations li1
i i+1
last triple ending with tu , i.e. for 0 < i < |(u)|  1, an alternate path ui exists between
u and lu that does not go through lu . See Figure 2 for an example.
li1
i+1
i

2. Initial blank. In the initial state, l1u is blank (i.e. unoccupied).
3. Target isolation. No target interferes with the  or -paths of the other units. More formally,
both of the following hold for tu :
(a) (v  U \ {u}) : tu 
/ (v); and
(b) (v  U, i  {1, . . . , |(v)|  1}) : tu 
/ vi .
An instance belongs to the class S LIDABLE iff all units u  U are S LIDABLE.
The three conditions can be verified in polynomial time. The verification includes attempting to
compute the  and  paths for each unit. Since each state space that A* has to explore here is linear
in m, each A* search time is polynomial in m. The checks for a blank location in the first step, and
for not passing through other targets, are trivial. The process that checks the S LIDABLE conditions
serves for an important additional purpose. By the time the checks succeed and an instance is known
to belong to S LIDABLE, we have completed all the search that is needed to solve the instance. The
remaining part of the algorithm will simply tell units when to wait, when to move forward, and
when to move backwards along the already computed  and  paths.
Notice that the three conditions are not restricted to grid maps only. They work on the standard
assumption that one graph node can only be occupied by one unit at a time, and that moving along
an edge neither depends nor interferes with other parts of the graph except for two nodes at the ends
of that edge.
61

fiWANG & B OTEA

Algorithm 1 Overview of M APP.
1: for each u  U do
2:
compute (u) and s (as needed) from su to tu
3:
if S LIDABLE conditions hold then
4:
mark u as S LIDABLE
5: initialize A as the set of S LIDABLE units {optional: make all units active, as discussed in text}
6: while A 6=  do
7:
do progression step
8:
do repositioning step if needed

5. Basic M APP
We present the basic version of the M APP algorithm, which is complete on the S LIDABLE class of
problems. A main feature of Basic M APP (and its extensions presented in Sections 7 and 8) is that
it is deadlock-free and cycle-free, due to a total ordering of active units. Units of lower priority do
not interfere with the ability of higher priority units to advance.
As illustrated in Algorithm 1, for each problem instance, M APP starts by computing a path (u)
for each unit u to its target (goal), constructing and caching alternate paths  along the way. Note
that all paths  and alternate paths  need to satisfy the conditions in Definition 1. If the for loop in
lines 14 succeeds for all units, M APP can tell that the instance at hand belongs to S LIDABLE, for
which M APP is complete.
If only a subset of units are marked as S LIDABLE, M APP is guaranteed to solve them. This
is equivalent to solving a smaller instance that is S LIDABLE. Optionally, M APP can attempt to
solve the remaining units as well, by adding them to the set of active units but giving them a lower
priority than S LIDABLE units. It is important to stress out that, in the remaining part of the paper,
the implicit assumption is that M APP attempts to solve only the provably solvable units, unless we
explicitly state the opposite. In the experiments section, however, we discuss both options.
The set of S LIDABLE units is partitioned into a subset S of solved units that have already reached
their targets, and a subset A of active units. Initially, all units are active. In the S LIDABLE class,
after becoming solved, units do not interfere with the rest of the problem (as ensured by the target
isolation condition). As shown later, in Basic M APP solved units never become active again, and do
not have to be considered in the remaining part of the solving process.
Definition 2. The advancing condition of an active unit u is satisfied iff its current position, pos(u),
belongs to the path (u) and the next location on the path is blank.
Definition 3. A state is well positioned iff all active units have their advancing condition satisfied.
Lines 68 in Algorithm 1 describe a series of two-step iterations. A progression step advances
active units towards their targets. As shown later, each progression step brings at least one active
unit to its target, shrinking the active set A and ensuring that the algorithm terminates, reaching the
state where all units are solved. A progression could result in breaking the advancing condition of
one or more active units, if any remain. The objective of a repositioning step is to ensure that each
active unit has its advancing condition satisfied before starting the next progression step. Note that
a repositioning step is necessary after every progression step except for the last.
62

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

a
b
a
b

a

a

b

b

a

a

b

a
b

i

b
ii

b
iii

b
iv

v

Figure 3: Example of how M APP works.

5.1 Example
A simple example of how M APP works is illustrated in Figure 3. There are two units, a and b.
M APP uses a total ordering of the active units in each progression step (Section 5.3). Here, a has a
higher priority than b. The targets of a and b are drawn as stars. In Figure 3 (i), as a and b progress
towards their targets, a becomes blocked by b. In (ii), a blank is brought in front of a by sliding
b down ai (outlined in bold); as a side effect, b is pushed off its path. At the end of the current
progression step (iii), a reaches its target. In the repositioning step (iv), since a is already solved,
as moves are ignored. Repositioning undoes bs moves until b is back on its path and it has a blank
it front of it. Now bs advancing condition is restored and therefore the global state in this example
is well positioned. In the next progression step (v), b reaches its target. The algorithm terminates.
5.2 Path Computation
For each problem instance, we compute each path (u) individually. The paths (u) are fixed
throughout the solving process. To ensure that paths satisfy the alternate connectivity condition
(Definition 1), we modify the standard A* algorithm as follows. When expanding a node x0 , a
neighbour x00 is added to the open list only if there is an alternate path between x00 and x, the parent
of x0 . By this process we compute each path (u) and its family of alternate paths  simultaneously.
To give each neighbour x00 of the node x0 a chance to be added to the open list, node x0 might have
to be expanded at most three times, once per possible parent x. Therefore, O(m) node expansions
are required by A* search to find each  path, where m is the number of locations on the map.
Equivalently, computing a  path could also be seen as a standard A* search in an extended space
of pairs of neighbouring nodes (at most four nodes are created in the extended space for each original
node).
Since alternate paths depend only on the triple locations, not the unit, we can re-use this information when planning paths for all units of the same problem. This means that the alternate path
for any set of three adjacent tiles on the map is computed at most once per problem instance, and
cached for later use. Given a location l on a grid map, there are at most eight locations that could
be on a path two moves away on a four-connect grid. As shown in Figure 4a, these eight locations
form a diamond shape around l. For each of the four locations that are on a straight line from l
63

fiWANG & B OTEA

1
8

1
2

l

7
6

8
3

7

4

i

2

l

ii

6

3

4

5

5

a

b

Figure 4: (a) The eight locations two moves away from l. (b) Two two-move paths from l to location
2 go through i and ii.

(locations 1, 3, 5, 7), we precompute an alternate path that avoids the in-between location and any
targets. For each of the other four locations (labeled 2, 4, 6, 8), we need to compute (at most) two
alternate paths. For example, there are two possible paths between l and 2 that are two moves long:
through i or ii (Figure 4b). We need one alternate path to avoid each intermediate location, i and
ii. In summary, we precompute at most 12 paths for each l. For at most m locations on a map, we
need at most 12m
2 = 6m alternate paths (only one computation for each triple, since an alternate
path connects its two endpoints both ways).
A possible optimization is to reuse alternate paths across S LIDABLE instances on the same map.
Alternate paths that overlap targets in the new instance need to be re-computed. We discuss this
option in the experiments section.
5.3 Progression
Algorithm 2 shows the progression step in pseudocode. At each iteration of the outer loop, active
units attempt to progress by one move towards their targets. They are processed in order (line 2). If
unit v is processed before unit w, we say that v has a higher priority and write v < w. The ordering
is fixed inside a progression step, but it may change from one progression step to another. The actual
ordering affects neither the correctness nor the completeness of the method, but it may impact the
speed and the solution length. The ordering of units can be chosen heuristically, e.g. giving higher
priority to units that are closer to target. Thus, these units could get to their target more quickly, and
once solved they are out of the way of the remaining units in the problem.
To ensure that lower priority units do not harm the ability of higher priority units to progress,
we introduce the notion of a private zone. We will see in Algorithm 2 that a unit cannot cause
moves that will occupy the private zone of a higher-priority unit.1 Given a unit u, let pos(u) be
u
its current position, and let int((u)) = {l1u , . . . , l|(u)|1
} be the interior of its precomputed path
(u). As shown in Algorithm 2, a unit u might get pushed off its precomputed path, in which case
pos(u) 
/ (u).
u , lu } if pos(u) = lu  int((u)).
Definition 4. The private zone, (u), of a unit u is (u) = {li1
i
i
Otherwise, (u) = {pos(u)}. In other words, the private zone includes the current location of the

1. A move caused by a unit u is either a move of u along its own (u) path, or a move of a different unit w, which is
being pushed around by u as a side effect of blank travel.

64

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Algorithm 2 Progression step.
1: while changes occur do
2:
for each u  A in order do
3:
if pos(u) 
/ (u) then
4:
do nothing {u has been pushed off track as a result of blank travel}
u in current progression step then
5:
else if u has already visited li+1
6:
do nothing
u , belongs to the private zone of a higher priority unit, i.e.
7:
else if the next location, li+1
u
v < u : li+1  (v) then
u is released by v}
8:
do nothing {wait until li+1
u is blank then
9:
else if li+1
u
10:
move u to li+1
u then
11:
else if can bring blank to li+1
u
12:
bring blank to li+1
u
13:
move u to li+1
14:
else
15:
do nothing

unit. In addition, when the unit is on its pre-computed path but not on the start position, the location
behind the unit belongs to the private zone as well.
Lines 315 in Algorithm 2 show the processing of u, the active unit at hand. If u has been pushed
off its precomputed path, then no action is taken (lines 34). Lines 5 and 6 cover the situation when
unit u has been pushed around (via blank travel) by higher-priority units back to a location on (u)
already visited in the current progression step. In such a case, u doesnt attempt to travel again on
a previously traversed portion of its path, ensuring that the bounds on the total travelled distance
u
introduced later hold. If u is on its path but the next location li+1
is currently blocked by a higheru is available, u
priority unit v, then no action is taken (lines 78). Otherwise, if the next location li+1
u
moves there (lines 910). Finally, if li+1 is occupied by a smaller-priority unit, an attempt is made
u
to first bring a blank to li+1
and then have u move there (lines 1113). When u moves to a new
u
u is the target location of u. If this
location li+1 (lines 10 and 13), a test is performed to check if li+1
is the case, then u is marked as solved by removing it from A and adding it to S, the set of solved
units.
u
Bringing a blank to li+1
(lines 11 and 12) was illustrated in Figure 1. Here we discuss the
process in more detail. A location l  ui is sought with the following properties: (1) l is blank,
u (inclusive) along u belongs to the private zone of a higher(2) none of the locations from l to li+1
i
u
priority unit, and (3) l is the closest (along ui ) to li+1
with this property. If such a location l is
u
found, then the test on line 11 succeeds. The actual travel of the blank from l to li+1
along ui
(line 12) is identical to the movement of tiles in a sliding-tile puzzle. Figure 1 shows an example
u
before and after blank traveling. The intuition behind seeking a blank along ui is that, often, li1
u to lu and until the test on line 11 is
remains blank during the time interval after u advances from li1
i
performed. This is guaranteed to always hold in the case of the active unit with the highest priority,
which we call the master unit.
Let us introduce and characterize the behaviour of the master unit more formally. At the beginning of a progression step, one master unit u is selected. It is the unit with the highest priority among
65

fiWANG & B OTEA

the units that are active at the beginning of the progression step. The status of being the master unit
is preserved during the entire progression step, even after u becomes solved. At the beginning of
the next progression step, a new master unit will be selected among the remaining active units.
Lemma 5. The master unit u can always bring a blank to its front, if it needs one.
u , belongs to its private zone, (u), and no other unit can
Proof. Since us previous location, li1
u .
move into the private zone of the highest priority unit, u is guaranteed to always find a blank at li1
u to lu can belong to the private zone of a higher priority
Moreover, no location along ui from li1
i+1
unit since there are no units with a higher priority. Note also that ui is free of physical obstacles by
u to lu .
construction. So it must be possible for the blank to travel from li1
i+1

Lemma 6. The master unit u is never pushed off its -path.
Proof. If u is pushed off (u) in blank travelling performed by another unit, it contradicts with u
being the highest priority unit.
Theorem 7. As long as the master unit u is not solved, it is guaranteed to advance along (u) at
each iteration of the outer (while) loop in Algorithm 2. By the end of the current progression
step, at least u has reached its target.
Proof. Using the previous two lemmas, it is easy to check that u never enters a do nothing line in
Algorithm 2. Similar to Lemma 6, u is never pushed and cannot revisit a previous location. Also,
since u has the highest priority, its next location cannot be held in the private zone of another unit.
Hence, us progress to its target is guaranteed.
The following result is useful to ensure that a progression step always terminates, either in a
state where all units are solved or in a state where all remaining active units are stuck.
Theorem 8. Algorithm 2 generates no cycles (i.e., no repetitions of the global state).
Proof. We show a proof by contradiction. Assume that there are cycles. Consider a cycle and the
active unit u in the cycle that has the highest priority. Since no other unit in the cycle dominates u,
it means that the movements of u cannot be part of a blank travel triggered by a higher priority unit.
Therefore, the movements of u are a result of either line 10 or line 13. That is, all us moves are
along its path (u). Since (u) contains no cycles, u cannot run in a cycle.
5.4 Repositioning
By the end of a progression step, some of the remaining active units (if any are left) have their
advancing condition broken. Recall that this happens for a unit u when either pos(u) 
/ (u) or u
is placed on its precomputed path but the next location on the path is not blank. A repositioning
step ensures that a well positioned state is reached (i.e., all active units have the advancing condition
satisfied) before starting the next progression step.
A simple and computationally efficient method to perform repositioning is to undo a block of the
most recent moves performed in the preceding progression step. Undoing a move means carrying
out the reverse move. Solved units are not affected. For those remaining active units, we undo their
moves, in reverse global order, until a well positioned state is encountered. We call this strategy
reverse repositioning. An example is provided in Section 5.1.
66

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Proposition 9. If the reverse repositioning strategy is used at line 7 of Algorithm 1 (when needed),
then all progression steps start from a well positioned state.
Proof. This lemma can be proven by induction on the iteration number j in Algorithm 1. Since the
initial state is well positioned (this follows easily from Definitions 1 and 3), the proof for j = 1
is trivial. Assume that a repositioning step is performed before starting the iteration j + 1. In the
worst case, reverse repositioning undoes all the moves of the remaining active units (but not the
moves of the units that have become solved), back to their original positions at the beginning of j-th
progression step. In other words, we reach a state s that is similar to the state s0 at the beginning
of the previous progression step, except that more units are on their targets in s. Since s0 is well
positioned (according to the induction step), it follows easily that s is well positioned too.

6. Worst-case and Best-case Analysis
We give here bounds on the runtime, memory usage, and solution length for the M APP algorithm
on a problem in S LIDABLE with n units on a map of m traversable tiles. We examine the worst case
scenario in each case, and also discuss a best-case scenario at the end.
We introduce an additional parameter, , to measure the maximal length of alternate paths . In
the worst case,  grows linearly with m. However, in many practical situations,  is a small constant,
since the ends of an  path are so close to each other. Our analysis discusses both scenarios.
Theorem 10. Algorithm 1 has a worst-case running time of O(max(n2 m, m2 log m)) when  is a
constant, and O(n2 m2 ) when  grows linearly with m.
Proof. As outlined in Section 5.2, each single-agent A* search with a consistent heuristic 2 expands
O(m) nodes. Hence, assuming that the open list is implemented as a priority queue, each A* search
takes O(m log m) time. Note that, on graphs where all edges have the same cost, the log m factor
could in principle be eliminated using a breadth-first search to find an optimal path. Grid maps with
only cardinal moves fit into this category. However, for simplicity, here we assume that the log m
factor is present.
Hence, in the worst case, the searches for -paths take O(nm log m) time for all n units. The
A* searches for all s take O(m2 log m) time.
In a single progression step, outlined in Algorithm 2, suppose blank travel is required by all n
units, for every move along the way except the first and last moves. Since the length of  paths is
bounded by m and the length of alternate paths  is bounded by , the total number of moves in a
progression step is O(nm), and so is the running time of Algorithm 2.
Clearly, the complexity of a repositioning step cannot exceed the complexity of the previous
progression step. Hence the complexity of each iteration in Algorithm 1 (lines 57) is O(nm).
The number of iterations is at most n, since the size of A reduces by at least one in each iteration. So
M APP takes O(max(nm log m, m2 log m, n2 m)) time to run, which is O(max(n2 m, m2 log m))
when  is constant and O(n2 m2 ) when  grows linearly with m.
Theorem 11. The maximum memory required to execute M APP is O(nm) when  is a constant, or
O(nm2 ) when  grows linearly with m.
2. It is well known that the Manhattan heuristic, which we used in our implementation, is consistent. The proof is
easy, being a direct result of 1) the definition of consistency and 2) the way the Manhattan distance is computed (by
pretending that there are no obstacles on the map).

67

fiWANG & B OTEA

Proof. Caching the possible  paths for the entire problem as described in Section 5.2 takes O(m)
memory. The A* searches for the  paths are performed one at a time. After each search,  is stored
in a cache, and the memory used for the open and closed lists is released. The A* working memory
takes only O(m) space, and storing the  paths takes O(nm) space. Overall, path computation
across all units requires O(nm + m) space.
Then, in lines 57 of Algorithm 1, memory is required to store a stack of moves performed
in one progression step, to be used during repositioning. As shown in the proof of Theorem 10,
the number of moves in a progression step is within O(nm). So, the overall maximum memory
required to execute the program is O(nm), which is O(nm) when  is a constant and O(nm2 )
when  grows linearly with m.
Theorem 12. The total distance travelled by all units is at most O(n2 m) when  is a constant, or
O(n2 m2 ) when  grows linearly with m.
Proof. As shown previously, the number of moves in a progression step is within O(nm). The
number of moves in a repositioning step is strictly smaller than the number of moves in the previous
progression step. There are at most n progression steps (followed by repositioning steps). Hence,
the total travelled distance is within O(n2 m).
Corollary 13. Storing the global solution takes O(n2 m) memory when  is a constant, or O(n2 m2 )
when  grows linearly with m.
We discuss now a best case scenario. M APP computes optimal solutions in the number of moves
when the paths  are optimal and all units reach their targets without any blank traveling (i.e., units
travel only along the paths ). An obvious example is where all paths  are disjoint. In such a case,
solutions are makespan optimal too. As well as preserving the optimality in the best case, the search
effort in M APP can also be smaller than that spent in a centralised A* search, being n single-agent
m!
O(m) searches, compared to searching in the combined state space of n units, with up to (mn)!
states.

7. Extending the Completeness Range
To extend M APPs completeness beyond the class S LIDABLE, we evaluated the impact of each of
the three S LIDABLE conditions in a preliminary experiment. We ran Basic M APP on the same data
set that we also used in the main experiments (the data set and the main experiments are described
in Section 9). In the preliminary experiment, we switched off one S LIDABLE condition at a time
and counted how many units satisfy the remaining two conditions. A larger increase in the number
of solvable units suggests that relaxing the definition of the condition at hand could provide a more
significant increase in the completeness range.
This initial experimental evaluation indicates that Basic M APP with all three S LIDABLE conditions solves 70.57% of units (basic case). If the alternate connectivity requirement is switched off,
87.06% units satisfy the remaining two conditions. Switching off target isolation makes 85.05%
units satisfy the remaining two conditions. However, ignoring the blank availability condition has a
very small impact, increasing the percentage only slightly, from 70.57% in the basic case to 70.73%.
These results suggest that the alternate connectivity and the target isolation conditions are more restrictive than the blank availability condition. Thus, we focus on relaxing these two conditions.
68

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

For target isolation, our extension allows a unit to plan its path through other targets, when doing
so can still guarantee that a clearly identified set of units will reach their targets. This is the topic
of Section 7.1. To extend alternate connectivity, we developed a technique that allows  paths to
be planned through regions with no alternate paths, such as tunnels. The blank travelling operation
for a tunnel-crossing unit now uses the blank positions ahead of this unit, along the remaining of its
pre-computed path, as we describe in detail in Section 7.2. An empirical analysis of each of these
features is provided in Section 9.
7.1 Relaxing the Target Isolation Condition
When several targets are close to each other, the target isolation condition, forbidding  and  paths
to pass through targets, can make targets behave as a virtual wall, disconnecting two areas of the
map. As a result, Basic M APP can report many units as non-S LIDABLE.
The extension we introduce allows a unit u to plan its path through the target of another unit v,
if subsequently v is never assigned a higher priority than u. More specifically, a partial ordering 
is defined, such that u  v iff the target of v belongs to the -path of u or to any
path along us
S u 2
ui ). Every
-path. Written more formally, u  v iff tv  (u), where (u) = ((u)  ki=1
time we mention  we refer to its transitive closure. We show in this section that, if paths can be
planned in such a way that the (possibly empty) relation  creates no cycles of the type u  u, then
an instance can be solved with a slight modification of Basic M APP.
Units plan their  and  paths through a foreign target only when they have no other choice. To
achieve this strategy, in the A* searches we assign a very high cost to graph search edges adjacent
to a foreign target. This has the desirable outcome of reducing the interactions caused by the target
isolation relaxation. In particular, the way the original S LIDABLE units compute their paths is
preserved, as no foreign targets will be crossed in such cases. In other words, instances in S LIDABLE
are characterized by an empty  relation.
Definition 14. An instance belongs to class T I-S LIDABLE iff for every unit u there exists a path (u)
satisfying the alternate connectivity and the initial blank condition as in the definition of S LIDABLE
(Definition 1). Furthermore, no cycles are allowed in the  relation.
Assume for a moment that a (possibly empty)  relation without cycles is available. Aspects
related to obtaining one are discussed later in this section.
Definition 15. For solving T I-S LIDABLE instances, the extended algorithm, T I M APP, has two
small modifications from the original algorithm:
1. The total ordering < inside each progression step stays consistent with : u  v  u < v.
2. If u  v, then v cannot be marked as solved (i.e. moved from A to S) unless u has already
been marked as solved.
With these extra conditions at hand, we ensure that even if a unit x arrives at its target tx before
other units clear tx on their -paths, those units can get past x by performing the normal blank
travel. Following that, x can undo its moves back to tx in the repositioning step, as in Basic M APP.
To prove that T I M APP terminates, we first prove the following two lemmas hold for the highest
priority unit, u, in any progression step:
69

fiWANG & B OTEA

Lemma 16. No other unit will visit the target of u, tu , in the current progression step.
Proof. Since u is the master unit, it follows that u < v for any other active unit v. According to
point 1 of Definition 15, it follows that u  v. Relation  has no cycles, which means that v  u.
Therefore, by applying the definition of , it follows that tu 
/ (v). This completes the proof, as
in M APP all movements are performed along  and  paths.
Since a repositioning step can only undo moves made in the previous progression step, units
will only revisit locations visited during that progression step. So, the following is a direct result of
Lemma 16:
Corollary 17. No other unit will visit tu in the repositioning step that follows.
Corollary 18. After u is solved, it cannot interfere with the rest of the problem.
Theorem 19. T I M APP terminates.
Proof. Showing that at least the highest priority unit u reaches its target in a given progression step
is virtually identical to the proof for Lemma 7. Corollary 18 guarantees that, after solving u, it does
not interfere with the rest of the problem. Hence, the number of active units strictly decreases after
each progression step, and the algorithm eventually terminates.
Let us get back to the question of how to provide a cycle-free  relation. Testing whether all
units can plan their paths in such a way that no cycle is introduced might end up being expensive.
When a unit u cant possibly avoid all other targets, it might have to choose between crossing the
target of v or crossing the target of w. One option might lead to a cycle whereas the other might
avoid cycles. Therefore, a systematic search might be required to seek a cycle-free relation .
Rather than searching systematically, our T I M APP takes a cheaper, greedy approach. If there
are cycles, we mark a number of units as not being T I-S LIDABLE. These are selected in such a way
that all other units remain cycle-free (we call these T I-S LIDABLE units). T I-S LIDABLE units are
guaranteed to be solved.
As a result of its greedy approach, T I M APP is not complete on class T I-S LIDABLE. Still, it
is complete on a superset of S LIDABLE and it is able to identify many units (often all) that will be
provably solved.
Finally, we wrap up the discussion of this extension by concluding that the upper bounds for
M APP, given in Section 6, still apply to T I M APP. The proof is identical because we can make the
same worst-case assumptions as before: only the master unit gets solved in a progression step, and
every move along each units  path requires blank travel. Moreover, note the additional step after
path pre-computation for topologically sorting the partial order, , into a linear priority order <,
can be done cheaply in time linear on the number of units (Tarjan, 1976).
7.2 Relaxing the Alternate Connectivity Condition
As we will show in Section 9, the previous extension of target isolation significantly improves
M APPs success ratio (i.e., percentage of solvable units). Yet, there was significant room for further
improvement. In particular, maps with single-width tunnels still showed a bottleneck in terms of
success ratio. Tunnels make the alternate connectivity condition  connecting the two ends of
a consecutive triple locations without going through the middle  harder or even impossible to
70

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

satisfy. When a single-width tunnel bridges two otherwise disjoint regions, as shown in Figure 5,
the versions of M APP presented so far fail to find a path between the two regions, because alternate
connectivity is broken for triples inside the tunnel.

Figure 5: An example where all units have their targets on the other side, and the only way is to
cross the single-width bridge. Units are drawn as circles, and their corresponding targets
are squares of the same shade.

In this section we introduce the buffer zone extension, our solution for relaxing the alternate
connectivity condition. It allows many paths, corresponding to many units, to cross the same singlewidth tunnel. The intuition is simple. Often, there are plenty of blank positions ahead of a unit, along
the remaining locations of its precomputed -path and the corresponding  paths along it. Our
tunnel-crossing operation is essentially a generalisation of blank travelling, where a blank position
is sought from the path ahead, instead of the alternate path of the current location triple.
Definition 20. For each precomputed path (u) that crosses tunnels, we define the following:
 The buffer zone, ((u)), is the portion of (u) between the target and the end of the
last tunnel (at theSj-th move on (u)), together with the corresponding alternate paths:
((u)) =
{liu }  ui .
i{j+2,...,ku 1}

 A dynamic counter, ((u)), keeps track of how many positions in the buffer zone are blank.
The counter is initialized at the beginning with the appropriate value, and it is incremented
and decremented as necessary later on.
 A threshold  ((u)) is set to length(t) + 2, where t is the longest tunnel to be crossed by the
unit u. This threshold acts as a minimal value of ((u)) that guarantees that u can cross
tunnels safely.

71

fiWANG & B OTEA

When a unit u attempts to cross a tunnel, it can push units of lower priorities to the closest blank
locations in the buffer zone, until u exits the tunnel. For such a tunnel-crossing operation to be
possible, enough blanks have to be available in the buffer zone. Before we analyse the new extended
algorithm in detail, we introduce the extended class AC S LIDABLE, whose definition includes those
units meeting the new buffer zone extension.
Definition 21. In relaxing the alternate connectivity condition, we allow (u) to go through one or
more single-width tunnels iff there are enough blanks in us buffer zone, with at least  ((u)) blank
locations in ((u)) in the initial state, i.e., ((u))   ((u)). As before, alternate paths are still
needed for all locations outside of tunnels.
Definition 22. A unit u  U belongs to the extended class, which we call AC S LIDABLE, iff it has
a path (u) meeting the initial blank and the target isolation conditions as given in the definition of
S LIDABLE (Definition 1), and the relaxed alternate connectivity condition (Definition 21 above).
AC M APP is modified from Basic M APP in the following two ways to integrate our buffer
zone technique for relaxing the alternate connectivity condition. Firstly, a repositioning step cannot
finish if a counter () has a value below the threshold  (). In other words, we need to ensure that
enough blanks are available in a buffer zone before a progression step begins. The following is the
new advancing condition, updated from Definition 2 by adding the extra, aforementioned condition.
Definition 23. The advancing condition of an active, tunnel-crossing unit u is satisfied iff its current
position belongs to the path (u) and the next location on the path is blank (as given in Definition 2),
and also ((u))   ((u)).
Secondly, we need to preserve one of Basic M APPs main features, where units of lower priority
never block units with higher priority, ensuring that M APP does not run into cycles or deadlocks.
Hence, a unit u with a lower priority than v cannot cause moves that bring ((v)) below the
threshold (i.e., from  ((v)) to  ((v))  1). Recall that a move caused by u is either a move
of u along its own (u) path (checked in lines 7-8 of Algorithm 4), or a move of a different unit
w, which has been pushed around by u as a side effect of blank travel (checked in lines 7-11 of
Algorithm 3). Thus the buffer zone of u acts as a generalised private zone, in which u holds at least
 ((u)) locations that are not accessible to units with lower priorities.
Our extensions for AC M APP maintain the following properties of Basic M APP.
Lemma 24. As long as the master unit u is not solved, it is guaranteed to advance along (u) at
each iteration of the outer (while) loop in Algorithm 4. By the end of the current progression
step, at least u has reached its target.
Proof. Most of this result follows directly from the proof for Lemma 7. The parts that are new
in Algorithm 4 compared to Algorithm 2 (the progression step in Basic M APP) are the check in
lines 7-8 and the modified blank travelling operation (lines 13-15). Since u has the highest priority
of the current progression step, it can cause moves affecting the buffer zone of every other unit, but
no other unit can move into the buffer zone of u when doing so would bring the number of blanks
below the threshold, i.e. ((u)) <  ((u)). Hence u is guaranteed to have enough blanks to cross
each tunnel on (u).
The proof for Lemma 25 below is very similar to Lemma 8 in Section 5.3.
72

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

u )
Algorithm 3 AC M APP  canBringBlank( unit u, location li+1
1: if u is outside of tunnels then
2:
look for a nearest blank b along ui
3: else if u is inside a tunnel then
4:
look for a nearest blank b from ((u))
5: if no b can be found then
6:
return false
u } do {segment along u or ((u)) from above}
7: for each location l  {b, . . . , li+1
i
8:
if v < u : l  (v) then {check if causing another unit to move into the private zone of a
higher priority unit, v}
9:
return false
10:
else if v < u : l  ((v)) & ((v))   ((v)) then {check if causing another unit to
move into the buffer zone of a higher priority unit, v}
11:
return false
12: return true

Algorithm 4 AC M APP  Progression step.
1: while changes occur do
2:
for each u  A in order do
3:
if pos(u) 
/ (u) then
4:
do nothing
u
5:
else if v < u : li+1
 (v) then
6:
do nothing
u
7:
else if v < u : li+1
 ((v)) & ((v))   ((v)) then {check when moving into the
buffer zone of a higher priority unit}
8:
do nothing {wait until v has more blanks in its buffer zone}
u in current progression step then
9:
else if u has already visited li+1
10:
do nothing
u is blank then
11:
else if li+1
u
12:
move u to li+1
u ) then {Algorithm 3}
13:
else if canBringBlank( u, li+1
u
14:
bring blank to li+1
u
15:
move u to li+1
16:
else
17:
do nothing

Lemma 25. Algorithm 4 generates no cycles (i.e., no repetitions of the global state).
Theorem 26. AC M APP terminates.
Proof. It follows from Lemmas 24 and 25 that the number of active units strictly decreases over
successive iterations of Algorithm 4. Hence, the algorithm AC M APP eventually terminates.
Since we have shown that the algorithm AC M APP is guaranteed to solve the class AC S LID the completeness result shown below follows directly.

ABLE ,

73

fiWANG & B OTEA

Corollary 27. AC M APP is complete on the class AC S LIDABLE.
The AC M APP extension preserves the upper bounds on running time, memory usage, and solution length given in Section 6. Here, we introduce max to denote the maximal length of tunnels that
units have to cross. In our worst case analysis, all units initiate blank travelling for every move along
the way, which now involves tunnels. So, depending on whether max or , the maximal length of
 paths, is longer, AC M APP runs in O(n2 mmax ) or O(n2 m) time. Since both parameters are
often constant in practice, or grow at worst linear in m, the running time is O(n2 m) or O(n2 m2 ),
as before. The bounds for total travel distance and global solution follow directly. Lastly, there is
virtually no additional memory required for storing the buffer zones, except for one counter and one
threshold variable, per unit.
7.3 Combining Target Isolation and Alternate Connectivity Relaxations
We show that the two extensions to the S LIDABLE class can be combined.
Definition 28. An instance belongs to the extended class, T I+AC S LIDABLE, iff for every unit u
there exists a path (u) meeting the initial blank condition as given in Definition 1, and the relaxed
alternate connectivity condition from Definition 22. Furthermore, the (possibly empty)  relation
introduced as a result of target isolation relaxation has to be cycle-free, just as in Definition 14.
We obtain an extended algorithm, T I+AC M APP, by combining T I M APP (Definition 15) and
AC M APP (Algorithms 3 and 4).
Theorem 29. T I+AC M APP terminates.
Proof. As in the proof for Lemma 24, we can show that at least the highest priority unit u reaches its
target in a progression step, as follows. From Definition 21, u is guaranteed to have enough blanks
to clear through any single-width tunnels along its path. Definitions 14 and 15 guarantee that,
outside of tunnels, u can always bring a blank when needed, as stated in Lemma 5. Furthermore,
this progression step generates no cycles. This can be proved as in the cases of Lemmas 25 and 8.
We also know that the solved unit u does not interfere with the rest of the problem, from the
results of Lemmas 16 and 18, and Corollary 17. Note that the tricky cases where units have their
targets inside single-width tunnels are excluded from the extended class T I+AC S LIDABLE, because
they have zero buffer capacity according to how  was defined in Definition 20.
Since each iteration of the algorithm solves at least one unit, T I+AC M APP terminates.

8. Improving Solution Length
As mentioned before, to avoid replanning, units pushed off-track in blank travelling by other units
undo some of their moves to get back on their -paths in the immediate repositioning step. We have
observed that, in practice, the reverse repositioning strategy (defined in Section 5.4) can introduce
many unnecessary moves, which increase the solution length, increase the running time, and may
hurt the visual quality of solutions.
Recall that, in a standard reverse repositioning step, new moves are added to the solution that
is being built. These moves undo, in reverse order, moves of active units (i.e., those not solved
yet) made in the previous progression step. The process continues until a well positioned state is
74

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

u

(v)

u

(v)

v

v

(u)

(u)

Figure 6: Two examples of global checking for a well positioned state.

reached, which means that all active units have their advancing condition satisfied (i.e., have every
active unit on its -path with a blank in front).
After each undo move, the well-positioned check is performed globally. In other words, Basic
M APP checks the advancing condition for all active units, not only for the unit affected by the most
recent undo move. This global checking guarantees to eventually reach a well-positioned state, as
proved in Proposition 9, but, as mentioned earlier, it can often create many unnecessary moves.
We provide two simple examples in Figure 6, to illustrate one case when global checking is
useful, and one case when global checking is too strong a condition, adding unnecessary moves.
First, consider two units, u and v, such that undoing one move off the global moves stack places u
back on its path, with a blank in front. Assume further that us current position is on the way of vs
future undo moves, as shown on the left of Figure 6. Therefore, even if us advancing condition is
satisfied, u needs additional undo moves, to make room for the undo moves of other units, such as v,
in order to reach a globally well positioned state. In this case, global checking is useful. As a second
example, imagine that u and vs moves in the most recent progression step are independent from
each other, possibly even in two map areas far away from each other. A simple case is shown on the
right of Figure 6. In the most recent progression, vs last move (when v was derailed) was followed
by a sequence of us moves. Only the final move of u pushed it off track, whereas the preceding
moves were along us -path, (u). Reverse repositioning would undo moves in the reverse global
order, which means undoing all us moves before undoing vs last move. However, only one undo
move for u and one undo move for v are sufficient to restore both units to a well positioned state.
As we just illustrated, a global checking of the advancing condition could be too strong, whereas
a local checking could be insufficient. The solution that we introduce in this section, which is
called repositioning with counting, finds a middle ground between the two extremes, improves the
number of moves and still maintains the guarantee of reaching a well-positioned state. Intuitively,
the undo moves for a unit u can stop as soon as (a) us advancing condition is satisfied, (b) its
current position cannot possibly interfere with future undo moves of other units, (c) no other unit
performing repositioning can possibly stop in the blank position in the front of u on us -path,
75

fiWANG & B OTEA

and (d) u doesnt stop in the initial second location of another active unit v. The initial second
location of a unit v is the position ahead of v at the beginning of the most recent progression step.
The fourth condition ensures that all units can have a blank in front at the end, as in the worst case
they will revert back to the initial position at the beginning of the most recent progression step.3
Definition 30. For each location l on a  or an  path, we keep a counter, c(l), such that:
 At the beginning of each progression step, the counter c(l) is reset to 0, if l is empty, and to 1,
if l is occupied.
 Every time l is visited during the progression step, c(l) is incremented.
 Every time a unit leaves l as a result of an undo move during the repositioning step, c(l) is
decremented.
Following directly from the definition of c(l) given above, we formulate the following two
results for c(l) at repositioning time:
Lemma 31. If c(l) = 0, then no unit will pass through l in the remaining part of the current
repositioning step.
Lemma 32. For a given active unit u, at its current position pos(u), if c(pos(u)) = 1, then all
progression moves through the location pos(u) have already been undone. In other words, no other
unit in the remainder of this repositioning step will pass through pos(u).
We now introduce our new enhancement for M APP, aimed at eliminating many useless undo
moves in its repositioning steps.
Definition 33. The enhanced algorithm, R C M APP, uses the repositioning with counting strategy
at line 7 of Algorithm 1. This means that each active unit u stops undoing moves in the current
repositioning step, as soon as it meets all of the following conditions:
(a) The advancing condition of u is satisfied according to Definition 2, plus the extension in Definition 23.
(b) For us current location, pos(u), c(pos(u)) = 1
u , c(lu ) = 0
(c) For the location in front of u, li+1
i+1

(d) The current location is not the initial second location of another active unit.
Theorem 34. All repositioning steps in R C M APP end in a well-positioned state.
3. Condition d) can be ignored without invalidating the algorithms ability to make progress towards the goal state. Even
if some units could possibly end up in a state without a blank in front, it is guaranteed that at least one unit (i.e., the
one that finishes repositioning first) will have a blank in front. This further guarantees that at least one unit will be
solved in the next progression step.

76

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Proof. Recall that all the moves made in a progression step are kept as a totally ordered list. We can
prove directly that repositioning with counting, by undoing a subset of those moves, reaches a wellpositioned state. Since the counter c(l) is incremented and decremented according to Definition 30,
a unit u satisfying all three conditions in Definition 33 has restored its own advancing condition.
Furthermore, the combined results from Lemmas 31 and 32 guarantee that no other units will later
get in us way, and that u is out of the way of the other units repositioning moves.

By Theorem 34, applying R C in the repositioning steps of our extended algorithm T I+AC M APP
has no negative impact on completeness.

9. Experimental Results
In this section we present our empirical evaluation of the M APP algorithm. We first point out the
impact of each newly added feature. Then we put our T I+AC+R C enhanced M APP to the test
in a comparison with existing state-of-the-art decoupled, incomplete methods. Specifically, our
benchmarks are FAR (Wang & Botea, 2008), and an extended version of Silvers (2005) W HCA *
algorithm by Sturtevant and Buro (2006), called W HCA *(w,a), which applies abstraction to the
expensive initial backward A* searches. As for M APP, these algorithms have been tested on rather
large problems, both in terms of map size and number of units. We are not aware of other programs
that scale as well as FAR and W HCA *. The strengths of these two methods are the potential ability
to find a solution quickly, but their weakness is that they cannot tell whether they would be able to
solve a given instance.
We implemented M APP from scratch and integrated it in the Hierarchical Open Graph4 (HOG)
framework. The source code for the extended W HCA * algorithm, W HCA *(w, a) (Sturtevant &
Buro, 2006), with the extra features of spatial abstraction and diagonal moves (but without a priority
system for unit replanning), was obtained from Nathan Sturtevant. The FAR algorithm was our
implementation as used in previous experiments (Wang & Botea, 2008).
Experiments were run on a data set of randomly generated instances used in previously published work (Wang & Botea, 2008). The input grid maps5 are 10 of the largest from the game Baldurs Gate6 , which range from 13765 to 51586 traversable tiles in size, as listed in Table 1. These
game maps are quite challenging, containing different configurations of obstacles forming different
shapes of rooms, corridors, and narrow tunnels. We test each map with 100 to 2000 mobile units
in increments of 100. A 10-minute timeout per instance is set. In the W HCA *(w, a) experiments,
we set the window size, w, to 8, and use the first level of abstraction (a = 1). This seems to be a
good parameter setting in the work of Sturtevant and Buro (2006), and our experiments comparing
with W HCA *(20,1) show W HCA *(8,1) to work better on this data set. Abstraction allows W HCA *
to build the heuristic on a graph that is smaller than the actual graph where the movement takes
place. In FAR, units make reservations for k = 2 steps ahead, which is the recommended setting.
All experiments were run on a 2.8 GHz Intel Core 2 Duo iMac with 2GB of RAM.
77

fiWANG & B OTEA

Basic MAPP

Number in SLIDABLE

2000

700
414
400
500
300
204
602
411
603
307

1500
1000
500
0
0

500
1000
1500
Total number of agents

2000

TI MAPP
414
400
204
500
411
300
700
602
603
307

1500
1000
500
0

AC MAPP
2000

0

Number in AC-SLIDABLE

Number in TI-SLIDABLE

2000

500
1000
1500
Total number of agents

2000

700
500
300
400
414
204
602
411
603
307

1500
1000
500
0
0

500
1000
1500
Total number of agents

2000

TI + AC MAPP
Number in TI+AC-SLIDABLE

2000

700
300
500
414
400
602
204
603
411
307

1500
1000
500
0
0

500
1000
1500
Total number of agents

2000

Figure 7: MAPPs widened completeness range after each relaxation: each graph line represents
the number of units solved for problem instances on a map. Here, only provably solvable
units are counted.

78

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

FAR

Number of agents solved

2000

414
204
400
411
700
500
300
602
603
307

1500

1000

500

0

0

500
1000
1500
Total number of agents

2000

WHCA* with diagonals

Number of agents solved

2000

400
204
414
411
700
500
300
603
602
307

1500
1000
500
0
0

500
1000
1500
Total number of agents

2000

WHCA* no diagonals

Number of agents solved

2000

204
414
400
411
700
500
300
603
602
307

1500
1000
500
0
0

500
1000
1500
Total number of agents

2000

Figure 8: The success ratios (averaged over 10 trials) of FAR, and W HCA *(8,1), with and without
diagonals, on the same set of problem instances. The timeout was set to 10 minutes per
instance for all 3 incomplete algorithms.
79

fiWANG & B OTEA

Map ID
AR0700SR
AR0500SR
AR0300SR
AR0400SR
AR0602SR
AR0414SR
AR0204SR
AR0307SR
AR0411SR
AR0603SR

Short ID
700
500
300
400
602
414
204
307
411
603

# nodes
51586
29160
26950
24945
23314
22841
15899
14901
14098
13765

Table 1: The 10 maps in descending order, in terms of number of nodes.

9.1 Scalability as Percentage of Solved Units
We compare FAR, W HCA *(8,1) and four versions of M APP: Basic M APP with the original S LID ABLE definitions; T I M APP , the version with only the target isolation relaxation switched on; AC
M APP, based on relaxing the alternate connectivity condition; and T I +AC M APP, relaxing both the
target isolation condition, and the alternate connectivity condition. We measure the success ratio,
which is defined as the percentage of solved units. Note that repositioning with counting (R C) does
not have to be considered in this section, since it has no impact on the success ratio, being designed
to improve solution length.
The M APP versions used in this section attempt to solve only units that are provably solvable
(i.e., units marked as S LIDABLE, T I S LIDABLE, AC S LIDABLE, T I+AC S LIDABLE respectively).
The reason is that we want to evaluate how many units fall in each of these subclasses in practice.
The next section will show data obtained with a version of M APP that attempts to solve all units.
Figure 7 summarizes the success ratio data from each version of the M APP algorithm on all
maps. The closer a curve is to the top diagonal line (being the total number of units), the better the
success ratio on that map. Basic M APP exhibits a mixed behaviour, having a greater success ratio on
six maps. On the four challenging maps (602, 411, 603, and 307), the success ratio gets often below
50% as the number of mobile units increases. These maps have a common feature of containing
long narrow corridors and even single-width tunnels, connecting wider, more open regions of the
map. Thus it is not surprising that, as mentioned in Section 7, the alternate path and target isolation
conditions are identified as the greatest causes for failing to find a S LIDABLE path.
Relaxing the target isolation condition (T I M APP) significantly improves the success ratio on
all maps. Now a very good success ratio (93% or higher) is achieved for 7 maps across the entire
range of the number of mobile units. The other 3 maps contain not only a high proportion of narrow
corridors, but also single-width tunnels.
Relaxing alternate connectivity as well (T I +AC M APP) yields an excellent success ratio for
all unit numbers on all maps. For example, in the scenarios with 2000 units, which are the most
4. http://webdocs.cs.ualberta.ca/nathanst/hog.html
5. Our experimental maps can be viewed online, at: http://users.cecs.anu.edu.au/cwang/gamemaps
6. http://www.bioware.com/games/baldurs_gate/

80

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

challenging according to Figures 7 and 8, the smallest success ratio is 92% (map 307) and the largest
one is 99.7%. In scenarios with fewer mobile units, T I+AC M APP has even better success ratios.
Next we compare the success ratio of T I+AC M APP (bottom plot of Figure 7) to those of FAR
(top plot of Figure 8) and W HCA *(8,1) (middle and bottom of Figure 8, with and without diagonals,
respectively). Extended M APP is the clear winner in terms of scalability. FAR and W HCA * suffer
when the number of units is increased. These incomplete algorithms often time out even in scenarios
with significantly fewer units than 2000. With 2000 units, FAR solves as few as 17.5% of units,
while W HCA * solves as few as only 16.7% (no diagonal moves) and 12.3% (with diagonal moves)
of the units. Over the entire data set, T I+AC M APP solved 98.82% of all the units, FAR solved
81.87% of units, while 77.84% and 80.87% are solved by W HCA * with and without diagonal moves
allowed, respectively.
9.2 Scalability when Attempting to Solve All Units
As in the previous section, we compare FAR, W HCA * and M APP. The T I+AC M APP version
used here attempts to solve all units, not only the provably solvable ones (attempt-all feature). As
mentioned earlier, this is achieved by marking all units as active at the beginning. Active units are
partitioned into three categories: i) provably solvable units that did not reach their target; ii) other
units that have not reached their target; and iii) units that have reached their target location, but they
are still active because other units still have to cross through that location. The total ordering < of
active units must respect the conditions that units in category i) have a higher priority than units in
category ii), which have a higher priority than units in category iii).
With the attempt-all feature turned on, T I+AC M APPs percentage of solved units increases
from 98.82% (Section 9.1) to 99.86%.
Next we focus on the number of solved instances. An instance is considered to be solved iff all
units are solved. M APP is successful in 84.5% of all instances. This is significantly better than FAR
(70.6%), W HCA * with no diagonal moves (58.3%), and W HCA * with diagonals (71.25%).
The attempt-all feature has a massive impact on the percentage of fully solved instances, improving it from 34% to 84.5%. It might seem counter-intuitive that the attempt-all feature has a
small impact on the percentage of solved units but a great impact on the percentage of solved instances. The explanation is the following. When M APP fails in an instance, it does so because of
a very small percentage of units that remain unsolved. Often, there can be as few as one or two
unsolved units in a failed instance. Managing to solve the very few remaining units as well with the
attempt-all feature will result in the whole instance changing its label from failed to solved, even
though the change in the overall percentage of solved units will be small.
The remaining sections use the attempt-all feature as well. The reason is that it increases the
number of solved instances and therefore we obtain a larger set of data to analyse.
9.3 Total Travel Distance
Factors that may impact the length of plans are the lengths of the initial  paths, and the extra movements caused by blank travel and repositioning. In our experiments, the length of the precomputed
 paths has virtually no negative impact on the travel distance. Even when M APPs  paths have to
satisfy additional constraints, such as avoiding other targets when possible, they are very similar in
length with the normal unconstrained shortest paths, being only 1.4% longer on average.
81

fiWANG & B OTEA

RC-Improved Travel Distance (Map 400)
800000

MAPP total
RC MAPP total
MAPP Pre-Computed Pi
MAPP undos
RC MAPP undos

Distance (moves)

700000
600000
500000
400000
300000
200000
100000
0
0

2

4

6 8 10 12 14 16 18 20
Number of agents (100s)

Figure 9: A typical case of improved distances of R C M APP over normal M APP. Note the precomputed -paths are not affected by the R C enhancement.

In this section, we first evaluate the improvement of repositioning with counting (R C) over
standard reverse repositioning. Then we compare the total distance travelled by R C M APP against
FAR and W HCA *.
9.3.1 R EDUCING U NDO M OVES
We identified an excessive undoing of moves in repositioning as a bottleneck in Basic M APP. Figure 9 shows the benefits of repositioning with counting (R C), the enhancement described in Section 8. The figure compares the total travelled distance, as well as the number of undo moves, of
R C+T I+AC M APP (shown as R C M APP for short) to T I+AC M APP (M APP for short) on an average case. As shown, repositioning with counting turns out to be quite effective, eliminating many
unnecessary undo moves (that do not help to reach the globally restored state). Averaged over the
entire data set, R C M APP has 59.7% shorter undo distance than M APP with the standard reverse
repositioning, which results in reducing the total travelled distance by 30.4% on average.
9.3.2 C OMPARING T OTAL D ISTANCE WITH FAR AND W HCA *(8,1)
Now we evaluate the solution length of attempt-all R C+T I+AC M APP as compared to FAR and
W HCA *(8,1). We plot the total travel distance averaged over the subset of input instances that all
algorithms considered here can fully solve.
Figures 10 and 11 show average results for all maps. For M APP, we show the length of the
precomputed  paths, the number of the undo (repositioning) moves, and the total travelled distance.
According to this performance criterion, the set of maps is roughly partitioned into three subsets.
In a good case, such as map 307, M APP performs better than W HCA *(8,1) without diagonals
in terms of total travel distance, and is even comparable to FAR. In an average case, M APPs travel
82

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Total Travel Distance: Good Case (307)
90000

WHCA*(8,1) noD
FAR
MAPP Total
WHCA*(8,1)
MAPP Pi
MAPP Undos

70000
60000
50000

250000
Distance

80000

Distance

Total Distance Travelled: Average Case (411)
300000

40000
30000

200000

WHCA* noD
MAPP Total
FAR
WHCA*(8,1)
MAPP Pi
MAPP Undos

150000
100000

20000

50000

10000
0
100

200

300

400 500 600
Number of agents

700

800

0
900

200

Total Distance Travelled (603)
160000

Distance

140000
120000
100000

WHCA* noD
MAPP Total
WHCA* D
FAR
MAPP Pi
MAPP Undos

250000

80000
60000

200000

MAPP Total
WHCA* noD
FAR
MAPP Pi
WHCA* D
MAPP Undos

150000
100000

40000

50000

20000
0
100

600 800 1000 1200 1400 1600
Number of agents

Total Distance Travelled (602)
300000

Distance

180000

400

200

300

400 500 600 700
Number of agents

800

0
900

200

400

600
800
1000
Number of agents

1200

Figure 10: Distance travelled plotted averaged over instances fully solved by all algorithms.

distance is roughly comparable to W HCA * without diagonals. Maps 603, 411, and 602 belong to
this category. Finally, in a harder case, M APPs total distance increases at a faster rate than the
others, which is a direct result of an increasingly larger number of undo moves. These harder cases
include maps 204, 414, 700, 400, 300, and 500. Upon inspection, these cases typically involve a
high number of turns and corners. In M APPs case, this results in a high degree of path overlapping,
as units keep close to the edge when rounding a corner, to obtain shorter -paths.
To summarise the overall results, M APPs travel distance ranges from 18.5% shorter than W HCA *
without diagonal moves, to at most 132% longer, being 7% longer on average. Compared to the
version of W HCA * with diagonal moves enabled, M APPs total distance is 31% longer on average,
varying from 5.8% shorter to at most 154% longer. Compared to FAR, M APPs solutions range
from 4.8% shorter to at most 153% longer, being 20% longer on average.
A closer look at the results reveals that, even with repositioning by counting in use, M APP can
still make unnecessary undo moves. Each useless undo move counts double in the final solution
length, since the undo has to be matched with a new forward move in the next progression step.
Improving the solution length further is a promising direction for future work.
9.4 Running Time Analysis
As in the case of travel distance analysis, for a meaningful runtime comparison, we also restrict the
analysis to the subset of instances completed by all algorithms (FAR, both W HCA * versions, and
83

fiWANG & B OTEA

Total Travel Distance: Harder Case (700)
300000

Distance

250000
200000

MAPP Total
WHCA*(8,1) noD
FAR
MAPP Pi
WHCA* D
MAPP Undos

150000

Total Distance Travelled (300)
400000
350000
300000
Distance

350000

250000
200000
150000

100000

100000

50000

50000

0
200

400

0
600 800 1000 1200 1400
Number of agents

MAPP Total
WHCA* noD
FAR
MAPP Pi
WHCA* D
MAPP Undos

200

MAPP Total
WHCA* noD
FAR
MAPP Pi
WHCA* D
MAPP Undos

250000
200000

MAPP Total
WHCA*(8,1) noD
FAR
MAPP Pi
WHCA*(8,1)
MAPP Undos

150000
100000
50000
0

200 400 600 800 1000 1200 1400 1600 1800 2000
Number of agents

200 400 600 800 1000 1200 1400 1600 1800 2000
Number of agents

Total Distance Travelled (414)
300000

Distance

250000
200000

MAPP Total
WHCA* noD
FAR
MAPP Pi
WHCA* D
MAPP Undos

150000

Total Distance Travelled (500)
300000
250000
Distance

350000

200000

MAPP Total
WHCA* noD
FAR
MAPP Pi
WHCA* D
MAPP Undos

150000
100000

100000

50000

50000
0

600 800 1000 1200 1400
Number of agents

Total Travel Distance (204)
300000

Distance

Distance

Total Distance Travelled (400)
500000
450000
400000
350000
300000
250000
200000
150000
100000
50000
0

400

0
200 400 600 800 1000 1200 1400 1600 1800 2000
Number of agents

200

400

600 800 1000 1200 1400
Number of agents

Figure 11: Distance travelled continued: the remaining six maps.

T I+AC+R C M APP with the attempt-all feature turned on). We show both overall summary data, in
Tables 2 and 3, and charts for all 10 maps, in Figures 12 and 13.
Our implementation of M APP builds from scratch all the required paths, including the -paths.
However, most -paths can be re-used between instances on the same map. Only -paths that
contain a target in the current instance might have to be recomputed. This is a small percentage
of all -paths, since the number of targets is typically much smaller than the map size. Such
evidence strongly supports taking the -path computations offline into a map pre-processing step
to improve M APPs running time. Hence, we distinguish between the case when M APP performs
all computations from scratch, and the case when the alternate paths (i.e.,  paths) are already
available (e.g., from previous instances on the map at hand, or as a result of preprocessing). Note
84

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Time ratio:
Average
Min
Max

vs FAR
10.14
2.90
60.46

vs W HCA *
0.96
0.08
4.57

vs W HCA *+d
0.93
0.11
4.92

Table 2: M APPs runtime divided by the runtime of FAR, W HCA *, and W HCA *+d. In this table,
we assume that M APP performs all computations, including alternate-path search, from
scratch.

Time ratio:
Average
Min
Max

vs FAR
2.18
0.56
7.00

vs W HCA *
0.21
0.01
0.99

vs W HCA *+d
0.19
0.01
0.70

Table 3: M APPs runtime divided by the runtime of FAR, W HCA *, and W HCA *+d. In this table,
the time to compute alternate paths is omitted, as they could be re-used for instances on
the same map.

that with FAR and W HCA *, most computation depends on every units start and target locations,
and therefore cannot easily be taken into a map pre-processing step (since storing entire search trees
take up too much memory to be practical).
Table 2 shows that, when M APP performs all computations from scratch, it is comparable in
speed with W HCA *, being actually slightly faster on average. However, this version of M APP is
about 10 times slower than FAR on average. When  paths are already available, M APPs speed
improves significantly, as -path computation is the most expensive part of M APP. As can be seen
in Table 3, M APPs speed ratio vs FAR reduces to 2.18. M APP also becomes 4.85.2 times faster
than W HCA *(with and without diagonals) on average.
Figure 12 shows more detailed runtime data for 8 out of 10 maps. Even when it does all computation from scratch, M APP is faster than W HCA *+d (i.e., with diagonal moves enabled). It is also
often faster, or at least comparable, with W HCA * without diagonals. M APP with offline preprocessing is reasonably close to FAR, even though FAR is consistently faster or at least comparable to
M APP. The remaining two maps, which represent the most difficult cases for M APP, are presented
in Figure 13. On map 700 especially, the largest in our data set, which is also significantly larger
than the rest (almost 24 times larger), M APP has significantly higher total time, as shown at the top
right of Figure 13.
A break down of M APPs total running time (shown at the bottom of Figure 13 for map 700)
consistently shows that the search time dominates. Furthermore, in node expansions, the  node
expansions are generally several times greater than  node expansions, resulting in the majority of
path computation time being spent searching for -paths.
85

fiWANG & B OTEA

Total Running Times (411)
400

WHCA*(8,1) noD
WHCA*(8,1)
MAPP total
MAPP with preprocessing
FAR

300
250

120
100
Time (s)

350

Time (s)

Total Running Times (307)
140

200
150

80
60
40

100

20

50
0

WHCA*(8,1) noD
WHCA*(8,1)
MAPP total
MAPP with preprocessing
FAR

200

400

0
100

600 800 1000 1200 1400 1600
Number of agents

200

400

900

300

100
700

800

0
900

200 400 600 800 1000 1200 1400 1600 1800 2000
Number of agents
Total Running Times (602)
400

WHCA*(8,1) noD
WHCA*(8,1)
MAPP total
MAPP with preprocessing
FAR

350
300
Time (s)

Time (s)

400

800

200

WHCA*(8,1)
WHCA*(8,1) noD
MAPP total
MAPP with preprocessing
FAR

500

700

WHCA*(8,1)
MAPP total
WHCA*(8,1) noD
MAPP with preprocessing
FAR

500

Total Running Times (414)
600

400 500 600
Number of agents

Total Running Times (204)
600

Time (s)

Time (s)

Total Running Times (603)
500
WHCA*(8,1) noD
450
WHCA*(8,1)
MAPP total
400
350 MAPP with preprocessing
FAR
300
250
200
150
100
50
0
100 200 300 400 500 600
Number of agents

300

300
200

250
200
150
100

100
0

50
0
200 400 600 800 1000 1200 1400 1600 1800 2000
Number of agents
Total Running Times (300)

300

400
350
300
Time (s)

Time (s)

200
150
100

600
800
Number of agents

1000

1200

MAPP total
WHCA*(8,1) noD
WHCA*(8,1)
MAPP with preprocessing
FAR

250
200
150
100

50
0

400

Total Running Times (400)
450

WHCA*(8,1) noD
WHCA*(8,1)
MAPP total
MAPP with preprocessing
FAR

250

200

50
200

400

600
800 1000
Number of agents

1200

0
1400

200 400 600 800 1000 1200 1400 1600 1800 2000
Number of agents

Figure 12: Runtime data averaged over fully completed instances by all algorithms. Map IDs are
displayed in shorthand in brackets. M APP with preprocessing stands for the version
that computes no alternate paths.

86

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Total Running Times (500)
140

Time (s)

120
100

Total Running Times: Worst Case (700)
350

WHCA*(8,1)
MAPP total
WHCA*(8,1) noD
MAPP with preprocessing
FAR

MAPP total
WHCA*(8,1)
WHCA*(8,1) noD
250 MAPP with preprocessing
FAR
200
300

Time (s)

160

80
60

150

40

100

20

50

0
200

400

600
800 1000
Number of agents

1200

0
1400

200

400

600
800 1000
Number of agents

1200

1400

MAPP Times Breakdown (700)
350

Total Runtime
Total Search
Omega Search
Repositioning

300

Time (s)

250
200
150
100
50
0
200

400

600
800 1000
Number of agents

1200

1400

Figure 13: Top: hard cases for T I+AC+R C M APPs total runtime. Bottom: time breakdown, showing that the -path computation takes up the majority of M APPs search time.

10. Conclusion
Traditional multi-agent path planning methods trade off between optimality, completeness, and scalability. While a centralised method typically preserves optimality and (theoretical) completeness, a
decentralised method can achieve significantly greater scalability and efficiency. On the other hand,
both approaches have shortcomings. The former faces an exponentially growing state space in the
number of units. The latter gives up optimality and offers no guarantees with respect to completeness, running time and solution length. Our new approach, aimed at bridging these missing links,
identifies classes of multi-agent path planning problems that can be solved in polynomial time. We
also introduced an algorithm, M APP, to solve problems in these classes, with low polynomial upper
bounds for time, space and solution length.
We performed a detailed empirical evaluation of M APP. The extended M APPs completeness
range reaches 92%99.7%, even on the most challenging scenarios with 2000 mobile units. The
completeness range is even better in scenarios with fewer units. On our data set, M APP has a significantly better percentage of solved units (98.82% provably solvable, and 99.86% in the attempt-all
mode) than FAR (81.87%) and W HCA * (77.84% and 80.87%, with and without diagonal moves).
The attempt-all version of M APP solves 1326% more instances than the benchmark algorithms.
87

fiWANG & B OTEA

On instances solved by all algorithms, M APP is significantly faster than both variants of W HCA *,
and slower than the very fast FAR algorithm by a factor of 2.18 on average, when all alternate
paths needed in an instance are readily available. When performing all computations from scratch,
M APPs speed is comparable to W HCA *. M APPs solutions reported here are on average 20% longer
than FARs solutions and 731% longer than W HCA *s solutions. However, unlike algorithms such
as FAR and W HCA *, M APP does offer partial completeness guarantees and low-polynomial bounds
for runtime, memory and solution length. Thus, M APP combines strengths from two traditional
approaches, providing formal completeness and upper-bound guarantees, as well as being scalable
and efficient in practice.
Our findings presented here open up avenues for future research into large-scale multi-agent
pathfinding. In the long term, M APP can be part of an algorithm portfolio, since we can cheaply
detect when it is guaranteed to solve an instance. Thus it is worthwhile to investigate other tractable
classes, such as subclasses where FAR is complete. M APP can further be improved to run faster,
compute better solutions, and cover more instances. Solution quality can be measured not only as
total travel distance, but also in terms of makespan (i.e., total duration when actions can be run in
parallel) and total number of actions (including move and wait actions). So far, we have worked on
relaxing two of the original S LIDABLE conditions: target isolation and alternate connectivity. Future
work could address the initial blank condition. Moreover, some of the initially non-S LIDABLE units
in the problem could become S LIDABLE later on, as other S LIDABLE units are getting solved.
Extending M APP to instances where units are heterogeneous in size and speed is another promising
direction.

Acknowledgments
NICTA is funded by the Australian Governments Department of Communications, Information
Technology, and the Arts and the Australian Research Council through Backing Australias Ability
and the ICT Research Centre of Excellence programs.
Many thanks to Nathan Sturtevant for providing the HOG framework, and his help with our
understanding of the program. Thanks also to Philip Kilby, Jussi Rintanen, and Nick Hay for their
many helpful comments. We thank the anonymous reviewers for their valuable feedback.

References
Barraquand, J., Langlois, B., & Latombe, J.-C. (1991). Numerical potential field techniques for
robot path planning. In International Conference on Advanced Robotics (ICAR), Vol. 2, pp.
10121017.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2007). Graph Abstraction in Real-time Heuristic
Search. Journal of Artificial Intelligence Research (JAIR), 30, 51100.
Choset, H., Lynch, K., Hutchinson, S., Kantor, G., Burgard, W., Kavaraki, L., & Thrun, S. (2005).
Principles of Robot Motion: Theory, Algorithms, and Implementation. The MIT Press.
Erdmann, M., & Lozano-Perez, T. (1986). On Multiple Moving Objects. In IEEE International
Conference on Robotics and Automation (ICRA), pp. 14191424.
Geramifard, A., Chubak, P., & Bulitko, V. (2006). Biased Cost Pathfinding. In Artificial Intelligence
and Interactive Digital Entertainment conference (AIIDE), pp. 112114.
88

fiM APP : S CALABLE M ULTI -AGENT PATH P LANNING

Hart, P., Nilsson, N., & Raphael, B. (1968). A Formal Basis for the Heuristic Determination of
Minimum Cost Paths. IEEE Transactions on Systems Science and Cybernetics, 4(2), 100
107.
Hopcroft, J. E., Schwartz, J. T., & Sharir, M. (1984). On the complexity of motion planning for
multiple independent objects: PSPACE-hardness of the warehousemans problem. International Journal of Robotics Research (IJRR), 3(4), 7688.
Jansen, R., & Sturtevant, N. (2008). A New Approach to Cooperative Pathfinding. In International
Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 14011404.
Kant, K., & Zucker, S. W. (1986). Toward Efficient Trajectory Planning: The Path-Velocity Decomposition. International Journal of Robotics Research (IJRR), 5(3), 7289.
Kornhauser, D., Miller, G., & Spirakis, P. (1984). Coordinating pebble motion on graphs, the diameter of permutation groups, and applications. In Proceedings of the 25th Annual Symposium
on Foundations of Computer Science (FOCS), pp. 241250.
Latombe, J.-C. (1991). Robot Motion Planning. Kluwer Academic Publishers.
Papadimitriou, C., Raghavan, P., Sudan, M., & Tamaki, H. (1994). Motion planning on a graph. In
35th Annual Symposium on Foundations of Computer Science, pp. 511520.
Pottinger, D. (1999). Coordinated Unit Movement. http://www.gamasutra.com/view/
feature/3313/coordinated_unit_movement.php.
Rabin, S. (2000). A* Speed Optimizations. In Deloura, M. (Ed.), Game Programming Gems, pp.
272287. Charles River Media.
Ratner, D., & Warmuth, M. (1986). Finding a shortest solution for the N  N extension of the 15puzzle is intractable. In Proceedings of AAAI National Conference on Artificial Intelligence
(AAAI-86), pp. 168172.
Ryan, M. R. K. (2008). Exploiting Subgraph Structure in Multi-Robot Path Planning. Journal of
Artificial Intelligence Research (JAIR), 31, 497542.
Samet, H. (1988). An Overview of Quadtrees, Octrees, and Related Hierarchical Data Structures.
NATO ASI Series, Vol. F40.
Silver, D. (2005). Cooperative Pathfinding. In Artificial Intelligence and Interactive Digital Entertainment conference (AIIDE), pp. 117122.
Silver, D. (2006). Cooperative pathfinding. AI Programming Wisdom, 3, 99111.
Standley, T. (2010). Finding Optimal Solutions to Cooperative Pathfinding Problems. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10), pp. 173178.
Stout, B. (1996). Smart Moves: Intelligent Pathfinding. Game Developer Magazine.
Sturtevant, N. R., & Buro, M. (2006). Improving collaborative pathfinding using map abstraction..
In Artificial Intelligence and Interactive Digital Entertainment (AIIDE), pp. 8085.
Surynek, P. (2009a). An Application of Pebble Motion on Graphs to Abstract Multi-robot Path Planning. In Proceedings of the 21st International Conference on Tools with Artificial Intelligence
(ICTAI), pp. 151158.
Surynek, P. (2009b). A novel approach to path planning for multiple robots in bi-connected graphs.
In IEEE International Conference on Robotics and Automation (ICRA), pp. 36133619.
89

fiWANG & B OTEA

Surynek, P. (2010a) personal communication.
Surynek, P. (2010b). An Optimization Variant of Multi-Robot Path Planning is Intractable. In
Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI-10), pp. 1261
1263.
Surynek, P. (2010c). Multi-robot Path Planning, pp. 267290. InTech - Open Access Publisher.
Tarjan, R. E. (1976). Edge-disjoint spanning trees and depth-first search. Acta Informatica, 6(2),
171185.
Tozour, P. (2002). Building a Near-Optimal Navigation Mesh. In Rabin, S. (Ed.), AI Game Programming Wisdom, pp. 171185. Charles River Media.
Wang, K.-H. C., & Botea, A. (2008). Fast and Memory-Efficient Multi-Agent Pathfinding. In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),
pp. 380387.
Wang, K.-H. C., & Botea, A. (2009). Tractable Multi-Agent Path Planning on Grid Maps. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 1870
1875.
Wang, K.-H. C., & Botea, A. (2010). Scalable Multi-Agent Pathfinding on Grid Maps with
Tractability and Completeness Guarantees. In Proceedings of the European Conference on
Artificial Intelligence (ECAI), pp. 977978.
Zelinsky, A. (1992). A mobile robot navigation exploration algorithm. IEEE Transactions of
Robotics and Automation, 8(6), 707717.

90

fiJournal of Artificial Intelligence Research 42 (2011) 917-943

Submitted 03/11; published 12/11

Interpolable Formulas in Equilibrium Logic and
Answer Set Programming
Dov Gabbay

dov.gabbay@kcl.ac.uk

Bar Ilan University Israel, Kings College London and
University of Luxembourg.

David Pearce

david.pearce@upm.es

AI Dept, Universidad Politecnica de Madrid, Spain.

Agustn Valverde

a valverde@ctima.uma.es

Dept of Applied Mathematics, Universidad de Malaga, Spain.

Abstract
Interpolation is an important property of classical and many non-classical logics that
has been shown to have interesting applications in computer science and AI. Here we study
the Interpolation Property for the the non-monotonic system of equilibrium logic, establishing weaker or stronger forms of interpolation depending on the precise interpretation
of the inference relation. These results also yield a form of interpolation for ground logic
programs under the answer sets semantics. For disjunctive logic programs we also study
the property of uniform interpolation that is closely related to the concept of variable forgetting. The first-order version of equilibrium logic has analogous Interpolation properties
whenever the collection of equilibrium models is (first-order) definable. Since this is the
case for so-called safe programs and theories, it applies to the usual situations that arise
in practical answer set programming.

1. Introduction
The interpolation property is an important and much discussed topic in logical systems,
both classical and non-classical (Gabbay & Maksimova, 2005). Its importance in computer
science is also becoming recognised nowadays. The interpolation property has been applied
in various areas of computer science, for example in software specification (Diaconescu,
Goguen, & Stefaneas, 1993; Bicarregui, Dimitrakos, Gabbay, & Maibaum, 2001), in the
construction of formal ontologies (Kontchakov, Wolter, & Zakharyaschev, 2008) and in
model checking and related subareas (McMillan, 2005). In the first two areas interpolation
is important as a metatheoretical property, in particular it may provide a basis for the modular composition and decomposition of theories; for instance, for Kontchakov et al. (2008)
it plays a key role in the study of the modular decomposition of ontologies. In other
cases, interpolants themselves play a role as special formulas applied in automated deduction (McMillan, 2005).
To date interpolation has received much less attention in systems of non-monotonic
reasoning and logic programming, despite their importance in AI and computer science. In
this note we study the interpolation property for the system of nonmonotonic reasoning
known as equilibrium logic (Pearce, 2006). Since this in turn can be regarded as a logical
foundation for stable model reasoning and answer set programming (ASP), our results
transfer immediately to the sphere of ASP. We shall focus here mainly on interpolation as a
c
2011
AI Access Foundation. All rights reserved.

fiGabbay, Pearce, & Valverde

metatheoretical property and our primary interest is in establishing this property for certain
cases of interest. Although in Section 8 we consider a case where an interpolant (actually
a uniform interpolant) is explicitly constructed, we are mainly concerned here with pure
existence theorems. Discussion of complexity issues as well as possible applications of the
interpolation property in ASP are left to future work. However, it seems likely that, as in the
case of studies involving formal ontologies (Konev, Walther, & Wolter, 2009), interpolation
may be a useful property for applications of ASP in knowledge representation. In a previous
paper (Pearce & Valverde, 2012), the interpolation and Beth properties of the underlying,
monotonic logic of ASP were used to characterise strong kinds of intertheory relations. To
capture weaker kinds of intertheory relations it may be important to be able to rely on
interpolation holding in the extended, non-monotonic logic. We plan to explore this avenue
in the future.
To introduce the property of interpolation, let us start with some notation and terminology. Let us assume the syntax of first-order logic with formulas denoted by lower case
Greek letters and predicates by lower case Latin letters.
Let  be a monotonic inference relation and suppose that   . An interpolant
for (, ) is a formula  such that
 & 

(1)

where  contains only predicate and constant symbols that belong to both  and . A logic L
with inference relation L is said to have the interpolation property if an interpolant exists
for every pair of formulas (, ) such that  L . As is well-known, classical logic as well
as many non-classical logics possess interpolation.
Suppose now we deal with a non-monotonic logical system with an inference relation |.
To express the idea that a formula is an interpolant it will not generally suffice simply to
replace  by | in (1). One problem is that, since | is non-monotonic, it is in general not
transitive. Instead, following the idea of Gabbay and Maksimova (2005), we can modify
condition (1) and proceed in a two-stage fashion. We make use of the fact that nonmonotonic consequence can be defined in terms of minimal models in some monotonic
logical system, say that the consequence relation | is appropriately captured by means of
minimal models in a logic L with consequence relation |=L . Now suppose that  | . Then
as an interpolant for (, ) we look for a formula  such that
 |  &  |=L 

(2)

where all predicate and constant symbols of  occur in both  and . Since | is to be
defined via a subclass of minimal L-models, we already assume that |=L  |. Moreover
we should assume too that L is a well-behaved sublogic in the sense that L-equivalent
formulas have the same |-consequences and that the L-consequences of |-consequences
are themselves |-consequences (so e.g. from (2) we can derive  | ). In non-monotonic
reasoning these last two properties are known as left and right absorption, respectively.
Given these conditions, it follows at once from (2) that any formula in the language of 
that is L-equivalent to  will also be an interpolant for (, ). Likewise if  is an interpolant
for (, ) and  |=L  then  |  and  is an interpolant for (, ).
Now, to find an interpolant for (, ) satisfying (2), or to prove that one always exists,
we can proceed as follows. We look for an L-formula  say, that precisely L-defines the
918

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

minimal models of . Since  |  it follows that  |=L . Now, if L has the interpolation
property as defined earlier, we apply this theorem to obtain or infer the existence of an
L-interpolant  in the sense of (1) for ( , ). Hence (2) follows.
Notice that this two-stage procedure relies on three key features: (i) that we can identify
a suitable monotonic sublogic L for |, (ii) that a formulas minimal models are L-definable,
and (iii) that L has the interpolation property. These conditions are prima facie independent. As we shall see, we may have (i) and (iii) but lack (ii). The situation with respect
to equilibrium logic is as follows. In the propositional case all three conditions are met, so
we can establish the interpolation property in the general case. The situation for quantified
equilibrium logic is more complicated. In the general case, we lack condition (ii). More
precisely, we have an appropriate monotonic sublogic L and this logic has the interpolation
property, but the equilibrium models of a formula need not be first-order definable in L.
So the procedure outlined does not yield interpolants in all cases. However some recent results on a generalised concept of (first-order) stable model imply that there are interesting
classes of interpolable formulas: we shall consider in more detail one such class, that of safe
formulas. In particular, if  is a safe formula and  | , then there exists an interpolant 
such that (2) holds. Other classes of interpolable formulas are so-called tight formulas, and
formulas possessing a finite, complete set of what are called loops.
Safety, tightness and loop formulas have been studied at some length in answer set programming (ASP). The implications of these results for ASP can be summarised as follows.
In the case of (finite) ground programs the interpolation property holds. In the first-order or
non-ground case, interpolation holds for finite, safe programs without function symbols, and
hence practically for all finite programs currently admitted by answer set solvers. Moreover,
since safety is now defined for arbitrary formulas in a function-free language, the class of
safe formulas in this sense goes beyond the class of expressions normally admitted in ASP,
even if auxiliary concepts like weight constraints and aggregates are included.

2. Logical Preliminaries
We work with standard propositional and predicate languages, where the latter may in the
general case contain constant and function symbols. Propositional languages are based on
a set V of propositional variables, and formulas are built-up in the usual way using the
logical constants , , , , standing respectively for conjunction, disjunction, implication
and negation. If  is a propositional formula, we denote by V () the set of propositional
variables appearing in .
A first-order language L = hC, F, P i consists of a set of constants C, function symbols
F and predicate symbols P ; each function symbol f in F and predicate symbol p  P
has an assigned arity. Moreover, we assume a fixed countably infinite set of variables, the
symbols , , , , , , and auxiliary parentheses (, ). Atoms, terms and
formulas are constructed as usual; closed formulas, or sentences, are those where each
variable is bound by some quantifier. If  is a (first-order) formula, L() denotes the
language associated with , i.e. the set of constants, function and predicate symbols occuring
in it.
We make use of the following notation and terminology. Boldface x stands for a tuple
of variables, x = (x1 , . . . , xn ), while (x) = (x1 , . . . , xn ) is a formula whose free variables
919

fiGabbay, Pearce, & Valverde

are x1 ,. . . , xn , and x = x1 . . . xn . If ti are terms, then t = (t1 , . . . , tn ) denotes a vector
of terms. A theory  is a set of sentences. Variable-free terms, atoms, formulas, or theories
are also called ground.
As usual the symbols  and |=, possibly with subscripts, are used to denote logical
inference and consequence relations, respectively. A logic L is said to be monotonic if its
inference relation L satisfies the monotonicity property:
 L  &      L 
To distinguish non-monotonic from monotonic inference relations, we use | to symbolise
the former. In most cases a non-monotonic logic can be understood in terms of an inference
relation that extends a suitable monotonic logic. When this extension is well-behaved we
say that the monotonic logic forms a deductive base 1 (Pearce, 2006) for it. This can be
made precise as follows.
Definition 1 Let | be any nonmonotonic inference relation. We say that a logic L with
monotonic inference relation L is a deductive base for | iff (i) L  |; (ii) If 1 L 2
then 1  2 ; (iii) If  |  and  L , then  | .
Here L denotes ordinary logical equivalence in L, while  denotes non-monotonic equivalence, i.e. 1  2 means that 1 and 2 have the same non-monotonic consequences.
Furthermore, we say that a deductive base is strong if it satisfies the additional condition:
1 6L 2 

there exists  such that 1   6 2  .

In terms of nonmonotonic consequence operations, (ii) and (iii) correspond to conditions
known as left absorption and right absorption respectively (Makinson, 1994).
2.1 Interpolation
We now turn to the interpolation property.
Definition 2 A logic L with inference relation L is said to have the interpolation property
if whenever
L   
there exists a sentence  (the interpolant) such that
L    & L   
where all predicate, function and constant symbols of  are contained in both  and ,
i.e. L()  L()  L(). In the case of propositional logic, the requirement is that V () 
V ()  V ().
As explained in the introduction, for non-monotonic logics we can consider two forms
of interpolation, one weaker one stronger. The stronger form makes use of an underlying
monotonic logic.
1. It is close to the concept of fully absorbing inferential frame used by Dietrich (1994).

920

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

Definition 3 Suppose that  | . A (|, L ) interpolant for (, ) is a formula  such that
 |  &  L 

(3)

where L is a deductive base for | and  contains only predicate, function and constant
symbols that belong to both  and . A non-monotonic logic with inference relation |
is said to have the (|, ) interpolation property if for a suitable deductive base logic L
an (|, L ) interpolant exists for every pair of formulas (, ) such that  | .
The requirement that L form a deductive base ensures that some desirable properties of
interpolation are met.
Proposition 1 Let  be a (|, L ) interpolant for (, ).
(a) For any  such that  L ,  is a (|, L ) interpolant for (, ).
(b) For any  such that  L  , and any   such that  L   ,  is a (|, L )
interpolant for ( ,   ).
The property of deductive base also guarantees that the (|, L ) relation is transitive in the
sense that if (3) holds for any , , , then also | . This last property will not necessarily
hold for the second, weaker form of interpolation that we call (|, |) interpolation.
Definition 4 Suppose that  | . A (|, |) interpolant for (, ) is a formula  such that
 |  &  | 

(4)

where  contains only predicate, function and constant symbols that belong to both  and .
In the case of propositional logic, the requirement is that V ()  V ()  V ().
Analogous to the previous case, we say that a non-monotonic logic with inference relation | has the (|, |) interpolation property if a (|, |) interpolant exists for every pair of
formulas (, ) such that  | . Notice that (|, ) is the stronger form of interpolation
because if a logic has (|, ) interpolation it must also have (|, |) interpolation, again as
a consequence of the deductive base requirement (first clause).
Evidently the properties expressed in Proposition 1 are not directly applicable to the
second form of interpolation that does not refer to any underlying base logic. Nevertheless
an important feature of the interpolation properties we shall establish below is that we can
formulate and prove analogous properties even for (|, |) interpolation.
We can also consider restricted variants of interpolation when the property holds for
certain types of formulas, in other words, when there is an interpolant for (, ) given
 |  whenever  and  belong to specific syntactic classes. In such cases we can refer to
interpolable formulas. Later on we shall consider both kinds of restrictions, where  belongs
to a specific class or alternatively when  does.
2.2 Review of the Logic of Here-and-There
Equilibrium logic is based on the nonclassical logic of here-and-there, which we denote by
HT in the propositional case. In the quantified or first-order case we denote the logic by
QHT, with subscripts/superscripts to denote specific variants.
921

fiGabbay, Pearce, & Valverde

In both propositional and quantified cases the logic is based on the axioms and rules of
intuitionistic logic and is captured by the usual Kripke semantics for intuitionistic logic (van
Dalen, 1997). However the additional axioms of HT and QHT mean that we can use very
simple kinds of Kripke structures. In the first-order case we regard these structures as
sets of atoms built over arbitrary non-empty domains D; we denote by At(D, F, P ) the set
of atomic sentences of hD, F, P i (if D = C, we obtain the set of atomic sentence of the
language L = hC, F, P i);2 and we denote by T (D, F ) the set of ground terms of hD, F, P i.
If L = hC, F, P i and L = hC  , F  , P  i, we write L  L if C  C  , F  F  and P  P  .
By an L-interpretation over a set D we mean a subset of At(D, F, P ). A classical Lstructure can be regarded as a tuple I = h(D, I), I  i where I  is an L-interpretation over D
and I : T (C  D, F )  D, called the assignment, verifies that I(d) = d for all d  D and is
recursively defined.3 If D = T (C, F ) and I = id, I is known as an Herbrand structure. On
the other hand, a here-and-there L-structure with static domains, or QHTs (L)-structure,
is a tuple I = h(D, I), I h , I t i where h(D, I), I h i and h(D, I), I t i are classical L-structures
such that I h  I t .
Thus we can think of a here-and-there structure I as similar to a first-order classical
model, but having two parts, or components, h and t that correspond to two different points
or worlds, here and there, in the sense of Kripke semantics for intuitionistic logic, where
the worlds are ordered by h  t. At each world w  {h, t} one verifies a set of atoms I w
in the expanded language for the domain D. We call the model static, since, in contrast to
say intuitionistic logic, the same domain serves each of the worlds. Since h < t, whatever is
verified at h remains true at t. The satisfaction relation for I is defined so as to reflect the
two different components, so we write I, w |=  to denote that  is true in I with respect to
the w component. Although we only need to define the satisfaction relation in L = hC, P i,
the recursive definition forces us to consider formulas from hC  D, F, P i. In particular,
if p(t1 , . . . , tn )  At(C  D, F, P ) then I, w |= p(t1 , . . . , tn ) iff p(I(t1 ), . . . , I(tn ))  I w for
every t1 , . . . , tn  T (C  D, F ). Then |= is extended recursively as follows4 :
 I, w |=    iff I, w |=  and I, w |= .
 I, w |=    iff I, w |=  or I, w |= .
 I, t |=    iff I, t 6|=  or I, t |= .
 I, h |=    iff I, t |=    and I, h 6|=  or I, h |= .
 I, w |=  iff I, t 6|= .
 I, t |= x(x) iff I, t |= (d) for all d  D.
 I, h |= x(x) iff I, t |= x(x) and I, h |= (d) for all d  D.
 I, w |= x(x) iff I, w |= (d) for some d  D.
2. We can think of the objects in D as additional constants; this approach allow us to use a simplified
notation where the objects are not distinguished from their names.
3. That is, for every a  C, I(a)  D and for every f  F with arity n, a mapping f I : Dn  D is defined;
so the recursive definition is given by I(f (t1 , . . . , tn )) = f I (I(t1 ), . . . , I(tn )).
4. The following corresponds to the usual Kripke semantics for intuitionistic logic given our assumptions
about the two worlds h and t and the single domain D,

922

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

Truth of a sentence in a model is defined as follows: I |=  iff I, w |=  for each
w  {h, t}. A sentence  is valid if it is true in all models, denoted by |= . A sentence  is
a consequence of a set of sentences , denoted  |= , if every model of  is a model of .
The resulting logic is called Quantified Here-and-There Logic with static domains (Lifschitz, Pearce, & Valverde, 2007) denoted by QHTs . In terms of satisfiability and validity
this logic is equivalent to the logic introduced by Pearce and Valverde (2005).
A complete axiomatisation of QHTs can be obtained as follows (Lifschitz et al., 2007).
We take the axioms and rules of first-order intuitionistic logic and add the axiom of Hosoi
  (  (  ))

(5)

which determines 2-element here-and-there models in the propositional case, together with
the axiom:
x((x)  x(x)).
.
We also consider the equality predicate, =6 P , interpreted by the following condition for
every w  {h, t}
.
 I, w |= t1 = t2 iff I(t1 ) = I(t2 ).
To obtain a complete axiomatisation, we then need to add the axiom of decidible equality
.
.
xy(x = y  x =
6 y).
We denote the resulting logic by QHTs= (Lifschitz et al., 2007) and its inference relation
by . By compactness a strong form of completeness can be established such that  |= 
if and only if   .
In the context of logic programs, the following assumptions often play a role. In the case
of both classical and QHTs= models, we say that the parameter names assumption (PNA)
applies in case I|T (C,F ) is surjective, i.e. there are no unnamed individuals in D; the unique
names assumption (UNA) applies in case I|T (C,F ) is injective; in case both the PNA and
UNA apply, the standard names assumption (SNA) applies, i.e. I|T (C,F ) is a bijection.
As usual in first order logic, satisfiability and validity are independent of the signature.
If I = h(D, I), I h , I t i is an L -structure and L  L, we denote by I|L the restriction of I
to the sublanguage L:
I|L = h(D, I|L ), I h |L , I t |L i
Proposition 2 Suppose that L  L,  is a theory in L and M is an L -model of . Then
M|L is a L-model of .
Proposition 3 Suppose that L  L and   L. Then  is valid (resp. satisfiable) in
QHTs= (L) if and only if is valid (resp. satisfiable) in QHTs= (L ).
This proposition allows us to omit reference to the signature in the logic so it can be
denoted simply by QHTs= .
To simplify notation we also symbolise a QHTs= structure I = h(D, I), I h , I t i by
hU, H, T i, where U = (D, I) is the universe, and H, T respectively are the sets of atoms
I h , I t . In the case of propositional HT logic, Kripke structures can be regarded as pairs
hH, T i of set of atoms in the obvious way. A (strongly) complete axiomatisation for HT is
obtained from intuitionistic logic by adding just the Hosoi axiom (5).
923

fiGabbay, Pearce, & Valverde

2.3 Interpolation in the Logic of Here-and-There
An important and useful property of HT is the fact that it is the strongest propositional
intermediate logic (i.e. strengthening of intuitionistic logic) that is properly contained in
classical logic. Moreover it in turn properly contains all other such intermediate logics. In
addition HT is one of precisely seven superintuitionistic propositional logics possessing the
interpolation property (Maksimova, 1977; Gabbay & Maksimova, 2005).
For languages without function symbols Ono showed that interpolation holds in the
logic QHTs of quantified here-and-there with constant domains (Ono, 1983).5 In addition,
Maksimova (1997, 1998) showed that adding pure equality axioms, e.g. the decidible equality axiom, to any superintuitionistic logic preserves the interpolation property (Gabbay &
Maksimova, 2005). We conclude therefore
Proposition 4 The logic QHTs= possesses the interpolation property.
Note that by the strong completeness theorem for QHTs= we can work equivalently with 
or with |=.
Here we can make the further observation that interpolation continues to hold for languages that include function symbols. This can be established using the following property.
Proposition 5 For every formula , it is possible to build a formula , such that   ,
and the atoms of  are of one of the following types:
.
 x = a for some a  C,
.
 f (x1 , . . . xn ) = y for some f  F (where every xi and y are variables),
 p(t1 , . . . , tm ) (where every xi and y are variables).
Theorem 1 Let L be a language containing function symbols. Then QHTs= (L) has the
interpolation property.
Proof sketch: Let us assume that    ; from the previous proposition, we can assume,
without loss of generality, that the function symbols in  and  are in atoms of type
.
f (x1 , . . . xn ) = y. Now, we consider a language L obtained from L by replacing every
function symbol f by a fresh predicate symbol, Pf , such that the Arity(Pf ) = 1 + Arity(f ).
Let  and   be formulas in L build from  and  respectivelly, by replacing every atom
.
f (x1 , . . . xn ) = y by Pf (x1 , . . . xn , y). Trivially,      and, for the interpolation property
of QHTs= (L ), there exists an interpolant   :      ,       . If we replace in   the
.
predicates Pf (t1 , . . . , tn , tn+1 ) by atoms f (t1 , . . . tn ) = tn+1 we obtain the interpolant  for
the initial pair of formulas. 
5. Onos axiomatisation of QHTs uses the constant domains axiom x((x)  )  (x(x)  ), as well as
alternative axioms for propositional here-and-there, viz. p  (p  (q  q)) and (p  q)  (q  p)  (p 
q). However, the axioms given here are equivalent to Onos.

924

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

2.4 Equilibrium Logic
Equilibrium logic is a non-monotonic logic based on certain kinds of minimal models in
QHTs= or HT. We give the definition for QHTs= ; the propositional version is easily
obtained from it.
Definition 5 Among quantified here-and-there structures we define the order E as follows:
h(D, I), I h , I t i E h(D  , J), J h , J t i

if

D = D  , I = J, I t = J t , I h  J h .

If the subset relation holds strictly, we write .
Definition 6 (Equilibrium model) Let  be a theory and I = h(D, I), I h , I t i a model
of .
1. I is said to be total if I h = I t .
2. I is said to be an equilibrium model of  (or short, we say: I is in equilibrium) if
it is minimal under E among models of , and it is total.
In other words, equilibrium models are total models for which there is no smaller non-total
model. Evidently a total QHTs= model of a theory  can be equivalently regarded as a
classical first order model of ; and in what follows we make tacit use of this equivalence.
In the propositional case, equilibrium models are defined in the same way, where now the
ordering is between propositional HT models. In the usual way a formula or theory is said
to be consistent if it has a QHTs= model and additionally we say that it is coherent if it
has an equilibrium model.
The following definition give a preliminary notion of equilibrium entailment, which
agrees with standard versions of equilibrium logic (Pearce, 2006).
Definition 7 The relation |, called equilibrium entailment, is defined as follows. Let 
be a set of formulas.
1. If  is non-empty and coherent (has equilibrium models), then  |  if every equilibrium model of  is a model of  in QHTs= (respectively HT).
2. If either  is empty or has no equilibrium models, then  |  if   .
Notice that unless we need to distinguish propositional from first-order reasoning we use
the symbols |,  and |= for either version.
A few words may help to explain the concept of equilibrium entailment. First, we
define the basic notion of entailment as truth in every intended (equilibrium) model. In
nonmonotonic reasoning this is a common approach and sometimes called a skeptical or
cautious notion of entailment or inference; its counterpart brave reasoning being defined
via truth in some intended model. Since equilibrium logic is intended to provide a logical
foundation for the answer set semantics of logic programs, the cautious variant of entailment
is the natural one to choose: the standard consequence relation associated with answer
sets is given by truth in all answer sets of a program. Note however that in ASP as a
programming paradigm each answer set may correspond to a particular solution of the
problem being modelled and is therefore of interest in its own right.
925

fiGabbay, Pearce, & Valverde

Secondly, it is useful to have a nonmonotonic consequence or entailment relation that is
non-trivially defined for all consistent theories. As we shall see below, however, not all such
theories possess equilibrium models. For such cases it is natural to use monotonic consequence as the entailment relation. In particular in the propositional case HT is a maximal
logic with the property that logically equivalent theories have the same equilibrium models.
Evidently situation 2 also handles correctly the cases that  is empty or inconsistent.
Despite these qualifications, there remains an ambiguity in the concept of equilibrium
entailment that we now need to settle. Suppose that L  L,  is a theory in L and  is a
sentence in L (i.e. L = L()). How should we understand the expression  |  ?
Evidently, if we fix a language in advance, say as the language L , then we can simply
consider the equilibrium models of  in L . But if  represents a knowledge base or a logic
program, for instance, we may also take the view that L() is the appropriate signature to
work with. In that case, the query  is as such not fully interpreted as it contains some
terms not in the theory language L().
For any language L and theory  whose language is contained in L, let EML () be
the collection of all equilibrium models of  in QHTs= (L). Now consider the following two
variants of entailment.
Definition 8 (Equilibrium entailment) Assume  is a theory in L, is non-empty and
has equilibrium models, then:
(i) Let us write |cw  if and only if M |=  for each M  EML (), where L = LL():
(ii) let us write  |ow  if and only if M |=  for each M  EML () L() , where in

general EML () L denotes the collection of all expansions of elements of EML () to
models in L  L , i.e. where the vocabulary of L \ L is interpreted arbitrarily.
(iii) If either  is empty or has no equilibrium models, then  |cw  iff  |ow  iff   .
A simple example will illustrate the difference between |cw and |ow . Let  be an
L-sentence and let q(x) be a predicate not in L. Let a be a constant in L and let L be the
language L  {q}. By the first method we have  |cw   (q(a)  q(a)). In fact we have
the stronger entailment  |cw   q(a). The reason is that when we form the equilibrium
models of  in L , q(a) will be false in each as an effect of taking minimal models. On the
other hand, if we expand equilibrium models of  in QHTs= (L) to QHTs= (L ), the new
predicate q receives an arbitrary interpretation in QHTs= (L ). Since this logic is 3-valued
we do not obtain  |ow q(a)  q(a).
For standard, monotonic logics, there is no difference between these two forms of entailment. If in Definition 8 we replace everywhere equilibrium model by simply model (in
QHTs= ), variants (i) and (ii) give the same result.
In the context of logic programming and deductive databases the more orthodox view is
that reasoning is based on a closed world assumption (CWA). Accordingly a ground atomic
query like q(a)?, where the predicate q does not belong to the language of the program or
database, would simply be assigned the value false. This is also the case with the first kind
of equilibrium entailment and we use the label |cw since this variant appears closer to a
closed world form of reasoning. On the other hand, there may be legitimate cases where we
do not want to apply the CWA and where unknown values should be assigned to an atom
that is not expressed in the theory language. Then the second form of entailment, |ow ,
926

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

nearer to open world reasoning, may be more appropriate. For present purposes, however,
the suffices cw and ow should be thought of merely as mnemonic labels.
A thorough analysis of closed world versus open world reasoning in this context would
lead us to consider assumptions such as UNA or SNA and is outside the scope of this paper.
However, it has been observed in logic programming that the use of CWA can lead to
certain apparent anomalies. Notably this occurs with programs that are unsafe (see Section
5 below), such as the following, formulated in traditional notation for logic programs:6
q(x, y) :  not p(x, y).
p(x, x).
Given restrictions such as SNA or Herbrand models, the query
?  q(a, z).
yields no answer for z because it cannot be satisfied in models with only a single domain
element a, while the query
?  q(a, b).
is satisfiable, given the new constant b. In logic programming, where these restrictions
are usually assumed, different solutions to this problem have been proposed (Gelder, Ross,
& Schlipf, 1991; Kunen, 1987; Maher, 1988). Here we would like to point out that for
equilibrium logic generally speaking this kind of program or theory does not create any
special difficulties. Neither the query
?  q(a, z).
which is understood as zq(a, z), nor the query
?  q(a, b).
is true in all equilibrium models. In particular, in an equilibrium model whose domain is a
singleton element, even q(a, b) need not be true; evidently in the general case that UNA for
instance does not apply. On the other hand in answer set programming, where UNA is often
assumed, it is also typically assumed that programs are safe. By the safety condition the
above program is excluded because variables appearing in the head of a rule do not appear
in its positive body and this makes answer sets sensitive to the set of constants appearing
in the language or those that are used for grounding the program. In this paper, where the
application of interpolation in ASP is concerned, we restrict attention to safe programs and
theories complying with a generalised form of safety (Section 5 below).

3. Interpolation in Propositional Equilibrium Logic
In this section we deal with interpolation in propositional equilibrium logic. It is clear that
by its semantic construction propositional equilibrium logic has HT as a deductive base.
This base is actually maximal.
Proposition 6 HT is a strong and maximal deductive base for (propositional) equilibrium
entailment.
The first property is precisely the strong equivalence theorem of Lifschitz, Pearce and
Valverde (2001). Maximality follows from the fact that any logic strictly stronger than
HT would have to contain classical logic which is easily seen not to be a deductive base,
e.g. violating condition (ii) of Definition 1. We have:
6. We are grateful to an anonymous referee for raising this point and the example.

927

fiGabbay, Pearce, & Valverde

Lemma 1 Let  be a coherent HT-formula and EM () its set of equilibrium models. Then
there is formula  of HT in v() that defines EM () in the sense that M  EM () if
and only if M |=  .
Proof. Suppose that  is coherent. and let
M1 = hT1 , T1 i, M2 = hT2 , T2 i, . . . , Mn = hTn , Tn i
be an enumeration of its equilibrium models. We show how to define EM (). Suppose
each Ti , has ki elements and denote them by Ai1 , . . . , Aij , . . . , Aiki . Let Ti be the complement
of Ti ; then we can list its members as Aik1 +1 , . . . Ail . . . , Ai|v()| . Set
i =

^
j=1,...,ki

Aij  (

_

Ail ),

l=ki+1 ,...,|v()|

and  =

_

i

i=1,...,n

We claim that M |=  if and only if M = Mi for some i = 1, . . . , n, i.e. the models of 
are precisely M1 , . . . , Mn . To verify this claim, note that each Mi |= i and so Mi |=  .
Conversely, suppose that M |=  . From the semantics of HT it is clear that M |=    iff
M |=  or M |= , so in particular M |=  implies M |= i for some i = 1, . . . , n. However,
each i defines a complete theory whose models are total. It follows that if M |= i , then
M = Mi . This establishes the claim.

Although we shall now demonstrate interpolation in the (|, |) form for the relation |cw , we actually establish a stronger result. One consequence of this is that if we
are concerned with |ow entailment then the (|, ) form of interpolation actually holds.
Proposition 7 (|, |-Interpolation) Let ,  be formulas and set v = v()  v() and
v  = v() \ v() and suppose that B1 , . . . Bn is an enumeration of v  . If  |cw , there is
a formula  such that v()  v()  v(),  | , and   B1  . . .  Bn |= . Hence in
particular  |cw .
Proof. Let ,  and v, v  be as in the statement of the proposition, and suppose that |cw .
Then  holds in all equilibrium models of  in the language v. Case (i): suppose that  is
coherent and form its set of equilibrium models, EMv ().
By the equilibrium construction it is easy to see that in each model M  EMv () each
atom Bi is false, for i = 1, n. Construct the formulas i and the formula  exactly as in the
proof of Lemma 1. Now consider the formula (B1  . . .  Bn )   . Clearly this formula
defines the set of equilibrium models of  in HT(v). Consequently, (B1 . . .Bn ) |= 
and so   (B1  . . .  Bn )  . We can now apply the interpolation theorem for HT
to infer that there is a formula  such that    and   (B1  . . .  Bn )  , where
v()  v( )  v() and hence v()  v()  v(). Since HT is a deductive base, we
conclude that
 |  &   B1  . . .  Bn  .
Now, since v()  v()  v(), Bi 6 v() for i = 1, n. It follows that in HT(v()), each Bi
is false in every equilibrium model of . So each such model M satisfies (B1  . . .  Bn ).7
Since each also satisfies , we have  |cw .
7. Notice that in this case adding to  the sentence (B1  . . .  Bn ) does not change its set of equilibrium
models.

928

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

Case (ii). If  has no equilibrium models then the hypothesis is that   . In that
case we simply choose an interpolant  for (, ).

Corollary 1 (|, -Interpolation) Let ,  be formulas such that  |cw  and v() 
v(). There is a formula  such that v()  v()  v() and  |cw  and   .
Proof. Immediate from Proposition 7 by the fact that v() \ v() = .



Proposition 8 (|, -Interpolation) Let ,  be formulas and set v = v()  v() and
v  = v() \ v(). If  |ow , there is a formula  such that v()  v()  v(),  | , and
  .
Proof. Let ,  and v, v  be as in the statement of the proposition and suppose that |ow .
Then  holds in all expansions of elements of EMv() () to the language v. Case (i): suppose
that  is coherent and consider EMv() ().
Again construct the formulas i and the formula  exactly as in the proof of Lemma 1.
Now consider  which defines the set EMv() (). Then  holds in all expansions of models
of  to v. Hence  |=  and therefore    We can now apply the interpolation
theorem for HT to infer that there is a formula  such that    and   , where
v()  v( )  v() and hence v()  v()  v(). Since  |ow  and HT is a deductive
base we conclude that
 |ow  &   .
Case (ii). If  has no equilibrium models, choose  as an interpolant for (, ).



4. Interpolation in Quantified Equilibrium Logic
We now turn to first-order logic. Given inferences of the form  | , a key element in the
proofs of Propositions 7 and 8 is the existence of a formula  that defines the collection
EMv() () of equilibrium models. In the propositional case we have seen how the existence
of such an  can be established. In the first-order case, on the other hand, such an  need
not exist. In other words, EML() () need not be first-order definable for arbitrary . This
fact is not hard to show. As Ferraris et al. (2007) have pointed out, in the general form of
answer set programming where first-order formulas are allowed, and a fortiori in quantified
equilibrium logic, the property of transitive closure is expressible. Yet this property is not
definable in classical first-order logic and therefore it also cannot be defined in QHTs= .
In the usual way we say that a collection K of QHTs= (L) models is (QHTs= ) definable
if there is an L-sentence, , such that M  K  M |= . It is easy to see that whenever
the class EML() () is first-order definable in QHTs= we do obtain first-order analogs
of Propositions 7 and 8. The method of proof is essentially the same as before. For
completeness we outline the main steps for the case of (|, |)-interpolation.
Proposition 9 (|, |-Interpolation) Let ,  be formulas such that the collection of equilibrium models of  is QHTs= - definable. Set L = L()  L() and L = L() \ L(). Let
{pi : i = 1, n} be the (finite, possibly empty) set of predicates in L and suppose for each i
929

fiGabbay, Pearce, & Valverde

that pi is of arity ki . If  |cw , there is a formula  such that L()  L()  L(),  | ,
and
^

xpi (x) |= 
i=1,n

Hence in particular  |cw .
Proof. Assume the hypotheses. Then  holds in all equilibrium models of  in the language
L. We treat just the case where  is coherent and has a non-empty collection of equilibrium
models, EML() (). By assumption this collection is definable by a QHTs= (L())-sentence,
 , say. Now consider the equilibrium models of  in the expanded language L, i.e. the
collection EML (). By the equilibrium construction we claim that EML () |= xpi (x),
for all i = 1, n. Since we are now working with the first-order semantics, let us rehearse
briefly the argument for this. If it were not true there would be a model hU, T, T i  EML (),
a predicate symbol pi  L and some tuple a of elements in the domain of hU, T, T i, such
that hU, T, T i |= pi (a), ie pi (a)  T . However, since  does not refer to the relation
pi , the structure hU, H, T i with H = T \ pi (a) must Valso be a model of , contradicting
that hU, T, T i is in equilibrium.
So EML () |=   i=1,n xpi (x) and since  defines
V
EML() () clearly   i=1,n xpi (x) defines EML ().
V
Now we proceed as in the propositional case.   i=1,n xpi (x)  , so
^
xpi (x)  .
 
i=1,n

By the interpolation
theorem for QHTs= there is a formula  such that L()  L()  L(),
V
   and   i=1,n xpi (x)  . Consequently we also have
^
xpi (x)  
 |  &  
i=1,n

By the same token as in the propositional case, we infer that  |cw .

The case of (|, )-interpolation for |ow is analogous and we state the main property
without proof.
Proposition 10 (|, -Interpolation) Let ,  be formulas such that the collection of
equilibrium models of  is QHTs= - definable. If  |ow , there is a formula  such that
L()  L()  L(),  |  and   .

5. An Illustration: Interpolation for Safe Formulas
How restrictive is the definability assumption? Is it often met in practice? Actually in
mainstream answer set programming, whose language equilibrium logic captures and extends (see the next section), non-definable classes of answers sets play no significant role.
The reason is that for query answering existing solvers rely on grounders that instantiate all
or parts of a program before computing the intended models or solutions. The grounding
process essentially eliminates variables and reduces the original program to propositional
form. In such practical cases, then, the collection of stable or equilibrium models will be
definable.
930

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

For this computational approach to work in general, syntactic restrictions need to be
imposed on admissible programs or theories. The most common form of restriction is called
safety. For standard types of logic programs based on rules one regards a rule as safe
if every variable appearing in rules head also appears in its body. For the more complex
formulas admitted by equilibrium logic and by the general approach to answer sets (Ferraris
et al., 2007; Ferraris, 2008), new concepts of safety need to be devised. Proposals for suitable
safety concepts were made by Lee, Lifschitz and Palla (2008b) for general first-order formulas
and by Bria, Faber and Leone (2008) for a more restricted syntactic class. More recently
Cabalar, Pearce and Valverde (2009) have generalised both these approaches and suggested
a safety concept for arbitrary function-free formulas in equilibrium logic. Since this new
concept of safety defines a quite broad class of interpolable formulas, let us review here its
main features. In the following section we will mention some other kinds of interpolable
formulas that may arise in answer set programming.
5.1 General Concept of Safety
For the remainder of this section we assume that all languages are function-free. As usual
a sentence is said to be in prenex form if it has the following shape, for some n  0:
Q 1 x1 . . . Q n xn 
where Qi is  or  and  is quantifier-free. A sentence is said to be universal if it is
in prenex form and all quantifiers are universal. A universal theory is a set of universal
sentences. The safety concept is defined for prenex formulas which provide a normal form
for QHTs= (Pearce & Valverde, 2005).
We first introduce a concept called semi-safety. The main property of semi-safety formulas will be that their equilibrium models only refer to objects from their language. Note
that for the remainder of this section we use the fact that negation can be treated as a
defined operator, by     , and do not consider additional semantic clauses for
negation.
Definition 9 (Semi-safety) A quantifier free formula  is semi-safe it is has not nonsemi-safe variable; that is, NSS() = , where the NSS operator is recursively defined as
follows:
 If  is an atom, NSS() is the set of variables in ;
 NSS(1  2 ) = NSS(1 )  NSS(2 );
 NSS(1  2 ) = NSS(1 )  NSS(2 );
 NSS(1  2 ) = NSS(2 ) r RV(1 ),
where operator RV computes the restricted variables as follows:
 For atomic , if  is an equality between two variables then RV() = ; otherwise,
RV() is the set of all variables occurring in ;
 RV() = ;
931

fiGabbay, Pearce, & Valverde

 RV(1  2 ) = RV(1 )  RV(2 );
 RV(1  2 ) = RV(1 )  RV(2 );
 RV(1  2 ) = .
This definition of semi-safe formulas was introduced by Cabalar, Pearce and Valverde (2009)
and generalises the former definition of Lee, Lifschitz and Palla (2008b). In short, a variable x is semi-safe in  if every occurrence is inside some subformula    such that,
either x  RV() or x is semi-safe in .
Some examples of semi-safe formulas are, for instance:
p(x)  (q(x)  r(x))
p(x)  q  r(x)

(6)

Note how in (6), x is not restricted in p(x)  q but the consequent r(x) is semi-safe and
thus the formula itself. On the contrary, the following formulas are not semi-safe:
p(x)  q  r(x)
p(x)  r(x)  q(x)
The following results set the previously referred property for semi-safe formulas: their
equilibrium models only include objects from the language.
Proposition 11 (Cabalar et al., 2009)
If  is function free, semi-safe, and h(D, I), T, T i |= , then h(D, I), T |C , T i |= .
Theorem 2 (Cabalar et al., 2009) If  is function free, semi-safe, and h(D, I), T, T i is
an equilibrium model of , then T |C = T .
The equilibrium models of semi-safe formulas only refer to objects from the language,
however a model could be or not in equilibrium depending of the considered domain. To
guarantee the independence from the domain, we need an additional property to the semisafety. Specifically, we need to analyse whether the unnamed elements could modify an
interpretation of the formula. To do that, we use the assignments of the Kleenes threevalued logic; the three-valued interpretation  : At  {0, 1/2, 1}, are extended to evaluate
arbitrary formulas () as follows:
(  ) = min((), ())
(  ) = max((), ())

() = 0
(  ) = max(1  (), ())

For every variable x, we are going to use the Kleenes interpretations x , defined as follows:
x () = 0 if x occurs in the atom  and x () = 1/2 otherwise. Intuitively, x () fixes
all atoms containing the variable x to 0 (falsity) leaving all the rest undefined and then
evaluates  using Kleenes three-valued operators, that is nothing else but exploiting the
defined values 1 (true) and 0 (false) as much as possible.
An occurrence of a variable x in Qx  is weakly-restricted if it occurs in a subformula
 of  such that:
932

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

 Q = ,  is positive8 and x () = 1
 Q = ,  is negative and x () = 0
 Q = ,  is positive and x () = 0
 Q = ,  is negative and x () = 1
In all cases, we further say that  makes the ocurrence weakly restricted in . This property
is added to the semi-safety condition to complete the definition of safety.
Definition 10 A semi-safe sentence is said to be safe if all its positive occurrences of
universally quantified variables, and all its negative occurrences of existentially quantified
variables are weakly restricted.
For instance, the formula  = x(q(x)  (r  p(x))) is safe: the occurrence of x in
p(x) is negative, whereas the occurrence in q(x) is inside a positive subformula,  itself, for
which x is weakly-restricted, since x () = 0  ( 1/2  0) = 1. Another example of a safe
formula is x((p(x)  q(x))  r).
Proposition 12 (Cabalar et al., 2009) If  is function free, safe, and prenex formula,
then: h(D, I), T, T i is an equilibrium model of  if and only if it is an equilibrium model of
GrC () (the grounding of  over C).
5.2 Interpolation
On the basis of Proposition 12 we could already establish interpolation theorems for safe
formulas in prenex form, essentially by replacing such formulas by their ground versions and
working in propositional logic. However, we can also apply Propositions 9 and 10 directly by
noting the property shown by Cabalar et al. (2009) that safe prenex formulas have definable
classes of equilibrium models.
Theorem 3 (interpolation for safe formulas)
Safe formulas in prenex form have
QHTs= -definable classes of equilibrium models. Therefore for such formulas (|, |)-interpolation for |cw inference holds as in Proposition 9 and (|, )-interpolation holds for |ow
inference as in Proposition 10.

6. Interpolation in Answer Set Semantics
Answer set programming (ASP) has become an established form of declarative, logic-based
programming and its basic ideas are now well-known. For a textbook treatment the reader
is referred to Barals book (2003). As is also well-known, the origins of ASP lie in the
stable model and answer set semantics for logic programs introduced by Gelfond and Lifschitz (1988, 1990, 1991). This semantics made use of a fixpoint condition involving a certain
reduct operator. Subsequent extensions of the concept to cover more general kinds of rules
8. Recall that a subexpression of a formula is said to be positive in it if the number of implications that
contain that subexpression in the antecedent is even, and negative if it is odd. Here we also consider
that  is defined as   .

933

fiGabbay, Pearce, & Valverde

also relied on a reduct operator of similar sort. For the original definitions, the reader is
referred to the various papers cited.
The correspondence between answer set semantics and equilibrium logic is also wellestablished and has been discussed in many publications, beginning with Pearce (1997), who
first showed how the answer sets of disjunctive programs can be regarded as equilibrium
models (Lifschitz et al., 2001, 2007; Ferraris et al., 2007; Pearce & Valverde, 2005, 2006,
2008). For our purposes it will suffice to recall just two important syntactic classes of
programs and the main features of the correspondence with equilibrium logic.
At one extreme we have ground, disjunctive logic programs; we treat them here without
strong negation, so their answer sets are simply collections of atoms. These programs consist
of sets of ground rules of the form
K1  . . .  Kk  L1 , . . . Lm , notLm+1 , . . . , notLn

(7)

where the Li and Kj are atoms. The translation from the syntax of programs to HT
propositional formulas is the trivial one, viz. (7) corresponds to the HT sentence
L1  . . .  Lm  Lm+1  . . .  Ln  K1  . . .  Kk

(8)

Under this translation the correspondence between the answer sets and the equilibrium
models of ground disjunctive programs is also the direct one:
Proposition 13 Let  be a disjunctive logic program. Then hT, T i is an equilibrium model
of  if and only if T is an answer set of .
This was first shown by Pearce (1997) but the basic equivalence was later shown to hold
for more general classes of programs by Pearce, P. de Guzman and Valverde (2000).
It is also common to treat non-ground rules of form (7) where variables may appear.
These variables are thought of as being universally quantified, so the corresponding translation into a logical formula would simply be the universal closure of formula (8).
At the other extreme, Ferraris, Lee and Lifschitz (2007) provided a new definition of
stable model for arbitrary first-order formulas. In this case the property of being a stable
model is defined syntactically via a second-order condition that resembles parallel circumscription. However they also showed that the new notion of stable model is equivalent to
that of equilibrium model as defined here for first-order languages. In a sequel to this paper,
Lee, Lifschitz and Palla (2008a) have applied the new definition and made the following refinements. The stable models of a formula are defined as Ferraris et al. (2007) were, while
the answer sets of a formula are those Herbrand models of the formula that are stable in
the sense of Ferraris et al. Using this new terminology, it follows that in general stable
models and equilibrium models coincide, while answer sets are equivalent to SNA-QHT
models that are equilibrium models.
In between these two extremes many syntactically different kinds of programs have
been considered and several variations in the concept of answer set have been proposed.
However all the main varieties display a similar correspondence to equilibrium logic. It is
merely necessary in some cases to restrict attention to specific kinds of equilibrium models, e.g. Herbrand models, UNA-models or SNA-models. It is important to notice also that
this correspondence extends to many of the additional constructs that have been introduced
934

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

in ASP, such as cardinality and weight constraints and even general forms of aggregates (Lee
& Meng, 2009). All these can be accommodated in equilibrium logic by translation into
logical formulas.
In ASP the main emphasis is on finding answer sets and this is what most answer
set solvers compute. Less attention is placed on implementing a non-monotonic inference
relation or a query answering mechanism. However there is a standard, skeptical concept of
inference or entailment associated with answer set semantics. This notion of entailment or
consequence for programs under the answer set semantics is that a query Q is entailed by
a program  if Q is true in all answer sets of  (Balduccini, Gelfond, & Nogueira, 2000).
Let us denote this entailment or consequence relation by |AS . Evidently atoms are true in
an answer set if and only if they belong to it. Conjunctions and disjunctions are handled
in the obvious way (Lifschitz, Tang, & Turner, 1999; Balduccini et al., 2000). Sometimes,
queries of the form not a, or in logical notation a, are not explicitly dealt with (Balduccini
et al., 2000). However it seems to be in keeping with the semantics to regard a formula of
form not  or  to be true in an answer set if and only if  is not true. Another way
to express this would be to say that an answer set satisfies  if it does not violate the
constraint { }, understanding constraint violation as Lifschitz, Tang and Turner (1999).9
In this way we would say that  |AS A if no answer set of  contains A. Similarly, the
interpretation of queries containing quantifiers in answer set semantics should also conform
to that of equilibrium logic, taking account of any specific restrictions, such as Herbrand
models, that might be imposed.
We can therefore transfer interpolation properties from equilibrium logic to answer set
semantics and ASP. It remains to consider whether |AS is best identified with the closed
world version of inference, |cw , or the more open world version, |ow . Again, since ASP
solvers do not generally implement inference engines, the difference is largely a theoretical one. In traditional logic programming, however, a query that does not belong to the
language of the program is usually answered false. It also seems quite natural in an ASP
context that, given a program  and a query Q, one should consider the stable models of 
in the language L()  L(Q) even if this is a proper extension of the language of .10 So in
general |cw seems a natural choice for answer set inference. On the other hand, there are
contexts where answer set semantics is used in a more open world setting, for example in the
setting of hybrid knowledge bases (Rosati, 2005) where non-monotonic rules are combined
with ontologies formalised in description logics. For such systems a semantics in terms of
equilibrium logic was provided by de Bruijn, Pearce, Polleres and Valverde (2007). Here an
entailment relation in the style of |ow might sometimes be more appropriate.
In general answer set semantics is defined only for coherent programs or theories. For
these, by identifying |AS with |cw , we can apply Proposition 9 directly:
Corollary 2 For coherent formulas , (|, |)-interpolation in the form of Proposition 9
holds for entailment |AS in answer set semantics.
9. In logical terms this constraint would be written   .
10. Notice that by Proposition 12 if a program consists of safe formulas, an atomic query q(a) is automatically
false if a does not belong to the language of the program (even if q does), simply because grounding with
the program constants is sufficient to generate all answer sets.

935

fiGabbay, Pearce, & Valverde

7. An Application of Interpolation
The Interpolation property has been applied in various areas of computer science, notably
in software specification (Bicarregui et al., 2001) and in the construction of formal ontologies (Lutz & Wolter, 2010). In both areas it is relevant to modularity issues. Here we
discuss a simple application related to a concept described by Lutz and Wolter that we can
adapt to the case of nonmonotonic logic programs.
One way to compare two theories is via their nonmonotonic consequence relations. When
two theories produce the same answers for a given query language, we can call them inseparable; this term is used in mathematical logic and also in the study of formal ontologies (Lutz
& Wolter, 2010).
Let us say therefore that 1 and 2 are L-inseparable if for any  such that V ()  L,
1 |   2 | .
Proposition 14 Let 1 and 2 be L-inseparable theories such that V (1 ) = V (2 ) = V ,
say. Then for any L  L such that V  L  L, 1 and 2 are L -inseparable.
Proof. Assume that 1 and 2 are L-inseparable and that L is an extension of L such
that V  L  L. Suppose 1 | , where V () = L . Suppose L \ V = {B1 , . . . Bn }. By
Proposition 7 there is an interpolant  for (1 , ) such that  |= B1 . . .Bn  . Since
1 |  and V ()  L, by L-inseparability we have 2 | . By right absorption therefore
2 | B1  . . .  Bn  . However it is clear that B1 , . . . Bn are false in all equilibrium
models of 2 , so 2 | . Repeating this argument with 1 and 2 interchanged shows that
the theories are L -inseparable.

The above proof is similar to the argument given by Lutz and Wolter (2010) for Theorem 7 of that paper, applied to TBoxes in description logics. The property described is
called there robustness under signature extensions. Notice however that, since | is not in
general transitive we cannot immediately infer from 2 |  and  |  that also 2 | .
This highlights the added strength of using explicitly the set {B1 , . . . Bn } and the property
that HT forms a deductive basis for |.
In the study of modularity and logical relations between programs in ASP, it is more
common to compare their sets of answer sets rather than their consequence classes. However
it turns out that the notion of inseparability is very close to a concept that has already
been studied in ASP. Two theories or programs are said to be projectively equivalent if
the projections of their answer sets onto a common sublanguage agree (Eiter, Tompits,
& Woltran, 2005). Formally, let 1 , 2 be theories and L be a signature such that L 
V (1 )  V (2 ). Then 1 and 2 are said to be projectively equivalent relative to L if
E(1 )L = E(2 )L , where for any class of models K, KL = {ML : M  K}.
Proposition 15 Let 1 , 2 be theories and L a signature such that L  V (1 )  V (2 ).
1 and 2 are projectively equivalent relative to L if and only if they are L-inseparable.
In other words these two concepts agree whenever L is a common sublanguage of 1 , 2 .
The main advantage of L-inseparability is that it seems the more natural one to use if we
want to consider signatures that extend the language of either program or theory.
936

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

8. Uniform Interpolation and Forgetting
A stronger form of interpolation known as uniform interpolation is also important for certain
applications in computer science (Konev et al., 2009). As usual, given ,  with   , we
are interested in interpolants  such that
 & 

(9)

where  contains only predicate and constant symbols that belong to both  and . The
difference now is that  is said to be a uniform interpolant if (9) holds for any  in the
same signature such that   . A logic is said to have the uniform interpolation property
if such uniform interpolants exist for all , .
In classical propositional logic, the uniform interpolation holds, however it fails in first
order classical logic and in many non-classical logics. It may hold when certain restrictions
are placed on the theory language in which  is formulated and on the query language
containing . For example it has been shown to hold for some description logics (Kontchakov
et al., 2008) where such syntactic restrictions apply. Even in ASP it turns out that a form of
uniform interpolation holds for a very restricted query language, essentially one that allows
just instance retrieval. We can show this by using some known results in ASP about the
concept of forgetting (Eiter & Wang, 2008) that is quite closely related to interpolation.
Variable forgetting, as studied by Eiter and Wang (2008), is concerned with the following
problem. Given a disjunctive logic program  and a certain atom a occurring in , construct
a new program, to be denoted by forget(, a), that does not contain a but whose answer
sets are in other respects as close as possible to those of . For the precise notion of
closeness the reader is referred to paper of Eiter and Wang, however some consequences will
be evident shortly. Eiter and Wang define forget(, a) (as a generic term), show that such
programs exist whenever  is coherent, and provide different algorithms to compute such
programs.
Given coherent  and a in , the results forget(, a), of forgetting about a in  may
be different but are always answer set equivalent. Moreover for our purposes they satisfy
the following key property, where  is coherent, a, b are distinct atoms in  and as usual |
denotes nonmonotonic consequence,
 | b



forget(, a) | b.

(10)

showing that indeed the answer sets of  and forget(, a) are closely related.
To establish a version of uniform interpolation for the case of disjunctive programs and
simple, atomic queries, we need to show that we can always find a  = forget(, a) such
that  |  . For this we can examine the first algorithm of Eiter and Wang for computing
forget(, a); this is also the simplest of the three algorithms presented. Let  be a coherent
program with rules of form (7) that we write as formulas of form (8) and let a be an atom
in . The method for constructing a  = forget(, a) is as follows.
1. Compute the equilibrium models E().
2. Let E  be the result of removing a from each M  E().
3. Remove from E  any model that is non-minimal to form E  (= {A1 , . . . , Am }, say).
937

fiGabbay, Pearce, & Valverde

4. Construct a program  whose answer sets are precisely {A1 , . . . , Am } as follows:
 for each Ai , set i = {Ai  a : a  Ai }, where Ai = V () \ Ai .
 Set  = 1  . . .  m .
We can now verify the desired property. Let L be the simple query language composed of
conjunctions of literals.
Proposition 16 In equilibrium logic (or answer set programming) uniform interpolation
holds for (coherent) disjunctive programs and queries in L(V ()).
Proof. To prove the claim we shall show the following. Let  be a coherent disjunctive
program and let L = L(V ) for some V  V (). Then there is a program  such that
V ( ) = V and for any   L,
 |   ( |  &  | )
To begin, let  and  be as above with  | . Let X = {a1 , . . . , an } = V () \ V . Then
we choose  to be the result of forgetting about X in , defined by Eiter and Wang (2008)
as follows:
forget(, X) := forget(forget(forget(, a1 ), a2 ), . . . , an ),
and it is shown there that the order of the atoms in X does not matter. Now we know
by (10) that for any atom a  V and any i = 1, n,
 | a  forget(, ai ) | a,

(11)

 | a  forget(, X) | a.

(12)

therefore
Let  be forget(, X) as determined by algorithm 1 of Eiter and Wang (2008) described
above. It is easy to see by (11) and the semantics of | that (12) continues to hold where a
is replaced by a negated atom b and therefore also by any conjunction of literals since a
conjunction is entailed only if each element holds in every equilibrium model.11 So it remains
to show that  |  . Again, it will suffice to show this entailment for one member of the
sequence forget(, ai ) and since the order is irrelevant wlog we can choose the first element
forget(, a1 ) and show that  | forget(, a1 ). We compute the programs 1 , . . . , m as
in the algorithm. Then we need to check that  | i for any i = 1, n, i.e. that for each
M  E(), M |= {Ai  a : a  Ai }.
Consider M  E() where M = hT, T i. We distinguish two cases. (i) Ai  T . Then
M |= a for each a  Ai . It follows that M |= Ai  a for each a  Ai and so
M |= {Ai  a : a  Ai }. Case (ii) Ai 6 T . Then T and Ai are incomparable. In
particular we cannot have T  Ai by the minimality property of Ai obtained in step 3.
Hence T  Ai 6= . Choose a  T  Ai . Then M |= a , so M 6|= a and hence M 6|= Ai .
Consequently, for any a , M |= Ai  a and so M |= {Ai  a : a  Ai }. It follows
that for any i,  | i and so by construction  |  , which establishes the proposition. 
11. As Eiter and Wang (2008) point out, if an atom b is true in some answer set of forget(, a), then it
must also be true in some answer set of , showing that (12) holds for literals.

938

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

8.1 Extending the Query Language
If we establish uniform interpolation in ASP using the method of forgetting, as defined
by Eiter and Wang (2008), it seems clear that we cannot extend in a non-trivial way the
expressive power of the query language L. Since the method of forgetting a in  removes
non-minimal sets from E() (once a has been removed), an atom b might be true in some
equilibrium model of  but not in any equilibrium model of forget(, a). Hence we might
have a disjunction, say a  b, derivable from  but not from forget(, a). Likewise, if we
consider programs with variables in a first-order setting, we cannot in general extend L to
include existential queries.
On the other hand, the property of uniform interpolation certainly holds for any L(V )
even without the condition that V  V (). Suppose that  |  where V () \ V () 6= ,
say V () \ V () = {b1 , . . . , bk }. Then b1 , . . . , bk are false in all equilibrium models of .
Trivially, if b is not in V () we can regard the result of forgetting about b in  as just . So
we can repeat the proof of Proposition 16, but now setting X = {V () \ V }  {V \ V ()}.
All the relevant properties will continue to hold.
An interesting open question is whether we can extend the theory language to include
more general kinds of program rules such as those allowing negation in the head. Accommodating these kinds of formulas would constitute an important generalisation since they
amount to a normal form in equilibrium logic. However, the answer sets of such programs do
not satisfy the minimality property that holds for the answer sets of disjunctive programs,
so it is clear that the definition of forgetting would need to be appropriately modified - a
task that we do not attempt here.

9. Literature and Related Work
The interpolation theorem for classical logic is due to Craig (1957); it was extended to intutionistic logic by Schutte (1962). Maksimova (1977) characterised the super-intuitionistic
propositional logics possessing interpolation. A modern, comprehensive treatment of interpolation in modal and intuitionistic logics can be found in the monograph of Gabbay and
Maksimova (2005).
In non-monotonic logics, interpolation has received little attention. A notable exception
is an article (Amir, 2002) establishing some interpolation properties for circumscription and
default logic. By the well-known relation between the answer sets of disjunctive programs
and the extensions of corresponding default theories, he also derives a form of interpolation
for ASP. With regard to answer set semantics, the approach of Amir is quite different from
ours. Since it is founded on an analysis of default logic, it uses classical logic as an underlying
base. So Amirs version of interpolation is a form of (3) where L is classical logic; there is
no requirement that L form a well-behaved sublogic of |, e.g. a deductive base. As Amir
remarks, one cannot deduce in general from property (4) that | . However if L is classical
logic one cannot even deduce  |  from (3). More generally, there is no counterpart to
our Proposition 1 in this case. Another difference with respect to our approach is that
Amir does not discuss the nature of the | relation for ASP in detail, in particular how
to understand  |  in case  contains atoms not present in the program . In fact, if
we interpret |AS as in Section 6 above, it is easy to refute (|, L )-interpolation where L
is classical logic. Let  be the program B  A and q the query B  C. Then clearly
939

fiGabbay, Pearce, & Valverde

 |AS q, but there is no formula in the vocabulary B that would classically entail C.
Under any interpretation of answer set inference such that atoms not in the program are
regarded as false, (|, L )-interpolation would be refuted.

10. Conclusions
We have discussed two kinds of interpolation properties for non-monotonic inference relations and shown that these properties hold in turn for the two different inference relations
that we can associate with propositional equilibrium logic. In each case we use the fact that
the collection of equilibrium models is definable in the logic HT of here-and-there and that
this logic possesses the usual form of interpolation. One of the forms of inference studied
seems to be in many cases an appropriate concept to associate with answer set programming, although in general ASP systems are not tailored to query answering or deduction.
Using results of Eiter and Wang (2008) about variable forgetting in ASP, we could also show
how the property of uniform interpolation holds for disjunctive programs and a restricted
query language.
We have also discussed the interpolation property for first-order equilibrium logic based
on a quantified version QHT of the logic of here-and-there, obtaining analogous results as
for the propositional case whenever the collection of equilibrium models is definable. These
positive results transfer to answer set programming under the assumption usually made in
ASP systems that programs are safe and therefore have definable collections of answer sets.
As we saw, the notion of safety can be quite generally defined for theories and is not limited
to normal or disjunctive programs.

Acknowledgments
David Pearce is partially supported by MEC projects TIN2009-14562-C05-02 and CSD200700022. Agustn Valverde is partially supported by MEC project TIN2009-14562-C05-01, and
Junta de Andalucia projects P09-FQM-05233 and TIC-115. The authors are grateful to the
anonymous referees for helpful comments.

References
Amir, E. (2002). Interpolation theorems for nonmonotonic reasoning systems.. In Proceedings of NMR02, pp. 4150.
Balduccini, M., Gelfond, M., & Nogueira, M. (2000). A-prolog as a tool for declarative
programming. In Proc. of SEKE 2000).
Baral, C. (2003). Knowledge Representation, Reasoning and Declarative Problem Solving.
Cambridge University Press.
Bicarregui, J., Dimitrakos, T., Gabbay, D. M., & Maibaum, T. S. E. (2001). Interpolation
in practical formal development. Logic Journal of the IGPL, 9 (2).
Bria, A., Faber, W., & Leone, N. (2008). Normal form nested programs. In Holldobler, S.,
Lutz, C., & Wansing, H. (Eds.), Proc. of JELIA08, Vol. 5293 of LNCS, pp. 7688.
Springer.
940

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

Cabalar, P., Pearce, D., & Valverde, A. (2009). A revised concept of safety for general answer
set programs. In Erdem, E., Lin, F., & Schaub, T. (Eds.), Proc. of LPNMR09, Vol.
5753 of LNCS, pp. 5870. Springer.
Craig, W. (1957). Linear reasoning. a new form of the herbrand-gentzen theorem.. J. Symb.
Logic, 22, 250268.
de Bruijn, J., Pearce, D., Polleres, A., & Valverde, A. (2007). Quantified equilibrium logic
and hybrid rules. In Marchiori, M., Pan, J. Z., & de Sainte Marie, C. (Eds.), Proc. of
RR07, Vol. 4524 of LNCS, pp. 5872. Springer.
Diaconescu, R., Goguen, J., & Stefaneas, P. (1993). Logical support for modularisation. In
Logical Environments, pp. 83130. Cambridge University Press.
Dietrich, J. (1994). Deductive bases of nonmonononic inference operations. Ntz report,
University of Leipzig.
Eiter, T., Tompits, H., & Woltran, S. (2005). On solution correspondences in answer-set
programming.. In Kaelbling, L. P., & Saffiotti, A. (Eds.), Proc. of IJCAI05, pp.
97102. Professional Book Center.
Eiter, T., & Wang, K. (2008). Semantic forgetting in answer set programming. Artificial
Intelligence, 172 (14), 16441672.
Ferraris, P. (2008). Logic programs with propositional connectives and aggregates. CoRR,
abs/0812.1462.
Ferraris, P., Lee, J., & Lifschitz, V. (2007). A new perspective on stable models. In Veloso,
M. M. (Ed.), Proc. of IJCAI07, pp. 372379.
Gabbay, D. M., & Maksimova, L. (2005). Interpolation and Definability: Modal and Intuitionistic Logic. Oxford University Press, USA.
Gelder, A. V., Ross, K. A., & Schlipf, J. S. (1991). The well-founded semantics for general
logic programs. Journal of ACM, 38 (3), 620650.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming.
In Kowalski, R. A., & Bowen, K. (Eds.), Proc. of ICLP88, pp. 10701080. The MIT
Press.
Gelfond, M., & Lifschitz, V. (1990). Logic programs with classical negation. In Warren,
David H.D.; Szerdei, P. (Ed.), Proc. of ICLP90, pp. 579597. MIT Press.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive
databases. New Generation Computing, 9, 365385.
Konev, B., Walther, D., & Wolter, F. (2009). Forgetting and uniform interpolation in largescale description logic terminologies. In Boutilier, C. (Ed.), Proc. of IJCAI09, pp.
830835.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). Can you tell the difference
between dl-lite ontologies?. In Brewka, G., & Lang, J. (Eds.), Principles of Knowledge
Representation and Reasoning: Proc. of KR08, pp. 285295. AAAI Press.
Kunen, K. (1987). Negation in logic programming. Journal of Logic Programming, 4 (4),
289308.
941

fiGabbay, Pearce, & Valverde

Lee, J., Lifschitz, V., & Palla, R. (2008a). A reductive semantics for counting and choice in
answer set programming. In Fox, D., & Gomes, C. P. (Eds.), Proc. of AAAI08, pp.
472479. AAAI Press.
Lee, J., Lifschitz, V., & Palla, R. (2008b). Safe formulas in the general theory of stable
models (preliminary report). In de la Banda, M. G., & Pontelli, E. (Eds.), Proc. of
ICLP08, Vol. 5366 of LNCS, pp. 672676. Springer.
Lee, J., & Meng, Y. (2009). On reductive semantics of aggregates in answer set programming. In Erdem, E., Lin, F., & Schaub, T. (Eds.), Proc. of LPNMR09, Vol. 5753 of
LNCS, pp. 182195. Springer.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM
Transactions on Computational Logic, 2 (4), 526541.
Lifschitz, V., Pearce, D., & Valverde, A. (2007). A characterization of strong equivalence
for logic programs with variables. In Baral, C., Brewka, G., & Schlipf, J. S. (Eds.),
Proc. of LPNMR07, Vol. 4483 of LNCS, pp. 188200. Springer.
Lifschitz, V., Tang, L. R., & Turner, H. (1999). Nested expressions in logic programs. Annals
of Mathematics and Artificial Intelligence, 25 (34), 369389.
Lutz, C., & Wolter, F. (2010). Deciding inseparability and conservative extensions in the
description logic el. Journal of Symbolic Computation, 45 (2), 194228.
Maher, M. J. (1988). Equivalences of logic programs. In Foundations of Deductive Databases
and Logic Programming., pp. 627658. Morgan Kaufmann.
Makinson, D. (1994). General patterns in nonmonotonic reasoning, pp. 35110. Oxford
University Press, Inc.
Maksimova, L. (1997). Interpolation in superintuitionistic predicate logics with equality.
Algebra and Logic, 36, 543561.
Maksimova, L. (1998). Interpolation in superintuitionistic and modal predicate logics with
equality. In M.Kracht, de Rijke, M., Wansing, H., & Zakharyaschev, M. (Eds.), Advances in Modal Logic, Vol. I, pp. 133141. CSLI Publications.
Maksimova, L. (1977). Craigs interpolation theorem and amalgamable varieties. Doklady
Akademii Nauk SSSR, 237 (6), 12811284.
McMillan, K. L. (2005). Applications of craig interpolants in model checking. In Halbwachs,
N., & Zuck, L. D. (Eds.), Proc. of TACAS05, Vol. 3440 of LNCS, pp. 112. Springer.
Ono, H. (1983). Model extension theorem and craigs interpolation theorem for intermediate
predicate logics. Reports on Mathematical Logic, 15, 4158.
Pearce, D. (1997). A new logical characterization of stable models and answer sets. In Dix,
J., Pereira, L. M., & Przymusinski, T. C. (Eds.), Proc. of NMELP96, Vol. 1216 of
LNCS, pp. 5770. Springer.
Pearce, D. (2006). Equilibrium logic. Annals of Mathematics and Artificial Intelligence,
47 (1-2), 341.
Pearce, D., de Guzman, I. P., & Valverde, A. (2000). Computing equilibrium models using
signed formulas. In Proc. of CL2000, Vol. 1861 of LNCS, pp. 688703. Springer.
942

fiInterpolable Formulas in Equilibrium Logic and Answer Set Programming

Pearce, D., & Valverde, A. (2005). A first order nonmonotonic extension of constructive
logic. Studia Logica, 80 (2-3), 321346.
Pearce, D., & Valverde, A. (2006). Quantified equilibrium logic. Technical report, Universidad Rey Juan Carlos. (http://www.matap.uma.es/investigacion/tr/ma06_02.
pdf).
Pearce, D., & Valverde, A. (2008). Quantified equilibrium logic and foundations for answer
set programs. In de la Banda, M. G., & Pontelli, E. (Eds.), Proc. of ICLP08, Vol.
5366 of LNCS, pp. 546560. Springer.
Pearce, D., & Valverde, A. (2012). Synonymous theories and knowledge representations in
answer set programming. Journal of Computer and System Sciences, 78, 86104.
Rosati, R. (2005). Semantic and computational advantages of the safe integration of ontologies and rules. In Fages, F., & Soliman, S. (Eds.), Proc. of PPSWR05, Vol. 3703
of LNCS, pp. 5064. Springer.
Schutte, K. (1962). Der interpolationsatz der intuitionistischen pradikatenlogik.. Math.
Ann., 148, 192200.
van Dalen, D. (1997). Logic and Structure (3th edition). Springer.

943

fiJournal of Artificial Intelligence Research 42 (2011) 181-209

Submitted 5/11; published 10/11

Topological Value Iteration Algorithms
Peng Dai

DAIPENG @ CS . WASHINGTON . EDU

Google Inc.
1600 Amphitheatre Pkwy
Mountain View, CA 94043
USA

Mausam
Daniel S. Weld

MAUSAM @ CS . WASHINGTON . EDU
WELD @ CS . WASHINGTON . EDU

Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195
USA

Judy Goldsmith

GOLDSMIT @ CS . UKY. EDU

Department of Computer Science
University of Kentucky
Lexington, KY 40508
USA

Abstract
Value iteration is a powerful yet inefficient algorithm for Markov decision processes (MDPs)
because it puts the majority of its effort into backing up the entire state space, which turns out to
be unnecessary in many cases. In order to overcome this problem, many approaches have been
proposed. Among them, ILAO* and variants of RTDP are state-of-the-art ones. These methods
use reachability analysis and heuristic search to avoid some unnecessary backups. However, none
of these approaches build the graphical structure of the state transitions in a pre-processing step
or use the structural information to systematically decompose a problem, whereby generating an
intelligent backup sequence of the state space. In this paper, we present two optimal MDP algorithms. The first algorithm, topological value iteration (TVI), detects the structure of MDPs and
backs up states based on topological sequences. It (1) divides an MDP into strongly-connected
components (SCCs), and (2) solves these components sequentially. TVI outperforms VI and other
state-of-the-art algorithms vastly when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm, focused topological value iteration (FTVI), is an extension of TVI. FTVI restricts
its attention to connected components that are relevant for solving the MDP. Specifically, it uses
a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows
FTVI to find smaller connected components, thus running faster. We demonstrate that FTVI outperforms TVI by an order of magnitude, averaged across several domains. Surprisingly, FTVI
also significantly outperforms popular heuristically-informed MDP algorithms such as ILAO*,
LRTDP, BRTDP and Bayesian-RTDP in many domains, sometimes by as much as two orders of
magnitude. Finally, we characterize the type of domains where FTVI excels  suggesting a way
to an informed choice of solver.

1. Introduction
Markov Decision Processes (MDPs) (Bellman, 1957) are a powerful and widely-adopted formulation for modeling autonomous decision making under uncertainty. For instance, NASA researchers
c
2011
AI Access Foundation. All rights reserved.

fiDAI , M AUSAM , W ELD , & G OLDSMITH

use MDPs to model the next-generation Mars rover planning problems (Bresina, Dearden, Meuleau,
Ramkrishnan, Smith, & Washington, 2002; Feng & Zilberstein, 2004; Mausam, Benazera, Brafman,
Meuleau, & Hansen, 2005; Meuleau, Benazera, Brafman, Hansen, & Mausam, 2009). MDPs are
also used to formulate the military operations planning (Aberdeen, Thiebaux, & Zhang, 2004) and
coordinated multi-agent planning (Musliner, Carciofini, Goldman, E. H. Durfee, & Boddy, 2007),
etc.
Classical dynamic programming algorithms, such as value iteration (VI), solve an MDP optimally by iteratively updating the value of every state in a fixed order, one state per iteration. This
can be very inefficient, since it overlooks the graphical structure of a problem, which can provide
vast information about state dependencies.
During the past decade researchers have developed heuristic search algorithms that use reachability information and heuristic functions to avoid some unnecessary backups. These approaches,
such as improved LAO* (ILAO*) (Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003b),
HDP (Bonet & Geffner, 2003a), BRTDP (McMahan, Likhachev, & Gordon, 2005) and Bayesian
RTDP (Sanner, Goetschalckx, Driessens, & Shani, 2009), frequently outperform value iteration.
On some problems, however, heuristic search algorithms offer little benefit and it is difficult to predict when they will excel. This raises an important, open question, What attributes of problems
and problem domains make them best suited for heuristic search algorithms?
In this paper we present two algorithms that solve MDPs optimally and speed up the convergence of value iteration: topological value iteration (TVI) (Dai & Goldsmith, 2007) and focused
topological value iteration (FTVI) (Dai, Mausam, & Weld, 2009b). TVI makes use of the graphical
structure of an MDP. It performs Bellman backups in a more intelligent order after performing an
additional topological analysis of the MDP state space. TVI first divides an MDP into strongly connected components (SCCs) and then solves each component sequentially in topological order. Experimental results demonstrate significant performance gains over VI and, surprisingly, over heuristic search algorithms (despite TVI not using reachability information itself) in a specific kind of
domain  one that has multiple, close-to-equal-sized SCCs.
TVI is very general, as it is independent of any assumptions on the start state and can find the
optimal value function for the entire state space. However, many benchmark problems cannot be
broken into roughly equal-sized SCCs, leaving TVIs performance no better (or often worse, due to
the overhead of generating SCCs) than other MDP algorithms. For instance, many domains (e.g.,
Blocksworld) have reversible actions. Problems from these domains that have most of the states
connected by reversible actions end up being in one (large) SCC, thus, eliminating the benefit of
TVI.
FTVI addresses the weaknesses of TVI. It first performs a phase of heuristic search and eliminates provably sub-optimal actions found during the search. Then it builds a more informative
graphical structure based on the remaining actions. We find that a very short phase of heuristic
search is often able to eliminate many actions leading to an MDP structure that is amenable to
efficient, topology-based solutions.
We evaluate FTVI across several benchmark domains and find that FTVI outperforms TVI by
significant margins. Surprisingly, we also find that FTVI outperforms other state-of-the-art heuristic
search algorithms in most of the domains. This is unexpected, since common wisdom dictates that
heuristic-guided search is much faster than all-state dynamic programming. To better understand
this big improvement, we study the convergence speed of algorithms on a few problem features. We
discover two important features of problems that are hard for heuristic search algorithms: smaller
182

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

number of goal states and long search distance to the goal. These features are commonly found in
many domains, e.g., Mountain car (Wingate & Seppi, 2005) and Drive (Bonet, 2006). We show that,
in such domains, FTVI outperforms heuristic search in convergence speed by an order of magnitude
on average, and sometimes by even two orders of magnitude.
Comparing with the previous conference versions (Dai & Goldsmith, 2007; Dai et al., 2009b),
this paper makes several significant improvements: (1) We add a convergence test module in the
search phase of FTVI. With the module, FTVI works as good as the best heuristic search algorithms
in domains where it used to be significantly outperformed. (2) We perform extensive empirical study
on both TVI (Figures 2 and 3 are new) and FTVI (Figure 5 is new, and we added the Blocksworld
domain). (3) We describe TVI and FTVI in a consistent way and improve the pesudo-codes. (4) We
add the convergence proof of TVI (Theorem 2).
The outline of the rest of the paper is as follows: Section 2 formally defines MDPs, and reviews
algorithms that solve MDPs. Section 3 describes the topological value iteration algorithm, and
compares it empirically with other algorithms on a special MDP domain. Section 4 introduces the
focused topological value iteration algorithm and provides a thorough empirical evaluation. We
present related work in Section 5 and conclude in Section 6.

2. Background
We provide an overview of Markov decision process (MDP) and dynamic programming algorithms
that solve an MDP.
2.1 Markov Decision Processes for Planning
AI researchers typically use MDPs to formulate fully-observable probabilistic planning problems.
An MDP is defined as a five-tuple hS, A, Ap, T, Ci, where
 S is a finite set of discrete states.
 A is a finite set of all applicable actions.
 Ap : S  P(A) is the applicability function. Ap(s) denotes the set of actions that can be
applied in state s. P(A) is the power set of the set of actions.
 T : S  A  S  [0, 1] is the transition function describing the effect of an action execution.
 C : S  A  R+ is the cost of executing an action in a state.
The agent executes its actions in discrete time steps. At each step, the system is at one distinct
state s  S. The agent can execute any action a from a set of applicable actions Ap(s)  A, incurring a cost of C(s, a). The action takes the system to a new state s0 stochastically, with probability
Ta (s0 |s).
The horizon of an MDP is the number of steps for which costs are accumulated. We concentrate
on a special set of MDPs called stochastic shortest path (SSP) problems. Despite its simplicity, SSP
is a general MDP representation. Any infinite-horizon, discounted-reward MDP can be easily converted to an SSP problem (Bertsekas & Tsitsiklis, 1996). The horizon in such an MDP is indefinite,
i.e., finite but unbounded, and the costs are accumulated with no discounting. There are two more
components of an SSP:
183

fiDAI , M AUSAM , W ELD , & G OLDSMITH

 s0 is the initial state.
 G  S is the set of sink goal states. Reaching any one of g  G terminates an execution.
The cost of an execution is the sum of all costs along the path from s0 to the first goal state
encountered.
We assume full observability, i.e., after executing an action and transitioning stochastically to
a next state as governed by T , the agent has full knowledge of the state. A policy,  : S  A, of
an MDP is a mapping from the state space to the action space, indicating which action to execute
at each state. To solve the MDP we need to find an optimal policy (  : S  A), a probabilistic
execution plan that reaches a goal state with the minimum expected cost. We evaluate any policy 
by its value function, the set of values that satisfy the following equation:
V  (s) = C(s, (s)) +

X

T(s) (s0 |s)V  (s0 ).

(1)

s0 S

Any optimal policy must satisfy the following system of Bellman equations:
V  (s) = 0 if s  G, else
"
V  (s) =

min

#
X

C(s, a) +

aAp(s)

Ta (s0 |s)V  (s0 ) .

(2)

s0 S

The corresponding optimal policy can be extracted from the value function:
"
#
X
  (s) = argminaAp(s) C(s, a) +
Ta (s0 |s)V  (s0 ) , s  S  G.

(3)

s0 S

Given an implicit optimal policy   in the form of its optimal value function V  (), the Q-value
of a state-action pair (s, a) is defined as the value of state s, if an immediate action a is performed,
followed by   afterwards. More concretely,
X
Q (s, a) = C(s, a) +
Ta (s0 |s)V  (s0 ).
(4)
s0 S

Therefore, the optimal value function can be expressed by:
V  (s) = minaAp(s) Q (s, a).
2.2 Dynamic Programming
Most optimal MDP algorithms are based on dynamic programming, whose utility was first proved
by a simple yet powerful algorithm named value iteration (Bellman, 1957). Value iteration first
initializes the value function arbitrarily, for example all zero. Then, the values are updated iteratively
using an operator called the Bellman backup (Line 7 of Algorithm 1) to create successively better
approximations for each state per iteration. We define the Bellman residual of a state to be the
absolute difference of a state value before and after a Bellman backup. Value iteration stops when
the value function converges. In implementation, it is typically signaled by when the Bellman error,
184

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

Algorithm 1 (Gauss-Seidel) Value Iteration
1: Input: an MDP M = hS, A, Ap, T, Ci, : the threshold value
2: initialize V arbitrarily
3: while true do
4:
Bellman error  0
5:
for each state s  S do
6:
oldV  V (s)


P
7:
V (s)  minaAp(s) C(s, a) + s0 S Ta (s0 |s)V (s0 )
8:
Bellman residual(s)  |V (s)  oldV |
9:
Bellman error  max(Bellman error, Bellman residual(s))
10:
if Bellman error <  then
11:
return V
the largest Bellman residual of all states, becomes less than a pre-defined threshold, . We call a
Bellman backup a contraction operation (Bertsekas, 2001), if for every state, its Bellman residual
never increase with the iteration number.
Value iteration converges to the optimal value function in time polynomial in |S| (Littman, Dean,
& Kaelbling, 1995; Bonet, 2007), yet in practice it is usually inefficient, since it blindly performs
backups over the state space iteratively, often introducing many unnecessary backups.
2.2.1 H EURISTIC S EARCH
To improve the efficiency of dynamic programming, researchers have explored various ideas from
traditional heuristic-guided search, and have consistently demonstrated their usefulness for MDPs
(Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001; Bonet & Geffner, 2003b, 2006;
McMahan et al., 2005; Smith & Simmons, 2006; Sanner et al., 2009). The basic idea of heuristic search is to consider an action only when necessary, which leads to a more conservative backup
strategy. This strategy helps to save a lot of unnecessary backups.
We define a heuristic function h : S  R+ , where h(s) is an estimate of V  (s). A heuristic
function h is admissible if it never over-estimates the value of a state,
h(s)  V  (s), s  S.

(5)

We also interchangeably write an admissible heuristic function as Vl , to emphasize that Vl (s) is a
lower bound of V  (s).
Definition A greedy policy  is the best policy by one-step lookahead given the current value
function, V :
"
#
X
(s) = argminaAp(s) C(s, a) +
Ta (s0 |s)V (s0 ) , s  S  G.
(6)
s0 S

A policy graph, G = (V, E), for an MDP with the set of states S and policy  is a directed,
connected graph with vertices V  S, where s0  V, and for any s  S, s  V iff s is reachable
from s0 under policy . Furthermore, s, s0  V, hs, s0 i  E (the edges of the policy graph) iff
T(s) (s0 |s) > 0.
185

fiDAI , M AUSAM , W ELD , & G OLDSMITH

Heuristic search algorithms have two main features: (1) The search is limited to states that are
reachable from the initial state. Given the heuristic value, a heuristic search algorithm generates
a running greedy policy, as well as its policy graph. The algorithm performs a series of heuristic
searches, until all states on the greedy policy graph converge. A search typically starts from the
initial state, with successor states explored in a best-first manner. Visited states have their values
backed up during the search. (2) Since heuristic search algorithms do fewer backups than value
iteration, they require special care to guarantee final optimality. So values of the state space have to
be initialized by an admissible heuristic function. Note that value iteration can also take advantage
of initial heuristic values as an informative starting point, but does not require the heuristics to be
admissible to guarantee optimality.
Different heuristic search algorithms use different search strategies and therefore perform Bellman backups in different orders.
The AO* algorithm (Nilson, 1980) solves acyclic MDPs, so it is not applicable to general MDPs.
LAO* (Hansen & Zilberstein, 2001) is an extension to the AO* algorithm that can handle MDPs
with loops. Improved LAO* (ILAO*) (Hansen & Zilberstein, 2001) is an efficient variant of LAO*.
It iteratively performs complete searches that discover a running greedy policy graph. In detail, the
greedy policy graph only contains the initial state s0 when a search starts. New states are added
to the graph by means of expansions over a frontier state in a depth-first manner, until no more
states can be added. In a state expansion, one of its greedy actions is chosen, and all the actions
successor states are added into the graph. States that are not expanded yet but contain successors are
called frontier states. Later, states of the greedy policy graph are backed up only once in the postorder when they are visited. Each search iteration performs at most |S| backups, but in practice this
number is typically much smaller. ILAO* terminates when all states of the current greedy policy
graph have a Bellman residual less than a given .
Real-time dynamic programming (RTDP) (Barto et al., 1995) is another popular algorithm for
MDPs. It interleaves dynamic programming with search through plan execution trials. An execution
trial is a path that originates from s0 and ends at any goal state or by a bounded-step cutoff. Each
execution step simulates the result of one-step plan execution. The agent greedily picks an action a
of the current state s, and mimics the state transition to a new current state s0 , chosen stochastically
based on the transition probabilities of the action, i.e., s0  Ta (s0 |s). Dynamic programming
happens when states are backed up immediately when they are visited. RTDP is good at finding
a good sub-optimal policy relatively quickly. However, in order for RTDP to converge, states on the
optimal policy have to be backed up sufficiently, so its convergence is usually slow. To overcome
the slow convergence problem of RTDP, researchers later proposed several heuristic search variants
of the algorithm.
Bonet and Geffner (2003b) introduced a smart labeling technique in a RTDP extension named
labeled RTDP (LRTDP). They label a state s solved if every state reachable from s by applying
the greedy policy is either a goal state, or is solved, or has a Bellman residual no greater than the
threshold . States that are labeled as solved no longer get backed up in any future search. Labeling
helps speed up convergence as it avoids many unnecessary backups over states that have already
converged. After an execution trial, LRTDP tries to label every unsolved state in the reverse order
of visit. To label a state s, LRTDP initiates a DFS from s0 and checks if all states reachable under
the greedy policy rooted at s are solved, and back them up, otherwise. LRTDP terminates when all
states of the current policy graph are solved. Bonet and Geffner also applied the labeling technique
in another algorithm called HDP (Bonet & Geffner, 2003a). HDP uses Tarjans algorithm to find all
186

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

the strongly connected component of an MDP to help label solved states and implicitly control the
order in which states are backed up in a search trial.
McMahan et al. (2005) proposed another extension named bounded RTDP (BRTDP), which not
only uses a lower bound heuristic of the value function Vl , but also an upper bound Vu . BRTDP
has two key differences from the original RTDP algorithm. First, once BRTDP backs up a state
s, it updates both the lower bound and the upper bound. Second, when choosing the next state s0 ,
the difference of its two bounds, Vu (s0 )  Vl (s0 ), is also taken into consideration. More concretely,
s0  Ta (s0 |s)[Vu (s0 )  Vl (s0 )], which focuses search on states that are less likely to be converged.
One feature of BRTDP is its adaptive trial termination criterion, which is very helpful in practice.
Smith and Simmons (2006) introduced a similar algorithm named focused RTDP (FRTDP). They
define occupancy as an intuitive measure of the expected number of times a state is visited before
execution termination. Therefore occupancy of a state indicates its relevance to a policy. Similar to
BRTDP, FRTDP also keeps two bounds for a state. FRTDP uses the product of a states occupancy
and the difference of its bounds for picking the next state. Also, FRTDP assumes a discounted cost
setting, so it is not immediately applicable to SSP problems.
Recently Sanner et al. (2009) described another advanced RTDP variant named Bayesian RTDP,
which also uses two value bounds. The basic motivation of Bayesian RTDP is that anytime performance for sub-optimal policies is important, when finding an optimal policy can be very timeconsuming. This is especially true when some sub-optimal policy performs close to an optimal one,
but is much faster to generate. Its key assumption is that the true value function of a state s, V  (s),
is uniformly distributed on the interval [Vl (s), Vu (s)]. Therefore, the probability density function of
1
V  (s) is 1v[Vl (s),Vu (s)] [ Vu (s)V
], and E[V  (s)] = 21 [Vl (s) + Vu (s)]. To evaluate how important
l (s)
it is to pick state s0 as the next state, it refers to the notion of value of perfect information (VPI),
which intuitively tells the expected Q-value difference of the current state-action pair, Q(s, a), with
and without the knowledge of V  (s0 ). To choose s0 , Bayesian RTDP uses a metric that combines
the BRTDP metric and the VPI value.
2.3 A Limitation of Previous Solvers
Value iteration backs up states iteratively based on some fixed order. Heuristic search backs up
states in a dynamic, informed order, implied by when they are visited in the search. A state can be
backed up in the pre-order (when it is first visited, e.g., variants of RTDP), or the post-order (when
searches back track, e.g., ILAO*). None of the algorithms use an MDPs graphical structure, an
intrinsic property that governs the complexity of solving a problem (Littman et al., 1995), in a way
to decide the order in which states are solved.
Consider a PhD program in some Finance department. Figure 1 shows an MDP that describes
the progress of a PhD student. For simplicity reasons, we omit the action nodes, the transition
probabilities, and the cost functions. The goal state set is a singleton G = {g}, which indicates
a student gets her PhD degree. A directed edge between two states means the head state is one
successor state of the tail state under at least one action. The initial state, s0 , describes the status
of an entry-level student. She has to first pass the qualifying exam, which consists of finding a
supervisor and passing an exam. Before passing the exam one can choose to work with a different
supervisor (back to state s0 in the figure). State s1 indicates the student has found a supervisor.
Then she works on her proposal, which consists of a written document and an oral exam. She has
187

fiDAI , M AUSAM , W ELD , & G OLDSMITH

s0

s1

s3

s2

s4

g

s4
Figure 1: A simple MDP example. The action nodes, the transition probabilities, and the cost functions are
omitted. The goal state set is a singleton G = {g}. A directed edge between two states means the
head state is one successor state of the tail state under some action.

to pass both in two consecutive quarters; otherwise back to state s2 . After passing the proposal, at
state s4 , she needs to defend her thesis, passing which reaches the goal state g.
Observing the MDP, we find the optimal order to back up states is s4 , then s2 and s3 , till
they converge, followed by s0 and s1 . The reason is that the value of s4 does not depend on the
values of other non-goal states. Similarly, the values of s2 and s3 do not depend on the values of
either s0 or s1 . Value iteration as well as heuristic search algorithms do not take advantage of the
graphical structure and apply this backup order, as they do not contain an intelligent subroutine
that discovers the graphical structure, nor use this information in the dynamic programming step.
The intuition of our new approaches is to discover the intrinsic complexity of solving an MDP by
studying its graphical structure, which later contributes to a more intelligent backup order.

3. Topological Value Iteration
We now describe the topological value iteration (TVI) algorithm (Dai & Goldsmith, 2007).
First observe that the value of a state depends on the values of its successors. For example,
suppose state s2 is a successor state of s1 under action a (Ta (s2 |s1 ) > 0). By the Bellman equations
V  (s1 ) is dependent on V  (s2 ). In this case, we define state s1 causally depends on state s2 . Note
that the causal dependence relationship is transitive. We can find out all causally dependent states
implicitly by building a reachability graph GR of the MDP. The set of vertices of GR equals the set
of states that are reachable from s0 . A directed edge from vertex s1 to s2 means that there exists
at least an action a  Ap(s1 ), such that Ta (s2 |s1 ) > 0. As the causal relationship is transitive, a
directed path from state s1 to sk in GR means s1 is causally dependent on sk , or V  (s1 ) depends on
V  (sk ). Also note that two vertices can be causally dependent on each other, which we call mutual
causal dependence.
Due to causal dependence, it is usually more efficient to back up s2 ahead of s1 . With this
observation, we have the following theorem.
Theorem 1 Optimal Backup Order (Bertsekas, 2001): If an MDP is acyclic, then there exists an
optimal backup order. By applying the optimal order, the optimal value function can be found with
each state needing only one backup.
The theorem is easy to prove and, furthermore, the optimal backup order is a topological order of
the vertices in GR . However, in general, MDPs contain cycles and it is common for one state to
mutually causally depend on another.
If two states are mutually causally dependent, the best order to back up them is unclear. On the
other hand, if neither state is causally dependent on the other, the order of backup does not matter.
Finally, if one state is causally dependent on the other (and not vice versa), it is better to order the
188

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

backups so that the state which is causally dependent is updated later. To apply this idea we then
group together states that are mutually causally dependent and make them a meta-state. We make
a new directed graph GM where a directed edge between two meta-states X and Y exists if and
only if there exists two states s1 and s2 and an action a  Ap(s1 ) such that s1  X , s2  Y and
Ta (s2 |s1 ) > 0. It is clear that GM is acyclic, otherwise all states on such a cycle are mutually
causally dependent, and by our construction rule they should belong to the same meta-state. In
this case, we can back up states in GM in their topological order. By Theorem 1, each such state
only requires one meta-backup. It is called a meta-backup since a meta-state may contain multiple
states. To perform a meta-backup, we can apply any dynamic programming algorithm, such as value
iteration, on all states belonging to the corresponding meta-state.
The pseudo-code of TVI is shown in Algorithm 2. We first apply Kosarajus algorithm (Cormen,
Leiserson, Rivest, & Stein, 2001) to find the set of strongly connected components (SCCs, or metastates) in the causality graph GR , and its topological order. (id[s] indicates the topological order
of the SCC that state s belongs to.) It is based on the fact that by reversing all the edges in GR ,
the resulting graph, G0R , has the same strongly connected components as the original. From using
that, we can get the SCCs by doing a forward traversal to find an ordering of vertices, followed by a
traversal of the reverse of the graph in the order generated by the first traversal. Kosarajus algorithm
is efficient, as its time complexity is linear in the number of states. When the state space is large,
running the algorithm leads to unavoidable yet acceptable overhead. In many cases the overhead is
well compensated by the computational gain. We then use value iteration to solve each SCC C (as a
meta-backup) in its topological order.
Algorithm 2 Topological Value Iteration
1: Input: an MDP M = hS, A, Ap, T, Ci, : the threshold value
2: SCC(M )
3: for i  1 to cpntnum do
4:
S 0  the set of states s where id[s] = i
5:
M 0  hS 0 , A, Ap, T, Ci
6:
VI(M 0 , )
7:

Function SCC(M )
construct GR of M
10: construct a graph G0R which reverses the head and tail vertices of every edge in GR
11: {call Kosarajus algorithm (Cormen et al., 2001). It inputs GR and G0R and outputs cpntnum,
the total number of SCCs, and id : S  [1, cpntnum], the id of the SCC each state belongs to,
by topological order.}
12: return (cpntnum, id)
8:

9:

3.1 Convergence
When the Bellman operator is a contraction operation (Bertsekas, 2001), we have:
Theorem 2 Topological Value Iteration is guaranteed to converge to a value function with a Bellman error that is no greater than .
189

fiDAI , M AUSAM , W ELD , & G OLDSMITH

Proof We first prove that TVI is guaranteed to terminate in finite time. Since each MDP contains
a finite number of states, it contains a finite number of connected components. In solving each of
these components, TVI uses value iteration. Because value iteration is guaranteed to converge in
finite time (given a finite ), TVI, which is essentially a finite number of value iterations, terminates
in finite time.
We then prove TVI is guaranteed to converge to an optimal value function with Bellman error
at most . We prove by induction.
First, if an MDP contains only one SCC, then TVI coincides with VI, an optimal algorithm.
By the contraction property of Bellman backups, when VI converges, the Bellman error of the state
space is at most .
Now, consider the case where an MDP contains multiple SCCs. At any point, TVI is working
on one component C. We know that the optimal value of every state s  C, V  (s), depends only
on the optimal values of the states that are descendants of s. We also know that any descendant s0
of s must belong either to C, or a component C 0 that is topologically no later than C. This means
either its value is computed by VI in the same batch as s (s0  C), or state s0 is already converged
(s0  C 0 ). In the latter case, its value is a convex combination of states having error at most . Inside
each maximization operation of an Bellman equation is an affine combination of values with a total
weight of 1, which leads to an overall convex combination error of no more than . Therefore, when
VI finishes solving C, the value of s must converge with Bellman residual at most . Also note that
the values of all states that belong to a component that is earlier than C does not depend on those
of states in component C. As a result, after component C converges, the Bellman residual of states
in those components remain unchanged and thus are at most . Combining the results we conclude
that when TVI terminates, the Bellman residuals of all states are at most . This means the Bellman
error of the state space is at most .
From the high-level perspective, TVI decomposes an MDP into sub-problems and finds the
value of the state space in a batch manner, component by component. When a component is converged, all its states will be safely treated as sink states, as their values do not depend on values of
states belonging to later components.
3.2 Implementation
We made two optimizations in implementing TVI. The first one is an uninformed reachability analysis. TVI does not depend on any initial state information. However, once given that information,
TVI is able to mark the reachable components and later ignore the unreachable ones in the dynamic
programming step. The reachable state space can be found by a depth-first search starting from s0 ,
with an overhead that is linear in |S| and |A|. It is extremely useful when only a small portion of
the state space is reachable (e.g., most domains from the International Planning Competition 2006,
see Bonet, 2006).
The second optimization is to use heuristic values Vl () as a starting point. We used the hmin
(Bonet & Geffner, 2003b), an admissible heuristic:
hmin (s) = 0 if s  G, else


hmin (s) =
min C(s, a) + mins0 :Ta (s0 |s)>0 hmin (s0 ) .
aAp(s)

(7)

To implement it, we first construct a new deterministic problem. For each action and successor
pair of the original MDP, we add to the new problem a deterministic action with the same cost
190

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

and the same, deterministic successor. We then solve this new problem by a single, backward,
breadth-first search from the set of goal states. Values of the deterministic problem are hmin .
3.3 Experiments
We address the following questions in our experiments: (1) How does TVI compare with VI and
heuristic search algorithms on MDPs that contain multiple SCCs? (2) What are the most favorable
problem features for TVI?
We compared TVI with several other optimal algorithms, including VI (Bellman, 1957), ILAO*
(Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003b), BRTDP (McMahan et al., 2005),
Bayesian RTDP (Sanner et al., 2009) (BaRTDP), and HDP (Bonet & Geffner, 2003a)1 . We used
the fully optimized C code of ILAO* provided by Eric A. Hansen and additionally implemented
the rest of the algorithms over the same framework. We performed all experiments on a 2.5GHz
Dual-Core AMD Opteron(tm) Processor with 2GB memory. Recall that BRTDP and BaRTDP use
upper bounds. We used upper bounds as described in Section 4.2. We used  = 2  106 and
 = 10 for BRTDP and BaRTDP.2 For BaRTDP, we used the probabilistic termination condition in
Algorithm 3 of Sanner et al. (2009). 3
We compared all algorithms on running time, time between an algorithm starts solving a problem until generating a policy with a Bellman error of at most (= 106 ). We terminated an algorithm if it did not find such a policy within five minutes. Note that there are other performance
measures such as anytime performance (the original motivation of BaRTDP) and space consumption, but the main motivation of TVI is to decrease convergence time. We expect TVI to have a
very steep anytime performance curve, because it postpones backing up the initial state till it starts
working on the SCC where the initial state belongs to. Space, on the other hand, is less interesting
because in-memory MDPs algorithms requires that the MDP model stored in the main memory before dynamic programming can apply. Therefore, they all share the same space limit. For work on
overcoming space limitation, see, for example the work of Dai et al. (2008, 2009a).
We tested all algorithms on a set of artificially-generated layered MDPs. For each such MDP
of state size |S|, we partition the state space evenly into a number nl of layers, labeled by integers
1, . . . , nl . We allow states in higher numbered layers to be the successors of states in lower numbered layers, but not vice versa, so each state s only has a limited set of allowable successor states,
named succ(s). A layered MDP is parameterized by two other variables: the number of actions
per state, na , and the maximum number of successor states per action, ns . When generating the
transition function of a state-action pair (s, a), we draw an integer k uniformly from [1, ns ]. Then
k distinct successors are uniformly sampled from succ(s) with random transition probabilities. We
pick one state from layer nl as the only goal state. One property of a layered MDP is that it contains
at least nl connected components.
1. Notice that this comparison is somewhat unfair to TVI, since heuristic search algorithms may not expand portions
of the state space, if their sub-optimality can be proved. Still, we make this comparison to understand the practical
benefits of TVI v.s. all other known optimal MDP algorithms
2.  is the termination threshold of BRTDP (it terminates when vu (s0 )  Vl (s0 ) < ).  indicates the stopping
condition of each heuristic search trial. For more detailed discussions on the two parameters, please refer to the work
of McMahanet al. (2005). We carefully tuned these parameters.
3. This termination condition may result in sub-optimal policies, so the reported times of BaRTDP in this paper are
lower bounds. Note that BaRTDP mainly aims at improving the anytime performance of RTDP, which is orthogonal
to convergence time. We report its convergence speed for thorough investigation purposes.

191

fiRunning time (seconds)

DAI , M AUSAM , W ELD , & G OLDSMITH

100

VI
ILAO*
LRTDP

10

TVI
BRTDP

1
1

10
100
Number of layers

1000

BaRTDP

Figure 2: Running times of algorithms with different number of layers nl on random layered MDPs with
|S| = 50000, na = 10, and ns = 10. Note that the two coordinates are both log-scaled. When
nl > 10 TVI not only outperforms VI, but also other state-of-the-art heuristic search algorithms.

There are several planning domains that lead to multi-layered MDPs. An example is the game
Bejeweled, or any game with difficulty levels: each level is at least one layer. Or consider a chess
variant without pawn promotions, played against a stochastic opponent. Each set of pieces that
could appear on the board together leads to at least one strongly connected component. But we
know of no multi-layered standard MDP benchmarks. Therefore, we compare, in this section, on
artificial problems to study TVIs performance across controlled parameters, such as nl and |S|.
Next section contains more comprehensive experiments on benchmark problems.
We generated problems with different parameter configurations and ran all algorithms on the
same set of problems. The running times, if the process converged within the cut-off, are reported
in Figures 2 and 3. Each element of the table represents the median convergence time of running
10 MDPs with the same configuration.4 Note that varying |S|, nl , na , and ns yields many MDP
configurations. We tried more combinations than the representative ones reported. We found HDP
much slower than the other algorithms, so did not include its performance.
For the first experiment, we fixed |S| to be 50,000 and varied nl from 1 to 1,000. Observing
Figure 2 we first find that, when there is only one layer, the performance of TVI is slightly worse
than VI, as such an MDP probably contains an SCC that contains the majority of the state space,
which defeats the benefit of TVI. But TVI consistently outperforms VI if nl > 1. When nl  10,
TVI equals or beats ILAO*, the fastest heuristic search algorithm for this set of problems. When
nl > 10, TVI outperforms all the other algorithms in all cases by a visible margin. Also note that,
as the number of layers increases the running times of all algorithms decrease. This is because
4. We picked median instead of mean just to avoid an unexpected hard problem, which takes a long time to solve,
thereby dominating the performance.

192

fiRunning time (seconds)

T OPOLOGICAL VALUE I TERATION A LGORITHMS

162.31

60
50
40
30
20
10
0

VI
ILAO*
LRTDP
TVI
BRTDP
0

50000
State space size

100000

BaRTDP

Figure 3: Running times of algorithms with different state space size |S| with fixed nl = 100, na = 10, and
ns = 10. TVI not only outperforms VI, but also other state-of-the-art heuristic search algorithms.
The relative performance of TVI improves as |S| increases.

the MDPs become more structured, therefore simpler to solve. The running time of TVI decreases
second fastest to that of LRTDP. LRTDP is very slow when nl = 1 and its running time drops
dramatically when nl increases from 1 to 20. As TVI spends nearly constant time in generating the
topological order of the SCCs, its fast convergence is mainly due to the fact that VI is much more
efficient in solving many small (and roughly equal-sized) problems than a large problem whose size
is the same as the sum of the small ones. This experiment shows TVI is good at solving MDPs with
many SCCs.
For the second experiment, we fixed nl to be 100 and varied |S| from 10,000 to 100,000. We
find that, when the state space is 10,000 TVI outperforms VI, BRTDP and BaRTDP, but slightly
underperforms ILAO* and LRTDP. However, as the problem size grows TVI soon takes the lead. It
outperforms all the other algorithms when the state space is 20,000 or larger. When the state space
grows to 100,000, TVI solves a problem 6 times as fast as VI, 4 times as fast as ILAO*, 2 times as
fast as LRTDP, 21 times as fast as BRTDP, and 3 times as fast as BaRTDP. This experiment shows
that TVI is even more efficient when the problem space is larger.

4. Focused Topological Value Iteration
Topological value iteration improves the performance of value iteration most significantly when an
MDP has many equal-sized strongly connected components. However, we also observe that many
MDPs do not have evenly distributed connected components. This is due to the following reason:
a state can have many actions, most of which are sub-optimal. These sub-optimal actions, although
not part of an optimal policy, may lead to connectivity between a lot of states. For example, domains
like Blocksworld have reversible actions. Due to these actions most states are mutually causally
193

fiDAI , M AUSAM , W ELD , & G OLDSMITH

s4

a7

a5
C13 a6

s1

a3
s5

s3

C11 a
1

a4

C1

s2
C12

a12 s7

C21

a2

a9
a8

a11
a10
s6
C22

C2

Figure 4: The graphical representation of an MDP and its set of strongly connected components (before and
after the knowledge of some sub-optimal actions). Arcs represent probabilistic transitions, e.g.,
a7 has two probabilistic successors  s5 and s7 .

dependent. As a result, states connected by reversible actions end up forming a large connected
component, making TVI slow.
On the other hand, heuristic search is a powerful solution technique, which successfully concentrates computation, in the form of backups, on states and transitions that are more likely to be
part of an optimal policy. However, heuristic search uses the same backup strategy on all problems,
thus missing out on the potential savings from knowing the graphical structure information.
If we knew about the existence of an action in the optimal policy, we could eliminate the rest
actions for its outgoing state, thus breaking some connectivity. Of course, such information is never
available. However, with a little help from heuristic search, we can eliminate sub-optimal actions
from a problem leading to a reduced connectivity and hopefully, smaller sizes of strongly connected
components.
Figure 4 shows the graphical representation of a part of one simple MDP that has 7 states
and 12 actions. In the figure, successors of probabilistic actions are connected by an arc. For
simplicity, transition probabilities Ta , costs C(s, a), initial state and goal states are omitted. Using
TVI, we can divide the MDP into two SCCs C1 and C2 . However, suppose we are given some
additional information that a5 and a12 are sub-optimal. Based on the remaining actions, C1 and C2
can be sub-divided into three and two smaller components respectively (as shown in the figure).
Dynamic programming will greatly benefit from the new graphical structure, since solving smaller
components can be much easier than a large one.
4.1 The FTVI Algorithm
The key insight of our novel algorithm is to break the big components into smaller parts, by removing actions that can be proven to be suboptimal for the current problem at hand. This exploits
the knowledge of the current initial state and goal, which TVI mostly ignores. We call our new
algorithm focused topological value iteration (FTVI) (Dai et al., 2009b). The pseudo-code is shown
in Algorithm 3.
At its core, FTVI makes use of the action elimination theorem, which states:
194

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

Theorem 3 Action Elimination (Bertsekas, 2001): If a lower bound of Q (s, a) is greater than an
upper bound of V  (s) then action a cannot be an optimal action for state s.
This gives us a template to eliminate actions, except that we need to compute a lower bound
for Q and an upper bound for V  . FTVI keeps two bounds of V  simultaneously: the lower
bound Vl () and the upper bound Vu (). Vl () is initialized via the admissible heuristic. We note
two properties of Vl : (1) Ql (s, a) computed by a one-step lookahead given the current lower bound
value Vl () (Line 30, Algorithm 3) is a lower bound of Q (s, a), and (2) all the V values remain
lower bounds throughout the algorithm execution process, if they were initialized by an admissible
heuristic. So, this lets us easily compute a lower bound of Q , which also improves as more backups
are performed.
Similar properties hold for Vu , the upper bound of V  , i.e., if we initialize Vu by an upper bound
and perform backups based on Vu then each successive value estimate remains an upper bound. The
later implementation section lists our exact procedure to compute the lower and upper bounds in
a domain-independent manner. We note that to employ action elimination we can use any lower
and upper bounds, so if a domain has informative, domain-dependent bounds available, that can be
easily plugged into FTVI.
FTVI contains two sequential steps. In the first step, which we call the search step, FTVI
performs a small number of heuristic searches similar to ILAO*, i.e., backs up a state at most once
per iteration. This makes the searches in FTVI fast, but still useful enough to eliminate sub-optimal
actions. There are two main differences in common heuristic search and the search phase of FTVI.
First, in each backup, we update the upper bound in the same manner as the lower bound. This is
reminiscent of backups in BRTDP (McMahan et al., 2005). Second, we also check and eliminate
sub-optimal actions using action elimination (Lines 3032).
In the second step, the computation step, FTVI generates a directed graph GSR in the same
manner as TVI generates GR , but only based on the remaining actions. More concretely, a directed
edge from vertex s1 to s2 exists if there is an uneliminated action a such that Ta (s2 |s1 ) > 0. It
is easy to see that the graph GSR generated is always a sub-graph of GR . FTVI then finds all
connected components of GSR , their topological order, and solves each component sequentially in
the topological order.
We can state the following theorem for FTVI.
Theorem 4 FTVI is guaranteed to converge to the optimal value function.
The correctness of the theorem is based on two facts: (1) action elimination preserves soundness,
and (2) TVI is an optimal planning algorithm (Theorem 2).
4.2 Implementation
There are several interesting questions to answer in implementation. How to calculate the initial
upper and lower bounds? How many search iterations do we need to perform in the search step? Is
it possible that FTVI converges in the search step? What if there still remains a large component
even after action elimination?
We used the same lower bound Vl as in TVI (see Section 3.2). For the upper bound, we started
with a simple upper bound:
195

fiDAI , M AUSAM , W ELD , & G OLDSMITH

Algorithm 3 Focused Topological Value Iteration
1: Input: an MDP hS, A, Ap, T, Ci, x: the number of search iterations in a batch, y: the lower bound of

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:

the percentage of change in the initial state value for a new batch of search iterations, : the threshold
value
{step 1: search}
while true do
old value  Vl (s0 )
for iter  1 to x do
Bellman error  0
for every state s do
mark every state as unvisited
s  s0
Search(s)
if Bellman error <  then {The value function converges}
return Vl
if old value/Vl (s0 ) > (100  y)% then
break
{step 2: computation}
M  hS, A, Ap, T, Ci
TVI(M , ) {by applying the backup operator with action elimination}
Function Search(s)
if s 
/ G then
mark s as visited
a  argmina Q(s, a)
for every unvisited successor s0 of action a do
Search(s0 )
Bellman error  max(Bellman error, Back  up(s))

Function Back  up(s)
for each action a do
P
Q(s, a)  C(s, a) + s0 S Ta0 (s0 |s)Vl (s0 )
if Ql (s, a) > Vu (s) then
eliminate a from Ap(s)
oldVl  Vl (s)
Vl (s)  minaAp(s) Q(s, a)
P
Vu (s)  minaAp(s) [C(s, a) + s0 S Ta0 (s0 |s)Vu (s0 )]
36: return |Vl (s)  oldVl |

Vu (s) = 0 if s  G, else Vu (s) = .

(8)

This initialization gives us a global yet very loose upper bound. To improve its tightness, we
performed a backward best-first search from the set of goal states. States visited have their Vu values
updated as in Algorithm 3, Line 35. We can iteratively get tighter and tighter bounds when more
backward searches are performed.
The time spent on search can have a significant impact on FTVI. Very few search iterations
might not eliminate enough sub-optimal actions. However, too many search iterations will turn
196

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

50
40
30

20
10
0
1

10
100
1000
Heuristic search trial #

10000

Running time (seconds)

Running time (seconds)

FTVI into a heuristic search algorithm and trade off the advantage of FTVI. We did a control experiment by varying the total number of heuristic search trials on two problems. Figure 5 shows that
the performance on a Wet-floor problem matches our hypothesis perfectly. For the Drive problem,
the number of search trials does not affect the convergence speed too much, but too many search
trials turn out to be harmful.
15
10
5
0
1

10
100
1000
Heuristic search trial #

10000

Figure 5: Running times of FTVI with different number of initial search trials on (left) a Wet-floor problem
and (right) a Drive problem. Too few trials are sometimes less helpful for eliminating enough
sup-optimal actions, and too many trials are harmful.

Considering the tradeoff, we let the algorithm automatically determine the number of search
iterations. FTVI incrementally performs a batch of x search iterations. After the batch, it computes
the amount of change to the Vl (s0 ) value. If the change is greater than y%, a new batch of search
is performed. Otherwise, the search phase is considered complete. In our implementation, we use
x = 100, and y = 3.
An interesting case occurs when the optimal value is found during the search step. Although
FTVI performs a limited number of search iterations, it is possible that a problem is optimally solved
within the search step. It is helpful to keep track of optimality information during the search step, so
that FTVI can potentially skip some unnecessary search iterations and the entire computation step.
To do this, we only need to maintain a Bellman error of the current search iteration, and terminate
FTVI if the error is smaller than the threshold (Lines 1112). In our experiment, we find this simple
optimization to be extremely helpful in promoting the performance of FTVI.
Sometimes there are cases where GSR still contains large connected components. This can
be caused by two reasons (1) An optimal policy indeed has large components, or (2) the connectivity caused by many suboptimal actions is not successfully eliminated by search. To try to further decompose these large components, we let FTVI perform additional intra-component heuristic
searches. An intra-component heuristic search takes place only inside a particular component. Its
purpose is to find new, sub-optimal actions, which might help decompose the component. Given a
component C of GSR , we define SourceC to be the set of states where none of its incoming transitions are from states in C. In other words, states in SourceC are the incoming bridge states between
C and rest of the MDP. An intra-component heuristic search of C originates from a state in SourceC .
A search branch terminates when a state outside C is encountered.
We did some experiments and compared the performance of FTVI with and without additional
intra-component search on problems from four domains, namely Wet-floor (Bonet & Geffner, 2006),
197

fiDAI , M AUSAM , W ELD , & G OLDSMITH

Single-arm pendulum (Wingate & Seppi, 2005), Drive, and Elevator (Bonet, 2006). Our results
show that additional intra-component search only provided limited gains in Wet-floor problems, in
which it helped decrease the size of the largest components by approximately 50% on average, and
sped up the convergence by 10% at best. However, intra-component search turned out to be harmful
for the other domains, as it did not provide any new graphical information (no smaller components
were generated). On the contrary, the search itself introduced a lot of unnecessary overhead. So we
used the version that does not perform additional intra-component search throughout the rest of the
experiments.
4.3 Experiments
We address the following two questions in our experiments: (1) How does FTVI compare with
other algorithms on a broad range of domain problems? (2) What are the specific kind of domains
on which FTVI should be preferred over heuristic search?
We implemented FTVI on the same framework as in Section 3.3, and used the same cut-off time
of 5 minutes for each algorithm per problem. To investigate the helpfulness of action elimination,
we also implemented a VI variant that applies action elimination in backups. We used the same
threshold value  = 106 , and ran BRTDP and BaRTDP on the same upper bound as FTVI.
4.3.1 R ELATIVE S PEED OF FTVI
Problem
MCar100
MCar300
MCar700
SAP100
SAP300
SAP500
WF200
WF400
DAP10
DAP20
Drive
Drive
Drive
Elevator (IPPC p13)
Elevator (IPPC p15)
Tireworld (IPPC p5)
Tireworld (IPPC p6)
Blocksworld (IPPC p4)
Blocksworld (IPPC p5)

VI
1.40
26.12
278.16
2.30
42.61
174.71
19.95
105.79
0.77
21.41
2.00
20.58
236.91
33.88
47.88
17.69
14.19

VI (w/ a.e.)
0.74
13.40
124.34
1.06
19.90
77.99
13.71
98.97
0.67
17.62
1.39
14.20
133.80
16.46
23.04
17.69
14.19

ILAO*
1.91
11.91
101.65
1.81
32.40
131.17
11.22
73.88
1.01
32.68
1.60
96.09
227.53
27.35
0.00
0.00
0.02
0.00

LRTDP
1.23
229.70
2.58
51.45
0.69
273.37
0.14
0.16
0.26
0.11

BRTDP
2.81
117.23
216.01
9.39
22.08
97.73
3.04
144.12
7.85
163.91
0.01
0.01
1.93
0.66

BaRTDP
63.55 (*)
180.64 (*)
262.92 (*)
111.59 (*)
1.99 (*)
103.87 (*)
222.33 (*)
4.17 (*)
4.17 (*)
3.94 (*)
0.03
0.04
-

TVI
0.68
23.22
233.98
2.37
44.2
20.58
100.78
0.75
21.95
1.23
13.03
74.70
58.46
14.59
2.26
48.81
54.35
54.34

FTVI
0.22
2.35
13.06
0.17
2.96
9.56
8.81
74.24
0.59
17.49
1.07
10.63
41.93
54.11
12.11
0.00
0.00
0.02
0.00

Table 1: Total running times of the different algorithms on problems in various domains. FTVI outperforms
all algorithms by vast margins. (Fastest times are bolded. - in Time means that the algorithm
failed to solve the problem within 5 minutes. The *s mean the algorithm terminated with suboptimal solutions.)

198

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

Problem
MCar100
MCar300
MCar700
SAP100
SAP300
SAP500
WF200
WF400
DAP10
DAP20
Drive
Drive
Drive
Elevator (IPPC p13)
Elevator (IPPC p15)
Tireworld (IPPC p5)
Tireworld (IPPC p6)
Blocksworld (IPPC p4)
Blocksworld (IPPC p5)

Reachable |S|
10,000
90,000
490,000
10,000
90,000
250,000
40,000
160,000
10,000
160,000
4,563
29,403
75,840
539,136
539,136
671,687
724,933
103,121
103,121

TVI
BC size
T ime
7,799
0.68
71,751
23.22
390,191 233.98
9,999
2.37
89,999
44.2
39,999
20.58
159,999 100.78
9,454
0.75
150,489
21.95
4,560
1.23
29,400
13.03
75,840
74.70
1,053
58.46
1,053
14.59
23
2.26
618,448
48.81
103,104
54.35
103,204
54.34

BC size
1
1
1
n/a
n/a
n/a
15,039
141,671
n/a
n/a
4,560
29,400
75,840
1,053
1,053
n/a
n/a
n/a
n/a

FTVI
Tsearch
0.20
2.22
12.29
0.17
2.96
9.56
3.30
14.27
0.59
17.49
0.11
0.15
0.18
0.01
0.01
0.00
0.00
0.02
0.00

Tgen
0.01
0.13
0.76
n/a
n/a
n/a
0.12
0.36
n/a
n/a
0.02
0.15
0.40
1.73
1.60
n/a
n/a
n/a
n/a

T ime
0.22
2.35
13.06
0.17
2.96
9.56
8.81
74.24
0.59
17.49
1.07
10.63
41.93
54.11
12.11
0.00
0.00
0.02
0.00

Table 2: Detailed performance statistics for TVI and FTVI. (BC size means the size of the biggest connected
component. n/a means FTVI converged in the search step and skipped the computation step. All
running times are in seconds. Tsearch represents the time used by the search step, and Tgen the
time spent in generating the graphical structure. Fastest times are bolded. - in Time means that
the algorithm failed to solve the problem within 5 minutes.)

We evaluated the various algorithms on problems from eight domains  Mountain Car, Single
and Double Arm Pendulum (Wingate & Seppi, 2005), Wet-floor (Bonet & Geffner, 2006)5 , and
four domains from International Planning Competition 2006  Drive, Elevators, TireWorld and
Blocksworld. A mountain car problem usually has many source states.6 We chose each source state
as an initial state, and averaged the statistics per problem. Table 1 lists the running times for the
various algorithms. For FTVI, we additionally report (in Table 2) the time used by the searches
(Tsearch ), and the time spent in generating the graphical structure (Tgen ), if a problem is not solved
during the search phase, where the leftover is the time spent in solving the SCCs. We also compared
the size of the biggest component (BC size) generated by TVI and FTVI.
Overall we find that FTVI outperforms the other five algorithms on most of these domains.
FTVI outperforms TVI in all domains. Notice that on the MCar problems, FTVI establishes very
favorable graphical structures (strongly connected components of size one) during the search step.7
This graphical structure makes the second step of FTVI trivial. But TVI has to solve much bigger
components, so it runs much slower. For the Drive domain, even if it does not find a more informed
graphical structure, the advanced backup with action elimination enables FTVI converge faster.
5. Note that we used the probability of wet cells, p = 0.5.
6. A source state is a state with no incoming transitions.
7. If we allow FTVI to perform the computation step as opposed to stop at the search step when a problem is solved, it
will find similar structures in the Tireworld and Blocksworld problems.

199

fiDAI , M AUSAM , W ELD , & G OLDSMITH

FTVI outperforms heuristic search algorithms most significantly in domains such as MCar, SAP
and Drive. It is faster than ILAO* by an order of magnitude. This shows the extreme effectiveness
of FTVIs decomposing a problem into small sub-problems using advanced graphical information
and solving these sub-problems sequentially. The three RTDP algorithms are not competitive with
the other algorithms in these domains, and fail to return a solution by the cutoff time for many
problems. FTVI shows limited speedup against heuristic search in domains such as Wet-floor, DAP,
and Elevator. FTVI is on par with ILAO*, and vastly outperforms TVI in Tireworld and Blocksworld
domains, as it converges within the search step. The convergence speed of value iteration is typically
slow, as it backs up states iteratively by a fixed order. Adding action elimination to Bellman backups
increases the convergence speed of VI up to two times, especially in the Mountain Car, Single Arm
Pendulum, and Elevator domains, but its convergence speed is usually at least one magnitude slower
than those of FTVI.
4.3.2 FACTORS D ETERMINING P ERFORMANCE
We have shown that FTVI is faster than heuristic search algorithms in many domains, but its relative
speedup is domain-dependent. Can we find any domain features that are particularly beneficial for
FTVI or worse for heuristic search algorithms? In this evaluation we performed control experiments
by varying the domains across different features and study the effect on planning time of various
algorithms.
We make an initial prediction of three features.
1. The number of goals in the domain: If the number of goal states is small, search may take a
long time before it discovers a path to a goal. Therefore, many sub-optimal policies might be
evaluated by a heuristic search algorithm.
2. Search depth from the initial state to a goal state: This depth is a lower bound of the length
of an execution trial and also of the size of any policy graph. A greater depth implies more
search steps per iteration, which might make evaluating a policy time-consuming.
3. Heuristic informativeness: The performance of a heuristic search algorithm depends a lot on
the quality of the initial heuristic function. We expect the win from FTVI to increase when
heuristic is less informed.
The Number of Goals. As far as we know, there is no suitable domain where we can specify
the total number of goal states arbitrarily, so we used an artificial domain. In this domain each
state has two applicable actions, and each action has at most two random successors. We tested all
algorithms on domains of two sizes, 10,000 (Figure 6(left)) and 50,000 (Figure 6(right)). For each
problem size, we fixed the shortest goal distance but varied the number of goal states, |G|. More
concretely, after generating the state transitions, we performed a BFS from the initial state, and
randomly picked goal states on a same search depth. For each |G| value, we generated 10 problems,
and reported the median running time of four algorithms (LRTDP and BaRTDP were slow in this
domain). We observe that all algorithms take more time to solve a problem with a smaller number
of goal states than with a larger number. However, beyond a point (|G| > 20 in our experiments),
the running times become stable. FTVI runs only marginally slower when |G| is small, suggesting
that its performance is less dependent on the number of goal states. BRTDP is the second best in
handling small goal sets, and it runs nearly as fast as FTVI when the goal set is large. Even though
200

fi1.4
1.2
1
0.8
0.6
0.4
0.2
0

ILAO*

Running time (seconds)

Running time (seconds)

T OPOLOGICAL VALUE I TERATION A LGORITHMS

TVI
FTVI
BRTDP

0

20
40
60
80
Number of goal states

100

7
6
5
4
3
2
1
0

ILAO*
TVI
FTVI
BRTDP

0

20

40
60
80
Number of goal states

100

Figure 6: Running times of algorithms with different number of goal states and problem size (left) |S| =
10, 000 (right) |S| = 50, 000 in random MDPs. FTVI and TVI slow down the least significantly
when the number of goal states is small.

TVI runs the slowest among the four algorithms, its performance shows less severe dependence on
the number of goal states. It runs almost as fast as ILAO* when the goal set size is 1. In contrast,
ILAO* runs twice as fast as TVI when the goal set size is greater than 20.
Search Depth. In this experiment, we studied how the search depth of a goal from the initial
state influences the performance of various algorithms. We chose a Mountain car problem and a
Single-arm pendulum problem. We randomly picked 100 initial states from the state space8 and
measured the shallowest search depth, or, the shortest distance, d, to a goal state. The running times
in Figure 7 are ordered by d. BaRTDP does not terminate with an optimal policy for many instances,
so its performance is not shown. BRTDP has the biggest variance so its performance is not included
for clarity purposes.
As we can see, FTVI is the fastest algorithm in this suite of experiments. It converges very
quickly for all initial states (usually around one or two seconds on Mcar300, and less than 10 seconds on SAP300). TVIs performance is unaffected by the search depth, which is expected, since
it is a variant of value iteration and has no search component. In the MCar300 problem, we do not
find strong evidence that the running time of any algorithm depends on the search depth. FTVI runs
an order of magnitude faster than TVI, ILAO*, and BRTDP and two orders of magnitude faster
than LRTDP. In the SAP300 problems, the running times of all algorithms except TVI increase as
search depth increases. LRTDP runs fast when d is relatively small, but it slows down considerably
and is unable to solve many problems when d becomes larger. ILAO*s convergence speed varies
a bit when the distance is small. As d increases, its running time also increases. BRTDPs performance (not included) is close to that of ILAO* when d is small, but becomes slower and performs
similar to LRTDP when d is large. In this problem, heuristic search algorithms unanimously suffer
significantly from the increase in the search depth, as their running times increase by at least two
orders of magnitude from small to large d values. On the other hand, FTVI slows down by only one
order of magnitude, which makes it converge one order of magnitude faster than ILAO*, one to two
orders of magnitude faster than BRTDP and TVI, and two orders of magnitude faster than LRTDP
for large depths.
8. Note that these problems have well-defined initial states. Here we picked initial states arbitrarily from S.

201

fiDAI , M AUSAM , W ELD , & G OLDSMITH

1000

TVI

100

FTVI

10
1
0.1

0

200

0.01

0.001

Running time (seconds)

Running time (seconds)

1000

FTVI

10
1
0

100

200

Running time (seconds)

Running time (seconds)

FTVI

1
0.1

0

200

0.01
Shortest goal distance

1000

TVI

100

0.01

LRTDP

10

0.001

Shortest goal distance

1000

0.1

ILAO*

100

LRTDP

10

FTVI

1
0.1
0.01

Shortest goal distance

ILAO*

100

0

100

200

Shortest goal distance

Figure 7: Running times of algorithms with different shortest distance to the goal on (top) mountain car
300  300 (MCar300), (bottom) single-arm pendulum 300  300 (SAP300) problems, (left) comparison of FTVI and TVI, and (right) comparison of FTVI and heuristic search algorithms. Heuristic search algorithms slow down massively (note the log scale) when the search depth is large.

Heuristic Quality. Finally we studied the effect of the heuristic informativeness on the algorithms. We conducted two sets of experiments, based on two sets of consistent heuristics. We found
BRTDP slower than other algorithms in all problems and BaRTDP to be comparable (about 50%
slower than LRTDP) only on the Wet100 problem, so did not include their running times. In the
first experiment, we pre-computed the optimal value function of a problem using value iteration,
and used a fraction of the optimal value as an initial heuristic. Given a fraction f  (0, 1], we
calculated h(s) = f  V  (s). Figure 8 plots the running times of different algorithms against f for
three problems. Note that f = 1 means the initial heuristic is already optimal, so a problem is trivial
for all algorithms, but TVI has the overhead of building a topological structure. FTVI, however, is
able to detect convergence in the search step and circumvent this overhead, so it is fast. LRTDP is
slow in the Wet100 problem, so its running times in that problem are omitted from the figure. The
figure shows that as f increases (i.e. as the heuristic becomes more informative) the running times
of all algorithms decrease almost linearly. This is true even for TVI, which is not a heuristic-guided
algorithm, but takes less time, probably because the initial values affect the number of iterations
required until convergence.
To thoroughly study the influence of the heuristics, we conducted a second set of experiments.
In this experiment, we used a fractional Vl value as our initial heuristic. Recall that Vl is a lower
bound of V  computed by the value of a deterministic problem. We calculated the initial heuristic
202

fiILAO*
LRTDP

0.5

TVI
FTVI

0
0

0.5
f

1

3
2.5
2
1.5
1
0.5
0

ILAO*
LRTDP
TVI
FTVI
0

0.5
f

1

5
4
3

ILAO*

2

TVI

1

FTVI

0
0

0.5
f

1

Running time (seconds)

1

Running time (seconds)

1.5

Running time (seconds)

Running time (seconds)

Running time (seconds)

Running time (seconds)

T OPOLOGICAL VALUE I TERATION A LGORITHMS

8
6
ILAO*

4

LRTDP

2

TVI

0

FTVI
0

0.5
f

1

3
2.5
2
1.5
1
0.5
0

ILAO*
LRTDP
TVI
FTVI
0

0.5
f

1

5
4
3

ILAO*

2

TVI

1

FTVI

0
0

0.5
f

1

Figure 8: Running times of algorithms with different initial heuristic on (top) mountain car 100  100
(MCar100), (middle) single-arm pendulum 100100 (SAP100), and (bottom) wet-floor 100100
(WF100)
problems.
sensitive
P
P All algorithms are equally
P
P to the heuristic informativeness. (left) f
= sS h(s)/ sS V  (s) (right) f = sS h(s)/ sS Vl (s).

by h(s) = f  Vl (s). All included algorithms show a similar smooth decrease in running time when
f increases. BRTDP, however, shows strong dependence on the heuristics in the Wet100 problem.
Its running time decreases sharply from 96.91 seconds to 0.54 seconds and from 99.81 seconds to
6.21 seconds from when f = 0.02 to when f = 1 in the two experiments. Stable changes in the two
experiments suggests the following for algorithms except BRTDP. (1) No algorithm is particularly
vulnerable to a less informed heuristic function; (2) extremely informative heuristics (when f is
very close to 1) do not necessarily lead to extra fast convergence. This result is in-line with results
for deterministic domains (Helmert & Roger, 2008).
203

fiDAI , M AUSAM , W ELD , & G OLDSMITH

4.3.3 D ISCUSSION
From the experiments, we learn that FTVI is vastly better in domains whose problems have a small
number of goal states and a long search depth from the initial state to a goal (such as MCar, SAP
and Drive). But the convergence control module of FTVI helps in successfully matching the performance of FTVI with the fastest heuristic search algorithm. In addition, FTVI displays limited
advantage over heuristic search in the two intermediate cases where a problem has (1) many goal
states but long search depth (Elevator), or (2) a short depth but fewer goal states (DAP). In conclusion, FTVI is our algorithm of choice whenever a problem has either a small number of goal states
or a long search depth.

5. Related Work
Besides TVI several other researchers have proposed decomposing an MDP into sub-problems and
combining their solutions for the final policy, e.g., the work of Hauskrecht et al. (1998) and Parr
(1998). However, these approaches typically assume some additional structure in the problem,
either known hierarchies, or known decomposition into weakly coupled sub-MDPs, etc., whereas
FTVI assumes no additional structure.
BRTDP (McMahan et al., 2005), Bayesian RTDP (Sanner et al., 2009) and Focused RTDP
(Smith & Simmons, 2006) (FRTDP) also keep an upper bound for the value function. However, all
algorithms use the upper bound purely to judge how close a state is to convergence, by comparing
the difference between the upper and lower bound values. For example, BRTDP tries to make
searches focus more on states whose two bounds have larger differences, or intuitively, states whose
values are less converged. Unlike FTVI, all three algorithms do not perform action elimination, nor
do they use any connected component information to solve an MDP. The performance of BRTDP
(and similarly Bayesian RTDP) is highly dependent on the quality of the heuristics. Furthermore,
FRTDP only works for the discounted setting, thus is not immediately applicable for stochastic
shortest path problems.
HDP is similar to TVI in the sense that it uses the Tarjans algorithm (slightly different from the
Kosarajus algorithm) to find the strongly connected components of a greedy graph. It computes
the SCCs multiple times and dynamically during the depth-first searches when HDP tries to label
solved states. But it does not find the topological order of the SCCs nor decompose a problem and
use the topological order to sequentially solve each SCC.
Prioritized sweeping (Moore & Atkeson, 1993) and its extensions, focussed dynamic programming (Ferguson & Stentz, 2004) and improved prioritized sweeping (McMahan & Gordon, 2005),
order backups intelligently with the help of a priority queue. Each state in the queue is prioritized
based on the potential improvement in value of a backup over that state. Dai and Hansen (2007)
demonstrate that these algorithms have large overhead in maintaining a priority queue so they are
outperformed by a simple backward search algorithm, which implicitly prioritizes backups without
a priority queue. Moreover, prioritized sweeping and improved prioritized sweeping find the optimal
value of the entire state space of an MDP, as they do not use the initial state information. Focussed
dynamic programming, however, is able to make use of the initial state information, but it is not
an optimal algorithm. All three algorithms are massively outperformed by an LAO* variant (Dai &
Hansen, 2007).
When an MDP is too large to be solved optimally, another thread of work solves MDPs approximately. The typical way to do this is to use deterministic relaxations of the MDP and/or basis
204

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

functions (Guestrin, Koller, Parr, & Venkataraman, 2003; Poupart, Boutilier, Patrascu, & Schuurmans, 2002; Patrascu, Poupart, Schuurmans, Boutilier, & Guestrin, 2002; Yoon, Fern, & Givan,
2007; Kolobov, Mausam, & Weld, 2009, 2010a, 2010b). The techniques of these algorithms are
orthogonal to the ones by FTVI, and an interesting future direction is to approximate FTVI by
applying basis functions.
When an MDP maintains a logical representation, another type of algorithm aggregates groups
of states of an MDP by features, represents them as a factored MDP using algebraic and Boolean decision diagrams (ADDs and BDDs) and solves the factored MDP using ADD and BDD operations;
SPUDD (Hoey, St-Aubin, Hu, & Boutilier, 1999), sLAO* (Feng & Hansen, 2002), sRTDP (Feng,
Hansen, & Zilberstein, 2003) are examples. The factored representation can be exponentially simpler than a flat MDP, but the computation efficiency is problem-dependent. The idea of these algorithms are orthogonal to those of (F)TVI. Exploring ways of combining the ideas of (F)TVI with
compact logical representation to achieve further performance improvements remains future work.
Action elimination was originally proposed by Bertsekas (2001). It has been proved to be
helpful for RTDP in the factored MDP setting (Kuter & Hu, 2007), when the cost of an action
depends on only a few state variables. Action elimination is also very useful in temporal planning
(Mausam & Weld, 2008). It has been extended to combo-elimination, a rule to prune irrelevant
action combinations in a setting when multiple actions can be executed at the same time.
The idea of finding the topological order of strongly connected components of an MDP has
been extended to solving partially-observable MDPs (POMDPs). A POMDP problem is typically
much harder than an MDP problem since the decision agent only has partial information of the
current state (Littman et al., 1995). The topological order-based planner (POT) (Dibangoye, Shani,
Chaib-draa, & Mouaddib, 2009) uses the topological order information of the underlying MDPs to
help solve a POMDP problem faster. We believe the idea can be extended to help solve even harder
problems, such as decentralized POMDP (Bernstein, Givan, Immerman, & Zilberstein, 2002), in
the future.

6. Conclusions
This work makes several contributions. First, we present two new optimal algorithms to solve
MDPs, topological value iteration (TVI) and focused topological value iteration (FTVI). TVI studies
the graphical structure of an MDP by breaking it into strongly connected components and solves the
MDP based on the topological order of the components. FTVI extends topological value iteration
algorithm by focusing the construction of strongly connected components on transitions that likely
belong to an optimal policy. FTVI does this by using a small amount of heuristic search to eliminate
provably suboptimal actions. In contrast to TVI, which does not care about goal-state information,
FTVI removes transitions which it determines to be irrelevant to an optimal policy for reaching the
goal. In this sense, FTVI builds a much more informative topological structure than TVI.
Second, we show empirically that TVI outperforms VI and other state-of-the-art algorithms
when an MDP contains many strongly connected components. We find that TVI is the most advantageous on problems with multiple equal-sized components.
Third, we show empirically that FTVI outperforms TVI and VI in a large number of domains,
usually by an order of magnitude. This performance is due to the success of a more informed
graphical structure, since the sizes of the connected components found by FTVI are vastly smaller
than those constructed by TVIs.
205

fiDAI , M AUSAM , W ELD , & G OLDSMITH

Fourth, we find surprisingly that for many domains FTVI massively outperforms popular heuristic search algorithms in convergence speed, such as ILAO*, LRTDP, BRTDP and BaRTDP. After
analyzing the performance of these algorithms over different problems, we find that a smaller number of goal states and long search depth to a goal are two key features of problems that are especially
hard for heuristic search to handle. Our results show that FTVI outperforms heuristic search in such
domains by an order of magnitude.
Finally, as a by-product we also compare ILAO*, LRTDP, BRTDP and BaRTDP (four popular,
state-of-the-art heuristic search algorithms) and find that the strength of each algorithm is usually
domain-specific. Generally, ILAO* is faster in convergence than other algorithms. BRTDP and
BaRTDP are slow in some domains probably due to the fact that they are vulnerable to those problems lack of informed upper bounds.

Acknowledgments
This work was conducted when Peng Dai was a student at the University of Washington. This work
was supported by Office of Naval Research grant N00014-06-1-0147, National Science Foundation
IIS-1016465, ITR-0325063 and the WRF / TJ Cable Professorship. We thank Eric A. Hansen for
sharing his code for ILAO*, and anonymous reviewers for excellent suggestions on improving the
manuscript.

References
Aberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-Theoretic Military Operations Planning. In Proc. of the 14th International Conference on Automated Planning and Scheduling
(ICAPS-04), pp. 402412.
Barto, A., Bradtke, S., & Singh, S. (1995). Learning to act using real-time dynamic programming.
Artificial Intelligence J., 72, 81138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The Complexity of Decentralized Control of Markov Decision Processes. Mathematics of Opererations Research, 27(4),
819840.
Bertsekas, D. P. (2000-2001). Dynamic Programming and Optimal Control, Vol. 2. Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Bonet, B., & Geffner, H. (2003a). Faster Heuristic Search Algorithms for Planning with Uncertainty
and Full Feedback. In Proc. of 18th International Joint Conf. on Artificial Intelligence (IJCAI03), pp. 12331238. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2003b). Labeled RTDP: Improving the Convergence of Real-time Dynamic Programming. In Proc. 13th International Conference on Automated Planning and
Scheduling (ICAPS-03), pp. 1221.
206

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

Bonet, B. (2006). Non-Deterministic Planning Track of the 2006 International Planning Competition.. http://www.ldc.usb.ve/bonet/ipc5/.
Bonet, B. (2007). On the Speed of Convergence of Value Iteration on Stochastic Shortest-Path
Problems. Mathematics of Operations Research, 32(2), 365373.
Bonet, B., & Geffner, H. (2006). Learning in Depth-First Search: A Unified Approach to Heuristic
Search in Deterministic Non-deterministic Settings, and Its Applications to MDPs. In Proc.
of the 16th International Conference on Automated Planning and Scheduling (ICAPS-06), pp.
142151.
Bresina, J. L., Dearden, R., Meuleau, N., Ramkrishnan, S., Smith, D. E., & Washington, R. (2002).
Planning under Continuous Time and Resource Uncertainty: A Challenge for AI. In Proc. of
18th Conf. in Uncertainty in AI (UAI-02), pp. 7784.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to Algorithms,
Second Edition. The MIT Press.
Dai, P., & Goldsmith, J. (2007). Topological Value Iteration Algorithm for Markov Decision Processes. In Proc. of IJCAI, pp. 18601865.
Dai, P., & Hansen, E. A. (2007). Prioritizing Bellman Backups Without a Priority Queue. In Proc.
of the 17th International Conference on Automated Planning and Scheduling (ICAPS-07), pp.
113119.
Dai, P., Mausam, & Weld, D. S. (2008). Partitioned External-Memory Value Iteration. In AAAI, pp.
898904.
Dai, P., Mausam, & Weld, D. S. (2009a). Domain-Independent, Automatic Partitioning for Probabilistic Planning. In IJCAI, pp. 16771683.
Dai, P., Mausam, & Weld, D. S. (2009b). Focused Topological Value Iteration. In Proc. of ICAPS,
pp. 8289.
Dibangoye, J. S., Shani, G., Chaib-draa, B., & Mouaddib, A.-I. (2009). Topological Order Planner
for POMDPs. In Proc. of IJCAI, pp. 16841689.
Feng, Z., & Hansen, E. A. (2002). Symbolic Heuristic Search for Factored Markov Decision Processes. In Proc. of the 17th National Conference on Artificial Intelligence (AAAI-05).
Feng, Z., Hansen, E. A., & Zilberstein, S. (2003). Symbolic Generalization for On-line Planning. In
Proc. of the 19th Conference in Uncertainty in Artificial Intelligence (UAI-03), pp. 209216.
Feng, Z., & Zilberstein, S. (2004). Region-Based Incremental Pruning for POMDPs. In Proc. of
UAI, pp. 146153.
Ferguson, D., & Stentz, A. (2004). Focussed Dynamic Programming: Extensive Comparative Results. Tech. rep. CMU-RI-TR-04-13, Carnegie Mellon University, Pittsburgh, PA.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient Solution Algorithms for
Factored MDPs. J. of Artificial Intelligence Research, 19, 399468.
207

fiDAI , M AUSAM , W ELD , & G OLDSMITH

Hansen, E. A., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds solutions
with loops. Artificial Intelligence J., 129, 3562.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
Solution of Markov Decision Processes using Macro-actions. In Proc. of UAI, pp. 220229.
Helmert, M., & Roger, G. (2008). How Good is Almost Perfect?. In Proc. of AAAI, pp. 944949.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic Planning using Decision
Diagrams. In Proc. of the 15th Conference on Uncertainty in Artificial Intelligence (UAI-95),
pp. 279288.
Kolobov, A., Mausam, & Weld, D. S. (2009). ReTrASE: Intergating Paradigms for Approximate
Probabilistic Planning. In Proc. of IJCAI, pp. 17461753.
Kolobov, A., Mausam, & Weld, D. S. (2010a). Classical Planning in MDP Heuristics: With a Little
Help from Generalization. In Proc. of ICAPS, pp. 97104.
Kolobov, A., Mausam, & Weld, D. S. (2010b). SixthSense: Fast and Reliable Recognition of Dead
Ends in MDPs. In Proc. of AAAI.
Kuter, U., & Hu, J. (2007). Computing and Using Lower and Upper Bounds for Action Elimination
in MDP Planning. In SARA, pp. 243257.
Littman, M. L., Dean, T., & Kaelbling, L. P. (1995). On the Complexity of Solving Markov Decision
Problems. In Proc. of the 11th Annual Conference on Uncertainty in Artificial Intelligence
(UAI-95), pp. 394402 Montreal, Quebec, Canada.
Mausam, Benazera, E., Brafman, R. I., Meuleau, N., & Hansen, E. A. (2005). Planning with
Continuous Resources in Stochastic Domains. In Proc. of IJCAI, pp. 12441251.
Mausam, & Weld, D. S. (2008). Planning with Durative Actions in Stochastic Domains. J. of
Artificial Intelligence Research (JAIR), 31, 3382.
McMahan, H. B., & Gordon, G. J. (2005). Fast Exact Planning in Markov Decision Processes. In
Proc. of the 15th International Conference on Automated Planning and Scheduling (ICAPS05).
McMahan, H. B., Likhachev, M., & Gordon, G. J. (2005). Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees. In Proceedings of
the 22nd international conference on Machine learning (ICML-05), pp. 569576.
Meuleau, N., Benazera, E., Brafman, R. I., Hansen, E. A., & Mausam (2009). A Heuristic Search
Approach to Planning with Continuous Resources in Stochastic Domains. J. of Artificial
Intellegence Research (JAIR), 34, 2759.
Moore, A., & Atkeson, C. (1993). Prioritized Sweeping: Reinforcement Learning with Less Data
and Less Real Time. Machine Learning, 13, 103130.
208

fiT OPOLOGICAL VALUE I TERATION A LGORITHMS

Musliner, D. J., Carciofini, J., Goldman, R. P., E. H. Durfee, J. W., & Boddy, M. S. (2007). Flexibly
Integrating Deliberation and Execution in Decision-Theoretic Agents. In ICAPS Workshop
on Planning and Plan-Execution for Real-World Systems.
Nilson, N. J. (1980). Principles of Artificial Intelligence. Tioga Publishing Company, Palo Alto,
Ca.
Parr, R. (1998). Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems. In Proc. of UAI, pp. 422430.
Patrascu, R., Poupart, P., Schuurmans, D., Boutilier, C., & Guestrin, C. (2002). Greedy Linear
Value-Approximation for Factored Markov Decision Processes. In Proc. of the 17th National
Conference on Artificial Intelligence (AAAI-02), pp. 285291.
Poupart, P., Boutilier, C., Patrascu, R., & Schuurmans, D. (2002). Piecewise Linear Value Function
Approximation for Factored MDPs. In Proc. of the 18th National Conference on Artificial
Intelligence (AAAI-02), pp. 292299.
Sanner, S., Goetschalckx, R., Driessens, K., & Shani, G. (2009). Bayesian Real-Time Dynamic
Programming. In Proc. of IJCAI, pp. 17841789.
Smith, T., & Simmons, R. G. (2006). Focused Real-Time Dynamic Programming for MDPs:
Squeezing More Out of a Heuristic. In Proc. of the 21th National Conference on Artificial
Intelligence (AAAI-06).
Wingate, D., & Seppi, K. D. (2005). Prioritization Methods for Accelerating MDP Solvers. J. of
Machine Learning Research, 6, 851881.
Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: A Baseline for Probabilistic Planning. In Proc.
of the 17th International Conference on Automated Planning and Scheduling (ICAPS-07), pp.
352359.

209

fiJournal of Artificial Intelligence Research 42 (2011) 427-486

Submitted 1/11; published 11/11

Adaptive Submodularity: Theory and Applications in Active Learning
and Stochastic Optimization
Daniel Golovin

DGOLOVIN @ CALTECH . EDU

California Institute of Technology
Pasadena, CA 91125, USA

Andreas Krause

KRAUSEA @ ETHZ . CH

Swiss Federal Institute of Technology
8092 Zurich, Switzerland

Abstract
Many problems in artificial intelligence require adaptively making a sequence of decisions with
uncertain outcomes under partial observability. Solving such stochastic optimization problems
is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of
adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove
that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be
competitive with the optimal policy. In addition to providing performance guarantees for both
stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed
up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept
by giving several examples of adaptive submodular objectives arising in diverse AI applications
including management of sensing resources, viral marketing and active learning. Proving adaptive
submodularity for these problems allows us to recover existing results in these applications as
special cases, improve approximation guarantees and handle natural generalizations.

1. Introduction
In many problems arising in artificial intelligence one needs to adaptively make a sequence of decisions, taking into account observations about the outcomes of past decisions. Often these outcomes
are uncertain, and one may only know a probability distribution over them. Finding optimal policies
for decision making in such partially observable stochastic optimization problems is notoriously intractable (see, e.g. Littman, Goldsmith, & Mundhenk, 1998). A fundamental challenge is to identify
classes of planning problems for which simple solutions obtain (near-) optimal performance.
In this paper, we introduce the concept of adaptive submodularity, and prove that if a partially
observable stochastic optimization problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to obtain near-optimal solutions. In fact, under reasonable complexity-theoretic
assumptions, no polynomial time algorithm is able to obtain better solutions in general. Adaptive
submodularity generalizes the classical notion of submodularity1 , which has been successfully used
to develop approximation algorithms for a variety of non-adaptive optimization problems. Submodularity, informally, is an intuitive notion of diminishing returns, which states that adding an element
to a small set helps more than adding that same element to a larger (super-) set. A celebrated result of
the work of Nemhauser, Wolsey, and Fisher (1978) guarantees that for such submodular functions,
a simple greedy algorithm, which adds the element that maximally increases the objective value,
1. For an extensive treatment of submodularity, see the books of Fujishige (2005) and Schrijver (2003).

c
2011
AI Access Foundation. All rights reserved.

fiG OLOVIN & K RAUSE

selects a near-optimal set of k elements. Similarly, it is guaranteed to find a set of near-minimal
cost that achieves a desired quota of utility (Wolsey, 1982), using near-minimum average time to do
so (Streeter & Golovin, 2008). Besides guaranteeing theoretical performance bounds, submodularity allows us to speed up algorithms without loss of solution quality by using lazy evaluations (Minoux, 1978), often leading to performance improvements of several orders of magnitude (Leskovec,
Krause, Guestrin, Faloutsos, VanBriesen, & Glance, 2007). Submodularity has been shown to be
very useful in a variety of problems in artificial intelligence (Krause & Guestrin, 2009a).
The challenge in generalizing submodularity to adaptive planning  where the action taken in
each step depends on information obtained in the previous steps  is that feasible solutions are
now policies (decision trees or conditional plans) instead of subsets. We propose a natural generalization of the diminishing returns property for adaptive problems, which reduces to the classical
characterization of submodular set functions for deterministic distributions. We show how these
results of Nemhauser et al. (1978), Wolsey (1982), Streeter and Golovin (2008), and Minoux (1978)
generalize to the adaptive setting. Hence, we demonstrate how adaptive submodular optimization
problems enjoy theoretical and practical benefits similar to those of classical, nonadaptive submodular problems. We further demonstrate the usefulness and generality of the concept by showing
how it captures known results in stochastic optimization and active learning as special cases, admits
tighter performance bounds, leads to natural generalizations and allows us to solve new problems
for which no performance guarantees were known.
As a first example, consider the problem of deploying (or controlling) a collection of sensors to
monitor some spatial phenomenon. Each sensor can cover a region depending on its sensing range.
Suppose we would like to find the best subset of k locations to place the sensors. In this application,
intuitively, adding a sensor helps more if we have placed few sensors so far and helps less if we have
already placed many sensors. We can formalize this diminishing returns property using the notion
of submodularity  the total area covered by the sensors is a submodular function defined over all
sets of locations. Krause and Guestrin (2007) show that many more realistic utility functions in
sensor placement (such as the improvement in prediction accuracy w.r.t. some probabilistic model)
are submodular as well. Now consider the following stochastic variant: Instead of deploying a fixed
set of sensors, we deploy one sensor at a time. With a certain probability, deployed sensors can fail,
and our goal is to maximize the area covered by the functioning sensors. Thus, when deploying
the next sensor, we need to take into account which of the sensors we deployed in the past failed.
This problem has been studied by Asadpour, Nazerzadeh, and Saberi (2008) for the case where
each sensor fails independently at random. In this paper, we show that the coverage objective is
adaptive submodular, and use this concept to handle much more general settings (where, e.g., rather
than all-or-nothing failures there are different types of sensor failures of varying severity). We also
consider a related problem where the goal is to place the minimum number of sensors to achieve the
maximum possible sensor coverage (i.e., the coverage obtained by deploying sensors everywhere),
or more generally the goal may be to achieve any fixed percentage of the maximum possible sensor
coverage. Under the first goal, the problem is equivalent to one studied by Goemans and Vondrak
(2006), and generalizes a problem studied by Liu, Parthasarathy, Ranganathan, and Yang (2008).
As with the maximum coverage version, adaptive submodularity allows us to recover and generalize
previous results.
As another example, consider a viral marketing problem, where we are given a social network,
and we want to influence as many people as possible in the network to buy some product. We do that
by giving the product for free to a subset of the people, and hope that they convince their friends
428

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

to buy the product as well. Formally, we have a graph, and each edge e is labeled by a number
0  pe  1. We influence a subset of nodes in the graph, and for each influenced node, their
neighbors get randomly influenced according to the probability annotated on the edge connecting
the nodes. This process repeats until no further node gets influenced. Kempe, Kleinberg, and
Tardos (2003) show that the set function which quantifies the expected number of nodes influenced
is submodular. A natural stochastic variant of the problem is where we pick a node, get to see
which nodes it influenced, then adaptively pick the next node based on these observations and so
on. We show that a large class of such adaptive influence maximization problems satisfies adaptive
submodularity.
Our third application is in active learning, where we are given an unlabeled data set, and we
would like to adaptively pick a small set of examples whose labels imply all other labels. The same
problem arises in automated diagnosis, where we have hypotheses about the state of the system (e.g.,
what illness a patient has), and would like to perform tests to identify the correct hypothesis. In both
domains we want to pick examples / tests to shrink the remaining version space (the set of consistent
hypotheses) as quickly as possible. Here, we show that the reduction in version space probability
mass is adaptive submodular, and use that observation to prove that the adaptive greedy algorithm
is a near-optimal querying policy, recovering and generalizing results by Kosaraju, Przytycka, and
Borgstrom (1999) and Dasgupta (2004). Our results for active learning and automated diagnosis
are also related to recent results of Guillory and Bilmes (2010, 2011) who study generalizations of
submodular set cover to an interactive setting. In contrast to our approach however, Guillory and
Bilmes analyze worst-case costs, and use rather different technical definitions and proof techniques.
We summarize our main contributions below, and provide a more technical summary in Table 1.
At a high level, our main contributions are:
 We consider a particular class of partially observable adaptive stochastic optimization problems, which we prove to be hard to approximate in general.
 We introduce the concept of adaptive submodularity, and prove that if a problem instance satisfies this property, a simple adaptive greedy policy performs near-optimally, for both adaptive
stochastic maximization and coverage, and also a natural min-sum objective.
 We show how adaptive submodularity can be exploited by allowing the use of an accelerated
adaptive greedy algorithm using lazy evaluations, and how we can obtain data-dependent
bounds on the optimum.
 We illustrate adaptive submodularity on several realistic problems, including Stochastic Maximum Coverage, Stochastic Submodular Coverage, Adaptive Viral Marketing, and Active
Learning. For these applications, adaptive submodularity allows us to recover known results
and prove natural generalizations.
1.1 Organization
This article is organized as follows. In 2 (page 430) we set up notation and formally define the relevant adaptive optimization problems for general objective functions. For the readers convenience,
we have also provided a reference table of important symbols on page 480. In 3 (page 433) we review the classical notion of submodularity and introduce the novel adaptive submodularity property.

429

fiG OLOVIN & K RAUSE

Name
A.S. Maximization

New Results
Tight (1  1/e)-approx. for A.M.S. objectives

A.S. Min Cost Coverage

Tight logarithmic approx. for A.M.S. objectives

A.S. Min Sum Cover

Tight 4-approx. for A.M.S. objectives

Data Dependent Bounds

Generalization to A.M.S. functions

Accelerated Greedy
Stochastic Submodular
Maximization
Stochastic Set Cover

Generalization of lazy evaluations to the adaptive setting
Generalization of the previous (1  1/e)-approx. to
arbitrary peritem set distributions, and to item costs
Generalization of the previous (ln(n) + 1)-approx. to
arbitrary per-item set distributions, with item costs
Adaptive analog of previous (1  1/e)-approx. for nonadaptive viral marketing, under more general reward
functions; tight logarithmic approx. for the adaptive min
cost cover version
Improved approx. factor of generalized binary search
and its approximate versions with and without item costs
(|E|1 )-approximation hardness for A.S. Maximization, Min Cost Coverage, and Min-Sum Cover, if f is
not adaptive submodular.

Adaptive Viral
Marketing

Active Learning
Hardness in the absence
of Adapt. Submodularity

Location
5.1,
page 438
5.2,
page 440
5.3,
page 443
5.1,
page 438
4, page 436
6, page 445
7, page 446
8, page 448

9, page 454
12,
page 464

Table 1: Summary of our theoretical results. A.S. is shorthand for adaptive stochastic, and A.M.S.
is shorthand for adaptive monotone submodular.
In 4 (page 436) we introduce the adaptive greedy policy, as well as an accelerated variant. In 5
(page 438) we discuss the theoretical guarantees that the adaptive greedy policy enjoys when applied
to problems with adaptive submodular objectives. Sections 6 through 9 provide examples on how to
apply the adaptive submodular framework to various applications, namely Stochastic Submodular
Maximization (6, page 445), Stochastic Submodular Coverage (7, page 446), Adaptive Viral Marketing (8, page 448), and Active Learning (9, page 454). In 10 (page 459) we report empirical
results on two sensor selection problems. In 11 (page 462) we discuss the adaptivity gap of the
problems we consider, and in 12 (page 464) we prove hardness results indicating that problems
which are not adaptive submodular can be extremely inapproximable under reasonable complexity
assumptions. We review related work in 13 (page 465) and provide concluding remarks in 14
(page 467). The Appendix (page 468) gives details of how to incorporate item costs and includes
all of the proofs omitted from the main text.

2. Adaptive Stochastic Optimization
We start by introducing notation and defining the general class of adaptive optimization problems
that we address in this paper. For sake of clarity, we will illustrate our notation using the sensor
placement application mentioned in 1. We give examples of other applications in 6, 7, 8, and 9.
430

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

2.1 Items and Realizations
Let E be a finite set of items (e.g., sensor locations). Each item e  E is in a particular (initially
unknown) state from a set O of possible states (describing whether a sensor placed at location e
would malfunction or not). We represent the item states using a function  : E  O, called a
realization (of the states of all items in the ground set). Thus, we say that (e) is the state of e
under realization . We use  to denote a random realization. We take a Bayesian approach and
assume that there is a known prior probability distribution p () := P [ = ] over realizations (e.g.,
modeling that sensors fail independently with failure probability), so that we can compute posterior
distributions2 . We will consider problems where we sequentially pick an item e  E, get to see
its state (e), pick the next item, get to see its state, and so on (e.g., place a sensor, see whether it
fails, and so on). After each pick, our observations so far can be represented as a partial realization
, a function from some subset of E (i.e., the set of items that we already picked) to their states
(e.g.,  encodes where we placed sensors and which of them failed). For notational convenience,
we sometimes represent  as a relation, so that   E  O equals {(e, o) : (e) = o}. We use the
notation dom() = {e : o.(e, o)  } to refer to the domain of  (i.e., the set of items observed
in ). A partial realization  is consistent with a realization  if they are equal everywhere in the
domain of . In this case we write   . If  and  0 are both consistent with some , and
dom()  dom( 0 ), we say  is a subrealization of  0 . Equivalently,  is a subrealization of  0 if
and only if, when viewed as relations,    0 .
Partial realizations are similar to the notion of belief states in Partially Observable Markov
Decision Problems (POMDPs), as they encode the effect of all actions taken (items selected) and
observations made, and determine our posterior belief about the state of the world (i.e., the state of
all items e not yet selected, p ( | ) := P [ =  |   ]).
2.2 Policies
We encode our adaptive strategy for picking items as a policy , which is a function from a set of
partial realizations to E, specifying which item to pick next under a particular set of observations
(e.g.,  chooses the next sensor location given where we have placed sensors so far, and whether they
failed or not). We also allow randomized policies that are functions from a set of partial realizations
to distributions on E, though our emphasis will primarily be on deterministic policies. If  is not in
the domain of , the policy terminates (stops picking items) upon observation of . We use dom()
to denote the domain of . Technically, we require that dom() be closed under subrealizations.
That is, if  0  dom() and  is a subrealization of  0 then   dom(). We use the notation
E(, ) to refer to the set of items selected by  under realization . Each deterministic policy 
can be associated with a decision tree T  in a natural way (see Fig. 1 for an illustration). Here, we
adopt a policy-centric view that admits concise notation, though we find the decision tree view to
be valuable conceptually.
Since partial realizations are similar to POMDP belief states, our definition of policies is similar
to the notion of policies in POMDPs, which are usually defined as functions from belief states
to actions. We will further discuss the relationship between the stochastic optimization problems
considered in this paper and POMDPs in Section 13.
2. In some situations, we may not have exact knowledge of the prior p (). Obtaining algorithms that are robust to
incorrect priors remains an interesting source of open problems. We briefly discuss some robustness guarantees of
our algorithm in 4 on page 437.

431

fiG OLOVIN & K RAUSE

Figure 1: Illustration of a policy , its corresponding decision tree representation, and the decision
tree representation of [2] , the level 2 truncation of  (as defined in 5.1).

2.3 Adaptive Stochastic Maximization, Coverage, and Min-Sum Coverage
We wish to maximize, subject to some constraints, a utility function f : 2E  OE  R0 that
depends on which items we pick and which state each item is in (e.g., modeling the total area
covered by the working sensors). Based on this notation, the expected utility of a policy  is
favg () := E [f (E(, ), )] where the expectation is taken with respect to p (). The goal of the
Adaptive Stochastic Maximization problem is to find a policy   such that
   arg max favg () subject to |E(, )|  k for all ,

(2.1)



where k is a budget on how many items can be picked (e.g., we would like to adaptively choose k
sensor locations such that the working sensors provide as much information as possible in expectation).
Alternatively, we can specify a quota Q of utility that we would like to obtain, and try to find the
cheapest policy achieving that quota (e.g., we would like to achieve a certain amount of information,
as cheaply as possible in expectation). Formally, we define the average cost cavg () of a policy as
the expected number of items it picks, so that cavg () := E [|E(, )|]. Our goal is then to find
   arg min cavg () such that f (E(, ), )  Q for all ,

(2.2)



i.e., the policy   that minimizes the expected number of items picked such that under all possible
realizations, at least utility Q is achieved. We call Problem 2.2 the Adaptive Stochastic Minimum
Cost Cover problem. We will also consider the problem where we want to minimize the worst-case
cost cwc () := max |E(, )|. This worst-case cost cwc () is the cost incurred under adversarially
chosen realizations, or equivalently the depth of the deepest leaf in T  , the decision tree associated
with .
Yet another important variant is to minimize the average time required by a policy to obtain
its utility. Formally, let u(, t) be the expected utility obtained by  after t steps3 , let Q =
E [f (E, )] be the maximum
possible expected utility, and define the min-sum cost c () of a
P
policy as c () := t=0 (Q  u(, t)). We then define the Adaptive Stochastic Min-Sum Cover
problem as the search for
   arg min c () .
(2.3)


3. For a more formal definition of u(, t), see A.5 on page 478.

432

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

P Unfortunately, as we will show in 12, even for linear functions f , i.e., those where f (A, ) =
eA we, is simply the sum of weights (depending on the realization ), Problems (2.1), (2.2),
and (2.3) are hard to approximate under reasonable complexity theoretic assumptions. Despite the
hardness of the general problems, in the following sections we will identify conditions that are
sufficient to allow us to approximately solve them.
2.4 Incorporating Item Costs
Instead of quantifying the cost of a set E(, ) by the number of elements |E(, )|, we can also
consider
the case where each item e  E has a cost c(e), and the cost of a set S  E is c(S) =
P
c(e).
We can then consider variants of Problems (2.1), (2.2), and (2.3) with the |E(, )|
eS
replaced by c(E(, )). For clarity of presentation, we will focus on the unit cost case, i.e., c(e) = 1
for all e, and explain how our results generalize to the non-uniform case in the Appendix.

3. Adaptive Submodularity
We first review the classical notion of submodular set functions, and then introduce the novel notion
of adaptive submodularity.
3.1 Background on Submodularity
Let us first consider the very special case where p () is deterministic or, equivalently, |O| = 1
(e.g., in our sensor placement applications, sensors never fail). In this case, the realization  is
known to the decision maker in advance, and thus there is no benefit in adaptive selection. Given
the realization , Problem (2.1) is equivalent to finding a set A  E such that
A  arg max f (A, ) such that |A|  k.

(3.1)

AE

For most interesting classes of utility functions f , this is an NP-hard optimization problem. However, in many practical problems, such as those mentioned in 1, f (A) = f (A, ) satisfies submodularity. A set function f : 2E  R is called submodular if, whenever A  B  E and e  E \ B
it holds that
f (A  {e})  f (A)  f (B  {e})  f (B),
(3.2)
i.e., adding e to the smaller set A increases f by at least as much as adding e to the superset B.
Furthermore, f is called monotone, if, whenever A  B it holds that f (A)  f (B) (e.g., adding
a sensor can never reduce the amount of information obtained). A celebrated result by Nemhauser
et al. (1978) states that for monotone submodular functions with f () = 0, a simple greedy algorithm that starts with the empty set, A0 =  and chooses
Ai+1 = Ai  {arg max f (Ai  {e})}

(3.3)

eE\Ai

guarantees that f (Ak )  (1  1/e) max|A|k f (A). Thus, the greedy set Ak obtains at least a (1 
1/e) fraction of the optimal value achievable using k elements. Furthermore, Feige (1998) shows
that this result is tight if P 6= NP; under this assumption no polynomial time algorithm can do strictly
better than the greedy algorithm, i.e., achieve a (1  1/e + )-approximation for any constant  > 0,
even for the special case of Maximum k-Cover where f (A) is the cardinality of the union of sets
433

fiG OLOVIN & K RAUSE

indexed by A. Similarly, Wolsey (1982) shows that the same greedy algorithm also near-optimally
solves the deterministic case of Problem (2.2), called the Minimum Submodular Cover problem:
A  arg min |A| such that f (A)  Q.

(3.4)

AE

Pick the first set A` constructed by the greedy algorithm such that f (A` )  Q. Then, for integervalued submodular functions, ` is at most |A |(1 + log maxe f (e)), i.e., the greedy set is at most a
logarithmic factor larger than the smallest set achieving quota Q. For the special case of Set Cover,
where f (A) is the cardinality of a union of sets indexed by A, this result matches a lower bound
by Feige (1998): Unless NP  DTIME(nO(log log n) ), Set Cover is hard to approximate by a factor
better than (1  ) ln Q, where Q is the number of elements to be covered.
Now let us relax the assumption that p () is deterministic. In this case, we may still want to
find a non-adaptive solution (i.e., a constant policy A that always picks set A independently of
) maximizing favg (A ). If f is pointwise submodular, i.e., f (A, ) is submodular in A for any
fixed , the function f (A) = favg (A ) is submodular, since nonnegative linear combinations of
submodular functions remain submodular. Thus, the greedy algorithm allows us to find a nearoptimal non-adaptive policy. That is, in our sensor placement example, if we are willing to commit
to all locations before finding out whether the sensors fail or not, the greedy algorithm can provide
a good solution to this non-adaptive problem.
However, in practice, we may be more interested in obtaining a non-constant policy , that
adaptively chooses items based on previous observations (e.g., takes into account which sensors are
working before placing the next sensor). In many settings, selecting items adaptively offers huge
advantages, analogous to the advantage of binary search over sequential (linear) search4 . Thus, the
question is whether there is a natural extension of submodularity to policies. In the following, we
will develop such a notion  adaptive submodularity.
3.2 Adaptive Monotonicity and Submodularity
The key challenge is to find appropriate generalizations of monotonicity and of the diminishing
returns condition (3.2). We begin by again considering the very special case where p () is deterministic, so that the policies are non-adaptive. In this case a policy  simply specifies a sequence of items (e1 , e2 , . . . , er ) which it selects in order. Monotonicity in this context can be
characterized as the property that the marginal benefit of selecting an item is always nonnegative, meaning that for all such sequences (e1 , e2 , . . . , er ), items e and 1  i  r it holds that
f ({ej : j  i}  {e})  f ({ej : j  i})  0. Similarly, submodularity can be viewed as the
property that selecting an item later never increases its marginal benefit, meaning that for all
sequences (e1 , e2 , . . . , er ), items e, and all i  r, f ({ej : j  i}  {e})  f ({ej : j  i}) 
f ({ej : j  r}  {e})  f ({ej : j  r}).
We take these views of monotonicity and submodularity when defining their adaptive analogues,
by using an appropriate generalization of the marginal benefit. When moving to the general adaptive
setting, the challenge is that the items states are now random and only revealed upon selection. A
natural approach is thus to condition on observations (i.e., partial realizations of selected items),
and take the expectation with respect to the items that we consider selecting. Hence, we define our
4. We provide a wellknown example in active learning that illustrates this phenomenon crisply in 9; see Fig. 4 on
page 454. We consider the general question of the magnitude of the potential benefits of adaptivity in 11 on page 462
.

434

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

adaptive monotonicity and submodularity properties in terms of the conditional expected marginal
benefit of an item.
Definition 3.1 (Conditional Expected Marginal Benefit). Given a partial realization  and an
item e, the conditional expected marginal benefit of e conditioned on having observed , denoted
(e | ), is
fi


fi
fi
(e | ) := E f (dom()  {e} , )  f (dom(), ) fi   
(3.5)
where the expectation is computed with respect to p ( | ) = P [ =  |   ]. Similarly, the
conditional expected marginal benefit of a policy  is
fi


fi
fi
( | ) := E f (dom()  E(, ), )  f (dom(), ) fi    .
(3.6)
In our sensor placement example, (e | ) quantifies the expected amount of additional area covered by placing a sensor at location e, in expectation over the posterior distribution p(e) (o) :=
P [(e) = o |   ] of whether the sensor will fail or not, and taking into account the area covered by the placed working sensors as encoded by . Note that the benefit we have accrued upon
observing  (and hence after having selected the items in dom()) is E [f (dom(), ) |   ],
which is the benefit term subtracted out in Eq. (3.5) and Eq. (3.6). Similarly, the expected total
benefit obtained after observing  and then selecting e is E [f (dom()  {e} , ) |   ]. The
corresponding benefit for running  after observing  is slightly more complex. Under realization
  , the final cumulative benefit will be f (dom()  E(, ), ). Taking the expectation with
respect to p ( | ) and subtracting out the benefit already obtained by dom() then yields the
conditional expected marginal benefit of .
We are now ready to introduce our generalizations of monotonicity and submodularity to the
adaptive setting:
Definition 3.2 (Adaptive Monotonicity). A function f : 2E OE  R0 is adaptive monotone with
respect to distribution p () if the conditional expected marginal benefit of any item is nonnegative,
i.e., for all  with P [  ] > 0 and all e  E we have
(e | )  0.

(3.7)

Definition 3.3 (Adaptive Submodularity). A function f : 2E  OE  R0 is adaptive submodular
with respect to distribution p () if the conditional expected marginal benefit of any fixed item does
not increase as more items are selected and their states are observed. Formally, f is adaptive
submodular w.r.t. p () if for all  and  0 such that  is a subrealization of  0 (i.e.,    0 ), and
for all e  E \ dom( 0 ), we have

(e | )   e |  0 .
(3.8)
From the decision tree perspective, the condition (e | )  (e |  0 ) amounts to saying that
for any decision tree T , if we are at a node v in T which selects an item e, and compare the expected
marginal benefit of e selected at v with the expected marginal benefit e would have obtained if it
were selected at an ancestor of v in T , then the latter must be no smaller than the former. Note
that when comparing the two expected marginal benefits, there is a difference in both the set of
435

fiG OLOVIN & K RAUSE

items previously selected (i.e., dom() vs. dom( 0 )) and in the distribution over realizations (i.e.,
p ( | ) vs. p ( |  0 )). It is also worth emphasizing that adaptive submodularity is defined relative
to the distribution p () over realizations; it is possible that f is adaptive submodular with respect
to one distribution, but not with respect to another.
We will give concrete examples of adaptive monotone and adaptive submodular functions that
arise in the applications introduced in 1 in 6, 7, 8, and 9. In the Appendix, we will explain
how the notion of adaptive submodularity can be extended to handle non-uniform costs (since, e.g.,
the cost of placing a sensor at an easily accessible location may be smaller than at a location that is
hard to get to).
3.3 Properties of Adaptive Submodular Functions
It can be seen that adaptive monotonicity and adaptive submodularity enjoy similar closure properties as monotone submodular functions. In particular, if w1 , . . . , wm P
 0 and f1 , . . . , fm are
adaptive monotone submodular w.r.t. distribution p (), then f (A, ) = m
i=1 wi fi (A, ) is adaptive monotone submodular w.r.t. p (). Similarly, for a fixed constant c  0 and adaptive monotone
submodular function f , the function g(E, ) = min(f (E, ), c) is adaptive monotone submodular. Thus, adaptive monotone submodularity is preserved by nonnegative linear combinations
and by truncation. Adaptive monotone submodularity is also preserved by restriction, so that if
f : 2E  OE  R0 is adaptive monotone submodular w.r.t. p (), then for any e  E, the function g : 2E\{e}  OE  R0 defined by g(A, ) := f (A, ) for all A  E \ {e} and all  is also
adaptive submodular w.r.t. p (). Finally, if f : 2E  OE  R0 is adaptive monotone submodular
w.r.t. p () then for each partial realization  the conditional function g(A, ) := f (Adom(), )
is adaptive monotone submodular w.r.t. p ( | ) := P [ =  |   ].
3.4 What Problem Characteristics Suggest Adaptive Submodularity?
Adaptive submodularity is a diminishing returns property for policies. Speaking informally, it can
be applied in situations where there is an objective function to be optimized does not feature synergies in the benefits of items conditioned on observations. In some cases, the primary objective might
not have this property, but a suitably chosen proxy of it does, as is the case with active learning with
persistent noise (Golovin, Krause, & Ray, 2010; Bellala & Scott, 2010). We give example applications in 6 through 9. It is also worth mentioning where adaptive submodularity is not directly
applicable. An extreme example of synergistic effects between items conditioned on observations
is the class of treasure hunting instances used to prove Theorem 12.1 on page 464, where the (binary) state of certain groups of items encode the treasures location in a complex manner. Another
problem feature which adaptive submodularity does not directly address is the possibility that items
selection can alter the underlying realization , as is the case for the problem of optimizing policies
for general POMDPs.

4. The Adaptive Greedy Policy
The classical non-adaptive greedy algorithm (3.3) has a natural generalization to the adaptive setting. The greedy policy  greedy tries, at each iteration, to myopically increase the expected objective
value, given its current observations. That is, suppose f : 2E  OE  R0 is the objective, and
 is the partial realization indicating the states of items selected so far. Then the greedy policy will

436

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

select the item e maximizing the expected increase in value, conditioned on the observed states of
items it has already selected (i.e., conditioned on   ). That is, it will select e to maximize the
conditional expected marginal benefit (e | ) as defined in Eq. (3.5). Pseudocode of the adaptive
greedy algorithm is given in Algorithm 1. The only difference to the classic, non-adaptive greedy
algorithm studied by Nemhauser et al. (1978), is Line 6, where an observation (e ) of the selected
item e is obtained. Note that the algorithms in this section are presented for Adaptive Stochastic
Maximization. For the coverage objectives, we simply keep selecting items as prescribed by  greedy
until achieving the quota on objective value (for the min-cost objective) or until we have selected
every item (for the min-sum objective).
4.1 Incorporating Item Costs
The adaptive greedy algorithm can be naturally modified to handle non-uniform item costs by replacing its selection rule by
(e | )
e  arg max
.
c(e)
e
In the following, we will focus on the uniform cost case (c  1), and defer the analysis with costs
to the Appendix.
4.2 Approximate Greedy Selection
In some applications, finding an item maximizing (e | ) may be computationally intractable, and
the best we can do is find an -approximation to the best greedy selection. This means we find an
e0 such that

1
 e0 |   max (e | ) .
 e
We call a policy which always selects such an item an -approximate greedy policy.

1
2
3
4
5
6

Input: Budget k; ground set E; distribution p (); function f .
Output: Set A  E of size k
begin
A  ;   ;
for i = 1 to k do
foreach e  E \ A do compute (e | ) = E [f (A  {e} , )  f (A, ) |   ] ;
Select e  arg maxe (e | );
Set A  A  {e };
Observe (e ); Set     {(e , (e ))};
end
Algorithm 1: The adaptive greedy algorithm, which implements the greedy policy.

4.3 Robustness & Approximate Greedy Selection
As we will show, -approximate greedy policies have performance guarantees on several problems.
The fact that these performance guarantees of greedy policies are robust to approximate greedy
selection suggests a particular robustness guarantee against incorrect priors p (). Specifically, if
our incorrect prior p0 is such that when we evaluate (e | ) we err by a multiplicative factor of at
437

fiG OLOVIN & K RAUSE

most , then when we compute the greedy policy with respect to p0 we are actually implementing
an -approximate greedy policy (with respect to the true prior), and hence obtain the corresponding
guarantees. For example, a sufficient condition for erring by at most a multiplicative factor of  is
that there exists c  1 and d  1 with  = d/c such that c p ()  p0 ()  d p () for all , where
p is the true prior.
4.4 Lazy Evaluations and the Accelerated Adaptive Greedy Algorithm
The definition of adaptive submodularity allows us to implement an accelerated version of the
adaptive greedy algorithm using lazy evaluations of marginal benefits as originally suggested for
the non-adaptive case by Minoux (1978). The idea is as follows. Suppose we run  greedy under
some fixed realization , and select items e1 , e2 , . . . , ek . Let  i := {(ej , (ej ) : j  i)} be the
partial realizations observed during the run of  greedy . The adaptive greedy algorithm computes
(e |  i ) for all e  E and 0  i < k, unless e  dom( i ). Naively, the algorithm thus needs
to compute (|E|k) marginal benefits (which can be expensive to compute). The key insight is
that i 7 (e |  i ) is nonincreasing for all e  E, because of the adaptive submodularity of the
objective. Hence, if when deciding which item to select as ei we know (e0 |  j )  (e |  i ) for
some items e0 and e and j < i, then we may conclude (e0 |  i )  (e |  i ) and hence eliminate
the need to compute (e0 |  i ). The accelerated version of the adaptive greedy algorithm exploits
this observation in a principled manner, by computing (e | ) for items e in decreasing order of
the upper bounds known on them, until it finds an item whose value is at least as great as the upper
bounds of all other items. Pseudocode of this version of the adaptive greedy algorithm is given in
Algorithm 2.
In the non-adaptive setting, the use of lazy evaluations has been shown to significantly reduce
running times in practice (Leskovec et al., 2007). We evaluated the naive and accelerated implementations of the adaptive greedy algorithm on two sensor selection problems, and obtained speedup
factors that range from roughly 4 to 40 for those problems. See 10 on page 459 for details.

5. Guarantees for the Greedy Policy
In this section we show that if the objective function is adaptive submodular with respect to the
probabilistic model of the environment in which we operate, then the greedy policy inherits precisely the performance guarantees of the greedy algorithm for classic (non-adaptive) submodular
maximization and submodular coverage problems, such as Maximum k-Cover and Minimum Set
Cover, as well as min-sum submodular coverage problems, such as Min-Sum Set Cover. In fact,
we will show that this holds true more generally: approximate greedy policies inherit precisely
the performance guarantees of approximate greedy algorithms for these classic problems. These
guarantees suggest that adaptive submodularity is an appropriate generalization of submodularity
to policies. In this section we focus on the unit cost case (i.e., every item has the same cost). In
the Appendix we provide the proofs omitted in this section, and show how our results extend to
non-uniform item costs if we greedily maximize the expected benefit/cost ratio.
5.1 The Maximum Coverage Objective
In this section we consider the maximum coverage objective, where the goal is to select k items
adaptively to maximize their expected value. The task of maximizing expected value subject to

438

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Input: Budget k; ground set E; distribution p (); function f .
Output: Set A  E of size k
begin
A  ;   ; Priority Queue Q  EMPTY QUEUE;
1
2
foreach e  E do Q. insert(e, +);
3
for i = 1 to k do
4
max  ; emax  NULL;
5
while max < Q. maxPriority( ) do
6
e  Q. pop( );
7
  (e | ) = E [f (A  {e} , )  f (A, ) |   ];
8
Q. insert(e, );
9
if max <  then
10
max  ; emax  e;
11
A  A  {emax }; Q. remove(emax );
12
Observe (emax ); Set     {(emax , (emax ))};
end
Algorithm 2: The accelerated version of the adaptive greedy algorithm. Here, Q. insert(e, )
inserts e with priority , Q. pop( ) removes and returns the item with greatest priority,
Q. maxPriority( ) returns the maximum priority of the elements in Q, and Q. remove(e)
deletes e from Q.
more complex constraints, such as matroid constraints and intersections of matroid constraints, is
considered in the work of Golovin and Krause (2011). Before stating our result, we require the
following definition.
Definition 5.1 (Policy Truncation). For a policy , define the level-k-truncation [k] of  to be the
policy obtained by running  until it terminates or until it selects k items, and then terminating.
Formally, dom([k] ) = {  dom() : || < k}, and [k] () = () for all   dom([k] ).
We have the following result, which generalizes the classic result of the work of Nemhauser et al.
(1978) that the greedy algorithm achieves a (1  1/e)-approximation to the problem of maximizing
monotone submodular functions under a cardinality constraint. By setting ` = k and  = 1 in
Theorem 5.2, we see that the greedy policy which selects k items adaptively obtains at least (11/e)
of the value of the optimal policy that selects k items adaptively, measured with respect to favg . For a
proof see Theorem A.10 in Appendix A.3, which generalizes Theorem 5.2 to nonuniform item costs.
Theorem 5.2. Fix any   1. If f is adaptive monotone and adaptive submodular with respect to
the distribution p (), and  is an -approximate greedy policy, then for all policies   and positive
integers ` and k,



favg ([`] ) > 1  e`/k favg ([k]
).

In particular, with ` = k this implies any -approximate greedy policy achieves a 1  e1/
approximation to the expected reward of the best policy, if both are terminated after running for an
equal number of steps.
If the greedy rule can be implemented only with small absolute error rather than small relative
error, i.e., (e0 | )  maxe (e | )  , an argument similar to that used to prove Theorem 5.2
439

fiG OLOVIN & K RAUSE

shows that




)  `.
favg ([`] )  1  e`/k favg ([k]

This is important, since small absolute error can always be achieved (with high probability) whenever f can be evaluated efficiently, and sampling p ( | ) is efficient. In this case, we can approximate
N

1 X
(e | ) 
f (dom()  {e} , i )  f (dom(), i ) ,
N
i=1

where i are sampled i.i.d. from p ( | ).
5.1.1 DATA D EPENDENT B OUNDS
For the maximum coverage objective, adaptive submodular functions have another attractive feature:
they allow us to obtain data dependent bounds on the optimum, in a manner similar to the bounds
for the non-adaptive case (Minoux, 1978). Consider the non-adaptive problem of maximizing a
monotone submodular function f : 2A  R0 subject to the constraint |A|  k. Let A be an
optimal solution, and fix any A  E. Then
X
f (A )  f (A) + max
(f (A  {e})  f (A))
(5.1)
B:|B|k

eB

P
because setting B = A we have f (A )  f (A  B)  f (A) + eBP
(f (A  {e})  f (A)). Note
that unlike the original objective, we can easily compute maxB:|B|k eB (f (A  {e})  f (A))
by computing (e) := f (A  {e})  f (A) for each e, and summing the k largest values. Hence
we can quickly compute an upper bound on our distance from the optimal value, f (A )  f (A).
In practice, such data-dependent bounds can be much tighter than the problem-independent performance guarantees of Nemhauser et al. (1978) for the greedy algorithm (Leskovec et al., 2007).
Further note that these bounds hold for any set A, not just sets selected by the greedy algorithm.
These data dependent bounds have the following analogue for adaptive monotone submodular
functions. See Appendix A.2 for a proof.
Lemma 5.3 (The Adaptive Data Dependent Bound). Suppose we have made observations  after
selecting dom(). Let   be any policy such that |E(  , )|  k for all . Then for adaptive
monotone submodular f
X
(  | ) 
max
(e | ) .
(5.2)
AE,|A|k

eA

Thus, after running any policy , we can efficiently compute a bound on the additional benefit
that the optimal solution   could obtain beyond the reward of . We do that by computing the
conditional expected marginal benefits for all elements e, and summing the k largest of them. Note
that these bounds can be computed on the fly when running the greedy algorithm, in a similar
manner as discussed by Leskovec et al. (2007) for the non-adaptive setting.
5.2 The Min Cost Cover Objective
Another natural objective is to minimize the number of items selected while ensuring that a sufficient
level of value is obtained. This leads to the Adaptive Stochastic Minimum Cost Coverage problem
described in 2, namely    arg min cavg () such that f (E(, ), )  Q for all . Recall that
440

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

cavg () is the expected cost of , which in the unit cost case equals the expected number of items
selected by , i.e., cavg () := E [|E(, )|]. If the objective is adaptive monotone submodular,
this is an adaptive version of the Minimum Submodular Cover problem (described on line (3.4) in
3.1). Recall that the greedy algorithm is known to give a (ln(Q) + 1)-approximation for Minimum
Submodular Cover assuming the coverage function is integer-valued in addition to being monotone
submodular (Wolsey, 1982). Adaptive Stochastic Minimum Cost Coverage is also related to the
(Noisy) Interactive Submodular Set Cover problem studied by Guillory and Bilmes (2010, 2011),
which considers the worst-case setting (i.e., there is no distribution over states; instead states are
realized in an adversarial manner). Similar results for active learning have been proved by Kosaraju
et al. (1999) and Dasgupta (2004), as we discuss in more detail in 9.
We assume throughout this section that there exists a quality threshold Q such that f (E, ) = Q
for all , and for all S  E and all , f (S, )  Q. Note that, as discussed in Section 3, if we
replace f (S, ) by a new function g(S, ) = min(f (S, ), Q0 ) for some constant Q0 , g will be
adaptive submodular if f is. Thus, if f (E, ) varies across realizations, we can instead use the
greedy algorithm on the function truncated at some threshold Q0  min f (E, ) achievable by all
realizations.
In contrast to Adaptive Stochastic Maximization, for the coverage problem additional subtleties
arise. In particular, it is not enough that a policy  achieves value Q for the true realization; in order
for  to terminate, it also requires a proof of this fact. Formally, we require that  covers f :
Definition 5.4 (Coverage). Let  = (, ) be the partial realization encoding all states observed
during the execution of  under realization . Given f : 2E  OE  R, we say a policy  covers
 with respect to f if f (dom(), 0 ) = f (E, 0 ) for all 0  . We say that  covers f if it covers
every realization with respect to f .
Coverage is defined in such a way that upon terminating,  might not know which realization
is the true one, but has guaranteed that it has achieved the maximum reward in every possible case
(i.e., for every realization consistent with its observations). We obtain results for both the average
and worst-case cost objectives.
5.2.1 M INIMIZING THE AVERAGE C OST
Before presenting our approximation guarantee for the Adaptive Stochastic Minimum Cost Coverage, we introduce a special class of instances, called selfcertifying instances. We make this
distinction because the greedy policy has stronger performance guarantees for selfcertifying instances, and such instances arise naturally in applications. For example, the Stochastic Submodular
Cover and Stochastic Set Cover instances in 7, the Adaptive Viral Marketing instances in 8, and
the Pool-Based Active Learning instances in 9 are all selfcertifying.
Definition 5.5 (SelfCertifying Instances). An instance of Adaptive Stochastic Minimum Cost Coverage is selfcertifying if whenever a policy achieves the maximum possible value for the true
realization it immediately has a proof of this fact. Formally, an instance (f, p ()) is selfcertifying
if for all , 0 , and  such that    and 0  , we have f (dom(), ) = f (E, ) if and only if
f (dom(), 0 ) = f (E, 0 ).
One class of selfcertifying instances which commonly arise are those in which f (A, ) depends
only on the state of items in A, and in which there is a uniform maximum amount of reward that
can be obtained across realizations. Formally, we have the following observation.
441

fiG OLOVIN & K RAUSE

Proposition 5.6. Fix an instance (f, p ()). If there exists Q such that f (E, ) = Q for all  and
there exists some g : 2EO  R0 such that f (A, ) = g ({(e, (e)) : e  A}) for all A and ,
then (f, p ()) is selfcertifying.
Proof. Fix , 0 , and  such that    and 0  . Assuming the existence of g and treating  as a
relation, we have f (dom(), ) = g() = f (dom(), 0 ). Hence f (dom(), ) = Q = f (E, )
if and only if f (dom(), 0 ) = Q = f (E, 0 ).
For our results on minimum cost coverage, we also need a stronger monotonicity condition:
Definition 5.7 (Strong Adaptive Monotonicity). A function f : 2E  OE  R is strongly adaptive
monotone with respect to p () if, informally selecting more items never hurts with respect to the
expected reward. Formally, for all , all e 
/ dom(), and all possible outcomes o  O such that
P [(e) = o |   ] > 0, we require
E [f (dom(), ) |   ]  E [f (dom()  {e} , ) |   , (e) = o] .

(5.3)

Strong adaptive monotonicity implies adaptive monotonicity, as the latter means that selecting
more items never hurts in expectation, i.e.,
E [f (dom(), ) |   ]  E [f (dom()  {e} , ) |   ] .
We now state our main result for the average case cost cavg ():
Theorem 5.8. Suppose f : 2E  OE  R0 is adaptive submodular and strongly adaptive monotone with respect to p () and there exists Q such that f (E, ) = Q for all . Let  be any value
such that f (S, ) > Q   implies f (S, ) = Q for all S and . Let  = min p () be the mini be an optimal policy minimizing the expected number of
mum probability of any realization. Let avg
items selected to guarantee every realization is covered. Let  be an -approximate greedy policy.
Then in general
  

Q

cavg ()   cavg (avg ) ln
+1

and for selfcertifying instances
cavg () 


 cavg (avg
)




ln

Q





+1 .

Note that if range(f )  Z, then  = 1 is a valid choice, and then for general and selfcertifying
 ) (ln(Q/) + 1) and c

instances we have cavg ()   cavg (avg
avg ()   cavg (avg ) (ln(Q) + 1),
respectively.
5.2.2 M INIMIZING THE W ORST-C ASE C OST
For the worst-case cost cwc () := max |E(, )|, strong adaptive monotonicity is not required;
adaptive monotonicity suffices. We obtain the following result.
Theorem 5.9. Suppose f : 2E  OE  R0 is adaptive monotone and adaptive submodular with
respect to p (), and let  be any value such that f (S, ) > f (E, )   implies f (S, ) = f (E, )
 be
for all S and . Let  = min p () be the minimum probability of any realization. Let wc
442

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

the optimal policy minimizing the worst-case number of queries to guarantee every realization is
covered. Let  be an -approximate greedy policy. Finally, let Q := E [f (E, )] be the maximum
possible expected reward. Then

  
Q

+1 .
cwc ()   cwc (wc ) ln

The proofs of Theorems 5.8 and 5.9 are given in Appendix A.4.
Thus, even though adaptive submodularity is defined w.r.t. a particular distribution, perhaps
surprisingly, the adaptive greedy algorithm is competitive even in the case of adversarially chosen
realizations, against a policy optimized to minimize the worst-case cost. Theorem 5.9 therefore
suggests that if we do not have a strong prior, we can obtain the strongest guarantees if we choose
a distribution that is as uniform as possible (i.e., maximizes ) while still guaranteeing adaptive
submodularity.
5.2.3 D ISCUSSION
Note that the approximation factor for selfcertifying instances in Theorem 5.8 reduces to the
(ln(Q) + 1)-approximation guarantee for the greedy algorithm for Set Cover instances with Q
elements, in the case of a deterministic distribution p (). Moreover, with a deterministic distribution p () there is no distinction between average-case and worst-case cost. Hence, an immediate corollary of the result of Feige (1998) mentioned in 3 is that for every constant  > 0
there is no polynomial time (1  ) ln (Q/) approximation algorithm for selfcertifying instances
of Adaptive Stochastic Min Cost Cover, under either the cavg () or the cwc () objective, unless
NP  DTIME(nO(log log n) ). It remains open to determine whether or not Adaptive Stochastic
Min Cost Cover with the worst-case cost objective admits a ln (Q/) + 1 approximation for self
certifying instances via a polynomial time algorithm, and in particular whether the greedy policy has such an approximation guarantee. However, in Lemma A.14 we show that Feiges result
also implies there is no (1  ) ln (Q/) polynomial time approximation algorithm for general
(non self-certifying) instances of Adaptive Stochastic Min Cost Cover under either objective, unless
NP  DTIME(nO(log log n) ). In that sense, each of the three results comprising Theorem 5.8 and
Theorem 5.9 are best-possible under reasonable complexity-theoretic assumptions. As we show
in Section 9, our result for the average-case cost of greedy policies for selfcertifying instances
also matches (up to constant factors) results on hardness of approximating the optimal policy in the
special case of active learning, also known as the Optimal Decision Tree problem.
5.3 The Min-Sum Cover Objective
Yet another natural objective is the min-sum objective, in which an unrealized reward of x incurs a
cost of x in each time step, and the goal is to minimize the total cost incurred.
5.3.1 BACKGROUND ON THE N ON - ADAPTIVE M IN -S UM C OVER P ROBLEM
In the non-adaptive setting, perhaps the simplest form of a coverage problem with this objective is
the Min-Sum Set Cover problem (Feige, Lovasz, & Tetali, 2004) in which the input is a set system
(U, S), the output is a permutation of the sets hS1 , S2 , . . . , Sm i, and the goal is to minimize the sum
of element coverage times, where the coverage time of u is the index of the first set that contains
it (e.g., it is j if u  Sj and u 
/ Si for all i < j). In this problem and its generalizations the
443

fiG OLOVIN & K RAUSE

min-sum objective is useful in modeling processing costs in certain applications, for example in
ordering diagnostic tests to identify a disease cheaply (Kaplan, Kushilevitz, & Mansour, 2005),
in ordering multiple filters to be applied to database records while processing a query (Munagala,
Babu, Motwani, Widom, & Thomas, 2005), or in ordering multiple heuristics to run on boolean
satisfiability instances as a means to solve them faster in practice (Streeter & Golovin, 2008). A
particularly expressive generalization of min-sum set cover has been studied under the names MinSum Submodular Cover (Streeter & Golovin, 2008) and L1 -Submodular Set Cover (Golovin, Gupta,
Kumar, & Tangwongsan, 2008). The former paper extends the greedy algorithm to a natural online
variant of the problem, while the latter studies a parameterized family of Lp -Submodular Set Cover
problems in which the objective is analogous to minimizing the Lp norm of the coverage times for
Min-Sum Set Cover instances. In the Min-Sum Submodular Cover problem, there is a monotone
submodular function f : 2E  R0 defining the reward obtained from a collection of elements5 .
There is an integral cost c(e) for each element, and the output is a sequence of all of the elements
 = he1 , e2 , . . . , en i. For each t  R0 , we define the set of elements in the sequence  within a
budget of t:




X
[t] := ei :
c(ej )  t .


ji

The cost we wish to minimize is then
c () :=


X


f (E)  f ([t] ) .

(5.4)

t=0

Feige et al. (2004) proved that for Min-Sum Set cover, the greedy algorithm achieves a 4-approximation
to the minimum cost, and also that this is optimal in the sense that no polynomial time algorithm
can achieve a (4  )-approximation, for any  > 0, unless P = NP. Interestingly, the greedy algorithm also achieves a 4-approximation for the more general Min-Sum Submodular Cover problem
as well (Streeter & Golovin, 2008; Golovin et al., 2008).
5.3.2 T HE A DAPTIVE S TOCHASTIC M IN -S UM C OVER P ROBLEM
In this article, we extend the result of Streeter and Golovin (2008) and Golovin et al. (2008) to an
adaptive version of Min-Sum Submodular Cover. For claritys sake we will consider the unit-cost
case here (i.e., c(e) = 1 for all e); we show how to extend adaptive submodularity to handle general
costs in the Appendix. In the adaptive version of the problem, [t] plays the role of [t] , and favg
plays the role of f . The goal is to find a policy  minimizing
c () :=


X
t=0


X
 X

E [f (E, )]  favg ([t] ) =
p ()
f (E, )  f (E([t] , ), ) .


(5.5)

t=0

We call this problem the Adaptive Stochastic Min-Sum Cover problem. The key difference between
this objective and the minimum cost cover objective is that here, the cost at each step is only the
fractional extent that we have not covered the true realization, whereas in the minimum cost cover
objective we are charged in full in each step until we have completely covered the true realization
5. To encode Min-Sum Set Cover instance (U, S), let E := S and f (A) := | eA e|, where each e  E is a subset of
elements in U .

444

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

(according to Definition 5.4). We prove the following result for the Adaptive Stochastic Min-Sum
Cover problem with arbitrary item costs in Appendix A.5.
Theorem 5.10. Fix any   1. If f is adaptive monotone and adaptive submodular with respect to
the distribution p (),  is an -approximate greedy policy with respect to the item costs, and   is
any policy, then c ()  4 c (  ).

6. Application: Stochastic Submodular Maximization
As our first application, consider the sensor placement problem introduced in 1. Suppose we
would like to monitor a spatial phenomenon such as temperature in a building. We discretize the
environment into a set E of locations. We would like to pick a subset A  E of k locations that is
most informative, where we use a set function f(A) to quantify the informativeness of placement
A. Krause and Guestrin (2007) show that many natural objective functions (such as reduction
in predictive uncertainty measured in terms of Shannon entropy with conditionally independent
observations) are monotone submodular.
Now consider the problem, where the informativeness of a sensor is unknown before deployment (e.g., when deploying cameras for surveillance, the location of objects and their associated
occlusions may not be known in advance, or varying amounts of noise may reduce the sensing
range). We can model this extension by assigning a state (e)  O to each possible location, indicating the extent to which a sensor placed at location e is working. To quantify the value of a
set of sensor deployments under a realization  indicating to what extent the various sensors are
working, we first define (e, o) for each e  E and o  O, which represents the placement of a
sensor at location e which is in state o. We then suppose there is a function f : 2EO  R0
which quantifies the informativeness of a set of sensor deployments in arbitrary states. (Note f is a
set function taking a set of (sensor deployment, state) pairs as input.) The utility f (A, ) of placing
sensors at the locations in A under realization  is then
f (A, ) := f({(e, (e)) : e  A}).
We aim to adaptively place k sensors to maximize our expected utility. Q
We assume that sensor
failures at each location are independent of each other, i.e., P [ = ] = eE P [(e) = (e)] ,
where P [(e) = o] is the probability that a sensor placed at location e will be in state o. Asadpour
et al. (2008) studied a special case of our problem, in which sensors either fail completely (in which
case they contribute no value at all) or work perfectly, under the name Stochastic Submodular Maximization. They proved that the adaptive greedy algorithm obtains a (1  1/e) approximation to
the optimal adaptive policy, provided f is monotone submodular. We extend their result to multiple
types of failures by showing that f (A, ) is adaptive submodular with respect to distribution p ()
and then invoking Theorem 5.2. Fig. 2 illustrates an instance of Stochastic Submodular Maximization where f (A, ) is the cardinality of union of sets index by A and parameterized by .
Q
Theorem 6.1. Fix a prior such that P [ = ] = eE P [(e) = (e)] and an integer k, and let
the objective function f : 2EO  R0 be monotone submodular. Let  be any -approximate
greedy policy attempting to maximize f , and let   be any policy. Then for all positive integers `,



favg ([`] )  1  e`/k favg ([k]
).

 ).
In particular, if  is the greedy policy (i.e.,  = 1) and ` = k, then favg ([k] )  1  1e favg ([k]
445

fiG OLOVIN & K RAUSE

Proof. We prove Theorem 6.1 by first proving f is adaptive monotone and adaptive submodular in
this model, and then applying Theorem 5.2. Adaptive monotonicity is readily proved after observing
that f (, ) is monotone for each . Moving on to adaptive submodularity, fix any ,  0 such that
   0 and any e 
/ dom( 0 ). We aim to show (e |  0 )  (e | ). Intuitively, this is clear,
as (e |  0 ) is the expected marginal benefit of adding e to a larger base set than is the case with
(e | ), namely dom( 0 ) as compared to dom(), and the realizations are independent. To prove
it rigorously, we define a coupled distribution  over pairs of realizations
   and 0   0 such
Q
that (e0 ) = 0 (e0 ) for all e0 
/ dom( 0 ). Formally, (, 0 ) = eE\dom() P [(e) = (e)] if
0
0
0
  ,    , and (e ) = 0 (e0 ) for all e0 
/ dom( 0 ); otherwise (, 0 ) = 0. (Note that
0
0
0
(, 0 ) > 0 implies (e0 ) = 0 (e0 ) for
since
P all e  0dom() as0 well,
P   ,0    , and
0
0
   .) Also note that p ( | ) =
0 (,  ) and p ( |  ) =
 (,  ). Calculating
(e |  0 ) and (e | ) using , we see that for any (, 0 ) in the support of ,

	
f (dom( 0 )  {e} , 0 )  f (dom( 0 ), 0 ) = f( 0  (e, 0 (e)) )  f( 0 ))
 f(  {(e, (e))})  f())
= f (dom()  {e} , )  f (dom(), )
from the submodularity of f. Hence
P
0
0
0
0
0
(e |  0 ) =
(,0 ) (,  ) (f (dom( )  {e} ,  )  f (dom( ),  ))
P
0
= (e | )

(,0 ) (,  ) (f (dom()  {e} , )  f (dom(), ))
which completes the proof.

7. Application: Stochastic Submodular Coverage
Suppose that instead of wishing to adaptively place k unreliable sensors to maximize the utility of
the information obtained, as discussed in 6, we have a quota on utility and wish to adaptively place
the minimum number of unreliable sensors to achieve this quota. This amounts to a minimum-cost
coverage version of the Stochastic Submodular Maximization problem introduced in 6, which we
call Stochastic Submodular Coverage.
As in 6, in the Stochastic Submodular Coverage problem we suppose there is a function
f : 2EO  R0 which quantifies the utility of a set of sensors
in arbitrary states. Also, the
Q
states of each sensor are independent, so that P [ = ] = eE P [(e) = (e)]. The goal is
to obtain
a quota Q of utility at
n
o minimum cost. Thus, we define our objective as f (A, ) :=

min Q, f ({(e, (e)) : e  A}) , and want to find a policy  covering every realization and minimizing cavg () := E [|E(, )|]. We additionally assume that this quota can always be obtained
using sufficiently many sensor placements; formally, this amounts to f (E, ) = Q for all . We
obtain the following result, whose proof we defer until the end of this section.
Q
Theorem 7.1. Fix a prior with independent sensor states so that P [ = ] = eE P [(e) = (e)],
 EO  R0 be a monotone submodular function. Fix Q  R0 such that f (A, ) :=
and let
 f :2

min Q, f({(e, (e)) : e  A}) satisfies f (E, ) = Q for all . Let  be any value such that
f (S, ) > Q   implies f (S, ) = Q for all S and . Finally, let  be an -approximate greedy
446

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Figure 2: Illustration of part of a Stochastic Set Cover instance. Shown are the supports of two
distributions over sets, indexed by items e (marked in blue) and e0 (yellow).

policy for maximizing f , and let   be any policy. Then
  

Q

cavg ()   cavg ( ) ln
+1 .

7.1 A Special Case: The Stochastic Set Coverage Problem
The Stochastic Submodular Coverage problem is a generalization of the Stochastic Set Coverage
problem (Goemans & Vondrak, 2006). In Stochastic Set Coverage the underlying submodular objective f is the number of elements covered in some input set system. In other words, there is a
ground set U of n elements to be covered, and items E such that each item e is associated with a
distribution over subsets of U . When an item is selected, a set is sampled from its distribution, as
illustrated in Fig. 2. The problem is to adaptively select items until all elements of U are covered
by sampled sets, while minimizing the expected number of items selected. Like us, Goemans and
Vondrak also assume that the subsets are sampled independently for each item, and every element
of U can be covered in every realization, so that f (E, ) = |U | for all .
Goemans and Vondrak primarily investigated the adaptivity gap (quantifying how much adaptive
policies can outperform non-adaptive policies) of Stochastic Set Coverage, for variants in which
items can be repeatedly selected or not, and prove adaptivity gaps of (log n) in the former case,
and between (n) and O(n2 ) in the latter. They also provide an n-approximation algorithm. More
recently, Liu et al. (2008) considered a special case of Stochastic Set Coverage in which each item
may be in one of two states. They were motivated by a streaming database problem, in which
a collection of queries sharing common filters must all be evaluated on a stream element. They
transform the problem to a Stochastic Set Coverage instance in which (filter, query) pairs are to be
covered by filter evaluations; which pairs are covered by a filter depends on the (binary) outcome
of evaluating it on the stream element. The resulting instances satisfy the assumption that every
element of U can be covered in every realization. They study, among other algorithms, the adaptive
greedy algorithm specialized to thisQsetting, and show that if the subsets areP
sampled independently
for each item, so that P [ = ] = e P [(e) = (e)], then it is an Hn := nx=1 x1 approximation.
(Recall ln(n)  Hn  ln(n) + 1 for all n  1.) Moreover, Liu et al. report that it empirically
outperforms a number of other algorithms in their experiments.
447

fiG OLOVIN & K RAUSE

The adaptive submodularity framework allows us to recover Liu et al.s result, and generalize
it to richer item distributions over subsets of U , all as a corollary of Theorem 7.1. Specifically, we
obtain a (ln(n)+1)-approximation for the Stochastic Set Coverage problem, where n := |U |, which
matches the approximation ratio for the greedy algorithm for classical Set Cover that Stochastic Set
Coverage generalizes. Like Liu et al.s result, our result is tight if NP * DTIME(nO(log log n) ),
since it matches Feiges lower bound of (1  ) ln n for the approximability of Set Cover under that
assumption (Feige, 1998).
We model the Stochastic Set Coverage problem by letting (e)  U indicate the random
set
sampled
from es distribution. Since the sampled sets are independent we have P [ = ] =
Q
P
[(e)
=
(e)]. For any A  E let f (A, ) := | eA (e)| be the number of elements of U
e
covered by the sets sampled from items in A. As in the previous work mentioned above, we assume
f (E, ) = n for all . Therefore we may set Q = n. Since the range of f includes only integers,
we may set  = 1. Applying Theorem 7.1 then yields the following result.
Corollary 7.2. The adaptive greedy algorithm achieves a (ln(n) + 1)-approximation for Stochastic
Set Coverage, where n := |U | is the size of the ground set.
We now provide the proof of Theorem 7.1.
Proof of Theorem 7.1: We will ultimately prove Theorem 7.1 by applying the bound from Theorem 5.8 for selfcertifying instances. The proof mostly consists of justifying this final step.

Without
n loss of
o generality we may assume f is truncated at Q, otherwise we may use g(S) =
min Q, f(S) in lieu of f. This removes the need to truncate f . Since we established the adaptive
submodularity of f in the proof of Theorem 6.1, and by assumption f (E, ) = Q for all , to apply
Theorem 5.8 we need only show that f is strongly adaptive monotone, and that the instances under
consideration are selfcertifying.
We begin by showing the strong adaptive monotonicity of f . Fix a partial realization , an item
e
/ dom() and a state o. Let  0 =   {(e, o)}. Then treating  and  0 as subsets of E  O, and
using the monotonicity of f, we obtain


E [f (dom(), ) |   ] = f()  f( 0 )  E f (dom( 0 ), ) |    0 ,
which is equivalent to the strong adaptive monotonicity condition.
Next we prove that these instances are selfcertifying. Consider any  and , 0 consistent with
. Then
f (dom(), ) = f() = f (dom(), 0 ).
Since f (E, ) = f (E, 0 ) = Q by assumption, it follows that f (dom(), ) = f (E, ) iff
f (dom(), 0 ) = f (E, 0 ), so the instance is selfcertifying.
We have shown that f and p () satisfy the assumptions of Theorem 5.8 on this selfcertifying
instance. Hence we may apply it to obtain the claimed approximation guarantee.

8. Application: Adaptive Viral Marketing
For our next application, consider the following scenario. Suppose we would like to generate demand for a genuinely novel product. Potential customers do not realize how valuable the new
product will be to them, and conventional advertisements are failing to convince them to try it. In
448

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Figure 3: Illustration of the Adaptive Viral Marketing problem. Left: the underlying social network.
Middle: the people influenced and the observations obtained after one person is selected.

this case, we may try to spur demand by offering a special promotional deal to a select few people,
and hope that demand builds virally, propagating through the social network as people recommend
the product to their friends and associates. Supposing we know something about the structure of
the social networks people inhabit, and how ideas, innovation, and new product adoption diffuse
through them, this begs the question: to which initial set of people should we offer the promotional
deal, in order to spur maximum demand for our product?
This, broadly, is the viral marketing problem. The same problem arises in the context of spreading technological, cultural, and intellectual innovations, broadly construed. In the interest of unified
terminology we follow Kempe et al. (2003) and talk of spreading influence through the social network, where we say people are active if they have adopted the idea or innovation in question, and
inactive otherwise, and that a influences b if a convinces b to adopt the idea or innovation in question.
There are many ways to model the diffusion dynamics governing the spread of influence in
a social network. We consider a basic and well-studied model, the independent cascade model,
described in detail below. For this model Kempe et al. (2003) obtain a very interesting result; they
show that the eventual spread of the influence f (i.e., the ultimate number of customers that demand
the product) is a monotone submodular function of the seed set S of people initially selected. This,
in conjunction with the results of Nemhauser et al. (1978) implies that the greedy algorithm obtains
at least 1  1e of the value of the best feasible seed set of size at most k, i.e., arg maxS:|S|k f (S),
where we interpret k as the budget for the promotional campaign. Though Kempe et al. consider
only the maximum coverage version of the viral marketing problem, their result in conjunction with
that of Wolsey (1982) also implies that the greedy algorithm will obtain a quota Q of value at a cost
of at most ln(Q) + 1 times the cost of the optimal set arg minS {c(S) : f (S)  Q} if f takes on
only integral values.
8.1 Adaptive Viral Marketing
The viral marketing problem has a very natural adaptive analog. Instead of selecting a fixed set of
people in advance, we may select a person to offer the promotion to, make some observations about
449

fiG OLOVIN & K RAUSE

the resulting spread of demand for our product, and repeat. See Fig. 3 for an illustration. In 8.2, we
use the idea of adaptive submodularity to obtain results analogous to those of Kempe et al.(2003)
in the adaptive setting. Specifically, we show that the greedy policy obtains at least 1  1e of the
value of the best policy. Moreover, we extend this result by achieving that guarantee not only for
the case where our reward is simply the number of influenced people, but also for any (nonnegative)
monotone submodular function of the set of people influenced. In 8.3 we consider the minimum
cost cover objective, and show that the greedy policy obtains a logarithmic approximation for it. To
our knowledge, no approximation results for this adaptive variant of the viral marketing problem
have been known.
8.1.1 I NDEPENDENT C ASCADE M ODEL
In this model, the social network is a directed graph G = (V, A) where each vertex in V is a
person, and each edge (u, v)  A has an associated binary random variable Xuv indicating if u
will influence v. That is, Xuv = 1 if u will influence v once it has been influenced, and Xuv = 0
otherwise. The random variables Xuv are independent, and have known means puv := E [Xuv ]. We
will call an edge (u, v) with Xuv = 1 a live edge and an edge with Xuv = 0 a dead edge. When a
node u is activated, the edges Xuv to each neighbor v of u are sampled, and v is activated if (u, v)
is live. Influence can then spread from us neighbors to their neighbors, and so on, according to
the same process. Once active, nodes remain active throughout the process, however Kempe et al.
(2003) show that this assumption is without loss of generality, and can be removed.
8.1.2 T HE F EEDBACK M ODEL
In the Adaptive Viral Marketing problem under the independent cascades model, the items correspond to people we can activate by offering them the promotional deal. How we define the states
(u) depends on what information we obtain as a result of activating u. Given the nature of the
diffusion process, activating u can have wide-ranging effects, so the state (u) has more to do
with the state of the social network on the whole than with u in particular. Specifically, we model
(u) as a function u : A  {0, 1, ?}, where u ((u, v)) = 0 means that activating u has revealed that (u, v) is dead, u ((u, v)) = 1 means that activating u has revealed that (u, v) is live,
and u ((u, v)) = ? means that activating u has not revealed the status of (u, v) (i.e., the value of
Xuv ). We require each realization to be consistent and complete. Consistency means that no edge
should be declared both live and dead by any two states. That is, for all u, v  V and a  A,
(u (a), v (a)) 
/ {(0, 1), (1, 0)}. Completeness means that the status of each edge is revealed by
some activation. That is, for all a  A there exists u  V such that u (a)  {0, 1}. A consistent
and complete realization thus encodes Xuv for each edge (u, v). Let A() denote the live edges as
encoded by . There are several candidates for which edge sets we are allowed to observe when
activating a node u. Here we consider what we call the Full-Adoption Feedback Model: After activating u we get to see the status (live or dead) of all edges exiting v, for all nodes v reachable from
u via live edges (i.e., reachable from u in (V, A()), where  is the true realization. We illustrate
the full-adoption feedback model in Fig. 3.
8.1.3 T HE O BJECTIVE F UNCTION
In the simplest case, the reward for influencing a set U  V of nodes is f(U ) := |U |. Kempe et al.
(2003) obtain an 1  1e -approximation for the slightly more general case in which each node u
450

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

P
has a weight wu indicating its importance, and the reward is f(U ) := uU wu . We generalize
this result further, to include arbitrary nonnegative monotone submodular reward functions f. This
allows us, for example, to encode a value associated with the diversity of the set of nodes influenced, such as the notion that it is better to achieve 20% market penetration in five different (equally
important) demographic segments than 100% market penetration in one and 0% in the others.

8.2 Guarantees for the Maximum Coverage Objective
We are now ready to formally state our result for the maximum coverage objective.

Theorem 8.1. The greedy policy  greedy obtains at least 1  1e of the value of the best policy
for the Adaptive Viral Marketing problem with arbitrary monotone submodular reward functions,
in the independent cascade and full-adoption feedback models discussed above. That is, if (S, )
is the set of all activated nodes when S is the seed set of activated nodes and  is the realization,
f : 2V  R0 is an arbitrary monotone submodular function indicating the reward for influencing
a set, and the objective function is f (S, ) := f((S, )), then for all policies  and all k  N we
have


1
greedy
favg ([k] )  1 
favg ([k] ).
e

 ).
More generally, if  is an -approximate greedy policy then `  N, favg ([`] )  1  e`/k favg ([k]
Proof. Adaptive monotonicity follows immediately from the fact that f (, ) is monotonic for each
. It thus suffices to prove that f is adaptive submodular with respect to the probability distribution
on realizations p (), because then we can invoke Theorem 5.2 to complete the proof.
We will say we have observed an edge (u, v) if we know its status, i.e., if it is live or dead.
Fix any ,  0 such that    0 and any v 
/ dom( 0 ). We must show (v |  0 )  (v | ).
To prove this rigorously, we define a coupled distribution  over pairs of realizations    and
0   0 . Note that given the feedback model, the realization  is a function of the random variables {Xuw : (u, w)  A} indicating the status of each edge. For conciseness we use the notation
X = {Xuw : (u, w)  A}. We define  implicitly in terms of a joint distribution  on X  X0 ,
where  = (X) and 0 = 0 (X0 ) are the realizations induced by the two distinct sets of random
edge statuses, respectively. Hence ((X), (X0 )) = (X, X0 ). Next, let us say a partial realization  observes an edge e if some w  dom() has revealed its status as being live or dead. For
edges (u, w) observed by , the random variable Xuw is deterministically set to the status observed
0 is deterministically set
by . Similarly, for edges (u, w) observed by  0 , the random variable Xuw
to the status observed by  0 . Note that since    0 , the state of all edges which are observed by
 are the same in  and 0 . All (X, X0 )  support() have these properties. Additionally, we will
construct  so that the status of all edges which are unobserved by both  0 and  are the same in X
0 for all such edges (u, w), or else (X, X0 ) = 0.
and X0 , meaning Xuw = Xuw
The above constraints leave us with the following degrees of freedom: we may select Xuw for
all (u, w)  A which are unobserved by . We select them independently, such that E [Xuw ] = puw
as with the prior p (). Hence for all (X, X0 ) satisfying the above constraints,
Y
1Xuw
uw
(X, X0 ) =
pX
,
uw (1  puw )
(u,w) unobserved by 

451

fiG OLOVIN & K RAUSE

and otherwise (X, X0 ) = 0. Note that p ( | ) =
We next claim that for all (, 0 )  support()

P

0

(, 0 ) and p (0 |  0 ) =

0
 (,  ).

P

f (dom( 0 )  {v} , 0 )  f (dom( 0 ), 0 )  f (dom()  {v} , )  f (dom(), ). (8.1)
Recall f (S, ) := f((S, )), where (S, ) is the set of all activated nodes when S is the seed set
of activated nodes and  is the realization. Let B = (dom(), ) and C = (dom()  {v} , )
denote the active nodes before and after selecting v after dom() under realizations , and similarly
define B 0 and C 0 with respect to  0 and 0 . Let D := C \ B, D0 := C 0 \ B 0 . Then Eq. (8.1) is
equivalent to f(B 0  D0 )  f(B 0 )  f(B  D)  f(B). By the submodularity of f, it suffices to
show that B  B 0 and D0  D to prove the above inequality, which we will now do.
We start by proving B  B 0 . Fix w  B. Then there exists a path from some u  dom()
to w in (V, A()). Moreover, every edge in this path is not only live but also observed to be live,
by definition of the feedback model. Since (, 0 )  support(), this implies that every edge in
this path is also live under 0 , as edges observed by  must have the same status under both  and
0 . It follows that there is a path from u to w in (V, A(0 )). Since u is clearly also in dom( 0 ), we
conclude w  B 0 , hence B  B 0 .
Next we show D0  D. Fix some w  D0 and suppose by way of contradiction that w 
/ D.
Hence there exists a path P from v to w in (V, A(0 )) but no such path exists in (V, A()). The
edges of P are all live under 0 , and at least one must be dead under . Let (u, u0 ) be such an edge
in P . Because the status of this edge differs in  and 0 , and (, 0 )  support(), it must be
that (u, u0 ) is observed by  0 but not observed by . Because it is observed by  0 , in our feedback
model it must be that u is active after dom( 0 ) is selected, i.e., u  B 0 . However, this implies that
all nodes reachable from u via edges in P are also active after dom( 0 ) is selected, since all the
edges in P are live. Hence all such nodes, including w, are in B 0 . Since D0 and B 0 are disjoint, this
implies w 
/ D0 , a contradiction.
Having proved Eq. (8.1), we now proceed to use it to show (v |  0 )  (v | ) as in 6.
P
0
0
0
0
0
(v |  0 ) =
(,0 ) (,  ) (f (dom( )  {v} ,  )  f (dom( ),  ))
P
0
= (v | )

(,0 ) (,  ) (f (dom()  {v} , )  f (dom(), ))
which completes the proof.
8.2.1 C OMPARISON WITH S TOCHASTIC S UBMODULAR M AXIMIZATION
It is worth contrasting the Adaptive Viral Marketing problem with the Stochastic Submodular Maximization problem of 6. In the latter problem, we can think of the items as being random independently distributed sets. In Adaptive Viral Marketing by contrast, the random sets (of nodes
influenced when a fixed node is selected) depend on the random status ofthe edges, and hence may
be correlated through them. Nevertheless, we can obtain the same 1  1e approximation factor for
both problems.

8.3 The Minimum Cost Cover Objective
We may also wish to adaptively run our campaign until a certain level of market penetration has
been achieved, e.g., a certain number of people have adopted the product. We can formalize this
452

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

goal using the minimum cost cover objective. For this objective, we have an instance of Adaptive
Stochastic Minimum Cost Cover, in which we are given a quota Q  f(V ) (quantifying the desired
level of market penetration) and we must adaptively select nodes to activate until the set of all active
nodes S satisfies f(S)  Q. We obtain the following result.
Theorem 8.2. Fix a monotone submodular function f : 2V  R0 indicating thenreward for influo
encing a set, and a quota Q  f(V ). Suppose the objective is f (S, ) := min Q, f((S, )) ,
where (S, ) is the set of all activated nodes when S is the seed set of activated nodes and
 is the realization. Let  be any value such that f(S) > Q   implies f(S)
  Qfor all
S. Then any -approximate greedy policy  on average costs at most  ln Q
 + 1 times
the average cost of the best policy obtaining Q reward for the Adaptive Viral Marketing problem in the independent
model with full-adoption feedback as described above. That is,
   cascade

Q
cavg ()   ln  + 1 cavg (  ) for any   that covers every realization.
Proof. We prove Theorem 8.2 by recourse to Theorem 5.8. We have already established that f is
adaptive submodular, in the proof of Theorem 8.1. It remains to show that f is strongly adaptive
monotone, that these instances are selfcertifying, and that Q and  equal the corresponding terms
in the statement of Theorem 5.8.
We start with strong adaptive monotonicity. Fix , e 
/ dom(), and o  O. We must show
E [f (dom(), ) |   ]  E [f (dom()  {e} , ) |   , (e) = o] .

(8.2)

Let V + () denote the active nodes after selecting dom() and observing . By definition of
the full adoption feedback model, V + () consists of precisely those nodes v for which there exists a path Puv from some u  dom() to v via exclusively live edges. The edges whose status we observe consist of all edges exiting nodes in V + (). It follows that every path from any
u  V + () to any v  V \ V + () contains at least one edge which is observed by  to be
dead. Hence, in every   , the set of nodes activated by selecting dom() is the same. Therefore E [f (dom(), ) |   ] = f(V + ()). Similarly, if we define  0 :=   {(e, o)}, then
E [f (dom()  {e} , ) |   , (e) = o] = f(V + ( 0 )). Note that once activated, nodes never
become inactive. Hence,    0 implies V + ()  V + ( 0 ). Since f is monotone by assumption,
this means f(V + ())  f(V + ( 0 )) which implies Eq. (8.2) and strong adaptive monotonicity.
Next we establish
thatothese instances are selfcertifying. Note that for every  we have
n
f (V, ) = min Q, f(V ) = Q. From our earlier remarks, we know that f (dom(), ) =
f(V + ()) for every   . Hence for all  and , 0 consistent with , we have f (dom(), ) =
f (dom(), 0 ) and so f (dom(), ) = Q if and only if f (dom(), 0 ) = Q, which proves that the
instance is selfcertifying.
Finally we show that Q and  equal the corresponding terms in the statement of Theorem 5.8.
As noted earlier, f (V, ) = Q for all . n
We defined
 as o
some valueosuch that f(S) > Q implies
n
f(S)  Q for all S. Since range(f ) = min Q, f(S) : S  V , it follows that we cannot have
f (S, )  (Q  , Q) for any S and , so that  satisfies the requirements of the corresponding term
in Theorem 5.8. Hence we may apply Theorem 5.8 on this selfcertifying instance with Q and  to
obtain the claimed result.

453

fiG OLOVIN & K RAUSE

9. Application: Automated Diagnosis and Active Learning
An important problem in AI is automated diagnosis. For example, suppose we have different hypotheses about the state of a patient, and can run medical tests to rule out inconsistent hypotheses.
The goal is to adaptively choose tests to infer the state of the patient as quickly as possible.
A similar problem arises in active learning. Obtaining labeled data to train a classifier is typically
expensive, as it often involves asking an expert. In active learning (c.f., Cohn, Gharamani, & Jordan,
1996; McCallum & Nigam, 1998), the key idea is that some labels are more informative than others:
labeling a few unlabeled examples can imply the labels of many other unlabeled examples, and thus
the cost of obtaining the labels from an expert can be avoided. As is standard, we assume that
we are given a set of hypotheses H, and a set of unlabeled data points X where each x  X
is independently drawn from some distribution D. Let L be the set of possible labels. Classical
learning theory yields probably approximately correct (PAC) bounds, bounding the number n of
examples drawn i.i.d. from D needed to output a hypothesis h that will have expected error at most
 with probability at least 1  , for some fixed ,  > 0. That is, if h is the target hypothesis (with
zero error), and error(h) := PxD [h(x) 6= h (x)] is the error of h, we require P [error(h)  ] 
1  . The latter probability is taken with respect to D(X); the learned hypothesis h and thus
error(h) depend on it. A key challenge in active learning is to avoid bias: actively selected examples
are no longer i.i.d., and thus sample complexity bounds for passive learning no longer apply. If one
is not careful, active learning may require more samples than passive learning to achieve the same
generalization error. One natural approach to active learning that is guaranteed to perform at least
as well as passive learning is pool-based active learning (McCallum & Nigam, 1998): The idea is
to draw n unlabeled examples i.i.d. However, instead of obtaining all labels, labels are adaptively
requested until the labels of all unlabeled examples are implied by the obtained labels. Now we have
obtained n labeled examples drawn i.i.d., and classical PAC bounds still apply. The key question is
how to request the labels for the pool to infer the remaining labels as quickly as possible.
In the case of binary labels (or test outcomes) L = {1, 1}, various authors have considered
greedy policies which generalize binary search (Garey & Graham, 1974; Loveland, 1985; Arkin,
Meijer, Mitchell, Rappaport, & Skiena, 1993; Kosaraju et al., 1999; Dasgupta, 2004; Guillory &
Bilmes, 2009; Nowak, 2009). The simplest of these, called generalized binary search (GBS) or the
splitting algorithm, works as follows. Define the version space V to be the set of hypotheses consistent with the observed labels (here we assumefiP
that there is fino label noise). In the worst-case setting,
fi
fi
GBS selects a query x  X that minimizes
hV h(x) . In the Bayesian setting we assume we
given a prior pHfi over hypotheses; in this case GBS selects a query x  X that minimizes
fiare
fiP
fi
hV pH (h)  h(x) . Intuitively these policies myopically attempt to shrink a measure of the version space (i.e., the cardinality or the probability mass) as quickly as possible. The former provides
an O(log |H|)-approximation for the worst-case number of queries (Arkin et al., 1993), and the
latter provides an O(log minh 1pH (h) )-approximation for the expected number of queries (Kosaraju
et al., 1999; Dasgupta, 2004) and a natural generalization of GBS obtains the same guarantees
with a larger set of labels (Guillory& Bilmes, 2009). Kosaraju
	 et al. also prove that running GBS
0
2
on a modified prior pH (h)  max pH (h), 1/|H| log |H| is sufficient to obtain an O(log |H|)approximation.
Viewed from this perspective of the previous sections, shrinking the version space amounts to
covering all false hypotheses with stochastic sets (i.e., queries), where query x covers all hypotheses that disagree with the target hypothesis h at x. That is, x covers {h : h(x) 6= h (x)}. As in 8,

454

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Figure 4: Illustration of the Active Learning problem, in the simple special case of one-dimensional
data and binary threshold hypotheses H = {h :   R}, where h (x) = 1 if x   and
0 otherwise.

these sets may be correlated in complex ways determined by the set of possible hypotheses. As we
will show, the reduction in version space mass is adaptive submodular, and this allows us to obtain a
new analysis of GBS using adaptive submodularity, which is arguably more amenable to extensions
and generalizations than previous analyses. Our new analysis further allows us to improve
on the


1
previous best bound on the approximation factor of GBS (Dasgupta, 2004) from 4 ln minh pH (h)


to ln minh 1pH (h) + 1. We also show that when we apply GBS to a modified prior distribution, the
approximation factor is improved to O(ln |H|). This result matches a lower bound of (ln |H|) of
Chakaravarthy, Pandit, Roy, Awasthi, and Mohania (2007) up to constant factors.
Theorem 9.1. In the Bayesian setting in which there is aprior pH ona finite
 set of hypotheses H,
the generalized binary search algorithm makes OPT  ln minh 1pH (h) + 1 queries in expectation
to identify a hypothesis drawn from pH , where OPT is the minimum expected number of queries
made by any policy.
If minh pH	(h) is sufficiently small, running the algorithm on a modified prior

p0H (h)  max pH (h), 1/|H|2 improves the approximation factor to O(ln |H|).
We devote the better part of the remainder of this section to the proof of Theorem 9.1, which has
several components. We first address the important special case of a uniform prior over hypotheses,
i.e., pH (h) = 1/|H| for all h  H, and then we reduce the case with a general prior to a uniform
prior. We wish to appeal to Theorem 5.8, so we convert the problem into an Adaptive Stochastic
Min Cost Cover problem.
9.1 The Reduction to Adaptive Stochastic Min Cost Cover
Define a realization h for each hypothesis h  H. The ground set is E = X, and the outcomes are
binary; we define O = {1, 1} instead of using {0, 1} to be consistent with our earlier exposition.
For all h  H we set h  h, meaning h (x) = h(x) for all x  X. To define the objective
function, we first need some notation. Given observed labels   X  O, let V () denote the
version space, i.e., the set of hypotheses for which h(x) = (x) for all x  dom(). See Fig. 4
for an illustration of an active
Plearning problem in the case of indicator hypotheses. For a set of
hypotheses V , let pH (V ) := hV pH (h) denote their total prior probability. Finally, let (S, h) =
{(x, h(x)) : x  S} be the function with domain S that agrees with h on S. We define the objective
455

fiG OLOVIN & K RAUSE

function by
f (S, h ) := 1  pH (V ((S, h))) = pH



	
h0 : x  S, h0 (x) 6= h(x)

and use p (h ) = pH (h) = 1/|H| for all h. Let   be an optimal policy for this Adaptive Stochastic
Min Cost Cover instance. Note that there is an exact correspondence between policies for the
original problem of finding the target hypothesis and our problem of covering the true realization;
h is identified as the target hypothesis if and only if the version space is reduced to {h } which
occurs if and only if h is covered. Hence cavg (  ) = OPT. Note that because we have assumed a
uniform prior over hypotheses, we have f (X, h ) = 1  1/|H| for all h. Furthermore, the instances
are selfcertifying.
Lemma 9.2. The instances described above are selfcertifying for arbitrary priors pH .
Proof. Intuitively, theses instances are selfcertifying because to cover h a policy must identify
h . More formally, these instances are selfcertifying because for any h and  such that h  ,
we have that f (dom(), h ) = f (X, h ) implies V () = {h}. This in turn means that h is
the only realization consistent with , which trivially implies that any realization 0   also has
f (dom(), 0 ) = f (X, 0 ); hence the instance is selfcertifying.
9.2 The Uniform Prior
We next prove that the instances generated are adaptive submodular and strongly adaptive monotone
under a uniform prior.
Lemma 9.3. In the instances described above, f is strongly adaptive monotone and adaptive submodular and with respect to a uniform prior pH .
Proof. Demonstrating strong adaptive monotonicity under a uniform prior amounts to proving that
adding labels cannot grow the version space, which is clear in our model. That is, each query x
eliminates some subset of hypotheses, and as more queries are performed, the subset of hypotheses
eliminated by x cannot grow. Moving on to adaptive submodularity, consider the expected marginal
contribution of x under two partial realizations ,  0 where  is a subrealization of  0 (i.e.,  
 0 ), and x 
/ dom( 0 ). Let [x/o] be the partial realization with domain dom()  {x} that
agrees with  on its domain, and maps x to o. For each o  O, let ao := pH (V ([x/o])), bo :=
pH (V ( 0 [x/o])). Since a hypothesis eliminated from the version space cannot later appear in the
version space, we have ao  bo for all o. Next, note the expected reduction in version space mass
(and hence the expected marginal contribution) due to selecting x given partial realization  is
P
P
X  o0 6=o ao0 
X
o6=o0 ao ao0
P
(x | ) =
ao  P [(x) 6= o |   ] =
ao
= P
.
(9.1)
o0 ao0
o0 ao0
o
oO

The corresponding quantity for  0 has bo substituted for ao in Eq. (9.1), for each o  O. To prove
adaptive submodularity we must show (x | )  (x |  0 ) and to do soit suffices to show that

	
P
P
P
/zo  0 for each o and ~z  ~c  [0, 1]O : o co > 0 , where (~z ) :=
o6=o0 zo zo0 / ( o0 zo0 )
has the same functional form as the expression for (x | ) in Eq. (9.1). This is because /zo  0
for each o implies that growing the version space in any manner cannot decrease the expected
456

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

marginal benefit of query x, and hence shrinking it in any manner cannot increase the expected
marginal benefit of x. It is indeed the case that /zo  0 for each o. More specifically, it holds
that
P
P
2

b6=a zb +
(b,c):b6=c,b6=a,c6=a zb zc
=
 0,
P
za
( b zb )2
which can be derived through elementary calculus.
Hence we can apply Theorem 5.8 to this selfcertifying instance with maximum reward threshold Q = 11/|H|, and minimum gap  = 1/|H|, to obtain an upper bound of OPT (ln (|H|  1) + 1)
on the number of queries made by the generalized binary search algorithm (which corresponds exactly to the greedy policy for Adaptive Stochastic Min Cost Cover) under the assumption of a
uniform prior over H.
9.3 Arbitrary Priors
Now consider general priors over H. We construct the Adaptive Stochastic Min Cost Cover instance
as before, only we change the objective function to
f (S, h ) := 1  pH (V ((S, h))) + pH (h).

(9.2)

First note that the instances remain selfcertifying. The proof of Lemma 9.2 goes through completely unchanged by the modification of f . We proceed to show adaptive submodularity and strong
adaptive monotonicity.
Lemma 9.4. The objective function f as described in Eq. (9.2) is strongly adaptive monotone and
adaptive submodular and with respect to arbitrary priors pH .
Proof. The modified objective is still adaptive submodular, because (S, h ) 7 pH (h) is clearly so,
and because adaptive submodularity is defined via linear inequalities it is preserved under taking
nonnegative linear combinations. Note that f (X, h ) = 1 for all h .
Showing f is strongly adaptive monotone requires slightly more work than before. Fix , x 
/
dom(), and o  O. We must show
E [f (dom(), ) |   ]  E [f (dom()  {x} , ) |   , (x) = o] .

(9.3)

Plugging in the definition of f , the inequality we wish to prove may be simplified to
E [pH () |   ]  E [pH () |   [x/o]]  pH (V ())  pH (V ([x/o])).

(9.4)

where  is the random realization of the hypothesis, and pH (h ) = pH (h) for all h. Let Velim :=
V ()  V ([x/o]) be the set of hypotheses eliminated from the version space by the observation
h(x) = o. Rewriting Eq. (9.4), we get
X
hV ()

pH (h)2

pH (V ())

X
hV ([x/o])

pH (h)2
 pH (Velim ).
pH (V ([x/o]))

457

(9.5)

fiG OLOVIN & K RAUSE

Let LHS9.5 denote the left hand side of Eq. (9.5). We prove Eq. (9.5) as follows.
P
2
LHS9.5 
[since pH (V ([x/o]))  pH (V ())]
hVelim pH (h) /pH (V ())
P

hVelim pH (h)  pH (V ())/pH (V ()) [since h  V ()  pH (h)  pH (V ())]
= pH (Velim )
We conclude that f is adaptive submodular and strongly adaptive monotone.
Hence we can apply Theorem 5.8 to this selfcertifying instance with maximum reward threshold Q = 1, and minimum gap  = 1/ minh pH (h). As a result we obtain an upper bound of
OPT (ln (1/ minh pH (h)) + 1) on the number of queries made by generalized binary search for
arbitrary priors, completing the proof of Theorem 9.1.
9.4 Improving the Approximation Factor for Highly Nonuniform Priors
To improve this to an O(log |H|)-approximation in the event that minh pH (h) is extremely small
using the observation of Kosaraju et al. (1999), call a policy  progressive ifit eliminates at 	least
one hypothesis from its version space
Let p0H	(h) = max pH (h), 1/|H|2 /Z
 query.
P in each
0
be the modified prior, where Z := h0 max pH (h ), 1/|H|2 is the normalizing
Pconstant. Let
c(, h) be the cost (i.e., # of queries) of  under target h. Then cavg (, p) :=
h c(, h)p(h)
is the expected cost of  under prior p. We will show that cavg (, p0H ) is a good approximation to cavg (, pH ). Call h rare if pH (h) < 1/|H|2 , and common otherwise. First, note that

	
P
0
2  1 + 1/|H|, and so p0 (h)  |H| p (h), for all h. Hence for all ,
h0 max pH (h ), 1/|H|
H
|H|+1 H
|H|
cavg (, pH ). Next, we show cavg (, p0H )  cavg (, pH ) + 1. Consider
we have cavg (, p0H )  |H|+1
P
the quantity cavg (, p0H )  cavg (, pH ) = h c(, h) (p0H (h)  pH (h)). The positive contributions
must come from rare hypotheses. However, the total probability mass of these under p0H is at most
1/|H|, and since is progressive
c(, h)  |H| for all h, hence the difference in costs is at most


1
one. Let  := ln minh p0 (h) + 1  ln |H|2 + |H| + 1 be the approximation factor for generalH

ized binary search when run on p0H . Let  be the policy of generalized binary search, and let   be
an optimal policy under prior pH . Then

|H| + 1
|H| + 1
|H| + 1
cavg (, p0H ) 
 cavg (  , p0H ) 
 cavg (  , pH ) + 1 .
|H|
|H|
|H|


With some further algebra, we can derive cavg (, pH )  cavg (  , pH ) + 1 ln 2e|H|2 . Thus
for a general prior a simple modification of GBS yields an O(log |H|)-approximation.
cavg (, pH ) 

9.5 Extensions to Arbitrary Costs, Multiple Classes, and Approximate Greedy Policies
This result easily generalizes to handle the setting of multiple classes / test outcomes (i.e., |O|  2),
and -approximate greedy policies, where we lose a factor of  in the approximation factor. As we
describe in the Appendix, we can generalize adaptive submodularity to incorporate costs on items,
which allows us to extend this result to handle query costs as well. We can therefore recover these
extensions of Guillory
and Bilmes
(2009), while improving the approximation factor for GBS with


1
item costs to ln minh pH (h) + 1. Guillory and Bilmes also showed how to extend the technique
458

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS


 
x c(x)
-approximation with costs using a
of Kosaraju et al. (1999) to obtain an O log |H| max
minx c(x)
greedy policy, which may be combined with our tighter analysis as well to give a similar result
with an improved leading constant. Recently, Gupta, Krishnaswamy, Nagarajan, and Ravi (2010)
showed how to simultaneously remove the dependence on both costs and probabilities from the
approximation ratio. Specifically, within the context of studying an adaptive travelling salesman
problem they investigated the Optimal Decision Tree problem, which is equivalent to the active
learning problem we consider here. Using a clever, more complex algorithm than adaptive greedy,
they achieve an O (log |H|)-approximation in the case of non-uniform costs and general priors.
9.6 Extensions to Active Learning with Noisy Observations
Theorem 9.1 and the extensions mentioned so far are in the noise free case, i.e., the result of query x
and observes h (x), where h is the target hypothesis. Many practical problems may have noisy observations. Nowak (2009) considered the case in which the outcomes are binary, i.e., O = {1, 1},
the same query may be asked multiple times, and for each instance of each query the noise is independent. In this case he gives performance guarantees for generalized binary search. While this
setting may be appropriate if the noise is due to measurement error, in some applications the noise
is persistent, i.e., if query x is asked several times, the observation is always the same. Recently,
Golovin et al. (2010) and Bellala and Scott (2010) have used the adaptive submodularity framework to obtain the first algorithms with provable (logarithmic) approximation guarantees for active
learning with persistent noise.

10. Experiments
Greedy algorithms are often straightforward to develop and implement, which explains their popular use in practical applications, such as Bayesian experimental design and Active Learning, as
discussed in 9 (also see the excellent introduction of Nowak, 2009) and Adaptive Stochastic Set
Cover, e.g., for filter design in streaming databases as discussed in 7. Besides allowing us to prove
approximation guarantees for such algorithms, adaptive submodularity provides the following immediate practical benefits:
1. The ability to use lazy evaluations to speed up its execution.
2. The ability to generate data-dependent bounds on the optimal value.
In this section, we empirically evaluate their benefits within a sensor selection application, in a
setting similar to the one described by Deshpande, Guestrin, Madden, Hellerstein, and Hong (2004).
In this application, we have deployed a network V of wireless sensors, e.g., to monitor temperature
in a building or traffic in a road network. Since sensors are battery constrained, we must adaptively
select k sensors, and then, given those sensor readings, predict, e.g., the temperature at all remaining
locations. This prediction is possible since temperature measurements will typically be correlated
across space. Here, we will consider the case where sensors can fail to report measurements due to
hardware failures, environmental conditions or interference.
10.1 The Sensor Selection Problem with Unreliable Sensors
More formally, we imagine every location v  V is associated with a random variable Xv describing
the temperature at that location, and there is a joint probability distribution p (xV ) := P [XV = xV ]
that models the correlation between temperature values. Here, XV = [X1 , . . . , Xn ] is the random
459

fiG OLOVIN & K RAUSE

vector over all temperature values. We follow Deshpande et al. (2004) and assume that the joint
distribution of the sensors is multivariate Gaussian. A sensor v can make a noisy observation Yv =
Xv + v , where v is zero mean Gaussian noise with known variance  2 . If some measurements
YA = yA are obtained at a subset of locations, then the conditional distribution p (xV | yA ) :=
P [XV = xV | YA = yA ] allows predictions at the unobserved locations, e.g., by predicting E[XV |
YA = yA ] (which minimizes the mean squared error). Furthermore, this conditional distribution
quantifies the uncertainty in the prediction: Intuitively, we would like to select sensors that minimize
the predictive uncertainty. One way to quantify the predictive uncertainty is to use the remaining
Shannon entropy
H (XV | YA = yA ) := E [ log2 (p (XV | yA ))] .
We would like to adaptively select k sensors, to maximize the expected reduction in Shannon entropy (c.f., Sebastiani & Wynn, 2000; Krause & Guestrin, 2009b). However, in practice, sensors are
often unreliable, and might fail to report their measurements. We assume that after selecting a sensor, we find out whether it has failed or not before deciding which sensor to select next. We suppose
that each sensor has an associated probability pfail (v) of failure, in which case no reading is reported,
and that sensor failures are independent of each other and of the ambient temperature at v. Thus we
have an instance of the Stochastic Maximization problem with E := V , O := {working, failed},
and

f (A, ) := H (XV )  H XV | y{v : (v)=working} .
(10.1)
For multivariate normal distributions, the entropy is given as
fi
fi
1
1
fi
fi
H (XV | YA = yA ) = ln(2e)n fiV A AA +  2 I
AV fi ,
2
where for sets A and B, AB denotes the covariance (matrix) between random vectors XA and XB .
Note that the predictive covariance does not depend on the actual observations yA , only on the set
A of chosen locations. Thus,
H (XV | YA = yA ) = H (XV | YA ) ,
where as usual, H (XV | YA ) = E [H (XV | YA = yA )]. As Krause and Guestrin (2005) show,
the function
g(A) := I (XV ; YA ) = H (XV )  H (XV | YA )
(10.2)
is monotone submodular, whenever the observations YV are conditionally independent given XV .
This insight allows us to apply the result of 6 to show that the objective f defined in Eq. (10.1)
is adaptive monotone submodular, using f(S) := g({v : (v, working)  S}) for any S  E  O.
10.1.1 DATA AND E XPERIMENTAL S ETUP
Our first data set consists of temperature measurements from the network of 46 sensors deployed at
Intel Research Berkeley, which were sampled at 30 second intervals for 5 consecutive days (starting
Feb. 28th , 2004). We define our objective function with respect to the empirical covariance estimated
from the data.
We also use data from traffic sensors deployed along the highway I-880 South in California.
We use traffic speed data for all working days from 6 AM to 11 AM for one month, from 357
sensors. The goal is to predict the speed on all 357 road segments. We again estimate the empirical
covariance matrix.
460

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

10.1.2 T HE B ENEFITS OF L AZY E VALUATION
For both data sets, we run the adaptive greedy algorithm, using both the naive implementation (Algorithm 1) and the accelerated version using lazy evaluations (Algorithm 2). We vary the probability
of sensor failure, and evaluate the execution time and the number of evaluations of the function g
(defined in Eq. (10.2)) each algorithm makes. Figures 5(a) and 5(b) plot execution time given a
50% sensor failure rate, on a computer with a 2.26 GHz dual core processor and 4 GB RAM. In
these applications, function evaluations are the bottleneck in the computation, so the number of
them serves as a machine-independent proxy for the running time. Figures 5(c) and 5(d) show the
performance ratio in terms of this proxy. On the temperature data set, lazy evaluations speed up the
computation by a factor of between roughly 3.5 and 7, depending on the failure probability. On the
larger traffic data set, we obtain speedup factors between 30 and 38. We find that the benefit of the
lazy evaluations increases with the problem size and with the failure probability. The dependence on
problem size must ultimately be explained in terms of structural properties of the instances, which
also benefit the nonadaptive accelerated greedy algorithm. The dependence on failure probability
has a simpler explanation. Note that in these applications, if the accelerated greedy algorithm selects v, which then fails, then it does not need to make any additional function evaluations to select
the next sensor. Contrast this with the naive greedy algorithm, which makes a function evaluation
for each sensor that has not been selected so far.
10.1.3 T HE B ENEFITS OF THE DATA D EPENDENT B OUND
While adaptive submodularity allows us to prove worst-case performance guarantees for the adaptive greedy algorithm, in many practical applications it can be expected that these bounds are quite
loose. For our sensor selection application, we use the data dependent bounds of Lemma 5.3 to
compute an upper bound avg on max favg ([k] ) as described below, and compare it with the performance guarantee of Theorem 5.2. For the accelerated greedy algorithm, we use the upper bounds
on the marginal benefits stored in the priority queue instead of recomputing the marginal benefits,
and thus expect somewhat looser bounds. We find that for our application, the bounds are tighter
than the worst case bounds. We also find that the lazy data dependent bounds are almost as tight as
the eager bounds using the eagerly recomputed marginal benefits (e | ) for the latest and greatest , though the former have slightly higher variance. Figures 5(e) and 5(f) show the performance
of the greedy algorithm as well as the three bounds on the optimal value.
Two subtleties arise when using the data-dependent bounds to bound max favg ([k] ). The


P
 |
first is that Lemma 5.3 tells us that  [k]
 maxAE,|A|k
eA (e | ), whereas we
would like to bound the difference between
h the optimal reward and the algorithms current
i expected

reward, conditioned on seeing , i.e., E f (E([k] , ), )  f (dom(), ) |    . However,
in our applications f is strongly adaptive monotone, and strong adaptive monotonicity implies that
for any   we have
h
i




E f (E([k]
, ), )  f (dom(), ) |      [k]
| .
(10.3)


Hence, if we let OPT() := max E f (E([k] , ), ) |    , Lemma 5.3 implies that
OPT()  E [f (dom(), ) |   ] +

461

max
AE,|A|k

X
eA

(e | ) .

(10.4)

fiG OLOVIN & K RAUSE

The second subtlety is that we obtain a sequence of bounds from Eq. (10.4). If we consider the
(random) sequence of partial realizations observed by the adaptive greedy algorithm,  =  0 
 1       k ,P
we obtain k + 1 bounds 0 , . . . , k , where i := E [f (dom( i ), ) |    i ] +
maxAE,|A|k
eA (e |  i ). Taking the expectation over , note that for any , and any i,
favg ([k] )  E [OPT( i )]  E [i ] .
Therefore for any 0  i  k , i is a random variable whose expectation is an upper bound on
the optimal expected reward of any policy. At this point we may be tempted to use the minimum
of these, i.e., min := mini {i } as our ultimate bound. However, a collection of random variables
X0 , . . . , Xk with E [Xi ]   for all i does not, in general, satisfy mini {Xi }   . While it is
possible in our case, with its independent sensor failures, to use concentration inequalities to bound
mini {i }  mini {E [i ]} with high probability, and thus add an appropriate term to obtain a true
upper bound from min , we take a different approach; we simply use the average bound avg :=
1 Pk
i=0 i . Of course, depending on the application, a particular bound i (chosen independently
k+1
of the sequence  0 ,  1 , . . . ,  k ) may be superior. For example, if g is modular, then 0 is best,
whereas if g exhibits strong diminishing returns, then bounds i with larger values of i may be
significantly tighter.

11. Adaptivity Gap
An important question in adaptive optimization is how much better adaptive policies can perform
when compared to non-adaptive policies. This is quantified by the adaptivity gap, which is the
worst-case ratio, over problem instances, of the performance of the optimal adaptive policy to the
optimal non-adaptive solution. Asadpour et al. (2008) show that in the Stochastic Submodular
Maximization problem with independent failures (as considered in 6), the expected value of the
optimal non-adaptive policy is at most a constant factor 1  1/e worse than the expected value
of the optimal adaptive policy. While we currently do not have lower bounds for the adaptivity
gap of the general Adaptive Stochastic Maximization problem (2.1), we can show that even in the
case of adaptive submodular functions, the min-cost cover and min-sum cover versions have large
adaptivity gaps, and thus there is a large benefit of using adaptive algorithms. In these cases, the
adaptivity gap is defined as the worst-case ratio of the expected cost of the optimal non-adaptive
policy divided by the expected cost of the optimal adaptive policy. For the Adaptive Stochastic
Minimum Cost Coverage problem (2.2), Goemans and Vondrak (2006) show the special case of
Stochastic Set Coverage without multiplicities has an adaptivity gap of (|E|). Below we exhibit
an adaptive stochastic optimization instance with adaptivity gap of (|E|/ log |E|) for the Adaptive
Stochastic Min-Sum Cover problem (2.3), which also happens to have the same adaptivity gap for
Adaptive Stochastic Minimum Cost Coverage.
Theorem 11.1. Even for adaptive submodular functions, the adaptivity gap of Adaptive Stochastic
Min-Sum Cover is (n/ log n), where n = |E|.
Proof. Suppose E = {1, . . . , n}. Consider the active learning problem where our hypotheses
h : E  {1, 1} are threshold functions, i.e., h(e) = 1 if e  ` and h(e) = 1 if e < `
for some threshold `. There is a uniform distribution over thresholds `  {1, . . . , n + 1}. In
order to identify the correct hypothesis with threshold `, our policy must observe at least one of
`  1 or ` (and both of them if 1 < `  n). Let  be an optimal non-adaptive policy for this
462

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Time for Standard and Accelerated Adaptive Greedy

0.45
0.4

Time for Standard and Accelerated Adaptive Greedy

40
35

Adaptive Greedy

0.35

Adaptive Greedy

30

0.3

25

0.25
20

0.2
15

0.15

Accelerated Adaptive Greedy
10

0.1

5

0.05
0

0

5

10

15

20

25

30

35

40

45

0

50

Accelerated Adaptive Greedy
0

50

100

150

200

250

300

350

(a) Temperature Data: Execution time (sec) for
the naive vs accelerated implementations of adaptive greedy vs. the budget k on number of sensors
selected, when pfail (v) = 0.5 for all v, plotted
with standard errors.

(b) Traffic Data: Execution time (sec) for the
naive vs accelerated implementations of adaptive greedy vs. the budget k on number of sensors selected, when pfail (v) = 0.5 for all v,
plotted with standard errors.

8

45

Redu ction in Function Evaluations vs. Pr[failure]

7
6
5
4

Reduction in Function Evaluations vs . Pr [failur e]

40

90%
80%
70%
60%
50%
40%
30%
20%
10%

75%

35

50%
25%

30
25
20
15

3

10

2

5

1
0

10

20

30

40

0
0

50

100

200

300

400

(c) Temperature Data: The ratio of function evaluations made by the naive vs accelerated implementations of adaptive greedy vs. the budget k
on number of sensors selected, for various failure
rates. Averaged over 100 runs.

(d) Traffic Data: The ratio of function evaluations
made by the naive vs accelerated implementations
of adaptive greedy vs. the budget k on number of
sensors selected, for various failure rates. Averaged over 10 runs.

Reward for Adaptive Greedy, with DataDependent Bounds

Reward for Adaptive Greedy, with DataDependent Bounds

100

150

90

Standard Bound
Lazy Adaptive Bound
Adaptive Bound

Standard Bound
Lazy Adaptive Bound
Adaptive Bound

80
70

100

60
50
40
50

30

Adaptive Greedy

Adaptive Greedy
20
10
0

0

5

10

15

20

25

30

35

40

45

0

50

(e) Temperature Data: Rewards & bounds on the
optimal value when pfail (v) = 0.5 for all v vs. the
budget k on number of sensors selected, plotted
with standard errors.

0

50

100

150

200

250

300

350

400

(f) Traffic Data: Rewards & bounds on the optimal value when pfail (v) = 0.5 for all v vs. the
budget k on number of sensors selected, plotted
with standard errors.

Figure 5: Experimental results.
463

fiG OLOVIN & K RAUSE

problem. Note that  can be represented as a permutation of E, because observing an element
multiple times can only increase the cost while providing no benefit over observing it once, and
each element must eventually be selected to guarantee coverage. For the min-sum cover objective,
consider playing  for n/4 time steps. Then P [` observed in n/4 steps] = n/4(n + 1). Likewise
P [`  1 observed in n/4 steps] = n/4(n + 1). Since at least one of these events must occur to
identify the correct hypothesis, by a union bound
P [ identifies the correct hypothesis in n/4 steps]



n
2(n + 1)



1/2.

Thus a lower bound on the expected cost of  is n/8, since for n/4 time steps a cost of at least
1/2 is incurred. Thus, for both the min-cost and min-sum cover objectives the cost of the optimal
non-adaptive policy is (n).
As an example adaptive policy, we can implement a natural binary search strategy, which is
guaranteed to identify the correct hypothesis after O(log n) steps, thus incurring cost O(log n),
proving an adaptivity gap of (n/ log n).

12. Hardness of Approximation
In this paper, we have developed the notion of adaptive submodularity, which characterizes when
certain adaptive stochastic optimization problems are well-behaved in the sense that a simple greedy
policy obtains a constant factor or logarithmic factor approximation to the best policy.
In contrast, we can also show that without adaptive submodularity, the adaptive stochastic optimization problems (2.1), (2.2), and (2.3) are extremely inapproximable, even with (pointwise)
modular objective functions (i.e., those where for each , f : 2E  OE  R is modular/linear in
the first argument): We cannot hope to achieve an O(|E|1 ) approximation ratio for these problems, unless the polynomial hierarchy collapses down to P2 .
Theorem 12.1. For all (possibly non-constant)   1, no polynomial time algorithm for Adaptive
Stochastic Maximization with a budget of k items can approximate the reward of an optimal policy
with a budget of only k items to within a multiplicative factor of O(|E|1 /) for any  > 0, unless
PH = P2 . This holds even for pointwise modular f .
We provide the proof of Theorem 12.1 in Appendix A.7. Note that by setting  = 1, we
obtain O(|E|1 ) hardness for Adaptive Stochastic Maximization. It turns out that in the instance
distribution we construct in the proof of Theorem 12.1 the optimal policy covers every realization
(i.e., always finds the treasure) using a budget of k = O(|E|/2 ) items. Hence if PH 6= P2
then any randomized polynomial time algorithm wishing to cover this instance must have a budget
 = (|E|1 ) times larger than the optimal policy, in order to ensure the ratio of rewards, which
is (|E|1 /), equals one. This yields the following corollary.
Corollary 12.2. No polynomial time algorithm for Adaptive Stochastic Min Cost Coverage can
approximate the cost of an optimal policy to within a multiplicative factor of O(|E|1 ) for any
 > 0, unless PH = P2 . This holds even for pointwise modular f .
Furthermore, since in the instance distribution we construct the optimal policy   covers every
realization using a budget of k, it has c (  )  k. Moreover, since we have shown that under
our complexity theoretic assumptions, any polynomial time randomized policy  with budget k
464

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

achieves at most o(/|E|1 ) of the (unit) value obtained by the optimal policy with budget k, it
follows that c () = (k). Since we require  = (|E|1 ) to cover any set of realizations
constituting, e.g., half of the probability mass, we obtain the following corollary.
Corollary 12.3. No polynomial time algorithm for Adaptive Stochastic Min-Sum Cover can approximate the cost of an optimal policy to within a multiplicative factor of O(|E|1 ) for any  > 0,
unless PH = P2 . This holds even for pointwise modular f .

13. Related Work
There is a large literature on adaptive optimization under partial observability which relates to adaptive submodularity, which can be broadly organized into several different categories. Here, we only
review relevant related work that is not already discussed elsewhere in the manuscript.
13.1 Adaptive Versions of Classic Non-adaptive Optimization Problems
Many approaches consider stochastic generalizations of specific classic non-adaptive optimization
problems, such as Set Cover (Goemans & Vondrak, 2006; Liu et al., 2008), Knapsack (Dean, Goemans, & Vondrak, 2008, 2005) and Traveling Salesman (Gupta et al., 2010). In contrast, in this
paper our goal is to introduce a general problem structure  adaptive submodularity  that unifies
a number of adaptive optimization problems. This is similar to how the classic notion of submodularity unifies various optimization problems such as Set Cover, Facility Location, nonadaptive
Bayesian Experimental Design, etc.
13.2 Competitive Online Optimization
Another active area of research in sequential optimization is the study of competitive online algorithms. A particularly relevant example is Online Set Cover (Alon, Awerbuch, Azar, Buchbinder,
& Naor, 2009), where there is a known set system, an arbitrary sequence of elements is presented
to the algorithm, and the algorithm must irrevocably select sets to purchase such that at all times
the purchased sets cover all elements which have appeared so far. Alon et al. (2009) obtain a
polylogarithmic approximation to this problem, via an online primaldual framework which has
been profitably applied to many other problems. Buchbinder and Naor (2009) provide a detailed
treatment of this framework. Note that competitive analysis focuses on worstcase scenarios. In
contrast, we assume probabilistic information about the world and optimize for the average case.
13.3 (Noisy) Interactive Submodular Set Cover
Recent work by Guillory and Bilmes (2010, 2011) considers a class of adaptive optimization problems over a family of monotone submodular objectives {fh : h  H}. In their problem, one must
cover a monotone submodular objective fh which depends on the (initially unknown) target hypothesis h  H, by adaptively issuing queries and getting responses. Unlike traditional pool-based
active learning, each query may generate a response from a set of valid responses depending on the
target hypothesis. The reward is calculated by evaluating fh on the set of (query, response) pairs
observed, and the goal is to obtain some threshold Q of objective value at minimum total query cost,
where queries may have nonuniform costs. In the noisy variant of the problem (Guillory & Bilmes,
2011), the set of (query, response) pairs observed need not be consistent with any hypothesis in H,
465

fiG OLOVIN & K RAUSE

and the goal is to obtain Q of value for all hypotheses that are close to being consistent with the
observations. For both variants, Guillory and Bilmes consider the worst-case policy cost, and provide greedy algorithms optimizing clever hybrid objective functions. They prove an approximation
guarantee of ln(Q|H|) + 1 for integer valued objective functions {fh }hH in the noisefree case,
and similar logarithmic approximation guarantees for the noisy case.
While similar in spirit to this work, there are several significant differences between the two.
Guillory and Bilmes focus on worst-case policy cost, while we focus mainly on average-case policy
cost. The structure of adaptive submodularity depends on the prior p (), whereas there is no such
dependence in Interactive Submodular Set Cover. This dependence in turn allows us to obtain
results, such as Theorem 5.8 for selfcertifying instances, whose approximation guarantee does not
depend on the number of realizations in the way that the guarantees for Interactive Submodular Set
Cover depend on |H|. As Guillory and Bilmes prove, the latter dependence is fundamental under
reasonable complexity-theoretic assumptions6 . An interesting open problem within the adaptive
submodularity framework that is highlighted by the work on Interactive Submodular Set Cover is
to identify useful instance-specific properties that are sufficient to improve upon the worst-case
approximation guarantee of Theorem 5.9.
13.4 Greedy Frameworks for Adaptive Optimization
The paper that is perhaps closest in spirit to this work is the one on Stochastic Depletion problems
by Chan and Farias (2009), who also identify a general class of adaptive optimization problems
than can be near-optimally solved using greedy algorithms (which in their setting give a factor
2 approximation). However, the similarity is mainly on a conceptual level: The problems and
approaches, as well as example applications considered, are quite different.
13.5 Stochastic Optimization with Recourse
A class of adaptive optimization problems studied extensively in operations research (since Dantzig,
1955) is the area of stochastic optimization with recourse. Here, an optimization problem, such as
Set Cover, Steiner Tree or Facility Location, is presented in multiple stages. At each stage, more
information is revealed, but costs of actions increase. A key difference to the problems studied in
this paper is that in these problems, information gets revealed independently of the actions taken by
the algorithm. There are general efficient, sampling based (approximate) reductions of multi-stage
optimization to the deterministic setting (see, e.g., Gupta, Pal, Ravi, & Sinha, 2005).
13.6 Bayesian Global Optimization
Adaptive Stochastic Optimization is also related to the problem of Bayesian Global Optimization
(c.f., Brochu, Cora, & de Freitas, 2009, for a recent survey of the area). In Bayesian Global Optimization, the goal is to adaptively select inputs in order to maximize an unknown function that is
expensive to evaluate (and can possibly only be evaluated using noisy observations). A common approach that has been successful in many applications (c.f., Lizotte, Wang, Bowling, & Schuurmans,
2007, for a recent application in machine learning), is to assume a prior distribution, such as a Gaus6. They reduce to Set Cover and use the result of Feige (1998), which requires the assumption NP *
DTIME(nO(log log n) ), but it suffices to assume only P 6= NP using the Set Cover approximation hardness result
of Raz and Safra (1997) instead.

466

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

sian process, over the unknown objective function. Several criteria for selecting inputs have been
developed, such as the Expected Improvement (Jones, Schonlau, & Welch, 1998) criterion. However, while recently performance guarantees where obtained in the no-regret setting (Grunewalder,
Audibert, Opper, & Shawe-Taylor, 2010; Srinivas, Krause, Kakade, & Seeger, 2010), we are not
aware of any approximation guarantees for Bayesian Global Optimization.
13.7 Probabilistic Planning
The problem of decision making under partial observability has also been extensively studied in
stochastic optimal control. In particular, Partially Observable Markov Decision Processes (POMDPs,
Smallwood & Sondik, 1973) are a general framework that captures many adaptive optimization
problems under partial observability. Unfortunately, solving POMDPs is PSPACE hard (Papadimitriou & Tsitsiklis, 1987), thus typically heuristic algorithms with no approximation guarantees are
applied (Pineau, Gordon, & Thrun, 2006; Ross, Pineau, Paquet, & Chaib-draa, 2008). For some
special instances of POMDPs related to Multi-armed Bandit problems, (near-)optimal policies can
be found. These include the (optimal) Gittins-index policy for the classic Multi-armed Bandit problem (Gittins & Jones, 1979) and approximate policies for the Multi-armed Bandit problem with
metric switching costs (Guha & Munagala, 2009) and special cases of the Restless Bandit problem (Guha, Munagala, & Shi, 2009). The problems considered in this paper can be formalized as
POMDPs, albeit with exponentially large state space (where the world state represents the selected
items and state/outcome of each item). Thus our results can be interpreted as widening the class of
partially observable planning problems that can be efficiently approximately solved.
13.8 Previous Work by the Authors & Subsequent Developments
This manuscript is an extended version of a paper that appeared in the Conference on Learning
Theory (COLT; Golovin & Krause, 2010). More recently, Golovin and Krause (2011) proved performance guarantees for the greedy policy for the problem of maximizing the expected value of a
policy under constraints more complex than simply selecting at most k items. These include matroid constraints, where a policy can only select independent sets of items and the greedy policy
obtains a 1/2approximation for adaptive monotone submodular objectives, and more generally
p-independence system constraints, where the greedy policy obtains a 1/(p + 1)approximation.
Golovin et al. (2010) and, shortly thereafter, Bellala and Scott (2010), used the adaptive submodularity framework to obtain the first algorithms with provable (logarithmic) approximation guarantees
for the difficult and fundamental problem of active learning with persistent noise. Finally, Golovin,
Krause, Gardner, Converse, and Morey (2011) used adaptive submodularity in the context of a
dynamic conservation planning, and obtain competitiveness guarantees for an ecological reserve
design problem.

14. Conclusions
Planning under partial observability is a central but notoriously difficult problem in artificial intelligence. In this paper, we identified a novel, general class of adaptive optimization problems under
uncertainty that are amenable to efficient, greedy (approximate) solution. In particular, we introduced the concept of adaptive submodularity, generalizing submodular set functions to adaptive
policies. Our generalization is based on a natural adaptive analog of the diminishing returns prop-

467

fiG OLOVIN & K RAUSE

erty well understood for set functions. In the special case of deterministic distributions, adaptive
submodularity reduces to the classical notion of submodular set functions. We proved that several
guarantees carried by the non-adaptive greedy algorithm for submodular set functions generalize to
a natural adaptive greedy algorithm in the case of adaptive submodular functions, for constrained
maximization and certain natural coverage problems with both minimum cost and minimum sum
objectives. We also showed how the adaptive greedy algorithm can be accelerated using lazy evaluations, and how one can compute data-dependent bounds on the optimal solution. We illustrated
the usefulness of the concept by giving several examples of adaptive submodular objectives arising
in diverse AI applications including sensor placement, viral marketing, automated diagnosis and
pool-based active learning. Proving adaptive submodularity for these problems allowed us to recover existing results in these applications as special cases and lead to natural generalizations. Our
experiments on real data indicate that adaptive submodularity can provide practical benefits, such
as significant speed ups and tighter data-dependent bounds. We believe that our results provide an
interesting step in the direction of exploiting structure to solve complex stochastic optimization and
planning problems under partial observability.
Acknowledgments
An extended abstract of this work appeared in COLT 2010 (Golovin & Krause, 2010). We wish to
thank the anonymous referees for their helpful suggestions. This research was partially supported by
ONR grant N00014-09-1-1044, NSF grant CNS-0932392, NSF grant IIS-0953413, DARPA MSEE
grant FA8650-11-1-7156, a gift by Microsoft Corporation, an Okawa Foundation Research Grant,
and by the Caltech Center for the Mathematics of Information.

Appendix A. Additional Proofs and Incorporating Item Costs
In this appendix we provide all of the proofs omitted from the main text. For the results of 5, we
do so by first explaining how our results generalize to the case where items have costs, and then
proving generalizations which incorporate item costs.
A.1 Incorporating Costs: Preliminaries
In this section we provide the preliminaries required to define and analyze the versions of our problems with non-uniform item costs. We suppose each item
P e  E has a cost c(e), and the cost of a
set S  E is given by the modular function c(S) = eS c(e). We define the generalizations of
problems (2.1), (2.2), and (2.3) in A.3, A.4, and A.5, respectively.
Our results are with respect to the greedy policy  greedy and -approximate greedy policies.
With costs, the greedy policy selects an item maximizing (e | ) /c(e), where  is the current
partial realization.
Definition A.1 (Approximate Greedy Policy with Costs). A policy  is an -approximate greedy
policy if for all  such that there exists e  E with (e | ) > 0,



(e | )
1
(e0 | )
()  e :

max
,
c(e)
 e0
c(e0 )
and  terminates upon observing any  such that (e | )  0 for all e  E. That is, an approximate greedy policy always obtains at least (1/) of the maximum possible ratio of condi468

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

tional expected marginal benefit to cost, and terminates when no more benefit can be obtained in
expectation. A greedy policy is any 1-approximate greedy policy.
It will be convenient to imagine the policy executing over time, such that when a policy  selects
an item e, it starts to run e, and finishes running e after c(e) units of time. We next generalize
the definition of policy truncation. Actually we require three such generalizations, which are all
equivalent in the unit cost case.
Definition A.2 (Strict Policy Truncation). The strict level t truncation of a policy , denoted by
[t] , is obtained by running  for t time units, and unselecting items whose runs have not finn
o
P
ished by time t. Formally, [t] has domain   dom() : c(()) + edom() c(e)  t ,
and agrees with  everywhere in its domain.
Definition A.3 (Lax Policy Truncation). The lax level t truncation of a policy , denoted by [t] ,
is obtained by running  for t time units, and selecting the items running at time t. Formally, [t]
o
n
P
c(e)
<
t
, and agrees with  everywhere in its domain.
has domain   dom() :
edom()
Definition A.4 (Policy Truncation with Costs). The level-t-truncation of a policy , denoted by
[t] , is a randomized policy obtained by running  for t time units, and if some item e has been
running for 0   < c(e) time at time t, selecting e independently with probability  /c(e). Formally, [t] is a randomized policy that agrees with  everywhere in its domain, has dom([t] ) 
dom([t] )  dom([t] ) with certainty, and includes each   dom([t] ) \ dom([t] ) in its


P
domain independently with probability t  edom() c(e) /c(()).
In the proofs that follow, we will need a notion of the conditional expected cost of a policy, as
well as an alternate characterization of adaptive monotonicity, based on a notion of policy concatenation. We prove the equivalence of our two adaptive monotonicity conditions in Lemma A.8.
Definition A.5 (Conditional Policy Cost). The conditional policy cost of  conditioned on , denoted c ( | ), is the expected cost of the items  selects under p ( | ). That is, c ( | ) :=
E [c(E(, )) |   ].
Definition A.6 (Policy Concatenation). Given two policies 1 and 2 define 1 @2 as the policy
obtained by running 1 to completion, and then running policy 2 as if from a fresh start, ignoring
the information gathered7 during the running of 1 .
Definition A.7 (Adaptive Monotonicity (Alternate Version)). A function f : 2E  OE  R0
is adaptive monotone with respect to distribution p () if for all policies  and  0 , it holds that
favg ()  favg ( 0 @), where favg () := E [f (E(, ), )] is defined w.r.t. p ().
Lemma A.8 (Adaptive Monotonicity Equivalence). Fix a function f : 2E  OE  R0 . Then
(e | )  0 for all  with P [  ] > 0 and all e  E if and only if for all policies  and  0 ,
favg ()  favg ( 0 @).
7. Technically, if under any realization  policy 2 selects an item that 1 previously selected, then 1 @2 cannot be
written as a function from a set of partial realizations to E, i.e., it is not a policy. This can be amended by allowing
partial realizations to be multisets over elements of E  O, so that, e.g., if e is played twice then (e, (e)) appears
twice in . However, in the interest of readability we will avoid this more cumbersome multiset formalism, and abuse
notation slightly by calling 1 @2 a policy. This issue arises whenever we run some policy and then run another
from a fresh start.

469

fiG OLOVIN & K RAUSE

Proof. Fix policies  and  0 . We begin by proving favg ( 0 @) = favg (@ 0 ). Fix any  and note
that E( 0 @, ) = E( 0 , )  E(, ) = E(@ 0 , ). Hence




favg ( 0 @) = E f (E( 0 @, ), ) = E f (E(@ 0 , ), ) = favg (@ 0 ).
Therefore favg ()  favg ( 0 @) holds if and only if favg ()  favg (@ 0 ).
We first prove the forward direction. Suppose (e | )  0 for all  and all e  E. Note the
expression favg (@ 0 )  favg () can be written as a conical
P combination of (nonnegative) (e0| )
0
terms, i.e., for some   0, favg (@ )  favg () = ,e (,e) (e | ). Hence favg (@ ) 
favg ()  0 and so favg ()  favg (@ 0 ) = favg ( 0 @).
We next prove the backward direction, in contrapositive form. Suppose (e | ) < 0 for some
 with P [  ] > 0 and e  E. Let e1 , . . . , er be the items in dom() and define policies 
and  0 as follows. For i = 1, 2, . . . , r, both  and  0 select ei and observe (ei ). If either policy
observes (ei ) 6= (ei ) it immediately terminates, otherwise it continues. If  succeeds in selecting
all of dom() then it terminates. If  0 succeeds in selecting all of dom() then it selects e and then
terminates. We claim favg (@ 0 )  favg () < 0. Note that E(@ 0 , ) = E(, ) unless   ,
and if    then E(@ 0 , ) = E(, )  {e} and also E(, ) = dom(). Hence


favg (@ 0 )  favg () = E f (E(@ 0 , ), )  f (E(, ), )


= E f (E(@ 0 , ), )  f (E(, ), ) |     P [  ]
= E [f (dom()  {e} , )  f (dom(), ) |   ]  P [  ]
= (e | )  P [  ]
The last term is negative, as P [  ] > 0 and (e | ) < 0 by assumption. Therefore favg () >
favg (@ 0 ) = favg ( 0 @), which completes the proof.
A.2 Adaptive Data Dependent Bounds with Costs
The adaptive data dependent bound has the following generalization with costs.
Lemma A.9 (The Adaptive Data Dependent Bound with Costs). Suppose we have made observations  after selecting dom(). Let   be any policy. Then for adaptive monotone submodular
f : 2E  OE  R0


(e | )


( | )  Z  c ( | ) max
(A.1)
e
c(e)
	
P
P
 | ) and e  E, 0  w  1 .
w
(e
|
)
:
c(e)w

c
(
where Z = maxw
e
e
e
eE
e
Proof. Order the items in dom() arbitrarily, and consider the policy  that for each e  dom()
in order selects e, terminating if (e) 6= (e) and proceeding otherwise, and, should it succeed
in selecting all of dom() without terminating (which occurs iff   ), then proceeds to run  
as if from a fresh start, forgetting the observations in . By construction the expected marginal
benefit of running the   portion of  conditioned on    equals (  | ). For all e  E, let
w(e) = P [e  E(, ) |   ] be the probability that e is selected when running , conditioned
on   . Whenever some e  E \ dom() is selected by , the current partial realization
 0 contains  as a subrealization; hence adaptive submodularity implies (e |  0 )  (e | ). It
470

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

follows that the total contribution of e to (  | ) is upperPbounded by w(e)  (e | ). Summing
over e  E \ dom(), we get a bound of (  | ) 
eE\dom() w(e)(e | ). Next, note
that
w(e)c(e) cost to c (  | ). Hence it must be the case that
P each e  E \ dom() contributes

eE\dom() w(e)c(e)
P  c ( | ). Obviously, w(e)  [0, 1] for all e, since w(e) is a probability.

Hence ( | )  eE\dom() w(e)(e | )  Z because setting we = w(e) is feasible for the
the linear program for which Z is the optimal value.
To show Z  c (  | ) maxe ((e | ) /c(e)), consider any feasible solution w to the linear
program defining Z. It attains objective value




X
X
(e | ) X
(e | )
(e | )
we (e | ) 
we c(e)
 c (  | ) max

we c(e) max
eE
eE
c(e)
c(e)
c(e)
eE
eE
eE
P
since eE we c(e)  c (  | ) by the feasibility of w.
A simple greedy algorithm can be used to compute Z; we provide pseudocode for it in Algorithm 3. The correctness of this algorithm is more readily discerned upon rewriting the linear
program using variables xe = c(e)we to obtain
(
)
X
X

Z = max
xe ((e | ) /c(e)) :
xe  c ( | ) and e  E, 0  xe  c(e) .
x

eE

e

Intuitively, it is clear that to optimize x we should shift
variables with the highest
Pmass towards

(e | ) /c(e) ratio. Clearly, any optimal solution has e xe = c ( | ). Moreover, in any optimal solution, (e | ) /c(e) > (e0 | ) /c(e0 ) implies xe = c(e) or xe0 = 0, since otherwise
it would be possible to shift mass from xe0 to xe and obtain an increase in objective value. If the
(e | ) /c(e) values are distinct for distinct items, there will be a unique solution satisfying these
constraints, which Algorithm 3 will compute. Otherwise, we imagine perturbing each (e | ) by
independent random quantities e drawn uniformly from [0, ] to make them distinct. This changes
the optimum value
Pby at most |E|, which vanishes as we let  tend towards zero. Hence any solution satisfying e xe = c (  | ) and (e | ) /c(e) > (e0 | ) /c(e0 ) implies xe = c(e) or
xe0 = 0 is optimal. Since Algorithm 3 outputs the value of such a solution, it is correct.
A.3 The Max-Cover Objective
With item costs, the Adaptive Stochastic Maximization problem becomes one of finding some
   arg max favg ([k] )

(A.2)



where k is a budget on the cost of selected items, and we define favg () for a randomized policy  to
be favg () := E [f (E(, ), )] as before, where the expectation is now over both  and the internal randomness of  which determines E(, ) for each . We prove the following generalization
of Theorem 5.2.
Theorem A.10. Fix any   1 and item costs c : E  N. If f is adaptive monotone and adaptive
submodular with respect to the distribution p (), and  is an -approximate greedy policy, then for
all policies   and positive integers ` and k



favg ([`] ) > 1  e`/k favg ([k]
).
471

fiG OLOVIN & K RAUSE

Input: Groundset E; Partial realization ; Costs c : E  N; Budget C = c (  | );
Conditional expected marginal benefits (e | ) for all e  E.
Output: Z = P
	
P

maxw
eE we (e | ) :
e c(e)we  c ( | ) and e  E, 0  we  1
begin
(e2 | )
(en | )
1 | )
Sort E by (e | ) /c(e), so that (e
c(e1 )  c(e2 )  . . .  c(en ) ;
Set w  0; i  0; a  0; z  0; e  NULL;
while a < C do
i  i + 1; e  ei ;
we  min {1, C  a};
a  a + c(e)we ; z  z + we (e | );
Output z;
end
Algorithm 3: Algorithm to compute the data dependent bound Z of Lemma A.9.
Proof. The proof goes along the lines of the performance analysis of the greedy algorithm for
maximizing a submodular function subject to a cardinality constraint of Nemhauser et al. (1978).
An extension of that analysis to -approximate greedy algorithms, which is analogous to ours but
for the nonadaptive case, is shown by Goundan and Schulz (2007). For brevity, we will assume
 . Then for all i, 0  i < `
without loss of generality that  = [`] and   = [k]

favg (  )  favg ([i] @  )  favg ([i] ) + k favg ([i+1] )  favg ([i] ) .
(A.3)
The first inequality is due to the adaptive monotonicity of f and Lemma A.8, from which we may
infer favg (2 )  favg (1 @2 ) for any 1 and 2 . The second inequalitymay be obtained as a corol	
lary of Lemma A.9 as follows. Fix any partial realization  of the form (e, (e)) : e  E([i] , )
for some . Consider (  | ), which equals the expected marginal benefit of the   portion of
[i] @  conditioned on   . Lemma A.9 allows us to bound it as
E [(  | )]  E [c (  | )]  max ((e | ) /c(e)) ,
e

where the expectations are taken over the internal randomness of   , if there is any. Note that since
0 for some  0 we know that for all , E [c(E(  , ))]  k, where the expectation
  has the form [k]
is again taken over the internal randomness of   . Hence E [c (  | )]  k for all . It follows
that E [(  | )]  k  maxe ((e | ) /c(e)). By definition of an -approximate greedy policy,
 obtains at least (1/) maxe ((e | ) /c(e))  E [(  | )] /k expected marginal benefit per
unit cost in a step immediately following its observation of . Next we take an appropriate convex combination of the previous inequality
with different values of . Let  be a	
random partial

realization distributed as p () := P  =  | .  = (e, (e)) : e  E([i] , ) Then



1
(e | )
favg ([i+1] )  favg ([i] )  E
max
 e
c(e)



E [( | )]
 E
k
favg ([i] @  )  favg ([i] )
=
k
472

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

A simple rearrangement of terms then yields the second inequality in (A.3).
Now define i := favg (  )  favg ([i] ), so that (A.3) implies i  k(i  i+1 ), from


1
1 `
which we infer i+1  1  k
i and hence `  1  k
0 < e`/k 0 , where for this
x
last inequality we have used the fact that 1  x < e for all x > 0. Thus favg (  )  favg ([`] ) <

e`/k favg (  )  favg ([0] )  e`/k favg (  ) so favg () > (1  e`/k )favg (  ).

A.4 The Min-Cost-Cover Objective
In this section, we provide arbitrary item cost generalizations of Theorem 5.8 and Theorem 5.9.
With item costs the Adaptive Stochastic Minimum Cost Cover problem becomes one of finding, for
some quota on utility Q,
   arg min cavg () such that f (E(, ), )  Q for all ,

(A.4)



where cavg () := E [c(E(, ))]. Without loss of generality, we may take a truncated version of f ,
namely (A, ) 7 min {Q, f (A, )}, and rephrase Problem (A.4) as finding
   arg min cavg () such that  covers  for all .

(A.5)



Hereby, recall that  covers  if E [f (E(, ), )] = f (E, ), where the expectation is over any
internal randomness of . We will consider only Problem (A.5) for the remainder. We also consider
the worst-case variant of this problem, where we replace the expected cost cavg () objective with
the worst-case cost cwc () := max c(E(, )).
The definition of coverage (Definition 5.4 in 5.2 on page 441) requires no modification to handle item costs. Note, however, that coverage is all-or-nothing in the sense that covering a realization
 with probability less than one does not count as covering it. A corollary of this is that only items
whose runs have finished help with coverage, whereas currently running items do not. For a simple
example, consider the case where E = {e}, c(e) = 2, f (A, ) = |A|, and policy  that selects
e and then terminates. Then [1] is a randomized policy which is  with probability 12 , and is the
empty policy with probability 12 , so E [f (E(, ), )] = 12 < 1 = f (E, ) for each . Hence, even
though half the time [1] covers all realizations, it is counted as not covering any.
We begin with the approximation guarantee for the average-case policy cost with arbitrary item
costs.
Theorem A.11. Suppose f : 2E  OE  R0 is adaptive submodular and strongly adaptive
monotone with respect to p () and there exists Q such that f (E, ) = Q for all . Let  be any
value such that f (S, ) > Q   implies f (S, ) = Q for all S and . Let  = min p () be
 be an optimal policy minimizing the expected
the minimum probability of any realization. Let avg
number of items selected to guarantee every realization is covered. Let  be an -approximate
greedy policy with respect to the item costs. Then in general
  

Q

cavg ()   cavg (avg ) ln
+1


473

fiG OLOVIN & K RAUSE

and for selfcertifying instances

  
Q

+1 .
cavg ()   cavg (avg
) ln

Note that if range(f )  Z, then  = 1 is a valid choice, so for general and selfcertifying in ) (ln(Q/) + 1) and c

stances we have cavg ()   cavg (avg
avg ()   cavg (avg ) (ln(Q) + 1),
respectively.
Proof. Consider running -approximate greedy policy  to completion, i.e., until it covers the true
realization. It starts off with v0 := E [f (, )]  0 reward in expectation, and terminates with Q
reward. Along the way it will go through some sequence of partial realizations specifying its current
observations,  0   1       ` , such that dom( i ) \ dom( i1 ) consists precisely of the ith
item selected by . We call this sequence the trace  =  () of . For a realization  and x  R0 ,
we define  (, x) as the partial realization seen by  just before it achieved x reward in expectation
under . Formally,
 (, x)  arg max {| dom()| :    (), E [f (dom(), ) |   ] < x} .

(A.6)

Note that  (, x) exists for all x  (v0 , Q], and when it exists it is unique since no two elements of
the trace have equally large domains. Also note that by the strong adaptive monotonicity of f , the
function i 7 E [f (dom( i ), ) |    i ] must be nondecreasing for any trace  0 ,  1 , . . . ,  ` .
Our overall strategy will be to bound the expected cost cavg () of  by bounding the price it pays
per unit of expected reward gained as it runs, and then
 integrating
 over the run. Note that Lemma A.9
 |  /c   |  for all . An -approximate greedy
tells us that maxe ((e | ) /c(e))   avg
avg
policy obtains at least 1/ of this rate. Hence we may bound its price, which we denote by , as




()   c avg
|  / avg
| .
(A.7)
Rather than try to bound the expected price as  progresses in time, we will bound the expected
price as it progresses in the expected reward it obtains, measured as E [f (dom(),
 ) |   ]
 |  (, x)  Q  x for
where  is the current partial realization. We next claim that  avg
all  and x. Note that E [f (dom( (, x)), ) |    (, x)] < x by definition of  (, x), and
 , ), ) = Q for all  since   covers every realization. Since Q is the maximum possible
f (E(avg
avg

 |  (, x) < Q  x then we can generate a violation of strong adaptive monoreward, if  avg
 , 0 ), and then selecting dom( (, x)) to
tonicity by fixing some 0   (, x), selecting E(avg

 |  (, x)  Q  x, and we infer
reduce the expected reward. Thus  avg


 |  (, x)
 |  (, x)
 c avg
 c avg
 
( (, x)) 
.
(A.8)
 |  (, x)
Qx
 avg
Next, we take an expectation over . Let (x) := E [( (, x))]. Let  x1 , . . . ,  xr be the possible
values of  (, x). Then because {{ :    xi } : i = 1, 2, . . . , r} partitions the set of realizations,
r
X
X





E c avg
|  (, x)
=
P [   xi ]
p ( |  xi )  c avg
|
i=1

=

X

(A.9)



p ()  c


avg
|



(A.10)



= cavg (avg
)

474

(A.11)

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

It follows that
(x) 

 )
 cavg (avg
.
Qx

(A.12)

Let cavg (, Q0 ) denote the expected cost to obtain expected reward Q0 . Then we can bound cavg (, Q0 )
as


Z Q0
Z Q0
 cavg (  )
Q
cavg (, Q0 ) =
(x)dx 
dx =  cavg (  ) ln
.
(A.13)
Qx
Q  Q0
x=0
x=0
We now use slightly different analyses for general instances and for selfcertifying instances.
We begin with general instances. For these, we set Q0 = Q   and use a more refined argument
to bound the cost of getting the remaining expected reward. Fix   dom() and any 0  . We
say  covers 0 if  covers 0 by the time it observes . By definition of  and , if some 0 is not
covered by  then Q  E [f (dom(), ) |   ]  . Hence the last item that  selects, say
upon observing , must increase its conditional expected value from E [f (dom(), ) |   ] 
Q   to Q. By Eq. (A.8), it follows that for x  [Q  , Q],


 |  (, x)
 |  (, x)
 c avg
 c avg
 
( (, x)) 
.
 |  (, x)

 avg
 )/ for all x  [Q 
As before, we may take the expectation over  to obtain (x)   cavg (avg
, Q]. This fact together with Eq. (A.13) yield

cavg ()  cavg (, Q) = cavg (, Q  ) +

RQ
x=Q

  cavg (  ) ln (Q/) +

E [(x)] dx

RQ
x=Q

cavg (  )
dx


=  cavg (  ) (ln (Q/) + 1)
which completes the proof for general instances.
For selfcertifying instances we use a similar argument. For these instances we set Q0 = Q  ,
and argue that the last item that  selects must increase its conditional expected value from at
most Q   to Q. For suppose  currently observes , and has not achieved conditional value Q,
i.e., E [f (dom(), ) |   ] < Q. Then some    is uncovered. Since the instance is self
certifying, every  with    then has f (dom(), ) < f (E, ) = Q. By definition of , for each
 with    we then have f (dom(), )  Q  , which implies E [f (dom(), ) |   ] 
Q  . Reasoning analogously as with general instances, we may derive from this that (x) 
R
 )/ for all x  [Q  , Q]. Computing c () = c (, Q0 ) + Q
 cavg (avg
avg
avg
x=Q0 E [(x)] dx as
before gives us the claimed approximation ratio for selfcertifying instances, and completes the
proof.
Next we consider the worst-case cost. We generalize Theorem 5.9 by incorporating arbitrary item
costs.
Theorem A.12. Suppose f : 2E OE  R0 is adaptive monotone and adaptive submodular with
respect to p (), and let  be any value such that f (S, ) > f (E, )   implies f (S, ) = f (E, )
 be
for all S and . Let  = min p () be the minimum probability of any realization. Let wc
the optimal policy minimizing the worst-case cost cwc () while guaranteeing that every realization
475

fiG OLOVIN & K RAUSE

is covered. Let  be an -approximate greedy policy with respect to the item costs. Finally, let
Q := E [f (E, )] be the maximum possible expected reward. Then

  
Q

+1 .
cwc ()   cwc (wc ) ln

 ), let ` = k ln (Q/), and
Proof. Let  be an -approximate greedy policy. Let k = cwc (wc
apply Theorem A.10 with these parameters to yield





`/k


favg ([`] ) > 1  e
favg (wc ) = 1 
favg (wc
).
(A.14)
Q
 covers every realization by assumption, f (  ) = E [f (E, )] = Q, so rearranging
Since wc
avg wc
terms of Eq. (A.14) yields Q  favg ([`] ) < . Since favg ([`] )  favg ([`] ) by the adaptive
monotonicity of f , it follows that Q  favg ([`] ) < . By definition of  and , if some  is not
covered by [`] then Q  favg ([`] )  . Thus Q  favg ([`] ) <  implies Q  favg ([`] ) =
0, meaning [`] covers every realization.
We next claim that [`] has worst-case cost at most ` + k. It is sufficient to show that the
final item executed by [`] has cost at most k for any realization. As we will prove, this follows
 covers every realization at cost at
from the facts that  is an -approximate greedy policy and wc
most k. The data dependent bound, Lemma A.9 on page 470, guarantees that


 | )
 | )
(e | )
(wc
(wc
max


.
(A.15)
 | )
e
c(e)
c (wc
k
 | ). Supposing this
Suppose   dom(). We would like to say that maxe (e | )  (wc
 | ) /k, and hence
is true, any item e with cost c(e) > k must have (e | ) /c(e) < (wc
cannot be selected by any -approximate greedy policy upon observing  by Eq. (A.15), and thus
the final item executed by [`] has cost at most k for any realization. So we next show that
 | ). Towards this end, note that Lemma A.13 implies
maxe (e | )  (wc

max (e | )  E [f (E, ) |   ]  E [f (dom(), ) |   ] .
e

(A.16)

 | ) it suffices to show
and to prove maxe (e | )  (wc

E [f (E, ) |   ]  E [f (E(wc
, )  dom(), ) |   ] .

(A.17)

Proving Eq. (A.17) is quite straightforward if f is strongly adaptive monotone. Given that f
is only adaptive monotone, it requires some additional effort. So fix A  E and let A be a nonadaptive policy that selects all items in A in some arbitrary order. Let P := { : dom() = A}.
 and any   P to obtain
Apply Lemma A.13 with  0 = A @wc

E [f (E(wc
, )  A, ) |   ]  E [f (E, ) |   ] .

(A.18)

P
 , )  A, ) |   ] = f ( @  )  f (  ) =
Note that P P [  ]  E [f (E(wc
avg A
avg wc
wc
P
E [f (E, )]. Since we know E [f (E, )] = P P [  ]  E [f (E, ) |   ], an averaging
argument together with Eq. (A.18) then implies that for all   P

E [f (E(wc
, )  A, ) |   ] = E [f (E, ) |   ] .

476

(A.19)

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Since  was an arbitrary partial realization with dom() = A, and A  E was arbitrary,
fix   dom() and let A = dom(). With these settings, Eq. (A.19) implies Eq. (A.17), and
 | ), and thus an -approximate greedy policy can never select an
thus maxe (e | )  (wc
 ). Hence c (
item with cost exceeding k, where k = cwc (wc
wc [`] )  cwc ([`] )  k, and so
cwc ([`] )  ` + k. This completes the proof.
Lemma A.13. Fix adaptive monotone submodular objective f . For any policy  and any  
dom() we have
E [f (E(, ), ) |   ]  E [f (E, ) |   ] .
Proof. Augment  to a new policy  0 as follows. Run  to completion, and let  0 be the partial
realization consisting of all of the states it has observed. If    0 , then proceed to select all the
remaining items in E in any order. Otherwise, if  *  0 then terminate. Then


E [f (E(, ), ) |   ]  E f (E( 0 , ), ) |    = E [f (E, ) |   ] (A.20)
where the inequality is by repeated application of the adaptive monotonicity of f , and the equality
is by construction.
In 5.2 we described how the result of Feige (1998) implies that there is no polynomial time
(1  ) ln (Q/) approximation algorithm for selfcertifying instances of Adaptive Stochastic Min
Cost Cover, unless NP  DTIME(nO(log log n) ). Here we show a related result for general instances.
Lemma A.14. For every constant  > 0, there is no (1  ) ln (Q/) polynomial time approximation algorithm for general instances of Adaptive Stochastic Min Cost Cover, for either the average
case objective cavg () or the worst-case objective cwc (), unless NP  DTIME(nO(log log n) ).
Proof. We offer a reduction from the Set Cover problem. Fix a Set Cover instance with ground
set U and sets {S1 , S2 , . . . , Sm }  2U with unit-cost sets. Fix Q,  and  such that 1/ and
Q
Q/ are positive integers, and 
= |U |. Let E := {S1 , S2 , . . . , Sm }, and set the cost of each
item to one. Partition U into 1/ disjoint, equally sized subsets U1 , U2 , . . . , U1/ . Construct a
realization i for each Ui . Let the set of states be O = {NULL}. Hence i (e) = NULL for all
i and e, so that no knowledge of the true realization is revealed by selecting items. We use a
uniform distribution over realizations, i.e., p (i ) =  for all i. Finally, our objective is f (C, i ) :=
| SC (S  Ui )|, i.e., the number of elements in Ui that we cover with sets in C. Since |O| = 1,
every realization is consistent with every possible partial realization . Hence for any , we have
E [f (dom(), ) |   ] =  f(dom()), where f(C) = | SC S| is the objective function of
the original set cover instance. Since f is submodular, f is adaptive submodular. Likewise, since
f is monotone, and |O| = 1, f is strongly adaptive monotone. Now, to cover any realization, we
must obtain the maximum possible value for all realizations, which means selecting a collection
of sets C such that SC S = U . Conversely, any C such that SC S = U clearly covers f .
Hence this instance of Adaptive Stochastic Min Cost Cover, with either the average case objective
cavg () or the worst-case objective cwc (), is equivalent to the original Set Cover instance. Therefore,
the result from Feige (1998) implies that there is no polynomial time algorithm for obtaining a
(1  ) ln |U | = (1  ) ln (Q/) approximation for Adaptive Stochastic Min Cost Cover unless
NP  DTIME(nO(log log n) ).

477

fiG OLOVIN & K RAUSE

A.5 The Min-Sum Objective
In this section we prove Theorem 5.10, which appears on page 445, in the case where the items have
arbitrary costs. Our proof resembles the analogous proof of Streeter and Golovin (2007) for the
non-adaptive min-sum submodular cover problem, and, like that proof, ultimately derives from an
extremely elegant performance analysis of the greedy algorithm for min-sum set cover due to Feige
et al. (2004).
The objective function c () generalized to arbitrary cost items uses the strict truncation8 [t] in
place of [t] in the unit-cost definition:
c () :=


X


X
 X

E [f (E, )]  favg ([t] ) =
p ()
f (E, )  f (E([t] , ), ) .

t=0



t=0

(A.21)
We will prove that any -approximate greedy policy  achieves a 4-approximation for the minsum objective, i.e., c ()  4 c (  ) for all policies   . To do so, we require the following
lemma.
Lemma A.15. Fix an -approximate greedy policy
  for some adaptive monotone submodular
function f and let si :=  favg ([i+1] )  favg ([i] ) . For any policy   and nonnegative integers i
 )f
and k, we have favg ([k]
avg ([i] ) + k  si .
 )  f (

Proof. Fix ,   , i, and k. By adaptive monotonicity favg ([k]
avg [i] @[k] ). We next aim
to prove

favg ([i] @[k]
)  favg ([i] ) + k  si
(A.22)

which is sufficient to complete the proof. Towards this end,
 fix a partial realization  of the

	
 |  , which equals the expected
form (e, (e)) : e  E([i] , ) for some . Consider  [k]
 portion of 

marginal benefit of the [k]
[i] @[k] conditioned on   . Lemma A.9 allows us to
bound it as


h 
i
h 
i
(e | )


E  [k] | 
 E c [k] |   max
,
e
c(e)

where the expectations
h are taken over
i the internal randomness of  , if there is any. Note that
 , ))  k, where the expectation is again taken over the internal
for all , we have E c(E([k]
h 
i
h 
i
 . Hence E c   | 
 |
randomness of [k]
 k for all . It follows that E  [k]

[k]
k  maxe ((e | ) /c(e)). By definition of an -approximate greedy policy,  obtains at least
h 
i

(1/) max ((e | ) /c(e))  E  [k]
|  /k
(A.23)
e

expected marginal benefit per unit cost in a step immediately following its observation of . Next
we take an appropriate convex combination of the previous inequality with different
values of .
	
Let  be a random partial realization distributed as (e, (e)) : e  E([i] , ) . Then taking the
8. See Definition A.2 on page 469.

478

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Figure 6: An illustration of the inequality

R

x=0 h(x)dx



P

i0 xi (yi

 yi+1 ).

expectation of Eq. (A.23) over  yields
i 
 h 
 |
 )  f (
E  [k]
favg ([i] @[k]
avg [i] )


favg ([i+1] )  favg ([i] )  E
=
.
k
k

(A.24)


Multiplying Eq. (A.24) by k, and substituting in si =  favg ([i+1] )  favg ([i] ) , we conclude
 )f (
ksi  favg ([i] @[k]
avg [i] ) which immediately yields Eq. (A.22) and concludes the proof.
Using Lemma A.15, together with a geometric argument developed by Feige et al. (2004), we now
prove Theorem 5.10.
Proof of Theorem 5.10: Let Q := E [f (E, )] be the maximum possible expected reward, where
the expectation
greedy policy. Define Ri :=
 is taken w.r.t. p (). Let  be an
 -approximate
Pi
Q  favg [i] and define Pi := Q  favg [i] . Let xi := 2s
,
let yi := R2i , and let h(x) :=
i


 ). We claim f
Q  favg ([x]
avg [i]  favg [i] and so Pi  Ri . This clearly holds if [i] is
the empty policy, and otherwise  can always select an item that contributes zero marginal benefit,
namely an item it has already played previously. Hence an -approximate greedy
policy  can

never select items with negative expected marginal benefit, and so favg [i]  favg [i] . By




Lemma A.15, favg [x
 favg [i] + xi si . Therefore
i]
Pi
Ri
h(xi )  Q  favg ([i] )  xi  si = Pi 

= yi
(A.25)
2
2




For similar reasons that favg [i]  favg [i] , we have favg [i1]  favg [i] , and so
the sequence hy1 , y2 , . . .i is non-increasing. The adaptive monotonicity and adaptive submodular ) >
ity of f imply that h(x) is non-increasing. Informally, this is because otherwise, if favg ([x]

favg ([x+1]
) for some x, then the optimal policy must be sacrificing immediate rewards at time x
in exchange for greater returns later, and it can be shown that if such a strategy is optimal, then
adaptive
submodularity
cannot hold. Eq. (A.25) and the monotonicity of h and i 7 yi imply that
R
P
h(x)dx

x
(yi  yi+1 ) (see Figure 6). The left hand side is a lower bound for c (  ),
i
i0
x=0
1 P
1
and because si =  (Ri  Ri+1 ) the right hand side simplifies to 4
i0 Pi = 4 c (), proving

c ()  4  c ( ).
479

fiG OLOVIN & K RAUSE

A.6 A Symbol Table
E, e  E
O, o  O


, 

p
p( | )

E(, )
(e | )
( | )
[e/o]
k
[k]
[k]
[k]
@ 0
f
favg
c
cavg
cwc
c
c ( | )

Q

1P

Ground set of items, and an individual item.
States an item may be in, or outcomes of selecting an item, and an individual
state/outcome.
A realization, i.e., a function from items to states.
A partial realization, typically encoding the current set of observations;
each   E  O is a partial mapping from items to states.
A random realization and a random partial realization, respectively.
The consistency relation:    means (e) = (e) for all e  dom().
The probability distribution on realizations.
The conditional distribution on realizations: p( | ) := P [ =  |   ].
A policy, which maps partial realizations to items.
The set of all items selected by  when run under realization .
The conditional expected marginal benefit of e conditioned on :
(e | ) := E [f (dom()  {e} , )  f (dom(), ) |   ].
The conditional expected marginal benefit of policy  conditioned on :
( | ) := E [f (dom()  E(, ), )  f (dom(), ) |   ].
Shorthand for   {(e, o)}.
Budget on the cost of selected item sets.
A truncated policy. See Definition 5.1 on page 439 (unit costs) and Definition A.4
on page 469.
A strictly truncated policy. See Definition A.2 on page 469.
A laxly truncated policy. See Definition A.3 on page 469.
Policies  and  0 concatenated together. See Definition A.6 on page 469.
An objective function, of type f : 2E  OE  R0 unless stated otherwise.
Average benefit: favg () := E [f (E(, ), )].
P
Item costs c : E  N. Extended to sets via c(S) := eS c(e).
Average cost of a policy: cavg () := E [c(E(, ))].
Worst-case cost of a policy: cwc () := max c(E(, )).

P
Min-sum cost of a policy: c () := 
t=0 E [f (E, )]  favg ([t] ) .
Conditional average policy cost: c ( | ) := E [c(E(, )) |   ].
Approximation factor for greedy optimization in an -approximate greedy policy.
Benefit quota. Often Q = E [f (E, )].
Coverage gap:  = sup { 0 : f (S, ) > Q   0 implies f (S, )  Q for all S, }.
The indicator for proposition P , which equals one if P is true and zero if P is false.
Table 2: Important symbols and notations used in this article

480

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

A.7 Proof of Approximation Hardness in the Absence of Adaptive Submodularity
We now provide the proof of Theorem 12.1 whose statement appears on page 464 in 12.
Proof of Theorem 12.1: We construct a hard instance based on the following intuition. We make the
algorithm go treasure hunting. There is a set of t locations {0, 1, , . . . , t  1}, there is a treasure
at one of these locations, and the algorithm gets unit reward if it finds it, and zero reward otherwise.
There are m maps, each consisting of a cluster of s bits, and each purporting to indicate where
the treasure is, and each map is stored in a (weak) secret-sharing way, so that querying few bits of
a map reveals nothing about where it says the treasure is. Moreover, all but one of the maps are
fake, and there is a puzzle indicating which map is the correct one indicating the treasures location.
Formally, a fake map is one which is probabilistically independent of the location of the treasure,
conditioned on the puzzle.
Our instance will have three types of items, E = ET ] EM ] EP , where |ET | = t encodes
where the treasure is, |EM | = ms encodes the maps, and |EP | = n3 encodes the puzzle, where
m, t, s and n are specified below. All outcomes are binary, so that O = {0, 1}, and we identify
items with bit indices. Accordingly we say that (e) is the value of bit e. For all e  EM  EP ,
P [(e) = 1] = .5 independently. The conditional distribution of (ET ) given (EM  EP ) will
be deterministic as specified below. Our objective function f is linear, and defined as follows:
f (A, ) = |{e  A  ET : (e) = 1}|.
` for a suitWe now describe the puzzle, which is to compute i(P ) := (perm(P ) mod p) Pmod 2Q
able random matrix P , and suitable prime p and integer `, where perm(P ) = Sn ni=1 Pi(i)
is the permanent of P . We exploit Theorem 1.9 of Feige and Lund (1997) in which they show that
if there exist constants ,  > 0 such that a randomized polynomial time algorithm can compute
(perm(P ) mod p) mod 2` correctly with probability 2` (1+1/n ), where P is drawn uniformly

at random from {0, 1, 2, . . . , p  1}nn , p is any prime superpolynomial in n, and `  p 12   ,
then PH = AM = P2 . To encode the puzzle, we fix a prime p  [2n2 , 2n1 ] and use the n3 bits of
(EP ) to sample P = P () (nearly) uniformly
at random from {0, 1, 2, . . . , p  1}nn as follows.
P
For a matrix P  Znn , we let rep(P ) := ij Pij  p(i1)n+(j1) define a base p representation
of P . Note rep() is one-to-one for n  n matrices with entries in Zp , so we can define its inverse
3
rep1 (). The encoding
P () kinterprets the bits (EP ) as an integer x in [2n ], and computes y = x
j
2
3
2
2
mod (pn ). If x  2n /pn pn , then P = rep1 (y). Otherwise, P is the all zero matrix. This
2

3

2

latter event occurs with probability at most pn /2n  2n , and in this case we simply suppose
2
the algorithm under consideration finds the treasure and so gets unit reward. This adds 2n to its
expected reward. So let us assume from now on that
U P is drawn uniformly at random.
Next we consider the maps. Partition EM = m
i=1 Mi into m maps Mi , each consisting of s
items. For each map Mi , partition its items into s/ log2 t groups of log2 t bits each, so that the bits
of each group encode a log2 t bit binary string. Let vi  {0, 1, . . . , t  1} be the XOR of these
s/ log2 t binary strings, interpreted as an integer (using any fixed encoding). We say Mi points to
vi as the location of the treasure. A priori, each vi is uniformly distributed in {0, ..., t  1}. For
a particular realization of (EP  EM ), define v() := vi(P ()) . We set v() to be the location
of the treasure under realization , i.e., we label ET = {e0 , e1 , . . . , et1 } and ensure (ej ) = 1 if
j = vi(P ()) , and (e) = 0 for all other e  ET . Note the random variable v = v() is distributed
uniformly at random in {0, 1, . . . , t  1}. Note that this still holds if we condition on the realizations
481

fiG OLOVIN & K RAUSE

of any set of s/ log2 t  1 items in a map, because in this case there is still at least one group whose
bits remain completely unobserved.
Now consider the optimal policy with a budget of k = n3 + s + 1 items to pick. Clearly, its
reward can be at most 1. However, given a budget of k, a computationally unconstrained policy can
exhaustively sample EP , solve the puzzle (i.e., compute i(P )), read the correct map (i.e., exhaustively sample Mi(P ) ), decode the map (i.e., compute v = vi(P ) ), and get the treasure (i.e., pick ev )
thereby obtaining a reward of one.
Now we give an upper bound on the expected reward R of any randomized polynomial time
algorithm A with a budget of k items, assuming P2 6= PH. Fix a small constant  > 0, and set
s = n3 and m = t = n1/ . We suppose we give A the realizations (EM ) for free. We also replace
its budget of k items with a budget of k specifically for map items in EM and an additional
budget of k specifically for the treasure locations in ET . Obviously, this can only help it. As
noted, if it selects less than s/ log2 t bits from the map Mi(P ) indicated by P , the distribution over
vi(P ) conditioned on those realizations is still uniform. Of course, knowledge of vi for i 6= i(P )
is useless for getting reward. Hence A can try at most k log2 (t)/s = o(k) maps in an attempt
to find Mi(P ) . Note that if we have a randomized algorithm which given a random P drawn from
{0, 1, 2, . . . , p  1}nn always outputs a set S of integers of size  such that P [i(P )  S]  q,
then we can use it to construct a randomized algorithm that, given P , outputs an integer x such that
P [i(P ) = x]  q/, simply by running the first algorithm and then selecting a random item of S.
If A does not find Mi(P ) , the distribution on the treasures location is uniform given its knowledge.
Hence its budget of k treasure locations can only earn it expected reward at most k/t. Armed
with these observations and Theorem 1.9 in the work of Feige and Lund (1997) and our complexity
2
theoretic assumptions, we infer E [R]  o(k)  2` (1 + 1/n ) + k/t + 2n . Since s = n3 and
m = t = n1/ and  = (1) and  = 1 and ` = log2 m and k = n3 + s + 1 = 2n3 + 1, we have
E [R] 

k
(1 + o(1)) = 2n31/ (1 + o(1)).
t

Next note that |E| = t + ms + n3 = n3+1/ (1 + o(1)). Straightforward algebra shows that in
order to ensure E [R] = o(/|E|1 ), it suffices to choose   /6. Thus, under our complexity
theoretic assumptions, any polynomial time randomized algorithm A with budget k achieves at
most o(/|E|1 ) of the value obtained by the optimal policy with budget k, so the approximation
ratio is (|E|1 /).

References
Alon, N., Awerbuch, B., Azar, Y., Buchbinder, N., & Naor, J. S. (2009). The online set cover
problem. SIAM Journal on Computing, 39, 361370.
Arkin, E. M., Meijer, H., Mitchell, J. S. B., Rappaport, D., & Skiena, S. S. (1993). Decision trees for
geometric models. In Proceedings of Symposium on Computational Geometry, pp. 369378,
New York, NY, USA. ACM.
Asadpour, A., Nazerzadeh, H., & Saberi, A. (2008). Stochastic submodular maximization. In WINE
08: Proceedings of the 4th International Workshop on Internet and Network Economics, pp.
477489, Berlin, Heidelberg. Springer-Verlag.
Bellala, G., & Scott, C. (2010). Modified group generalized binary search with near-optimal performance guarantees. Tech. rep., University of Michigan.
482

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Brochu, E., Cora, M., & de Freitas, N. (2009). A tutorial on Bayesian optimization of expensive cost
functions, with application to active user modeling and hierarchical reinforcement learning.
Tech. rep. TR-2009-23, Department of Computer Science, University of British Columbia.
Buchbinder, N., & Naor, J. S. (2009). The design of competitive online algorithms via a primaldual
approach. Foundations and Trends in Theoretical Computer Science, 3, 93263.
Chakaravarthy, V. T., Pandit, V., Roy, S., Awasthi, P., & Mohania, M. (2007). Decision trees for
entity identification: Approximation algorithms and hardness results. In Proceedings of the
ACM-SIGMOD Symposium on Principles of Database Systems.
Chan, C. W., & Farias, V. F. (2009). Stochastic depletion problems: Effective myopic policies for a
class of dynamic optimization problems. Mathematics of Operations Research, 34(2), 333
350.
Cohn, D. A., Gharamani, Z., & Jordan, M. I. (1996). Active learning with statistical models. Journal
of Artificial Intelligence Research (JAIR), 4, 129145.
Dantzig, G. B. (1955). Linear programming under uncertainty. Management Science, 1, 197206.
Dasgupta, S. (2004). Analysis of a greedy active learning strategy. In NIPS: Advances in Neural
Information Processing Systems 17, pp. 337344. MIT Press.
Dean, B., Goemans, M., & Vondrak, J. (2005). Adaptivity and approximation for stochastic packing
problems. In Proceedings of the 16th ACM-SIAM Symposium on Discrete Algorithms,, pp.
395404.
Dean, B., Goemans, M., & Vondrak, J. (2008). Approximating the stochastic knapsack problem:
The benefit of adaptivity. Mathematics of Operations Research, 33, 945964.
Deshpande, A., Guestrin, C., Madden, S., Hellerstein, J., & Hong, W. (2004). Model-driven data
acquisition in sensor networks. In Proceedings of the International Conference on Very Large
Data Bases (VLDB), pp. 588599.
Feige, U. (1998). A threshold of ln n for approximating set cover. Journal of the ACM, 45(4), 634
 652.
Feige, U., Lovasz, L., & Tetali, P. (2004). Approximating min sum set cover. Algorithmica, 40(4),
219234.
Feige, U., & Lund, C. (1997). On the hardness of computing the permanent of random matrices.
Computational Complexity, 6(2), 101132.
Fujishige, S. (2005). Submodular functions and optimization (2nd edition)., Vol. 58. Annals of
Discrete Mathematics, North Holland, Amsterdam.
Garey, M. R., & Graham, R. L. (1974). Performance bounds on the splitting algorithm for binary
testing. Acta Informatica, 3, 347355.
Gittins, J. C., & Jones, D. M. (1979). A dynamic allocation index for the discounted multiarmed
bandit problem. Biometrika, 66(3), 561565.
Goemans, M. X., & Vondrak, J. (2006). Stochastic covering and adaptivity. In Proceedings of 7th
International Latin American Symposium on Theoretical Informatics, pp. 532543.

483

fiG OLOVIN & K RAUSE

Golovin, D., Gupta, A., Kumar, A., & Tangwongsan, K. (2008). All-norms and all-Lp -norms
approximation algorithms. In Hariharan, R., Mukund, M., & Vinay, V. (Eds.), IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science
(FSTTCS 2008), Dagstuhl, Germany. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik,
Germany.
Golovin, D., & Krause, A. (2010). Adaptive submodularity: A new approach to active learning and
stochastic optimization. In 23rd Annual Conference on Learning Theory, pp. 333345.
Golovin, D., & Krause, A. (2011). Adaptive submodular optimization under matroid constraints.
CoRR, abs/1101.4450.
Golovin, D., Krause, A., Gardner, B., Converse, S. J., & Morey, S. (2011). Dynamic resource
allocation in conservation planning. In AAAI 11: Proceedings of the TwentyFifth AAAI
Conference on Artificial Intelligence, pp. 13311336. AAAI Press.
Golovin, D., Krause, A., & Ray, D. (2010). Near-optimal Bayesian active learning with noisy
observations. In NIPS: Advances in Neural Information Processing Systems 23, pp. 766774.
Goundan, P. R., & Schulz, A. S. (2007). Revisiting the greedy approach to submodular set function
maximization. Tech. rep., Massachusetts Institute of Technology.
Grunewalder, S., Audibert, J.-Y., Opper, M., & Shawe-Taylor, J. (2010). Regret bounds for Gaussian
process bandit problems. In Proceedings of the 13th International Conference on Artificial
Intelligence and Statistics.
Guha, S., & Munagala, K. (2009). Multi-armed bandits with metric switching costs. In Proceedings
of the International Colloquium on Automata, Languages and Programming (ICALP).
Guha, S., Munagala, K., & Shi, P. (2009). Approximation algorithms for restless bandit problems.
Tech. rep. 0711.3861v5, arXiv.
Guillory, A., & Bilmes, J. (2009). Average-case active learning with costs. In The 20th International
Conference on Algorithmic Learning Theory, University of Porto, Portugal.
Guillory, A., & Bilmes, J. (2010). Interactive submodular set cover. In Proceedings of the International Conference on Machine Learning (ICML), No. UWEETR-2010-0001, Haifa, Israel.
Guillory, A., & Bilmes, J. A. (2011). Simultaneous learning and covering with adversarial noise. In
International Conference on Machine Learning (ICML), Bellevue, Washington.
Gupta, A., Krishnaswamy, R., Nagarajan, V., & Ravi, R. (2010). Approximation algorithms for
optimal decision trees and adaptive TSP problems. In Proceedings of the International Colloquium on Automata, Languages and Programming (ICALP), Vol. 6198 of Lecture Notes in
Computer Science, pp. 690701. Springer.
Gupta, A., Pal, M., Ravi, R., & Sinha, A. (2005). What about Wednesday? Approximation algorithms for multistage stochastic optimization. In Proceedings of the 8th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX).
Jones, D. R., Schonlau, M., & Welch, W. J. (1998). Efficient global optimization of expensive
black-box functions. Journal of Global Optimization, 13, 455492.
Kaplan, H., Kushilevitz, E., & Mansour, Y. (2005). Learning with attribute costs. In Proceedings of
the 37th ACM Symposium on Theory of Computing, pp. 356365.
484

fiA DAPTIVE S UBMODULARITY: T HEORY AND A PPLICATIONS

Kempe, D., Kleinberg, J., & Tardos, E. (2003). Maximizing the spread of influence through a social
network. In KDD 03: Proceedings of the ninth ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 137146, New York, NY, USA. ACM.
Kosaraju, S. R., Przytycka, T. M., & Borgstrom, R. S. (1999). On an optimal split tree problem.
In Proceedings of the 6th International Workshop on Algorithms and Data Structures, pp.
157168, London, UK. Springer-Verlag.
Krause, A., & Guestrin, C. (2005). Near-optimal nonmyopic value of information in graphical
models. In Proceedings of Uncertainty in Artificial Intelligence (UAI).
Krause, A., & Guestrin, C. (2007). Near-optimal observation selection using submodular functions.
In Conference on Artificial Intelligence (AAAI) Nectar track, pp. 16501654.
Krause, A., & Guestrin, C. (2009a). Intelligent information gathering and submodular function
optimization. Tutorial at the International Joint Conference in Artificial Intelligence.
Krause, A., & Guestrin, C. (2009b). Optimal value of information in graphical models. Journal of
Artificial Intelligence Research (JAIR), 35, 557591.
Leskovec, J., Krause, A., Guestrin, C., Faloutsos, C., VanBriesen, J., & Glance, N. (2007). Costeffective outbreak detection in networks. In KDD 07: Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 420429, New York,
NY, USA. ACM.
Littman, M., Goldsmith, J., & Mundhenk, M. (1998). The computational complexity of probabilistic
planning. Journal of Artificial Intelligence Research, 9, 136.
Liu, Z., Parthasarathy, S., Ranganathan, A., & Yang, H. (2008). Near-optimal algorithms for shared
filter evaluation in data stream systems. In SIGMOD 08: Proceedings of the 2008 ACM
SIGMOD international conference on Management of data, pp. 133146, New York, NY,
USA. ACM.
Lizotte, D., Wang, T., Bowling, M., & Schuurmans, D. (2007). Automatic gait optimization with
Gaussian process regression. In Proceedings of the Twentieth International Joint Conference
on Artificial Intelligence (IJCAI), pp. 944949.
Loveland, D. W. (1985). Performance bounds for binary testing with arbitrary weights. Acta Informatica, 22(1), 101114.
McCallum, A., & Nigam, K. (1998). Employing EM and pool-based active learning for text classification. In Proceedings of the International Conference on Machine Learning (ICML), pp.
350358.
Minoux, M. (1978). Accelerated greedy algorithms for maximizing submodular set functions. In
Proceedings of the 8th IFIP Conference on Optimization Techniques, pp. 234243. Springer.
Munagala, K., Babu, S., Motwani, R., Widom, J., & Thomas, E. (2005). The pipelined set cover
problem. In Proceedings of the Intl. Conf. on Database Theory, pp. 8398.
Nemhauser, G. L., Wolsey, L. A., & Fisher, M. L. (1978). An analysis of approximations for maximizing submodular set functions - I. Mathematical Programming, 14(1), 265294.
Nowak, R. (2009). Noisy generalized binary search. In NIPS: Advances in Neural Information
Processing Systems 22, pp. 13661374.
485

fiG OLOVIN & K RAUSE

Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processses.
Mathematics of Operations Research, 12(3), 441450.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations for large
POMDPs. Journal of Artificial Intelligence Research (JAIR), 27, 335380.
Raz, R., & Safra, S. (1997). A sub-constant errorprobability lowdegree test, and a subconstant
error-probability PCP characterization of NP. In STOC 97: Proceedings of the twenty-ninth
annual ACM Symposium on Theory of Computing, pp. 475484, New York, NY, USA. ACM.
Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms for POMDPs.
Journal of Artificial Intelligence Research, 32, 663704.
Schrijver, A. (2003). Combinatorial optimization : polyhedra and efficiency. Volume B, Part IV,
Chapters 39-49. Springer.
Sebastiani, P., & Wynn, H. P. (2000). Maximum entropy sampling and optimal Bayesian experimental design. Journal of the Royal Statistical Society, Series B, 62(1), 145157.
Smallwood, R., & Sondik, E. (1973). The optimal control of partially observable Markov decision
processes over a finite horizon. Operations Research, 21, 10711088.
Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian process optimization in the
bandit setting: No regret and experimental design. In Proceedings of the International Conference on Machine Learning (ICML).
Streeter, M., & Golovin, D. (2007). An online algorithm for maximizing submodular functions.
Tech. rep. CMU-CS-07-171, Carnegie Mellon University.
Streeter, M., & Golovin, D. (2008). An online algorithm for maximizing submodular functions. In
NIPS: Advances in Neural Information Processing Systems 21, pp. 15771584.
Wolsey, L. A. (1982). An analysis of the greedy algorithm for the submodular set covering problem.
Combinatorica, 2(4), 385393.

486

fiJournal of Artificial Intelligence Research 42 (2011) 275-308

Submitted 05/11; published 11/11

Revisiting Centrality-as-Relevance:
Support Sets and Similarity as Geometric Proximity
Ricardo Ribeiro

ricardo.ribeiro@inesc-id.pt

Instituto Universitario de Lisboa (ISCTE-IUL)
Av. das Forcas Armadas, 1649-026 Lisboa, Portugal
L2F - INESC ID Lisboa
Rua Alves Redol, 9, 1000-029 Lisboa, Portugal

David Martins de Matos

david.matos@inesc-id.pt

Instituto Superior Tecnico, Universidade Tecnica de Lisboa
Av. Rovisco Pais, 1049-001 Lisboa, Portugal
L2F - INESC ID Lisboa
Rua Alves Redol, 9, 1000-029 Lisboa, Portugal

Abstract
In automatic summarization, centrality-as-relevance means that the most important
content of an information source, or a collection of information sources, corresponds to
the most central passages, considering a representation where such notion makes sense
(graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based
relevance model for automatic summarization that relies on the use of support sets to
better estimate the relevant content. Geometric proximity is used to compute semantic
relatedness. Centrality (relevance) is determined by considering the whole input source
(and not only local information), and by taking into account the existence of minor topics
or lateral subjects in the information sources to be summarized. The method consists
in creating, for each passage of the input source, a support set consisting only of the
most semantically related passages. Then, the determination of the most relevant content
is achieved by selecting the passages that occur in the largest number of support sets.
This model produces extractive summaries that are generic, and language- and domainindependent. Thorough automatic evaluation shows that the method achieves state-of-theart performance, both in written text, and automatically transcribed speech summarization,
including when compared to considerably more complex approaches.

1. Introduction
A summary conveys to the end user the most relevant content of one or more information
sources, in a concise and comprehensible manner. Several difficulties arise when addressing
this problem, but one of utmost importance is how to assess the significant content. Usually, approaches vary in complexity if processing text or speech. While in text summarization, up-to-date systems make use of complex information, such as syntactic (Vanderwende,
Suzuki, Brockett, & Nenkova, 2007), semantic (Tucker & Sparck Jones, 2005), and discourse
information (Harabagiu & Lacatusu, 2005; Uzeda, Pardo, & Nunes, 2010), either to assess
relevance or reduce the length of the output, common approaches to speech summarization
try to cope with speech-related issues by using speech-specific information (for example,
prosodic features, Maskey & Hirschberg, 2005, or recognition confidence scores, Zechner
& Waibel, 2000) or by improving the intelligibility of the output of an automatic speech
c
2011
AI Access Foundation. All rights reserved.

fiRibeiro & de Matos

recognition system (by using related information, Ribeiro & de Matos, 2008a). In fact,
spoken language summarization is often considered a much harder task than text summarization (McKeown, Hirschberg, Galley, & Maskey, 2005; Furui, 2007): problems like speech
recognition errors, disfluencies, and the accurate identification of sentence boundaries not
only increase the difficulty in determining the salient information, but also constrain the
applicability of text summarization techniques to speech summarization (although in the
presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible, Christensen, Gotoh, Kolluru, & Renals, 2003). Nonetheless, shallow
text summarization approaches such as Latent Semantic Analysis (LSA) (Landauer, Foltz,
& Laham, 1998; Gong & Liu, 2001) and Maximal Marginal Relevance (MMR) (Carbonell
& Goldstein, 1998) seem to achieve performances comparable to the ones using specific
speech-related features (Penn & Zhu, 2008).
Following the determination of the relevant content, the summary must be composed
and presented to the user. If the identified content consists of passages found in the input
source that are glued together to form the summary, that summary is usually designated
as extract; on the other hand, when the important content is devised as a series of concepts
that are fused into a smaller set and then used to generate a new, concise, and informative text, we are in the presence of an abstract. In between extraction and concept-to-text
generation, especially in text summarization, text-to-text generation methods, which rely
on text rewritingparaphrasing, of which sentence compression is a major representative, are becoming an up-to-date subject (Cohn & Lapata, 2009). Given the hardness of
abstraction, the bulk of the work in the area consists of extractive summarization.
A common family of approaches to the identification of the relevant content is the
centrality family. These methods base the detection of the most salient passages on the
identification of the central passages of the input source(s). One of the main representatives of this family is centroid-based summarization. Centroid-based methods build on the
idea of a pseudo-passage that represents the central topic of the input source (the centroid )
selecting as passages (x) to be included in the summary the ones that are close to the centroid. Pioneer work (on multi-document summarization) by Radev, Hatzivassiloglou, and
McKeown (1999) and Radev, Jing, and Budzikowska (2000) creates clusters of documents
by representing each document as a tf-idf vector; the centroid of each cluster is also defined
as a tf-idf vector, with the coordinates corresponding to the weighted average of the tf-idf
values of the documents of the cluster; finally, sentences that contain the words of the centroids are presumably the best representatives of the topic of the cluster, thus being the
best candidates to belonging to the summary.
centrality(x) = similarity(x, centroid)

(1)

Another approach to centrality estimation is to compare each candidate passage to every
other passage (y) and select the ones with higher scores (the ones that are closer to every
other passage). One simple way to do this is to represent passages as vectors using a
weighting scheme like the aforementioned tf-idf ; then, passage similarity can be assessed
using, for instance, the cosine, assigning to each passage a centrality score as defined in
Eq. 2.
1 X
centrality(x) =
similarity(x, y)
(2)
N y
276

fiRevisiting Centrality-as-Relevance

These scores are then used to create a sentence ranking: sentences with highest scores are
selected to create the summary.
A major problem of this relevance paradigm is that by taking into account the entire
input source in this manner, either to estimate centroids or average distances of input source
passages, we may be selecting extracts that being central to the input source are, however,
not the most relevant ones. In cognitive terms, the information reduction techniques in
the summarization process are quite close to the discourse understanding process (EndresNiggemeyer, 1998), which, at a certain level, works by applying rules that help uncovering
the macrostructure of the discourse. One of these rules, deletion, is used to eliminate
from the understanding process propositions that are not relevant to the interpretation
of the subsequent ones. This means that it is common to find, in the input sources to
be summarized, lateral issues or considerations that are not relevant to devise the salient
information (discourse structure-based summarization is based on the relevance of nuclear
text segments, Marcu, 2000; Uzeda et al., 2010), and that may affect centrality-based
summarization methods by inducing inadequate centroids or decreasing the scores of more
suitable sentences.
As argued by previous work (Gong & Liu, 2001; Steyvers & Griffiths, 2007), we also
assume that input sources are mixtures of topics, and propose to address that aspect using
the input source itself as guidance. By associating to each passage of the input source a
support set consisting only of the most semantically related passages in the same input
source, groups of related passages are uncovered, each one constituting a latent topic (the
union of the supports sets whose intersection is not empty). In the creation of these support
sets, semantic relatedness is assessed by geometric proximity. Moreover, while similar work
usually explores different weighting schemes to address specific issues of the task under
research (Orasan, Pekar, & Hasler, 2004; Murray & Renals, 2007; Ribeiro & de Matos,
2008b), we explore different geometric distances as similarity measures, analyzing their
performance in context (the impact of different metrics from both theoretical and empirical
perspectives in a clustering setting was shown in Aggarwal, Hinneburg, & Keim, 2001). To
build the summary, we select the sentences that occur in the largest number of support sets
hence, the most central sentences, without the problem that affects previous centrality-based
summarization.
Our method produces generic, language- and domain-independent summaries, with low
computational requirements. We test our approach both in speech and text data. In the empirical evaluation of the model over text data, we used an experimental setup previously used
in published work (Mihalcea & Tarau, 2005; Antiqueira, Oliveira Jr., da Fontoura Costa, &
Nunes, 2009), which enabled an informative comparison to the existing approaches. In what
concerns the speech experiments, we also used a corpus collected in previous work (Ribeiro
& de Matos, 2008a), as well as the published results. This allowed us to compare our model
to state-of-the-art work.
The rest of this document is structured as follows: in Section 2, we analyze representative models of both centrality-as-relevance approachespassage-to-centroid similarity-based
centrality and pair-wise passage similarity-based centrality; Section 3 describes the support
sets-based relevance model; the evaluation of the model is presented in Section 4, where
we compare its performance against other centrality-as-relevance models and discuss the
achieved results; final remarks conclude the document.
277

fiRibeiro & de Matos

2. Centrality-as-Relevance
There are two main approaches to centrality-based summarization: passage-to-centroid
similarity and pair-wise passage similarity.
2.1 Passage-to-Centroid Similarity-based Centrality
In centroid-based summarization, passage centrality is defined by the similarity between the
passage and a pseudo-passage that, considering a geometrical representation of the input
source, is the center of the space defined by the passages of the input source, the centroid.
The work in multi-document summarization by Radev et al. (1999, 2000) and Radev, Jing,
Stys, and Tam (2004) and the work developed by Lin and Hovy (2000) are examples of this
approach.
Radev et al. present a centroid-based multi-document summarizer (MEAD) that has
as input a cluster of documents. Associated to each cluster of documents is a centroid.
Documents are represented by vectors of tf-idf weights and the centroid of each cluster
consists of a vector which coordinates are the weighted averages of the tf-idf values of the
documents of the cluster, above a pre-defined threshold. Thus, the centroid of a cluster of
documents is, in this case, a pseudo-document composed by the terms that are statistically
relevant. Given a cluster of documents segmented into sentences IS , {s1 , s2 , . . . , sN },
a centroid C, and a compression rate, summarization is done by selecting the appropriate
number (according to the compression rate) of the sentences with the highest scores assigned
by a linear function (Eq. 3) of the following features: centroid value (Ci ), position value
(Pi ), and first-sentence overlap value (Fi ).
score(si ) = wc Ci + wp Pi + wf Fi ,

1iN

(3)

P
The centroid value, defined as Ci = tsi Ct,i , establishes that the sentences closer to the
centroid (the ones that contain more terms t from the centroid) have higher scores. Position value (Pi ) scores sentences according to their position in the encompassing document.
Finally, first-sentence overlap value (Fi ) scores sentences according to their similarity to the
first sentence of the document.
Lin and Hovy (2000) designate the centroid as topic signature and define it as a set of
related terms: T S , {topic, < (t1 , w1 ), . . . , (tT , wT ) >}, where ti represents a term related
to the topic topic and wi is an associated weight that represents the degree of correlation
of ti to the topic. Topic signatures are computed from a corpus of documents, previously
classified as relevant or non-relevant for a given topic, using the log-likelihood-ratio-based
quantity 2log(). This quantity, due to its asymptotic relation to the 2 distribution as
well as the adequacy of the log-likelihood-ratio to sparse data, is used to rank the terms
that will define the signature, and to select a cut-off value that will establish the number
of terms in the signature. Summarization is carried out by ranking the sentences according
to the topic signature score and selecting the top ranked ones. The topic signature score
(tss) is computed in a similar manner to MEADs centroid value: given an input source
IS , {p1 , p2 , . . . , pN }, where pi , ht1 , . . . , tM i, the most relevant passages are the ones with
more words from the topic (Eq. 4).
278

fiRevisiting Centrality-as-Relevance

tss(pi ) =

M
X

wj , with wj weight of tj as defined in a topic signature

(4)

j=1

2.2 Pair-wise Passage Similarity-based Centrality
In pair-wise passage similarity-based summarization, passage centrality is defined by the
similarity between each passage and every other passage. The work presented by Erkan and
Radev (2004), as well as the work developed by Mihalcea and Tarau (2005), are examples
of this approach.
Erkan and Radev (2004) propose three graph-based approaches to pair-wise passage
similarity-based summarization with similar performance: degree centrality, LexRank, and
continuous LexRank. Degree centrality is based on the degree of a vertex. Pair-wise sentence similarity is used to build a graph representation of the input source: vertices are
sentences and edges connect vertices which corresponding sentences are similar above a
given threshold. Sentences similar to a large number of other sentences are considered the
most central (relevant) ones. Degree centrality is similar to the model we propose. However, in the model we propose, we introduce the concept of support set to allow the use of
a different threshold for each sentence. This improves the representation of each sentence,
leading to the creation of better summaries.
LexRank, based on Googles PageRank (Brin & Page, 1998), builds on degree centrality
(degree) by making the centrality of a sentence s be influenced by similar sentences, the
adjacent ones in the graph representation (Eq. 5).
X centralityScore(t)
degree(t)

centralityScore(s) =

(5)

tadj[s]

The ranking model is similar to PageRank except in what concerns the similarity (adjacency)
graph, that, in this case, is undirected (Eq. 6, d is a damping factor and N the number of
sentences).
X centrality(t)
d
centrality(s) =
+ (1  d)
(6)
N
degree(t)
tadj[s]

Continuous LexRank is a weighted version of LexRank (it uses Eq. 7 instead of Eq. 5).
centralityScore(s) =

X

sim(s, t)
centralityScore(t)
uadj[t] sim(u, t)

P
tadj[s]

(7)

Mihalcea and Tarau (2005), in addition to Googles PageRank, also explore the HITS
algorithm (Kleinberg, 1999) to perform graph-based extractive text summarization: again,
documents are represented as networks of sentences and these networks are used to globally
determine the importance of each sentence. As it happens in the models proposed by
Erkan and Radev, sentences are vertices (V ) and edges (w) between vertices are established
by passage similarity. The TextRank (Mihalcea & Tarau, 2004)how the model based on
PageRank was designated and the main contributionformalization is similar to Continuous
LexRank (see Eq. 8), although Mihalcea and Tarau also explore directed graphs in the
279

fiRibeiro & de Matos

representation of the text12 . For summarization, the best results were obtained using a
backward directed graph: the orientation of the edges from a vertex representing a sentence
is to vertices representing previous sentences in the input source.
X
w(Vt , Vs )
P
T extRank(Vs ) = (1  d) + d 
T extRank(Vt )
(8)
Vu Out[Vt ] w(Vt , Vu )
Vt In[Vs ]

Passage similarity is based on content overlap3 and is defined in Eq. 9. Given two sets
P , p1 , p2 , ..., pn and Q , q1 , q2 , ..., qn , each corresponding to a passage, similarity consists
in the cardinality of the intersection over the sum of the logarithms of the cardinality of
each set.
|{t : t  P  t  Q}|
w(VP , VQ ) = sim(P, Q) =
(9)
log(|P |) + log(|Q|)
A similar graph-based approach is described by Antiqueira et al. (2009). This work
uses complex networks to perform extractive text summarization. Documents are also
represented as networks, where the sentences are the nodes and the connections between
nodes are established between sentences sharing common meaningful nouns.
2.3 Beyond Automatic Summarization
Apart from summarization, and considering that PageRank and HITS stem from the area
of Information Retrieval, centrality-based methods similar to the ones previously described
have been successfully applied to re-rank sets of documents returned by retrieval methods.
Kurland and Lee (2005, 2010) present a set of graph-based algorithms, named influx,
that are similar to our model, to reorder a previously retrieved collection of documents (C).
The method starts by defining a k -nearest-neighbor (k NN) graph over the initial collection
based on generation links defined as in Eq. 10 (KL, Kullback-Leibler divergence; M LE,
maximum-likelihood estimate; , smoothing-parameter of a Dirichlet-smoothed version of
p(); d and s, documents).
fifi



fifi []
M LE
(s)
,
exp

KL
p
()
p
()
(10)
pKL,
fi
fi
s
d
d
Centrality is determined as defined in Eq. 11. Edges can be weighted (weight given by
pKL,
(s)) or not (weight is 1). Edges corresponding to generation probabilities below the k
d
highest ones are not considered.
X
centralityScore(d) ,
wt(o  d)
(11)
oC

1. In A Language Independent Algorithm for Single and Multiple Document Summarization (Mihalcea
& Tarau, 2005), the weighted PageRank equation has a minor difference from the one in TextRank:
Bringing Order into Texts (Mihalcea & Tarau, 2004). The latter presents the correct equation.
2. Although both LexRank and TextRank are based on PageRank, different equations are used in their
formalization. The equation used in TextRank formalization is the same of PageRank original publication, however PageRank authors observe that the PageRanks form a probability distribution over Web
pages, so the sum of all Web pages PageRanks will be one. This indicates the need of the normalization
factor that is observed in LexRank formalization and currently assumed to be the correct PageRank
formalization.
3. The metric proposed by Mihalcea and Tarau (2004) has an unresolved issue: the denominator is 0 when
comparing two equal sentences with length one (something that can happen when processing speech
transcriptions). Instead, the Jaccard similarity coefficient (1901) could be used.

280

fiRevisiting Centrality-as-Relevance

There are also recursive versions of this centrality model, which are similar to PageRank/LexRank and Continuous LexRank.

3. Support Sets and Geometric Proximity
In this work, we hypothesize that input sources to be summarized comprehend different
topics (lateral issues beyond the main topic), and model this idea by defining a support set,
based on semantic relatedness, for every passage in the input source. Semantic relatedness
is estimated within the geometric framework, where we explore several distance metrics
to compute proximity. The most relevant content is determined by computing the most
central passages given the collection of support sets. The proposed model estimates the
most salient passages of an input source, based exclusively on information drawn from the
used input source.
3.1 Model
The leading concept in our model is the concept of support set: the first step of our method
to assess the relevant content is to create a support set for each passage of the input source
by computing the similarity between each passage and the remaining ones, selecting the
closest passages to belong to the support set. The most relevant passages are the ones that
occur in the largest number of support sets.
Given a segmented information source I , p1 , p2 , ..., pN , support sets Si associated with
each passage pi are defined as indicated in Eq. 12 (sim() is a similarity function, and i is
a threshold).
Si , {s  I : sim(s, pi ) > i  s 6= pi }
(12)
The most relevant segments are given by selecting the passages that satisfy Eq. 13.
fi
fi
arg max fi{Si : s  Si }fi

(13)

sn
i=1 Si

A major difference from previous centrality models and the main reason to introduce
the support sets is that by allowing different thresholds to each set (i ), we let centrality
be influenced by the latent topics that emerge from the groups of related passages. In the
degenerate case where all i are equal, we fall into the degree centrality model proposed
by Erkan and Radev (2004). But using, for instance, a nave approach of having dynamic
thresholds (i ) set by limiting the cardinality of the support sets (a k NN approach), centrality is changed because each support set has only the most semantically related passages of
each passage. From a graph theory perspective, this means that the underlying representation is not undirected, and the support set can be interpreted as the passages recommended
by the passage associated to the support set. This contrasts with both LexRank models,
which are based on undirected graphs. On the other hand, the models proposed by Mihalcea and Tarau (2005) are closer to our work in the sense that they explore directed
graphs, although only in a simple way (graphs can only be directed forward or backward).
Nonetheless, semantic relatedness (content overlap) and centrality assessment (performed
by the graph ranking algorithms HITS and PageRank) is quite different from our proposal.
In what concerns the work of Kurland and Lee (2005, 2010), which considering this k NN
281

fiRibeiro & de Matos

approach to the definition of the support set size, is the most similar to our ideias, although
not addressing automatic summarization, the neighborhood definition strategy is different than ours: Kurland and Lee base neighborhood definition on generation probabilities
(Eq. 10), while we explore geometric proximity. Nevertheless, from the perspective of our
model, the k NN approach to support set definition is only a possible strategy (others can be
used): our model can be seen as a generalization of both k NN and NN approaches, since
what we propose is the use of differentiated thresholds (i ) for each support set (Eq. 12).
3.2 Semantic Space
We represent the input source I in a term by passages matrix A, where each matrix element
aij = f (ti , pj ) is a function that relates the occurrences of each term ti within each passage
pj (T is the number of different terms; N is the number of passages).



a1,1 . . . a1,N

A =  ...
aT,1 . . . aT,N

(14)

In what concerns the definition of the weighting function f (ti , pj ), several term weighting
schemes have been explored in the literaturefor the analysis of the impact of different
weighting schemes on either text or speech summarization see the work of Orasan et al.
(2004), and Murray and Renals (2007) or Ribeiro and de Matos (2008b), respectively. Since
the exact nature of the weighting function, although relevant, is not central to our work, we
opted for normalized frequency for simplicity, as defined in Eq. 15, where ni,j is the number
of occurrences of term ti in passage pj .
ni,j
f (ti , pj ) = tfi = P
k nk,j

(15)

Nevertheless, this is in line with the work of Sahlgren (2006) that shows that in several tasks
concerning term semantic relatedness, one of the most effective weighting schemes for small
contexts is the binary term weighting scheme (Eq. 16), alongside raw or dampened counts,
that is, weighting schemes, based on the frequency, that do not use global weights (note
also that in such small contexts, most of the words have frequency 1, which normalized or
not is similar to the binary weighting scheme).
(
1 if ti  pj
f (ti , pj ) =
(16)
0 if ti 
/ pj
3.3 Semantic Relatedness
As indicated by Sahlgren (2006), the meanings-are-locations metaphor is completely vacuous without the similarity-is-proximity metaphor. In that sense, we explore the prevalent
distance measures found in the literature, based on the general Minkowski distance (Eq. 17).
distminkowski (x, y) =

n
X
i=1

282

|xi  yi |N

1

N

(17)

fiRevisiting Centrality-as-Relevance

Semantic relatedness is computed using the Manhattan distance (N = 1, Eq. 19), the
Euclidean distance (N = 2, Eq. 20), the Chebyshev distance (N  , Eq. 21), and
fractional distance metrics (we experimented with N = 0.1, N = 0.5, N = 0.75, and
N = 1.(3). Note that, when 0 < N < 1, Eq. 17 does not represent a metric, since the
triangle inequality does not hold (Koosis, 1998, page 70). In this case, it is common to use
the variation defined in Eq. 18.
distN (x, y) =

n
X

|xi  yi |N , 0 < N < 1

(18)

i=1

Moreover, we also experiment with the general Minkowski equation, using the tuple dimension as N .
n
X
distmanhattan (x, y) =
|xi  yi |
(19)
i=1

v
u n
uX
(xi  yi )2
disteuclidean (x, y) = t

(20)

i=1

distchebyshev (x, y) = lim

N 

n
X

|xi  yi |N

i=1

1

N

= max (|xi  yi |)
i

(21)

The cosine similarity (Eq. 22), since it is one of the most used similarity metrics, especially
when using spatial metaphors for computing semantic relatedness, was also part of our
experiments.
Pn
x i yi
xy
simcos (x, y) =
(22)
= qP i=1qP
n
n
kxkkyk
2
2
x
y
i=1 i
i=1 i
Grounding semantic relatedness on geometric proximity enables a solid analysis of the
various similarity metrics. For instance, when using the Euclidean distance (Eq. 20), differences between tuple coordinate values less than 1 make passages closer, while values
greater than 1 make passages more distant; Chebyshevs distance (Eq. 21) only takes into
account one coordinate: the one with the greatest difference between the two passages; and,
the Manhattan distance (Eq. 19) considers all coordinates evenly. In the cosine similarity
(Eq. 22), tuples representing passages are vectors and the angle they form establishes their
relatedness. In contrast, Mihalcea and Tarau (2005) and Antiqueira et al. (2009) define
passage similarity as content overlap. Figure 1 (N ranges from 0.1, with an almost imperceptible graphical representation, to N  , a square) shows how the unit circle is
affected by the several geometric distances (Manhattan, N = 1, and Euclidean, N = 2, are
highlighted).
Although geometric proximity enables a solid analysis of the effects of using a specific
metric, it mainly relies on lexical overlap. Other metrics could be used, although the costs
in terms of the required resources would increase. Examples are corpus-based vector space
models of semantics (Turney & Pantel, 2010), like LSA (Landauer et al., 1998), Hyperspace
Analogue to Language (Lund, Burgess, & Atchley, 1995), or Random Indexing (Kanerva,
Kristoferson, & Holst, 2000; Kanerva & Sahlgren, 2001), or similarity metrics based on
knowledge-rich semantic resources, such as WordNet (Fellbaum, 1998).
283

fiRibeiro & de Matos

1

0.75

0.5

0.25

-1

-0.75

-0.5

-0.25

0

0.25

0.5

0.75

1

-0.25

-0.5

-0.75

-1

Figure 1: Unit circles using various fractional distance metrics (N equals to 0.1, 0.5, 0.75,
and 1.(3)), the Manhattan distance (N = 1), the Euclidean distance (N = 2),
and the Chebyshevs distance (N  ).

3.4 Threshold Estimation
As previously mentioned, a simple approach to threshold estimation is to define a fixed
cardinality for all support sets, a k NN approach. This means that thresholds, although
unknown, are different for each support set.
A simple heuristic that allows to automatically set per passage thresholds is to select
as members of the support set the passages which distance to the passage associated to the
support set under construction is smaller than the average distance. In the next sections,
we explore several heuristics inspired by the nature of the problem that can be used as
possibly better approaches to threshold estimation. However, this subject merits further
study.
3.4.1 Heuristics Based on Distance Progression Analysis
One possible approach is to analyze the progression of the distance values between each
passage and the remaining ones in the creation of the respective support set. This type
of heuristics uses a sorted permutation, di1  di2      diN 1 , of the distances of the
passages, sk , to the passage pi (corresponding to the support set under construction), with
dik = dist(sk , pi ), 1  k  N  1, and N the number of passages.
284

fiRevisiting Centrality-as-Relevance

We explore three approaches: a standard deviation-based approach, where i is given by
Eq. 23, with  a parameter that controls the width of interval around the average distance in
relation to the standard deviation; an approach based on the diminishing differences between
consecutive distances, dik+2  dik+1 < dik+1  dik , 1  k  N  3, where i = dik+2 , such that
k is the largest one that 1jk+1 j : dij+2  dij+1 < dij+1  dij ; and, an approach based on the
average difference between consecutive distances,

dik+1 dik

PN 2

<

l=1

(dil+1 dil )
,1
N 2
P

where i = dik+1 , such that k is the largest one that 1jk j : dij+1  dij <
i = i  i , with
v
u
N
1
1
u 1 NX
1 X i
i =
dk , and i = t
(dik  i )2
N 1
N 1
k=1

 k  N 2,

N 2 i
i
l=1 (dl+1 dl )

N 2

.

(23)

k=1

3.4.2 Heuristics Based on Passage Order
The estimation of specific thresholds aims at defining support sets containing the most
important passages to the passage under analysis. In that sense, in this set of heuristics we
explore the structure of the input source to partition the candidate passages to be in the
support set in two subsets: the ones closer to the passage associated with the support set
under construction, and the ones further appart.
These heuristics use a permutation, di1 , di2 ,    , diN 1 , of the distances of the passages, sk ,
to the passage, pi , related to the support set under construction, with dik = dist(sk , pi ), 1 
k  N  1, corresponding to the order of occurrence of passages sk in the input source.
Algorithm 1 describes the generic procedure.
3.4.3 Heuristics Based on Weighted Graph Creation Techniques
There are several ways to define a weighted graph, given a dataset. The main ideia is that
similar nodes must be connected by an edge with a large weight. In this set of heuristics, we
explore two weight functions (Zhu, 2005) (Eqs. 24 and 25) considering that if the returned
value is above a given threshold, , the passage sk belongs to the support set of passage pi ,
with dik = dist(sk , pi ).
exp((dik 

tanh((dik 

min (dij ))2 /2 ) > 

1jN 1

N 1

1 X i
dj )) + 1 /2 > 
N 1

(24)

(25)

j=1

3.5 Integrating Additional Information
As argued by Wan, Yang, and Xiao (2007) and Ribeiro and de Matos (2008a), the use of
additional related information helps to build a better understanding of a given subject, thus
improving summarization performance. Wan et al. propose a graph-based ranking model
that uses several documents about a given topic to summarize a single one of them. Ribeiro
and de Matos, using the LSA framework, present a method that combines the input source
285

fiRibeiro & de Matos

Input: Two values r1 and r2 , each a representative of a subset, and the set of the passages
sk and corresponding distances dik to the passage associated with the support set
under construction
Output: The support set of the passage under analysis
R1  , R2  ;
for k  1 to N  1 do
if |r1  dik | < |r2  dik | then
r1  (r1 + dik )/2;
R1  R1  {sik };
else
r2  (r2 + dik )/2;
R2  R2  {sik };
end
end
l  arg min1kN 1 (dik );
if sl  R1 then
return R1 ;
else
return R2 ;
end

Algorithm 1: Generic passage order-based heuristic.

consisting of a spoken document, with related textual background information, to cope with
the difficulties of speech-to-text summarization.
The model we propose may be easily expanded to integrate additional information. By
using both an information source I , p1 , p2 , ..., pN and a source for additional relevant
information B, we may redefine Eq. 12 as shown in Eq. 26 to integrate the additional
information.
Si , {s  I  B : sim(s, pi ) > i  s 6= pi }

(26)

Matrix A (from Eq. 14) should be redefined as indicated in Eq. 27, where aidk represents
j
the weight of term ti , 1  i  T (T is the number terms), in passage pdk , 1  k  D (D is
j

the number of documents used as additional information) with dk1  dkj  dks , of document
dk ; and ainl , 1  l  s, are the elements associated with the input source to be summarized.


a1d1 ... a1d1s
 1
...
A=
aT d11 ... aT d1s

...

a1dD ... a1dD
s
1

... aT dD ... aT dD
s
1

a1n1 ... a1ns
aT n1 ... aT ns





(27)

Given the new definition of support set and a common representation for the additional
information, the most relevant content is still assessed using Eq. 13.
The same line of thought can be applied to extend the model to multi-document summarization.
286

fiRevisiting Centrality-as-Relevance

4. Evaluation
Summary evaluation is a research subject by itself. Several evaluation models have been put
forward in the last decade: beyond the long-established precision and recall (mostly useful
when evaluating extractive summarization using also extractive summaries as models), literature is filled with metrics (some are automatic, others manual) like Relative utility (Radev
et al., 2000; Radev & Tam, 2003), SummACCY (Hori, Hori, & Furui, 2003), ROUGE (Lin,
2004), VERT (de Oliveira, Torrens, Cidral, Schossland, & Bittencourt, 2008), or the Pyramid method (Nenkova, Passonneau, & McKeown, 2007). For a more comprehensive analysis
of the evaluation field see the work by Nenkova (2006) and Nenkova et al. (2007).
Despite the number of approaches to summary evaluation, the most widely used metric
is still ROUGE and is the one we use in our study. We chose ROUGE not only owing to
its wide adoption, but also because one of the data sets used in our evaluation has been
used in published studies, allowing us to easily compare the performance of our model with
other known systems.
P
P
S{Reference Summaries}
gramN S countmatch (gramN )
P
ROUGE-N = P
(28)
S{Reference Summaries}
gramN S count(gramN )
Namely, we use the ROUGE-1 score, known to correlate well with human judgment (Lin,
2004). ROUGE-N is defined in Eq. 28. Moreover, we estimate confidence intervals using
non-parametric bootstrap with 1000 resamplings (Mooney & Duval, 1993).
Since we are proposing a generic summarization model, we conducted experiments both
in text and speech data.
4.1 Experiment 1: Text
In this section, we describe the experiments performed and analyze the corresponding results
when using as input source written text.
4.1.1 Data
The used corpus, known as TeMario, consists of 100 newspaper articles in Brazilian Portuguese (Pardo & Rino, 2003). Although our model is general and language-independent,
this corpus was used in several published studies, allowing us to perform an informed comparison of our results. The articles in the corpus cover several domains, such as world,
politics, and foreign affairs. For each of the 100 newspaper articles, there is a reference
human-produced summary. The text was tokenized and punctuation removed, maintaining
sentence boundary information. Table 1 sumarizes the properties of this data set.
4.1.2 Evaluation Setup
To compare the performance of our model when the input is not affected by speech-related
phenomena, we use previously published state-of-the-art results for text summarization.
However, since there was no information available about any kind of preprocessing for the
previous studies, we could not guarantee a fair comparison of our results with the previous
ones, without the definition of an adequate methodology for the comparisons.
The following systems were evaluated using the TeMario dataset:
287

fiRibeiro & de Matos

#Words

#Sentences

Average

Minimum

Maximum

News Story (NS)
NS Sentence
Summary (S)
S Sentence

608
21
192
21

421
1
120
1

1315
100
345
87

News Story
Summary

29
9

12
5

68
18

Table 1: Corpus characterization.
 a set of graph-based summarizers presented by Mihalcea and Tarau (2005), namely
PageRank Backward, HITSA Backward and HITSH Forward;
 SuPor-v2 (Leite, Rino, Pardo, & Nunes, 2007), a classifier-based system that uses
features like the occurrence of proper nouns, lexical chaining, and an ontology;
 two modified versions of Mihalceas PageRank Undirected, called TextRank + Thesaurus and TextRank + Stem + StopwordsRem(oval) presented by Leite et al. (2007);
and,
 several complex networks summarizers proposed by Antiqueira et al. (2009).
Considering the preprocessing step we applied to the corpus and the observed differences
in the published results, we found it important to evaluate the systems under the same
conditions. Thus, we implemented the following centrality models:
 Uniform Influx (corresponds to the non-recursive, unweighted version of the model),
proposed by Kurland and Lee (2005, 2010) for re-ranking in document retrieval (we
experimented with several k in graph definition, the sames used for support set cardinality in the kNN strategy, and 10, 20, 50, 100, 200, 500, 1000, 2000, 5000,
10000, and present only the best results);
 PageRank, proposed by both Mihalcea and Tarau (2004, 2005) and Erkan and Radev
(2004) (passage similarity metrics differ and Mihalcea and Tarau also explore directed
graphs);
 Degree centrality as proposed by Erkan and Radev (2004) (we experimented with
several thresholds , ranging from 0.01 to 0.09, and show only the best results); and,
 Baseline, in which the ranking is defined by the order of the sentences in the news
article, with relevance decreasing from the begining to the end.
Table 2 further discriminates PageRank-based models. PageRank over a directed forward
graph performs consistently worse (Mihalcea & Tarau, 2005) than over undirected and
directed backward graphs, and it was not included in our trials. Degree and Continuous
LexRank bound the performance of the LexRank model, and are the ones we use in this
288

fiRevisiting Centrality-as-Relevance

Proposed model

Generic designation

Similarity metric

Continuous LexRank
TextRank Undirected
TextRank Backward

PageRank Undirected
PageRank Undirected
PageRank Backward

Cosine
Content overlap
Content overlap

Table 2: Models based on PageRank.
evaluation. Moreover, to assess the influence of the similarity metrics in these graphbased centrality models, we tested the best-performing metric of our model, the Manhattan
distance, with the PageRank model. Additionally, given that the models proposed by Erkan
and Radev (2004) use idf, we present some results (clearly identified) using both weighting
schemes: using and not using idf.
Concerning summary size, the number of words in the generated summaries directly
depends on the number of words of the reference abstracts, which consisted in compressing
the input sources to 25-30% of the original size.
4.1.3 Results
Table 3 illustrates the comparison between the previously proposed models and our model.
In this table, our model is identified in boldface by the distance name, and the conditions
used by that particular instance. Every time the best performance is achieved by an instance
using supports sets whose cardinality is specified in absolute terms (15), we also present
the best performance using support sets whose cardinality is specified in relative terms
(10%90% of the input source). For the fractional metrics, we also present the value of N
in Eq. 17, if N  1, or Eq. 18, if 0 < N < 1. For the automatically set thresholds, we
identify which heuristic produced the best results using the following notation: H0 means
the heuristic based on the average distance; H1 means heuristics based on the analysis
of the distances progression, with H1.1 corresponding to the one based on the standard
deviation, H1.2 corresponding to the one based on the diminishing differences between
consecutive distances, and H1.3 corresponding to the one based on the average difference
between consecutive distances; H2 means heuristics based on passage order, with H2.1 using
as r1 the minimum distance, and as r2 the average of the distances, H2.2 using as r1 the
minimum distance, and as r2 the maximum distance, and H2.3, using as r1 the distance
of the first passage and r2 the distance of the second passage, according to the required
permutation defined in Section 3.4.2; H3 means heuristics based on weighted graph creation
techniques, with H3.1 based on Eq. 24, and H3.2 based on Eq. 25.
The best overall results were obtained by the support sets-based centrality model using
both the Fractional, with N = 1.(3) and using idf, and the Manhattan distance. The next
best-performing variants of our model were Cosine, Minkowski (N defined by the dimension
of the semantic space), and Euclidean, all over-performing both TextRank Undirected and
the Uniform Influx model. The best PageRank variant, using a backward directed graph
and the cosine similarity with idf, achieved a performance similar to the Cosine (SSC = 4,
idf ) and the Minkowski (SSC = 2) variants of our model. TextRank Undirected, Uniform
Influx, and Continuous LexRank (idf ) obtained performances similar to the Euclidean (SSC
289

fiRibeiro & de Matos

Systems

ROUGE-1

Confidence Interval

Fractional (N = 1.(3), idf, H1.3)
Manhattan (SSC = 2)
Manhattan (10%)
Manhattan (idf, H2.1)
Cosine (idf, SSC = 4)
PageRank Backward Cosine (idf )
Minkowski (SSC = 2)
Minkowski (H2.1)
Cosine (idf, H0)
Manhattan (H1.2)
Euclidean (idf, SSC = 5)
TextRank Undirected
Uniform Influx (10%NN,  = 10000)
Cosine (90%)
Continuous LexRank (idf )
Fractional (N = 1.(3), H1.3)
Fractional (N = 1.(3), SSC = 1)
PageRank Backward Cosine
Degree ( = 0.02, idf )
TextRank Backward
Minkowski (10%)
Euclidean (H2.3)
Cosine (H1.3)
Fractional (N = 1.(3), 80%)
Chebyshev (H1.2)
PageRank Backward Manhattan
Euclidean (10%)
Chebyshev (SSC = 2)
Chebyshev (10%)
Continuous LexRank
PageRank Undirected Manhattan

0.442
0.442
0.440
0.439
0.439
0.439
0.439
0.437
0.437
0.437
0.436
0.436
0.436
0.436
0.436
0.435
0.435
0.435
0.435
0.434
0.434
0.434
0.432
0.432
0.432
0.432
0.431
0.429
0.429
0.428
0.428

[0.430,
[0.430,
[0.429,
[0.428,
[0.428,
[0.427,
[0.427,
[0.426,
[0.425,
[0.425,
[0.424,
[0.424,
[0.422,
[0.423,
[0.424,
[0.422,
[0.423,
[0.423,
[0.423,
[0.423,
[0.422,
[0.422,
[0.420,
[0.420,
[0.419,
[0.419,
[0.418,
[0.417,
[0.417,
[0.415,
[0.415,

0.455]
0.454]
0.453]
0.451]
0.451]
0.451]
0.452]
0.450]
0.449]
0.450]
0.448]
0.448]
0.449]
0.448]
0.448]
0.447]
0.448]
0.447]
0.447]
0.446]
0.447]
0.448]
0.444]
0.445]
0.444]
0.442]
0.444]
0.442]
0.442]
0.441]
0.440]

Baseline

0.427

[0.415, 0.440]

Fractional (N = 0.1, H1.1)
Degree ( = 0.06)
Fractional (N = 0.5, H1.1)
Fractional (N = 0.75, H1.1)
Fractional (N = 0.75, 10%)
Fractional (N = 0.1, 90%)
Fractional (N = 0.5, 90%)

0.427
0.426
0.422
0.421
0.417
0.417
0.413

[0.414,
[0.414,
[0.409,
[0.410,
[0.404,
[0.405,
[0.403,

0.439]
0.439]
0.434]
0.433]
0.429]
0.429]
0.425]

Table 3: ROUGE-1 scores for the text experiment (SSC stands for Support Set Cardinality).

290

fiRevisiting Centrality-as-Relevance

= 5, idf ) and the Cosine (90%) variants. Notice that although not exhaustively analyzing
the effects of term weighting, the use of idf clearly benefits some metrics: see, for instance,
the Cosine and Fractional N = 1.(3) variants of our model, the PageRank variants based
on the cosine similarity, and Degree. It is relevant to note that our model, which has low
computational requirements, achieves results comparable to graph-based state-of-the-art
systems (Ceylan, Mihalcea, Ozertem, Lloret, & Palomar, 2010; Antiqueira et al., 2009).
Notice that although the estimated confidence intervals overlap, the performance of the
Manhattan SCC=2 variant is significantly better, using the directional Wilcoxon signed
rank test with continuity correction, than the ones of TextRank Undirected, (W = 2584,
p < 0.05), Uniform Influx (W = 2740, p < 0.05), and also Continuous LexRank (W =
2381.5, p < 0.1).4 The only variants of our model that perform below the baseline are the
Fractional variants with N < 1. Fractional distances with N < 1, as can be seen by the
effect of the metric on the unit circle (Figure 1), increase the distance between all passages,
negatively influencing the construction of the support sets and, consequently the estimation
of relevant content.
Concerning the automatically set per passage thresholds, it is possible to observe that
the best overall performance was achieved by a metric, Fractional N = 1.(3), with idf, using
the heuristic based on the average difference between consecutive distances. For Cosine,
Manhattan, Euclidean, and Minkowski variants, the heuristic based on the average distance
(Cosine) and the heuristics based on passage order achieved results comparable to the best
performing kNN approaches. For Chebyshev and Fractional (with N < 1) variants the best
results were obtained using the heuristics based on the analysis of the progression of the
distances.
Figure 2 shows the improvements over the baseline and over the previous best-performing
system. It is possible to perceive that the greatest performance jumps are introduced by
Euclidean (10%) and Euclidean (H2.3), Minkowski (SSC=2), and the best-performing Manhattan, all instances of the support sets-based relevance model. Additionally, it is important
to notice that the improvement of CN-Voting over the baseline (computed in the same conditions of CN-Voting) is of only 1%, having a performance worse than the poorest TextRank
version which had an improvement over the baseline of 1.6%. In what concerns the linguistic knowledge-based systems (SuPor-2 and the enriched versions of TextRank Undirected),
we cannot make an informed assessment of their performance since we cannot substantiate
the used baseline, taken from the work of Mihalcea and Tarau (2005). Nonetheless, using
that baseline, it is clear that linguistic information improves the performance of extractive
summarizers beyond what we achieved with our model: improvements over the baseline
range from 9% to 17.5%. Notice however, that it would be possible to enrich our model
with linguistic information, in the same manner of TextRank.
Regarding the effect of the similarity metric on the PageRank-based systems, it is possible to observe that PageRank Undirected based on Content Overlap (TextRank Undirected)
has a better performance than when similarity is based on a geometric metriceither Manhattan or Cosine (Continuous LexRank). However, the same does not happen when considering the results obtained by the several variants of PageRank Backward. Although the use
of Content Overlap, in fact, leads to a better performance than using a Manhattan-based
4. Statistical tests were computed using R (R Development Core Team, 2009).

291

fiRibeiro & de Matos

PageRank Undirected Manhattan
Continuous LexRank
Chebyshev (10%)
Chebyshev (SSC = 2)
Euclidean (10%)
PageRank Backward Manhattan
Chebyshev (H1.2)
Fractional (N=1.(3), 80%)
Cosine (H1.3)
Euclidean (H2.3)
Minkowski (10%)
TextRank Backward
Degree (! = 0.02, idf)
PageRank Backward Cosine
Fractional (N = 1.(3), SSC = 1)
Fractional (N = 1.(3), H1.3)
Continuous LexRank (idf)
Cosine (90%)
Uniform Influx (10%NN, =10000)
TextRank Undirected
Euclidean (SSC = 5, idf)
Manhattan (H1.2)
Cosine (idf, H0)
Minkowski (H2.1)
Minkowski (SSC = 2)
PageRank Backward Cosine (idf)
Cosine (SSC = 4, idf)
Manhattan (idf, H2.1)
Manhattan (10%)
Manhattan (SSC = 2)
Fractional (N = 1.(3), idf, H1.3)
0.00% 0.50% 1.00% 1.50% 2.00% 2.50% 3.00% 3.50% 4.00%
Improvement over the previous system

Improvement over the baseline

Figure 2: Analysis of the increase in performance of each model.

similarity metric, the use of the cosine similarity results in a performance comparable to
the one of using the Content Overlap metric. The Manhattan-based similarity metric is
defined in Eq. 29.

simmanhattan (x, y) =

1
1 + distmanhattan (x, y)

(29)

4.2 Experiment 2: Speech
In this section, we describe the experiments performed and analyze the corresponding results
when using as input source automatically transcribed speech.
292

fiRevisiting Centrality-as-Relevance

4.2.1 Data
To evaluate our ideas in the speech processing setting, we used the same data of Ribeiro and
de Matos (2008a): the automatic transcriptions of 15 broadcast news stories in European
Portuguese, part of a news program. Subject areas include society, politics, sports,
among others. Table 4 details the corpus composition. For each news story, there is a
human-produced reference summary, which is an abstract. The average word recognition
error rate is 19.5% and automatic sentence segmentation attained a slot error rate (SER,
commonly used to evaluate this kind of task) of 90.2%. As it is possible to observe in Table 4,
it is important to distinguish between the notion of sentence in written text and that of
sentence-like unit (SU) in speech data. Note, in particular, the difference in the average
number of words per sentence in the summary versus the average number of words per SU in
the news story. According to Liu, Shriberg, Stolcke, Hillard, Ostendorf, and Harper (2006),
the concept of SU is different from the concept of sentence in written text, since, although
semantically complete, SUs can be smaller than a sentence. This is corroborated by the fact
that it is possible to find news stories with SUs of length 1 (this corpus has 8 SUs of length
1). Beyond the definition of SU, note that an SER of 90.2% is a high value: currently, the
automatic punctuation module responsible for delimiting SUs achieves an SER of 62.2%,
using prosodic information (Batista, Moniz, Trancoso, Meinedo, Mata, & Mamede, 2010).

#Words

#SUs
#Setences

Average

Minimum

Maximum

News Story (NS)
NS SU
Summary (S)
S Sentence

287
11
33
20

74
1
9
8

512
91
72
33

News Story
Summary

27
2

6
1

51
4

Table 4: Corpus characterization.

4.2.2 Evaluation Setup
Regarding speech summarization, even considering the difficulties concerning the applicability of text summarization methods to spoken documents, shallow approaches like LSA or
MMR seem to achieve performances comparable to the ones using specific speech-related
features (Penn & Zhu, 2008), especially in unsupervised approaches. Given the implemented
models, in this experiment we compare the support sets relevance model to the following
systems:
 An LSA baseline.
 The following graph-based methods: Uniform Influx (Kurland & Lee, 2005, 2010),
Continuous LexRank and Degree centrality (Erkan & Radev, 2004), and TextRank (Mihalcea & Tarau, 2004, 2005).
293

fiRibeiro & de Matos

 The method proposed by Ribeiro and de Matos (2008a), which explores the use of
additional related information, less prone to speech-related errors (e.g. from online
newspapers), to improve speech summarization (Mixed-Source).
 Two human summarizers (extractive) using as source the automatic speech transcriptions of the news stories (Human Extractive).
Before analyzing the results, it is important to examine human performance. One of
the relevant issues that should be assessed is the level of agreement between the two human
summarizers: this was accomplished using the kappa coefficient (Carletta, 1996), for which
we obtained a value of 0.425, what is considered a fair to moderate/good agreement (Landis
& Kosh, 1977; Fleiss, 1981). Concerning the selected sentences, Figure 3 shows that human
summarizer H2 consistently selected the first n sentences, and that in H1 choices there is
also a noticeable preference for the first sentences of each news story.

1
Sentence position

2
3
H1

4

H2
5
Remaining sentences
0

5

10

15

20

Number of times selected

Figure 3: Human sentence selection patterns.
To be able to perform a good assessment of the automatic models, we conducted two
experiments: in the first one, the number of SUs extracted to compose the automatic
summaries was defined in accordance to the number of sentences of the reference human
abstracts (which consisted in compressing the input source to about 10% of the original
size); in the second experiment, the number of extracted SUs of the automatic summaries
was determined by the size of the shortest corresponding human extractive summary. Notice
that Mixed-Source and human summaries are the same in both experiments.
4.2.3 Results
Table 5 shows the ROUGE-1 scores obtained, for both speech experiments. In this table,
it is possible to find more than one instance of some models, since sometimes the bestperforming variant when using as summary size the size of the abstracts was different from
the one using as summary size the size of the human extracts.
294

fiRevisiting Centrality-as-Relevance

Systems

Using as Summary Size Reference
Human Abstracts Shortest Human Extracts

Human Extractive 1
Human Extractive 2
Cosine (idf, H2.3)
PageRank Backward Cosine
TextRank Backward
PageRank Backward Cosine (idf )
First Sentences
PageRank Backward Manhattan
Chebyshev (H2.3)
Chebyshev (10%)
Cosine (H2.3)
Minkowski (H2.3)
Cosine (40%)
Euclidean (H2.3)
Mixed-Source
Cosine (idf, 40%)
Minkowski (40%)
Fractional (N = 1.(3), idf, H2.3)
Manhattan (H3.1)
Fractional (N = 1.(3), SSC=4)
Cosine (80%)
Fractional (N = 1.(3), H3.2)
Degree ( = 0.06)
Fractional (N = 1.(3), 20%)
Manhattan (10%)
Euclidean (20%)
Euclidean (10%)
Euclidean (SSC=3)
Fractional (N = 1.(3), 10%)
Fractional (N = 1.(3), SSC=3)
TextRank Undirected
Degree ( = 0.03, idf )
Uniform Influx (10%NN,  = 500)

0.544
0.514
0.477
0.473
0.470
0.467
0.462
0.462
0.458
0.443
0.410
0.407
0.404
0.401
0.392
0.389
0.381
0.380
0.373
0.371
0.365
0.361
0.351
0.347
0.346
0.344
0.337
0.336
0.336
0.333
0.332
0.328
0.314

LSA Baseline

0.308 [0.239, 0.407]

0.338 [0.260, 0.432]

Continuous LexRank (idf )
Fractional (N = 0.1, 90%)
Continuous LexRank
PageRank Undirected Manhattan
Fractional (N = 0.5, 90%)
Fractional (N = 0.75, 90%)

0.303
0.301
0.279
0.234
0.224
0.208

0.362
0.368
0.335
0.328
0.284
0.235

[0.452,
[0.392,
[0.374,
[0.363,
[0.360,
[0.360,
[0.360,
[0.355,
[0.351,
[0.329,
[0.306,
[0.316,
[0.306,
[0.310,
[0.340,
[0.287,
[0.288,
[0.274,
[0.276,
[0.279,
[0.290,
[0.268,
[0.247,
[0.280,
[0.246,
[0.277,
[0.262,
[0.262,
[0.263,
[0.256,
[0.242,
[0.232,
[0.211,

[0.215,
[0.191,
[0.212,
[0.163,
[0.133,
[0.149,

0.640]
0.637]
0.580]
0.583]
0.580]
0.571]
0.572]
0.577]
0.571]
0.576]
0.520]
0.509]
0.512]
0.504]
0.452]
0.500]
0.495]
0.496]
0.494]
0.483]
0.458]
0.469]
0.463]
0.432]
0.478]
0.418]
0.432]
0.430]
0.429]
0.442]
0.423]
0.428]
0.427]

0.402]
0.438]
0.343]
0.295]
0.336]
0.281]

0.544
0.514
0.505
0.510
0.505
0.516
0.514
0.514
0.506
0.483
0.446
0.449
0.440
0.440
0.392
0.464
0.435
0.451
0.431
0.402
0.443
0.431
0.383
0.374
0.412
0.371
0.404
0.405
0.405
0.407
0.361
0.369
0.382

[0.455,
[0.402,
[0.405,
[0.399,
[0.391,
[0.393,
[0.390,
[0.392,
[0.388,
[0.356,
[0.329,
[0.349,
[0.344,
[0.341,
[0.339,
[0.355,
[0.325,
[0.354,
[0.343,
[0.303,
[0.345,
[0.316,
[0.276,
[0.292,
[0.311,
[0.288,
[0.296,
[0.300,
[0.305,
[0.308,
[0.262,
[0.260,
[0.258,

[0.263,
[0.252,
[0.246,
[0.240,
[0.176,
[0.165,

0.650]
0.652]
0.619]
0.628]
0.625]
0.646]
0.637]
0.648]
0.618]
0.615]
0.562]
0.571]
0.547]
0.547]
0.449]
0.577]
0.554]
0.556]
0.533]
0.526]
0.555]
0.563]
0.499]
0.467]
0.532]
0.474]
0.524]
0.519]
0.529]
0.530]
0.464]
0.476]
0.511]

0.471]
0.498]
0.441]
0.432]
0.412]
0.302]

Table 5: ROUGE-1 scores, with 95% confidence intervals computed using bootstrap statistics, for the speech experiment (SSC stands for Support Set Cardinality; sorted
using the scores of the human abstracts).

295

fiRibeiro & de Matos

A first observation concerns a particular aspect of corpus: as it can be seen, especially
in the experiment using as reference size the size of the shortest human extracts, both Human 2 and First Sentences summarizers attained the same ROUGE-1 scores (this does not
happen in the experiment using the abstracts size only, due to the fact that First Sentences
summaries are shorter, adapted to the experiment required size, than the ones of Human
2, which were not changed). In fact, the summaries are equal, which shows a consistent
bias indicating that the most relevant sentences tend to occur in the beginning of the news
stories. This bias, although not surprising, since the corpus is composed of broadcast news
stories, is also not that common as can be seen in previous work (Ribeiro & de Matos,
2007; Lin, Yeh, & Chen, 2010). Second, it is interesting to notice the performance of the
PageRank-based models: while in text there is no observable trend concerning the directionality of the graph, and both LexRank versions performed above the baseline, in speech
only the backward versions achieved a good performance (the four undirected versions performed around the baseline, with LexRank obtaining results below the LSA baseline, with
exception for the experiment using the extracts size and idf ). From a models perspective,
and considering the performance of backward versions in both text and speech, the use of
backward directionality seems the main reason for the good performance in speech, where
input sources consist of transcriptions of broadcast news stories from a news program. In
fact, as mentioned before, this kind of input source is usually short (cf. Table 4) and the
main information is given in the opening of the news story. This suggests that directionality introduces position information in the model, which is only relevant for specific types
of input source (this is also discussed in Mihalcea & Tarau, 2005). Moreover, note that
Continuous LexRank performance was close to the LSA Baseline, which implies that the
model is quite susceptible to the referred bias, to the noisy input, or to both. Taking into
consideration that the model is based on pair-wise passage similarity and that one of the
best-performing support sets-based instance was Cosine, the same similarity metric used
by LexRank, it seems that the model was not able to account for the structure of the input sources of this data set. In fact, Degree centrality, also based on the cosine similarity
performed better than all PageRank Undirected models. The Influx model performed close
to Degree centrality, far from the best performing approaches, which, in this case, suggests
that the method for generating the graph, the generation probabilities, is affected by the
noisy input, especially when considering small contexts like passages. Approaches based on
generation probabilities seem more adequate to larger contexts, such as documents (Kurland
& Lee, 2005, 2010; Erkan, 2006a). Erkan (2006b) mentions that results in query-based summarization using generation probabilities were worse than the ones obtained by LexRank
in generic summarization.
Concerning the overall results, performance varies according to the size of the summaries. When using the abstracts size, the best-performing instance is Cosine with idf
using an heuristic based on the passage order; when using the reference extracts size, the
best performance was achieved by the backward PageRank model, followed by the Chebyshev variant also using an heuristic based on passage order and the same Cosine variant.
Both variants achieved better results than TextRank Backward. Given the success of the
heuristic H2.3 in these experiments, it seems that this heuristic may also be introducing
position information in the model. Although not achieving the best performance in the
experiment using the extracts size, there is no significant difference between the best sup296

fiRevisiting Centrality-as-Relevance

port sets-based relevance model instance, the Chebyshev variant using an heuristic based
on passage order, and the ones achieved by human summarizers: applying the directional
Wilcoxon signed rank test with continuity correction, the test values when using the shortest human extracts size are W = 53, p = 0.5. This means a state-of-the-art performance in
the experiment using the abstracts size, and comparable to a human (results similar to First
Sentence, which is similar to Human Extractive 2) when using the shortest human extracts
size. In fact, Chebyshev (10%), to avoid the influence of possible position information, is
also not significantly different than Human Extractive 2 (W = 11, p = 0.2092). Cosine
with idf and using H2.3 has a better performance with statistical significance than Degree
with  = 0.06 (W = 53.5, p < 0.005 when using the abstracts size; W = 54, p < 0.005
when using the shortest human extracts size), TextRank Undirected (W = 92.5, p < 0.05
when using the abstracts size; W = 96, p < 0.05 when using the shortest human extracts
size), and Uniform Influx (W = 60, p < 0.01 when using the abstracts size; W = 51,
p < 0.06 when using the shortest human extracts size), using the same statistical test. The
obtained results, in both speech transcriptions and written text, suggest that the model is
robust, being able to detect the most relevant content without specific information of where
it should be found and performing well in the presence of noisy input. Moreover, cosine
similarity seems to be a good metric to use in the proposed model, performing among the
top ranking variants, in both written and spoken language.
Fractional variants with N < 1 were, again, the worst performing approaches (we did
not include values for the automatically set per passage thresholds in Table 5, since they
were worse than the simple kNN approach) because their effect on the similarity assessment
boosts the influence of the recognition errors. On the other hand, Chebyshev seems more
imune to that influence: the single use of the maximal difference through all the dimensions
makes it less prone to noise (recognition errors). The same happens with the variant using
the generic Minkowski distance with N equal to the number of dimensions of the semantic
space.
Figures 4 and 5 shows the performance variation introduced by the different approaches.
Notice that, in the speech experiments, performance increments are a magnitude higher
when compared to the ones in written text. Overall, the Chebyshev variant of the support
sets-based relevance model introduces the highest relative gains, close to 10% in the experiment using the abstracts size, close to 5% in the experiment using the extracts size. In
the experiment using the extracts size, TextRank Undirected also achieves relative gains
of near 10% over the previous best-performing system, the LSA baseline. Similar relative
improvements are introduced by the human summarizers in the experiment using the abstracts size. As expected, increasing the size of the summaries increases the coverage of the
human abstracts (bottom of Figure 5).
Further, comparing our model to more complex (not centrality-based), state-of-the-art
models like the one presented by Lin et al. (2010) suggests that at least similar performance
is attained: the relative performance increment of our model over LexRank is of 57.4% and
39.8% (both speech experiments), whereas the relative gain of the best variant of the model
proposed by Lin et al. over LexRank is of 39.6%. Note that this can only be taken as
indicative, since an accurate comparison is not possible because data sets differ, Lin et al.
do not explicit which variant of LexRank is used, and do not address statistical significance.
297

fiRibeiro & de Matos

Summary Size Determined by Human Abstracts
Continuous LexRank (idf)
LSA Baseline
Uniform Influx (10%NN, =500)
Degree (! = 0.03, idf)
TextRank Undirected
Fractional (N=1.(3), SSC=3)
Fractional (N=1.(3), 10%)
Euclidean (SSC=3)
Euclidean (10%)
Euclidean (20%)
Manhattan (10%)
Fractional (N=1.(3), 20%)
Degree (! = 0.06)
Fractional (N=1.(3), H3.2)
Cosine (80%)
Fractional (N=1.(3), SSC=4)
Manhattan (H3.1)
Fractional (N=1.(3), idf, H2.3)
Minkowski (40%)
Cosine (idf, 40%)
Mixed-Source
Euclidean (H2.3)
Cosine (40%)
Minkowski (H2.3)
Cosine (H2.3)
Chebyshev (10%)
Chebyshev (H2.3)
PageRank Backward Manhattan
First Sentences
PageRank Backward Cosine (idf)
TextRank Backward
PageRank Backward Cosine
Cosine (idf, H2.3)
Human Extractive 2
Human Extractive 1
-10%

0%

10%

20%

Improvement over the previous system

30%

40%

50%

60%

70%

80%

Improvement over the baseline

Figure 4: Analysis of the increase in performance of each model (Experiment using the
abstracts size).

4.3 Influence of the Size of the Support Sets on the Assessment of Relevance
We do not propose a method for determining an optimum size for the support sets. Nonetheless, we analyze the influence of the support set size on the assessment of the relevant
content, both in text and speech.
Figure 6 depicts the behavior of the model variants with a performance above the baseline over written text, while Figure 7 illustrates the variants under the same conditions over
298

fiRevisiting Centrality-as-Relevance

Summary Size Determined by Shortest Human Extracts
TextRank Undirected
Continuous LexRank (idf)
Degree (! = 0.03, idf)
Euclidean (20%)
Fractional (N=1.(3), 20%)
Uniform Influx (10%NN, =500)
Degree (! = 0.06)
Mixed-Source
Fractional (N=1.(3), SSC=4)
Euclidean (10%)
Fractional (N=1.(3), 10%)
Euclidean (SSC=3)
Fractional (N=1.(3), SSC=3)
Manhattan (10%)
Fractional (N=1.(3), H3.2)
Manhattan (H3.1)
Minkowski (40%)
Euclidean (H2.3)
Cosine (40%)
Cosine (80%)
Cosine (H2.3)
Minkowski (H2.3)
Fractional (N=1.(3), idf, H2.3)
Cosine (idf, 40%)
Chebyshev (10%)
TextRank Backward
Cosine (idf, H2.3)
Chebyshev (H2.3)
PageRank Backward Cosine
PageRank Backward Manhattan
First Sentences
Human Extractive 2
PageRank Backward Cosine (idf)
Human Extractive 1
-10%

0%

10%

20%

Improvement over the previous system

30%

40%

50%

60%

70%

80%

Improvement over the baseline

Figure 5: Analysis of the increase in performance of each model (Experiment using the
abstracts size).

automatic speech transcriptions (in this case, error bars were omitted for clarity). We analyze the general performance of those variants, considering as support set size the number of
passages of the input source in 10% increments. Given the average size of an input source,
both in written text (Table 1), and speech transcriptions (Table 4), absolute cardinalities
(SSC) ranging from 1 to 5 passages broadly cover possible sizes in the interval 0-10%.
A first observation concerns the fact that varying the cardinality of the support sets
when the input sources consist of written text has a smooth effect over the performance.
This allows the analysis of generic tendencies. In contrast, when processing automatic
299

fiRibeiro & de Matos

Cosine

Manhattan (N=1)

Fractional (N=1.(3))

90%

80%

70%

60%

50%

40%

90%

80%

70%

60%

50%

40%

30%

20%

10%

SSC=5

SSC=3

SSC=4

SSC=2

0.39

SSC=1

0.4

0.39

30%

0.41

0.4

20%

0.42

0.41

10%

0.43

0.42

SSC=5

0.43

SSC=3

0.44

SSC=4

0.45

0.44

SSC=2

0.46

0.45

SSC=1

0.46

Euclidean (N=2)

70%

80%

90%

80%

90%

60%

50%

70%

Minkowski (variable N)

40%

90%

80%

70%

60%

50%

40%

30%

20%

10%

SSC=5

SSC=4

SSC=3

SSC=2

0.39

SSC=1

0.4

0.39

30%

0.41

0.4

20%

0.42

0.41

10%

0.43

0.42

SSC=5

0.43

SSC=4

0.44

SSC=3

0.45

0.44

SSC=2

0.46

0.45

SSC=1

0.46

Chebyshev (N!!)

60%

50%

40%

90%

80%

70%

60%

50%

40%

30%

20%

10%

SSC=5

SSC=4

SSC=3

SSC=2

0.39

SSC=1

0.4

0.39

30%

0.41

0.4

20%

0.42

0.41

10%

0.43

0.42

SSC=5

0.43

SSC=4

0.44

SSC=3

0.45

0.44

SSC=2

0.46

0.45

SSC=1

0.46

Figure 6: Analysis of the impact of the cardinality of the support sets over text summarization. Y axes are ROUGE-1 scores and X axes are support sets cardinalities
(absolute and relative to the length of the input source, in terms of passages).

speech transcriptions, it is possible to perceive several irregularities. These irregularities
can have two different causes: the intrinsic characteristics of speech transcriptions such as
recognition errors, sentence boundary detection errors, and the type of discourse; or, the
specificities of the data setin particular, the global size of the corpus and the specific
news story structure. However, considering the performance of the metrics over both text
and speech, the irregularities seem to be mainly caused by the intrinsic properties of speech
transcriptions and the specific structure of the news story.
300

fiRevisiting Centrality-as-Relevance

90%

80%

60%

50%

40%

70%
70%

80%

90%

70%

80%

90%

60%

50%

40%

30%

60%

50%

40%

30%

20%

90%

80%

70%

60%

50%

40%

30%

20%

10%

SSC=5

SSC=4

0.2

SSC=3

0.2

SSC=2

0.3
0.25
SSC=1

0.3
0.25

10%

0.4
0.35

SSC=5

0.4
0.35

SSC=4

0.5
0.45

SSC=3

0.5
0.45

SSC=2

Chebyshev (N!!)

SSC=1

Minkowski (variable N)

20%

90%

80%

70%

60%

50%

40%

30%

20%

10%

SSC=5

SSC=4

0.2

SSC=3

0.2

SSC=2

0.3
0.25
SSC=1

0.3
0.25

10%

0.4
0.35

SSC=5

0.4
0.35

SSC=4

0.5
0.45

SSC=3

0.5
0.45

SSC=2

Euclidean (N=2)

SSC=1

Fractional (N=1.(3))

30%

90%

80%

70%

60%

50%

40%

30%

20%

10%

SSC=5

SSC=3

0.2

SSC=4

0.2

SSC=2

0.25
SSC=1

0.3

0.25

20%

0.35

0.3

10%

0.35

SSC=5

0.4

SSC=3

0.45

0.4

SSC=4

0.45

SSC=2

Manhattan (N=1)
0.5

SSC=1

Cosine
0.5

Figure 7: Analysis of the impact of the cardinality of the support sets over speech-to-text
summarization. Y axes are ROUGE-1 scores and X axes are support sets cardinalities (absolute and relative to the length of the input source in terms of passages).
Lines with square marks correspond to the experiment using as summary size the
size of human abstracts; lines with circle marks correspond to the experiment
using as summary size the size of the shortest human extracts. Horizontal lines
correspond to baselines.

301

fiRibeiro & de Matos

Concerning performance itself, in written text, the best performances are achieved using
low cardinalities (absolute cardinalities of 2 or 3 passages, or of about 10% of the passages
of the input source). Moreover, an increase in the size of the support sets leads to a decay
of the results (except when using the cosine similarity). When processing automatic speech
transcriptions, it is difficult to find a clear definition of when the best results are achieved.
Considering absolute cardinalities, with the exception of the Manhattan distance, every
variant has a peak when using support sets with 4 passages. However, it is not possible
to extend such line of thought to relative sizes due to the previously referred irregularities.
Nonetheless, higher cardinalities (70%90%) lead to worse results, what is expected given
the nature of the model (again with exception of when using the cosine similarity). In
addition, note that increasing the size of the summaries improves the distinction from the
baseline (summaries based on the size of the shortest human extracts are longer than the
ones based on the size of the human abstracts). This means that the model is robust to
needs regarding summary size, continuing to select good content even for larger summaries.

5. Conclusions
The number of up-to-date examples of work on automatic summarization using centralitybased relevance models is significant (Garg, Favre, Reidhammer, & Hakkani-Tur, 2009;
Antiqueira et al., 2009; Ceylan et al., 2010; Wan, Li, & Xiao, 2010). In our work, we
assessed the main approaches of the centrality-as-relevance paradigm, and introduced a
new centrality-based relevance model for automatic summarization. Our model uses support
sets to better characterize the information sources to be summarized, leading to a better
estimation of the relevant content. In fact, we assume that input sources comprehend
several topics that are uncovered by associating to each passage a support set composed
by the most semantically related passages. Building on the ideas of Ruge (1992), [...] the
model of semantic space in which the relative position of two terms determines the semantic
similarity better fits the imagination of human intuition [about] semantic similarity [...],
semantic relatedness was computed by geometric proximity. We explore several metrics and
analyze their impact on the proposed model as well as (to a certain extent) on the related
work. Centrality (relevance) is determined by taking into account the whole input source,
and not only local information, using the support sets-based representation. Moreover,
although not formally analyzed, notice that the proposed model has low computational
requirements.
We conducted a thorough automatic evaluation, experimenting our model both on written text and transcribed speech summarization. The obtained results suggest that the
model is robust, being able to detect the most relevant content without specific information
of where it should be found and performing well in the presence of noisy input, such as
automatic speech transcriptions. However, it must be taken into consideration that the
use of ROUGE in summary evaluation, although generalized, allowing to easily compare
results and replicate experiments, is not an ideal scenario, and consequently, results should
be corroborated by a perceptual evaluation. The outcome of the performed trials show that
the proposed model achieves state-of-the-art performance in both text and speech summarization, including when compared to considerably more complex approaches. Nonetheless,
we identified some limitations. First, although grounding semantic similarity on geometric
302

fiRevisiting Centrality-as-Relevance

proximity, in the current experiments we rely mainly on lexical overlap. While maintaining
the semantic approach, the use of more complex methods (Turney & Pantel, 2010) may improve the assessment of semantic similarity. Second, we did not address a specific procedure
for estimating optimum thresholds, leaving it for future research. Nonetheless, we explored
several heuristics that achieved top ranking performance. Moreover, we carried out in this
document an analysis that provides some clues for the adequate dimension of the support
sets, but a more analytical analysis should be performed.

Acknowledgments
We would like to thank the anonymous reviewers for their insightful comments. This work
was supported by FCT (INESC-ID multiannual funding) through the PIDDAC Program
funds.

References
Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (2001). On the Surprising Behavior of
Distance Metrics in High Dimensional Space. In den Bussche, J. V., & Vianu, V.
(Eds.), Database Theory  ICDT 2001, 8th International Conference London, UK,
January 46, 2001 Proceedings, Vol. 1973 of Lecture Notes in Computer Science, pp.
420434. Springer.
Antiqueira, L., Oliveira Jr., O. N., da Fontoura Costa, L., & Nunes, M. G. V. (2009).
A complex network approach to text summarization. Information Sciences, 179 (5),
584599.
Batista, F., Moniz, H., Trancoso, I., Meinedo, H., Mata, A. I., & Mamede, N. J. (2010).
Extending the punctuation module for European Portuguese. In Proceedings of the
11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010), pp. 15091512. ISCA.
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual Web search engine.
Computer Networks and ISDN Systems, 30, 107117.
Carbonell, J., & Goldstein, J. (1998). The Use of MMR, Diversity-Based Reranking for
Reordering Documents and Producing Summaries. In SIGIR 1998: Proceedings of the
21st Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, pp. 335336. ACM.
Carletta, J. (1996). Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22 (2), 249254.
Ceylan, H., Mihalcea, R., Ozertem, U., Lloret, E., & Palomar, M. (2010). Quantifying the
Limits and Success of Extractive Summarization Systems Across Domains. In Human
Language Technologies: The 2010 Annual Conference of the North American Chapter
of the ACL, pp. 903911. Association for Computational Linguistics.
Christensen, H., Gotoh, Y., Kolluru, B., & Renals, S. (2003). Are Extractive Text Summarisation Techniques Portable To Broadcast News?. In Proceedings of the IEEE Work303

fiRibeiro & de Matos

shop on Automatic Speech Recognition and Understanding (ASRU 03), pp. 489494.
IEEE.
Cohn, T., & Lapata, M. (2009). Sentence Compression as Tree Transduction. Journal of
Artificial Intelligence Research, 34, 637674.
de Oliveira, P. C. F., Torrens, E. W., Cidral, A., Schossland, S., & Bittencourt, E. (2008).
Evaluating Summaries Automatically  a system proposal. In Proceedings of the Sixth
International Language Resources and Evaluation (LREC08), pp. 474479. ELRA.
Endres-Niggemeyer, B. (1998). Summarizing Information. Springer.
Erkan, G. (2006a). Language Model-Based Document Clustering Using Random Walks. In
Proceedings of the Human Language Technology Conference of the North American
Chapter of the ACL, pp. 479486. Association for Computational Linguistics.
Erkan, G. (2006b). Using Biased Random Walks for Focused Summarization. In Proceedings
of the Document Understanding Conference.
Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Research, 22, 457479.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press.
Fleiss, J. L. (1981). Statistical methods for rates and proportions (2nd edition). John Wiley.
Furui, S. (2007). Recent Advances in Automatic Speech Summarization. In Proceedings
of the 8th Conference on Recherche dInformation Assistee par Ordinateur (RIAO).
Centre des Hautes Etudes Internationales dInformatique Documentaire.
Garg, N., Favre, B., Reidhammer, K., & Hakkani-Tur, D. (2009). ClusterRank: A Graph
Based Method for Meeting Summarization. In Proceedings of the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH 2009),
pp. 14991502. ISCA.
Gong, Y., & Liu, X. (2001). Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis. In SIGIR 2001: Proceedings of the 24st Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval, pp.
1925. ACM.
Harabagiu, S., & Lacatusu, F. (2005). Topic Themes for Multi-Document Summarization.
In SIGIR 2005: Proceedings of the 28th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 202209. ACM.
Hori, C., Hori, T., & Furui, S. (2003). Evaluation Method for Automatic Speech Summarization. In Proceedings of the 8th EUROSPEECH - INTERSPEECH 2003, pp.
28252828. ISCA.
Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des Alpes
et des Jura. Bulletin del la Societe Vaudoise des Sciences Naturelles, 37, 547579.
Kanerva, P., Kristoferson, J., & Holst, A. (2000). Random Indexing of text samples for
Latent Semantic Analysis. In Gleitman, L. R., & Joshi, A. K. (Eds.), Proceedings
of the 22nd annual conference of the Cognitive Science Society, p. 1036. Psychology
Press.
304

fiRevisiting Centrality-as-Relevance

Kanerva, P., & Sahlgren, M. (2001). Foundations of real-world intelligence, chap. From
words to understanding, pp. 294311. No. 26. Center for the Study of Language and
Information.
Kleinberg, J. M. (1999). Authoritative Sources in a Hyperlinked Environment. Journal of
the ACM, 46 (5), 604632.
Koosis, P. (1998). Introduction to Hp Spaces. Cambridge Universisty Press.
Kurland, O., & Lee, L. (2005). PageRank without Hyperlinks: Structural Re-Ranking using
Links Induced by Language Models. In SIGIR 2005: Proceedings of the 28th Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 306313. ACM.
Kurland, O., & Lee, L. (2010). PageRank without Hyperlinks: Structural Reranking using
Links Induced by Language Models. ACM Transactions on Information Systems,
28 (4), 138.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An Introduction to Latent Semantic
Analysis. Discourse Processes, 25 (2), 259284.
Landis, J. R., & Kosh, G. G. (1977). The Measurement of Observer Agreement for Categorical Data. Biometrics, 33, 159174.
Leite, D. S., Rino, L. H. M., Pardo, T. A. S., & Nunes, M. G. V. (2007). Extractive Automatic Summarization: Does more linguitic knowledge make a difference?. In Proceedings of the Second Workshop on TextGraphs: Graph-based Algorithms for Natural
Language Processing, pp. 1724. Association for Computational Linguistics.
Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In Moens,
M.-F., & Szpakowicz, S. (Eds.), Text Summarization Branches Out: Proceedings of
the ACL-04 Workshop, pp. 7481. Association for Computational Linguistics.
Lin, C.-Y., & Hovy, E. (2000). The Automated Acquisition of Topic Signatures for Text
Summarization. In Coling 2000: The 18th International Conference on Computational
Linguistics, Vol. 1, pp. 495501. Association for Computational Linguistics.
Lin, S.-H., Yeh, Y.-M., & Chen, B. (2010). Extractive Speech Summarization  From
the View of Decision Theory. In Proceedings of the 11th Annual Conference of the
International Speech Communication Association (INTERSPEECH 2010), pp. 1684
1687. ISCA.
Liu, Y., Shriberg, E., Stolcke, A., Hillard, D., Ostendorf, M., & Harper, M. (2006). Enriching Speech Recognition with Automatic Detection of Sentence Boundaries and
Disfluencies. IEEE Transactions on Speech and Audio Processing, 14 (5), 15261540.
Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic and associative priming in highdimensional semantic space. In Moore, J. D., & Lehman, J. F. (Eds.), Proceedings of
the 17th annual conference of the Cognitive Science Society, pp. 660665. Psychology
Press.
Marcu, D. (2000). The Theory and Practice of Discourse Parsing and Summarization. The
MIT Press.
305

fiRibeiro & de Matos

Maskey, S. R., & Hirschberg, J. (2005). Comparing Lexical, Acoustic/Prosodic, Strucural
and Discourse Features for Speech Summarization. In Proceedings of the 9th EUROSPEECH - INTERSPEECH 2005.
McKeown, K. R., Hirschberg, J., Galley, M., & Maskey, S. R. (2005). From Text to Speech
Summarization. In 2005 IEEE International Conference on Acoustics, Speech, and
Signal Processing. Proceedings, Vol. V, pp. 9971000. IEEE.
Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing Order into Texts. In Proceedings
of the 2004 Conference on Empirical Methods in Natural Language Processing, pp.
404411. Association for Computational Linguistics.
Mihalcea, R., & Tarau, P. (2005). A Language Independent Algorithm for Single and
Multiple Document Summarization. In Proceedings of the Second International Joint
Conference on Natural Language Processing: Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial Abstracts, pp. 1924. Asian Federation of Natural Language Processing.
Mooney, C. Z., & Duval, R. D. (1993). Bootstrapping : a nonparametric approach to statistical inference. Sage Publications.
Murray, G., & Renals, S. (2007). Term-Weighting for Summarization of Multi-Party Spoken
Dialogues. In Popescu-Belis, A., Renals, S., & Bourlard, H. (Eds.), Machine Learning
for Multimodal Interaction IV, Vol. 4892 of Lecture Notes in Computer Science, pp.
155166. Springer.
Nenkova, A. (2006). Summarization Evaluation for Text and Speech: Issues and Approaches.
In Proceedings of INTERSPEECH 2006 - ICSLP, pp. 15271530. ISCA.
Nenkova, A., Passonneau, R., & McKeown, K. (2007). The pyramid method: incorporating
human content selection variation in summarization evaluation. ACM Transactions
on Speech and Language Processing, 4 (2).
Orasan, C., Pekar, V., & Hasler, L. (2004). A comparison of summarisation methods based
on term specificity estimation. In Proceedings of the Fourth International Language
Resources and Evaluation (LREC04), pp. 10371041. ELRA.
Pardo, T. A. S., & Rino, L. H. M. (2003). TeMario: a corpus for automatic text summarization. Tech. rep. NILC-TR-03-09, Nucleo Interinstitucional de Lingustica Computacional (NILC), Sao Carlos, Brazil.
Penn, G., & Zhu, X. (2008). A Critical Reassessment of Evaluation Baselines for Speech
Summarization. In Proceeding of ACL-08: HLT, pp. 470478. Association for Computational Linguistics.
R Development Core Team (2009). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.
Radev, D. R., Hatzivassiloglou, V., & McKeown, K. R. (1999). A Description of the CIDR
System as Used for TDT-2. In Proceedings of the DARPA Broadcast News Workshop.
Radev, D. R., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In
306

fiRevisiting Centrality-as-Relevance

NAACL-ANLP 2000 Workshop: Automatic Summarization, pp. 2130. Association
for Computational Linguistics.
Radev, D. R., Jing, H., Stys, M., & Tam, D. (2004). Centroid-based summarization of
multiple documents. Information Processing and Management, 40, 919938.
Radev, D. R., & Tam, D. (2003). Summarization Evaluation using Relative Utility. In
Proceedings of the 12th international conference on Information and Knowledge Management, pp. 508511. ACM.
Ribeiro, R., & de Matos, D. M. (2007). Extractive Summarization of Broadcast News: Comparing Strategies for European Portuguese. In Matousek, V., & Mautner, P. (Eds.),
Text, Speech and Dialogue  10th International Conference, TSD 2007, Pilsen, Czech
Republic, September 3-7, 2007. Proceedings, Vol. 4629 of Lecture Notes in Computer
Science (Subseries LNAI), pp. 115122. Springer.
Ribeiro, R., & de Matos, D. M. (2008a). Mixed-Source Multi-Document Speech-to-Text
Summarization. In Coling 2008: Proceedings of the 2nd workshop on Multi-source
Multilingual Information Extraction and Summarization, pp. 3340. Coling 2008 Organizing Committee.
Ribeiro, R., & de Matos, D. M. (2008b). Using Prior Knowledge to Assess Relevance in
Speech Summarization. In 2008 IEEE Workshop on Spoken Language Technology,
pp. 169172. IEEE.
Ruge, G. (1992). Experiments on linguistically-based term associations. Information Processing and Management, 28 (3), 317332.
Sahlgren, M. (2006). The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Stockholm University.
Steyvers, M., & Griffiths, T. (2007). Handbook of Latent Semantic Analysis, chap. Probabilistic Topic Models, pp. 427448. Lawrence Erlbaum Associates.
Tucker, R. I., & Sparck Jones, K. (2005). Between shallow and deep: an experiment in automatic summarising. Tech. rep. 632, University of Cambridge Computer Laboratory.
Turney, P. D., & Pantel, P. (2010). From Frequency to Meaning: Vector Space Models of
Semantics. Journal of Artificial Intelligence Research, 37, 141188.
Uzeda, V. R., Pardo, T. A. S., & Nunes, M. G. V. (2010). A comprehensive comparative
evaluation of RST-based summarization methods. ACM Transactions on Speech and
Language Processing, 6 (4), 120.
Vanderwende, L., Suzuki, H., Brockett, C., & Nenkova, A. (2007). Beyond SumBasic:
Task-focused summarization and lexical expansion. Information Processing and Management, 43, 16061618.
Wan, X., Li, H., & Xiao, J. (2010). EUSUM: Extracting Easy-to-Understand English Summaries for Non-Native Readers. In SIGIR 2010: Proceedings of the 33th Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 491498. ACM.
307

fiRibeiro & de Matos

Wan, X., Yang, J., & Xiao, J. (2007). CollabSum: Exploiting Multiple Document Clustering
for Collaborative Single Document Summarizations. In SIGIR 2007: Proceedings of
the 30th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 143150. ACM.
Zechner, K., & Waibel, A. (2000). Minimizing Word Error Rate in Textual Summaries of
Spoken Language. In Proceedings of the 1st conference of the North American chapter
of the ACL, pp. 186193. Morgan Kaufmann.
Zhu, X. (2005). Semi-Supervised Learning with Graphs. Ph.D. thesis, Language Technologies
Institute, School of Computer Science, Carnegie Mellon University.

308

fiJournal of Artificial Intelligence Research 42 (2011) 91-124

Submitted 5/11; published 10/11

Scheduling Bipartite Tournaments to
Minimize Total Travel Distance
Richard Hoshino
Ken-ichi Kawarabayashi

richard.hoshino@gmail.com
k keniti@nii.ac.jp

National Institute of Informatics,
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan

Abstract
In many professional sports leagues, teams from opposing leagues/conferences compete
against one another, playing inter-league games. This is an example of a bipartite tournament. In this paper, we consider the problem of reducing the total travel distance of
bipartite tournaments, by analyzing inter-league scheduling from the perspective of discrete optimization. This research has natural applications to sports scheduling, especially
for leagues such as the National Basketball Association (NBA) where teams must travel
long distances across North America to play all their games, thus consuming much time,
money, and greenhouse gas emissions.
We introduce the Bipartite Traveling Tournament Problem (BTTP), the inter-league
variant of the well-studied Traveling Tournament Problem. We prove that the 2n-team
BTTP is NP-complete, but for small values of n, a distance-optimal inter-league schedule
can be generated from an algorithm based on minimum-weight 4-cycle-covers. We apply
our theoretical results to the 12-team Nippon Professional Baseball (NPB) league in Japan,
producing a provably-optimal schedule requiring 42950 kilometres of total team travel, a
16% reduction compared to the actual distance traveled by these teams during the 2010
NPB season. We also develop a nearly-optimal inter-league tournament for the 30-team
NBA league, just 3.8% higher than the trivial theoretical lower bound.

1. Introduction
Consider a tournament involving two teams X and Y , each with n players. In a bipartite
tournament, players from team X compete against players from team Y , with the goal of
determining the superior team. Labeling the players {x1 , x2 , . . . , xn } and {y1 , y2 , . . . , yn },
we represent each match by the ordered pair (xi , yj ), with indices i, j  {1, 2, . . . , n}.
The Davis Cup is an example of a bipartite tournament, where each country fields a
tennis squad consisting of two singles players and a doubles team. Their are five matches
played between the two countries, with the doubles teams squaring off on Day 2, sandwiched
between the singles matches (x1 , y1 ), (x2 , y2 ) on Day 1, and (x1 , y2 ), (x2 , y1 ) on Day 3.
Another example is the biennial Ryder Cup championship, where the United States and
Europe field teams consisting of the top twelve male golfers. The competition culminates
with twelve head-to-head matches on the last day, with the ith ranked golfer from the United
States facing off against the ith ranked golfer from Europe.
In a single round-robin (SRR) bipartite tournament, each player from X competes
against every player from Y once, with everyone playing one match in each time slot. This
produces a tournament with n2 matches spread out over n time slots. In a double roundc
2011
AI Access Foundation. All rights reserved.

fiHoshino & Kawarabayashi

robin (DRR) bipartite tournament, each pair plays twice, thus producing a tournament
with 2n2 matches spread out over 2n time slots. SRR bipartite tournaments are common in
tennis and ping-pong, while DRR bipartite tournaments are used in chess, so that xi plays
against each yj twice, with one game as white and one game as black. The aforementioned
Ryder Cup is an example of a partial bipartite tournament, where each player from X plays
against some proper subset of players from Y .
While there has been much research conducted on the theory of bipartite tournaments
(Kendall, Knust, Ribeiro, & Urrutia, 2010), all previous papers have dealt with feasibility
and fairness, specifically in constructing balanced tournament designs and minimizing carryover effects (Easton, Nemhauser, & Trick, 2004) to ensure competitive balance for all the
players on each team.
By replacing the words team and player by league and team, respectively, we
can view X and Y as two n-team sports leagues, where a bipartite tournament between
X and Y represents inter-league play. For example, Major League Baseball (MLB) holds
four weeks of inter-league games each season, with every American League team playing 18
games against a half-dozen teams from the National League. MLB inter-league play is an
example of a partial bipartite tournament, where some/many of the scheduled games are
based on historical rivalry or geographic proximity.
In this light, we consider the problem of minimizing the total travel distance of bipartite
tournaments. For chess and tennis, the issue of travel is irrelevant as all tournament matches
take place in the same venue. However, in the case of inter-league play in professional
baseball, teams must travel long distances to play their games all across North America,
and so finding a schedule that reduces total travel distance is important, for both economic
and environmental reasons.
To answer this question of creating a distance-optimal inter-league schedule, we introduce a variant of the Traveling Tournament Problem (TTP), in which every pair of teams
plays twice, with one game at each teams home stadium. The output is an optimal schedule that minimizes the sum total of distances traveled by the teams as they move from city
to city, subject to several natural constraints that ensure balance and fairness. Unlike the
TTP which models a double round-robin intra-league tournament, our variant, the Bipartite Traveling Tournament Problem (BTTP), seeks the best possible double round-robin
inter-league tournament.
Since its introduction (Easton, Nemhauser, & Trick, 2001), the TTP has emerged as
a popular area of study within the operations research community (Kendall et al., 2010)
due to its incredible complexity, where challenging benchmark problems remain unsolved.
Research on the TTP has led to the development of powerful techniques in integer programming, constraint programming, as well as advanced heuristics such as simulated annealing (Anagnostopoulos, Michel, Hentenryck, & Vergados, 2006) and hill-climbing (Lim,
Rodrigues, & Zhang, 2006). More importantly, the TTP has direct applications to scheduling optimization, and can aid professional sports leagues as they make their regular season
schedules more efficient, saving time and money, as well as reducing greenhouse gas emissions.
The purpose of this paper is to consider the problem of creating distance-optimal interleague tournaments, thus connecting the techniques and methods of sports scheduling to
the theory of bipartite tournaments, producing new directions for research in scheduling op92

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

timization. Optimizing inter-league tournaments is a natural next step in the field of sports
scheduling, especially since the introduction of inter-league play to professional sports. For
example, in Major League Baseball, inter-league play began only in 1997, six decades after
it was first proposed. In Japan, the Nippon Professional Baseball (NPB) league was formed
in 1950, yet NPB inter-league play did not commence until 2005.
The authors were motivated to analyze the Japanese NPB schedule, due to puzzling
inefficiencies in the regular season schedule that we believed could be improved. We developed a multi-round generalization of the TTP (Hoshino & Kawarabayashi, 2011c) based on
Dijkstras shortest path algorithm to create a distance-optimal intra-league schedule that
reduced the total travel distance by over 60000 kilometres as compared to the 2010 NPB
schedule. We elaborated further on the intricacies of intra-league scheduling in a journal
paper (Hoshino & Kawarabayashi, 2011d). Inspired by the success of analyzing intra-league
scheduling, we asked whether our techniques and methods could be extended to inter-league
play, wondering whether the 2010 NPB schedule requiring 51134 kilometres of total team
travel could be minimized to optimality. We answered that question by presenting the Bipartite Traveling Tournament Problem (Hoshino & Kawarabayashi, 2011b), and providing a
rigorous analysis of BTTP for the NPB distance matrix, producing a provably-optimal interleague schedule requiring 42950 kilometres of total team travel (Hoshino & Kawarabayashi,
2011a).
The purpose of this paper is to expand upon our two inter-league conference papers and
provide a more thorough discussion of BTTP and its properties. We present a rigorous proof
to a lemma we omitted due to space constraints (Hoshino & Kawarabayashi, 2011b), that
is key to proving the NP-completeness of BTTP. We also present an application of BTTP
beyond Japanese baseball, by considering the problem of optimizing inter-league scheduling
for the 30-team National Basketball Association (NBA) in North America. While we briefly
alluded to the NBA inter-league problem (Hoshino & Kawarabayashi, 2011b), we are able
to provide a full analysis in this paper.
In Section 2, we formally define BTTP and discuss uniform and non-uniform schedules.
In Section 3, we prove that BTTP on 2n teams is NP-complete by obtaining a reduction
from 3-SAT, the well-known NP-complete problem on boolean satisfiability (Garey & Johnson, 1979). Despite its computational intractability for general n, we present a simple yet
powerful heuristic involving minimum-weight 4-cycle-covers and apply it to the 12-team
NPB league in Japan, as well as the 30-team NBA.
In Section 4, we solve BTTP for the NPB, producing an optimal schedule whose total
travel distance of 42950 kilometres is 16% less than the 51134 kilometres traveled by these
teams during the five weeks of inter-league play in the 2010 season. In Section 5, we produce
a nearly-optimal solution to BTTP for the NBA, developing a bipartite tournament schedule
whose total travel distance of 537791 miles is just 3.8% higher than the trivial theoretical
lower bound. In Section 6, we conclude the paper with several open problems and present
directions for future research.

2. Definitions
Let there be 2n teams, with n teams in each league. Let X and Y be the two leagues,
with X = {x1 , x2 , . . . , xn } and Y = {y1 , y2 , . . . , yn }. Let D be the 2n  2n distance matrix,
93

fiHoshino & Kawarabayashi

where entry Dp,q is the distance between the home stadiums of teams p and q. By definition,
Dp,q = Dq,p for all p, q  X Y , and all diagonal entries Dp,p are zero. Similar to the original
TTP, we require a compact double round-robin bipartite tournament schedule satisfying the
following conditions:
(a) at-most-three: No team may have a home stand or road trip lasting more than three
games.
(b) no-repeat: A team cannot play against the same opponent in two consecutive games.
(c) each-venue: For all 1  i, j  n, teams xi and yj play twice, once in each others
home venue.

x1
x2
x3
y1
y2
y3

1
y1
y2
y3
x1
x2
x3

2
y2
y3
y1
x3
x1
x2

3
y3
y1
y2
x2
x3
x1

4
y1
y2
y3
x1
x2
x3

5
y2
y3
y1
x3
x1
x2

6
y3
y1
y2
x2
x3
x1

x1
x2
x3
y1
y2
y3

1
y3
y1
y2
x2
x3
x1

2
y2
y3
y1
x3
x1
x2

3
y1
y2
y3
x1
x2
x3

4
y3
y1
y2
x2
x3
x1

5
y1
y2
y3
x1
x2
x3

6
y2
y3
y1
x3
x1
x2

Table 1: Two feasible inter-league tournaments for n = 3.
To illustrate, Table 1 provides two feasible tournaments satisfying all of the above conditions for the case n = 3. In this table, as in all other schedules that will be subsequently
presented, home games are marked in bold.
Following the convention of the TTP, whenever a team is scheduled for a road trip
consisting of multiple away games, the team doesnt return to their home city but rather
proceeds directly to their next away venue. Furthermore, we assume that every team begins
the tournament at home, and returns home after playing their last away game. For example,
in Table 1, team x1 would travel a distance of Dx1 ,y1 + Dy1 ,y2 + Dy2 ,y3 + Dy3 ,x1 when playing
the schedule on the left and a distance of Dx1 ,y3 + Dy3 ,y2 + Dy2 ,x1 + Dx1 ,y1 + Dy1 ,x1 when
playing the schedule on the right. The desired solution to BTTP is the tournament schedule
that minimizes the total distance traveled by all 2n teams subject to the given conditions.
Define a trip to be a pair of consecutive games not occurring in the same city, i.e., any
situation where that team doesnt play at the same location in time slots s and s + 1, and
therefore has to travel from one venue to another. In Table 1, the schedule on the left has
24 total trips, while the schedule on the right has 32 trips. One may conjecture that the
total distance of schedule S1 is lower than the total distance of schedule S2 iff S1 has fewer
trips than S2 .
To see that this is actually not the case, let the teams x1 , x3 , y1 , and y2 be located
at (0, 0) and let x2 and y3 be located at (1, 0). Then the schedule on the left has total
distance 16 and the schedule on the right has total distance 12. So minimizing trips does
not correlate to minimizing total travel distance; while the former is a trivial problem, the
latter is extremely difficult, even for the case n = 3.
The six teams x1 , x2 , x3 , y1 , y2 , y3 can be located in the Cartesian plane so that the
distance-optimal solution occurs via a schedule with 27 trips, although in the majority of
cases, the distance-optimal schedule consists of 24 trips, the fewest number possible. This
94

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

inspires several interesting open problems which we will present at the conclusion of this
paper. To provide an example with 27 trips, locate the six teams at x1 = (8, 0), x2 = (9, 0),
x3 = (0, 4), y1 = (6, 1), y2 = (0, 7), and y3 = (3, 5). Then a computer search proves that
the minimal distance is






18 + 16 5 + 16 2 + 3 13 + 5 10 + 2 130 + 61  133.646,
with equality iff the inter-league schedule is one of the two appearing in Table 2. Note that
each of these 27-trip distance-optimal schedules is a mirror image of the other.
x1
x2
x3
y1
y2
y3

1
y1
y2
y3
x1
x2
x3

2
y2
y3
y1
x3
x1
x2

3
y3
y1
y2
x2
x3
x1

4
y1
y2
y3
x1
x2
x3

5
y2
y3
y1
x3
x1
x2

6
y3
y1
y2
x2
x3
x1

x1
x2
x3
y1
y2
y3

1
y3
y1
y2
x2
x3
x1

2
y2
y3
y1
x3
x1
x2

3
y1
y2
y3
x1
x2
x3

4
y3
y1
y2
x2
x3
x1

5
y2
y3
y1
x3
x1
x2

6
y1
y2
y3
x1
x2
x3

Table 2: The 27-trip distance-optimal schedules for a special selection of 6 points.
Let BTTP* be the restriction of BTTP to the set of tournament schedules where in any
given time slot, the teams in each league either all play at home, or all play on the road. For
example, the left schedule in Table 1 is a feasible solution of both BTTP and BTTP*. We say
that such schedules are uniform. While this uniformity constraint significantly reduces the
number of potential tournaments, it allows us to quickly generate an approximate solution
to BTTP from an algorithm based on minimum-weight 4-cycle-covers.
We now prove that both BTTP and BTTP* are NP-complete by obtaining a reduction
from 3-SAT, the well-known NP-complete problem of deciding whether a boolean formula
in conjunctive normal form with three literals per clause admits a satisfying assignment
(Garey & Johnson, 1979).

3. Theoretical Results
To establish our reduction, we first express BTTP in its decision form:
INSTANCE:
(a) 2n teams, in which n teams belong to league X and n teams belong to league Y .
(b) A 2n  2n distance matrix D whose entries are the distances between each pair of
teams in X  Y .
(c) An integer T  0.
QUESTION: does there exist a double round-robin bipartite tournament for which:
(a) The at-most-three, no-repeat, and each-venue conditions are all satisfied.
(b) The sum of the distances traveled by the 2n teams is at most T .
95

fiHoshino & Kawarabayashi

Similarly, we can express BTTP* in its decision form, by adding the uniformity constraint (i.e., for any given time slot, a team plays at home iff every other team in that league
also plays at home). We now reduce these two problems to 3-SAT.
Let S = C1  C2  . . .  Cm be the conjunction of m clauses with three literals on the
variables {u1 , u2 , . . . , ul }. From S, we will define the sets XS and YS representing the teams
in leagues X and Y . From this set of |XS | + |YS | vertices, we will describe a polynomialtime algorithm that constructs a complete graph and assigns edge weights to produce the
distance matrix DS . We then prove the existence of an integer T = T (m) for which the
solutions to BTTP and BTTP* have total travel distance  T iff S is satisfiable. This will
establish the desired polynomial-time reductions.
We can assume that literals ui and ui occur equally often in S for each 1  i  l. To
see why, assume without loss that ui occurs less frequently than ui . By repeated addition
of the tautological clause (ui  ui+1  ui+1 ), which does not affect the satisfiability of S, we
can ensure that the number of occurrences of ui in S matches that of ui .
Let r(i) denote the number of occurrences of ui in S. In Figure 1, we present a gadget
for each variable ui , where the vertices ui,r and ui,r correspond respectively to the r th
occurrence of ui and ui in S, vertex ai,r is adjacent to ui,r1 and ui,r , and vertex bi,r is
adjacent to ui,r and ui,r . (Note: ui,0 := ui,r(i) for all i.)

Figure 1: Gadget for 3-SAT reduction.
This gadget was used to establish the NP-completeness of deciding whether an undirected graph contains a given number of vertex-disjoint s-t paths of a specified length (Itai,
Perl, & Shiloach, 1982) and to prove that the original TTP is NP-complete (Thielen &
Westphal, 2010).
There are l gadgets, one for each ui , i = 1, 2, . . . , l. Now we define the gadget graph GS .
We create vertices cj and dj for 1  j  m, one pair for each clause in S. Join cj to dj .
Now connect cj to vertex ui,r iff clause Cj contains the r th occurrence of ui in S. Similarly,
connect cj to vertex ui,r iff clause Cj contains the r th occurrence of ui in S.
To illustrate, let S = C1  C2  C3  C4  C5  C6  C7  C8 , where C1 = (u1  u2  u3 ),
C2 = (u1  u2  u3 ), C3 = (u1  u2  u4 ), C4 = (u2  u3  u4 ), C5 = (u1  u3  u4 ),
C6 = (u1  u2  u4 ), C7 = (u2  u3  u4 ), and C8 = (u1  u3  u4 ). By definition, S is an
instance of 3-SAT. The gadget graph GS is given in Figure 2.
Since each literal occurs as often as its negation, and each clause has three literals, the
number of clauses in S must be even. Hence, m = 2k for some integer k  1. From the
instance S, we will define a set XS with 18k vertices corresponding to the teams in league
X. We will then define another set YS , with just 3 vertices (labelled p, q, and r), and place
96

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

Figure 2: The gadget graph GS for the instance S.
6k teams at each of these three vertices. This will create a 36k-team league, with 18k teams
in both X and Y . The weight of each edge will just correspond to the distance between
the teams located at those vertices. Using the gadget graph GS , we will define the edge
weights in such a way that S is satisfiable iff the solutions to BTTP and BTTP* have total
distance at most T = T (k) = 96k2 (2900k2 + 375k + 11). This will establish the desired
polynomial-time reductions from 3-SAT.
We first define XS . Let C = {c1 , c2 , . . . , c2k } and D = {d1 , d2 , . . . , d2k }, which are the
same set of vertices from the corresponding gadget graph GS . Let U be the set of 6k
vertices of the form ui,r or ui,r that appear in GS , and let A and B be respectively the set
of vertices of the form ai,r and bi,r that appear in GS . Finally, we present two additional
sets, E = {e1 , e2 , . . . , ek } and F = {f1 , f2 , . . . , fk }, which will be matched up to the vertices
of U in our cycle cover.
We define XS = A  B  C  D  E  F  U . Hence, |XS | = |A| + |B| + |C| + |D| + |E| +
|F | + |U | = 3k + 3k + 2k + 2k + k + k + 6k = 18k.
Having defined XS , we now define the edge weights connecting each pair of vertices
in XS , thus producing a complete graph on 18k vertices. The weight of each edge will
be a function of k. For readability, we will express each weight as a function of z, where
z := 20k + 1. To each edge in this complete graph, we assign a weight from the set
{z 2 , z 2 + z, 2z 2  1} as follows:
(1) A weight of z 2 is given to every edge that appears in the gadget graph GS , the 6k2
edges from U to E, and the k edges connecting ei to fi (for each 1  i  k).
(2) A weight of z 2 + z is given to the 6k2 edges from U to F , the 6k edges connecting A
to B through a common neighbour in U , and the 6k edges connecting D to U through
a common neighbour in C.
(3) A weight of 2z 2  1 is given to every other edge.
We now create an inter-league tournament with 36k total teams. First, we assign the
18k teams in league X to the 18k vertices of graph XS , where the distance between the
97

fiHoshino & Kawarabayashi

home venues of two teams is the edge weight between the corresponding two vertices in the
complete graph.
Let YS = {p, q, r}. Now define the 18k teams in league Y as follows: place 6k teams at
point p, 6k teams at point q, and 6k teams at point r.
Therefore, |XS  YS | = 18k + 3. We now extend our complete graph on 18k vertices to
include these three additional vertices. To assign an edge weight connecting each pair of
inter-league vertices, we read off the matrix given in Table 3.
aA
bB
cC
dD
eE
f F
uU

p  YS
z2
z2
2
2z  1
z2
2
2z  1
z2
2
z +z

q  YS
z2 + z
2z 2  1
z2
2
2z  1
z2 + z
z2
2
z + 2z

r  YS
2z 2  1
z2 + z
z2 + z
z2
z2
2z 2  1
z 2 + 2z

Table 3: Weights of all edges connecting XS to YS .
For example, the edge from ci to p is given a weight of 2z 2  1, for all i = 1, 2, . . . , 2k.
We repeat the same process for each of the 7  3 = 21 pairs connecting a vertex in XS =
A  B  C  D  E  F  U to a vertex in YS = {p, q, r}.
Finally, let the weights of edges pq, pr, and qr all be 2z 2  1. As a result, we have
now created a complete graph on the vertex set XS  YS , and assigned a weight to each
edge. Moreover, the weight of each edge appears in the set {z 2 , z 2 + z, z 2 + 2z, 2z 2  1},
where z = 20k + 1. As most versions of the TTP require the teams to be located at points
satisfying the Triangle Inequality, we have chosen the weights in our inter-league variant
BTTP to ensure that the Triangle Inequality holds for any triplet of points in XS  YS .
We now partition the 18k vertices of XS into groups of cardinality at most three and
attach them to each y  {p, q, r} = YS to produce a union of cycles of length at most 4.
More formally, we define the following:
Definition 1. For each y  YS , a y-rooted 4-cycle-cover is a union of cycles of length at
most 4, where every cycle contains y, no cycle contains a vertex from YS \{y}, and every
vertex of XS appears in exactly one cycle.

Figure 3: A p-rooted 4-cycle-cover with 18 vertices in set XS .
98

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

To illustrate, Figure 3 gives a p-rooted 4-cycle-cover with |XS | = 18. This definition is
motivated by our tournament construction, where we will show that the total travel distance
is minimized by creating a uniform schedule where each team takes the maximum number
of three-game road trips to play their 18k away games. In the case of the 6k teams of YS
located at vertex p, their 6k three-game road trips will correspond to the 6k 4-cycles in the
minimum weight p-rooted 4-cycle-cover. For example, if p-u1,1 -c5 -d5 -p appears as one of the
6k cycles, then each team in YS located at vertex p will play three consecutive road games
during the tournament against the teams of XS located at u1,1 , c5 , and d5 .
So the total distance traveled by each team at y  YS is bounded below by the sum of
the edge weights of the minimum weight y-rooted 4-cycle-cover.
Definition 2. We define three special types of cycles that may appear in a p-rooted 4-cyclecover.
(1) A (p, a, u, b, p)-cycle is a 4-cycle with vertices p, a, u, b in that order, where p  YS ,
a  A, u  U , b  B, where au and ub are both edges in the gadget graph GS .
(2) A (p, u, c, d, p)-cycle is a 4-cycle with vertices p, u, c, d in that order, where p  YS ,
u  U , c  C, d  D, where uc and cd are both edges in the gadget graph GS .
(3) A (p, u, e, f, p)-cycle is a 4-cycle with vertices p, u, e, f in that order, where p  YS ,
u  U , e  E, f  F , where e and f have the same index (i.e., ei and fi for some
1  i  k.)
For example, for our instance S whose gadget graph was illustrated in Figure 2, p-a1,2 u1,1 -b1,1 -p is a (p, a, u, b, p)-cycle, but p-a1,2 -u1,1 -b4,2 -p is not. Similarly, p-u4,3 -c8 -d8 -p is a
(p, u, c, d, p)-cycle, but p-u4,3 -c7 -d7 -p is not.
Following the convention of the TTP (Easton, Nemhauser, & Trick, 2002), we define
ILBt to be the individual lower bound for team t. This value represents the minimum
possible distance that can be traveled by team t in order to complete all of their games
under the constraints of BTTP, independent of the other teams schedules. By definition,
for each team t located at y  YS , the value of ILBt is the minimum weight of a y-rooted
4-cycle-cover.
Similarly, we define the league lower bound LLBT to be the minimum possible distance
traveled by all of the teams t in league T , and the tournament lower bound T LB to be the
minimum possible distance traveled by all the teams in both leagues. We note the following
trivial inequalities:

LLBX

T LB  LLBX + LLBY
X
X

ILBt , LLBY 
ILBt .
tX

tY

By definition, the solution to BTTP is a tournament schedule whose total travel distance
is T LB.
We now have all of the definitions we need to complete the proof of the NP-completeness
of BTTP and BTTP*. We will create an inter-league tournament between the 18k teams
of XS and the 18k teams of YS (with one-third of the teams at each vertex of YS ), and
99

fiHoshino & Kawarabayashi

show that there exists a distance-optimal uniform tournament with total distance at most
T (k) = 96k2 (2900k2 + 375k + 11) iff S is satisfiable. This will establish our polynomialtime reduction from 3-SAT, since all of the transformations in our construction are clearly
polynomial.
The desired result will follow from the next four lemmas. In each lemma, we let KS
be the complete graph on the 18k + 3 vertices of XS  YS , with edge weights as described
in our construction. For the interested reader, the proofs to these three lemmas appear in
Appendix A.
Lemma 1. The following statements are equivalent:
(i) S = C1  C2  . . .  C2k is satisfiable.
(ii) There exists a p-rooted 4-cycle-cover of KS with exactly 3k (p, a, u, b, p)-cycles, 2k
(p, u, c, d, p)-cycles, and k (p, u, e, f, p)-cycles.
Lemma 2. The following statements are equivalent:
(i) A p-rooted 4-cycle-cover of KS has exactly 3k (p, a, u, b, p)-cycles, 2k (p, u, c, d, p)cycles, and k (p, u, e, f, p)-cycles.
(ii) A p-rooted 4-cycle-cover of KS has total edge weight k(24z 2 + 3z).
Lemma 3. Let ILBy be the minimum total edge weight of a y-rooted 4-cycle-cover of KS .
Then

if y = p
 k(24z 2 + 3z)
ILBy =
k(24z 2 + 20z)
if y = q

k(24z 2 + 19z)
if y = r

Let us illustrate these three lemmas with a specific example. Let S = C1  C2  C3 
C4  C5  C6  C7  C8 be the instance of 3-SAT whose gadget graph GS was presented in
Figure 2. Recall that we defined C1 = (u1  u2  u3 ), C2 = (u1  u2  u3 ), C3 = (u1  u2  u4 ),
C4 = (u2  u3  u4 ), C5 = (u1  u3  u4 ), C6 = (u1  u2  u4 ), C7 = (u2  u3  u4 ), and
C8 = (u1  u3  u4 ).
Suppose S is satisfiable, i.e., there is a function  : {u1 , u2 , u3 , u4 }  {TRUE, FALSE}
so that each clause Ci evaluates to TRUE for all 1  i  8. By symmetry, we may
assume without loss that (u4 ) is TRUE. Then from clauses C6 , C7 , and C8 , we see that
for 1  i  3, (ui ) must be all TRUE or all FALSE. In the former, clause C2 is FALSE,
and in the latter, clause C1 is FALSE. Therefore, S is not satisfiable.
Since S is not satisfiable, by Lemma 1, there does not exist a p-rooted 4-cycle-cover
of KS with 12 (p, a, u, b, p)-cycles, 8 (p, u, c, d, p)-cycles, and 4 (p, u, e, f, p)-cycles. And by
Lemma 2 and Lemma 3, the minimum weight of a p-rooted 4-cycle-cover of KS is strictly
larger than 4(24z 2 + 3z).
We now show that such a (non-satisfiable) instance S cannot yield a graph KS forming
a distance-optimal inter-league tournament, but that a satisfiable instance S indeed does.
Just as we defined special 4-cycles rooted at p (e.g. (p, a, u, b, p)-cycles), we can similarly define 4-cycles rooted at q and r. In Lemma 3, the lower bound ILBq occurs when
the q-rooted 4-cycle-cover consists of 3k (q, u, b, a, q)-cycles, 2k (q, c, d, u, q)-cycles, and k
100

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

(q, f, u, e, q)-cycles, with total edge weight 3k(4z 2 + 4z) + 2k(4z 2 + 3z) + k(4z 2 + 2z) =
k(24z 2 + 20z). The lower bound ILBr occurs when the r-rooted 4-cycle-cover consists of 3k
(r, b, a, u, r)-cycles, 2k (r, d, u, c, r)-cycles, and k (r, e, f, u, r)-cycles, with total edge weight
3k(4z 2 + 4z) + 2k(4z 2 + 2z) + k(4z 2 + 3z) = k(24z 2 + 19z). We apply this information in
the following lemma in constructing our distance-optimal bipartite tournament.
Lemma 4. If S is satisfiable, then there exists a uniform
schedule (i.e., a solution to
P
BTTP as well as BTTP*) whose total travel distance is
ILBt = k2 (696z 2 + 408z  48) =
2
2
96k (2900k + 375k + 11).
Proof. From Lemma 1, if S is satisfiable, there exists a p-rooted 4-cycle-cover of KS with
exactly 3k (p, a, u, b, p)-cycles, 2k (p, u, c, d, p)-cycles, and k (p, u, e, f, p)-cycles. Consider
such a p-rooted 4-cycle-cover. We now relabel the teams in XS as follows:
First let {x0 , x1 , x2 }, {x3 , x4 , x5 }, . . . , {x9k3 , x9k2 , x9k1 } be the vertices of the 3k
(p, a, u, b, p)-cycles, where x3i  A, x3i+1  U , and x3i+2  B for all 0  i  3k  1.
Then let {x9k , x9k+1 , x9k+2 }, {x9k+3 , x9k+4 , x9k+5 }, . . . , {x15k3 , x15k2 , x15k1 } be the
vertices of the 2k (p, u, c, d, p)-cycles, where x3i  U , x3i+1  C, and x3i+2  D for all
3k  i  5k  1.
Finally, let {x15k , x15k+1 , x15k+2 }, {x15k+3 , x15k+4 , x15k+5 }, . . . , {x18k3 , x18k2 , x18k1 }
be the vertices of the k (p, u, e, f, p)-cycles, where x3i  U , x3i+1  E, and x3i+2  F
for all 5k  i  6k  1.
To explain our proof more clearly, we use this relabeling of the teams in XS , letting each vertex be xi for some 0  i  18k  1. We also relabel the teams in YS ,
so that {p0 , p1 , . . . , p6k1 } are the teams at p, {q0 , q1 , . . . , q6k1 } are the teams at q, and
{r0 , r1 , . . . , r6k1 } are the teams at r.
Since every team plays two games against each of the 18k teams in the other league, the
tournament has 36k time slots. We now build a double round-robin bipartite tournament
where the teams in each league play their home games in the same slots (i.e., the schedule
is uniform.) Specifically, each team in XS will play three consecutive home games followed
by three consecutive road games and repeat that pattern 6k times. Similarly, each team in
YS will play three consecutive road games followed by three consecutive home games and
repeat that pattern until the end of the tournament. Given the way we constructed the
edge weights, this is the natural way to construct a distance-optimal tournament, where
each team takes as few trips as possible.
In Lemma 3, we determined the value of ILBv for each v  YS = P  Q  R. we
have ILBpi = k(24z 2 + 3z), ILBqi = k(24z 2 + 20z), and ILBri = k(24z 2 + 19z), for all
0  i  6k  1. Therefore, LLBYS  6k2 (24z 2 + 3z) + 6k2 (24z 2 + 20z) + 6k2 (24z 2 + 19z) =
6k2 (72z 2 + 42z).
We now determine the value of ILBt for each t  XS = A B  C  D  E  F  U . Every
team t  XS plays a road game against each of the 18k teams in YS , with 6k teams located
at points p, q, and r. Team t must make at least 6k
3 = 2k trips to each of p, q, and r, since
the maximum length of a road trip is three games. Therefore, ILBt  2k(Dt,p +Dt,q +Dt,r ),
where Dt,v is the distance from t  XS to y  YS for all choices of t and y. Note that equality
can occur, specifically when the road trips of team t are scheduled in the most efficient way,
with each trip consisting of three consecutive games against three teams located at the same
point.
101

fiHoshino & Kawarabayashi

From Table 3, we determine that ILBt = 2k(Dt,p + Dt,q + Dt,r ) = 4k(4z 2 + z  1) for all
t  A  B  C  E. Similarly, ILBt = 4k(4z 2  1) for all t  D  F , and ILBt = 4k(3z 2 + 5z)
for all t  U . Thus, we have
LLBXS
 4k(4z 2 + z  1)(|A| + |B| + |C| + |E|) + 4k(4z 2  1)(|D| + |F |) + 4k(3z 2 + 5z)(|U |)

= 4k(4z 2 + z  1)(3k + 3k + 2k + k) + 4k(4z 2  1)(2k + k) + 4k(3z 2 + 5z)(6k)

= 36k2 (4z 2 + z  1) + 12k2 (4z 2  1) + 24k2 (3z 2 + 5z)

= k2 (264z 2 + 156z  48).

P
Therefore, T LB  LLBXS + LLBYS 
ILBt = k2 (264z 2 + 156z  48) + 6k2 (72z 2 +
2
2
42z) = k (696z + 408z  48). To complete the proof, it suffices to construct a tournament
for which each teams
distance matches its individual lower bound. This will
P total travel
prove that T LB =
ILBt = k2 (696z 2 + 408z  48).
For each 0  i  6k  1 and 0  j  6k  1, we determine the opponent of teams pi , qi ,
ri in time slots 6j + 1, 6j + 2, 6j + 3, 6j + 4, 6j + 5, and 6j + 6. In Table 4, we provide the
schedule of games in slots 6j + 1, 6j + 2, and 6j + 3, where the teams in XS play at home
and the teams in YS play on the road. In this table, the function f (i, j) is always reduced
modulo 18k, so that x18k+z := xz for all 0  z  18k  1.
Game
pi
qi
ri

6j + 1
x3(i+j)+0
x3(i+j)+1
x3(i+j)+2

6j + 2
x3(i+j)+1
x3(i+j)+2
x3(i+j)+0

6j + 3
x3(i+j)+2
x3(i+j)+0
x3(i+j)+1

Game
pi
qi
ri

6j + 1
x3(i+j)+0
x3(i+j)+2
x3(i+j)+1

6j + 2
x3(i+j)+1
x3(i+j)+0
x3(i+j)+2

6j + 3
x3(i+j)+2
x3(i+j)+1
x3(i+j)+0

Table 4: The left table lists the schedule of matches when i and j satisfy i + j 
{0, 1, . . . , 5k  1} (mod 6k), while the right table lists the schedule when i and j
satisfy i + j  {5k, 5k + 1, . . . , 6k  1} (mod 6k).
Fix i. By this construction, each team pi , qi , ri will play each of {x0 , x1 , . . . , x18k1 } on
the road exactly once. Now fix j. In time slot 6j + k (with 1  k  3), each team in XS
appears exactly once, playing a unique opponent from YS . Each teams schedule corresponds
to a rooted 4-cycle-cover. By our labeling scheme, the 4-cycle-cover of each team pi consists
of 3k (p, a, u, b, p)-cycles, 2k (p, u, c, d, p)-cycles and k (p, u, e, f, p)-cycles. Similarly, the
4-cycle-cover of each team qi consists of 3k (q, u, b, a, q)-cycles, 2k (q, c, d, u, q)-cycles, and
k (q, f, u, e, q)-cycles. Finally, the 4-cycle-cover of each team ri consists of 3k (r, b, a, u, r)cycles, 2k (r, d, u, c, r)-cycles, and k (r, e, f, u, r)-cycles. Therefore, each team in YS plays
their 6k road trips so that its total travel distance is equal to the minimum weight of a
4-cycle-cover rooted at that vertex, which by definition is equal to that teams individual
lower bound. Thus, we have constructed a schedule with LLBYS = 6k2 (72z 2 + 42z).
Now we construct the other half of our schedule, where the teams in YS play at home
and the teams in XS play on the road. This is a much simpler construction. For example,
one way to build this half of the schedule is to match each triplet of teams in XS (e.g.
{x0 , x1 , x2 }) with a triplet of teams from the same vertex in YS (e.g. {p0 , p1 , p2 }), and
have three consecutive slots of games between the two triplets all at the home venues of
102

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

the teams in league YS . Repeating this process, we can ensure that each of the 6k triplets
in XS play all 6k triplets of YS via three-game road trips. Thus, this schedule satisfies
LLBXS = k2 (264z 2 + 156z  48).
All that is required when putting the schedules together is to ensure the no-repeat rule,
which is a simple matter given all of the flexibility we have in constructing this half of the
tournament schedule.
Therefore, we have completed
then the bipartite tournament
Pour proof. 2If S is satisfiable,
2
with teams XS YS has T LB = ILBt = k (696z +408z48). Recalling that z = 20k+1,
we conclude that T LB = 96k2 (2900k2 + 375k + 11).
To illustrate the preceding proof, Table 5 gives a distance-optimal schedule for the case
k = 1, with 18 teams in each league. We just present the schedule for the teams in YS since
we can immediately derive the schedule for the teams in XS from this table. As always,
home games are marked in bold.
Game
p0
q0
r0
p1
q1
r1
..
.
p5
q5
r5

1
x0
x1
x2
x3
x4
x5
..
.
x15
x17
x16

2
x1
x2
x0
x4
x5
x3
..
.
x16
x15
x17

3
x2
x0
x1
x5
x3
x4
..
.
x17
x16
x15

4
x0
x6
x12
x1
x7
x13
..
.
x5
x11
x17

5
x2
x8
x14
x0
x6
x12
..
.
x4
x10
x16

6
x1
x7
x13
x2
x8
x14
..
.
x3
x9
x15

7
x3
x4
x5
x6
x7
x8
..
.
x0
x1
x2

8
x4
x5
x3
x7
x8
x6
..
.
x1
x2
x0

9
x5
x3
x4
x8
x6
x7
..
.
x2
x0
x1

10
x3
x9
x15
x4
x10
x16
..
.
x1
x7
x13

11
x5
x11
x17
x3
x9
x15
..
.
x2
x8
x14

12
x4
x10
x16
x5
x11
x17
..
.
x0
x6
x12

...
...
...
...
...
...
...
..
.
...
...
...

...
...
...
...
...
...
...
..
.
...
...
...

31
x15
x17
x16
x0
x1
x2
..
.
x12
x13
x14

32
x16
x15
x17
x1
x2
x0
..
.
x13
x14
x12

33
x17
x16
x15
x12
x0
x1
..
.
x14
x12
x13

34
x9
x15
x3
x10
x16
x4
..
.
x7
x13
x1

35
x11
x17
x5
x9
x15
x3
..
.
x8
x14
x2

36
x10
x16
x4
x11
x17
x5
..
.
x6
x12
x0

Table 5: A distance-optimal inter-league tournament with 18 teams in each league.
Having provided all of the lemmas, we can now prove the main theorem of this paper.
Theorem 1. BTTP and BTTP* are NP-complete.
Proof. Let S be an instance of 3-SAT with 2k clauses, and create sets XS and YS , with edge
weights as described in our construction. Consider an inter-league tournament between the
18k teams at XS and the 18k teams at YS (with one-third of the teams at each vertex of
YS ).
By Lemma 4, if S is satisfiable, then there exists a uniform double round-robin bipartite
tournament with total distance at most 96k2 (2900k2 + 375k + 11). By definition, this
tournament is a feasible solution to BTTP and BTTP*. We now prove the converse.
Let T (k) = 96k2 (2900k2 + 375k + 11). Consider an inter-league tournament P
between
these 36k teams with total travel distance at most T (k). By Lemma 4, T (k) =
ILBt .
Hence, every team t  XS  YS must travel the shortest possible distance of ILBt to play
all of their games. By Lemma 3, this implies that every team located at p  YS must travel
a distance of ILBp = k(24z 2 + 3z).
By Lemma 2, if each team p  YS travels a distance of k(24z 2 + 3z), then the graph KS
must contain exactly 3k (p, a, u, b, p)-cycles, 2k (p, u, c, d, p)-cycles, and k (p, u, e, f, p)-cycles.
And by Lemma 1, this occurs iff S is satisfiable.
Therefore, we have constructed a double round-robin bipartite tournament KS on 36k
teams with distance matrix DS for which the solutions to BTTP and BTTP* have total
103

fiHoshino & Kawarabayashi

distance  T (k) iff the instance S with 2k clauses is satisfiable. This establishes the desired
polynomial-time reduction from 3-SAT, proving the NP-hardness of BTTP and BTTP*.
Finally, we note that both problems are clearly in NP, since the distance traveled by the
teams can be calculated in polynomial time. Therefore, we conclude that BTTP and BTTP*
are NP-complete.
To illustrate the difference between BTTP and BTTP*, we provide a concrete illustration for the case n = 3. Let the teams be X = {x1 , x2 , x3 } and Y = {y1 , y2 , y3 }. In
Figure 4, the teams are located on the Cartesian plane, where x1 and x2 represent the same
point, y1 and y2 represent the same point, and the non-negative distances a, b, c satisfy the
Pythagorean equation a2 + b2 = c2 .

Figure 4: Illustration of BTTP for the case n = 3.

It is straightforward to show that ILBx1 = ILBx2 = ILBx3 = a + b + c, ILBy1 =
ILBy2 = 4a and ILBy3 = 2a + 2c. Hence, T LB  LLBX + LLBY  (3a + 3b + 3c) + (10a +
2c) = 13a + 3b + 5c.
In order for T LB = 13a + 3b + 5c, each of the teams in X must play a three-game
road stand with consecutive games against y1 and y2 , and each of the teams in Y must
play a three-game road stand with consecutive games against x1 and x2 . One can quickly
show that such a scenario is impossible, but that a nearly-best schedule can be achieved
by making either y1 or y2 take an extra trip, adding 2a to the total distance. Hence, the
solution to BTTP must have distance at least 15a + 3b + 5c.
For BTTP*, we have T LB  16a + 4b + 4c since LLBX = 4a + 4b + 2c and LLBY =
12a + 2c in a uniform schedule. For both problems, we justify optimality by presenting a
feasible tournament satisfying the stated tournament lower bounds. This is presented in
Table 6.
Team
x1
x2
x3
y1
y2
y3

1
y1
y2
y3
x1
x2
x3

2
y2
y3
y1
x3
x1
x2

3
y3
y1
y2
x2
x3
x1

4
y1
y2
y3
x1
x2
x3

5
y2
y3
y1
x3
x1
x2

6
y3
y1
y2
x2
x3
x1

Team
x1
x2
x3
y1
y2
y3

1
y1
y2
y3
x1
x2
x3

2
y3
y1
y2
x2
x3
x1

3
y2
y3
y1
x3
x1
x2

4
y1
y2
y3
x1
x2
x3

5
y3
y1
y2
x2
x3
x1

6
y2
y3
y1
x3
x1
x2

Table 6: Solutions to BTTP* and BTTP, with total distance 16a+4b+4c and 15a+3b+5c,
respectively.

104

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

For this example, the solution to BTTP requires 25 trips, one more trip than the solution
to BTTP*, yet the tournament lower bound is reduced by a + b  c > 0. As we will see
in the concluding section, there are many examples where the solution to the n = 3 BTTP
requires more than 24 trips.
To illustrate with an example for the case n = 6, consider a 12-team league with six
teams in each of X and Y . Place three points A, B, C equally spaced around a unit circle,
so that 4ABC is equilateral. Place {x1 , x2 } at A, {x3 , x4 } at B, and {x5 , x6 } at C. Now
place {y1 , y2 , . . . , y6 } at the centre of the circle. Then the best lower bound of ILByj = 6
occurs when yj plays two-game road trips against {x1 , x2 }, {x3 , x4 }, {x5 , x6 } in pairs rather
than 
in three-game road trips such as {x1 , x2 , x3 } and {x4 , x5 , x6 }, which has total distance
4 + 2 3 > 6. And clearly the best lower bound ILBxi = 4 occurs when xi plays three-game
road trips against the teams in Y , making just two trips to the centre of the circle.
Table 7 provides an distance-optimal schedule which is uniform, thus proving that for
this simple example, the solution to BTTP is the same as BTTP*. However, note that
unlike our proof of Theorem 1, in this 12-team scenario, the best schedule requires 102 total
trips, six more than the fewest possible number of total trips.
x1
x2
x3
x4
x5
x6
y1
y2
y3
y4
y5
y6

1
y1
y2
y3
y4
y5
y6
x1
x2
x3
x4
x5
x6

2
y2
y1
y4
y3
y6
y5
x2
x1
x4
x3
x6
x5

3
y1
y2
y3
y4
y5
y6
x1
x2
x3
x4
x5
x6

4
y2
y3
y1
y5
y6
y4
x3
x1
x2
x6
x4
x5

5
y3
y1
y2
y6
y4
y5
x2
x3
x1
x5
x6
x4

6
y4
y3
y6
y5
y2
y1
x6
x5
x2
x1
x4
x3

7
y3
y4
y5
y6
y1
y2
x5
x6
x1
x2
x3
x4

8
y4
y5
y6
y1
y2
y3
x4
x5
x6
x1
x2
x3

9
y5
y6
y4
y2
y3
y1
x6
x4
x5
x3
x1
x2

10
y6
y4
y5
y3
y1
y2
x5
x6
x4
x2
x3
x1

11
y5
y6
y1
y2
y3
y4
x3
x4
x5
x6
x1
x2

12
y6
y5
y2
y1
y4
y3
x4
x3
x6
x5
x2
x1

Table 7: Solution to BTTP and BTTP* for a scenario with n = 6.
Having provided simple illustrations for n = 3 and n = 6, we now analyze BTTP for two
professional sports leagues, namely the Nippon Professional Baseball league (with n = 6)
and the National Basketball Association (with n = 15).

4. Japanese Baseball
Nippon Professional Baseball (NPB) is Japans largest professional sports league. In the
NPB, the teams are split into two leagues of six teams, with each team playing 120 intraleague and 24 inter-league games during the regular season. The intra-league problem was
analyzed recently by the authors (Hoshino & Kawarabayashi, 2011c), where we developed
a multi-round generalization of the TTP based on Dijkstras shortest path algorithm and
applied it to produce a distance-optimal schedule reducing the total travel distance by over
60000 kilometres (a 25% reduction) as compared to the 2010 NPB intra-league schedule
(Hoshino & Kawarabayashi, 2011d). Given that Japan is a small island country, a 60000
kilometre reduction represents a significant amount.
105

fiHoshino & Kawarabayashi

We now consider the inter-league problem, where the six teams in the NPB Pacific
League each play four games against all six teams in the NPB Central League, with one
two-game set played at the home of the Pacific League team, and the other two-game set
played at the home of the Central League team. All inter-league games take place during
a five-week stretch between mid-May and mid-June, with no intra-league games occurring
during that period. Thus, the NPB inter-league scheduling problem is precisely BTTP, for
the case n = 6.

Figure 5: Location of the 12 teams in the NPB.
The locations of each teams home stadium is marked in Figure 5. For readability,
we label each team as follows: the Pacific League teams are p1 (Fukuoka), p2 (Orix), p3
(Saitama), p4 (Chiba), p5 (Tohoku), p6 (Hokkaido), and the Central League teams are c1
(Hiroshima), c2 (Hanshin), c3 (Chunichi), c4 (Yokohama), c5 (Yomiuri), and c6 (Yakult).
The actual 12  12 NPB distance matrix is provided in Appendix B.
We now solve BTTP for the NPB, producing an inter-league schedule requiring 42950
kilometres of total travel, representing a 16% reduction compared to the 51134 kilometres
traveled by these teams during the 2010 inter-league schedule (Hoshino & Kawarabayashi,
2011a). To accomplish this, we present two powerful reduction heuristics. To motivate
these heuristics, we first require several key definitions.
For each t  X  Y , let St be the set of possible schedules that can be played by team t
satisfying the at-most-three and each-venue constraints. Let t  St be a possible schedule
for team t. For each t , we just list the opponents of the six road sets, and ignore the home
sets, since we can determine the total distance traveled by team t just from the road sets.
To give an example, below is a feasible schedule x1  Sx1 for the case n = 6:
x1

1
y1

2
y6

3
y

4
y

5
y3

6
y5

7
y4

8
y

9
y

10
y

11
y2

12
y

In the following team schedule x1 , each y represents a home set played by x1 against a
unique opponent in Y . Note that x1 satisfies the at-most-three and each-venue constraints.
Let  = (x1 , x2 , . . . , xn , y1 , y2 , . . . , yn ), where t  St for each t  X  Y . Since
road sets of X correspond to home sets of Y and vice-versa, it suffices to list just the time
slots and opponents of the n road sets in each t , since we can then uniquely determine the
full schedule of 2n sets for every team t  X Y , thus producing an inter-league tournament
106

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

schedule . We note that  is a feasible solution to BTTP iff each team plays a unique
opponent in every time slot, and no team schedule t violates the no-repeat constraint. In
this section, we will frequently refer to team schedules t and tournament schedules . From
the context it will be clear whether the schedule is for an individual team t  X  Y , or for
all 2n teams in X  Y .
As before, define ILBt to be the individual lower bound of team t, the minimum possible
distance that can be traveled by team t in order to complete its 2n sets.
For each t  St , let d(t ) be the integer for which d(t )+ILBt equals the total distance
traveled by team t when playing the schedule t . By definition, d(t )  0.
For each  = (x1 , . . . , xn , y1 , . . . , yn ), define
X
d() =
d(t ).
tXY

P

Since
ILBt is fixed, the optimal solution to BTTP is the schedule  for which d() is
minimized. This is the motivation for the function d().
For each subset St  St , define the lower bound function
B(St ) = min d(t ).
t St

If St = St , then B(St ) = 0 by the definition of ILBt . For each subset St , we define |St | to
be its cardinality.
For example, consider the n = 6 instance in Table 7, where we located the six teams
in league X so that two teams were assigned to each vertex of an equilateral triangle. As
mentioned at the end of Section 3, ILBy1 = 6, with equality occurring iff y1  Y plays
two-set road trips against {x1 , x2 }, {x3 , x4 }, and {x5 , x6 }. Now let Sy1 be the restriction
of Sy1 to the subset of schedules where y1 starts with three consecutive roadsets against
teams x1 , x2 , and x3
. Then any such
schedule must have total distance  4+ 2 3, implying

that B(Sy1 ) = 4 + 2 3  ILBy1 = 2 3  2 > 0.
If n is a multiple of 3, we define for each team the set R3t as the subset of schedules in
St for which the n road sets occur in n3 blocks of three (i.e., team t takes n3 three-set road
trips). For example, in Table 5 (which has n = 18), every team t plays a schedule t  R3t .
Finally, we define  to be a global constraint that fixes some subset of matches, and St
to be the subset of schedules in St which are consistent with that global constraint. For
example, if  is the simple constraint that forces y2 to play against x1 at home in time slot
3, then Sx1 would only consist of the team schedules where slot 3 is a road set against y2 .
If  is a much more complex global constraint (e.g. where the number of fixed matches is
large), then each |St | will be significantly less than |St |.
To illustrate this concept, consider the above n = 6 instance and the global constraint
 that y1 starts with three consecutive road sets against teams x1 , x2 , and x3 (in that
order). One can show that there are only 34 valid home-road patterns in Sy1 , including
RRR-HHH-RRR-HHH and RRR-H-R-HH-R-HH-R-H. For each home-road pattern, there
are 3! = 6 ways to assign {x4 , x5 , x6 } to the last three road sets. Thus, |Sy1 | = 343! = 204,
which is significantly less than |Sy1 | which can be shown to equal 616  6! = 443520.
This simple notion of global constraints inspires our first result, a powerful reduction
heuristic that drastically cuts down the computation time.
107

fiHoshino & Kawarabayashi

Proposition 1. Let M be a fixed positive integer. For any global constraint , define for
each t  X  Y ,
(
)
X



Zt = t  St : d(t )  M + B(St ) 
B(Su ) .
uXY

If  = (x1 , . . . , xn , y1 , . . . , yn ) is a feasible tournament schedule consistent with  so
that d()  M , then for each t  X  Y , team ts schedule t appears in Zt .
Proof. Consider all tournament schedules consistent with . If there is no  with d()  M ,
then there
to prove. So assume some
. Letting
P schedule  satisfies
P d()  M
P is nothing
 ), so that
 ), we have M  d() =
d(
)

B(S
Q =
B(S
u
u
u
uXY
uXY
uXY
M  Q.
If t  Zt , then Zt  St implying that d(t )  B(St ). Now suppose there exists
some v  X  Y with v 
/ Zv . Since v is consistent with , v  Sv and d(v ) >


M + B(Sv )  Q  B(Sv ). This is a contradiction, as
X
d() = d(v ) +
d(u )
uXY,u6=v

> (M + B(Sv )  Q) +
= (M +

B(Sv ) 

X

B(Su )

uXY,u6=v

Q) + (Q  B(Sv )) = M.

Hence, if  = (x1 , . . . , xn , y1 , . . . , yn ) is a feasible tournament schedule consistent
with  so that d()  M , then t  Zt for all t  X  Y .

Proposition 1 shows how to perform some reduction prior to propagation, and may
be applicable to other problems. To apply this proposition, we will reduce BTTP to k
scenarios where in each scenario the six home sets for four of the Pacific League teams are
pre-determined. Expressing these scenarios as the global constraints 1 , 2 , . . . , k , each i
fixes 24 of the 72 total matches.
For every i , we determine Zcj for the Central League teams and by setting a low
threshold M , we show that each |Zcj | is considerably smaller than |Scj |, thus reducing the
search space to an amount that can be quickly analyzed. From there, we run a simple
six-loop that generates all 6-tuples (c1 , c2 , c3 , c4 , c5 , c6 ) that can appear in a feasible
schedule  with d()  M . By Proposition 1, each cj  Zcj for 1  j  6. From this list
of possible 6-tuples, we can quickly find the optimal schedule  which corresponds to the
solution to BTTP.
We now present a result that works only for the case n = 6, when two teams from one
league are located quite far from the other 10 teams, forcing the distance-optimal schedule
 to have a particular structure.
Proposition 2. Let M be a fixed positive integer, and define St = {t  St : d(t )  M }.
x
Suppose there exist two teams xi , xj  X = {x1 , x2 , . . . , x6 } for which Sxi  R3xi , Sxj  R3 j ,

and for each team yk  Y , every schedule in Syk has the property that yk plays their road
sets against xi and xj in two consecutive time slots. If  = (x1 , . . . , x6 , y1 , . . . , y6 ) is a
feasible tournament schedule with d()  M where each t  St , then the team schedules
108

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

xi and xj both have the home-road pattern HH-RRR-HH-RRR-HH; moreover, each teams
six home slots must have the following structure for some permutation (a, b, c, d, e, f ) of
{1, 2, 3, 4, 5, 6}:
xi
xj

1
ya
yb

2
yb
ya

3
y
y

4
y
y

5
y
y

6
yc
yd

7
yd
yc

8
y
y

9
y
y

10
y
y

11
ye
yf

12
yf
ye

Proof. We first note that if xi and xj have the above structure, they satisfy all the given
x
conditions since xi  R3xi , xj  R3 j , and every team yk  Y plays road sets against xi
and xj in two consecutive time slots. For example, yd plays road sets against xj in slot 6
and against xi in slot 7. We now prove that xi and xj must have this structure.
For each team xt  X and time slot s  [1, 12], define O(xt , s) to be the opponent of
team xt in set s. We define O(xt , s) only when xt is playing at home; for the sets when xt
plays on the road, O(xt , s) is undefined.
Since xi  Sxi and Sxi  R3xi , there are four possible cases to consider:
(1) xi plays set 1 at home, and sets 2 to 4 on the road.
(2) xi plays sets 1 and 2 at home, and sets 3 to 5 on the road.
(3) xi plays sets 1 to 3 at home, and sets 4 to 6 on the road.
(4) xi plays sets 1 to 3 on the road, and set 4 at home.
We examine the cases one by one. In each, suppose there exists a feasible schedule 
satisfying all the given conditions. We finish with case (2).
In (1), let O(xi , 1) = ya . Then O(xj , 2) = ya , since ya must play road sets against xi
x
and xj in consecutive time slots. Since xj  R3 j and xj plays at home in set 2, xj must
also play at home in set 1. Thus, O(xj , 1) = yb for some yb , which forces O(xi , 2) = yb .
This is a contradiction as xi plays set 2 on the road.
In (3), let O(xi , 1) = ya , O(xi , 2) = yb , and O(xi , 3) = yc . Then O(xj , 2) = ya and
O(xj , 4) = yc . Either O(xj , 1) = yb or O(xj , 3) = yb . In either case, we violate the at-mostx
three constraint or the condition that xj  R3 j .
In (4), team xi starts with a three-set road trip. In order to satisfy the at-most-three
constraint, xi must have the pattern RRR-HHH-RRR-HHH. Then this reduces to case
(3), as we can read the schedule  backwards, letting O(xi , 12) = ya , O(xi , 11) = yb ,
O(xi , 10) = yc , and applying the argument in the previous paragraph.
In (2), let O(xi , 1) = ya and O(xi , 2) = yb . Then O(xj , 2) = ya and O(xj , 1) = yb . If
O(xj , 3) = yc for some yc , then O(xi , 4) = yc , forcing xi to play a single road set in slot 3.
Thus, xj must play on the road in set 3, and therefore also in sets 4 and 5. Hence, both
xi and xj start with two home sets followed by three road sets. Since this is the only case
remaining, by symmetry xi and xj must end with two home sets preceded by three road
sets. Thus, these two teams must have the pattern HH-RRR-HH-RRR-HH.
In order for each yk to play their road sets against xi and xj in two consecutive time
slots, we must have O(xi , 6) = O(xj , 7), O(xi , 7) = O(xj , 6), O(xi , 11) = O(xj , 12), and
O(xi , 12) = O(xj , 11). This completes the proof.
109

fiHoshino & Kawarabayashi

We will use Proposition 2 to solve BTTP, since teams p5 and p6 are located quite far
from the other ten teams (see Figure 5). This heuristic of isolating two teams and finding
its common structure significantly reduces the search space and enables us to solve BTTP
for the 12-team NPB in hours rather than weeks.
By applying these results, we do not require weeks of computation time on multiple
processors. With these two heuristics, BTTP can be solved in less than ten hours on a
single laptop. All of the code was written in Maple and compiled using Maplesoft 13 using
a single Toshiba laptop under Windows with a single 2.10 GHz processor and 2.75 GB
RAM.
Table 8 presents an inter-league tournament schedule  that is a solution to BTTP with
d() = (0 + 4 + 0 + 0 + 1 + 1) + (51 + 9 + 31 + 58 + 19 + 13) = 187.
p1
p2
p3
p4
p5
p6
c1
c2
c3
c4
c5
c6

1
c3
c5
c4
c2
c1
c6
p5
p4
p1
p3
p2
p6

2
c5
c3
c2
c4
c6
c1
p6
p3
p2
p4
p1
p5

3
c1
c2
c6
c5
c4
c3
p1
p2
p6
p5
p4
p3

4
c3
c1
c5
c4
c6
c2
p2
p6
p1
p4
p3
p5

5
c2
c3
c4
c6
c5
c1
p6
p1
p2
p3
p5
p4

6
c1
c6
c3
c5
c2
c4
p1
p5
p3
p6
p4
p2

7
c6
c1
c5
c3
c4
c2
p2
p6
p4
p5
p3
p1

8
c2
c4
c1
c6
c3
c5
p3
p1
p5
p2
p6
p4

9
c4
c5
c3
c1
c2
c6
p4
p5
p3
p1
p2
p6

10
c5
c6
c2
c3
c1
c4
p5
p3
p4
p6
p1
p2

11
c6
c4
c1
c2
c5
c3
p3
p4
p6
p2
p5
p1

12
c4
c2
c6
c1
c3
c5
p4
p2
p5
p1
p6
p3

Table 8: Solution to BTTP with total distance 42950 km.
In Table 8, we see that only seven of the twelve teams satisfy t  R3t , namely c1 and
all six of the Pacific League teams. However, every Central League team in this schedule
plays road sets against p5 and p6 in consecutive time slots. This explains why each d(cj ) in
 is small.
P
We claim that  is an optimal solution, with total distance d() + ILBt = 187 +
42763 = 42950. To prove this, we set M = 187. Define St = {t  St : d(t )  M }, from
which we determine that Sp5  R3p5 and Sp6  R3p6 .
Define Tci  Sci to be the subset of schedules for which ci does not play their road sets
against p5 and p6 in two consecutive time slots. From this, we can show that B(Tc3 ) = 153,
and that B(Tcj ) > M = 187 for j  {1, 2, 4, 5, 6}. We claim that if  satisfies d()  187,
then cj 
/ Tcj for all 1  j  6.
It suffices to prove the claim for j = 3. There are 144 schedules in Tc3 , all of which
belong to the set R3c3 . For example, one such schedule c3 is
c3

1
p

2
p2

3
p1

4
p6

5
p

6
p

7
p

8
p3

9
p4

10
p5

11
p

12
p

Suppose there exists a tournament schedule  with d()  187 and c3  Tc3 . There are
nine possible home-road patterns for p5  R3p5 (e.g. HHH-RRR-H-RRR-HH and H-RRRHHH-RRR-HH), each of which gives rise to 6! = 720 possible orderings for the six home
110

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

sets. Thus, there are 9  720 = 6480 ways we can select the time slots and opponents for the
six home sets in p5 . Similarly, there are 6480 ways to do this for p6 . A simple Maplesoft
procedure shows that only 140 of the 64802 possible pairs (p5 , p6 ) are consistent with at
least one c3  Tc3 .
For each of these 140 cases, define the global constraints 1 , 2 , . . . , 140 , obtained from
fixing the twelve home sets in {p5 , p6 }. For each k , define for each j  {1, 2, 4, 5, 6} the
set Zcj = {cj  Scjk : d(cj )  M  B(Tc3 ) = 34}. Then we run our six-loop to compute
all possible 6-tuples (c1 , c2 , . . . , c6 ) satisfying the given conditions with c3  Tc3 and
cj  Zcj for j 6= 3. Within twenty minutes, Maplesoft solves all 140 cases and returns no
feasible 6-tuples that can appear in a schedule  with d()  187.
Therefore, in , each cj must play road sets against p5 and p6 in consecutive time slots.
Thus, teams p5 and p6 satisfy the conditions of Proposition 2. Hence, the home-road pattern
of p5 and p6 in  must be HH-RRR-HH-RRR-HH.
Without loss, assume that p5 plays a home set against c1 within the first six time slots;
otherwise we can read the schedule backwards by symmetry. Thus, there are 6!2 = 360
ways to assign opponents to the six home sets in p5 . By Proposition 2, each of these 360
arrangements uniquely determines the six home sets in p6 .
A short calculation shows that in order for d()  M = 187, teams p1 and p3 must also
play their six road sets in two blocks of three. In other words, p1  R3p1 and p3  R3p3 . As
mentioned earlier, there are 9  6! possible ways to select the six home sets for each of p1
and p3 .
Thus, there are 360  (9  6!)  (9  6!) ways we can select the 24 home sets played
by the teams in {p1 , p3 , p5 , p6 }. We eliminate all scenarios in which some pi and pj play
against some ck in the same time slot. For the possibilities that remain, we create a global
constraint to apply Proposition 1.
Let {1 , 2 , . . . , k } be the complete set of global constraints derived from the above
process, where each i fixes 24 of the 72 matches, corresponding to the home sets of
{p1 , p3 , p5 , p6 }. The reduction heuristic of Proposition 1 allows us to quickly verify the
existence of feasible tournament schedules  consistent with i for which d()  M .
To explain this procedure, let us illustrate with the inter-league schedule in Table 8. Let
 be the constraint that fixes the 24 home sets of teams p1 , p3 , p5 , and p6 in that table.
Then Sc5 , defined as the subset of schedules in Sc5 consistent with , consists only of team
schedules c5 for which c5 plays road sets against p1 in slot 2, p3 in slot 7, p5 in slot 11, and
p6 in slot 12.
We find that there are only 11 schedules c5  Sc5 with d(c5 )  M that are consistent
with . Furthermore, each d(c5 )  {19, 41, 46, 48}, implying that B(Sc5 ) = 19. Similarly,
we can calculate the other values of B(Scj ).
P
P
We find that 6j=1 B(Spj ) = 0 and 6j=1 B(Scj ) = 51 + 9 + 31 + 58 + 19 + 13 = 181,
implying that Zc5 = {c5  Sc5 : d(c5 )  187 + 19  181 = 25}. Hence, Zc5 reduces to just
the two schedules with d(c5 ) = 19, including the team schedule c5 in Table 8.
By Proposition 1, any schedule  consistent with  satisfying d()  M must have the
property that t  Zt for each team t. Since each |Zcj | is small, the calculation is extremely
fast. Of course, if any |Zcj | = 0, then no schedule  can exist.
This algorithm, based on Propositions 1 and 2, runs in 34716 seconds in Maplesoft (just
under 10 hours). Maplesoft generates zero inter-league schedules with d() < 187 and 14
111

fiHoshino & Kawarabayashi

inter-league schedules with d() = 187, including the schedule given in Table 8. Since we
made the assumption that p5 plays a home set against c1 within the first six time slots,
there are actually twice as many distance-optimal schedules by reading each schedule 
backwards.
In each of the 28 distance-optimal schedules , we find that (d(p1 ), d(p2 ), . . . , d(p6 )) =
(0, 4, 0, 0, 1, 1) and (d(c1 ), d(c2 ), . . . , d(c6 )) = (51, 9, 31, 58, 19, 13).
Therefore, we have proven that Table 8 is an optimal inter-league schedule for the NPB,
reducing the total travel distance by 8184 kilometres, or 16.0%, compared to the 2010 NPB
schedule.

5. American Basketball
The National Basketball Association (NBA) is one of the worlds most lucrative sports
leagues, with over four billion dollars in annual revenue, and an average franchise value
of 400 million dollars. There are 15 teams in the Western Conference and 15 teams in
the Eastern Conference. Every NBA team plays 82 regular-season games, of which 30 are
inter-league (with one home game and one away game against each of the 15 teams from
the other conference.) The geographic location of each team is provided in Figure 6.

Figure 6: Map of the NBAs 15 Western Conference teams and 15 Eastern Conference
teams.
Given that NBA teams play inter-league games, we consider BTTP for this league,
where we attempt to find a distance-optimal inter-league tournament. In this theoretical
problem, we will assume that all inter-league games take place during a consecutive stretch
in the regular season, as is done currently in the Japanese NPB. We will also enforce all the
constraints of BTTP, including no team having a home stand or road trip lasting more than
3 games. We note that these strict conditions are not part of the NBA scheduling requirement, as evidenced by the San Antonio Spurs playing 6 consecutive home games followed
immediately by 8 consecutive road games during the 2009-10 regular season. Furthermore,
we will require that our inter-league schedule be compact, i.e., having each team play one
game in each time slot. Of course, this compactness condition is not part of a typical NBA
schedule, as one team might play five games by the time another team has played just two.
112

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

We determine the 30  30 NBA distance matrix from an online website1 that lists the
flight distance (in statute miles) between each pair of major cities in North America. This
matrix is found in Appendix B.
Unlike the 12-team NPB where we could solve BTTP, it appears highly unlikely that we
can solve this problem for the 30-team NBA. Nonetheless, we can generate
an inter-league
P
tournament whose total distance is close to the trivial lower bound of
ILBt , by grouping
each leagues fifteen teams into five triplets so that the travel distance of each team t is
extremely close to ILBt , the minimum weight of a t-rooted 4-cycle-cover. From this, we
construct a uniform tournament, i.e., a feasible solution to BTTP*, where each Western
Conference team alternates by playing three away games followed by three home games.
Given the geographic location of the 30 teams, it is easy to show that each teams ILBt
occurs when playing the fifteen away games in five groups of three. We note that this is not
always the case; to give a concrete example, consider a variant of the scenario we presented
at the end of Section 3. Let points X, Y, Z be equally spaced around a unit circle, so that
4XY Z is equilateral. Place {e1 , e2 } at X, {e3 , e4 } at Y , and {e5 , e6 } at Z. Now place
{w, e7 , e8 , . . . , e15 } at the centre of the circle. Then the best lower bound of ILBw = 6
occurs when w plays two-game road trips against {e1 , . . . , e6 } in pairs rather than
 threegame road trips like {e1 , e2 , e3 } and {e4 , e5 , e6 }, which has total distance 4 + 2 3 > 6.
However, for the NBA distance matrix, each teams ILBt occurs when that team has five
road trips, where in each trip that team plays three opponents located close to each other.
Thus, for each team wi , there exists some permutation  for which the lower bound
ILBwi is attained by playing away games against the fifteen Eastern Conference teams in
the order e(1) , e(2) , . . . , e(15) . Note that for this permutation, the total distance traveled
by wi is
ILBwi =

5
X
{Dwi ,e(3j2) + De(3j2) ,e(3j1) + De(3j1) ,c(3j) + De(3j) ,wi }.
j=1

The five triplets {{e(3j2) , e(3j1) , e(3j) } : j = 1, 2, . . . , 5} can be permuted in 5! ways
without changing the total distance. Also, within each triplet, we can change the order of the
first and third element while retaining the same total. Thus, we can compute ILBwi from
15!
a simple enumeration of 5!2
5 cases, which can be done in minutes using Maplesoft. From
P
this, we calculate P
ILBt for each team t, giving LLBW  tW ILBt = 251795. Similarly,
we have LLBE  tE ILBt = 266137, and so T LB  LLBW + LLBE  517932.
In nearly every case, the bounds ILBwi and ILBei are attained by selecting the road
trips as indicated in Figure 7, corresponding to the minimum-weight triangle packing for
each league. For example, in this minimum-weight triangle packing, every Eastern Conference team makes just one trip to the northwest, to play Portland, Golden State, and
Sacramento in some order. Similarly, every Western Conference team makes just one trip
to the southeast, to play Atlanta, Orlando, and Miami in some order. We note the natural
connection between minimum-weight triangle packings and minimum-weight 4-cycle-covers,
remarking that the former generates an approximation for the latter.
Re-label the fifteen Western Conference teams so that five triplets occur side-by-side (i.e.,
w1 is Portland, w2 is Golden State, w3 is Sacramento), and similarly re-label the Eastern
1. http://www.savvy-discounts.com/discount-travel/JavaAirportCalc.html

113

fiHoshino & Kawarabayashi

Figure 7: The minimum-weight triangle packing for the 30 NBA teams.

Conference teams. Similar to our construction in Table 5, we build a tournament from the
5  5 = 25 pairs of inter-league triplets, where each team from one triplet plays the three
teams from the other triplet in three consecutive time slots (e.g., e1 plays {w1 , w2 , w3 }, e2
plays {w2 , w3 , w1 } and e3 plays {w3 , w1 , w2 }.) This construction produces a schedule where
the Eastern Conference teams travel 286683 miles and the Western Conference teams travel
258443 miles, for a total of 545126 miles.
We can improve this bound slightly by noting that the away teams are not forced to
travel according to the triplets given in Figure 7. Specifically, suppose we are considering the
road trips for the Eastern Conference. Let the triplets be {{e(3j2) , e(3j1) , e(3j) } : j =
1, 2, . . . , 5}, for some permutation . Then each triplet {e(3j2) , e(3j1) , e(3j) } travels west
for three-game road trips against each of {w1 , w2 , w3 }, {w4 , w5 , w6 }, . . . , {w13 , w14 , w15 }. Ex15!
amining all 5!2
5 non-equivalent possibilities for , we show that the best permutation is
 = (1, 6, 12, 2, 8, 13, 3, 7, 11, 4, 10, 14, 5, 9, 15), so that the teams in {e1 , e6 , e12 } play their
first three games on the road against each of {w1 , w2 , w3 }, the teams in {e2 , e8 , e13 } play
their first three games on the road against each of {w4 , w5 , w6 }, and so on. In this optimal
schedule, the Eastern Conference teams travel a total of 280294 miles. Similarly, in the best
possible case, the Western Conference teams travel a total of 257497 miles.
From this, we produce Table 9, a uniform inter-league tournament with total P
distance
280294 + 257497 = 537791 miles, just 3.8% more than the trivial lower bound of
ILBt .
The labeling of the 30 teams (e.g. PT = Portland Trailblazers, MB = Milwaukee Bucks) is
given in Appendix B.
P
While we are certain that the trivial lower bound of
ILBt cannot be achieved for
either the BTTP or BTTP*, we conjecture that the 3.8% figure can be reduced using more
sophisticated techniques. But how close can we get? We leave this as a challenge for the
interested reader.
Problem 1. Determine better (best?) bounds for BTTP and BTTP*, for the 30  30 NBA
distance matrix.
114

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

PT
GW
SK
LC
LL
PS
UJ
DN
OT
SS
DM
HR
MT
MG
NH

1
MB
TR
NK
IP
WW
BC
OM
CC
AH
CU
DP
PS
CB
NN
MH

2
IP
CC
NN
CU
CB
NK
MH
DP
OM
MB
TR
WW
PS
BC
AH

3
CU
DP
BC
MB
PS
NN
AH
TR
MH
IP
CC
CB
WW
NK
OM

4
MB
CC
CB
NN
TR
MH
DP
OM
IP
BC
WW
AH
PS
CU
NK

5
CB
MB
CC
MH
NN
TR
PS
DP
WW
IP
BC
NK
OM
AH
CU

PT
GW
SK
LC
LL
PS
UJ
DN
OT
SS
DM
HR
MT
MG
NH

16
BC
WW
IP
NK
CU
AH
MB
CO
TR
NN
MH
OM
CC
DP
PS

17
IP
BC
WW
AH
NK
CU
CC
MB
MH
TR
NN
PS
CO
OM
DP

18
WW
IP
BC
CU
AH
NK
CO
CC
NN
MH
TR
DP
MB
PS
OM

19
NK
PS
AH
NN
MB
MH
CC
CO
DP
BC
WW
IP
CU
OM
TR

20
NN
WW
OM
BC
CU
AH
DP
PS
TR
NK
CO
MB
IP
MH
CC

6
CC
CB
MB
TR
MH
NN
OM
PS
BC
WW
IP
CU
DP
NK
AH

21
BC
CO
MH
NK
IP
OM
TR
WW
CC
NN
PS
CU
MB
AH
DP

7
AH
MB
TR
OM
NK
CC
PS
IP
WW
MH
CU
BC
NN
DP
CB

22
DP
PS
OM
IP
BC
WW
CU
AH
MB
CC
CO
MH
NK
TR
NN

8
OM
IP
CC
MH
NN
DP
WW
CU
CB
AH
MB
NK
BC
TR
PS

9
MH
CU
DP
AH
BC
TR
CB
MB
PS
OM
IP
NN
NK
CC
WW

10
CU
NK
AH
CC
MB
CB
TR
MH
DP
PS
OM
WW
NN
IP
BC

11
AH
CU
NK
CB
CC
MB
NN
TR
OM
DP
PS
BC
MH
WW
IP

23
OM
DP
PS
WW
IP
BC
NK
CU
CO
MB
CC
NN
AH
MH
TR

24
PS
OM
DP
BC
WW
IP
AH
NK
CC
CO
MB
TR
CU
NN
MH

25
TR
NN
WW
DP
OM
PS
CU
BC
MB
CC
NK
AH
MH
CO
IP

26
CC
NK
CO
TR
MH
WW
IP
NN
CU
DP
BC
OM
AH
PS
MB

12
NK
AH
CU
MB
CB
CC
MH
NN
PS
OM
DP
IP
TR
BC
WW

27
DP
BC
PS
CC
AH
CO
MB
NK
IP
TR
NN
MH
OM
WW
CU

13
WW
MH
MB
CB
TR
IP
BC
OM
NK
PS
AH
DP
CC
CU
NN

28
TR
NN
MH
DP
PS
OM
BC
IP
CU
NK
AH
CO
WW
MB
CC

14
CB
OM
IP
PS
CC
CU
NK
AH
NN
WW
MH
TR
DP
MB
BC

15
PS
AH
CU
WW
DP
MB
NN
MH
BC
CB
OM
CC
TR
IP
NK

29
MH
TR
NN
OM
DP
PS
WW
BC
AH
CU
NK
CC
IP
CO
MB

30
NN
MH
TR
PS
OM
DP
IP
WW
NK
AH
CU
MB
BC
CC
CO

Table 9: A close-to-optimal solution for the NBA BTTP*.

6. Conclusion
In this paper, we introduced the Bipartite Traveling Tournament Problem and applied it to
two professional sports leagues in Japan and North America, illustrating the richness and
complexity of bipartite tournament scheduling.
In Section 4, we introduced two heuristics that enabled us to solve BTTP for the n = 6
NPB. While Proposition 2 is only applicable for certain 12-team configurations satisfying
a specific geometric property, we note that Proposition 1 is a general technique that can
be applied to other scheduling problems. Our method of reduction prior to propagation
breaks a complex problem into a large number of scenarios, and sets up each scenario as a
global constraint to reduce the search space. We are confident that Proposition 1 can be
applied to more complicated problems in sports scheduling.
In Section 5, we determined an algorithm that produced an approximate solution to
BTTP for the n = 15 NBA. By finding minimum-weight rooted 4-cycle-covers, we determined a trivial lower bound to BTTP, from which our method of creating a uniform schedule
based on the minimum-weight triangle packing generated a close-to-optimal feasible solution. For the NBA inter-league problem, this process produced an optimality gap of just
3.8%. We are hopeful that these ideas can be abstracted and refined further, leading to
more powerful tools to tackle even harder problem instances.
115

fiHoshino & Kawarabayashi

Perhaps there are other sports leagues for which BTTP is applicable, such as in professional hockey and football. We can also expand our analysis to model tripartite and
multipartite tournament scheduling, where a league is divided into three or more conferences. A specific example of this is the newly-created Super 15 Rugby League, consisting
of five teams from South Africa, Australia, and New Zealand. In addition to intra-country
games, each team plays four games (two home and two away) against teams from each
of the other two countries. It would be interesting to see whether we can determine the
distance-optimal tripartite tournament schedule using the methods developed in this paper.
We conclude by motivating several interesting questions, including those dealing with
geometric probability and extremal combinatorics, and leave them as open problems for the
interested reader.
Our solution to the non-uniform BTTP required 10 hours of computations. Furthermore,
we were only able to solve BTTP by applying Proposition 2, whose requirements would not
hold for a randomly-selected 12  12 distance matrix. As a result, we require a more
sophisticated technique that improves upon our two heuristics, perhaps using methods in
constraint programming and integer programming, such as a hybrid CP/IP. We wonder
if there exists a general algorithm that would solve BTTP given any distance matrix, for
small values of n such as n = 6, n = 7, and n = 8. We pose this as an open problem.
Problem 2. Develop a computational procedure (or algorithm) that can routinely solve
BTTP and BTTP* instances with n  6.
At the end of Section 3, we presented a simple example (see Figure 4) to illustrate the
difference between BTTP and BTTP* for the case n = 3. We located the six points to
form two sets of Pythagorean triangles, and showed that the solutions to the two problems
had total distance 15a + 3b + 5c and 16a + 4b + 4c, respectively. If (a, b, c) = (3, 4, 5),
then the tournament lower bounds are 82 and 84, respectively. In other words, by relaxing
the uniformity requirement, we can reduce the optimal travel distance from 84 to 82, an
improvement of 2.38%. Using elementary calculus, we can show that for this particular
choice of six points, the percentage reduction function is at most 2.39%, with equality iff
5+3 5
b
. However, if we selected a different set of six points, could we achieve a better
a =
8
percentage reduction? This motivates the following question:
Problem 3. Consider six points X = {x1 , x2 , x3 } and Y = {y1 , y2 , y3 } in the Cartesian
plane. Let D  and D be the tournament lower bounds of BTTP* and BTTP, respectively.
Determine the smallest constant c for which D   c  D for all possible selections of the six
points in X  Y .
One may conjecture that in order to minimize the tournament lower bound, we must
minimize the total number of trips taken by the 2n teams. But as we saw in Table 6,
this conjecture is false for n = 3, as we located six points for which the solution to BTTP*
requires 24 trips, while the solution to BTTP requires 25 trips. However, there are numerous
examples (e.g. our scenario in Table 7) where the 2n points can be located so that the
solution to BTTP* matches that of BTTP. This motivates the following question: given
a random selection of 2n points, what is the probability that the solutions to BTTP* and
BTTP are identical?
116

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

To illustrate, consider the case n = 3. We can quickly show that there exist 60  29 =
30720 feasible inter-league tournaments, of which 60  23 = 480 are uniform. We run a
simulation on Maplesoft, where in each scenario, we randomly select six points (x, y) in the
Cartesian plane, calculate the 15  1 column vector of pairwise distances, and apply it to
the set of feasible inter-league tournaments to determine the distance-optimal schedule. We
run the simulation 100000 times, where in each scenario, we note the number of trips taken
in the optimal solution. The results appear in Table 10.
Trips
Scenarios

24
55800

25
33077

26
10967

27
43

28
0

 29
0

Table 10: Results of simulation: number of trips in the distance-optimal tournament.
We note that the sum total is not 100000, as there were 113 scenarios that ended in a
tie (e.g. there were two tournaments, one with 24 trips and another with 26 trips, both
with equal total distance after rounding to two decimal places.)
As expected, in the majority of scenarios, the six points X  Y had the property that
the distance-optimal bipartite tournament involved 24 trips, where each team played three
consecutive road games. Without much difficulty, one can show (Hoshino & Kawarabayashi,
2011a) that this forces the home game slots to be uniform, i.e., all the teams in each league
must play their home games at the same time, and so each team in X plays three consecutive
home games followed by three consecutive road games, or vice-versa. Therefore, in 55.8%
of our randomly-selected scenarios, the solution to BTTP* is identical to the solution to
BTTP.
Our simulation motivates an interesting question in geometric probability. Given that
the 2n points of X  Y are chosen at random, what is the probability that the tournament
lower bound is achieved with a schedule consisting of t trips? We formally define the
question below and present it as an open problem for the reader.
Problem 4. Let the 2n points X = {x1 , x2 , . . . , xn } and Y = {y1 , y2 , . . . , yn } be randomly
selected in the Cartesian plane. Let t be the number of trips taken in a distance-optimal
solution of BTTP, with the teams located at X and Y . Determine the value Pn (t) for each
t, where Pn (t) represents the probability that the distance-optimal tournament involves the
2n teams taking exactly t trips.
For the case n = 3, it appears that Pn (t) = 0 for all t  23 and t  28. While it is
trivial to show that we must have at least 24 trips, we do not have a formal proof that there
cannot exist a selection of six points X  Y in the plane for which the solution to BTTP
has more than 27 trips. If we could prove that for each n, the number of total trips in a
distance-optimal solution is bounded above by some function f (n), then this would enable
us to solve BTTP without having to enumerate all feasible schedules, i.e., a small fraction
would suffice. Such a result would certainly aid in solving BTTP for larger n, where a full
enumeration of all feasible tournament schedules is too computationally laborious. This
motivates our final problem.
Problem 5. Consider a 2n-team bipartite tournament, with the teams located at X =
{x1 , x2 , . . . , xn } and Y = {y1 , y2 , . . . , yn }. For each n, determine the smallest integer f (n)
117

fiHoshino & Kawarabayashi

for which the solution to BTTP involves the teams taking at most f (n) trips, regardless of
where the 2n teams are located.

Acknowledgments
This research has been partially supported by the Japan Society for the Promotion of Science
(Grant-in-Aid for Scientific Research), the C & C Foundation, the Kayamori Foundation,
and the Inoue Research Award for Young Scientists.

Appendix A.
We provide the proof of our three lemmas (from Section 3), beginning with Lemma 1.
Proof. First, we prove (i)  (ii).
If S is satisfiable, then there exists a function  that is a valid truth assignment, i.e.,
a function for which (ui )  {TRUE, FALSE} for each 1  i  l that ensures that each
clause Cj evaluates to TRUE for all 1  j  2k. From , we build a p-rooted 4-cycle-cover
of KS with exactly 3k (p, a, u, b, p)-cycles, 2k (p, u, c, d, p)-cycles, and k (p, u, e, f, p)-cycles.
We first identify the 3k (p, a, u, b, p)-cycles. For each 1  i  l, if (ui ) is FALSE,
then select all 4-cycles of the form p-ai,r -ui,r -bi,r -p, for each r = 1, 2, . . . , r(i). And if
(ui ) is TRUE, then select all 4-cycles of the form p-ai,r+1 -ui,r -bi,r -p, for each r (where
ai,r(i)+1 = ai,1 ). Repeating this construction for each i, we produce 3k (p, a, u, b, p)-cycles,
covering the 6k vertices of A  B, as well as 3k vertices of U .
Now consider any clause Cj . Since  is a valid truth assignment, at least one of the
three literals in Cj evaluates to TRUE. In other words, there must exist some index i for
which ui  Cj and (ui ) is TRUE, or ui  Cj and (ui ) is FALSE.
In the former case, where ui  Cj and (ui ) is TRUE, there exists some index r for
which ui,r -cj is an edge of the gadget graph GS . Then p-ui,r -cj -dj -p is a (p, u, c, d, p)-cycle.
Note that ui,r has not been previously selected in a (p, a, u, b, p)-cycle since (ui ) is TRUE
(and so only the vertices ui,1 , ui,2 , . . . , ui,r(i) were covered earlier.)
In the latter case, where ui  Cj and (ui ) is FALSE, there exists some index r for
which ui,r -cj is an edge of the gadget graph GS . Then p-ui,r -cj -dj -p is a (p, u, c, d, p)-cycle.
Note that ui,r has not been previously selected in a (p, a, u, b, p)-cycle since (ui ) is FALSE
(and so only the vertices ui,1 , ui,2 , . . . , ui,r(i) were covered earlier.)
Repeating this construction for each j, we produce 2k (p, u, c, d, p)-cycles, covering the 4k
vertices of C  D. Note that no u  U can be chosen twice since each vertex in U is adjacent
to only one vertex in C. Thus, these 2k cycles cover a set of 6k vertices in XS , completely
disjoint from the 9k vertices covered by the previously-constructed 3k (p, a, u, b, p)-cycles.
As a result, we are left with 3k vertices in XS still to be covered, specifically k vertices in
each of U , E, and F . These vertices can be trivially partitioned into k (p, u, e, f, p)-cycles by
just ensuring that ej and fj belong to the same cycle for each 1  j  k. When this process
is complete, our p-rooted 4-cycle-cover of KS will contain exactly 3k (p, a, u, b, p)-cycles, 2k
(p, u, c, d, p)-cycles, and k (p, u, e, f, p)-cycles.
Having established the first direction, we now prove (ii)  (i).
118

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

Consider a p-rooted 4-cycle-cover of KS containing exactly 3k (p, a, u, b, p)-cycles, 2k
(p, u, c, d, p)-cycles, and k (p, u, e, f, p)-cycles. We prove there exists a function  that is a
satisfying truth assignment for S, where (ui )  {TRUE, FALSE} for each 1  i  l.
Define an a-b path to be any path on three vertices whose endpoints are ai,j and bi,k , for
some indices i, j, k. Consider the problem of maximizing the number of vertex-disjoint a-b
paths in the ith gadget. One can quickly see that a maximum packing of a-b paths occurs
iff the r(i) paths are chosen in one of the following trivial ways:
(a) Taking all paths of the form ai,r , ui,r , bi,r for each r = 1, 2, . . . , r(i).
(b) Taking all paths of the form ai,r+1 , ui,r , bi,r for each r = 1, 2, . . . , r(i).
ai,r(i)+1 = ai,1 .)

(Note:

In order for us to cover all of the vertices in A  B, in each gadget we must select our a-b
paths either vertically (a) or diagonally (b). Thus, in our p-rooted 4-cycle-cover containing
3k (p, a, u, b, p)-cycles, one of the following scenarios must hold true in the ith gadget:
(1) For each r = 1, 2, . . . , r(i), vertex ui,r appears in some (p, a, u, b, p)-cycle, while no
vertex ui,r appears in any (p, a, u, b, p)-cycle.
(2) For each r = 1, 2, . . . , r(i), vertex ui,r appears in some (p, a, u, b, p)-cycle, while no
vertex ui,r appears in any (p, a, u, b, p)-cycle.
In our given p-rooted 4-cycle-cover of KS , for each i define (ui ) = FALSE in scenario
(1) and define (ui ) = TRUE in scenario (2). We claim that this is our desired function .
To prove this, consider the 2k (p, u, c, d, p)-cycles in our 4-cycle-cover. For each 1  j 
2k, the (p, u, c, d, p)-cycle containing cj also contains some other vertex in U . This vertex
is either ui,r or ui,r , for some indices i and r.
In the former case, ui,r and cj appear in the same (p, u, c, d, p)-cycle, implying that ui,r cj is an edge of the gadget graph GS , and that ui is a literal in clause Cj . Since ui,r appears
in this (p, u, c, d, p)-cycle and therefore not in any (p, a, u, b, p)-cycle, this implies scenario
(2) above. Since (ui ) = TRUE and ui  Cj , clause Cj evaluates to TRUE.
In the latter case, ui,r and cj appear in the same (p, u, c, d, p)-cycle, implying that ui,r -cj
is an edge of the gadget graph GS , and that ui is a literal in clause Cj . Since ui,r appears
in this (p, u, c, d, p)-cycle and therefore not in any (p, a, u, b, p)-cycle, this implies scenario
(1) above. Since (ui ) = FALSE and ui  Cj , clause Cj evaluates to TRUE.
Since Cj evaluates to TRUE for all 1  j  2k, this implies that  is a valid truth
assignment. We conclude that S = C1  C2  . . .  C2k is satisfiable.
We now prove Lemma 2.
Proof. First, we prove (i)  (ii).

In a (p, a, u, b, p)-cycle, the edges au and ub appear in the gadget graph GS . Therefore,
the edge weights of au and ub are both z 2 . From Table 3, we see that a (p, a, u, b, p)-cycle
has edge weight z 2 + z 2 + z 2 + z 2 = 4z 2 . Similarly, a (p, u, c, d, p)-cycle has edge weight
(z 2 +z)+z 2 +z 2 +z 2 = 4z 2 +z, and a (p, u, e, f, p)-cycle has edge weight (z 2 +z)+z 2 +z 2 +z 2 =
4z 2 + z.
119

fiHoshino & Kawarabayashi

So if a p-rooted 4-cycle-cover of KS has exactly 3k (p, a, u, b, p)-cycles, 2k (p, u, c, d, p)cycles, and k (p, u, e, f, p)-cycles, then its total edge weight is exactly 3k(4z 2 ) + 2k(4z 2 +
z) + k(4z 2 + z) = k(24z 2 + 3z).
Having established the first direction, we now prove (ii)  (i).
Let R be a p-rooted 4-cycle-cover of KS which is the union of r cycles, with total edge
weight k(24z 2 + 3z). Since each of the 18k vertices of XS is covered by exactly one cycle of
R, the number of edges in R is |XS | + r = 18k + r. Since no cycle has length greater than
4, we have r  18k
3 = 6k. Now suppose r  6k + 1. Then there are at least 24k + 1 edges
in R, all of which have weight at least z 2 given the construction of our complete graph KS .
Hence, the total edge weight of R is at least (24k +1)z 2 = 24kz 2 +z 2 = 24kz 2 +z(20k +1) >
24kz 2 + 3zk = k(24z 2 + 3z), a contradiction.
It follows that r = 6k, and that R must be the union of 6k cycles of length 4. Recall
that the weight of each edge appears in the set {z 2 , z 2 + z, z 2 + 2z, 2z 2  1}. Suppose
that one of these 24k edges has weight 2z 2  1. Then the total edge weight of R is at least
(24k1)z 2 +(2z 2 1) = 24kz 2 +z 2 1 = 24kz 2 +z(20k+1)1 > 24kz 2 +3zk = k(24z 2 +3z),
a contradiction. Hence, all edges of R must have weight z 2 , z 2 + z, or z 2 + 2z.
From Table 3, we see that no edges p-c and p-e can appear in our 4-cycle-cover R, since
all edges from p to C  E have weight 2z 2  1. It follows that there must exist 2k 4-cycles
of the form p-?-ci -?-p and k 4-cycles of the form p-?-ei -?-p, with each of these 2k + k = 3k
4-cycles containing a unique element from C  E. Each blank space (denoted by a question
mark) can only be filled with a vertex from D, F , or U , as the weights of edges ca, cb, ea,
eb are all 2z 2  1 for all a  A, b  B, c  C, and e  E.

Since edge p-u has weight z 2 + z, if some vertex u  U is chosen to appear in one of
these 3k 4-cycles, then this adds edge weight z 2 + z, producing a 4-cycle of weight at least
4z 2 + z. But if no vertices u  U are chosen to replace these blank spaces, then the cycles
must be of the form p-dj -ci -dk -p or p-fj -ei -fk -p, both of which lead to the addition of at
least one edge of weight 2z 2  1 (since we cannot simultaneously have i = j, i = k, and
j 6= k). It follows that these 2k + k = 3k 4-cycles containing the vertices of C  E must each
have weight at least 4z 2 + z, thus contributing at least k(12z 2 + 3z) to the total distance of
the p-rooted 4-cycle-cover R.
Since the given 4-cycle-cover R has weight k(24z 2 + 3z), this implies that the rest of the
3k 4-cycles must each have weight exactly 4z 2 , and that in each of the 2k cycles of the form
p-?-ci -?-p and k cycles of the form p-?-ei -?-p, the total edge weight must be exactly 4z 2 + z
to ensure that the total edge weight of R does not exceed k(24z 2 + 3z). This implies that
in these two scenarios, we cannot replace the two blank spaces with two distinct vertices
from U , as that would create a cycle of weight 4z 2 + 2z. It follows that R must have 2k
(p, u, c, d, p)-cycles and k (p, u, e, f, p)-cycles.
We are now left with 3k vertices from each of A, B, and U to form our remaining 3k
4-cycles. In order for the total edge weight of R to not exceed k(24z 2 + 3z) = 3k(4z 2 + z) +
12kz 2 , each of the remaining 12k edges must have weight z 2 . Since edge p-u has weight
z 2 + z for all u  U , the 3k remaining vertices in U must each appear in a unique 4-cycle,
none adjacent to the root vertex p. It follows that the remaining 3k 4-cycles of R must all
be (p, a, u, b, p)-cycles.
120

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

We now prove Lemma 3.

Proof. From the proof of Lemma 2, we see that ILBp = k(24z 2 + 3z), so this handles the
case y = p. We now consider the case y = q.
Let R be a q-rooted 4-cycle-cover of KS which is the union of r cycles. Suppose on the
contrary that there exists an R for which its total edge weight is less than k(24z 2 + 20z).
We will derive a contradiction.
Since each of the 18k vertices of XS is covered by exactly one cycle of R, the number of
edges in R is |XS |+r = 18k+r. As in the previous proof, r  6k. If r  6k+1, then the total
edge weight of R is at least (24k + 1)z 2 = 24kz 2 + z 2 = 24kz 2 + z(20k + 1) > k(24z 2 + 20z),
a contradiction.
Hence, r = 6k, and so R must be the union of 6k cycles of length 4. Now suppose
that one of these 24k edges has weight 2z 2  1. Then the total edge weight of R is at least
(24k  1)z 2 + (2z 2  1) = 24kz 2 + z 2  1 = 24kz 2 + z(20k + 1)  1 > k(24z 2 + 20z), another
contradiction.
Therefore, no edges q-b and q-d can appear in our 4-cycle-cover R, since all edges from
p to B  D have weight 2z 2  1. It follows that there must exist 3k 4-cycles of the form
q-?-bi -?-q and 2k 4-cycles of the form q-?-di -?-q. No blank space (denoted by a question
mark) can be filled with a vertex from E or F since the weights of edges be, bf , de, df are
2z 2  1 for all b  B, d  D, e  E, and f  F .
It follows that the k remaining 4-cycles must include all of the k + k = 2k vertices in
E  F . If any of these 4-cycles contains three elements of E  F (e.g. the cycle q-ei -fj ek -q or the cycle q-ei -ej -fk -q), then that creates at least one edge with weight 2z 2  1, a
contradiction. Thus, there must be exactly two vertices from E F in each of these 4-cycles.
Moreover, since the weights of edges ae, af , ce, cf are all 2z 2  1 for all a  A, c  C, it
follows that the final vertex of these remaining k 4-cycles must be an element of U , thus
producing a 4-cycle such as q-ui,r -ej -fk -q or q-fi -uj,r -fk . From Table 3, we see that every
valid cycle has edge weight  4z 2 + 2z.
Hence, we must have k 4-cycles in the cycle cover R, containing all 2k vertices in E  F
and k vertices in U , contributing total weight  k(4z 2 + 2z). Of the 3k 4-cycles of the
form q-?-bi -?-q, no vertex in C can appear, as otherwise there would be an edge with weight
2z 2  1. Similarly, of the 2k 4-cycles of the form q-?-di -?-q, no vertex in A can appear.
Thus, in each of the 3k 4-cycles containing bi , the other two vertices must be selected
from A  U . From Table 3, we see that every such 4-cycle has edge weight  4z 2 + 4z. And
in each of the 2k 4-cycles containing di , the other two vertices must be selected from C  U .
Also from Table 3, we see that every such 4-cycle has edge weight  4z 2 + 3z.
Therefore, any q-rooted 4-cycle-cover of KS has total edge weight  k(4z 2 + 2z) +
3k(4z 2 + 4z) + 2k(4z 2 + 3z) = k(24z 2 + 20z), establishing our desired contradiction. We
conclude that ILBq = k(24z 2 + 20z).
The proof for the r-rooted 4-cycle-cover is identical. We just apply the mapping
{a, b, c, d, e, f, u}  {b, a, e, f, c, d, u} to the vertices in the preceding paragraphs to reach
the same conclusion. In this case, we have ILBr = k(4z 2 +3z)+3k(4z 2 +4z)+2k(4z 2 +2z) =
k(24z 2 + 19z).
121

fiHoshino & Kawarabayashi

Appendix B.
We now provide the 12  12 distance matrix for the NPB league (from Section 4), and the
30  30 distance matrix for the NBA (from Section 5).
As mentioned in Section 4, the Pacific League teams are p1 (Fukuoka), p2 (Orix), p3
(Saitama), p4 (Chiba), p5 (Tohoku), p6 (Hokkaido), and the Central League teams are c1
(Hiroshima), c2 (Hanshin), c3 (Chunichi), c4 (Yokohama), c5 (Yomiuri), and c6 (Yakult).
In Table 11, we only provide Dci ,cj and Dpi ,pj for i < j since the case i > j is equivalent by
symmetry.
Team
c1
c2
c3
c4
c5
c6
p1
p2
p3
p4
p5
p6

c1
0

c2
323
0

c3
488
195
0

c4
808
515
334
0

c5
827
534
353
37
0

c6
829
536
355
35
7
0

p1
258
577
742
916
926
923
0

p2
341
27
213
533
552
554
595
0

p3
870
577
396
63
51
48
958
595
0

p4
857
564
383
58
37
39
934
582
86
0

p5
895
654
511
364
331
333
1100
670
374
361
0

p6
1288
1099
984
886
896
893
1466
1115
928
904
580
0

Table 11: Distance Matrix for the Japanese NPB League.
To calculate each entry of this distance matrix, we determined how the teams travel
from one stadium to another, taking into account the actual mode(s) of transportation.
For example, the distance Dc2 ,c5 = 534 was found by adding the travel distance for each
component of the trip from Hanshins home stadium to Yomiuris home stadium, namely
the 15 km bus ride from Koshien Stadium to Shin-Osaka Station, the 515 km bullet-train
ride to Tokyo Station, followed by the 4 km bus ride to the Tokyo Dome. This is a more
rigorous approach that simply calculating the flight distance between the airports in Osaka
and Tokyo. Noting when teams travel by airplane, bullet train, and bus, we repeat the
analysis for each of the 12
2 = 66 pairs of cities to produce the matrix in Table 11.
Finally, we provide the 30  30 distance matrix for the NBA, as well the labeling of
the 30 teams in Table 9. There are fifteen teams in the Western Conference, namely
the Portland Trailblazers (PT), Golden State Warriors (GW), Sacramento Kings (SK),
Los Angeles Clippers (LC), Los Angeles Lakers (LL), Phoenix Suns (PS), Utah Jazz (UJ),
Denver Nuggets (DN), Oklahoma Thunder (OT), San Antonio Spurs (SS), Dallas Mavericks
(DM), Houston Rockets (HR), Minnesota Timberwolves (MT), Memphis Grizzlies (MG),
and New Orleans Hornets (NH).
There are fifteen teams in the Eastern Conference, namely the Milwaukee Bucks (MB),
Chicago Bulls (CU), Indiana Pacers (IP), Detroit Pistons (DP), Toronto Raptors (TR),
Cleveland Cavaliers (CC), Boston Celtics (BC), New York Knicks (NK), New Jersey Nets
(NN), Philadelphia Sixers (PS), Washington Wizards (WW), Charlotte Bobcats (CB), Atlanta Hawks (AH), Orlando Magic (OM), and Miami Heat (MH). Note that two teams
(Chicago Bulls and Charlotte Bobcats) have the same initials, and thus we have represented the former as CU and the latter as CB to avoid ambiguity.
122

fiScheduling Bipartite Tournaments to Minimize Total Travel Distance

Team
PT
GW
SK
LC
LL
PS
UJ
DN
OT
SS
DM
HR
MT
MG
NH

PT
0

GW
536
0

SK
473
75
0

LC
824
333
368
0

LL
824
333
368
0
0

PS
997
636
637
365
365
0

UJ
620
580
524
582
582
501
0

DN
969
931
884
837
837
582
375
0

OT
1462
1353
1320
1169
1169
820
852
493
0

SS
1691
1455
1441
1192
1192
831
1072
785
402
0

DM
1602
1445
1420
1227
1227
866
985
645
178
244
0

HR
1799
1605
1586
1358
1358
994
1178
853
391
187
215
0

MT
1403
1555
1494
1514
1514
1258
976
683
686
1084
843
1023
0

MG
1826
1770
1733
1594
1594
1244
1242
867
425
617
417
462
692
0

NH
2020
1875
1850
1645
1645
1281
1408
1052
559
486
430
300
1027
345
0

Table 12: Distance Matrix for the NBA Western Conference (intra-league).
Team
MB
CU
IP
DP
TR
CC
BC
NK
NN
PS
WW
CB
AH
OM
MH

MB
0

CU
66
0

IP
235
175
0

DP
248
249
249
0

TR
414
430
433
190
0

CC
323
310
257
90
191
0

BC
847
853
805
605
439
553
0

NK
734
728
655
486
361
418
184
0

NN
714
708
634
466
343
398
198
20
0

PS
680
667
578
434
342
357
276
93
80
0

WW
602
580
468
372
341
284
407
225
209
133
0

CB
642
591
422
502
582
425
718
534
522
442
317
0

AH
661
599
427
602
731
548
933
749
735
657
526
224
0

OM
1047
985
811
950
1036
877
1101
926
919
844
742
456
392
0

MH
1244
1183
1009
1143
1220
1068
1243
1077
1074
1002
911
643
589
198
0

Table 13: Distance Matrix for the NBA Eastern Conference (intra-league).
Team
MB
CU
IP
DP
TR
CC
BC
NK
NN
PS
WW
CB
AH
OM
MH

PT
1690
1711
1848
1934
2064
2014
2497
2415
2395
2368
2291
2247
2140
2491
2661

GW
1806
1807
1903
2052
2214
2117
2651
2535
2515
2472
2372
2251
2097
2397
2540

SK
1750
1753
1855
1998
2157
2064
2594
2482
2461
2419
2320
2209
2060
2367
2514

LC
1730
1718
1786
1967
2143
2022
2572
2437
2417
2365
2252
2092
1917
2181
2307

LL
1730
1718
1786
1967
2143
2022
2572
2437
2417
2365
2252
2092
1917
2181
2307

PS
1439
1418
1466
1665
1848
1711
2265
2120
2100
2403
1926
1747
1562
1817
1942

UJ
1227
1230
1333
1474
1634
1540
2072
1958
1937
1895
1799
1700
1565
1897
2058

DN
894
886
973
1135
1307
1194
1739
1612
1592
1544
1441
1327
1190
1526
1692

OT
726
683
678
908
1098
934
1482
1324
1305
1242
1118
926
749
1049
1206

SS
1082
1028
973
1220
1406
1224
1739
1564
1547
1474
1342
1080
861
1023
1126

DM
840
787
745
990
1177
1001
1532
1362
1344
1275
1147
912
710
955
1094

HR
973
915
834
1083
1264
1077
1575
1397
1380
1306
1173
899
680
839
950

MT
292
330
495
531
667
612
1106
1012
992
965
894
917
894
1286
1483

MG
550
485
376
624
801
614
1123
950
932
861
731
503
327
668
849

NH
893
827
699
935
1097
906
1349
1166
1151
1074
942
642
419
540
665

Table 14: Distance Matrix between the two NBA Conferences (inter-league).

For readability, the 30  30 distance matrix is broken into 15  15 matrices, providing
both intra-league and inter-league distances. We remark that the Los Angeles Clippers and
Los Angeles Lakers play their games in the same arena, which explains why their distance
is zero. Each entry in Tables 12 through 14 is expressed in miles, unlike the NPB distance
matrix which was expressed in kilometres.
123

fiHoshino & Kawarabayashi

References
Anagnostopoulos, A., Michel, L., Hentenryck, P. V., & Vergados, Y. (2006). A simulated
annealing approach to the traveling tournament problem. Journal of Scheduling, 9,
177193.
Easton, K., Nemhauser, G., & Trick, M. (2001). The traveling tournament problem: description and benchmarks. Proceedings of the 7th International Conference on Principles
and Practice of Constraint Programming, 580584.
Easton, K., Nemhauser, G., & Trick, M. (2002). Solving the travelling tournament problem:
A combined integer programming and constraint programming approach. Proceedings of the 4th International Conference on the Practice and Theory of Automated
Timetabling, 319330.
Easton, K., Nemhauser, G., & Trick, M. (2004). Sports scheduling. In Leung, J. T. (Ed.),
Handbook of Scheduling, chap. 52, pp. 119. CRC Press.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A guide to the theory of
NP-completeness. W.H. Freeman, New York.
Hoshino, R., & Kawarabayashi, K. (2011a). The distance-optimal inter-league schedule
for Japanese pro baseball. Proceedings of the ICAPS 2011 Workshop on Constraint
Satisfaction Techniques for Planning and Scheduling Problems (COPLAS), 7178.
Hoshino, R., & Kawarabayashi, K. (2011b). The inter-league extension of the traveling
tournament problem and its application to sports scheduling. Proceedings of the 25th
AAAI Conference on Artificial Intelligence, to appear.
Hoshino, R., & Kawarabayashi, K. (2011c). The multi-round balanced traveling tournament
problem. Proceedings of the 21st International Conference on Automated Planning and
Scheduling (ICAPS), 106113.
Hoshino, R., & Kawarabayashi, K. (2011d). A multi-round generalization of the traveling
tournament problem and its application to Japanese baseball. European Journal of
Operational Research, doi: 10.1016/j.ejor.2011.06.014.
Itai, A., Perl, Y., & Shiloach, Y. (1982). The complexity of finding maximum disjoint paths
with length constraints. Networks, 12, 278286.
Kendall, G., Knust, S., Ribeiro, C., & Urrutia, S. (2010). Scheduling in sports: An annotated
bibliography. Computers and Operations Research, 37, 119.
Lim, A., Rodrigues, B., & Zhang, X. (2006). A simulated annealing and hill-climbing
algorithm for the traveling tournament problem. European Journal of Operational
Research, 174, 14591478.
Thielen, C., & Westphal, S. (2010). Complexity of the traveling tournament problem.
Theoretical Computer Science, 412, 345351.

124

fiJournal of Artificial Intelligence Research 42 (2011) 31-53

Submitted 4/11; published 9/11

On the Link between Partial Meet, Kernel, and
Infra Contraction and its Application to Horn Logic
Richard Booth

richard.booth@uni.lu

Universite du Luxembourg
Luxembourg

Thomas Meyer

tommie.meyer@meraka.org.za

Centre for Artificial Intelligence Research
University of KwaZulu-Natal and CSIR Meraka Institute
South Africa

Ivan Varzinczak

ivan.varzinczak@meraka.org.za

Centre for Artificial Intelligence Research
University of KwaZulu-Natal and CSIR Meraka Institute
South Africa

Renata Wassermann

renata@ime.usp.br

Universidade de Sao Paulo
Brazil

Abstract
Standard belief change assumes an underlying logic containing full classical propositional logic. However, there are good reasons for considering belief change in less expressive
logics as well. In this paper we build on recent investigations by Delgrande on contraction
for Horn logic. We show that the standard basic form of contraction, partial meet, is too
strong in the Horn case. This result stands in contrast to Delgrandes conjecture that
orderly maxichoice is the appropriate form of contraction for Horn logic. We then define a
more appropriate notion of basic contraction for the Horn case, influenced by the convexity
property holding for full propositional logic and which we refer to as infra contraction. The
main contribution of this work is a result which shows that the construction method for
Horn contraction for belief sets based on our infra remainder sets corresponds exactly to
Hanssons classical kernel contraction for belief sets, when restricted to Horn logic. This
result is obtained via a detour through contraction for belief bases. We prove that kernel
contraction for belief bases produces precisely the same results as the belief base version
of infra contraction. The use of belief bases to obtain this result provides evidence for the
conjecture that Horn belief change is best viewed as a hybrid version of belief set change
and belief base change. One of the consequences of the link with base contraction is the
provision of a representation result for Horn contraction for belief sets in which a version
of the Core-retainment postulate features.

1. Introduction
In his seminal paper, Delgrande (2008) has shed some light on the theoretical underpinnings
of belief change by weakening a usual assumption in the belief change community, namely
that the underlying logical formalism should be at least as strong as (full) classical propositional logic (Gardenfors, 1988). Delgrande investigated contraction functions for belief
sets (sets of sentences that are closed under logical consequence) restricted to Horn formuc
2011
AI Access Foundation. All rights reserved.

fiBooth, Meyer, Varzinczak, & Wassermann

las (1951). Delgrandes main contributions were essentially threefold. Firstly, he showed
that the move to Horn logic leads to two different types of contraction functions, referred
to as entailment-based contraction (e-contraction) and inconsistency-based contraction (icontraction), which coincide in the full propositional case. Secondly, he showed that Horn
contraction for belief sets does not satisfy the controversial Recovery postulate, but exhibits
some characteristics that are usually associated with the contraction of belief bases (arbitrary sets of sentences). And finally, Delgrande made a tentative conjecture that a version
of Horn contraction usually referred to as orderly maxichoice contraction is the appropriate
method for contraction in Horn theories.
While Delgrandes partial meet constructions are appropriate choices for contraction in
Horn logic, we show that they do not constitute all the appropriate forms of Horn contraction. Moreover, as referred to above, although Horn contraction is defined for Horn belief
sets, it is related in some ways to contraction for belief bases, an aspect which has not yet
been explored properly in the literature.
In this paper we continue the investigation into contraction for Horn logic, and address
both the issues mentioned above, as well as others. Focusing on Delgrandes entailmentbased contraction, we start by providing a more fine-grained construction for belief set
contraction which we refer to in this paper as infra contraction. We then bring into the
picture a construction method for contraction first introduced by Hansson (1994), known
as kernel contraction. Although kernel contraction is usually associated with belief base
contraction, it can be applied to belief sets as well. Our main contribution is a result which
shows that infra contraction corresponds exactly to Hanssons kernel contraction for belief
sets, when restricted to Horn logic. In order to prove this, we first take a close look at
contraction for belief bases, defining a base version of infra contraction and proving that
this construction is equivalent to kernel contraction for Horn belief bases. Since Horn belief
sets are not closed under classical logical consequence, they can be seen as a hybrid between
belief sets and belief bases. This justifies the use of belief bases to obtain results for belief
set Horn contraction.
Horn logic has found extensive use in Artificial Intelligence, in particular in logic programming, truth maintenance systems, and deductive databases. This explains, in part,
our interest in belief change for Horn logic. (Despite our interest in Horn formulas, it is
worth noting that in this work we do not consider logic programming explicitly and we do
not use negation as failure at all.) Another reason for focusing on this topic is because of its
application to debugging and repairing ontologies in description logics (Baader, Calvanese,
McGuinness, Nardi, & Patel-Schneider, 2007). In particular, Horn logic can be seen as
the backbone of the EL family of description logics (Baader, Brandt, & Lutz, 2005), and
therefore a proper understanding of belief change for Horn logic is important for finding
solutions to similar problems expressible in the EL family.
The remainder of the present paper is organized as follows: After some logical preliminaries (Section 2), we give the background on belief set contraction (Section 3) and on belief
base contraction (Section 4) that is necessary for the core section of the paper (Section 5).
There we prove that kernel contraction and infra contraction are equivalent on the level
of belief bases. This enables us to prove that kernel contraction and infra contraction are
equivalent on the Horn belief set level as well. From this we are led to provide a characterization of infra contraction for Horn belief sets in which a version of the Core-retainment
32

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

postulate (Hansson, 1994) for bases features. This provides even more evidence for the
hybrid aspect of Horn belief change. All these results are stated for entailment-based
contraction (e-contraction). In Section 6, where we discuss related work, we also mention
similar results for Delgrandes i-contraction and another relevant type of Horn contraction,
namely Booth et als (2009) package contraction. We conclude with a summary of our contributions as well as a discussion on future directions of investigation. Proofs of all new
results can be found in Appendix A.

2. Preliminaries
We work in a finitely generated propositional language over a set of propositional atoms P,
together with the distinguished atom > (true), and with the standard model-theoretic
semantics. Atoms are denoted by p, q, . . ., possibly with subscripts. The formulas of our
language are denoted by , , . . . Those are recursively defined as follows:
 ::= p | > |  |   
All the other connectives (, , , . . . ) and the special atom  (false) are defined in terms
of  and  in the usual way. With LP we denote the set of all formulas of the language.
Classical logical consequence and logical equivalence are denoted by |= and  respectively. For X  LP , the set of sentences logically entailed by X is denoted by Cn(X). A
belief set is a logically closed set, i.e., for a belief set K, K = Cn(K). We usually denote
belief sets by K, possibly decorated by primes. P(X) denotes the power set (set of all subsets) of X. When displaying belief sets we sometimes follow the convention of displaying
one representative of each equivalence class modulo logical equivalence, and dropping the
representative for the tautologies. As an example, for LP generated by the two atoms p
and q, the set Cn({p}) can be represented as {p, p  q, p  q}.
A Horn clause is a sentence of the form p1  p2  . . .  pn  q where n  0, pi , q  P
for 1  i  n (recall that the pi s and q may be one of  or > as well). If n = 0 we write
q instead of  q. A Horn formula is a conjunction of Horn clauses. A Horn set is a set of
Horn formulas.
Given a propositional language LP , the Horn language LH generated from LP is simply
the set of Horn formulas occurring in LP . The Horn logic obtained from LH has the same
semantics as the propositional logic obtained from LP , but just restricted to Horn formulas.
Hence, we have that |=, , and all other related notions are defined relative to the logic we
are working in. We use |=PL and CnPL (.) to denote classical entailment and consequence for
propositional logic. For Horn logic, we define CnHL (.) as follows: CnHL (X) =def CnPL (X) 
LH for X  LH . And we define |=HL as follows: For X  LH and   LH , X |=
 if and only
HL
if X |=PL .
The consequence operator CnHL (.) is a Tarskian consequence operator in the sense that
it satisfies the following properties for all Horn sets X, X 0 :
 X  CnHL (X)

(Inclusion)

 CnHL (X) = CnHL (CnHL (X))

(Idempotency)

 If X  X 0 , then CnHL (X)  CnHL (X 0 )

(Monotonicity)
33

fiBooth, Meyer, Varzinczak, & Wassermann

A Horn belief set, usually denoted by H (possibly with primes), is a Horn set closed
under the operator CnHL (.), i.e., H = CnHL (H). We shall dispense with such subscripts
whenever the context makes it clear which logic we are dealing with.
In the AI tradition, a given set of formulas of the underlying logical language is called
a knowledge base, or simply a set of beliefs. Belief change deals with situations in which an
agent has to modify its beliefs about the world, usually due to new or previously unknown
incoming information, also represented as formulas of the language. Common operations of
interest in belief change are the expansion (Gardenfors, 1988) of an agents current beliefs X
by a given formula  (usually denoted as X + ), where the basic idea is to add  regardless
of the consequences, and the revision (Gardenfors, 1988) of its current beliefs by  (denoted
as X ? ), where the intuition is to incorporate  into the current beliefs in some way while
ensuring consistency of the resulting theory at the same time. Perhaps the most basic
operation in belief change is that of contraction (Alchourron, Gardenfors, & Makinson,
1985; Gardenfors, 1988), which is intended to represent situations in which an agent has to
give up  from its current stock of beliefs (denoted as X  ). Indeed the revision operation
can be defined in terms of contraction and simple expansion via the Levi identity (Levi,
1977). Therefore, contraction is the focus of the present paper and in what follows we shall
investigate it in detail. Throughout Sections 3 and 4 we assume we work in the language
of full propositional logic LP .

3. Belief Set Contraction
We commence with a discussion on belief set contraction, where the aim is to describe
contraction on the knowledge level (Gardenfors, 1988), i.e., independently of how beliefs are
represented syntactically. Thus, contraction is defined only for belief sets.
Definition 1 (Belief Set Contraction) A belief set contraction  for a belief set K is
a function from LP to P(LP ).
By the principle of categorical matching (Gardenfors & Rott, 1995) the contraction of a
belief set by a sentence is expected to yield a new belief set.
One of the standard approaches for constructing belief contraction operators is based
on the notion of a remainder set of a set K with respect to a formula : a maximal subset
of K not entailing  (Alchourron et al., 1985). Here we define this for belief sets, but it has
been known in the literature that its basic principle can be applied to belief bases as well
(we shall recall this in Section 4).
Definition 2 (Remainder Sets) Given a belief set K and a formula   LP , X  K
if and only if
 X  K;
 X 6|= ; and
 For every X 0 such that X  X 0  K, X 0 |= .

We call the elements of K the remainder sets of K with respect to .
34

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

It is easy to verify that K =  if and only if |=  (Gardenfors, 1988).
Since there is no unique method for choosing between possibly different remainder sets,
there is a presupposition of the existence of a suitable selection function for doing so:
Definition 3 (Selection Functions) A selection function  for a set K is a (partial)
function from P(P(LP )) to P(P(LP )) such that
 (K) = {K} if K = ; and
   (K)  K otherwise.

Selection functions provide a mechanism for identifying the remainder sets judged to be
most appropriate. The resulting contraction is obtained by taking the intersection of the
chosen remainder sets.
Definition 4 (Partial Meet Contraction) For a selection function
, the belief set conT
traction operator  generated by  and defined as K   =def (K) is a partial meet
contraction.
Two subclasses of belief set partial meet deserve special mention:
Definition 5 (Maxichoice and Full Meet) Given a selection function ,  is a maxichoice contraction if and only if (K) is always a singleton set. It is a full meet contraction if and only if (K) = K whenever K 6= .
It is worth mentioning that belief set full meet contraction is unique, while belief set
maxichoice contraction usually is not.
Kernel contraction was introduced by Hansson (1994) as a generalization of safe contraction (Alchourron & Makinson, 1985). Instead of looking at maximal subsets not implying
a given formula, kernel operations are based on minimal subsets that do imply it.
Definition 6 (Kernel) For a belief set K, X  K   if and only if
 X  K;
 X |= ; and
 For every X 0 such that X 0  X, X 0 6|= .

K   is called the kernel set of K with respect to  and the elements of K   are called
the -kernels of the belief set K.
The result of a kernel contraction is obtained by removing at least one element from
every (non-empty) -kernel of K, using an incision function.
Definition 7 (Incision Functions) An incision function  for a set K is a function from
the set of kernel sets of K to P(LP ) such that
35

fiBooth, Meyer, Varzinczak, & Wassermann

 (K 
 ) 

S

(K 
 ); and

 If  =
6 X  K
 , then X  (K  ) 6= .

Given a belief set K and an incision function  for K, ideally we would want the kernel
contraction  for K generated by  be defined as:
K   =def K \ (K  )

(1)

It turns out that for a belief set K, the contraction operator in (1) does not respect
the principle of categorical matching. That is because, for a belief set as input, the kernel
contraction  from (1) above does not necessarily produce a belief set as a result: K  
is not in general closed under logical consequence, as shown by the following example.
Example 1 Let K = Cn({p  q, q  r}) and assume that we want to contract p  r
from K. We have K 
 (p  r) = {{p  r}, {p  q, q  r}, {p  q, p  q  r}}. For
the incision function  defined such that (K  (p  r)) = {p  r, p  q, p  q  r}, we
have that applying the operator in (1) gives us K 0 such that K 0 |= p  q  r, but obviously
pq r 
/ K 0.
Of course, it is possible to ensure that one obtains a belief set by closing the result
obtained from the operator above under logical consequence.
Definition 8 (Belief Set Kernel Contraction) Given a belief set K and an incision
function  for K, the belief set contraction operator  for K generated by  and defined
as K   =def Cn(K  ) is a belief set kernel contraction.
We shall see in Section 4 that belief set kernel contraction is closely related to a version of
belief base contraction referred to as saturated base kernel contraction (Hansson, 1999).
Belief set contraction defined in terms of partial meet contraction corresponds exactly to
what is perhaps the best-known approach to belief change: the so-called AGM approach (Alchourron et al., 1985). AGM requires that (basic) belief set contraction be characterized by
the following set of postulates:
(K  1) K   = Cn(K  )

(Closure)

(K  2) K    K

(Inclusion)

(K  3) If  
/ K, then K   = K

(Vacuity)

(K  4) If 6|= , then  
/ K 

(Success)

(K  5) If   , then K   = K  

(Extensionality)

(K  6) If   K, then Cn((K  )  {}) = K
36

(Recovery)

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

Full AGM contraction involves two extended postulates in addition to the basic postulates
given above. We shall not elaborate in detail on the intuition of these postulates. For that
we refer the reader to the book by Gardenfors (1988) or the handbook by Hansson (1999).
Alchourron et al. (1985) have shown that these postulates characterize belief set partial
meet contraction exactly, as shown by the following result:
Theorem 1 (Alchourron et al., 1985) Every belief set partial meet contraction satisfies Postulates (K  1)(K  6). Conversely, every belief set contraction which satisfies
Postulates (K  1)(K  6) is a belief set partial meet contraction.
Hansson (1994) has shown that Postulates (K  1)(K  6) also characterize kernel
contraction for belief sets:
Theorem 2 (Hansson, 1994) Every belief set kernel contraction satisfies the Postulates
(K 1)(K 6). Conversely, every belief set contraction which satisfies Postulates (K 1)
(K  6) is a belief set kernel contraction.
We conclude this section with an important new observation whose usefulness will become apparent in Section 5.
Theorem 3 (Convexity) Let K be a belief set, let mc be a (belief set) maxichoice contraction, and let fm denote (belief set) full meet contraction. For every   LP and every
belief set X such that K fm   X  K mc , there is a (belief set) partial meet
contraction pm such that K pm  = X .
This result shows that every belief set between the results obtained from full meet
contraction and some maxichoice contraction can also be obtained from some partial meet
contraction. So, it is possible to define a version of belief set contraction based on such sets.
Definition 9 (Infra Remainder Sets) Given a formula   LP , T
and belief sets K and
0
0
00
K , K  K   if and only if there is some K  K such that ( K)  K 0  K 00 .
We refer to the elements of K   as the infra remainder sets of K with respect to .
It is easy to see that K   =  if and only if |= . Note that by definition the set of infra
remainder sets of a belief set K are required to be belief sets themselves.
All remainder sets with respect to a formula  are also infra remainder sets with respect
to , and so is the intersection of any set of remainder sets with respect to . Indeed, the
intersection of any set of infra remainder sets with respect to  is also an infra remainder
set with respect to . So the set of infra remainder sets with respect to  contains all belief
sets between some remainder set with respect to  and the intersection of all remainder
sets with respect to . This explains why infra contraction below is not defined as the
intersection of infra remainder sets (cf. Definition 4).
Definition 10 (Infra Contraction) Let K be a belief set. An infra selection function 
for K is a (partial) function from P(P(LP )) to P(LP ) such that  (K  ) = K whenever
K   = , and  (K  )  K   otherwise. A belief set contraction operator  is a belief
set infra contraction if and only if K   =  (K  ).
37

fiBooth, Meyer, Varzinczak, & Wassermann

In a sense, then, infra contraction is closer to maxichoice contraction than to partial meet
contraction, since it chooses just a single element from the set of infra remainder sets, much
like maxichoice contraction chooses just a single element from the set of remainder sets.
One immediate consequence of Theorem 3 and Definition 10 is the following:
Corollary 1 Infra contraction and partial meet contraction coincide for belief sets.

4. Belief Base Contraction
Now we turn our attention to belief base contraction, where an agents beliefs are represented
as a (usually finite) set of sentences not necessarily closed under logical consequence, also
known as a base. We usually denote bases by B, possibly decorated with primes.
Definition 11 (Belief Base Contraction) A base contraction  for a base B is a function from LP to P(LP ).
Intuitively the idea is that, for a fixed belief base B, contraction of a formula  from B
produces a new base B  .
Given the construction methods for belief set contraction already discussed in Section 3,
two obvious ways to define belief base contraction are to consider both partial meet contraction and kernel contraction for bases. Below we present both cases.
All definitions required for partial meet in the base case are analogous to those given in
the belief set case in Section 3. For the readers convenience, we state them explicitly here:
Definition 12 (Base Remainder Sets) Given a base B and a formula , X  B if
and only if
 X  B;
 X 6|= ; and
 For every X 0 such that X  X 0  B, X 0 |= .

We call the elements of B the base remainder sets of B with respect to .
Similarly to the belief set case, it is easy to verify that B =  if and only if |= .
Definition 13 (Selection Functions) A selection function  for a base B is a (partial)
function from P(P(LP )) to P(P(LP )) such that
 (B) = {B} if B = ; and
   (B)  B otherwise.

Definition 14 (Base Partial Meet Contraction) For a selection function
T , the belief
base contraction operator  generated by  and defined as B   =def (B) is a
base partial meet contraction.
38

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

Definition 15 (Base Maxichoice and Full Meet) Given a selection function ,  is
a base maxichoice contraction if and only if (B) is always a singleton set. It is a base
full meet contraction if and only if (B) = B whenever B 6= .
As in the belief set case, it can be checked that belief base full meet contraction is unique,
while base maxichoice contraction usually is not.
Hansson (1992) showed that belief base partial meet contraction can be characterized
by the following postulates:
(B  1) If 6|= , then B   6|= 

(Success)

(B  2) B    B

(Inclusion)

(B  3) If B 0 |=  if and only if B 0 |=  for all B 0  B, then B   = B   (Uniformity)
(B  4) If   (B \ (B  )), then there is a B 0 such that
(B  )  B 0  B and B 0 6|= , but B 0  {} |= 

(Relevance)

Again, we shall not elaborate in detail on the intuition of these postulates. For that we
refer the reader to the handbook by Hansson (1999).
Theorem 4 (Hansson, 1992) Every base partial meet contraction operator satisfies Postulates (B  1)(B  4). Conversely, every base contraction which satisfies (B  1)(B  4)
is a base partial meet contraction.
One question that arises is whether the following belief base version of the convexity
principle from Theorem 3 holds:
(Base Convexity) For a belief base B, let mc be a base maxichoice contraction, and
let fm denote base full meet contraction. For every set X and  such that B fm  
X  B mc , there is a base partial meet contraction pm such that B pm  = X.
As in the belief set case, this principle simply states that every set between the results
obtained from base full meet contraction and some base maxichoice contraction can be
obtained from some belief base partial meet contraction. The following example shows that
it does not hold.
Example 2 Let B = {p  q, q  r, pq  r, pr  q} and consider contraction by p  r.
It is easily verified that base maxichoice gives either B  (p  r) = B 0 = {p  q, p  r  q}
or B(p  r) = B 00 = {q  r, pq  r, pr  q}. Therefore the only other result obtained
from base partial meet contraction is that which is provided by base full meet contraction:
B  (p  r) = B 000 = {p  r  q}. But observe that even though it is the case that
B 000  X  B 00 where X = {p  q  r, p  r  q}, X is not equal to any of B 0 , B 00 , or B 000 .
We shall come back to this issue at the end of the section.
Below we give the definitions for kernel contraction in the belief base case. Again, they
are analogous to those given in the belief set case in the previous section, but for the readers
convenience we state them explicitly here.
39

fiBooth, Meyer, Varzinczak, & Wassermann

Definition 16 (Base Kernels) For a belief base B and a formula , X  B   if and
only if
 X  B;
 X |= ; and
 For every X 0 such that X 0  X, X 0 6|= .

B   is called the kernel set of B with respect to , and the elements of B   are called
the -kernels of B.
The result of a base kernel contraction is obtained by removing at least one element
from every (non-empty) -kernel of B, using an incision function.
Definition 17 (Incision Functions for Bases) An incision function  for a base B is a
function from the set of kernel sets of B to P(LP ) such that
S
 (B 
 )  (B 
 ); and
 If  =
6 X  B
 , then X  (B  ) 6= .

We refer to an incision function  as minimal if and only if for every , and every incision
function  0 , (B 
 )   0 (B 
 ). S
The (unique) maximum incision function  is defined
as follows: for every , (B 
 ) = (B  ).
It is worthy of mention that minimal incision functions always exist.
Definition 18 (Base Kernel Contraction) Given an incision function  for a base B,
the base kernel contraction  for B generated by  is defined as: B   = B \ (B  ).
Base kernel contraction can be characterized by the same postulates as base partial meet
contraction, except that Relevance is replaced by the Core-retainment postulate below:
(B  5) If   (B \ (B  )), then
there is some B 0  B such that B 0 6|=  but B 0  {} |= 

(Core-retainment)

Theorem 5 (Hansson, 1994) Every base kernel contraction satisfies Postulates (B  1)
(B  3) and (B  5). Conversely, every base contraction which satisfies Postulates (B  1)
(B  3) and (B  5) is a base kernel contraction.
Clearly, Core-retainment is slightly weaker than Relevance. And indeed, it thus follows
that all base partial meet contractions are base kernel contractions, but as the following
example from Hansson (1999) shows, some base kernel contractions are not base partial
meet contractions.
Example 3 Let B = {p, p  q, p  q}. Then B  (p  q) = {{p, p  q}, {p  q, p  q}}.
So there is an incision function  for B such that (B  (p  q)) = {p  q, p  q}, and then
B  (p  q) = {p}. On the other hand, B(p  q) = {{p, p  q}, {p  q}}, from which it
follows that base partial meet contraction B  (p  q) yields either {p, p  q}, or {p  q},
or {p, p  q}  {p  q} = , none of which are equal to B  (p  q) = {p}.
40

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

As we briefly pointed out after Definition 8, our definition of kernel contraction for belief
sets is closely related to a version of belief base contraction that Hansson (1999) refers to
as saturated base kernel contraction:
Definition 19 (Saturated Base Kernel Contraction) Given a belief base B and an
incision function  for B, the base contraction  for B generated by  and defined as
B   =def B  Cn(B  ) is a saturated base kernel contraction.
It is easily shown (Hansson, 1994) that when the set B in the definition for saturated base
kernel contraction is a belief set, the two notions coincide.
Observation 1 For a belief set K and an incision function  for K, the saturated base
kernel contraction for  and the belief set kernel contraction for  are identical.
While saturated base kernel contractions are not in general base partial meet contractions (Hansson, 1999, p. 91), this distinction disappears when considering belief sets only:
Theorem 6 (Hansson, 1994) Let K be a belief set. A belief set contraction  is a saturated base kernel contraction if and only if it is a belief set partial meet contraction.
And as a result of Observation 1 and Theorem 6 we immediately have the following corollary:
Corollary 2 Let K be a belief set. A belief set contraction  is a belief set kernel contraction if and only if it is a belief set partial meet contraction.
Thanks to Theorem 3 (Convexity) we can extend Corollary 2 to show that kernel contraction, partial meet contraction, and infra contraction all coincide for belief sets.
Corollary 3 Let K be a belief set. A belief set contraction  is a belief set kernel contraction if and only if it is a belief set partial meet contraction if and only if it is a belief set
infra contraction.
So far we have considered remainder sets for bases and kernel sets for bases, but not infra
remainder sets for bases (cf. end of Section 3). We conclude this section with a discussion
on this commencing with the definition of base infra remainder sets.
Definition 20 (Base Infra Remainder Sets) Given a formula
, and bases B and B 0 ,
T
0
00
B  B   if and only if there is some B  B such that ( B)  B 0  B 00 . We refer
to the elements of B   as the base infra remainder sets of B with respect to .
Observe that the definition of base infra remainder sets is the same as for infra remainder
sets, differing only in that it deals with belief bases and not belief sets. Note in particular
that the elements are not required to be belief sets themselves. Base infra remainder sets
can be used to define a form of base contraction in a way that is similar to the definition of
(belief set) infra contraction (cf. Definition 10).
41

fiBooth, Meyer, Varzinczak, & Wassermann

Definition 21 (Base Infra Contraction) A base infra selection function  is a (partial)
function from P(P(LP )) to P(LP ) such that  (B  ) = B whenever B   = , and
 (B  )  B   otherwise. A belief base contraction operator  generated by  and
defined as B   =def  (B  ) is a base infra contraction.
A natural question to ask is how base infra contraction compares with base partial meet
contraction and base kernel contraction. The following result, which plays a central role in
this paper, shows that base infra contraction corresponds exactly to base kernel contraction.
Theorem 7 A base contraction for a belief base B is a belief base kernel contraction for B
if and only if it is a base infra contraction for B.
From Example 3 we know that base kernel contraction is more general than base partial
meet contraction: every base partial meet contraction is also a base kernel contraction, but
the converse does not hold. From Theorem 7 the following result thus follows.
Observation 2 Base infra contraction is more general than base partial meet contraction.
Theorem 7 has a number of other interesting consequences as well. On a philosophical
note, it provides evidence for the contention that the kernel contraction approach is more
appropriate than the partial meet approach. As mentioned earlier, it is well-known that
kernel contraction and partial meet contraction coincide for belief sets, but differ for belief
bases (kernel contraction being more general than partial meet contraction in this case).
The importance of Theorem 7 is that it shows a new and seemingly different approach to
contraction (infra contraction) to be identical to kernel contraction for both belief sets and
belief bases, but different from partial meet contraction for belief bases, thereby tilting the
scales of evidence towards kernel contraction. As we shall see in the next section, Theorem 7
is also instrumental in lifting this result to the level of Horn belief sets.

5. Horn Belief Set Contraction
In the previous sections we recalled several results from the belief change literature and
stated new ones, connecting different constructions to sets of rationality postulates. It is
important to note that although most of the examples in the literature assume that the
underlying logic contains classical propositional logic, the results are usually valid for a
broader set of languages. This has been discussed in different contexts by Hansson and
Wassermann (2002), Flouris et al. (2006), Ribeiro and Wassermann (2009a, 2009b, 2010),
Varzinczak (2008, 2010), and Wassermann (2011), among others. For belief bases, it was
shown by Hansson and Wassermann (2002) that in order to keep the representation theorems
for partial meet and kernel contraction, the logic only needs to be compact and monotonic.
This means that the results transfer directly to Horn logics. However, for belief sets the
representation theorems demand more restrictions of the logic. As shown by Flouris et
al. (2006), a contraction operation satisfying the basic AGM postulates exists only if the
logic is decomposable.
Definition 22 (Flouris et al., 2006) A logic is called decomposable if and only if for all
sets of formulas X, X 0 , such that Cn()  Cn(X 0 )  Cn(X), there exists a set of formulas
X 00 such that Cn(X 00 )  Cn(X) and Cn(X) = Cn(X 0  X 00 ).
42

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

It was shown by Ribeiro (2010) that Horn logic is not decomposable. Therefore, new
constructions and sets of postulates are needed to deal with contraction of Horn belief sets.
Delgrande (2008) investigated two distinct classes of contraction functions for Horn
belief sets: entailment-based contraction (e-contraction), for removing an unwanted consequence; and inconsistency-based contraction (i-contraction), for removing formulas leading
to inconsistency; while Booth et al. (2009) subsequently extended the work of Delgrande
by providing more fine-grained versions of his constructions. Our focus in this paper is on
e-contraction, although Delgrande, as well as Booth et al., also consider i-contraction.
Recall that we use H, sometimes decorated with primes, to denote a Horn belief set.
Definition 23 (e-Contraction) An e-contraction  for a Horn belief set H is a function
from LH to P(LH ).
Delgrandes method of construction for e-contraction is in terms of partial meet contraction. The definitions of remainder sets (Definition 2), selection functions (Definition 3),
partial meet contraction (Definition 4), as well as maxichoice and full meet contraction (Definition 5) all carry over for e-contraction, with the set K in each case being replaced by a
Horn belief set H, and we refer to these as e-remainder sets (denoted by He ), e-selection
functions, partial meet e-contraction, maxichoice e-contraction and full meet e-contraction
respectively (we leave out the reference to the term Horn, since there is no room for
ambiguity here). As in the full propositional case, it is easy to verify that all e-remainder
sets are also Horn belief sets, and that all partial meet e-contractions (and therefore the
maxichoice e-contractions, as well as full meet e-contraction) produce Horn belief sets.
Although Delgrande defines and discusses partial meet e-contraction, he argues that
maxichoice e-contraction is the appropriate approach for e-contraction. To be more precise,
the version of maxichoice e-contraction that Delgrande advocates is actually a restricted version of maxichoice e-contraction that we refer to as orderly maxichoice e-contraction. (This
is just the Horn case of the maxichoice contraction operators satisfying all the AGM postulates, including the supplementary ones). Whereas (ordinary) maxichoice e-contraction
is constructed by setting, for every   LH , H   equal to any element of H, orderly
maxichoice e-contraction is more systematicSin nature. Associated with every orderly maxichoice e-contraction is a linear order  on LH (H), the union of all e-remainder sets
of H with respect to all sentencesS  LH . Intuitively, the higher up in the linear order,
the more plausible an element of LH (H) is intended to be. The orderly maxichoice
e-contraction generated from  is then obtained as follows: for every   LH , K   is
selected to be the most plausible e-remainder set of H with respect to  (the one that is
the highest up in the order ).
Here we argue that although all partial meet e-contractions are appropriate choices for
e-contraction, they do not make up the set of all appropriate e-contractions. In other words,
Delgrandes approach is not complete. The argument for appropriate e-contractions other
than partial meet e-contraction is based on the observation that the convexity result for full
propositional logic in Theorem 3 does not hold for Horn logic.
Example 4 Let H = CnHL ({p  q, q  r}). For the e-contraction of p  r from H, maxi1 = Cn ({p  q}) or H 2 = Cn ({q  r, p  r  q}), whereas full
choice yields either Hmc
HL
HL
mc
43

fiBooth, Meyer, Varzinczak, & Wassermann

meet yields Hf m = CnHL ({p  r  q}). These are the only three partial meet e-contractions.
Now consider the Horn belief set H 0 = CnHL ({p  q  r, p  r  q}). It is clear that
2 , but there is no partial meet e-contraction yielding H 0 .
Hf m  H 0  Hmc
In order to rectify this situation, we propose that e-contraction should be extended to
include cases such as H 0 above. The argument is as follows: Since full meet e-contraction is
deemed to be appropriate, it stands to reason that any belief set H 0 bigger than it should
also be seen as appropriate, provided that H 0 does not contain any irrelevant additions. But
since H 0 is contained in some maxichoice e-contraction, H 0 cannot contain any irrelevant
additions. After all, the maxichoice Horn e-contraction contains only relevant additions,
since it is an appropriate form of contraction. Hence H 0 is also an appropriate result of
e-contraction.
In summary, every Horn belief set between full meet and some maxichoice e-contraction
ought to be seen as an appropriate candidate for e-contraction. This is captured by the
definition below.
Definition 24 (Infra e-Remainder Sets) For Horn
belief sets H and H 0 , H 0  H e 
T
if and only if there is some H 00  He  such that ( He )  H 0  H 00 . We refer to the
elements of H e  as the infra e-remainder sets of H with respect to .
As with the case for full propositional logic (cf. Definition 9), e-remainder sets are also infra
e-remainder sets, and so is the intersection of any set of e-remainder sets. Similarly, the
intersection of any set of infra e-remainder sets is also an infra e-remainder set, and the
set of infra e-remainder sets contains all Horn belief sets between some e-remainder set and
the intersection of all e-remainder sets. As in the full propositional case, this explains why
e-contraction is not defined as the intersection of infra e-remainder sets (cf. Definition 4).
Definition 25 (Horn e-Contraction) Let H be a Horn belief set and   LH be a Horn
formula. An infra e-selection function  is a (partial) function from P(P(LH )) to P(LH )
such that
  (H e ) = H whenever H e  = ; and
  (H e )  H e  otherwise.

An e-contraction  is an infra e-contraction if and only if H   =  (H e ).
The results on how kernel contraction, partial meet contraction and infra contraction
compare for the base case (kernel contraction and infra contraction are identical, while both
are more general than partial meet contraction) invite the question whether similar results
hold for Horn belief sets. Before we provide an answer to this, we need a suitable version
of kernel contraction for Horn belief sets.
Definition 26 (Horn kernel e-contraction) Given a Horn belief set H and an incision function  for H, the Horn kernel e-contraction for H, abbreviated as the kernel econtraction for H, is defined as H e  =def CnHL (H  ), where  is the belief base
kernel contraction for  obtained from .
44

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

It turns out that infra e-contraction and kernel e-contraction coincide, as the following
result shows.
Theorem 8 An e-contraction for a Horn belief set H is an infra e-contraction for H if
and only if it is a kernel e-contraction for H.
From Theorem 8 and Example 4 it follows that partial meet e-contraction is more
restrictive than kernel e-contraction. When it comes to Horn belief sets, we therefore have
exactly the same pattern as we have for belief bases: kernel contraction and infra contraction
coincide, while both are strictly more permissive than partial meet contraction. Contrast
this with the case for belief sets for full propositional logic where infra contraction, partial
meet contraction and kernel contraction all coincide.
One conclusion to be drawn from this is that the restriction to the Horn case produces
a curious hybrid between belief sets and belief bases for full propositional logic. On the one
hand, Horn contraction deals with sets that are logically closed. But on the other hand, the
results for Horn logic obtained in terms of construction methods are close to those obtained
for belief base contraction.
Either way, the new results on belief base contraction prove to be quite useful in the
investigation of contraction for Horn belief sets. We conclude this section by providing
a representation result for Horn contraction inspired by the new results on belief base
contraction in this paper.
Theorem 9 Every infra e-contraction satisfies (K  1), (K  2), (K  4), (K  5) and
Core-retainment. Conversely, every e-contraction which satisfies (K  1), (K  2), (K  4),
(K  5), and Core-retainment is an infra e-contraction.
This result was inspired by Theorem 7 which shows that base kernel and base infra
contraction coincide. Given that Core-retainment is used in characterizing base kernel
contraction, Theorem 7 shows that there is a link between Core-retainment and base infra
contraction, and raises the question of whether there is a link between Core-retainment and
infra e-contraction. The answer, as we have seen in Theorem 9, is yes. This result provides
more evidence for the hybrid nature of contraction for Horn belief sets. In this case the
connection with base contraction is strengthened.

6. Related Work
To our knowledge, the first formal proposal of taking non-maximal remainder sets for a
contraction following the AGM principles was made by Restall and Slaney (1995). The
construction appears in a context very different from ours: they use a four-valued logic
and show that by dropping the Recovery postulate, they obtain a representation result
for partial-meet with non-maximal remainders. Note that there is no restriction as to the
remainder sets containing a minimal core, as was made for infra remainders, and as a result
there is no postulate associated with any kind of minimal change.
While there has been some work on revision for Horn formulas (Eiter & Gottlob, 1992;
Liberatore, 2000; Langlois, Sloan, Szorenyi, & Turan, 2008), it is only recently that attention
has been paid to contraction for Horn logic. Apart from the work of Delgrande (2008) which
45

fiBooth, Meyer, Varzinczak, & Wassermann

has been discussed, there is some recent work on obtaining a semantic characterisation of
Horn contraction using the system of spheres by Fotinopoulos and Papadopoulos (2009),
and via epistemic entrenchment by Zhuang and Pagnucco (2010).
Billington et al. (1999) considered revision and contraction for defeasible logic which is
quite different from Horn logic in many respects, but nevertheless has a rule-like flavour to
it which has some similarity to Horn logic.
The present paper is an extension of the work by Booth et al. (2009) in which they
show that infra e-contraction is captured precisely by the six AGM postulates for belief set
contraction, except that Recovery is replaced by the following (weaker) postulate (H e 6)
together with the Failure postulate:
(H e 6) If   H \ (H  ), then T
there exists an X such that (He )  X  H and X 6|= , but X  {} |= 
(H e 7) If |= , then H e  = H

(Failure)

Theorem 10 (Booth et al., 2009) Every infra e-contraction operator satisfies Postulates (K  1)(K  5), (H e 6) and (H e 7). Conversely, every e-contraction which
satisfies (K  1)(K  5), (H e 6) and (H e 7) is an infra e-contraction.
Postulate (H e 6) bears some resemblance to the Relevance postulate for base contraction in that it states that all sentences removed from H during a -contraction must have
been removed for a reason: adding them again brings back . Postulate (H e 7) simply
states that contracting with a tautology leaves the initial belief set unchanged.
It is worth noting that (H e 6) is a somewhat unusual postulate in that it refers
directly to the construction method it is intended to characterize, i.e., to e-remainder sets.
Here we have provided a more elegant characterization of infra e-contraction by taking a
detour through base contraction. Firstly, we replaced (H e 6) with the Core-retainment
postulate that is used in the characterization of base kernel contraction. Then it turns out
that the Vacuity postulate (K  3) and the Failure postulate (H e 7) both follow from
Core-retainment and the Inclusion postulate (K  2), from which we then obtained our
characterization of infra e-contraction.
In addition to e-contraction, Delgrande also investigated a version of Horn contraction
he refers to as inconsistency-based contraction (or i-contraction) where the purpose is to
modify an agents Horn belief set in such a way as to avoid inconsistency when a sentence 
is provided as input. That is, an i-contraction i should be such that (H i )  {} 6|=
.
HL
In addition to both e- and i-contraction, Booth et al. (2009) considered package contraction (or p-contraction), a version of contraction studied by Fuhrmann and Hansson (1994)
for the classical case (i.e., for logics containing full propositional logic). Given a set of
formulas X, the goal is to make sure that none of the sentences in X is in the result obtained from p-contraction. For full propositional logic this is similar to contracting with
the disjunction of the sentences in X. For Horn logic, which does not have full disjunction,
package contraction is more interesting. Although it seems that the new results presented
in this paper can be applied to both i-contraction and p-contraction, this still has to be
verified in detail.
46

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

Recent work by Delgrande and Wassermann (2010) draws inspiration from the semantic
representation of remainder sets. In the classical AGM approach, a remainder set can be
obtained semantically by adding to the models of a belief set H a counter-model of the
formula  for contraction. With Horn clauses, this construction will not necessarily lead to
sets that correspond to remainder sets. This is because, as is known in Horn logic, given
any two models of m1 , m2 of a Horn belief set H, the model m1 u m2 is also a model of H,
where m1 u m2 denotes that model in which the atoms true in it are precisely the atoms
which are true in both m1 and m2 . To illustrate this, consider the following example:
Example 5 (Delgrande & Wassermann, 2010) Let P = {p, q, r} and let the Horn belief set H = CnHL (p  q). Consider candidates for H  (p  q). There are three e-remainder
sets, given by the Horn closures of p  (r  q), q  (r  p), and (p  q)  (q  p)  (r 
p)  (r  q). Any infra e-remainder set contains the closure of (r  p)  (r  q).
In Example 5, the unique model of the set {p, q, r} is a countermodel of p  q. If we add it
to the models of H and close them under u so that the result represents a Horn belief set,
we obtain the set of models of the formula p, which cannot be an e-remainder set, as there
is an e-remainder set (CnHL (p  (r  q))) containing it. This means that in an approach
based on e-remainder sets we cannot have contraction operators with the same behaviour
as the classical ones.
Delgrande and Wassermann propose to mimic the semantic construction of classical
remainders: a weak-remainder of H and  is formed by intersecting H with a maximal
consistent Horn theory not containing . Equivalently, the weak-remainders can be semantically characterized by taking the closure under intersection of the models of H together
with a single counter-model of . Representation results for contraction based on weakremainders are then provided.
Concerning the connection between partial meet contraction and kernel contraction for
bases, Falappa et al. (2006) have shown how to construct a partial meet contraction from
a kernel contraction, which is always possible, and how to construct a kernel contraction
from a partial meet contraction, when this is possible. These results can be generalized so
that they apply to base infra contraction and base kernel contraction. That is, it should be
possible to construct a base infra contraction from every base kernel contraction, as well as
a base kernel contraction from every base infra contraction.

7. Concluding Remarks
In bringing Hanssons kernel contraction into the picture, we have made meaningful contributions to the investigation into contraction for Horn logic. The main contributions of
the present paper are as follows: (i) A result which shows that infra contraction and kernel
contraction for the belief base case coincide; and (ii) Lifting the previous results to Horn
belief sets to show that infra contraction and kernel contraction for Horn belief sets coincide.
The investigation into base contraction also allowed us to improve on the rather unsatisfactory representation result proved by Booth et al. for infra contraction, which relies
on a postulate referring directly to the construction method it is intended to characterize.
We obtained a more elegant representation result by replacing the postulate introduced by
47

fiBooth, Meyer, Varzinczak, & Wassermann

Booth et al. with the well-known Core-retainment postulate, which is usually associated
with base contraction. The presence of Core-retainment here further enforces the hybrid
nature of Horn belief change, lying somewhere between belief set change and base change.
We have seen that kernel e-contraction and infra e-contraction are more general than
partial meet e-contraction. But there is also evidence that even these forms of Horn contraction may not be sufficient to obtain all meaningful answers. Consider, for example, our
Horn belief set example CnHL ({p  q, q  r}) encountered in Example 4. If we view basic
Horn clauses (clauses with exactly one atom in the head and the body) as representative
of arcs in a graph, in the style of the old inheritance networks, there is a case to be made
for regarding the three arcs p  q, q  r, and p  r, as the basic information used to
generate the belief set. Then one possible desirable outcome of a contraction by p  r is
CnHL (q  r). However, as we have seen in Example 4, this is not an outcome supported
by infra e-contraction (and therefore not by kernel e-contraction either). Ideally, a truly
comprehensive e-contraction approach for Horn logic would be able to account for such
cases as well.
Here we focus only on basic Horn contraction. For future work we plan to investigate
Horn contraction for full AGM contraction, obtained by adding the extended postulates.
Another interesting question for further investigation is whether Theorem 9 can be generalized to other logics, notably extensions of (full) propositional logic.
Finally, as mentioned earlier, one of the reasons for focusing on this topic is because of
its application to debugging and repairing ontologies in description logics. In particular,
Horn logic can be seen as the backbone of the EL family of description logics (Baader et al.,
2005), and therefore a proper understanding of belief change for Horn logic is important
for finding solutions to similar problems expressible in the EL family. We are currently
investigating these possibilities.

Acknowledgments
This paper extends the work by Booth, Meyer and Varzinczak that appeared at IJCAI (Booth
et al., 2009), and the work by Booth, Meyer, Varzinczak and Wassermann that appeared
at NMR (Booth et al., 2010a) and at ECAI (Booth et al., 2010b).
The authors are grateful to the anonymous referees for their constructive and useful
remarks, which helped improving the quality and presentation of this work. We would also
like to thank Eduardo Ferme who provided important hints on the connection between infra
and kernel contraction.
The work of Richard Booth was supported by the FNR INTER project Dynamics of
Argumentation. The work of Thomas Meyer and Ivan Varzinczak was supported by the
National Research Foundation under Grant number 65152. The work of Renata Wassermann was supported by CNPq, the Brazilian National Research Council, under grants
304043/2010-9 and 471666/2010-6.

Appendix A. Proofs of Main Theorems
For the proofs of Lemma A.1 and Theorem 3 we need a few model-theoretic notions. We
denote by W the set of valuations of LP and by [] the set of models of , i.e., the set of
48

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

valuations which satisfy . For a set of sentences X, we denote by [X] the set of models
of X, i.e., the set of valuations which satisfy all sentences in X. For any V  W, we let
T h(V ) = {  LP | V  []}.
The results in Lemma A.1 below are well-known model-theoretic descriptions of full
meet, partial meet, and maxichoice contraction, first presented by Katsuno & Mendelzon (1991). The results, as stated below, are not stated directly by Katsuno & Mendelzon,
but follow as easy consequences of their Theorem 3.3.
Lemma A.1 (Katsuno & Mendelzon, 1991) Let K be a belief set.
1. Let fm be the (belief set) full meet contraction. For every   LP , K fm  =
Th([K]  []);
2. Let mc be a (belief set) maxichoice contraction. For every   LP , there is a w  []
such that K mc  = Th([K]  {w});
3. Given   LP , let V  [] be such that V 6= . There is a (belief set) partial meet
contraction pm such that for every   LP , K pm  = Th([K]  V ).
Theorem 3 (Convexity) Let K be a belief set, let mc be a (belief set) maxichoice contraction, and let fm denote (belief set) full meet contraction. For every   LP , let X
be a belief set such that (K fm )  X  K mc . There is a (belief set) partial meet
contraction pm such that for every   LP , K pm  = X .
Proof:
The cases where |=  and  
/ K hold easily, so we suppose that 6|=  and   K. Now, pick
any belief set X such that K fm   X  K mc . By Points (1) and (2) of Lemma A.1
this means that there is a w  [] such that [K]  {w}  [X ]  K  []. From this
it follows that [X ] = [K]  [V ] for some V  []. And from Point (3) of Lemma A.1
it then follows that there is a partial meet contraction pm such that, for every   LP ,
K pm  = X = T h([K]  V ).
[qed]
Lemma A.2 (Falappa et al., 2006) Every base kernel contraction defined by a minimal
incision function is a base maxichoice contraction.
Lemma A.3 The base kernel contraction defined by the maximal incision function is the
full meet base contraction.
Proof:
S
T
We need to prove that B \ (B 
 ) = (B). If |=  or B 6|=  the result follows
immediately. So we suppose
that 6|=  and B |= . For the left-to-right direction, suppose
S
that   B and  
/ (B 
 ), and assume there is some base remainder set of B with
respect to , say X, such that  
/ X.S Then it must be the case that  is contained
in some -kernel of B. But then   (B  ): a contradiction. For the right-to-left
direction, suppose
 is in every base remainder set of BSwith respect to , and assume
S
that  
/ B \ (B 
 ). Since   B it follows that   (B  ). That is,  is in some
-kernel of B. So there is at least one minimal incision function  such that   (B  ).
49

fiBooth, Meyer, Varzinczak, & Wassermann

Now, from Lemma A.2 it follows that there is a base remainder set of B with respect to ,
say Y , such that Y = B \ (B 
 ). So  
/ Y , which contradicts the supposition that 
is in every base remainder set of B with respect to .
[qed]
Theorem 7 A base contraction for a belief base B is a belief base kernel contraction for B
if and only if it is a base infra contraction for B.
Proof:
For the only if part, let  be a base kernel contraction for B, let  be the incision function
for B which generates , and pick a   LP . If |=  or B 6|=T the result holds easily, so we
suppose that 6|=  and B |= . It remains
to be shown that (B)  B    B 0 for some
S
0
B  B. Observe firstly
it
Tthat B \ (B  )  B \(B  ) = B . From Lemma A.3
0
then follows directly that (B)  B. Next, pick any minimal incision function  such
that  0 (B 
 )  (B 
 ) ( 0 clearly exists). So B   = B \ (B  )  B \  0 (B  )
and from Lemma A.2 it follows that B \  0 (B  ) = B 0 for some B 0  B.
For the if part, let  be a base infra contraction for B. We construct an incision
function  such that the base kernel contraction it generates is exactly . Pick a   LP
and suppose that 6|=  and B |=  (for the remaining cases the result easily holds). Let
(B  ) = B \(B ). Since
S B   B we have that B  = B \(B  ). First we need
to show
that
(B


)

(B 
 ). To do so, observe T
that since B    B  , it follows
T
that (B)  B  . Therefore B \ (B  )  B \ (B). By
T Lemma A.3
S and the
construction of  we then have that (B  ) = B \ (B  )  B \ (B) = (B  ).
It remains to be shown that for every Y such that   Y  B  , Y  (B  ) 6= .
Pick a Y such that   Y  B 
 , and assume that Y  (B  ) = . Then it must be
the case that Y  B  . But Y |=  and therefore B   |= , a contradiction.
[qed]
Theorem 8 An e-contraction for a Horn belief set H is an infra e-contraction for H if and
only if it is a kernel e-contraction for H.
Proof:
Consider a belief base B and a formula . From Theorem 7 it follows that the set of base
infra remainder sets of B with respect to  (i.e., the set B  ) is equal to the set of results
obtained from the base kernel contraction of B by , call it KCB . Now let B be such that
it is a set of Horn formulas closed under Horn consequence (a Horn belief set) and  a Horn
formula. The elements of KCB are not necessarily closed under Horn consequence, but if
we do close them, we obtain exactly the set of results obtained from kernel e-contraction
when contracting B by  (by the definition of kernel e-contraction). Let us refer to this
latter set as CnHL (KCB ), i.e., CnHL (KCB ) = {CnHL (X) | X  KCB }. Also, the elements
of B   are not closed under Horn consequence, but if we do close them the resulting
set (refer to this set as CnHL (B  )) contains exactly the infra e-remainder sets of B with
respect to , i.e., CnHL (B  ) = B e . To see why, observe that since B is closed under
Horn consequence, the (base) remainder sets of B with respect to  (i.e., the elements of
B) are also closed under Horn consequence. So theT
elements of B   are all the sets (not
necessarily closed under Horn consequence) between (B) and some element of B.
Therefore the elements of CnHL (B  ) are all those elements of B   that are closed under
Horn consequence, i.e., CnHL (B  ) = B e  as was claimed. But since B   = KCB , it is
also the case that CnHL (B  ) = CnHL (KCB ), and therefore CnHL (KCB ) = B e . [qed]
50

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

Theorem 9 Every infra e-contraction satisfies (K  1), (K  2), (K  4), (K  5) and
Core-retainment. Conversely, every e-contraction which satisfies (K  1), (K  2), (K  4),
(K  5), and Core-retainment is an infra e-contraction.
Proof:
Let H be a Horn belief set, and let  be an infra e-contraction. If H   = H, then
(K  1), (K  2), (K  4), (K  5) and Core-retainment are trivially satisfied. Suppose
H   6= H. Then H   = X for some X  H e . Then, by definition, H   is a
Horn belief set, and hence H   = CnHL (H  ) (Postulate (K  1)). Moreover, there is
an X 0  He  such that X  X 0 . Since X 0  H, it also follows that X  H, and then
H    H (Postulate (K  2)). (K  4) is also satisfied from the definition of H e  and
the monotonicity of classical logic. (K  5) follows straightforwardly from the fact that we
are working with Horn belief sets. Finally, for Core-retainment, suppose that   H and
 
/ H  . Now assume that H 0  {} 6|=
 for every H 0  H such that H 0 6|=
. In
HL
HL
0
0
particular then, for every H  He , H  {}T6|=HL . From that it follows that   H 0
for every H 0  He , and therefore that   (He ). And from this we have that
  H  , which contradicts our supposition.
Conversely, let H be a Horn belief set and let  be an e-contraction which satisfies
Postulate (K  1), (K  2), (K  4), (K  5), and Core-retainment. We need to show that
for every   LH , H    H e . Observe firstly that  satisfies (K  3). To see why, note
that from Core-retainment it follows that if  
/ H then H \ (H  ) = , and therefore
that H  H  . From (K  2) it then follows that H = H  .
If  
/ H it follows directly from (K  3) that H    H e . If |=
 it follows directly
HL
from (K  2) and Core-retainment that H   = H, and therefore that H    H e .
So, we suppose that   H and 6|=
. To show that H    T
H e , we need to show
HL
that H   is a Horn belief set (from Postulate (K  1)) such that (He )  H    H 0
for some H 0  He . Observe firstly that, since H    H (by Postulate (K  2)) and
0
0

/ H   (by (K  4)),
T there has to be an H  He  such that H    H . Finally,
assume there is a   (He ) such that  
/ H  . By Core-retainment it then follows
00
00
that there is an H  H such that H 6|=
 but H 00  {} |=
. From this it follows
HL
HL
that there is a Horn belief set X such that H 00  X and X 6|=HL  but
T X  {} |=HL . So
X  He , T
but  
/ X, which contradicts our assumption that   (He ). It therefore
follows that (H)  H  .
[qed]

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet contraction and revision functions. Journal of Symbolic Logic, 50, 510
530.
Alchourron, C., & Makinson, D. (1985). On the logic of theory change: safe contraction.
Studia Logica, 44, 405422.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings of the 19th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 364369. Morgan Kaufmann Publishers.
51

fiBooth, Meyer, Varzinczak, & Wassermann

Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2007).
The Description Logic Handbook: Theory, Implementation and Applications (2 edition). Cambridge University Press.
Billington, D., Antoniou, G., Governatori, G., & Maher, M. (1999). Revising nonmonotonic
theories: The case of defeasible logic. In Proceedings of the 23rd Annual German
Conference on Artificial Intelligence, No. 1701 in LNAI, pp. 101112. Springer-Verlag.
Booth, R., Meyer, T., & Varzinczak, I. (2009). Next steps in propositional Horn contraction. In Boutilier, C. (Ed.), Proceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI), pp. 702707. AAAI Press.
Booth, R., Meyer, T., Varzinczak, I., & Wassermann, R. (2010a). A contraction core for Horn
belief change: Preliminary report. In 13th International Workshop on Nonmonotonic
Reasoning (NMR).
Booth, R., Meyer, T., Varzinczak, I., & Wassermann, R. (2010b). Horn belief change:
A contraction core. In Proceedings of the 19th European Conference on Artificial
Intelligence (ECAI), pp. 10651066.
Delgrande, J. (2008). Horn clause belief change: Contraction functions. In Lang, J., &
Brewka, G. (Eds.), Proceedings of the 11th International Conference on Principles
of Knowledge Representation and Reasoning (KR), pp. 156165. AAAI Press/MIT
Press.
Delgrande, J., & Wassermann, R. (2010). Horn clause contraction functions: Belief set
and belief base approaches. In Proceedings of the 12th International Conference on
Principles of Knowledge Representation and Reasoning (KR). AAAI Press.
Eiter, T., & Gottlob, G. (1992). On the complexity of propositional knowledge base revision,
updates, and counterfactuals. Artificial Intelligence, 57 (23), 227270.
Falappa, M., Ferme, E., & Kern-Isberner, G. (2006). On the logic of theory change: Relations between incision and selection functions. In Brewka, G., Coradeschi, S., Perini,
A., & Traverso, P. (Eds.), Proceedings of the 17th European Conference on Artificial
Intelligence (ECAI), pp. 402406. IOS Press.
Flouris, G., Plexousakis, D., & Antoniou, G. (2006). On generalizing the AGM postulates.
In Proceedings of the 3rd Starting AI Researchers Symposium, pp. 132143.
Fotinopoulos, A., & Papadopoulos, V. (2009). Semantics for Horn contraction. In 7th
PanHellenic Logic Symposium, pp. 4247.
Fuhrmann, A., & Hansson, S. (1994). A survey of multiple contractions. Journal of Logic,
Language and Information, 3, 3976.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States.
MIT Press.
Gardenfors, P., & Rott, H. (1995). Belief revision. In Handbook of Logic in Artificial
Intelligence and Logic Programming, Vol. 4, pp. 35132. Clarendon Press.
Hansson, S. (1992). A dyadic representation of belief. In Belief Revision, Vol. 29 of Cambridge Tracts in Theoretical Computer Science, pp. 89121. Cambridge University
Press.
52

fiPartial Meet, Kernel, and Infra Contraction in Horn Logic

Hansson, S. (1994). Kernel contraction. Journal of Symbolic Logic, 59 (3), 845859.
Hansson, S. (1999). A Textbook of Belief Dynamics: Theory Change and Database Updating.
Kluwer Academic Publishers.
Hansson, S., & Wassermann, R. (2002). Local change. Studia Logica, 70 (1), 4976.
Horn, A. (1951). On sentences which are true of direct unions of algebras. Journal of
Symbolic Logic, 16, 1421.
Katsuno, H., & Mendelzon, A. (1991). Propositional knowledge base revision and minimal
change. Artificial Intelligence, 3 (52), 263294.
Langlois, M., Sloan, R., Szorenyi, B., & Turan, G. (2008). Horn complements: Towards
Horn-to-Horn belief revision. In Fox, D., & Gomes, C. (Eds.), Proceedings of the 23rd
National Conference on Artificial Intelligence (AAAI), pp. 466471. AAAI Press.
Levi, I. (1977). Subjunctives, dispositions and chances. Synthese, 34, 423455.
Liberatore, P. (2000). A framework for belief update. In Proceedings of the 7th European
Conference on Logics in Artificial Intelligence (JELIA), pp. 361375.
Restall, G., & Slaney, J. (1995). Realistic belief revision. In Proceedings of the Second World
Conference on Foundations of Artificial Intelligence (WOCFAI), pp. 367378.
Ribeiro, M. (2010). Belief Revision in Description Logics and Other Non-Classical Logics.
Ph.D. thesis, University of Sao Paulo.
Ribeiro, M., & Wassermann, R. (2009a). AGM revision in description logics. In Workshop
on Automated Reasoning about Context and Ontology Evolution (ARCOE).
Ribeiro, M., & Wassermann, R. (2009b). Base revision for ontology debugging. Journal of
Logic and Computation, 19 (5), 721743.
Ribeiro, M., & Wassermann, R. (2010). More about AGM revision in description logics. In
Workshop on Automated Reasoning about Context and Ontology Evolution (ARCOE).
Varzinczak, I. (2008). Action theory contraction and minimal change. In Lang, J., &
Brewka, G. (Eds.), Proceedings of the 11th International Conference on Principles
of Knowledge Representation and Reasoning (KR), pp. 651661. AAAI Press/MIT
Press.
Varzinczak, I. (2010). On action theory change. Journal of Artificial Intelligence Research,
37, 189246.
Wassermann, R. (2011). On AGM for non-classical logics. Journal of Philosophical Logic,
40 (1), 124.
Zhuang, Z., & Pagnucco, M. (2010). Horn contraction via epistemic entrenchment. In
Janhunen, T., & Niemela, I. (Eds.), Proceedings of the 12th European Conference on
Logics in Artificial Intelligence (JELIA), No. 6341 in LNCS, pp. 339351. SpringerVerlag.

53

fi