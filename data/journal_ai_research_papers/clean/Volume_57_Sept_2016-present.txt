Journal of Artificial Intelligence Research 57 (2016) 187-227

Submitted 9/15; published 10/16

Multi-objective Reinforcement Learning through
Continuous Pareto Manifold Approximation
Simone Parisi

parisi@ias.tu-darmstadt.de

Technische Universitat Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany

Matteo Pirotta
Marcello Restelli

matteo.pirotta@polimi.it
marcello.restelli@polimi.it

Politecnico di Milano
Piazza Leonardo da Vinci 32, 20133 Milano, Italy

Abstract
Many real-world control applications, from economics to robotics, are characterized by
the presence of multiple conflicting objectives. In these problems, the standard concept
of optimality is replaced by Paretooptimality and the goal is to find the Pareto frontier,
a set of solutions representing different compromises among the objectives. Despite recent advances in multiobjective optimization, achieving an accurate representation of the
Pareto frontier is still an important challenge. In this paper, we propose a reinforcement
learning policy gradient approach to learn a continuous approximation of the Pareto frontier in multiobjective Markov Decision Problems (MOMDPs). Differently from previous
policy gradient algorithms, where n optimization routines are executed to have n solutions,
our approach performs a single gradient ascent run, generating at each step an improved
continuous approximation of the Pareto frontier. The idea is to optimize the parameters
of a function defining a manifold in the policy parameters space, so that the corresponding
image in the objectives space gets as close as possible to the true Pareto frontier. Besides
deriving how to compute and estimate such gradient, we will also discuss the nontrivial
issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally,
the properties of the proposed approach are empirically evaluated on two problems, a
linear-quadratic Gaussian regulator and a water reservoir control task.

1. Introduction
Multiobjective sequential decision problems are characterized by the presence of multiple
conflicting objectives and can be found in many real-world scenarios, such as economic
systems (Shelton, 2001), medical treatment (Lizotte, Bowling, & Murphy, 2012), control of
water reservoirs (Castelletti, Pianosi, & Restelli, 2013), elevators (Crites & Barto, 1998) and
robots (Nojima, Kojima, & Kubota, 2003; Ahmadzadeh, Kormushev, & Caldwell, 2014),
just to mention a few. Such problems are often modeled as Multiobjective Markov Decision
Processes (MOMDPs), where the concept of optimality typical of MDPs is replaced by the
one of Pareto optimality, that defines a compromise among the different objectives.
In the last decades, Reinforcement Learning (RL) (Sutton & Barto, 1998) has established
as an effective and theoretically grounded framework that allows to solve singleobjective
MDPs whenever either no (or little) prior knowledge is available about system dynamics or
the dimensionality of the system to be controlled is too high for classical optimal control
2016 AI Access Foundation. All rights reserved.

fiParisi, Pirotta, & Restelli

methods. Multiobjective Reinforcement Learning (MORL), instead, concerns MOMDPs
and tries to solve sequential decision problems with two or more conflicting objectives.
Despite the successful development in RL theory and a high demand for multiobjective
control applications, MORL is still a relatively young and unexplored research topic.
MORL approaches can be divided in two categories, based on the number of policies
they learn (Vamplew, Dazeley, Berry, Issabekov, & Dekker, 2011): single and multiple
policy. Although most of MORL approaches belong to the former category, here we present
a multiplepolicy approach, able to learn a set of policies approximating the Pareto frontier.
A representation of the complete Pareto frontier, in fact, allows a posteriori selection of a
solution and encapsulates all the trade-offs among the objectives, giving better insights into
the relationships among the objectives. Among multiplepolicy algorithms it is possible to
identify two classes: valuebased (Lizotte et al., 2012; Castelletti et al., 2013; Van Moffaert
& Nowe, 2014), that search for optimal solutions in value functions space, and policy gradient approaches (Shelton, 2001; Parisi, Pirotta, Smacchia, Bascetta, & Restelli, 2014), that
search through policy space. In practice, each approach has different advantages. Value
based methods usually have stronger guarantees of convergence, but are preferred in domains with lowdimensional state-action spaces as they are prone to suffer from the curse
of dimensionality (Sutton & Barto, 1998). On the other hand, policy gradient methods
have been very favorable in many domains such as robotics as they allow taskappropriate
prestructured policies to be integrated straightforwardly (Deisenroth, Neumann, & Peters,
2013) and experts knowledge can be incorporated with ease. By selecting a suitable policy
parametrization, the learning problem can be simplified and stability as well as robustness
can frequently be ensured (Bertsekas, 2005). Nonetheless, both approaches lack of guarantees of uniform covering of the true Pareto frontier and the quality of the approximate
frontier, in terms of accuracy (distance from the true frontier) and covering (its extent),
is related to the metric used to measure the discrepancy from the true Pareto frontier.
However, nowadays the definition of such metric is an open problem in MOO literature.
In this paper, we overcome these limitations proposing a novel gradientbased MORL
approach and alternative quality measures for approximate frontiers. The algorithm, namely
ParetoManifold Gradient Algorithm (PMGA), exploiting a continuous approximation of
the locally Paretooptimal manifold in the policy space, is able to generate an arbitrarily
dense approximate frontier. This article is an extension of a preliminary work presented
by Pirotta, Parisi, and Restelli (2015) and its main contributions are: the derivation of the
gradient approach in the general case, i.e., independent from the metric used to measure
the quality of the current solution (Section 3), how to estimate such gradient from samples
(Section 4), a discussion about frontier quality measures that can be effectively integrated
in the proposed approach (Section 5), a thorough empirical evaluation of the proposed
algorithm and metrics performance in a multiobjective discrete-time Linear-Quadratic
Gaussian regulator and in a water reservoir management domain (Sections 6 and 7).

2. Preliminaries
In this section, we first briefly summarize the terminology as used in the paper and discuss
about state-of-the-art approaches in MORL. Subsequently, we focus on describing policy
gradient techniques and we introduce the notation used in the remainder of the paper.
188

fiMORL through Continuous Pareto Manifold Approximation

2.1 Problem Formulation
A discretetime continuous Markov Decision Process (MDP) is a mathematical framework
for modeling decision making. It is described by a tuple hS, A, P, R, , Di, where S  Rn
is the continuous state space, A  Rm is the continuous action space, P is a Markovian
transition model where P(s0 |s, a) defines the transition density between state s and s0 under
action a, R : S  A  S  R is the reward function,   [0, 1) is the discount factor, and D
is a distribution from which the initial state is drawn. In this context, the behavior of an
agent is defined by a policy, i.e., a density distribution (a|s) that specifies the probability
of taking action a in state s. Given the initial state distribution D, it is possible to define
the expected return J  associated to a policy  as
"T 1
#
X

t
J =
E
 R(st , at , st+1 )|s0  D ,
st P,at 

t=0

being R(st , at , st+1 ) the immediate reward obtained when state st+1 is reached executing
action at from state st , and T the finite or infinite time horizon. The goal of the agent is
to maximize such a return.
Multiobjective Markov Decision Processes (MOMDPs) are an extension of MDPs in
which several pairs of reward functions and discount factors are defined, one for each
objective. Formally, a MOMDP is described by a tuple hS, A, P, R, , Di, where R =
[R1 , . . . , Rq ]T and  = [1 , . . . , q ]T are qdimensional column vectors of reward functions
Ri : S  A  S  R and discount factors i  [0, 1), respectively.
In MOMDPs, any policy




 is associated to q expected returns J = J1 , . . . , Jq , where
"T 1
#
X
Ji =
E
it Ri (st , at , st+1 )|s0  D .
st P,at 

t=0

Unlike what happens in MDPs, in MOMDPs a single policy dominating all the others
usually does not exist, as when conflicting objectives are considered, no policy can simultaneously maximize all of them. For this reason, in Multiobjective Optimization (MOO)
the concept of Pareto dominance is used. Policy  strongly dominates policy  0 , denoted
by    0 , if it is superior on all objectives, i.e.,
0

   0  i  {1, . . . , q} , Ji > Ji .
Similarly, policy  weakly dominates policy  0 , denoted by    0 , if it is not worse on all
objectives, i.e.,
0

0

   0  i  {1, . . . , q} , Ji  Ji  i  {1, . . . , q} , Ji = Ji .
If there is no policy  0 such that  0  , the policy  is Paretooptimal. We can also speak
of locally Paretooptimal policies, for which the definition is the same as above, except
that we restrict the dominance to a neighborhood of . In general, there are multiple
(locally) Paretooptimal policies. Solving
a MOMDP

	 is equivalent to determine the set
 =  | @ 0 ,  0   , which maps to the socalled Pareto
of all Paretooptimal
policies

 
	
frontier F = J |    .1
1. As done by Harada, Sakuma, and Kobayashi (2006), we assume that locally Paretooptimal solutions
that are not Paretooptimal do not exist.

189

fiParisi, Pirotta, & Restelli

2.2 Related Work
In Multiobjective Optimization (MOO) field, there are two common solution concepts:
multiobjective to singleobjective strategy and Pareto strategy. The former approach
derives a scalar objective from the multiple objectives and, then, uses the standard Single
objective Optimization (SOO) techniques: weighted sum (Athan & Papalambros, 1996),
normbased (Yu & Leitmann, 1974; Koski & Silvennoinen, 1987), sequential (Romero,
2001), constrained (Waltz, 1967), physical programming (Messac & Ismail-Yahaya, 2002)
and min-max methods (Steuer & Choo, 1983). The latter strategy is based on the concept of
Pareto dominance and considers Paretooptimal solutions as non-inferior solutions among
the candidate solutions. The main exponent of this class is the convex hull method (Das &
Dennis, 1998; Messac, Ismail-Yahaya, & Mattson, 2003).
Similar to MOO, current MORL approaches can be divided into two categories based
on the number of policies they learn (Vamplew et al., 2011). Singlepolicy methods aim
at finding the best policy that satisfies a preference among the objectives. The majority
of MORL approaches belong to this category and differ for the way in which preferences
are expressed. They are easy to implement, but require a priori decision about the type
of the solution and suffer of instability, as small changes on the preferences may result
in significant variations in the solution (Vamplew et al., 2011). The most straightforward
and common singlepolicy approach is the scalarization where a function is applied to the
reward vector in order to produce a scalar signal. Usually, a linear combination weighted
sum of the rewards is performed and the weights are used to express the preferences over
multiple objective (Castelletti, Corani, Rizzolli, Soncinie-Sessa, & Weber, 2002; Natarajan
& Tadepalli, 2005; Van Moffaert, Drugan, & Nowe, 2013). Less common is the use of non
linear mappings (Tesauro, Das, Chan, Kephart, Levine, Rawson, & Lefurgy, 2008). The
main advantage of scalarization is its simplicity. However, linear scalarization presents some
limitations: it is not able to find solutions that lie in the concave or linear region of the
Pareto frontier (Athan & Papalambros, 1996) and a uniform distribution of the weights may
not produce accurate and evenly distributed points on the Pareto frontier (Das & Dennis,
1997). In addition, even if the frontier is convex, some solutions cannot be achieved through
scalarization because a loss in one objective may not be compensated by an increment
in another one (Perny & Weng, 2010). Different singlepolicy approaches are based on
thresholds and lexicographic ordering (Gabor, Kalmar, & Szepesvari, 1998) or different
kinds of preferences over the objective space (Mannor & Shimkin, 2002, 2004).
Multiplepolicy approaches, on the contrary, aim at learning multiple policies in order
to approximate the Pareto frontier. Building the exact frontier is generally impractical in
real-world problems, thus, the goal is to build an approximation of the frontier that contains
solutions that are accurate, evenly distributed along the frontier and have a range similar
to Pareto one (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003). There are many
reasons behind the superiority of the multiplepolicy methods: they permit a posteriori
selection of the solution and encapsulate all the trade-offs among the multiple objectives.
In addition, a graphical representation of the frontier can give better insights into the relationships among the objectives that can be useful for understanding the problem and the
choice of the solution. However, all these benefits come at a higher computational cost,
that can prevent learning in online scenarios. The most common approach to approximate
190

fiMORL through Continuous Pareto Manifold Approximation

the Pareto frontier is to perform multiple runs of a singlepolicy algorithm by varying the
preferences among the objectives (Castelletti et al., 2002; Van Moffaert et al., 2013). It
is a simple approach but suffers from the disadvantages of the singlepolicy method used.
Besides this, few other examples of multiplepolicy algorithms can be found in literature.
Barrett and Narayanan (2008) proposed an algorithm that learns all the deterministic policies defining the convex hull of the Pareto frontier in a single learning process. Recent
works have focused on the extension of fitted Q-iteration to the multiobjective scenario.
While Lizotte, Bowling, and Murphy (2010), and Lizotte et al. (2012) have focused on a
linear approximation of the value function, Castelletti, Pianosi, and Restelli (2012) are able
to learn the control policy for all the linear combinations of preferences among the objectives in a single learning process. Finally, Wang and Sebag (2013) proposed a MonteCarlo
Tree Search algorithm able to learn solutions lying in the concave region of the frontier.
Nevertheless, these classic approaches exploit only deterministic policies that result in
scattered Pareto frontiers, while stochastic policies give a continuous range of compromises
among objectives (Roijers, Vamplew, Whiteson, & Dazeley, 2013; Parisi et al., 2014). Shelton (2001, Section 4.2.1) was the pioneer both for the use of stochastic mixture policies and
gradient ascent in MORL. He achieved two well known goals in MORL: simultaneous and
conditional objectives maximization. In the former, the agent must maintain all goals at the
same time. The algorithm starts with a mixture of policies obtained by applying standard
RL techniques to each independent objective. The policy is subsequently improved following
a convex combination of the gradients in the policy space that are nonnegative w.r.t. all
the objectives. For each objective i, the gradient gi of the expected return w.r.t. the policy
is computed and the vector vi having the highest dot product with gi and simultaneously
satisfying the nonnegativity condition for all the returns is used as improving direction
for the i-th reward. The vectors vi are combined in a convex form to obtain the direction
of the parameter improvement. The result is a policy that belongs to the Pareto frontier.
An approximation of the Pareto frontier is obtained by performing repeated searches with
different weights of the reward gradients vi . On the other hand, conditional optimization
consists in maximizing an objective while maintaining a certain level of performance over
the others. The resulting algorithm is a gradient search in a reduced policy space in which
the value of constrained objectives are greater than the desired performance.
Only a few studies followed the work of Shelton (2001) in regard to policy gradient
algorithms applied to MOMDPs. Recently Parisi et al. (2014) proposed two policy gradient
based MORL approaches that, starting from some initial policies, perform gradient ascent
in the policy parameters space in order to determine a set of nondominated policies. In
the first approach (called Radial ), given the number p of Pareto solutions that are required
for approximating the Pareto frontier, p gradient ascent searches are performed, each one
following a different (uniformly spaced) direction within the ascent simplex defined by the
convex combination of singleobjective gradients. The second approach (called Pareto
Following) starts by performing a singleobjective optimization and then it moves along the
Pareto frontier using a two-step iterative process: updating the policy parameters following
some other gradient ascent direction, and then applying a correction procedure to move the
new solution onto the Pareto frontier. Although such methods exploit stochastic policies and
proved to be effective in several scenarios, they still return scattered solutions and are not
guaranteed to uniformly cover the Pareto frontier. To the best of our knowledge, nowadays
191

fiParisi, Pirotta, & Restelli

there is no MORL algorithm returning a continuous approximation of the Pareto frontier2 .
In the following sections we present the first approach able to do that: the ParetoManifold
Gradient Algorithm (PMGA).
2.3 Policy Parametrization in PolicyGradient Approaches
In singleobjective
MDPs,

	 policygradient approaches consider parameterized policies  
 =  :     Rd , where  is a compact notation for (a|s, ) and  is the policy
parameters space. Given a policy parametrization , we assume the policy performance
J :   F  Rq to be at least of class C 2 .3 F is called objectives space and J is defined as
the expected reward over the space of all possible trajectories T
Z
p ( |) r( )d,
J () =
T

where   T is a trajectory drawn from density distribution p( |) with reward vector
r( ) that
the accumulated expected discounted reward over trajectory  , i.e.,
Prepresents
1 t
i Ri (st , at , st+1 ). Examples of parametrized policies used in this context are
ri ( ) = Tt=0
Guassian policies and Gibbs policies. In MOMDPs, q gradient directions are defined for
each policy parameter  (Peters & Schaal, 2008b), i.e.,
Z


 Ji () =
 p ( |) ri ( )d = E T  ln p ( |) ri ( )
T
"
#
T
1
X
b  Ji (),
 E T ri ( )
 ln  (at |st , ) = 
(1)
t=0

where each direction  Ji is associated to a particular discount factorreward function
b  Ji () is its sample-based estimate. As shown by Equation (1), the
pair < i , Ri > and 
differentiability of the expected return is connected to the differentiability of the policy by
 ln p ( |) =

T
1
X

 ln (at |st , ).

t=0

A remark on notation. In the following we will use the symbol DX F to denote the
derivative of a generic function F : Rmn  Rpq w.r.t. matrix X.4 Notice that the
following relationship holds for scalar functions of vector variable: x f = (Dx f )T . Finally,
the symbol Ix will be used to denote an x  x identity matrix.

3. Gradient Ascent on Policy Manifold for Continuous Pareto Frontier
Approximation
In this section we first provide a general definition of the optimization problem that we want
to solve and then we explain how we can solve it in the MOMDP scenario using a gradient
based approach. The novel contributes of this section are summarized in Lemma 3.1 where
2. A notable exception is the MOO approach by Calandra, Peters, and Deisenrothy (2014) where Gaussian
Processes are used to obtain a continuous approximation of the Pareto frontier.
3. A function is of class C 2 when it is continuous, twice differentiable and the derivatives are continuous.
4. The derivative operator is well defined for matrices, vectors and scalar functions. Refer to the work
of Magnus and Neudecker (1999) for details.

192

fiMORL through Continuous Pareto Manifold Approximation

the objective function and its gradient are described. In particular, we provide a solution
to the problem of evaluating the performance of a continuous approximation of the Pareto
frontier w.r.t. to an indicator function. This problem is non trivial in MORL because we
do not have direct access to the Pareto frontier and we can only manipulate the policy
parameters. We provide a step-by-step derivation of these results leveraging on manifold
theory and matrix calculus.
3.1 Continuous Pareto Frontier Approximation in Multiobjective
Optimization
It has been shown that locally Paretooptimal solutions locally forms a (q  1)dimensional
manifold, assuming d > q (Harada, Sakuma, Kobayashi, & Ono, 2007). It follows that in
2objective problems, Paretooptimal solutions can be described by curves both in policy
parameters and objective spaces. The idea behind this work is to parametrize the locally
Paretooptimal solution curve in the objectives space, in order to produce a continuous
representation of the Pareto frontier.
Let the generative space T be an open set in Rb with b  q. The analogous high
dimensional function of a parameterized curve is a smooth map  : T  Rq of class
C l (l  1), where t  T and   P  Rk are the free variables and the parameters,
respectively. The set F =  (T ) together with the map  constitute a parametrized
manifold of dimension b, denoted by F (T ) (Munkres, 1997). This manifold represents our
approximation of the Pareto frontier. The goal is to find the best approximation, i.e., the
parameters  that minimize the distance from the real frontier
 = arg max I  (F (T )) ,

(2)

P

where I  : Rq  R is some indicator function measuring the quality of F (T ) w.r.t. the
true Pareto frontier. Notice that Equation (2) can be interpreted as a special projection
operator (refer to Figure 1a for a graphical representation). However, since I  requires
the knowledge of the true Pareto frontier, a different indicator function is needed. The
definition of such metric is an open problem in literature. Recently, several metrics have
been defined, but each candidate presents some intrinsic flaws that prevent the definition
of a unique superior metric (Vamplew et al., 2011). Furthermore, as we will see in the
remainder of the section, the proposed approach needs a metric that is differentiable w.r.t.
policy parameters. We will investigate this topic in Section 5.
In general, MOO algorithms compute the value of the frontier as the sum of the value
of the points composing the discrete approximation. In our scenario, where a continuous
approximate frontier is available, it maps to an integration on the Pareto manifold
Z
IdV,
(3)
L () =
F (T )

where L () is the manifold value, dV denotes the integral w.r.t. the volume of the manifold
and I : F (T )  R is an indicator function measuring the Pareto optimality of each point
of F (T ). Assuming I to be continuous, the above integral is given by (Munkres, 1997)
Z
Z
L () =
IdV 
(I   ) V ol (Dt  (t)) dt,
F (T )

T

193

fiParisi, Pirotta, & Restelli

(a)

(b)

Figure 1: Transformation maps in a generic MOO setting (Figure (a)) and in MORL (Figure (b)). While in MOO it is also possible to consider parametrized solutions as in Figure (b), in MORL this is necessary, as the mapping between  i and Fi is not known in
closed form but determined by the (discounted) sum of the rewards.
 1
provided this integral exists and V ol (X) = det X T  X 2 . A standard way to maximize
the previous equation is by performing gradient ascent, updating the parameters according
to the gradient of the manifold value w.r.t. the parameters , i.e.,    +   L () .
3.2 Continuous Pareto Frontier Approximation in Multiobjective
Reinforcement Learning
While in standard multiobjective optimization the function  is free to be designed, in
MORL it must satisfy some conditions. The first thing to notice is that the direct map
between the parameters space T and the objective space is unknown, but can be easily
defined through a reparameterization involving the policy space , as shown in Figure 1b. In
the previous section we have mentioned that there is a tight relationship between the (local)
manifold in the objective space and the (local) manifold in the policy parameters space.
This mapping is well known and it is defined by the performance function J() defining the
utility of a policy   . This means that, given a set of policy parameterizations, we can define
the associated points in the objective space. As a consequence, the optimization problem
can be reformulated as the search for the best approximation of the Pareto manifold in the
policy parameter space, i.e., to the search of the manifold in the policy parameter space
that best describes the optimal Pareto frontier.
Formally, let  : T   be a smooth map of class C l (l  1) defined on the same
domain of  . We think of the map  as a parameterization of the subset  (T ) of :
each choice of a point t  T gives rise to a point  (t) in  (T )  . This means that only
a subset  (T ) of the space  can be spanned by map  , i.e.,  (T ) is a bdimensional
parametrized manifold in the policy parameters space, i.e.,
 (T ) = { :  =  (t), t  T } ,
and, as a consequence, the associated parameterized Pareto frontier is the bdimensional
open set defined as
F (T ) = {J () :    (T )} .
194

fiMORL through Continuous Pareto Manifold Approximation

3.3 Gradient Ascent in the Manifold Space
At this point we have introduced all the notation needed to derive the gradient  L ().
Lemma 3.1. (Pirotta et al., 2015) Let T be an open set in Rb , let F (T ) be a manifold
parametrized by a smooth map  expressed as composition of maps J and  , (i.e.,  =
J   : T  Rq ). Given a continuous function I defined at each point of F (T ), the
integral w.r.t. the volume is given by
Z
Z
L () =
IdV =
(I  (J   )) V ol (D J()Dt  (t)) dt,
F (T )

T

provided this integral exists. The associated gradient w.r.t. the parameters i is given by
Z

L ()
=
(I  (J   )) V ol (T) dt
i

T
i

Z

T T 

T
T
(I  (J   )) V ol (T) vec T T
+
Nb Ib  T Di Tdt, (4)
T

where T = D J()Dt  (t),  is the Kronecker product, Nb = 12 (Ib2 + Kbb ) is a symmetric
(b2  b2 ) idempotent matrix with rank 21 b(b + 1) and Kbb is a permutation matrix (Magnus
& Neudecker, 1999). Finally,


Di T = Dt  (t)T  Iq D (D J()) Di  (t) + (Ib  D J()) Di (Dt  (t)) .
Proof. The equation of the manifold value L () follows directly from the definition of
volume integral of a manifold (Munkres, 1997) and the definition of function composition.
In the following, we provide a detailed derivation of the i-th component of the gradient.
Let T = D J( t )Dt  (t), then
Z
L ()

=
(I  (J   )) V ol (T) dt
i
T i

Z
det TT T
1
+
(I  (J   ))
dt.
2V ol (T)
i
T
The indicator derivative and the determinant derivative can be respectively expanded as

(I  (J   )) = DJ I(Jt )  D J( t )  Di  (t),
i


det TT T
det TT T vec TT T T
=
,
T
T 
i
(vec
T)
(vec
T)
i
|
{z
} |
{z
} | {z } |{z}
11

1b2

b2 qb

qb1

where

det TT T
(vec T)T
TT T
(vec T)T



T T
T
,
= det T T
vec T T


T



= 2Nb Ib  TT ,

195

fiParisi, Pirotta, & Restelli

and  is the Kronecker product, Nb = 12 (Ib2 + Kbb ) is a symmetric (b2  b2 ) idempotent
matrix with rank 21 b(b + 1) and Kbb is a permutation matrix (Magnus & Neudecker, 1999).
(T)
The last term to be expanded is Di T  vec
i . We start from a basic property of the
differential, i.e.,
d (D J()Dt  (t)) = d(D J())Dt  (t) + D J() d(Dt  (t))
then, applying the vector operator,
dvec (D J()Dt  (t)) = vec (d(D J())Dt  (t)) + vec (D J() d(Dt  (t)))


= Dt  (t)T  Iq dvec (D J()) + (Ib  D J()) dvec (Dt  (t)) .
{z
} |
{z
}|
{z
}
{z
}|
|
dq1

bqdq

bqbd

bd1

Finally, the derivative is given by

 vec D J()  (t)
vec Dt  (t)


Di T = Dt  (t)T  Iq
+ (Ib  D J())
T

i

|
{z
} | {zi }
|
{z
}
dqd



d1

bd1



T

= Dt  (t)  Iq D (D J()) Di  (t) + (Ib  D J()) Di (Dt  (t)) .

It is interesting to notice that the gradient of the manifold value L () requires to
compute the second derivatives of the policy performance J(). However, D (D J()) =
vec D J()
does not denote the Hessian matrix but a transformation of it
 T
(m,n)
H
Ji

=

2
Dn,m
Ji ()


=
 n



Ji
 m



= Dp,n (D J()) ,

where p = i + q(m  1) and q (number of objectives) is the number of rows of the Jacobian
matrix. Recall that the Hessian
 matrixis defined as the derivative of the transpose of the
Jacobian, i.e., H J() = D D J()T .
Up to now, little research has been done on second-order methods5 and in particular
on Hessian formulations. A first analysis was performed by Kakade (2001), who provided a
formulation based on the policy gradient theorem (Sutton, McAllester, Singh, & Mansour,
2000). Recently, an extended comparison between Newton method, EM algorithm and
natural gradient was presented by Furmston and Barber (2012). For the sake of clarity, we
report the Hessian formulation provided by Furmston and Barber (2012) using our notation
and we introduce the optimal baseline (in terms of variance reduction) for such formulation.
Lemma 3.2. For any MOMDP, the Hessian H J() of the expected discounted reward J
w.r.t. the policy parameters  is a qd  d matrix obtained by stacking the Hessian of each
5. Notable exceptions are the natural gradient approaches that, although they do not explicitly require to
compute second-order derivatives, are usually considered second-order methods.

196

fiMORL through Continuous Pareto Manifold Approximation

component
H J() =


vec
 T



Ji ()
 T

T



H J1 ()


..
=
,
.
H Jq ()

where
Z
H Ji () =

!
p ( |) (ri ( )  bi )  ln p ( |)  ln p ( |)T + H ln p ( |) d,

(5)

T

and
 ln p ( |) =

T
1
X

 ln (at |st , ),

H ln p ( |) =

t=0

T
1
X

H ln (at |st , ).

t=0
(m,n)

The optimal baseline of the Hessian estimate H
Ji provided in Equation (5) can
be computed as done by Greensmith, Bartlett, and Baxter (2004) in order to reduce the
variance of the gradient estimate. It is given component-wise by


2 
(m,n)
E p(|) Ri ( ) G
( )
(m,n)

bi
=
2  ,
(m,n)
E p(|) G
( )
(m,n)

(m,n)

n
where G
( ) = m
 ln p ( |)  ln p ( |)+H
to Appendix A.

ln p ( |). For its derivation, we refer

4. Manifold Gradient Estimation from Sample Trajectories
In MORL, having no prior knowledge about the reward function and the state transition
model, we need to estimate the gradient  L () from trajectory samples. This section
aims to provide a guide to the estimation of the manifold gradient. In particular, we review
results related to the estimation of standard RL components (expected discounted return
and its gradient) and we provide a finite-sample analysis of the Hessian estimate.
The formulation of the gradient  L () provided in Lemma 3.1 is composed by terms
related to the parameterization of the manifold in the policy space and terms related to
the MDP. Since the map  is free to be designed, the associated terms (e.g., Dt  (t)) can
be computed exactly. On the other hand, the terms related to the MDP (J (), D J()
and H J()) need to be estimated. While the estimate of the expected discounted reward
and the associated gradient is an old topic in RL literature and several results have been
proposed (Kakade, 2001; Pirotta, Restelli, & Bascetta, 2013), literature lacks of an explicit
analysis of the Hessian estimate. Recently, the simultaneous perturbation stochastic approximation technique was exploited to estimate the Hessian (Fonteneau & Prashanth, 2014).
However, we rely on the formulation provided by Furmston and Barber (2012) where the
Hessian is estimated from trajectory samples obtained through the current policy, removing
the necessity of generating policy perturbations.
197

fiParisi, Pirotta, & Restelli

Algorithm 1 ParetoManifold Gradient Algorithm
Define policy , parametric function  , indicator I and learning rate 
Initialize parameters 
Repeat until terminal condition is reached
Collect n = 1 . . . N trajectories
Sample free variable t[n] from the generative space

Sample policy parameters  [n] =  t[n]
n
o
[n] [n] [n] T
Execute trajectory and collect data st , at , rt,
t=1

b  Ji () according to Equation (1)
Compute gradients 
b  Ji () according to Equation (6)
Compute Hessians H
Compute manifold value derivative  L () according to Equation (4)
Update parameters    +   L ()
Since p ( |) is unknown, the expectation is approximated by the empirical average.
Assuming to have access to N trajectories, the Hessian estimate is
!
N
T
1
X
X
1
b  Ji () =
H
it rnt,i  b
N
n=1
t=0
!T T 1
!
T
1
T
1
X
X
X

 ln ant ,snt
 ln ant ,snt
+
H ln ant ,snt ,
(6)
t=0

where

o
n
[n] [n] [n] T
st , at , rt,

t=1

t=0

t=0

denotes the n-th trajectory. This formulation resembles the def-

inition of REINFORCE estimate given by Williams (1992) for the gradient  J(). Such
estimates, known as likelihood ratio methods, overcome the problem of determining the perturbation of the parameters occurring in finite-difference methods. Algorithm 1 describes
the complete PMGA procedure.
In order to simplify the theoretical analysis of the Hessian estimate, we make the following assumptions.
Assumption 4.1 (Uniform boundedness). The reward function, the log-Jacobian and the
log-Hessian of the policy are uniformly bounded: i = 1, . . . , q, m = 1, . . . , d, n =
1, . . . , d, (s, a, s0 )  S  A  S ,   
fi
fi
fi
fi
fi
fi
fi
fi (m)
fi
fi (m,n)
fi
0 fi
ln (a|s, )fi  G.
fiRi (s, a, s )fi  Ri ,
fiD ln (a|s, )fi  D,
fiH
Lemma 4.2. Given a parametrized policy (a|s, ), under Assumption 4.1, the i-th component of the log-Hessian of the expected return can be bounded by
kH Ji ()kmax 


Ri T  T 
2
TD + G ,
1

where the max norm of a matrix is defined as kAkmax = maxi,j {aij }.
198

fiMORL through Continuous Pareto Manifold Approximation

Proof. Consider the definition of the Hessian in Equation (5). Under assumption 4.1, the
Hessian components can be bounded by (m, n)
fi
"
T
1
T
1
fi
fi fiZ
X
X


fi (m,n)
fi fi
ln (at |st , )
ln (aj |sj , )
Ji ()fi = fi p ( |) ri ( )
fiH
fi T
 m
 n
t=0
j=0
#fi
fi
2
fi
+
ln (at |st , ) fi
fi
 m  n


T
1
T
1
T
1

X
X
X
Ri T  T 
2
D
 Ri
 l1 
D + G =
TD + G .
1
l=0

t=0

j=0

The previous result can be used to derive a bound on the sample complexity of the
Hessian estimate.
Theorem 4.3. Given a parametrized policy (a|s, ), under Assumption 4.1, using the
following number of T -step trajectories

 2 2
1
Ri T  T 
2
N 2
TD + G
ln

2i (1  )
b  Ji () generated by Equation (6) is such that with probability 1  
the gradient estimate H


b

 i .
H Ji ()  H Ji ()
max

Proof. Hoeffdings inequality implies that m, n
N 2 2
fi
fi
i
 PN
fi b (m,n)
fi
(m,n)
(bi ai )2
i=1
=.
P fiH
Ji ()  H
Ji ()  i fi  2e

Solving the equation for N and noticing that Lemma 4.2 provides a bound on each sample,
we obtain

2 2
1
Ri T  T 
2
N= 2
TD + G
ln .

2i (1  )

The integral estimate can be computed using standard MonteCarlo techniques. Several
statistical bounds have been proposed in literature, we refer to Robert and Casella (2004)
for a survey on MonteCarlo methods.
At this point of the paper, the reader may expect an analysis of the convergence (or
convergence rate) to the optimal parametrization. Although we consider this analysis theoretically challenging and interesting, we will not provide any result related to this topic.
This analysis is hard (or even impossible) to provide in general settings since the objective
function is nonlinear and nonconcave. Moreover, an analysis of a simplified scenario (if
possible) will be almost useless in real applications.
199

fiParisi, Pirotta, & Restelli

5. Metrics for Multiobjective Optimization
In this section, we review some indicator functions proposed in literature, underlining advantages and drawbacks, and propose some alternatives. Recently, MOO has focused on
the use of indicators to turn a multiobjective optimization problem into a singleobjective
one by optimizing the indicator itself. The indicator function is used to assign to every
point of a given frontier a scalar measure that gives a rough idea of the discrepancy between the candidate frontier and the Pareto one. Since instead of optimizing the objective
functions directly indicatorbased algorithms aim at finding a solution set that maximizes
the indicator metric, a natural question arises about the correctness of this change in the
optimization procedure and on the properties the indicator functions enjoy. For instance,
the hypervolume indicator and its weighted version are among the most widespread metrics
in literature. These metrics have gained popularity because they are refinements of the
Pareto dominance relation (Zitzler, Thiele, & Bader, 2010). Recently, several works have
been proposed in order to theoretically investigate the properties of the hypervolume indicator (e.g., Friedrich, Horoba, & Neumann, 2009). Nevertheless, it has been argued that the
hypervolume indicator may introduce a bias in the search. Furthermore another important
issue when dealing with the hypervolume indicator is the choice of the reference point. From
our perspective, the main issues of this metric are the high computational complexity (the
computation of the hypervolume indicator is a #Phard problem, see Friedrich et al., 2009)
and, above all, the non differentiability. Several other metrics have been defined in the field
of MOO, we refer to the work by Okabe, Jin, and Sendhoff (2003) for a survey. However, the
MOO literature has not been able to provide a superior metric and among the candidates
no one is suited for our scenario. Again, the main issues are the non differentiability, the
capability of evaluating only discrete representations of the Pareto frontier and the intrinsic
nature of the metrics. For example, the generational distance, another widespread measure
based on the minimum distance from a reference frontier, is not available in our settings.
To overcome these issues, we mixed different indicator concepts into novel differentiable
metrics. The insights that have guided our metrics definition are related to the MOO
desiderata. Recall that the goal of MOO is to compute an approximation of the frontier
including solutions that are accurate, evenly distributed and covering a range similar to the
actual one (Zitzler et al., 2003). Note that the uniformity of the frontier is intrinsically guaranteed by the continuity of the approximation we have introduced. Having these concepts
in mind, we need to induce accuracy and extension through the indicator function.
We have not stressed but it is clear from the definition that we want the indicator to
be maximized by the real Pareto frontier. We also must ensure that the indicator function
induces a partial ordering over frontiers: manifold F2 solutions are all (weakly) dominated
by manifold F1 ones, then F1 manifold value must be better than F2 one.
Definition 5.1 (Consistent Indicator Function). Let F be the set of all (q 1)dimensional
manifolds associated to a MOMDP with q objectives. Let k   be the manifold in the
policy parameters
space mapping to Fk  F and F  be the true Pareto frontier. Let
R
LI (F) = F IdV be the manifold value. An indicator function I is consistent if
Fk 6= Fh , LI (Fh ) > LI (Fk )  Fh  F  ,

and

h , k ,  i  k ,  j  h , j  i = LI (Fh ) > LI (Fk ).
200

fiMORL through Continuous Pareto Manifold Approximation

5.1 Accuracy Metrics
Given a reference point p, a simple indicator can be obtained by computing the distance
between every point of a frontier F and the reference point, i.e.,
I = kJ  pk22 .
As mentioned for the hypervolume indicator, the choice of the reference point may be
critical. However, a natural choice is the utopia (ideal) point (pU ), i.e., the point that
optimizes all the objectives. In this case the goal is the minimization of such indicator
function, denoted by IU (utopia indicator ). Since any dominated policy is farther from the
utopia than at least one Paretooptimal solution, the accuracy can be easily guaranteed. On
the other hand, since it has to be minimized, this measure forces the solution to collapse
into a single point, thus it is not consistent. Note that this problem can be mitigated
(but not solved) by forcing the transformation  to pass through the singleobjective
optima. Although this trick can be helpful, as we will discuss in Section 6, it requires to
find the singleobjective optimal policies in order to constrain the parameters. However,
this information is also required to properly set the utopia.
Concerning the accuracy of the frontier, from a theoretical perspective, it is possible to
define another metric using the definition of Pareto optimality. A point  is Paretooptimal
when (Brown & Smith, 2005)
l(, ) =

q
X

i  Ji () = 0,

q
X

i=1

i = 1,

i  0,

i=1

that is, it is not possible to identify an ascent direction that simultaneously improves all
the objectives. As a consequence, the Paretoascent direction l of any point on the Pareto
frontier is null. Formally, a metric that respects the Paretooptimality can be defined as
follows:
q
X
I = minq kl(, )k22 ,
i = 1, i  0.
R

i=1

We denote this indicator with IPN (Pareto norm indicator ). As for the utopiabased metric,
the extent of the frontier is not taken into account and without any constraint the optimal
solution collapses into a single point on the frontier.
5.2 Covering Metrics
If the extension of the frontier is the primary concern, maximizing the distance from the
antiutopia (pAU ) results in a metric that grows with the frontier dimension. However,
on the contrary of the utopia point, the antiutopia is located in the half space that can
be reached by the solutions of the MOO problems. This means that by considering the
antiutopiabased metric the maximization problem could become unbounded by moving
solutions arbitrary far from both the Pareto frontier and the antiutopia point. Therefore
this measure, denoted by IAU (antiutopia indicator ), does not provide any guarantee about
accuracy.
201

fiParisi, Pirotta, & Restelli

5.3 Mixed Metrics
All the mentioned indicators provide only one of the desiderata. As a consequence, the
resulting approximate frontier might be arbitrary far from the actual one. In order to
consider both the desiderata we can mix the previous concepts into the following indicator:
I = IAU  w
where w is a penalization function, i.e., it is a monotonic function that decreases as the
accuracy of the input increases, e.g., w = 1  IPN or w = 1  IU . These metrics, denoted
respectively by I,PN and I,U , take advantage of the expansive behavior of the antiutopia
based indicator and the accuracy of some optimalitybased indicator. In this way all the
desiderata can be met by a single scalar measure, that is also C l (l  1) differentiable.
Another solution is to mix utopia and antiutopiabased indicators in a different way.
As we want solutions that are simultaneously far from the antiutopia and close to the utopia,
we consider the following metric I (to be maximized):
I = 1

IAU
 2 ,
IU

where 1 and 2 are free parameters.
In the next section, we will show that the proposed mixed metrics are effective in driving
PMGA close to the Pareto frontier both in exact and approximate scenarios. However, we
want to make clear that their consistency is not guaranteed as it strongly depends on the
free parameters , 1 and 2 . More insights are discussed in Section 7.

6. Experiments
In this section, we evaluate our algorithm on two problems, a Linear-Quadratic Gaussian
regulator and a water reservoir control task. PMGA is compared to state-of-the-art methods
(Peters, Mulling, & Altun, 2010; Castelletti et al., 2013; Parisi et al., 2014; Beume, Naujoks,
& Emmerich, 2007) using the hypervolume (Vamplew et al., 2011) and an extension of a
previously defined performance index (Pianosi, Castelletti, & Restelli, 2013), named loss,
measuring the distance of an approximate Pareto front from a reference one. For 2objective
problems, the hypervolume is exactly computed. For 3objective problems, given its high
computational complexity, the hypervolume is approximated with a MonteCarlo estimate
as the percentage of points dominated by the frontier in the cube defined by the utopia and
antiutopia points. For the estimate one million points were used.
}
The idea of the loss index is to compare the true Pareto frontier FW = {Jw
wW over a
M
space of weights W to the frontier JW = {Jbw }wW returned by an algorithm M over the
same weights (Jw denotes the discounted return of a new singleobjective MDP defined by
the linear combination of the objectives over w). Formally the loss function l is defined as
l(J

M

Jw  maxM Jbw

Z

J

, F, W, p) =
wW

Jw

p(dw),

(7)

where p() is a probability density over the simplex W and Jw = w  J is the normalization factor, where the i-th component of J is the difference between the best and the
202

fiMORL through Continuous Pareto Manifold Approximation

worst value of the i-th objective of the Pareto frontier, i.e., Ji = max(Ji )  min(Ji ). This
M.
means that, for each weight, the policy that minimizes the loss function is chosen in JW
If the true Pareto frontier F is not known, a reference one is used.
Since PMGA returns continuous frontiers and the two scores are designed for discrete
ones, for the evaluation all the frontiers have been discretized. Also, figures presented in
this section show discretized frontiers in order to allow a better representation. Besides the
hypervolume and the loss function, we report also the number of solutions returned by an
algorithm and the number of rollouts (i.e., the total number of episodes simulated during
the learning process). All data have been collected in simulation and results are averaged
over ten trials6 . In all the experiments, PMGA learning rate is
s

,
(8)
=
T
 L () M 1  L ()
where M is a positive definite, symmetric matrix and  is a userdefined parameter. This
stepsize rule comes from the formulation of the gradient ascent as a constrained problem
with a predefined distance metric M (Peters, 2007) and underlies the derivation of natural
gradient approaches. However, since our algorithm exploits the vanilla gradient (i.e., we
consider the Euclidean space) the metric M is the identity matrix I.
The remainder of the section is organized as follows. We start by studying the behavior
of the metrics proposed in Section 5 and the effects of the parametrization  (t) on the LQG.
Subsequently, we focus our attention on sample complexity, meant as the number of rollouts
needed to approximate the Pareto front. Finally, we analyze the quality of our algorithm
on the water reservoir control task, a more complex real world scenario, and compare it
to some state-of-the-art multiobjective techniques. For each case study, domains are first
presented and then results are reported and discussed.
6.1 Linear-Quadratic Gaussian Regulator (LQG)
The first case of study is a discrete-time Linear-Quadratic Gaussian regulator (LQG) with
multi-dimensional and continuous state and action spaces (Peters & Schaal, 2008b). The
LQG problem is defined by the following dynamics
st+1 = Ast + Bat ,

at  N (K  st , )

R(st , at ) = st T Qst  at T Rat
where st and at are n-dimensional column vectors, A, B, Q, R  Rnn , Q is a symmetric
semidefinite matrix, and R is a symmetric positive definite matrix. Dynamics are not
coupled, i.e., A and B are identity matrices. The policy is Gaussian with parameters
 = vec(K), where K  Rnn . Finally, a constant covariance matrix  = I is used.
The LQG can be easily extended to account for multiple conflicting objectives. In
particular, the problem of minimizing the distance from the origin w.r.t. the i-th axis has
been taken into account, considering the cost of the action over the other axes
X
Ri (st , at ) = s2t,i 
a2t,j .
i6=j

6. Source code available at https://github.com/sparisi/mips.

203

fiParisi, Pirotta, & Restelli

Since the maximization of the i-th objective requires to have null action on the other axes,
objectives are conflicting. As this reward formulation violates the positiveness of matrix
Ri , we change it adding a sufficiently small -perturbation




X
X
Ri (st , at ) = (1  ) s2t,i +
a2t,j    
s2t,j + a2t,i  .
i6=j

j6=i

The parameters used for all the experiments are the following:  = 0.9,  = 0.1 and initial
state s0 = [10, 10]T and s0 = [10, 10, 10]T for the 2 and 3objective case, respectively. The
following sections compare the performance of the proposed metrics under several settings.
We will made use of tables to summarize the results at the end of each set of experiments.
6.1.1 2objective Case Results
The LQG scenario is particular instructive since all terms involved in the definition of returns, gradients and Hessians can be computed exactly. We can therefore focus on studying
different policy manifold parametrizations  (t) and metrics I.
Unconstrained Parametrization. The domain is problematic since it is defined only
for control actions in the range [1, 0] and controls outside this range lead to divergence of
the system. Our primary concern was therefore related to the boundedness of the control
actions, leading to the following parametrization of the manifold in the policy space:


(1 + exp(1 + 2 t))1
,
t  [0, 1].
 =  (t) =
(1 + exp(3 + 4 t))1
Utopia and antiutopia points are [150, 150] and [310, 310], respectively, and metrics IAU and
IU are normalized in order to have 1 as reference point.7 The learning step parameter  in
Equation (8) is  = 1.
In this case, exploiting nonmixed metrics, PMGA was not able to learn a good approximation of the Pareto frontier in terms of accuracy and covering. Using utopiabased
indicator, the learned frontier collapses in one point on the knee of the front. The same
behavior occurs using IPN . Using antiutopia point as reference point the solutions are
dominated and the approximate frontier gets wider, diverging from the true frontier and
expanding on the opposite half space. These behaviors are not surprising, considering the
definition of these indicator functions, as explained in Section 5.
On the contrary, as shown in Figure 2, all mixed metrics are able to achieve both
accuracy and covering. The starting 0 was set to [1, 2, 0, 3]T , but the algorithm was also
able to learn even starting from different random parameters. The free metric parameters
were set to  = 1.5 for I,PN ,  = 1 for I,U and to 1 = 3, 2 = 1 for I .8 Although not
shown in the figure, I,U behaved very similarly to I,PN . We can notice that in both cases
first accuracy is obtained by pushing the parametrization onto the Pareto frontier, then the
frontier is expanded toward the extrema in order to attain covering.
7. Recall that we have initially defined I = kJ  pk22 . Here we slightly modify it by normalizing the policy
performance w.r.t. the reference point: I = kJ/p  1k22 , where / is a component-wise operator.
8. In Section 7 we will study the sensitivity of the proposed metrics to their parameters  and .

204

fiMORL through Continuous Pareto Manifold Approximation

Table 1: Summary of 2dimensional LQG (unconstrained)
Metrics
Nonmixed
Issues:

Accuracy
Covering
7
7
IU , IPN : frontier collapses in one point
IAU : diverging behavior and dominated solutions found
3
3

Mixed

Partial solution
Final approximation
True Pareto frontier

300

250

16
L ()

23

J2

100

20

200

50

1

21

end

0

150
150

200

250

300

0

50

J1

100

Iterations

(a) Learning process with mixed metric I,PN .

300

15

250
10

L ()

J2

1,000

1

200

500

0

5
end
150
150

200

250

500

300

0

J1

50

100

Iterations

(b) Learning process with mixed metric I .

Figure 2: Learning processes for the 2objective LQG without any constraint on the
parametrization. Numbers denote the iteration, end denotes the frontier obtained when
the terminal condition is reached. On the left, the approximated Pareto frontiers, on the
right the corresponding L (). Using both I,PN (Figure (a)) and I (Figure (b)) the approximated frontier overlaps with the true one. However, using I , PMGA converges faster.

205

fiParisi, Pirotta, & Restelli

Constrained Parametrization. An alternative approach consists in forcing the policy
manifold to pass through the extreme points of the true front by knowing the parameterizations of the singleobjective optimal policies. In general, this requires additional
optimizations and the collection of additional trajectories that must be accounted for in the
results. However, the extreme points are required to set the utopia and antiutopia. Moreover, in our case the optimal singleobjective policies were available in literature. For these
reasons, we do not count additional samples when we report the total number of rollouts.
Using a constrained parameterization, two improvements can be easily obtained. First,
the number of free parameters decreases and, as a consequence, the learning process is
simplified. Second, the approximate frontier is forced to have a sufficiently large area to
cover all the extrema. Thus, the problem of covering shown by nonmixed indicators can
be alleviated or, in some cases, completely eliminated. For the 2dimensional LQG, a
parametrization forced to pass through the extrema of the frontier is the following:


(1 + exp(2.18708  1 t2 + (3.33837 + 1 )t))1
 =  (t) =
,
t  [0, 1].
(1 + exp(1.15129  2 t2 + (3.33837 + 2 )t))1
The initial parameter vector is 0 = [2, 2]T . The constraint was able to correct the diverging
behavior of IU and IPN , which returned an accurate and wide approximation of the Pareto
frontier, as shown in Figure 2a. We also notice a much faster convergence, since the algorithm is required to learn fewer parameters (two instead of four). However, IAU still shows
the same diverging behavior for some initial parameters 0 (in Figure 2b, 0 = [6, 6]T ). On
the contrary, solutions obtained with the other metrics are independent from the initial 0 ,
as the algorithm converges close to the true frontier even starting from a parametrization
generating an initial frontier far away from the true one.
6.1.2 3objective Case Results
Unconstrained Parametrization.


(1 + exp(1 + 2 t1 + 3 t2 ))1
 =  (t) = (1 + exp(4 + 5 t1 + 6 t2 ))1  ,
(1 + exp(7 + 8 t1 + 9 t2 ))1

t  simplex([0, 1]2 ).

Utopia and antiutopia points are [195, 195, 195] and [360, 360, 360], respectively, and metrics
IAU , IU are normalized. The initial parameters are drawn from a uniform distribution 0 
U nif ((0, 0.001)) (0 = 0 causes numerical issues) and the learning rate parameter is  = 1.
As in the 2objective scenario, frontiers learned with IU and IPN collapse in a single
point, while IAU has a divergent trend (Figure 3a). However, unlike the 2objective LQR,
I,PN also failed in correctly approximate the Pareto frontier. The reason is that the tuning
of  is difficult, given the difference in magnitude between IPN and IAU On the contrary,
I,U with  = 1.5 and I with 1 = 3, 2 = 1 returned a high quality approximate frontier.
The latter is shown in Figure 3b. Although some small areas of the true Pareto frontier
are not covered by the approximate one, we stress the fact that all the policies found were
Paretooptimal. The strength of these metrics is to be found in the normalization of both
utopia and antiutopiabased indicators. This expedient, indeed, allows for an easier tuning
of the free metric parameters, as the magnitude of the single components is very similar.
More insights into the tuning of mixed metrics parameters are discussed in Section 7.
206

fiMORL through Continuous Pareto Manifold Approximation

Table 2: Summary of 2dimensional LQG (constrained)
Metrics
Nonmixed: IU , IPN
Nonmixed: IAU
Issues:
Mixed

Accuracy
Covering
3
3
7
7
IAU : diverging behavior and dominated solutions found
3
3

Partial solution
Final approximation
True Pareto frontier

300

1

120
L ()

J2

250

2

200

3

130

end
150
150

200

250

140

300

0

5

J1

10

15

20

25

100

120

Iterations

(a) Learning process with utopiabased metric IU .

300

23

250

600

J2

7

200

L ()

3
1

400
200

150

0

150

200

250

300

0

J1

20

40

60

80

Iterations

(b) Learning process with antiutopiabased metric IAU .

Figure 3: Learning process for the 2objective LQG with a parametrization forced to pass
through the extreme points of the frontier. The constraints are able to correct the behavior
of IU (Figure (a)) and the convergence is faster than the previous parametrization. However,
IAU still diverges (Figure (b)) and the returned frontier includes dominated solutions, since
the metric considers only the covering of the frontier and not the accuracy.

207

fiParisi, Pirotta, & Restelli

Table 3: Summary of 3dimensional LQG (unconstrained)
Metrics
Nonmixed
Issues:

Accuracy
Covering
7
7
IU , IPN : frontier collapses in one point
IAU : diverging behavior and dominated solutions found
7
7
I,PN : difficult tuning of 
3
3

Mixed: I,PN
Issues:
Mixed: I,U , I

True Pareto frontier
Approximate frontier

J3

1,000

500

500

500

1,000

J1

1,000

500

1,000

500

J1

J2

1,000

J2

(a) Frontier approximated with antiutopiabased metric IAU .

J3

True Pareto frontier
Approximate frontier

300
350

200
300

200
200

200
J2

300

300

J1

250
250

300
200

J1

J2

350

(b) Frontier approximated with mixed metric I .

Figure 4: Resulting frontiers for the 3objective LQG using an unconstrained parametrization. Frontiers have been discretized for better representation. With IAU the learning
diverges (Figure (a)) while I correctly approximates the Pareto frontier (Figure (b)).

208

fiMORL through Continuous Pareto Manifold Approximation

Constrained Parametrization.


(1 + exp(a + 1 t1  (b  2 )t2  1 t21  2 t22  3 t2 t1 ))1
,
(1 + exp(a  (b  4 )t1 + 5 t2  4 t21  5 t22  6 t1 t2 ))1
 =  (t) = 
2
2
1
(1 + exp(c + (7 + b)t1 + (8 + b)t2  7 t1  8 t2  9 t1 t2 ))
a = 1.151035476,

b = 3.338299811,

t  simplex([0, 1]2 ).

c = 2.187264336,

The initial parameters are 0 = 0. Numerical results are reported in Table 4, where the
hypervolume has been computed normalizing the objective w.r.t. the antiutopia. Figure 5
shows the frontiers obtained using utopia and antiutopiabased indicators. We can clearly
see that, unlike the 2objective case, even with a constrained parametrization these metrics
lead to poor solutions, failing in providing all MO desiderata. In Figure 5a, using IU the
frontier still tends to collapse towards the center of the true one, in order to minimize the
distance from the utopia point (only the constraint on  prevents that). Although not shown
in the figures, a similar but slightly broader frontier is returned using IPN . However, we
stress that all solutions belong to the Pareto frontier, i.e., only nondominated solutions are
found. Figure 5b shows the frontier obtained with IAU . As expected, the algorithm tries to
produce a frontier as wide as possible, in order to increase the distance from the antiutopia
point. This behavior leads to dominated solutions and the learning process diverges.
On the contrary, using mixed metrics I,PN ( = 30), I,U ( = 1.4) and I (1 =
2.5, 2 = 1) PMGA is able to completely and accurately cover the Pareto frontier, as shown
in Figures 6a and 6b. It is worth to notice the different magnitude of the free parameter  in
I,PN compared to the 2objective case, for which  was 1.5. As already discussed, this is due
to the substantial difference in magnitude between IAU and IPN . On the contrary, the tuning
for the other mixed metrics was easier, as similar parameters used for the unconstrained
parametrization proved to be effective. We will come back to this topic in Section 7.
Finally, as shown in Table 4, I,U and I achieve the best numerical results, as the first
attains the highest hypervolume and the lowest loss, while the latter attains the fastest
convergence. Their superiority also resides in their easy differentiability and tuning, especially compared to I,PN . For these reasons, we have chosen them for an empirical analysis
on sample complexity and for a comparison against some state-of-the-art algorithms on a
real-world MO problem, which will be discussed in the next sections.
Table 4: Performance comparison between different metrics on the 3objective LQG with
constrained parametrization. The reference frontier has a hypervolume of 0.7297.
Metric

Hypervolume

Loss

#Iterations

IU

0.6252

2.9012e-02

59

IAU

0





IPN

0.7167

1.9012e-02

133

I,PN

0.7187

5.2720e-04

47

I,U

0.7212

4.9656e-04

33

I

0.7204

5.0679e-04

15

209

fiParisi, Pirotta, & Restelli

Table 5: Summary of 3dimensional LQG (constrained)
Metrics
Nonmixed
Issues:
Mixed

Accuracy
Covering
7
7
IU , IPN : frontier collapses in one point
IAU : diverging behavior and dominated solutions found
3
3

J3

True Pareto frontier
Approximate frontier

300
350

200
300

200
200

200
J2

300

300

J1

250
250

300
200

J1

J2

350

(a) Frontier approximated with utopiabased metric IU .

J3

True Pareto frontier
Approximate frontier

500
200

350
250

300
200

200
300
J2

J1

300
250

300

J2

350
J1

200

(b) Frontier approximated with antiutopiabased metric IAU .

Figure 5: Results with a parametrization forced to pass through the extreme points of the
frontier. Using IU (Figure (a)) the frontier shrinks as much as allowed by the parametrization. The constraint is therefore not able to solve the issues of the metric as in the 2
objective scenario. On the contrary, using IAU the frontier gets wider and diverges from the
true one (in Figure (b) an intermediate frontier is shown).

210

fiMORL through Continuous Pareto Manifold Approximation

J3

True Pareto frontier
Approximate frontier

300

3

200
200

200
J2

300

300

0.4
0.6
0.8

J1

0.4

0.8

0.6
1

(a) Frontier in objectives space.

0.4

0.6
2
0.8

(b) Frontier in policy parameters space.

Figure 6: Results using I and a constrained parametrization. As shown in Figure (a),
the approximate frontier perfectly overlaps the true one, despite small discrepancies in the
policy parameters space between the learned parameters and the optimal ones (Figure (b)).
Similar frontiers are obtainable with I,PN and I,U .

6.1.3 Empirical Sample Complexity Analysis
In this section, we provide an empirical analysis of the sample complexity of PMGA, meant
as the number of rollouts needed to approximate the Pareto frontier. The goal is to identify
the most relevant parameter in the estimate of MDP terms J(), D J() and HJ().
The analysis is performed on the 2dimensional LQG domain by varying the number of
policies used to estimate the integral per iteration of PMGA and the number of episodes
for each policy evaluation. The steps of each episode are fixed to 50. We first used the
parametrization forced to pass through the extreme points of the frontier with 0 = [3, 7]T ,
that produces an initial approximate frontier far from the true one. The parameter of the
learning rate in Equation (8) was set to  = 0.5 and the parameter of I,U was set to
 = 1. As performance criterion, we choose the total number of rollouts required to reach
a loss smaller than 5  104 and a hypervolume larger than 99.5% of the reference one.
These criteria are also used as conditions for convergence (both have to be satisfied). For
the evaluation, MDP terms are computed in closed form. The terminal condition must be
reached in 100, 000 episodes otherwise the algorithm is forced to end. The symbol  is used
to represent the latter case.
From Table 6a it results that the most relevant parameter is the number of episodes
used to estimate the MDP terms. This parameter controls the variance in the estimate,
i.e., the accuracy of the estimate of  L (). By increasing the number of episodes, the
estimation process is less prone to generate misleading directions, as happens, for instance,
in the oneepisode case where parameters move towards a wrong direction. On the contrary,
the number of points used to estimate the integral (denoted in the table by #t) seems to
have no significant impact on the final performance of the algorithm, but it influences the
number of model evaluations needed to reach the prescribed accuracy. The best behavior,
211

fiParisi, Pirotta, & Restelli

Table 6: Total number of episodes needed to converge on varying the number of points #t
to approximate the integral and the number of episodes #ep per point. The symbol  is
used when the terminal condition is not reached.
(a) If the parametrization is constrained to pass through the extreme points of the frontier, only one
point t is sufficient to move the whole frontier towards the right direction.

#ep

1

5

10

25

50

1



695  578

560  172

1, 850  757

1, 790  673

5



2, 550  1, 509

3, 440  2, 060

5, 175  3, 432

8, 250  2, 479

10



4, 780  4, 623

6, 820  3, 083

10, 500  3, 365

11, 800  1, 503

25



7, 525  2, 980

15, 100  9, 500

18, 375  6, 028

24, 250  7, 097

50



8, 700  5, 719

18, 000  6, 978

26, 750  7, 483

50, 000  1, 474

#t

(b) On the contrary, using an unconstrained parametrization, PMGA needs both a sufficient number
of episodes and enough points t for a correct update step.

#ep

1

5

10

25

50

1











5









29, 350  7, 310

10







44, 100  9, 466

64, 500  1, 359

25







60, 500  1, 000

83, 500  8, 923

50





47, 875  18, 558

84, 250  1, 457



#t

from a samplebased perspective, has been obtained by exploiting only one point for the
integral estimate. Although it can be surprising, a simple explanation exists. By forcing
the parameterization to pass through the singleobjective optima, a correct estimation of
the gradient direction of a single point t is enough to move the entire frontier toward the
true one, i.e., to move the parameters towards the optimal ones.
On the contrary, if the unconstrained parametrization is used, one point is not sufficient
anymore, as shown in Table 6b. In this case, the initial parameter vector was set to 0 =
[1, 1, 0, 0]T , the learning rate parameter to  = 0.1 and the terminal condition requires a
frontier with loss smaller than 103 and hypervolume larger than 99% of the reference
frontier. Without any constraint, the algorithm needs both accuracy in the evaluation of
single points i.e., a sufficient number of episodes and enough points t to move the whole
frontier towards the right direction. The accuracy of the gradient estimate  L () therefore
depends on both the number of points t and the number of episodes, and PMGA requires
much more rollouts to converge. The best behavior, from a samplebased perspective, has
been obtained by exploiting five points for the integral estimate and 50 episodes for the
policy evaluation.
212

fiMORL through Continuous Pareto Manifold Approximation

6.2 Water Reservoir
A water reservoir can be modeled as a MOMDP with a continuous state variable s representing the water volume stored in the reservoir, a continuous action a controlling the
water release, a state-transition model depending also on the stochastic reservoir inflow ,
and a set of conflicting objectives. This domain was proposed by Pianosi et al. (2013).
Formally, the state-transition function can be described by the mass balance equation
st+1 = st + t+1  max(at , min(at , at )) where st is the reservoir storage at time t; t+1 is the
reservoir inflow from time t to t + 1, generated by a white noise with normal distribution
t+1  N (40, 100); at is the release decision; at and at are the minimum and the maximum
releases associated to storage st according to the relations at = st and at = max(st 100, 0).
In this work we consider three objectives: flooding along the lake shores, irrigation
supply and hydro-power supply. The immediate rewards are defined by
R1 (st , at , st+1 ) =  max(ht+1  h, 0),
R2 (st , at , st+1 ) =  max(  t , 0),
R3 (st , at , st+1 ) =  max(e  et+1 , 0),
where ht+1 = st+1 /S is the reservoir level (in the following experiments S = 1), h is the
flooding threshold (h = 50), t = max(at , min(at , at )) is the release from the reservoir,  is
the water demand ( = 50), e is the electricity demand (e = 4.36) and et+1 is the electricity
production
et+1 =  g  H2 0 t ht+1 ,
where  = 106 /3.6 is a dimensional conversion coefficient, g = 9.81 the gravitational
acceleration,  = 1 the turbine efficiency and H2 0 = 1, 000 the water density. R1 denotes
the negative of the cost due to the flooding excess level, R2 is the negative of the deficit in
water supply and R3 is the negative of the deficit in hydro-power production.
Like in the original work, the discount factor is set to 1 for all the objectives and the
initial state is drawn from a finite set. However, different settings are used for the learning
and evaluation phases. Given the intrinsic stochasticity of the problem, all policies are
evaluated over 1,000 episodes of 100 steps, while the learning phase requires a different
number of episodes over 30 steps, depending on the algorithm. We will discuss the details
in the results section.
Since the problem is continuous we exploit a Gaussian policy model


(a|s, ) = N  + (s)T ,  2 ,
where  : S  Rd are the basis functions, d = || and  = {, , }. As the optimal policies
for the objectives are not linear in the state variable, we use a radial basis approximation


i (s) = e

ksci k2
wi

.

We used four centers ci uniformly placed in the interval [20, 190] and widths wi of 60, for
a total of six policy parameters.
213

fiParisi, Pirotta, & Restelli

6.2.1 Results
To evaluate the effectiveness of our algorithm we have analyzed its performance against the
frontiers found by a weighted sum Stochastic Dynamic Programming (Pianosi et al., 2013),
Multi-objective FQI (Pianosi et al., 2013), the episodic version of Relative Entropy Policy
Search (Peters et al., 2010; Deisenroth et al., 2013), SMS-EMOA (Beume et al., 2007),
and two recent policy gradient approaches, i.e., Radial Algorithm and ParetoFollowing
Algorithm (Parisi et al., 2014). Since the optimal Pareto front is not available, the one
found by SDP is chosen as reference one for the loss computation. MOFQI learns only
deterministic policies (i.e., the standard deviation  of the Gaussian is set to zero) and
has been trained using 10, 000 samples with a dataset of 50, 000 tuples for the 2objective
problem and 20, 000 samples with a dataset of 500, 000 tuples for the 3objective problem.
The remaining competing algorithms all learn stochastic policies. The number of episodes
required for a policy update step is 25 for REPS, 100 for PFA and RA, 50 for SMS-EMOA.
Given its episodic formulation, REPS draws the parameters  from an upper distribution
(|) = N (, ) ,
where  is a diagonal covariance matrix, while  is set to zero. However, since the algorithm
learns the parameters  = {, }, the overall learned policy is still stochastic. SMS-EMOA
has a maximum population size of 100 and 500 for the 2 and 3objective case, respectively.
The crossover is uniform and the mutation, which has a chance of 80% to occur, adds a white
noise to random chromosomes. At each iteration, the top 10% individuals are kept in the
next generation to guarantee that the solution quality will not decrease. Finally, MOFQI
scalarizes the objectives using the same weights as SDP, i.e., 11 and 25 weights for the 2 and
3objective case, respectively. REPS uses instead 50 and 500 linearly spaced weights. RA
also follows 50 and 500 linearly spaced directions and, along with PFA, exploits the natural
gradient (Peters & Schaal, 2008a) and the adaptive learning step in Equation (8), with  = 4
and M = F , where F is the Fisher information matrix. Concerning the parametrization of
PMGA, we used a complete first degree polynomial for the 2objective case


66  1 t2 + (1  16)t
105  2 t2 + (2 + 20)t


 18  3 t2 + (3  16)t 

,
t  [0, 1].
 =  (t) = 
2

 23  4 t + (4 + 53)t 
 39  5 t2 + (5 + 121)t 
0.01  6 t2 + (6 + 0.1)t
Similarly, for the 3objective case a complete second degree polynomial is used


36 + (15  1 )t2 + (1 + 1)t1 t2 + 30t21 + (1  1)t22
 57  (27 + 2 )t2 + (2 + 1)t1 t2  48t21 + (2  1)t22 


 13 + (7  23 )t1 + (3 + 1)t1 t2 + (23  2)t21  11t22 
2

 =  (t) = 
30 + (9  24 )t1 + (4 + 1)t1 t2 + (24  2)t2 + 60t2  , t  simplex([0, 1] ).
1
2

 104 + (57  5 )t2 + (5 + 1)t1 t2  65t2 + (5  1)t2 
1

0.05 + (1  6 )t2 + (6 + 1)t1 t2 + (6  1)t22

2

Both parameterizations are forced to pass near the extreme points of the Pareto frontier,
computed through singleobjective policy search. In both cases the starting parameter
214

fiMORL through Continuous Pareto Manifold Approximation

103
9.5

L ()

J2 (Water Demand)

0

1

2

10

SDP
PMGA(0 )
PMGA(end )

10.5
11
11.5

50

100 150 200
Iterations

250

4

(a)

3.5

3

2.5 2 1.5
J1 (Flooding)

1

(b)

Figure 7: Results for the 2objective water reservoir. Even starting from an arbitrary poor
initial parametrization, PMGA is able to approach the true Pareto frontier (Figure (b)). In
Figure (a), the trend of the manifold metric L () averaged over ten trials.

vector is 0 = [0, 0, 0, 0, 0, 50]T . The last parameter is set to 50 in order to guarantee
the generation of sufficiently explorative policies, as  6 is responsible for the variance of
the Gaussian distribution. However, for a fair comparison, also all competing algorithms
take advantage of such information, as the mean of their initial policies is calculated accordingly to the behavior of the optimal ones described by Castelletti et al. (2012), i.e.,
 = [50, 50, 0, 0, 50]T . The initial standard deviation is set to  = 20 to guarantee sufficient exploration. This parametrization avoids completely random and poor quality initial
policies. Utopia and antiutopia points were set to [0.5, 9] and [2.5, 11] for the 2
objective case, [0.5, 9, 0.001] and [65, 12, 0.7] for the 3objective one.
According to the results presented in Section 6.1.3, the integral estimate in PMGA is
performed using a MonteCarlo algorithm fed with only one random point. For each instance of variable t, 50 trajectories by 30 steps are used to estimate the gradient and the
Hessian of the policy. Regarding the learning rate, the adaptive one described in Equation (8) was used with  = 2. For the evaluation, 1,000 and 2,000 points are used for the
integral estimate in the 2 and 3objective case, respectively. As already discussed, given
the results obtained for the LQG problem and in order to show the capability of the approximate algorithm, we have decided to consider only the indicator I (1 = 1 and 2 = 1).
The main reasons are its efficiency (in Table 4 it attained the fastest convergence) and its
easy differentiability. Finally, we recall that all the results are averaged over ten trials.
Figure 7b reports the initial and final frontiers when only the first two objectives are
considered. Even starting very far from the true Pareto frontier, PMGA is able to approach
it, increasing covering and accuracy of the approximate frontier. Also, as shown in Figure 7a, despite the very low number of exploited samples, the algorithm presents an almost
monotonic trend during the learning process, which converges in a few iterations.
215

fiParisi, Pirotta, & Restelli

J2 (Water Demand)

9.5

10

10.5

SDP
PFA
RA
MOFQI
REPS
SMS-EMOA
PMGA
2.6

2.4

2.2

2

1.8

1.6

J1 (Flooding)

1.4

1.2

1

0.8

Figure 8: Visual comparison for the 2objective water reservoir. PMGA frontier is comparable to the ones obtained by state-of-the-art algorithms in terms of accuracy and covering.
However, it is the only continuous one, as the others are scattered.
Table 7: Numerical algorithm comparison for the 2objective water reservoir. The SDP
reference frontier has a hypervolume of 0.0721 and nine solutions.
Algorithm

Hypervolume

Loss

#Rollouts

#Solutions

0.0620  0.0010

0.0772  0.0045

16, 250  1, 072



PFA

0.0601  0.0012

0.0861  0.0083

27, 761  4, 849

51.1  10.9

RA

0.0480  0.0005

0.1214  0.0043

59, 253  3, 542

16.1  2.9

-

0.1870  0.0090

10, 000

-

REPS

0.0540  0.0009

0.1181  0.0030

37, 525  2, 235

17.0  4.1

SMS-EMOA

0.0581  0.0022

0.0884  0.0019

149, 825  35, 460

14.2  2.4

PMGA

MOFQI

Figure 8 offers a visual comparison of the Pareto points and Tables 7 and 8 report a
numerical evaluation, including the hypervolume and the loss achieved by the algorithms
w.r.t. the SDP approximation9 . PMGA attains the best performance both in the 2 and 3
objective cases, followed by PFA. SMS-EMOA also returns a good approximation, but is the
slowest, requiring more than ten times the amount of samples used by PMGA. Only MOFQI
outperforms PMGA on sample complexity, but its loss is the highest. Finally, Figure 9
shows the hypervolume trend for PMGA and a comparison on sample complexity for the
2objective case. PMGA is substantially more sample efficient than the other algorithms,
attaining a larger hypervolume with much fewer rollouts. For example, it is capable of
generating a frontier with the same hypervolume of RA with only one tenth of the rollouts,
or it outperforms PFA with only half of the samples needed by the latter.
9. Results regarding MOFQI include only the loss and the number of rollouts as the hypervolume and the
number of solutions are not available from the original paper.

216

fiMORL through Continuous Pareto Manifold Approximation

0.065

PMGA

PFA (27,761)
SMS-EMOA (149,825)

Hypervolume

0.06

REPS (37,525)

0.055

RA (59,253)

0.05
0.045
0.04
2,000

4,000

6,000

8,000

10,000

12,000

14,000

16,000

#Rollouts
Figure 9: Comparison of sample complexity on the 2objective case using the hypervolume
as evaluation score. In brackets the number of rollouts needed by an algorithm to produce
its best frontier. PMGA clearly outperforms all the competing algorithms, as it requires
much fewer samples to generate frontiers with better hypervolume.
Table 8: Numerical algorithm comparison for the 3objective water reservoir. The SDP
reference frontier has a hypervolume of 0.7192 and 25 solutions.
Algorithm

Hypervolume

Loss

#Rollouts

#Solutions

0.6701  0.0036

0.0116  0.0022

62, 640  7, 963



PFA

0.6521  0.0029

0.0210  0.0012

343, 742  12, 749

595  32.3

RA

0.6510  0.0047

0.0207  0.0016

626, 441  35, 852

137.3  25.4

-

0.0540  0.0061

20, 000

-

REPS

0.6139  0.0003

0.0235  0.0014

187, 565  8, 642

86  9.7

SMS-EMOA

0.6534  0.0007

0.0235  0.0020

507, 211  56, 823

355.6  13.9

PMGA

MOFQI

7. Metrics Tuning
In this section we want to examine more deeply the tuning of mixed metric parameters, in
order to provide the reader with better insights for a correct use of such metrics. The performance of PMGA strongly depends on the indicator used and, thereby, their configuration
is critical. To be more precise, mixed metrics, which obtained the best approximate Pareto
frontiers in the experiments conducted in Section 6, include a trade-off between accuracy
and covering, expressed by some parameters. In the following, we analyze the fundamental
concepts behind these metrics and study how their performance is influenced by changes in
the parameters.
217

fiParisi, Pirotta, & Restelli

Approximate frontier
1,000

True Pareto frontier

300

300

250

250

200

200

500

150
500

(a)  = 1

1,000

150
150

200

250

300

(b)  = 1.5

150

200

250

300

(c)  = 2

Figure 10: Approximate frontiers for the 2objective LQG learned by PMGA using I,PN
on varying . In Figure (a) the indicator does not penalize enough for dominated solutions,
while in Figure (c) the frontier is not wide enough. On the contrary, in Figure (b) the
algorithm achieves both accuracy and covering.

7.1 I Tuning
The first indicator (to be maximized) that we analyze is
I = IAU  w,
where w is a penalization term. In the previous sections we proposed w = 1  IPN and
w = 1  IU , in order to take advantage of the expansive behavior of the antiutopiabased
indicator and the accuracy of an optimalitybased indicator. In this section we study the
performance of this mixed metric by changing , proposing a simple tuning process. The
idea is to set  to an initial value and then increase (or decrease) it if the approximate
frontier contains dominated solutions (or is not wide enough). Figure 10 shows different
approximate frontiers obtained with different values of  in the exact 2objective LQG after
50 iterations and using w = 1  IPN . Starting with  = 1 the indicator behaves mostly
like IAU , meaning that  was too small (Figure 10a). Increasing  to 2 (Figure 10c) the
algorithm converges, but the approximate frontier does not completely cover the true one,
i.e., IPN mostly condition the behavior of the metric. Finally, with  = 1.5 (Figure 10b) the
approximate frontier perfectly matches the true one and the metric correctly mixes the two
single indicators.
However, as already discussed in Section 6, the use of w = 1  IPN can be problematic
as the difference in magnitude between IAU and IPN can make the tuning of  hard up to
the point the metric becomes ineffective. Such a drawback can be solved using w = 1  IU
and normalizing the reference point indicators (i.e., IU and IAU ) by I(J, p) = kJ/p  1k22 ,
as the normalization bounds the utopia and antiutopiabased metrics in similar intervals,
i.e., (0, ) and [0, ), respectively.10
10. The ratio between two vectors a/b is a component-wise operation.

218

fiMORL through Continuous Pareto Manifold Approximation

J2

J2

U

1

10

J1

AU

1
(a)

J2

U

J1

AU

1
(b)

U

1

J1

AU

1
(c)

Figure 11: Examples of Pareto frontiers. In Figures (a) and (b) the frontiers are convex,
but in the latter objectives are not normalized. In Figure (c) the frontier is concave.

7.2 I Tuning
The second mixed indicator (to be maximized) also takes advantage of the expansive behavior of the antiutopiabased indicator and the accuracy of the utopiabased one. It is
defined as
IAU
I = 1
 2 ,
IU
where 1 and 2 are free parameters.
To better understand the insights that have guided our metric definition, we can consider
different scenarios according to the shape of the Pareto frontier. In Figure 11a the frontier
is convex and we normalized the objectives. In this case any point that is closer to the
antiutopia than the utopia is, for sure, a dominated solution. The ratio IAU /IU of any
point on the frontier will always be greater than 1 and hence it is reasonable to set 1
and 2 both to 1. Therefore, we do not need to know exactly the antiutopia point and the
drawback of the antiutopiabased metric IAU disappears, since we also take into account the
distance from the utopia point. Nevertheless, the setting of these points is critical, as their
magnitude can strongly affect PMGA performance. An example is shown in Figure 11b,
where the frontier is not normalized and the objectives have different magnitude. In this
case, setting both 1 and 2 to 1, the indicator I evaluated at the extrema of the frontier
(J1 = [1, 0]T and J2 = [0, 10]T ) is equal to 0.99 and 99, respectively. As the first value
is negative, an approximate frontier that includes all the points of the true Pareto frontier,
but J1 would perform better than the true Pareto frontier.
On the contrary, if the frontier is concave (Figure 11c) it is not true that any point that
is closer to the antiutopia than the utopia is a dominated solution, and the ratio IAU /IU
of any point on the frontier (with the exception, eventually, of its ends) will always be
smaller than one. Keeping 1 = 1 and 2 = 1, PMGA would try to collapse the frontier
into a single point, in order to maximize the indicator. Therefore, the parameters need to
be changed accordingly by trial-and-error. For instance, if the returned frontier does not
achieve accuracy, a possible solution is to decrease 1 or to increase 2 .
219

fiParisi, Pirotta, & Restelli

8. Conclusion
In this paper we have proposed a novel gradientbased approach, namely ParetoManifold
Gradient Algorithm (PMGA), to learn a continuous approximation of the Pareto frontier in
MOMDPs. The idea is to define a parametric function  that describes a manifold in the
policy parameters space, that maps to a manifold in the objectives space. Given a metric
measuring the quality of the manifold in the objectives space (i.e., the candidate frontier),
we have shown how to compute (and estimate from trajectory samples) its gradient w.r.t.
the parameters of  . Updating the parameters along the gradient direction generates a new
policy manifold associated to an improved (w.r.t. the chosen metric) continuous frontier
in the objectives space. Although we have provided a derivation independent from the
parametric function and the metric used to measure the quality of the candidate solutions,
both these terms strongly influence the final result. Regarding the former, we achieved
high quality results by forcing the parameterization to pass through the singleobjective
optima. However, this trick might require domain expertise and additional samples and
therefore could not always be applicable. Regarding the latter, we have presented different
alternative metrics, examined pros and cons of each one, shown their properties through
an empirical analysis and discussed a general tuning process for the most promising ones.
The evaluation also included a sample complexity analysis to investigate the performance
of PMGA, and a comparison to state-of-the-art algorithms in MORL. From the results, our
approach outperforms the competing algorithms both in quality of the frontier and sample
complexity. It would be interesting to study these properties from a theoretical perspective
in order to provide support to the empirical evidence. We leave as open problems the
investigation of the convergence rate and of the approximation error of the true Pareto
frontier. However, we think it will be hard to provide this analysis in the general setting.
Future research will further address the study of metrics and parametric functions that
can produce good results in the general case. In particular, we will investigate problems
with many objectives (i.e., more than three) and highdimensional policies. Since the complexity of the manifold parameterization grows with the number of objectives and policy
parameters, a polynomial parameterization could not be effective in more complex problems and alternative parameterizations have to be found. Another interesting direction of
research concerns importance sampling techniques for reducing the sample complexity in
the gradient estimate. Since the frontier is composed of a continuum of policies, it is likely
that a trajectory generated by a specific policy can be partially used also for the estimation
of quantities related to similar policies, thus decreasing the number of samples needed for
the MonteCarlo estimate of the integral. Moreover, it would be interesting to investigate automatic techniques for the tuning of the metric parameters and the applicability of
PMGA to the multi-agent scenario (e.g., Roijers, Whiteson, & Oliehoek, 2015).

220

fiMORL through Continuous Pareto Manifold Approximation

Appendix A. Optimal Baseline
Theorem A.1 (Componentdependent baseline). The optimal baseline for the (i, j)-component
(i,j)
of the Hessian estimate HRF, JD () given in Equation (6) is

(i,j)
bH,



2
(i,j)
G ( )



E T R( )

=
2 
(i,j)
E G ( )

,

where
(i,j)

G

(i,j)

( ) = i ln p ( |) j ln p ( |) + H

ln p ( |) .

Given a baseline b, the variance reduction obtained through the optimal baseline bH, is
Var (HRF, JD (, b))  Var (HRF, J (, bH, )) =


(i,j) 2

(i,j)
2 
b
 bH,
(i,j)
E
G ( )
.
 T
N
(i,j)

Proof. Let G

( ) be the (i, j)-th component of G ( )
(i,j)

G

(i,j)

( ) = i ln p ( |) j ln p ( |) + H

ln p ( |) .

(i,j)

The variance of HRF, JD () is given by11
Var



(i,j)
HRF, JD



i2
2 
2   h

(i,j)
(i,j)
(i,j)
G ( )
 E R( )  b(i,j) G ( )
() = E R( )  b





2 

2 
(i,j)
(i,j)
2
(i,j) 2
+E b
G ( )
= E R( ) G ( )





2
 h
i2
(i,j)
(i,j)
 2b(i,j) E R( ) G ( )
 E R( )G ( )
.




Minimizing the previous equation w.r.t. b(i,j) we get

(i,j)

bH,



2 
(i,j)
E R( ) G ( )

=
2  .
(i,j)
E G ( )

11. We use the compact notation E [] to denote E T [].

221

fiParisi, Pirotta, & Restelli

The excess of variance is given by




(i,j)
(i,j)
(i,j)
Var G ( )(R( )  b(i,j) )  Var G ( )(R( )  bH, )




2 
2 
2 

2 
(i,j)
(i,j)
(i,j)
(i,j)
(i,j)
2
 2b
E R( ) G ( )
+E b
G ( )
= E R( ) G ( )





 h

2 
 
2 
i2
(i,j)
(i,j) 2
(i,j)
(i,j)
2
 E bH,
 E R( )G ( )
 E R( ) G ( )
G ( )





2   h
i2
(i,j)
(i,j)
(i,j)
+ E R( )G ( )
+ 2bH, E R( ) G ( )




2  
2 

2 
(i,j)
(i,j)
(i,j)
(i,j)
= b
E G ( )
 2b
E R( ) G ( )




2 

2 
2 
(i,j)
(i,j)
(i,j)
(i,j)
 bH, E G ( )
+ 2bH, E R( ) G ( )






2 
2

2

(i,j)
(i,j)
(i,j)
2
(i,j)
E G ( )
 2b
E R( ) G ( )
= b








2  2



(i,j)

2 
 E R( ) G ( )

(i,j)




G ( )
2   E

(i,j)
E G ( )
 

2  
(i,j)


2 

 E R( ) G ( )
(i,j)
2




R( ) G ( )
+ 2

2
E

(i,j)
E G ( )

2 

2 
2 

(i,j)
(i,j)
(i,j)
(i,j)
 2b
E R( ) G ( )
E G ( )
= b




 

2 2
(i,j)
E R( ) G ( )

+
2 
(i,j)
E G ( )




2
(i,j)
G ( )



E R( )


 (i,j) 2
(i,j)

= b
 2b
2 

(i,j)
E G ( )
E







(i,j)

= b

(i,j)

G


( )

2 


(i,j) 2
bH, E




2
(i,j)
G ( )


.

222



2  2 
(i,j)
 
 E R( ) G ( )
 


+



 
2

(i,j)
E G ( )


fiMORL through Continuous Pareto Manifold Approximation

References
Ahmadzadeh, S., Kormushev, P., & Caldwell, D. (2014). Multi-objective reinforcement
learning for auv thruster failure recovery. In Adaptive Dynamic Programming and
Reinforcement Learning (ADPRL), 2014 IEEE Symposium on, pp. 18.
Athan, T. W., & Papalambros, P. Y. (1996). A note on weighted criteria methods for compromise solutions in multi-objective optimization. Engineering Optimization, 27 (2),
155176.
Barrett, L., & Narayanan, S. (2008). Learning all optimal policies with multiple criteria.
In Proceedings of the 25th International Conference on Machine Learning, ICML 08,
pp. 4147, New York, NY, USA. ACM.
Bertsekas, D. P. (2005). Dynamic programming and suboptimal control: A survey from
ADP to MPC*. European Journal of Control, 11 (4-5), 310  334.
Beume, N., Naujoks, B., & Emmerich, M. (2007). Sms-emoa: Multiobjective selection based
on dominated hypervolume. European Journal of Operational Research, 181 (3), 1653
 1669.
Brown, M., & Smith, R. E. (2005). Directed multi-objective optimization. International
Journal of Computers, Systems, and Signals, 6 (1), 317.
Calandra, R., Peters, J., & Deisenrothy, M. (2014). Pareto front modeling for sensitivity
analysis in multi-objective bayesian optimization. In NIPS Workshop on Bayesian
Optimization, Vol. 5.
Castelletti, A., Corani, G., Rizzolli, A., Soncinie-Sessa, R., & Weber, E. (2002). Reinforcement learning in the operational management of a water system. In IFAC Workshop
on Modeling and Control in Environmental Issues, Keio University, Yokohama, Japan,
pp. 325330.
Castelletti, A., Pianosi, F., & Restelli, M. (2012). Tree-based fitted q-iteration for multiobjective markov decision problems. In Neural Networks (IJCNN), The 2012 International Joint Conference on, pp. 18.
Castelletti, A., Pianosi, F., & Restelli, M. (2013). A multiobjective reinforcement learning
approach to water resources systems operation: Pareto frontier approximation in a
single run. Water Resources Research, 49 (6), 34763486.
Crites, R. H., & Barto, A. G. (1998). Elevator group control using multiple reinforcement
learning agents. Machine Learning, 33 (2-3), 235262.
Das, I., & Dennis, J. (1997). A closer look at drawbacks of minimizing weighted sums of
objectives for pareto set generation in multicriteria optimization problems. Structural
optimization, 14 (1), 6369.
Das, I., & Dennis, J. E. (1998). Normal-boundary intersection: A new method for generating
the pareto surface in nonlinear multicriteria optimization problems. SIAM Journal
on Optimization, 8 (3), 631657.
Deisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics.
Foundations and Trends in Robotics, 2 (1-2), 1142.
223

fiParisi, Pirotta, & Restelli

Fonteneau, R., & Prashanth, L. A. (2014). Simultaneous perturbation algorithms for batch
off-policy search. In 53rd IEEE Conference on Decision and Control, CDC 2014, Los
Angeles, CA, USA, December 15-17, 2014, pp. 26222627. IEEE.
Friedrich, T., Horoba, C., & Neumann, F. (2009). Multiplicative approximations and the
hypervolume indicator. In Proceedings of the 11th Annual Conference on Genetic and
Evolutionary Computation, GECCO 09, pp. 571578, New York, NY, USA. ACM.
Furmston, T., & Barber, D. (2012). A unifying perspective of parametric policy search
methods for markov decision processes. In Pereira, F., Burges, C., Bottou, L., &
Weinberger, K. (Eds.), Advances in Neural Information Processing Systems 25, pp.
27172725. Curran Associates, Inc.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.
In Shavlik, J. W. (Ed.), Proceedings of the Fifteenth International Conference on
Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 24-27, 1998, pp.
197205. Morgan Kaufmann.
Greensmith, E., Bartlett, P. L., & Baxter, J. (2004). Variance reduction techniques for
gradient estimates in reinforcement learning. Journal of Machine Learning Research,
5, 14711530.
Harada, K., Sakuma, J., & Kobayashi, S. (2006). Local search for multiobjective function
optimization: Pareto descent method. In Proceedings of the 8th Annual Conference
on Genetic and Evolutionary Computation, GECCO 06, pp. 659666, New York, NY,
USA. ACM.
Harada, K., Sakuma, J., Kobayashi, S., & Ono, I. (2007). Uniform sampling of local paretooptimal solution curves by pareto path following and its applications in multi-objective
GA. In Lipson, H. (Ed.), Genetic and Evolutionary Computation Conference, GECCO
2007, Proceedings, London, England, UK, July 7-11, 2007, pp. 813820. ACM.
Kakade, S. (2001). Optimizing average reward using discounted rewards. In Helmbold, D. P.,
& Williamson, R. C. (Eds.), Computational Learning Theory, 14th Annual Conference
on Computational Learning Theory, COLT 2001 and 5th European Conference on
Computational Learning Theory, EuroCOLT 2001, Amsterdam, The Netherlands, July
16-19, 2001, Proceedings, Vol. 2111 of Lecture Notes in Computer Science, pp. 605
615. Springer.
Koski, J., & Silvennoinen, R. (1987). Norm methods and partial weighting in multicriterion optimization of structures. International Journal for Numerical Methods in
Engineering, 24 (6), 11011121.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2012). Linear fitted-q iteration with multiple
reward functions. Journal of Machine Learning Research, 13, 32533295.
Lizotte, D. J., Bowling, M. H., & Murphy, S. A. (2010). Efficient reinforcement learning with
multiple reward functions for randomized controlled trial analysis. In Furnkranz, J.,
& Joachims, T. (Eds.), Proceedings of the 27th International Conference on Machine
Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 695702. Omnipress.
224

fiMORL through Continuous Pareto Manifold Approximation

Magnus, J. R., & Neudecker, H. (1999). Matrix Differential Calculus with Applications
in Statistics and Econometrics. Wiley Ser. Probab. Statist.: Texts and References
Section. Wiley.
Mannor, S., & Shimkin, N. (2002). The steering approach for multi-criteria reinforcement
learning. In Dietterich, T., Becker, S., & Ghahramani, Z. (Eds.), Advances in Neural
Information Processing Systems 14, pp. 15631570. MIT Press.
Mannor, S., & Shimkin, N. (2004). A geometric approach to multi-criterion reinforcement
learning. J. Mach. Learn. Res., 5, 325360.
Messac, A., & Ismail-Yahaya, A. (2002). Multiobjective robust design using physical programming. Structural and Multidisciplinary Optimization, 23 (5), 357371.
Messac, A., Ismail-Yahaya, A., & Mattson, C. A. (2003). The normalized normal constraint method for generating the pareto frontier. Structural and multidisciplinary
optimization, 25 (2), 8698.
Munkres, J. R. (1997). Analysis On Manifolds. Adv. Books Classics Series. Westview Press.
Natarajan, S., & Tadepalli, P. (2005). Dynamic preferences in multi-criteria reinforcement
learning. In Raedt, L. D., & Wrobel, S. (Eds.), Machine Learning, Proceedings of
the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August
7-11, 2005, Vol. 119 of ACM International Conference Proceeding Series, pp. 601608.
ACM.
Nojima, Y., Kojima, F., & Kubota, N. (2003). Local episode-based learning of multiobjective behavior coordination for a mobile robot in dynamic environments. In Fuzzy
Systems, 2003. FUZZ 03. The 12th IEEE International Conference on, Vol. 1, pp.
307312 vol.1.
Okabe, T., Jin, Y., & Sendhoff, B. (2003). A critical survey of performance indices for
multi-objective optimisation. In Evolutionary Computation, 2003. CEC 03. The 2003
Congress on, Vol. 2, pp. 878885 Vol.2.
Parisi, S., Pirotta, M., Smacchia, N., Bascetta, L., & Restelli, M. (2014). Policy gradient
approaches for multi-objective sequential decision making. In 2014 International Joint
Conference on Neural Networks, IJCNN 2014, Beijing, China, July 6-11, 2014, pp.
23232330. IEEE.
Perny, P., & Weng, P. (2010). On finding compromise solutions in multiobjective markov
decision processes. In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), ECAI 2010 19th European Conference on Artificial Intelligence, Lisbon, Portugal, August 16-20,
2010, Proceedings, Vol. 215 of Frontiers in Artificial Intelligence and Applications, pp.
969970. IOS Press.
Peters, J. (2007). Machine Learning of Motor Skills for Robotics. Ph.D. thesis, University
of Southern California.
Peters, J., Mulling, K., & Altun, Y. (2010). Relative entropy policy search. In Fox, M.,
& Poole, D. (Eds.), Proceedings of the Twenty-Fourth AAAI Conference on Artificial
Intelligence (AAAI 2010), pp. 16071612. AAAI Press.
225

fiParisi, Pirotta, & Restelli

Peters, J., & Schaal, S. (2008a). Natural actor-critic. Neurocomputing, 71 (7-9), 1180  1190.
Progress in Modeling, Theory, and Application of Computational Intelligenc 15th
European Symposium on Artificial Neural Networks 2007 15th European Symposium
on Artificial Neural Networks 2007.
Peters, J., & Schaal, S. (2008b). Reinforcement learning of motor skills with policy gradients.
Neural Networks, 21 (4), 682  697. Robotics and Neuroscience.
Pianosi, F., Castelletti, A., & Restelli, M. (2013). Tree-based fitted q-iteration for multiobjective markov decision processes in water resource management. Journal of Hydroinformatics, 15 (2), 258270.
Pirotta, M., Parisi, S., & Restelli, M. (2015). Multi-objective reinforcement learning with
continuous pareto frontier approximation. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,
2015, Austin, Texas, USA., pp. 29282934. AAAI Press.
Pirotta, M., Restelli, M., & Bascetta, L. (2013). Adaptive step-size for policy gradient
methods. In Burges, C. J. C., Bottou, L., Ghahramani, Z., & Weinberger, K. Q. (Eds.),
Advances in Neural Information Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pp. 13941402.
Robert, C., & Casella, G. (2004). Monte Carlo Statistical Methods. Springer Texts in
Statistics. Springer-Verlag New York.
Roijers, D. M., Vamplew, P., Whiteson, S., & Dazeley, R. (2013). A survey of multi-objective
sequential decision-making. Journal of Artificial Intelligence Research, 48, 67113.
Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2015). Computing convex coverage sets
for faster multi-objective coordination. Journal of Artificial Intelligence Research, 52,
399443.
Romero, C. (2001). Extended lexicographic goal programming: a unifying approach. Omega,
29 (1), 6371.
Shelton, C. R. (2001). Importance Sampling for Reinforcement Learning with Multiple
Objectives. Ph.D. thesis, Massachusetts Institute of Technology.
Steuer, R. E., & Choo, E.-U. (1983). An interactive weighted tchebycheff procedure for
multiple objective programming. Mathematical Programming, 26 (3), 326344.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. A Bradford
book. Bradford Book.
Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. (2000). Policy gradient
methods for reinforcement learning with function approximation. In Solla, S., Leen,
T., & Muller, K. (Eds.), Advances in Neural Information Processing Systems 12, pp.
10571063. MIT Press.
Tesauro, G., Das, R., Chan, H., Kephart, J., Levine, D., Rawson, F., & Lefurgy, C. (2008).
Managing power consumption and performance of computing systems using reinforcement learning. In Platt, J., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances in
Neural Information Processing Systems 20, pp. 14971504. Curran Associates, Inc.
226

fiMORL through Continuous Pareto Manifold Approximation

Vamplew, P., Dazeley, R., Berry, A., Issabekov, R., & Dekker, E. (2011). Empirical evaluation methods for multiobjective reinforcement learning algorithms. Machine Learning,
84 (1-2), 5180.
Van Moffaert, K., Drugan, M. M., & Nowe, A. (2013). Scalarized multi-objective reinforcement learning: Novel design techniques. In Adaptive Dynamic Programming And
Reinforcement Learning (ADPRL), 2013 IEEE Symposium on, pp. 191199.
Van Moffaert, K., & Nowe, A. (2014). Multi-objective reinforcement learning using sets of
pareto dominating policies. Journal of Machine Learning Research, 15, 34833512.
Waltz, F. M. (1967). An engineering approach: Hierarchical optimization criteria. Automatic
Control, IEEE Transactions on, 12 (2), 179180.
Wang, W., & Sebag, M. (2013). Hypervolume indicator and dominance reward based multiobjective monte-carlo tree search. Machine Learning, 92 (2-3), 403429.
Williams, R. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8 (3-4), 229256.
Yu, P., & Leitmann, G. (1974). Compromise solutions, domination structures, and salukvadzes solution. Journal of Optimization Theory and Applications, 13 (3), 362378.
Zitzler, E., Thiele, L., & Bader, J. (2010). On set-based multiobjective optimization. Evolutionary Computation, IEEE Transactions on, 14 (1), 5879.
Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., & da Fonseca, V. G. (2003). Performance assessment of multiobjective optimizers: an analysis and review. Evolutionary
Computation, IEEE Transactions on, 7 (2), 117132.

227

fiJournal of Artificial Intelligence Research 57 (2016) 465-508

Submitted 03/16; published 11/16

PROMOCA: Probabilistic Modeling and Analysis
of Agents in Commitment Protocols
Akn Gunay
Yang Liu
Jie Zhang

akingunay@ntu.edu.sg
yangliu@ntu.edu.sg
zhangj@ntu.edu.sg

School of Computer Science and Engineering
Nanyang Technological University, Singapore

Abstract
Social commitment protocols regulate interactions of agents in multiagent systems. Several methods have been developed to analyze properties of commitment protocols. However,
analysis of an agents behavior in a commitment protocol, which should take into account
the agents goals and beliefs, has received less attention. In this paper we present ProMoca
framework to address this issue. Firstly, we develop an expressive formal language to model
agents with respect to their commitments. Our language provides dedicated elements to
define commitment protocols, and model agents in terms of their goals, behaviors, and
beliefs. Furthermore, our language provides probabilistic and non-deterministic elements
to model uncertainty in agents beliefs. Secondly, we identify two essential properties of an
agent with respect to a commitment protocol, namely compliance and goal satisfaction. We
formalize these properties using a probabilistic variant of linear temporal logic. Thirdly,
we adapt a probabilistic model checking algorithm to automatically analyze compliance
and goal satisfaction properties. Finally, we present empirical results about efficiency and
scalability of ProMoca.

1. Introduction
Social commitments provide a formal framework to define, regulate, and reason about interactions of agents in multiagent systems (Singh, 1999). A commitment is made from a debtor
to a creditor to bring about a condition. For instance, a merchant (debtor) is committed
to a customer (creditor) to deliver the goods that are purchased by the customer. Every
commitment has a state that changes according to some events. For instance, when the
merchant delivers the purchased goods, her commitment to the customer becomes fulfilled.
Commitments do not enforce agents to bring about certain events. Instead, they regulate
agents by defining how events affect states of their commitments. In other words, agents decide on fulfilling or violating their commitments autonomously. For instance, the merchant
may decide not to deliver the goods to the customer. However, this event result in violation
of her commitment, which may have consequences (e.g., the merchant may be sanctioned
and also loses reputation). Through regulation, commitments establish the desired level
of interdependence among agents without interfering with their autonomy. Commitments
can be combined to form commitment protocols, which can capture complex interactions
among agents (Yolum & Singh, 2002).
Analysis of commitment protocols properties is essential to ensure their effective operation. However, such analysis is challenging due to the rapidly increasing complexity
c
2016
AI Access Foundation. All rights reserved.

fiGunay, Liu & Zhang

of interaction in such protocols. Hence, development of efficient formal analysis methods
for commitment protocols, which can cope with their complexity, is essential to create effective multiagent systems. Various formal properties of commitment protocols have been
studied and several methods have been developed to analyze them (Yolum, 2007; Desai,
Cheng, Chopra, & Singh, 2007a; Desai, Narendra, & Singh, 2008; El Menshawy, Bentahar,
El Kholy, & Dssouli, 2013; El Kholy, Bentahar, Menshawy, Qu, & Dssouli, 2014).
However, analysis of an agents properties when enacting a commitment protocol has
received less attention (Marengo, Baldoni, Baroglio, Chopra, Patti, & Singh, 2011; Gunay &
Yolum, 2013; Kafal, Gunay, & Yolum, 2014). Such analysis aims to verify formal properties
of an agents behavior with respect to a commitment protocol (e.g., compliance of an agents
behavior with a protocol), which is crucial for developing effective agents. A key challenge
of analyzing an agents behavior with respect to a commitment protocol is uncertainty,
which naturally occurs in multiagent systems due to several factors. One major factor is
agent autonomy, which mainly corresponds to epistemic uncertainty. Specifically, agents act
autonomously to pursue their own private goals. Hence, an agent cannot be certain about
behaviors of other agents. Similarly, an agents lack of awareness about its environment,
which may be a result of limited sensory and reasoning capabilities, leads to epistemic
uncertainty. Furthermore, many physical systems involve irreducible aleatory uncertainty
that occurs due to physical variability present in an agents environment. Agents cope with
uncertainty by utilizing reasoning methods that can use potentially wrong or incomplete
beliefs instead of exact knowledge (Halpern, 2003).
Most of the previous work on commitments do not address uncertainty. To fill this gap,
in our previous work, we developed an analysis method for commitment protocols using
probabilistic model checking, which can handle uncertainty in behaviors of agents (Gunay,
Songzheng, Liu, & Zhang, 2015). Our method uses an abstract formalism (specifically, a
probabilistic automaton) for modeling and analyzing behaviors of agents in commitment
protocols. However, from practical point of view, manual definition of commitment protocols and behaviors of agents in such an abstract formalism is a time consuming and error
prone task. Besides, it requires a modeler to have knowledge and expertise on the specific
abstract formalism. Because of these issues, use of an abstract formalism for modeling is
not adequate for practical development settings (e.g., when a developer verifies her own
agents implementation). The adequate approach is to use an expressive high level formal
language for modeling, which can be automatically translated into an abstract formalism for
formal analysis. To the best of our knowledge, there is no such dedicated formal modeling
language to capture uncertainty of agents in the context of commitment protocols.
In this paper we present ProMoca framework to address this issue. ProMoca provides an expressive modeling language that includes various language elements to model
commitment protocols, and also various aspects of agents, such as different goal types,
beliefs, and behaviors. Besides, ProMoca supports probabilistic modeling to capture uncertainty in behaviors and beliefs of agents. In ProMoca we also pay special attention to
two essential properties of an agents behavior with respect to a commitment protocol. The
first of these properties is compliance of an agents behavior with a commitment protocol.
That is, whether an agents behavior fulfills the agents commitments. The second property
considers whether an agents behavior satisfies the agents goals in the context of a commitment protocol. Since agents are interdependent, neither compliance nor goal satisfaction
466

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

can be analyzed just considering an agents own behavior in isolation. To cope with interdependence, ProMoca uses an agents beliefs about other agents behaviors. ProMoca
adopts our previous probabilistic model checking method (Gunay et al., 2015) for analyzing
these properties. To evaluate efficiency and scalability of ProMoca we use three realistic examples. Besides, we compare ProMocas performance with PRISM, which is a
state-of-the-art probabilistic model checker. Our main contributions are as follows:
 We develop the new modeling language ProMoca (we use the name ProMoca both
for the modeling language and the analysis framework interchangeably). We define
formal syntax and operational semantics of ProMoca, which provides dedicated
language elements to define and manage a commitment protocol. ProMoca also
provides various language elements to define behaviors, goals, and beliefs of an agent in
the context of the commitment protocol. Moreover, ProMoca provides probabilistic
and non-deterministic elements to model uncertainty in an agents beliefs. To the
best of our knowledge, ProMoca is the first formal language that provides dedicated
elements for modeling commitment protocols, agents, and uncertainty in a unified
environment.
 We identify and develop formal definitions of compliance and goal satisfaction properties. Different than the previous definitions of these properties in the literature, in our
formalization we take uncertainty of an agents beliefs about other agents behaviors
into account using a probabilistic variant of linear temporal logic. Accordingly, we
can define these properties in a more precise and flexible manner.
 To analyze an agents behavior in the context of a commitment protocol by verifying
compliance and goal satisfaction, we define the complete ProMoca framework, in
which modeling is done by using our new formal language that we develop in this
paper, and analysis is done by adopting our previously developed probabilistic model
checking method (Gunay et al., 2015).
 We substantially extend our earlier preliminary experimental result (Gunay et al.,
2015), by conducting a detailed empirical study to validate ProMocas practical
usefulness and scalability in three well-studied scenarios from the literature, namely
aerospace aftercare, international insurance (Jakob, Pechoucek, Miles, & Luck, 2008),
and NetBill (Sirbu & Tygar, 1995). Our results show that ProMoca outperforms
the state-of-the-art general purpose probabilistic model checker PRISM when verifying
compliance and goal satisfaction properties of an agents behavior in the context of a
commitment protocol.
This paper is organized as follows. In Section 2, we introduce the fundamental concepts
about commitment protocols, agents, and analysis of their behaviors. In Section 3, we define
ProMocas formal syntax and operational semantics. In Section 4, we define our analysis
framework and model checking algorithm. In Section 5, we present an empirical evaluation
of our framework. Section 6 provides a survey of related work. Finally, in Section 7, we
conclude the paper with a discussion of our approach and listing our future directions.
467

fiGunay, Liu & Zhang

2. Background: Commitments, Agents, and Analysis
In this section we provide an overview of commitments, and agent concepts, such as behaviors, goals, and beliefs, which are key to ProMoca. We also discuss compliance and goal
satisfaction properties with respect to these concepts to motivate our research.
2.1 Commitments
A commitment is made from one agent to another to bring about a condition (Singh, 1999).
Conventionally, a commitment is denoted by C(debtor, creditor, antecedent, consequent)
in which debtor and creditor are agents, and antecedent and consequent are conditions.
Intuitively, a commitment means that the debtor is committed to the creditor to bring
about the consequent, if the antecedent holds. For instance, the commitment C(merchant,
customer, goods-purchased, goods-delivered ) captures the merchants commitment to the
customer to deliver some goods (represented by the proposition goods-delivered ), if the
goods are purchased by the customer (represented by the proposition goods-purchased ).
The exact meaning of a commitment depends on the type of the condition that is used
as its consequent. Achievement conditions are the most widely used type for a commitments consequent. For brevity, we call a commitment with such a condition simply as an
achievement commitment. The above commitment of the merchant to deliver the purchased
goods to the customer is an example of an achievement commitment. Use of a maintenance
condition for a commitments consequent has been also considered in the literature (Fornara
& Colombetti, 2002; Mallya & Huhns, 2003; Gunay & Yolum, 2011; Chesani, Mello, Montali, & Torroni, 2013). A commitment with a maintenance condition as its consequent is
fulfilled, if the condition is maintained until another termination condition occurs. We call
a commitment with such a condition simply as a maintenance commitment. For instance,
an internet service provider may be committed to a customer to provide internet connection
for a month, if the customer purchases a data plan from the internet service provider.
Commitments can also be considered in a subscription model. For instance, instead of
purchasing a data plan for a single month, a customer may subscribe to a data plan for a
year in a monthly basis (i.e., duration of each period in the subscription is a month). As a
result, the internet service provider has a separate commitment for each month of the year to
provide internet connection to the customer, as long as the customer pays the corresponding
monthly subscription fee. In technical terms, the subscription itself can be considered as
a template (e.g., if the customer pays the subscription fee for a specific month, the service
provider becomes committed to provide internet connection for that month) for creating
the concrete commitment instances for each period of the subscription. For example, we
create twelve commitment instances from the above template (one for each month) by
setting each commitments antecedent and consequent to propositions that model payment
of subscription fee and provision of internet for a specific month, respectively. To the best
of our knowledge, such a subscription model is not formally considered in the previous
research. However, subscriptions are part of many real world settings. Accordingly, we
formalize them in ProMoca. Note that the subscription model can be considered as a
special case of the meta-commitment concept (Chopra & Singh, 2015, 2016a), which is
more general since it is not bound by the specification of a subscription.
468

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

null
creditor
releases
released

creditor
releases

debtor
creates

debtor
discharges

expired

conditional
creditor
detaches

active

fulfilled

antecedent
expires

compensated

violated
debtor cancels or
fails to discharge

debtor
compensates

Figure 1: The lifecycle of a commitment.
A commitments state evolves over time according to public events (i.e., they are independent from an agents internal state). The lifecycle of a commitment has been studied
extensively in the literature (Yolum & Singh, 2002; Fornara & Colombetti, 2002; Chesani
et al., 2013). Here, we use the commitment lifecycle that we show in Figure 1, where the
labels of the rectangles represent commitment states, and the edge labels show events with
corresponding agents who trigger the state changes.
A commitment is in null state before its creation. A commitment is created by its
debtor in conditional state. In this state neither the antecedent nor the consequent of the
commitment holds. For instance, the commitment from the merchant to the customer is
created by the merchant (i.e., debtor) and initially there is neither a payment nor a delivery.
Hence, the commitment is conditional. The intuitive meaning of a conditional commitment
is similar to an offer that states, if the antecedent starts to holds, the debtor becomes
committed to the creditor to bring about the consequent.
If a commitments antecedent is not satisfied, the commitment becomes expired. The
creditor of a conditional commitment may also explicitly release the debtor from its commitment, which makes the commitment released. Expired and released states are terminal
states. If the antecedent of a conditional commitment is satisfied, the creditor detaches the
commitment, and the commitment becomes active. Detachment of a commitment is independent from how its antecedent is satisfied. That is, the antecedent may be satisfied either
by an event that occurs as a direct result of an action of the creditor (e.g., customer pays),
or by an external event (e.g., the customers bank may pay on behalf of the customer).
An active commitment intuitively means that the debtor is committed to bring about the
consequent of the commitment.
Fulfillment and violation of a commitment depends on the type of the commitments
consequent. If the consequent of an active achievement commitment is satisfied, the debtor
immediately discharges the commitment, and the commitment becomes fulfilled (e.g., the
merchant delivers). A maintenance commitment is fulfilled, if the consequent of the commitment is maintained until another condition that determines the termination of the commitment occurs (e.g., the internet service provider maintains internet connection of the user
until the termination of the data plan). As in the case of the detachment, a commitments
469

fiGunay, Liu & Zhang

fulfillment is also independent from how the consequent is satisfied (e.g., the merchant herself may deliver, or she could delegate the delivery to a courier). A fulfilled commitment
intuitively means that the debtor has honored her responsibility, and fulfilled state is terminal. As in the case of a conditional commitment, a creditor may also release a debtor
from its active commitment.
If either the consequent of an active commitment is not satisfied, or the debtor explicitly
cancels her commitment, the commitment becomes violated. For an achievement commitment, failure of the consequent may depend on another condition (e.g., a deadline). For a
maintenance commitment, failure of the consequent occurs, if the consequent condition does
not hold at any moment before the termination condition of the commitment holds. Note
that due to autonomy of agents, a debtor may intentionally choose to violate a commitment,
even if she can fulfill it. For instance, the merchant may decide to sell the goods to another
customer for better profit, and may violate her commitment to the original customer. On
the other hand, the debtor may also violate a commitment unintentionally. For instance,
the merchant may fail to deliver because of bad weather conditions. In any case, a violated
commitment intuitively means that the debtor failed to honor her responsibility. Depending
on the domain of application, a violated commitment can be compensated by the debtor by
taking a certain action, which makes the commitment state compensated (Torroni, Chesani,
Mello, & Montali, 2010; Kafal & Torroni, 2012; Chopra & Singh, 2015). For instance, if the
merchant fails to deliver the goods to the customer, she violates her commitment. However,
she can refund the customer to compensate her commitments violation. Compensation
allows agents to restore their interaction back to a desirable state, when it is interrupted
due to a violated commitment. Compensated state is terminal. If a commitment is violated
and there is no way for compensation, violated state is counted as terminal.
Note that a subscription does not have a state by itself, since it does not define a concrete
commitment, but it only provides a template for instantiating concrete commitments for
each period of the subscription. Hence, the notion of state for a subscription is captured in
an abstract manner by the states of corresponding concrete commitment instances.
In many applications, agents engage complex interactions that cannot be represented
by a single commitment. To capture all the aspects of such complex interactions, multiple
commitments are considered together as a commitment protocol (Yolum & Singh, 2002).
For instance, while the merchants commitment to the customer captures the payment and
delivery aspects of their interaction, another commitment C(merchant, customer, goodsdefective, goods-replaced ), which states that the merchant is committed to the customer to
replace a defective good, captures the warranty related aspects of their interaction. These
two commitments (and other prospective commitments) form a commitment protocol.
2.2 Agents and Analysis of their Behaviors
In this paper, our main objective is to develop a dedicated formal language for modeling
and analyzing an agents behavior in a commitment protocol. When it is not clear from the
context, we call this particular agent as the target agent to distinguish it from other agents.
In our analysis we use three kinds of information, which are available to the target
agent. The first kind of information is the target agents goals. We divide goals into
two types as achievement and maintenance goals as it is customary in the agent literature
470

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

(van Riemsdijk, Dastani, & Meyer, 2009; Chopra, Dalpiaz, Giorgini, & Mylopoulos, 2010).
Achievement goals model situations, where an agent aims to achieve a certain satisfaction
condition. For instance, a customer may aim to possess some goods. Maintenance goals
model situations where an agent aims to maintain a certain satisfaction condition. For
instance, an internet service provider may aim to maintain its internet service up and
running. Furthermore, both goal types can be considered as one-time or persistent goals.
One-time goals are pursued only once, and in general, they are subject to preconditions. For
example, if the customer needs a certain type of good and she does not possess it, only than
she has a goal to possess the good. Similarly, the internet service provider aims to provide
and maintain internet connection for a customer, only if the customer has subscribed for
the service. One-time goals might also have a termination condition (e.g., a deadline). If
the termination condition of a goal holds before its satisfaction condition, the goal fails.
Contrary to one-time goals, persistent goals do not have pre or termination conditions, and
they persist for the whole lifespan of an agent. In the case of a persistent achievement goal,
an agent aims to satisfy a condition that may fail from time to time, but always restored
again after a while. For instance, a merchant may aim to dispose obsolete goods (e.g., old
versions of a mobile phone) from its warehouse. Occasionally, some obsolete goods may be
kept in the warehouse (e.g., when a new version of the mobile phone first appears, it takes
a while to dispose the old phones), but they should be disposed eventually. On the other
hand, a persistent maintenance goals represents a condition that an agent aims to keep
satisfied continuously during its whole lifespan without any precondition. For instance, the
merchant might aim to maintain a positive bank balance during its whole lifespan.
The second kind of information, which we use in our analysis, is a target agents behavior.
We assume that a target agents behavior is a persistent (i.e., non-terminating) computation,
in which the agent uses a set of if-then type rules to decide on its next action (e.g., if the
merchant has a commitment to make a delivery and the commitment is active, then she
delivers). If there is uncertainty about the results of the agents actions, the decision rules
may include non-deterministic and probabilistic components to model uncertainty. Suppose
that the merchant may fail to deliver on time with probability 0.1 depending on weather
conditions. In this case, the if-then rule that defines the merchants behavior would be:
if the merchant is committed to deliver and weather condition is bad, then the merchant
delivers on time (and fulfills her commitment) with 0.9 probability, and fails to deliver on
time (and violates her commitment) with 0.1 probability. As we demonstrate in the rest of
this paper, combination of if-then rules, non-determinism, and probabilistic choice, in the
context of a persistent computation provides us a rich and flexible model to define various
complex agent behaviors.
The last kind of information that we use in our analysis is the target agents beliefs about
behaviors of other agents. Uncertainty is a natural element of this kind of information, since
other agents are autonomous. Accordingly, we define the target agents beliefs about other
agents behaviors in a similar manner to the target agents own behavior using probabilistic
choice and non-determinism. For instance, suppose that the merchant is dependent on a
courier to make a delivery, which is necessary to fulfill her commitment to a customer. In
such a situation the merchant may believe that the courier successfully delivers on time with
probability 0.95 (and fails to deliver on time with probability 0.05). As we demonstrate
later, we capture such a situation using a probabilistic choice in the beliefs of the target
471

fiGunay, Liu & Zhang

agent in a similar manner to our earlier example. Note that, in this paper we assume that
probability values in an agents behavior and beliefs are set by a modeler. These values
may reflect intuition of the modeler, or they can be obtained from a statistical model (e.g.,
previous delivery results of the merchant or courier in different weather conditions).
Considering these three kinds of information, our objective is to analyze two key properties about a target agents behavior in a commitment protocol. The first property is
compliance, which holds if a target agents behavior fulfills all of its active commitments
in a protocol. A relaxed version of compliance may take compensation into account. That
is, an agent complies with a commitment protocol by fulfilling its active commitments and
also by compensating its violated commitments. The second property is goal satisfaction,
which holds if a target agent satisfies its goals by enacting a commitment protocol. A target agents beliefs about other agents play a crucial role in analyzing both properties since
there is interdependence among agents. In other words, other agents behaviors directly
affect target agent. For instance, when the merchant relies on a courier for delivery, the
merchants compliance with the protocol and goal satisfaction cannot be correctly verified
without taking the couriers behavior into account. In fact, if we consider only the merchants own behavior, neither compliance nor goal satisfaction hold for the merchant, since
she does not have delivery capability. However, if the merchant believes that the courier
delivers with a high probability, then we can conclude that the merchant complies with the
protocol and also satisfies her goal by enacting this protocol.
2.3 Running Example
In the rest of the paper we use a running example from aerospace aftercare domain, which
is introduced by Jakob et al. (2008), and used in the literature for the evaluation of
commitments and other normative models (Modgil, Faci, Meneguzzi, Oren, Miles, & Luck,
2009; Desai, Chopra, & Singh, 2009). In this example scenario, there is a manufacturer that
provides aircraft engines to airline operators. When the manufacturer sells an engine to an
airline operator, the manufacturer becomes responsible for keeping the engine operational
by periodically servicing the engine. The airline operator should pay a service fee to the
manufacturer. Besides, the airline operator should also provide operational data of the
engine to the manufacturer, which is needed by the manufacturer to analyze the status
of the engine. In order to service an engine, the manufacturer needs spare engine parts
from several suppliers. However, the manufacturer can only use certain engine parts for
servicing, which are approved by a monitoring aerospace agency. The airline operator may
be monitored by different aerospace agencies depending on the regions in which the airline
operates. Different agencies may have different policies about approved spare engine parts.
Hence, use of a certain part for repair depends on the airline and the respective agencies. If
the operational status of an engine is not maintained, the manufacturer should compensate
this situation by paying a penalty to the airline operator. The manufacturer should also
bring the engine back to the operational state within a certain amount of time. If the
manufacturer fails to compensate, the contract may be canceled by the airline operator.
We consider this example from the engine manufacturers point of view to analyze its
compliance and goal satisfaction.
472

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

3. Modeling of Commitment Protocols and Agents in PROMOCA
We first present the formal syntax of ProMoca in Section 3.1. Then, we define the
operational semantics of ProMoca in Section 3.2, and formalize compliance and goal
satisfaction properties in Section 3.3. Finally, we discuss our key design decisions about
ProMoca in Section 3.4.
3.1 Syntax
A ProMoca model is composed of three blocks, which define a set of global variables, a
commitment protocol, and a target agent (i.e., the agent that we aim to verify). Below we
define the syntax of each block with illustrative examples.
3.1.1 Global Variables
A ProMoca model includes a finite set of global variables, which are defined within the
block globals{var; . . . var;}. Each variable var is defined using the keyword variable with
the following syntax:
var ::= variable variable-name : {value-set} = value
where variable-name is the unique name of the variable, value-set is a finite set of strings that
defines the domain of the variable (i.e., an enumeration of the variables possible values),
and value is the initial value of the variable, which must be an element of value-set. For
instance, the status of an aircraft engine can be modeled using the following variable:
variable engine-status : {unknown, operational, malfunction, maintenance} = unknown;

The name of the variable is engine-status. The domain of the variable consists of four
values, which capture different operational states of the engine. The initial value of this
variable is unknown.
3.1.2 Commitment Protocol
A commitment protocol is a composition of a finite set of commitments. The commitment
protocol is defined within the block protocol{comm; . . . comm;}. Each commitment comm
is defined using the keyword commitment and the following syntax:
comm ::= commitment(commitment-id, commitment-type, subscription-period,
debtor-id, creditor-id, antecedent, expiration, consequent, termination)
{observers}[commitment-id]
The first parameter commitment-id is the unique string identifier of the commitment.
The type of the commitment is defined by commitment-type, which is either achievement or
maintenance. Subscriptions are modeled using the subscription-period parameter, which is a
finite integer greater than 0 that represents the total number of the subscriptions periods.
If the commitment is not a subscription, this parameter can be omitted, and ProMoca sets
473

fiGunay, Liu & Zhang

its value to 1 by default. The parameters debtor-id and creditor-id are the identifiers of the
commitments debtor and creditor, respectively. The parameters antecedent, expiration, consequent, and termination are expressions to define the antecedent, expiration, consequent,
and termination condition of the commitment, respectively. The optional parameter observers is a set of agent identifiers that includes the agents, who can observe the state of the
commitment. By default, the debtor and the creditor of a commitment can always observe
the state of their commitment, and it is not required to list them as observers. If there are
no observers other than the commitments debtor and creditor, observers can be omitted.
The last optional parameter commitment-id is the identifier of another commitment that
can be used to compensate violation of the commitment. If there is no compensation of the
commitments violation, this parameter can be omitted.
An expression (e.g., antecedent parameter of a commitment) is a logical expression that
is a compositions of conjunctions and disjunctions over global variable comparisons. Formal
syntax of an expression is as follows:
expression
comparison
const

::=
::=
::=

expression and expression | expression or expression | comparison
variable-name == value | variable-name != value | const
TRUE | FALSE

In an expression, we use the standard semantics of the logical operators and and or.
Similarly, in comparison we use the standard equal to and not equal to semantics for
operators == and !=, respectively. TRUE and FALSE are the standard boolean constants.
Parentheses can be used regularly to define precedence of the logical operators, which we
omit in the formal syntax for brevity.
ProMoca automatically creates several variables to capture the lifecycle of a protocols commitments. Firstly, ProMoca creates a status variable for each commitment in
the protocol. The names of these variables are automatically set by ProMoca using the
<commitment-id>-state pattern (e.g., for a commitment with commitment-id c-1, the corresponding variables name is c-1-state). Each such status variable has the domain {null,
conditional, active, fulfilled, violated, expired, released, compensated} to capture the
corresponding state of the commitment. These variables can be used in a ProMoca model
as read-only variables. Secondly, ProMoca creates a subscription counter for each commitment to track the fulfillment of subscriptions. However, these counters are internal to
ProMoca, and they cannot be directly accessed within a ProMoca model. The semantics
of these variables are formalized later in Section 3.2.
Now we present some example commitments from the aerospace aftercare scenario to
illustrate use of ProMocas commitment syntax. Let us start with the simple achievement
commitment: if the operator pays the price of an engine to the manufacturer, the manufacturer is committed to deliver the engine. Suppose that there are payment and delivery
deadlines (defined as variables). The commitment can be observed by the aerospace agency.
If the commitment is violated, it cannot be compensated. This commitment is written in
ProMoca as follows:
commitment(c-1, achievement, 1, manufacturer, operator,
engine-paid == done, payment-deadline == past,

474

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

engine-delivered == done, delivery-deadline == past)
{aerospace-agency};

In our second example we consider maintenance of an engine according to the commitment: if the manufacturer delivers an engine to the operator, the manufacturer is
committed to the operator to maintain the engine operational for a year, which we identify as c-2. If the manufacturer fails to maintain the engine operational, and violates c-2, it
should compensate this situation by repairing the engine and also paying a penalty, which
is defined in the compensating commitment c-3. Note that c-2 explicitly declares c-3 as its
compensating commitment. This declaration is essential for correctly managing the lifecycles of these commitments as we show later in Section 3.2. Note that we use the variable
contract in the commitments to capture whether the contract between the manufacturer
and operator is still valid. If the contract between these parties is terminated (e.g., delivery
of an engine is canceled), their commitments expire.
commitment(c-2, maintenance, 1, manufacturer, operator,
contract == valid and engine-delivered == done, contract == terminated,
engine-status == operational or engine-status == maintenance, end-of-year == past)
{aerospace-agency}[c-3];

commitment(c-3, achievement, 1, manufacturer, operator,
contract == valid and c-2-status == violated,
contract == terminated or c-2-status == fulfilled or c-2-status == released,
engine-status == operational and penalty-paid == done,
compensation-deadline == past)
{aerospace-agency};

Before we continue, let us emphasize the different meaning of the termination condition
in achievement and maintenance commitments. In an achievement commitment, the debtor
is committed to bring about the consequent at some point before the occurrence of the
termination condition. For example, in the achievement commitment c-1, the termination
condition is the delivery deadline, and c-1 is fulfilled, if the engine is delivered at any
point before this deadline. Otherwise, c-1 becomes violated. On the other hand, in a
maintenance commitment, the debtor is committed to maintain the consequent at every
point from the commitments detachment until the occurrence of the termination condition.
For example, in the maintenance commitment c-2, the termination condition is completion
of one year after delivery of an engine, and fulfillment occurs if the engines operational
status is preserved from the detachment of the commitment until the completion of the
year. Otherwise, the commitment becomes violated immediately.
To complement c-2, we need another commitment c-4 that defines monthly servicing
of an engine using a subscription model as follows. After the delivery of an engine, the
manufacturer is committed to the operator to service the engine in a monthly basis for a
475

fiGunay, Liu & Zhang

year, as long as the operator pays the monthly service fee and provides the engine usage
reports. For brevity, we omit the observers and compensation for this commitment.
commitment(c-4-, achievement, 12, manufacturer, operator,
contract == valid and engine-delivered == done and service-fee-paid-* == done and
engine-report-provided-* == done,
contract == terminated or fee-deadline-* == past or engine-report-provided-* == failed,
engine-serviced-* == done,
engine-serviced-* == failed or engine-serviced-* == late);

Since the commitment is modeled as a subscription for a year in a monthly basis, there
are twelve instances of this commitment. However, some conditions of the commitment
apply only to individual instances. For example, there is a separate payment of the service
fee for each month. ProMoca provides * notation for the variables that correspond to such
instance conditions. For example, service-fee-paid-* means that there are twelve variables
(e.g., service-fee-paid-1, service-fee-paid-2, etc.) to model each separate payment of the
service fee. The first instance of the commitment becomes active, if service-fee-paid-1 and
the other conditions of the antecedent hold. Note that only some conditions are instance
specific. Other conditions, such as the delivery of the engine (i.e., engine-delivered), are used
by all instances referring to the same variable.
3.1.3 Target Agent Specification
The target agent specification is defined in the block agent[agent-id]{agent-spec}, where
agent-id is the unique identifier of the target agent. The target agent specification agent-spec
is composed of four parts. The first part is the definition of the target agents finite set
of local variables, which are enclosed by the block locals{var; . . . var;}. Local variables are
intended to model the internal state of the target agent. Hence, they cannot be used in
the context of global elements, and accordingly they cannot be accessed by other agents.
For instance, the antecedent and the consequent of a commitment cannot include a local
variable of an agent. Local variables are defined using the global variable syntax.
The second part of the target agent specification is the definition of the target agents
goals, which are defined in the block goals{goal; . . . goal;}. The syntax of goal is as follows:
goal
pa-goal
pm-goal
a-goal
m-goal

::=
::=
::=
::=
::=

pa-goal | pm-goal | a-goal | m-goal
pagoal(satisfaction)
pmgoal(satisfaction)
agoal(precondition, satisfaction, termination)
mgoal(precondition, satisfaction, termination)

Persistent achievement and maintenance goals are denoted by pa-goal and pm-goal, respectively. In both goal types, satisfaction parameters are expressions that model satisfaction
condition of the goal, which can include only global and local variable comparisons (i.e., no
commitment-state variables are allowed in these conditions). For example, the persistent
maintenance goal of the manufacturer to keep a positive bank balance while interacting
with operators and suppliers can be modeled by the following goal:
476

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

pmgoal(bank-balance == positive);

One-time achievement and maintenance goals are denoted by a-goal and m-goal, respectively. For both goal types, precondition, satisfaction, termination are expressions that
model the pre, satisfaction, and termination condition of the goal. The pre and termination
conditions of both goal types can include global, local, and commitment state variable comparisons. However, the satisfaction condition for both goal types can include only global
and local variable comparisons.
For example, the manufacturers one-time goal to deliver an engine when a payment
is made (captured by the active status of commitment c-1) can be modeled using the
following achievement goal.
agoal(c-1-state == active, engine-delivered == done,
engine-delivered == failed or engine-delivered == late);

Note that if the delivery of the engine fails or it is delivered later than the deadline (i.e.,
delivered == failed or delivered == late), the one-time goal becomes terminated.
The last two parts of the target agent specification are the definition of the target agents
own behavior and the target agents beliefs about the other agents behaviors. The target
agents own behavior is defined in the block behavior{behavior}. The target agents beliefs about the other agents behaviors are defined in the block beliefs{[agent-id]{behavior};
. . . [agent-id]{behavior};}. A behavior (either the target agents own behavior or a believed
behavior of another agent) is defined according to the following syntax:
behavior

::=

cont | stop |
action-label{assign, . . . , assign} -> behavior |
commit{commitment-id} -> behavior |
release{commitment-id} -> behavior |
cancel{commitment-id} -> behavior |
behavior <> behavior |
[expression] behavior |
(probability) behavior . . . (probability) behavior

Let us explain the intuitive meaning of each element in the behavior syntax. The
primitive behaviors cont and stop restarts and terminates the current behavior, respectively.
We use action-label{assign; . . . ; assign;} to capture agent actions and their effects on the
variables. Each action has a label action-label that is used only to improve readability. That
is, the label of an action does not have any meaning and the same label can be used with
different sets of assignments at different places of the model. The effects of an action are
captured by assigning new values to a finite set of variables. Actions in ProMoca are
atomic. Hence, all the assignments that occur as the result of an action, are done in the
given order without interruption. The syntax of an assignment assign is as fallows:
assign ::= var-name = value
477

fiGunay, Liu & Zhang

Beside domain dependent actions (e.g., delivering an engine), ProMoca also provides
three meta-actions commit, release, and cancel that capture the corresponding commitment operations to alter the state of a commitment (i.e., commitment state variable). The
action commit is used by a debtor to create a commitment, release is used by a creditor to release the debtor from its commitment, and cancel is used by a debtor to cancel
her commitment. Other than these three, ProMoca does not provide any meta-action
to manipulate the state of a commitment directly. Instead, the state of a commitment is
captured internally by ProMoca according to the values of the commitments parameters
(e.g., consequent) as we define in the semantics of ProMoca.
A non-deterministic choice over behaviors is captured by behavior <> behavior. That is,
either the first or the second behavior can be performed by the agent, and the decision is
non-deterministic. [expression] behavior captures a guarded behavior (i.e., if-then rule). The
guard condition expression is a logical expression (as defined before). If the guard condition
holds, then the corresponding behavior is performed. Otherwise, the behavior becomes
blocked until the condition holds. Note that arbitrary number of guarded behaviors can be
combined using a non-deterministic choice to model complex decision procedures. If this is
the case, one of the behaviors, for which the corresponding guard condition holds, is selected
for execution in a non-deterministic manner. Finally, (probability) behavior . . . (probability)
behavior denotes a probabilistic choice among the possible behaviors. Each probability is a
real value and the sum of all probabilities is equal to 1 for a probabilistic choice. Probabilistic and non-deterministic choices are the main elements of ProMoca to incorporate
uncertainty into the modeling and analysis processes.
An important distinction between the behavior and beliefs blocks is the use of variable
types. Global variables can be used (both for reading and assignment) in both blocks.
However, local variables of the target agent can only be used in the behavior block, since
they are private to the target agent. Local variables are not accessible either for reading or
for assignment in the beliefs block, since this block models other agents behaviors, which
should not use the target agents local variables. Accessibility of commitment state variables
(always as read-only) depend on the role of the corresponding agent in the commitments.
For instance, if the target agent is a debtor, creditor, or observer of a commitment, the
corresponding state variable can be queried from the behavior block. Similarly, if another
agent is a debtor, creditor, or observer of a commitment, the part of the beliefs block that
corresponds to this agents behavior, can query the state of a commitment.
Let us present a (partial) behavior example for the manufacturer as below, which is
interpreted as follows. If a commitment of the manufacturer to service an engine (i.e.,
an instance of c-4) becomes active, the manufacturer may behave in three different ways,
according to the current situation. If the manufacturer already has the necessary parts to
service the engine (modeled by the local variable part-availability), the manufacturer services
the engine on time with 0.99 probability. There is a small probability (0.01) of failure to
complete the service on time, which violates c-4. When the manufacturer does not have
the necessary parts to service the engine, it can order them either from the first or second
supplier. The manufacturer believes that (as we demonstrate later) the first supplier mostly
delivers ordered parts on time. The manufacturer also believes that the second supplier
delivers ordered parts late almost all the time. Furthermore, the manufacturer believes
that the second supplier may even completely fail to deliver ordered parts. Accordingly,
478

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

the manufacturer prefers to work with the first supplier. However, if the first supplier is
not approved by the aerospace agency for the particular operator, who owns the engine,
the manufacturer works with the second supplier. This situation is captured in the guard
statements. Note that in the case of late delivery of ordered parts, timely service of the
engine is affected. That is, if a late delivery occurs, the manufacturer services on time only
with 0.75 probability. Hence, there is a bigger risk for violating c-4. Finally, after servicing
and engine, availability of the spare parts for the next service period is determined as a
non-deterministic choice. That is, the manufacturer does not know in advance how many
spare parts it needs, and therefore any possible result (e.g., running out of spare parts)
should be considered.
[c-4-state == active] {
[part-availability == available] {
(0.99) service-on-time{engine-serviced = done} ->
all-parts-consumed{part-availability == unavailable} <> cont
(0.01) service-late{engine-serviced = late} ->
all-parts-consumed{part-availability == unavailable} <> cont
} <>
[part-availability == unavailable and supplier-1 == approved] {
[part-delivery == on-time]
(0.99) service-on-time{engine-serviced = done} ->
all-parts-consumed{part-availability == unavailable} <> cont
(0.01) service-late{engine-serviced = late} ->
all-parts-consumed{part-availability == unavailable} <> cont
} <>
[part-delivery == late] {
(0.75) service-on-time{engine-serviced = done} ->
all-parts-consumed{part-availability == unavailable} <> cont
(0.25) service-late{engine-serviced = late} ->
all-parts-consumed{part-availability == unavailable} <> cont
}
} <>
[part-availability == unavailable and supplier-1 != approved and supplier-2 == approved] {
[part-delivery == late]
(0.75) service-on-time{engine-serviced = done} ->
all-parts-consumed{part-availability == unavailable} <> cont
(0.25) service-late{engine-serviced = late} ->
all-parts-consumed{part-availability == unavailable} <> cont
} <>
[part-delivery == failed]
service-failed{engine-serviced = failed} -> cont
}
}
}

479

fiGunay, Liu & Zhang

Lastly, below we demonstrate some beliefs of the manufacturer about the suppliers.
Suppose that the interaction of the manufacturer with the first and second supplier is
regulated by the commitments c-5 and c-6, respectively. These commitments state that if
the manufacturer orders a batch of spare parts from a supplier, the supplier is committed to
deliver the parts to the manufacturer. For brevity we do not show these commitments. As
we have mentioned earlier, the manufacturer believes that the first supplier is more reliable.
Precisely, the first supplier delivers ordered parts on time with 0.8 probability, and late
with 0.2 probability. The manufacturer also believes that the second supplier is unreliable.
Precisely, the second supplier delivers order parts late with 0.95 probability. Besides, with
0.05 probability, the second supplier fails altogether to deliver ordered parts.
beliefs {
[supplier-1] {
[c-5-state == active] {
(0.8) delivers-on-time{part-delivery = on-time} -> cont
(0.2) delivers-late{part-delivery = late} -> cont
}
};
[supplier-2] {
[c-6-state == active] {
(0.95) delivers-late{part-delivery = late} -> cont
(0.05) fails-to-deliver{part-delivery = failed} -> cont
}
};
}

3.2 Semantics
Now we define ProMocas operational semantics. We first define a ProMoca model and
a configuration that captures the global state of a ProMoca model.
Definition 1 (ProMoca Model). A ProMoca model is a tuple O = (Var , Vinit , C, G,
Bagn |||Bbel1 ||| . . . |||Bbeln ) where Var is the set of variables (i.e., the composed set of global,
local, commitment state, and internal variables), Vinit is the initial valuation of the variables in Var , C is the commitment protocol (i.e., a set of commitments), and G is the target
agents goal set. Bagn |||Bbel1 ||| . . . |||Bbeln is the parallel composition of the interleaved behaviors of the target agent (represented by Bagn ) and the other n number of agents (i.e.,
Bbeli represents the ith agents believed behavior).
Definition 2 (Configuration). A configuration is a tuple (V, B), where V is a valuation of
the variables in Var , and B is a behavior.
We define the operational semantics of each behavioral element using firing rules, which
operate over configurations. Below,  denotes the set of all (visible) agent actions, and
 denotes an internal (invisible) action. We use  to denote any action in . We write
V |= expression to denote that the logical expression expression evaluates to true for valuation
480

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

V . We start from the operational semantics of cont, which causes the current behavior B
to continue from its initial location without making any changes to the valuation V . This
case is handled by the invisible  action, as it is defined by the rule [cont].
cont is encountered in B


(V, cont) 
 (V, B)

[cont]

Rule [prefix] captures how an action  of a behavior B modifies the values of the variables
in a valuation V .



(V, {assign, . . . , assign} -> B) 
 (update(V, {assign, . . . , assign}, C), B)

[prefix]

For readability, we use the auxiliary update function to handle assignment of new values
to variables when modifying a configuration. We present this function in Algorithm 1. In
Algorithm 1 (and also in the other related algorithms) we slightly abuse our notation for
readability as follows. We assume that each assignment assign is represented as a variable
and value pair (i.e., (variable, value)). We use the bracket notation V [variable] to access to
variable in valuation V . The internal state (c-id-state) and subscription (c-id-subscription)
variables are available to the algorithm for each commitment with identifier c-id. We also
omit the observer lists of commitments, since they are irrelevant to assignments.
Input arguments of the update function are the current valuation V , the set of assignments assignments that occur as the result of the action , and the commitment protocol
C. The function first updates the global and local variables of V according to assignments
(lines 12). Then, for each commitment c in the protocol, the function updates the state of c
(i.e., c-id) with respect to the updated values of the variables if necessary as follows (lines 3
22). If the commitment is conditional and the expiration condition of the commitment holds
in the updated valuation, then the commitment expires and its subscription counter is updated by the auxiliary function update-subscription, which we present in Algorithm 2, and
explain in detail later (lines 57). Otherwise, if the antecedent of the conditional commitment holds in the updated valuation, the commitment becomes active (lines 89). If neither
of these conditions hold, the state of the conditional commitment does not change, which
we do not show in Algorithm 1 for brevity.
If the commitment is active, it is necessary to consider its type (i.e., achievement or maintenance) to determine its next state. If the termination condition of an active achievement
commitment holds in the updated valuation, the commitment becomes violated (lines 12
14). Otherwise, if the consequent of the commitment holds in the updated valuation, the
commitment becomes fulfilled (lines 1516). If neither of these conditions hold, the state of
the active achievement commitment does not change. If the consequent of an active maintenance commitment does not hold in the updated valuation, the commitment becomes
violated (lines 1820). Otherwise, if the termination condition of the commitment holds in
the updated valuation, the commitment becomes fulfilled (lines 2122). If neither of these
conditions hold, the state of the maintenance commitment does not change. Finally, update
returns the updated valuation (line 23).
Subscription and fulfillment status of a commitment are handled by the auxiliary functions update-subscription and fulfillment as we define in Algorithms 2, and 3, respectively.
481

fiGunay, Liu & Zhang

Algorithm 1: Function update(V, assignments, C) returns the updated valuation V .
input : valuation V , set of assignments assignments, commitment protocol C
output: updated valuation V
1
2
3
4
5
6
7
8
9
10
11
12
13
14

foreach (variable, value) in assignments do
V [variable]  value
foreach commitment(c-id, c-type, subs, deb, cre, ant, exp, con, ter)[c-id] c in C do
if V [c-id-state] = conditional then
if V |= exp then
V [c-id-state]  expired
V  update-subscription(V, c)
else if V |= ant then
V [c-id-state]  active
else if V [c-id-state] = active then
if c-type = achievement then
if V |= ter then
V [c-id-state]  violated
V  update-subscription(V, c)
else if V |= con then
V  fulfillment(V, c, C)

15
16
17
18
19
20

else if c-type = maintenance then
if V 6|= con then
V [c-id-state]  violated
V  update-subscription(V, c)
else if V |= ter then
V  fulfillment(V, c, C)

21
22

23

return V

When a commitment reaches to a terminal state (e.g., fulfilled) the internal subscription
counter of the commitment should be updated. If the commitment is not a subscription
(i.e., it is not a template commitment and enacted only once), this update has no effect.
Otherwise, the update occurs as it is defined in Algorithm 2, which takes the current valuation V and the commitment c with the identifier c-id as input arguments. First, the internal
subscription counter (i.e., c-id-subscription) of the commitment is increased by one (line 1).
Then, if the incremented subscription counter is less than or equal to the total subscription period subs (i.e., some periods of the subscription have not been considered yet), the
commitment that corresponds to the next period of the subscription becomes conditional
(lines 2-4). Finally, the updated valuation is returned. Remember that the subscription
counter of any commitment is initially set by ProMoca to 1. Hence, the condition at
line 2 should include equal to case. Also note that a non-subscription commitment has a
482

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

Algorithm 2: Function update-subscription(V, c) returns the updated valuation V .
input : valuation V , commitment c
output: updated valuation V

4

V [c-id-subscription]  V [c-id-subscription] + 1
if V [c-id-subscription]  subs then
s-id-state  concatenate (c-id, c-id-subscription)
V [s-id-state]  conditional

5

return V

1
2
3

default period limit of 1. Hence, the condition at line 2 always fails for non-subscription
commitments as a result of increasing the subscription counter by one at line 1.
Algorithm 3 defines the auxiliary fulfillment function, which takes the current valuation
V , the fulfilled commitment c with the identifier c-id, and the commitment protocol C as
input arguments. The function first sets the state of the commitment (i.e., c-id-state) to
fulfilled (line 1). Then, it updates the subscription counter of the commitment (line 2).
Lastly, if the fulfilled commitment c is the compensation of another commitment c0 in
the context of C, which is determined by the auxiliary function compensates, and the
commitment c0 is currently violated, the state of c0 is set to compensated (lines 35). Note
that, if c is a subscription, it compensates c0 only after all the periods of c are fulfilled (i.e.,
condition subs < V [c-id-subscription] holds).
Algorithm 3: Function fulfillment(V, c, C) updates the valuation V with respect to
fulfillment of commitment c.
input : valuation V , commitment c, commitment protocol C
output: updated valuation V

5

V [c-id-state]  fulfilled
V  update-subscription(V, c)
c0  compensates(c, C)
if c0 6= none  V [c-id-state] = violated  subs < V [c-id-subscription] then
V [c-id-state]  compensated

6

return V

1
2
3
4

Note that, all these algorithms can be encoded trivially as a set of firing rules, which
are similar to the ones that we use to define operational semantics of the other ProMoca
elements. However, we prefer our algorithmic representation for readability. In practice,
these algorithms can be implemented efficiently using index structures from the variables
of a valuation to commitments that include an index variable in their conditions. However,
we prefer to explain them in the presented (less efficient but more intuitive) iterative form
for clarity.
We continue with the semantics of the meta-operations to create, release and cancel a
commitment, which are shown in rules [commit], [release] and [cancel], respectively. Here we
use auxiliary updatecommit , updaterelease and updatecancel functions to handle change in the
483

fiGunay, Liu & Zhang

valuation of the corresponding commitments state variable (as we do for assignments). For
instance, if [commit] fires for a commitment with the identifier commitment-id, updatecommit
function assigns the value conditional to the variable commitment-id-state, sets the corresponding subscription counter to 1, and returns the updated valuation V .
V |= commitment-id-state == null


(V, commit{commitment-id} -> B) 
 (updatecommit (V, commitment-id), B)
V |= commitment-id-state == conditional or commitment-id-state == active


(V, release{commitment-id} -> B) 
 (updaterelease (V, commitment-id), B)
V |= commitment-id-state == active


(V, cancel{commitment-id} -> B) 
 (updatecancel (V, commitment-id), B)

[commit]

[release]

[cancel]

These three rules do not define how a configuration changes when the condition of a rule
does not hold. For instance, [cancel] rule does not define how the configuration changes,
if the commitment is canceled when it is not active. To handle these situations we define
the following three rules, which ignore a commitment operation and do not change the
current configuration, if the condition of the rule does not hold. For instance, in [!cancel],
if a commitment that is not active, is canceled, the cancel operation is ignored and the
commitments current state is preserved (i.e., the current configuration does not change).
V |= commitment-id-state ! = null


(V, commit{commitment-id} -> B) 
 (V, B)
V |= commitment-id-state ! = conditional and commitment-id-state ! = active


(V, release{commitment-id} -> B) 
 (V, B)
V |= commitment-id-state ! = active


(V, cancel{commitment-id} -> B) 
 (V, B)

[!commit]

[!release]

[!cancel]

Rules [non-1] and [non-2] capture the semantics of non-deterministic choice between the
behaviors Bi and Bj . If there is a non-deterministic choice between Bi and Bj , either Bi or
Bj is selected in a non-deterministic manner for progressing according to the rules [non-1]
or [non-2], respectively.


[non-1]



[non-2]

(V, Bi <> Bj ) 
 (V, Bi )
(V, Bi <> Bj ) 
 (V, Bj )

Rule [guard] captures the semantics of guarded behaviors. If the expression guard that
is associated to the behavior B is evaluated to true according to the valuation V , then
the action  occurs and the model progresses to the configuration (V 0 , B 0 ). Otherwise,
484

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

B becomes blocked until the model progresses to another configuration, in which guard
evaluates to true. This may happen, if an interleaving behavior assigns new values to some
variables, which causes the model to progress to a new configuration, in which guard is
evaluated to true. However, if there is no such interleaving behavior, the current behavior
is permanently blocked (i.e., deadlocked).


V |= guard and (V, B) 
 (V 0 , B 0 )


(V, [guard]B) 
 (V 0 , B 0 )

[guard]

Lastly, we define operational semantics of interleaving behaviors parallel composition,
which is denoted by |||. Remember that ProMoca models both the target agents own
behavior and the target agents beliefs about the other agents behaviors in separate interleaving behaviors. We compose these behaviors into a single behavior to verify properties
of the target agents behavior as we explain in Section 4. We achieve this using rules [int-1]
and [int-2]. Intuitively, if there are two interleaving behaviors Bi and Bj (e.g., the target
agents own behavior and a believed behavior of another agent), either Bi or Bj is executed
independently from the other behavior.


(V, Bi ) 
 (V 0 , Bi0 )


(V, Bi ||| Bj ) 
 (V 0 , Bi0 ||| Bj )

[int-1]



(V, Bj ) 
 (V 0 , Bj0 )


(V, Bi ||| Bj ) 
 (V 0 , Bi ||| Bj0 )

[int-2]

To clarify parallel composition of two interleaving behaviors, we present a simple example in Figure 2, which shows possible executions of the two interleaving behaviors
Bi = {ia1{vi = 1} -> ia2{vi = 2}} and Bj = {ja1{vj = 1} -> ja2{vj = 2}} with
respect to the initial valuation V = ((vi : 0), (vj : 0)) for the variables vi and vj. Note
that we omit stop statements at the end of the behaviors for brevity. Each box in Figure 2
corresponds to a composed configuration such that the first line is the valuation V and the
rest is the interleaving behaviors Bi and Bj . Edges between boxes show transitions between
configurations according to actions taken by the behaviors, which are shown as edge labels.
Note that once an action is taken, it is removed from the corresponding behavior to indicate
its completion. For instance, there are two possible actions that can be taken in the initial
configuration, namely ia1 in Bi and ja1 in Bj . If ia1 is taken, the value of vi is set to 1,
and Bi becomes {ia2{vi = 2}} in the new configuration that we reach as the result of ia1.
In the final configuration, the valuation V is ((vi : 2), (vj : 2)), and both behaviors Bi
and Bj are empty, hence they stop (i.e., all possible actions are taken).
Our firing rules define how a behavior progresses from one configuration to another in
a ProMoca model. Now we define execution of a ProMoca model with respect to these
rules as a Markov Decision Process (MDP) (Bellman, 1957), which is expressive enough to
capture both probabilistic and non-deterministic interleaving behaviors.
Definition 3 (Markov Decision Process). A Markov decision process is a tuple M =
(S, Act, P, init , AP, L) where
 S is a finite set of states,
485

fiGunay, Liu & Zhang

((vi : 0), (vj : 0)),
(Bi = {ia1{vi = 1} -> ia2{vi = 2}}
|||
Bj = {ja1{vj = 1} -> ja2{vj = 2}})
ia1{vi = 1}

ja1{vj = 1}

((vi : 1), (vj : 0)),
(Bi = {ia2{vi = 2}}
|||
Bj = {ja1{vj = 1} -> ja2{vj = 2}})
ia2{vi = 2}

((vi : 0), (vj : 1)),
(Bi = {ia1{vi = 1} -> ia2{vi = 2}}
|||
Bj = {ja2{vj = 2}})
ja1{vj = 1}

((vi : 2), (vj : 0)),
(Bi = {}
|||
Bj = {ja1{vj = 1} -> ja2{vj = 2}})
ja1{vj = 1}

ia1{vi = 1}

((vi : 1), (vj : 1)),
(Bi = {ia2{vi = 2}}
|||
Bj = {ja2{vj = 2}})
ia2{vi = 2}

ja2{vj = 2}
((vi : 0), (vj : 2)),
(Bi = {ia1{vi = 1} -> ia2{vi = 2}}
|||
Bj = {})

ja2{vj = 2}

ia1{vi = 1}

((vi : 2), (vj : 1)),
((vi : 2), (vj : 2)),
((vi : 1), (vj : 2)),
(Bi = {}
(Bi = {}
ja2{vj = 2}
ia2{vi = 2} (Bi = ia2{vi = 2}}
|||
|||
|||
Bj = {ja2{vj = 2}})
Bj = {})
Bj = {})

Figure 2: Possible executions of the interleaving behaviors Bi and Bj .
 Act is a finite set of actions,
 P: S  Act  S 7 [0, 1] is a transition probability function such that for every state s
in S and for every action  in Act:
X
P(s, , s0 )  {0, 1}
s0 S

 init : S 7 1 is the initial distribution such that

P

sS init (s)

= 1,

 AP is a finite set of atomic propositions,
 L : S 7 2AP is a labeling function.
Without loss of generality we consider only finite MDPs (i.e., S, Act and AP are finite
sets), and there are no terminal states in an MDP. Precisely, if Act(s) denotes the set of
enabled actions in s, then for any sP S it is the case that Act(s) 6= . An action  is
enabled in a state s if and only if s0 S P(s, , s0 ) = 1. A state s0 is an -successor of
another state s, if P(s, , s0 ) > 0. An MDP executes as follows. An initial state sinit is
selected according to a stochastic experiment of the initial distribution init . In any state s,
first a non-deterministic choice is made between the enabled actions. Assume that action
  Act(s) is selected. Then, an -successor of s is selected randomly according to the
distribution P(s, , ).
486

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

Now we are ready to define the two necessary rules to systematically create an MDP
from a ProMoca model using our operational semantics. Rule [non-prob] captures non
probabilistic cases. That is, for any non-probabilistic firing rule (V, B) 
 (V 0 , B 0 ), if 
is performed from a configuration (V, B), the result is a single distribution  that maps
configuration (V 0 , B 0 ) to 1.


(V, B) 
 (V 0 , B 0 ) is a firing rule


(V, B) 
  where ((V 0 , B 0 )) = 1

[non-prob]

Rule [prob] handles probabilistic choices, where the resulting distribution  associates
probability di with (V, Bi ) for all i. Note that V remains unmodified during a  transition.


(V, (p1 )B1 . . . (pn )Bn ) 
  where ((V, Bi )) = pi for all i

[prob]

Proposition 1. For any ProMoca model O = (Var , Vinit , C, G, Bagn |||Bbel1 ||| . . . |||Bbeln )
there exists a corresponding MDP M = (S, Act, P, init , AP, L).
Proof: For any given ProMoca model O, the following procedure constructs a corresponding MDP M. Below, we use Dom(var) to denote the domain of a variable var  Var and
B to denote Bagn |||Bbel1 ||| . . . |||Bbeln .
1. Creation of AP : For every value val  Dom(var) of every variable var  Var add a
new proposition ap to AP . For instance, if the variable engine-status is an element of
V ar with the domain Dom(engine-status) = {unknown, operational, malfunction,
maintenance}, add propositions engine-status-unknown, engine-status-operational,
engine-status-malfunction, and engine-status-maintenance to AP , which correspond
to the values unknown, operational, malfunction, maintenance of engine-status,
respectively.
2. Creation of S and L: Add a state s for every combination of the propositions in
AP omitting combinations which include mutually exclusive propositions. A set of
propositions are mutually exclusive (i.e., they cannot hold in the same state), if they
are created from the domain of the same variable in Step 1. For instance, all the four
propositions that are created from the domain of engine-status are mutually exclusive.
Update the labeling L for each created state using the corresponding combination
of propositions. For instance, considering the propositions in the first step, we create four states s0 , s1 , s2 , and s3 , and set the labeling function as follows: L(s0 ) =
{engine-status-unknown}, L(s1 ) = {engine-status-operational }, L(s2 ) = {enginestatus-malfunction}, and L(s3 ) = {engine-status-maintenance}. Note that there is
a one-to-one correspondence between labels of states and valuations of a behaviors
configurations. As a result, there is also a direct correspondence between states and
configurations of a behavior.
3. Creation of init : Set the init such that the probability of the state, which has the
labeling that corresponds to the valuation Vinit , is 1, and probabilities of all other
states are 0.
487

fiGunay, Liu & Zhang

4. Creation of Act: For every assignment  in behavior B add a corresponding action
to Act. Besides, for every commitment c  C add a set of meta-actions to Act, which
correspond to the state changes of c.
5. Creation of P: First, for each state s  S determine the set of enabled actions
E
ActE
s  Act. An action act is enabled in a state s (i.e., act  Acts ) only if one of the
following conditions hold:
 act corresponds to an assignment  that occurs from the configuration conf ,
such that the valuation V of conf corresponds to the labeling of the state s
(i.e., L(s)). Note that the correspondence between a state and a configuration is
already described in Step 2, and the correspondence between an action and an
assignment is already described in Step 3. Intuitively, this condition captures the
[prefix] rule. That is, if the next behavior element to apply in a configuration is
an assignment, the action that corresponds to the assignment is enabled in the
state that corresponds to the configuration.
 act corresponds to a meta-action to change the state of a commitment, and
act can be performed (according to the algorithms and rules which define the
semantics of commitments) in the configuration conf , which corresponds to the
to the labeling of the state s (i.e., L(s)). This condition captures the state
changes of commitments.
Then, for each state s, if ActE
s includes one or more actions to change the state(s) of
some commitment(s), a single transition with probability 1 is set to the destination
state that labels the updated commitment states according to the algorithms and rules
which define the semantics of commitments. The rest of the actions are ignored, if
ActE
s includes one or more actions to change the state(s) of some commitment(s). In
other words, commitment states are updated instantaneously when needed, before any
further agent action occurs. Otherwise, if there are no actions to update
Pcommitment
i=N
states, a transition for each action act  ActE
with
probability
p
/
act
s
i=0 pi is set
for the destination states that are labeled according to the correspondences between
the actions and the assignments, where pi is the probability value of the action acti
as it is defined in the configuration conf that corresponds to the state s, and N is the
total number of actions in ActE
s .
Note that the procedure that we describe in the proof of Proposition 1 creates a finite
MDP, since the number of variables (and their domains), and the number of assignments in
a ProMoca model are finite. Also note that the actual computational complexity of this
procedure occurs due to the last step, where we determine the transition probabilities of
the MDP, which can be done in linear time with respect to the number of generated states
in Step 2 and actions in Step 4. However, it is clear that the number of states of an MDP is
exponential to the number of variables and their domains in the corresponding ProMoca
model (see Step 1). This situation is commonly known as the state space explosion problem
and constitutes the theoretical lower bound of the probabilistic model checking (Baier &
Katoen, 2008).
488

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

3.3 Formal Properties
In this section we formalize the compliance and goal satisfaction properties using PLTL
(Baier & Katoen, 2008), which is a probabilistic variant of Linear Temporal Logic (LTL)
(Pnueli, 1977). An LTL formula  is defined over a set of atomic propositions AP using
the temporal operators globally (G), eventually (F), and until (U). Satisfaction of an LTL
formula is defined with respect to an infinite sequence of states  = s0 , s1 , . . . as follows.
 G  holds in si , if  holds in all future states sj for i  j.
 F  holds in si , if  holds at least in one future state sj such that i  j.
  U 0 holds in si , if  holds in all future states sj until 0 holds in a future sk , such
that i  j < k.
PLTL extends LTL with a probabilistic operator Px (), where x  R and  is an
LTL formula. Intuitively, Px () holds for an MDP model, if probability of satisfying  is
greater or equal to x. We define computation of this probability in detail in Section 4. In
Sections 3.3.1 and 3.3.2, we use , ,  and  in PLTL formulae for logical conjunction,
disjunction, implication, and negation, respectively.
3.3.1 Compliance
Compliance of a target agents behavior with a commitment protocol is the first property
that we particularly aim to verify in ProMoca. Basically, if the target agent fulfills all
of its active commitments in a commitment protocol, then the target agents behavior
complies with the commitment protocol. However, due to interdependence among agents
and uncertainty about other agents behaviors, it is usually not possible to determine an
agents compliance exactly. For instance, in the aerospace example, the manufacturers
compliance depends on the behaviors of the suppliers. Hence, we define compliance of a
target agents behavior with a commitment protocol in a relaxed manner with respect to a
threshold C that defines the minimal acceptable ratio of fulfillment for the target agents
active commitments. Precisely, for each commitment of the target agent, if a commitment
becomes active, the commitments fulfillment probability should be greater or equal to the
threshold C . Below, B is Bagn |||Bbel1 ||| . . . |||Bbeln .
Definition 4. Given a ProMoca model (V ar, Vinit , C, G, B) and the corresponding MDP
(S, Act, P, init , AP, L), the target agents behavior Bagn complies with the commitment
protocol C with respect to the threshold C , if the following PLTL formula holds:
 commitment(c-id, debtor, . . . )  C such that debtor = agn :
PC (G(c-id-state-active  F(c-id-state-fulfilled  c-id-state-released)))
Intuitively, for every commitment of a protocol, in which the target agent is the debtor,
if the commitment becomes active at any given state of the MDP, then the commitments
probability of finally becoming fulfilled or released in a future state should be greater or
489

fiGunay, Liu & Zhang

equal to the threshold C . Note that in Definition 4, we omit commitment parameters (i.e.,
we instead use . . .) that are irrelevant to compliance.
Even tough the threshold C relaxes the requirements of compliance, Definition 4 is
still a strict notion of compliance for some domains, since it requires fulfillment of every
active commitment, and ignores compensations in the case of violation. However, as we
have discussed in Section 2, compensation is a useful and widely used mechanism in many
domains. Accordingly, we define weak compliance, which considers compensation, as follows:
Definition 5. Given a ProMoca model (V ar, Vinit , C, G, B) and the corresponding MDP
(S, Act, P, init , AP, L), the target agents behavior Bagn weakly complies with the commitment protocol C with respect to the threshold C , if the following PLTL formula holds:

 commitment(c-id, debtor, . . . )  C such that debtor = agn :
PC (G(c-id-state-active 
F(c-id-state-fulfilled  c-id-state-released  c-id-state-compensated)))
Intuitively, for every commitment of the protocol, in which the target agent is the debtor,
if the commitment becomes active at any given state of the MDP, then the commitments
probability of finally becoming fulfilled, released, or compensated in a future state should
be greater or equal to the threshold C .
Let us explain how compliance can be satisfied or failed on an example. Since computation of probabilities on an MDP is rather involved and not adequate for doing manually,
we use a simplified case from our aerospace aftercare example, where the manufacturer can
service an engine only if a supplier timely delivers the necessary parts for maintenance.
Suppose that the manufacturer believes that the supplier delivers the parts on time with
probability 0.9. Ignoring all other details, if the compliance threshold C is set to 0.8, the
verification process concludes that the manufacturer complies with the protocol, since the
probability of timely servicing the engine is above of the threshold. On the other hand,
if the compliance threshold C is set to 0.95, the verification process concludes that the
manufacturer fails to comply with the protocol, since the probability of timely servicing the
engine is below of the threshold.
3.3.2 Goal Satisfaction
The second property that we aim to analyze in ProMoca is goal satisfaction, which defines
whether a target agent can satisfy its goals by enacting a commitment protocol. However,
as in the case of compliance, it is usually not possible to exactly determine goal satisfaction
due to interdependence among agents and uncertainty in a multiagent system. Hence, as
before, we define a threshold S that defines the acceptable ratio of satisfaction for the goals
of a target agent.
Definition 6. Given a ProMoca model (V ar, Vinit , C, G, B) and the corresponding MDP
(S, Act, P, init , AP, L), the target agent can satisfy its goals in G by enacting the commitment protocol C with respect to the threshold S , if for each goal in G, the PLTL formula
that corresponds to the goals type holds as below:
490

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

 A persistent achievement goal can be satisfied, if the probability of achieving the
satisfaction condition sat infinitely often is greater or equal to S .
 pagoal(sat)  G : PS GF(sat)

[pa-sat]

 A persistent maintenance goal can be satisfied, if the probability of maintaining the
satisfaction condition sat in every reachable state is greater or equal to S .
 pmgoal(sat)  G : PS G(sat)

[pm-sat]

 A one-time achievement goal for which the precondition pre holds, can be satisfied if
probability of not reaching the termination condition ter until the satisfaction condition sat is achieved, is greater or equal to S . Intuitively, the goal should not be
terminated until it is achieved.
 agoal(pre, sat, ter)  G : PS G(pre  ( ter U sat))

[a-sat]

 A one-time maintenance goal for which the precondition pre holds, can be satisfied if
probability of maintaining the satisfaction condition sat until reaching the termination
condition ter is greater or equal to S . Intuitively, the satisfaction condition should
be maintained until the goals termination.
 mgoal(pre, sat, ter)  G : PS G(pre  (sat U ter))

[m-sat]

Let us explain how goal satisfaction can be satisfied or failed on a simplified case from
our NetBill example, where the merchant has a persistent achievement goal to receive
payments from customers. Suppose that the merchant believes that there is a customer
who purchases a certain good every day with probability 0.5. Ignoring all other details, if
the goal satisfaction threshold S is set to 0.25, the verification process concludes that the
merchant satisfies its goal, since the probability of receiving a payment on a daily basis is
higher than the threshold. On the other hand, if the goal satisfaction threshold C is set to
0.75, the verification process concludes that the merchant fails to satisfy its goal, since the
probability of receiving a payment on a daily basis is lower than the threshold.
3.4 Remarks about PROMOCA Syntax and Semantics
While designing ProMocas modeling language we are influenced from process algebra and
communicating sequential processes (Hoare, 1978), which are successfully used for modeling
various concurrent systems. However, in line with our objective of verifying properties of an
agents behavior in the context of a commitment protocol under uncertainty, we designed
a novel language to model commitment protocols and agents, taking into account their
goals, beliefs, and behaviors. One of our main motivations while designing ProMoca is to
ease modeling of commitment protocols and agents by providing an intuitive language with
elements to model them, which differs ProMoca from existing analysis tools. We discuss
about ProMocas modeling related benefits comparing to existing tools in more detail in
Section 7. In the rest of this section we discuss some important aspects of ProMocas
modeling language.
491

fiGunay, Liu & Zhang

A ProMoca model involves three types of variables that capture global, local and
internal (commitment and subscription) state. This distinction among the variable types is
used for type checking in the syntactic level to restrict the use of different variable types in
certain language elements (e.g., commitments can include only global variables since they
are public, and a target agents beliefs about other agents behaviors cannot include its
local variables since they are private, etc.). On the other hand, in the semantic model of
ProMoca, all variables are interpreted as global variables. It is straightforward to see
that type checking of variables in the syntactic level is sufficient to correctly define and
verify any PLTL property, including compliance and goal satisfaction. Hence, it is safe to
consider all variables as global variables in the semantic level as long as they are checked in
the syntactic level.
As we discuss in ProMoca semantics, states of a commitment are determined by
ProMoca according to evaluation of the commitments conditions (e.g., consequent).
Hence, ProMoca does not provide explicit meta-actions for most transitions (e.g., fulfillment). The exceptions are commit, release, and cancel. Initiation of a commitment is
handled by commit as it is done by all previous work. Furthermore, explicit release and
cancellation provides flexibility for modeling. For instance, many protocols involve some
commitments to regulate exceptional situations. These commitments normally do not become active unless the corresponding exception occurs. Hence, they stay in conditional state
even after the interaction of the involved parties has been completed. In such a situation,
the creditors of such commitments can release the debtors from their conditional commitments. Cancellation can be used by a debtor to immediately terminate a commitment (by
violating it) without waiting the occurrence of the commitments termination condition.
This is useful if the debtor realizes that it cannot fulfill its commitment. By canceling the
commitment, the debtor gives time to the creditor to recover from the undesirable situation
that occurs due to the violation of the commitment.
As a final remark on probabilistic modeling, note that probability computations in a
ProMoca model may become rather complex. As our examples demonstrate, in many
situations behaviors and belief may have arbitrary nesting and long sequences of actions,
which involve non-deterministic and probabilistic choices. Furthermore, parallel composition of interleaving behaviors introduces additional complexity. As a result scalability of
model checking may suffer when verifying compliance and goal satisfaction in large models.
This is a common problem of probabilistic model checking. We address this issue in detail in
the next two sections, and show that ProMoca can verify compliance and goal satisfaction
in many realistic situations, although it is affected from scalability issues.

4. Verification
In this section we present the overall verification process of ProMoca, which is depicted
in Figure 3. The inputs of the verification process are a ProMoca model, the type of
the property (i.e., compliance or goal satisfaction) that is aimed to be verified, and a real
value that corresponds to the threshold of the property (i.e., C or S ). First, we create
an MDP M from the input ProMoca model according to the operational semantics of
ProMoca as in Section 3. In parallel, we extract the PLTL property  (i.e., the property
type and relevant propositions of the ProMoca model), which we will verify, according to
492

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

input
PROMOCA model, property type, threshold
extract PLTL property

create corresponding MDP

PLTL formula 
MDP M
create corresponding Rabin automaton
Rabin automaton R

create product MDP from M and R
Product MDP M  R
apply probabilistic model checking algorithm



property fails

property holds

>

Figure 3: The overview of ProMocas verification process.

the input. Then, we create a (deterministic) Rabin automaton R from  using the standard
translation of a PLTL formula into a Rabin automaton (Baier & Katoen, 2008). Once we
have the MDP M and the Rabin automaton R, we create their product M  R, which
is also an MDP (Baier & Katoen, 2008). In the last step, we use our probabilistic model
checking algorithm that we present in Algorithm 4, on M  R to compute the satisfaction
probability of  in M. If the result of this computation is greater or equal to the input
threshold value, the verification process returns > to indicate that the property is satisfied.
On the other hand, if the computed value is below the threshold, the verification process
returns  to indicate that the property is not satisfied.
When a property is not satisfied in non-probabilistic model checking, a trace (i.e., counterexample) is generated to explain why the failure occurs as a result of model checking.
However, in probabilistic model checking there is no clear definition of such a counterexample since models are probabilistic, and properties are defined with respect to thresholds.
Although, recent research in probabilistic model checking has addressed this issue and several approaches have been proposed (Andres, DArgenio, & Rossum, 2009; Han, Katoen, &
Berteun, 2009; Schmalz, Varacca, & Volzer, 2009), ProMoca currently does not provide
any functionality to generate a counterexample.
ProMoca uses the reachability checking algorithm that we have developed in our previous work (Gunay et al., 2015). We present this algorithm in Algorithm 4 for completeness.
The inputs of Algorithm 4 are an MDP MR = (S, Act, P, init , AP, L), that is the product
of the MDP M and the Rabin automaton R, which are extracted from the input ProMoca
model, and T  S, the set of accept states of R as input. Algorithm 4 returns P R (T ), which
is the minimal probability of reaching the set of projected accept states T in the product
MDP. In Algorithm 4, we use the auxiliary function P re(s) that returns the pre-states of
a state s in an MDP. Formally, P re(s) = {s0 | P(s, , s0 ) > 0}. We use ps to record the
493

fiGunay, Liu & Zhang

Algorithm 4: Computation of reachability probabilities for target states.
input : M  R = (S, Act, P, init , AP, L), T  S
output: ps0
1
2
3
4
5
6
7
8
9
10
11
12

let S cur  T and S pre  ;
foreach s  S cur do
let ps  1;
while S cur 6=  do
foreach s  S cur do
S cur  S cur \{s};
foreach s0  P re(s) do
S pre  S pre  {s0 };
foreach (s0 , t, )  P r do
let pn = 0.0;
foreach s00  S such that (s00 ) > 0 do
pn  pn + (s00 )  ps00 ;
ps0  M in(ps0 , pn );

13
14
15
16

S cur  S pre ;
S pre  ;
return ps0 ;

probability of reaching T from s. Due to the existence of non-determinism in MDP, the
result of reachability checking is a range instead of a single value. In Algorithm 4 we adopt
a cautious approach and compute the minimal probability of reaching T .
The main idea of reachability checking is to start from target states and proceed backwards step by step while updating reachability probabilities of MDP states. Accordingly,
first T 0 is assigned to S cur (Line 1), which represents the current states in the iterative
process, and the probabilities of these states are set to 1 (Lines 2-3). Then, a state s is
removed from S cur (Lines 5-6) and for each pre-state s0 of s (Line 7) the probabilities are
updated as follows: for each enabled transition t from s0 with distribution , a variable pn
is created (Line 9-10) to record the probability of reaching T 0 from s0 via . Afterwards, the
sum of (s00 )  ps00 for all s00 satisfying (s00 ) > 0 is assigned to pn , i.e., pn is the sum of the
transition probabilities of this distribution times the corresponding successor states probability to T 0 (Lines 11-12). To keep the minimal probability, ps0 is set to the minimum value
of ps0 and pn using M in function (Line 13). When all states in S cur are considered, S cur
is set to pre-states of s (Line 14). The while loop at Line 4 terminates when no pre-states
left (i.e., the minimal probability from s0 to T is computed and stored in ps0 ). Finally, ps0
is returned as the probability of reaching T from s0 (Line 16).

5. Evaluation
We implement ProMoca framework in PAT (Process Analysis Toolkit) (Sun, Liu, Dong,
& Pang, 2009), which is a state-of-the-art extendable model checking framework that pro494

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

vides various probabilistic model checking techniques such as PLTL model checking and
reachability checking. To evaluate efficiency and scalability of ProMoca, we conducted a
set of computational experiments. In these experiments we compared ProMocas performance with PRISM (Kwiatkowska, Norman, & Parker, 2011), which is the most well-known
probabilistic model checker. We used the following four examples in our experiments:
 AeroBase: In this example we use our aerospace aftercare case. However, we assume
that the manufacturer always has sufficient supply of spare parts. Hence, we do not
consider the interactions between the manufacturer and suppliers. The example involves a commitment to model the purchase of an engine and a commitment to model
the overall maintenance agreement. Both of these commitments have also compensating commitments. Hence, there are four commitments to model the base terms of
the agreement. Besides, for the whole duration of the agreement, there are two commitments for each month to model the servicing responsibility of the manufacturer
(e.g., for a twelve month agreement there are 24 commitments). We consider this
scenario from the manufacturers perspective. The beliefs of the manufacturer involve
probability of engine failure and probability of repairing a failed engine on-time.
 AeroFull: In this example, we use our aerospace aftercare case, but we do not assume
that the manufacturer always has sufficient supply of spare parts. Hence, we consider
a new commitment for each month of the agreement to model provision of supplies by
the suppliers (e.g., there are 36 commitments for a twelve month agreement). Since,
the manufacturer interacts with the suppliers, the beliefs of the manufacturer involve
probability of successfully receiving spare parts from the suppliers.
 NetBill: In this example we model the well-known NetBill protocol (Sirbu & Tygar, 1995). NetBill provides a secure protocol for transactions between two parties
(e.g., a merchant and a customer) in online environments. It is a standard protocol
that is widely used in the commitment literature for evaluation. It involves three
commitments to model offer, payment, and delivery phases of a transaction. In our
experiments, we consider arbitrary number of transactions between a merchant and
a set of customers from the merchants perspective. The merchants beliefs involve
probability of the customers acceptance for the merchants offers.
 AGFIL: This example models a real world car insurance claim processing case (Desai
et al., 2009). AGF Irish Life Holdings (AGFIL), a subsidiary of Allianz, is an insurance
company in Ireland that underwrites car insurance policies. AGFIL cooperates with a
call center and a consulting firm to process its policy holders claims. A policy holder
contacts the call center to make a claim. The call center forwards the claim to AGFIL.
Then, AGFIL requests an investigation for the claim from the consultant to decide
on the claims validity. The consultant provides a report to inform AGFIL about the
results of its investigation. Using the investigation report, AGFIL decides on whether
to pay the claimed repair cost. This example includes three commitments to model the
interactions of AGFIL with the other three stakeholders, namely, the policy holder,
the call center, and the consultant. In our experiments, we consider arbitrary number
of claims from a set of policy holders and verify properties of AGFILs behavior.
Beliefs of AGFIL involve probability of claim validity.
495

fiGunay, Liu & Zhang

We run our experiments on a PC equipped with an Intel i7 3.0 GHz processor and 8GB
RAM, running an 64-bit Windows 8 operating system. We conducted our experiments to
verify both the compliance and goal satisfaction properties. However, in our experiments,
we observed similar results for both properties. Hence, for brevity, we report our results
only for the compliance propertys verification. In Table 1 we report execution times (in
seconds) of ProMoca and PRISM for verifying the compliance properties of the described
examples. If the model checking process takes more than 1200 seconds, we report timeout
(T/O). For each case, we reported average execution time of thirty runs to eliminate any
spike in execution times that may occur due to use of resources by other processes in our
system. We do not report variance of execution times since we observed negligible values.
To evaluate ProMocas scalability with respect to different complexities of our examples, we use a control parameter in each example that determines the size of the corresponding ProMoca model. In the AeroBase and AeroFull examples this parameter is the
number of months the engine manufacturer is committed to service an engine, which is
represented by mon in Table 1. In the NetBill example the control parameter is the number
of customers the merchant interacts, which is represented by cus in Table 1. In the AGFIL
example the control parameter is the number of claims made by policy holders, which is
represented by cla in Table 1.
Before discussing about our results in Table 1, let us explain the characteristics of the
example models and how the described parameters affect them. In the aerospace examples,
the outcome of a commitment that models a service agreement for a particular month,
depends on the outcomes of the previous months commitments. That is, if the manufacturer
fails to service the engine in a previous month, there is a higher risk of engine failure for
the current month, which may cause the manufacturer to violate its commitment to keep
the engine operational. This is modeled in the manufacturers beliefs. Accordingly, because
of such dependencies, the models of the aerospace examples become substantially more
complex when the duration of a service agreement (i.e., number of months mon) increases.
Furthermore, the AeroFull example considers also the behaviors of the suppliers as part
of the manufacturers beliefs, which increases the complexity of the corresponding models
even further. Hence, these examples capture complex realistic situations.
The models of NetBill and AGFIL examples are relatively simpler than the aerospace
examples. We use this situation to evaluate ProMoca for cases where more than one
commitment protocols are composed to have a more complex protocol. There are two
possible compositions, namely sequential and parallel. We used NetBill example to examine
parallel composition of commitment protocols. To this end, we increase the number of
customers a merchant interacts in parallel from 1 to 10. That is, if there are 10 customers,
the merchant enacts 10 instances of NetBill protocol in parallel. Parallel composition causes
a models size to grow exponentially. Hence, it has a significant impact on model complexity.
We used AGFIL example to examine sequential composition of independent protocols. To
this end, we increase the number of customer claims from 10 to 100. Each claim is processed
sequentially one by one (i.e., first come first served) by AGFIL. Impact of independent
sequential protocols on model complexity is substantially less than parallel composition of
protocols, since each protocol instance can be verified independently. Therefore, model size
growth is linear to the number of claims. Note that in Table 1, the first reported execution
time for AeroBase example is when mon = 6. When mon is less than 6 both ProMoca
496

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

AeroBase

AeroFull

NetBill

AGFIL

mon ProMoca PRISM mon ProMoca PRISM cus ProMoca PRISM cla ProMoca PRISM

6
7
8
9
10
11
12

0.42
1.33
3.76
9.81
27.86
84.49
242.37

1.32
7.35
33.68
175.81
T/O
T/O
T/O

3
4
5
6
7
8
9

0.03
0.16
0.75
3.62
17.01
85.56
365.82

0.10
0.27
3.80
27.68
591.36
T/O
T/O

4
5
6
7
8
9
10

0.01 
0.01
0.02
0.04
0.09
0.15
0.31

0.07
0.12
0.18
0.38
1.25
4.68
32.23

40
50
60
70
80
90
100

0.024
0.030
0.041
0.051
0.063
0.072
0.087

2.822
4.197
5.431
6.843
8.396
9.841
11.384

Table 1: Execution times of ProMoca and PRISM to verify compliance of the target agent
in AeroBase, AeroFull, NetBill, and AGFIL examples.

and PRISM can verify compliance almost instantaneously in a few milliseconds. Similarly,
we report our results in AeroFull, NetBill, and AGFIL examples for the control parameters
starting from mon = 3, cus = 4 and cla = 40, respectively.
Table 1 indicates two main results. First, ProMoca clearly outperforms PRISM in
terms of execution time for verifying an agents compliance with its commitments under
uncertainty. The main reason of this is the consideration of commitments as first-class objects in the syntax and semantics of ProMoca. Accordingly, ProMoca takes advantage
of this dedicated representation, and uses several model checking techniques, which we have
developed particularly for commitment protocols, for reducing model size to improve efficiency of model checking. For instance, ProMoca uses a specific partial order reduction
technique that exploits dependencies among commitments to reduce the size of an MDP
models state space (Gunay et al., 2015). Effects of this reduction technique is easy to see
especially in the NetBill example, where there are a large number of parallel independent
commitments in each NetBill protocol instance with a different customer. ProMoca effectively uses our dedicated partial order reduction technique to detect independence of
commitments, and reduces model size, which provides a clear advantage in the NetBill example. In conclusion, our results show that development of dedicated model checking tools
such as ProMoca is necessary for efficiently verifying an agents behavior with respect to
commitment protocols taking uncertainty into account.
Our second main result is usability of ProMoca in practical cases. In NetBill example
ProMoca verifies compliance in milliseconds when ten protocols are executed in parallel.
Similarly, in AGFIL example, ProMoca verifies the case, where 100 claims are considered,
almost immediately. In AeroBase example, ProMoca verifies compliance even for the
whole 12 month agreement within reasonable time. In the complete aerospace scenario,
ProMoca verifies nine month agreement successfully. Note that given sufficient time and
resources ProMoca can also verify a whole 12 month agreement. However, when a model
involves large number of dependent commitments, and complex behaviors and beliefs, as
our results show, ProMoca suffers from exponentially growing execution times, which is
the result of rapidly growing model size in such situations. This is a well known issue of
probabilistic model checking, which is unavoidable (Clarke, Grumberg, & Peled, 1999; Baier
497

fiGunay, Liu & Zhang

& Katoen, 2008). Nevertheless, our comparative results with PRISM show that ProMoca
is significantly more efficient than general purpose probabilistic model checkers for verifying
the compliance and goal satisfaction properties of an agent in a commitment protocol.

6. Related Work
In this section we provide a non-exhaustive survey of related research. We start with
previous work, which study various general properties of commitment protocols (i.e., independent from behaviors of enacting agents), and their verification. Yolum (2007) developed
a framework to verify effectiveness, consistency, recovery, and fault-tolerance properties of
commitment protocols. A commitment protocol is effective, if it is deadlock-free. It is consistent, if it does not involve conflicting propositions in commitment conditions. Finally, if
at least one role of a commitment protocol is capable of taking a certain recovery action
in the case of failure, the commitment protocol is recoverable. Besides, if any role is capable of recovering a commitment protocol, it is fault-tolerant. In addition to defining these
properties, Yolum also provides a set of algorithms for their verification, which are based
on analyzing states in arbitrary runs of commitment protocols. Desai et al. (2007a) also
study deadlock-free and live commitment protocols. They develop several models of commitment protocols in PROMELA language of SPIN model checker (Holzmann, 2004), and
define deadlock-freeness and liveness properties in LTL. Later, Telang and Singh (2012) use
NuSMV model checker and Computation Tree Logic (CTL) to verify correctness of business
patterns that are modeled as commitment protocols. Gerard and Singh (2013) introduce an
approach to specify commitment protocols and their refinements through guarded messages.
They implement their approach using MCMAS model checker.
Montali, Calvanese, and De Giacomo (2014) develop a data-aware framework using a
first-order formalism to study impact of data that is available to agents, on commitments
evolution. They also show that a rich set of temporal properties in -calculus can be verified
in their framework. El-Menshawy et al. (2013) develop ACTLC , which is an extension to
CTL, by introducing a set of operators to model semantics of active commitments. The
proposed semantics of the commitment operators are influenced by a previous proposal of
Singh (2008). The paper shows that the proposed logic can be reduced to another logic
GCTL*, which can be verified by CWB-NC model checker (Bhat, Cleaveland, & Groce,
2001). Later, El Kholy et al. (2014) propose another extension to CTL, which is called
CTLCC , to capture lifecycle of conditional commitments. They also extend the standard
symbolic model checking algorithm of CTL in line with their proposal. Sultan, Bentahar,
Wan, and Al-Saqqar (2014) propose PCTLC, which extends probabilistic CTL, to verify
commitment protocols. PCTLC includes social operators to represent active commitments
and their fulfillment. The proposed model checking technique consists of a set of reduction
rules to reduce the PCTLC model checking problem to PCTL model checking, which are
implemented on PRISM model checker.
Our proposal differs from these studies at several points. Firstly, none of these studies
consider development of a modeling language as we do in ProMoca. They either use modeling languages of general purpose model checkers or an abstract formalism for modeling.
Secondly, these studies do not consider behaviors of agents, since they aim to verify general
properties of commitment protocols. Finally, probabilistic models are not considered in the
498

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

previous work. The exception is the work by Sultan et al. (2014), which uses a probabilistic variant of CTL. However, Sultan et al. only consider active commitments. Besides,
due to the use of CTL, they consider a different model checking algorithm than we use in
ProMoca.
Some recent work aim to analyze a commitment protocol from an agents point of view.
Objectives of these studies are closer to the objective of ProMoca. Marengo et al. (2011)
discuss agents control on events in commitment protocols. Basically, an agent has control
over an event, if the agent itself can initiate the event, or another agent that is capable to
initiate the event is committed to do so. They also discuss a notion of safety. A commitment
is safe for its debtor, if the debtor controls the antecedent and can avoid situations in which
the commitment becomes active, or if the debtor controls the consequent and therefore
able to fulfill the commitment when it becomes active. Marengo et al. develop REGULA
framework to formalize control and safety properties, which also provides reasoning rules
to evaluate systems for these properties. However, they do not develop a practical reasoner
and also do not address uncertainty.
Gunay and Yolum (2013) study feasibility of a commitment protocol for an agent, considering time constraints and resources requirements that should be satisfied by the agent
to fulfill its commitments. A commitment protocol is feasible for an agent, if the agent owns
sufficient resources, or the agent is the creditor of a set of commitments that provide the
agent the resources to fulfill its own commitments on time. They, model verification of feasibility as a constraint satisfaction problem. Gunay and Yolum discuss potential behaviors
of agents and describe how unexpected behaviors (e.g., violations of commitments) may be
handled. However, their consideration of behaviors do not address probabilistic models and
requires manual configuration of their framework. Kafal, Gunay, and Yolum (2014) develop GOSU framework that provides a reasoning mechanism to determine goal support in
a commitment protocol, which is similar to our goal satisfaction property. They model goal
support as a reachability property and use reactive event calculus for reasoning (Chesani
et al., 2013). Their approach considers only achievement goals, and avoid uncertainty by
assuming that agents always honor their commitments.
Torroni and colleagues develop a monitoring framework for commitments using SCIFF
abductive logic programming proof-procedure (Alberti, Chesani, Gavanelli, Lamma, Mello,
& Torroni, 2008; Chesani et al., 2013). Their main motivation is to develop a formal and
operational framework that efficiently monitors commitments and verifies compliance of
agents actions with protocols that they enact. Since the main interest of this framework is
run-time monitoring, Torroni and colleagues pay special attention to a commitments time
constraints. Although such constraints have been studied also by other researchers, Torroni
and colleagues provide a concrete operational framework that can handle these constraints
at run-time. For this purpose they utilize an event driven implementation of event calculus
called reactive event calculus, which is based on the maximum validity interval concept
of cached event calculus introduced by Chittaro and Montanari (1996). Use of reactive
event calculus eliminates the necessity of backward reasoning at each event occurrence and
accordingly it is possible to do reasoning efficiently at run-time. Their monitoring framework
and ProMoca are complementary to each other. While ProMoca handles design-time
issues, their framework addresses run-time monitoring.
499

fiGunay, Liu & Zhang

Compliance is addressed also in the context of norms (e.g., prohibitions). Vasconcelos (2005) develops a declarative approach to analyze electronic institutions to determine
whether agents commit and fulfill norms of institutions. Aldewereld, Vazquez-Salceda,
Dignum, and Meyer (2006) develop a framework to verify norm compliance of agent behavior templates, which they call as protocols. They consider only sequential protocols and
define norm compliance using LTL. A semi-automated theorem-proving approach is used
for verification instead of model checking. Craven and Sergot (2008) develop nC+ as an
extension to the action language C+ (Giunchiglia, Lee, Lifschitz, McCain, & Turner, 2004).
nC+ introduces two new forms of rules, namely state and action permission laws, for representing normative aspects of multiagent systems. Semantics of their language is defined
with respect to colored labeled transition systems, which represent desired and undesired
states, and also transition of a modeled multiagent system. By associating a subset of transitions with a particular agents actions, they can verify compliance and other properties
of agent behaviors. nC+ provides a rich language to model multiagent systems. However,
Craven and Sergot do not consider uncertainty and accordingly they do not provide probabilistic modeling and reasoning. Besides, norms are not defined with respect to a lifecycle
as we do for commitments. Instead, norms are considered as (if-then) rules. Furthermore,
they do not represent relations between different norms explicitly as we do, for instance for
compensation. An interesting future direction is to investigate how these two formalism
can be combined to develop a more expressive modeling and analysis environment.
In terms of model checking, the most relevant work to ProMoca is MCMAS, which
is a state-of-the-art model checker dedicated to verification of multiagent systems (Lomuscio, Qu, & Raimondi, 2009). MCMAS uses Interpreted Systems Programming Language
(ISPL) for modeling, which is based on the interpreted systems semantics (Fagin, Halpern,
Moses, & Vardi, 2003). In ISPL, a multiagent system is modeled as a composition of a set
of agents and their environment. Each agent is defined as a set of internal states using a set
of private variables, and a protocol, which models the decision making mechanism of the
agent. Agents interact through publicly observable actions. Local states of agents evolve
according to an evolution function, which uses joint actions of agents. MCMAS supports
verification of agent-oriented logics, such as Alternating-time Temporal Logic (Alur, Henzinger, & Kupferman, 2002) and epistemic operators (Fagin et al., 2003), using Ordered
Binary Decision Diagrams (Bryant, 1986) and symbolic model checking techniques. There
are several differences between MCMAS and ProMoca. While MCMAS is a general model
checker for all types of multiagent systems, ProMoca focuses on verifying agent behaviors
in commitment protocols. Accordingly, ProMoca provides dedicated language elements
for defining commitment protocols. Moreover, ProMoca aims to verify an agents behavior
taking uncertainty of the agents beliefs into account. To achieve this, ProMoca provides
probabilistic modeling and reasoning capabilities for agent beliefs. ProMoca does not use
interpreted systems semantics, since we do not aim to verify epistemic logic specifications.
Finally, ProMoca uses an automata based approach instead of symbolic model checking,
which is more appropriate for verifying our properties.
In the recent years, commitments have been used to model various practical situations.
Desai, Chopra, Arrott, Specht, and Singh (2007b) provide a commitment-based solution
for formalizing foreign exchange market protocols. They show that rigorous specification
and verification of protocols via commitments solve many issues that emerge in existing
500

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

systems due their informal business semantics. Singh, Chopra, and Desai (2009) propose
a commitment-based service oriented architecture, which replaces invocation-based service
objects with engagement-based autonomous business services. A set of transaction patterns
over commitments are proposed, which reflect business requirements of various common
business transactions, and provide basic building blocks to develop complex interactions.
Benefits of this approach are flexible enactment of transactions, and ease of specification
and composition of business processes. Kafal, Gunay, and Yolum (2012, 2013) develop
a method using commitments to capture violation of privacy policies in social networks.
They model privacy policies between parties in social networks using commitments, and
employ model checking on these policies to capture violations. Their proposed approach
can capture privacy breaches that cannot be detected in traditional systems. Furthermore,
a prediction tool is also developed that utilizes Semantic Web technologies to capture potential privacy breaches that may occur in the future due to evolution of social network
relations. Telang, Kalia and, Singh (2012, 2015) conducted an experimental evaluation of
the commitment-based Comma methodology, and show that Comma outperforms the traditional HL7 Messaging Standard for healthcare process modeling. Chopra and Singh (2016b)
introduce Interaction-Oriented Software Engineering as a commitment-based paradigm to
capture social aspects of sociotechnical system emphasizing openness, autonomy, and accountability. Chopra and Singh (2016a) also develop Custard framework, which provides a
relational schema and queries for commitments and their lifecycle, to build an abstraction
layer over underlying information stores such as databases.

7. Discussion
In this paper we presented ProMoca framework for modeling and analyzing agent behaviors in commitment protocols taking uncertainty into account. Our main motivation for
developing ProMoca is to analyze a target agents behavioral properties when enacting a
commitment protocol with respect to its goals, and beliefs about other agents uncertain
behaviors. In the recent years, commitments are applied to solve practical challenges in
many domains such as e-commerce, sociotechnical systems, privacy, security, and healthcare. Modeling and verification are essential aspects of the development process for such
practical systems, mainly to ensure correctness and effectiveness of a systems. ProMoca
provides a novel analysis tool to handle these two key aspects of development by providing
an expressive modeling language and an efficient model checking algorithm. The modeling
language of ProMoca is expressive enough to model practical situations as we demonstrate in our examples. In fact, ProMoca provides more expressive power than needed by
most of the previous work on commitments. The model checking algorithm of ProMoca
is efficient and can handle complex situations as our empirical results show.
However, ProMoca has also certain limitations. In terms of modeling, ProMoca can
be extended to model a more wider range of practical cases by introducing new language
elements as we discuss in our future work at the end of this section. In terms of verification,
well-known scalability issues of probabilistic model checking applies also to ProMoca.
However, our results show ProMocas efficiency in many practical situations. Our comparison with the state-of-the-art general purpose probabilistic model checker PRISM also
shows that ProMoca outperforms PRISM when verifying an agents compliance and goal
501

fiGunay, Liu & Zhang

satisfaction in a commitment protocol. Finally, social commitments framework itself is not
an all-around solution to model every practical situation. However, it can be integrated with
other approaches in multiagent systems to model and reason about more complex practical situations, e.g., integration with artifacts (Baldoni, Baroglio, & Capuzzimati, 2015) and
other normative concepts such as prohibitions and authorizations (Chopra & Singh, 2016a).
As we stated earlier, there are various general purpose tools, such as PRISM and MCMAS, which can be used to verify agents with respect to commitment protocols. There
are also several reasoning methodologies to handle uncertainty (Eiter & Lukasiewicz, 2003;
Richardson & Domingos, 2006). Let us justify, why we develop a new tool while such tools
and methodologies already exist. There are mainly two motivations behind the development of ProMoca as a new tool. The first one is the ease of modeling. Since general
purpose tools do not support commitments, they do not provide any facilities to model
them. Therefore, to be able to use one of these tools for commitments, first a model of a
commitment should be developed in the tool. According to our experience, this is a nontrivial and error-prone task. Furthermore, such models are mostly developed considering
some specific properties that are aimed to be verified in a target system. Hence, reuse of
such models for other properties and systems is also limited. ProMoca solves these issues
by providing an expressive modeling language that includes various facilities to model commitments. Hence, users can easily model commitment protocols, without worrying about
the underlying model of commitments. Besides, ProMoca is based on a general model
of commitments that is independent from particular properties and application specific
assumptions. Hence, it can be used in any domain to verify arbitrary properties. Our
second motivation is efficiency. As our experimental results clearly demonstrate, the stateof-the-art probabilistic model checker PRISM cannot achieve efficiency of ProMoca while
verifying an agents compliance and goal satisfaction in a commitment protocol. The main
reason of ProMocas efficiency is the utilization of commitment semantics to efficiently
verify the addressed properties.
ProMoca can be used to model a wide majority of commitment protocols that are
considered in the previous work. However, it is still open for many improvements. A major
improvement is to extend ProMoca with an explicit representation of time. Currently,
time can be modeled in ProMoca in an abstract manner using regular variables as we
demonstrated in our examples. This is sufficient for many domains, however, especially
for modeling commitments and agent in real-time systems, an explicit notion of time is
necessary. Another improvement is introduction of numerical variables and arithmetic operations to ProMoca. Such variables are necessary to precisely model resource related
issues (e.g., money, number of available spare parts, etc.). Addition of these features is
fairly straightforward from syntactic and semantic point of view. However, verification of
time and numerical variables increase complexity of model checking substantially. Hence,
development of novel abstraction and reduction techniques that use commitment semantics,
is essential for efficient and scalable model checking of such models. ProMoca can also be
extended with more syntactic elements to simplify modeling of commitments. An example
is the use of parameters in commitment specifications for modeling generic commitments.
Beside the above improvements to ProMoca, we also aim to extend ProMoca to
model other commitment concepts such as choice and coordination (Baldoni, Baroglio,
Chopra, Desai, Patti, & Singh, 2009), and relevant properties such as feasibility (Gunay &
502

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

Yolum, 2013) and safety (Marengo et al., 2011). Besides, we plan to support other normative
concepts such as obligations and prohibitions, which are relevant to commitments (Boella
& van der Torre, 2004; Craven & Sergot, 2008; Agotnes, van der Hoek, & Wooldridge,
2010; Criado, Argente, & Botti, 2011). Last but not least integration of ProMoca with
the recent work on dynamic protocol creation in open systems is an interesting future work
(Yolum & Singh, 2007; Artikis, 2009; Meneguzzi, Telang, & Singh, 2013; Gunay, Winikoff,
& Yolum, 2013, 2015; Cranefield, Savarimuthu, Meneguzzi, & Oren, 2015). This research
aims to automate creation of protocols at run-time, which requires agents to agree on a
commitment protocol to regulate their interaction according to their own requirements. To
achieve this, individual agents should be able of analyze their behaviors with respect to
their requirements and a commitment protocol, where ProMoca can be a valuable tool.

Acknowledgments
We thank to the anonymous reviewers for their insightful comments. This work is supported
by Formal Verification on Cloud project under Grant No: M4081155.020 and Bring the
Advanced Model Checking Techniques to Real-world Problems project under Grant No:
M4011178.020.

References
Alberti, M., Chesani, F., Gavanelli, M., Lamma, E., Mello, P., & Torroni, P. (2008). Verifiable agent interaction in abductive logic programming: The SCIFF framework. ACM
Transactions on Computational Logic, 9 (4), 29:129:43.
Aldewereld, H., Vazquez-Salceda, J., Dignum, F., & Meyer, J.-J. C. (2006). Verifying norm
compliancy of protocols. In Agents, Norms and Institutions for Regulated Multi-Agent
Systems, pp. 231245.
Alur, R., Henzinger, T. A., & Kupferman, O. (2002). Alternating-time temporal logic.
Journal of the ACM, 49 (5), 672713.
Andres, M. E., DArgenio, P., & Rossum, P. (2009). Significant diagnostic counterexamples
in probabilistic model checking. In Proceedings of the 4th International Haifa Verification Conference on Hardware and Software: Verification and Testing, pp. 129148.
Springer-Verlag.
Artikis, A. (2009). Dynamic protocols for open agent systems. In Proceedings of the 8th
International Conference on Autonomous Agents and Multiagent Systems, pp. 97104.
Baier, C., & Katoen, J.-P. (2008). Principles of Model Checking. The MIT Press.
Baldoni, M., Baroglio, C., & Capuzzimati, F. (2015). Programming JADE and Jason agents
based on social relationships using a uniform approach. In Koch, F., Guttmann, C.,
& Busquets, D. (Eds.), Advances in Social Computing and Multiagent Systems, Vol.
541, pp. 167184. Springer.
Baldoni, M., Baroglio, C., Chopra, A. K., Desai, N., Patti, V., & Singh, M. P. (2009).
Choice, interoperability, and conformance in interaction protocols and service chore503

fiGunay, Liu & Zhang

ographies. In Proceedings of The 8th International Conference on Autonomous Agents
and Multiagent Systems, pp. 843850.
Bellman, R. (1957). Markovian decision processes. Journal of Mathematics and Mechanics,
38, 716719.
Bhat, G., Cleaveland, R., & Groce, A. (2001). Efficient model checking via Buchi tableau
automata. In Proceedings of the 13th International Conference on Computer Aided
Verification, pp. 3852.
Boella, G., & van der Torre, L. (2004). Regulative and constitutive norms in normative
multiagent systems. In Proceedings of 9th International Conference on the Principles
of Knowledge Representation and Reasoning, pp. 255265.
Bryant, R. E. (1986). Graph-based algorithms for boolean function manipulation. IEEE
Transactions on Compututers, 35 (8), 677691.
Chesani, F., Mello, P., Montali, M., & Torroni, P. (2013). Representing and monitoring
social commitments using the event calculus. Autonomous Agents and Multi-Agent
Systems, 27 (1), 85130.
Chittaro, L., & Montanari, A. (1996). Efficient temporal reasoning in the cached event
calculus. Computational Intelligence, 12 (3), 359382.
Chopra, A. K., Dalpiaz, F., Giorgini, P., & Mylopoulos, J. (2010). Reasoning about agents
and protocols via goals and commitments. In Proceedings of the Ninth International
Conference on Autonomous Agents and Multiagent Systems, pp. 457464.
Chopra, A. K., & Singh, M. P. (2015). Cupid: Commitments in relational algebra. In
Proceedings of 29th AAAI Conference on Artificial Intelligence, pp. 20522059.
Chopra, A. K., & Singh, M. P. (2016a). Custard: Computing norm states over information
stores. In Proceedings of the 2016 International Conference on Autonomous Agents
and Multiagent Systems, pp. 10961105.
Chopra, A. K., & Singh, M. P. (2016b). From social machines to social protocols: Software engineering foundations for sociotechnical systems. In Proceedings of the 25th
International Conference on World Wide Web, pp. 903914.
Clarke, Jr., E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press,
Cambridge, MA, USA.
Cranefield, S., Savarimuthu, T., Meneguzzi, F., & Oren, N. (2015). A bayesian approach
to norm identification. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 17431744.
Craven, R., & Sergot, M. (2008). Agent strands in the action language n C+. Journal of
Applied Logic, 6 (2), 172191.
Criado, N., Argente, E., & Botti, V. (2011). Open issues for normative multi-agent systems.
AI Communications, 24 (3), 233264.
Desai, N., Cheng, Z., Chopra, A. K., & Singh, M. P. (2007a). Toward verification of commitment protocols and their compositions. In Proceedings of the 6th International
Joint Conference on Autonomous Agents and Multiagent Systems, pp. 33:133:3.
504

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

Desai, N., Chopra, A. K., Arrott, M., Specht, B., & Singh, M. P. (2007b). Engineering foreign
exchange processes via commitment protocols. In IEEE International Conference on
Services Computing, pp. 514521.
Desai, N., Chopra, A. K., & Singh, M. P. (2009). Amoeba: A methodology for modeling
and evolving cross-organizational business processes. ACM Transactions on Software
Engineering and Methodology, 19 (2), 6:16:45.
Desai, N., Narendra, N. C., & Singh, M. P. (2008). Checking correctness of business contracts via commitments. In Proceedings of the 7th International Joint Conference on
Autonomous Agents and Multiagent Systems, pp. 787794.
Eiter, T., & Lukasiewicz, T. (2003). Probabilistic reasoning about actions in nonmonotonic causal theories. In Proceedings of the Nineteenth Conference on Uncertainty in
Artificial Intelligence, pp. 192199.
El Kholy, W., Bentahar, J., Menshawy, M. E., Qu, H., & Dssouli, R. (2014). Conditional
commitments: Reasoning and model checking. ACM Transactions on Software Engineering Methodology, 24 (2), 9:19:49.
El Menshawy, M., Bentahar, J., El Kholy, W., & Dssouli, R. (2013). Verifying conformance
of multi-agent commitment-based protocols. Expert Systems with Applications, 40,
122138.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (2003). Reasoning About Knowledge.
MIT Press, Cambridge, MA, USA.
Fornara, N., & Colombetti, M. (2002). Operational specification of a commitment-based
agent communication language. In Proceedings of the 1st International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 536542.
Gerard, S. N., & Singh, M. P. (2013). Formalizing and verifying protocol refinements. ACM
Transactions on Intelligent Syststems and Technology, 4 (2), 21:121:27.
Giunchiglia, E., Lee, J., Lifschitz, V., McCain, N., & Turner, H. (2004). Nonmonotonic
causal theories. Artificial Intelligence, 153 (1-2), 49104.
Gunay, A., Songzheng, S., Liu, Y., & Zhang, J. (2015). Automated analysis of commitment
protocols using probabilistic model checking. In Proceedings of 29th AAAI Conference
on Artificial Intelligence, pp. 20602066.
Gunay, A., Winikoff, M., & Yolum, P. (2013). Commitment protocol generation. In Declarative Agent Languages and Technologies X, Vol. 7784 of LNAI, pp. 136152. Springer.
Gunay, A., Winikoff, M., & Yolum, P. (2015). Dynamically generated commitment protocols
in open systems. Journal of Autonomous Agents and Multi-agent Systems, 29, 192
229.
Gunay, A., & Yolum, P. (2011). Detecting conflicts in commitments. In Sakama, C.,
Sardina, S., Vasconcelos, W., & Winikoff, M. (Eds.), Declarative Agent Languages
and Technologies IX, Vol. 7169 of LNAI, pp. 5166. Springer.
Gunay, A., & Yolum, P. (2013). Constraint satisfaction as a tool for modeling and checking
feasibility of multiagent commitments. Applied Intelligence, 39 (3), 489509.
505

fiGunay, Liu & Zhang

Halpern, J. Y. (2003). Reasoning About Uncertainty. MIT Press, Cambridge, MA, USA.
Han, T., Katoen, J.-P., & Berteun, D. (2009). Counterexample generation in probabilistic
model checking. IEEE Transactions on Software Engineering, 35 (2), 241257.
Hoare, C. A. R. (1978). Communicating sequential processes. Communications of ACM,
21 (8), 666677.
Holzmann, G. J. (Ed.). (2004). The SPIN Model Checker: Primer and Reference Manual.
Addison-Wesley.
Jakob, M., Pechoucek, M., Miles, S., & Luck, M. (2008). Case studies for contract-based
systems. In Proceedings of the 7th International Joint Conference on Autonomous
Agents and Multiagent Systems: Industrial Track, pp. 5562.
Kafal, O., Gunay, A., & Yolum, P. (2012). PROT OSS: A run time tool for detecting
PRivacy viOlaT ions in Online Social networkS. In IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pp. 429433.
Kafal, O., Gunay, A., & Yolum, P. (2013). Detecting and predicting privacy violations in
online social networks with PROT OSS. Distributed and Parallel Databases, 32 (1),
161190.
Kafal, O., Gunay, A., & Yolum, P. (2014). GOSU: Computing goal support with commitments in multiagent systems. In Proceedings of 21st European Conference on Artificial
Intelligence, pp. 477482.
Kafal, O., & Torroni, P. (2012). Exception diagnosis in multiagent contract executions.
Annals of Mathematics and Artificial Intelligence, 64 (1), 73107.
Kwiatkowska, M., Norman, G., & Parker, D. (2011). PRISM 4.0: Verification of probabilistic
real-time systems. In Proceedings of the 23rd International Conference on Computer
Aided Verification, pp. 585591.
Lomuscio, A., Qu, H., & Raimondi, F. (2009). MCMAS: A model checker for the verification of multi-agent systems. In Proceedings of the 21st International Conference on
Computer Aided Verification, pp. 682688.
Mallya, A. U., & Huhns, M. N. (2003). Commitments among agents. IEEE Internet
Computing, 7 (4), 9093.
Marengo, E., Baldoni, M., Baroglio, C., Chopra, A. K., Patti, V., & Singh, M. P. (2011).
Commitments with regulations: Reasoning about safety and control in REGULA.
In Proceedings of the Tenth International Conference on Autonomous Agents and
Multiagent Systems, pp. 467474.
Meneguzzi, F., Telang, P. R., & Singh, M. P. (2013). A first-order formalization of commitments and goals for planning. In Proceedings of the 27th AAAI Conference on
Artificial Intelligence, pp. 697703.
Modgil, S., Faci, N., Meneguzzi, F., Oren, N., Miles, S., & Luck, M. (2009). A framework for
monitoring agent-based normative systems. In Proceedings of The 8th International
Conference on Autonomous Agents and Multiagent Systems, pp. 153160.
506

fiProMoca: Probabilistic Modeling and Analysis of Agents in Commitment Protocols

Montali, M., Calvanese, D., & De Giacomo, G. (2014). Verification of data-aware
commitment-based multiagent system. In Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems, pp. 157164.
Pnueli, A. (1977). The temporal logic of programs. In Proceedings of the 18th Annual
Symposium on Foundations of Computer Science, pp. 4657.
Agotnes, T., van der Hoek, W., & Wooldridge, M. (2010). Robust normative systems and
a logic of norm compliance. Logic Journal of IGPL, 18 (1), 430.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine Learning, 62 (1-2),
107136.
Schmalz, M., Varacca, D., & Volzer, H. (2009). Counterexamples in probabilistic ltl model
checking for markov chains. In Proceedings of the 20th International Conference on
Concurrency Theory, pp. 587602. Springer-Verlag.
Singh, M. P. (1999). An ontology for commitments in multiagent systems: Toward a unification of normative concepts. Artificial Intelligence and Law, 7 (1), 97113.
Singh, M. P. (2008). Semantical considerations on dialectical and practical commitments.
In Proceedings of the 23rd National Conference on Artificial Intelligence, pp. 176181.
Singh, M. P., Chopra, A. K., & Desai, N. (2009). Commitment-based service-oriented
architecture. IEEE Computer, 42 (11), 7279.
Sirbu, M. A., & Tygar, J. D. (1995). NetBill: An internet commerce system optimized for
network delivered services. IEEE Personal Communications, 2 (4), 3439.
Sultan, K., Bentahar, J., Wan, W., & Al-Saqqar, F. (2014). Modeling and verifying probabilistic multi-agent systems using knowledge and social commitments. Expert Systems
and Applications, 41 (14), 62916304.
Sun, J., Liu, Y., Dong, J. S., & Pang, J. (2009). PAT: Towards flexible verification under
fairness. In Proceedings of the 21th International Conference on Computer Aided
Verification (CAV), Vol. 5643 of Lecture Notes in Computer Science, pp. 709714.
Springer.
Telang, P., & Singh, M. (2012). Specifying and verifying cross-organizational business
models: an agent-oriented approach. IEEE Transations on Services Computing, 5 (3),
305318.
Telang, P. R., Kalia, A. K., & Singh, M. P. (2015).
Modeling healthcare processes using commitments: An empirical evaluation.
PLoS ONE, 10 (11),
doi:10.1371/journal.pone.0141202.
Telang, P. R., & Singh, M. P. (2012). Comma: A commitment-based business modeling
methodology and its empirical evaluation. In Proceedings of the 11th International
Conference on Autonomous Agents and Multiagent Systems, pp. 10731080.
Torroni, P., Chesani, F., Mello, P., & Montali, M. (2010). Social commitments in time: Satisfied or compensated. In Proceedings of the 7th International Conference on Declarative
Agent Languages and Technologies, pp. 228243, Berlin, Heidelberg. Springer-Verlag.
507

fiGunay, Liu & Zhang

van Riemsdijk, M. B., Dastani, M., & Meyer, J.-J. C. (2009). Goals in conflict: Semantic
foundations of goals in agent programming. Autonomous Agents and Multi-Agent
Systems, 18 (3), 471500.
Vasconcelos, W. W. (2005). Norm verification and analysis of electronic institutions. In
Proceedings of the Second International Conference on Declarative Agent Languages
and Technologies, pp. 166182.
Yolum, P. (2007). Design time analysis of multiagent protocols. Data and Knowledge
Engineering, 63 (1), 137154.
Yolum, P., & Singh, M. P. (2002). Flexible protocol specification and execution: Applying
event calculus planning using commitments. In Proceedings of the 1st International
Joint Conference on Autonomous Agents and Multiagent Systems, pp. 527534.
Yolum, P., & Singh, M. P. (2007). Enacting protocols by commitment concession. In
Proceedings of the 6th International Joint Conference on Autonomous Agents and
Multiagent Systems, pp. 27:127:8.

508

fiJournal of Artificial Intelligence Research 57 (2016) 421-464

Submitted 06/16; published 11/16

Embarrassingly Parallel Search in Constraint Programming
Arnaud Malapert
Jean-Charles Regin
Mohamed Rezgui

arnaud.malapert@unice.fr
jean-charles.regin@unice.fr
rezgui@i3s.unice.fr

Universite Cote dAzur, CNRS, I3S, France

Abstract
We introduce an Embarrassingly Parallel Search (EPS) method for solving constraint
problems in parallel, and we show that this method matches or even outperforms state-ofthe-art algorithms on a number of problems using various computing infrastructures. EPS
is a simple method in which a master decomposes the problem into many disjoint subproblems which are then solved independently by workers. Our approach has three advantages:
it is an efficient method; it involves almost no communication or synchronization between
workers; and its implementation is made easy because the master and the workers rely on
an underlying constraint solver, but does not require to modify it. This paper describes
the method, and its applications to various constraint problems (satisfaction, enumeration,
optimization). We show that our method can be adapted to different underlying solvers
(Gecode, Choco2, OR-tools) on different computing infrastructures (multi-core, data centers, cloud computing). The experiments cover unsatisfiable, enumeration and optimization
problems, but do not cover first solution search because it makes the results hard to analyze. The same variability can be observed for optimization problems, but at a lesser
extent because the optimality proof is required. EPS offers good average performance, and
matches or outperforms other available parallel implementations of Gecode as well as some
solvers portfolios. Moreover, we perform an in-depth analysis of the various factors that
make this approach efficient as well as the anomalies that can occur. Last, we show that
the decomposition is a key component for efficiency and load balancing.

1. Introduction
In the second half of the 20th century, the frequency of processors doubled every 18 months
or so. It has now been clear for a few years that this period of free lunch, as put by Sutter
and Larus (2005), is behind us. As outlined by Bordeaux, Hamadi, and Samulowitz (2009),
the available computational power will keep increasing exponentially, but the increase will be
in terms of number of available processors, not in terms of frequency per unit. Multi-core
processors are now the norm which raises significant challenges for software development.
Data centers for high-performance computing are readily accessible by many in academia
and industry. Cloud computing (Amazon, Microsoft Azure, Google, . . . ) offers massive
infrastructures for rent on which computing and storage can be used on demand. With
such facilities anyone can now gain access to super-computing facilities at a moderate cost.
Distributed Computing offers possibilities to put computational resources in common
and effectively obtains massive capabilities. Examples include Seti@home (Anderson, Cobb,
Korpela, Lebofsky, & Werthimer, 2002), Distributed.net (Distributed Computing Technologies Inc, 20) and Sharcnet (Bauer, 2007). The main challenge is therefore to scale, i.e.,
to cope with this growth.
c
2016
AI Access Foundation. All rights reserved.

fiMalapert, Regin, & Rezgui

Constraint programming (CP) is an appealing technology for a variety of combinatorial
problems which has grown steadily in the last three decades. The strengths of CP are
the use of constraint propagation combined with efficient search algorithms. Constraint
propagation aims at removing combinations of values from variable domains which cannot
appear in any solution. Over a number of years, possible gains offered by the parallel
computing have attracted the attention.
Parallel computing is a form of computation in which many calculations are carried out
simultaneously (Almasi & Gottlieb, 1989) operating on the principle that large problems
can often be divided into smaller ones, which are then solved in parallel. Different forms
of parallel computing exist: bit-level, instruction level, data and task parallelism. Task
parallelism is a common approach for parallel branch-and-bound (B&B) algorithms (Mattson, Sanders, & Massingill, 2004) and is achieved when each processor executes a different
thread (or process) on same or different data. Parallel computer programs are more difficult to write than sequential ones, because concurrency introduces several new classes of
potential software bugs, of which race conditions are the most common. For example, when
memory is shared, several tasks of an algorithm can modify the same data at the same
time. This could render the program incorrect. Mutual exclusion allows a worker to lock
certain resources to obtain exclusive access, but can create starvation because the other
workers must wait until the worker frees the resources. Moreover, the indeterminism of the
parallel programs makes the behaviour of the execution unpredictable, i.e. the results of
different program runs may differ. So, communication and synchronization among different
sub-tasks can address this issue, but are typically some of the greatest obstacles to good
performance. Another central bottleneck is load balancing, i.e. keeping all processors busy
as much as possible.
Wilkinson and Allen (2005) introduced the Embarrassingly Parallel paradigm which assumes that a computation can be divided into a number of completely independent parts
and that each part can be executed by a separate processor. In this paper, we introduce
an Embarrassingly Parallel Search (EPS) method for constraint problems and show that
this method often outperforms state-of-the-art parallel B&B algorithms for a number of
problems on various computing infrastructures. A master decomposes the problem into
many disjoint subproblems which are then solved independently by workers. Since a constraint program is not trivially embarrassingly parallel, the decomposition procedure must
be carefully designed. Our approach has three advantages: it is an efficient method; it
involves almost no communication, synchronization, or mutual exclusion between workers;
its implementation is simple because the master and the workers rely on an underlying
constraint solver but does not require to modify it. Additionally, it is deterministic under
certain restrictions.
This paper integrates results from a series of publications (Regin, Rezgui, & Malapert,
2013, 2014; Rezgui, Regin, & Malapert, 2014). However, this paper includes novel contributions, implementations, and results. A new implementation of EPS on the top of
the Java library Choco2 (Choco, 2010) uses a new decomposition procedure. New results
are given for the implementations on the top of the C++ library Gecode (Schulte, 2006)
and OR-tools (Perron, Nikolaj, & Vincent, 2012), More problems types and instances
are tested. EPS is compared with other parallelizations of Gecode and with several static
422

fiEmbarrassingly Parallel Search in CP

solvers portfolios, We perform in-depth analysis of various components, especially the decomposition procedures, as well as the anomalies that can occur.
The paper is organized as follows. Section 2 presents the constraint programming background, Amdahls law, and related work about parallel constraint solving. Section 3 gives
a detailed description of our embarrassingly parallel search method. Section 4 gives extensive experimental results for the various implementations (Gecode, Choco2, OR-tools) on
different computing infrastructures (multi-core, data center, cloud computing) as well as
comparisons to other state-of-the-art parallel implementations and static solver portfolios.

2. Related Work
Here, we present the constraint programming background, two important parallelization
measures related to Amdahls law, and related work about parallel constraint solving.
2.1 Constraint Programming Background
Constraint programming (CP) has attracted high attention among experts from many areas
because of its potential for solving hard real-life problems. For an extensive review on
constraint programming, we refer the reader to the handbook by Rossi, Van Beek, and
Walsh (2006). A constraint satisfaction problem (CSP) consists of a set X of variables
defined by a corresponding set of possible values (the domains D) and a set C of constraints.
A constraint is a relation between a subset of variables that restricts the possible values that
variables can take simultaneously. The important feature of constraints is their declarative
manner, i.e. they only specify which relationship must hold. The current domain D(x) of
each variable x  X is always a (non-strict) subset of its initial domain. A partial assignment
represents the case where the domains of some variables have been reduced to a singleton
(namely a variable has been assigned a value). A solution of a CSP is an assignment of a
value to each variable such that all constraints are simultaneously satisfied.
Solutions can be found by searching systematically through the possible assignments of
values to variables. A backtracking scheme incrementally extends a partial assignment A
that specifies consistent values for some of the variables, toward a complete solution, by
repeatedly choosing a value for another variable. The variables are labeled (given a value)
sequentially. At each node of the search tree, an uninstantiated variable is selected and the
node is extended so that the resulting new branches out of the node represent alternative
choices that may have to be examined in order to find a solution. The branching strategy
determines the next variable to be instantiated, and the order in which the values from its
domain are selected. If a partial assignment violates any of the constraints, backtracking is
performed to the most recently assigned variable that still has alternative values available
in its domain. Clearly, whenever a partial assignment violates a constraint, backtracking is
able to eliminate a subspace from the Cartesian product of variable domains.
A filtering algorithm is associated with each constraint which removes inconsistent values
from the domains of the variables, i.e. assignments which cannot belong to a solution of
the constraint. Constraints are handled through a constraint propagation mechanism which
allows the reduction of the domains of variables until a global fixpoint is reached (no more
domain reductions are possible). In fact, a constraint specifies which relationship must hold
and its filtering algorithm is the computational procedure that enforces the relationship.
423

fiMalapert, Regin, & Rezgui

Generally, consistency techniques are not complete, i.e. they do not remove all inconsistent
values from the domains of the variables.
Both backtracking scheme and consistency techniques can be used alone to completely
solve a CSP, but their combination allows the search space to be explored in a complete
and more efficient way. The propagation mechanism allows the reduction of the variable
domains and the pruning of the search tree whereas the branching strategy can improve the
detection of solutions (or failures for unsatisfiable problems).
Here, we consider a complete standard backtracking scheme with depth-first traversal of
the search tree combined to the following variable selection strategies. Note that different
variable selection strategies can be used although only one at a time. lex selects a variable
according to lexicographic ordering. dom selects the variable with the smallest remaining domain (Haralick & Elliott, 1980). ddeg selects a variable with largest dynamic degree (Beck,
Prosser, & Wallace, 2005), that is, the variable that is constrained with the largest number
of unassigned variables. Boussemart, Hemery, Lecoutre, and Sais (2004) proposed conflictdirected variable ordering heuristics in which every time a constraint causes a failure during
search, its weight is incremented by one. Each variable has a weighted degree, which is
the sum of the weights over all constraints in which this variable occurs. wdeg selects the
variable with the largest weighted degree. The current domain of the variable can be incorporated to give dom/ddeg or dom/wdeg which selects the variable with minimum ratio
between current domain size and its dynamic or weighted degree (Boussemart et al., 2004;
Beck et al., 2005). dom/bwdeg is a variant which follows a binary labeling scheme. impact
selects the variable/value pair which has the strongest impact, i.e. leads in the strongest
search space reduction (Refalo, 2004).
For optimization problems, we consider a standard top-down algorithm which maintains
a lower bound, lb, and an upper bound, ub, on the objective value. When ub  lb, the subtree
can be pruned because it cannot contain a better solution.
2.2 Parallelization Measures and Amdahls Law
Two important parallelization measures are speedup and efficiency. Let t(c) be the wallclock time of the parallel algorithm where c is the number of cores and let t(1) be the
wall-clock time of the sequential algorithm. The speedup su(c) = t(1) / t(c) is a measure
indicating how many times the parallel algorithm performs faster due to parallelization.
The efficiency eff (c) = su(c) / c is a normalized version of speedup, which is the speedup
value divided by the number of cores. The maximum possible speedup of a single program
as a result of parallelization is known as Amdahls law (Amdahl, 1967). It states that a
small portion of the program which cannot be parallelized will limit the overall speedup
available from parallelization. Let B  [0, 1] be the fraction of the algorithm that is strictly
sequential, the time t(c) that
 an algorithm takes to finish when being executed on c cores
corresponds to: t(c) = t(1) B + 1c (1  B) . Therefore, the theoretical speedup su(c) is:

su(c) =

1
B + (1  B)
1
c

424

fiEmbarrassingly Parallel Search in CP

According to Amdahls law, the speedup can never exceed the number of cores, i.e. a linear
speedup. This, in terms of efficiency measure, means that efficiency will always be less
than 1.
Note that the sequential and parallel B&B algorithms do not always explore the same
search space. Therefore, super-linear speedups in parallel B&B algorithms are not in contradiction with Amdahls law because processors can access high quality solutions in early
iterations, which in turn brought a reduction in the search tree and problem size.
2.3 Parallel Constraint Solving
Designing and developing parallel programs has been a manual process where the programmer was responsible for both identifying and implementing parallelism (Barney & Livermore, 2016). In this section, we only discuss parallel constraint solving. About parallel
logic programming, we refer the reader to the surveys of De Kergommeaux and Codognet
(1994), and Gupta, Pontelli, Ali, Carlsson, and Hermenegildo (2001). About parallel integer
programming, we refer the reader to the surveys of Crainic, Le Cun, and Roucairol (2006),
Bader, Hart, and Phillips (2005), and Gendron and Crainic (1994).
The main approaches to parallel constraint solving can roughly be divided into the following main categories: search space shared in memory; search space splitting; portfolio algorithms; problem splitting. Most approaches require communication and synchronization,
but the most important issue is load balancing which refers to the practice of distributing
approximately equal amounts of work among tasks so that all processors are kept busy all
the time.
2.3.1 Search Space in Shared Memory
These methods are implemented by having many cores sharing a list of open nodes in the
search tree (nodes for which there is at least one of the children that is still unvisited).
Starved processors just pick up the most promising node in the list and expand it. By
defining different node evaluation functions, one can implement different strategies (DFS,
BFS and others). Perron (1999) proposed a comprehensive framework tested with at most
4 processors. Vidal, Bordeaux, and Hamadi (2010) reported good performance for a parallel
best-first search up to 64 processors. Although this kind of mechanism intrinsically provides
excellent load balancing, it is known not to scale beyond a certain number of processors;
beyond that point, performance starts to decrease. Indeed, on a shared memory system,
threads must contend with each other for communicating with memory and the problem is
exacerbated by cache consistency transactions.
2.3.2 Search Space Splitting
Search Space Splitting strategies exploring the parallelism provided by the search space
are common approaches: when a branching is done, different branches can be explored
in parallel (Pruul, Nemhauser, & Rushmeier, 1988). One challenge is load balancing: the
branches of a search tree are typically extremely imbalanced and require a non-negligible
overhead of communication for work stealing (Lai & Sahni, 1984).
The work stealing method was originally proposed by Burton and Sleep (1981) and first
implemented in Lisp parallel machines (Halstead, 1984). The search space is dynamically
425

fiMalapert, Regin, & Rezgui

split during the resolution. When a worker finished to explore a subproblem, it asks other
workers for another subproblem. If another worker agrees to the demand, then it splits
dynamically its current subproblem into two disjoint subproblems and sends one subproblem
to the starving worker. The starving worker steals some work to the busy one. Note
that some form of locking is necessary to avoid that several starving workers steal the
same subproblems. The starving worker asks other workers in turn until it receives a new
subproblem. Termination of work stealing method must be carefully designed to reduce the
overhead when almost all workers are starving, but almost no work remains. Recent works
based on this approach are those by Zoeteweij and Arbab (2004), Jaffar, Santosa, Yap, and
Zhu (2004), Michel, See, and Hentenryck (2009), and Chu, Schulte, and Stuckey (2009).
Because work stealing uses both communication, synchronization and computation time,
this cannot easily be scaled up to thousands of processors. To address these issues, Xie and
Davenport (2010) allocated specific processors to coordination tasks, allowing an increase
in the number of processors (linear scaling up to 256 processors) that can be used on a
parallel supercomputer before performance starts to decline.
Machado, Pedro, and Abreu (2013) proposed a hierarchical work stealing scheme correlated to the cluster physical infrastructure, in order to reduce the communication overhead.
A worker first tries to steal from its local node, before considering remote nodes (starting
with the closest remote node). This approach achieved good scalability up to 512 cores for
the n-queens and quadratic assignment problems. For constraint optimization problems,
maintaining the best solution for each worker would require a large communication and
synchronization overhead. But, Machado et al. observed that the scalability was lowered
because the lazy dissemination of the so-far best solution, i.e. because some workers use
obsolete best solution.
General-purpose programming languages designed for multi-threaded parallel computing
like Charm++ (Kale & Krishnan, 1993) and Cilk++ (Leiserson, 2010; Budiu, Delling, &
Werneck, 2011) can ease the implementation of work stealing approaches. Otherwise, a
work stealing framework like Bobpp (Galea & Le Cun, 2007; Le Cun, Menouer, & VanderSwalmen, 2007) provides an interface between solvers and parallel computers. In Bobpp, the
work is shared via a global priority queue and the search tree is decomposed and allocated to
the different cores on demand during the search algorithm execution. Periodically, a worker
tests if starving workers exist. In this case, the worker stops the search and the path from
the root node to the highest right open node is saved and inserted into the global priority
queue. Then, the worker continues the search with the left open node. Otherwise, if no
starving worker exists, the worker continues the search locally using the solver. The starving
workers are notified of the insertions in the global priority queue, and each one picks up a
node and starts the search. Using OR-tools as an underlying solver, Menouer and Le Cun
(2013), and Menouer and Le Cun (2014) observed good speedups for the Golomb Ruler
problem with 13 marks (41.3 with 48 workers) and the 16-queens problem (8.63 with 12
workers). Other experiments investigate the exploration overhead caused by their approach.
Bordeaux et al. (2009) proposed another promising approach based on a search space
splitting mechanism not based on a work stealing approach. They use a hashing function
allocating implicitly the leaves to the processors. Each processor applies the same search
strategy in its allocated search space. Well-designed hashing constraints can address the
load balancing issue. This approach gives a linear speedup for up to 30 processors for the
426

fiEmbarrassingly Parallel Search in CP

n-queens problem, but then the speedups stagnate at 30 until to 64 processors. However,
it only got moderate results 100 industrial SAT instances.
We have presented earlier works on the Embarrassingly Parallel Search method based
on search space splitting with loose communications (Regin et al., 2013, 2014; Rezgui et al.,
2014).
Fischetti, Monaci, and Salvagnin (2014) proposed another paradigm called SelfSplit in
which each worker is able to autonomously determine, without any communication between
workers, the job parts it has to process. SelfSplit can be decomposed in three phases: the
same enumeration tree is initially built by all workers (sampling); when enough open nodes
have been generated, the sampling phase ends and each worker applies a deterministic rule
to identify and solve the nodes that belong to it (solving); a single worker gathers the results
from others (merging). SelfSplit exhibited linear speedups up to 16 processors and good
speedups up to 64 processors on five benchmark instances. SelfSplit assumes that sampling
is not a bottleneck in the overall computation whereas that can happen in practice (Regin
et al., 2014).
Sometimes, for complex applications where very good domain specific strategies are
known, the parallel algorithm should exploit the domain-specific strategy. Moisan, Gaudreault, and Quimper (2013), and Moisan, Quimper, and Gaudreault (2014) proposed a
parallel implementation of the classic backtracking algorithm, Limited Discrepancy Search
(LDS), that is known to be efficient in centralized context when a good variable/value
selection heuristic is provided (Harvey & Ginsberg, 1995). Xie and Davenport (2010) proposed that each processor locally uses LDS to search in the trees allocated to them (by a
tree splitting or work stealing algorithm) but the global system does not replicate the LDS
strategy.
Cube-and-Conquer (Heule, Kullmann, Wieringa, & Biere, 2012) is an approach for parallelizing SAT solvers. A cube is a conjunction of literals and a DNF formula a disjunction of
cubes. The SAT problem is split into several disjoint subproblems that are DNF formulas
which are then solved independently by workers. Cube-and-Conquer using the ConflictDriven Clause Learning (CDCL) solver Lingeling outperforms other parallel SAT solvers
on some instances of the SAT 2009 benchmarks, but is also outperformed on many other
instances. Thus, Concurrent Cube-and-Conquer (Van Der Tak, Heule, & Biere, 2012) tries
to predict on which instances it works well and abort the parallel search after a few seconds
in favor of a sequential CDCL solver if not.
2.3.3 Las Vegas Algorithms / Portfolios
They explore the parallelism provided by different viewpoints on the same problem, for
instance by using different algorithms or parameter tuning. This idea has also been exploited
in a non-parallel context (Gomes & Selman, 2000). No communication is required and an
excellent level of load balancing is achieved (all workers visit the same search space). Even
if this approach causes a high level of redundancy between processors, it shows really good
performance. It was greatly improved by using randomized restarts (Luby, Sinclair, &
Zuckerman, 1993) where each worker executes its own restart strategy. More recently, Cire,
Kadioglu, and Sellmann (2014) executed the Luby restart strategy, as a whole, in parallel.
They proved that it achieves asymptotic linear speedups and, in practice, often obtained
427

fiMalapert, Regin, & Rezgui

linear speedups. Besides, some authors proposed to allow processors to share information
learned during the search (Hamadi, Jabbour, & Sais, 2008).
One challenge is to find a scalable source of diverse viewpoints that provide orthogonal
performance and are therefore of complementary interest. We can distinguish between
two aspects of parallel portfolios: if assumptions can be made on the number of available
processors then it is possible to handpick a set of solvers and settings that complement
each other optimally. If we want to face an arbitrarily high number of processors, then we
need automated methods to generate a portfolio of any size on demand (Bordeaux et al.,
2009). So, portfolio designers became interested in feature selection (Gomes & Selman, 1997,
1999, 2001; Kautz, Horvitz, Ruan, Gomes, & Selman, 2002). Features characterize problem
instances like number of variables, domain sizes, number of constraints, constraints arities.
Many portfolios select the best candidate solvers from a pool based on static features or by
learning the dynamic behaviour of solvers. The SAT portfolio iSAC (Amadini, Gabbrielli,
& Mauro, 2013) and the CP portfolio CPHydra (OMahony, Hebrard, Holland, Nugent, &
OSullivan, 2008) use feature selection to choose the solvers that yield the best performance.
Additionally, CPHydra exploits the knowledge coming from the resolution of a training set
of instances by each candidate solver. Then, given an instance, CPHydra determines the k
most similar instances of the training set and determines a time limit for each candidate
solver based on constraint program maximizing the number of solved instances within a
global time limit of 30 minutes. Briefly, CPHydra determines a switching policy between
solvers (Choco2, AbsCon, Mistral).
Many recent SAT solvers are based on a portfolio such as ManySAT (Hamadi et al.,
2008), SATzilla (Xu, Hutter, Hoos, & Leyton-Brown, 2008), SArTagnan (Stephan & Michael,
2011), Hydra (Xu, Hoos, & Leyton-Brown, 2010), Pminisat (Chu, Stuckey, & Harwood,
2008) based on Minisat (Een & Sorensson, 2005). Most of them combine portfolio-based
algorithm selection to automatic algorithm configuration using different underlying solvers.
For example, SATzilla (Xu et al., 2008) exploits the per-instance variation among solvers
using learned runtime models.
In general, the main advantage of the algorithms portfolio approach is that many strategies will be automatically tried at the same time. This is very useful because defining good
search strategies is a difficult task.
2.3.4 Problem Splitting
Problem Splitting is another idea that relates to parallelism, where the problem itself is split
into pieces to be solved by each processor. The problem typically becomes more difficult to
solve than in the centralized case because no processor has a complete view on the problem.
So, reconciling the partial solutions of each subproblem becomes challenging. Problem
splitting typically relates to distributed CSPs, a framework introduced by Yokoo, Ishida,
and Kuwabara (1990) in which the problem is naturally split among agents, as for privacy
reasons. Other distributed CSP frameworks have been proposed such as those by Hirayama
and Yokoo (1997), Chong and Hamadi (2006), Ezzahir, Bessiere, Belaissaoui, and Bouyakhf
(2007), Leaute, Ottens, and Szymanek (2009), and Wahbi, Ezzahir, Bessiere, and Bouyakhf
(2011).
428

fiEmbarrassingly Parallel Search in CP

2.3.5 Parallel Constraint Propagation
Other approaches can be thought of, typically based on the parallelization of one key algorithm of the solver, for instance constraint propagation (Nguyen & Deville, 1998; Hamadi,
2002; Rolf & Kuchcinski, 2009). However, parallelizing propagation is challenging (Kasif,
1990) and the scalability is limited by Amdahls law. Some other approaches focus on
particular topologies or make assumptions on the problem.
2.3.6 Concluding Remarks
Note that for the oldest approaches, scalability issues are still to be investigated because of
the small number of processors, typically around 16 and up to 64 processors. One major
issue is that all approaches may (and a few must) resort to communication. Communication
between parallel agents is costly in general: in shared-memory models such as multi-core,
this typically means an access to a shared data structure for which one cannot avoid some
form of locking; the cost of message-passing cross-CPU is even significantly higher. Communication additionally makes it difficult to get insights on the solving process since the
executions are highly inter-dependent and understanding parallel executions is notoriously
complex.
Most parallel B&B algorithms explore leaves of the search tree in a different order than
they would be on a single-processor system. This could be a pity in situations where we
know a really good search strategy, which is not entirely exploited by the parallel algorithm.
For many approaches, experiments with parallel programming involve a great deal of nondeterminism: running the same algorithm twice on the same instance, with identical number
of threads and parameters, may result in different solutions, and sometimes in different
runtimes.

3. Embarrassingly Parallel Search
In this section, we present the details of our embarrassingly parallel search. First, Section 3.1
introduces the key concepts that guided our design choices. Then, Section 3.2 introduces
several search space splitting strategies implemented via the top-down or bottom-up decomposition procedures presented in Section 3.3. Section 3.4 gives details about the architecture
and the communication. Section 3.5 explains how to manage the queue of subproblems in
order to obtain a deterministic parallel algorithm. Section 4.1 gives more details about the
implementation.
3.1 Key Concepts
We introduce the key concepts that guided our design choices: massive static decomposition;
loose communication; non-intrusive implementation; toward a deterministic algorithm.
3.1.1 Massive Static Decomposition
The master decomposes the problem into p subproblems once and for all which are then
solved in parallel and independently by the workers. So, the solving process is equivalent to
the real-time scheduling of p jobs on w parallel identical machines known as P ||Cmax (Korf &
429

fiMalapert, Regin, & Rezgui

Schreiber, 2013). Efficient algorithms exists for P ||Cmax and even the simple list scheduling
algorithms (based on priority rules) are a (2  w1 )-approximation. The desirable properties
defined in Section 3.2 should ensure low precision processing times that makes the problems
easier. If we hold the precision and number of workers fixed, and increase the number of
subproblems, then problems get harder until perfect schedules appear, and then they get
easier. In our case, the number p of subproblems should range between one and three
orders of magnitude larger than the number of workers w. If it is too low, the chance of
finding perfect schedules, and therefore obtain good speedups, are low. If it is too large, the
decomposition takes longer and becomes more difficult. If these conditions are met, then
it is unlikely that a worker will be assigned more work than any other, and therefore, the
decomposition will be statistically balanced. Beside, to reach good speedups in practice,
the total solving time of all subproblems must be close to the sequential solving time of the
problem.
An advantage is that the master and workers are independent. They can use different
filtering algorithms, branching strategies, or even underlying solvers. The decomposition is
a crucial step, because it can be a bottleneck of the computation and its quality also greatly
impacts the parallelization efficiency.
3.1.2 Loose Communication
p subproblems are solved in parallel and independently by the w workers. As load balancing
must be statistically obtained from the decomposition, we do not allow work stealing in
order to drastically reduce communication. Of course, some communication is still needed
to dispatch the subproblems, to gather the results and possibly to exchange useful additional
information, like objective bound values. Loose communication allows to use a star network
without risk of congestion. A central node (foreman) is connected to all other nodes (master
and workers).
3.1.3 Non-intrusive Implementation
For the sake of laziness and efficiency, we rely as much as possible on the underlying solver(s)
and on the computing infrastructure. Consequently, we modify as little as possible the underlying solver. We do not consider nogoods or clauses exchanges because these techniques
are intrusive and increase the communication overhead. Additionally, logging and fault
tolerance are respectively delegated to the underlying solver and to the infrastructure.
3.1.4 Toward Determinism
A deterministic algorithm is an algorithm which, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states. If the determinism is already challenging for sequential B&B algorithms
due to their complexity (randomization, restarts, learning, optimization), it is still more
difficult for parallel B&B algorithms.
Here, we will always guarantee reproducibility if the real-time assignment of subproblems to workers is stored. Reproducibility means that it is always possible to replay the
solving process. With some restrictions detailed later, our parallel algorithm can be made
deterministic with no additional cost. Moreover, the parallel algorithm should be able to
430

fiEmbarrassingly Parallel Search in CP

mimic the sequential algorithm, i.e. they produce identical solutions. It requires that the
parallel algorithm visits the tree leaves in the same order as the sequential algorithm. More
generally, it would be useful for debugging, performance evaluation, or incremental problem
solving that the parallel algorithm may produce identical solutions no matter how many
workers are present or which computing infrastructure is used.
Conversely, any real-time scheduling algorithm can be applied to subproblems. It would
allow to improve diversification by using more randomization, or to exploit past information
provided by the solving process. In the experiments, we will only use FIFO scheduling of
the subproblems, because other scheduling policy would change the shape and size of the
search tree and, therefore, reduces the relevance of speedups. Unlike EPS, work stealing
approaches are not deterministic and offer no control on subproblem scheduling.
3.2 Search-Space Splitting Strategies
Here, we extend the approach to search-space splitting proposed by Bordeaux et al. (2009),
called splitting by hashing. Let us recall that C is the set of constraints of the problem. To
split the search space of a problem into p parts, one approach is to assign each subproblem
i (1  i  p) an extended set of constraints C  Hi where Hi is a hashing constraint, which
constrains subproblem i to a particular subset of the search space. Hashing constraints
must necessarily be sound and should be effective, nontrivial, and statistically balanced.
Sound Hashing constraints must partition the search space: pi=1 Hi must cover the entire
initial search space (completeness), and the mutual intersections Hi  Hj (1  i < j  p)
should preferably be empty (non-overlapping).
Effective The addition of the hashing constraints should effectively allow each worker to
efficiently skip the portions of the search space not assigned to its current subproblem. Each
subproblem must be significantly easier than the original problem. This causes overhead,
to which we refer to as recomputation overhead.
Nontrivial The addition of the hashing constraints should not lead to an immediate
failure of the underlying solver. Thus, generating trivial subproblems might be paid by some
exploration overhead, because many of them would have been discarded by the propagation
mechanism of the sequential algorithm.
Statistically Balanced All workers should be given about the same amount of work. If
the decomposition is appropriate, then the number p of subproblems is significantly larger
than the number w of workers. It is thus unlikely that a given worker would be assigned
significantly more work than any other worker by any real-time scheduling algorithm. However, it is possible that solving one subproblem requires significantly more work than another
subproblem.
Bordeaux et al. (2009) defined hashing constraints by selecting a subset X of the variables
P
of the problem and stating Hi (1  i  p) as follows: xX x  i mod p. This effectively
decomposes a problem into p problems if p is within reasonable limits. For p = 2, it imposes
a parity constraints over the sum of the variables. Splitting can be repeated to scale-up to
an arbitrary number of processors. This splitting is obviously sound, but less effective for
431

fiMalapert, Regin, & Rezgui

CP solvers than for SAT solvers. Here, we study assignment splitting and node splitting to
generate a given number p? of subproblems.
3.2.1 Assignment Splitting
Let us consider a non empty subset X  X of d ordered variables: X = (x1 , . . . , xd ). A vector  = (v1 , . . . , vd ) is a tuple on X if vj  D(xj ) (j = 1, . . . , d). Let H( ) = dj=1 (xj = vj )
be the hashing constraints which restrict the search space to solutions extending the tuple  .
Q
A total decomposition on X splits the initial problem into di=1 D(xi ) subproblems, i.e. one
subproblem per tuple. A total decomposition is clearly sound and effective, but not efficient
in practice. Indeed, Regin et al. (2013) showed that the number of trivial subproblems can
grow exponentially.
In a table decomposition, a subproblem is defined by a set of tuples that allows to reach
exactly the number p? of subproblems. Let T be an ordered list ofj tuples
on X such that
k
|T |
?
|T | > p . Then, the first subproblem is defined by the first k = p? tuples, the second
subproblem is defined by the following k tuples, and so on. So, all subproblems are defined
by the same number of tuples possibly with the exception of the last.
A tuple  is solver-consistent if the propagation of the extended set of constraints C 
H( ) by the underlying solver does not detect unsatisfiability. In order to obtain nontrivial
decompositions, total and table decompositions are restricted to solver-consistent tuples.
3.2.2 Node Splitting
Node splitting allows the parallel algorithm to exploit domain-specific strategies for the
decomposition when a good strategy is known. Let us recall some concepts on search
trees (Perron, 1999) that are the basis of the decomposition procedures introduced later.
To decompose the problems, one needs to be able to map individual parts of the search tree
to hashing constraints. These parts are called open nodes. Once open nodes are defined,
we present how a search tree is decomposed into a set of open nodes.
Open Nodes and Node Expansion The search tree is partitioned into three sets, open
nodes, closed nodes, and unexplored nodes. Here, we do not make any assumption about
the arity of the search tree, i.e. the maximal number of children of its nodes. These subsets
have the following properties.
 All the ancestors of an open node are closed nodes.
 Each unexplored node has exactly one open node as its ancestor.
 No closed node has an open node as its ancestor.
The set of open nodes is called the search frontier as illustrated in Figure 1. The search
active
path
closed

open

Frontier

unexplored

Figure 1: Node status and search frontier of a search tree.
432

fiEmbarrassingly Parallel Search in CP

frontier evolves simply through a process known as node expansion. It removes an open node
from the frontier, transforms the removed node into a closed node, and adds its unexplored
children to the frontier. Node expansion is the only operation that happens during the
search. It corresponds to the branch operation in a B&B algorithm.
At any point of the search, the search frontier is a sound and nontrivial decomposition
of the original problem where each open node is associated to a subproblem. The decomposition is effective if and only if the branching strategy is effective. Let us remark that
assignment splitting can be seen as a special case of node splitting in which a static ordering
is used for variables and values.
Active Path and Jumps in the Search Tree Expanding one node after another may
require changing the state (at least the variables domains) of the search process from the
first node to the second. So, the worker in charge of exploring an open node must reconstruct
the state it visits. This is done using an active path and a jumping operation.
When going down in the search tree, our search process builds an active path, which is
the list of ancestors of the current open node, as illustrated in Figure 1. When a worker
moves from one node to another, it has to jump in the search tree. To make the jump, it
simply recomputes every move from the root until it gets to the target node. This causes
overhead, to which we refer to as recomputation overhead. Recomputation does not change
the search frontier because it does not expand a node.
3.3 Decomposition Procedures
The decomposition challenge is to find the depth at which the search frontier contains
approximately p? nodes. The assignment splitting strategy is implemented by a top-down
procedure which starts from the root node and incrementally visits the next levels, whereas
the node splitting strategy is implemented by a bottom-up procedure which starts form a
level deep enough and climbs back to the previous levels.
3.3.1 Top-Down Decomposition
The challenge of the top-down decomposition is to find d ordered variables which produce
approximately p? solver-consistent tuples. Algorithm 1 realizes a solver-consistent table
decomposition by iterated depth-bounded depth-first searches with early removals of inconsistent assignments (Regin et al., 2013).
The algorithm starts at the root node with an
empty list of tuples (line 1). It computes a list T of p? tuples that are a solver-consistent
table decomposition by iterativly increasing the decomposition depth. Let us assume that
there exists a static order of the variables. At each iteration, it determines a new lower
bound (line 4) on the decomposition depth d, i.e. the number of variables involved in
the decomposition. This lower bound uses the Cartesian product of the current domains
of the next variables xd+1 , xd+2 , . . . Then, a depth-bounded depth-first search extends the
decomposition to its new depth and updates the list of tuples (line 5). The current tuples
are added to the constraints of the model during the search (line 5) in order to reduce
redundant work. After the search, the extended tuples are propagated (line 7) to reduce
the domains, and to improve the next lower bound on the decomposition depth. Each tuple
was solver-consistent (not proven infeasible) during the last search. The process is repeated
until the number |T | of tuples is greater or equal to p? . At the end, tuples are aggregated
433

fiMalapert, Regin, & Rezgui

Algorithm 1: Top-down decomposition.

1
2

3

4

5
6

7
8

9

10

Data: A CSP (X , D, C) and a number of subproblems p?
Result: A list of tuples T
d  0;
T ;
/* Simulate a breadth-first search: iterated depth bounded DFSs.
repeat
/* Determine
on the decomposition
depth. */
o
n fi a lower bound
Ql
fi
?
d  min l fi max(1, |T |)  i=d+1 |D(xi )|  p ;

*/

/* Extend the current decomposition with new variables. */
T  depthBoundedDFS (X , C  { T H( )}, D, {x1 , . . . , xd });
if T ==  then break;
/* Propagate the tuples (without failure). */
D  propagate (X , C  { T H( )}, D);
until |T | < p? ;
/* Aggregate tuples to generate exactly p? subproblems */
T  aggregateTuples(T ) /* All subproblems become simultaneously available.
*/
foreach   T do sendSubProblem (X , C  H( ), D);

to generate exactly p? subproblems. In practice, consecutive tuples of T are aggregated.
All subproblems become simultaneously available after the aggregation.
Sometimes, the sequential decomposition is a bottleneck because of Amdahls law. So,
the parallel decomposition procedure increases the scalability (Regin et al., 2014). Only
two steps differ from Algorithm 1. First, instead of starting at depth 0 with an empty list
of tuples (line 1 of Algorithm 1), a first list is quickly generated with at least five tuples per
worker.
n fi
o
1
2

fi Ql
d  min l fi i=1 |D(xi )|  5  w ;
Qd
T  i=1 D(xi );

Second, at each iteration, each tuple is extended in parallel instead of extending sequentially all tuples (line 5 of Algorithm 1). The parallel decomposition can change the ordering
of T compared to the sequential one. Again, all subproblems only become available at the
end of the decomposition.

8

T 0  ;
run in parallel
foreach   T do
/* extend each tuple in parallel */
T 0  T 0  depthBoundedDFS (X , C  H( ), D, {x1 , . . . , xd });

9

T  T 0;

5
6
7

Both top-down procedures assume that the variable ordering used in the decomposition is static. The next decomposition procedure bypasses this limitation and handles any
branching strategy.
434

fiEmbarrassingly Parallel Search in CP

Algorithm 2: Bottom-up decomposition.
1

2
3
4
5
6

7
8
9
10

Data: A CSP (X , D, C), a decomposition depth d? , and a subproblem limit P .
p  0;
/* Generate subproblems by visiting the top of the real tree. */
Before Node Callback decomposition(node) is
if depth(node)  d? then
sendSubProblem (node);
p  p + 1;
if p  P then
/* Decrease dynamically the depth. */
d?  max(1, d?  1);
P  2  P;
backtrack;
DFS (X ,C,D);

3.3.2 Bottom-Up Decomposition
The bottom-up decomposition explores the search frontier at the depth d? with approximately p? nodes. In its simplest form, the decomposition depth d? can be provided by a
user with good knowledge of the problem. Algorithm 2 explores the search frontier at depth
d? using a depth-first search as illustrated in Figure 2(a). A search callback identifies each
node at level d? (line 2), and sends immediately its active path, which defines a subproblem,
so that the subproblem will be solved by a worker. If the decomposition depth is dynamic,
then it is reduced if the number of subproblems becomes too large (line 6). It aims to
compensate a poor choice of the decomposition depth d? . In practice, the depth is reduced
by one unit if the current number of subproblems exceeds a given limit P . This limit is
initially set up to P = 2  p? and is doubled each time it is reached. On the contrary, the
depth is static (P = +) if it never changes whatever be the number of subproblems.
In practice, it is not common that the user provides the decomposition depth, and
an automated procedure without any users intervention is needed. Algorithm 3 aims at
identifying the topmost search frontier with approximately p? open nodes by sampling and
estimation. The procedure can be divided into three phases: build a partial tree by sampling

Final depth D

Search Frontier

Dynamic



p nodes

Static
P nodes

Initial depth D

2P nodes

(a) Decomposition.

(b) Estimation.

Figure 2: Bottom-up decomposition and estimation.
435

fiMalapert, Regin, & Rezgui

Algorithm 3: Bottom-up estimation.

1

2
3
4
5
6
7
8
9

10
11

Data: A CSP (X , D, C) and a number of subproblems p? .
Data: A time limit t, a node limit n, and a maximum depth D large enough
Result: A decomposition depth d?
/* Set counters for the width of the levels */
foreach d  [1, D] do width[d]  0;
/* Build a partial tree by sampling. */
Before Node Callback estimation(node) is
d  depth(node);
if d  D then
width[d]  width[d] + 1;
if width[d]  p? then D  d  1 ;
else backtrack;
if hasFinished (t,n) then break;
DFS (X ,C,D);
/* Estimate the level widths of the tree and the decomposition depth.
width  estimateWidths(width);
d?  estimateDepth(width, p? );

*/

the top of the real search tree; estimate the level widths of the real tree; and then determine
the decomposition depth d? with a greedy heuristic.
Since we need to explore the top of the search tree, an upper bound D on the decomposition depth is fixed. The maximum decomposition depth D must be chosen according
to the number of workers and the expected number of subproblems per worker. If D is
too small, the decomposition could generate too few subproblems. If D is too large, the
sampling time increases while the decomposition quality could decrease.
The sampling phase builds a partial tree with at most p? open nodes on a level using
a callback of a depth-first search. The number of open nodes at each level of the partial
tree is counted by the callback. The maximum depth D is reduced each time p? nodes are
opened at a given level (line 6). If the sampling ends within its limits, then the top of the
tree has been entirely visited and no estimation is needed. Otherwise (line 8), one needs
to estimate the widths of the topmost levels of the tree depending on the partial tree. The
estimation is a straightforward adaptation of the one proposed by Cornuejols, Karamanov,
and Li (2006) to deal with n-ary search tree (line 10). In practice, the main issue is that the
higher the arity is, the lower is the precision of the estimation. Therefore, a greedy heuristic
determines the decomposition depth based on the estimated number of nodes per level, but
also on the number of nodes in the partial tree (line 11). The heuristics minimizes the
absolute deviation between the estimated number of nodes and the expected number p? . If
several levels have an identical absolute deviation, then the lowest level with an estimated
number of subproblems greater than or equal to p? is selected.
3.4 Architecture and Communication
We describe messages exchanged by the actors depending on the problems type. Then,
a typical use case illustrates the solving process for an optimization problem. Briefly, the
436

fiEmbarrassingly Parallel Search in CP

communication network is a star network where the foreman acts as a pipe to transmit
messages between the master and workers.
3.4.1 Actors and Messages
The total number of messages depends linearly of the number of workers (w) and the
number of subproblems (p). All messages are synchronous for sake of simplicity which
means that work must wait until the communications have completed (Barney & Livermore,
2016). Interleaving computation with communication is the single greatest benefit for using
asynchronous communications since work can be done while the communications are taking
place. However, asynchronous communications complicate the architecture, for instance if
a message requests a answer.
Master is the control unit which decomposes the problem and collects the final results.
It sends the following messages: create the foreman; give a subproblem to the foreman;
wait for the foreman to gather all results; destroy the foreman. The master only deals
with the foreman. The decomposition time is the elapsed time between the create and
wait messages. The workers time is the elapsed time between the first give and destroy
messages. The wall-clock time is the elapsed time from the creation to the destruction of
the master.
Foreman is the central node of the star network. It is a queuing system which stores subproblems received from the master and dispatches them to workers. It also gathers results
collected from the workers. The foreman allows the master to concentrate on the problems
decomposition, which is a performance bottleneck, by handling all communications with the
workers. It sends the following messages: create a worker; give a subproblem to a worker;
collect (send) the final results to the master; destroy a worker. When the foreman
detects that the search has ended, it sends a collect-message containing the final results
to the master.
Workers are search engines. They send the following messages: find a subproblem (the
foreman must answer by a give-message); collect (send) results to the foreman. The
results contain essential information about the solution(s) and the solving process. Workers
only know the foreman. When a worker acquires new work (receives a give-message from
the foreman), the acquired subproblem is recomputed which causes recomputation overhead.
In work stealing context, Schulte (2000) noticed that the higher the node is in the search
tree, the smaller is the recomputation overhead. By construction, only the topmost nodes
are used here.
3.4.2 Problems Types
We discuss the specificities of first solution, all solution, and best solution searches.
First Solution Search The search is complete as soon as a solution has been found.
Other workers must be immediately terminated as well as the decomposition procedure.
All Solution Search The search is complete when all subproblems have been solved.
437

fiMalapert, Regin, & Rezgui

Best Solution Search The main design issue in best-solution search is to maintain the
so-far best solution. The sequential B&B algorithm always knows the so-far best solution.
This is difficult to achieve in a concurrent setting with several workers. Maintaining the best
solution for each worker could lead to large communication and synchronization overheads.
Instead we prefer a solution where both the foreman and workers maintain the so-far best
solution as follows. By default, the give and collect messages between the foreman and the
workers carry the objective information. Additionally, a worker can send better messages
to the foreman with an intermediate solution, or the foreman can send its best solution to
all workers. For instance, when a worker finds a new solution, it informs the foreman by
sending a better message if the solution is accepted by a threshold function. Similarly,
when the foreman receives a new solution through a collect or better message, it checks
whether the solution is really better. If the solution is accepted by the threshold function,
the foreman sends another better message to all workers. The architecture sketched above
entails that a worker might not always know the so-far best solution. In consequence, some
parts of the search tree are explored, and they should have been pruned away if the worker
had had exact knowledge. Thus, the loose coupling might be paid by some exploration
overhead.

Master

Foreman

Worker 1

Worker 2

opt

[Allocate Resources]
<< create >>
<< create >>
<< create >>
give

find
give
opt

better

[Best Solution Search]

give

give

give
collect
find
wait

give
collect
better

opt

better

[Best Solution Search]

collect

collect
opt
[Release Resources]
<< destroy >>
<< destroy >>

<< destroy >>

Master

Foreman
Master

Worker
Master1

Worker
Master2

Figure 3: Sequence diagram of the solving process with two workers.
438

fiEmbarrassingly Parallel Search in CP

3.4.3 Use Case
Figure 3 is a sequence diagram illustrating the solving process for an optimization problem
with two workers. It shows how actors operate with each other in chronological order.
The first horizontal frame is the resource allocation. The master creates the foreman.
The foreman creates the workers. Immediately after creation, the master and each worker
load the original problem. The foreman transparently manages a concurrent queue of subproblems produced by the master and consumed by workers. After that, workers will only
jumps in the search tree.
After the foreman creation, the master starts the decomposition of the original problem
into p = 3 subproblems. As soon as a subproblem is generated, the master gives it to
the foreman. Here, the give and find messages are interleaved as in the node splitting
decomposition proposed in Section 3.3.2. The assignment splitting decomposition proposed
in Section 3.3.1 would produce a unique give message with all subproblems. When the
decomposition is finished, the master sends a wait message to the foreman and waits for
a collect response containing the final result. This last collect message triggers the
resource deallocation.
Each time a worker is starving, it asks the foreman for a subproblem and waits for it.
Here, the first subproblem is assigned to the first worker while the second worker waits for
the second subproblem. The Best Solution Search frames correspond to specific messages
for optimization problems. The first worker quickly finds a good solution and sends it to
the foreman via a better message. A second subproblem is generated by the master and
then given to the foreman. In turn, the foreman gives the second subproblem and updated
objective information to the second worker. The second problem is quickly solved by the
second worker which sends a collect message to the foreman. The collect message also
stands for a find message. Then, the third, and last, subproblem is assigned to the second
worker.
The foreman broadcasts a better message because of the good quality of the solution
received from the first worker. Note that this message is useless for the first worker. The
foreman detects the termination of the solving process and sends the collect message to the
master if the three following conditions are met: the master is waiting; the subproblems
queue is empty; and all workers are starving. The last horizontal frame is the resource
deallocation.
3.5 Queuing and Determinism
The foreman plays the role of a queuing system which receives subproblems from the master
and dispatches them to the workers. In this section, we show that EPS can be modified
to return the same solution than the sequential algorithm which can be useful in several
scenarios such as debugging or performance evaluation. Generally, any queuing policy can
be applied to select the next subproblem to solve.
Let us assume that the subproblems P1 , P2 , . . . , Pp are sent to the foreman in a fixed
order which is the case for the sequential top-down procedure and the bottom-up procedure.
Otherwise, a fixed order of subproblems can be obtained by sorting the subproblems.
The first solution found by the sequential algorithm belongs to the satisfiable subproblem
Pi with the smallest index, i.e. the leftmost solution. Let us assume that the parallel
439

fiMalapert, Regin, & Rezgui

algorithm finds a first solution for the subproblem Pj such that j > i. Then, it is not
necessary to solve problems Pk such that k > j and one must only wait for each problem
Pk such that k < j and then determine the leftmost solution, the satisfiable subproblem
with the smallest index.
It can easily be extended for optimization problems by slightly modifying the cutting
constraints. Usually, a cutting constraint is stated when a new solution is found that
only allows strictly improving solution. On the contrary to other constraints, the cutting
constraint is always propagated while backtracking. Here, if a solution is found when solving
the subproblem Pj , then the cutting constraint only allows strictly improving solution for
subproblems k  j, but also allows equivalent solution for subproblems k < j.
So, the parallel algorithm returns the same solution than the sequential one if the
subproblem are visited in the same order. Moreover, the solution returned by the parallel
algorithm does not depend on the number of workers, but only on the decomposition. In
our experiments, the queuing policy is the FIFO policy that ensures that subproblems are
solved in the same order so that the speedups are relevant. However, there is no guaranty
that the sequential and parallel algorithms return the same solution.

4. Experimental Results
Here, we describe experiments on EPS and carry out a detailed data analysis. We aim
to answer the following questions. Is EPS efficient? With different number of workers?
With different solvers? On different computing platforms? Compared to other parallel
approaches? What is the influence of the different components (decomposition procedures,
search strategies, constraint models)? Is EPS robust and flexible? Which anomalies can
occur?
Section 4.1 presents the benchmark instances, execution environments, parameters settings, and the different implementations. First, in Section 4.2, we analyze and evaluate the
top-down and bottom-up decomposition procedures as well as the importance of the search
strategy, especially for the decomposition. Then, we evaluate the efficiency and scalability
of parallel solvers on a multi-core machine (Section 4.3), on a data center (Section 4.4),
and on a cloud platform (Section 4.5). In these sections, we compare our implementations
of EPS with work stealing approaches whenever it is possible. In Section 4.4, we also analyze the efficiency of a parallel solver depending on the search strategy. In Section 4.6,
we transform with reasonable effort a parallel solver into a distributed parallel solver by
using the batch scheduler provided by the data center. Some anomalies of a parallel solver
are explained and resolved by its distributed equivalent. Last, Section 4.7 discusses the
performance of parallel solvers compared with static portfolios built from the underlying
sequential solvers on the data center.
4.1 Experimental Protocol
In this section, we introduce the benchmark instances, execution environments, metrics and
notations. We also give more details about the implementations.
440

fiEmbarrassingly Parallel Search in CP

4.1.1 Benchmark Instances
A lot of benchmark instances are available in the literature. We aim to select difficult
instances with various models that represent problems tackled by CP. Ideally, an instance
is difficult if none of the solvers can solve it quickly. Indeed, parallel solving is relevant
only if it shortens a long wall-clock time. Here, we only consider unsatisfiable, enumeration
and optimization problems instances. We will ignore the problem of finding a first feasible
solution because the parallel speedup can be completely uncorrelated to the number of
workers, making the results hard to analyze. We will consider optimization problems for
which the same variability can be observed, but at a lesser extent because the optimality
proof is required. The variability for unsatisfiable and enumeration instances is lowered, and
therefore, they are often used as a test bed for parallel computing. Besides, unsatisfiable
instances have a practical importance, for instance in software testing, and enumeration is
important for users to compare various solutions.
The first set called fzn is a selection of 18 instances selected from more than 5000
instances either from the repository maintained by Kjellerstrand (2014) or directly from the
Minizinc 1.6 distribution written in the FlatZinc language (NICTA Optimisation Research
Group, 2012). Each instance is solved in more than 500 seconds and less than 1 hour with
Gecode. The selection is composed of 1 unsatisfiable, 6 enumeration, and 11 optimization
instances.
The set xcsp is composed of instances from the categories ACAD and REAL of XCSP
2.1 (Roussel & Lecoutre, 2008). It consists of difficult instances that can be solved within
24 hours by Choco2 (Malapert & Lecoutre, 2014). A first subset called xcsp1 is composed
of 5 unsatisfiable and 5 enumeration instances whereas the second subset called xcsp2 is
composed of 11 unsatisfiable and 3 enumeration instances. The set xcsp1 is composed of
instances easier to solve than those of xcsp2.
Besides, we will consider two classical problems, the n-queens and the Golomb ruler
problems which have been widely used in the literature (Gent & Walsh, 1999).
4.1.2 Implementation Details
We implemented EPS method on top of three solvers: Choco2 2.1.5 written in Java, Gecode
4.2.1 and OR-tools rev. 3163 written in C++. We use two parallelism implementation
technologies: Threads (Mueller et al., 1993; Kleiman, Shah, & Smaalders, 1996) and
MPI (Lester, 1993; Gropp & Lusk, 1993). The typical difference between both is that
threads (of the same process) run in a shared memory space, while MPI is a standardized
and portable message-passing system to exchange information between processes running
in separate memory spaces. Therefore, Thread technology does not handle multiple nodes
of a cluster whereas MPI does.
In C++, we use Threads implemented by pthreads, a POSIX library (Mueller et al.,
1993; Kleiman et al., 1996) used by Unix systems. In Java, we use the standard Java Thread
technology (Hyde, 1999).
There are many implementations for MPI like OpenMPI (Gabriel, Fagg, Bosilca, Angskun,
Dongarra, Squyres, Sahay, Kambadur, Barrett, Lumsdaine, et al., 2004), Intel MPI (Intel
Corporation, 2015), MPI-CH (MPI-CH Team, 2015) and MS-MPI (Krishna, Balaji, Lusk,
Thakur, & Tiller, 2010; Lantz, 2008). MPI is a standard API, so the characteristics of the
441

fiMalapert, Regin, & Rezgui

machine are never taken into account. So, the machine providers like Bull, IBM or Intel
provide their own MPI implementation according to the specifications of the delivered machine. Thus, the cluster provided by Bull has a custom Intel MPI 4.0 library, but OpenMPI
1.6.4 is also installed, and Microsoft Azure only supports its own MS-MPI 7 library.
OR-tools uses a sequential top-down decomposition and C++ Threads. Gecode uses a
parallel top-down decomposition and C++ Threads or MPI technologies. In fact, Gecode
will use C++ pthread on the multi-core computer, OpenMPI on the data center, and
MS-MPI on the cloud platform. Gecode and OR-tools both use the lex variable selection
heuristic because the top-down decomposition requires a fixed variable ordering. Choco2
uses a bottom-up decomposition and Java Threads. In every case, the foreman schedules
the jobs in FIFO to mimic as much as possible the sequential algorithm so that speedups
are relevant. When needed, the master and the workers read the model from the same file.
We always take the value selection heuristic which selects the smallest value whatever be
the variable selection heuristic.
4.1.3 Execution Environments
We use three execution environments that are representative of computing platforms available nowadays.
Multi-core is a Dell computer with 256 GB of RAM and 4 Intel E7-4870 2.40 GHz processors running on Scientific Linux 6.0 (each processor has 10 cores).
Data Center is the Centre de Calcul Interactif hosted by the Universite Nice Sophia
Antipolis which provides a cluster composed of 72 nodes (1152 cores) running on CentOS
6.3, each node with 64 GB of RAM and 2 Intel E5-2670 2.60 GHz processors (8 cores). The
cluster is managed by OAR (Capit, Da Costa, Georgiou, Huard, Martin, Mounie, Neyron,
& Richard, 2005), i.e., a versatile resource and task manager. As Thread technology is
limited to a single node of a cluster, Choco2 can use up to 16 physical cores whereas Gecode
can use any number of nodes thanks to MPI.
Cloud Computing is a cloud platform managed by the Microsoft company (Microsoft
Azure) that enables to deploy applications on Windows Server technology (Li, 2009). Each
node has 56 GB of RAM and Intel Xeon E5-2690E 2.6 GHz processors (8 physical cores)
We were allowed to simultaneously use 3 nodes (24 cores) managed by the Microsoft HPC
Cluster 2012 (Microsoft Corporation, 2015).
Some computing infrastructures provide hyper-threading technologies. Hyper-threading
improves parallelization of computations (doing multiple tasks at once). For each core
that is physically present, the operating system addresses two logical cores, and shares the
workload among them when possible. The multi-core computer provides hyper-threading,
whereas it is deactivated on the cluster, and not available on the cloud.
4.1.4 Setting up the Parameters
The time limit for solving each instance is set to 12 hours whatever be the solver. If the
number of workers is strictly less than the number of cores (w < c), then there will always
be unused cores. Usually, one chooses w = c, so that all workers can work simultaneously.
On the multi-core computer, we use two workers per physical core (w = 2c) because hyperthreading is efficient as experimentally demonstrated in Appendix A. The target number
442

fiEmbarrassingly Parallel Search in CP

p? of subproblems depends linearly on the number w of workers (p? = 30  w) that allows
statistical balance of the workload without increasing too much the total overhead (Regin
et al., 2013).
In our experiments, the network and RAM memory loads are low in regards to the
capacities of the computing infrastructures. Indeed, the total number of messages depends
linearly of the number of workers and the number of subproblems. RAM is pre-allocated
if the computing infrastructure allows it. Last, workers almost produce no input/output or
disk access.
4.1.5 Metrics and Notations
Let t be the solving time (in seconds) of an algorithm and let su be the speedup of a parallel
algorithm. In the tables, a row gives the results obtained by different algorithms for a given
instance. For each row, the best solving times and speedups are indicated in bold. Dashes
indicate that the instance is not solved by the algorithm. Question marks indicate that
the speedup cannot be computed because the sequential solver does not solve the instance
within the time limit. Arithmetic means, abbreviated AM, are computed for solving times,
whereas geometrical means, abbreviated GM, are computed for speedups and efficiency.
Missing values, i.e. dashes and question marks, are ignored when computing statistics.
We also use a scoring procedure based on the Borda count voting system (Brams &
Fishburn, 2002). Each benchmark instance is treated like a voter who ranks the solvers.
Each solver scores points related to the number of solvers that it beats. More precisely, a
solver s scores points on problem P by comparing its performance with each other solver s0
as follows:
 if s gives a better answer than s0 , it scores 1 point;
 else if s did not answer or gives a worse answer than s0 , it scores 0 point;
 else scoring is based on execution time comparison (s and s0 give indistinguishable
answers).
Let t and t0 respectively denote the wall-clock times of solvers s and s0 for a given problems
instance. In case of indistinguishable answers, s scores f (t, t0 ) according to the Borda system
used in the Minizinc challenge. But, the function f does not capture users preferences very
well. Indeed, if the solver s solves n problems in 0.1 seconds and n others in 1000 seconds
whereas the solver s0 solves the first n problems in 0.2 seconds and the n others in 500
seconds, then both solvers obtain the same score n whereas most users would certainly
prefer s0 . So, we use another scoring function g(t, t0 ) in which g(t) can be interpreted as
the utility function for solving the problems instance within t seconds. The function g(t) is
strictly decreasing from 0.5 toward 0. The remaining points are shared using the function f .

f (t, t0 ) =

t0
t + t0

g(t, t0 ) = g(t)+(1g(t)g(t0 ))f (t, t0 )

g(t) =

1
2  (loga (t + 1) + 1)

Using the function g (a = 10) in the previous example, solvers s and s0 are respectively
scored 0.81  n and 1.19  n points.
443

fiMalapert, Regin, & Rezgui

4.2 Analysis of the Decomposition
In this section, we compare the quality and performance of the top-down and bottom-up
decomposition procedures introduced in Section 3.3.
4.2.1 Decomposition Quality
The top-down decomposition always returns the target number p? = 30  w of subproblems
whereas it is not guaranteed with the bottom-up decomposition. Figure 4(a) is a boxplot
of the number of subproblems per worker (p / w) with the bottom-up decomposition of
Choco2 depending on the number of workers. Boxplots display differences among populations without making any assumptions of the underlying statistical distribution: they are
non-parametric. The box in the boxplot spans the range of values from the first quartile
to the third quartile. The whiskers extend from each end of the box for a range equal to
1.5 times the interquartile range. Any points that lie outside the range of the whiskers are
considered outliers: they are drawn as individual circles.
For each number of workers w  {16, 80, 512}, the decompositions of xcsp instances
using one variable selection heuristic among lex, dom, dom/ddeg,dom/wdeg, dom/bwdeg,
and impact, combined with minVal, are considered. The bottom-up decomposition obtains
satisfying average performance (mostly between 10 and 100 subproblems per worker) while
respecting as much as possible the branching strategy. However, a few anomalies occur.
First, the decomposition is sensitive to the shape of the search tree. Sometimes, the model
only contains a few variables with large domains which forbid an accurate decomposition.
For instance, the first and second levels of the knights-80-5 search tree respectively contain
more than 6000 and 50000 nodes. There can also be a significant underestimation of the
tree size, especially if the branching has high arity. For instance, the width of the second
level of fapp07-0600-7 is estimated around 950 nodes while it contains more than 6000
nodes. On the contrary, an underestimation can occur if top nodes are eliminated from a
search tree with a low arity. Apart for a few underestimation, the decomposition is accurate
for search trees with low arity.
The top-down decomposition is accurate, but requires a fixed variable ordering, whereas
the bottom-up decomposition is less accurate, but handles any branching strategy.
1

0.8

100
instances (%)

subproblems per worker

1000

10

0.6

0.4

1
0.2

0.1
16

80

0

512

workers

Choco2 w=80
Choco2 w=512
Gecode w=80
Gecode w=512
0.1

1

10

100

time (s)

(a) Number of subproblems per worker.

(b) Decomposition time.

Figure 4: Analysis of the decomposition procedures (w = 16, 80, 512).
444

1000

fiEmbarrassingly Parallel Search in CP

4.2.2 Decomposition Time
Figure 4(b) gives the percentage of decompositions done within a given time. The Choco2
times are reported for all variable selection heuristics on xcsp instances. The Gecode times
are reported for lex on xcsp and fzn instances.
Because of the implementation differences, times reported for Choco2 and Gecode are
slightly different. Indeed, the decomposition time alone is given for Gecode. The Choco2
times take also into account the estimation time, the time taken by the foreman to fill
the queue of subproblems, and the time taken by workers to empty the queue. Let us
also remind that subproblems only become available after the top-down decomposition is
complete whereas they become available on the fly during the bottom-up decomposition.
In both cases, the reported time is a lower bound on the solving time.
The top-down decomposition is faster than the bottom-up decomposition because of its
parallelism. In fact, the Gecode decomposition is often faster than the estimation time alone.
One compelling example is the instance knights-80-5 which has the highest time (around
800 seconds) as well as a poor quality because the structure of the problem is unsuited
for the bottom-up decomposition: there are only a few variables with large domains (more
than 6000 values); there is almost no domain reduction in the top of the tree; and the
propagation is very long.
To conclude, the parallel top-down decomposition of Gecode is fast and accurate while
the bottom-up decomposition offers greater flexibility, but less robustness.
4.2.3 Influence of the Search Strategy
To analyze the influence of search strategies on the decomposition and the resolution, we
apply a variable selection heuristic during the decomposition (master) and another one
during the resolution (workers). Table 1 gives the solving times for the combinations of lex
or dom when solving the instances xcsp1. Results are not reported if there is no significant
differences among solving times. The choice of the variable selection heuristic is more critical
for the decomposition than for the resolution. Indeed, initial choices made by the branching
are both the least informed and the most important, as they lead to the largest subtrees and
the search can hardly recover from early mistakes. From now on, the master and workers
will use the same variable selection heuristic.

Instances

Worker
Master

costasArray-14
latinSquare-dg-8 all
lemma-100-9-mod
pigeons-14
quasigroup5-10
queenAttacking-6
squares-9-9

lex

dom

lex

dom

lex

dom

191.2
479.4
109.7
1003.8
182.2
872.4
126.8

240.9
323.8
125.9
956.3
125.3
598.3
1206.5

191.4
470.6
101.8
953.2
188.5
867.8
127.8

240.0
328.1
123.4
899.1
123.5
622.5
1213.0

Table 1: Solving times with different search strategies (Choco2, multi-core, w = 2c = 80).
445

fiMalapert, Regin, & Rezgui

4.3 Multi-core
In this section, we use parallel solvers based on Thread technologies to solve the instances
of xcsp1 or the n-queens problem using a multi-core computer. Let us recall that there is
two worker per physical core because hyper-threading is activated (w = 2c = 80). We show
that EPS frequently gives linear speedups, and outperforms the work stealing approach
proposed by Schulte (2000), and Nielsen (2006).
4.3.1 Performance Analysis
Table 2 gives the solving times and speedups of the parallel solvers using 80 workers for the
xcsp1 instances. Choco2 is tested with lex and dom whereas Gecode and OR-tools only
use lex. They are also compared to a work stealing approach denoted Gecode-WS (Schulte,
2000; Nielsen, 2006). First, implementations of EPS are faster and more efficient than the
work stealing. EPS often reaches linear speedups in the number of cores whereas it never
happens for the work stealing. Even worse, three instances are not solved within the 12
hours time limit using the work stealing whereas they are using the sequential solver.
For Choco2, dom is more efficient in parallel than lex but remains slightly slower in
average. Decomposition is a key of the bad performance on the instances knights-80-5 and
lemma-100-9-mod. As outlined before, the decomposition of knights-80-5 takes more than
1100 seconds and generates too much subproblems, which forbids any speedup. The issue
is lessened using the sequential decomposition of OR-tools and is resolved by the parallel
top-down decomposition of Gecode. Note also that the sequential solving times of OR-tools
and Gecode respectively are 20 and 40 times higher. Similarly, the long decomposition time
of Choco2 for lemma-100-9-mod leads to a low speedup. However, the moderate efficiency
of Choco2 and Gecode for squares-9-9 is not caused by the decomposition.
Gecode and OR-tools are often more efficient and faster than Choco2. The solvers show
different behaviors even when using the same variable selection heuristic because their
Instances

costasArray-14
knights-80-5
latinSquare-dg-8 all
lemma-100-9-mod
ortholatin-5
pigeons-14
quasigroup5-10
queenAttacking-6
series-14
squares-9-9
AM (t) or GM (su)
Borda score (rank)

Choco2-lex

Choco2-dom

Gecode

OR-tools

Gecode-WS

t

su

t

su

t

su

t

su

t

su

191.2
1138.3
479.4
109.7
248.7
1003.8
182.2
872.4
39.3
126.8

31.4
1.2
39.0
4.0
30.0
13.8
30.7
23.4
29.9
19.0

240.0
1133.1
328.1
123.4
249.9
899.1
123.5
622.5
39.3
1213.0

38.8
1.5
39.2
4.1
36.0
15.5
32.5
28.5
32.9
16.1

62.3
548.7
251.7
6.7
421.7
211.8
18.6
15899.1
11.3
17.9

19.1
37.6
42.0
10.1
13.5
39.1
26.4
?
34.2
18.4

50.9
2173.9
166.6
1.8
167.7
730.3
17.0

16.2
81.4

33.4
18.5
35.2
22.9
38.1
18.5
36.9

28.7
35.0

594.0

4488.5
3.0
2044.6

22.8

552.3
427.8

2.0

2.4
22.3
2.8

21.5

0.7
0.8

439.2

15.9

497.2

17.4

1745.0

24.0

378.4

28.7

1161.9

3.3

20.6 (3)

19.7 (4)

26.1 (1)

22.8 (2)

9.8 (5)

Table 2: Solving times and speedups (multi-core, w = 2c = 80). Gecode and OR-tools use
the lex heuristic.

446

fiEmbarrassingly Parallel Search in CP

propagation mechanisms and decompositions differ. Furthermore, the parallel top-down
decomposition of Gecode does not preserve the ordering of the subproblems in regard to
the sequential algorithm.
4.3.2 Variations About the N-Queens Problem
Here, we verify the effectiveness of EPS in classic CSP settings. We consider four models for
the well-known n-queens problem (n = 17). The n-queens puzzle is the problem of placing
n chess queens on an n  n chessboard so that no two queens threaten each other. Here, we
enumerate all solutions and the heuristics lex or dom are reasonable choices. The models are:
allDifferent global constraints which enforce arc-consistency (AC); allDifferent constraints which enforce bound-consistency (BC); arithmetic inequalities constraints (NEQ);
and a dedicated global constraint (JC) (Milano & Trick, 2004, ch. 3).
Table 3 gives the solving times and speedups of Choco2 with 80 workers when the
decomposition depth is either 3 or 4. What is striking for this result is that our splitting
technique gives excellent results, with a linear speedup for up to 40 processors with the
exception of the JC model. It is unfortunate since the JC model is clearly the best model
for the sequential solver. Here, dom is always a better choice than lex. The number of
subproblems for dom is the same whatever the model whereas the total number of nodes
changes. It indicates that the filtering is weak at the top of the search tree.
Most other works report good results, and often linear speedups for the n-queens problem. Bordeaux et al. (2009) reported linear speedups up to 30 cores for the 17 queens, but
no more improvement until 64 cores, whereas Machado et al. (2013) scales up to 512 workers using their hierarchical work stealing approach. Menouer and Le Cun (2014) reported
speedups around 8 using 12 cores for the 16 queens, and Pedro, Abreu, Pedro, and Abreu
(2010) reported speedups around 20 using 24 cores. Zoeteweij and Arbab (2004) reported
linear speedups up to 16 cores for the 15 queens, Pedro et al. (2010) reported a speedup
of 20 using 24 cores, So, the EPS efficiency is slightly above the average, because similar
results are observed with 15 and 16 queens.
The previous experimental setting is in favor of EPS because we are exploring a search
space exhaustively, and the problem is highly symmetric. Indeed, the variance of the subproblems solving time is low, especially with higher levels of consistency. Note that the
lower speedups of the JC model are probably not caused by load balancing issues because
the subproblems of the NEQ model have a greater mean and variance.

Model

lex

dom

d=3

BC
AC
NEQ
JC

d=4

d=3

d=4

t

su

t

su

t

su

t

su

838.8
3070.2
280.7
202.4

38.3
38.8
31.8
20.4

835.1
3038.9
241.9
196.9

38.5
39.3
36.9
21.0

640.4
2336.2
188.8
140.6

38.7
38.8
36.4
24.2

635.5
2314.7
181.1
148.8

39.0
39.2
37.9
22.9

Table 3: Variations about the 17 queens problem (Choco2, multi-core, w = 2c = 80).
447

fiMalapert, Regin, & Rezgui

Instances

lex
t

dom
su

t

dom/ddeg
su

t

dom/bwdeg

su

t

su

dom/wdeg
t

cc-15-15-2
1947.1 5.2 25701.7
?

 1524.9 4.6 2192.1
costasArray-14
500.4 12.4
641.9 12.1 895.3 8.6 4445.0 2.4
649.9
crossword-m11
506.1 4.4




492.0 1.9
204.6
crossword-m1c2
2376.9 0.6 1173.9 0.6 1316.2 0.5 1471.3 0.7 1611.9
fapp07-0600-7





 1069.5 2.1 2295.7
knights-20-9
359.3 17.2
353.9 17.3 357.4 14.9 5337.5 1.3
491.3
knights-25-9
855.3 17.8
840.6 18.0 986.1 13.3 13264.8 1.3 1645.2
knights-80-5
708.5 2.0
726.9 2.0 716.4 2.1 1829.5 0.9 1395.6
langford-3-17
38462.7
? 708.3 12.5 5701.6 2.5 6397.5 1.9 3062.2
40465.2
? 148.2 14.4 1541.9 2.2 1307.1 2.1
538.3
langford-4-18
langford-4-19

 747.2 16.9
 0.0 7280.1 2.3 2735.3
latinSquare-dg3
1161.7 14.3
903.2 12.2 812.0 14.4
416.9 4.2
294.8
lemma-100-9-mod
110.5 4.1
117.6 3.7 180.4 3.7
154.4 3.5
145.3
572.6 13.5
558.9 13.5 475.5 11.5
453.1 11.6 362.4
ortholatin-5
pigeons-14
1330.1 9.8 1492.6 8.3 1471.6 11.8 6331.1 2.6 2993.3
397.2 12.6 277.3 12.9 1156.6 3.6
733.5 5.2
451.5
quasigroup5-10
queenAttacking-6
2596.7 7.3 1411.8 10.6 4789.7 4.2 2891.0 1.9
706.4
queensKnights4





 1517.8 0.2 5209.5
ruler-70-12-a3
137.4 16.8 2410.6 17.5


51.5 2.4
42.8
6832.0 3.9 4021.1 4.7 7549.2 2.2 1412.0 0.9 1331.3
ruler-70-12-a4
scen11-f5





 38698.7 0.0

series-14
77.8 14.8
89.1 12.4 9828.6 3.4 1232.2 2.6
338.9
220.7 10.5 1987.4 7.2 129.7 9.4
697.2 2.2 115.9
squares-9-9
squaresUnsat5





 3766.1 1.2 3039.8
AM (t) or GM (su)
Borda score (rank)
1
4

5243.1

7.5

65.1 (6)

2332.2

8.6 2369.3

72.8 (5)

4.8

4282.3

49.6 (8)

1.6 1385.0

91.2 (2)

2

crossword-m1-words-05-06
crossword-m1c-words-vg7-7 ext
queensKnights-20-5-mul 5 squaresUnsat-19-19

3

su

impact
t

su

2.1 31596.1
?
11.4
652.2 10.5
5.1 179.5 2.9
0.6 4689.6 1.9
1.8


17.5 215.4 16.5
14.1 550.8 16.9
3.4
896.3 2.5
3.7 5995.6
?
4.8 1041.5 10.0
5.6 4778.9
?
11.3
28.7 5.0
3.5
226.8 2.5
13.7
641.7 10.6
5.1 3637.2 4.2
7.9
308.4 27.8
5.4 427.1 6.2
1.0


6.7
24.8 12.1
2.3 102.9 24.4
0.0


9.9
346.5 8.5
11.0
138.9 10.8
2.9


4.9

100.3 (1)

2823.9

7.7

81.4 (3)

latinSquare-dg-8 all

Table 4: Detailed speedups and solving times depending on the variable selection heuristics
(Choco2, data center, w = 16).

30

25

speedup

20

15

10

5

0

lex

dom

ddeg

bwdeg

wdeg

impact

variable selection

Figure 5: Speedups of the variable selection heuristics (Choco2, data center, w = 16).

448

fiEmbarrassingly Parallel Search in CP

4.4 Data Center
In this section, we study the influence of the search strategy on the solving times and
speedups, the scalability up to 512 workers, and compare EPS to a work stealing approach.
4.4.1 Influence of the Search Strategy
We study the performance of Choco2 using 16 workers for solving the xcsp instances using
the variable selection heuristics presented in Section 2.1. Figure 5 is a boxplot of the
speedups for each variable selection heuristic. First, speedups are lower for dom/bwdeg
because the decomposition is not effective. The binary branching states the constraint x = a
in the left branch and x 6= a in the right branch. So, the workload between the left and right
branches is imbalanced. In this case, only the positive decisions in the left branches should
be taken into account. Second, without learning (lex and dom), the parallel algorithm
is more efficient and robust in terms of speedup. With learning (dom/bwdeg, dom/wdeg,
impact), the parallel algorithm may explore a different search tree than the sequential
one. Indeed, the master only explores the top of the tree which changes the learning, and
possibly the branching decisions. The worker also learns only from their subproblems, and
not from the whole search tree. This frequently causes exploration overhead as for solving
queensKnights-20-5-mul (twelve times more nodes using dom/wdeg) or, sometimes gives a
super-linear speedup as for solving quasigroup5-10 (three times less nodes using impact).
Last, low speedups occur for all variable selection heuristics.
Table 4 gives solving times and speedups obtained for the different variable selection
heuristics. Borda scores are computed for Choco2 (Table 4) and Gecode (Table 5). First,
no variable selection heuristics strictly dominates the others either in sequential or parallel.
However, dom/wdeg is the most robust as outlined by the Borda scores. In fact, the variability of the solving times between the different heuristics is reduced by the parallelization,
but remains important. Second, in spite of low speedups, dom/bwdeg remains the second
best variable selection heuristic for parallel solving because it was the best one in sequential.
In average, using advanced variable selection heuristics such as dom/bwdeg, dom/wdeg, or
impact gives lower solving times than lex or dom in spite of lower speedups. It highlights the
fact that decomposition procedures should handle any branching strategy. In Section 4.6.1,
we will investigate the very low speedups for the instance crossword-m1c-words-vg7-7
that are not caused by the variable selection heuristics.
4.4.2 Scalability up to 512 Workers
Table 5 compares the Gecode implementations of EPS and work stealing (WS) for solving
xcsp instances using 16 or 512 workers. EPS is faster and more efficient than the work
stealing. With 16 workers, the work stealing is ranked last using the Borda score. With
512 workers, EPS is in average almost 10 times faster than the work stealing. It is also
more efficient because they both parallelize the same sequential solver. On the multi-core
machine, Gecode was faster than Choco2 on most instances of xcsp1. Here, the performance
of Gecode are more mitigated as outlined by the Borda scores. Five instances that are not
solved within the time limit by Gecode are not reported in Table 5. Six instances are
not solved with 16 workers whereas twelve instances were not solved with the sequential
solver. By way of comparison, only five instances are not solved by Choco2 using the lex
449

fiMalapert, Regin, & Rezgui

w = 16

Instances

w = 512

EPS

cc-15-15-2
costasArray-14
crossword-m1c1
crossword-m12
knights-20-9
knights-25-9
knights-80-5
langford-3-17
langford-4-18
langford-4-19
latinSquare-dg-8 all
lemma-100-9-mod
ortholatin-5
pigeons-14
quasigroup5-10
queenAttacking-6
ruler-70-12-a3
ruler-70-12-a4
series-14
squares-9-9

EPS

WS

t

su

t

su

t

su

t

su


64.4
240.6
171.7
5190.7
7462.3
1413.7
24351.5
3203.2
26871.2
613.5
3.4
309.5
383.3
27.1
42514.8
96.6
178.9
22.5
22.8


13.6
13.1
14.5
?
?
11.5
?
?
?
13.1
14.7
14.1
14.5
13.5
?
15.1
14.4
13.4
11.1


69.3
482.1
178.5
38347.4

8329.2
21252.3
25721.2

621.2
5.8
335.8
6128.9
33.7
37446.1
105.5
185.2
56.9
44.3


12.7
6.6
13.9
?

2.0
?
?

13.0
8.6
13.0
0.9
10.8
?
13.8
13.9
5.3
5.7


3.6
18.7
13.3
153.4
214.9
49.3
713.5
94.6
782.5
23.6
1.0
10.4
15.3
1.7
1283.9
3.7
6.0
1.1
1.3


243.8
168.6
187.3
?
?
329.8
?
?
?
341.7
51.4
422.0
363.1
211.7
?
389.3
429.5
264.0
191.7


17.7
83.1
57.8
3312.4

282.6
7443.5
5643.1

124.4
2.5
71.7
2320.2
9.8
9151.5
67.7
34.1
8.2
7.6


49.8
38.0
43.0
?

57.5
?
?

64.7
19.7
61.0
2.4
37.3
?
21.5
75.5
36.9
33.7

5954.8

13.5

8196.7

7.4

178.53

246.2

1684.6

33.5

AM (t) or GM (su)
Borda score (rank)
1

WS

76.9 (4)

crossword-m1-words-05-06

2

60.3 (7)

crossword-m1c-words-vg7-7 ext

Table 5: Speedups and solving times for xcsp (Gecode, lex, data center, w = 16 or 512).
heuristics whereas all instances are solved in sequential or parallel when using dom/wdeg or
dom/bwdeg. Once again, it highlights the importance of the search strategy.
Figure 6 is a boxplot of the speedups with different numbers of workers for solving fzn
instances. The median of speedups are around w2 in average and their dispersion remains
low.
512
256

speedup (su)

128
64
32
16
8
4
2

16

32

64

128

256

512

workers (w)

Figure 6: Scalability up to 512 workers (Gecode, lex, data center).
450

fiEmbarrassingly Parallel Search in CP

Instance

EPS

market split s5-02
market split s5-06
market split u5-09
pop stress 0600
nmseq 400
pop stress 0500
fillomino 18
steiner-triples 09
nmseq 300
golombruler 13
cc base mzn rnd test.11
ghoulomb 3-7-20
still life free 8x8
bacp-6
depot placement st70 6
open stacks 01 wbp 20 20 1
bacp-27
still life still life 9
talent scheduling alt film117
AM (t) or GM (su)

WS

t

su

t

su

467.1
452.7
468.1
874.8
342.4
433.2
160.2
108.8
114.5

24.3
24.4
24.4
10.8
8.5
10.1
13.9
17.2
6.6

658.6
650.7
609.2
2195.7
943.2
811.0
184.6
242.4
313.1

17.3
17.0
18.7
4.3
3.1
5.4
12.1
7.7
2.4

154.0
1143.6
618.2
931.2
400.8
433.9
302.7
260.2
189.0
22.7

20.6
7.3
6.8
9.6
16.4
18.3
17.6
16.4
16.9
74.0

210.4
2261.3
3366.0
1199.4
831.0
1172.5
374.1
548.4
196.8
110.5

15.1
3.7
1.2
7.5
7.9
6.8
14.3
7.8
16.2
15.2

414.7

15.1

888.4

7.7

Table 6: Solving times and speedups for fzn (Gecode, lex, cloud, w = 24).

4.5 Cloud Computing
EPS can be deployed on the Microsoft Azure cloud platform. The available computing
infrastructure is organized as follows: cluster nodes computes the application; one head
node manages the cluster nodes; and proxy nodes load-balances communication between
cluster nodes. On the contrary to a data center, cluster nodes may be far from each other
and communication time may take longer. Proxy nodes requires 2 cores and are managed
by the service provider. Here, 3 nodes of 8 cores with 56 GB of RAM memory provide 24
workers (cluster nodes) managed by MPI.
Table 6 compares the Gecode implementations of EPS and work stealing for solving the
fzn instances with 24 workers. Briefly, EPS is always faster than the work stealing, and
therefore, more efficient because they both parallelize the same sequential solver. The work
stealing suffers from a higher communication overhead in the cloud than in a data center.
Furthermore, the architecture of the computing infrastructure and the location of cluster
nodes are mostly unknown which forbid improvements of the work stealing such as those
proposed by Machado et al. (2013), or by Xie and Davenport (2010).
4.6 Embarrassingly Distributed Search
In this section, we transform with reasonable effort a parallel solver (EPS) into a distributed
parallel solver (EDPS) by using the batch scheduler OAR (Capit et al., 2005) provided by
451

fiMalapert, Regin, & Rezgui

the data center. In fact, the batch scheduler OAR plays the foreman. The parallel Choco2
solver is modified so that the workers write the subproblems into files instead of solving
them. Then, a script submits the jobs/subproblems to the OAR batch scheduler, waits
for their termination, and gathers the results. OAR schedules jobs on the cluster using
priority FIFO with backfilling and fair-share based priorities. Backfilling allows to start
lower priority jobs without delaying highest priority jobs whereas fair-share means that no
user/application is preferred in any way. The main drawback is that a new worker must
be created for each subproblem. Each worker process is allocated by OAR with predefined
resources. A worker is either a sequential (EDS) or a parallel solver (EDPS).
This approach offers a practical advantage for resource reservation in a data center.
Indeed, when asking for an MPI process, one has to wait until enough resources are available
before the process starts. Here, resources (cores or nodes) are nibbled as soon as they
become available which can drastically reduce the waiting time. Furthermore, it bypasses
limitations of the Threads Technology by allowing to use multiple nodes of the data center.
However, it clearly increases the recomputation overhead because, a worker solves a single
subproblem instead of multiple subproblems. So, the model creation and initial propagation
are realized more often. It also introduces a non-negligible submission overhead which is
the time taken to create and submit all jobs to the OAR batch scheduler.
4.6.1 Anomaly in crossword-m1c-words-vg7-7 ext
We investigate the very low speedups for solving the instance crossword-m1c-words-vg7-7
with any variable selection heuristic (see Table 4). We compare the results of the parallel
(EPS, w = 16) and distributed (EDS with sequential worker) algorithms for different decomposition depths (d = 1, 2, 3). Table 7 gives the solving times, speedups, and efficiencies.
The number of distinct cores used by the distributed algorithm is a bad estimator for computing efficiencies, because some of them are used only for a short period of time. Therefore,
the number c of cores used to compute the efficiency of EDS or EDPS is estimated as the
ratio of the total runtime over the wall-clock time.
First, the parallel algorithm is always slower than the sequential one. However, the
speedups of the distributed algorithms are significant even if they decrease quickly as the
decomposition depth increases. The fall of the efficiency shows that EDS is not scalable
with sequential workers. Indeed, the recomputation, and especially the submission overhead
become too important when the number of subproblems increases.
Second, the bad performance of the parallel algorithms are not caused by a statistically
imbalanced decomposition because we would observe similar performance for the distributed
algorithm. Profiling the parallel algorithm on this particular instances suggests that the bad
EDS
d

p

2
3
4

186
827
2935

EPS (w = 16)

t

su

eff

t

su

eff

73.0
229.0
797.0

10.2
3.3
1.1

0.435
0.128
0.039

1069.9
1074.2
1091.8

0.7
0.7
0.7

0.044
0.044
0.044

Table 7: EDS and EPS for the crossword instance (Choco2, dom, data center).
452

fiEmbarrassingly Parallel Search in CP

performance comes from the underlying solver itself. Indeed, the number of instructions is
similar for the sequential and parallel algorithms whereas the numbers of context switches,
cache references and cache misses increase considerably. In fact, the parallel algorithms
spent more than half of its time in some internal methods of extensional constraints, i.e. the
relation of the constraint is specified by listing its satisfying tuples. This issue occurred on all
computing infrastructure and for different Java virtual machines. Note that other instances
use extensional constraints, but they impose fewer consequences. This issue would not
happen with an MPI implementation because there is no shared memory. So, it advocates
for implementations of EPS based on MPI rather than on the Thread Technology.
4.6.2 Variations About the Golomb Ruler Problem
A Golomb ruler is a set of marks at integer positions along an imaginary ruler such that no
two pairs of marks are the same distance apart. The number of marks on the ruler is its
order, and the largest distance between two of its marks is its length. Here, we enumerate
the optimal rulers (minimal length for the specific number of marks) with a simple constraint
model inspired from the one by Galinier, Jaumard, Morales, and Pesant (2001) for which the
heuristics lex or dom are a reasonable choice. Table 8 gives the solving times, speedups, and
efficiencies for the parallel algorithm (w = 16), the distributed algorithm with sequential
workers (w = 1), and the distributed algorithms with parallel workers (w = 16 and the
worker decomposition depth is dw = 2) with different master decomposition depths d.
First, EPS obtains almost linear speedup if the decomposition depth is large enough.
Without surprise, speedups are lower if there are not enough subproblems. Second, the
distributed algorithm EDS with sequential workers is efficient only if the number of subproblems remains low. Otherwise, it can still give some speedups (dom), but wastes the
resources since the efficiency is very low. In fact, submitting too many jobs to the batch
scheduler (lex) lead to a high submission overhead (around 13 minutes) and globally degrades the performance. Finally, the distributed algorithms with parallel workers offer a
good trade-off between speedups and efficiencies because it allows to use many resources
while only submitting a few jobs thus reducing the submission and recomputation overheads. Note that EDS with d = 1 is not tested because it is roughly equivalent to EPS with
16 workers, and EDPS with d = 3 is not tested because the submission overhead becomes
too important.

EDPS (w = 16, dw = 2)

EDS
d

p

t

su

eff

t

su

lex

1
2
3

20
575
14223


769.0
17880.0


66.8
2.9


0.846
0.005

572.0
497.0


89.7
103.3


dom

1
2
3

20
222
5333


2394.0
3018.0


50,5
40,0


0,989
0,146

1538.0
366.0


78,6
330,2


EPS (w = 16)
t

su

eff

0.968
0.232


11141.7
4084.2
3502.6

4.6
12.6
14.7

0.288
0.786
0.916

0,935
0,742


28299,9
9703,6
8266,6

4,3
12,4
14,6

0,267
0,778
0,914

eff

Table 8: EDS and EPS for Golomb Ruler with 14 marks (Choco2, data center).
453

fiMalapert, Regin, & Rezgui

Most other parallel approaches reported good performance for the Golomb ruler problem. For instance, Michel et al. (2009), and Chu et al. (2009) respectively reported linear
speedups for 4 and 8 workers. EDS is more efficient than the work stealing proposed
by Menouer and Le Cun (2014) using 48 workers for the ruler with 13 marks and as efficient
as the selfsplit by Fischetti et al. (2014) using 64 workers for the ruler with 14 marks.
Last, we enumerated optimal Golomb Rulers with 15 and 16 marks using EDPS. The
Master and workers use the lex heuristic. The master decomposition depth d is equal to
2 that generates around 800 hundreds subproblems. There are 16 parallel workers with a
decomposition depth dw equal to 2. With this settings, we used more than 700 cores of the
data center during the solving process. So, it bypasses the limitations on the number of
cores used by MPI imposed by the administrator. Furthermore, the solving process starts
immediately because cores are grabbed as soon as they become available whereas a MPI
process waits that enough cores becomes simultaneously available. Enumerating optimal
rulers with 15 and 16 marks respectively took 1422 and 5246 seconds. To our knowledge,
this is the first time where a constraint solver finds these rulers, and furthermore in a reasonable amount of time. However, these optimal rulers have been discovered via an exhaustive
computer search (Shearer, 1990). More recently, Distributed Computing Technologies Inc
(20) found optimum rulers up to 26 marks. Beside, plane construction (Atkinson & Hassenklover, 1984) allows to find larger optimal rulers.
4.7 Comparison With Portfolios
Portfolio approaches exploit the variability of performance that is observed between several
solvers, or several parameter settings for the same solver. We use 4 portfolios. The portfolio
CPHydra (OMahony et al., 2008) uses features selection on the top of the solvers Mistral,
Gecode, and Choco2. CPHydra uses case-based reasoning to determine how to solve an
unseen problem instance by exploiting a case base of problem solving experience. It aims
to find a feasible solution within 30 minutes, it does not handle optimization or all solution problems and the time limit is hard-coded. The other static and fixed-size portfolios
(Choco2, CAG, OR-tools) use different variable selection heuristics (see Section 2.1) as well
as randomization and restarts. Details about Choco2 and CAG can be found in (Malapert &
Lecoutre, 2014). The CAG portfolio extends the Choco2 portfolio by also using the solvers
AbsCon and Gecode. So, CAG always produces better results than Choco2. The OR-tools
portfolio was the gold medal of the Minizinc challenge 2013 and 2014. It can seem unfair
to compare parallel solvers and portfolios using different numbers of workers, but designing
scalable portfolio (up to 512 workers) is a difficult task and almost no implementation is
publicly available.
Table 9 gives the solving times of EPS and portfolios for solving the xcsp instances on the
data center. First, CPHydra with 16 workers only solves 2 among 16 unsatisfiable instances
(cc-15-15-2 and pigeons-14), but in less than 2 seconds whereas these are difficult for
all other approaches. OR-tools is the second less efficient approach because it solves fewer
problems and often takes longer as confirmed by its low Borda score. The parallel Choco2
using dom/wdeg is better in average than the Choco2 portfolio even if the portfolio solves a
few instances much faster such as scen11-f5 or queensKnights-20-5-mul. In this case, the
diversification provided by the portfolio outperforms the speedups offered by the parallel
454

fiEmbarrassingly Parallel Search in CP

Instances

EPS
Choco2

cc-15-15-2
costasArray-14
crossword-m1-words-05-06
crossword-m1c-words-vg7-7 ext
fapp07-0600-7
knights-20-9
knights-25-9
knights-80-5
langford-3-17
langford-4-18
langford-4-19
latinSquare-dg-8 all
lemma-100-9-mod
ortholatin-5
pigeons-14
quasigroup5-10
queenAttacking-6
queensKnights-20-5-mul
ruler-70-12-a3
ruler-70-12-a4
scen11-f5
series-14
squares-9-9
squaresUnsat-19-19
Arithmetic mean
Borda score (rank)

Portfolio

Gecode

Choco2

CAG

OR-tools

w = 16

w = 16

w = 512

w = 14

w = 23

w = 16

2192.1
649.9
204.6
1611.9
2295.7
491.3
1645.2
1395.6
3062.2
538.3
2735.3
294.8
145.3
362.4
2993.3
451.5
706.4
5209.5
42.8
1331.3

338.9
115.9
3039.8


64.4
240.6
171.7

5190.7
7462.3
1413.7
24351.5
3203.2
26871.2
613.5
3.4
309.5
383.3
27.1
42514.8

96.6
178.9

22.5
22.8



3.6
18.7
13.3

153.4
214.9
49.3
713.5
94.6
782.5
23.6
1.0
10.4
15.3
1.7
1283.9

3.7
6.0

1.1
1.3


1102.6
6180.8
512.3
721.2
37.9
3553.9
9324.8
1451.5
8884.7
2126.0
12640.2
65.1
435.3
4881.2
12336.9
3545.8
2644.5
235.3
123.5
1250.2
45.3
1108.3
1223.7
4621.1

3.5
879.4
512.3
721.2
3.2
0.8
1.1
301.6
8884.7
2126.0
12640.2
36.4
50.1
4371.0
5564.5
364.3
2644.5
1.0
123.5
1250.2
8.5
302.1
254.3
4621.1

1070.0
1368.8
22678.1
13157.2



32602.6



4599.8
38.2
4438.7
12279.6
546.0


8763.1


416.2
138.3


1385.0

5954.8

178.5

3293.8

1902.7

7853.6

65.0 (3)

52.2 (5)

77.1 (1)

57.0 (4)

72.8 (2)

20.0 (6)

Table 9: Solving times of EPS and portfolio (data center).
B&B algorithm. This is emphasized for the CAG portfolio that solves all instances and
obtains several of the best solving times. The parallel Gecode with 16 workers is often slower
and less robust than the portfolios Choco2 and CAG. However, increasing the number of
workers to 512 clearly makes it the fastest solver, but still less robust because five instances
are not solved within the time limit.
To conclude, Choco2 and CAG portfolios are more robust thanks to the inherent diversification, but the solving times vary more from one instance to another. With 16 workers,
implementations of EPS outperform the CPHydra and OR-tools portfolio, are competitive
with the Choco2 portfolio, and are slightly dominated by the CAG portfolio. In fact, the
good scaling of EPS is a key to beat the portfolios.

5. Conclusion
We have introduced an Embarrassingly Parallel Search (EPS) method for solving constraint
satisfaction problems and constraint optimization problems. This approach has several
advantages. First, it is an efficient method which matches or even outperforms state-of-the455

fiMalapert, Regin, & Rezgui

art algorithms on a number of problems using various computing infrastructures. Second,
it involves almost no communication or synchronization and mostly relies on the underlying
sequential solver so that the implementation and debugging is made easier. Last, the
simplicity of the method allows to propose many variants adapted to specific applications
or computing infrastructures. Moreover, under certain restrictions, the parallel algorithm
can be deterministic, and even mimic the sequential algorithm which is very important in
practice either in production or for debugging.
There are several interesting perspectives around EPS. First, it can be modified in order
to provide diversification and to learn useful information when solving subproblems. For
instance, it can easily be combined with a portfolio approach in which subproblems can be
solved by several search strategies. Second, thanks to its simplicity, the simplest variants
of EPS could be implemented as meta-searches (Rendl, Guns, Stuckey, & Tack, 2015), and
would offer a convenient way to parallelize applications with a satisfactory efficiency. Last,
another perspective is to predict the solution time of a large combinatorial problem, based
on known solution times of a small set of subproblems based on statistical or machine
learning approaches.

Acknowledgments
We would like to thank very much Christophe Lecoutre, Laurent Perron, Youssef Hamadi,
Carine Fedele, Bertrand Lecun and Tarek Menouer for their comments and advices which
helped to improve the paper. This work has been supported by both CNRS and OSEO
(BPI France) within the ISI project Pajero. This work was granted access to the HPC and
visualization resources of Centre de Calcul Interactif hosted by Universite Nice Sophia
Antipolis, and also to the Microsoft Azure Cloud. We also wish to thank the anonymous
referees for their comments.

Appendix A. Efficiency of Hyper-Threading
In this section, we show that hyper-threading technology improves the efficiency of EPS
for solving the instances of xcsp1 on a multi-core computer. Figure 7 is a boxplot of the
speedups provided by the hyper-threading for each parallel solver among Choco2, Gecode,
OR-tools. Here, the speedups indicate how many times a parallel solver using 80 workers
(w = 2c) is faster than one using 40 workers (w = c). The maximum speedup according to
Amdahls law is 2.
Choco2 is tested with lex and dom whereas Gecode and OR-tools only use lex. They
are also compared to a work stealing approach proposed by Schulte (2000) and denoted
Gecode-WS. Hyper-threading clearly improves the parallel efficiency of EPS whereas the
performance of the work stealing roughly remains unchanged. It is interesting because
EPS has a very high CPU demand and resources of each physical core are shared by
its two logical cores. Indeed, the performance of hyper-threading are known to be very
application-dependent. With the exception of lemma-100-9-mod and squares-9-9, Choco2
and OR-tools are faster with 80 workers. For lemma-100-9-mod, the Choco2 decomposition
for 80 workers takes longer and generates too many subproblems. The instance is solved
456

fiEmbarrassingly Parallel Search in CP

hyperthreading speedup

2

1

0.5

Choco2-lex

Choco2-dom

Gecode

OR-tools

Gecode-WS

Figure 7: Speedups provided by hyper-threading (multi-core, w = 40, 80).
easily by OR-tools (less than two seconds) and it becomes difficult to improve its efficiency. For squares-9-9, the decomposition changes according to the number of workers,
but it cannot explain why hyper-threading does not improve EPS. The parallel efficiency of
Gecode is reduced for multiple instances and the interest of hyper-threading is less obvious
than for Choco2 or OR-tools. To conclude, hyper-threading globally improves the efficiency
of EPS while it has a limited interest on the work stealing.

References
Almasi, G. S., & Gottlieb, A. (1989). Highly Parallel Computing. Benjamin-Cummings
Publishing Co., Inc., Redwood City, CA, USA.
Amadini, R., Gabbrielli, M., & Mauro, J. (2013). An Empirical Evaluation of Portfolios
Approaches for Solving CSPs In Gomes, C., & Sellmann, M.Eds., Integration of
AI and OR Techniques in Constraint Programming for Combinatorial Optimization
Problems, Vol. 7874 of Lecture Notes in Computer Science, pp. 316324. Springer
Berlin Heidelberg.
Amdahl, G. (1967). Validity of the Single Processor Approach to Achieving Large Scale
Computing Capabilities In Proceedings of the April 18-20, 1967, Spring Joint Computer Conference, AFIPS 67, pp. 483485, New York, NY, USA. ACM.
Anderson, D. P., Cobb, J., Korpela, E., Lebofsky, M., & Werthimer, D. (2002). Seti@home:
An experiment in public-resource computing Commun. ACM, 45 (11), 5661.
Atkinson, M. D., & Hassenklover, A. (1984). Sets of Integers with Distinct Differences Tech.
Rep. SCS-TR-63, School of Computer Science, Carlton University, Ottawa Ontario,
Canada.
Bader, D., Hart, W., & Phillips, C. (2005). Parallel Algorithm Design for Branch and
Bound In G, H.Ed., Tutorials on Emerging Methodologies and Applications in Operations Research, Vol. 76 of International Series in Operations Research & Management
Science, pp. 51544. Springer New York.
457

fiMalapert, Regin, & Rezgui

Barney, B., & Livermore, L. (2016). Introduction to Parallel Computing
computing.llnl.gov/tutorials/parallel comp/.

https://

Bauer, M. A. (2007). High performance computing: The software challenges In Proceedings
of the 2007 International Workshop on Parallel Symbolic Computation, PASCO 07,
pp. 1112, New York, NY, USA. ACM.
Beck, C., Prosser, P., & Wallace, R. (2005). Trying Again to Fail-First In Recent Advances
in Constraints, pp. 4155. Springer Berlin Heidelberg.
Bordeaux, L., Hamadi, Y., & Samulowitz, H. (2009). Experiments with Massively Parallel
Constraint Solving. In Boutilier (Boutilier, 2009), pp. 443448.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004). Boosting Systematic Search
by Weighting Constraints In Proceedings of the 16th Eureopean Conference on Artificial Intelligence, ECAI2004, including Prestigious Applicants of Intelligent Systems,
PAIS, pp. 146150.
Boutilier, C.Ed.. (2009). IJCAI 2009, Proceedings of the 21st International Joint Conference
on Artificial Intelligence, Pasadena, California, USA, July 11-17.
Brams, S. J., & Fishburn, P. C. (2002). Voting procedures In Arrow, K. J., Sen, A. K.,
& Suzumura, K.Eds., Handbook of Social Choice and Welfare, Vol. 1 of Handbook of
Social Choice and Welfare, chap. 4, pp. 173236. Elsevier.
Budiu, M., Delling, D., & Werneck, R. (2011). DryadOpt: Branch-and-bound on distributed
data-parallel execution engines In Parallel and Distributed Processing Symposium
(IPDPS), 2011 IEEE International, pp. 12781289. IEEE.
Burton, F. W., & Sleep, M. R. (1981). Executing Functional Programs on a Virtual Tree
of Processors In Proceedings of the 1981 Conference on Functional Programming
Languages and Computer Architecture, FPCA 81, pp. 187194, New York, NY, USA.
ACM.
Capit, N., Da Costa, G., Georgiou, Y., Huard, G., Martin, C., Mounie, G., Neyron, P., &
Richard, O. (2005). A Batch Scheduler with High Level Components In Proceedings
of the Fifth IEEE International Symposium on Cluster Computing and the Grid (CCGrid05) - Volume 2 - Volume 02, CCGRID 05, pp. 776783, Washington, DC, USA.
IEEE Computer Society.
Choco, T. (2010). Choco: an open source java constraint programming library Ecole des
Mines de Nantes, Research report, 1, 1002.
Chong, Y. L., & Hamadi, Y. (2006). Distributed Log-Based Reconciliation In Proceedings
of the 2006 Conference on ECAI 2006: 17th European Conference on Artificial Intelligence August 29  September 1, 2006, Riva Del Garda, Italy, pp. 108112, Amsterdam,
The Netherlands, The Netherlands. IOS Press.
Chu, G., Schulte, C., & Stuckey, P. J. (2009). Confidence-Based Work Stealing in Parallel Constraint Programming In Gent, I. P.Ed., CP, Vol. 5732 of Lecture Notes in
Computer Science, pp. 226241. Springer.
Chu, G., Stuckey, P. J., & Harwood, A. (2008). PMiniSAT: A Parallelization of MiniSAT
2.0 Tech. Rep., NICTA : National ICT Australia.
458

fiEmbarrassingly Parallel Search in CP

Cire, A. A., Kadioglu, S., & Sellmann, M. (2014). Parallel Restarted Search In Proceedings
of the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI14, pp. 842
848. AAAI Press.
Cornuejols, G., Karamanov, M., & Li, Y. (2006). Early Estimates of the Size of Branchand-Bound Trees INFORMS Journal on Computing, 18, 8696.
Crainic, T. G., Le Cun, B., & Roucairol, C. (2006). Parallel branch-and-bound algorithms
Parallel combinatorial optimization, 1, 128.
De Kergommeaux, J. C., & Codognet, P. (1994). Parallel logic programming systems ACM
Computing Surveys (CSUR), 26 (3), 295336.
Distributed Computing Technologies Inc (20). The Distributed.net home page http://
www.distributed.net/.
Een, N., & Sorensson, N. (2005). MiniSat: A SAT solver with conflict-clause minimization
Sat, 5, 1.
Ezzahir, R., Bessiere, C., Belaissaoui, M., & Bouyakhf, E. H. (2007). DisChoco: A platform
for distributed constraint programming In DCR07: Eighth International Workshop
on Distributed Constraint Reasoning - In conjunction with IJCAI07, pp. 1621, Hyderabad, India.
Fischetti, M., Monaci, M., & Salvagnin, D. (2014). Self-splitting of workload in parallel
computation In Simonis, H.Ed., Integration of AI and OR Techniques in Constraint
Programming: 11th International Conference, CPAIOR 2014, Cork, Ireland, May 1923, 2014. Proceedings, pp. 394404, Cham. Springer International Publishing.
Gabriel, E., Fagg, G., Bosilca, G., Angskun, T., Dongarra, J., Squyres, J., Sahay, V., Kambadur, P., Barrett, B., Lumsdaine, A., et al. (2004). Open MPI: Goals, Concept,
and Design of a next generation MPI implementation In Recent Advances in Parallel
Virtual Machine and Message Passing Interface, pp. 97104. Springer.
Galea, Fran c., & Le Cun, B. (2007). Bob++ : a Framework for Exact Combinatorial
Optimization Methods on Parallel Machines In International Conference High Performance Computing & Simulation 2007 (HPCS07) and in conjunction with The 21st
European Conference on Modeling and Simulation (ECMS 2007), pp. 779785.
Galinier, P., Jaumard, B., Morales, R., & Pesant, G. (2001). A Constraint-Based Approach
to the Golomb Ruler Problem In 3rd International Workshop on integration of AI
and OR techniques.
Gendron, B., & Crainic, T. G. (1994). Parallel branch-and-bound algorithms: Survey and
synthesis Operations research, 42 (6), 10421066.
Gent, I., & Walsh, T. (1999). CSPLIB: A Benchmark Library for Constraints In Proceedings of the 5th International Conference on Principles and Practice of Constraint
Programming, CP 99, pp. 480481.
Gomes, C., & Selman, B. (1997). Algorithm Portfolio Design: Theory vs. Practice In
Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence, pp.
190197.
459

fiMalapert, Regin, & Rezgui

Gomes, C., & Selman, B. (1999). Search strategies for hybrid search spaces In Tools with
Artificial Intelligence, 1999. Proceedings. 11th IEEE International Conference, pp.
359364. IEEE.
Gomes, C., & Selman, B. (2000). Hybrid Search Strategies For Heterogeneous Search Spaces
International Journal on Artificial Intelligence Tools, 09, 4557.
Gomes, C., & Selman, B. (2001). Algorithm Portfolios Artificial Intelligence, 126, 4362.
Gropp, W., & Lusk, E. (1993). The MPI communication library: its design and a portable
implementation In Scalable Parallel Libraries Conference, 1993., Proceedings of the,
pp. 160165. IEEE.
Gupta, G., Pontelli, E., Ali, K. A., Carlsson, M., & Hermenegildo, M. V. (2001). Parallel
execution of prolog programs: a survey ACM Transactions on Programming Languages
and Systems (TOPLAS), 23 (4), 472602.
Halstead, R. (1984). Implementation of Multilisp: Lisp on a Multiprocessor In Proceedings
of the 1984 ACM Symposium on LISP and Functional Programming, LFP 84, pp.
917, New York, NY, USA. ACM.
Hamadi, Y. (2002). Optimal Distributed Arc-Consistency Constraints, 7, 367385.
Hamadi, Y., Jabbour, S., & Sais, L. (2008). ManySAT: a Parallel SAT Solver. Journal on
Satisfiability, Boolean Modeling and Computation, 6 (4), 245262.
Haralick, R., & Elliott, G. (1980). Increasing Tree Search Efficiency for Constraint Satisfaction Problems Artificial intelligence, 14 (3), 263313.
Harvey, W. D., & Ginsberg, M. L. (1995). Limited Discrepancy Search In Proceedings of
the Fourteenth International Joint Conference on Artificial Intelligence, IJCAI 95,
Montreal Quebec, Canada, August 20-25 1995, 2 Volumes, pp. 607615.
Heule, M. J., Kullmann, O., Wieringa, S., & Biere, A. (2012). Cube and conquer: Guiding
CDCL SAT solvers by lookaheads In Hardware and Software: Verification and Testing,
pp. 5065. Springer.
Hirayama, K., & Yokoo, M. (1997). Distributed Partial Constraint Satisfaction Problem In
Principles and Practice of Constraint Programming-CP97, pp. 222236. Springer.
Hyde, P. (1999). Java thread programming, Vol. 1. Sams.
Intel Corporation (2015). Intel MPI Library https://software.intel.com/en-us/intel
-mpi-library.
Jaffar, J., Santosa, A. E., Yap, R. H. C., & Zhu, K. Q. (2004). Scalable Distributed DepthFirst Search with Greedy Work Stealing In 16th IEEE International Conference on
Tools with Artificial Intelligence, pp. 98103. IEEE Computer Society.
Kale, L., & Krishnan, S. (1993). CHARM++: a portable concurrent object oriented system
based on C++, Vol. 28. ACM.
Kasif, S. (1990). On the Parallel Complexity of Discrete Relaxation in Constraint Satisfaction networks Artificial Intelligence, 45, 275286.
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C., & Selman, B. (2002). Dynamic Restart Policies
18th National Conference on Artificial Intelligence AAAI/IAAI, 97, 674681.
460

fiEmbarrassingly Parallel Search in CP

Kjellerstrand, H. (2014). Hakan Kjellerstrands Blog http://www.hakank.org/.
Kleiman, S., Shah, D., & Smaalders, B. (1996). Programming with threads. Sun Soft Press.
Korf, R. E., & Schreiber, E. L. (2013). Optimally Scheduling Small Numbers of Identical
Parallel Machines In Borrajo, D., Kambhampati, S., Oddi, A., & Fratini, S.Eds.,
ICAPS. AAAI.
Krishna, J., Balaji, P., Lusk, E., Thakur, R., & Tiller, F. (2010). Implementing MPI on
Windows: Comparison with Common Approaches on Unix In Recent Advances in
the Message Passing Interface, Vol. 6305 of Lecture Notes in Computer Science, pp.
160169. Springer Berlin Heidelberg.
Lai, T.-H., & Sahni, S. (1984). Anomalies in Parallel Branch-and-bound Algorithms Commun. ACM, 27 (6), 594602.
Lantz, E. (2008). Windows HPC Server : Using Microsoft Message Passing Interface (MSMPI).
Le Cun, B., Menouer, T., & Vander-Swalmen, P. (2007). Bobpp http://forge.prism
.uvsq.fr/projects/bobpp.
Leaute, T., Ottens, B., & Szymanek, R. (2009). FRODO 2.0: An open-source framework
for distributed constraint optimization. In Boutilier (Boutilier, 2009), pp. 160164.
Leiserson, C. E. (2010). The Cilk++ concurrency platform The Journal of Supercomputing,
51 (3), 244257.
Lester, B. (1993). The art of parallel programming. Prentice Hall Englewood Cliffs, NJ.
Li, H. (2009). Introducing Windows Azure. Apress, Berkely, CA, USA.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal Speedup of Las Vegas Algorithms
Inf. Process. Lett., 47, 173180.
Machado, R., Pedro, V., & Abreu, S. (2013). On the Scalability of Constraint Programming
on Hierarchical Multiprocessor Systems In ICPP, pp. 530535. IEEE.
Malapert, A., & Lecoutre, C. (2014). A propos de la bibliotheque de modeles XCSP
In 10emes Journees Francophones de Programmation par Contraintes(JFPC15),
Angers, France.
Mattson, T., Sanders, B., & Massingill, B. (2004). Patterns for Parallel Programming (First
ed.). Addison-Wesley Professional.
Menouer, T., & Le Cun, B. (2013). Anticipated Dynamic Load Balancing Strategy to Parallelize Constraint Programming Search In 2013 IEEE 27th International Symposium
on Parallel and Distributed Processing Workshops and PhD Forum, pp. 17711777.
Menouer, T., & Le Cun, B. (2014). Adaptive N To P Portfolio for Solving Constraint
Programming Problems on Top of the Parallel Bobpp Framework In 2014 IEEE 28th
International Symposium on Parallel and Distributed Processing Workshops and PhD
Forum.
Michel, L., See, A., & Hentenryck, P. V. (2009). Transparent Parallelization of Constraint
Programming INFORMS Journal on Computing, 21, 363382.
461

fiMalapert, Regin, & Rezgui

Microsoft Corporation (2015). Microsoft HPC Pack 2012 R2 and HPC Pack 2012 http://
technet.microsoft.com/en-us/library/jj899572.aspx.
Milano, M., & Trick, M. (2004). Constraint and Integer Programming: Toward a Unified
Methodology. Springer US, Boston, MA.
Moisan, T., Gaudreault, J., & Quimper, C.-G. (2013). Parallel Discrepancy-Based Search
In Principles and Practice of Constraint Programming, Vol. 8124 of Lecture Notes in
Computer Science, pp. 3046. Springer Berlin Heidelberg.
Moisan, T., Quimper, C.-G., & Gaudreault, J. (2014). Parallel Depth-bounded Discrepancy
Search In Simonis, H.Ed., Integration of AI and OR Techniques in Constraint Programming: 11th International Conference, CPAIOR 2014, Cork, Ireland, May 19-23,
2014. Proceedings, pp. 377393, Cham. Springer International Publishing.
MPI-CH Team (2015). High-Performance Portable MPI http://www.mpich.org/.
Mueller, F., et al. (1993). A Library Implementation of POSIX Threads under UNIX. In
USENIX Winter, pp. 2942.
Nguyen, T., & Deville, Y. (1998). A distributed arc-consistency algorithm Science of
Computer Programming, 30 (12), 227  250. Concurrent Constraint Programming.
NICTA Optimisation Research Group (2012). MiniZinc and FlatZinc http://www.g12
.csse.unimelb.edu.au/minizinc/.
Nielsen, M. (2006). Parallel Search in Gecode Masters thesis, KTH Royal Institute of
Technology.
OMahony, E., Hebrard, E., Holland, A., Nugent, C., & OSullivan, B. (2008). Using casebased reasoning in an algorithm portfolio for constraint solving In Irish Conference
on Artificial Intelligence and Cognitive Science, pp. 210216.
Pedro, V., Abreu, S., Pedro, V., & Abreu, S. (2010). Distributed Work Stealing for Constraint Solving CoRR, abs/1009.3800, 118.
Perron, L. (1999). Search Procedures and Parallelism in Constraint Programming In Principles and Practice of Constraint Programming  CP99: 5th International Conference,
CP99, Alexandria, VA, USA, October 11-14, 1999. Proceedings, pp. 346360, Berlin,
Heidelberg. Springer Berlin Heidelberg.
Perron, L., Nikolaj, V. O., & Vincent, F. (2012). Or-Tools Tech. Rep., Google.
Pruul, E., Nemhauser, G., & Rushmeier, R. (1988). Branch-and-bound and Parallel Computation: A historical note Operations Research Letters, 7, 6569.
Refalo, P. (2004). Impact-Based Search Strategies for Constraint Programming In Wallace,
M.Ed., Principles and Practice of Constraint Programming, 10th International Conference, CP 2004, Toronto, Canada, Vol. 3258 of Lecture Notes in Computer Science,
pp. 557571. Springer.
Regin, J.-C., Rezgui, M., & Malapert, A. (2013). Embarrassingly Parallel Search In Principles and Practice of Constraint Programming: 19th International Conference, CP
2013, Uppsala, Sweden, September 16-20, 2013. Proceedings, pp. 596610. Springer
Berlin Heidelberg, Berlin, Heidelberg.
462

fiEmbarrassingly Parallel Search in CP

Regin, J.-C., Rezgui, M., & Malapert, A. (2014). Improvement of the Embarrassingly Parallel Search for Data Centers In OSullivan, B.Ed., Principles and Practice of Constraint
Programming: 20th International Conference, CP 2014, Lyon, France, September 812, 2014. Proceedings, Vol. 8656 of Lecture Notes in Computer Science, pp. 622635.
Springer International Publishing, Cham.
Rendl, A., Guns, T., Stuckey, P., & Tack, G. (2015). MiniSearch: A Solver-Independent
Meta-Search Language for MiniZinc In Pesant, G., Pesant, G., & Pesant, G.Eds.,
Principles and Practice of Constraint Programming: 21st International Conference,
CP 2015, Cork, Ireland, August 31  September 4, 2015, Proceedings, Vol. 9255 of
Lecture Notes in Computer Science, pp. 376392. Springer International Publishing,
Cham.
Rezgui, M., Regin, J.-C., & Malapert, A. (2014). Using Cloud Computing for Solving
Constraint Programming Problems In First Workshop on Cloud Computing and Optimization, a conference workshop of CP 2014, Lyon, France.
Rolf, C. C., & Kuchcinski, K. (2009). Parallel Consistency in Constraint Programming
PDPTA 09: The 2009 International Conference on Parallel and Distributed Processing Techniques and Applications, 2, 638644.
Rossi, F., Van Beek, P., & Walsh, T.Eds.. (2006). Handbook of Constraint Programming.
Elsevier.
Roussel, O., & Lecoutre, C. (2008). Xml representation of constraint networks format
http://www.cril.univ-artois.fr/CPAI08/XCSP2 1Competition.pdf.
Schulte, C. (2000). Parallel Search Made Simple In Proceedings of TRICS: Techniques
foR Implementing Constraint programming Systems, a post-conference workshop of
CP 2000, pp. 4157, Singapore.
Schulte, C. (2006). Gecode: Generic Constraint Development Environment http://www
.gecode.org/.
Shearer, J. B. (1990). Some New Optimum Golomb Rulers IEEE Trans. Inf. Theor., 36 (1),
183184.
Stephan, K., & Michael, K. (2011). SArTagnan - A parallel portfolio SAT solver with
lockless physical clause sharing In Pragmatics of SAT.
Sutter, H., & Larus, J. (2005). The free lunch is over: A fundamental turn toward toward
Concurrency Dr. Dobbs Journal, 30, 202210.
Van Der Tak, P., Heule, M. J., & Biere, A. (2012). Concurrent cube-and-conquer In Theory
and Applications of Satisfiability TestingSAT 2012, pp. 475476. Springer.
Vidal, V., Bordeaux, L., & Hamadi, Y. (2010). Adaptive K-Parallel Best-First Search:
A Simple but Efficient Algorithm for Multi-Core Domain-Independent Planning In
Proceedings of the Third International Symposium on Combinatorial Search. AAAI
Press.
Wahbi, M., Ezzahir, R., Bessiere, C., & Bouyakhf, E.-H. (2011). DisChoco 2: A Platform
for Distributed Constraint Reasoning In Proceedings of the IJCAI11 workshop on
Distributed Constraint Reasoning, DCR11, pp. 112121, Barcelona, Catalonia, Spain.
463

fiMalapert, Regin, & Rezgui

Wilkinson, B., & Allen, M. (2005). Parallel Programming: Techniques and Application
Using Networked Workstations and Parallel Computers (2nd ed.). Prentice-Hall Inc.
Xie, F., & Davenport, A. (2010). Massively Parallel Constraint Programming for Supercomputers: Challenges and Initial Results In Integration of AI and OR Techniques
in Constraint Programming for Combinatorial Optimization Problems: 7th International Conference, CPAIOR 2010, Bologna, Italy, June 14-18, 2010. Proceedings, Vol.
6140 of Lecture Notes in Computer Science, pp. 334338, Berlin, Heidelberg. Springer
Berlin Heidelberg.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically Configuring Algorithms for Portfolio-Based Selection In AAAI Conference on Artificial Intelligence,
Vol. 10, pp. 210216.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based Algorithm Selection for SAT Journal of Artificial Intelligence Research, 32, 565606.
Yokoo, M., Ishida, T., & Kuwabara, K. (1990). Distributed Constraint Satisfaction for DAI
Problems In Proceedings of the 1990 Distributed AI Workshop, Bandara, TX.
Zoeteweij, P., & Arbab, F. (2004). A Component-Based Parallel Constraint Solver In De
Nicola, R., Ferrari, G. L., & Meredith, G.Eds., Coordination, Vol. 2949 of Lecture
Notes in Computer Science, pp. 307322. Springer.

464

fiJournal of Artificial Intelligence Research 57 (2016) 273-306

Submitted 11/15; published 10/16

Effective Heuristics for Suboptimal Best-First Search
Christopher Wilt
Wheeler Ruml

wilt at cs.unh.edu
ruml at cs.unh.edu

Department of Computer Science
University of New Hampshire
Durham, NH 03824 USA

Abstract
Suboptimal heuristic search algorithms such as weighted A* and greedy best-first search
are widely used to solve problems for which guaranteed optimal solutions are too expensive
to obtain. These algorithms crucially rely on a heuristic function to guide their search.
However, most research on building heuristics addresses optimal solving. In this paper,
we illustrate how established wisdom for constructing heuristics for optimal search can fail
when considering suboptimal search. We consider the behavior of greedy best-first search
in detail and we test several hypotheses for predicting when a heuristic will be effective for
it. Our results suggest that a predictive characteristic is a heuristics goal distance rank
correlation (GDRC), a robust measure of whether it orders nodes according to distance to
a goal. We demonstrate that GDRC can be used to automatically construct abstractionbased heuristics for greedy best-first search that are more effective than those built by
methods oriented toward optimal search. These results reinforce the point that suboptimal
search deserves sustained attention and specialized methods of its own.

1. Introduction
A* is a best-first search that expands nodes in order of f (n) where f (n) = g(n) + h(n).
While the optimal solutions provided by A* (Hart, Nilsson, & Raphael, 1968) are the most
desirable, time and memory often prevent the application of this algorithm. When A* fails
because of either insufficient time or memory, practitioners sometimes turn to bounded
suboptimal algorithms that may not return the optimal solution, but that return a solution
that is guaranteed to be no more than a certain factor more expensive than the optimal
solution.
The most well-known of these is likely Weighted A* (Pohl, 1970), which is a best-first
search that expands nodes in f  order, where f  (n) = g(n) + w  h(n) : w  (1, ). Variants
of Weighted A* are used in a wide variety of applications, including domain-independent
planning (Helmert, 2006; Richter & Westphal, 2010) and robotics (Likhachev, Gordon, &
Thrun, 2003; Likhachev & Ferguson, 2009). Weighted A* is also a component of a number
of anytime algorithms. For example, Anytime Restarting Weighted A* (Richter, Thayer,
& Ruml, 2009) and Anytime Repairing A* (Likhachev et al., 2003) both use Weighted A*.
Anytime Nonparametric A* (van den Berg, Shah, Huang, & Goldberg, 2011) doesnt use
Weighted A* per se, but rather its limiting case, greedy best-first search (Doran & Michie,
1966), best-first search on h(n). All of these anytime algorithms have, built in, the implicit
assumption that Weighted A* with a high weight or greedy best-first search will find a
solution faster than A* or Weighted A* with a small weight.
c
2016
AI Access Foundation. All rights reserved.

fiWilt & Ruml

In many popular heuristic search benchmark domains (e.g., sliding tile puzzles, grid path
planning, Towers of Hanoi, TopSpin, robot motion planning and the traveling salesman
problem) increasing the weight does lead to a faster search, until the weight becomes so
large that Weighted A* has the same expansion order as greedy best-first search, which
results in the fastest search. The first contribution of this paper is to provide illustrations
of how, in some domains, greedy best-first search performs worse than Weighted A*, and is
sometimes even worse than A*.
We show that the failure of greedy best-first search is not merely a mathematical curiosity, only occurring in hand crafted counterexamples, but rather a phenomenon that can
occur in real domains, including variants of popular single-agent heuristic benchmarks. Our
second contribution is to empirically characterize conditions when this occurs, knowledge
that is important for anyone using a suboptimal search. This is also an important first step
in a predictive theoretical understanding of the behavior of suboptimal heuristic search.
The root cause of the failure of greedy best-first search can be ultimately traced back
to the heuristic, which is used to guide a greedy best-first search to a goal. For A*, there
are a number of well-documented techniques for constructing an effective heuristic. We
revisit these guidelines in the context of greedy best-first search. Our third contribution is
to show that, if one follows the well-established guidelines for creating a quality heuristic for
A*, the results can be poor. We present several examples where following the A* wisdom
for constructing a heuristic leads to slower results for greedy best-first search. We use
these examples to understand the requirements that greedy best-first search places on its
heuristic.
Our fourth contribution is a quantitative metric for assessing a greedy heuristic, goal
distance rank correlation (GDRC). GDRC can be used to predict whether or not greedy
best-first search is likely to perform well. GDRC can also be used to compare different
heuristics for the same domain, allowing us to make more informed decisions about which
heuristic to select if there are a variety of choices, as is the case for abstraction-based
heuristics like pattern databases. This quantitative metric can be used to automatically
construct a heuristic for greedy best-first search by iteratively refining an abstraction and
measuring how good each candidate heuristic is. We show that iteratively refining an
abstraction using a simple hill-climbing search guided by GDRC can yield heuristics that
are more powerful than those built by traditional methods oriented toward optimal search.
This work increases our understanding of greedy best-first search, one of the most popular and scaleable heuristic search techniques. More generally, it suggests that techniques
developed for optimal search are not necessarily appropriate for suboptimal search. Suboptimal search is markedly different from optimal search, and deserves its own theory and
methods.

2. A Conundrum: Ineffective Weighted A*
The starting point for our investigation of heuristics for suboptional search begins with a
curious empirical observation: although weighted A* is one of the most popular way of
speeding up heuristic search, increasing the weight of Weighted A* does not always work.
In order to get a better grasp on the question of when increasing the weight is ineffective,
we first need some empirical data.
274

fiEffective Heuristics for Suboptimal Best-First Search

Domain
Dynamic Robot
Hanoi (14)
Pancake (40)
11 Tiles (unit)
Grid
TopSpin (3)
TopSpin (4)
11 Tiles (inverse)
City Navigation 3 3
City Navigation 4 4
City Navigation 5 5

Average Solution
Length
187.45
86.92
38.56
36.03
2927.40
8.52
10.04
37.95
15.62
14.38
13.99

Total
States
20,480,000
268,435,456
8  1047
239,500,800
1,560,000
479,001,600
479,001,600
239,500,800
22,500
22,500
22,500

Branching
Factor
0-240
6
40
1-3
0-3
12
12
1-3
3-8
3-10
3-12

Unit-cost
No
Yes
Yes
Yes
Yes
Yes
Yes
No
No
No
No

Table 1: Domain Attributes for benchmark domains considered
2.1 Benchmark Domains
We consider six standard benchmark domains: the sliding tile puzzle, the Towers of Hanoi
puzzle, grid path planning, the pancake problem, TopSpin, and dynamic robot navigation.
We selected these domains because they represent a wide variety of interesting heuristic
search features, such as branching factor, state space size, and solution length. Since we
would like to compare against A*, we are forced to use somewhat smaller puzzles than it is
possible to solve using state of the art suboptimal searches. Our requirement for problem
size was that the problem be solvable by A*, Weighted A*, and greedy best-first search in
main memory (eight gigabytes). Basic statistics about each of these domain variants are
summarized in Table 1.
For the sliding tile 11 puzzle (3  4), we used random instances and the Manhattan
distance heuristic. We used the 11 puzzle, rather than the 15 puzzle for two reasons. First,
optimally solving 15 puzzles using A* without running out of memory requires significant
resources (At least 27 gigabytes, significantly more than our eight gigabyte limit, according
to Burns et al., 2012). In addition to that, we consider the sliding tile puzzle with non-unit
cost functions. These non-unit problems are significantly more difficult to solve than the
unit-cost variants. The non-unit version of sliding tile puzzle we consider uses the inverse
cost function, where the cost of moving a tile n is 1/n. The Manhattan distance heuristic,
when weighted appropriately, is both admissible and consistent for this cost function. For
the Towers of Hanoi, we considered the 14-disk-4 peg problem, and used two disjoint pattern
databases, one for the bottom 12 disks, and one for the top two disks (Korf & Felner,
2002). For the pancake problem, we used the gap heuristic (Helmert, 2010). For grid path
planning, we used maps that were 2000x1200 cells, with 35% of the cells blocked, using the
Manhattan distance heuristic with four way movement. In the TopSpin puzzle, the objective
is to sort a circular permutation by iteratively reversing a continuous subsequence of fixed
size. An example of a TopSpin puzzle is in Figure 1. We considered a problem with 12
disks with a turnstile that would turn either three or four disks, denoted by TopSpin(3) and
TopSpin(4). For a heuristic, we used a pattern database with 6 contiguous disks present,
275

fiWilt & Ruml

Figure 1: A 20 disk TopSpin puzzle.
and the remaining 6 disks abstracted. For the dynamic robot navigation problem, we used a
200x200 world, with 32 headings and 16 speeds. In dynamic robot navigation, the objective
is to navigate a robot from one location and heading to another location and heading, while
respecting the dynamics of the robot. The robot is not able to change direction and speed
instantaneously, so not all combinations of heading/speed can be reached from a given
state. In addition to that, some states in this domain represent dead ends. For example,
a state where the robot is moving at full speed directly towards an obstacle will produce
no children, because the robot will crash no matter what control action is applied. The
objective is to minimize the total travel time; the actions do not all have the same cost.
We also introduce a new domain we call City Navigation, designed to simulate navigation
using a system similar to American interstate highways or air transportation networks. In
this domain, there are cities scattered randomly on a 100x100 square, connected by a random
tour which guarantees it is possible to get from any city to any other city. Each city is also
connected to its nc nearest neighbors. All links between cities cost the Euclidean distance
+ 2. Each city contains a collection of locations, randomly scattered throughout the city
(which is a 1x1 square). Locations in a city are connected in a random tour, with each
place also connected to the nearest np places. Links between places cost the true distance
multiplied by a random number between 1 and 1.1. Within each city there is a special
nexus node that contains all connections in and out of this city. The goal is to navigate
from a randomly selected start location to a randomly selected end location. For example,
we might want to go from Location 3 in City 4 to Location 23 in City 1. Each citys nexus
node is Location 0, so to reach the goal in the example problem we must navigate from
Location 3 to Location 0 in City 4, then find a path from City 4 to City 1, then a path from
Location 0 in City 1 to Location 23 in City 1. An example instance of this type can be
seen in Figure 2. The circles in the left part of the figure are locations, connected to other
locations. The nexus node, Location 0, is also connected to the nexus nodes of neighboring
276

fiEffective Heuristics for Suboptimal Best-First Search

Figure 2: A city navigation problem with np = nc = 3, with 15 cities and 15 locations in
each city.

cities. The right part of the figure shows the entire world, with cities shrunk down to a
circle.
City Navigation instances are classified by np and nc . We consider problems with varying
numbers of connections, but always having 150 cities and 150 places in each city. Since each
location within a city has a global position, the heuristic is direct Euclidean distance. In
this domain, solutions vary in length, and it is straightforward to manipulate the accuracy
of the heuristic. This domain bears some similarity to the IPC Logistics domain in which
locations within cities are connected by roads, but special airport locations are used to
travel between cities.
2.2 Results
Figures 3 and 4 show the number of expansions required by A*, greedy best-first search, and
Weighted A* with weights of 1.1, 1.2, 2.5, 5, 10, and 20. These plots allow us to compare
greedy best-first search with Weighted A* and A*, and to determine whether increasing the
weight speeds up the search, or slows down the search.
Looking at the plots in Figure 3, it is easy to see that as we increase the weight the
number of expansions goes down, but in Figure 4, the opposite is true. In each of these
domains, increasing the weight initially speeds up the search, as A* is relaxed into Weighted
A*, but as Weighted A* transforms into greedy best-first search, the number of nodes
required to solve the problem increases. In two of the domains, TopSpin with a turnstile of
size 4 and City Navigation 3 3, the number of nodes expanded by greedy best-first search is
higher than the number of nodes expanded by A*. Explaining this phenomenon is a central
goal of this paper.
277

fiWilt & Ruml

600000

300000

200000

100000

16000

8000

0

0

0

A* 1.1 1.2 2.5 5 10 20 G

A* 1.1 1.2 2.5 5 10 20 G

Dynamic Robot

A* 1.1 1.2 2.5 5 10 20 G

TopSpin(3)

Unit Tile

1e+06

500000

Total Nodes Expanded

800

Total Nodes Expanded

Total Nodes expanded

Total Nodes Expanded

Total Nodes Expanded

Total Nodes Expanded

1.2e+06

40 Pancake Problem

Grid Navigation

14 disk Hanoi

40000

20000

0

0

A* 1.1 1.2 2.5 5 10 20 G

400

0

A* 1.1 1.2 2.5 5 10 20 G

A* 1.1 1.2 2.5 5

10 20 G

Figure 3: Domains where increasing the weight speeds up search. Numbers denote Weighted
A* run with a specific weight, and G denotes greedy best-first search.

3. Characteristics of Effective Heuristics
We have established that increasing the weight in Weighted A* does not always speed up
the search, and in some situations can actually slow down search. The fact that A* is
sometimes faster than greedy best-first search and sometimes slower than greedy best-first
search suggests that some heuristics work well for A* and poorly for greedy best-first search,
and that some heuristics work well for greedy best-first search but not for A*. Thus, the
question is precisely what is driving this difference, and what each algorithm, A* and greedy
best-first search, needs out of the heuristic.
We first review the literature for suggestions about how to make a good heuristic for
A*. With this in mind, we then apply the A* rules for constructing an effective heuristic
to greedy best-first search. This leads us to observations on effective heuristics for greedy
best-first search that are distinct from the common recommendations for building a good
heuristic for A*.
3.1 Effective Heuristics for A*
Much of the literature about what constitutes a good heuristic centers on how well the
heuristic works for A*. For finding optimal solutions using A*, the first and most important
278

fiEffective Heuristics for Suboptimal Best-First Search

Inverse Tile

TopSpin(4)

Total Nodes Expanded

Total Nodes Expanded

160000

80000

0

16000

8000

0

A* 1.1 1.2 2.5 5 10 20 G

A* 1.1 1.2 2.5 5 10 20 G
City Navigation 4 4

City Navigation 3 3

4000

Total Nodes Expanded

Total Nodes Expanded

3000

2000

0

2000

1000

0

A* 1.1 1.2 2.5 5

10 20 G

A* 1.1 1.2 2.5 5

10 20 G

Figure 4: Domains where increasing the weight slows down search. Numbers denote
Weighted A* with the specified weight, and G denotes greedy best-first search.

requirement is that the heuristic be admissible, meaning for all nodes n, h (n)  the true
cheapest path from n to a goal  is greater than or equal to h(n). If the heuristic is not
admissible, A* degenerates into A (no star) which is not guaranteed to find the shortest
path.
It is generally believed that consistency is also important, due to the fact that inadmissible heuristics can lead to an exponential number of re-expansions (Martelli, 1977).
This situation, however, rarely arises in practice and Felner et. al. (2011) argue that
inconsistency is generally not as much of a problem as is generally believed.
The most widespread rule for making a good heuristic for A* is: dominance is good (Nilsson, 1980; Pearl, 1984). A heuristic h1 is said to dominate h2 if n  G : h1 (n)  h2 (n).
This makes sense, because due to admissibility, larger values are closer to h . Furthermore
A* must expand every node n it encounters where f (n) is less than the cost of an optimal
solution, so large h often reduces expansions. Dominance represents the current gold standard for comparing two heuristics. In practice, heuristics are often informally evaluated by
279

fiWilt & Ruml

their average value or by their value at the initial state over a benchmark set. In either
case, the general idea remains the same: bigger heuristics are better.
If we ignore the effects of tie breaking as well as the effects of duplicate states, A* and
the last iteration of IDA* expand the same number of nodes. This allows us to apply the
formula from Korf, Reid, and Edelkamp (2001). They predict that the number of nodes
IDA* will expand at cost bound c is:
E(N, c, P ) =

c
X

Ni P (c  i)

i=0

The function P (h) in the KRE equation represents the equilibrium heuristic distribution,
which is the probability that a node chosen randomly and uniformly among all nodes at a
given depth of the brute-force search tree has heuristic value less than or equal to h (Korf
et al., 2001). This quantity tends to decrease as h gets larger, depending on how the nodes
in the space are distributed. The dominance relation also transfers to the KRE equation,
meaning that if a heuristic h1 dominates a different heuristic h2 , the KRE equations predicts
that the expected expansions using h1 will be less than or equal to the expected expansions
using h2 .
When considering pattern database (PDB) heuristics, Korfs conjecture (1997) can lend
m
insight into the performance of IDA*, which tells us that we can expect 1+log(m)
t = n
with m being the amount of memory the PDB in question takes up, t is the amount of time
we expect an IDA* search to consume, and n is a constant (Korf, 2007). If we are willing
to apply results regarding IDA* to A* this equation tells us that we should expect larger
pattern databases to provide faster search for A*. To summarize, the prevailing wisdom
regarding heuristics is that bigger is better, both in terms of average heuristic value and
pattern database size.
3.2 The Behavior of Greedy Best-First Search
As we shall see, this advice regarding heuristics is all very helpful when considering only
A*. What happens if we apply this same wisdom to greedy best-first search? We answer
this question by taking a detailed look at the behavior of greedy best-first search on three
of our benchmark problems: the Towers of Hanoi, the TopSpin puzzle, and the sliding tile
puzzle.
3.2.1 Towers of Hanoi
The first domain we consider is the Towers of Hanoi. The most successful heuristic for
optimally solving 4 peg Towers of Hanoi problems is disjoint pattern databases (Korf &
Felner, 2002). Disjoint pattern databases boost the heuristic value by providing information
about the disks on the top of the puzzle. For example, consider a 12-disk puzzle, split into
two disjoint pattern databases: eight disks in the bottom pattern database, and four disks
in the top pattern database. With A*, the best results are achieved when using the full
disjoint pattern database. With greedy best-first search, however, faster search results when
we do not use a disjoint pattern database, and instead only use the 8 disk pattern database.
The exact numbers are presented in the Unit rows of Table 2. All problems are randomly
generated Towers of Hanoi states, with the goal being to get all disks onto the first peg.
280

fiEffective Heuristics for Suboptimal Best-First Search

Cost
Unit
Square
Rev Square

Heuristic
8/4 PDB
8/0 PDB
8/4 PDB
8/0 PDB
8/4 PDB
8/0 PDB

A* Exp
2,153,558
4,618,913
239,653
329,761
3,412,080
9,896,145

Greedy Exp
36,023
771
4,663
892
559,250
730

Table 2: Average number of nodes expanded to solve 51 12-disk Towers of Hanoi problems.
30

25

25

20

Minimum h

Minimum h

20

15

15

10

10

5
00

5
200

400
600
Expansions

800

1000

00

5000 10000 15000 20000 25000 30000 35000
Expansions

Figure 5: The minimum h value on open as the search progresses, using different pattern
databases (single on left, two disjoint additive ones on the right).

The theory for A* corroborates the empirical evidence observed here: the disjoint pattern database dominates the single pattern database, so absent unusual effects from tiebreaking, it is no surprise that the disjoint pattern database results in faster A* search.
The reason for the different behaviour of A* and greedy best-first search is simple. With
greedy best-first search using a single pattern database, it is possible to follow the heuristic
directly to a goal, having the h value of the head of the open list monotonically decrease.
To see this, note that every combination of the bottom disks has an h value, and all possible
arrangements of the disks on top will also share that same h value. The disks on top can
always be moved around independently of where the bottom disks are. Consequently, it is
always possible to arrange the top disks such that the next move of the bottom disks can
be done, while not disturbing any of the bottom disks, thus leaving h constant. Eventually,
h decreases because more progress has been made putting the bottom disks of the problem
in order. This process repeats until h = 0, at which point greedy best-first search simply
considers possible configurations of the top disks until the goal has been found.
This phenomenon can be seen in the left pane of Figure 5, where the minimum h value
of the open list monotonically decreases as the number of expansions the search has done
281

fiWilt & Ruml

h = 10

h=9
Figure 6: Two Towers of Hanoi states, one near a goal (top) and one far from a goal
(bottom).

increases. The heuristic created by the single pattern database creates an extremely effective
gradient for the greedy best-first search algorithm to follow for two reasons. First, there are
no local minima at all, only the global minimum where the goal is. In this context, we define
a minimum as a region of the space M where n  M , every path from n to a goal node
has at least one node n with h(n ) > h(n). Second, there are exactly 256 states associated
with each configuration of the bottom 8 disks. This means that every 256 expansions, h is
guaranteed to decrease. In practice, a state with a lower h tends to be found much faster.
In the right pane of Figure 5, the heuristic is a disjoint pattern database. We can see
that the h value of the head of the open list fluctuates substantially when using a disjoint
pattern database, indicating that greedy best-first searchs policy of follow small h is much
less successful. This is because those states with the bottom disks very near their goal that
are paired with a very poor arrangement of the disks on top are assigned large heuristic
values, which delays the expansion of these nodes. This is illustrated in Figure 6. The top
state is significantly closer to a goal, despite having a higher h value than the bottom state.
If we ignore the top disks completely, the top state has h = 1 compared to the bottom
states h = 9, which correctly conveys the fact that the top state is significantly closer to
a goal. The disjoint PDB causes substantial confusion for greedy best-first search, because
prior to making any progress with any of the 8 bottom disks, the greedy best-first search
considers states where the top 4 disks are closer to their destination. If the bottom state is
expanded, it will produce children with lower heuristic values which will be explored before
ever considering the top state, which is the state that should be explored first. Eventually,
all descendants of the bottom state with h  9 are explored, at which point the top state
is expanded, but this causes the h value of the head of the open list to go up and down.
To summarize, the disjoint pattern database makes a gradient that is more difficult
for greedy best-first search to follow because nodes can have a small h for more than one
reason: being near the goal because the bottom pattern database is returning a small value,
or being not particularly near the goal, but having the top disks arranged on the target peg.
This suggests the following observation regarding heuristics for greedy best-first search:
282

fiEffective Heuristics for Suboptimal Best-First Search

Observation 1. All else being equal, greedy best-first search tends to work well when it is
possible to reach the goal from every node via a path where h monotonically decreases along
the path.
While this may seem self-evident, our example has illustrated how it conflicts with the
common wisdom in heuristic construction. It is also important to note that this observation
makes no comment about the relative magnitude of the heuristic, which for greedy best-first
is completely irrelevant; all that matters is the relative ordering of the nodes when ordered
using the heuristic.
Another way to view this phenomenon is in analogy to the Sussman Anomaly (Sussman,
1975). The Sussman anomaly occurs when one must undo a subgoal prior to being able
to reach the global goal. In the context of Towers of Hanoi problems, the goal is to get
all of the disks on the target peg, but solving the problem may involve doing and then
undoing some subgoals of putting the top disks on the target peg. The presence of the top
pattern database encourages greedy best-first searches to privilege states where subgoals
which eventually have to be undone have been accomplished.
Korf (1987) discusses different kinds of subgoals, and how different kinds of heuristic
searches are able to leverage subgoals. Greedy best-first search uses the heuristic to create
subgoals, attempting to follow the h to a goal. For example, in a unit-cost domain, the first
subgoal is to find a node with h = h(root)  1. If the heuristic follows Observation 1, these
subgoals form a perfect serialization, and the subgoals can be achieved one after another.
As the heuristic deviates from Observation 1, the subgoals induced by the heuristic cannot
be serialized.
Another important factor is, of course, the number of distinct nodes at each heuristic
level one encounters prior to finding a better node. Consider, for example, one of the worst
heuristics, h = 0. Technically, this heuristic follows Observation 1 because all paths only
contain nodes with h = 0, but the one plateau contains all nodes in the entire space, which is
obviously undesirable. Hoffmann (2005) discusses this general idea using the term maximal
bench exit distance, and once again, the idea is that in domains in which this quantity is
small, both greedy best-first search and his Enforced Hill Climbing method perform well,
because finding nodes with lower h is straightforward.
These effects can be exacerbated if the cost of the disks on the top is increased relative
to the cost of the disks on the bottom. If we define the cost of moving a disk as being
proportional to the disks size, we get the Square cost metric, where the cost of moving disk
n is n2 . We could also imagine the tower being stacked in reverse, requiring that the larger
disks always be on top of the smaller disks, in which case we get the Reverse Square cost
function. In either case, we expect that the number of expansions that greedy best-first
search will require will be lower when using only the bottom pattern database, and this is
indeed the effect we observe in Table 2. However, if the top disks are heavier than the disks
on the bottom, greedy best-first search suffers even more than when we considered the unit
cost problem, expanding an order of magnitude more nodes. This is because the pattern
database with information about the top disks is returning values that are substantially
larger than the bottom pattern database, due to the fact that the top pattern database
considers the most expensive operators. If the situation is reversed, however, and the top
pattern database uses only the lowest cost operators, the top pattern databases contribution
to h is a much smaller proportion of the total expansions. Since greedy best-first search
283

fiWilt & Ruml

1400

1800

Greedy Search with Disjoint PDB

Greedy Search with Disjoint PDB

1600

1200

1400
1200

800

Minimum h

Minimum h

1000

1000

600

800
600

400

400
200
0
0

200
1000

2000
3000
Expansions

4000

0
0

5000

100000

200000

300000 400000
Expansions

500000

600000

700000

Figure 7: The minimum h value on open during searches using disjoint pattern databases
with different cost functions (square on left, reverse square on right).

performs best when the top pattern database isnt even present, it naturally performs better
when the contribution of the top pattern database is smaller.
This phenomenon is vividly illustrated in the execution times in Figure 7. In the left of
the figure, the disks in the top pattern database are much cheaper to move than the disks
in the bottom pattern database, and are therefore contributing a much smaller proportion
of the total value of h. In the right part of the figure, the disks in the top pattern database
are much more expensive to move than the disks in the bottom pattern database, so the
top pattern database makes a much larger contribution to h, causing substantially more
confusion.
Hoffmann (2005) notes that the success of the FF heuristic in many domains is attributable to the fact that the h+ heuristic produces a heuristic with no local minima. A
heuristic with no local minima precisely matches our Observation 1, because it will always
be possible to reach the goal via a path where h monotonically decreases.
3.2.2 TopSpin
We considered TopSpin with 12 disks and a turnstile that flipped 4 disks using pattern
databases that contained 5, 6, 7, and 8 of the 12 total disks.
Korfs conjecture predicts that the larger pattern databases will be more useful for
A*, and should therefore be considered to be stronger heuristics, and indeed, as the PDB
becomes larger, the number of expansions done by A* dramatically decreases. This can be
seen in Figure 8. Each box plot (Tukey, 1977) is labeled with either A* or G (for greedy
best-first search), and a number, denoting the number of disks that the PDB tracks. Each
box denotes the middle 50% of the data, so the top of the box is the upper quartile, the
bottom of the box is the bottom quartile, and the height of the box is the interquartile
range. The horizontal line in the middle of the box represents the median. The grey stripe
indicates the 95% confidence interval about the mean. The circles denote points that are
more than 1.5 times the interquartile range away from either the first quartile or the third
284

fiEffective Heuristics for Suboptimal Best-First Search

4/12 TopSpin with Different PDB's

Expansions

60000

30000

A*5 G5 A*6 G 6 A*7 G7 A*8 G 8

Figure 8: TopSpin puzzle with different heuristics. A followed by a number denotes A
with that number of disks in the PDB heuristic. G followed by a number denotes
greedy best-first search with that number of disks in the PDB heuristic.

quartile, and the whiskers represent the range of the non-outlier data. As we move from left
to right, as the PDB heuristic tracks more disks, it gets substantially better for A*. While
there are also reductions for greedy best-first search in terms of expansions, the gains are
nowhere near as impressive as compared to A*.
The reason that greedy best-first search does not perform better when given a larger
heuristic is that, with the larger heuristic, states with h = 0 may still be quite far from a
goal. For example, consider the TopSpin state represented as follows, where A denotes an
abstracted disk:
State 1: 0 1 2 3 4 5 A A A A A A
The turnstile swaps the orientation of 4 disks, but there are configurations such that
putting the abstracted disks in order requires moving a disk that is not abstracted, such as:
State 2: 0 1 2 3 4 5 6 7 8 9 11 10
For a TopSpin state, the abstraction process takes the largest N disks and converts
them to abstracted disks, and abstracted disks are all treated the same, so State 2 would be
abstracted into State 1, which means that it abstracts to the same state as the goal, making
its heuristic 0. If we wanted to expand State 2, we could do so and one of the children is
State 3, whose heuristic is still 0:
State 3: 0 1 2 3 4 5 6 7 10 11 9 8
Consider a different child, for example the child obtained by rotating the middle 4 disks:
285

fiWilt & Ruml

State 4: 0 1 2 3 7 6 5 4 8 9 10 11
which abstracts into:
State 5: 0 1 2 3 A A 5 4 A A A A
The heuristic for State 4 is not 0, because State 4 abstracts into State 5. State 5 is an
abstract state that is different from State 1 (the abstracted goal) so the heuristic of State
4 is not 0.
If we abstract disks 6-11, we still have the same abstract state as before, so the heuristic
is still 0. Moving a disk that is not abstracted will increase the heuristic, but moving only
abstracted disks will leave the heuristic at 0. Unfortunately, transforming State 2 into a
goal cannot be done without moving at least one of the disks whose index is between 0 and
5, because the turnstile is of size 4.
This means that the subgraph consisting of only nodes with h = 0 in the TopSpin
problem is disconnected. Thus, when greedy best-first search encounters a state with h = 0,
the state could be a h = 0 state that is connected to the goal via only h = 0 states, which
would be desirable, or the state could be a h = 0 state that is connected to the goal via only
paths that contain at least one h 6= 0 nodes, which would be undesirable. If this is the case,
greedy best-first search will first expand all the h = 0 nodes connected to the first h = 0
node (which by hypothesis is not connected to a goal node via paths only containing h = 0
nodes), and will then return to expanding nodes with h = 1, looking to find a different
h = 0 node.
The abstraction controls the number and size of h = 0 regions. For example, if we
abstract 6 disks, there are two strongly connected regions of h = 0 nodes, each containing
360 nodes. If we instead abstract 5 disks, there are 12 strongly connected h = 0 regions,
each with 10 nodes. For the heuristic that abstracts 6 disks, there is a 50% chance that
any given h = 0 node is connected to the goal via only h = 0 nodes, but once greedy
best-first search has entered the correct h = 0 region, finding the goal node is largely up
to chance. For the heuristic that abstracts 5 disks, the probability that any given h = 0
node is connected to the goal via only h = 0 nodes is lower. Once the correct h = 0 region
is found, however, it is much easier to find the goal, because the region contains only 10
nodes, as compared to 360 nodes. Empirically, we can see that these two effects roughly
cancel one another out, because the total number of expansions done by greedy best-first
search remains roughly constant no matter which heuristic is used. This brings us to our
next observation.
Observation 2. All else being equal, nodes with h = 0 should be connected to goal nodes
via paths that only contain h = 0 nodes.
One can view this as an important specific case of Observation 1. Interestingly, some types
of heuristics, such as the delete-relaxation heuristics used in domain-independent planning,
obey this observation implicitly by never allowing non-goal states to have h values of 0.
One obvious way to make a heuristic satisfy this recommendation is to change the
heuristic for all non-goal states to be the same as the minimum cost operator from the
domain with cost of . If we do this, we can simply restate the recommendation substituting
 for 0, and we arrive at a similar result.
286

fiEffective Heuristics for Suboptimal Best-First Search

A
8

A
A
9

A
A
10

3
7
11

4
A

1
A
9

A
6
A

3
A
11

Figure 9: Different tile abstractions. A denotes a tile that is abstracted..
Abstraction
Outer L (Figure 9 left)
Checker (Figure 9 right)
Outer L Missing 3
Outer L Missing 3 and 7
Instance Specific
GDRC Generated
Average 6-tile PDB
Worst 6-tile PDB

Greedy Exp
258
11,583
3,006
20,267
8,530
427
17,641
193,849

A* Exp
1,251,260
1,423,378
DNF
DNF
480,250
1,197,789
1,609,995
2,168,785

Table 3: Average number of expansions required by Greedy best-first search and A* to solve
3  4 tile instances with different pattern databases. DNF denotes at least one
instance would require more than 8GB to solve.

3.2.3 Sliding Tiles
The sliding tile puzzle is one of the most commonly used benchmark domains in heuristic
search. As such, this domain is one of the best understood. Pattern database heuristics
have been shown to be the strongest heuristics for this domain, and have been the strongest
heuristics for quite some time (Korf & Taylor, 1996; Felner, Korf, Meshulam, & Holte,
2007). We use the 11 puzzle (4  3) as a case study because the smaller size of this puzzle
allows creating and testing hundreds of different pattern databases. The central problem
when constructing a pattern database for a sliding tile puzzle is selecting a good abstraction.
The abstraction that keeps only the outer L, shown in the left part of Figure 9, is
extremely effective for greedy best-first search, because once greedy best-first search has
put all abstracted tiles in their proper places, all that remains is to find the goal, which is
easy to do using even a completely uninformed search on the remaining puzzle, as there are
only 6!2 = 360 states with h = 0 and the h = 0 states form a connected subgraph. This is
analogous to the heuristic directing the search algorithm to follow the process outlined by
Parberry (1995), in which large sliding tile puzzles are solved by first solving the outer L,
and then treating the remaining problem as a smaller sliding tile puzzle.
Compare this to what happens when greedy best-first search is run on a checkerboard
abstraction, as shown in the right part of Figure 9. Once greedy best-first search has identified a node with h = 0, there is a very high chance that the remaining abstracted tiles
are not configured properly, and that at least one of the non-abstracted tiles will have to
be moved. This effect can be seen in Table 3, where the average number of expansions required by A* is comparable with either abstraction, while the average number of expansions
required by greedy best-first search is larger by two orders of magnitude.
287

fiWilt & Ruml

The sheer size of the PDB is not as important for greedy best-first search as it is for
A*. In Table 3, we can see that as we weaken the pattern database by removing the 3 and
7 tiles, the number of expansions required increases by a factor of 10 for greedy best-first
search. For A* using the PDB with the 3 tile missing, 3 instances are unsolvable within 8
GB of memory (approximately 25 million nodes in our Java implementation). With both
the 3 and the 7 tile missing, A* is unable to solve 16 instances within the same limit. It is
worth noting that even without the 3 tile, the outer L abstraction is still more effective for
greedy best-first search as compared to the checkerboard abstraction.
The underlying reason behind the inefficiency of greedy best-first search using certain
pattern databases is the fact that the less useful pattern databases have nodes with h = 0
that are nowhere near the goal. This provides further evidence in favor of Observation 2;
greedy best-first search concentrates its efforts on finding and expanding nodes with a low h
value, and if some of those nodes are, in reality, not near a goal, this clearly causes problems
for the algorithm. Because A* uses f , and g contributes to f , A* is able to eliminate some
of these states from consideration (not expand them) because the high g value helps to give
the node a high f value, which causes A* to relegate the node to the back of the expansion
queue.
The checkerboard pattern database also helps to make clear another problem facing
greedy best-first search heuristics. Once the algorithm discovers a node with h = 0, if that
node is not connected to any goal via only h = 0 nodes, the algorithm will eventually run
out of h = 0 nodes to expand, and will begin expanding nodes with h = 1. When expanding
h = 1 nodes, greedy best-first search will either find more h = 0 nodes to examine for goals,
or it will eventually exhaust all of the h = 1 nodes as well, and be forced to consider h = 2
nodes. A natural question to ask is how far the algorithm has to back off before it will be
able to find a goal. This leads us to our next observation.

Observation 3. All else being equal, greedy best-first search tends to work well when the
difference between the minimum h value of the nodes in a local minimum and the minimum
h that will allow the search to escape from the local minimum and reach a goal is low.

This phenomenon is clearly illustrated when considering instance-specific pattern databases
(Holte, Grajkowskic, & Tanner, 2005). In an instance-specific pattern database, the tiles
that start out closest to their goals are abstracted first, leaving the tiles that are furthest
away from their goals to be represented in the pattern database. This helps to maximize
the heuristic values of the states near the root, but due to consistency this can also have the
undesirable side effect of making states that are required to be included in a path to the goal
have high heuristic values as well. Raising the heuristic value of the initial state is helpful for
A* search, as evidenced by the reduction in the number of expansions for A* using instancespecific abstractions of the same size, shown in Table 3. Unfortunately, this approach is
still not as powerful for greedy best-first search as the simpler outer L abstraction, or even
the smaller variant missing the 3. This is because some instance-specific pattern databases
use patterns that are difficult for greedy best-first search to use effectively, similar to the
problems encountered when using the checkerboard abstraction.
288

fiEffective Heuristics for Suboptimal Best-First Search

Domain

Greedy
Works

Greedy
Fails

Towers of Hanoi
Grid
Pancake
Dynamic Robot
Unit Tiles
TopSpin(3)
TopSpin(4)
Inverse Tiles
City Nav 3 3
City Nav 4 4

Heuristic
% Error
29.47
25.11
2.41
15.66
33.37
25.95
32.86
29.49
44.51
37.41

h(n)-h (n)
Correlation
(Pearson)
0.9652
0.9967
0.9621
0.9998
0.7064
0.5855
0.2827
0.6722
0.5688
0.7077

h(n)-h (n)
Correlation
(Spearman)
0.9433
0.9958
0.9593
0.9983
0.7065
0.4598
0.3196
0.6584
0.6132
0.7518

h(n)-h (n)
Correlation
(Kendall)
0.8306
0.9527
0.9198
0.9869
0.5505
0.4158
0.2736
0.4877
0.4675
0.6238

Table 4: Average % error and correlation between h(n) and h (n)

4. Predicting Effectiveness of Greedy Heuristics
In the previous section, we saw that common wisdom regarding effective heuristics for
optimal search did not carry over to suboptimal search. Instead, our examples motivated
three general observations regarding what greedy best-first search looks for in a heuristic.
While these qualitative observations are perhaps helpful heuristics for heuristic design, it is
also useful to have a simple, quantitative metric for evaluating and comparing heuristics.
We begin by considering two intuitively reasonable quantitative metrics, the percent
error in h, and the correlation between h and h . For each of these metrics, we show that
the metric cannot be used to predict whether or not greedy best-first search will perform
worse than Weighted A*. Then we consider a measure of search distance to go called d .
d (n) is the same as h if we change the graph by making all edges cost 1. We find that
the correlation between h and d can be used to predict when greedy best-first search will
perform poorly.
4.1 Percent Error in h(n)
The first metric we consider is perhaps the most intuitive measure of heuristic performance:
the percent error in h. We define the percent error in the heuristic as h (n)h(n)
. Since greedy
h (n)
best-first search increases the importance of the heuristic, it is reasonable to conclude that
if the heuristic has a large amount of error, relying upon it heavily, as greedy best-first
search does, is not going to lead to a fast search.
In Table 4, we have the average percent error in the heuristic for each of the domains
considered. Surprisingly, the average percentage error bears little relation to whether or
not greedy best-search will be a poor choice. Towers of Hanoi, unit tiles, and TopSpin(3),
three domains where greedy best-first search is effective, have as much or more heuristic
error than domains where greedy best-first search works poorly. This leads us to conclude
that you cannot measure the average heuristic percent error and use this to predict whether
or not increasing the weight will speed up or slow down search.
289

fiWilt & Ruml

To see intuitively why this makes sense, note that greedy best-first search only really
requires that the nodes get put in h (n) order by the heuristic. The exact magnitude, and
therefore error, of the heuristic is unimportant, but magnitude has a huge effect on the
average percent error. This can be seen if we consider the heuristic h(n) = h R(n) : R  R+
for a very large or very tiny R, which will always guide greedy best-first search directly to
an optimal goal, while exhibiting arbitrarily high average percent error in the heuristic as
R increases or decreases away from 1.
4.2 h  h Correlation
The next metric we consider is the correlation between h and h . While considering the
percent error in h as a metric, we noted that greedy best-first search has run time linear
in the solution length of the optimal solution if the nodes are in h (n) order. One way to
quantify this observation is to measure the correlation between the two values. We will do
this in three different ways.
The most well known correlation coefficient is Pearsons correlation coefficient r, which
measures how well the relationship between h(n) and h (n) can be modeled using a linear
function. Such a relationship would mean that weighting the heuristic appropriately can
reduce the error in the heuristic, which could reasonably be expected to lead to a faster
search. In addition, if the relationship between h(n) and h (n) is a linear function, then
order will be preserved: putting nodes in order of h(n) will also put the nodes in order of
h (n), which leads to an effective greedy best-first search. For each domain, we calculated
Pearsons correlation coefficient between h (n) and h(n), and the results are in the second
column of Table 4.
Another reasonable way to measure the heuristic correlation is to use rank correlation.
Rank correlation measures how well one permutation (or order) respects another permutation (or order). In the context of search, we can use this to ask how similar the order one
gets by putting nodes in h order is to the order one gets by putting nodes in h order. Rank
correlation coefficients are useful because they are less sensitive to outliers, and are able to
detect relationships that are not linear.
Spearmans rank correlation coefficient () is the best known rank correlation coefficient.
 is Pearsons r between the ranked variables. This means that the smallest of N heuristic
values is mapped to 0, the largest of the n heuristic values is mapped to N . This is done
for both h and h , at which point we simply calculate Persons r using the rankings. In
the context of greedy best-first search, if Spearmans rank correlation coefficient is high,
this means that the h(n) and h (n) put nodes in very close to the same order. Expanding
nodes in h (n) order leads to greedy best-first search running in time linear in the solution
length, so it is reasonable to conclude that a strong Spearmans rank correlation coefficient
between h (n) and h(n) would lead to an effective greedy best-first search. For each domain,
we calculate the Spearmans rank correlation coefficient between h (n) and h(n), and the
results are in the third column of Table 4.
A more natural metric for measuring this relationship can be achieved by using Kendalls
 (1938). Kendalls  is another rank correlation coefficient, but it measures the amount
of concordance between the two rankings. Concordance is having the rankings for two
elements agree. In the context of greedy best-first search, a concordant pair is a pair of
290

fiEffective Heuristics for Suboptimal Best-First Search

nodes such that h(n1 ) > h(n2 ) and h (n1 ) > h (n2 ) or h(n1 ) < h(n2 ) and h (n1 ) < h (n2 ).
Kendalls  is the proportion of pairwise comparisons that are concordant. If h puts nodes
in h order, all pairwise comparisons will be concordant, and Kendalls  will be 1. If h puts
nodes in reverse h order, all comparisons will be discordant, and Kendalls  will be -1. If
sorting nodes on h puts nodes in a random order, we expect that half of the comparisons
will be concordant and half of the comparisons will be discordant.
Kendalls  can also be understood in the context of bubble sort. The Kendall  distance
is the number of swaps that a bubble sort would do in order to change one list into the
other. In this case, it is the number of swaps that a bubble sort would do rearranging a list
of nodes sorted on h so the list is sorted on h . Kendalls  is calculated by normalizing the
Kendall  distance, which is done by dividing by N (N  1)/2. 1
Since  and  are both rank correlation coefficients, they are related, but we argue that
 is the more natural statistic. Consider this question: given an open list containing n
nodes, how likely is it that the node with the smallest h will be at the front of the open
list, given that the nodes are ordered on h? We can use  to predict that the node will,
on average, be in the middle of the list if h and h are completely unrelated, and closer to
the front of the open list the stronger the  (h, h ) correlation is. The reason is that if we
assume that the nodes on the open list are a random selection of nodes,  tells us how often
a random comparison is correct. We can use therefore  to predict how far back the node
with the minimum h is.  has no such natural interpretation, making  the more natural
statistic. It is worth nothing that  and  are generally related to one another, in that one
can be used to predict the other (Gibbons, 1985). This relationship means that in practice,
it is generally possible to use either metric.
Returning to Table 4, the results lead us to reject the correlation between h and h as
a metric for predicting how well greedy best-first search will work. For all three correlation
coefficients, there are examples of domains where greedy best-first search fails with high
h(n)-h (n) correlations, and examples of domains where greedy best-first search works well
with poor h(n)-h (n) correlations. For example, in TopSpin(3), we have a Kendalls  of
.42, but this is lower than the  for Inverse Tiles and both City Navigation problems we
consider.
4.3 h  d Correlation
The strategy of greedy best-first search is to discover a goal quickly by expanding nodes
with small h(n) values. If nodes with small h(n) are far away from a goal it is reasonable to
believe greedy best-first search would perform poorly. We will denote by d (n) the count of
edges between a node n and the nearest goal, where distance is not measured by summing
the cost of the edges in the path, but rather by counting the edges in the path.
d (n) is equivalent to h (n) if we modify the graph so that all edges cost 1. Looking at
the plot of h(n) vs h (n) in the left half of Figure 10, we can see that for City Navigation
4 4 there is a reasonable relationship between h(n) and h (n), in that the nodes with low
h(n) tend to have small h (n) values. We denote the distance to the nearest goal in terms
1. Malte Helmert has noted (personal communication) that Kendalls  , as described, is not an ideal metric
to use for sequences that contain ties. For integer-valued heuristics, especially, ties may be very common.
One way to account for ties in the rankings to use Kendalls  -b statistic (Kendall & Gibbons, 1990)
instead of  (also known as  -a). Kendalls  -b accounts for ties in the rankings.

291

fiWilt & Ruml

h vs h* in City Navigation 4 4

h vs d* in City Navigation 4 4
18

120

d*

h*

12

60

6
0
0

80

160

h

40

80

h

Figure 10: Plot of h(n) vs h (n), and h(n) vs d (n) for City Navigation 4 4
Domain

Greedy
Works

Greedy
Fails

Towers of Hanoi
Grid
Pancake
Dynamic Robot
Unit Tiles
TopSpin(3)
TopSpin(4)
Inverse Tiles
City Nav 3 3
City Nav 4 4

h(n)-d (n)
Correlation
(Pearson)
0.9652
0.9967
0.9621
0.9998
0.7064
0.5855
0.2827
0.5281
0.0246
0.0853

h(n)-d (n)
Correlation
(Spearman)
0.9433
0.9958
0.9593
0.9983
0.7065
0.4598
0.3196
0.5173
-0.0338
0.1581

h(n)-d (n)
Correlation
(Kendall)
0.8306
0.9527
0.9198
0.9869
0.5505
0.4158
0.2736
0.3752
-0.0267
0.1192

Table 5: Correlation between h(n) and d (n)

of the number of edges in the state space graph as d (n). The right half of Figure 10 shows
a plot of h(n) vs d (n). We can clearly see that in the City Navigation 4 4 domain, there
is almost no relationship between h(n) and d (n), meaning that nodes that receive a small
h(n) value can be found any distance away from a goal, which could explain why greedy
best-first search works so poorly for this domain, despite the fact that h(n) and h (n) are
so closely related.
If nodes with small h(n) values are also likely to have small d (n) values (and these
nodes are therefore close to a goal, in terms of expansions away) expanding nodes with
small h(n) values will quickly lead to a goal. The converse is also reasonable. If the nodes
with small h(n) value have a uniform distribution of d (n) values (and thus many of these
nodes are far away from a goal in terms of expansions away), expanding these nodes will
not quickly lead to a goal.
292

fiEffective Heuristics for Suboptimal Best-First Search

7.0
6.5

log(Expansions)

6.0
5.5
5.0
4.5
4.0
3.5
3.0
2.5
0.0

0.1

0.2

0.3

0.4
GDRC

0.5

0.6

0.7

0.8

Figure 11: Average log of expansions done by greedy best-first search with different heuristics, plotted according to their GDRC.

For each domain, we quantify this concept by calculating Pearsons correlation coefficient, Spearmans rank correlation coefficient, and Kendalls  between d (n) and h(n).
Looking at Table 5, we can see that, using both Kendalls  and Pearsons r, we are finally
able to separate the domains on which greedy best-first search performs well from the domains on which greedy best-first search performs poorly. For Kendalls  , we can draw a
line at approximately 0.4 that can be used to separate the domains where greedy best-first
search works well and the domains where greedy best-first search works poorly. Likewise,
for Pearsons r, we can draw a line at approximately .55. We will call this type of metric
the Goal Distance Rank Correlation (GDRC) and, unless otherwise noted, compute it using
Kendalls  .
The correlation between d (n) and h(n) connects to our three observations, although
the connection is not a mathematical necessity (as counterexamples can be constructed).
Note that a heuristic that obeys Observation 1 will produce paths where h monotonically
decreases to a goal. Consider the nodes along a path to a goal. By hypothesis, h will
monotonically decrease along this path. Now, consider one of the nodes at the goal end of
the path. Since h monotonically decreases along the path, the nodes at the goal end of the
path have a low h, and because they are near the end of the path, they also have a low d
value. While little else can be said about the nodes in general, this restriction improves the
heuristics GDRC compared to a situation in which the nodes with low d are allowed to
have high h values. A similar argument can be used to show how following Observation 2
helps to produce a heuristic with a high GDRC.
Observation 3 discusses nodes in a local minimum, and the difference in h between nodes
in the local minimum, and nodes that are on the edge of the local minimum. If we assume
that in order to escape a local minimum one must go through one of the nodes on the edge of
293

fiWilt & Ruml

the local minimum, then we know that the nodes in the local minimum must have a higher
d than the nodes on the edge of the local minimum, but we also know that their h is lower
(because the node is in the local minimum). This means that the h ranking incorrectly
orders all nodes in the local minimum as compared to the nodes on the edge of the local
minimum, a clear problem for producing a high GDRC. If the nodes in the local minimum
have a low d because they can get to the goal through a very high h node, the relationship
between this observation and GDRC is weaker. Consider personal transportation as an
example. An action such as, call a taxi might result in reaching states very near the goal
in one step, but at very high cost. If the heuristic recognizes the cost of this action, the node
will correctly have a high h, but be very close to the goal as measured with d because of the
call a taxi path. While such a situation clearly causes problems for domains attempting to
follow Observation 3, we believe that domains with this kind of attribute are, in practice,
quite uncommon. For example, none of our example domains exhibit this trait.
4.4 Comparing Heuristics
Because it is a quantitative metric, GDRC can be used to compare different heuristics for
the same domain. To test its effectiveness, we ran experiments on the Towers of Hanoi
problem using 17 different disjoint and non-disjoint pattern databases. We considered
pattern databases with between 3 and 8 disks, as well as a selection of pairings of the PDBs
where the total number of disks is less than or equal to 12. For each pattern database, we
calculated the GDRC for the heuristic produced by the PDB. In Figure 11 we plot, for each
PDB, the GDRC of the PDB on the X axis and the average of the log of the number of
expansions required by greedy best-first search to solve 51 random 12-disk Towers of Hanoi
problems on the Y axis. As we can see from the figure, when the GDRC is below roughly
0.4, greedy best-first search performs very poorly, but as the GDRC increases, the average
number of expansions done by greedy best-first search decreases. This suggests that it is
possible to use GDRC to directly compare heuristics against one another.
We can see similar behavior in a different domain in the left part of Figure 12. Each
dot represents one of the 462 possible disjoint 5/6 pattern databases (one 6 tile PDB and
one 5 tile PDB that are disjoint) for the 3  4 sliding tile puzzle with inverse costs. On the
Y axis is the log of the average expansions required to solve 100 random instances. On the
X axis is the GDRC. Since we are using a non-unit problem, h and d are not the same,
so we can also calculate the correlation between h and h . In the right part of Figure 12,
this correlation is on the X axis.
As we can see, GDRC and the rank correlation between h and h can both yield useful
information about how well greedy best-first search is likely to work.
For the domains we have tested, the correlation between h and d neatly predicts when
greedy best-first search performs worse than Weighted A* (or A*). It is not perfect, however.
If we consider the heuristic h(n) = h (n), any measure of the correlation between h(n) and
h (n) will be perfect, but the relationship between h(n) and d (n) for such a heuristic could
be arbitrarily poor. As the heuristic approaches truth, the h(n)-h (n) correlations will
approach 1, which allows Weighted A* to scale gracefully, as greedy best-first search will
have linear run time, no matter what the correlation between h(n) and d (n) is. In this
situation, looking solely to the correlation between h(n) and d (n) to determine whether
294

fi5.0

5.0

4.5

4.5

4.0

4.0
Log10(expansions)

Log10(expansions)

Effective Heuristics for Suboptimal Best-First Search

3.5
3.0

3.5
3.0

2.5

2.5

2.0

2.0

1.5
0.35

0.40

0.45

GDRC

0.50

0.55

1.5
0.45

0.60

0.50

0.55

0.60
0.65
0.70
0.75
GDRC (with h* instead of d*)

0.80

0.85

Figure 12: Average log of expansions done by greedy best-first search with each of the
possible 462 5/6 disjoint PDB heuristics, plotted against GDRC (left) and the
correlation between h(n) and h (n)
Domain

City Nav 5 5

Heuristic h(n)-h (n)
% Error Correlation
(Pearson)
31.19
0.9533

h(n)-h (n)
Correlation
(Spearman)
0.9466

h(n)-d (n)
Correlation
(Pearson)
0.0933

h(n)-d (n)
Correlation
(Spearman)
0.0718

Table 6: Average % error, correlation between h(n) and h (n), and correlation between
h(n) and d (n) in City Nav 5 5

or not greedy best-first search will be faster than Weighted A* may produce an incorrect
answer.
This can be seen in the City Navigation 5 5 domain. City Navigation 5 5 is similar
to the other City Navigation problems we consider, except that the cities and places are
better connected, allowing more direct routes to be taken. Since the routes are more direct,
and thus shorter, the heuristic is more accurate. Table 6 shows the various correlations
and percent error in h(n) for City Navigation 5 5. Figure 13 shows that as we increase the
weight, despite the very weak correlation between h(n) and d (n), there is no catastrophe:
greedy best-first search expands roughly the same number of nodes as Weighted A* with
the best weight for speed. This occurs because of the extreme strength of the heuristic,
which correlates to h (n) at .95, an extremely strong correlation.
The next question is which correlation matters more: h (n) or d (n). Clearly, a perfect
correlation between h (n) and h(n) or d (n) and h(n) will lead to a fast greedy best-first
search, which leads us to the conclusion that in order for greedy best-first search to be
effective, nodes with small h(n) that get expanded are required to have at least one virtue:
they should either be close to the goal measured in terms of remaining search distance
(small d (n)) or close to the goal measured in terms of remaining cost (small h (n)). We
295

fiWilt & Ruml

City Navigation 5 5

Total Nodes Expanded

3000

2000

1000

0

A* 1.1 1.2 2.5 5

10 20 G

Figure 13: Expansions done by A*, Weighted A*, and greedy best-first search on City
Navigation 5 5

have seen empirically that as the two correlations break down, the d (n) correlation allows
greedy best-first search to survive longer: in tested domains where the d (n)-h(n) is above
.58, greedy best-first search does well, whereas we have seen domains where the h (n)h(n) correlation is as high as .70 (or .75, depending on which correlation metric is being
used) where greedy best-first search performs poorly.
The importance of the correlation between h(n) and d (n) reflects the importance of
node ordering for greedy best-first search. In optimal search, the search cannot terminate
when a solution is found, but rather when the solution is known to be optimal because
all other paths have been pruned. The larger the heuristic values, the sooner nodes can
be pruned. This means that in optimal search, heuristic size is of paramount importance:
bigger is better. With greedy best-first search, the heuristic is used to guide the search to a
solution, so relative magnitude of the heuristic (or the error in the heuristic) has no bearing
on the performance of the search, as we saw when we considered the percent error in h. It
is common for researchers to say that A*s heuristic guides the search, but our discussion
reveals why this language should be reserved for suboptimal search.
Some heuristics are able to satisfy both the needs of A* and greedy best-first search
simultaneously. For example, the dynamic robot navigation heuristic works extremely well
for both A* and greedy best-first search, because it is both big, and therefore good for A*,
and good at differentiating nodes that are near the goal from those far away from the goal,
helping greedy best-first search.

5. Building a Heuristic by Searching on GDRC
As shown by Haslum, Botea, Helmert, Bonet, and Koenig (2007), given a metric for assessing the quality of a heuristic, we can use that metric to automatically construct effective
abstraction-based heuristics simply by searching the space of abstractions. In many domains, a heuristic can be constructed by initially abstracting everything, and slowly refining
296

fiEffective Heuristics for Suboptimal Best-First Search

the abstraction to construct a heuristic. While Haslum et al. (2007) were concerned with
optimal search and hence use pruning power to evaluate heuristics, our focus on greedy bestfirst search suggests that GDRC might serve as a useful metric. For example, in the TopSpin
problem, we begin with a heuristic that abstracts all disks. We then consider all PDBs that
can be devised by abstracting everything except one disk, and measure the GDRC of each
pattern database. The GDRC can be effectively estimated by doing a breadth-first search
backwards from the goal (we used 10,000 nodes for a 12 disk problem) to establish d values
for nodes, and the h value can be looked up in the pattern database. We then sample 10% of
the nodes generated in this way, and used the sample to calculate an estimate of Kendalls
 . While we elected to sample 10%, of the nodes, a sample of any size can be taken provided
the confidence interval is sufficiently small to tell which  is better. Last, we take the PDB
with the highest value as the incumbent PDB. This process repeats until either all PDBs
have a worse GDRC than the previous best PDB, or until the PDB has reached the desired
size. The reason we allow the algorithm to possibly terminate early is to cover the case of
GDRC decreasing with larger PDBs. If increasing the size of the PDB decreases GDRC,
it is likely that further increasing the size of the PDB will degrade the GDRC even more,
so we elect to terminate. The full algorithm is detailed in Algorithm 1. While this simple
hill-climbing search appears effective, a more sophisticated search strategy could certainly
be employed instead.
5.1 TopSpin
Algorithm 1 Hill Climbing PDB Builder
1: AllTokens = {Tokens in problem that can be abstracted}
2: RemainingTokens = AllT okens
3: BestPDB = build PDB by abstracting AllT okens
4: BestTau = 0
5: function tryPDB(tokens)
6:
pdb = build PDB by abstracting AllT okens \ tokens
7:
allNodes = nodes discovered by breadth first search backwards from the goal state(s)
8:
sample = randomly sample 10% of the nodes from allNodes
9:
return calcTau(sample, pdb)
10: while BestP DB.size < Max Allowed Size do
11:
LocalBestPDB, LocalBestTau, LocalBestToken = (N one, BestT au, N one)
12:
for all CurrentToken  RemainingT okens do
13:
CurrentTau, CurrentPDB = tryPDB(Ref inedT okens  {token})
14:
if CurrentT au > LocalBestT au then
15:
set local best variables to current
16:
if LocalBestP DB 6= None then
17:
set best variables to local best variables
18:
RemainingTokens = RemainingT okens \ LocalBestT okens
19:
else
20:
Break
21: return BestPDB
297

fiWilt & Ruml

PDB
Contiguous
Big Operators
Random

Greedy Exp
411.19
961.11
2,386.81

A* Exp
10,607.45
411.27
26,017.25

Avg. Value
52.35
94.37
47.99

Table 7: Expansions to solve TopSpin problem with the stripe cost function using different
PDBs

When used to generate unit-cost TopSpin pattern databases, hill-climbing on GDRC
always produced PDBs where the abstracted disks were all connected to one another, and
the refined disks were also all connected to one another. This prevents the abstraction from
creating regions where h = 0, but where the goal is nowhere near the h = 0 nodes, per
Observation 2.
With unit-cost TopSpin problems, abstractions where all of the disks are connected to
one another work well for both greedy best-first search and A*. If we change the cost
function such that moving an even disk costs 1 and moving an odd disk costs 10, we get the
stripe cost function, so called because the costs are striped across the problem. The most
effective PDBs for A* are those that keep as many odd disks as possible, because moving
the odd disks is much more expensive than moving an even disk. If we use such a big
operator pattern database for greedy best-first search, the algorithm will align the high
cost odd disks, but will have great difficulty escaping from the resulting local minimum. If
we use hill climbing on GDRC to build the heuristic, we end up with a contiguous heuristic
that keeps the abstracted and the refined disks connected to one another. Table 7 provides
results of how the various pattern databases did solving a suite of instances. We can see the
importance of creating a good pattern database when we consider the Random row in the
table, which contains the average number of expansions from 20 different randomly selected
6 disk pattern databases.
5.2 Towers of Hanoi
We can already infer from Figure 11 that, if we greedily select the PDB with the best 
from a collection of PDBs, we would select the best one. But it is certainly also possible
to use a hill-climbing search to incrementally construct a PDB. When creating a PDB
heuristic for the Towers of Hanoi, one maps the full size problem onto an abstracted version
of the problem by removing some of the disks in the larger problem, and re-indexing the
remaining disks so they map to the disks in the smaller problem. With this technique, a
critical component in terms of performance is which disks are abstracted.
We define a mapping as a selection of disks to abstract. In our example, we once again
consider a 12 disk problem using an 8 disk PDB, so we must select 4 of the 12 total disks
to abstract. In Figure 14 the + glyphs each represent a randomly selected abstraction,
and the heuristic it produced. As we can see, some abstractions produced extremely poor
quality heuristics as measured by GDRC and by the average number of expansions done
by greedy best-first search solving problems using that heuristic. Other heuristics fared
significantly better both in terms of GDRC and average expansions by greedy best-first
298

fiEffective Heuristics for Suboptimal Best-First Search

Analysis
of heuristics generated by different Hanoi PDB mappings
8
7

Log expansions

6
5
4
3
2
0.2

Randomly selected mappings
Best Mapping
Hill Climbing Mappings
0.0

0.2

0.4
GDRC

0.6

0.8

1.0

Figure 14: Expansions using different Towers of Hanoi PDB abstractions.
search. If we examine the plot in Figure 14 we can see that there are several clusters of
heuristics. The heuristics with a GDRC of about 0 are clustered together, all requiring
greedy best-first search to expand between 107 and 108 nodes. These heuristics are the
worst heuristics, because they largest disk is abstracted. The next cluster of heuristics have
a GDRC of about 0.4 to 0.5 and require greedy best-first search to expand between 106.5 and
107 nodes. These are the heuristics where the largest disk is not abstracted, but the second
largest disk is abstracted. These abstractions also produce very poor quality heuristics,
but the heuristics represent a significant improvement over the heuristics where the largest
disk is abstracted. Each color in Figure 14 represents a mapping with a different largest
abstracted disk, with the blue X glyph representing the best pattern database where only
the smallest disks are abstracted. As we can see in this plot, there is a definite overall trend
where mappings that product heuristics with a higher GDRC tend to fare better overall in
terms of average total expansions used by greedy best-first search.
The hill climbing algorithm selected the heuristic that contained disks 0, 1, 2, 4, 5, 6,
7, and 8 (skipping the 3 disk). An example of how the hill climbing algorithm selected
this heuristic can be seen in the green circles and line in Figure 14 when starting from
an abstraction that abstracted the largest disk. The hill climbing algorithm climbed a hill
leading to a reasonable, albeit not the most effective, heuristic. Despite failing to find the
optimal heuristic, the selected heuristic is quite reasonable nonetheless, falling between the
86th and the 75th percentile overall, a significant improvement for an automated approach.
5.3 City Navigation
In addition to building pattern database heuristics using hill climbing on GDRC, it is also
possible to build a portal-style heuristic by hill climbing on GDRC. Using the city navigation
domain, we defined a portal heuristic (Goldenberg, Felner, Sturtevant, & Schaeffer, 2010)
by selecting a number of nodes to be portal nodes (we used the same number of nodes as
299

fiWilt & Ruml

Heuristic
Random Portals
Nexus Portals
Hill Climbed Portals

Average GDRC
0.44
0.76
0.60

Average greedy best-first search expansions
2200
488
1117

Table 8: Expansions and GDRC using different ways to select portal nodes

cities, 150), and calculating the true distance from every node to the closest portal node.
The heuristic for two nodes is the true distance between the portals associated with each
node minus the distance of each node to its own portal. In the event that this quantity is
negative, 0 is used. This heuristic is highly accurate across long distances because it uses
the true distance between the portals, but it is obviously less accurate when comparing
two nodes that share the same portal. When constructing portal heuristics, the critical
difference between an effective portal heuristic and a poor quality portal heuristic is the
selection of which nodes are portals. We allowed our algorithm to automate this process
by hill climbing on GDRC. The algorithm is initialized with a random array of nodes as
the portals. At each step, the algorithm iterates through the indexes in its array of portals,
considering moving the location that is currently serving as a portal to a different location.
In our implementation, we considered two times the number of cities, or 300 different
random places. We then assessed the GRDC of the new heuristic using a sample of 100,000
randomly selected pairs of places. If moving the city to a new location improved GDRC, we
kept the portal array with the new place, otherwise, we discarded the change as its GDRC
is inferior to that of the incumbent. When we reach the end of the array, we restart at
the beginning. If we reach a point where we are at the same position in the array, and all
other aspects of the array remain unchanged since the last time we modified that index, the
algorithm terminates, returning the array of portals for use in a heuristic.
Results from this experiment are shown in Table 8. The average GDRC is the GDRC
that one obtains by selecting 100,000 random pairs of start and end nodes and calculating
GDRC using those nodes. The average greedy best-first search expansions is the average
number of expansions needed to solve a City Navigation problem with a random start and
goal.
We considered three different methods for selecting portal nodes. The first was to
completely randomize the selection of portal nodes, which unsurprisingly resulted in the
lowest GRDC and the highest number of expansions. The most successful method for
selecting portal nodes was to identify the nexus nodes, and use those nodes as the portals.
Unsurprisingly, this method led to the highest GDRC, and the fewest number of expansions.
This result further demonstrates the usefulness of GDRC in identifying a quality heuristic for
greedy best-first search. Last, our automatic algorithm for finding portal nodes performed
significantly better than random, while still trailing the hand-selected portals. We believe
that a better search strategy may be able to better capture the potential performance gain
offered by high GDRC heuristics.
300

fiEffective Heuristics for Suboptimal Best-First Search

5.4 Sliding Tile Puzzle
We can also compare the GDRC-generated PDBs to instance-specific PDBs for the sliding
tile puzzle (Holte et al., 2005). On this domain, in order to get an accurate estimate
of  , we had to increase the number of nodes expanded going backwards from 10,000 to
1,500,000. Following the hill climbing procedure, the algorithm selected a pattern database
that tracked the 1, 3, 4, 7, 8, and 11 tiles. The results of using this PDB are shown in Table
3. While this abstraction is not as strong as the outer L abstraction, it is the fourth best
PDB for minimizing the average number of expansions done by greedy best-first search
out of the 462 possible 6-tile pattern databases. The automatically constructed PDB is
two orders of magnitude faster than the number of expansions one would expect to do
using an average 6-tile PDB, and three orders of magnitude faster than the worst 6-tile
PDB for greedy best-first search. The GDRC-generated PDB works substantially better for
greedy best-first search then the state-of-the-art instance-specific PDBs, requiring about one
twentieth of the expansions. One additional advantage that the GDRC-generated PDB has
over instance-specific PDBs is the fact that GDRC produces a single PDB, unlike instance
specific PDBs, which produce a new PDB for every problem.
In summary, these results show that GDRC is useful for predicting the relative quality
of heuristics for greedy best-first search. They also showed that it is possible to leverage this
quantitative metric to automatically construct a heuristic for greedy best-first search, and
that the automatically created heuristics are extraordinarily effective for greedy best-first
search.

6. Related Work
As a metric, GDRC predicts that heuristics that have a high rank correlation with d will
work well. In general, the objective of h is to approximate h , not d , so one alternative way
to find a quality heuristic is to leverage this fact and try to construct a heuristic directly
that mimics d , generally referred to as d. Indeed, this approach is generally quite successful
(as opposed to relying exclusively on h), handily outperforming h in many situations (Wilt
& Ruml, 2014).
Gaschnig (1977) describes how to predict the worst case number of nodes expanded
by A*, and also discusses how weighting the heuristic can affect the worst case final node
expansion count. His predictions, however, have two limitations. First, the predictions
assume the search space is a tree, and not a graph, as is the case for many applications of
heuristic search. In addition to that, the worst case predictions only depend on the amount
of error present in the heuristic, where error is measured as relative deviation from h (n).
For A*, this criterion makes a certain amount of sense, but for greedy best-first search,
we have seen that relative deviation from h (n) cannot be used to predict when greedy
best-first search will perform poorly. Gaschnig points out that increasing the weight ad
infinitium may decrease performance, which is precisely the phenomenon we documented
in Section 2.
Chenoweth and Davis (1991) show that if the heuristic is rapidly growing with logarithmic cluster, a greedy best-first search can be done in polynomial time. A heuristic is
rapidly growing with logarithmic cluster if, for every node n, h(n) is within a logarithmic
factor of a monotonic function f of h (n), and f grows at least as fast as the function
301

fiWilt & Ruml

g(x) = x. We are not aware of any heuristics that have been proven to be rapidly growing
with logarithmic cluster.
A number of works consider the question of predicting search algorithm performance
(Korf et al., 2001; Pearl, 1984; Helmert & Roger, 2008), although the subject attracting
by far the most attention is determining how many nodes will be expanded by an optimal
search algorithm. As we saw in Section 3, the behavior of optional search does not in general
predict the behavior of GBRS. Lelis, Zilles, and Holte (2011) did an empirical analysis of
suboptimal search algorithms, predicting the number of nodes that would be expanded by
Weighted IDA*, but it is not clear if those methods can predict greedy best-first search
behavior, and thus tell us if increasing the weight too far can be detrimental.
Korf (1993) provides an early discussion of how increasing the weight may actually be
bad, showing that when recursive best first search or iterative deepening A* is used with a
weight that is too large, expansions actually increase. This paper is also an early example
of exploring how the weight interacts with the expansion count, something central to our
work.
Hoffmann (2005) discusses why the FF heuristic (Hoffmann & Nebel, 2001) is an effective way to solve many planning benchmarks when used in conjunction with enforced
hill climbing. The paper shows that in many benchmark problems, the heuristic has small
bounded-size plateaus, implying that the breadth-first search part of the enforced hill climbing algorithm is bounded, which means that those problems can be solved quickly, sometimes in linear time. Although enforced hill climbing is a kind of greedy best-first search,
its behaviour is very different from greedy best-first search when a promising path turns
into a local minimum. Greedy best-first search considers nodes from all over the search
space, possibly allowing very disparate nodes to compete with one another for expansion.
Enforced hill climbing limits consideration to nodes that are near the local minimum (with
nearness measured in edge count), which means that the algorithm only cares about how
the heuristic performs in a small local region of the space. Hoffmann (2011) extends this
concept, describing a process for automatically proving that a domain will have small local
minima.
Xu, Fern, and Yoon (2009) discuss constructing heuristics for a suboptimal heuristic
search, but the algorithm they consider is a beam search. Beam searches inadmissibly
prune nodes to save space and time, so their function is ultimately being used not to rank
nodes, but to make a decision as to whether or not to keep any one node. The function
that Xu et al. create can be used to rank nodes, but the input function requires a variety of
features of the state to function, and is created by using training data from trial search runs.
Our approach of creating a heuristic by hill-climbing on GDRC does not require training
instances, nor does it require any information about the states themselves. Hill-climbing
on GDRC does, however, have the limitation that the automatic generation of heuristics
only works when an appropriate search space can be defined, as with abstraction-based
heuristics.

7. Conclusion
Suboptimal heuristic searches rely heavily on the heuristic node evaluation function. We
first showed that greedy best-first search can sometimes perform worse than A*, and that
302

fiEffective Heuristics for Suboptimal Best-First Search

although in many domains there is a general trend where a larger weight on the heuristics
in Weighted A* leads to a faster search, there are also domains where a larger weight leads
to a slower search. It has long been understood that greedy best-first search has no bounds
on performance, and given a poor heuristic, greedy best-first search could very well expand
the entire state space, or never terminate if the state space is infinite. Our work shows
that poor performance is not just a theoretical curiosity, but that this behavior can occur
in practice.
We then considered characteristics of effective heuristics for greedy best-first search. We
showed several examples in which the conventional guidelines for building heuristics for A*
can actually harm the performance of greedy best-first search. We used this experience
to develop alternative observations and desiderata for heuristics for use with greedy bestfirst search. The first is that from every node, there should be a path to a goal that only
decreases in h. The second, an important special case of the first, is that nodes with h = 0
should be connected to a goal via nodes with h = 0. The third observation is that nodes
that require including high h nodes in the solution should themselves have as high an h
value as possible.
We then showed that the domains where greedy best-first search is effective share a
common trait of the heuristic function: the true distance from a node to a goal, defined
as d (n), correlates well with h(n). This information is important for anyone running
suboptimal search in the interest of speed, because it allows them to identify whether or
not the assumption that weighting speeds up search is true or not, critical knowledge for
deciding which algorithm to use.
Finally, we showed that goal distance rank correlation (GDRC) can be used to compare
different heuristics for greedy best-first search, and demonstrated how it can be used to
automatically construct effective abstraction heuristics for greedy best-first search.
Recent work has shown that search algorithms explicitly designed for the suboptimal
setting can outperform methods like weighted A*, which is a simple unprincipled derivative
of an optimal search (Thayer & Ruml, 2011; Thayer, Benton, & Helmert, 2012; Stern,
Puzis, & Felner, 2011). Our results indicate that the same holds true for heuristic functions
as well: suboptimal search deserves its own specialized methods. Given the importance of
suboptimal methods in solving large problems quickly, we hope that this investigation spurs
further analysis of suboptimal search algorithms and the heuristic functions they rely on.

8. Acknowledgments
We gratefully acknowledge support from NSF (award 1150068). Preliminary expositions of
these results were published by Wilt and Ruml (2012, 2015).

References
Burns, E. A., Hatem, M., Leighton, M. J., & Ruml, W. (2012). Implementing fast heuristic
search code. In Proceedings of the Fifth Symposium on Combinatorial Search.
Chenoweth, S. V., & Davis, H. W. (1991). High-performance A* search using rapidly
growing heuristics. In Proceedings of the Twelfth International Joint Conference on
Articial Intelligence, pp. 198203.
303

fiWilt & Ruml

Doran, J. E., & Michie, D. (1966). Experiments with the graph traverser program. In
Proceedings of the Royal Society of London. Series A, Mathematical and Physical
Sciences, pp. 235259.
Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.
Journal of Artificial Intelligence Research (JAIR), 30, 213247.
Felner, A., Zahavi, U., Holte, R., Schaeffer, J., Sturtevant, N. R., & Zhang, Z. (2011).
Inconsistent heuristics in theory and practice. Artificial Intelligence, 175 (9-10), 1570
1603.
Gaschnig, J. (1977). Exactly how good are heuristics?: Toward a realistic predictive theory
of best-first search. In Proceedings of the Fifth International Joint Conference on
Articial Intelligence, pp. 434441.
Gibbons, J. D. (1985). Nonparametric Statistical Inference. Marcel Decker, Inc.
Goldenberg, M., Felner, A., Sturtevant, N., & Schaeffer, J. (2010). Portal-based truedistance heuristics for path finding. In Proceedings of the Third Symposium on Combinatorial Search.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics,
SSC-4 (2), 100107.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction of pattern database heuristics for cost-optimal planning. In Proceedings
of AAAI-07, pp. 10071012.
Helmert, M. (2006). The fast downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M. (2010). Landmark heuristics for the pancake problem. In Proceedings of the
Third Symposium on Combinatorial Search.
Helmert, M., & Roger, G. (2008). How good is almost perfect?. In Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-2008), pp. 944949.
Hoffmann, J. (2005). Where Ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artifial Intelligence Research, 24, 685758.
Hoffmann, J. (2011). Analyzing search topology without running any search: On the connection between causal graphs and h+ . Journal of Artificial Intelligence Research, 41,
155229.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Holte, R., Grajkowskic, J., & Tanner, B. (2005). Hierachical heuristic search revisitied. In
Symposium on Abstracton Reformulation and Approximation, pp. 121133.
Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30 (1/2), 8193.
Kendall, M., & Gibbons, J. D. (1990). Rank Correlation Methods (Fifth edition). Edward
Arnold.
304

fiEffective Heuristics for Suboptimal Best-First Search

Korf, R., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134, 922.
Korf, R. E. (1987). Planning as search: A quantitative approach. Artificial Intelligence,
33 (1), 6588.
Korf, R. E. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Korf, R. E. (1997). Finding optimal solutions to Rubiks cube using pattern databases. In
Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI97,
pp. 700705. AAAI Press.
Korf, R. E. (2007). Analyzing the performance of pattern database heuristics. In Proceedings
of the 22nd National Conference on Artificial Intelligence, AAAI07, pp. 11641170.
AAAI Press.
Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity of iterative-deepening-A*.
Artificial Intelligence, 129, 199218.
Korf, R. E., & Taylor, L. A. (1996). Finding optimal solutions to the twenty-four puzzle.
In AAAI, Vol. 2, pp. 12021207.
Lelis, L., Zilles, S., & Holte, R. C. (2011). Improved prediction of IDA*s performance via
epsilon-truncation. In Proceedings of the Fourth Symposium on Combinatorial Search.
Likhachev, M., Gordon, G., & Thrun, S. (2003). ARA*: Anytime A* with provable bounds
on sub-optimality. In Proceedings of the Seventeenth Annual Conference on Neural
Information Processing Systems.
Likhachev, M., & Ferguson, D. (2009). Planning long dynamically feasible maneuvers for
autonomous vehicles. International Journal Robotic Research, 28 (8), 933945.
Martelli, A. (1977). On the complexity of admissible search algorithms. Artificial Intelligence, 8 (1), 113.
Nilsson, N. J. (1980). Principles of Artificial Intelligence. Tioga Publishing Co.
Parberry, I. (1995). A real-time algorithm for the (n2 -1)-puzzle. Information Processing
Letters, 56 (1), 2328.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving.
Addison-Wesley.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence,
1, 193204.
Richter, S., Thayer, J. T., & Ruml, W. (2009). The joy of forgetting: Faster anytime search
via restarting. In Proceedings of the Twentieth International Conference on Automated
Planning and Scheduling.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. Journal of Artifial Intelligence Research, 39, 127177.
Stern, R. T., Puzis, R., & Felner, A. (2011). Potential search: A bounded-cost search
algorithm. In Proceedings of the 21st International Conference on Automated Planning
and Scheduling, ICAPS.
305

fiWilt & Ruml

Sussman, G. J. (1975). A Computer Model of Skill Acquisition. New York: New American
Elsevier.
Thayer, J. T., Benton, J., & Helmert, M. (2012). Better parameter-free anytime search by
minimizing time between solutions. In Proceedings of the Fifth Annual Symposium on
Combinatorial Search, SOCS 2012.
Thayer, J. T., & Ruml, W. (2011). Bounded suboptimal search: A direct approach using inadmissible estimates. In Proceedings of the Twenty Sixth International Joint
Conference on Articial Intelligence (IJCAI-11), pp. 674679.
Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley, Reading, MA.
van den Berg, J., Shah, R., Huang, A., & Goldberg, K. Y. (2011). Anytime nonparametric
A*. In Proceedings of the Twenty Fifth National Conference on Articial Intelligence.
Wilt, C., & Ruml, W. (2012). When does weighted A* fail?. In Proceedings of the Fifth
Symposium on Combinatorial Search.
Wilt, C., & Ruml, W. (2014). Speedy versus greedy search. In Proceedings of the Seventh
Symposium on Combinatorial Search.
Wilt, C., & Ruml, W. (2015). Building a heuristic for greedy search. In Proceedings of the
Eighth Symposium on Combinatorial Search.
Xu, Y., Fern, A., & Yoon, S. (2009). Learning linear ranking functions for beam search with
application to planning. The Journal of Machine Learning Research, 10, 15711610.

306

fiJournal of Artificial Intelligence Research 57 (2016) 1-37

Submitted 03/16; published 09/16

Learning Continuous Time Bayesian Networks in
Non-stationary Domains
Simone Villa
Fabio Stella

villa@disco.unimib.it
stella@disco.unimib.it

Department of Informatics, Systems and Communication
University of Milano-Bicocca
Viale Sarca 336, 20126 Milan, Italy

Abstract
Non-stationary continuous time Bayesian networks are introduced. They allow the
parents set of each node to change over continuous time. Three settings are developed for
learning non-stationary continuous time Bayesian networks from data: known transition
times, known number of epochs and unknown number of epochs. A score function for each
setting is derived and the corresponding learning algorithm is developed. A set of numerical
experiments on synthetic data is used to compare the effectiveness of non-stationary continuous time Bayesian networks to that of non-stationary dynamic Bayesian networks. Furthermore, the performance achieved by non-stationary continuous time Bayesian networks
is compared to that achieved by state-of-the-art algorithms on four real-world datasets,
namely drosophila, saccharomyces cerevisiae, songbird and macroeconomics.

1. Introduction
The identification of relationships and statistical dependencies between components in multivariate time-series, and the ability of reasoning about whether and how these dependencies
change over time is crucial in many research domains such as biology, economics, finance,
traffic engineering and neurology, to mention just a few. In biology, for example, knowing
the gene regulatory network allows to understand complex biological mechanisms ruling the
cell. In such a context, Bayesian networks (BNs) (Pearl, 1989; Segal, Peer, Regev, Koller,
& Friedman, 2005; Scutari & Denis, 2014), dynamic Bayesian networks (DBNs) (Dean
& Kanazawa, 1989; Zou & Conzen, 2005; Vinh, Chetty, Coppel, & Wangikar, 2012) and
continuous time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller, 2002; Acerbi,
Zelante, Narang, & Stella, 2014) have been used to reconstruct transcriptional regulatory
networks from gene expression data. The effectiveness of discrete DBNs has been investigated to identify functional correlations among neuroanatomical regions of interest (Burge,
Lane, Link, Qiu, & Clark, 2009), while a useful primer on BNs for functional magnetic resonance imaging data analysis has been made available (Mumford & Ramsey, 2014). However,
the mentioned applications require the time-series to be generated from a stationary distribution, i.e. one which does not change over time. While stationarity is a reasonable
assumption in many situations, there are cases where the data generating process is clearly
non-stationary. Indeed, in the last years, researchers from different disciplines, ranging from
economics to computational biology, to sociology and to medicine have become interested
in representing relationships and dependencies which change over time.
c
2016
AI Access Foundation. All rights reserved.

fiVilla & Stella

Specifically, researchers have been interested in analyzing the temporal evolution of
genetic networks (Lebre, Becq, Devaux, Stumpf, & Lelandais, 2010), the flow over neural
information networks (Smith, Yu, Smulders, Hartemink, & Jarvis, 2006), heart failure (Liu,
Hommersom, van der Heijden, & Lucas, 2016), complications in type 1 diabetes (Marini,
Trifoglio, Barbarini, Sambo, Camillo, Malovini, Manfrini, Cobelli, & Bellazzi, 2015) and
the dependence structure among financial markets during crisis (Durante & Dunson, 2014).
According to the specialized literature on evolution models (Robinson & Hartemink, 2010),
they can be divided into two main categories: structurally non-stationary, i.e. those models
which are allowed to change their structure over time, and parametrically non-stationary,
i.e. those models which only allow the parameters values to change over time.
In this paper, the structurally non-stationary continuous time Bayesian network model
(nsCTBN) is introduced. A nsCTBN consists of a sequence of CTBNs which improves expressiveness over a single CTBN. Indeed, a nsCTBN allows the parents set of each node to
change over time at specific transition times and thus it allows to model non-stationary systems. To learn a nsCTBN, the Bayesian score for learning CTBNs is extended (Nodelman,
Shelton, & Koller, 2003). The nsCTBN version of the Bayesian score is still decomposable
by variable and it depends on the knowledge setting which can be: known transition times,
where transition times are known, known number of epochs, where only the number of transition times is known, and unknown number of epochs, where the number of transition times
is unknown. A learning algorithm for each knowledge setting is designed and developed.
Experiments against non-stationary dynamic Bayesian networks (nsDBNs) (Robinson &
Hartemink, 2010), i.e. the discrete time counterparts of nsCTBNs, have been performed.
The main contributions of this paper are the following:
 definition of the structurally non-stationary continuous time Bayesian network model;
 derivation of the Bayesian score decomposition under each knowledge setting;
 the design of algorithms for learning nsCTBNs under different knowledge settings.
A novel dynamic programming algorithm for learning nsCTBNs under the known
transition times setting is described, while learning nsCTBNs under the others settings
is performed by simulated annealing, exploiting the dynamic programming algorithm;
 performance comparison between nsCTBNs and nsDBNs under all knowledge settings
for a rich set of synthetic data generated by nsCTBNs and nsDBNs;
 performance comparison between nsCTBNs and state-of-the-art algorithms on realworld datasets, namely drosophila, saccharomyces cerevisiae and songbird;
 a nsCTBN learned on a macroeconomics dataset consisting of variables evolving at
different time granularities spanning from 1st January 1986 to 31st March 2015.
The rest of the paper is organized as follows. In Section 2 continuous time Bayesian networks are introduced together with their learning problem from complete data. Section 3
introduces non-stationary continuous time Bayesian networks, presents three learning settings and derives their corresponding Bayesian score functions. Algorithms for learning
nsCTBNs under different learning settings are described in Section 4. Numerical experiments on synthetic and real-world datasets are presented in Section 5. Section 6 closes the
paper by making conclusions and indicating directions for further research activities.
2

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

2. Continuous Time Bayesian Networks
Continuous time Bayesian networks combine Bayesian networks and homogeneous Markov
processes together to efficiently model discrete state continuous time dynamical systems
(Nodelman et al., 2002). They are particularly useful for modeling domains in which variables evolve at different time granularities, such as to model the presence of people at their
computers (Nodelman & Horvitz, 2003), to study reliability of dynamical systems (Boudali
& Dugan, 2006), to model failures in server farms (Herbrich, Graepel, & Murphy, 2007), to
detect network intrusion (Xu & Shelton, 2008), to analyze social networks (Fan & Shelton,
2009), to model cardiogenic heart failure (Gatti, Luciani, & Stella, 2011) and to reconstruct
gene regulatory networks (Acerbi & Stella, 2014; Acerbi, Vigano, Poidinger, Mortellaro,
Zelante, & Stella, 2016). Recently, the complexity of inference in continuous time Bayesian
networks has been studied (Sturlaugson & Sheppard, 2014).
2.1 Basics
The representation ability of continuous time Bayesian networks is inherent to the factorization of the system dynamics into local continuous time Markov processes that depend on
a limited set of states. The continuous time Bayesian network model is defined as follows:
Definition 1. Continuous time Bayesian network (Nodelman et al., 2002). Let X be a
set of random variables X = {X1 , X2 , . . . , XN }. Each X has a finite domain of values
V al(X) = {x1 , x2 , . . . , xI }. A continuous time Bayesian network over X consists of two
0 , specified as a Bayesian network over X,
components: the first is an initial distribution PX
the second is a continuous time transition model specified as: a directed (possibly cyclic)
P a(X)
graph G whose nodes are X1 , X2 , . . . , XN ; a conditional intensity matrix (CIM), QX
,
for each variable X  X, where P a(X) denotes the set of parents of X in the graph G.
P a(X)

The conditional intensity matrix QX

consists of the set of intensity matrices

qxpa1 u
 qxpa2 xu1
=

.
qxpaI xu1


u
Qpa
X

.
.
.
.


qxpa1 xuI
qxpa2 xuI 
,

.
pau
qxI

where
pau ranges over all possible configurations of the parents set P a(X), while qxpai u =
P
pau
pau
pau
xj 6=xi qxi xj . Off-diagonal elements of QX , i.e. qxi xj , are proportional to the probability
that the variable X transitions from state xi to state xj given the parents state pau . The
pau
u
intensity matrix Qpa
X can be equivalently summarized with two independent sets: q X =
pau
{qxi : 1  i  I}, i.e. the set of intensities parameterizing the exponential distributions
pau
pau
pau
u
over when the next transition occurs, and  pa
X = {xi xj = qxi xj /qxi : 1  i, j  I, j 6= i},
i.e. the set of probabilities parameterizing the multinomial distributions over where the
state transitions. Note that the CTBN model assumes that only one single variable can
change state at any specific instant, while its transition dynamics are specified by its parents
via the CIM, and they are independent of all other variables given its Markov Blanket.
3

fiVilla & Stella

2.2 Structural Learning
Given a fully observed dataset D, i.e. a dataset consisting of multiple trajectories1 whose
states and transition times are fully known, the problem of learning the structure of a CTBN
has been addressed as the problem of selecting the graph G  which maximizes the Bayesian
score computed on the dataset D (Nodelman et al., 2003):
BS (G : D) = ln P (G) + ln P (D|G).

(1)

where P (G) is the prior over the graph G and P (D|G) is the marginal likelihood.
The prior P (G) over the graph G, which allows us to prefer some CTBNs structures
over others, is usually assumed to satisfy the structure modularity property (Friedman &
Koller, 2000), i.e. to decompose into the following product of terms:
Y
P (G) =
P (P a(X) = P aG (X)),
(2)
XX

a term for each parents set P aG (X) in the graph G. A uniform prior over G is often used.
The marginal likelihood P (D|G) depends on the prior over parameters P (q G ,  G |G) which
is usually assumed to satisfy the global parameter independence, the local parameter independence and the parameter modularity properties, which are outlined below.
Global parameter independence (Spiegelhalter & Lauritzen, 1990) states that the paramP a (X)
P a (X)
eters q X G
and  X G
associated with each variable X in a graph G are independent,
thus the prior over parameters decomposes by variable as follows:
Y
P a (X) P a (X)
P (q G ,  G |G) =
P (q X G ,  X G |G).
(3)
XX

Local parameter independence (Spiegelhalter & Lauritzen, 1990) asserts that the parameters associated with each configuration pau of the parents P aG (X) of a variable X are
independent. Therefore, the parameters associated with each variable X are decomposable
by parent configuration pau as follows:
YY
P a (X) P a (X)
u
(4)
P (q X G ,  X G |G) =
P (qxpai u ,  pa
xi |G).
pau xi

Parameter modularity (Geiger & Heckerman, 1997) asserts that if a variable X has the same
parents P aG (X) = P aG 0 (X) in two distinct graphs G and G 0 , then the probability density
functions of the parameters associated with X must be identical:
P aG (X)

P (q X

P aG (X)

, X

P aG 0 (X)

|G) = P (q X

P aG 0 (X)

, X

|G 0 ).

(5)

Furthermore, we also assume that the sets of parameters characterizing the exponential distributions are independent of the sets of parameters characterizing the multinomial
distributions:
P (q G ,  G |G) = P (q G |G)P ( G |G).
(6)
1. A trajectory is defined to be a sequence of pairs (t, X(t)), where each transition time t  [0, T ] is
associated with the state X(t) of all the random variables corresponding to the nodes of the CTBN.

4

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

A Dirichlet distribution is selected as the prior for the parameters associated with the multinomial distribution, while a gamma distribution is selected as the prior for the parameters
associated with the exponential distribution, i.e.

P (qxpai u ) Gamma xpai u , xpai u ,
(7)

pau
pau
pau
P ( xi ) Dir xi x1 , . . . , xi xI ,
(8)
where xpai u , xpai u , xpai xu1 , . . . , xpai xuI are the priors hyperparameters. In particular, the  hyperparameters represent the pseudocounts for the number of transitions from state to state,
while the  parameter represents the imaginary amount of time spent in each state before
any data is observed. Note that the hyperparameter xpai u is inversely proportional to the
number of joint states of the parents of X. Conditioning on the dataset D, we obtain the
following posteriors over parameters:

u
,
(9)
P (qxpai u |D) Gamma xpai u + Mxpai u , xpai u + Txpa
i

pau
pau
pau
pau
pau
(10)
P ( xi |D) Dir xi x1 + Mxi x1 , . . . , xi xI + Mxi xI ,
u
and Mxpai xuj are the sufficient statistics of the CTBN (Nodelman et al., 2003).
where Txpa
i
u
In particular, Txpa
is the amount of time spent by the variable X in the state xi while its
i
parents P a(X) are in state pau , while Mxpai xuj is the number of times that the variable X
transitions from the state xi to the state xj while its parents P a(X) are in state pau 2 .
In the Bayesian score (1) the term P (G) does not grow with the size of dataset D.
Thus, the significant term is the marginal likelihood P (D|G). In the case of complete data,
while exploiting the parameters independence (6) and the global parameter independence
property (3), the marginal likelihood can be written as follows:
Y
P a (X)
P a (X)
P (D|G) =
M L(q X G |D) M L( X G |D),
(11)

XX
P aG (X)

where M L(q X

|D) is the marginal likelihood of q derived as follows:
YY
pau xi

P aG (X)

and M L( X

 (xpai u + Mxpai u + 1) (xpai u )
u
 (xpai u + 1) (xpai u + Txpa
i )

u
(pa
xi +1)

pau
u
(pa
xi +Mxi +1)

,

|D) is the marginal likelihood of  derived as follows:

Y Y
Y  xpai xuj + Mxpai xuj
 (xpai u )
,
pau
pau 
pau 

(
+
M
)


x
x
x
i
i
i xj
pa x =x
x 6=x
u

i

j

i

(12)

(13)

j

under the Bayesian-Dirichlet equivalent (BDe) metric version for CTBNs (Nodelman, 2007).
In this case, the BDe metric uses the priors (7) and (8), while the parameter modularity (5),
as well as the global (3) and the local (4) parameter independence properties are assumed
to be satisfied.
2. Please note that the number of times the P
variable X leaves the state xi while its parents P a(X) are in
pau
u
state pau is computed as follows Mxpa
=
xj 6=xi Mxi xj .
i

5

fiVilla & Stella

In conclusion, the Bayesian score (1) can be computed in closed form by assuming the
structure modularity property (2) is satisfied, and using the BDe metric as follows:
X
P a (X)
P a (X)
ln P (P a(X) = P aG (X)) + ln M L(q X G |D) + ln M L( X G |D). (14)
BS(G : D) =
XX

Since the graph G of a CTBN does not have acyclicity constraints, it is possible to maximize
the Bayesian score (14) by separately optimizing the parents set P a(X) for each variable
X. It is worthwhile to mention that if the maximum number of parents is set, then the
search of the optimal value of the Bayesian score (14) can be performed in polynomial time.
The search can be performed by enumerating each possible parents set or by using a greedy
hill-climbing procedure with operators to add, delete or reverse edges of the graph G.

3. Non-stationary Continuous Time Bayesian Networks
Continuous time Bayesian networks are both structurally stationary, as the graph does not
change over time, and parametrically stationary, as the conditional intensity matrices do
not change over time. These stationarity assumptions are reasonable in many situations,
but there are cases where the data generating process is intrinsically non-stationary and
thus CTBNs can no longer be used. Therefore, in this section, we extend CTBNs to become
structurally non-stationary. i.e. we allow the CTBNs structure to change over continuous
time.
3.1 Definition
In the non-stationary continuous time Bayesian network model, the graph of the CTBN
is replaced by a graphs sequence G = (G1 , G2 , . . . , GE ), where a graph Ge represents the
causal dependency structure of the model for the epoch e  {1, 2, . . . , E}3 . This model is
structurally non-stationary because of the introduction of the graphs sequence and it can
handle transition times that are common to the whole network and/or node-specific.
Following the notations and definitions used for non-stationary dynamic Bayesian networks, we let T = (t1 , . . . , tE1 ) be the transition times sequence, i.e. the times at which
the causal dependency structure Ge , active at epoch e, is replaced by the causal dependency
structure Ge+1 , which becomes active at epoch e + 1. An epoch is defined to be the period
of time between two consecutive transitions, i.e. the epoch e is active during the period of
time starting at te1 and ending at te . The graph Ge+1 , which is active during the epoch
e + 1, differs from the graph Ge , which is active during the epoch e, in a set of edges that
we call the set of edge changes Ge .
Figure 1 shows a graphs sequence G = (G1 , G2 , G3 , G4 ) consisting of four epochs (E = 4)
with transition times T = (t1 , t2 , t3 ). Each epoch is associated with a set of edge changes.
Specifically, the graph G2 differs from the graph G1 by the following set of edge changes
G1 = {X3  X2 , X2 6 X3 , X1 6 X2 }, the graph G3 differs from the graph G2 by the
following set of edge changes G2 = {X2  X1 } and the graph G4 differs from the graph
G3 by the following set of edge changes G3 = {X3  X4 , X4  X1 , X1 6 X4 , X4 6 X3 }.
3. It is worthwhile to mention that the first epoch, i.e. the epoch starting at time 0 and ending at time t1
is associated with the graph G1 , while the last epoch, i.e. the epoch starting at time tE1 and ending at
time T (the supremum of the considered time interval, i.e. [0,T]) is associated with the graph GE .

6

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

X1

X2

X1

X2

X1

X2

X1

X2

X4

X3

X4

X3

X4

X3

X4

X3

0

3

2

1

t2

t1

4

t3

T

Figure 1: Graphs sequence G = (G1 , G2 , G3 , G4 ) of a nsCTBN with four epochs, E = 4, and
three transition times, T = (t1 , t2 , t3 ), where the edges are gained and lost over time.
Non-stationary continuous time Bayesian networks allow each node to have its own
sequence of parents sets, each parents set being active at a given epoch. Therefore, we
introduce the concept of homogeneous interval H(X) = (h1 , . . . , hM ) associated with node
X, which is defined as the union of consecutive epochs during which the same parents set
P a(X) is active for the node X. Note that if each epoch is associated with a different
parents set, then M is equal to E.
A non-stationary continuous time Bayesian network is defined as follows.
Definition 2. (Structurally) non-stationary continuous time Bayesian network. Let X be
a set of random variables X1 , . . . , XN . Each X has a finite domain of values V al(X) =
{x1 , . . . , xI }. A (structurally) non-stationary continuous time Bayesian network Nns =
(B, Mns ) over X consists of two components:
0 , specified as a Bayesian network B over X,
 an initial distribution PX

 a non-stationary continuous time transition model Mns specified as:
 a sequence of directed (possibly cyclic) graphs G = (Ge )E
e=1 whose nodes are
X1 , . . . , XN , where E represents the number of epochs;
P a (X)

G
 a conditional intensity matrix, QX,H(X)
, X  X, where P aG (X) denotes the
parents sets of X in G, and H(X) denotes the intervals associated with X.

P a (X)

G
The conditional intensity matrix QX,H(X)
consists of a set of intensity matrices
u
qxpa1 ,h
m
pa
 q u
x2 x1 ,hm
=

.
pau
qxI x1 ,hm



u
Qpa
X,hm

.
.
.
.


qxpa1 xuI ,hm
qxpa2 xuI ,hm 
,

.
pau
qxI ,hm

one for each configuration pau of each parents set P a(X)  P aG (X) which is active during
the interval hm  H(X).4
u
4. Note that the following equation qxpai ,h
=
m

P

xj 6=xi

7

qxpai xuj ,hm still holds.

fiVilla & Stella

3.2 Learning Framework
Learning a nsCTBN from a fully observed dataset D can be done using the Bayesian learning
framework taking into account the entire graphs sequence G. In the nsCTBNs case, we must
specify the prior probability over the graphs sequence G and, for each possible sequence, the
density measure over possible values of the parameters q G and  G . Once they prior P (G)
and the likelihood P (q G ,  G |G) are given, the marginal likelihood P (D|G) can be computed
and the Bayesian score can be evaluated. It is important to note that we are focused on
recovering the graphs sequence G and not on detecting possible changes of the parameters.
In fact, we identify non-stationarity in the parameters of the model, i.e. the entries of the
conditional intensity matrices, that are significant enough to result in structural changes of
the graph. Others changes are assumed to be small enough not to alter the graph structure.
3.2.1 Prior Probability over Graphs
Given the transition times T , and thus the number of epochs E, we assume that the prior
over the nsCTBNs structure G can be written as follows:
P (G|T ) = P (G1 , ..., GE |T ) = P (G1 , G1 , ..., GE1 |T ) = P (G1 )P (G1 , ..., GE1 |T ).
(15)
Equation (15) is justified because we assume that the probability distribution over edge
changes only is a function of the number of changes performed, which can also be defined
independently of the initial graph G1 . If some knowledge about particular edges or the
overall topology is available for the initial network, then we can use an informative prior
P (G1 ) otherwise we can resort to a uniform distribution. As in CTBNs, P (G1 ) must satisfy the structure modularity assumption (2), while the prior over the set of edge changes
P (G1 , . . . , GE1 |T ) defines the way in which edges change through adjacent epochs.
3.2.2 Prior Probability over Parameters
The prior over parameters P (q G ,  G |G, T ) is selected to satisfy the following assumptions:
independence between the sets of parameters characterizing the exponential and the multinomial distributions (6), parameter modularity (5) and parameter independence. The latter
assumption is divided into three components for nsCTBNs: global parameter independence,
interval parameter independence and local parameter independence.
Global parameter independence asserts that the parameters associated with each node
in a nsCTBNs graphs sequence are independent, so the prior over parameters decomposes
by variable X as follows:
Y
P aG (X)
P aG (X)
P (q G ,  G |G, T ) =
P (q X,H(X)
,  X,H(X)
|G, T ).
(16)
XX

Interval parameter independence states that the parameters associated with each interval
of the active parents for each node are independent, so the parameters associated with each
X and its parents sets P aG (X) are decomposable by interval hm  H(X) as follows:
P a (X)

P a (X)

G
G
P (q X,H(X)
,  X,H(X)
|G, T ) =

Y
hm

8

P a (X)

P a (X)

P (q X,hGm ,  X,hGm |G, T ).

(17)

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Local parameter independence states that the parameters associated with each state of
a variable in a given interval are independent, thus the parameters associated with each X
in the interval hm  H(X) are decomposable by parent configuration pau as follows:
YY
P a (X) P a (X)
u
u
(18)
,  pa
P (qxpai ,h
P (q X,hGm ,  X,hGm |G, T ) =
xi ,hm |G, T ).
m
pau xi

As in the CTBNs case, a Dirichlet distribution is used as prior for the parameters of the
multinomial distribution and a gamma distribution is used as prior for the parameters of
u
is the
the exponential distribution. The sufficient statistics are modified as follows: Txpa
i ,hm
amount of time spent in state X = xi while P a(X) = pau in the interval H(X) = hm , while
Mxpai xuj ,hm is the number of transitions from state X = xi to state X = xj while P a(X) = pau
P
in the interval H(X) = hm . We let Mxpai ,hu m = xj 6=xi Mxpai xuj ,hm to be the number of times
X leaves state xi while its parents P a(X) are in state pau during the interval H(X) = hm .
3.2.3 Marginal Likelihood
Given the graphs sequence G, and the transition times T , the marginal likelihood P (D|G, T )
of the dataset D can be computed in closed form using the priors and the sufficient statistics
previously defined. To derive the Bayesian-Dirichlet equivalent metric for nsCTBNs, we
make the same assumptions as those for CTBNs. In this case, the parameter independence
assumption is divided into global (16), interval (17) and local (18) parameter independence.
Therefore, the marginal likelihood becomes:
Y
P aG (X)
P aG (X)
P (D|G, T ) =
M L(q X,H(X)
|D) M L( X,H(X)
|D).
(19)
XX

The marginal likelihood of q in equation (19) can be calculated as follows:
(pau +1)


xi ,hm
pau
pau
u
+
1

+
M
 xpai ,h
Y
Y
Y
xi ,hm
xi ,hm
m
P aG (X)
M L(q X,H(X) |D) =
(pau +M pau +1) ,


xi ,hm
xi ,hm
pau
pau
pa
hm pau xi   u + 1
xi ,hm + Txi ,hm
xi ,hm

(20)

while the marginal likelihood of  in equation (19) can be calculated as follows:




pau
pau
u
 xpai ,h


+
M
Y
Y
Y
Y
xi xj ,hm
xi xj ,hm
m
P aG (X)




M L( X,H(X)
|D) =
.
pau
pau
pau
 xi xj ,hm
hm pau xi =xj  xi ,hm + Mxi ,hm
xi 6=xj
(21)
It is important to note that for nsCTBNs, the pseudocounts  as well as the imaginary
amount of time  are associated with each interval. This aspect requires a careful choice in
order not to be too biased towards these values when small intervals are analyzed.
A possible correction is to weight the CTBNs hyperparameters by a quantity proportional to the time interval width (hm  hm1 ), where hM denotes the total time. Thus, the
nsCTBNs hyperparameters could be defined as follows:
xpai xuj ,hm
xpai ,hu m

(hm  hm1 )
,
hM
(hm  hm1 )
= xpai u
.
hM
= xpai xuj

9

(22)
(23)

fiVilla & Stella

If you want to control the parameter priors using only two hyperparameters  and  ,
then you can use the uniform BDe for nsCTBNs (BDeu). In this case, the hyperparameters
defined in (22) and (23) are divided by the number U of possible configurations of the
parents P a(X) of node X times the cardinality I of the domain of X, as follows:
xpai xuj ,hm

=

xpai ,hu m

=

 (hm  hm1 )
,
UI
hM
 (hm  hm1 )
.
UI
hM

(24)
(25)

Equations (22) and (23) rescale the hyperparameters in such a way not to be biased with
respect to the epochs length, while equations (24) and (25) are based on the uniform
distribution and they have been used for performing all numerical experiments.
3.3 Bayesian Score Decomposition
The Bayesian score can be decomposed by variable based on the information available
about the transition times. In this regard, three knowledge settings are used to derive the
Bayesian score, namely: known transition times (KTT), known number of epochs (KNE)
and unknown number of epochs (UNE).
3.3.1 Known Transition Times
In this setting, the transition times T are known. Thus, the prior probability over the
graphs sequence P (G|T ) decomposes as in equation (15), while the marginal likelihood
decomposes by variable X according to equation (19).
Therefore, the Bayesian score BS(G : D, T ) can be written as follows:
BS(G : D, T ) = ln P (G1 ) + ln P (G1 , . . . , GE1 |T )
P a (X)

P a (X)

G
G
+ ln M L(q X,H(X)
|D) + ln M L( X,H(X)
|D).

(26)

In such a setting the structural learning problem of a non-stationary continuous time
Bayesian network consists of finding the graph G1 active during the first epoch (e = 1) and
the E  1 sets of edge changes G1 , . . . , GE1 together with the corresponding parameters
values, which maximize the Bayesian score defined in equation (26).
The graphs G2 , . . . , GE are selected by making assumptions on the ways by which the
edges change over continuous time. A common approach (Robinson & Hartemink, 2010)
consists of assuming that the graphs sequence G = (G1 , . . . , GE ) depends on a parameter
which controls the number of edge changes over continuous time. This approach uses a
truncated geometric distribution, with parameter p = 1  exp(c ), to model the number
of parents changes occurring at transition time te+1 :
X
ce =
|Ge (X)|.
(27)
XX

The variable ce counts the number of edge changes between two consecutive graphs Ge and
Ge+1 , while the parameter c controls the impact of the number of edge changes ce on the
score function (26).
10

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

If the edge changes Ge are assumed to be mutually independent, then the probability
for the edge changes through subsequent epochs can be written as follows:
P (G1 , . . . , GE1 |T ) =

E1
Y
e=1

E1
Y
(1  exp(c ))(exp(c ))ce

(exp(c ))ce ,
1  (exp(c ))cmax +1

(28)

e=1

where cmax is the truncation term. Therefore, if we assume a truncated geometric distribution on the number of parents changes occurring at each transition times and equation
(28) holds, then the Bayesian score (26) decomposes by variable X as follows:
BS(G : D, T ) =

X

ln P (P a(X) = P aG1 (X))  c

XX
P a (X)

G
+ ln M L(q X,H(X)
|D) +

E1
X

ce
e=1
P aG (X)
ln M L( X,H(X)
|D).

(29)

It is worthwhile to notice that the number of parents changes ce for each epoch e
penalizes the Bayesian score, and thus it discourages sudden variations in the parents set
between consecutive epochs, while the parameter c controls the impact of such changes on
the score function (26).
3.3.2 Known Number of Epochs
If the transition times T are unknown, then the Bayesian score can be written as follows:
BS(G, T : D) = ln P (G, T ) + ln P (D|G, T ).

(30)

Assuming that P (G, T ) = P (G)P (T ) the Bayesian score (30) becomes:
BS(G, T : D) = ln P (G) + ln P (T ) + ln P (D|G, T ).

(31)

If the number of epochs E is known, then the prior probability P (G) over the graphs
sequence G decomposes as in equation (15), while a truncated geometric distribution can
be used on the number of parents changes occurring at each transition time, as in the
known transition times setting.
Any choice for P (T ) can be made to include prior knowledge about the set of transition
times. However, if no information is available, a uniform prior on P (T ) is used, implying
that all possible values of transition times are equally likely for a given number of epochs
E. Thus, the Bayesian score (31) can be decomposed by variable X as follows:
BS(G, T : D) = ln P (T ) +
+

X

ln P (P a(X) = P aG1 (X))  c

XX
P aG (X)
ln M L(q X,H(X) |D)

E1
X

ce

e=1
P a (X)

G
+ ln M L( X,H(X)
|D),

(32)

where ce counts the number of edge changes between two consecutive parents sets, while c
controls the impacts on BS(G, T : D) of such edge changes, as it happens under the KTT
setting.
11

fiVilla & Stella

3.3.3 Unknown Number of Epochs
If the number of epochs E is unknown, then transition times T are unknown as well.
Under this setting, we learn a nsCTBN by exploiting what introduced under the KTT
and KNE settings. We assume that the structure of the non-stationary continuous time
Bayesian network can evolve at different speeds over continuous time. Such an assumption
is incorporated by using a truncated geometric distribution with parameter p = 1exp(e )
on the number of epochs. In general, large values of e encode the strong prior belief that
the structure of the nsCTBN changes slowly (i.e. few epochs exist).
Following what we presented under the KTT setting, the Bayesian score can be obtained
by subtracting the parameter e times the number of epochs E. Therefore, the Bayesian
score BS(G, T : D) decomposes by variable X as follows:
BS(G, T : D) = ln P (T )  e E +

X

ln P (P a(X) = P aG1 (X))  c

ce

e=1

XX
P a (X)

E1
X

P a (X)

G
G
+ ln M L(q X,H(X)
|D) + ln M L( X,H(X)
|D).

(33)

Note that the Bayesian score (33) contains two parameters, namely c and e , which
encode our prior belief about the structure of the nsCTBN. Specifically, the parameter c
regulates our prior belief about the smoothness of the edge changes (e.g. encouraging or
discouraging the edge changes per epoch), while the parameter e regulates our prior belief
about the number of epochs (e.g. encouraging or discouraging the creation of epochs).

4. Structural Learning
The optimal structure of nsCTBNs can be found by separately maximizing the components
of the Bayesian score associated with each node. This can be achieved by using an exact optimization algorithm based on dynamic programming when the transition times are
given. By contrast, when only the number of epochs is known or no information about the
transition times is available, we have to resort to approximate techniques based on Monte
Carlo or on simulated annealing. We present the exact algorithm for solving the structural
learning problem under the KTT setting. Then, we briefly outline the stochastic algorithms
to solve the structural learning problem under the KNE setting and under the UNE setting.
4.1 Known Transition Times
Under this setting the Bayesian score decomposes according to equation (29). Thus, the
optimal graphs sequence G  can be found by separately searching the optimal parents sequence G X for each node X. To solve the problem of finding the optimal parents sequence
G X for node X we consider a sequence consisting of M intervals H(X) = (h1 , . . . , hM ) and
S possible parents, so to have Z = 2S possible parents sets. To find the optimal parents
sequence G X we must compute M  Z marginal likelihood terms associated with q and ,
one marginal likelihood term for each possible parents set P az (X) and each interval hm .
Then, an optimization algorithm can be used to find the maximum of the component of the
Bayesian score associated with the node X.
12

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

An exhaustive search would be prohibitive, as it would require evaluating Z M scores,
one for each possible parents sequence G X . Unfortunately, also a greedy search strategy
that computes the parents set which maximizes the Bayesian score for each interval is not
viable. In fact, the function that counts the parents changes ce in (29) binds the choice of
the subsequent parents set, i.e. it binds Ge to Ge+1 .
However, the relation between the score of the variable X associated with the parents set
P a(X)
P a(X) in the interval hm , denoted as BSX,hm , and the score associated with the parents set
P a(X)

P a(X) in the interval hm1 , denoted as BSX,hm1 , can be defined by recursion as follows:
n
o
P a(X)
P a (X)
P a(X) P a(X)
BSX,hm = max BSX,hzm1  c cX,e + ln M L(q X,hm ,  X,hm |D) ,
(34)
P az

where cX,e = |Ge (X)|, while the marginal likelihoods of q and  are grouped together.
P a(X)
The score BSX,hm , associated with the parents set P a(X) for node X in the interval hm , is
introduced to clarify the recursion used in the Algorithm 1. Note that this score depends on
all the components of the score up to hm . In particular, not only the marginal likelihoods
component is involved, but also the term cX,e , which counts the parents changes, is included
as it binds the choice of subsequent parents sets. Equation (34) is exploited by dynamic
programming to select the optimal parents sequence G X for each node X.
Algorithm 1 takes as input the marginal likelihoods of q and  for each interval and
parents set, the prior probability about the initial parents set, the number of parents
changes, and the parameter c . Algorithm 1 ensures the optimal parents sequence G X for
the node X and its corresponding optimal Bayesian score. Its core is the computation of
the M  Z score matrix, denoted by SC, through the dynamic programming recursion. The
dynamic programming recursion for the interval h1 (m = 1) is defined as follows:
P a (X)

SC1z = ln M L(q X,hz1

P a (X)

,  X,hz1

|D) + ln P (P az (X) = P aGh1 (X)),

(35)

for 1  z  Z, while, for the intervals hm (m = 2, . . . , M ), the recursion is:
n
o
P a (X) P a (X)
z
u
SCm
= max SCm1
+ ln M L(q X,hzm ,  X,hzm |D)  c cX,e .
1uZ

After filling the score matrix SC, the value maxz {SC[M, z]} is the optimal Bayesian score,
while the optimal parents sequence is reconstructed backwards from M to 1 by using the
index matrix IN . The cost of computing the dynamic programming recursion is O(M Z 2 ),
which is polynomial for a fixed maximum number of parents S.
The problem of selecting the optimal parents sequence has an interesting graph representation. Indeed, it is possible to create a graph whose nodes are associated with marginal
likelihoods of q and  for the interval hm and for the parents set P az (X), while each node associated with the interval hm is linked with all the nodes associated with the interval hm+1 .
Each arc is associated with a weight computed as the difference between the marginal likelihoods in the interval hm for the parents set P az (X) and the cost of switching from the
parents set of the interval hm1 to the parents set of the interval hm . Two special nodes
are added to represent the start and the end of the optimal parents sequence. Such a graph
does not have cycles, thus the selection of the optimal parents sequence for each node can
be reduced to the longest path problem from the start node to the end node of a directed
acyclic graph, and thus it can be solved using either dynamic or linear programming.
13

fiVilla & Stella

Algorithm 1 LearnKTTX
Require: matrix containing the marginal likelihoods of q and  M LX[M, Z], vector containing the prior probability about the initial parents set P R[Z], matrix containing the
number of parents changes C[Z, Z] and the parameter for the parents changes c .
Ensure: score matrix SC[M, Z] and index matrix IN [M, Z].
1: Initialize SC[m, z]  , IN [m, z]  0.
2: for m  1, . . . , M do
3:
for z  1, . . . , Z do
4:
if (m = 1) then
5:
SC[m, z]  ln M LX[m, z] + ln P R[z]
6:
else
7:
for w  1, . . . , Z do
8:
score  SC[m  1, w] + ln M LX[m, z]  c C[w, z]
9:
if (score > SC[m, z]) then
10:
SC[m, z]  score
11:
IN [m, z]  w
12:
end if
13:
end for
14:
end if
15:
end for
16: end for
Learning a nsCTBN can be done following the following four steps procedure: i) use
u
the dataset D to compute for each variable X the sufficient statistics Txpa
and Mxpai xuj ,hm
i ,hm
according to the given transition times T ; ii) compute the marginal likelihoods (20) and (21),
and then fill the M LX matrix; iii) run Algorithm 1 for each node X to get the corresponding
optimal parents sequence; iv) collect the optimal parents sequence for each node X and
compute the corresponding CIMs using the sufficient statistics already computed in step i).
If we allow the intervals to differ from the transition times, i.e. they can be obtained
as one of all the possible unions of transition times; then we have to repeat the learning
procedure for all the E  (E  1)/2 cases. It is possible to speed up the computation
because the sufficient statistics can be aggregated through intervals. In such a way, we read
the dataset once, while the precomputed marginal likelihoods can be stored and reused for
the same intervals. Moreover, the computations can be performed in parallel for each node.
4.2 Known Number of Epochs
In this setting, we know the number of epochs, but the transition times are not given, so we
cannot directly apply Algorithm 1. However, once a tentative allocation T of the transition
times is given, we can apply Algorithm 1 to obtain the optimal nsCTBNs structure, under
the assumption that T is not too different from the true transition times T . To find an
optimal tentative allocation T  , i.e. an allocation that is as close as possible to T , we apply
the simulated annealing (SA) algorithm (Kirkpatrick, Gelatt, & Vecchi, 1983).
14

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Simulated annealing is an iterative algorithm that attempts to find the global optimum
x of a given function f (x) through a stochastic search over the feasible region. At iteration
k, when the SA algorithm is assumed to be in state xk , it samples a proposal state x0
according to some proposal distribution x0  P 0 (|xk ). Then, the SA algorithm computes the
quantity  = exp ((f (x)  f (x0 ))/CT ), where CT is the computational temperature. The
SA algorithm accepts the proposal state x0 with probability equal to min{1, }. Concisely,
SA always accepts any proposal state x0 where f (x0 ) > f (x) by setting xk+1 = x0 , while it
accepts the proposal state x0 when f (x0 ) < f (x) with probability  by setting xk+1 = x0
with probability  and xk+1 = xk with probability (1  ), i.e. in this case the state of
the SA algorithm does not change. The computational temperature reduces over iterations
according to a cooling schedule. It has been shown that if one cools sufficiently slowly, then
the algorithm will probably find the global optimum (Kirkpatrick et al., 1983). The design
of the cooling schedule is an important part of the SA algorithm (Bertsimas & Tsitsiklis,
1993). A possible approach is to use an exponential cooling schedule defined as follows:
CTk = CT0   k , where CT0 represents the initial temperature, typically set to 1.0,  is the
cooling rate, usually set to be close to 0.8, while k is the current iteration (Murphy, 2012).
In the nsCTBNs case, the state of the SA algorithm x is associated with the tentative
allocation T , while the function f (x) is the Bayesian score (32). Algorithm 2 takes as input
the sufficient statistics, the parameters used to run Algorithm 1 and the parameters of the
SA algorithm. It solves the structural learning problem under the KNE setting for a given
variable X by ensuring the optimal tentative allocation T  and its corresponding score.
Algorithm 2 LearnKNEX
Require: sufficient statistics SuffStatsX, prior probability P R[], number of parents
changes C[, ], parameter c , tentative allocation T , initial temperature CT0 , cooling
rate , number of iterations Iters, truncation parameter z and standard deviation .
Ensure: optimal tentative allocation T  and best Bayesian score bestSC.
1: Initialize k  0, T   T .
2: M LX  GetMLX(SuffStatsX , T )
3: bestSC  LearnKTTX(M LX, P R[], C[, ], c )
4: while (k < Iters) do
5:
T  TentativeAllocation(T  , z, )
6:
M LX  GetMLX(SuffStatsX , T )
7:
tentSC  LearnKTTX(M LX, P R[], C[, ], c )
8:
CT  CT0   kn

o
9:
accP rob  min 1, exp  (bestSCtentSC)
CT
10:
ur  UniRand()
11:
if (ur  accP rob) then
12:
T   T
13:
currSC  tentSC
14:
end if
15:
k k+1
16: end while
17: bestSC  currSC
15

fiVilla & Stella

The simulated annealing parameters we used include the tentative allocation T , the
initial temperature CT0 , the cooling rate  and the number of iterations Iters for the
exponential cooling schedule. Moreover, the truncation parameter z and standard deviation
 are used for the selection of the new tentative allocation T 0 according to the random
procedure shown in Algorithm 3. This procedure selects a transition time through a discrete
uniform distribution, UniRandDiscr(T ), and perturbs it according to a truncated normal
distribution, StdNormRand(), having a standard deviation equal to , with the addition of
point masses at z and z, where z represents the truncation parameter.
Algorithm 3 TentativeAllocation
Require: tentative allocation T , truncation parameter z and standard deviation .
Ensure: new tentative allocation T 0 .
1: t  UniRandDiscr(T )
2: T 0  T \ t
3: nr  StdNormRand()
4: if (nr < z) then
5:
nr  z
6: end if
7: if (nr > z) then
8:
nr  z
9: end if
10: t  t + nr  
11: T 0  T  t

4.3 Unknown Number of Epochs
In this setting the number of epochs is unknown; thus the structural learning algorithm
must be able to move across a different number of epochs, as well as the corresponding
transition times. Also in this case, we used a simulated annealing algorithm where the state
x is the tentative allocation T and the function to be optimized f (x) is the Bayesian score
shown in equation (33). The cooling schedule has been set the same as the one used under
the KNE setting. The proposal distribution differs from the one used under the KNE setting
as it uses two additional operators, namely the split and the merge operators. The split
operator allows to split a given interval [tm ; tm+1 ) into two subintervals [tm ; t) and [t; tm+1 )
where tm , tm+1  T . The merge operator allows to merge contiguous intervals [tm1 ; tm )
and [tm ; tm+1 ) to form the wider interval [tm1 ; tm+1 ) where tm1 , tm , tm+1  T .
The new state is obtained by sampling the number of epochs changes ec from a multinoulli distribution with parameters (p1 , p2 , p3 ), where p1 represents the probability that the
number of epochs of the next iteration |T | is decreased by one; p3 represents the probability
that the number of epochs of the next iteration |T | is increased by one, and p2 represents
the probability that number of epochs of the next iteration |T | does not change with respect
to the current one. If ec is equal to 2, then Algorithm 2 is invoked, if ec is equal to 1, then
the merge operator is applied before invoking Algorithm 2, while if ec is equal to 3, then
the split operator is applied before invoking Algorithm 2.
16

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Algorithm 4 solves the structural learning problem of nsCTBN under the UNE setting
for a given node X by ensuring the optimal tentative allocation T  and its corresponding
Bayesian score. This algorithm is similar to the one used under the KNE settings, but it
uses Algorithm 5 to apply the split and merge operators. The left(t) function in Algorithm
5 returns the transition time in T which comes immediately before transition time t.
Algorithm 4 LearnUNEX
Require: sufficient statistics SuffStatsX, prior probability P R[], number of parents
changes C[, ], parameter c , parameter e , tentative allocation T , initial temperature
CT0 , cooling rate , number of iterations Iters, truncation parameter z, standard deviation , split probability sp and merge probability mp.
Ensure: optimal tentative allocation T  and best Bayesian score bestSC.
1: Initialize k  0, T   T .
2: bestSC LearnKTTX(GetMLX(SuffStatsX, T ), P R[], C[, ], c ) e |T |
3: while (k < Iters) do
4:
T  SplitMerge(T  , sp, mp)
5:
T  TentativeAllocation(T , z, )
6:
tentSC  LearnKTTX(GetMLX(SuffStatsX, T ), P R[], C[, ], c ) e |T |
7:
CT  CT0   kn

o
8:
accP rob  min 1, exp  (bestSCtentSC)
CT
9:
ur  UniRand()
10:
if (ur  accP rob) then
11:
T   T
12:
currSC  tentSC
13:
end if
14:
k k+1
15: end while
16: bestSC  currSC
Algorithm 5 SplitMerge
Require: tentative allocation T , split probability sp and merge probability mp.
Ensure: new tentative allocation T 0 .
1: T 0  T
2: p  UniRand()
3: if (p < mp) then
4:
t  UniRandDiscr(T )
5:
T 0  T \ t
6: else
7:
if (p < (mp + sp)) then
8:
t  UniRandDiscr(T  T )
9:
nt  left(t) + tleft(t)
2
10:
T 0  T  nt
11:
end if
12: end if
17

fiVilla & Stella

5. Numerical Experiments
Numerical experiments are performed on both synthetic and real-world datasets. Synthetic
datasets are used to compare nsCTBNs to nsDBNs under the KTT, KNE and UNE knowledge settings in terms of accuracy, precision, recall and F1 measure. The following real-world
datasets: drosophila, saccharomyces cerevisiae and songbird, are used to compare nsCTBNs
to state-of-the-art algorithms, i.e. TSNI (a method based on ordinary differential equations),
nsDBN (Robinson & Hartemink, 2010) and non-homogeneous dynamic Bayesian networks
with Bayesian regularization (TVDBN) (Dondelinger, Lebre, & Husmeier, 2013), under the
UNE knowledge setting. Drosophila, saccharomyces cerevisiae and songbird datasets are
collected at fixed time intervals, thus we analyzed an additional real-world dataset, consisting of financial/economic variables evolving at different time granularities, to exploit the
expressiveness of nsCTBNs when events occur asynchronously. Note that while the performance comparison using synthetic datasets benefits from the knowledge of the ground
truth, the same does not apply to the performance comparison using real-world datasets
because the ground truth is not available. In such cases, the comparison exploits partial
and meta-knowledge available in the specialized literature.
5.1 Synthetic Datasets
Artificially generated datasets include data sampled from a rich set of nsDBN models,
i.e. nsDBN generated datasets, and a rich set of nsCTBN models, i.e. nsCTBN generated
datasets. Such nsDBN and nsCTBN models consist of five nodes associated with binary
and ternary variables. Numerical experiments concern learning the parents sets, transition
times and the number of epochs for a single node. This choice is motivated by the fact that
structural learning for nsCTBN can be performed for each single node independently from
the remaining ones. However, when transition times are unknown, having multiple parents
sets changes could make it easier to correctly identify the times of change.
5.1.1 nsDBN Generated Datasets
nsDBN generated datasets were sampled from nsDBN models5 associated with the following
number of epochs E  {2, 3, 4, 5}. In particular, for each number of epochs E, 10 different
nsDBN instances were sampled to obtain a number of datasets equal to 10, each one consisting of a single trajectory. Thus, 40 synthetic datasets were used to learn the structure
of nsDBN and nsCTBN (number of models =2) under the KTT, KNE and UNE settings.
Structural learning experiments were performed with c = {1, 2, 4} and e = {5, 10, 15}
for nsCTBN and s = {1, 2, 4} and with m = {10, 50, 100} for nsDBN6 . An overall number
of 1,200 experiments have been performed. In particular, we performed number of epochs 
number of datasets  number of c or s  number of models = 41032 = 240 experiments
under the KTT setting, 240 under the KNE setting, while number of epochs  number of
datasets  number of c or s  number of e or m  number of models = 410332 = 720
experiments have been performed under the UNE setting.
5. Inter-slice arcs are allowed, while intra-slice arcs are not allowed. This holds true for all nsDBN models
sampled to obtain the nsDBN generated datasets.
6. It is worthwhile to mention that the s and m parameters are the nsDBN counterparts of the c and
e parameters for the nsCTBN.

18

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

The nsdbn jar executable7 (Robinson & Hartemink, 2010) was used for structural learning of nsDBN, where we set the maximum number of proposed networks to 500,000 and
the burn-in period to 50,000 for nsDBN. nsCTBN were learned by using the following parameters setting: Iters = 1,000, CT0 = 1,000,  = 0.8, z = 3,  = 1, sp = 0.3, mp = 0.3,
 = 1 and  = 0.1 using the BDeu metric. Furthermore, for nsDBN and nsCTBN we set
the maximum number of parents to 4. Only arcs that occurred in more than 90 percent
of the samples8 belong to the inferred nsDBN and nsCTBN models. Accuracy (Acc), precision (P rc), recall (Rec) and F1 measure (F1 ) achieved by nsDBN and nsCTBN learned
under the KTT, KNE and UNE settings are reported in Table 1, 2 and 3 respectively. It
is worthwhile to mention that under the KNE and UNE settings, nsDBNs and nsCTBNs
almost always identified the correct number of epochs and the location of their associated
transition times. Accuracy, precision, recall and F1 measure have been computed in two
different ways. Firstly, we included all arcs of the true network for each epoch. Secondly, we
excluded the self-reference arcs, i.e. those arcs connecting the same node in two consecutive
time-slices of the true network for each epoch. In fact, while each node of a nsCTBN has the
self-reference arc by default, the same does not happen for nsDBNs. This means that in the
first case a nsDBN is required to learn arcs that a nsCTBN is not required to do. Therefore,
to ensure a fair comparison of nsCTBN to nsDBN we adopted the second case. Tables 1,
2 and 3 report the performance measure values computed by excluding self-reference arcs
from the set of arcs of the true networks for each epoch.
Table 1: nsCTBN compared to nsDBN under the KTT setting for nsDBN generated data.
Average, min (subscript) and max (superscript) performance values over 10 networks and
c for nsCTBN and s for nsDBN.
Number of epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.961.00
0.93
0.901.00
0.67
0.771.00
0.40
0.801.00
0.57

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.93
0.791.00
0.50
0.670.83
0.43
0.710.91
0.47

1.00
0.920.75
1.00
1.001.00
1.00
0.860.63
1.00
0.920.77

0.950.99
0.89
0.871.00
0.40
0.650.90
0.25
0.730.95
0.31

1.00
0.820.63
1.00
0.960.75
1.00
0.700.38
1.00
0.800.50

0.940.97
0.89
0.851.00
0.50
0.580.80
0.25
0.690.86
0.35

0.820.95
0.55
0.991.00
0.80
0.710.91
0.33
0.820.95
0.47

According to Tables 1, 2 and 3, nsDBNs consistently achieve greater accuracy values
than those achieved by nsCTBNs under the three settings. Furthermore, for nsDBNs the
accuracy is stable with respect to the number of epochs E while the same does not happen
for nsCTBNs. Indeed, when the number of epochs E is greater than 3, nsCTBNs achieve
accuracy values which are significantly smaller than those achieved when the number of
epochs E is equal to 2 or 3. The same does not happen to nsDBNs where the accuracy is
robust with respect to the number of epochs E.
7. We acknowledge the precious help of Alex Hartemink who let us use the nsdbn jar executable program
for learning nsDBN models. Furthermore, he also provided the drosophila and songbird datasets.
8. Samples are obtained under the same parameters values.

19

fiVilla & Stella

Table 2: nsCTBN compared to nsDBN under the KNE setting for nsDBN generated data.
Average, min (subscript) and max (superscript) performance values over 10 networks and
c for nsCTBN and s for nsDBN.
Number of epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.941.00
0.88
0.911.00
0.69
0.761.00
0.39
0.801.00
0.58

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.92
0.791.00
0.50
0.680.84
0.35
0.720.91
0.51

1.00
0.920.75
1.00
1.000.95
1.00
0.860.63
1.00
0.920.77

0.940.99
0.89
0.860.97
0.55
0.650.91
0.25
0.740.95
0.38

1.00
0.810.63
1.00
0.950.75
1.00
0.710.38
1.00
0.810.50

0.930.96
0.87
0.850.96
0.55
0.590.78
0.24
0.700.74
0.35

0.820.95
0.55
0.981.00
0.80
0.700.91
0.33
0.810.95
0.47

Table 3: nsCTBN compared to nsDBN under the UNE setting for nsDBN generated data.
Average, min (subscript) and max (superscript) performance values over 10 networks and
c , e for nsCTBN and s , m for nsDBN.
Number of epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.951.00
0.88
0.910.98
0.70
0.750.98
0.38
0.790.96
0.55

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.93
0.801.00
0.52
0.660.81
0.41
0.700.89
0.48

1.00
0.920.75
1.00
1.001.00
1.00
0.860.63
1.00
0.920.77

0.930.98
0.89
0.870.95
0.67
0.650.87
0.26
0.740.87
0.41

0.99
0.810.61
1.00
0.950.73
0.99
0.700.36
0.99
0.800.48

0.920.96
0.89
0.840.90
0.71
0.570.78
0.23
0.680.85
0.34

0.810.93
0.55
0.981.00
0.80
0.690.87
0.33
0.810.93
0.47

A different picture emerges when focusing on the task to discover positive arcs. Indeed,
in such a case nsCTBNs achieve values of precision, recall and F1 measure, which are always
greater than those achieved by nsDBNs. nsCTBNs achieve precision values which are robust
with respect to the knowledge settings and the number of epochs E. The same does not
hold true for the recall performance measure. Indeed, nsCTBNs achieve a robust recall
with respect to the knowledge settings (KTT, KNE and UNE), while the recall achieved
by nsCTBNs significantly degrades when moving from 2 to 3 epochs under all knowledge
settings. The same happens to the F1 measure achieved by nsCTBNs. The results of
numerical experiments suggest that nsCTBNs are more effective than nsDBNs to discover
positive arcs, even if the datasets have been generated using nsDBNs. A possible explanation
for this behavior is that learning nsDBNs is more difficult than learning nsCTBNs. In
particular, nsDBNs must learn self-reference arcs while nsCTBNs do not. Furthermore, for
each node, nsCTBNs learn locally the sequence of parents sets while the same does not
happen for nsDBNs. In fact, nsDBNs learn globally the sequence of parents sets for all
nodes, i.e. they globally learn the sequence of networks, and thus they solve a learning
problem which is more difficult than the one solved by nsCTBNs.
20

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

5.1.2 nsCTBN Generated Datasets
We generated 40 synthetic datasets with E  {2, 3, 4, 5}, these datasets are then used to
learn the structure of nsCTBN under the three knowledge settings. The same parameters
setting is used as the one used for nsCTBN learning from nsDBN generated datasets (for
nsCTBN, we used  = 1,  = 0.1 and the BDeu metric), while, in this case, we did not
perform structural learning experiments for nsDBN models9 . The graphical structures of
the nsCTBN models sampled to obtain the datasets are the same as those sampled to
obtain the nsDBN datasets. The goal of these experiments is to analyze the performance
of nsCTBN structural learning algorithms under the three knowledge settings.
The analysis of data reported in Tables 4, 5 and 6 brings us to conclude that the
nsCTBN structural learning algorithms work very well under the three settings according
to the considered performance measures. Accuracy, recall and F1 measure decrease slightly
when the number of epochs increases from 2 to 5. In particular, the recall measure suffers
the greatest decrease from 1 to 0.95 when the number of epochs increases from 2 to 5.
Accuracy and F1 measure are very robust with respect to the number of epochs, while
precision is the most robust performance measure with respect to different datasets and
different values of the number of epochs under all knowledge settings.
Table 4: nsCTBN under the KTT setting for nsCTBN generated data. Average, min
(subscript) and max (superscript) performance values over 10 networks and c .

Acc
P rec
Rec
F1

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Number of
3
0.991.00
0.92
1.001.00
1.00
0.991.00
0.86
0.991.00
0.92

epochs E
4
0.991.00
0.94
1.001.00
1.00
0.991.00
0.89
0.991.00
0.94

5
0.981.00
0.90
1.001.00
1.00
0.961.00
0.86
0.981.00
0.92

Table 5: nsCTBN under the KNE setting for nsCTBN generated data. Average, min
(subscript) and max (superscript) performance values over 10 networks and c .

Acc
P rec
Rec
F1

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Number of
3
0.981.00
0.92
0.991.00
0.90
0.971.00
0.86
0.981.00
0.88

epochs E
4
0.991.00
0.94
1.001.00
0.99
0.981.00
0.89
0.991.00
0.94

5
0.971.00
0.90
1.001.00
0.97
0.961.00
0.85
0.981.00
0.90

9. nsCTBN generated data are asynchronous involving different time granularities, thus nsDBN cannot be
directly applied. An option is to preprocess these datasets to adapt them to nsDBNs. Given that this
would be strongly arbitrary and be penalizing for nsDBNs, we decided to learn only the nsCTBN models.

21

fiVilla & Stella

Table 6: nsCTBN under the UNE setting for nsCTBN generated data. Average, min
(subscript) and max (superscript) performance values over 10 networks and c and e .

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Acc
P rec
Rec
F1

Number of
3
0.991.00
0.92
1.001.00
1.00
0.991.00
0.86
0.991.00
0.92

epochs E
4
0.991.00
0.94
1.001.00
1.00
0.981.00
0.89
0.991.00
0.94

5
0.971.00
0.90
1.001.00
0.98
0.951.00
0.81
0.971.00
0.89

The best and worst values of accuracy for E = 5 reported in Table 6 belong to the
experiments performed on synthetic dataset number 3 and number 9 respectively. Their
results are illustrated hereafter. Figure 2(a) shows the graphs sequence of the true nsCTBN
for the synthetic datasets number 3, while Figure 2(b) displays the posterior distribution
over epochs (right), together with the distribution of the corresponding transition times
(left)10 of the learned nsCTBN in the UNE case. Figure 3 shows the same information as
those depicted in Figure 2, but for the synthetic dataset number 9. In the latter case, the
distribution over epochs is slightly in favor of the correct number of epochs.

(a) True nsCTBN model.
Distribution of the transition times

Distribution of the number of epochs

1

1
True
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

5

10

15

20

25

30

35 40
Time

45

50

55

60

65

70

0

5
Number of epochs

(b) Learned nsCTBN model results.

Figure 2: nsCTBN generated dataset number 3: (a) true graphs sequence over E=5 epochs
and (b) distribution of the transition times (left) and posterior over epochs (right) associated
with the nsCTBN inferred under the UNE setting.

10. Transition times whose distance is less than 0.1 have been aggregated.

22

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

(a) True nsCTBN model.
Distribution of the transition times

Distribution of the number of epochs

1

1
True
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

5

10

15

20

25

30

35

40 45
Time

50

55

60

65

70

75

80

0

4
5
Number of epochs

(b) Learned nsCTBN model results.

Figure 3: nsCTBN generated dataset number 9: (a) true graphs sequence over E=5 epochs
and (b) distribution of the transition times (left) and posterior over epochs (right) associated
with the nsCTBN inferred under the UNE setting.

5.2 Real-world Datasets
It is very difficult to find real-world datasets where the corresponding ground truth model
is completely known and/or a uniform consensus from domain experts has been reached.
Therefore, we decided to use the following three well-known datasets: drosophila, saccharomyces cerevisiae and songbird to compare the performance of nsCTBNs to that of nsDBNs
and other state-of-the-art algorithms, i.e. TSNI and TVDBN. Such datasets are publicly
available, clearly described and a rich and detailed discussion about their likely ground truth
models is given in the specialized literature. Furthermore, a macroeconomics dataset is introduced and analyzed. This dataset consists of 17 financial/economic variables collected
at different time granularity spanning from 1st January 1986 to 31st March 2015.
5.2.1 Drosophila
The drosophila dataset includes the mRNA expression levels of 4,028 genes at 67 successive time-points spanning the four stages of the Drosophila melanogaster life cycle (Lebre
et al., 2010): the embryonic (31 time-points), larval (10 time-points) and pupal stage (18
time-points) and the first 30 days of adulthood (8 time-points). For comparative purposes
(Dondelinger et al., 2013), we have analyzed the reduced drosophila dataset consisting of
gene expression time-series of 11 genes involved in wing muscle development. Given that
nsCTBNs are based on discrete variables, we binarized the expression level of the 11 genes
for the reduced drosophila dataset as done in the literature (Zhao, Serpedin, & Dougherty,
2006; Guo, Hanneke, Fu, & Xing, 2007; Robinson & Hartemink, 2010).
23

fiVilla & Stella

Firstly, the network inference task of the embryonic, larval, pupal and adulthood morphogenic stages was performed under the KTT setting (Robinson & Hartemink, 2010; Dondelinger et al., 2013). The nsCTBN structural learning was performed using the following
parameter values c = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} and by setting the maximum number of parents to 4. The nsCTBN learned with different c values are combined,
and only arcs that occurred in more than 20 percent of samples are included into the
inferred non-stationary continuous time Bayesian network. No other techniques predict
non-stationary directed networks (Robinson & Hartemink, 2010), so precision, recall and
F1 measure, computed with respect to networks inferred by Zhao et al. (2006) and Guo et
al. (2007), are reported in Table 7 for nsDBN, nsCTBN and TVDBN (Dondelinger et al.,
2013). The networks associated with the four epochs, as inferred with the nsCTBN on the
reduced drosophila dataset under the KTT setting, are depicted in Figure 4.
Table 7: Precision (Prec), recall (Rec) and F1 measure (F1 ) achieved by nsCTBN, nsDBN,
and TVDBN on the drosophila dataset are computed with respect to networks inferred by
Zhao et al. (2006) and Guo et al. (2007). Average values (Average) of precision, recall and
F1 measure achieved by Zhao et al. (2006) and Guo et al. (2007) are also reported.

nsDBN
nsCTBN
TVDBN

Zhao
Prec
0.58
0.33
0.17

et al. (2006)
Rec
F1
0.38 0.46
0.37 0.35
0.27 0.21

Guo et al. (2007)
Prec Rec
F1
0.47 0.34 0.39
0.41 0.43 0.42
0.36 0.61 0.45

Prec
0.52
0.37
0.27

Average
Rec
F1
0.36 0.42
0.40 0.39
0.44 0.33

According to Table 7, no optimal algorithm exists for the reduced drosophila dataset.
If the network retrieved by Zhao et al. (2006) is used as ground truth, then nsDBN is the
best model, while if the network retrieved by Guo et al. (2007) network is used as ground
truth, then TVDBN is the optimal one as far as the F1 measure is concerned. If the average
performance is computed, then nsDBN is the best model and TVDBN is the worst; while
nsCTBN achieves an F1 value that is close to the one achieved by nsDBN.
Secondly, we investigated whether the transition times inferred by structural learning of
nsCTBN under the UNE setting correspond to the known transitions between stages (Lebre
et al., 2010; Dondelinger et al., 2013). The network inference task was performed by learning
nsCTBN under the UNE setting with the following parameter values c = {0.2, 0.4, 1, 2}
and e = {0.5, 1, 2, 5}. Furthermore, we set the maximum number of parents to 2, the
number of iterations to 1,000 and the number of runs to 100.
Figure 5 shows the distribution of the transition times11 (left) and the posterior over
the number of epochs (right). The number of epochs is correctly detected to be 4 even if a
probability close to 0.1 is associated with 5 epochs. However, the transition times are not
all correctly identified. The embryonic stage is not correctly identified, the larval stage is
correctly discovered to start at time-point 31, while it is inferred to end at time-point 38
11. Each stem represents the posterior probability that the corresponding time-point starts a new epoch.
Therefore, a stem at time-point t means that an epoch ends at time-point t  1, while the next epoch
starts at time-point t.

24

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

instead of 40. nsCTBN did not identify the pupal and the adulthood stages, but it identified
two additional transition times (17 and 51). The same behavior is observed for nsDBNs,
while TVDBNs are capable to correctly identify the pupal and the adulthood stages. However, the TVDBN-0, TVDBN-Exp and TVDBN-Bino inferred networks (Dondelinger et al.,
2013) consist of a number of epochs ranging from 6 to 7.

mhc

mhc
gfl

gfl

mlc1

mlc1

eve

eve

msp300

msp300

actn

actn

myo61f

myo61f

up

up

prm

prm
twi

twi

sls

sls

(a) Embryonic (epoch from 0 to 30).

(b) Larval (epoch from 31 to 41).

mhc

mhc
gfl

gfl

mlc1

mlc1

eve

eve

msp300

msp300

actn

actn

myo61f

myo61f

up

up

prm

prm
twi

twi

sls

sls

(c) Pupal (epoch from 42 to 59).

(d) Adulthood (epoch from 60 to 66).

Figure 4: Networks inferred with nsCTBN under the KKT setting on the reduced drosophila
dataset. Only arcs that occurred in more than 20 percent of the networks associated with
different c values are included in the inferred nsCTBN model.

25

fiVilla & Stella

Distribution of the transition times

Distribution of the number of epochs

1

1
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66
Time

0

4
5
Number of epochs

Figure 5: Transition time graph (left) and posterior probability histogram over the number
of epochs E (right) associated with the nsCTBN model learned from the drosophila reduced
dataset under the UNE setting when c = 0.2 and e = 2.

5.2.2 Saccharomyces Cerevisiae
The saccharomyces cerevisiae dataset is obtained from a synthetic regulatory network with
5 genes in saccharomyces cerevisiae (Cantone, Marucci, Iorio, Ricci, Belcastro, Bansal,
Santini, di Bernardo, di Bernardo, & Cosma, 2009). It is obtained by measuring gene
expression time-series with RT-PCR (reverse transcription polymerase chain reaction) for
16 and 21 time-points under two conditions related to the carbon source: galactose (switch
on experimental condition) and glucose (switch off experimental condition). We merged the
time-series from the two experimental conditions under exclusion of the boundary point as
done in the literature (Dondelinger et al., 2013). The obtained time-series was binarized in
such a way that a 1 indicates that the gene expression level is greater than or equal to its
sample mean, while a 0 indicates the gene expression level is smaller than its sample mean.
The obtained dataset was used to infer the saccharomyces cerevisiae networks associated
with the switch on and switch off experimental conditions.
The network inference task was performed by learning a nsCTBN under the UNE setting
with the following parameter values c = {0.2, 0.4, 1, 2} and e = {0.2, 0.4, 1, 2}. Furthermore, we set the maximum number of parents to 4, the number of iterations to 1,000 and
the number of runs to 100. Only arcs that occurred in more than 50 percent of the runs are
included in the inferred nsCTBN model. Precision, recall and F1 measure values achieved
by nsCTBN are compared to those achieved by the state-of-the-art algorithms (i.e. TSNI,
nsDBN and TVDBN) in Table 8.
The result of the performed numerical experiment shows that nsCTBN is competitive with respect to state-of-the-art algorithms, while it achieves non-optimal results only
for precision associated with the switch on experimental condition. Under this condition,
5
nsCTBN achieves a precision equal to 0.5 ( 10
), while the optimal value achieved by TSNI
4
and TVDBN is 0.8 ( 5 ). On the contrary, nsCTBN achieves the best recall value, which is
equal to 0.63 ( 85 ). Under the switch off experimental condition, nsCTBN achieves the best
value for both precision, which is equal to 0.67 ( 69 ), and recall, which is equal to 0.75 ( 68 ).
26

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

We also computed the overall performance of the structural learning algorithms. In this
case, focusing the attention on the F1 measure, we can conclude that nsCTBN (0.63) is
comparable to TVDBN (0.60), which is considered to be the state-of-the-art algorithm for
the structural learning task applied to the saccharomyces cerevisiae dataset. The networks
inferred by the nsCTBN model under the switch on and switch off experimental conditions,
using c = 0.2 and c = 2, are depicted in Figure 6.
Table 8: nsCTBN compared to TSNI, nsDBN, and TVDBN when learning from the saccharomyces cerevisiae dataset. nsCTBN is learned under the UNE setting (c = 0.2, e = 2);
time-point 17 is used as the transition time between the switch on and the switch off experimental conditions. TSNI, nsDBN and TVDBN networks are described in the specialized
literature. Precision, recall and F1 measure are reported for the switch on and switch off
experimental conditions. The number of true positive arcs (superscript) and the sum of
true and false positive arcs (subscript) are reported for precision, while the number of true
positive arcs (superscript) and the sum of true positive and false negative arcs (subscript)
are reported for recall. Performance values achieved by aggregating the inferred networks
over the two epochs are also reported.

TSNI
nsDBN
TVDBN
nsCTBN

Switch on
P rec
Rec
0.8045 0.5048
0.3326 0.2528
0.8045 0.5048
0.50510 0.6358

F1
0.62
0.29
0.62
0.56

Switch off
P rec Rec
F1
0.6035 0.3838 0.46
0.6035 0.3838 0.46
0.5659 0.6358 0.59
0.6769 0.7568 0.71

GAL4

F1
0.54
0.37
0.60
0.63

GAL4

GAL80

CBF1

SWI5

Aggregated
P rec
Rec
0.70710 0.44716
0.45511 0.31516
0.64914 0.56916
0.5811
0.6911
19
16

GAL80

ASH1

SWI5

(a) Switch on network.

CBF1

ASH1

(b) Switch off network.

Figure 6: Switch on (a) and switch off (b) networks inferred with nsCTBN from the saccharomyces cerevisiae dataset under the UNE setting when c = 0.2, e = 2. The two pictures
report the positive arcs (black continuous), the false negative arcs (red dashed) and the
false positive arcs (green dotted) of the inferred networks.
27

fiVilla & Stella

Figure 7 shows the posterior distribution of the number of epochs (left) together with the
distribution of the transition times (right) for the nsCTBN learned with c = 0.2 and e = 2.
The transition between the switch on and switch off experimental conditions is known to
occur at time-point 17 (i.e. the switch off epoch starts at time-point 18). It is worthwhile
to notice that the small number of arcs, associated with the synthetic regulatory network
of saccharomyces cerevisiae, suggests that one should be very careful when evaluating the
result of the performed numerical experiment. In particular, we think that overstatements
on the effectiveness and/or superiority of different structural learning algorithms for the
learning task on the saccharomyces cerevisiae dataset should be avoided.
Distribution of the transition times

Distribution of the number of epochs

1

1
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

2

4

6

8

10 12 14 16 18 20 22 24 26 28 30 32 34 36
Time

0

2
Number of epochs

Figure 7: Transition time graph (left) and posterior probability over the number of epochs
(right) associated with the nsCTBN inferred from the saccharomyces cerevisiae dataset
under the UNE setting when c = 0.2 and e = 2. The maximum aposteriori estimate over
the number of epochs is associated with E = 2 epochs: epoch 1 starts at time-point 1 and
ends at time-point 17, while epoch 2 starts at time-point 18 and ends at time-point 36.

5.2.3 Songbird
The songbird dataset was collected with eight electrodes placed into the vocal nuclei of six
female zebra finches (Smith et al., 2006). Voltage changes were recorded from populations
of neurons while the birds were provided with four different two-second auditory stimuli,
each presented from 18 to 20 times. Voltages were post-processed with a root mean square
transformation and binned to 5 ms (Robinson & Hartemink, 2010).
The songbird dataset is used to learn neural information flow networks, i.e. the networks
that represent the transmission of information between different regions of the songbird
brain. A neural information flow network represents the dynamic utilization of the potential
pathways along which information can travel. The identification of the neural information
flow networks in songbirds during auditory stimuli allows you to understand how sounds
are stored and processed in the songbirds brain. The songbird dataset consists of data
of 8 variables recorded from electrodes for two seconds pre-stimulus, two seconds during
stimulus and two seconds post-stimulus for six birds. The stimuli are hear-song, i.e. the
bird hears another bird singing, and white-noise, i.e. the bird hears a white noise stimulus.
28

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

We show the results of the nsCTBN learned on two out of the six birds of the songbird
dataset, namely bird 648 and bird 841. The results obtained for the other four birds are
similar. Given that nsCTBNs are based on discrete variables, the values of the 8 variables
were discretized into three bins using uniform quantiles (0, 13 , 23 , 1) according to the literature
(Robinson & Hartemink, 2010). The inference task of the neural information flow networks
was performed by learning a nsCTBN under the UNE setting with the following parameter
values c = {0.25, 0.5, 1, 2, 5, 10} and e = {0.25, 0.5, 1, 2, 5, 10}. We set the maximum
number of parents to 3, the number of iterations to 500 and the number of runs to 10.
Figure 8 (a) and (b) show the probability of transition (left) and the posterior probability
over the number of epochs (right) for bird 648 and bird 841 under the white-noise stimulus.
Figure 9 (a) and (b) show the probability of transition (left) and the posterior probability
over the number of epochs (right) for bird 648 and bird 841 under the hear-song stimulus.
Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

0

6

3
4
Number of epochs

(a) white-noise stimulus for bird 648: learned model results.
Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

6

0

3
4
Number of epochs

(b) white-noise stimulus for bird 841: learned model results.

Figure 8: Distribution of the transition times and posterior distribution over epochs for
nsCTBN under the UNE setting on the songbird dataset for the white-noise stimulus.
29

fiVilla & Stella

The location of the transition time-points under the white-noise stimulus and the hearsong stimulus are accurately inferred for bird 648 and bird 841. The posterior distribution
over the number of epochs for birds 648 and 841 under the white-noise stimulus is nearly
equally split between 3 and 4 epochs, while under the hear-song stimulus it is peaked over 3
epochs. Therefore, both the number of epochs and the location of the transition time-points
are reliably recovered by the nsCTBN learned under the UNE setting. Unfortunately, we
were not able to find any additional information to validate the learned nsCTBNs for this
dataset. Moreover, a comparison across different birds to eventually develop a consensus
network is not possible due to the songbird data collection settings. Indeed, each of the six
birds is characterized by its own electrodes, which make difficult to obtain a correspondence
map across different birds.

Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

0

6

3
4
Number of epochs

(a) hear-song stimulus for bird 648: learned model results.
Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

6

0

3
4
Number of epochs

(b) hear-song stimulus for bird 841: learned model results.

Figure 9: Distribution of the transition times and posterior distribution over epochs for
nsCTBN under the UNE setting on the songbird dataset for the hear-song stimulus.

30

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

5.2.4 Macroeconomics
The macroeconomics dataset consists of 17 financial/economic time-series pertaining to the
economy of the United States. Time-series have different time granularity and span from
1st January 1986 to 31st March 2015. More specifically, five time-series have daily granularity, namely Crude oil (OIL), USD to EUR spot exchange rate (USDEUR), Gold (GOLD),
S&P500 equity index (S&P500) and the 10-years treasury bond yield rate (US10yrsNote).
Eleven time-series have monthly granularity, namely production of total industry (PTI),
real manufacturing and trade industries sales (RMTIS), personal income (PI), unemployment (UN), consumer price index (CPI), federal funds rate (RATE), producer price index
(PPI), non-farm payrolls (NFP), new one-family houses sold (NHSold), new houses for sale
(NHSale) and new private house permits (NHPermit). Finally, the gross domestic product
(GDP) time-series has quarterly granularity.
The goal of this study is to discover how the financial and economic environment evolves
over time. In particular, we focused the attention to detect business cycles12 and the
associated change of relationships among financial and economic variables. Given that the
duration of a business cycle is highly variable, the ability to identify the turning point of a
cycle (i.e. when a recession starts) is of considerable importance to policymakers, financial
companies as well as to individuals. A substantial literature is available about the business
cycle turning points detection generally relying on Markov-switching models (Hamilton &
Raj, 2005). However, these models are not able to represent some important features such
as the dependence structure among variables in each business cycle.
In order to use the nsCTBN model in such a context, we applied a binary discretization
to the variable associated with each time-series. Discretization was performed using a lookback period of 1 year, i.e. if the current value is greater than the past one, then the binary
variable is set to 1 otherwise, it is set to 0. The approach of looking back into the past
is widely used in finance (Moskowitz, Ooi, & Pedersen, 2012). nsCTBNs learning was
performed under the UNE setting using the following parameter values: c = {0.5, 1, 2},
e = {0.1, 1, 10}, 2 maximum parents per node, 300 iterations and 10 runs.
Figure 10 shows the probability of transition (left side, left axis) versus the S&P500
equity index used as a reference (left side, right axis) and the posterior probability over the
number of epochs (right side). The nsCTBN consists of three epochs with transition times
close to the end of July 2000 and the end of November 2007. If we compare these dates to
the turning points of the US business cycle reported by the National Bureau of Economic
Research13 , then we see that we are not far from the turning point of March 2001 and very
close to the one of December 2007, while we missed the turning point which occurred in
July 1990, probably because of the limited length of the dataset.
Figure 11 shows the structure of the nsCTBN model corresponding to the most probable
number of epochs, i.e. E = 3. An arc is included in the nsCTBN model when it occurs in
more than 75% of the performed runs in each epoch. The retrieved networks correspond to
the following time periods: from January 1986 to July 2000 (epoch 1), from August 2000
to November 2007 (epoch 2) and from December 2007 to March 2015 (epoch 3).
12. Business cycles are fluctuations in aggregate economic activity, they are recurrent (i.e. it is possible to
identify expansion-recession cycles), persistent and not periodic (i.e. they differ in length and severity).
13. The official business cycle turning points and dates are available at http://www.nber.org/cycles.html

31

fiVilla & Stella

Distribution of the transition times vs S&P500

Distribution of the number of epochs
2500

1

0.8

2000

0.8

0.6

1500

0.4

1000

0.2

500

1

Posterior probability

Value

Probability of transition

Retrieved (left)
S&P500 (right)

0
1985

1990

1995

2000
2005
Time

2010

0.4

0.2

0
2020

2015

0.6

0

2

3
4
5
Number of epochs

Figure 10: Distribution of the transition times and S&P500 behavior over time (left). Posterior probability over epochs (right) for the learned nsCTBN under the UNE setting.
USDEUR

OIL
GOLD

USDEUR
OIL

UN

GOLD

US10YRS

US10YRS

PPI

NHPer

CPI

PPI

NHPer

PTI

SP500

PTI

SP500

PI
RMTIS

NHSale

RATE

PI

CPI

RMTIS

RATE
NFP

GDP
NHSold
NFP

GDP
UN

NHSale

(a) Epoch 1 (Jan 1986 - Jul 2000).

NHSold

(b) Epoch 2 (Aug 2000 - Nov 2007).
USDEUR
OIL

PPI

US10YRS

GOLD
NHSale

SP500
PTI

CPI

RATE

NHPer
PI
RMTIS
NFP
UN
GDP

NHSold

(c) Epoch 3 (Dec 2007 - Mar 2015).

Figure 11: nsCTBN learned on the macroeconomics dataset under the UNE setting.
nsCTBN corresponds to the most probable number of epochs (E = 3). An arc is included
in the nsCTBN model when it occurs in more than 75% of the runs in each epoch.

32

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

The novelty of this approach to the economic analysis opens the door to many considerations and new speculations about the economic variables during business cycles. In
this paper, we highlight two patterns emerging from the learned nsCTBN model: the well
known relevant role of the personal income (PI) and its relation to the unemployment (UN)
(Mankiw, 2014) and the less known relation of the non-farm payrolls (NFP) to the S&P500
equity index (S&P500) (Miao, Ramchander, & Zumwalt, 2014).

6. Conclusions
We introduced non-stationary continuous time Bayesian networks and developed three
structural learning algorithms to be used under different knowledge settings (i.e. KTT,
KNE and UNE) for the problem to be analyzed. The structural learning algorithm in
the known transition times case is exact and it exploits graph theory to infer the optimal
nsCTBNs structure. It has a polynomial time complexity under the assumption that the
maximum number of parents for each node is fixed. All the nsCTBNs structural learning algorithms are competitive to state-of-the-art algorithms when synthetic and real-world
datasets are considered. This statement is proved by a rich set of numerical experiments.
nsCTBNs can be adapted to use different score metrics, as far as the considered score
metrics integrates over the non-structural parameters. nsCTBNs exploit an interesting
property of CTBNs and offer the possibility to learn the optimal nsCTBNs structure for
each single variable. This could be extremely useful in the case when the non-stationary
behavior of the analyzed system is not synchronous, and thus it may be the case that each
node changes its parents independently from how other nodes change their parents set.
However, two main limitations exist with nsCTBNs: i) the variables are assumed to be
discrete; specifically each variable of the dataset must take value over a countable number of
states and ii) finding the optimal value for the c and e hyperparameters can be extremely
difficult (the same is true for nsDBNs). Concerning i), the problem of discretizing continuous
variables has been studied for a long time and robust solutions have been described in the
specialized literature. Discretizing continuous variables whose value is measured over time
has not been studied intensively and many issues still remain. The problem ii) of selecting
the optimal value of hyperparameters is known in the specialized literature and much can
be done when experts provide their valuable apriori knowledge. However, when such apriori
knowledge is poor or not available at all, selecting optimal hyperparameter values can be
extremely difficult. It is important to note that one of the strong limitations to studying
and comparing non-stationary models is the lack of ground truth models.
Possible directions for further research include the application of nsCTBNs structural
learning algorithms to other datasets, such as the arabidopsis thaliana dataset (Grzegorczyk,
Aderhold, & Husmeier, 2015) as well as other financial datasets supported by in-depth
economic analyses. Another interesting perspective is the study and development of a
modeling approach, going towards the direction of allowing each node to change its parents
set asynchronously. Furthermore, we think that to increase the applicability to real-world
time-series data of the proposed nsCTBNs structural learning algorithms the issue of timeseries discretization must be addressed. In particular, we think that this issue must be
addressed in an integrated manner with the nsCTBNs structural learning algorithm.
33

fiVilla & Stella

Finally, it could be interesting to apply the framework of nsCTBNs to address the
task of classification of objects in a streaming context when using a probabilistic graphical model based approach (Borchani, Martinez, Masegosa, Langseth, Nielsen, Salmeron,
Fernandez, Madsen, & Saez, 2015a; Borchani, Martnez, Masegosa, Langseth, Nielsen,
Salmeron, Fernandez, Madsen, & Saez, 2015b).

Acknowledgments
The authors wish to thank Alexander Hartemink for having kindly provided the nsDBN
jar executable and the associated datasets. A special thank goes to Marco Grzegorczyk for
providing the arabidopsis thaliana dataset together with fundamental information to analyze
it. The authors are greatly indebted to anonymous referees for their constructive comments
and their extremely helpful suggestions, which contributed to significantly improve the
quality of the paper. A special thank goes to the Associate Editor Manfred Jaeger.
Fabio Stella is the corresponding author of this article.

References
Acerbi, E., & Stella, F. (2014). Continuous time bayesian networks for gene network reconstruction: a comparative study on time course data. In The 10th International
Symposium on Bioinformatics Research and Applications, Zhangjiajie, China, 2014,
10.
Acerbi, E., Vigano, E., Poidinger, M., Mortellaro, A., Zelante, T., & Stella, F. (2016).
Continuous time bayesian networks identify prdm1 as a negative regulator of th17 cell
differentiation in humans. Scientific Reports, 6, 23128.
Acerbi, E., Zelante, T., Narang, V., & Stella, F. (2014). Gene network inference using
continuous time bayesian networks: a comparative study and application to th17 cell
differentiation. BMC Bioinformatics, 15 (1).
Ahmed, A., & Xing, E. P. (2009). Recovering time-varying networks of dependencies in social
and biological studies. Proceedings of the National Academy of Sciences, 106 (29),
1187811883.
Bertsimas, D., & Tsitsiklis, J. (1993). Simulated annealing. Statistical Science, 8 (1), 1015.
Borchani, H., Martinez, A. M., Masegosa, A., Langseth, H., Nielsen, T. D., Salmeron, A.,
Fernandez, A., Madsen, A. L., & Saez, R. (2015a). Dynamic Bayesian modeling for
risk prediction in credit operations. In The 13th Scandinavian Conference on Artificial
Intelligence (SCAI 2015), Halmstad, Sweden.
Borchani, H., Martnez, A. M., Masegosa, A. R., Langseth, H., Nielsen, T. D., Salmeron,
A., Fernandez, A., Madsen, A. L., & Saez, R. (2015b). Modeling concept drift: A
probabilistic graphical model based approach. In The 14th International Symposium
on Intelligent Data Analysis (IDA 2015), Saint-Etienne, France.
Boudali, H., & Dugan, J. B. (2006). A continuous-time bayesian network reliability modeling, and analysis framework. IEEE Transactions on Reliability, 55 (1), 8697.
34

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Burge, J., Lane, T., Link, H., Qiu, S., & Clark, V. P. (2009). Discrete dynamic bayesian
network analysis of fmri data. Human brain mapping, 30 (1), 122137.
Cantone, I., Marucci, L., Iorio, F., Ricci, M. A., Belcastro, V., Bansal, M., Santini, S.,
di Bernardo, M., di Bernardo, D., & Cosma, M. P. (2009). A yeast synthetic network
for in vivo assessment of reverse-engineering and modeling approaches. Cell, 137 (1),
172  181.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Comput. Intell., 5 (3), 142150.
Dondelinger, F., Lebre, S., & Husmeier, D. (2013). Non-homogeneous dynamic bayesian
networks with bayesian regularization for inferring gene regulatory networks with
gradually time-varying structure. Machine Learning, 90 (2), 191230.
Durante, D., & Dunson, D. B. (2014). Bayesian dynamic financial networks with timevarying predictors. Statistics & Probability Letters, 93, 1926.
Fan, Y., & Shelton, C. R. (2009). Learning continuous-time social network dynamics. In
The 25th Conference on Uncertainty in Artificial Intelligence (UAI 2009), Montreal,
Canada.
Friedman, N., & Koller, D. (2000). Being bayesian about bayesian network structure: A
bayesian approach to structure discovery in bayesian networks. Machine Learning,
50, 95125.
Gatti, E., Luciani, D., & Stella, F. (2011). A continuous time bayesian network model
for cardiogenic heart failure. Flexible Services and Manufacturing Journal, 24 (2),
496515.
Geiger, D., & Heckerman, D. (1997). A characterization of dirchlet distributions through
local and global independence. Annals of Statistics, 25, 13441368.
Grzegorczyk, M., Aderhold, A., & Husmeier, D. (2015). Inferring bi-directional interactions between circadian clock genes and metabolism with model ensembles. Statistical
Applications in Genetics and Molecular Biology, 14 (2), 143167.
Guo, F., Hanneke, S., Fu, W., & Xing, E. P. (2007). Recovering temporally rewiring networks: a model-based approach. In Machine Learning, Proceedings of the 24th International Conference (ICML 2007), Corvallis, USA, June 20-24, 2007, pp. 321328.
Hamilton, J. D., & Raj, B. (Eds.). (2005). Advances in Markov-Switching Models: Applications in Business Cycle Research and Finance. Studies in Empirical Economics.
Springer-Verlag.
Herbrich, R., Graepel, T., & Murphy, B. (2007). Structure from failure. In The 2nd USENIX
workshop on Tackling computer systems problems with machine learning techniques
(SYSML 07), Cambridge, USA, pp. 16.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220 (4598), 671680.
Lebre, S., Becq, J., Devaux, F., Stumpf, M., & Lelandais, G. (2010). Statistical inference of
the time-varying structure of gene regulation networks. BMC Systems Biology, 4 (1),
130+.
35

fiVilla & Stella

Liu, M., Hommersom, A., van der Heijden, M., & Lucas, P. J. (2016). Hybrid time bayesian
networks. International Journal of Approximate Reasoning, .
Mankiw, N. G. (2014). Principles of Macroeconomics (7th edition). South-Western College
Pub.
Marini, S., Trifoglio, E., Barbarini, N., Sambo, F., Camillo, B. D., Malovini, A., Manfrini,
M., Cobelli, C., & Bellazzi, R. (2015). A dynamic bayesian network model for longterm simulation of clinical complications in type 1 diabetes. Journal of Biomedical
Informatics, 57, 369  376.
Miao, H., Ramchander, S., & Zumwalt, J. K. (2014). S&p 500 index-futures price jumps
and macroeconomic news. Journal of Futures Markets, 34 (10), 9801001.
Moskowitz, T. J., Ooi, Y. H., & Pedersen, L. H. (2012). Time series momentum. Journal
of Financial Economics, 104 (2), 228250.
Mumford, J. A., & Ramsey, J. D. (2014). Bayesian networks for fmri: A primer. Neuroimage,
86, 573582.
Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. The MIT Press.
Nodelman, U. (2007). Continuous Time Bayesian Networks. Ph.D. thesis, Stanford University.
Nodelman, U., & Horvitz, E. (2003). Continuous time bayesian networks for inferring users
presence and activities with extensions for modeling and evaluation. Tech. rep. MSRTR-2003-97, Microsoft Research.
Nodelman, U., Shelton, C. R., & Koller, D. (2002). Continuous time bayesian networks. In
The 18th Conference on Uncertainty in Artificial Intelligence (UAI 2002), Edmonton,
Canada, pp. 378387.
Nodelman, U., Shelton, C., & Koller, D. (2003). Learning continuous time bayesian networks. In The 19th Conference on Uncertainty in Artificial Intelligence (UAI 2003),
Acapulco, Mexico, pp. 451458.
Pearl, J. (1989). Probabilistic reasoning in intelligent systems - networks of plausible inference. Morgan Kaufmann series in representation and reasoning. Morgan Kaufmann.
Robinson, J. W., & Hartemink, A. J. (2010). Learning non-stationary dynamic bayesian
networks. Journal of Machine Learning Research, 11, 36473680.
Scutari, M., & Denis, J.-B. (2014). Bayesian Networks with Examples in R. Chapman and
Hall, Boca Raton. ISBN 978-1482225587.
Segal, E., Peer, D., Regev, A., Koller, D., & Friedman, N. (2005). Learning module networks. Journal of Machine Learning Research, 6, 557588.
Smith, A. V., Yu, J., Smulders, T. V., Hartemink, A. J., & Jarvis, E. D. (2006). Computational Inference of Neural Information Flow Networks. PLoS Computational Biology,
2 (11), e161+.
Spiegelhalter, D. J., & Lauritzen, S. L. (1990). Sequential updating of conditional probabilities on directed graphical structures. Networks, 20 (5), 579605.
36

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Sturlaugson, L., & Sheppard, J. W. (2014). Inference complexity in continuous time bayesian
networks. In The 30th Conference on Uncertainty in Artificial Intelligence (UAI
2014), Quebec City, Canada, pp. 772779.
Vinh, N. X., Chetty, M., Coppel, R., & Wangikar, P. P. (2012). Gene regulatory network
modeling via global optimization of high-order dynamic bayesian network. BMC
Bioinformatics, 13, 131.
Xu, J., & Shelton, C. R. (2008). Continuous time bayesian networks for host level network
intrusion detection. In The European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML PKDD 2008), Antwerp,
Belgium, pp. 613627.
Zhao, W., Serpedin, E., & Dougherty, E. R. (2006). Inferring gene regulatory networks
from time series data using the minimum description length principle. Bioinformatics,
22 (17), 21292135.
Zou, M., & Conzen, S. D. (2005). A new dynamic bayesian network (dbn) approach for identifying gene regulatory networks from time course microarray data. Bioinformatics,
21 (1), 7179.

37

fiJournal of Artificial Intelligence Research 57 (2016) 345420

Submitted 9/15; published 11/16

A Primer on Neural Network Models
for Natural Language Processing
Yoav Goldberg

yoav.goldberg@gmail.com

Computer Science Department
Bar-Ilan University, Israel

Abstract
Over the past few years, neural networks have re-emerged as powerful machine-learning
models, yielding state-of-the-art results in fields such as image recognition and speech
processing. More recently, neural network models started to be applied also to textual
natural language signals, again with very promising results. This tutorial surveys neural
network models from the perspective of natural language processing research, in an attempt
to bring natural-language researchers up to speed with the neural techniques. The tutorial
covers input encoding for natural language tasks, feed-forward networks, convolutional
networks, recurrent networks and recursive networks, as well as the computation graph
abstraction for automatic gradient computation.

1. Introduction
For over a decade, core NLP techniques were dominated by machine-learning approaches
that used linear models such as support vector machines or logistic regression, trained over
very high dimensional yet very sparse feature vectors.
Recently, the field has seen some success in switching from such linear models over
sparse inputs to non-linear neural-network models over dense inputs. While most of the
neural network techniques are easy to apply, sometimes as almost drop-in replacements of
the old linear classifiers, there is in many cases a strong barrier of entry. In this tutorial I
attempt to provide NLP practitioners (as well as newcomers) with the basic background,
jargon, tools and methodology that will allow them to understand the principles behind
the neural network models and apply them to their own work. This tutorial is expected
to be self-contained, while presenting the different approaches under a unified notation and
framework. It repeats a lot of material which is available elsewhere. It also points to
external sources for more advanced topics when appropriate.
This primer is not intended as a comprehensive resource for those that will go on and
develop the next advances in neural-network machinery (though it may serve as a good entry
point). Rather, it is aimed at those readers who are interested in taking the existing, useful
technology and applying it in useful and creative ways to their favourite NLP problems. For
more in-depth, general discussion of neural networks, the theory behind them, advanced
optimization methods and other advanced topics, the reader is referred to other existing
resources. In particular, the book by Bengio, Goodfellow, and Courville (2015) is highly
recommended.
c
2016
AI Access Foundation. All rights reserved.

fiGoldberg

1.1 Scope
The focus is on applications of neural networks to language processing tasks. However,
some subareas of language processing with neural networks were deliberately left out of
scope of this tutorial. These include the vast literature of language modeling and acoustic
modeling, the use of neural networks for machine translation, and multi-modal applications
combining language and other signals such as images and videos (e.g. caption generation).
Caching methods for efficient runtime performance, methods for efficient training with large
output vocabularies and attention models are also not discussed. Word embeddings are
discussed only to the extent that is needed to understand in order to use them as inputs
for other models. Other unsupervised approaches, including autoencoders and recursive
autoencoders, also fall out of scope. While some applications of neural networks for language
modeling and machine translation are mentioned in the text, their treatment is by no means
comprehensive.
1.2 A Note on Terminology
The word feature is used to refer to a concrete, linguistic input such as a word, a suffix, or
a part-of-speech tag. For example, in a first-order part-of-speech tagger, the features might
be current word, previous word, next word, previous part of speech. The term input
vector is used to refer to the actual input that is fed to the neural-network classifier.
Similarly, input vector entry refers to a specific value of the input. This is in contrast to
a lot of the neural networks literature in which the word feature is overloaded between
the two uses, and is used primarily to refer to an input-vector entry.
1.3 Mathematical Notation
I use bold upper case letters to represent matrices (X, Y, Z), and bold lower-case letters to
represent vectors (b). When there are series of related matrices and vectors (for example,
where each matrix corresponds to a different layer in the network), superscript indices are
used (W1 , W2 ). For the rare cases in which we want indicate the power of a matrix or
a vector, a pair of brackets is added around the item to be exponentiated: (W)2 , (W3 )2 .
Unless otherwise stated, vectors are assumed to be row vectors. We use [v1 ; v2 ] to denote
vector concatenation.
The choice to use row vectors, which are right multiplied by matrices (xW + b) is
somewhat non standard  a lot of the neural networks literature use column vectors that
are left multiplied by matrices (Wx + b). We trust the reader to be able to adapt to the
column vectors notation when reading the literature.1

1. The choice to use the row vectors notation was inspired by the following benefits: it matches the way
input vectors and network diagrams are often drawn in the literature; it makes the hierarchical/layered
structure of the network more transparent and puts the input as the left-most variable rather than being
nested; it results in fully-connected layer dimensions being din  dout rather than dout  din ; and it maps
better to the way networks are implemented in code using matrix libraries such as numpy.

346

fiA Primer on Neural Networks for NLP

2. Neural Network Architectures
Neural networks are powerful learning models. We will discuss two kinds of neural network
architectures, that can be mixed and matched  feed-forward networks and recurrent /
recursive networks. Feed-forward networks include networks with fully connected layers,
such as the multi-layer perceptron, as well as networks with convolutional and pooling
layers. All of the networks act as classifiers, but each with different strengths.
Fully connected feed-forward neural networks (Section 4) are non-linear learners that
can, for the most part, be used as a drop-in replacement wherever a linear learner is used.
This includes binary and multiclass classification problems, as well as more complex structured prediction problems (Section 8). The non-linearity of the network, as well as the
ability to easily integrate pre-trained word embeddings, often lead to superior classification accuracy. A series of works2 managed to obtain improved syntactic parsing results
by simply replacing the linear model of a parser with a fully connected feed-forward network. Straight-forward applications of a feed-forward network as a classifier replacement
(usually coupled with the use of pre-trained word vectors) provide benefits also for CCG
supertagging,3 dialog state tracking,4 pre-ordering for statistical machine translation5 and
language modeling.6 Iyyer, Manjunatha, Boyd-Graber, and Daume III (2015) demonstrate
that multi-layer feed-forward networks can provide competitive results on sentiment classification and factoid question answering.
Networks with convolutional and pooling layers (Section 9) are useful for classification
tasks in which we expect to find strong local clues regarding class membership, but these
clues can appear in different places in the input. For example, in a document classification
task, a single key phrase (or an ngram) can help in determining the topic of the document
(Johnson & Zhang, 2015). We would like to learn that certain sequences of words are good
indicators of the topic, and do not necessarily care where they appear in the document.
Convolutional and pooling layers allow the model to learn to find such local indicators,
regardless of their position. Convolutional and pooling architecture show promising results
on many tasks, including document classification,7 short-text categorization,8 sentiment
classification,9 relation type classification between entities,10 event detection,11 paraphrase
identification,12 semantic role labeling,13 question answering,14 predicting box-office rev-

2. Chen and Manning (2014), Weiss, Alberti, Collins, and Petrov (2015) and Pei, Ge, and Chang (2015)
and Durrett and Klein (2015)
3. Lewis and Steedman (2014)
4. Henderson, Thomson, and Young (2013)
5. de Gispert, Iglesias, and Byrne (2015)
6. Bengio, Ducharme, Vincent, and Janvin (2003) and Vaswani, Zhao, Fossum, and Chiang (2013)
7. Johnson and Zhang (2015)
8. Wang, Xu, Xu, Liu, Zhang, Wang, and Hao (2015a)
9. Kalchbrenner, Grefenstette, and Blunsom (2014) and Kim (2014)
10. Zeng, Liu, Lai, Zhou, and Zhao (2014), dos Santos, Xiang, and Zhou (2015)
11. Chen, Xu, Liu, Zeng, and Zhao (2015), Nguyen and Grishman (2015)
12. Yin and Schutze (2015)
13. Collobert, Weston, Bottou, Karlen, Kavukcuoglu, and Kuksa (2011)
14. Dong, Wei, Zhou, and Xu (2015)

347

fiGoldberg

enues of movies based on critic reviews,15 modeling text interestingness,16 and modeling
the relation between character-sequences and part-of-speech tags.17
In natural language we often work with structured data of arbitrary sizes, such as
sequences and trees. We would like to be able to capture regularities in such structures,
or to model similarities between such structures. In many cases, this means encoding
the structure as a fixed width vector, which we can then pass on to another statistical
learner for further processing. While convolutional and pooling architectures allow us to
encode arbitrary large items as fixed size vectors capturing their most salient features,
they do so by sacrificing most of the structural information. Recurrent (Section 10) and
recursive (Section 12) architectures, on the other hand, allow us to work with sequences
and trees while preserving a lot of the structural information. Recurrent networks (Elman,
1990) are designed to model sequences, while recursive networks (Goller & Kuchler, 1996)
are generalizations of recurrent networks that can handle trees. We will also discuss an
extension of recurrent networks that allow them to model stacks (Dyer, Ballesteros, Ling,
Matthews, & Smith, 2015; Watanabe & Sumita, 2015).
Recurrent models have been shown to produce very strong results for language modeling,18 ; as well as for sequence tagging,19 machine translation,20 dependency parsing,21
sentiment analysis,22 noisy text normalization,23 dialog state tracking,24 response generation,25 and modeling the relation between character sequences and part-of-speech tags.26
Recursive models were shown to produce state-of-the-art or near state-of-the-art results
for constituency27 and dependency28 parse re-ranking, discourse parsing,29 semantic relation
classification,30 political ideology detection based on parse trees,31 sentiment classification,32
target-dependent sentiment classification33 and question answering.34
15.
16.
17.
18.

19.
20.

21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.

Bitvai and Cohn (2015)
Gao, Pantel, Gamon, He, and Deng (2014)
dos Santos and Zadrozny (2014)
Some notable works are by Mikolov, Karafiat, Burget, Cernocky, and Khudanpur (2010), Mikolov,
Kombrink, Lukas Burget, Cernocky, and Khudanpur (2011), Mikolov (2012), Duh, Neubig, Sudoh, and
Tsukada (2013), Adel, Vu, and Schultz (2013), Auli, Galley, Quirk, and Zweig (2013) and Auli and Gao
(2014)
Irsoy and Cardie (2014), Xu, Auli, and Clark (2015), Ling, Dyer, Black, Trancoso, Fermandez, Amir,
Marujo, and Luis (2015b)
Sundermeyer, Alkhouli, Wuebker, and Ney (2014), Tamura, Watanabe, and Sumita (2014), Sutskever,
Vinyals, and Le (2014) and Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio
(2014b)
Dyer et al. (2015), Watanabe and Sumita (2015)
Wang, Liu, Sun, Wang, and Wang (2015b)
Chrupala (2014)
Mrksic, O Seaghdha, Thomson, Gasic, Su, Vandyke, Wen, and Young (2015)
Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, and Dolan (2015)
Ling et al. (2015b)
Socher, Bauer, Manning, and Ng (2013)
Le and Zuidema (2014), Zhu, Qiu, Chen, and Huang (2015a)
Li, Li, and Hovy (2014)
Hashimoto, Miwa, Tsuruoka, and Chikayama (2013), Liu, Wei, Li, Ji, Zhou, and Wang (2015)
Iyyer, Enns, Boyd-Graber, and Resnik (2014b)
Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts (2013), Hermann and Blunsom (2013)
Dong, Wei, Tan, Tang, Zhou, and Xu (2014)
Iyyer, Boyd-Graber, Claudino, Socher, and Daume III (2014a)

348

fiA Primer on Neural Networks for NLP

3. Feature Representation
Before discussing the network structure in more depth, it is important to pay attention
to how features are represented. For now, we can think of a feed-forward neural network
as a function NN(x) that takes as input a din dimensional vector x and produces a dout
dimensional output vector. The function is often used as a classifier, assigning the input x
a degree of membership in one or more of dout classes. The function can be complex, and is
almost always non-linear. Common structures of this function will be discussed in Section 4.
Here, we focus on the input, x. When dealing with natural language, the input x encodes
features such as words, part-of-speech tags or other linguistic information. Perhaps the
biggest conceptual jump when moving from sparse-input linear models to neural-network
based models is to stop representing each feature as a unique dimension (the so called
one-hot representation) and representing them instead as dense vectors. That is, each core
feature is embedded into a d dimensional space, and represented as a vector in that space.35
The embeddings (the vector representation of each core feature) can then be trained like
the other parameter of the function NN. Figure 1 shows the two approaches to feature
representation.
The feature embeddings (the values of the vector entries for each feature) are treated
as model parameters that need to be trained together with the other components of the
network. Methods of training (or obtaining) the feature embeddings will be discussed later.
For now, consider the feature embeddings as given.
The general structure for an NLP classification system based on a feed-forward neural
network is thus:
1. Extract a set of core linguistic features f1 , . . . , fk that are relevant for predicting the
output class.
2. For each feature fi of interest, retrieve the corresponding vector v(fi ).
3. Combine the vectors (either by concatenation, summation or a combination of both)
into an input vector x.
4. Feed x into a non-linear classifier (feed-forward neural network).
The biggest change in the input, then, is the move from sparse representations in which
each feature is its own dimension, to a dense representation in which each feature is mapped
to a vector. Another difference is that we extract only core features and not feature combinations. We will elaborate on both these changes briefly.
3.1 Dense Vectors vs. One-Hot Representations
What are the benefits of representing our features as vectors instead of as unique IDs?
Should we always represent features as dense vectors? Lets consider the two kinds of
representations:

35. Different feature types may be embedded into different spaces. For example, one may represent word
features using 100 dimensions, and part-of-speech features using 20 dimensions.

349

fiGoldberg

Figure 1: Sparse vs. dense feature representations. Two encodings of the information: current word is dog; previous word is the; previous pos-tag is DET.
(a) Sparse feature vector. Each dimension represents a feature. Feature combinations receive their own dimensions. Feature values are binary. Dimensionality
is very high. (b) Dense, embeddings-based feature vector. Each core feature is
represented as a vector. Each feature corresponds to several input vector entries. No explicit encoding of feature combinations. Dimensionality is low. The
feature-to-vector mappings come from an embedding table.

350

fiA Primer on Neural Networks for NLP

One Hot Each feature is its own dimension.
 Dimensionality of one-hot vector is same as number of distinct features.

 Features are completely independent from one another. The feature word is
dog  is as dis-similar to word is thinking  than it is to word is cat .
Dense Each feature is a d-dimensional vector.
 Dimensionality of vector is d.

 Model training will cause similar features to have similar vectors  information
is shared between similar features.
One benefit of using dense and low-dimensional vectors is computational: the majority
of neural network toolkits do not play well with very high-dimensional, sparse vectors.
However, this is just a technical obstacle, which can be resolved with some engineering
effort.
The main benefit of the dense representations is in generalization power: if we believe
some features may provide similar clues, it is worthwhile to provide a representation that
is able to capture these similarities. For example, assume we have observed the word dog
many times during training, but only observed the word cat a handful of times, or not at
all. If each of the words is associated with its own dimension, occurrences of dog will not
tell us anything about the occurrences of cat. However, in the dense vectors representation
the learned vector for dog may be similar to the learned vector from cat, allowing the
model to share statistical strength between the two events. This argument assumes that
good vectors are somehow given to us. Section 5 describes ways of obtaining such vector
representations.
In cases where we have relatively few distinct features in the category, and we believe
there are no correlations between the different features, we may use the one-hot representation. However, if we believe there are going to be correlations between the different features
in the group (for example, for part-of-speech tags, we may believe that the different verb
inflections VB and VBZ may behave similarly as far as our task is concerned) it may be
worthwhile to let the network figure out the correlations and gain some statistical strength
by sharing the parameters. It may be the case that under some circumstances, when the
feature space is relatively small and the training data is plentiful, or when we do not wish to
share statistical information between distinct words, there are gains to be made from using
the one-hot representations. However, this is still an open research question, and there are
no strong evidence to either side. The majority of work (pioneered in Collobert & Weston,
2008; Collobert et al. 2011; Chen & Manning, 2014) advocate the use of dense, trainable
embedding vectors for all features. For work using neural network architecture with sparse
vector encodings see the work by Johnson and Zhang (2015).
Finally, it is important to note that representing features as dense vectors is an integral
part of the neural network framework, and that consequentially the differences between
using sparse and dense feature representations are subtler than they may appear at first.
In fact, using sparse, one-hot vectors as input when training a neural network amounts
to dedicating the first layer of the network to learning a dense embedding vector for each
feature based on the training data. We touch on this in Section 4.6.
351

fiGoldberg

3.2 Variable Number of Features: Continuous Bag of Words
Feed-forward networks assume a fixed dimensional input. This can easily accommodate the
case of a feature-extraction function that extracts a fixed number of features: each feature
is represented as a vector, and the vectors are concatenated. This way, each region of the
resulting input vector corresponds to a different feature. However, in some cases the number
of features is not known in advance (for example, in document classification it is common
that each word in the sentence is a feature). We thus need to represent an unbounded
number of features using a fixed size vector. One way of achieving this is through a socalled continuous bag of words (CBOW) representation (Mikolov, Chen, Corrado, & Dean,
2013). The CBOW is very similar to the traditional bag-of-words representation in which
we discard order information, and works by either summing or averaging the embedding
vectors of the corresponding features:36
CBOW(f1 , ..., fk ) =

k
1X
v(fi )
k

(1)

i=1

A simple variation on the CBOW representation is weighted CBOW, in which different
vectors receive different weights:
1
WCBOW(f1 , ..., fk ) = Pk

i=1 ai

k
X

ai v(fi )

(2)

i=1

Here, each feature fi has an associated weight ai , indicating the relative importance of the
feature. For example, in a document classification task, a feature fi may correspond to a
word in the document, and the associated weight ai could be the words TF-IDF score.
3.3 Distance and Position Features
The linear distance in between two words in a sentence may serve as an informative feature.
For example, in an event extraction task37 we may be given a trigger word and a candidate
argument word, and asked to predict if the argument word is indeed an argument of the
trigger. The distance (or relative position) between the trigger and the argument is a strong
signal for this prediction task. In the traditional NLP setup, distances are usually encoded
by binning the distances into several groups (i.e. 1, 2, 3, 4, 510, 10+) and associating each
bin with a one-hot vector. In a neural architecture, where the input vector is not composed
of binary indicator features, it may seem natural to allocate a single input entry to the
distance feature, where the numeric value of that entry is the distance. However, this
approach is not taken in practice. Instead, distance features are encoded similarly to the
36. Note that if the v(fi )s were one-hot vectors rather than dense feature representations, the CBOW (eq
1) and WCBOW (eq 2) would reduce to the traditional (weighted) bag-of-words representations, which
is in turn equivalent to a sparse feature-vector representation in which each binary indicator feature
corresponds to a unique word.
37. The event extraction task involves identification of events from a predefined set of event types. For
example identification of purchase events or terror-attack events. Each event type can be triggered
by various triggering words (commonly verbs), and has several slots (arguments) that needs to be filled
(i.e. who purchased? what was purchased? at what amount?).

352

fiA Primer on Neural Networks for NLP

other feature types: each bin is associated with a d-dimensional vector, and these distanceembedding vectors are then trained as regular parameters in the network (Zeng et al., 2014;
dos Santos et al., 2015; Zhu et al., 2015a; Nguyen & Grishman, 2015).
3.4 Feature Combinations
Note that the feature extraction stage in the neural-network settings deals only with extraction of core features. This is in contrast to the traditional linear-model-based NLP systems
in which the feature designer had to manually specify not only the core features of interest
but also interactions between them (e.g., introducing not only a feature stating word is
X and a feature stating tag is Y but also combined feature stating word is X and tag
is Y or sometimes even word is X, tag is Y and previous word is Z). The combination
features are crucial in linear models because they introduce more dimensions to the input,
transforming it into a space where the data-points are closer to being linearly separable. On
the other hand, the space of possible combinations is very large, and the feature designer
has to spend a lot of time coming up with an effective set of feature combinations. One
of the promises of the non-linear neural network models is that one needs to define only
the core features. The non-linearity of the classifier, as defined by the network structure,
is expected to take care of finding the indicative feature combinations, alleviating the need
for feature combination engineering.
Kernel methods (Shawe-Taylor & Cristianini, 2004), and in particular polynomial kernels
(Kudo & Matsumoto, 2003), also allow the feature designer to specify only core features,
leaving the feature combination aspect to the learning algorithm. In contrast to neuralnetwork models, kernels methods are convex, admitting exact solutions to the optimization
problem. However, the computational complexity of classification in kernel methods scales
linearly with the size of the training data, making them too slow for most practical purposes,
and not suitable for training with large datasets. On the other hand, the computational
complexity of classification using neural networks scales linearly with the size of the network,
regardless of the training data size.
3.5 Dimensionality
How many dimensions should we allocate for each feature? Unfortunately, there are no theoretical bounds or even established best-practices in this space. Clearly, the dimensionality
should grow with the number of the members in the class (you probably want to assign
more dimensions to word embeddings than to part-of-speech embeddings) but how much is
enough? In current research, the dimensionality of word-embedding vectors range between
about 50 to a few hundreds, and, in some extreme cases, thousands. Since the dimensionality of the vectors has a direct effect on memory requirements and processing time, a good
rule of thumb would be to experiment with a few different sizes, and choose a good trade-off
between speed and task accuracy.
3.6 Vector Sharing
Consider a case where you have a few features that share the same vocabulary. For example,
when assigning a part-of-speech to a given word, we may have a set of features considering
353

fiGoldberg

the previous word, and a set of features considering the next word. When building the input
to the classifier, we will concatenate the vector representation of the previous word to the
vector representation of the next word. The classifier will then be able to distinguish the two
different indicators, and treat them differently. But should the two features share the same
vectors? Should the vector for dog:previous-word be the same as the vector of dog:nextword? Or should we assign them two distinct vectors? This, again, is mostly an empirical
question. If you believe words behave differently when they appear in different positions
(e.g., word X behaves like word Y when in the previous position, but X behaves like Z when
in the next position) then it may be a good idea to use two different vocabularies and assign
a different set of vectors for each feature type. However, if you believe the words behave
similarly in both locations, then something may be gained by using a shared vocabulary for
both feature types.
3.7 Networks Output
For multi-class classification problems with k classes, the networks output is a k-dimensional
vector in which every dimension represents the strength of a particular output class. That
is, the output remains as in the traditional linear models  scalar scores to items in a discrete
set. However, as we will see in Section 4, there is a d  k matrix associated with the output
layer. The columns of this matrix can be thought of as d dimensional embeddings of the
output classes. The vector similarities between the vector representations of the k classes
indicate the models learned similarities between the output classes.
3.8 Historical Note
Representing words as dense vectors for input to a neural network was popularized by Bengio
et al. (2003) in the context of neural language modeling. It was introduced to NLP tasks in
the pioneering work of Collobert, Weston and colleagues (2008, 2011).38 Using embeddings
for representing not only words but arbitrary features was popularized following Chen and
Manning (2014).

4. Feed-Forward Neural Networks
This section introduces feed-forward neural networks. It starts with the popular brain
inspired metaphor that triggered them, but quickly switches back to using mathematical
notation. We discuss the structure of feed forward neural networks, their representation
power, and common non-linearities and loss functions.
4.1 A Brain-Inspired Metaphor
As the name suggests, neural-networks are inspired by the brains computation mechanism,
which consists of computation units called neurons. In the metaphor, a neuron is a computational unit that has scalar inputs and outputs. Each input has an associated weight.
38. While the work by Bengio, Collobert, Weston and colleagues popularized the approaches, they were not
the first to use them. Earlier authors that use dense continuous-space vectors for representing word inputs
to neural networks include Lee et al. (1992) and Forcada and Neco (1997). Similarly, continuous-space
language models were used for machine-translation already by Schwenk et al. (2006).

354

fiA Primer on Neural Networks for NLP

The neuron multiplies each input by its weight, and then sums39 them, applies a non-linear
function to the result, and passes it to its output. The neurons are connected to each other,
forming a network: the output of a neuron may feed into the inputs of one or more neurons.
Such networks were shown to be very capable computational devices. If the weights are set
correctly, a neural network with enough neurons and a non-linear activation function can
approximate a very wide range of mathematical functions (we will be more precise about
this later).
Output
layer

Hidden
layer

Hidden
layer

Input layer

R

R

y1

y2

y3

R

R

R

R

R

R

R

R

x1

x2

x3

x4

R

Figure 2: Feed-forward neural network with two hidden layers.
A typical feed-forward neural network may be drawn as in Figure 2. Each circle is a
neuron, with incoming arrows being the neurons inputs and outgoing arrows being the neurons outputs. Each arrow carries a weight, reflecting its importance (not shown). Neurons
are arranged in layers, reflecting the flow of information. The bottom layer has no incoming
arrows, and is the input to the network. The top-most layer has no outgoing arrows, and is
the output of the network. The other layers are considered hidden. The sigmoid shape
inside the neurons in the middle layers represent a non-linear function (i.e., the logistic
function 1/(1 + exa )) that is applied to the neurons value before passing it to the output.
In the figure, each neuron is connected to all of the neurons in the next layer  this is called
a fully-connected layer or an affine layer.
While the brain metaphor is sexy and intriguing, it is also distracting and cumbersome
to manipulate mathematically. We therefore switch to using more concise mathematical
notation. The values of each row of neurons in the network can be thought of as a vector.
In Figure 2 the input layer is a 4 dimensional vector (x), and the layer above it is a 6 dimensional vector (h1 ). The fully connected layer can be thought of as a linear transformation
39. While summing is the most common operation, other functions, such as a max, are also possible

355

fiGoldberg

from 4 dimensions to 6 dimensions. A fully-connected layer implements a vector-matrix
multiplication, h = xW where the weight of the connection from the ith neuron in the
input row to the jth neuron in the output row is Wij .40 The values of h are then transformed by a non-linear function g that is applied to each value before being passed on to the
next input. The whole computation from input to output can be written as: (g(xW1 ))W2
where W1 are the weights of the first layer and W2 are the weights of the second one.
4.2 In Mathematical Notation
From this point on, we will abandon the brain metaphor and describe networks exclusively
in terms of vector-matrix operations.
The simplest neural network is the perceptron, which is a linear function of its inputs:
NNPerceptron (x) = xW + b

(3)

x  Rdin , W  Rdin dout , b  Rdout
W is the weight matrix, and b is a bias term.41 In order to go beyond linear functions, we
introduce a non-linear hidden layer (the network in Figure 2 has two such layers), resulting in
the Multi Layer Perceptron with one hidden-layer (MLP1). A feed-forward neural network
with one hidden-layer has the form:
NNMLP1 (x) = g(xW1 + b1 )W2 + b2

(4)

x  Rdin , W1  Rdin d1 , b1  Rd1 , W2  Rd1 d2 , b2  Rd2
Here W1 and b1 are a matrix and a bias term for the first linear transformation of the
input, g is a non-linear function that is applied element-wise (also called a non-linearity or
an activation function), and W2 and b2 are the matrix and bias term for a second linear
transform.
Breaking it down, xW1 +b1 is a linear transformation of the input x from din dimensions
to d1 dimensions. g is then applied to each of the d1 dimensions, and the matrix W2 together
with bias vector b2 are then used to transform the result into the d2 dimensional output
vector. The non-linear activation function g has a crucial role in the networks ability to
represent complex functions. Without the non-linearity in g, the neural network can only
represent linear transformations of the input.42
We can add additional linear-transformations and non-linearities, resulting in an MLP
with two hidden-layers (the network in Figure 2 is of this form):
NNMLP2 (x) = (g 2 (g 1 (xW1 + b1 )W2 + b2 ))W3

(5)

It is perhaps clearer to write deeper networks like this using intermediary variables:
40. To see why this is P
the case, denote the weight of the ith input of the jth neuron in h as wij . The value
of hj is then hj = 4i=1 xi  wij .
41. The network in figure 2 does not include bias terms. A bias term can be added to a layer by adding to
it an additional neuron that does not have any incoming connections, whose value is always 1.
42. To see why, consider that a sequence of linear transformations is still a linear transformation.

356

fiA Primer on Neural Networks for NLP

NNMLP2 (x) =y
h1 =g 1 (xW1 + b1 )
h2 =g 2 (h1 W2 + b2 )

(6)

y =h2 W3
The vector resulting from each linear transform is referred to as a layer. The outer-most
linear transform results in the output layer and the other linear transforms result in hidden
layers. Each hidden layer is followed by a non-linear activation. In some cases, such as in
the last layer of our example, the bias vectors are forced to 0 (dropped).
Layers resulting from linear transformations are often referred to as fully connected, or
affine. Other types of architectures exist. In particular, image recognition problems benefit
from convolutional and pooling layers. Such layers have uses also in language processing,
and will be discussed in Section 9. Networks with several hidden layers are said to be deep
networks, hence the name deep learning.
When describing a neural network, one should specify the dimensions of the layers and
the input. A layer will expect a din dimensional vector as its input, and transform it into a
dout dimensional vector. The dimensionality of the layer is taken to be the dimensionality
of its output. For a fully connected layer l(x) = xW + b with input dimensionality din and
output dimensionality dout , the dimensions of x is 1  din , of W is din  dout and of b is
1  dout .
The output of the network is a dout dimensional vector. In case dout = 1, the networks
output is a scalar. Such networks can be used for regression (or scoring) by considering
the value of the output, or for binary classification by consulting the sign of the output.
Networks with dout = k > 1 can be used for k-class classification, by associating each
dimension with a class, and looking for the dimension with maximal value. Similarly, if
the output vector entries are positive and sum to one, the output can be interpreted as
a distribution over class assignments (such output normalization is typically achieved by
applying a softmax transformation on the output layer, see Section 4.5).
The matrices and the bias terms that define the linear transformations are the parameters of the network. It is common to refer to the collection of all parameters as . Together
with the input, the parameters determine the networks output. The training algorithm is
responsible for setting their values such that the networks predictions are correct. Training
is discussed in Section 6.
4.3 Representation Power
In terms of representation power, it was shown by Hornik, Stinchcombe, and White (1989)
and Cybenko (1989) that MLP1 is a universal approximator  it can approximate with
any desired non-zero amount of error a family of functions43 that include all continuous
functions on a closed and bounded subset of Rn , and any function mapping from any finite
43. Specifically, a feed-forward network with linear output layer and at least one hidden layer with a squashing activation function can approximate any Borel measurable function from one finite dimensional space
to another.

357

fiGoldberg

dimensional discrete space to another. This may suggest there is no reason to go beyond
MLP1 to more complex architectures. However, the theoretical result does not discuss the
learnability of the neural network (it states that a representation exists, but does not say
how easy or hard it is to set the parameters based on training data and a specific learning
algorithm). It also does not guarantee that a training algorithm will find the correct function
generating our training data. Finally, it does not state how large the hidden layer should
be. Indeed, Telgarsky (2016) show that there exist neural networks with many layers of
bounded size that cannot be approximated by networks with fewer layers unless they layers
are exponentially large.
In practice, we train neural networks on relatively small amounts of data using local
search methods such as variants of stochastic gradient descent, and use hidden layers of
relatively modest sizes (up to several thousands). As the universal approximation theorem
does not give any guarantees under these non-ideal, real-world conditions, there is definitely
benefit to be had in trying out more complex architectures than MLP1. In many cases,
however, MLP1 does indeed provide strong results. For further discussion on the representation power of feed-forward neural networks, see book by Bengio et al. (2015, Section
6.5).

4.4 Common Non-linearities
The non-linearity g can take many forms. There is currently no good theory as to which
non-linearity to apply in which conditions, and choosing the correct non-linearity for a
given task is for the most part an empirical question. I will now go over the common nonlinearities from the literature: the sigmoid, tanh, hard tanh and the rectified linear unit
(ReLU). Some NLP researchers also experimented with other forms of non-linearities such
as cube and tanh-cube.

4.4.1 Sigmoid
The sigmoid activation function (x) = 1/(1 + ex ), also called the logistic function, is an
S-shaped function, transforming each value x into the range [0, 1]. The sigmoid was the
canonical non-linearity for neural networks since their inception, but is currently considered
to be deprecated for use in internal layers of neural networks, as the choices listed below
prove to work much better empirically.

4.4.2 Hyperbolic Tangent (tanh)
2x

The hyperbolic tangent tanh(x) = ee2x 1
activation function is an S-shaped function, trans+1
forming the values x into the range [1, 1].
358

fiA Primer on Neural Networks for NLP

4.4.3 Hard tanh
The hard-tanh activation function is an approximation of the tanh function which is faster
to compute and take derivatives of:


1 x < 1
hardtanh(x) = 1
(7)
x>1


x
otherwise
4.4.4 Rectifier (ReLU)
The Rectifier activation function (Glorot, Bordes, & Bengio, 2011), also known as the
rectified linear unit is a very simple activation function that is easy to work with and was
shown many times to produce excellent results.44 The ReLU unit clips each value x < 0 at
0. Despite its simplicity, it performs well for many tasks, especially when combined with
the dropout regularization technique (see Section 6.4).
(
0
ReLU(x) = max(0, x) =
x

x<0
otherwise

(8)

As a rule of thumb, ReLU units work better than tanh, and tanh works better than
sigmoid.45
4.5 Output Transformations
In many cases, the output layer vector is also transformed. A common transformation is
the softmax :
x =x1 , . . . , xk
e xi
softmax(xi ) = Pk
xj
j=1 e

(9)

44. The technical advantages of the ReLU over the sigmoid and tanh activation functions is that it does not
involve expensive-to-compute functions, and more importantly that it does not saturate. The sigmoid
and tanh activation are capped at 1, and the gradients at this region of the functions are near zero,
driving the entire gradient near zero. The ReLU activation does not have this problem, making it
especially suitable for networks with multiple layers, which are susceptible to the vanishing gradients
problem when trained with the saturating units.
45. In addition to these activation functions, recent works from the NLP community experiment with and
reported success with other forms of non-linearities. The Cube activation function, g(x) = (x)3 , was
suggested by Chen and Manning (2014), who found it to be more effective than other non-linearities in
a feed-forward network that was used to predict the actions in a greedy transition-based dependency
parser. The tanh cube activation function g(x) = tanh((x)3 + x) was proposed by Pei et al. (2015),
who found it to be more effective than other non-linearities in a feed-forward network that was used as
a component in a structured-prediction graph-based dependency parser.
The cube and tanh-cube activation functions are motivated by the desire to better capture interactions between different features. While these activation functions are reported to improve performance
in certain situations, their general applicability is still to be determined.

359

fiGoldberg

The result is a vector of non-negative real numbers that sum to one, making it a discrete
probability distribution over k possible outcomes.
The softmax output transformation is used when we are interested in modeling a probability distribution over the possible output classes. To be effective, it should be used in
conjunction with a probabilistic training objective such as cross-entropy (see Section 4.7.4
below).
When the softmax transformation is applied to the output of a network without a hidden
layer, the result is the well known multinomial logistic regression model, also known as a
maximum-entropy classifier.
4.6 Embedding Layers
Up until now, the discussion ignored the source of x, treating it as an arbitrary vector.
In an NLP application, x is usually composed of various embeddings vectors. We can be
explicit about the source of x, and include it in the networks definition. We introduce c(),
a function from core features to an input vector.
It is common for c to extract the embedding vector associated with each feature, and
concatenate them:
x = c(f1 , f2 , f3 ) =[v(f1 ); v(f2 ); v(f3 )]
NNMLP1 (x) =NNMLP1 (c(f1 , f2 , f3 ))
=NNMLP1 ([v(f1 ); v(f2 ); v(f3 )])

(10)

=(g([v(f1 ); v(f2 ); v(f3 )]W1 + b1 ))W2 + b2
Another common choice is for c to sum the embedding vectors (this assumes the embedding vectors all share the same dimensionality):
x = c(f1 , f2 , f3 ) =v(f1 ) + v(f2 ) + v(f3 )
NNMLP1 (x) =NNMLP1 (c(f1 , f2 , f3 ))
=NNMLP1 (v(f1 ) + v(f2 ) + v(f3 ))

(11)

=(g((v(f1 ) + v(f2 ) + v(f3 ))W1 + b1 ))W2 + b2
The form of c is an essential part of the networks design. In many papers, it is common
to refer to c as part of the network, and likewise treat the word embeddings v(fi ) as resulting
from an embedding layer or lookup layer. Consider a vocabulary of |V | words, each
embedded as a d dimensional vector. The collection of vectors can then be thought of as a
|V |  d embedding matrix E in which each row corresponds to an embedded feature. Let
fi be a |V |-dimensional vector, which is all zeros except from one index, corresponding to
the value of the ith feature, in which the value is 1 (this is called a one-hot vector). The
multiplication fi E will then select the corresponding row of E. Thus, v(fi ) can be defined
in terms of E and fi :
v(fi ) = fi E
360

(12)

fiA Primer on Neural Networks for NLP

And similarly:
CBOW(f1 , ..., fk ) =

k
X

(fi E) = (

i=1

k
X

fi )E

(13)

i=1

The input to the network is then considered to be a collection of one-hot vectors. While
this is elegant and well defined mathematically, an efficient implementation typically involves
a hash-based data structure mapping features to their corresponding embedding vectors,
without going through the one-hot representation.
In this tutorial, we take c to be separate from the network architecture: the networks
inputs are always dense real-valued input vectors, and c is applied before the input is passed
the network, similar to a feature function in the familiar linear-models terminology. However, when training a network, the input vector x does remember how it was constructed,
and can propagate error gradients back to its component embedding vectors, as appropriate
(error propagation is discussed in section 6).
4.6.1 A Note on Notation
When describing network layers that get concatenated vectors x, y and z as input, some
authors use explicit concatenation ([x; y; z]W +b) while others use an affine transformation
(xU + yV + zW + b). If the weight matrices U, V, W in the affine transformation are
different than one another, the two notations are equivalent.
4.6.2 A Note on Sparse vs. Dense Features
Consider a network which uses a traditional sparse representation for its input vectors,
and no embedding layer. Assuming the set of all available features is V and we have k on
features f1 , . . . , fk , fi  V , the networks input is:
x=

k
X

|V |

x  N+

fi

i=1

(14)

and so the first layer (ignoring the non-linear activation) is:
k
X
xW + b = (
fi )W

(15)

i=1

W  R|V |d , b  Rd
This layer selects rows of W corresponding to the input features in x and sums them,
then adding a bias term. This is very similar to an embedding layer that produces a CBOW
representation over the features, where the matrix W acts as the embedding matrix. The
main difference is the introduction of the bias vector b, and the fact that the embedding
layer typically does not undergo a non-linear activation but rather passed on directly to the
first layer. Another difference is that this scenario forces each feature to receive a separate
vector (row in W) while the embedding layer provides more flexibility, allowing for example
for the features next word is dog and previous word is dog to share the same vector.
361

fiGoldberg

However, these differences are small and subtle. When it comes to multi-layer feed-forward
networks, the difference between dense and sparse inputs is smaller than it may seem at
first sight.
4.7 Loss Functions
When training a neural network (more on training in Section 6 below), much like when
training a linear classifier, one defines a loss function L(y, y), stating the loss of predicting
y when the true output is y. The training objective is then to minimize the loss across
the different training examples. The loss L(y, y) assigns a numerical score (a scalar) for
the networks output y given the true expected output y.46 The loss function should be
bounded from below, with the minimum attained only for cases where the networks output
is correct.
The parameters of the network (the matrices Wi , the biases bi and commonly the embeddings E) are then set in order to minimize the loss L over the training examples (usually,
it is the sum of the losses over the different training examples that is being minimized).
The loss can be an arbitrary function mapping two vectors to a scalar. For practical
purposes of optimization, we restrict ourselves to functions for which we can easily compute
gradients (or sub-gradients). In most cases, it is sufficient and advisable to rely on a common
loss function rather than defining your own. For a detailed discussion on loss functions for
neural networks see work by LeCun, Chopra, Hadsell, Ranzato, and Huang (2006), LeCun
and Huang (2005) and Bengio et al. (2015). We now discuss some loss functions that are
commonly used in neural networks for NLP.
4.7.1 Hinge (binary)
For binary classification problems, the networks output is a single scalar y and the intended
output y is in {+1, 1}. The classification rule is sign(y), and a classification is considered
correct if y  y > 0, meaning that y and y share the same sign. The hinge loss, also known
as margin loss or SVM loss, is defined as:
Lhinge(binary) (y, y) = max(0, 1  y  y)

(16)

The loss is 0 when y and y share the same sign and |y|  1. Otherwise, the loss is linear.
In other words, the binary hinge loss attempts to achieve a correct classification, with a
margin of at least 1.
4.7.2 Hinge (multiclass)
The hinge loss was extended to the multiclass setting by Crammer and Singer (2002). Let
y = y1 , . . . , yn be the networks output vector, and y be the one-hot vector for the correct
output class.
The classification rule is defined as selecting the class with the highest score:
prediction = arg max yi
i

(17)

46. In our notation, both the models output and the expected output are vectors, while in many cases it is
more natural to think of the expected output as a scalar (class assignment). In such cases, y is simply
the corresponding one-hot vector.

362

fiA Primer on Neural Networks for NLP

Denote by t = arg maxi yi the correct class, and by k = arg maxi6=t yi the highest scoring
class such that k 6= t. The multiclass hinge loss is defined as:
Lhinge(multiclass) (y, y) = max(0, 1  (yt  yk ))

(18)

The multiclass hinge loss attempts to score the correct class above all other classes with a
margin of at least 1.
Both the binary and multiclass hinge losses are intended to be used with a linear output
layer. The hinge losses are useful whenever we require a hard decision rule, and do not
attempt to model class membership probability.
4.7.3 Log Loss
The log loss is a common variation of the hinge loss, which can be seen as a soft version
of the hinge loss with an infinite margin (LeCun et al., 2006).
Llog (y, y) = log(1 + exp((yt  yk ))

(19)

4.7.4 Categorical Cross-Entropy Loss
The categorical cross-entropy loss (also referred to as negative log likelihood ) is used when
a probabilistic interpretation of the scores is desired.
Let y = y1 , . . . , yn be a vector representing the true multinomial distribution over the
labels 1, . . . , n, and let y = y1 , . . . , yn be the networks output, which was transformed by the
softmax activation function, and represent the class membership conditional distribution
yi = P (y = i|x). The categorical cross entropy loss measures the dissimilarity between
the true label distribution y and the predicted label distribution y, and is defined as cross
entropy:
Lcross-entropy (y, y) = 

X

yi log(yi )

(20)

i

For hard classification problems in which each training example has a single correct
class assignment, y is a one-hot vector representing the true class. In such cases, the cross
entropy can be simplified to:
Lcross-entropy(hard classification) (y, y) =  log(yt )

(21)

where t is the correct class assignment. This attempts to set the probability mass assigned
to the correct class t to 1. Because the scores y have been transformed using the softmax
function and represent a conditional distribution, increasing the mass assigned to the correct
class means decreasing the mass assigned to all the other classes.
The cross-entropy loss is very common in the neural networks literature, and produces a
multi-class classifier which does not only predict the one-best class label but also predicts a
distribution over the possible labels. When using the cross-entropy loss, it is assumed that
the networks output is transformed using the softmax transformation.
363

fiGoldberg

4.7.5 Ranking Losses
In some settings, we are not given supervision in term of labels, but rather as pairs of
correct and incorrect items x and x0 , and our goal is to score correct items above incorrect
ones. Such training situations arise when we have only positive examples, and generate
negative examples by corrupting a positive example. A useful loss in such scenarios is the
margin-based ranking loss, defined for a pair of correct and incorrect examples:
Lranking(margin) (x, x0 ) = max(0, 1  (NN(x)  NN(x0 )))

(22)

where NN(x) is the score assigned by the network for input vector x. The objective is to
score (rank) correct inputs over incorrect ones with a margin of at least 1.
A common variation is to use the log version of the ranking loss:
Lranking(log) (x, x0 ) = log(1 + exp((NN(x)  NN(x0 ))))

(23)

Examples using the ranking hinge loss in language tasks include training with the auxiliary tasks used for deriving pre-trained word embeddings (see section 5), in which we are
given a correct word sequence and a corrupted word sequence, and our goal is to score
the correct sequence above the corrupt one (Collobert & Weston, 2008). Similarly, Van
de Cruys (2014) used the ranking loss in a selectional-preferences task, in which the network was trained to rank correct verb-object pairs above incorrect, automatically derived
ones, and Weston, Bordes, Yakhnenko, and Usunier (2013) trained a model to score correct
(head,relation,trail) triplets above corrupted ones in an information-extraction setting. An
example of using the ranking log loss can be found in the work of Gao et al. (2014). A
variation of the ranking log loss allowing for a different margin for the negative and positive
class is given in work by dos Santos et al. (2015).

5. Word Embeddings
A main component of the neural-network approach is the use of embeddings  representing
each feature as a vector in a low dimensional space. But where do the vectors come from?
This section will survey the common approaches.
5.1 Random Initialization
When enough supervised training data is available, one can just treat the feature embeddings
the same as the other model parameters: initialize the embedding vectors to random values,
and let the network-training procedure tune them into good vectors.
Some care has to be taken in the way the random initialization is performed. The method
used by the effective word2vec implementation (Mikolov et al., 2013; Mikolov, Sutskever,
Chen, Corrado, & Dean, 2013) is to initialize the word vectors to uniformly sampled random
1 1
numbers in the range [ 2d
, 2d ] where d is the number of dimensions. Another option is to
use xavier
(see Section 6.3.1) and initialize with uniformly sampled values
h  initialization
 i
6 6

from  d , d .
364

fiA Primer on Neural Networks for NLP

In practice, one will often use the random initialization approach to initialize the embedding vectors of commonly occurring features, such as part-of-speech tags or individual
letters, while using some form of supervised or unsupervised pre-training to initialize the
potentially rare features, such as features for individual words. The pre-trained vectors can
then either be treated as fixed during the network training process, or, more commonly,
treated like the randomly-initialized vectors and further tuned to the task at hand.
5.2 Supervised Task-Specific Pre-training
If we are interested in task A, for which we only have a limited amount of labeled data (for
example, syntactic parsing), but there is an auxiliary task B (say, part-of-speech tagging)
for which we have much more labeled data, we may want to pre-train our word vectors so
that they perform well as predictors for task B, and then use the trained vectors for training
task A. In this way, we can utilize the larger amounts of labeled data we have for task B.
When training task A we can either treat the pre-trained vectors as fixed, or tune them
further for task A. Another option is to train jointly for both objectives, see Section 7 for
more details.
5.3 Unsupervised Pre-training
The common case is that we do not have an auxiliary task with large enough amounts of
annotated data (or maybe we want to help bootstrap the auxiliary task training with better
vectors). In such cases, we resort to unsupervised methods, which can be trained on huge
amounts of unannotated text.
The techniques for training the word vectors are essentially those of supervised learning,
but instead of supervision for the task that we care about, we instead create practically
unlimited number of supervised training instances from raw text, hoping that the tasks
that we created will match (or be close enough to) the final task we care about.47
The key idea behind the unsupervised approaches is that one would like the embedding
vectors of similar words to have similar vectors. While word similarity is hard to define
and is usually very task-dependent, the current approaches derive from the distributional
hypothesis (Harris, 1954), stating that words are similar if they appear in similar contexts.
The different methods all create supervised training instances in which the goal is to either
predict the word from its context, or predict the context from the word.
An important benefit of training word embeddings on large amounts of unannotated
data is that it provides vector representations for words that do not appear in the supervised training set. Ideally, the representations for these words will be similar to those of
related words that do appear in the training set, allowing the model to generalize better on
unseen events. It is thus desired that the similarity between word vectors learned by the unsupervised algorithm captures the same aspects of similarity that are useful for performing
the intended task of the network.
47. The interpretation of creating auxiliary problems from raw text is inspired by Ando and Zhang (2005a)
and Ando and Zhang (2005b).

365

fiGoldberg

Common unsupervised word-embedding algorithms include word2vec 48 (Mikolov et al.,
2013, 2013), GloVe (Pennington, Socher, & Manning, 2014) and the Collobert and Weston
(2008, 2011) embeddings algorithm. These models are inspired by neural networks and
are based on stochastic gradient training. However, they are deeply connected to another
family of algorithms which evolved in the NLP and IR communities, and that are based on
matrix factorization (for a discussion see Levy & Goldberg, 2014b; Levy et al., 2015).
Arguably, the choice of auxiliary problem (what is being predicted, based on what kind
of context) affects the resulting vectors much more than the learning method that is being
used to train them. We thus focus on the different choices of auxiliary problems that are
available, and only skim over the details of the training methods. Several software packages
for deriving word vectors are available, including word2vec49 and Gensim50 implementing
the word2vec models with word-windows based contexts, word2vecf51 which is a modified
version of word2vec allowing the use of arbitrary contexts, and GloVe52 implementing the
GloVe model. Many pre-trained word vectors are also available for download on the web.
While beyond the scope of this tutorial, it is worth noting that the word embeddings
derived by unsupervised training algorithms have a wide range of applications in NLP
beyond using them for initializing the word-embeddings layer of a neural-network model.
5.4 Training Objectives
Given a word w and its context c, different algorithms formulate different auxiliary tasks.
In all cases, each word is represented as a d-dimensional vector which is initialized to a
random value. Training the model to perform the auxiliary tasks well will result in good
word embeddings for relating the words to the contexts, which in turn will result in the
embedding vectors for similar words to be similar to each other.
Language-modeling inspired approaches such as those taken by Mikolov et al. (2013),
Mnih and Kavukcuoglu (2013) as well as GloVe (Pennington et al., 2014) use auxiliary tasks
in which the goal is to predict the word given its context. This is posed in a probabilistic
setup, trying to model the conditional probability P (w|c).
Other approaches reduce the problem to that of binary classification. In addition to
the set D of observed word-context pairs, a set D is created from random words and
context pairings. The binary classification problem is then: does the given (w, c) pair
come from D or not? The approaches differ in how the set D is constructed, what is
the structure of the classifier, and what is the objective being optimized. Collobert and
Weston (2008, 2011) take a margin-based binary ranking approach, training a feed-forward
neural network to score correct (w, c) pairs over incorrect ones. Mikolov et al. (2013, 2014)
take instead a probabilistic version, training a log-bilinear model to predict the probability
P ((w, c)  D|w, c) that the pair come from the corpus rather than the random sample.
48. While often treated as a single algorithm, word2vec is actually a software package including various
training objectives, optimization methods and other hyperparameters. See work by Rong (2014) and
Levy, Goldberg, and Dagan (2015) for a discussion.
49. https://code.google.com/p/word2vec/
50. https://radimrehurek.com/gensim/
51. https://bitbucket.org/yoavgo/word2vecf
52. http://nlp.stanford.edu/projects/glove/

366

fiA Primer on Neural Networks for NLP

5.5 The Choice of Contexts
In most cases, the contexts of a word are taken to be other words that appear in its
surrounding, either in a short window around it, or within the same sentence, paragraph
or document. In some cases the text is automatically parsed by a syntactic parser, and
the contexts are derived from the syntactic neighbourhood induced by the automatic parse
trees. Sometimes, the definitions of words and context change to include also parts of words,
such as prefixes or suffixes.
Neural word embeddings originated from the world of language modeling, in which a
network is trained to predict the next word based on a sequence of preceding words (Bengio
et al., 2003). There, the text is used to create auxiliary tasks in which the aim is to predict
a word based on a context the k previous words. While training for the language modeling
auxiliary prediction problems indeed produce useful embeddings, this approach is needlessly
restricted by the constraints of the language modeling task, in which one is allowed to look
only at the previous words. If we do not care about language modeling but only about the
resulting embeddings, we may do better by ignoring this constraint and taking the context
to be a symmetric window around the focus word.
5.5.1 Window Approach
The most common approach is a sliding window approach, in which auxiliary tasks are
created by looking at a sequence of 2k + 1 words. The middle word is callled the focus word
and the k words to each side are the contexts. Then, either a single task is created in which
the goal is to predict the focus word based on all of the context words (represented either
using CBOW, see Mikolov et al., 2013 or vector concatenation, see Collobert & Weston,
2008), or 2k distinct tasks are created, each pairing the focus word with a different context
word. The 2k tasks approach, popularized by Mikolov et al. (2013) is referred to as a
skip-gram model. Skip-gram based approaches are shown to be robust and efficient to train
(Mikolov et al., 2013; Pennington et al., 2014), and often produce state of the art results.
Effect of Window Size The size of the sliding window has a strong effect on the resulting vector similarities. Larger windows tend to produce more topical similarities (i.e.
dog, bark and leash will be grouped together, as well as walked, run and walking), while smaller windows tend to produce more functional and syntactic similarities (i.e.
Poodle, Pitbull, Rottweiler, or walking,running,approaching).
Positional Windows When using the CBOW or skip-gram context representations, all
the different context words within the window are treated equally. There is no distinction
between context words that are close to the focus words and those that are farther from
it, and likewise there is no distinction between context words that appear before the focus
words to context words that appear after it. Such information can easily be factored in by
using positional contexts: indicating for each context word also its relative position to the
focus words (i.e. instead of the context word being the it becomes the:+2, indicating
the word appears two positions to the right of the focus word). The use of positional context
together with smaller windows tend to produce similarities that are more syntactic, with
a strong tendency of grouping together words that share a part of speech, as well as being
functionally similar in terms of their semantics. Positional vectors were shown by Ling,
367

fiGoldberg

Dyer, Black, and Trancoso (2015a) to be more effective than window-based vectors when
used to initialize networks for part-of-speech tagging and syntactic dependency parsing.
Variants Many variants on the window approach are possible. One may lemmatize words
before learning, apply text normalization, filter too short or too long sentences, or remove
capitalization (see, e.g., the pre-processing steps described in dos Santos & Gatti, 2014).
One may sub-sample part of the corpus, skipping with some probability the creation of tasks
from windows that have too common or too rare focus words. The window size may be
dynamic, using a different window size at each turn. One may weigh the different positions
in the window differently, focusing more on trying to predict correctly close word-context
pairs than further away ones. Each of these choices will effect the resulting vectors. Some
of these hyperparameters (and others) are discussed by Levy et al. (2015).
5.5.2 Sentences, Paragraphs or Documents
Using a skip-grams (or CBOW) approach, one can consider the contexts of a word to be all
the other words that appear with it in the same sentence, paragraph or document. This is
equivalent to using very large window sizes, and is expected to result in word vectors that
capture topical similarity (words from the same topic, i.e. words that one would expect to
appear in the same document, are likely to receive similar vectors).
5.5.3 Syntactic Window
Some work replace the linear context within a sentence with a syntactic one (Levy &
Goldberg, 2014a; Bansal, Gimpel, & Livescu, 2014). The text is automatically parsed
using a dependency parser, and the context of a word is taken to be the words that are
in its proximity in the parse tree, together with the syntactic relation by which they are
connected. Such approaches produce highly functional similarities, grouping together words
than can fill the same role in a sentence (e.g. colors, names of schools, verbs of movement).
The grouping is also syntactic, grouping together words that share an inflection (Levy &
Goldberg, 2014a).
5.5.4 Multilingual
Another option is using multilingual, translation based contexts (Hermann & Blunsom,
2014; Faruqui & Dyer, 2014). For example, given a large amount of sentence-aligned parallel
text, one can run a bilingual alignment model such as the IBM model 1 or model 2 (i.e.
using the GIZA++ software), and then use the produced alignments to derive word contexts.
Here, the context of a word instance are the foreign language words that are aligned to it.
Such alignments tend to result in synonym words receiving similar vectors. Some authors
work instead on the sentence alignment level, without relying on word alignments (Gouws,
Bengio, & Corrado, 2015) or train an end-to-end machine-translation neural network and
use the resulting word embeddings (Hill, Cho, Jean, Devin, & Bengio, 2014). An appealing
method is to mix a monolingual window-based approach with a multilingual approach,
creating both kinds of auxiliary tasks. This is likely to produce vectors that are similar to
the window-based approach, but reducing the somewhat undesired effect of the window368

fiA Primer on Neural Networks for NLP

based approach in which antonyms (e.g. hot and cold, high and low) tend to receive similar
vectors (Faruqui & Dyer, 2014).
5.5.5 Character-Based and Sub-word Representations
An interesting line of work attempts to derive the vector representation of a word from the
characters that compose it. Such approaches are likely to be particularly useful for tasks
which are syntactic in nature, as the character patterns within words are strongly related
to their syntactic function. These approaches also have the benefit of producing very small
model sizes (only one vector for each character in the alphabet together with a handful of
small matrices needs to be stored), and being able to provide an embedding vector for every
word that may be encountered. Dos Santos and Gatti (2014), dos Santos and Zadrozny
(2014) and Kim et al. (2015) model the embedding of a word using a convolutional network
(see Section 9) over the characters. Ling et al. (2015b) model the embedding of a word
using the concatenation of the final states of two RNN (LSTM) encoders (Section 10), one
reading the characters from left to right, and the other from right to left. Both produce
very strong results for part-of-speech tagging. The work of Ballesteros et al. (2015) show
that the two-LSTMs encoding of Ling et al. (2015b) is beneficial also for representing words
in dependency parsing of morphologically rich languages.
Deriving representations of words from the representations of their characters is motivated by the unknown words problem  what do you do when you encounter a word for
which you do not have an embedding vector? Working on the level of characters alleviates
this problem to a large extent, as the vocabulary of possible characters is much smaller
than the vocabulary of possible words. However, working on the character level is very
challenging, as the relationship between form (characters) and function (syntax, semantics)
in language is quite loose. Restricting oneself to stay on the character level may be an
unnecessarily hard constraint. Some researchers propose a middle-ground, in which a word
is represented as a combination of a vector for the word itself with vectors of sub-word
units that comprise it. The sub-word embeddings then help in sharing information between
different words with similar forms, as well as allowing back-off to the subword level when
the word is not observed. At the same time, the models are not forced to rely solely on
form when enough observations of the word are available. Botha and Blunsom (2014) suggest to model the embedding vector of a word as a sum of the word-specific vector if such
vector is available, with vectors for the different morphological components that comprise
it (the components are derived using Morfessor (Creutz & Lagus, 2007), an unsupervised
morphological segmentation method). Gao et al. (2014) suggest using as core features not
only the word form itself but also a unique feature (hence a unique embedding vector) for
each of the letter-trigrams in the word.

6. Neural Network Training
Neural network training is done by trying to minimize a loss function over a training set,
using a gradient-based method. Roughly speaking, all training methods work by repeatedly
computing an estimate of the error over the dataset, computing the gradient with respect
to the error, and then moving the parameters in the opposite direction of the gradient.
Models differ in how the error estimate is computed, and how moving in the opposite
369

fiGoldberg

direction of the gradient is defined. We describe the basic algorithm, stochastic gradient
descent (SGD), and then briefly mention the other approaches with pointers for further
reading. Gradient calculation is central to the approach. Gradients can be efficiently and
automatically computed using reverse mode differentiation on a computation graph  a
general algorithmic framework for automatically computing the gradient of any network
and loss function, to be discussed in Section 6.2.
6.1 Stochastic Gradient Training
The common approach for training neural networks is using the stochastic gradient descent
(SGD) algorithm (Bottou, 2012; LeCun, Bottou, Orr, & Muller, 1998a) or a variant of it.
SGD is a general optimization algorithm. It receives a function f parameterized by , a
loss function, and desired input and output pairs. It then attempts to set the parameters 
such that the loss of f with respect to the training examples is small. The algorithm works
as follows:
Algorithm 1 Online Stochastic Gradient Descent Training
1: Input: Function f (x; ) parameterized with parameters .
2: Input: Training set of inputs x1 , . . . , xn and desired outputs y1 , . . . , yn .
3: Input: Loss function L.
4: while stopping criteria not met do
5:
Sample a training example xi , yi
6:
Compute the loss L(f (xi ; ), yi )
7:
g  gradients of L(f (xi ; ), yi ) w.r.t 
8:
    t g
9: return 
PnThe goal of the algorithm is to set the parameters  so as to minimize the total loss
i=1 L(f (xi ; ), yi ) over the training set. It works by repeatedly sampling a training example and computing the gradient of the error on the example with respect to the parameters
 (line 7)  the input and expected output are assumed to be fixed, and the loss is treated
as a function of the parameters . The parameters  are then updated in the opposite
direction of the gradient, scaled by a learning rate t (line 8). The learning rate can either
be fixed throughout the training process, or decay as a function of the time step t.53 For
further discussion on setting the learning rate, see Section 6.3.
Note that the error calculated in line 6 is based on a single training example, and is thus
just a rough estimate of the corpus-wide loss that we are aiming to minimize. The noise in
the loss computation may result in inaccurate gradients. A common way of reducing this
noise is to estimate the error and the gradients based on a sample of m examples. This
gives rise to the minibatch SGD algorithm:
In lines 6  9 the algorithm estimates the gradient of the corpus loss based on the
minibatch. After the loop, g contains the gradient estimate, and the parameters  are
updated toward g. The minibatch size can vary in size from m = 1 to m = n. Higher
values provide better estimates of the corpus-wide gradients, while smaller values allow
53. Learning rate decay is required in order to prove convergence of SGD.

370

fiA Primer on Neural Networks for NLP

Algorithm 2 Minibatch Stochastic Gradient Descent Training
1: Input: Function f (x; ) parameterized with parameters .
2: Input: Training set of inputs x1 , . . . , xn and desired outputs y1 , . . . , yn .
3: Input: Loss function L.
4: while stopping criteria not met do
5:
Sample a minibatch of m examples {(x1 , y1 ), . . . , (xm , ym )}
6:
g  0
7:
for i = 1 to m do
8:
Compute the loss L(f (xi ; ), yi )
1
9:
g  g + gradients of m
L(f (xi ; ), yi ) w.r.t 
    t g
11: return 
10:

more updates and in turn faster convergence. Besides the improved accuracy of the gradients
estimation, the minibatch algorithm provides opportunities for improved training efficiency.
For modest sizes of m, some computing architectures (i.e. GPUs) allow an efficient parallel
implementation of the computation in lines 69. With a properly decreasing learning rate,
SGD is guaranteed to converge to a global optimum if the function is convex. However, it
can also be used to optimize non-convex functions such as neural-network. While there are
no longer guarantees of finding a global optimum, the algorithm proved to be robust and
performs well in practice.54
When training a neural network, the parameterized function f is the neural network, and
the parameters  are the linear-transformation matrices, bias terms, embedding matrices
and so on. The gradient computation is a key step in the SGD algorithm, as well as in
all other neural network training algorithms. The question is, then, how to compute the
gradients of the networks error with respect to the parameters. Fortunately, there is an
easy solution in the form of the backpropagation algorithm (Rumelhart, Hinton, & Williams,
1986; LeCun, Bottou, Bengio, & Haffner, 1998b). The backpropagation algorithm is a fancy
name for methodically computing the derivatives of a complex expression using the chainrule, while caching intermediary results. More generally, the backpropagation algorithm
is a special case of the reverse-mode automatic differentiation algorithm (Neidinger, 2010,
Section 7; Baydin, Pearlmutter, Radul, & Siskind, 2015; Bengio, 2012). The following
section describes reverse mode automatic differentiation in the context of the computation
graph abstraction.
6.1.1 Beyond SGD
While the SGD algorithm can and often does produce good results, more advanced algorithms are also available. The SGD+Momentum (Polyak, 1964) and Nesterov Momentum
(Sutskever, Martens, Dahl, & Hinton, 2013; Nesterov, 1983, 2004) algorithms are variants
of SGD in which previous gradients are accumulated and affect the current update. Adap54. Recent work from the neural-networks literature argue that the non-convexity of the networks is manifested in a proliferation of saddle points rather than local minima (Dauphin, Pascanu, Gulcehre, Cho,
Ganguli, & Bengio, 2014). This may explain some of the success in training neural networks despite
using local search techniques.

371

fiGoldberg

tive learning rate algorithms including AdaGrad (Duchi, Hazan, & Singer, 2011), AdaDelta
(Zeiler, 2012), RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2014) are
designed to select the learning rate for each minibatch, sometimes on a per-coordinate basis,
potentially alleviating the need of fiddling with learning rate scheduling. For details of these
algorithms, see the original papers or the book by Bengio et al. (2015, Sections 8.3, 8.4).
As many neural-network software frameworks provide implementations of these algorithms,
it is easy and sometimes worthwhile to try out different variants.
6.2 The Computation Graph Abstraction
While one can compute the gradients of the various parameters of a network by hand and
implement them in code, this procedure is cumbersome and error prone. For most purposes, it is preferable to use automatic tools for gradient computation (Bengio, 2012). The
computation-graph abstraction allows us to easily construct arbitrary networks, evaluate
their predictions for given inputs (forward pass), and compute gradients for their parameters
with respect to arbitrary scalar losses (backward pass).
A computation graph is a representation of an arbitrary mathematical computation as
a graph. It is a directed acyclic graph (DAG) in which nodes correspond to mathematical
operations or (bound) variables and edges correspond to the flow of intermediary values
between the nodes. The graph structure defines the order of the computation in terms of
the dependencies between the different components. The graph is a DAG and not a tree, as
the result of one operation can be the input of several continuations. Consider for example
a graph for the computation of (a  b + 1)  (a  b + 2):
*
+

+
*

1

a

b

2

The computation of a  b is shared. We restrict ourselves to the case where the computation
graph is connected.
Since a neural network is essentially a mathematical expression, it can be represented
as a computation graph.
For example, Figure 3a presents the computation graph for an MLP with one hiddenlayer and a softmax output transformation. In our notation, oval nodes represent mathematical operations or functions, and shaded rectangle nodes represent parameters (bound
variables). Network inputs are treated as constants, and drawn without a surrounding node.
Input and parameter nodes have no incoming arcs, and output nodes have no outgoing arcs.
The output of each node is a matrix, the dimensionality of which is indicated above the
node.
This graph is incomplete: without specifying the inputs, we cannot compute an output.
Figure 3b shows a complete graph for an MLP that takes three words as inputs, and predicts
the distribution over part-of-speech tags for the third word. This graph can be used for
prediction, but not for training, as the output is a vector (not a scalar) and the graph does
not take into account the correct answer or the loss term. Finally, the graph in 3c shows the
372

fiA Primer on Neural Networks for NLP

11
neg
11
log
11
(a)

(b)

(c)

pick

1  17

1  17

1  17

softmax

softmax

softmax

1  17

1  17

1  17

ADD

ADD

ADD

1  17

1  17

1  17

MUL

MUL

MUL

1  20
tanh

20  17
W2

1  20

1  17
b2

20  17
W2

tanh

1  20

1  17
b2

1  20

1  20

ADD

ADD

ADD

1  20

1  20

1  20

MUL

MUL

MUL

150  20
W1

1  150

1  20
b1

concat

150  20
W1

20  17

1  17

150  20

1  20

W2

tanh

1  20

1  150
x

5

1  150

1  20
b1

concat

W1

1  50

1  50

1  50

1  50

1  50

1  50

lookup

lookup

lookup

lookup

lookup

lookup

the

black

dog

the

black

dog

|V |  50
E

b2

b1

|V |  50
E

Figure 3: Computation Graph for MLP1. (a) Graph with unbound input. (b) Graph
with concrete input. (c) Graph with concrete input, expected output, and loss
node.

computation graph for a specific training example, in which the inputs are the (embeddings
of) the words the, black, dog, and the expected output is NOUN (whose index is
5). The pick node implements an indexing operation, receiving a vector and an index (in
this case, 5) and returning the corresponding entry in the vector.
Once the graph is built, it is straightforward to run either a forward computation (compute the result of the computation) or a backward computation (computing the gradients),
as we show below. Constructing the graphs may look daunting, but is actually very easy
using dedicated software libraries and APIs.
6.2.1 Forward Computation
The forward pass computes the outputs of the nodes in the graph. Since each nodes output
depends only on itself and on its incoming edges, it is trivial to compute the outputs of
all nodes by traversing the nodes in a topological order and computing the output of each
node given the already computed outputs of its predecessors.
373

fiGoldberg

More formally, in a graph of N nodes, we associate each node with an index i according
to their topological ordering. Let fi be the function computed by node i (e.g. multiplication.
addition, . . . ). Let (i) be the parent nodes of node i, and  1 (i) = {j | i  (j)} the
children nodes of node i (these are the arguments of fi ). Denote by v(i) the output of node
i, that is, the application of fi to the output values of its arguments  1 (i). For variable
and input nodes, fi is a constant function and  1 (i) is empty. The Forward algorithm
computes the values v(i) for all i  [1, N ].
Algorithm 3 Computation Graph Forward Pass
1: for i = 1 to N do
2:
Let a1 , . . . , am =  1 (i)
3:
v(i)  fi (v(a1 ), . . . , v(am ))

6.2.2 Backward Computation (Derivatives, Backprop)
The backward pass begins by designating a node N with scalar (11) output as a loss-node,
and running forward computation up to that node. The backward computation computes
N
the gradients with respect to that nodes value. Denote by d(i) the quantity
. The
i
backpropagation algorithm is used to compute the values d(i) for all nodes i.
The backward pass fills a table d(i) as follows:
Algorithm 4 Computation Graph Backward Pass (Backpropagation)
1: d(N )  1
2: for i = N-1 to 1 do
P
fj
3:
d(i)  j(i) d(j) 
i

fj
is the partial derivative of fj ( 1 (j)) w.r.t the argument i   1 (j).
i
This value depends on the function fj and the values v(a1 ), . . . , v(am ) (where a1 , . . . , am =
 1 (j)) of its arguments, which were computed in the forward pass.

The quantity

Thus, in order to define a new kind of node, one need to define two methods: one for
calculating the forward value v(i) based on the nodes inputs, and the another for calculating
fi
for each x   1 (i).
x
For further information on automatic differentiation see the work of Neidinger (2010,
Section 7) and Baydin et al. (2015). For more in depth discussion of the backpropagation
algorithm and computation graphs (also called flow graphs) see the work of Bengio et al.
(2015, Section 6.4), LeCun et al. (1998b) and Bengio (2012). For a popular yet technical
presentation, see the online post by Olah (2015a).
374

fiA Primer on Neural Networks for NLP

6.2.3 Software
Several software packages implement the computation-graph model, including Theano55 ,
Chainer56 , penne57 and CNN/pyCNN58 . All these packages support all the essential components (node types) for defining a wide range of neural network architectures, covering the
structures described in this tutorial and more. Graph creation is made almost transparent
by use of operator overloading. The framework defines a type for representing graph nodes
(commonly called expressions), methods for constructing nodes for inputs and parameters,
and a set of functions and mathematical operations that take expressions as input and result
in more complex expressions. For example, the python code for creating the computation
graph from Figure (3c) using the pyCNN framework is:
import pycnn as pc
# model initialization.
model = pc.Model()
pW1 = model.add_parameters((20,150))
pb1 = model.add_parameters(20)
pW2 = model.add_parameters((17,20))
pb2 = model.add_parameters(17)
words = model.add_lookup_parameters((100, 50))
# Building the computation graph:
pc.renew_cg() # create a new graph.
# Wrap the model parameters as graph-nodes.
W1 = pc.parameter(pW1)
b1 = pc.parameter(pb1)
W2 = pc.parameter(pW2)
b2 = pc.parameter(pb2)
def get_index(x): return 1 # place holder
# Generate the embeddings layer.
vthe
= pc.lookup(words, get_index("the"))
vblack = pc.lookup(words, get_index("black"))
vdog
= pc.lookup(words, get_index("dog"))
# Connect the leaf nodes into a complete graph.
x = pc.concatenate([vthe, vblack, vdog])
output = pc.softmax(W2*(pc.tanh(W1*x)+b1)+b2)
loss = -pc.log(pc.pick(output, 5))
loss_value = loss.forward()
loss.backward() # the gradient is computed
# and stored in the corresponding
# parameters.

Most of the code involves various initializations: the first block defines model parameters
that are be shared between different computation graphs (recall that each graph corresponds
to a specific training example). The second block turns the model parameters into the graphnode (Expression) types. The third block retrieves the Expressions for the embeddings of the
55.
56.
57.
58.

http://deeplearning.net/software/theano/
http://chainer.org
https://bitbucket.org/ndnlp/penne
https://github.com/clab/cnn

375

fiGoldberg

input words. Finally, the fourth block is where the graph is created. Note how transparent
the graph creation is  there is an almost a one-to-one correspondence between creating
the graph and describing it mathematically. The last block shows a forward and backward
pass. The other software frameworks follow similar patterns.
Theano involves an optimizing compiler for computation graphs, which is both a blessing
and a curse. On the one hand, once compiled, large graphs can be run efficiently on either
the CPU or a GPU, making it ideal for large graphs with a fixed structure, where only the
inputs change between instances. However, the compilation step itself can be costly, and it
makes the interface a bit cumbersome to work with. In contrast, the other packages focus on
building large and dynamic computation graphs and executing them on the fly without a
compilation step. While the execution speed may suffer with respect to Theanos optimized
version, these packages are especially convenient when working with the recurrent and
recursive networks described in Sections 10, 12 as well as in structured prediction settings
as described in Section 8.
6.2.4 Implementation Recipe
Using the computation graph abstraction, the pseudo-code for a network training algorithm
is given in Algorithm 5.
Algorithm 5 Neural Network Training with Computation Graph Abstraction (using minibatches of size 1)
1: Define network parameters.
2: for iteration = 1 to N do
3:
for Training example xi , yi in dataset do
4:
loss node  build computation graph(xi , yi , parameters)
5:
loss node.forward()
6:
gradients  loss node().backward()
7:
parameters  update parameters(parameters, gradients)
8: return parameters.
Here, build computation graph is a user-defined function that builds the computation
graph for the given input, output and network structure, returning a single loss node.
update parameters is an optimizer specific update rule. The recipe specifies that a new
graph is created for each training example. This accommodates cases in which the network
structure varies between training example, such as recurrent and recursive neural networks,
to be discussed in Sections 10  12. For networks with fixed structures, such as an MLPs, it
may be more efficient to create one base computation graph and vary only the inputs and
expected outputs between examples.
6.2.5 Network Composition
As long as the networks output is a vector (1  k matrix), it is trivial to compose networks
by making the output of one network the input of another, creating arbitrary networks.
The computation graph abstractions makes this ability explicit: a node in the computation
graph can itself be a computation graph with a designated output node. One can then
376

fiA Primer on Neural Networks for NLP

design arbitrarily deep and complex networks, and be able to easily evaluate and train
them thanks to automatic forward and gradient computation. This makes it easy to define
and train networks for structured outputs and multi-objective training, as we discuss in
Section 7, as well as complex recurrent and recursive networks, as discussed in Sections
1012.
6.3 Optimization Issues
Once the gradient computation is taken care of, the network is trained using SGD or another
gradient-based optimization algorithm. The function being optimized is not convex, and for
a long time training of neural networks was considered a black art which can only be done
by selected few. Indeed, many parameters affect the optimization process, and care has to
be taken to tune these parameters. While this tutorial is not intended as a comprehensive
guide to successfully training neural networks, we do list here a few of the prominent issues.
For further discussion on optimization techniques and algorithms for neural networks, refer
to the book by Bengio et al. (2015, ch. 8). For some theoretical discussion and analysis, refer
to the work of Glorot and Bengio (2010). For various practical tips and recommendations,
see work of LeCun et al. (1998a) and Bottou (2012).
6.3.1 Initialization
The non-convexity of the loss function means the optimization procedure may get stuck
in a local minimum or a saddle point, and that starting from different initial points (e.g.
different random values for the parameters) may result in different results. Thus, it is
advised to run several restarts of the training starting at different random initializations,
and choosing the best one based on a development set.59 The amount of variance in the
results is different for different network formulations and datasets, and cannot be predicted
in advance.
The magnitude of the random values has an important effect on the success of training.
An effective scheme due to Glorot and Bengio (2010), called xavier initialization after
Glorots first name, suggests initializing a weight matrix W  Rdin dout as:


#

6
6
W  U 
, +
din + dout
din + dout
"

(24)

where U [a, b] is a uniformly sampled random value in the range [a, b]. The suggestion is
based on properties of the tanh activation function, works well on many occasions, and is
the preferred default initialization method by many.
Analysis by He et al. (2015) suggests that when using ReLU non-linearities, the weights
should be initialized
by sampling from a zero-mean Gaussian distribution whose standard
q
2
deviation is
din . This initialization was found by He et al to work better than xavier
initialization in an image classification task, especially when deep networks were involved.
59. When debugging, and for reproducibility of results, it is advised to used a fixed random seed.

377

fiGoldberg

6.3.2 Vanishing and Exploding Gradients
In deep networks, it is common for the error gradients to either vanish (become exceedingly
close to 0) or explode (become exceedingly high) as they propagate back through the computation graph. The problem becomes more severe in deeper networks, and especially so
in recursive and recurrent networks (Pascanu, Mikolov, & Bengio, 2012). Dealing with the
vanishing gradients problem is still an open research question. Solutions include making the
networks shallower, step-wise training (first train the first layers based on some auxiliary
output signal, then fix them and train the upper layers of the complete network based on
the real task signal), performing batch-normalization (Ioffe & Szegedy, 2015) (for every
minibatch, normalizing the inputs to each of the network layers to have zero mean and unit
variance) or using specialized architectures that are designed to assist in gradient flow (e.g.,
the LSTM and GRU architectures for recurrent networks, discussed in Section 11). Dealing
with the exploding gradients has a simple but very effective solution: clipping the gradients
if their norm exceeds a given threshold. Let g be the gradients of all parameters in the
network, and kgk be their L2 norm. Pascanu et al. (2012) suggest to set: g  threshold
kgk g if
kgk > threshold.
6.3.3 Saturation and Dead Neurons
Layers with tanh and sigmoid activations can become saturated  resulting in output values
for that layer that are all close to one, the upper-limit of the activation function. Saturated
neurons have very small gradients, and should be avoided. Layers with the ReLU activation
cannot be saturated, but can die  most or all values are negative and thus clipped at zero
for all inputs, resulting in a gradient of zero for that layer. If your network does not train
well, it is advisable to monitor the network for layers with many saturated or dead neurons.
Saturated neurons are caused by too large values entering the layer. This may be controlled
for by changing the initialization, scaling the range of the input values, or changing the
learning rate. Dead neurons are caused by all signals entering the layer being negative (for
example this can happen after a large gradient update). Reducing the learning rate will
help in this situation. For saturated layers, another option is to normalize the values in the
saturated layer after the activation, i.e. instead of g(h) = tanh(h) using g(h) = k tanh(h)
tanh(h)k .
Layer normalization is an effective measure for countering saturation, but is also expensive
in terms of gradient computation. A related technique is batch normalization, due to Ioffe
and Szegedy (2015), in which the activations at each layer are normalized so that they
have mean 0 and variance 1 across each mini-batch. The batch-normalization techniques
became a key component for effective training of deep networks in computer vision. As of
this writing, it is less popular in natural language applications.
6.3.4 Shuffling
The order in which the training examples are presented to the network is important. The
SGD formulation above specifies selecting a random example in each turn. In practice, most
implementations go over the training example in order. It is advised to shuffle the training
examples before each pass through the data.
378

fiA Primer on Neural Networks for NLP

6.3.5 Learning Rate
Selection of the learning rate is important. Too large learning rates will prevent the network
from converging on an effective solution. Too small learning rates will take very long time to
converge. As a rule of thumb, one should experiment with a range of initial learning rates in
range [0, 1], e.g. 0.001, 0.01, 0.1, 1. Monitor the networks loss over time, and decrease the
learning rate once the loss stops improving. Learning rate scheduling decreases the rate as a
function of the number of observed minibatches. A common schedule is dividing the initial
learning rate by the iteration number. Leon Bottou (2012) recommends using a learning
rate of the form t = 0 (1 + 0 t)1 where 0 is the initial learning rate, t is the learning
rate to use on the tth training example, and  is an additional hyperparameter. He further
recommends determining a good value of 0 based on a small sample of the data prior to
running on the entire dataset.
6.3.6 Minibatches
Parameter updates occur either every training example (minibatches of size 1) or every k
training examples. Some problems benefit from training with larger minibatch sizes. In
terms of the computation graph abstraction, one can create a computation graph for each
of the k training examples, and then connecting the k loss nodes under an averaging node,
whose output will be the loss of the minibatch. Large minibatched training can also be
beneficial in terms of computation efficiency on specialized computing architectures such as
GPUs, and replacing vector-matrix operations by matrix-matrix operations. This is beyond
the scope of this tutorial.
6.4 Regularization
Neural network models have many parameters, and overfitting can easily occur. Overfitting
can be alleviated to some extent by regularization. A common regularization method is
L2 regularization, placing a squared penalty on parameters with large values by adding
an additive 2 kk2 term to the objective function to be minimized, where  is the set of
model parameters, k  k2 is the squared L2 norm (sum of squares of the values), and  is a
hyperparameter controlling the amount of regularization.
A recently proposed alternative regularization method is dropout (Hinton, Srivastava,
Krizhevsky, Sutskever, & Salakhutdinov, 2012). The dropout method is designed to prevent
the network from learning to rely on specific weights. It works by randomly dropping
(setting to 0) half of the neurons in the network (or in a specific layer) in each training
example. Work by Wager et al. (2013) establishes a strong connection between the dropout
method and L2 regularization.
The dropout technique is one of the key factors contributing to very strong results of
neural-network methods on image classification tasks (Krizhevsky, Sutskever, & Hinton,
2012), especially when combined with ReLU activation units (Dahl, Sainath, & Hinton,
2013). The dropout technique is effective also in NLP applications of neural networks.
379

fiGoldberg

7. Cascading and Multi-task Learning
The combination of online training methods with automatic gradient computations using
the computation graph abstraction allows for an easy implementation of model cascading,
parameter sharing and multi-task learning.
7.1 Model Cascading
is a powerful technique in which large networks are built by composing them out of smaller
component networks. For example, we may have a feed-forward network for predicting the
part of speech of a word based on its neighbouring words and/or the characters that compose
it. In a pipeline approach, we would use this network for predicting parts of speech, and
then feed the predictions as input features to neural network that does syntactic chunking
or parsing. Instead, we could think of the hidden layers of this network as an encoding
that captures the relevant information for predicting the part of speech. In a cascading
approach, we take the hidden layers of this network and connect them (and not the part
of speech prediction themselves) as the inputs for the syntactic network. We now have
a larger network that takes as input sequences of words and characters, and outputs a
syntactic structure. The computation graph abstraction allows us to easily propagate the
error gradients from the syntactic task loss all the way back to the characters.
To combat the vanishing gradient problem of deep networks, as well as to make better
use of available training material, the individual component networks parameters can be
bootstrapped by training them separately on a relevant task, before plugging them in to
the larger network for further tuning. For example, the part-of-speech predicting network
can be trained to accurately predict parts-of-speech on a relatively large annotated corpus,
before plugging its hidden layer into the syntactic parsing network for which less training
data is available. In case the training data provide direct supervision for both tasks, we can
make use of it during training by creating a network with two outputs, one for each task,
computing a separate loss for each output, and then summing the losses into a single node
from which we backpropagate the error gradients.
Model cascading is very common when using convolutional, recursive and recurrent
neural networks, where, for example, a recurrent network is used to encode a sentence into
a fixed sized vector, which is then used as the input of another network. The supervision
signal of the recurrent network comes primarily from the upper network that consumes the
recurrent networks output as it inputs.
7.2 Multi-task Learning
is used when we have related prediction tasks that do not necessarily feed into one another,
but we do believe that information that is useful for one type of prediction can be useful
also to some of the other tasks. For example, chunking, named entity recognition (NER)
and language modeling are examples of synergistic tasks. Information for predicting chunk
boundaries, named-entity boundaries and the next word in the sentence all rely on some
shared underlying syntactic-semantic representation. Instead of training a separate network
for each task, we can create a single network with several outputs. A common approach is to
have a multi-layer feed-forward network, whose final hidden layer (or a concatenation of all
380

fiA Primer on Neural Networks for NLP

hidden layers) is then passed to different output layers. This way, most of the parameters
of the network are shared between the different tasks. Useful information learned from one
task can then help to disambiguate other tasks. Again, the computation graph abstraction
makes it very easy to construct such networks and compute the gradients for them, by
computing a separate loss for each available supervision signal, and then summing the
losses into a single loss that is used for computing the gradients. In case we have several
corpora, each with different kind of supervision signal (e.g. we have one corpus for NER
and another for chunking), the training procedure will shuffle all of the available training
example, performing gradient computation and updates with respect to a different loss in
every turn. Multi-task learning in the context of language-processing is introduced and
discussed in the work of Collobert et al. (2011). For examples of cascaded Multi-task
learning in a feed forward network, see the work of Zhang and Weiss (2016). In the context
of recurrent neural networks, see the work of Luong, Le, Sutskever, Vinyals, and Kaiser
(2015) and Sgaard and Goldberg (2016).

8. Structured Output Prediction
Many problems in NLP involve structured outputs: cases where the desired output is not
a class label or distribution over class labels, but a structured object such as a sequence,
a tree or a graph. Canonical examples are sequence tagging (e.g. part-of-speech tagging)
sequence segmentation (chunking, NER), and syntactic parsing. In this section, we discuss
how feed-forward neural network models can be used for structured tasks. In later sections
we discuss specialized neural network models for dealing with sequences (Section 10) and
trees (Section 12).
8.1 Greedy Structured Prediction
The greedy approach to structured prediction is to decompose the structure prediction
problem into a sequence of local prediction problems and training a classifier to perform
each local decision. At test time, the trained classifier is used in a greedy manner. Examples
of this approach are left-to-right tagging models (Gimenez & Marquez, 2004) and greedy
transition-based parsing (Nivre, 2008). Such approaches are easily adapted to use neural
networks by simply replacing the local classifier from a linear classifier such as an SVM or
a logistic regression model to a neural network, as demonstrated by Chen and Manning
(2014) and Lewis and Steedman (2014).
The greedy approaches suffer from error propagation, where mistakes in early decisions
carry over and influence later decisions. The overall higher accuracy achievable with nonlinear neural network classifiers helps in offsetting this problem to some extent. In addition,
training techniques were proposed for mitigating the error propagation problem by either
attempting to take easier predictions before harder ones (the easy-first approach in Goldberg & Elhadad, 2010) or making training conditions more similar to testing conditions by
exposing the training procedure to inputs that result from likely mistakes (Hal Daume III,
Langford, & Marcu, 2009; Goldberg & Nivre, 2013). These are effective also for training
greedy neural network models, as demonstrated by Ma, Zhang, and Zhu (2014) (easy-first
tagger) and Ballesteros, Goldberg, Dyer, and Smith (2016) (dynamic oracle training for
greedy dependency parsing).
381

fiGoldberg

8.2 Search Based Structured Prediction
The common approach to predicting natural language structures is search based. For indepth discussion of search-based structure prediction in NLP, see the book by Smith (2011).
The techniques can easily be adapted to use a neural-network. In the neural-networks
literature, such models were discussed under the framework of energy based learning (LeCun
et al., 2006, Section 7). They are presented here using setup and terminology familiar to
the NLP community.
Search-based structured prediction is formulated as a search problem over possible structures:
predict(x) = arg max score(x, y)

(25)

yY(x)

where x is an input structure, y is an output over x (in a typical example x is a sentence
and y is a tag-assignment or a parse-tree over the sentence), Y(x) is the set of all valid
structures over x, and we are looking for an output y that will maximize the score of the
x, y pair.
The scoring function is defined as a linear model:
score(x, y) = w  (x, y)

(26)

where  is a feature extraction function and w is a weight vector.
In order to make the search for the optimal y tractable, the structure y is decomposed
into parts, and the feature function is defined in terms of the parts, where (p) is a part-local
feature extraction function:
X
(x, y) =
(p)
(27)
pparts(x,y)

Each part is scored separately, and the structure score is the sum of the component
parts scores:

score(x, y) =w  (x, y) = w 

X

(p) =

py

X
py

w  (p) =

X

score(p)

(28)

py

where p  y is a shorthand for p  parts(x, y). The decomposition of y into parts is such
that there exists an inference algorithm that allows for efficient search for the best scoring
structure given the scores of the individual parts.
One can now trivially replace the linear scoring function over parts with a neuralnetwork:

score(x, y) =

X

score(p) =

py

X

NN(c(p))

py

where c(p) maps the part p into a din dimensional vector.
In case of a one hidden-layer feed-forward network:
382

(29)

fiA Primer on Neural Networks for NLP

score(x, y) =

X

NNMLP1 (c(p)) =

X
(g(c(p)W1 + b1 ))w

(30)

py

py

c(p)  Rdin , W1  Rdin d1 , b1  Rd1 , w  Rd1 . A common objective in structured
prediction is making the gold structure y score higher than any other structure y 0 , leading
to the following (generalized perceptron) loss:

max
score(x, y 0 )  score(x, y)
0
y

(31)

In terms of implementation, this means: create a computation graph CGp for each of
the possible parts, and calculate its score. Then, run inference over the scored parts to
find the best scoring structure y 0 . Connect the output nodes of the computation graphs
corresponding to parts in the gold (predicted) structure y (y 0 ) into a summing node CGy
(CG0y ). Connect CGy and CG0y using a minus node, CGl , and compute the gradients.
As argued by LeCun et al. (2006, Section 5), the generalized perceptron loss may not
be a good loss function when training structured prediction neural networks as it does not
have a margin, and a margin-based hinge loss is preferred:

max(0, m + max
score(x, y 0 )  score(x, y))
0
y 6=y

(32)

It is trivial to modify the implementation above to work with the hinge loss.
Note that in both cases we lose the nice properties of the linear model. In particular, the
model is no longer convex. This is to be expected, as even the simplest non-linear neural
network is already non-convex. Nonetheless, we could still use standard neural-network
optimization techniques to train the structured model.
Training and inference is slower, as we have to evaluate the neural network (and take
gradients) |parts(x, y)| times.
Structured prediction is a vast field and is beyond the scope of this tutorial, but loss
functions, regularizers and methods described by, e.g., Smith (2011), such as cost-augmented
decoding, can be easily applied or adapted to the neural-network framework.60
8.2.1 Probabilistic Objective (CRF)
In a probabilistic framework (conditional random fields, CRF), we treat each of the parts
scores as a clique potential (see discussions in Smith, 2011 and Lafferty, McCallum, &
Pereira, 2001) and define the score of each structure y to be:
60. One should keep in mind that the resulting objectives are no longer convex, and so lack the formal guarantees and bounds associated with convex optimization problems. Similarly, the theory, learning bounds
and guarantees associated with the algorithms do not automatically transfer to the neural versions.

383

fiGoldberg

P
exp( py score(p))
P
scorecrf (x, y) = P (y|x) = P
y 0 Y(x) exp( py 0 score(p))
P
exp( py NN((p)))
P
=P
y 0 Y(x) exp( py 0 NN((p)))

(33)

The scoring function defines a conditional distribution P (y|x), and
P we wish to set the parameters of the network such that corpus conditional log likelihood (xi ,yi )training log P (yi |xi )
is maximized.
The loss for a given training example (x, y) is then:  log scorecrf (x, y). Taking the
gradient with respect to the loss is as involved as building the associated computation
graph. The tricky part is the denominator (the partition function) which requires summing
over the potentially exponentially many structures in Y. However, for some problems, a
dynamic programming algorithm exists for efficiently solving the summation in polynomial
time (i.e. the forward-backward viterbi recurrences for sequences and the CKY insideoutside recurrences for tree structures). When such an algorithm exists, it can be adapted
to also create a polynomial-size computation graph.
When an efficient enough algorithm for computing the partition function is not available,
approximate methods can be used. For example, one may use beam search for inference,
and for the partition function sum over the structures remaining in the beam instead of
over the exponentially large Y(x).
Sequence-level CRFs with neural-network clique potentials are discussed by Peng, Bo,
and Xu (2009) and Do, Arti, and others (2010), where they are applied to sequence labeling
of biological data, OCR data and speech signals, and by Wang and Manning (2013) who
apply them on traditional natural language tagging tasks (chunking and NER). A hinge
based approach was used by Pei et al. (2015) for arc-factored dependency parsing, and the
probabilistic approach by Durrett and Klein (2015) for a CRF constituency parser. The
approximate beam-based partition function was effectively used by Zhou et al. (2015) in a
transition based parser.
8.2.2 Reranking
When searching over all possible structures is intractable, inefficient or hard to integrate
into a model, reranking methods are often used. In the reranking framework (Charniak
& Johnson, 2005; Collins & Koo, 2005) a base model is used to produce a list of the kbest scoring structures. A more complex model is then trained to score the candidates in
the k-best list such that the best structure with respect to the gold one is scored highest.
As the search is now performed over k items rather than over an exponential space, the
complex model can condition on (extract features from) arbitrary aspects of the scored
structure. Reranking methods are natural candidates for structured prediction using neuralnetwork models, as they allow the modeler to focus on the feature extraction and network
structure, while removing the need to integrate the neural network scoring into a decoder.
Indeed, reranking methods are often used for experimenting with neural models that are not
straightforward to integrate into a decoder, such as convolutional, recurrent and recursive
networks, which will be discussed in later sections. Works using the reranking approach
384

fiA Primer on Neural Networks for NLP

include those of Schwenk et al. (2006), Socher et al. (2013), Auli et al. (2013), Le and
Zuidema (2014) and Zhu et al. (2015a).
8.2.3 MEMM and Hybrid Approaches
Other formulations are, of course, also possible. For example, an MEMM (McCallum,
Freitag, & Pereira, 2000) can be trivially adapted to the neural network world by replacing
the logistic regression (Maximum Entropy) component with an MLP.
Hybrid approaches between neural networks and linear models are also explored. In
particular, Weiss et al. (2015) report strong results for transition-based dependency parsing
in a two-stage model. In the first stage, a static feed-forward neural network (MLP2)
is trained to perform well on each of the individual decisions of the structured problem in
isolation. In the second stage, the neural network model is held fixed, and the different layers
(output as well as hidden layer vectors) for each input are then concatenated and used as
the input features of a linear structured perceptron model (Collins, 2002) that is trained to
perform beam-search for the best resulting structure. While it is not clear that such training
regime is more effective than training a single structured-prediction neural network, the use
of two simpler, isolated models allowed the researchers to perform a much more extensive
hyper-parameter search (e.g. tuning layer sizes, activation functions, learning rates and so
on) for each model than is feasible with more complicated networks.

9. Convolutional Layers
Sometimes we are interested in making predictions based on ordered sets of items (e.g.
the sequence of words in a sentence, the sequence of sentences in a document and so on).
Consider for example predicting the sentiment (positive, negative or neutral) of a sentence.
Some of the sentence words are very informative of the sentiment, other words are less
informative, and to a good approximation, an informative clue is informative regardless
of its position in the sentence. We would like to feed all of the sentence words into a
learner, and let the training process figure out the important clues. One possible solution is
feeding a CBOW representation into a fully connected network such as an MLP. However,
a downside of the CBOW approach is that it ignores the ordering information completely,
assigning the sentences it was not good, it was actually quite bad and it was not bad,
it was actually quite good the exact same representation. While the global position of
the indicators not good and not bad does not matter for the classification task, the
local ordering of the words (that the word not appears right before the word bad) is
very important. A naive approach would suggest embedding word-pairs (bi-grams) rather
than words, and building a CBOW over the embedded bigrams. While such an architecture
could be effective, it will result in huge embedding matrices, will not scale for longer ngrams, and will suffer from data sparsity problems as it does not share statistical strength
between different n-grams (the embedding of quite good and very good are completely
independent of one another, so if the learner saw only one of them during training, it
will not be able to deduce anything about the other based on its component words). The
convolution-and-pooling (also called convolutional neural networks, or CNNs) architecture
is an elegant and robust solution to this modeling problem. A convolutional neural network
is designed to identify indicative local predictors in a large structure, and combine them
385

fiGoldberg

to produce a fixed size vector representation of the structure, capturing these local aspects
that are most informative for the prediction task at hand.
Convolution-and-pooling architectures (LeCun & Bengio, 1995) evolved in the neural
networks vision community, where they showed great success as object detectors  recognizing an object from a predefined category (cat, bicycles) regardless of its position
in the image (Krizhevsky et al., 2012). When applied to images, the architecture is using
2-dimensional (grid) convolutions. When applied to text, we are mainly concerned with
1-d (sequence) convolutions. Convolutional networks were introduced to the NLP community in the pioneering work of Collobert, Weston and colleagues (2011) who used them for
semantic-role labeling, and later by Kalchbrenner et al. (2014) and Kim (2014) who used
them for sentiment and question-type classification.
9.1 Basic Convolution + Pooling
The main idea behind a convolution and pooling architecture for language tasks is to apply
a non-linear (learned) function over each instantiation of a k-word sliding window over
the sentence. This function (also called filter) transforms a window of k words into a d
dimensional vector that captures important properties of the words in the window (each
dimension is sometimes referred to in the literature as a channel). Then, a pooling
operation is used to combine the vectors resulting from the different windows into a single
d-dimensional vector, by taking the max or the average value observed in each of the d
channels over the different windows. The intention is to focus on the most important
features in the sentence, regardless of their location. The d-dimensional vector is then
fed further into a network that is used for prediction. The gradients that are propagated
back from the networks loss during the training process are used to tune the parameters
of the filter function to highlight the aspects of the data that are important for the task
the network is trained for. Intuitively, when the sliding window is run over a sequence, the
filter function learns to identify informative k-grams.
More formally, consider a sequence of words x = x1 , . . . , xn , each with their corresponding demb dimensional word embedding v(xi ). A 1d convolution layer61 of width k works
by moving a sliding window of size k over the sentence, and applying the same filter to
each window in the sequence [v(xi ); v(xi+1 ); . . . ; v(xi+k1 )]. The filter function is usually a
linear transformation followed by a non-linear activation function.
Let the concatenated vector of the ith window be wi = [v(xi ); v(xi+1 ); . . . ; v(xi+k1 )],
wi  Rkdemb . Depending on whether we pad the sentence with k  1 words to each side,
we may get either m = n  k + 1 (narrow convolution) or m = n + k + 1 windows (wide
convolution) (Kalchbrenner et al., 2014). The result of the convolution layer is m vectors
p1 , . . . , pm , pi  Rdconv where:
pi = g(wi W + b)

(34)

g is a non-linear activation function that is applied element-wise, W  Rkdemb dconv and
b  Rdconv are parameters of the network. Each pi is a dconv dimensional vector, encoding
61. 1d here refers to a convolution operating over 1-dimensional inputs such as sequences, as opposed to 2d
convolutions which are applied to images.

386

fiA Primer on Neural Networks for NLP

63
W

max

the quick brown fox jumped over the lazy dog
the quick brown

MUL+tanh

quick brown fox

MUL+tanh

brown fox jumped

MUL+tanh

fox jumped over

MUL+tanh

jumped over the

MUL+tanh

over the lazy

MUL+tanh

the lazy dog

MUL+tanh

convolution

pooling

Figure 4: 1d convolution+pooling over the sentence the quick brown fox jumped over the
lazy dog. This is a narrow convolution (no padding is added to the sentence)
with a window size of 3. Each word is translated to a 2-dim embedding vector
(not shown). The embedding vectors are then concatenated, resulting in 6-dim
window representations. Each of the seven windows is transfered through a 6  3
filter (linear transformation followed by element-wise tanh), resulting in seven
3-dimensional filtered representations. Then, a max-pooling operation is applied,
taking the max over each dimension, resulting in a final 3-dimensional pooled
vector.

the information in wi . Ideally, each dimension captures a different kind of indicative information. The m vectors are then combined using a max pooling layer, resulting in a single
dconv dimensional vector c.
cj = max pi [j]
1<im

(35)

pi [j] denotes the jth component of pi . The effect of the max-pooling operation is to get the
most salient information across window positions. Ideally, each dimension will specialize
in a particular sort of predictors, and max operation will pick on the most important
predictor of each type.
Figure 4 provides an illustration of the process.
The resulting vector c is a representation of the sentence in which each dimension
reflects the most salient information with respect to some prediction task. c is then fed
into a downstream network layers, perhaps in parallel to other vectors, culminating in an
output layer which is used for prediction. The training procedure of the network calculates
the loss with respect to the prediction task, and the error gradients are propagated all the
way back through the pooling and convolution layers, as well as the embedding layers. 62
62. Besides being useful for prediction, a by-product of the training procedure is a set of parameters W, B
and embeddings v() that can be used in a convolution and pooling architecture to encode arbitrary length

387

fiGoldberg

While max-pooling is the most common pooling operation in text applications, other
pooling operations are also possible, the second most common operation being average
pooling, taking the average value of each index instead of the max.
9.2 Dynamic, Hierarchical and k-max Pooling
Rather than performing a single pooling operation over the entire sequence, we may want
to retain some positional information based on our domain understanding of the prediction
problem at hand. To this end, we can split the vectors pi into ` distinct groups, apply the
pooling separately on each group, and then concatenate the ` resulting dconv -dimensional
vectors c1 , . . . , c` . The division of the pi s into groups is performed based on domain knowledge. For example, we may conjecture that words appearing early in the sentence are
more indicative than words appearing late. We can then split the sequence into ` equally
sized regions, applying a separate max-pooling to each region. For example, Johnson and
Zhang (2015) found that when classifying documents into topics, it is useful to have 20
average-pooling regions, clearly separating the initial sentences (where the topic is usually
introduced) from later ones, while for a sentiment classification task a single max-pooling
operation over the entire sentence was optimal (suggesting that one or two very strong
signals are enough to determine the sentiment, regardless of the position in the sentence).
Similarly, in a relation extraction kind of task we may be given two words and asked to
determine the relation between them. We could argue that the words before the first word,
the words after the second word, and the words between them provide three different kinds
of information (Chen et al., 2015). We can thus split the pi vectors accordingly, pooling
separately the windows resulting from each group.
Another variation is using a hierarchy of convolutional layers, in which we have a succession of convolution and pooling layers, where each stage applies a convolution to a sequence,
pools every k neighboring vectors, performs a convolution on the resulting pooled sequence,
applies another convolution and so on. This architecture allows sensitivity to increasingly
larger structures.
Finally, Kalchbrenner et al. (2014) introduced a k-max pooling operation, in which the
top k values in each dimension are retained instead of only the best one, while preserving
the order in which they appeared in the text. For example a, consider the following matrix:

1
9

2

7
3

2
6
3
8
4


3
5

1

1
1



A 1-max pooling over the column vectors will result in 9 8 5 , while a 2-max pooling


9 6 3
will result in the following matrix:
whose rows will then be concatenated to
7 8 5


9 6 3 7 8 5
sentences into fixed-size vectors, such that sentences that share the same kind of predictive information
will be close to each other.

388

fiA Primer on Neural Networks for NLP

The k-max pooling operation makes it possible to pool the k most active indicators that
may be a number of positions apart; it preserves the order of the features, but is insensitive
to their specific positions. It can also discern more finely the number of times the feature
is highly activated (Kalchbrenner et al., 2014).
9.3 Variations
Rather than a single convolutional layer, several convolutional layers may be applied in
parallel. For example, we may have four different convolutional layers, each with a different
window size in the range 25, capturing n-gram sequences of varying lengths. The result
of each convolutional layer will then be pooled, and the resulting vectors concatenated and
fed to further processing (Kim, 2014).
The convolutional architecture need not be restricted into the linear ordering of a sentence. For example, Ma et al. (2015) generalize the convolution operation to work over
syntactic dependency trees. There, each window is around a node in the syntactic tree,
and the pooling is performed over the different nodes. Similarly, Liu et al. (2015) apply a
convolutional architecture on top of dependency paths extracted from dependency trees. Le
and Zuidema (2015) propose to perform max pooling over vectors representing the different
derivations leading to the same chart item in a chart parser.

10. Recurrent Neural Networks  Modeling Sequences and Stacks
When dealing with language data, it is very common to work with sequences, such as words
(sequences of letters), sentences (sequences of words) and documents. We saw how feedforward networks can accommodate arbitrary feature functions over sequences through the
use of vector concatenation and vector addition (CBOW). In particular, the CBOW representations allows to encode arbitrary length sequences as fixed sized vectors. However,
the CBOW representation is quite limited, and forces one to disregard the order of features. The convolutional networks also allow encoding a sequence into a fixed size vector.
While representations derived from convolutional networks are an improvement above the
CBOW representation as they offer some sensitivity to word order, their order sensitivity is
restricted to mostly local patterns, and disregards the order of patterns that are far apart
in the sequence.
Recurrent neural networks (RNNs) (Elman, 1990) allow representing arbitrarily sized
structured inputs in a fixed-size vector, while paying attention to the structured properties
of the input.
10.1 The RNN Abstraction
We use xi:j to denote the sequence of vectors xi , . . . , xj . The RNN abstraction takes as
input an ordered list of input vectors x1 , ..., xn together with an initial state vector s0 ,
and returns an ordered list of state vectors s1 , ..., sn , as well as an ordered list of output
vectors y1 , ..., yn . An output vector yi is a function of the corresponding state vector
si . The input vectors xi are presented to the RNN in a sequential fashion, and the state
vector si and output vector yi represent the state of the RNN after observing the inputs
x1:i . The output vector yi is then used for further prediction. For example, a model for
389

fiGoldberg

predicting the conditional probability of an event e given the sequence m1:i can be defined
as p(e = j|x1:i ) = softmax(yi W + b)[j], the jth element in the output vector resulting
from the softmax operation. The RNN model provides a framework for conditioning on the
entire history x1 , . . . , xi without resorting to the Markov assumption which is traditionally
used for modeling sequences.63 Indeed, RNN-based language models result in very good
perplexity scores when compared to n-gram based models.
Mathematically, we have a recursively defined function R that takes as input a state
vector si and an input vector xi+1 , and results in a new state vector si+1 . An additional
function O is used to map a state vector si to an output vector yi .64 When constructing an
RNN, much like when constructing a feed-forward network, one has to specify the dimension
of the inputs xi as well as the dimensions of the outputs yi . The dimensions of the states
si are a function of the output dimension.65
RNN(s0 , x1:n ) =s1:n , y1:n
si = R(si1 , xi )

(36)

yi = O(si )
xi  Rdin , yi  Rdout , si  Rf (dout )
The functions R and O are the same across the sequence positions, but the RNN keeps
track of the states of computation through the state vector that is kept and being passed
between invocations of R.
Graphically, the RNN has been traditionally presented as in Figure 5.
yi

si1

R,O



xi

si

Figure 5: Graphical representation of an RNN (recursive).
63. The kth-order Markov assumption states that the observation at time i is independent of the observations
at times i  (k + j) j > 0 given the observations at times i  1,    , i  k. This assumption is at the
basis for many sequence modeling technique such as n-gram models and hidden markov models.
64. Using the O function is somewhat non-standard, and is used in order to unify the different RNN models
to to be presented in the next section. For the Simple RNN (Elman RNN) and the GRU architectures,
O is the identity mapping, and for the LSTM architecture O selects a fixed subset of the state.
65. While RNN architectures in which the state dimension is independent of the output dimension are
possible, the current popular architectures, including the Simple RNN, the LSTM and the GRU do not
follow this flexibility.

390

fiA Primer on Neural Networks for NLP

This presentation follows the recursive definition, and is correct for arbitrary long sequences.
However, for a finite sized input sequence (and all input sequences we deal with are finite)
one can unroll the recursion, resulting in the structure in Figure 6.
y1

s0

R,O

x1

y3

y2

s1

R,O

s2

R,O

x2

y4

s3

x3

R,O

x4

y5

s4

R,O

s5

x5



Figure 6: Graphical representation of an RNN (unrolled).

While not usually shown in the visualization, we include here the parameters  in order
to highlight the fact that the same parameters are shared across all time steps. Different
instantiations of R and O will result in different network structures, and will exhibit different
properties in terms of their running times and their ability to be trained effectively using
gradient-based methods. However, they all adhere to the same abstract interface. We will
provide details of concrete instantiations of R and O  the Simple RNN, the LSTM and the
GRU  in Section 11. Before that, lets consider modeling with the RNN abstraction.
First, we note that the value of si is based on the entire input x1 , ..., xi . For example,
by expanding the recursion for i = 4 we get:

s4 =R(s3 , x4 )
s

z }|3 {
=R(R(s2 , x3 ), x4 )
s

z }|2 {
=R(R(R(s1 , x2 ), x3 ), x4 )

(37)

s

z }|1 {
=R(R(R(R(s0 , x1 ), x2 ), x3 ), x4 )
Thus, sn (as well as yn ) could be thought of as encoding the entire input sequence.66 Is
the encoding useful? This depends on our definition of usefulness. The job of the network
training is to set the parameters of R and O such that the state conveys useful information
for the task we are tying to solve.
66. Note that, unless R is specifically designed against this, it is likely that the later elements of the input
sequence have stronger effect on sn than earlier ones.

391

fiGoldberg

10.2 RNN Training
Viewed as in Figure 6 it is easy to see that an unrolled RNN is just a very deep neural
network (or rather, a very large computation graph with somewhat complex nodes), in
which the same parameters are shared across many parts of the computation. To train an
RNN network, then, all we need to do is to create the unrolled computation graph for a
given input sequence, add a loss node to the unrolled graph, and then use the backward
(backpropagation) algorithm to compute the gradients with respect to that loss. This
procedure is referred to in the RNN literature as backpropagation through time, or BPTT
(Werbos, 1990).67 There are various ways in which the supervision signal can be applied.
10.2.1 Acceptor
One option is to base the supervision signal only on the final output vector, yn . Viewed this
way, the RNN is an acceptor. We observe the final state, and then decide on an outcome.68
For example, consider training an RNN to read the characters of a word one by one and
then use the final state to predict the part-of-speech of that word (this is inspired by Ling
et al., 2015b), an RNN that reads in a sentence and, based on the final state decides if it
conveys positive or negative sentiment (this is inspired by Wang et al., 2015b) or an RNN
that reads in a sequence of words and decides whether it is a valid noun-phrase. The loss
in such cases is defined in terms of a function of yn = O(sn ), and the error gradients will
backpropagate through the rest of the sequence (see Figure 7).69 The loss can take any
familiar form  cross entropy, hinge, margin, etc.
10.2.2 Encoder
Similar to the acceptor case, an encoder supervision uses only the final output vector, yn .
However, unlike the acceptor, where a prediction is made solely on the basis of the final
vector, here the final vector is treated as an encoding of the information in the sequence, and
is used as additional information together with other signals. For example, an extractive
document summarization system may first run over the document with an RNN, resulting
67. Variants of the BPTT algorithm include unrolling the RNN only for a fixed number of input symbols at
each time: first unroll the RNN for inputs x1:k , resulting in s1:k . Compute a loss, and backpropagate
the error through the network (k steps back). Then, unroll the inputs xk+1:2k , this time using sk as the
initial state, and again backpropagate the error for k steps, and so on. This strategy is based on the
observations that for the Simple-RNN variant, the gradients after k steps tend to vanish (for large enough
k), and so omitting them is negligible. This procedure allows training of arbitrarily long sequences. For
RNN variants such as the LSTM or the GRU that are designed specifically to mitigate the vanishing
gradients problem, this fixed size unrolling is less motivated, yet it is still being used, for example when
doing language modeling over a book without breaking it into sentences. A similar variant unrolls the
network for the entire sequence in the forward step, but only propagates the gradients back for k steps
from each position.
68. The terminology is borrowed from Finite-State Acceptors. However, the RNN has a potentially infinite
number of states, making it necessary to rely on a function other than a lookup table for mapping states
to decisions.
69. This kind of supervision signal may be hard to train for long sequences, especially so with the SimpleRNN, because of the vanishing gradients problem. It is also a generally hard learning task, as we do not
tell the process on which parts of the input to focus.

392

fiA Primer on Neural Networks for NLP

loss
predict &
calc loss
y5
s0

R,O

s1

x1

R,O

s2

x2

R,O

s3

x3

R,O

s4

x4

R,O

x5

Figure 7: Acceptor RNN Training Graph.
in a vector yn summarizing the entire document. Then, yn will be used together with other
features in order to select the sentences to be included in the summarization.
10.2.3 Transducer
Another option is to treat the RNN as a transducer, producing an output for each input
it reads in. Modeled this way, we can compute a local loss signal Llocal (yi , yi ) for each
of the outputs yP
i based on a true label yi . The loss for unrolled sequence will then be:
L(y1:n
 , y1:n ) = ni=1 Llocal (yi , yi ), or using another combination rather than a sum such
as an average or a weighted average (see Figure 8). One example for such a transducer
is a sequence tagger, in which we take xi:n to be feature representations for the n words
of a sentence, and yi as an input for predicting the tag assignment of word i based on
words 1:i. A CCG super-tagger based on such an architecture provides state-of-the art
CCG super-tagging results (Xu et al., 2015).
loss

sum

predict &
calc loss

predict &
calc loss

y1
s0

R,O

x1

predict &
calc loss

y2
s1

R,O

x2

predict &
calc loss

y3
s2

R,O

x3

predict &
calc loss

y4
s3

R,O

x4

y5
s4

R,O

x5

Figure 8: Transducer RNN Training Graph.
A very natural use-case of the transduction setup is for language modeling, in which the
sequence of words x1:i is used to predict a distribution over the (i + 1)th word. RNN based
language models are shown to provide better perplexities than traditional language models
(Mikolov et al., 2010; Sundermeyer, Schluter, & Ney, 2012; Mikolov, 2012; Jozefowicz,
Vinyals, Schuster, Shazeer, & Wu, 2016).
Using RNNs as transducers allows us to relax the Markov assumption that is traditionally taken in language models and HMM taggers, and condition on the entire prediction
393

fiGoldberg

history. The power of the ability to condition on arbitrarily long histories is demonstrated
in generative character-level RNN models, in which a text is generated character by character, each character conditioning on the previous ones (Sutskever, Martens, & Hinton, 2011).
The generated texts show sensitivity to properties that are not captured by n-gram language
models, including line lengths and nested parenthesis balancing. For a good demonstration
and analysis of the properties of RNN-based character level language models, see the work
of Karpathy, Johnson, and Li (2015).
10.2.4 Encoder - Decoder
Finally, an important special case of the encoder scenario is the Encoder-Decoder framework
(Cho, van Merrienboer, Bahdanau, & Bengio, 2014a; Sutskever et al., 2014). The RNN is
used to encode the sequence into a vector representation yn , and this vector representation
is then used as auxiliary input to another RNN that is used as a decoder. For example,
in a machine-translation setup the first RNN encodes the source sentence into a vector
representation yn , and then this state vector is fed into a separate (decoder) RNN that
is trained to predict (using a transducer-like language modeling objective) the words of
the target language sentence based on the previously predicted words as well as yn . The
supervision happens only for the decoder RNN, but the gradients are propagated all the
way back to the encoder RNN (see Figure 9).
loss

sum

predict &
calc loss

predict &
calc loss

y1
sd0

RD ,OD

y2
sd1

RE ,OE

x1

se1

sd2

RD ,OD

x2

RE ,OE

x2

se2

predict &
calc loss

y3

RD ,OD

x1

se0

predict &
calc loss

y4
sd3

RD ,OD

x3

RE ,OE

x3

se3

predict &
calc loss
y5
sd4

RD ,OD

x4

RE ,OE

x4

se4

x5

RE ,OE

se5

x5

Figure 9: Encoder-Decoder RNN Training Graph.
Such an approach was shown to be surprisingly effective for Machine Translation (Sutskever
et al., 2014) using LSTM RNNs. In order for this technique to work, Sutskever et al. found
it effective to input the source sentence in reverse, such that xn corresponds to the first
394

fiA Primer on Neural Networks for NLP

word of the sentence. In this way, it is easier for the second RNN to establish the relation
between the first word of the source sentence to the first word of the target sentence.
Another use-case of the encoder-decoder framework is for sequence transduction. Here,
in order to generate tags t1 , . . . , tn , an encoder RNN is first used to encode the sentence
x1:n into fixed sized vector. This vector is then fed as the initial state vector of another
(transducer) RNN, which is used together with x1:n to predict the label ti at each position
i. This approach was used by Filippova, Alfonseca, Colmenares, Kaiser, and Vinyals (2015)
to model sentence compression by deletion.
10.3 Multi-layer (Stacked) RNNs
RNNs can be stacked in layers, forming a grid (Hihi & Bengio, 1996). Consider k RNNs,
j
RNN1 , . . . , RNNk , where the jth RNN has states sj1:n and outputs y1:n
. The input for the
first RNN are x1:n , while the input of the jth RNN (j  2) are the outputs of the RNN
j1
k .
below it, y1:n
. The output of the entire formation is the output of the last RNN, y1:n
Such layered architectures are often called deep RNNs. A visual representation of a 3-layer
RNN is given in Figure 10.
y1

y2

y13
s30

R3 ,O3

y23
s31

y12
s20

R2 ,O2

R1 ,O1

x1

R3 ,O3

R2 ,O2

R3 ,O3

R2 ,O2

R1 ,O1

x2

R1 ,O1

x3

R3 ,O3

y53
s34

y42
s23

y31
s12

y5

y43
s33

y32
s22

y21
s11

y4

y33
s32

y22
s21

y11
s10

y3

R2 ,O2

R1 ,O1

x4

s35

y52
s24

y41
s13

R3 ,O3

R2 ,O2

s25

y51
s14

R1 ,O1

s15

x5

Figure 10: A 3-layer (deep) RNN architecture.
While it is not theoretically clear what is the additional power gained by the deeper
architecture, it was observed empirically that deep RNNs work better than shallower ones
on some tasks. In particular, Sutskever et al. (2014) report that a 4-layers deep architecture was crucial in achieving good machine-translation performance in an encoder-decoder
framework. Irsoy and Cardie (2014) also report improved results from moving from a onelayer biRNN to an architecture with several layers. Many other works report result using
layered RNN architectures, but do not explicitly compare to 1-layer RNNs.
10.4 Bidirectional RNNs (biRNN)
A useful elaboration of an RNN is a bidirectional-RNN (biRNN, also commonly referred
to as biRNN) (Schuster & Paliwal, 1997; Graves, 2008).70 Consider the task of sequence
tagging over a sentence x1 , . . . , xn . An RNN allows us to compute a function of the ith word
70. When used with a specific RNN architecture such as an LSTM, the model is called biLSTM.

395

fiGoldberg

xi based on the past  the words x1:i up to and including it. However, the following words
xi:n may also be useful for prediction, as is evident by the common sliding-window approach
in which the focus word is categorized based on a window of k words surrounding it. Much
like the RNN relaxes the Markov assumption and allows looking arbitrarily back into the
past, the biRNN relaxes the fixed window size assumption, allowing to look arbitrarily far
at both the past and the future.
Consider an input sequence x1:n . The biRNN works by maintaining two separate states,
f
si and sbi for each input position i. The forward state sfi is based on x1 , x2 , . . . , xi , while
the backward state sbi is based on xn , xn1 , . . . , xi . The forward and backward states are
generated by two different RNNs. The first RNN (Rf , Of ) is fed the input sequence x1:n
as is, while the second RNN (Rb , Ob ) is fed the input sequence in reverse. The state
representation si is then composed of both the forward and backward states.
The output at position i is based on the concatenation of the two output vectors
yi = [yif ; yib ] = [Of (sfi ); Ob (sbi )], taking into account both the past and the future. The
vector yi can then be used directly for prediction, or fed as part of the input to a more
complex network. While the two RNNs are run independently of each other, the error gradients at position i will flow both forward and backward through the two RNNs. A visual
representation of the biRNN architecture is given in Figure 11.
ythe

ybrown

concat

concat

sb5

Rb ,Ob
y1f

sf0

Rf ,Of

xthe

concat
y4b

y5b
sb44

Rb ,Ob

sb33

Rf ,Of

Rb ,Ob
y3f

sf2

Rf ,Of

xbrown

xfox

y

concat
y3b

y2f
sf1

yjumped

yfox

concat
y2b

sb22

Rb ,Ob
y4f

sf3

Rf ,Of

xjumped

y1b
sb11

sb00

Rb ,Ob
y5f

sf4

sf5

Rf ,Of

x

Figure 11: biRNN over the sentence the brown fox jumped ..
The use of biRNNs for sequence tagging was introduced to the NLP community by Irsoy
and Cardie (2014).
10.5 RNNs for Representing Stacks
Some algorithms in language processing, including those for transition-based parsing (Nivre,
2008), require performing feature extraction over a stack. Instead of being confined to
looking at the k top-most elements of the stack, the RNN framework can be used to provide
a fixed-sized vector encoding of the entire stack.
The main intuition is that a stack is essentially a sequence, and so the stack state can be
represented by taking the stack elements and feeding them in order into an RNN, resulting
in a final encoding of the entire stack. In order to do this computation efficiently (without
396

fiA Primer on Neural Networks for NLP

performing an O(n) stack encoding operation each time the stack changes), the RNN state
is maintained together with the stack state. If the stack was push-only, this would be
trivial: whenever a new element x is pushed into the stack, the corresponding vector x
will be used together with the RNN state si in order to obtain a new state si+1 . Dealing
with pop operation is more challenging, but can be solved by using the persistent-stack
data-structure (Okasaki, 1999; Goldberg, Zhao, & Huang, 2013). Persistent, or immutable,
data-structures keep old versions of themselves intact when modified. The persistent stack
construction represents a stack as a pointer to the head of a linked list. An empty stack is
the empty list. The push operation appends an element to the list, returning the new head.
The pop operation then returns the parent of the head, but keeping the original list intact.
From the point of view of someone who held a pointer to the previous head, the stack did
not change. A subsequent push operation will add a new child to the same node. Applying
this procedure throughout the lifetime of the stack results in a tree, where the root is an
empty stack and each path from a node to the root represents an intermediary stack state.
Figure 12 provides an example of such a tree. The same process can be applied in the
computation graph construction, creating an RNN with a tree structure instead of a chain
structure. Backpropagating the error from a given node will then affect all the elements
that participated in the stack when the node was created, in order. Figure 13 shows the
computation graph for the stack-RNN corresponding to the last state in Figure 12. This
modeling approach was proposed independently by Dyer et al. (2015) and Watanabe and
Sumita (2015) for transition-based dependency parsing.
head

head


head

a

a



(1) push a

head
a



b

(2) push b

b

d

head

c

a



(3) push c

b

c

a



(4) pop

(5) push d

head

d



a

b
head

(6) pop

c

d



a

b

c



a

b

c

b

head

e

e

d

d

c



a

b

f

c

head
(7) pop

(8) push e

(9) push f

Figure 12: An immutable stack construction for the sequence of operations push a; push b;
push c; pop; push d; pop; pop; push e; push f.

10.6 A Note on Reading the Literature
Unfortunately, it is often the case that inferring the exact model form from reading its
description in a research paper can be quite challenging. Many aspects of the models
397

fiGoldberg

ya,e

R,O

ya,e,f

sa,e

ya,b,d xe

sa

ya

so

R,O

xa

ya:b

sa

R,O

xb

sa:b

ya:c

R,O

sa,e,f

xf

sa,b,d

R,O

sa:b

R,O

xd

sa:c

xc

Figure 13: The stack-RNN corresponding to the final state in Figure 12.

are not yet standardized, and different researchers use the same terms to refer to slightly
different things. To list a few examples, the inputs to the RNN can be either one-hot vectors
(in which case the embedding matrix is internal to the RNN) or embedded representations;
The input sequence can be padded with start-of-sequence and/or end-of-sequence symbols,
or not; While the output of an RNN is usually assumed to be a vector which is expected
to be fed to additional layers followed by a softmax for prediction (as is the case in the
presentation in this tutorial), some papers assume the softmax to be part of the RNN itself;
In multi-layer RNN, the state vector can be either the output of the top-most layer, or a
concatenation of the outputs from all layers; When using the encoder-decoder framework,
conditioning on the output of the encoder can be interpreted in various different ways; and
so on. On top of that, the LSTM architecture described in the next section has many small
variants, which are all referred to under the common name LSTM. Some of these choices
are made explicit in the papers, other require careful reading, and others still are not even
mentioned, or are hidden behind ambiguous figures or phrasing.
As a reader, be aware of these issues when reading and interpret model descriptions. As
a writer, be aware of these issues as well: either fully specify your model in mathematical
notation, or refer to a different source in which the model is fully specified, if such a source
is available. If using the default implementation from a software package without knowing
the details, be explicit of that fact and specify the software package you use. In any case,
dont rely solely on figures or natural language text when describing your model, as these
are often ambiguous.
398

fiA Primer on Neural Networks for NLP

11. Concrete RNN Architectures
We now turn to present three different instantiations of the abstract RN N architecture
discussed in the previous section, providing concrete definitions of the functions R and O.
These are the Simple RNN (SRNN), the Long Short-Term Memory (LSTM) and the Gated
Recurrent Unit (GRU).
11.1 Simple RNN
The simplest RNN formulation, known as an Elman Network or Simple-RNN (S-RNN), was
proposed by Elman (1990) and explored for use in language modeling by Mikolov (2012).
The S-RNN takes the following form:
si =Rsrnn (si1 , xi ) = g(xi Wx + si1 Ws + b)
yi =Osrnn (si ) = si

(38)

si , yi  Rds , xi  Rdx , Wx  Rdx ds , Ws  Rds ds , b  Rds
That is, the state at position i is a linear combination of the input at position i and
the previous state, passed through a non-linear activation (commonly tanh or ReLU). The
output at position i is the same as the hidden state in that position.71
In spite of its simplicity, the Simple RNN provides strong results for sequence tagging
(Xu et al., 2015) as well as language modeling. For comprehensive discussion on using
Simple RNNs for language modeling, see the PhD thesis by Mikolov (2012).
11.2 LSTM
The S-RNN is hard to train effectively because of the vanishing gradients problem (Pascanu
et al., 2012). Error signals (gradients) in later steps in the sequence diminish quickly in
the back-propagation process, and do not reach earlier input signals, making it hard for
the S-RNN to capture long-range dependencies. The Long Short-Term Memory (LSTM)
architecture (Hochreiter & Schmidhuber, 1997) was designed to solve the vanishing gradients
problem. The main idea behind the LSTM is to introduce as part of the state representation
also memory cells (a vector) that can preserve gradients across time. Access to the
memory cells is controlled by gating components  smooth mathematical functions that
simulate logical gates. At each input state, a gate is used to decide how much of the new
input should be written to the memory cell, and how much of the current content of the
memory cell should be forgotten. Concretely, a gate g  [0, 1]n is a vector of values in the
range [0, 1] that is multiplied component-wise with another vector v  Rn , and the result is
then added to another vector. The values of g are designed to be close to either 0 or 1, i.e.
by using a sigmoid function. Indices in v corresponding to near-one values in g are allowed
to pass, while those corresponding to near-zero values are blocked.
71. Some authors treat the output at position i as a more complicated function of the state, e.g. a linear
transformation, or an MLP. In our presentation, such further transformation of the output are not
considered part of the RNN, but as separate computations that are applied to the RNNs output.

399

fiGoldberg

Mathematically, the LSTM architecture is defined as:72

sj = Rlstm (sj1 , xj ) =[cj ; hj ]
cj =cj1 fi f + g fi i

hj = tanh(cj ) fi o

i =(xj Wxi + hj1 Whi )

f =(xj Wxf + hj1 Whf )
o =(xj W

xo

+ hj1 W

ho

(39)

)

g = tanh(xj Wxg + hj1 Whg )
yj = Olstm (sj ) =hj

sj  R2dh , xi  Rdx , cj , hj , i, f , o, g  Rdh , Wx  Rdx dh , Wh  Rdh dh ,
The symbol fi is used to denote component-wise product. The state at time j is composed of two vectors, cj and hj , where cj is the memory component and hj is the hidden
state component. There are three gates, i, f and o, controlling for input, f orget and output.
The gate values are computed based on linear combinations of the current input xj and the
previous state hj1 , passed through a sigmoid activation function. An update candidate g
is computed as a linear combination of xj and hj1 , passed through a tanh activation function. The memory cj is then updated: the forget gate controls how much of the previous
memory to keep (cj1 fi f ), and the input gate controls how much of the proposed update
to keep (g fi i). Finally, the value of hj (which is also the output yj ) is determined based
on the content of the memory cj , passed through a tanh non-linearity and controlled by the
output gate. The gating mechanisms allow for gradients related to the memory part cj to
stay high across very long time ranges.
For further discussion on the LSTM architecture see the PhD thesis by Alex Graves
(2008), as well as the online-post by Olah (2015b). For an analysis of the behavior of an
LSTM when used as a character-level language model, see the work of Karpathy et al.
(2015).
For further explanation of the motivation behind the gating mechanism in the LSTM
(and the GRU) and its relation to solving the vanishing gradient problem in recurrent neural
networks, see Sections 4.2 and 4.3 in the detailed course notes of Cho (2015).
LSTMs are currently the most successful type of RNN architecture, and they are responsible for many state-of-the-art sequence modeling results. The main competitor of the
LSTM-RNN is the GRU, to be discussed next.
72. There are many variants on the LSTM architecture presented here. For example, forget gates were not
part of the original proposal by Hochreiter and Schmidhuber (1997), but are shown to be an important
part of the architecture. Other variants include peephole connections and gate-tying. For an overview
and comprehensive empirical comparison of various LSTM architectures see the work of Greff, Srivastava,
Koutnk, Steunebrink, and Schmidhuber (2015).

400

fiA Primer on Neural Networks for NLP

11.2.1 Practical Considerations
When training LSTM networks, Jozefowicz et al. (2015) strongly recommend to always
initialize the bias term of the forget gate to be close to one. When applying dropout to an
RNN with an LSTM, Zaremba et al. (2014) found out that it is crucial to apply dropout
only on the non-recurrent connection, i.e. only to apply it between layers and not between
sequence positions.
11.3 GRU
The LSTM architecture is very effective, but also quite complicated. The complexity of the
system makes it hard to analyze, and also computationally expensive to work with. The
gated recurrent unit (GRU) was recently introduced by Cho et al. (2014b) as an alternative
to the LSTM. It was subsequently shown by Chung et al. (2014) to perform comparably to
the LSTM on several (non textual) datasets.
Like the LSTM, the GRU is also based on a gating mechanism, but with substantially
fewer gates and without a separate memory component.
sj = RGRU (sj1 , xj ) =(1  z) fi sj1 + z fi sj
z =(xj Wxz + sj1 Wsz )
r =(xj Wxr + sj1 Wsr )
sj = tanh(xj Wxs + (sj1 fi r)Wsg )

(40)

yj = OGRU (sj ) =sj
sj , sj  Rds , xi  Rdx , z, r  Rds , Wx  Rdx ds , Ws  Rds ds ,
One gate (r) is used to control access to the previous state sj1 and compute a proposed update sj . The updated state sj (which also serves as the output yj ) is then determined based
on an interpolation of the previous state sj1 and the proposal sj , where the proportions of
the interpolation are controlled using the gate z.73
The GRU was shown to be effective in language modeling and machine translation.
However, the jury is still out between the GRU, the LSTM and possible alternative RNN
architectures, and the subject is actively researched. For an empirical exploration of the
GRU and the LSTM architectures, see work of Jozefowicz et al. (2015).
11.4 Other Variants
The gated architectures of the LSTM and the GRU help in alleviating the vanishing gradients problem of the Simple RNN, and allow these RNNs to capture dependencies that span
long time ranges. Some researchers explore simpler architectures than the LSTM and the
GRU for achieving similar benefits.
Mikolov et al. (2014) observed that the matrix multiplication si1 Ws coupled with the
nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo
73. The states s are often called h in the GRU literature.

401

fiGoldberg

large changes at each time step, prohibiting it from remembering information over long
time periods. They propose to split the state vector si into a slow changing component ci
(context units) and a fast changing component hi .74 The slow changing component ci is
updated according to a linear interpolation of the input and the previous component: ci =
(1  )xi Wx1 + ci1 , where   (0, 1). This update allows ci to accumulate the previous
inputs. The fast changing component hi is updated similarly to the Simple RNN update
rule, but changed to take ci into account as well:75 hi = (xi Wx2 + hi1 Wh + ci Wc ).
Finally, the output yi is the concatenation of the slow and the fast changing parts of the
state: yi = [ci ; hi ]. Mikolov et al. demonstrate that this architecture provides competitive
perplexities to the much more complex LSTM on language modeling tasks.
The approach of Mikolov et al. can be interpreted as constraining the block of the
matrix Ws in the S-RNN corresponding to ci to be a multiply of the identity matrix (see
Mikolov et al. (2014) for the details). Le, Jaitly, and Hinton (2015) propose an even simpler
approach: set the activation function of the S-RNN to a ReLU, and initialize the biases b
as zeroes and the matrix Ws as the identify matrix. This causes an untrained RNN to copy
the previous state to the current state, add the effect of the current input xi and set the
negative values to zero. After setting this initial bias towards state copying, the training
procedure allows Ws to change freely. Le et al. demonstrate that this simple modification
makes the S-RNN comparable to an LSTM with the same number of parameters on several
tasks, including language modeling.

12. Modeling Trees  Recursive Neural Networks
The RNN is very useful for modeling sequences. In language processing, it is often natural
and desirable to work with tree structures. The trees can be syntactic trees, discourse trees,
or even trees representing the sentiment expressed by various parts of a sentence (Socher
et al., 2013). We may want to predict values based on specific tree nodes, predict values
based on the root nodes, or assign a quality score to a complete tree or part of a tree. In
other cases, we may not care about the tree structure directly but rather reason about spans
in the sentence. In such cases, the tree is merely used as a backbone structure which helps
guide the encoding process of the sequence into a fixed size vector.
The recursive neural network (RecNN) abstraction (Pollack, 1990), popularized in NLP
by Richard Socher and colleagues (Socher, Manning, & Ng, 2010; Socher, Lin, Ng, & Manning, 2011; Socher et al., 2013; Socher, 2014) is a generalization of the RNN from sequences
to (binary) trees.76
Much like the RNN encodes each sentence prefix as a state vector, the RecNN encodes
each tree-node as a state vector in Rd . We can then use these state vectors either to predict
values of the corresponding nodes, assign quality values to each node, or as a semantic
representation of the spans rooted at the nodes.
74. We depart from the notation of Mikolov et al. (2014) and reuse the symbols used in the LSTM description.
75. The update rule diverges from the S-RNN update rule also by fixing the non-linearity to be a sigmoid
function, and by not using a bias term. However, these changes are not discussed as central to the
proposal.
76. While presented in terms of binary parse trees, the concepts easily transfer to general recursively-defined
data structures, with the major technical challenge is the definition of an effective form for R, the
combination function.

402

fiA Primer on Neural Networks for NLP

The main intuition behind the recursive neural networks is that each subtree is represented as a d dimensional vector, and the representation of a node p with children c1 and c2
is a function of the representation of the nodes: vec(p) = f (vec(c1 ), vec(c2 )), where f is a
composition function taking two d-dimensional vectors and returning a single d-dimensional
vector. Much like the RNN state si is used to encode the entire sequence x1 : i, the RecNN
state associated with a tree node p encodes the entire subtree rooted at p. See Figure 14
for an illustration.

S=
combine

N P2 =

VP =

combine

N P1 =

V =

Figure 14: Illustration of a recursive neural network. The representations of V and NP1
are combined to form the representation of VP. The representations of VP and
NP2 are then combined to form the representation of S.

12.1 Formal Definition
Consider a binary parse tree T over an n-word sentence. As a reminder, an ordered,
unlabeled tree over a string x1 , . . . , xn can be represented as a unique set of triplets (i, k, j),
s.t. i  k  j. Each such triplet indicates that a node spanning words xi:j is parent of the
nodes spanning xi:k and xk+1:j . Triplets of the form (i, i, i) correspond to terminal symbols
at the tree leaves (the words xi ). Moving from the unlabeled case to the labeled one, we can
represent a tree as a set of 6-tuples (A  B, C, i, k, j), whereas i, k and j indicate the spans
as before, and A, B and C are the node labels of of the nodes spanning xi:j , xi:k and xk+1:j
respectively. Here, leaf nodes have the form (A  A, A, i, i, i), where A is a pre-terminal
symbol. We refer to such tuples as production rules. For an example, consider the syntactic
tree for the sentence the boy saw her duck.
403

fiGoldberg

S
VP

NP

NP

Det Noun Verb
the

boy

saw

Det Noun
her

duck

Its corresponding unlabeled and labeled representations are :
Unlabeled
(1,1,1)
(2,2,2)
(3,3,3)
(4,4,4)
(5,5,5)
(4,4,5)
(3,3,5)
(1,1,2)
(1,2,5)

Labeled
(Det, Det, Det, 1, 1, 1)
(Noun, Noun, Noun, 2, 2, 2)
(Verb, Verb, Verb, 3, 3, 3)
(Det, Det, Det, 4, 4, 4)
(Noun, Noun, Noun, 5, 5, 5)
(NP, Det, Noun, 4, 4, 5)
(VP, Verb, NP, 3, 3, 5)
(NP, Det, Noun, 1, 1, 2)
(S, NP, VP, 1, 2, 5)

Corresponding Span
x1:1 the
x2:2 boy
saw
her
duck
her duck
saw her duck
the boy
the boy saw her duck

A
The set of production rules above can be uniquely converted to a set tree nodes qi:j
(indicating a node with symbol A over the span xi:j ) by simply ignoring the elements
(B, C, k) in each production rule. We are now in position to define the recursive neural
network.
A recursive neural network (RecNN) is a function that takes as input a parse tree over an
n-word sentence x1 , . . . , xn . Each of the sentences words is represented as a d-dimensional
vector xi , and the tree is represented as a set T of production rules (A  B, C, i, j, k).
A . The RecNN returns as output a corresponding set of
Denote the nodes of T by qi:j
A
d
inside state vectors si:j , where each inside state vector sA
i:j  R represents the corresponding
A , and encodes the entire structure rooted at that node. Like the sequence RNN,
tree node qi:j
the tree shaped RecNN is defined recursively using a function R, where the inside vector of
a given node is defined as a function of the inside vectors of its direct children.77 Formally:

d
A
RecNN(x1 , . . . , xn , T ) ={sA
i:j  R | qi:j  T }

sA
i:i =v(xi )

B
C
sA
i:j =R(A, B, C, si:k , sk+1:j )

(41)
B
C
 T , qk+1:j
T
qi:k

77. Le and Zuidema (2014) extend the RecNN definition such that each node has, in addition to its inside
state vector, also an outside state vector representing the entire structure around the subtree rooted
at that node. Their formulation is based on the recursive computation of the classic inside-outside
algorithm, and can be thought of as the biRNN counterpart of the tree RecNN. For details, see work by
Le and Zuidema.

404

fiA Primer on Neural Networks for NLP

The function R usually takes the form of a simple linear transformation, which may or
may not be followed by a non-linear activation function g:
C
B
C
R(A, B, C, sB
i:k , sk+1:j ) = g([si:k ; sk+1:j ]W)

(42)

This formulation of R ignores the tree labels, using the same matrix W  R2dd for all
combinations. This may be a useful formulation in case the node labels do not exist (e.g.
when the tree does not represent a syntactic structure with clearly defined labels) or when
they are unreliable. However, if the labels are available, it is generally useful to include them
in the composition function. One approach would be to introduce label embeddings v(A)
mapping each non-terminal symbol to a dnt dimensional vector, and change R to include
the embedded symbols in the combination function:
C
B
C
R(A, B, C, sB
i:k , sk+1:j ) = g([si:k ; sk+1:j ; v(A); v(B)]W)

(43)

(here, W  R2d+2dnt d ). Such approach is taken by Qian, Tian, Huang, Liu, Zhu, and
Zhu (2015). An alternative approach, due to Socher et al. (2013) is to untie the weights
according to the non-terminals, using a different composition matrix for each B, C pair of
symbols:78
BC
C
B
C
)
R(A, B, C, sB
i:k , sk+1:j ) = g([si:k ; sk+1:j ]W

(44)

This formulation is useful when the number of non-terminal symbols (or the number of
possible symbol combinations) is relatively small, as is usually the case with phrase-structure
parse trees. A similar model was also used by Hashimoto et al. (2013) to encode subtrees
in semantic-relation classification task.
12.2 Extensions and Variations
As all of the definitions of R above suffer from the vanishing gradients problem of the
Simple RNN, several authors sought to replace it with functions inspired by the Long ShortTerm Memory (LSTM) gated architecture, resulting in Tree-shaped LSTMs (Tai, Socher, &
Manning, 2015; Zhu, Sobhani, & Guo, 2015b). The question of optimal tree representation
is still very much an open research question, and the vast space of possible combination
functions R is yet to be explored. Other proposed variants on tree-structured RNNs includes
a recursive matrix-vector model (Socher, Huval, Manning, & Ng, 2012) and recursive neural
tensor network (Socher et al., 2013). In the first variant, each word is represented as a
combination of a vector and a matrix, where the vector defines the words static semantic
content as before, while the matrix acts as a learned operator for the word, allowing
more subtle semantic compositions than the addition and weighted averaging implied by
the concatenation followed by linear transformation function. In the second variant, words
are associated with vectors as usual, but the composition function becomes more expressive
by basing it on tensor instead of matrix operations.
78. While not explored in the literature, a trivial extension would condition the transformation matrix also
on A.

405

fiGoldberg

12.3 Training Recursive Neural Networks
The training procedure for a recursive neural network follows the same recipe as training
other forms of networks: define a loss, spell out the computation graph, compute gradients
using backpropagation79 , and train the parameters using SGD.
With regard to the loss function, similar to the sequence RNN one can associate a loss
either with the root of the tree, with any given node, or with a set of nodes, in which case
the individual nodes losses are combined, usually by summation. The loss function is based
on the labeled training data which associates a label or other quantity with different tree
nodes.
Additionally, one can treat the RecNN as an Encoder, whereas the inside-vector associated with a node is taken to be an encoding of the tree rooted at that node. The encoding
can potentially be sensitive to arbitrary properties of the structure. The vector is then
passed as input to another network.
For further discussion on recursive neural networks and their use in natural language
tasks, refer to the PhD thesis of Richard Socher (2014).

13. Conclusions
Neural networks are powerful learners, providing opportunities ranging from non-linear
classification to non-Markovian modeling of sequences and trees. We hope that this exposition helps NLP researchers to incorporate neural network models in their work and take
advantage of their power.

References
Adel, H., Vu, N. T., & Schultz, T. (2013). Combination of Recurrent Neural Networks and
Factored Language Models for Code-Switching Language Modeling. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 206211, Sofia, Bulgaria. Association for Computational
Linguistics.
Ando, R., & Zhang, T. (2005a). A High-Performance Semi-Supervised Learning Method
for Text Chunking. In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL05), pp. 19, Ann Arbor, Michigan. Association for
Computational Linguistics.
Ando, R. K., & Zhang, T. (2005b). A framework for learning predictive structures from
multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6,
18171853.
Auli, M., Galley, M., Quirk, C., & Zweig, G. (2013). Joint Language and Translation Modeling with Recurrent Neural Networks. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, pp. 10441054, Seattle, Washington, USA. Association for Computational Linguistics.
79. Before the introduction of the computation graph abstraction, the specific backpropagation procedure for
computing the gradients in a RecNN as defined above was referred to as the Back-propagation through
Structure (BPTS) algorithm (Goller & Kuchler, 1996).

406

fiA Primer on Neural Networks for NLP

Auli, M., & Gao, J. (2014). Decoder Integration and Expected BLEU Training for Recurrent
Neural Network Language Models. In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pp. 136142,
Baltimore, Maryland. Association for Computational Linguistics.
Ballesteros, M., Dyer, C., & Smith, N. A. (2015). Improved Transition-based Parsing by
Modeling Characters instead of Words with LSTMs. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 349359, Lisbon,
Portugal. Association for Computational Linguistics.
Ballesteros, M., Goldberg, Y., Dyer, C., & Smith, N. A. (2016). Training with Exploration
Improves a Greedy Stack-LSTM Parser. arXiv:1603.03793 [cs].
Bansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring Continuous Word Representations
for Dependency Parsing. In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), pp. 809815, Baltimore,
Maryland. Association for Computational Linguistics.
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2015). Automatic
differentiation in machine learning: a survey. arXiv:1502.05767 [cs].
Bengio, Y. (2012). Practical recommendations for gradient-based training of deep architectures. arXiv:1206.5533 [cs].
Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A Neural Probabilistic Language Model. J. Mach. Learn. Res., 3, 11371155.
Bengio, Y., Goodfellow, I. J., & Courville, A. (2015). Deep Learning. Book in preparation
for MIT Press.
Bitvai, Z., & Cohn, T. (2015). Non-Linear Text Regression with a Deep Convolutional
Neural Network. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 180185, Beijing, China. Association
for Computational Linguistics.
Botha, J. A., & Blunsom, P. (2014). Compositional Morphology for Word Representations
and Language Modelling. In Proceedings of the 31st International Conference on
Machine Learning (ICML), Beijing, China. *Award for best application paper*.
Bottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the
Trade, pp. 421436. Springer.
Charniak, E., & Johnson, M. (2005). Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL05), pp. 173180, Ann Arbor, Michigan. Association
for Computational Linguistics.
Chen, D., & Manning, C. (2014). A Fast and Accurate Dependency Parser using Neural
Networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 740750, Doha, Qatar. Association for Computational Linguistics.
407

fiGoldberg

Chen, Y., Xu, L., Liu, K., Zeng, D., & Zhao, J. (2015). Event Extraction via Dynamic
Multi-Pooling Convolutional Neural Networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 167
176, Beijing, China. Association for Computational Linguistics.
Cho, K. (2015). Natural Language Understanding with Distributed Representation.
arXiv:1511.07916 [cs, stat].
Cho, K., van Merrienboer, B., Bahdanau, D., & Bengio, Y. (2014a). On the Properties of
Neural Machine Translation: EncoderDecoder Approaches. In Proceedings of SSST8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,
pp. 103111, Doha, Qatar. Association for Computational Linguistics.
Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &
Bengio, Y. (2014b). Learning Phrase Representations using RNN EncoderDecoder for
Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 17241734, Doha, Qatar.
Association for Computational Linguistics.
Chrupala, G. (2014). Normalizing tweets with edit scripts and recurrent neural embeddings.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 680686, Baltimore, Maryland. Association for
Computational Linguistics.
Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical Evaluation of Gated
Recurrent Neural Networks on Sequence Modeling. arXiv:1412.3555 [cs].
Collins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory
and Experiments with Perceptron Algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pp. 18. Association for
Computational Linguistics.
Collins, M., & Koo, T. (2005). Discriminative Reranking for Natural Language Parsing.
Computational Linguistics, 31 (1), 2570.
Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing:
Deep neural networks with multitask learning. In Proceedings of the 25th international
conference on Machine learning, pp. 160167. ACM.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) from scratch. The Journal of Machine Learning
Research, 12, 24932537.
Crammer, K., & Singer, Y. (2002). On the algorithmic implementation of multiclass kernelbased vector machines. The Journal of Machine Learning Research, 2, 265292.
Creutz, M., & Lagus, K. (2007). Unsupervised Models for Morpheme Segmentation and
Morphology Learning. ACM Trans. Speech Lang. Process., 4 (1), 3:13:34.
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics
of Control, Signals and Systems, 2 (4), 303314.
408

fiA Primer on Neural Networks for NLP

Dahl, G., Sainath, T., & Hinton, G. (2013). Improving deep neural networks for LVCSR
using rectified linear units and dropout. In 2013 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 86098613.
Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014).
Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., & Weinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 27, pp.
29332941. Curran Associates, Inc.
de Gispert, A., Iglesias, G., & Byrne, B. (2015). Fast and Accurate Preordering for SMT
using Neural Networks. In Proceedings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 10121017, Denver, Colorado. Association for Computational Linguistics.
Do, T., Arti, T., & others (2010). Neural conditional random fields. In International
Conference on Artificial Intelligence and Statistics, pp. 177184.
Dong, L., Wei, F., Tan, C., Tang, D., Zhou, M., & Xu, K. (2014). Adaptive Recursive Neural
Network for Target-dependent Twitter Sentiment Classification. In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pp. 4954, Baltimore, Maryland. Association for Computational
Linguistics.
Dong, L., Wei, F., Zhou, M., & Xu, K. (2015). Question Answering over Freebase with
Multi-Column Convolutional Neural Networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 260
269, Beijing, China. Association for Computational Linguistics.
dos Santos, C., & Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment
Analysis of Short Texts. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pp. 6978, Dublin, Ireland.
Dublin City University and Association for Computational Linguistics.
dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying Relations by Ranking with
Convolutional Neural Networks. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 626634, Beijing,
China. Association for Computational Linguistics.
dos Santos, C., & Zadrozny, B. (2014). Learning Character-level Representations for Partof-Speech Tagging. In Proceedings of the 31st International Conference on Machine
Learning (ICML), pp. 18181826.
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Machine Learning Research, 12, 2121
2159.
Duh, K., Neubig, G., Sudoh, K., & Tsukada, H. (2013). Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation. In Proceedings
409

fiGoldberg

of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 678683, Sofia, Bulgaria. Association for Computational
Linguistics.
Durrett, G., & Klein, D. (2015). Neural CRF Parsing. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 302
312, Beijing, China. Association for Computational Linguistics.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). TransitionBased Dependency Parsing with Stack Long Short-Term Memory. In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pp. 334343, Beijing, China. Association for Computational Linguistics.
Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14 (2), 179211.
Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. In Proceedings of the 14th Conference of the European Chapter
of the Association for Computational Linguistics, pp. 462471, Gothenburg, Sweden.
Association for Computational Linguistics.
Filippova, K., Alfonseca, E., Colmenares, C. A., Kaiser, L., & Vinyals, O. (2015). Sentence
Compression by Deletion with LSTMs. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pp. 360368, Lisbon, Portugal.
Association for Computational Linguistics.
Forcada, M. L., & Neco, R. P. (1997). Recursive hetero-associative memories for translation.
In Biological and Artificial Computation: From Neuroscience to Technology, pp. 453
462. Springer.
Gao, J., Pantel, P., Gamon, M., He, X., & Deng, L. (2014). Modeling Interestingness with
Deep Neural Networks. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 213, Doha, Qatar. Association for
Computational Linguistics.
Gimenez, J., & Marquez, L. (2004). SVMTool: A general POS tagger generator based on
Support Vector Machines. In Proceedings of the 4th LREC, Lisbon, Portugal.
Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward
neural networks. In International conference on artificial intelligence and statistics,
pp. 249256.
Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectifier neural networks. In
International Conference on Artificial Intelligence and Statistics, pp. 315323.
Goldberg, Y., & Elhadad, M. (2010). An Efficient Algorithm for Easy-First Non-Directional
Dependency Parsing. In Human Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for Computational Linguistics, pp.
742750, Los Angeles, California. Association for Computational Linguistics.
Goldberg, Y., & Levy, O. (2014). word2vec Explained: deriving Mikolov et al.s negativesampling word-embedding method. arXiv:1402.3722 [cs, stat].
410

fiA Primer on Neural Networks for NLP

Goldberg, Y., & Nivre, J. (2013). Training Deterministic Parsers with Non-Deterministic
Oracles. Transactions of the Association for Computational Linguistics, 1 (0), 403
414.
Goldberg, Y., Zhao, K., & Huang, L. (2013). Efficient Implementation of Beam-Search
Incremental Parsers. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pp. 628633, Sofia, Bulgaria.
Association for Computational Linguistics.
Goller, C., & Kuchler, A. (1996). Learning Task-Dependent Distributed Representations
by Backpropagation Through Structure. In In Proc. of the ICNN-96, pp. 347352.
IEEE.
Gouws, S., Bengio, Y., & Corrado, G. (2015). BilBOWA: Fast Bilingual Distributed Representations without Word Alignments. In Proceedings of The 32nd International
Conference on Machine Learning, pp. 748756.
Graves, A. (2008). Supervised sequence labelling with recurrent neural networks. Ph.D.
thesis, Technische Universitat Munchen.
Greff, K., Srivastava, R. K., Koutnk, J., Steunebrink, B. R., & Schmidhuber, J. (2015).
LSTM: A Search Space Odyssey. arXiv:1503.04069 [cs].
Hal Daume III, Langford, J., & Marcu, D. (2009). Search-based Structured Prediction.
Machine Learning Journal (MLJ).
Harris, Z. (1954). Distributional Structure. Word, 10 (23), 146162.
Hashimoto, K., Miwa, M., Tsuruoka, Y., & Chikayama, T. (2013). Simple Customization
of Recursive Neural Networks for Semantic Relation Classification. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.
13721376, Seattle, Washington, USA. Association for Computational Linguistics.
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification. arXiv:1502.01852 [cs].
Henderson, M., Thomson, B., & Young, S. (2013). Deep Neural Network Approach for the
Dialog State Tracking Challenge. In Proceedings of the SIGDIAL 2013 Conference,
pp. 467471, Metz, France. Association for Computational Linguistics.
Hermann, K. M., & Blunsom, P. (2013). The Role of Syntax in Vector Space Models of
Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 894904, Sofia,
Bulgaria. Association for Computational Linguistics.
Hermann, K. M., & Blunsom, P. (2014). Multilingual Models for Compositional Distributed
Semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5868, Baltimore, Maryland.
Association for Computational Linguistics.
Hihi, S. E., & Bengio, Y. (1996). Hierarchical Recurrent Neural Networks for Long-Term
Dependencies. In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), Advances
in Neural Information Processing Systems 8, pp. 493499. MIT Press.
411

fiGoldberg

Hill, F., Cho, K., Jean, S., Devin, C., & Bengio, Y. (2014). Embedding Word Similarity
with Neural Machine Translation. arXiv:1412.6448 [cs].
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R.
(2012). Improving neural networks by preventing co-adaptation of feature detectors.
arXiv:1207.0580 [cs].
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation,
9 (8), 17351780.
Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are
universal approximators. Neural Networks, 2 (5), 359366.
Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training
by Reducing Internal Covariate Shift. arXiv:1502.03167 [cs].
Irsoy, O., & Cardie, C. (2014). Opinion Mining with Deep Recurrent Neural Networks.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 720728, Doha, Qatar. Association for Computational Linguistics.
Iyyer, M., Boyd-Graber, J., Claudino, L., Socher, R., & Daume III, H. (2014a). A Neural
Network for Factoid Question Answering over Paragraphs. In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.
633644, Doha, Qatar. Association for Computational Linguistics.
Iyyer, M., Enns, P., Boyd-Graber, J., & Resnik, P. (2014b). Political Ideology Detection
Using Recursive Neural Networks. In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 11131122,
Baltimore, Maryland. Association for Computational Linguistics.
Iyyer, M., Manjunatha, V., Boyd-Graber, J., & Daume III, H. (2015). Deep Unordered
Composition Rivals Syntactic Methods for Text Classification. In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 16811691, Beijing, China. Association for Computational Linguistics.
Johnson, R., & Zhang, T. (2015). Effective Use of Word Order for Text Categorization with
Convolutional Neural Networks. In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 103112, Denver, Colorado. Association for Computational
Linguistics.
Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu, Y. (2016). Exploring the
Limits of Language Modeling. arXiv:1602.02410 [cs].
Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). An Empirical Exploration of Recurrent Network Architectures. In Proceedings of the 32nd International Conference on
Machine Learning (ICML-15), pp. 23422350.
Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A Convolutional Neural Network
for Modelling Sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 655665, Baltimore,
Maryland. Association for Computational Linguistics.
412

fiA Primer on Neural Networks for NLP

Karpathy, A., Johnson, J., & Li, F.-F. (2015). Visualizing and Understanding Recurrent
Networks. arXiv:1506.02078 [cs].
Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 17461751, Doha, Qatar. Association for Computational Linguistics.
Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2015). Character-Aware Neural Language
Models. arXiv:1508.06615 [cs, stat].
Kingma, D., & Ba, J. (2014).
arXiv:1412.6980 [cs].

Adam: A Method for Stochastic Optimization.

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep
Convolutional Neural Networks. In Pereira, F., Burges, C. J. C., Bottou, L., & Weinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 25, pp.
10971105. Curran Associates, Inc.
Kudo, T., & Matsumoto, Y. (2003). Fast Methods for Kernel-based Text Analysis. In
Proceedings of the 41st Annual Meeting on Association for Computational Linguistics Volume 1, ACL 03, pp. 2431, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Lafferty, J., McCallum, A., & Pereira, F. C. (2001). Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of ICML.
Le, P., & Zuidema, W. (2014). The Inside-Outside Recursive Neural Network model for
Dependency Parsing. In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 729739, Doha, Qatar. Association for
Computational Linguistics.
Le, P., & Zuidema, W. (2015). The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization. In Proceedings
of the 2015 Conference on Empirical Methods in Natural Language Processing, pp.
11551164, Lisbon, Portugal. Association for Computational Linguistics.
Le, Q. V., Jaitly, N., & Hinton, G. E. (2015). A Simple Way to Initialize Recurrent Networks
of Rectified Linear Units. arXiv:1504.00941 [cs].
LeCun, Y., & Bengio, Y. (1995). Convolutional Networks for Images, Speech, and TimeSeries. In Arbib, M. A. (Ed.), The Handbook of Brain Theory and Neural Networks.
MIT Press.
LeCun, Y., Bottou, L., Orr, G., & Muller, K. (1998a). Efficient BackProp. In Orr, G., &
K, M. (Eds.), Neural Networks: Tricks of the trade. Springer.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998b). Gradient Based Learning Applied
to Pattern Recognition. Proceedings of the IEEE, 86 (11), 22782324.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energybased learning. Predicting structured data, 1, 0.
LeCun, Y., & Huang, F. (2005). Loss functions for discriminative training of energybased
models. In Proceedings of AISTATS. AIStats.
413

fiGoldberg

Lee, G., Flowers, M., & Dyer, M. G. (1992). Learning distributed representations of conceptual knowledge and their application to script-based story processing. In Connectionist
Natural Language Processing, pp. 215247. Springer.
Levy, O., & Goldberg, Y. (2014a). Dependency-Based Word Embeddings. In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pp. 302308, Baltimore, Maryland. Association for Computational
Linguistics.
Levy, O., & Goldberg, Y. (2014b). Neural Word Embedding as Implicit Matrix Factorization. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., & Weinberger,
K. Q. (Eds.), Advances in Neural Information Processing Systems 27, pp. 21772185.
Curran Associates, Inc.
Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving Distributional Similarity with
Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3 (0), 211225.
Lewis, M., & Steedman, M. (2014). Improved CCG Parsing with Semi-supervised Supertagging. Transactions of the Association for Computational Linguistics, 2 (0), 327338.
Li, J., Li, R., & Hovy, E. (2014). Recursive Deep Models for Discourse Parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 20612069, Doha, Qatar. Association for Computational Linguistics.
Ling, W., Dyer, C., Black, A. W., & Trancoso, I. (2015a). Two/Too Simple Adaptations of
Word2Vec for Syntax Problems. In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 12991304, Denver, Colorado. Association for Computational
Linguistics.
Ling, W., Dyer, C., Black, A. W., Trancoso, I., Fermandez, R., Amir, S., Marujo, L., &
Luis, T. (2015b). Finding Function in Form: Compositional Character Models for
Open Vocabulary Word Representation. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pp. 15201530, Lisbon, Portugal.
Association for Computational Linguistics.
Liu, Y., Wei, F., Li, S., Ji, H., Zhou, M., & Wang, H. (2015). A Dependency-Based Neural
Network for Relation Classification. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 285290, Beijing,
China. Association for Computational Linguistics.
Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., & Kaiser, L. (2015). Multi-task Sequence
to Sequence Learning. arXiv:1511.06114 [cs, stat].
Ma, J., Zhang, Y., & Zhu, J. (2014). Tagging The Web: Building A Robust Web Tagger
with Neural Network. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 144154, Baltimore,
Maryland. Association for Computational Linguistics.
Ma, M., Huang, L., Zhou, B., & Xiang, B. (2015). Dependency-based Convolutional Neural
Networks for Sentence Embedding. In Proceedings of the 53rd Annual Meeting of the
414

fiA Primer on Neural Networks for NLP

Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 174179, Beijing,
China. Association for Computational Linguistics.
McCallum, A., Freitag, D., & Pereira, F. C. (2000). Maximum Entropy Markov Models for
Information Extraction and Segmentation.. In ICML, Vol. 17, pp. 591598.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word
Representations in Vector Space. arXiv:1301.3781 [cs].
Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., & Ranzato, M. (2014). Learning Longer
Memory in Recurrent Neural Networks. arXiv:1412.7753 [cs].
Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., & Khudanpur, S. (2010). Recurrent
neural network based language model.. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba,
Japan, September 26-30, 2010, pp. 10451048.
Mikolov, T., Kombrink, S., Lukas Burget, Cernocky, J. H., & Khudanpur, S. (2011). Extensions of recurrent neural network language model. In Acoustics, Speech and Signal
Processing (ICASSP), 2011 IEEE International Conference on, pp. 55285531. IEEE.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. In Burges, C. J. C.,
Bottou, L., Welling, M., Ghahramani, Z., & Weinberger, K. Q. (Eds.), Advances in
Neural Information Processing Systems 26, pp. 31113119. Curran Associates, Inc.
Mikolov, T. (2012). Statistical language models based on neural networks. Ph.D. thesis, Ph.
D. thesis, Brno University of Technology.
Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noisecontrastive estimation. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z.,
& Weinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 26,
pp. 22652273. Curran Associates, Inc.
Mrksic, N., O Seaghdha, D., Thomson, B., Gasic, M., Su, P.-H., Vandyke, D., Wen, T.-H.,
& Young, S. (2015). Multi-domain Dialog State Tracking using Recurrent Neural
Networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pp. 794799, Beijing, China. Association for
Computational Linguistics.
Neidinger, R. (2010). Introduction to Automatic Differentiation and MATLAB ObjectOriented Programming. SIAM Review, 52 (3), 545563.
Nesterov, Y. (1983). A method of solving a convex programming problem with convergence
rate O (1/k2). In Soviet Mathematics Doklady, Vol. 27, pp. 372376.
Nesterov, Y. (2004). Introductory lectures on convex optimization. Kluwer Academic Publishers.
Nguyen, T. H., & Grishman, R. (2015). Event Detection and Domain Adaptation with
Convolutional Neural Networks. In Proceedings of the 53rd Annual Meeting of the
415

fiGoldberg

Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 365371, Beijing,
China. Association for Computational Linguistics.
Nivre, J. (2008). Algorithms for Deterministic Incremental Dependency Parsing. Computational Linguistics, 34 (4), 513553.
Okasaki, C. (1999). Purely Functional Data Structures. Cambridge University Press, Cambridge, U.K.; New York.
Olah, C. (2015a). Calculus on Computational Graphs: Backpropagation. Retrieved from
http://colah.github.io/posts/2015-08-Backprop/.
Olah, C. (2015b). Understanding LSTM Networks. Retrieved from http://colah.
github.io/posts/2015-08-Understanding-LSTMs/.
Pascanu, R., Mikolov, T., & Bengio, Y. (2012). On the difficulty of training Recurrent
Neural Networks. arXiv:1211.5063 [cs].
Pei, W., Ge, T., & Chang, B. (2015). An Effective Neural Network Model for Graph-based
Dependency Parsing. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 313322, Beijing, China. Association for Computational Linguistics.
Peng, J., Bo, L., & Xu, J. (2009). Conditional Neural Fields. In Bengio, Y., Schuurmans,
D., Lafferty, J. D., Williams, C. K. I., & Culotta, A. (Eds.), Advances in Neural
Information Processing Systems 22, pp. 14191427. Curran Associates, Inc.
Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 15321543, Doha, Qatar. Association for Computational Linguistics.
Pollack, J. B. (1990). Recursive Distributed Representations. Artificial Intelligence, 46,
77105.
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.
USSR Computational Mathematics and Mathematical Physics, 4 (5), 1  17.
Qian, Q., Tian, B., Huang, M., Liu, Y., Zhu, X., & Zhu, X. (2015). Learning Tag Embeddings
and Tag-specific Composition Functions in Recursive Neural Network. In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pp. 13651374, Beijing, China. Association for Computational Linguistics.
Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738 [cs].
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by
back-propagating errors. Nature, 323 (6088), 533536.
Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE
Transactions on Signal Processing, 45 (11), 26732681.
416

fiA Primer on Neural Networks for NLP

Schwenk, H., Dchelotte, D., & Gauvain, J.-L. (2006). Continuous space language models
for statistical machine translation. In Proceedings of the COLING/ACL on Main
conference poster sessions, pp. 723730. Association for Computational Linguistics.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Cambridge
University Press.
Smith, N. A. (2011). Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies. Morgan and Claypool.
Socher, R. (2014). Recursive Deep Learning For Natural Language Processing and Computer
Vision. Ph.D. thesis, Stanford University.
Socher, R., Bauer, J., Manning, C. D., & Ng, A. Y. (2013). Parsing with Compositional
Vector Grammars. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 455465, Sofia, Bulgaria.
Association for Computational Linguistics.
Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic Compositionality
through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pp. 12011211, Jeju Island, Korea. Association for Computational
Linguistics.
Socher, R., Lin, C. C.-Y., Ng, A. Y., & Manning, C. D. (2011). Parsing Natural Scenes
and Natural Language with Recursive Neural Networks. In Getoor, L., & Scheffer, T.
(Eds.), Proceedings of the 28th International Conference on Machine Learning, ICML
2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 129136. Omnipress.
Socher, R., Manning, C., & Ng, A. (2010). Learning Continuous Phrase Representations
and Syntactic Parsing with Recursive Neural Networks. In Proceedings of the Deep
Learning and Unsupervised Feature Learning Workshop of {NIPS} 2010, pp. 19.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013).
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pp. 16311642, Seattle, Washington, USA. Association for Computational
Linguistics.
Sgaard, A., & Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pp. 231235. Association for
Computational Linguistics.
Sordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y., Mitchell, M., Nie, J.-Y., Gao, J.,
& Dolan, B. (2015). A Neural Network Approach to Context-Sensitive Generation
of Conversational Responses. In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 196205, Denver, Colorado. Association for Computational
Linguistics.
Sundermeyer, M., Alkhouli, T., Wuebker, J., & Ney, H. (2014). Translation Modeling
with Bidirectional Recurrent Neural Networks. In Proceedings of the 2014 Conference
417

fiGoldberg

on Empirical Methods in Natural Language Processing (EMNLP), pp. 1425, Doha,
Qatar. Association for Computational Linguistics.
Sundermeyer, M., Schluter, R., & Ney, H. (2012). LSTM Neural Networks for Language
Modeling.. In INTERSPEECH.
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization
and momentum in deep learning. In Proceedings of the 30th international conference
on machine learning (ICML-13), pp. 11391147.
Sutskever, I., Martens, J., & Hinton, G. E. (2011). Generating text with recurrent neural
networks. In Proceedings of the 28th International Conference on Machine Learning
(ICML-11), pp. 10171024.
Sutskever, I., Vinyals, O., & Le, Q. V. V. (2014). Sequence to Sequence Learning with
Neural Networks. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., &
Weinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 27, pp.
31043112. Curran Associates, Inc.
Tai, K. S., Socher, R., & Manning, C. D. (2015). Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pp. 15561566, Beijing, China. Association for Computational Linguistics.
Tamura, A., Watanabe, T., & Sumita, E. (2014). Recurrent Neural Networks for Word
Alignment Model. In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 14701480, Baltimore,
Maryland. Association for Computational Linguistics.
Telgarsky, M. (2016). Benefits of depth in neural networks. arXiv:1602.04485 [cs, stat].
Tieleman, T., & Hinton, G. (2012). Lecture 6.5RmsProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.
Van de Cruys, T. (2014). A Neural Network Approach to Selectional Preference Acquisition. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2635, Doha, Qatar. Association for Computational
Linguistics.
Vaswani, A., Zhao, Y., Fossum, V., & Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, pp. 13871392, Seattle, Washington, USA. Association for Computational Linguistics.
Wager, S., Wang, S., & Liang, P. S. (2013). Dropout Training as Adaptive Regularization.
In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., & Weinberger, K. Q.
(Eds.), Advances in Neural Information Processing Systems 26, pp. 351359. Curran
Associates, Inc.
Wang, M., & Manning, C. D. (2013). Effect of Non-linear Deep Architecture in Sequence
Labeling.. In IJCNLP, pp. 12851291.
418

fiA Primer on Neural Networks for NLP

Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., & Hao, H. (2015a). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing (Volume 2: Short
Papers), pp. 352357, Beijing, China. Association for Computational Linguistics.
Wang, X., Liu, Y., Sun, C., Wang, B., & Wang, X. (2015b). Predicting Polarities of Tweets
by Composing Word Embeddings with Long Short-Term Memory. In Proceedings of
the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pp. 13431353, Beijing, China. Association for Computational Linguistics.
Watanabe, T., & Sumita, E. (2015). Transition-based Neural Constituent Parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pp. 11691179, Beijing, China. Association for Computational Linguistics.
Weiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured Training for Neural
Network Transition-Based Parsing. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 323333, Beijing,
China. Association for Computational Linguistics.
Werbos, P. J. (1990). Backpropagation through time: What it does and how to do it..
Proceedings of the IEEE, 78 (10), 1550  1560.
Weston, J., Bordes, A., Yakhnenko, O., & Usunier, N. (2013). Connecting Language and
Knowledge Bases with Embedding Models for Relation Extraction. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.
13661371, Seattle, Washington, USA. Association for Computational Linguistics.
Xu, W., Auli, M., & Clark, S. (2015). CCG Supertagging with a Recurrent Neural Network.
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pp. 250255, Beijing, China. Association for Computational
Linguistics.
Yin, W., & Schutze, H. (2015). Convolutional Neural Network for Paraphrase Identification.
In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 901911,
Denver, Colorado. Association for Computational Linguistics.
Zaremba, W., Sutskever, I., & Vinyals, O. (2014). Recurrent Neural Network Regularization.
arXiv:1409.2329 [cs].
Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. arXiv:1212.5701
[cs].
Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. In Proceedings of COLING 2014, the 25th International
419

fiGoldberg

Conference on Computational Linguistics: Technical Papers, pp. 23352344, Dublin,
Ireland. Dublin City University and Association for Computational Linguistics.
Zhang, Y., & Weiss, D. (2016). Stack-propagation: Improved representation learning for syntax. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 15571566. Association for Computational
Linguistics.
Zhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing. In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 12131222, Beijing, China. Association for Computational Linguistics.
Zhu, C., Qiu, X., Chen, X., & Huang, X. (2015a). A Re-ranking Model for Dependency
Parser with Recursive Convolutional Neural Network. In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pp. 11591168, Beijing, China. Association for Computational Linguistics.
Zhu, X., Sobhani, P., & Guo, H. (2015b). Long Short-Term Memory Over Tree Structures.
arXiv:1503.04881 [cs].

420

fiJournal of Artificial Intelligence Research 57 (2016) 113-149

Submitted 03/16; published 09/16

Optimal Partial-Order Plan Relaxation via MaxSAT
Christian Muise

cjmuise@cs.toronto.edu

Department of Computer Science,
Toronto, Ontario, Canada. M5S 3G4

J. Christopher Beck

jcb@mie.utoronto.ca

Department of Mechanical & Industrial Engineering
Toronto, Ontario, Canada. M5S 3G8

Sheila A. McIlraith

sheila@cs.toronto.edu

Department of Computer Science,
Toronto, Ontario, Canada. M5S 3G4

Abstract
Partial-order plans (POPs) are attractive because of their least-commitment nature,
which provides enhanced plan flexibility at execution time relative to sequential plans. Current research on automated plan generation focuses on producing sequential plans, despite
the appeal of POPs. In this paper we examine POP generation by relaxing or modifying
the action orderings of a sequential plan to optimize for plan criteria that promote flexibility. Our approach relies on a novel partial weighted MaxSAT encoding of a sequential
plan that supports the minimization of deordering or reordering of actions. Using a similar
technique, we further demonstrate how to remove redundant actions from the plan, and
how to combine this criterion with the objective of maximizing a POPs flexibility. Our
partial weighted MaxSAT encoding allows us to compute a POP from a sequential plan
effectively. We compare the efficiency of our approach to previous methods for POP generation via sequential-plan relaxation. Our results show that while an existing heuristic
approach consistently produces the optimal deordering of a sequential plan, our approach
has greater flexibility when we consider reordering the actions in the plan while also providing a guarantee of optimality. We also investigate and confirm the accuracy of the
standard flex metric typically used to predict the true flexibility of a POP as measured by
the number of linearizations it represents.

1. Introduction
For an agent to operate effectively in a dynamic world, its behaviour must be flexible in
the face of unexpected changes. In the context of AI planning, there are several approaches
to increase the flexibility of an agent, including giving it the option to select from different
plans (Graham, Decker, & Mersic, 2001) or expanding the applicability of existing plans
through plan generalization (Anderson & Farley, 1988). One example of the former is to
delay committing to the ordering of certain actions in a plan until absolutely necessary,
allowing the agent to dynamically choose how the plan proceeds at execution time (Veloso,
Pollack, & Cox, 1998). This flexibility is precisely what partial-order plans provide.
Partial-order planning reflects a least commitment strategy (Weld, 1994). Unlike a
sequential plan, which specifies a set of actions and a total order over those actions, an
ideal partial-order plan (POP) only specifies those action orderings necessary to achieve the
goal. In doing so, a POP embodies a family of sequential plans  a set of linearizations
c
2016
AI Access Foundation. All rights reserved.

fiMuise, Beck, & McIlraith

all sharing the same actions, but differing with respect to the order of the actions. During
execution, the agent is free to choose the next action to execute from the plan as long
as the chosen action has no preceding actions left to execute. Increasing the number of
linearizations in a plan translates directly to giving the agent more freedom at execution
time. Thus, we typically use the number of linearizations a POP has as a measure of its
flexibility. While the measure of a POPs linearizations is not perfect, it is quite useful as
a proxy for the plans flexibility.
The flexibility afforded by POPs makes them attractive for real-time execution, multiagent task assignment, and a range of other applications (Veloso et al., 1998; Weld, 1994).
Nevertheless, in recent years research on plan generation has shifted away from partial-order
planning towards sequential planning, primarily due to the effectiveness of heuristic-based
forward-search planners. To regain the least commitment nature of POPs, while leveraging
fast sequential plan generation, it is compelling to examine the computation of POPs via
sequential planning technology as is done, for example, in the forward-chaining partial-order
planner POPF (Coles, Coles, Fox, & Long, 2010).
In this paper, we present an alternative approach that first generates a sequential plan
with a state-of-the-art planner, and subsequently relaxes the plan to a minimally constrained POP. Deordering is the process of removing ordering constraints from a plan and
reordering is the process of allowing any arbitrary change to the ordering constraints; both
requiring that the POP remains valid. POP deordering and reordering have been theoretically investigated (Backstrom, 1998), and unfortunately optimal deordering and reordering are NP-hard to compute and difficult to approximate within a constant factor (unless
N P  DT IM E(npoly log n ), Backstrom, 1998). Despite this theoretical impediment, we
find that in practice we can often compute an optimal solution.
The minimum deordering and minimum reordering of a sequential plan cover a natural
aspect of least commitment planning  minimizing the ordering constraints placed on a plan.
Intuitively, a deordering involves removing existing ordering constraints while a reordering
allows both for ordering constraints to be removed as well as new ordering constraints to
be included. Both techniques naturally provide greater flexibility at execution time, as
there is an inverse correlation between the number of ordering constraints and the number
of linearizations in a POP. Reordering may achieve greater flexibility than deordering as
the addition of a new constraint may allow a number of existing ordering constraints to be
removed while still guaranteeing POP validity.
Our approach for computing an optimally relaxed POP is to use a family of novel encodings for partial weighted MaxSAT: an optimal solution to the MaxSAT problem corresponds
to an optimally relaxed POP. Unlike typical SAT-based planning techniques, we represent
an action instance only once, giving us a succinct representation. We empirically compare
our approach to an existing polynomial-time heuristic for relaxing a sequential plan due to
Kambhampati and Kedar (1994) and find that the latter is extremely proficient at computing a minimum deordering, matching the optimal solution in every problem tested. We find,
however, that a minimum reordering can be substantially more flexible than a minimum
deordering, having fewer ordering constraints and far more linearizations.
We also compare the efficiency of our technique with a related approach that uses a
Mixed Integer Linear Programming encoding to compute the minimum reordering (Do &
Kambhampati, 2003) and find that our approach consistently performs better on problems of
114

fiOptimal Partial-Order Plan Relaxation via MaxSAT

non-trivial size. Our approach represents a practical technique for computing a guaranteed
optimal deordering and reordering of a POP.
Using a modern MaxSAT solver to compute maximally flexible solutions provides two
key benefits: (1) the solver can be used as an any-time procedure that computes the optimally flexible reordering of a POP given enough time (where no such technique previously
existed), and (2) computing the optimal deordering of a POP allows us to evaluate the
efficiency of the existing heuristic algorithm.
1.1 Removing Redundant Actions
The generality of encoding allows us to easily define alternative objectives and optimization
criteria. To demonstrate a key aspect of this generality, we extend the characterization
to an orthogonal metric: minimizing the total cost of actions in a plan via the removal of
unnecessary actions. This is a metric commonly used as a measure of plan quality, and
interestingly can be directly at odds with the task of improving a plans flexibility. Here,
we consider one option for combining the two metrics that puts a higher priority on action
cost than the subsequent plan flexibility.
The majority of the encodings, theoretical foundations, and theorems presented in this
paper apply to the more general class of problems that incorporates both metrics. We refer
to a POP that has a minimum action cost (over the actions in the POP), and subsequently
a minimum number of ordering constraints, as a minimum cost least commitment POP
(MCLCP).1 An MCLCP is compelling because it is free of any redundant actions as well as
any redundant ordering constraints: an MCLCP contains only what is relevant to achieve
the goal.
We present the theoretical aspects of the more general MCLCP criterion, but as our
main focus is on maximizing the flexibility of POPs, we focus our experimental evaluation
on deorderings and reorderings exclusively. We leave the further evaluation of our method
for solving MCLCP as compared to plan repair techniques (e.g., Nebel & Koehler, 1995;
Gerevini & Serina, 2000) as a matter for future work.
1.2 Contributions
The following are the main contributions of this paper:
 We introduce a practical method for computing the optimal deordering and reordering of a plan. We accomplish this through a set of novel partial-weighted MaxSAT
encodings, differing by a set of clause schema to define the type of relaxation we desire. We model the encodings after standard partial-order planning concepts, causal
support and threat resolution, which we then draw upon to prove the correctness of
our encodings.
 We propose an extension to least commitment planning, MCLCP, that includes the
total cost of a solution. The optimization focuses first on minimizing the total action
cost before minimizing the number of ordering constraints included in the plan. We
1. Note that minimizing the total action cost in a uniform cost domain is equivalent to minimizing the
number of actions.

115

fiMuise, Beck, & McIlraith

further prove the correctness of our approach that uses a partial weighted MaxSAT
encoding for computing an MCLCP.
 We demonstrate, somewhat surprisingly, that an existing heuristic is extremely proficient at computing optimal deorderings. The existing algorithm produces only deorderings, and it is not theoretically guaranteed to find a minimal, let alone an optimal,
deordering. Nonetheless, we find empirically that the heuristic computes the optimal
deordering in every instance in our suite of benchmarks.
 We demonstrate the efficiency of our approach compared to a previous method that
uses a similar encoding for a different optimization framework. For problems that are
relatively difficult to relax (i.e., take more than a second to compute), our approach
improves on the previous work by solving 22% more of the problems within the given
time bound.
 We establish the empirical connection between the number of linearizations of a POP
and the standard flex measure, which captures a normalized measure of the number
of ordering constraints.
 We demonstrate the impact that the starting solution form will have on the final
relaxed plan. In particular, we consider using two types of layered plans that are
produced by the Mp and POPF planners (Rintanen, 2012; Coles et al., 2010).
 We show that we can achieve greater flexibility, compared to the optimal deordering,
when using the optimal reordering. This result justifies the need for an approach such
as ours to compute a more flexible plan.
The work in this paper extends the conference publications by Muise, McIlraith, and
Beck (2011, 2012). While we provide the full generality of using MCLCP as our base
encoding, in this paper we focus our evaluation on the minimum deordering and reordering
aspects. We have expanded on the theoretical framework of our approach, including proofs
of correctness, and significantly expanded the empirical evaluation.
1.3 Organization
We start by providing, in Section 2, the necessary background and notation for automated
planning and partial weighted MaxSAT. Next, we detail our approach in Section 3, including
both our new MCLCP criterion in Section 3.1 and the family of encodings for the various
optimization criteria in Section 3.2. Finally, we present our evaluation in Section 4 and
conclude with a discussion of related work and summary in Section 5.

2. Preliminaries
In this section, we present the necessary background notation and concepts for our work.
2.1 Classical Planning
Planning is the task of synthesizing a solution that dictates what actions an agent must
take in order to achieve some prescribed goal. In classical planning, we assume the world
116

fiOptimal Partial-Order Plan Relaxation via MaxSAT

is fully known and deterministic (Russell & Norvig, 2009). Classical planning has many
applications that range from robotics to modelling biological processes (Ghallab, Nau, &
Traverso, 2004). The standard approach for synthesizing a classical plan is to perform
search through the state space of a problem, using heuristics to guide the planner towards
a high quality solution. Here, we describe the most common formalism used for specifying
a planning problem: STRIPS (Fikes, Hart, & Nilsson, 1972).
In STRIPS, a planning problem is a tuple  = hF, I, G, Ai where F is a finite set of
fluents, I  F is the initial state, and G  F is the goal state, and A is the finite set of
actions. We characterize an action a  A by the following three sets:
 P RE(a): The fluents that must be true in order for a to be executable.
 ADD(a): The fluents that action a adds to the state.
 DEL(a): The fluents that action a deletes from the state.
For our work, the actions are instantaneous and we adopt the standard model of interleaved concurrency: no two actions can occur simultaneously. As such, we make the
simplifying assumption that for every action a, ADD(a)  DEL(a) = . This can be done
without loss of generality, and simplifies the theoretical results below.
We say that an action a is executable in state s iff P RE(a)  s. The resulting state
after executing action a in state s is defined as:
(
(s \ DEL(a))  ADD(a)
if a is executable in s
def
P(s, a) =
undefined
otherwise
For a planning problem  = hF, I, G, Ai, we associate a cost function c that maps
every action a  A to a non-negative real number: c : A  R+
0.
We will make use of two further items of notation with respect to a set of actions A:
 adders(f ): The set of actions in A that add the fluent f :
{a | a  A and f  ADD(a)}
 deleters(f ): The set of actions in A that delete the fluent f :
{a | a  A and f  DEL(a)}
The most common representation of a solution to a planning problem is a sequential
plan. A sequence of actions ~a = [a1 ,    , an ] is executable if the preconditions of each action
in the sequence are true in the corresponding state, and an executable sequence of actions is
a sequential plan for the problem  = hF, I, G, Ai if executing the actions in ~a in sequence,
when starting in state I, causes the goal to hold in the final state:
G  P(P(   P(I, a1 )    , an1 ), an )
117

fiMuise, Beck, & McIlraith

For readability, we abbreviate the progression of a sequential plan ~a from state s as
P  (s, ~a ). The cost of an action sequence ~a = [a1 , . . . , an ] is the sum of the individual
actions costs:
n
X
c (~a ) =
c (ai )
i=1

Rather than impose a total order on the actions in a plan, a partial-order plan (POP)
specifies a set of ordering constraints over the actions. We define a POP with respect to
a planning problem  as a tuple hA, Oi where A is the set of actions in the plan and O
is a set of ordering constraints between the actions in A (Russell & Norvig, 2009). While
the same action may appear more than once in A, we assume that every element of A is
uniquely identifiable. For the actions a1 , a2  A, we denote the ordering constraint between
a1 and a2 as (a1  a2 )  O and interpret the constraint as action a1 appears before action
a2 in the plan. A total ordering of the actions in A that respects O is a linearization. A
POP provides a compact representation for multiple linearizations. We assume the ordering
constraints O are transitively closed:
a1 , a2 , a3  A, (a1  a2 )  (a2  a3 )  (a1  a3 )
Assuming that O is transitively closed does not change the fundamental structure of the
POP  the set of linearizations remains the same  but it allows us to effectively compare
the flexibility of two POPs that share the same action set.
Similar to the cost of an action sequence, the cost of a POP P = hA, Oi is the sum of
the action costs for the actions in P :
X
c (P ) =
c (a)
aA

To simplify the exposition that follows, we designate two actions in the POP that
represent the initial state and goal state: aI and aG respectively. aI is ordered before
every other action, and aG is analogously ordered after every other action. For a planning
problem  = hF, I, G, Ai, the actions have the following definition:
P RE(aI ) = 

P RE(aG ) = G

ADD(aI ) = I

ADD(aG ) = 

DEL(aI ) = 

DEL(aG ) = 

The inclusion of aI and aG actions allow us to simplify the presentation of many algorithms,
avoiding special checks in the procedure (e.g., we can assume that there will always be a
first and last action in the POP).
Depending on how the POP was constructed, it may include a set of causal links, C.
Each causal link contains a pair of ordered actions, a1 , a2 (a1 may be aI and a2 may be aG ),
p
and a fluent, p, such that a1 achieves p for a2 : denoted as (a1  a2 ). Causal links often
serve as justifications for the ordering constraints in a POP.
Definition 1 (POP Validity: Notion 1). A POP P is valid for a planning problem  if
and only if every linearization of P is a sequential plan for .2
2. Note that notion 1 does not rely on the set of causal links C.

118

fiOptimal Partial-Order Plan Relaxation via MaxSAT

While simple and intuitive, notion 1 is rarely used to verify the validity of a POP because
there may be a prohibitively large number of linearizations represented by the POP. There
is, however, a tractable equivalent notion of POP validity that uses the concepts of causal
links, open preconditions and threats.
For a POP hA, Oi and a set of causal links C, an open precondition is a precondition p
of an action a  A that does not have an associated causal link:
p

@a0  A s.t. (a0  a)  C
If a precondition is not open, we say that it is supported, and we refer to the associated
action in the causal link as the achiever for the precondition. In a typical valid POP there
will only be one supporter for every precondition of an action included in the POP, but we
do not make that restriction in our work to keep the encoding general.
A threat in a POP refers to an action that can invalidate a causal link between two
p
other actions due to ordering constraints (or lack thereof). Formally, if (a1  a2 )  C, we
p
say that the action a3 (distinct from a1 and a2 ) threatens the causal link (a1  a2 ) if the
following two conditions hold:
 We can order a3 between a1 and a2 :
{(a3  a1 ), (a2  a3 )}  O = 
 The action a3 deletes p:
p  DEL(a3 )
The existence of a threat means that a linearization exists that violates some causal
link, and thus may not be executable. With the actions aI and aG included, we have the
following definition that characterizes the second notion of POP validity.
Definition 2 (POP Validity: Notion 2). Given a planning problem , POP P = hA, Oi
and set of causal links C, P is a valid POP for the planning problem  if no action in A
has an open precondition and no causal link in the set C has a threatening action in A.
A causal link structure can be implicitly assessed to verify POP validity in polynomial
time (Nebel & Backstrom, 1994), so the set C is not strictly necessary. However, implicitly
or explicitly, notion 2 requires all actions to be causally supported in a threat-free manner.
We subsequently have the following connection between the two notions of POP validity:
Theorem 1 (POP Validity, due to McAllester & Rosenblitt, 1991). If notion 2 of POP
validity holds, then notion 1 also holds. Additionally, if notion 1 holds for A and O, then
a set of causal links C must exist such that notion 2 holds for hA, Oi and C.
The final concept we use for a POP is based on the well-established metric for measuring
how constrained a POP is (Nguyen & Kambhampati, 2001; Siddiqui & Haslum, 2012): flex
is a measure of how many ordering constraints there are in the POP, normalized by the total
number of potential ordering constraints. The flex tends to 1 as the number of ordering
constraints tends to 0, and vice versa. As per usual, we assume that the set of ordering
constraints is transitively closed.
119

fiMuise, Beck, & McIlraith

Definition 3 (flex ). Given a POP hA, Oi, we define flex as,
|O|
flex (hA, Oi) = 1  P|A|1
i=1

i

P|A|1
We use i=1 i as the denominator instead of the traditional |A|2 as the latter over
counts the number of possible ordering constraints. With our definition of flex , a fully
unordered POP will have a flex value of 1 while a sequential plan will have a flex 0.
We strive to minimize the number of ordering constraints in the transitive closure.
Omitting the transitive closure would amount to optimizing over the transitive reduction
which, as noted by Backstrom (1998), has less appeal because it leads to more long chains
in the plan.
2.2 Deorderings and Reorderings
The aim of least commitment planning is to find flexible plans that allow us to defer decisions
regarding the execution of the plan. Considering only the ordering constraints of a POP,
two important notions for least commitment planning are the deordering and reordering of
a POP. Following Backstrom (1998), we define these formally as follows:
0

0

Definition 4 (Deordering and Reordering). Let P = hA, Oi and Q = hA , O i be two
POPs, and  a STRIPS planning problem:
1. Q is a deordering of P wrt.  iff P and Q are valid POPs for , A = A0 , and O0  O.
2. Q is a reordering of P wrt.  iff P and Q are valid POPs for , and A = A0 .
Recall that we assume the ordering constraints of a POP to be transitively closed, and
every action in a POP is uniquely named (i.e., every repetition of the same action is given
a unique name). A proper deordering is one where the ordering constraints form a proper
subset (i.e., O0 ( O). We define the minimum deordering / reordering as follows:
0

0

Definition 5 (Minimum Deorderings and Reorderings). Let P = hA, Oi and Q = hA , O i
be two POPs, and  a STRIPS planning problem:
1. Q is a minimum deordering of P wrt.  iff
(a) Q is a deordering of P wrt. , and
00

00

(b) there is no deordering hA , O i of P wrt.  s.t. |O00 | < |O0 |.
2. Q is a minimum reordering of P wrt.  iff
(a) Q is a reordering of P wrt. , and
00

00

(b) there is no reordering hA , O i of P wrt.  s.t. |O00 | < |O0 |.
3. Q is a minimal deordering of P wrt.  iff
(a) Q is a deordering of P wrt. , and
(b) there is no proper deordering of Q.
120

fiOptimal Partial-Order Plan Relaxation via MaxSAT

Note that we use cardinality rather than set containment for 1(b) and 2(b) because the
orderings in O0 and O00 need not overlap. We will equivalently refer to a minimum deordering
(resp. reordering) as an optimal deordering (resp. reordering). In both cases, we prefer
a POP that has the smallest set of ordering constraints. In other words, no POP exists
with the same actions and fewer ordering constraints while remaining valid with respect
to . The problem of finding a minimum deordering or reordering of a POP is NP-hard,
and cannot be approximated within a constant factor unless N P  DTIME(npoly log n )
(Backstrom, 1998). There may be many such optimal deorderings or reorderings, but in
our work we do not distinguish them further. We can compute a minimal deordering in
polynomial time by iteratively removing unnecessary ordering constraints (i.e., those that
do not cause the plan to become invalid).
2.3 Previous Approaches
There are many approaches to computing a partial-order plan, and we cover some of the
representative examples here.
2.3.1 Partial-Order Causal Link Algorithms
Traditional methods for producing a partial-order plan follow an approach called partialorder causal link (POCL) planning (Weld, 1994). In POCL planning, modifications are
iteratively made to an incomplete partial-order plan that consists of a set of actions, causal
links, and ordering constraints. A partial-order plan is considered complete if and only if
the conditions for Definition 2 are met.
The key difference between POCL planning and the standard state-based search is that
POCL planning is a search through plan space. In POCL planning search, every node in the
search space constitutes a partial plan, whereas in state-based search every node is a state
of the world; successor nodes are generated by applying actions to the state represented
by the current search node. In contrast, possible modifications to a partial plan represent
the choices available in the POCL planning search procedure. The typical partial plan
modifications include:
1. Add a new action to the partial plan.
2. Order two actions in the partial plan.
3. Create a causal link between two actions in the plan.
POCL planners were popular in the late 1970s, 1980s, and 1990s, starting with Tates
NONLIN planner (Tate, 1976), until forward search techniques such as the one employed
by the FF planner (Hoffmann & Nebel, 2001) led planning research in a new direction. The
most recent POCL planner is VHPOP (Younes & Simmons, 2003), but unfortunately it is
not competitive with the state-of-the-art forward search planners.
2.3.2 POPF
To take advantage of the flexibility afforded by a POP and the search efficiency of forward
state-based planners, Coles et al. introduced the forward-chaining partial-order planner
121

fiMuise, Beck, & McIlraith

POPF (Coles et al., 2010). The idea behind POPF is to restrict the modifications permitted
to the partially completed plan so that a complete state can be easily computed that
represents the truth of fluents after the partial plan is executed. Unlike POCL approaches
that add actions to achieve open preconditions, the actions in POPF are chosen so that
their preconditions are satisfied and heuristically lead to the goal (i.e., in a forward-search
manner). When a new action is added to the plan, it is placed at the end of the plan 
no action already in the incumbent plan can be ordered after the newly added action at
the time of its insertion, but the new action may be left unordered with respect to actions
already in the plan. Further, adding a new action requires that all of its preconditions have
causal links created immediately.
The approach used in POPF leverages the partial-order nature of planning domains
by avoiding some of the unnecessary reasoning about the permutations of unordered actions; ordering constraints are included only as required. Sequential planners may try to
complete the same partial-order plan multiple times with the only change being a different
permutation of unordered actions, and POPF can avoid this situation some of the time by
maintaining the partial-order structure. Further, using recently introduced techniques to
detect repeated states (Coles & Coles, 2016), the planner avoids even more unnecessary
permutations of the action sequences.
Finally, POPF leverages the powerful techniques of forward-search planners by maintaining the complete state of the world that will be reached by the plan. Having this state
information allows for powerful heuristics to be computed efficiently.
2.3.3 Petri Net Unfolding
Predating the work of Coles et al. (2010), an alternative approach to generating partially
ordered plans is via Petri net unfolding (Hickmott, 2008). The general idea is to encode
the evolution of a forward planning system through the repeated unfolding of a carefully
crafted Petri net: a mathematical structure used to model and analyze the dynamics of
discrete distributed systems (Murata, 1989). The unfolding process naturally represents a
parallel or partially ordered plan (Hickmott, Rintanen, Thiebaux, & White, 2007).
In 2009, Hickmott and Sardina detailed a theoretical property of Petri net unfolding for
partial-order plans, noting that the plan resulting from Petri net unfolding is a minimal
deordering or reordering that respects strong independence (Hickmott & Sardina, 2009).
Strong independence is a restriction on the unordered actions in the partial-order plan:
there can be no ambiguity with respect to which action produces a particular fluent. As a
result, if two different actions each produce the same fluent f , they can only be unordered if
neither is required to produce f  either in service of achieving the goal or in service of the
successful execution of some other action in the plan. This restriction makes the deorderings
and reorderings produced by the unfolding more restrictive than the optimal deorderings
or reorderings produced by our approach, as we do not require strong independence.
Similar to the POCL and POPF approaches, Petri net unfolding is exploited to produce
a partial-order plan directly, rather than finding a deordering or reordering of an existing
plan, as we do in this paper.
122

fiOptimal Partial-Order Plan Relaxation via MaxSAT

2.3.4 Relaxer Algorithm
Due to Kambhampati and Kedar (1994), the Relaxer Algorithm3 operates by removing
ordering constraints from a sequential plan in a systematic manner. A heuristic guides the
procedure and, as detailed by Backstrom (1998), the process does not provide any guarantee
that the resulting POP is minimally deordered. There is an error in the counterexample
used by Backstrom to demonstrate that Kambhampati and Kedars algorithm does not
necessarily produce a minimally deordered POP. However, the conclusion is correct and we
provide a new counterexample in Appendix A.
The intuition behind the algorithm is to remove any ordering (ai  ak ) from the sequential plan where ai is not the achiever of some precondition of ak and removing the ordering
does not lead to a threat. The algorithm heuristically attempts to choose the earliest possible action in the sequential plan as the achiever of a precondition. For example, consider
the case where our sequential plan is [a1    , ai ,    , ak ,    , an ] and p  P RE(ak ). The
algorithm will keep the ordering (ai  ak ) only if leaving it out would create a threat for a
precondition of one of the actions, or if ai is the earliest action in the sequence where the
following holds:
1. p  ADD(ai ): ai is an achiever for p
2. aj , i < j < k, p 
/ DEL(aj ): p is not threatened.
Algorithm 1 presents this approach formally. We use index(a,~a) to refer to the index
of action a in the sequence ~a, and assume every action in the plan is uniquely named.
If ~a is a valid plan, line 8 will evaluate to true before either line 11 evaluates to true or
the for-loop at line 6 runs out of actions. That is, we know an unthreatened achiever exists
and the earliest such one is found. The achiever is then ordered before the action requiring
the fluent as a precondition (line 14), and the for-loop at line 16 adds all of the necessary
ordering constraints so the achiever remains unthreatened. Note that for any deleter found
in this for-loop, either line 17 or 19 must evaluate to true. After going through the outer
loop at line 3, every action in the newly formed POP has an unthreatened supporting action
for each of its preconditions. The resulting POP will therefore be valid (cf., Kambhampati
& Kedar, 1994, section 5.2).
2.3.5 SAPA Post-Processing
As part of a post-processing phase for the SAPA planner, Do and Kambhampati (2003)
introduce an approach similar to ours for relaxing the ordering of a plan. In their setting,
they begin with a temporal plan with the actions assigned to specific time points, and the
objective is to optimize either the number of ordering constraints or some temporal aspect
of the resulting plan.
The strategy Do and Kambhampati take (abbreviated as DK here), is to model the task
of computing a partial-order relaxation in terms of a constraint satisfaction optimization
problem (CSOP). Variables are introduced to represent the ordering of actions, the timing
and duration of actions, the resource usage, etc. From the abstract CSOP formalism, a
concrete mixed integer linear program (MILP) is proposed to realize the set of constraints
3. Referred to as order generalization originally.

123

fiMuise, Beck, & McIlraith

Algorithm 1: Relaxer Algorithm

1
2
3
4
5
6
7
8
9
10
11
12

13
14
15
16
17
18
19
20

21

Input: Sequential plan, ~a, including aI and aG
Output: Relaxed Partial-order plan, hA, Oi
A = set(~a);
O = ;
foreach a  A do
foreach f  P RE(a) do
ach = null;
for i = (index(a, ~a)  1)    0 do
// See if we have an earlier achiever
if f  ADD(~a[i]) then
ach = ~a[i];
// Stop if we find a deleter of f
if f  DEL(~a[i]) then
break;
// Add the appropriate supporting link
O = O  {(ach  a)};
// Add orderings to avoid threats
foreach a0  deleters(f ) \ {a} do
if index(a0 , ~a) < index(ach, ~a) then
O = O  {(a0  ach)};
if index(a0 , ~a) > index(a, ~a) then
O = O  {(a  a0 )};
return hA, Oi;

that model a valid temporal plan. Similar to our work, DK contains the option for enforcing adherence to the original ordering constraints which allows either a deordering or a
reordering to be produced.
DK considers a number of optimization criteria including minimizing the makespan,
maximizing the sum of slack in the temporal variables, maximizing the flexibility in the
temporal variables, and minimizing the number of ordering constraints. While the first three
are related to temporal planning domains, the final one coincides with the optimization
criteria of our work. Experimental evaluation is provided for the temporal optimization
criterion, but Do and Kambhampati do not empirically investigate the minimization of
ordering constraints.
Differences between DK and our approach include the formalism (we do not focus on
temporal aspects), the model used (unique to our encoding are variables that represent
an action appearing in the plan and unique to their encoding are variables representing
time points and resources), the underlying solving technology (we rely on partial weighted
MaxSAT instead of MILP), and finally the MCLCP criterion. In Section 4.4 we compare
the efficiency of our approach for computing a minimum reordering with an implementation
of the DK approach that uses only the variables and constraints relevant to computing a
minimum reordering.
124

fiOptimal Partial-Order Plan Relaxation via MaxSAT

2.4 Partial Weighted MaxSAT
To compute a relaxed plan, we encode the task as a partial weighted MaxSAT problem
where a solution to the encoding corresponds to a minimally relaxed plan that optimizes
our desired criteria. Here, we review the notation for partial weighted MaxSAT that we use
throughout the paper.
In Boolean logic, the problem of Satisfiability (SAT) is to find a true/false setting of
Boolean variables such that a logical formula referring to those variables evaluates to true
(Biere, Heule, van Maaren, & Walsh, 2009). Typically, we write problems in Conjunctive
Normal Form (CNF), which is made up of a conjunction of clauses, where each clause is a
disjunction of literals. A literal is either a Boolean variable or its negation. A setting of the
variables satisfies a CNF formula iff every clause has at least one literal that evaluates to
true. For example, setting variables x and z to be true will satisfy the following theory:
(x  y)  (x  z)

(1)

The MaxSAT problem is the optimization variant of the SAT problem in which the
goal is to maximize the number of satisfied clauses (Biere et al., 2009, ch. 19). Although
we cannot satisfy every clause in the following theory, setting x, y to true and z to false
satisfies five clauses:
(x  y  z)  (x  z)  (y  z)  (x  y)  (z  x)  (z  y)

(2)

Adding non-uniform weights to each clause allows for a richer version of the optimization
problem, and we refer to maximizing the weight of satisfied clauses as the weighted MaxSAT
k

problem. We use the syntax (   ) to indicate the clause has a weight of k. Generally, the
weight must be a positive real number. Consider setting x to false and y, z to true in the
following theory:
3

1

1

1

1

(x)  (x  y)  (x  z)  (y)  (z)

(3)

While the setting satisfies four clauses, it only has a total weight of 4. With the aim of
maximizing the total weight of satisfied clauses, we can achieve a sum of 5 by assigning all
variables to true:
3

1

1

1

1

(x)  (x  y)  (x  z)  (y)  (z)

(4)

If we wish to force the solver to find a solution that satisfies a particular subset of the
clauses, we refer to clauses in this subset as hard, while all other clauses in the problem are


soft. The syntax we use to indicate a hard clause is (   ). When we have a mix of hard and
soft clauses, we have a partial weighted MaxSAT problem (Biere et al., 2009, ch. 19.6).
In a partial weighted MaxSAT problem, only the soft clauses are given a weight, and a
feasible solution corresponds to any setting of the variables that satisfies the hard clauses
125

fiMuise, Beck, & McIlraith

in the CNF. An optimal solution to a partial weighted MaxSAT problem is any feasible
solution that maximizes the sum of the weights on the satisfied soft clauses. In the following
example, setting variables x, y to false and z to true satisfies every hard clause and one of
the soft clauses:
1

2

3







(x)  (y)  (z)  (x  z)  (y  z)  (x  y)

(5)

Although not required for partial weighted MaxSAT in general, the encodings we create
will never contain a soft clause that has more than one literal. This special form of partial
weighted MaxSAT problem, referred to as a binate covering problem (Coudert, 1996), allows
us to flip the optimization criterion: minimizing the sum of the satisfied soft (unit) clauses
is equivalent to maximizing the sum of unit clauses that have the literal flipped (e.g., x
goes to x and vice versa). Using this technique to solve the minimization problem with a
partial weighted MaxSAT solver only works if the soft clauses contain a single literal. This
property is key to our encoding, as our objective is always to minimize.

3. Approach
We can view a sequential plan (also referred to as a total-order plan) as a special case of a
partial-order plan where there exists an ordering constraint between every pair of actions.
Quite often, many of these ordering constraints are not required: the ordering of certain
actions may be switched and the goal still achieved with the new sequence of actions. With
the aim of maximizing the flexibility of a POP, we strive to minimize the number of ordering
constraints included in the solution. This objective motivates the need to identify precisely
which ordering constraints in a POP are relevant to the POPs validity.
Definition 6 (Ordering Relevance). Given a planning problem  = hF, I, G, Ai and valid
POP P = hA, Oi for , the ordering constraint o  O is relevant with respect to  and P
iff hA, O  {o}i is not a valid POP for .4
Ordering relevance plays a central role in the definitions of minimal and minimum POP
deorderings: the relevant ordering constraints are precisely those that cannot be removed
without invalidating the POP (Backstrom, 1998). Additionally, the Relaxer Algorithm
of Kambhampati and Kedar (1994) operates by identifying a set of ordering constraints
suspected of being relevant (i.e., those selected as achievers for action preconditions).
To maximize the flexibility of a POP, we focus our encoding on retaining only the relevant
orderings. While difficult to measure efficiently, we strive to maximize the flexibility inherent
in a POP, loosely defined as the number of linearizations a POP represents. The number
of unordered pairs of actions in a POP, typically referred to as flex (Siddiqui & Haslum,
2012), provides an approximation for the POPs flexibility. In our evaluation, we quantify
the accuracy of flex as an approximation for a POPs flexibility.
As we have discussed earlier, verifying a POPs validity by way of the linearizations
is not always practical. Similarly, we will not attempt to compute POPs that maximize
4. Note that the transitive closure of P is necessarily different from the transitive closure of hA, O  {o}i
when o is relevant with respect to  and P .

126

fiOptimal Partial-Order Plan Relaxation via MaxSAT

the number of linearizations, but rather we will compute POPs that adhere to one of the
previously mentioned criteria for removing redundant orderings: minimum deordering or
minimum reordering.
3.1 Minimum Cost Least Commitment Criterion
While the notion of a minimum deordering or reordering of a POP addresses the commitment of ordering constraints, an orthogonal objective is to commit as few resources as
possible  typically measured as either the time for a plan to be executed in parallel or the
sum of action costs for the actions in a plan. Historically, the latter objective takes precedence over all other metrics. To this end, we provide the extended criterion of computing
a minimum cost least commitment POP (MCLCP).
0

0

Definition 7 (Minimum Cost Least Commitment POP). Let P = hA, Oi and Q = hA , O i
be two POPs valid for . Q is a minimum cost least commitment POP (MCLCP) of P iff
00
00
Q is a minimum reordering, A0  A, and there does not exist a valid POP R = hA , O i
for  such that A00  A and the following condition holds:
c (R) < c (Q)  (c (R) = c (Q)  |O00 | < |O0 |)
For this work, we assume that every action in  has positive cost. It may turn out that
preferring fewer actions causes us to commit to more ordering constraints, simply due to the
interaction between the actions we choose. In practice, however, we usually place a much
greater emphasis on minimizing the total cost of a plan. It is also worth noting that if no
plan exists with a proper subset of the actions in the input plan, computing the MCLCP is
equivalent to computing a minimum reordering.
Following the MCLCP criterion, we can evaluate the quality of a POP by the total
action cost and number of ordering constraints it contains; these metrics give us a direct
measure of the least commitment nature of a POP with the primary emphasis placed on
removing the unnecessary commitments to actions.
3.2 Encoding
We encode the task of finding a minimum deordering, reordering, or MCLCP as a partial
weighted MaxSAT problem given an input planning problem and corresponding initial plan.
An optimal solution to the default encoding will correspond to an MCLCP. That is, no
POP exists with a cheaper overall cost or with the same cost and fewer ordering constraints
in the transitive closure. We present this core encoding in Section 3.2.1 and prove the
soundness and completeness of the encoding in Section 3.2.2. We add further clauses to
produce encodings that correspond to optimal deorderings or reorderings, and present these
modifications in Section 3.2.3.
3.2.1 Basic Encoding
In contrast to the typical SAT encoding for a planning problem (e.g., Kautz & Selman,
1999), we do not require that the actions be replicated for successive plan steps. Instead,
we represent each action occurrence only once and reason about the ordering between
actions. The actions in the encoding come from a provided sequential or partial-order plan,
127

fiMuise, Beck, & McIlraith

P = hA, Oi. We use (P ) to denote the partial weighted MaxSAT encoding corresponding
to the POP P = hA, Oi, and refer to the POP corresponding to an encodings solution as
the target POP. A target POP can be reconstructed from an encodings solution by looking
at only the variables set to true. We use three types of propositional variables:
 xa : For every action a in A, xa indicates that action a appears in the target POP.
 (a1 , a2 ): For every pair of actions a1 , a2 in A, (a1 , a2 ) indicates that the ordering
constraint (a1  a2 ) appears in the target POP.
 (ai , p, aj ): For every action aj in A, p in PRE(aj ), and ai in adders(p), (ai , p, aj )
indicates ai supports aj with the fluent p in the target POP.
In a partial weighted MaxSAT encoding there is a distinction between hard and soft
clauses. We first present the hard clauses of the encoding as Boolean formulae which
we subsequently convert to CNF, and later describe the soft clauses with their associated
weights.5 We define the formulae that ensure that the target POP is acyclic, and the
ordering constraints include the transitive closure. Here, actions are universally quantified,
and for formula (9) we assume aI 6= ai 6= aG . We must ensure that:
 There are no self-loops:
((a, a))

(6)

 We include the initial and goal actions:
(xaI )  (xaG )

(7)

 If we use an ordering variable, then we include both actions:
(ai , aj )  xai  xaj

(8)

 An action cannot appear before the initial action (or after the goal):
xai  (aI , ai )  (ai , aG )

(9)

 A solution satisfies the transitive closure of ordering constraints:
(ai , aj )  (aj , ak )  (ai , ak )

(10)

Together, (6) and (10) ensure that the target POP will be acyclic (note that this implies
antisymmetry as well), while the remaining formulae tie the two types of variables together
and deal with the initial and goal actions. Finally, we include the formulae needed to ensure
that every action has its preconditions met, and there are no threats in the solution:


5. For readability, we omit the hard clause symbol, (   ), for constraints (6)-(12).

128

fiOptimal Partial-Order Plan Relaxation via MaxSAT

(ai , p, aj ) 

^

xak  (ak , ai )  (aj , ak )

(11)

ak deleters(p)

^

xaj 

_

(ai , aj )  (ai , p, aj )

(12)

pP RE(aj ) ai adders(p)

Intuitively, (ai , p, aj ) holds if ai is the achiever of precondition p for action aj and
no deleter of p will be allowed to occur between the actions ai and aj ; i.e., it corresponds
directly to an unthreatened causal link. Formula (11) ensures that every causal link remains
unthreatened in a satisfying variable setting, and we can view the two ordering variables
in the formula as a form of the common partial-order planning concepts of promotion and
demotion (Weld, 1994). Formula (12) ensures that if we include action aj in the target POP,
then every precondition p of aj must be satisfied by at least one achiever ai . (ai , aj ) orders
the achiever correctly, while (ai , p, aj ) removes the possibility of a threatening action.
So far, the constraints we have described capture what is required for a POP to be valid.
To go further and address the notion of ordering relevance presented in Definition 6, as well
as the metric of minimizing total action cost with MCLCP, we make use of soft clauses.
To generate an MCLCP, we prefer solutions that first minimize the total action cost, and
then minimize the number of ordering constraints. We add a soft unit clause, containing
the negation of the variable, for every action and ordering variable in our encoding. A
violation of any one of the unit clauses means that the solution includes the action or
ordering constraint corresponding to the violated clauses variable. The weight assigned is
as follows:
1

 ((ai , aj )), ai , aj  A
c (a)+|A|2 +1



(xa )

, a  A \ {aI , aG }

Note that the weight of any single action clause is greater than the weight of all ordering constraint clauses combined, because there can be no more than |A|2 total ordering
constraints. The increased weight guarantees that we generate solutions with a minimum
action cost.6 Because we enforce the transitive closure of the ordering constraints, the second type of soft clause will lead the solver to find a POP (among those with the cheapest
total action cost) that minimizes the size of the transitive closure.
Richer notions, such as a weighted trade-off between the ordering constraints and action
costs, are also easily modelled using an appropriate assignment of weights to the soft clauses
in the encoding. As we focus primarily on the deordering and reordering aspects in this
work, we leave alternative encodings as future work.
3.2.2 Theoretical Results
In this section we present theoretical properties of our core encoding.
6. If we wish to minimize the number of actions in the solution, we need only to replace c (a) with 0.

129

fiMuise, Beck, & McIlraith

Lemma 1 (Variable Setting Implies POP). Given a planning problem  and a valid POP
P = hA, Oi, any variable setting that satisfies the formulae (6)-(12) for (P ) will correspond to a valid POP for  where the ordering constraints are transitively closed.
Proof. We have already seen that the POP induced by a solution to the hard clauses will
be acyclic and transitively closed (due to formulae (6)-(10)). We can further see that there
will be no open preconditions because we include aG , and the conjunction of (12) ensures
that every precondition will be satisfied when the POP includes an action. Additionally,
there are no threats in the final solution because of formula (11), which will be enforced
every time a precondition is met by formula (12). Because the POP corresponding to any
solution to the hard clauses will have no open preconditions and no threats, Theorem 1
allows us to conclude that the target POP will be valid for .
Lemma 2 (POP Implies Variable Setting). Given a planning problem  and a valid POP
0
0
P = hA, Oi, any valid POP Q = hA , O i, where A0  A and O0 is transitively closed, has
a corresponding feasible variable assignment that satisfies (P ).
Proof. The lemma follows from the direct encoding of the POP Q where xa = true iff
a  A0 and (ai , aj ) = true iff (ai  aj )  O0 . If Q is a valid POP, then it will be acyclic,
include aI and aG , have all actions ordered after aI and before aG , and be transitively
closed (satisfying (6)-(10)). We further can see that (11) and (12) must be satisfied: if (12)
did not hold, then there would be an action a in the POP with a precondition p such that
every potential achiever of p has a threat that could be ordered between the achiever and
a. Such a situation is only possible when the POP is invalid, which is a contradiction.
Theorem 2 (Completeness). Given a planning problem  and a valid POP P = hA, Oi, a
complete partial weighted MaxSAT solver will find a solution to the soft clauses and formulae
(6)-(12) for (P ) that minimizes the total cost of actions in the corresponding POP, and
subsequently minimizes the number of ordering constraints.
Proof. Given |A| actions, there can only be |A|2 ordering constraints. Because every soft
clause corresponding to an ordering constraint has a weight of 1, the total sum of satisfying every ordering constraint clause will be |A|2 . Because the weight of satisfying any
action clause is greater than |A|2 , the soft clauses corresponding to actions dominate the
optimization criteria. As such, there will be no valid POP for  which has a subset of the
actions in P with a lower total action cost than a solution that satisfies formulae (6)-(12)
while maximizing the weight of the satisfied soft clauses.
Theorem 3 (Encoding Correctness). Given a planning problem , and a valid POP P for
, a solution to our partial weighted MaxSAT encoding (P ) is an MCLCP for P .
Proof. This Theorem follows directly from Lemmas 1, 2, and Theorem 2.
3.2.3 Variations
Observe that (P ) does not make use of the set of ordering constraints in P . An optimal
solution to the encoding will correspond to an MCLCP, but to enforce solutions that are
minimum deorderings or reorderings, we introduce two additional sets of hard clauses.
130

fiOptimal Partial-Order Plan Relaxation via MaxSAT

All Actions: For optimal deorderings and reorderings, we require every action to be a part
of the target POP. We consider a formula that ensures that we use every action (and so the
optimization works only on the ordering constraints). To achieve this, we simply need to
add each action as a hard clause:


(xa ), a  A

(13)

The soft unit clauses will all be trivially unsatisfiable, and are removed in the preprocessing phase of the MaxSAT solving process. An optimal solution to the soft constraints
and formulae (6)-(13), referred to as M R (P ), corresponds to a minimum reordering of P .
Deordering: For a deordering we must forbid any explicit ordering that contradicts the
input plan. Assuming our input plan is P = hA, Oi, we ensure that the computed solution
is a deordering by adding the following family of hard unit clauses:


((ai , aj )), (ai  aj ) 
/O

(14)

Similar to the introduction of hard unit clauses for action inclusion, using the clauses
from (14) will eliminate a number of ordering constraint soft clauses from the encoding
during the preprocessing phase of the MaxSAT solver. An optimal solution to the soft constraints and formulae (6)-(14), referred to as M D (P ), corresponds to a minimal deordering
of P . We additionally could use (14) and forgo the use of (13), but this variation is not one
typically studied, nor does it provide a benefit over computing an MCLCP.

4. Evaluation
We evaluate the ability and effectiveness of the state-of-the-art partial weighted MaxSAT
solver, Sat4j (Le Berre & Parrain, 2010), to optimally relax a plan using our proposed
encodings.7 We use the MD and MR encodings, which ensure that all actions are always
included in the solution (i.e., using the All Actions constraint (13)). We also investigate
the effectiveness of the Relaxer Algorithm (RX) to produce a minimally constrained deordering. To measure the quality of a POP, we use either its flex value (cf. Section 2), or
the number of linearizations (whenever feasible to compute).
For our analysis, we considered every STRIPS domain from the previous International
Planning Competitions (IPC, Hoffmann, 2016). We discarded two domains (childsnack
and tidybot) due to the difficulty that planners had in generating an initial solution. A
further 18 were discarded due to their constrained nature; the form of which offers little
or no flexibility (any domain with an average flex value of less than 10% was removed).8
Using them in the evaluation would be uninformative since they are already as relaxed as
7. Additionally, we evaluated the 2013 winner of the partial weighted MaxSAT contest for crafted instances,
MaxHS (Davies & Bacchus, 2013), however, we found that Sat4j outperformed MaxHS slightly in both
coverage and time.
8. The 18 overly constrained domains are visitall, blocksworld, sokoban, pegsol, ged, parking, barman, gripper, cybersec, psr-small, storage, nomystery, mystery, mprime, freecell, hiking, floortile, and thoughtful.

131

fiMuise, Beck, & McIlraith

possible, and the solver determined this trivially. We evaluate using only the most recent
version of a domain where multiple problem sets exist, and Table 1 shows the set of 15
domains that we considered throughout our evaluation.
We conducted all experiments on a Linux desktop with a 3.4GHz processor, and each
run of Sat4j was limited to 30 minutes and 4GB of memory. To generate an initial sequential
plan, we used the Mercury planner (Domshlak, Hoffmann, & Katz, 2015); the best performing non-portfolio planner from the most recent satisficing IPC competition. Additionally,
for some of the evaluation, we computed initial solutions using a state-of-the-art SAT-based
planner, Mp (Rintanen, 2012), and a state-of-the-art partial-order planner, POPF (Coles
et al., 2010). Both offer alternative methods that generate an initial partially ordered plan,
and we investigate the impact that the plans structure has on the relaxation process.
We assess various aspects of our approach through four separate experiments. First, we
evaluate the difficulty of computing a feasible solution in addition to the optimal one (we
obtain solutions of increasing quality by using Sat4j in an any-time fashion). Next, we look
at the quality of the POP produced by our encodings as well as the POP produced by the
Relaxer Algorithm. Here, we measure quality both as the flex of the plan in the transitive
closure, and as the number of linearizations in the plan wherever feasible to compute.
We also demonstrate empirically the accuracy of the flex measure as an indicator of the
number of linearizations. Next, we consider the impact that the initial plan form has on
the relaxation, taking into account the starting solution of the three planners. Finally, we
compare our approach for computing a minimum reordering with that of a similar approach
by Do and Kambhampati (2003).
4.1 Solving to Completion
We begin with a brief discussion of the various configurations of our approach and their
coverage, as well as the weaknesses of some of the methods. We report on only the problems
where the planner was able to find a plan within the resource limits.9 Table 1 shows the
following information for every domain:
 The number of problems in the domain is shown in brackets next to the domain name.
 The number of problems solved by each planner is under the Plans column.
 The Solved column indicates the number of plans successfully encoded and solved.
Every problem that could be encoded for MR also could be encoded for MD, and the
MaxSAT solver produced at least one solution for every encoded problem. Further,
every encoded MD problem was solved to completion within the resource limits.
 The MR column indicates the number of encoded MR problems solved to completion.
We must emphasize that it is not the purpose of this evaluation to compare the efficiency
of the three planners (as each have their own strengths and weaknesses). Rather, we consider
the type of plan that each produces as related to relaxing the ordering constraints on the
plan. Consequently, the purpose of Table 1 is to provide insight into which problems are
included in our further analysis, and to bring to light some of the challenges of encoding
and solving the problems to completion.
9. Providing twice the amount of time and memory to the planners did not lead to more problems solved.

132

fiOptimal Partial-Order Plan Relaxation via MaxSAT

Domain

Mercury
Plans Solved

MR

POPF
Plans Solved

MR

Plans

Mp
Solved

MR

airport (50)

32

29

29

24

21

21

32

29

29

depot (22)

21

21

19

10

10

10

20

20

14

driverlog (20)

20

20

16

15

15

15

17

15

10

elevators (20)

20

10

6

1

1

0

0

-

-

logistics (42)

35

9

5

5

5

5

12

7

2

parcprinter (20)

20

20

20

15

15

15

20

20

20

pipesworld (50)

42

42

42

23

23

22

14

14

14

rovers (40)

40

33

22

24

24

21

39

33

27

satellite (36)

35

29

29

13

13

13

26

25

17

scanalyzer (20)

20

17

12

10

7

7

15

15

14

tetris (20)

19

19

19

0

-

-

1

1

1

tpp (30)

30

28

11

13

13

8

20

20

11

transport (20)

20

6

5

0

-

-

0

-

-

woodwork (20)

20

20

20

5

4

4

20

20

20

zenotravel (20)

20

20

20

16

16

16

20

20

16

ALL (460)

394

323

275

174

167

157

256

239

195

Table 1: Per domain solver and relaxation coverage. Values in brackets indicate the benchmark size. The Plans column indicates how many problems the respective planner solved.
The Solved column indicates how many of the solved problems were successfully encoded
and solved: every encoded problem was solvable by both MD and MR, and every MD encoding was solvable to completion. The MR column indicates the number of problems that
were successfully encoded, and MR solved to completion.

When a problem could not be encoded, this was due to the large number of actions in
the plan; typically plans with more than 200 actions caused an issue. In domains where
this is problematic (e.g., elevators, logistics, and transport), we can see that the initial
coverage for the non-sequential planners suffers as well. The problem with encoding plans
that contain many actions is due to the number of transitivity clauses included for formula
(10), which are cubic in the number of actions.
The tetris domain proved extremely difficult for POPF and Mp to solve, although the
number of actions in the plans for Mercury were small enough to encode. Finally, we found
that proving the optimality of the MR encoding for tpp and rovers was the most difficult,
but there is no clear indication as to why: rovers has high flex , but not as high as other
domains, and the opposite is true for tpp. In both domains, however, good initial plans
were produced quickly by Sat4j, and the solver devoted the remaining time to making small
improvements and proving optimality.
133

fiMuise, Beck, & McIlraith

Figure 1: The number of problems solved to completion by Sat4j if given a limited amount
of time per problem, as well as the number of problems solved by the RX algorithm. Every
MD encoding was solved completely by Sat4j, and RX is a polynomial sub-optimal technique
shown only for comparison of solve time.

Mercury solved a strict superset of the problems solved by POPF and Mp. As such, we
use the sequential plans produced by Mercury as input for the majority of our evaluation
(Section 4.3 is the one exception). Figure 1 provides a view on how long it took for Sat4j
to optimally solve the MD and MR encodings from the initial plans Mercury produced: we
show the number of problems solved optimally as a function of time (including the encoding
phase). For comparison, we include the aggregate time for RX as well. The strong run-time
performance of RX is to be expected given that it is a polynomial time algorithm without
optimality guarantees.
4.2 Plan Quality
To begin, we discuss a surprising result for the Relaxer algorithm on the planning benchmarks. In every one of the 323 problems where Sat4j solved the MD encoding optimally, the
POP that was produced with the Relaxer algorithm contained the same number of ordering
constraints. Even though theoretically the Relaxer algorithm is not guaranteed to find a
minimal POP, it nonetheless computes a minimum deordering in every tested problem. RX
can produce only deorderings, and so this is the best RX could hope to achieve. Note that
there may be many candidates for a minimum deordering, and RX does not necessarily find
the same one that the MD encoding finds.
Next, we consider the difference in quality between the minimum deordering and minimum reordering. Quality is measured by the flex of the transitive closure of the generated
POP, and we include only those problems where both the MD and MR encodings can be
134

fiOptimal Partial-Order Plan Relaxation via MaxSAT

solved to completion by Sat4j (275 in total from the plans generated by Mercury). Table
2 shows the average flex for MD and MR in all domains, and Figure 2 shows the flex
comparison on a per-problem basis over all domains.
Domain

MD

MR

airport
depot
driverlog
elevators
logistics
parcprinter
pipesworld
rovers
satellite
scanalyzer
tetris
tpp
transport
woodwork
zenotravel

0.28
0.31
0.33
0.31
0.56
0.76
0.16
0.68
0.39
0.31
0.49
0.37
0.51
0.96
0.32

0.37
0.36
0.34
0.32
0.58
0.76
0.16
0.69
0.39
0.31
0.50
0.38
0.51
0.96
0.32

Table 2: Average flex

Figure 2: MD versus MR flex Comparison
135

fiMuise, Beck, & McIlraith

Domains where we see substantial improvement include airport and depot. Domains
that saw zero gain in terms of flexibility include parcprinter, transport, and woodwork. In
total, almost one third (76/275) of the problems showed an improvement in flex for the MR
over the MD by varying degrees.10
The flex value fails to convey the extreme amount of execution flexibility introduced by
the relaxations. To investigate this further, we computed the number of linearizations for
the plans wherever feasible. Determining the number of total orders for a partially ordered
graph is #P-Complete (Brightwell & Winkler, 1991), and in practice it is difficult to compute
precisely for many graphs. We were able to compute the number of linearizations for both
the MD and MR solutions in a total of 203 problems where a solution to both encodings was
computed. We found that approximately one quarter (51/203) showed a difference in the
number of linearizations, and we plot the ratio #Linears(MR) / #Linears(MD) for these
51 problems in Figure 3.

Figure 3: Ratio of Linearizations. The y-axis represents the number of linearizations induced
by the POP for the optimal reordering divided by the number of linearizations induced by
the POP for the optimal deordering. The x-axis ranges over all problems where the number
of linearizations differed (25%), and is sorted based on the y-axis.
At its most extreme, the improvement in the number of linearizations can be massive;
over 13 orders of magnitude in one airport problem. Conversely, we see an interesting
artefact resulting from optimizing a metric which acts as a proxy for the number of linearizations: while the flex value of MR will never be lower than that of MD, the POPs
produced by each approach using flex as an optimization criterion can have the opposite
effect in the number of linearizations.
10. Many of the smaller improvements do not show up in the scatter plot.

136

fiOptimal Partial-Order Plan Relaxation via MaxSAT

In three problems (one from tetris and two from depot), we found that the number of
linearizations in the POP produced from the MR encoding was fewer than the number of
linearizations in the POP produced from the MD encoding. While the number of ordering
constraints in a POP for a given number of actions is usually indicative of the number of
linearizations for that POP, these three problems indicate that this is not a universal rule.
For a concrete example, consider two POPs on four actions A = {a1 , a2 , a3 , a4 }. Ignoring
causal links, Figure 4 shows the structure of the POPs P1 and P2 . Both POPs have the
same number of actions and ordering constraints, but the number of linearizations differ:
P1 has 6 linearizations while P2 only has 5. These POPs serve as a basic example of how
the flex criterion does not capture fully the notion of POP flexibility that we use in our
work. There may be similar notions that do take such differences into account, and they
are left for future investigation (see Say, Cire, and Beck (2016) for some recent work in this
direction).

a2
a1

a3

a1

a4

a3

(a) P1

a2

a4
(b) P2

Figure 4: Two POPs with the same number of actions and ordering constraints, but a
different number of linearizations.

To demonstrate the correlation between a POPs flex and the number of linearizations,
we focused on random partial orders for a POP with 20 actions (not including the special
actionis aI and aG ).11 We constructed 10,000 random partial orders (8,959 of them unique)
with a spread of flex value from 0.0 to 1.0, and subsequently we computed the corresponding
number of linearizations in every POP. 100 POPs were constructed for each target flex value
(taken in 0.01 increments), and the method of construction was to iteratively add new edges
not present in the transitive closure until the POP reached the target flex value.
Qualitatively, the POPs resembled those found using planning techniques. The reason
we use randomly generated plans is due to the number of examples required for the trend to
present itself (comparing plans with a varying number of actions was uninformative). Figure
5 shows the flex as a function of the number of linearizations normalized by the total number
of linearizations possible (in all POPs, this equals 20! which is roughly 2.4  1018 ).
The Pearson correlation coefficient between the log of the normalized linearization count
and the flex value is 0.991, and this clear trend ties together the flex of a POP and the
number of linearizations. The red line in Figure 5 is the line-of-best-fit when using a log scole
for the linf lex values (as the plot x-axis does). Interestingly, if we were to use this line as a
11. Similar results hold for random POPs with a different number of actions.

137

fiMuise, Beck, & McIlraith

Figure 5: Comparison of the normalized number of linearizations and the flex value of
approximately 10,000 random POPs with 20 actions. Every point represents a unique
POP with 20 actions. The linf lex value is computed by normalizing the total number of
linearizations by all those possible (20!), and note that the x-axis uses a log scale. The red
line is the line-of-best-fit when using the log of linf lex.
predictor for the number of linearizations, flex overestimates the number of linearizations of
highly constrained plans and underestimates the number of linearizations for unconstrained
plans. Though minimizing the number of ordering constraints in the transitive closure of a
POP is not what we want to optimize directly, it does serve as a highly informative proxy
for maximizing the number of linearizations for the POP.
4.3 Initial Plan Impact
Different planning techniques generate solutions of varying forms. While sequential planners
are by far the most widely used, there are other planners that create inherently partially
ordered solutions. For example, the POPF planner uses a forward-chaining approach that
results in a partial-order plan that is represented as layered sets of unordered actions.
Similarly, SAT-based planners such as Mp produce solutions that contain layers of unordered
actions. The two approaches fundamentally differ in how they search for a solution, and are
again fundamentally different from how a sequential planner searches. One question that
arises from these differences is whether or not they lead to fundamentally different solutions;
amenable to relaxing in different ways. We investigate the impact that the starting solution
will have on the relaxed solution quality and the ability to compute an optimal solution.
The encoding for a minimum reordering does not take into account the original sequence
of actions. Therefore, this encoding can be used without modification for the plans produced
138

fiOptimal Partial-Order Plan Relaxation via MaxSAT

by POPF and Mp. In a similar sense, the plans produced by POPF and Mp can be encoded
for a minimum deordering by carefully applying equation 14: O will include a link between
every pair of actions that do not share the same layer. We obtained the layered plan
representation from POPF and Mp directly using the appropriate planner settings.
Across all domains, 287 problems were mutually solved to completion by Sat4j using
the solutions produced by all three planners and either the MD or MR encoding. Only 78
of those contained the same number of actions. Figure 6 shows the time that Sat4j required
to solve the problem to completion for Mercurys plans measured against the plans for the
other two planners. The first plot shows all 287 problems mutually solved, and we see a
performance improvement for solutions coming from the Mercury solver.12 However, when
we limit ourselves to just the 78 problems that contain the same number of actions in all
solutions, we find that the Sat4j solve-time is much more comparable to that of the other
solvers. Thus, there appears to be little effect on the solving efficiency based on the input
solution format. The primary factor in Sat4j solve time is the number of actions represented
in the encoding.

(a) All problems mutually solved by Sat4j using a (b) The subset of mutually solved problems that consource plan from each planner.
tain the same number of actions.

Figure 6: Comparison of the time to relax a Mercury plan versus the time to relax a POPF
or Mp plan. Both MD and MR encodings are included in the data.
In addition, we investigated the resulting flex of the produced POPs. Of the 78 problems
mutually solved with the same number of actions, only two (from the scanalyzer domain)
contained a different set of actions  these resulted in slightly higher flex values for the
minimum deordering and reordering of Mercurys solution compared to the other planners.
On the other hand, six problems from the airport domain had a lower flex value for the
minimum deordering of Mp solutions despite having the same number of actions. This
indicates that under some conditions, the initial layered plan produced by Mp may not
allow for as much relaxation compared to a forward search planner such as Mercury or
POPF. We should note, however, that in the vast majority of problems the flex from the
minimum deordering or reordering coincided across all initial plan types.
12. Note that the time does not include initial planner computation; only the time to encode and solve the
MaxSAT encoding.

139

fiMuise, Beck, & McIlraith

Finally, we investigated the improvement in flex for each planner compared to its initial
solution. For Mercury, the initial flex value is always 0, as it is a sequential planner. Because
a reordering is allowed to ignore all of the original ordering constraints, we consider only
the improvement in flex for the minimum deordering of plans coming from POPF and Mp.
Figure 7 shows the relative flex comparison between the original plan and the minimum
deordering that was computed. In these plots, we include every problem solved successfully
to completion by Sat4j for plans produced by POPF (167) and Mp (219).

(a) flex improvement for POPF

(b) flex improvement for Mp

Figure 7: Comparison of the original plan flex versus the flex of the minimum deordering.
We found that the majority of initial flex values for both Mp and POPF solutions fell
within the range of 0 to 0.2, and the difference in flex between the solutions from each solver
was minimal. There was a moderate correlation between the original and final flex value:
Mp and POPF solutions had a Pearson correlation coefficient of 0.57 and 0.42 respectively.
However, we observed no distinction between relaxing Mp solutions versus those of POPF
in either the time to compute the relaxation, or the flex of the final POP.
4.4 Comparison to MILP Encoding
The model for relaxing the ordering of a plan that is presented by Do and Kambhampati
(2003) involves temporal constraints and resources  both are aspects beyond what we
consider here. Nevertheless, a fragment of the model is capable of computing either the
minimum deordering or reordering of the plan, and so it is worthwhile to see how effective
it can be in finding an optimal reordering. We forgo testing the previous work for computing
the optimal deordering, as the Relaxer Algorithm is so effective in doing so. We should note
that Do and Kambhampati only considered using the model to heuristically guide the solver
to a reasonable solution instead of an optimal one.
The optimization framework Do and Kambhampati use to model the problem of relaxing
the ordering of a plan is Mixed Integer Linear Programming (MILP). A MILP consists of
a set of linear constraints that are defined over variables that can take on integer or real
values. The optimization criterion is specified as a weighted linear combination over a subset
of the variables in the problem that should either be maximized or minimized. We do not
140

fiOptimal Partial-Order Plan Relaxation via MaxSAT

need to go into further detail, as the MILP model presented in this section is quite basic
and uses only integer variables for the encoding.
Here, we present a version of the MILP model introduced by Do and Kambhampati
for comparison to our partial weighted MaxSAT model. The modifications fall under three
categories: (1) fixes for bugs in the original formulation, (2) removal of variables and constraints not relevant to our setting (i.e., the temporal and resource related portions of the
model), and (3) adding constraints to enforce that a solution is the transitive closure. The
variables we use for the model include the following:

Xafj ,ai

(
1
=
0

when ai supports aj with fluent f
otherwise

Yafi ,aj

(
1
=
0

when ai is ordered before aj due to interference on fluent f
otherwise

Oai ,aj

(
1
=
0

when ai is ordered before aj
otherwise

Note that Xafj ,ai and Oai ,aj are analogous to (ai , f, aj ) and (ai , aj ) respectively. The
interference variables Yafi ,aj are defined only for those cases where aj can conflict with
the execution of ai on fluent f : either f  (P RE(ai )  ADD(ai ))  DEL(aj ) or f 
(P RE(aj )  ADD(aj ))  DEL(ai ) holds. The constraints for the MILP model are as follows
(unbound variables are assumed to be universally quantified).
 Interfering actions must be ordered (defined only for pairs of actions that interfere):
Yafi ,aj + Yafj ,ai = 1
 Every precondition is supported exactly one way:13
X
f  P RE(aj ),
Xafj ,ai = 1
ai adders(f )

 Every support is threat free:
ad  deleters(f ), (1  Xafj ,ai ) + (Yafd ,ai + Yafj ,ad )  1
 Support implies ordering:
Oai ,aj  Xafj ,ai  0
13. The original paper had this constraint erroneously listed as

141

P

ai adders(f )

Xafi ,aj = 1.

fiMuise, Beck, & McIlraith

 Interference implies ordering:
Oai ,aj  Yafi ,aj  0
 Enforce the transitive closure of ordering constraints:
(1  Oai ,aj ) + (1  Oaj ,ak ) + Oai ,ak  1
 Forbid self loops in the ordering:
Oa,a = 0
 Order everything after the initial state action and before the goal action:
OaI ,a = 1
Oa,aG = 1
The final three constraints do not appear in the original model. The last one replaces
constraints that referenced temporal variables to achieve the same effect, and the first two
ensure that a solution is transitively closed. As mentioned earlier, optimizing the transitive closure is preferred over optimizing the transitive reduction. Finally, the optimization
criterion for the MILP model is as follows.
M inimize

X

Oa1 ,a2

a1 ,a2 A

The above model will produce reorderings of the input plan as feasible solutions, and
will find a minimum reordering if solved to completion. We implemented the MILP model
using the state-of-the-art MILP solver Gurobi (version 5.6.2) (Gurobi Optimization, Inc.,
2015), and measured the coverage over all domains as a function of time. Figure 8 contains
the results.
We found that using the MILP model was effective for the easier problems (those solved
in under 2 seconds), but for anything more difficult, solving the partial weighted MaxSAT
encoding with Sat4j proved more efficient. Overall, 275 problems were solved using Sat4j on
the partial weighted MaxSAT encoding while only 226 problems were solved using Gurobi
on the MILP model.
We additionally tested a MILP model that mirrors the partial weighted MaxSAT encoding presented above. However, the results were very similar to those shown in Figure
8, with the MILP encoding being consistently outperformed for problems that take more
than a second to solve.

5. Discussion
In this paper, we proposed a practical method for computing the optimal deordering and
reordering of a sequential or partial-order plan. Despite the theoretical complexity of computing the optimal deordering or reordering being NP-hard, we are able to compute the
142

fiOptimal Partial-Order Plan Relaxation via MaxSAT

Figure 8: For a given timeout (x-axis), the number of problems solved to completion within
that timeout bound (y-axis) by (1) Sat4j using the MR encoding and (2) Gurobi using the
MILP encoding described in the text.

optimal solution by leveraging the power of modern MaxSAT solvers. We further proposed
an extension to the classical least commitment criteria of minimal deordering and reordering: a minimum cost least commitment POP (MCLCP). An MCLCP considers the total
cost of actions in a solution before minimizing the number of ordering constraints. Central
to the encodings we propose is a notion of ordering relevance: we designed the optimization
criteria to minimize the ordering constraints in the resulting plan, leaving only those that
are relevant for plan validity.
Our approach uses a family of novel encodings for partial weighted MaxSAT, where a
solution corresponds to an optimal POP satisfying one of the three least commitment criteria
we investigate: minimum deordering, minimum reordering, and our proposed minimum cost
least commitment POP. We solve the former two encodings with a state-of-the-art partial
weighted MaxSAT solver, Sat4j, and find that the majority of problems are readily handled
by the MaxSAT solver in a reasonable amount of time.
We considered various input plan formats, as well as a similar encoding for optimizing
plan flexibility, and found that using a sequential plan as input to our encodings was the
most effective solution for computing a reordering; perhaps surprisingly, there was no benefit
observed when using a planner that naturally generates partial orders (Mp).
We also investigated an existing polynomial algorithm for deordering sequential plans
and discovered that it successfully computes the optimal deordering in every problem we
tested, despite its lack of theoretical guarantee. Because the algorithm is fast in practice,
it is well suited for relaxing a POP if we require a deordering. Finally, we also established
a strong empirical correspondence between the commonly used flex metric and the number
of linearizations represented by a POP.
143

fiMuise, Beck, & McIlraith

Here, we discuss related work and conclude with a discussion of potential future work.
5.1 Related Work
In Section 2.3, we detailed a variety of approaches that naturally produce partial-order
plans. Here, we review other work related to aspects of our approach.
The standard SAT-based planning encodings also produce a POP (Kautz, McAllester,
& Selman, 1996), but a significant difference between the standard encodings and our work
is that we avoid encoding an action in every layer in a planning graph by appealing to the
fact that we already know the (superset of) actions in the solution. Intuitively, we can
view the encoding as using MaxSAT to find the implicit layers for the actions in our plan
by way of computing the relevant ordering constraints. An additional difference is that
choosing a layer for every action unnecessarily restricts the timing of that action when it
can potentially appear in multiple adjacent layers.
The notion of MCLCP is related to that of plan repair (Nebel & Koehler, 1995; Gerevini
& Serina, 2000). A key difference, however, is that we do not consider the addition of new
actions  the cost of a plan is only improved by removing actions for MCLCP. As the focus
of this paper is on improving the flexibility of POPs, we forgo a full theoretical and empirical
comparison of the MCLCP criterion and the existing plan repair techniques. Preliminary
results on the effect of MCLCP as an action removal technique can be found in our previous
work on the subject (Muise et al., 2012; Muise, 2014).
Our core encoding is similar to the causal encodings of Kautz et al. (1996) and Variant-II
of Robinson, Gretton, Pham, and Sattar (2010). We similarly encode the ordering between
any pair of actions as a variable ((ai , aj ) in our case), but rather than encoding every
potential action occurrence or modelling a relaxed planning graph, we encode the formulae
that must hold for a valid POP on the specific set of actions provided as part of the input.
As mentioned in Section 2.3, there are also similarities between our work and that of Do and
Kambhampati (2003). In particular, the optimization criterion of minimizing the number of
ordering constraints coincide, as does the optional use of constraints to force a deordering.
While Do and Kambhampati focus on temporal relaxation in the context of action ordering,
we take the orthogonal view of minimizing the total action cost.
5.2 Conclusion
The use of our method for computing optimally relaxed plans provides two key advantages:
(1) if maximizing flexibility is paramount, then solving the MR encoding can lead to far
more flexible solutions than the MD encoding or Relaxer Algorithm can achieve, and (2)
the optimal deordering provides a useful baseline for demonstrating the effectiveness of the
Relaxer Algorithm. Our work leaves open the possibility for a heuristic approach similar to
the Relaxer Algorithm that is capable of producing reorderings of a partial-order plan.
One extension of our work is to consider alternative forms of optimization criteria. For
example, one may change the soft clauses so as to minimize the number of fluents from
the initial state that are required for plan validity. Doing so has the potential to improve
planning formalisms that attempt to minimize the reliance on information about the initial
state, such as assumption-based planning (Davis-Mendelow, Baier, & McIlraith, 2013).
Alternatively, the initial set of actions need not correspond directly to a plan. As long as a
144

fiOptimal Partial-Order Plan Relaxation via MaxSAT

subset of actions can achieve the goal, then we will compute a plan. This opens the door
to techniques for optimizing plans by adding more actions to select from, using techniques
such as those introduced by Davies, Pearce, Stuckey, and Sndergaard (2014).

Acknowledgments
The authors gratefully acknowledge funding from the Ontario Ministry of Innovation and
the Natural Sciences and Engineering Research Council of Canada (NSERC). Thanks also
go to the anonymous reviewers for their thoughtful feedback during the review process.

Appendix A. Relaxer Counterexample
The Relaxer Algorithm presented in Section 2.3.4 deorders an input plan, but as pointed
out by Backstrom (1998), the resulting POP may not be a minimal deordering. The counterexample provided by Backstrom, however, incorrectly states that the resulting POP is
not minimally deordered (Backstrom, 1998, Figure 14), when in fact Figure 14(b) is not a
deordering of 14(a), and thus 14(a) is a minimal deordering (although not a minimum reordering). Here, we present a new counterexample that supports the claim that the Relaxer
Algorithm may not produce a minimum deordering.
Both the domain theory and problem specification are shown in Figure 9. The input plan
is the sequence of actions [a1 , a2 , a3 ]. Because the Relaxer Algorithm seeks out the earliest
achiever for every precondition, the algorithm results in a deordering of the plan that has
two ordering constraints: (a1  a3 ) and (a2  a3 ). The problem with the deordering is that
a1 is chosen as the achiever for the fluent p, when in fact a2 can be used as the achiever for
both p and q (note that a2 is already required for fluent q).
The weakness of the Relaxer Algorithm is that it uses the earliest achiever. This weakness surfaces when an action later in the plan can be used as an achiever is already ordered
appropriately. Using this insight, there may be a modification of the Relaxer Algorithm that
finds achievers already ordered appropriately, as opposed to finding the earliest achiever.

145

fiMuise, Beck, & McIlraith

(define (domain counterexample)
(:requirements :strips)
(:predicates (p) (q) (g1) (g2) (g3) )
(:action a1
:parameters()
:precondition ()
:effect (and (g1) (p)))
(:action a2
:parameters()
:precondition ()
:effect (and (g2) (p) (q)))
(:action a3
:parameters()
:precondition (and (p) (q))
:effect (and (g3))))
(define (problem counterexample-problem)
(:domain counterexample)
(:init ())
(:goal (and (g1) (g2) (g3) )))
Figure 9: Counterexample Domain and Problem Description

References
Anderson, J. S., & Farley, A. M. (1988). Plan abstraction based on operator generalization.
In 7th International Conference on Artificial Intelligence, pp. 100104.
Backstrom, C. (1998). Computational aspects of reordering plans. Journal of Artificial
Intelligence Research, 9 (1), 99137.
Biere, A., Heule, M., van Maaren, H., & Walsh, T. (2009). Handbook of satisfiability,
frontiers in artificial intelligence and applications. IOS Press.
Brightwell, G., & Winkler, P. (1991). Counting Linear Extensions is #P-Complete. 23rd
Annual ACM Symposium on Theory of Computing, 8 (3), 175181.
Coles, A., & Coles, A. (2016). Have I Been Here Before? State Memoization in Temporal
Planning. In 26th International Conference on Automated Planning and Scheduling,
pp. 97105.
Coles, A., Coles, A., Fox, M., & Long, D. (2010). Forward-chaining partial-order planning.
In 20th International Conference on Automated Planning and Scheduling, pp. 4249.
Coudert, O. (1996). On solving covering problems. In 33rd Annual Design Automation
Conference, pp. 197202.
146

fiOptimal Partial-Order Plan Relaxation via MaxSAT

Davies, J., & Bacchus, F. (2013). Postponing optimization to speed up MAXSAT solving. In
19th International Conference on Principles and Practice of Constraint Programming,
pp. 247262.
Davies, T. O., Pearce, A. R., Stuckey, P. J., & Sndergaard, H. (2014). Fragment-based
planning using column generation. In Proceedings of the 24th International Conference
on Automated Planning and Scheduling, pp. 8391.
Davis-Mendelow, S., Baier, J. A., & McIlraith, S. A. (2013). Assumption-based planning:
Generating plans and explanations under incomplete knowledge. In Proceedings of the
27th AAAI Conference on Artificial Intelligence, pp. 209216.
Do, M. B., & Kambhampati, S. (2003). Improving the temporal flexibility of position
constrained metric temporal plans. In AIPS Workshop on Planning in Temporal
Domains.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: A new systematic
approach to partial delete relaxation. Artificial Intelligence, 221, 73114.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot
plans. Artificial intelligence, 3 (1), 251288.
Gerevini, A., & Serina, I. (2000). Fast plan adaptation through planning graphs: Local and
systematic search techniques. In Proceedings of the 5th International Conference on
Artificial Intelligence Planning Systems, pp. 112121.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning: Theory & Practice.
Morgan Kaufmann Publishers.
Graham, J. R., Decker, K. S., & Mersic, M. (2001). DECAF - A Flexible Multi Agent
System Architecture. Autonomous Agents and Multi-Agent Systems, 7 (1), 727.
Gurobi Optimization, Inc. (2015). Gurobi optimizer reference manual..
Hickmott, S., Rintanen, J., Thiebaux, S., & White, L. B. (2007). Planning via petri net
unfolding. In 20th International Joint Conference on Artificial Intelligence, pp. 1904
1911.
Hickmott, S., & Sardina, S. (2009). Optimality properties of planning via Petri net unfolding: A formal analysis. In 19th International Conference on Automated Planning and
Scheduling, pp. 170177.
Hickmott, S. L. (2008). Directed unfolding: reachability analysis of concurrent systems &
applications to automated planning. Ph.D. thesis, University of Adelaide.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14 (1), 253302.
Hoffmann, J. (2016). ICAPS competition page. http://ipc.icaps-conference.org/.
Accessed: 2016-09-06.
Kambhampati, S., & Kedar, S. (1994). A unified framework for explanation-based generalization of partially ordered and partially instantiated plans. Artificial Intelligence,
67 (1), 2970.
147

fiMuise, Beck, & McIlraith

Kautz, H. A., McAllester, D. A., & Selman, B. (1996). Encoding plans in propositional
logic. In 5th International Conference on the Principles of Knowledge Representation
and Reasoning, pp. 374384.
Kautz, H. A., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In 16th
International Joint Conference on Artificial Intelligence, pp. 318325.
Le Berre, D., & Parrain, A. (2010). The Sat4j library, release 2.2 system description. Journal
on Satisfiability, Boolean Modeling and Computation, 7, 5964.
McAllester, D. A., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings
of the 9th National Conference on Artificial Intelligence, pp. 634639.
Muise, C. (2014). Exploiting Relevance to Improve Robustness and Flexibility in Plan Generation and Execution. Ph.D. thesis, University of Toronto.
Muise, C., Mcilraith, S. A., & Beck, J. C. (2011). Optimization of partial-order plans via
MaxSAT. In ICAPS Workshop on Constraint Satisfaction Techniques for Planning
and Scheduling Problems, COPLAS.
Muise, C., McIlraith, S. A., & Beck, J. C. (2012). Optimally relaxing partial-order plans with
MaxSAT. In 22nd International Conference on Automated Planning and Scheduling,
pp. 358362.
Murata, T. (1989). Petri nets: Properties, analysis and applications. Proceedings of the
IEEE, 77 (4), 541580.
Nebel, B., & Backstrom, C. (1994). On the computational complexity of temporal projection, planning, and plan validation. Artificial Intelligence, 66 (1), 125160.
Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: A theoretical and
empirical analysis. Artificial Intelligence, 76 (1-2), 427454.
Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning. In Proceedings
of the 17th International Joint Conference on Artificial Intelligence, pp. 459466.
Rintanen, J. (2012). Planning as satisfiability: Heuristics. Artificial Intelligence, 193, 4586.
Robinson, N., Gretton, C., Pham, D. N., & Sattar, A. (2010). Partial weighted MaxSAT
for optimal planning. In 11th Pacific Rim International Conference on Artificial
Intelligence, pp. 231243.
Russell, S. J., & Norvig, P. (2009). Artificial intelligence: a modern approach. Prentice hall.
Say, B., Cire, A. A., & Beck, J. C. (2016). Mathematical programming models for optimizing
partial-order plan flexibility. In 22nd European Conference of Artificial Intelligence
(In Press).
Siddiqui, F. H., & Haslum, P. (2012). Block-structured plan deordering. In Australasian
Conference on Artificial Intelligence, pp. 803814.
Tate, A. (1976). Project planning using a hierarchic non-linear planner. In D.A.I. Research
Report No. 25. Department of Artificial Intelligence, University of Edinburgh.
Veloso, M. M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring for planning
in dynamic environments. In 4th International Conference on Artificial Intelligence
Planning Systems, pp. 171180.
148

fiOptimal Partial-Order Plan Relaxation via MaxSAT

Weld, D. S. (1994). An introduction to least commitment planning. AI Magazine, 15 (4),
2761.
Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: versatile heuristic partial order
planner. Journal of Artificial Intelligence Research, 20, 405430.

149

fiJournal of Artificial Intelligence Research 57 (2016) 229271

Submitted 03/16; published 10/16

Goal Probability Analysis in MDP Probabilistic Planning:
Exploring and Enhancing the State of the Art
Marcel Steinmetz
Jorg Hoffmann

STEINMETZ @ CS . UNI - SAARLAND . DE
HOFFMANN @ CS . UNI - SAARLAND . DE

Saarland University,
Saarland Informatics Campus,
Saarbrucken, Germany

Olivier Buffet

OLIVIER . BUFFET @ LORIA . FR

INRIA / Universite de Lorraine / CNRS,
Nancy, France

Abstract
Unavoidable dead-ends are common in many probabilistic planning problems, e.g. when actions may fail or when operating under resource constraints. An important objective in such settings
is MaxProb, determining the maximal probability with which the goal can be reached, and a policy
achieving that probability. Yet algorithms for MaxProb probabilistic planning are severely underexplored, to the extent that there is scant evidence of what the empirical state of the art actually is.
We close this gap with a comprehensive empirical analysis. We design and explore a large space
of heuristic search algorithms, systematizing known algorithms and contributing several new algorithm variants. We consider MaxProb, as well as weaker objectives that we baptize AtLeastProb
(requiring to achieve a given goal probabilty threshold) and ApproxProb (requiring to compute
the maximum goal probability up to a given accuracy). We explore both the general case where
there may be 0-reward cycles, and the practically relevant special case of acyclic planning, such
as planning with a limited action-cost budget. We design suitable termination criteria, search algorithm variants, dead-end pruning methods using classical planning heuristics, and node selection
strategies. We design a benchmark suite comprising more than 1000 instances adapted from the
IPPC, resource-constrained planning, and simulated penetration testing. Our evaluation clarifies
the state of the art, characterizes the behavior of a wide range of heuristic search algorithms, and
demonstrates significant benefits of our new algorithm variants.

1. Introduction
Many probabilistic planning problems contain unavoidable dead-ends (e.g. Kolobov, Mausam,
Weld, & Geffner, 2011; Teichteil-Konigsbuch, Vidal, & Infantes, 2011; Kolobov, Mausam, & Weld,
2012; Teichteil-Konigsbuch, 2012), i.e., no policy guarantees to eventually, under all circumstances,
attain the goal. Examples are planning under resource constraints or a limited budget, or situations
where actions may fail and we will eventually run out of options. One important objective then is
MaxProb, determining the maximal probability with which the goal can be reached (and identifying
a policy achieving that probability). MaxProb also partly underlies the International Probabilistic Planning Competition (IPPC) (Younes, Littman, Weissman, & Asmuth, 2005; Bryce & Buffet,
2008; Coles, Coles, Garca Olaya, Jimenez, Linares Lopez, Sanner, & Yoon, 2012), when planners are evaluated by how often they reach the goal in online policy execution. (The time limit
in the IPPC setting mixes MaxProb with a bias towards policies reaching the goal quickly. This
c
2016
AI Access Foundation. All rights reserved.

fiS TEINMETZ & H OFFMANN & B UFFET

also relates to the proposals by Kolobov et al., 2012 and Teichteil-Konigsbuch, 2012, asking for
the cheapest policy among those maximizing goal probability, and to the proposal by Chatterjee,
Chmelik, Gupta, & Kanodia, 2015, 2016, asking for the cheapest policy ensuring that a target state
is reached almost surely in a partially observable setting.)
We consider MDP-based probabilistic planning, with factored models (probabilistic extensions
of STRIPS) whose state spaces may be too large to build explicitly. We focus on the optimal offline
setting, i.e., solving MaxProb exactly. While this setup and objective certainly is relevant, there has
been little work towards developing solvers. The main effort was made by Kolobov et al. (2011),
which we discuss in detail below. Hou, Yeoh, and Varakantham (2014) consider several variants
of topological VI (Dai, Mausam, Weld, & Goldsmith, 2011), solving MaxProb but necessitating to
build the entire reachable state space. Other works addressing goal probability maximization do
not aim at guaranteeing optimality (e.g. Teichteil-Konigsbuch, Kuter, & Infantes, 2010; Camacho,
Muise, & McIlraith, 2016).
MDP heuristic search (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001; Bonet
& Geffner, 2003b; McMahan, Likhachev, & Gordon, 2005; Smith & Simmons, 2006; Bonet &
Geffner, 2006) has the potential to find optimal policies without building the entire state space, but
Kolobov et al. (2011) are the only authors addressing optimal MaxProb through heuristic search.
Part of the reason for this lack of research on heuristic search for MaxProb are the following two major obstacles. First, while MDP heuristic search has been successful in expected-cost minimization,
it suffers from a lack of admissible (upper-bounding) heuristic estimators of goal probability. The
best known possibility is to detect dead-ends and set their initial heuristic estimate to 0, using the
trivial upper bound 1 elsewhere. Second, MaxProb does not fit the stochastic shortest path (SSP)
framework (Bertsekas, 1995), due to 0-reward cycles. As pointed out by Kolobov et al. (2011),
MaxProb is equivalent to a non-discounted reward maximization problem, where non-goal cycles
receive 0 reward and thus improper policies do not accumulate reward .
To address the second problem, Kolobov et al. (2011) devised the FRET (find, revise, eliminate traps) framework, which admits heuristic search, yet requires several iterations of complete
searches. In between heuristic search iterations, FRET eliminates 0-reward cycles (traps). FRET iterates until no more such cycles persist. Kolobov et al.s contribution is mainly theoretical  considering not only MaxProb but a much larger class of generalized SSPs  and their empirical evaluation
serves merely as a proof of concept. They experiment with a single domain (ExplodingBlocks), and
run only one configuration of search (LRTDP, Bonet & Geffner, 2003b), with one possibility for
dead-end detection and thus non-trivial initial heuristic estimates (SixthSense, Kolobov, Mausam,
& Weld, 2010). This does outperform value iteration (VI), but the dead-end detection is not used
in VI, and it remains unclear to what extent the improvement is due to the actual heuristic search,
rather than the state pruning itself.
In summary, heuristic search for MaxProb is challenging, and has only been addressed by
Kolobov et al. (2011), with very limited experiments. Given this:
(i) What is actually the empirical state of the art in heuristic search for MaxProb? Are there other
known algorithms, or variants thereof, that work better?
We explore a large design space of such algorithms, and show that, indeed, some variants work
much better.
(ii) What about simpler yet still relevant special cases, and weaker objectives, that may be easier
to solve?
230

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

There are indeed practically relevant cases that do not necessitate FRET, and weaker objectives
that enable what we will refer to as early termination.
To elaborate first on (ii): If the state space of the planning task at hand is acyclic, then clearly FRET
is not needed: there are no cycles  so in particular no 0-reward cycles  and the state space is
finite, so that any execution will end in a (goal or non-goal) absorbing state; this implies that we are
within the realm of SSPs. This special case is, however, still practically relevant. As an illustration,
acyclic state spaces occur even in a standard IPPC benchmarks, namely in the TriangleTireworld
domain where moves can only be made in one direction. More importantly, planning with a limited
action-cost budget, limited-budget planning, has acyclic state spaces when action costs are non-0,
strictly decreasing the remaining budget. A similar class of scenarios is where every action consumes a non-0 amount of some non-replenishable resource. Another example are recently proposed
models of simulated penetration testing, as per Hoffmann (2015). The MDP there models a network
intrusion from the point of view of an attacker. The state space is acyclic because each exploit can
be attempted at most once (trying the same exploit again on the same network configuration would
yield the same outcome). States thus need to remember the remaining action set, and every action
application strictly reduces that set.
Regarding weaker objectives: As alternatives to MaxProb, it is reasonable to ask whether the
maximum goal probability exceeds a given threshold , or to require computing the maximum goal
probability up to a given accuracy . We refer to these objectives as AtLeastProb and ApproxProb
respectively.1 For example, in penetration testing, AtLeastProb naturally assesses the level of network security: Can an attacker reach a target host with probability greater than a given security
margin? E.g., can a customer data server be compromised with probability greater than 0.01?
AtLeastProb and ApproxProb allow early termination based on maintaining both, a lower (pessimistic) bound V L and an upper (admissible/optimistic) bound V U . This is especially promising
in AtLeastProb, where we can terminate if the lower bound already is good enough (V L  ), or if
the upper bound already proves infeasibility (V U < ). Good anytime behavior, on either or both
bounds, translates into early termination.
Let us now elaborate on (i), exploring the state of the art and beyond. We design an algorithm
space characterized by:
(a) Search algorithm. We design variants of AO (Nilsson, 1971), LRTDP (Bonet & Geffner,
2003b), and depth-first oriented heuristic searches (Bonet & Geffner, 2003a, 2006), maintaining
upper and lower bounds for early termination.
(b) FRET. We design a new variant of FRET better suited to problems with uninformative initial
upper bounds.
(c) Bisimulation reduction. We design a new probabilistic-state-space reduction method, via bisimulation relative to the all-outcomes determinization (e.g. Bonet & Geffner, 2003b; Yoon, Fern,
& Givan, 2007; Little & Thiebaux, 2007).
1. AtLeastProb relates to MDP model-checking, where one typically wants to validate that a given PCTL (Probabilistic
Computation Tree Logic) formula is valid with some probability (Baier, Groer, Leucker, Bollig, & Ciesinski, 2004;
Kwiatkowska, Parker, & Qu, 2011a; Kwiatkowska, Norman, & Parker, 2011b). It also relates to Constrained MDPs
(Altman, 1999), as enforcing a minimum success probability could be expressed through a constraint on a particular
quantity. Chance-Constrained POMDPs (Santana, Thibaux, & Williams, 2016) are different from AtLeastProb as
their constraint is on the probability to remain in safe states, not to reach goal states.

231

fiS TEINMETZ & H OFFMANN & B UFFET

(d) Dead-end pruning method. We employ classical-planning heuristic functions for dead-end detection in probabilistic planning, via the all-outcomes determinization, as previously done by
Teichteil-Konigsbuch et al. (2011). This is especially promising in limited-budget planning,
where we can prune a state s if an admissible classical-planning estimate exceeds the remaining
budget in s.
(e) Node selection strategy. We design a comprehensive arsenal of simple strategies, biasing tie
breaking in action and state selection in manners targeted at fostering early termination.
We implemented all these techniques within Fast Downward (FD) (Helmert, 2006), thus contributing, as a side effect of our work, an ideal implementation basis for exploiting classical-planning
heuristic search techniques in MDP heuristic search.2
The algorithm dimensions (a)  (e) are orthogonal (excepting some dependencies, in particular
that bisimulation reduction subsumes dead-end pruning). We explore the behavior of the resulting
design space on a large benchmark suite we design for that purpose. The suite includes domains
from the IPPC, resource-constrained planning, and penetration testing, each with with a limitedbudget version and an unlimited-budget version. The suite comprises 1089 benchmark instances in
total.3 Amongst other things, we observe:
 Heuristic search yields substantial benefits, even with the trivial admissible heuristic setting
the initial estimate to 1 everywhere (+9% total coverage across all benchmarks), more so
with admissible heuristics based on dead-end detection (+12%).
 Early termination yields substantial benefits (e.g. for AtleastProb +8% with  = 0.2 and
+7% with  = 0.9).
 Our FRET variant yields dramatic benefits (+32% total coverage on the cyclic benchmarks).
 Bisimulation reduction yields an optimal MaxProb solver that excells in TriangleTireworld,
even surpassing Prob-PRP (Muise, McIlraith, & Beck, 2012; Camacho et al., 2016)  and
this not only in the standard version where the goal can be achieved with certainty and hence
Prob-PRP is optimal, but also in the limited-budget version where that is not so.
On the side, we discover that landmarks compilation as per Domshlak and Mirkis (2015), employed
for dead-end pruning in their oversubscription planning setting, is actually, on its own, equivalent
to pruning against the remaining budget with a standard admissible landmark heuristic. This is
relevant to our work because, otherwise, that compilation would be a canonical candidate also for
dead-end pruning in our setting (indeed this is what we started out with in our investigation).
The paper is organized as follows. Section 2 describes our model syntax and semantics, for goal
probability analysis with and without an action-cost budget limit. Section 3 specifies our search
algorithm (a) and FRET variants (b). Section 4 describes our bisimulation reduction method (c).
Section 5 describes the dead-end pruning methods (d), and Section 6 describes the node selection
strategies (e). We present our experiments in Section 7, and we conclude in Section 8. There are
two appendices giving additional technical details that we only sketch in the main text, Appendix B
2. The source code is available at http://fai.cs.uni-saarland.de/downloads/fd-prob.tar.bz2
3. The
benchmark
suite
is
available
at
http://fai.cs.uni-saarland.de/downloads/
ppddl-benchmarks-acyclic.tar.bz2 (acyclic cases) and http://fai.cs.uni-saarland.
de/downloads/ppddl-benchmarks-cyclic.tar.bz2 (cyclic cases).

232

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

regarding Domshlak and Mirkis (2015) landmarks compilation, and Appendix A regarding depthfirst oriented heuristic searches. 4

2. MDP Models
We consider PPDDL-style models (Younes et al., 2005), more precisely probabilistic extensions of
STRIPS. We employ two formalism variants, with and without a limited action-cost budget. We
specify first the unlimited-budget version. Planning tasks are tuples  = (F, A, I, G) consisting of
a finite set F of facts, a finite set A of actions, an initial state I  F , and a goal G  F . Each
a  A is a pair (pre(a), O(a)) where pre(a)  F is the precondition, and O(a) is the finite set of
outcomes o. Each o  O(a) is a tuple (p(o), add (o), del (o))
P of outcome probability p(o), add list
add (o)  F , and delete list del (o)  F . We require that oO(a) p(o) = 1.
Given a task , its state space is a probabilistic transition system (S, P, I, S> ). Here, S is the
set of states, each s  S associated with its set F (s) of true facts. The initial state I is that of .
The set of goal states S>  S contains those s where G  F (s). Transitions, and the transition
probability function P : S  A  S 7 [0, 1], are defined as follows. Action a is applicable to state
s if pre(a)  F (s) and s 6 S> (goal states are absorbing, see also below). By A[s] we denote the
set of actions applicable in s. Given s, a  A[s], and an outcome o  O(a), by sJoK we denote the
result of outcome o in s, i.e., F (sJoK) := (F (s)  add (o)) \ del (o). We define P (s, a, t) := p(o)
if a is applicable to s and t = sJoK.5 Otherwise, we define P (s, a, t) := 0 (there is no transition).
Absorbing states are those with no outgoing transitions (no applicable actions). The set of non-goal
absorbing states  lost states  is denoted S .
For limited-budget planning, we extend the above as follows. A limited-budget task is a tuple
 = (F, A, I, G, b), as above but now including also a budget b  R+
0 , and associating each
.
In
addition
to
their
true
facts
F
(s),
states s are now also
action outcome o with a cost c(o)  R+
0
associated with their remaining budget b(s)  R. States with negative remaining budget b(s) < 0
are legal and may occur, but they are lost, s  S , due to the following definitions of goal states,
action applicability, and transitions. The goal states s  S> are those where G  F (s) and
b(s)  0, i.e., we must reach the goal with  0 remaining budget. The actions a applicable to s are
those where pre(a)  F (s) and at least one outcome fits within the remaining budget, i.e., there
exists o  O(a) so that c(o)  b(s). In the outcome states sJoK, the outcomes cost is deduced from
the remaining budget, i.e., b(sJoK) := b(s)  c(o).
A few notes are in order regarding limited-budget planning. If c(o) > 0 for all o, then the state
space  viewed as a directed graph with an arc (s, t) whenever there is an action mapping s into t
with non-0 probability  is acyclic because every transition strictly reduces the remaining budget.
The state space is infinite due to the continuous state variable b(s), but its reachable part (which our
algorithms consider) is finite. Note further that the remaining budget is local to each state. If some
states in a policy violate the budget, other parts of the policy (even other outcomes of the same
action) can still continue trying to reach the goal. This differs from constrained MDPs (Altman,
1999), where the budget bound is applied globally to the expected cost of the policy. Also note
4. This paper is an extension of a previous conference paper (Steinmetz, Hoffmann, & Buffet, 2016). We cover a larger
space of algorithms (now including depth-first oriented heuristic searches), provide comprehensive explanations and
discussions, and present our experiments in detail.
5. We assume here that each o  O(a) leads to a different outcome state. This is just to simplify notation (our
implementation does not make this assumption).

233

fiS TEINMETZ & H OFFMANN & B UFFET

that, while a single budget is considered here for the sake of simplicity, our framework and results
straightforwardly extend to models with multiple budget variables.
Limited-budget planning has been explored in a deterministic oversubscription setting, the objective being to maximize the reward from achieved (soft) goals subject to the budget (Domshlak
& Mirkis, 2015). A classical-planning variant would relate to resource-constrained planning (e.g.
Haslum & Geffner, 2001; Nakhost, Hoffmann, & Muller, 2012; Coles, Coles, Fox, & Long, 2013)
with a single consumed resource. Our probabilistic variant here has been previously considered
only by Hou et al. (2014). Prior work on probabilistic planning with resources (e.g. Marecki &
Tambe, 2008; Meuleau, Benazera, Brafman, Hansen, & Mausam, 2009; Coles, 2012) has often
assumed limited budgets and non-0 consumption, but has dealt with uncertain-continuous resource
consumption, in contrast to the discrete and fixed budget consumed by action costs.
Though relatively restricted, limited-budget probabilistic planning is quite natural. Decision
making is often constrained by a finite budget. Furthermore, non-0 costs are often reasonable to
assume. This applies to, for example, penetration testing. Problems asking to achieve a goal within
a given number of steps, e.g. finite-horizon goal probability maximization, are a special case.
Let us now define solutions to our planning tasks, as well as the objectives we wish these to
achieve. A policy is a partial function  : S \ (S>  S ) 7 A  {}, mapping each non-absorbing
state s within its domain either to an action applicable in s, or to the dont care symbol . That
symbol will be used (only) by policies that already achieve sufficient goal probability elsewhere,
so do not need to elaborate on how to act on s and its descendants. That is, we still require closed
policies (see below), and we use  to explicitly indicate special cases where actions may be chosen
arbitrarily. Formally, (s) =  extends the domain of  by picking, for every t 6 S> S reachable
from s and where (t) is undefined, an arbitrary action a applicable in t and setting (t) := a.
A policy  is closed for state s if, for every state t 6 S>  S reachable from s under , (t) is
defined.  is closed if it is closed for the initial state I.  is proper if, from every state s on which
 is defined,  eventually reaches an absorbing state with probability 1.6
Following Kolobov et al. (2011), we formulate goal probability as maximal non-discounted
expected reward where reaching the goal gives reward 1 and all other rewards are 0. The value
V  (s) of a policy  closed for state s then is:

s  S>
1
s  S
V  (s) = 0P
(1)


t P (s, (s), t)V (t) otherwise
The optimal value of state s is
V  (s) =

max

: closed for s

V  (s)

(2)

Observe here that, in difference to Kolobov et al. who consider problems more general than MaxProb, we dont need to exclude improper  from this maximization. This is because there are no
negative rewards, i.e., policies cannot gain anything from infinite cycles.
Given a value function V (any function mapping states to R), the Bellman update operator is
defined, as usual, through maximization over actions relative to the current values given by V :
6. Keep in mind here that the absorbing states in our setting are S>  S , i.e., goal states and lost states. While an SSP
policy can only be considered as valid when all executions end up in a goal state  because finding a shortest path
implies that a path exists  a MaxProb policy is valid when all executions end up in an absorbing (goal or non-goal)
state  executions may fail, but need to always terminate.

234

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING


s  S>
1
s  S
V (s) := 0
P

maxaA[s] t P (s, a, t)V (t) otherwise

(3)

The difference between V (s) prior to the update, and its updated value according to the right-hand
side, is called the Bellman residual.
The greedy policy  on a value function V selects in each non-absorbing state an action obtaining the maximum in the right-hand side of this equation (note that the greedy policy is unique only
up to tie-breaking). We will refer to the state space subgraph induced by those states reachable from
I using such a greedy policy  as the -greedy graph. By the V -greedy graph, we will refer to
the state space subgraph induced by those states reachable from I by any greedy policy on V , i.e.,
allowing in each state to choose any action greedy on V .
For acyclic state spaces, every run ends in an absorbing state in a finite number of steps, so
we are facing an SSP problem (subject to our definition of absorbing states, cf. above) and the
Bellman update operator has the unique fixed point V  , which it converges to from any initial V .
For cyclic state spaces, as pointed out by Kolobov et al. (2011), the Bellman update operator may
have multiple sub-optimal fixed points, and updates from an optimistic (upper-bound) initialization
V are not guaranteed to converge to the optimum V  . One can either use a pessimistic (lowerbound) initialization V , from which the updates are guaranteed to converge to V  ; or one can use
Kolobov et al.s FRET method described earlier.
We consider three different objectives (algorithmic problems) for goal probability analysis:
MaxProb: Find an optimal policy, i.e., a closed  s.t. V  (I) = V  (I).
AtLeastProb: Find a policy guaranteeing a user-defined goal probability threshold   [0, 1], i.e.,
a closed  s.t. V  (I)  . (Or prove that such  does not exist.)
ApproxProb: Find a policy optimal up to a user-defined goal probability accuracy   [0, 1], i.e.,
a closed  s.t. V  (I)  V  (I)  .
We now define our algorithm family addressing these problems. We cover search algorithms, bisimulation reduction, dead-end pruning, and node selection strategies, in this order.

3. Search Algorithms
We use value iteration (VI) as a baseline. We design variants of AO and LRTDP, as well as a family
of depth-first oriented heuristic searches, systematizing algorithm parameters underlying improved
LAO (here: ILAO ) (Hansen & Zilberstein, 2001), heuristic dynamic programming (Bonet &
Geffner, 2003a), and learning depth-first search (Bonet & Geffner, 2006). We furthermore design a
variant of FRET better suited to problems with uninformative initial upper bounds.
3.1 VI
As a pre-process to VI, we make one forward pass building the reachable state space (actually
its pruned subgraph, see Section 5). We initialize the value function pessimistically, simply as 0
everywhere. For acyclic cases, we then perform a single backward pass of Bellman updates, starting
at absorbing states and updating children before parents, thus computing the optimal value function
while updating every state exactly once.
235

fiS TEINMETZ & H OFFMANN & B UFFET

procedure GoalProb-AO
initialize  to consist only of I; Initialize(I)
loop do
if [MaxProb: V L (I) = 1]
[AtLeastProb:V L (I)  ]
[ApproxProb: V L (I)  1   or V U (I)  V L (I)  ] then
return  L endif /* early termination (positive) */
if [AtLeastProb: V U (I) < ] then
return impossible endif /* early termination (negative) */
if ex. leaf state s 6 S>  S in  reachable using  U then
select such a state s
else return  U endif /* regular termination */
for all a and t where P (s, a, t) > 0 do
if t not already contained in  then
insert t as child of s into ; Initialize(t)
else insert s as a new parent of t into 
endif
endfor
BackwardsUpdate(s)
endloop
procedure 
Initialize(s):
0 s  S
U
V (s) :=
 1 otherwise
1 s  S>
L
V (s) :=
0 otherwise
if s 6 S>  S then  L (s) :=  endif

Figure 1: AO* search for MaxProb, AtLeastProb, and ApproxProb (as indicated), on acyclic state
spaces.  U is the current greedy policy on V U ,  L is the current greedy policy on V L .
The BackwardsUpdate(s) procedure updates all of V U ,  U , V L ,  L . As states may have
several parents in , we first make a backwards sweep to collect the sub-graph |s ending
in s (to update V U and  U , the greedy sub-graph on V U suffices). Then we update |s
in reverse topological order.
For the general/cyclic case, we assume a convergence parameter  (likewise in all other algorithms addressing this case), and compute an -consistent value function, where the Bellman
residual on every state is at most . For efficient value iteration, we employ topological VI as per
Dai et al. (2011): we find the strongly connected components (SCC) of the state space, and handle
each SCC individually, children SCCs before parent SCCs. VI on an SCC stops when every state is
-consistent.
Dai et al. (2011) also introduce focused topological VI, which eliminates sub-optimal actions
in a pre-process to obtain smaller SCCs. While this can be much more runtime-effective, it still
requires building the entire state space. In our experiments, runtime/memory exhaustion during this
process, i.e., during building the state space, was the only reason for VI failures. So we do not
consider focused topological VI here.
3.2 AO
For AO , we restrict ourselves to the acyclic case, where the overhead for repeated value iteration
fixed points, inherent in LAO (Hansen & Zilberstein, 2001), disappears. (The ILAO variant,
236

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

where that issue has been addressed through a depth-first orientation, is covered as part of our
depth-first oriented heuristic search family introduced in Section 3.4 below.)
Figure 1 shows pseudo-code for our GoalProb-AO variant. The algorithm incrementally constructs a subgraph  of the state space. The handling of duplicates is simple, identifying search
nodes with states, as the state space is acyclic. For the same reason, simple backward updating
suffices to maintain the value function. Adopting ideas from prior work (e.g. McMahan et al., 2005;
Little, Aberdeen, & Thiebaux, 2005; Smith & Simmons, 2006; Kuter & Hu, 2007), we maintain
two value functions, namely both an upper bound V U and a lower bound V L on goal probability.
For lack of heuristic estimators of goal probability, both value functions are initialized trivially,
by 1 for V U and by 0 for V L , except for absorbing states where the exact value is known. (Dead-end
detection, as a simple but non-trivial V U initialization, will be discussed in Section 5.) Nevertheless,
both bounds can be useful for search, through early termination (V L and V U ), and through detecting
sub-optimal parts of the state space (V U ). To observe the latter, note that, to refute an action a, it
may suffice to reduce V U for just one of as outcomes. Hence, even for trivial initialization, V U
may allow to disregard parts of the search space, in the usual way of admissible heuristic functions.
As we shall see, this kind of behavior occurs frequently in practice (as reflected by our benchmarks).
Regarding early termination, the lower bound enables positive early termination when we can
already guarantee sufficient goal probability, namely 1 (MaxProb),  (AtLeastProb), or 1   (ApproxProb). The upper bound enables negative early termination in AtLeastProb, when V U (I) < .
In ApproxProb, clearly we can terminate when V U (I)  V L (I)  . A relevant observation
here is that the V L (I) = 1 (MaxProb) and V L (I)  1   (ApproxProb) criteria are redundant
when maintaining an upper bound, i.e., for heuristic search: If V L (I)  1  , then trivially also
V U (I)  V L (I)  . If V L (I) = 1, then there is a search branch achieving the goal with certainty,
so V U (I) = 1 there as well and search terminates regularly. In configurations not maintaining V U ,
however, these criteria can be very useful to reduce search.
The correctness of GoalProb-AO is easy to establish. By the standard properties of Bellman
updates, at any point in time during the execution of the algorithm, and for any state s in , we
have that V L (s)  V  (s)  V U (s), i.e., V L and V U are lower respectively upper bounds on goal
probability. Indeed, these bounds are monotone (Bertsekas & Tsitsiklis,
1996), precisely, V L and
P
U
L
V are exact on absorbing states, and satisfy V (s)  maxaA[s] t P (s, a, t)V L (t) respectively
P
V U (s)  maxaA[s] t P (s, a, t)V U (t) on non-absorbing ones. This is because V L and V U are
initialized with functions trivially satisfying these properties, and these properties are invariant over
Bellman updates on non-absorbing states (given monotonicity, V L can only grow, while V U can
only decrease). Thanks to monotonicity, with the same arguments as given for LAO (Hansen &
Zilberstein, 2001), we get that V U converges to V  in finite time on the  U -greedy graph.
Finally, we need to prove that, in case of early termination returning  L , the greedy policy  L
on V L actually achieves what we want, i.e., (1)  L is closed and (2)  L provides sufficient goal
L
probability, i.e., V  (I)  V L (I). For (1),  L is always a closed policy, because it applies the
dont care symbol  at the non-absorbing leaf states in . (Note also that  is applied only by  L
L
and only on those states.) For (2), we show that, for all states s, we have V  (s)  V L (s). This
claim is trivial for states s where  L (s) = , as these have never been updated so V L (s) = 0. For
other states s, the claim follows by a simple inductive reasoning over the maximal distance to an
L
absorbing state in the  L -greedy graph. For absorbing states s, we have V  (s) = V L (s) = V  (s),
P
L
L
so the claim is trivially satisfied. In the induction step, we have V  (s) = t P (s,  L (s), t)V  (t)
L
L
by definition of V  , while, by the induction hypothesis, V  (t)  V L (t) for all those states t
237

fiS TEINMETZ & H OFFMANN & B UFFET

procedure GoalProb-LRTDP
 := {I}; Initialize(I)
loop do
[early termination criteria exactly as in GoalProb-AO ]
if I is not labeled as solved then
LRTDP-Trial(I)
else return  U endif /* regular termination */
endloop
procedure LRTDP-Trial(s):
P := empty stack
while s is not labeled as solved do
push s onto P
if s  S>  S then break endif
[cyclic: if s is -consistent then break endif]
for all a and t where P (s, a, t) > 0 do
if t 6  then Initialize(t) endif
endfor
update V U (s),  U (s), V L (s),  L (s)
s := sample t according to P (s,  U (s), t)
endwhile
while P not empty do
pop s from P
[acyclic: if  CheckSolved(s, 0) then break endif]
[cyclic: if  CheckSolved(s, ) then break endif]
endwhile

Figure 2: LRTDP for MaxProb, AtLeastProb, and ApproxProb, on acyclic or general (cyclic) state
spaces.  U is the current greedy policy on V U ,  L is the current greedy policy on V L .
The CheckSolved(s, ) procedure is exactly as specified by Bonet and Geffner (2003b). It
visits states t reachable from s using  U , initializing t if not previously visited, stopping
at t if not -consistent. It then performs updates bottom-up, labeling t as solved iff all its
descendants are -consistent. Our only change is to update V L and  L along with V U
and  U .
P
L
where P (s,  L (s), t) > 0, so in other words V  (s)  t P (s,  L (s), t)V L (t). By plugging in
the definition of  L (s), and by using the monotonicity property, it is now easy to conclude that
L
V  (s)  V L (s), as desired.
3.3 LRTDP
Figure 2 shows pseudo-code for our GoalProb-LRTDP variant, applicable to the general case (cyclic
as well as acyclic problems). We assume that, in cyclic cases, the algorithm is run within the FRET
framework. The main change to the original version of LRTDP consists in maintaining a lower
bound in addition to the upper (optimistic) bound, and adding the same early termination criteria as
in GoalProb-AO . Correctness of early termination follows with the same arguments as before, i.e.,
V L (s) and V U (s) are monotone lower respectively upper bounds, and  L is always a closed policy.
Note that this is true even in the general/cyclic case, i.e., if early termination applies, then we can
terminate the overall FRET process.
The only other change we make is an additional stopping criterion for trials in the cyclic case,
namely if the current state s is -consistent. Kolobov et al. (2011) use this criterion to keep trials
238

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

procedure GoalProb-DFHS
 := {I}
loop do
[early termination criteria exactly as in GoalProb-AO ]
if (Label and I is not labeled as solved)
or (VI and  U changed after running VI on the  U -greedy graph) then
DFHS-Exploration(I)
clean visited-markers
else return  U endif /* regular termination */
endloop
procedure DFHS-Exploration(s):
if s 6  then Initialize(s) endif
if s  S>  S or s is labeled solved then
label s solved
return 
endif
f lag := 
if FW then
if V U (s) is not -consistent then f lag := > endif
update V U (s),  U (s), V L (s), and  L (s)
if Consist and f lag then return > endif
endif
mark s as visited
foreach t with P (s,  U (s), t) > 0 do
if t has not been visited then f lag := DFHS-Exploration(t)  f lag endif
done
if f lag or FW then
if V U (s) is not -consistent then f lag := > endif
update V U (s),  U (s), V L (s), and  L (s)
endif
if Label and f lag then label s solved endif
return f lag

Figure 3: Depth-First Heuristic Search (DFHS) for acyclic MaxProb, AtLeastProb, and ApproxProb. The cyclic version is shown in Appendix A and uses Tarjans SCC procedure
instead of depth-first search. VI, Label, FW, and Consist are Boolean algorithm parameters (see text). Recall that the  U -greedy graph is the set of states reachable from I using
the current greedy policy  U . The f lag returned by DFHS-Exploration is used inside the
recursion only (it is ignored in the top-level calls), to decide whether to backward-update
a state if forward-updates are in use.

from getting trapped in 0-reward (non-goal) cycles. The criterion preserves the property that, upon
regular termination, all states reachable using  U are -consistent.7

In the cyclic case, the V U fixed point found by LRTDP may be sub-optimal, so we have to use
FRET. In the acyclic case, we use  = 0, and a single call to LRTDP suffices.
239

fiS TEINMETZ & H OFFMANN & B UFFET

3.4 Depth-First Heuristic Search
We finally consider systematic heuristic searches (not based on trials like LRTDP) with a strong
depth-first orientation. Intuitively, such an orientation is especially beneficial in our context as it is
likely to lead to absorbing states, and thus to states with a non-trivial heuristic function initialization,
quickly. We refer to such algorithms as Depth-First Heuristic Search (DFHS). Known instances are
ILAO (Hansen & Zilberstein, 2001),8 heuristic dynamic programming (HDP) (Bonet & Geffner,
2003a), and learning depth-first search (LDFS) (Bonet & Geffner, 2006). Their commonality lies in
conducting depth-first searches (DFS) on the state-space subgraph defined by actions greedy on a
current upper bound V U , which is being updated backwards in DFS, until a termination criterion applies. The algorithms differ in how depth-first branches are terminated, how the overall algorithm is
terminated, and in whether or not updates are also performed in the forward direction. Here, we systematize these parameters, obtaining a DFHS algorithm family containing the previous algorithms
as family members.
Figure 3 gives a pseudo-code description of our DFHS algorithm family. For simplicity, the
figure considers acyclic problems only. For cyclic problems, instead of DFS the algorithms use
Tarjans depth-first SCC algorithm (Tarjan, 1972), in order to detect the SCCs at the same time
as doing the exploration and updates, as suggested by Bonet and Geffner (2003a). (Knowing the
SCCs is required for correct solved-labeling in the general case.) The pseudo-code description of
the DFHS algorithm family for the general (cyclic) case is given in Appendix A.
The algorithms search in the  U -greedy graph. A variant would be to instead search the V U greedy graph. That variant, employed by LDFS, is not effective for goal probability analysis because
V U is 1 everywhere initially, and the V U -greedy graph is the entire (dead-end pruned) reachable
state space. We hence omit this option, and therewith LDFS, from our DFHS family (matters may
change if better admissible heuristic functions are identified in future work, cf. Section 8).
All algorithms update values in the backward direction, when leaving a state. If the FW algorithm
parameter is true, then value updates are done also in the forward direction, when entering a state.
As that consistently yields (small) advantages empirically, we switch FW to true in all our algorithm
configurations, except in the one corresponding to the known algorithm ILAO which does not use
this technique. Detecting whether the optimal solution has been found can be done in two ways: (1)
Label, maintaining solved-labels while doing the DFS; or (2) VI, running value iteration on the  U greedy graph after DFS has terminated. In (1),  U is optimal if the initial state is labeled solved. In
(2), one can terminate if the greedy policy did not change during VI. If we do use forward updates,
then (as we already check the Bellman residual anyway) we have the additional option Consist to
stop the search at -inconsistent states, as opposed to stopping only at absorbing states. Overall, we
run 5 different parameter settings for DFHS, overviewed in Table 1.
Correctness of early termination follows again with the same arguments as before. For the
correctness of regular termination, we need to show that a fixed point policy is obtained, i.e., upon
regular termination, (*) the  U -greedy graph contains no -inconsistent states. This holds because
all our algorithm variants fit Bonet and Geffners (2003a) Find-and-Revise schema on a finite state
space with a monotone optimistic bound, where (1) in each search iteration we find and update at
7. The updates during trials are, in difference to the original LRTDP formulation, not related to a trial-stopping guarantee
in goal probability maximization. They just turn out to consistently yield (small) advantages empirically, so we keep
them in here.
8. The brief description of ILAO by Hansen and Zilberstein (2001)  and thus its depth-first orientation  can be subject
to interpretation. Our design here follows that of Bonet and Geffner (2005) in their mGPT tool.

240

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

Acronym
DFHSVI
DFHSFwd
VI
DFHSFwdCons
VI
DFHSFwd
Lab
DFHSFwdCons
Lab

Termination
VI
VI
VI
Label
Label

FW?
no
yes
yes
yes
yes

Cons?
no
no
yes
no
yes

Known?
yes: ILAO (Hansen & Zilberstein, 2001)
no: new variant
no: new variant
no: new variant
yes: HDP (Bonet & Geffner, 2003a)


Table 1: Depth-First Heuristic Search (DFHS) family overview. We do not include LDFS (Bonet
& Geffner, 2006) as, due to considering the V U -greedy graph rather than the  U -greedy
graph, LDFS does not work well on MaxProb (see text).
least one -inconsistent state, until (2) condition (*) is met. Given the depth-first search (respectively
Tarjans algorithm, in the general case) it is clear that (1) holds true. Regarding (2), this is obvious
when the VI termination option is used;9 it holds for the Label termination option because a state
is labeled solved only when all its descendant states in the  U -greedy graph are -consistent.
As done in Table 1, we will usually omit the GoalProb- in algorithm names. Keep in mind
though that our algorithms differ from the original ones, in particular in terms of early termination
which depends on the objective MaxProb, AtLeastProb, or ApproxProb. To study the termination
benefits of the lower vs. upper bound, we will switch each bound on and off individually. Where
X denotes one of our search algorithms, we denote by X|U and X|L the variants of X maintaining
only V U respectively only V L . We will sometimes write X|LU to make explicit that both bounds
are used. Early termination criteria involving the non-maintained bound are disabled. For X|U , this
leaves just the negative criterion V U (I) <  in AtLeastProb; X|L still has positive criteria.
We test a version X|L only for X=AO , as a canonical representative of (non-VI) blind search.
In AO |L , all non-absorbing leaf states in  are open (rather than only those reachable using  U ),
and in case of regular termination we return  L .
3.5 FRET
As previously hinted, Kolobov et al.s (2011) FRET performs an iteration of complete searches. It
starts with some upper-bound approximation V U of V  , which is continuously updated throughout
the FRET process. Within each FRET iteration, a heuristic search algorithm runs until termination,
i.e., until finding a fixed point policy. In between these iterations, FRET runs a trap elimination
step, which finds all traps in the V U -greedy graph. FRET forces the next search iteration to not
include these traps. FRET terminates if the V U -greedy graph does not contain a trap.
The trap elimination step works as follows. A trap is a subset T of non-absorbing states in
which any greedy policy will remain indefinitely, i.e., all outgoing transitions in the V U -greedy
graph of any s  T lead to another trap state t  T . A trap T is removed by collapsing T s states
into a single state sT . The incoming transitions of sT are those incoming to any state of T , and its
outgoing transitions are those transitions of T -states exiting T (note that these transitions are, by
construction, not contained in the V U -greedy graph).
This transformation obviously prevents T from occuring again in later iterations. It preserves

V as the trap states have identical V  values: as all trap states are non-absorbing and can reach each
other, these states can reach each other with 0-reward transitions (note that this holds regardless of
9. Note that, in the acyclic case, full VI is not actually needed so the algorithm could be simplified. We leave it this way
here, as used by ILAO in the general case, for simplicity of presentation.

241

fiS TEINMETZ & H OFFMANN & B UFFET

V U , i.e., it holds also on parts of the state space where V U has not yet converged). Because there
is only a finite number of possible traps in the state space, FRET eventually finds a V U whose V U greedy graph does not contain a trap. From that graph, a V U -greedy policy  is extracted, which
does not contain traps, hence is proper on the trap-collapsed state space, hence is optimal for that
state space. An optimal policy for the original task can be constructed from  by acting, within
collapsed traps, in a way so that the exit taken by  is eventually reached with certainty. (This is the
correctness argument given by Kolobov, 2013.)
Our new variant of FRET differs from the original version only in terms of the state space
subgraph considered: instead of the V U -greedy graph, we use the  U -greedy graph, i.e., we consider
only the actions selected into the current greedy policy (cf. our discussion of DFHS above). We will
refer to this design as FRET- U , and we will refer to Kolobov et al.s (2011) design as FRET-V U .
It is easy to see that FRET- U is still correct. The arguments above remain intact as stated.
FRET-V U potentially eliminates more traps in each iteration, and may hence require fewer iterations. Yet not all these traps may actually need to be eliminated (we might eventually find an optimal
policy not entering them), and each trap elimination step may be much more costly. In particular,
in goal probability analysis, FRET-V U is typically ineffective because, similarly as discussed above
for DFHS, in the first FRET step V U often is 1 almost everywhere, and the V U -greedy graph is
almost the entire reachable state space. As we shall see, FRET- U clearly outperforms FRET-V U .

4. State-Space Reduction via Determinized Bisimulation
Bisimulation is a known method to reduce state space size in MDPs/probabilistic planning (e.g.
Dean & Givan, 1997). The idea essentially is to group equivalent sets of states together as block
states, and then solve the smaller MDP over these block states. Here, we observe that this approach can be fruitfully combined with state-of-the-art classical planning techniques, namely mergeand-shrink heuristics (Drager, Finkbeiner, & Podelski, 2009; Helmert, Haslum, Hoffmann, & Nissim, 2014), which allow to effectively compute a bisimulation over the determinized state space.
Determinized-bisimilar states are bisimilar in the probabilistic state space as well, so this identifies a practical special case of probabilistic bisimulation given a factored (STRIPS-like) problem
specification.
Let us spell this out in a little more detail. Given a task  (with or without budget limit), a
probabilistic bisimulation for  is a partitioning P = {B1 , . . . , Bn } of s state set S so that, for
every Bi and Bj , every action a, and every s, t  Bi , the following two properties are satisfied
(Dean & Givan, 1997):
(i) a is applicable in s iff a is applicable in t; and
P
P
(ii) if a is applicable in s and t, then oO(a),sJoKBj p(o) = oO(a),tJoKBj p(o).
Dean and Givan show that an optimal solution to the bisimulation of an MDP induces an optimal
solution to the MDP itself. In other words, it suffices to work on the block states Bi .
Now, denote by det the all-outcomes determinization of  (e.g. Yoon et al., 2007; Little
& Thiebaux, 2007), with a separate action adet
for every a and o  O(a), inheriting as preo
condition and os adds, deletes, and cost. A determinized bisimulation for  is a partitioning
P = {B1 , . . . , Bn } of s states so that, for every Bi and Bj , every determinized action adet
o ,
and every s, t  Bi , the following two properties are satisfied (Milner, 1990; Helmert et al., 2014):
det
(a) adet
o is applicable in s iff ao is applicable in t; and

242

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

det
det
(b) if adet
o is applicable in s and t, then sJao K  Bj iff tJao K  Bj .

It is easy to see that such {B1 , . . . , Bn } also is a probabilistic bisimulation for . Since an action
adet
o is applicable in a state s iff the corresponding action a of the original MDP is applicable in s,
(a) directly implies (i). From (b), we know that for every action a applicable to s, t, and for each
det
outcome o  O(a), we have sJadet
o K  Bj iff tJao K  Bj . This obviously implies (ii); it is more
restrictive than needed as it insists on the subset of outcomes being the same on both sides, rather
than only their summed-up probability being the same.
But how to compute a determinized bisimulation for ? The nave solution is to build the state
space up front and then computing a determinized bisimulation on it. One can potentially do much
better though, by using merge-and-shrink with the widely employed shrinking strategies based on
bisimulation (Nissim, Hoffmann, & Helmert, 2011; Katz, Hoffmann, & Helmert, 2012; Helmert
et al., 2014). In a nutshell, this algorithm framework constructs an abstraction by starting with a
collection of abstractions each considering a single state variable only, then iteratively merging
two abstractions (replacing them with their synchronized product) until only a single abstraction is
left, and shrinking abstractions to a bisimulation thereof in between every merging step. As we
shall see in the experiments, this often still incurs a prohibitive overhead, but it can be feasible, and
lead to substantial state space size reductions. In some cases, it results in tremendous performance
improvements.

5. Dead-End Pruning
We refer to states s where V  (s) = 0, i.e., the goal cannot be reached at all from s, as dead-ends.
If one detects such s via some dead-end detection technique, then one can treat s exactly like a lost
state S (except for setting  L (s) :=  as we need to act on non-absorbing states). This constitutes
a pruning method in itself, useful for any search algorithm, as the state space below s needs no
longer be explored. Apart from this pruning itself, for the heuristic search algorithms, dead-end
detection provides a non-trivial initialization of V U , as we will initialize V U (s) = 0 instead of
V U (s) = 1 if we detected s to be a dead-end. This more informed initial upper bound typically
leads to additional search reductions.
But how to detect dead-ends? Kolobov et al. (2011) employ SixthSense (Kolobov et al., 2010),
which learns dead-end detection rules by generalizing from information obtained using a classical planner. Here we instead exploit the power of classical-planning heuristic functions  readily
available in our FD implementation framework  run on the all-outcomes determinization. This is
especially promising in limited-budget planning, where we can use lower bounds on determinized
remaining cost to detect states with insufficient remaining budget. Observe that this is natural and
effective using admissible remaining-cost estimators, yet would be impractical using an actual classical planner (which would need to be optimal and thus prohibitively slow). In the unlimited-budget
case, we can use any heuristic function able to detect dead-ends (returning ), which applies to
most known heuristics. Indeed, merge-and-shrink heuristics have recently been shown to be extremely competitive dead-end detectors (Hoffmann, Kissmann, & Torralba, 2014).
To make this concrete, consider a state s in a task , and denote as before by det the alloutcomes determinization of . Let h be a classical-planning heuristic function. If h guarantees
to return  only on dead-ends, and h(s) =  on det , then there exists no sequence of action
outcomes achieving the goal from s, so V  (s) = 0. If  is a limited-budget task, h is admissible,
and h(s) > b(s), then we cannot achieve the goal from s within the budget, and thus also V  (s) = 0.
243

fiS TEINMETZ & H OFFMANN & B UFFET

We experiment with state-of-the-art heuristic functions, namely (a) an admissible landmark
heuristic as per Karpas and Domshlak (2009), (b) LM-cut (Helmert & Domshlak, 2009), (c) several
variants of merge-and-shrink heuristics, and (d) hmax (Bonet & Geffner, 2001) as a simple and
canonical option. (a) turned out to perform consistently worse than (b), so we will report only on
(b)  (d).
For limited-budget planning, we also considered adopting the problem reformulation by Domshlak and Mirkis (2015) for oversubscription planning, which reduces the budget b using landmarks
and in exchange allows traversing yet unused landmarks at a reduced cost during search. It turns
out, however, that pruning states whose reformulated budget is < 0 is equivalent to the much simpler method pruning states whose heuristic (a) exceeds the (original/not reformulated) remaining
budget. The added value of Domshlak and Mirkis reformulation thus lies, not in its pruning per se,
but in its compilation into a planning language and the resulting combinability with other heuristics.
We give full details in Appendix B. To get an intuition why Domshlak and Mirkis reformulation
is, per se, equivalent to (a), assume for simplicity that L is a set of disjoint disjunctive action landmarks for the initial state, and assume that actions have unit costs. Say we prune s if its reduced budget, b0 (s), is < 0. The reduced initial budget is b0 := b|L|. The reduced costs allow applying member actions of yet non-used landmarks at 0 cost, where the non-used landmarks for a given search
path are those l  L not touched by the path. Consider now some state s reached on path ~a. Denote
the non-used landmarks by L0 . The cost saved on ~a thanks to the reformulation is exactly that of the
used landmarks, |L\L0 |. Hence b0 (s) = b0 (|~a||L\L0 |) = (b|L|)|~a|+|L\L0 | = b|~a||L0 |.
So s is pruned in the reformulation, b0 (s) < 0, iff b  |~a|  |L0 | < 0 iff b  |~a| < |L0 |. The latter
condition, however, is exactly the pruning condition using the simple method (a) instead.

6. Node Selection Strategies
In all our algorithms, good anytime behavior on V L and/or V U may translate into early termination.
We explore the potential of fostering this via (1) biasing the tie-breaking in the selection of best
actions  U greedy with respect to V U , and (2) biasing, respectively, the outcome-state sampling
during trials (LRTDP) and the choice of expanded leaf states (AO ). To be precise regarding the
latter: as usual, we maintain state open flags in AO , true if a state has open descendants within
the  U -greedy graph. We select the leaf state to expand by going forward from I using  U , and if
an action has more than one open outcome state t, we select a t best according to the bias (2). Note
that (2) is not as relevant in DFHS, which in every iteration of DFS explores all outcomes anyhow.
Hence, in DFHS, we use only the  U tie-breaking criteria (1) explained in what follows.
We experimented with a variety of strategies. In what follows, where a strategy specifies one
of (1) or (2) only, the other setting is as in the default strategy. That strategy corresponds to the
commonly used settings. It uses arbitrary tie-breaking for (1), but in a fixed manner, changing
 U (s) only if some other action becomes strictly better in s, as suggested by Bonet and Geffner
(2003b) for LRTDP. There is no bias (2) on outcome states in AO (an open outcome state is selected
arbitrarily). Bias (2) in LRTDP is by outcome probability. We also tried this most-prob-outcome
bias strategy in AO , where the most likely open outcome state is selected.
The h-bias strategy prefers states with smaller h value, where the heuristic h is the same one
used for dead-end pruning.10 Specifically, for action selection tie-breaking (1), from those actions
10. We also experimented with a strategy using merge-and-shrink with determinized action costs set to the negated
logarithm of outcome probability (compare e.g. Jimenez, Coles, & Smith, 2006). This is compelling in theory

244

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

P
U
a maximizing the optimistic expected
Pgoal probability t P (s, a, t)V (t), we select an a minimizing the expected heuristic value t P (s, a, t)h(t). The outcome-state bias (2) is obtained by
1
renormalizing the weighed probabilities h(t)
 P (s, a, t), so we prefer high probability outcomes
with small h value.
Inspired by BRTDP (McMahan et al., 2005), we experiment with a gap-bias strategy, biasing
U
L
search towards states with large
Precisely, for (1) we break ties in favor of actions a
PV  V gaps.
maximizing the expected gap t P (s, a, t)[V U (t)V L (t)], and for (2) we renormalize the weighed
probabilities [V U (t)  V L (t)]  P (s, a, t).
Inspired by common methods in classical planning (e.g. Hoffmann & Nebel, 2001; Helmert,
2006; Richter & Helmert, 2009), we experiment with a preferred actions strategy, which in (1)
U (s) to an action a participating in a delete-relaxed determinized plan for s, if such
prefers to set P
a maximizing t P (s, a, t)V U (t) exists.
AO |L is a special case, as we do not maintain an upper bound and thus there is no selection
(1) of actions  U greedy with respect to V U . We apply node selection strategies for (2) directly
to the set of (all) leaf states in the current search graph . The default strategy is depth-first, the
rationale being to try to reach absorbing states quickly. The h-bias strategy selects a deepest leaf
with minimal h value, the preferred actions strategy selects a deepest open leaf reachable using only
preferred actions. We furthermore experiment with a breadth-first strategy, just for comparison.

7. Experiments
We implemented all the algorithms in Fast Downward (FD) (Helmert, 2006), and ran experiments
on an extensive suite of benchmarks.11 In our evaluation we first summarize the results for acyclic
benchmarks (where FRET is not needed), and then the ones for cyclic benchmarks (where FRET is
needed).
7.1 Experiments Setup
We start with giving details of our implementation and describing the benchmark suite used for the
experiments.
7.1.1 I MPLEMENTATION
As our model pertains to goal-directed MDPs with a limited number of (explicitly listed) outcomes
per action, naturally we use PPDDL (Younes et al., 2005), rather than RDDL (Sanner, 2010; Coles
et al., 2012), as the surface-level language. FDs pre-processes were extended to handle PPDDL,
and we added support for specifying a (numeric) budget limit.
Given the FD implementation framework in contrast to previous works on optimal probabilistic
planning, we implemented all algorithms from scratch. For FRET, we closely followed the original
implementation, up to details not specified by Kolobov et al. (2011), based on personal communication with Andrey Kolobov. (Kolobovs original source code is not available anymore, which also
plays a role in our state-of-the-art comparison, see next.)
because, then, a bisimulation-based heuristic corresponds to the exact goal probability of the best outcome sequence
from a state. Yet, as already pointed out, computing such a heuristic is often infeasible.
11. The source code is available in an online appendix, and can be downloaded at http://fai.cs.unisaarland.de/downloads/fd-prob.tar.bz2

245

fiS TEINMETZ & H OFFMANN & B UFFET

Given the scant prior work on optimal goal probability analysis (cf. Section 1), the state of the art
is represented by topological VI, by LRTDP|U with dead-end pruning on acyclic problems, and by
FRET-V U using LRTDP|U with dead-end pruning on cyclic problems. All these configurations are
particular points in the space of configurations we explore, so the comparison to the state of the art
is part of our comparison across configurations. The only thing missing here is the particular form
of dead-end detection, which was SixthSense in the only prior work, by Kolobov et al. (2011). As
SixthSense is a complex method and advanced dead-end pruning via heuristic functions is readily
available in our framework, we did not re-implement SixthSense. Our discussion of cyclic problems
in Section 7.3 below includes a detailed comparison of our results with those by Kolobov et al., on
IPPC ExplodingBlocks which is the only domain Kolobov et al. considered.
Note that providing quality guarantees is an important property in this study. For this reason,
and for the sake of clarity, we do not compare against unbounded suboptimal approaches, such
as using an algorithm with a discounted criterion or assigning large finite penalties to dead-ends
(Teichteil-Konigsbuch et al., 2011; Kolobov et al., 2012).
Furthermore, as AtLeastProb is a special case of MDP model checking, one may wonder how
probabilistic model checking tools, e.g. PRISM (Kwiatkowska et al., 2011b), would fare on that
problem in planning benchmarks. We do not investigate that question here, which would entail a
translation from PPDDL into a model checking language, which is non-trivial and makes a direct
comparison  of algorithms taking different inputs  problematic. One may speculate that, given
their focus on blind searches, model checking tools are inferior to our heuristic search approaches
where those fare well; but that remains a question for future work.
7.1.2 B ENCHMARK S UITE
Our aim being to comprehensively explore the relevant problem space, we designed a broad suite
of benchmarks, 1089 instances in total, based on domains from the IPPC, resource-constrained
planning, and penetration testing (pentesting).
From the IPPC, we selected those PDDL domains in STRIPS format, or with moderate nonSTRIPS constructs easily compilable into STRIPS. This resulted in 10 domains from IPPC04 
IPPC08; we selected the most recent benchmark suite for each of these.
For resource-constrained planning, we adopted the NoMystery, Rovers, and TPP benchmarks
by Nakhost et al. (2012), more precisely those suites with a single consumed resource (fuel, energy, money), which correspond to limited-budget planning.12 We created probabilistic versions
by adding uncertainty about the underlying road map, akin to the Canadian Traveler scenario, each
road segment being present with a given probability (this is encoded through a separate, probabilistic, action attempting a segment for the first time). For simplicity, we set that probability to 0.8
throughout.
For pentesting, the general objective is  using exploits  to compromise computers in a network, one after another, until specific targets are reached (or no action is available). We modified
the POMDP generator by Sarraute, Buffet, and Hoffmann (2012), which itself is based on a test scenario used at Core Security (http://www.coresecurity.com/) to output PPDDL encodings of
Hoffmanns (2015) attack-asset MDP pentesting models. In these models, the network configura12. To make the benchmarks feasible for optimal probabilistic planning, we had to reduce their size parameters (number
of locations etc). We scaled all parameters with the same number < 1, chosen to get instances at the borderline of
feasibility for VI.

246

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

tion is known and fixed, and each exploit is callable once and succeeds (or fails) with some probability. The generator uses a network consisting of an exposed part, a sensitive part, and a user part.
It allows to scale the numbers H of hosts and E of exploits. Sarraute et al.s POMDP model and
solver (SARSOP, see Kurniawati, Hsu, & Lee, 2008, which does not guarantee optimality) scale to
H = 6, E = 10.13 For our benchmarks, we fixed H = E for simplicity (and to obtain a number of
instances similar to the other benchmark domains). We scaled the instances from 6 . . . 20 without
budget limit, and from 10 . . . 24 with budget limit.
From each of the above benchmark tasks  (except the pentesting ones for which we already
generated a separate limited-budget version anyway), we obtained several limited-budget benchmarks, as follows. We set outcome costs to 1 where not otherwise specified. We determined the
minimum budget, bmin , required to achieve non-0 goal probability. For the resource-constrained
benchmarks, bmin is determined by the generator itself, as the minimum amount of resource required
to reach the goal in the deterministic domain version. For all other benchmarks, we ran FD with A
and LM-cut on the all-outcomes determinization of . If this failed, we skipped , otherwise we
read bmin off the cost of the optimal plan and created several limited-budget tasks [C], differing in
their constrainedness level C. Namely, following Nakhost et al. (2012), we set the global budget b
in [C] to b := C  bmin , so that C is the factor by which the available budget exceeds the minimum
needed (to be able to reach the goal at all). We let C range in {1.0, 1.2, . . . , 2.0}.
For AtleastProb, we let  range in {0.1, 0.2, . . . , 1.0} ( = 0 is pointless). For ApproxProb,
we let  range in {0.0, 0.1, . . . , 0.9} ( = 1 is pointless). On cyclic problems, the convergence
parameter  was set to 0.00005 (the same value as used by Kolobov et al., 2011). All experiments
were run on a cluster of Intel E5-2660 machines running at 2.20 GHz, with time/memory cut-offs
of 30 minutes/4 GB.
7.2 Acyclic Planning
We consider first acyclic planning. This pertains to all budget-limited benchmarks, to pentesting
with and without budget limit, as well as to IPPC TriangleTireworld (moves can be made in only
one direction so the state space is acyclic). We consider the 3 objectives MaxProb, AtLeastProb,
and ApproxProb. We run all 16 search algorithm variants (VI, AO , LRTDP, 5 DFHS variants, with
subsets of bounds as applicable), each with up to 5 node selection strategies as explained. For deadend pruning, we run LM-cut, as well as merge-and-shrink (M&S) with the state-of-the-art shrinking
strategies based on bisimulation and an abstraction-size bound N ; we show data for N =  and
N = 100k (we also tried N  {10k, 50k, 200k} which resulted in similar behavior). We also run
variants without dead-end pruning. We use the deterministic-bisimulation (DB) reduced state space
only for VI: once (and if) a bisimulation is successfully computed, the block-state MDP is easily
solved by that simplest algorithm. Given DB, we do not require any dead-end pruning because all
dead-ends are already removed from the reduced state space.
Overall, this yields 577 different possible algorithm configurations. We do not actually test all
these configurations, of course, as not all of them are interesting, or needed to make the essential
observations. We instead organize our experiment in terms of three parts (1)(3), each focusing
on a particular issue of interest. Consider Table 2, which gives an overview of the configurations
considered in each experiment. The design of the experiments is as follows:
13. For modeling/solving the entire network, that is. With their domain-dependent decomposition algorithm 4AL,
trading accuracy for performance, Sarraute et al. scale up much further.

247

fiS TEINMETZ & H OFFMANN & B UFFET

Experiment

Search Algorithm


Pruning

Node selection

# Configs



AO |U ,
MaxProb search & prun- VI, AO |L ,
default
LRTDP|
,
DFHS|
U
U (5), ALL (4)
ing
VI on DB
VI, AO |L ,
AO |U ,

AtLeastProb & Approx- AO |LU ,
LRTDP|U ,
(2)
LM-cut
default
Prob parameters
LRTDP|LU ,
HDP|U ,
HDP|LU , VI on DB
VI, AO |L ,
AO |U ,
ALL (1, 4, 4, 5, 3,

LRTDP|U ,
AtLeastProb & Approx- AO |LU ,
(3)
LM-cut 4, 3, 4, and 1 respecLRTDP|LU ,
HDP|U ,
Prob node selection
tively)
HDP|LU , VI on DB
(1)

37

18

58

Table 2: Overview of algorithms tested on acyclic problems, Section 7.2. Numbers in brackets give
the number of options where that number is not obvious. In (2) and (3), note that the total
number of configurations gets multiplied by 2 because AtLeastProb vs. ApproxProb result
in different algorithm configurations (using different termination criteria). HDP is the
member of our DFHS family, corresponding to Bonet and Geffners (2003a)
DFHSFwdCons
Lab
HDP algorithm.
(1) We first evaluate different search algorithms and dead-end pruning methods on MaxProb, fixing
the node selection strategy to default.
We omit here all X|LU variants, because, as explained earlier, for MaxProb heuristic search,
maintaining V L is redundant (early termination is dominated by regular termination).
Using the default node selection strategy makes sense here because node selection strategies
are relevant only for anytime performance, i.e., early termination. This plays a minor role in
MaxProb, whose only early termination possibility is the exceptional case where the initial state
lower bound becomes V L (I) = 1.
(2) We next fix the best-performing dead-end pruning method, and analyze search algorithm performance in AtLeastProb and ApproxProb as a function of the parameter  respectively .
We again fix the node selection strategy to default here, leaving their examination to experiment
(3).
(3) We finally let the node selection strategies range, keeping otherwise the setting of experiment
(2).
We will conclude our discussion with (4) additional data illustrating typical anytime behavior. Each
part of the experiment is described in a separate sub-section in what follows.
7.2.1 (1) S EARCH A LGORITHMS & P RUNING M ETHODS IN M AX P ROB
Table 3 shows coverage data, i.e., the number of benchmark tasks for which MaxProb was solved
within the given time/memory limits.
Of the pruning methods, LM-cut clearly stands out. For every search algorithm, it yields the
by far best overall coverage. M&S has substantial advantages only in RectangleTireworld and
NoMystery-b. Note that, for N = , overall coverage is worse than for using no pruning at
all. This is due to the prohibitive overhead, in some domains, of computing a bisimulation on the
determinized state space. And, having invested this effort, it pays off more to use the bisimulation
248

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

Domain

#

TriaTire

10

Blocksw-b
Boxworl-b
Drive-b
Elevator-b
ExpBloc-b
Random-b
RecTire-b
Tirewor-b
TriaTire-b
Zenotra-b

66
18
90
90
84
60
36
90
60
36

NoMystery-b 60
Rovers-b
60
TPP-b
60
Pentest-b
Pentest
P

90
15
925

Domain

#

TriaTire

10

Blocksw-b
Boxworl-b
Drive-b
Elevator-b
ExpBloc-b
Random-b
RecTire-b
Tirewor-b
TriaTire-b
Zenotra-b

66
18
90
90
84
60
36
90
60
36

NoMystery-b 60
Rovers-b
60
TPP-b
60
Pentest-b
Pentest
P

90
15
925

DFHSFwd
DFHSFwdCons
|U
DFHSFwd
VI |U
VI
Lab |U
 LM M&S
 LM M&S
 LM M&S

N 
N 
N 
IPPC Benchmarks
9 10 10 10
9 8 8 8 10 10 10 10
9 8 8 8 10
IPPC Benchmarks with Budget Limit
24 28 24 24 24 28 24 24 24 28 24 24 24 28 24 24 24
0 3 0 0
0 3 0 0
0 3 0 0
0 3 0 0
0
90 90 90 52 90 90 90 52 90 90 90 52 90 90 90 52 90
78 86 79 33 79 86 79 33 78 86 79 33 79 86 79 33 78
37 60 39 37 37 60 39 37 36 66 39 37 37 60 39 37 36
36 44 36 33 36 44 36 33 36 44 36 33 36 44 36 33 36
28 31 36 36 30 31 36 36 30 31 36 36 30 31 36 36 30
90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90
46 55 55 55 46 55 55 54 46 55 55 55 46 55 55 54 46
15 17 17 18 15 17 17 18 12 15 14 15 15 17 17 18 14
Probabilistic Resource-Constrained Benchmarks with Budget Limit
12 40 50 50 12 41 50 50 12 42 50 50 12 41 50 50 12
25 46 36 46 25 46 36 46 25 46 36 47 25 46 36 46 25
19 38 27 25 19 38 27 25 20 39 27 25 19 38 27 25 20
Pentesting Benchmarks
57 63 62 37 57 63 62 37 57 63 62 37 57 63 62 37 57
9 9 9 8
9 9 9 8
8 8 8 8
9 9 9 8
9
575 710 660 554 578 709 658 551 574 716 656 552 578 709 658 551 577
DFHSVI |U
 LM M&S
N 

HDP|U
LM M&S
N 
10 10 10
28
3
90
86
66
44
31
90
55
16

24
0
90
79
39
36
36
90
55
16

42 50 50
46 36 47
39 27 25
63 62 37
9 9 8
718 659 553

AO |L
AO |U
LRTDP|U
HDP|U
 LM M&S
 LM M&S
 LM M&S
 LM M&S
N 
N 
N 
N 
IPPC Benchmarks
4 4 4 4
4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10
IPPC Benchmarks with Budget Limit
24 28 24 24 24 28 24 24 24 28 24 24 24 28 24 24 24 28 24 24
0 3 0 0
0 3 0 0
0 3 0 0
0 3 0 0
0 3 0 0
90 90 90 52 90 90 90 52 90 90 90 52 90 90 90 52 90 90 90 52
71 82 72 33 74 84 76 33 65 77 67 33 79 86 79 33 78 86 79 33
32 46 38 37 32 46 38 37 39 57 39 37 38 65 39 37 36 66 39 37
27 33 35 33 39 34 36 33 35 44 36 33 36 44 36 33 36 44 36 33
30 31 36 36 30 31 36 36 30 31 36 36 30 31 36 36 30 31 36 36
90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90
45 52 52 52 45 52 52 52 46 55 55 55 47 57 57 57 46 55 55 55
15 16 16 18 15 16 16 18 14 16 16 17 15 17 16 17 14 16 16 16
Probabilistic Resource-Constrained Benchmarks with Budget Limit
11 37 43 44 11 36 42 43 12 39 47 47 12 41 50 50 12 42 50 50
23 39 31 40 23 38 31 40 23 44 33 45 25 46 35 46 25 46 36 47
18 35 25 25 16 35 24 24 15 37 26 22 19 38 27 25 20 39 27 25
Pentesting Benchmarks
57 63 62 37 57 63 62 37 57 63 63 37 57 63 63 37 57 63 62 37
9 9 9 8
9 9 9 8
9 9 9 8
9 9 9 8
9 9 9 8
546 658 627 533 559 659 630 531 559 693 641 546 581 718 661 555 577 718 659 553
VI
 LM M&S
N 

24
0
52
33
37
33
36
90
55
16

VI
on
DB
10
24
0
52
33
37
33
36
90
60
17
51
50
26
37
8
564

Table 3: Acyclic planning. MaxProb coverage (number of tasks solved within time & memory
limits). Best values, within each table, in boldface. Top: DFHS variants (recall that HDP
is the DFHSFwdCons
member of our DFHS family; DFHSVI is ILAO ). Bottom: remaining
Lab
search algorithms, including also the overall best DFHS variant. Domains -b modified
with budget limit. #: number of instances. : no pruning; else pruning, against
remaining budget on -b domains, based on h =  on other domains. LM: LM-cut;
M&S: merge-and-shrink, N  size bound N = 100k,  no size bound. VI on DB:
VI run on reduced (deterministic-bisimulated) state space. Default node selection.

249

fiS TEINMETZ & H OFFMANN & B UFFET

107

107

106

106

LRTDP|U (LM-cut)

LRTDP|U

as a reduced MDP state space (VI on DB), rather than only for dead-end pruning. An extreme
example of the latter is TriangleTireworld. Far beyond the standard benchmarks in Table 3 (triangleside length 20), VI on DB scales to side length 74 in both the original domain and the limited-budget
version. For comparison, the hitherto best solver by far was Prob-PRP (Camacho et al., 2016), which
scales to side length 70 on the original domain, and is optimal only for goal probability 1, i.e., in the
presence of strong cyclic plans  which holds for the original domain but not for the limited-budget
version. (We could not actually run Prob-PRP on the limited-budget domain version, as Prob-PRP
does not natively support a budget, and hard-coding the budget into PPDDL resulted in encodings
too large to pre-process.)
Comparing the different DFHS|U variants, there is no configuration that clearly stands out.
Overall, they all perform equally well, though the FwdCons variants (cutting off the exploration at
 inconsistent states rather than absorbing states) have a slight edge. This difference mainly comes
from TriangleTireworld, ExplodingBlocks, and TPP-b, where the FwdCons configurations solve more
instances, while in Zenotravel-b the FwdCons configurations perform slightly worse than their counterparts. The termination parameter (VI vs. Label) has almost no effect on coverage. Due to the
gives the best coverage results,
similarity of the DFHS configurations, and because DFHSFwdCons
Lab
as
the
representative
of
the
DFHS
family
in
the remaining discussion. As
we will use DFHSFwdCons
Lab
FwdCons
corresponds to HDP, for simplicity we will from now on refer to it by that name.
DFHSLab
AO |L is better than VI only in case of early termination on V L = 1, when a full-certainty
policy is found before visiting the entire state space. This happens very rarely here, and AO |L is
dominated by VI (this changes for AtLeastProb, see Figures 5a and 7 below). All failures of VI are
due to memory or runtime exhaustion while building the reachable state space. LRTDP|U clearly
outperforms AO |U , presumably because it tends to find absorbing states more quickly. LRTDP|U
and HDP|U are about on par; with LM-cut they solve the exact same number of instances (though
not exactly the same instances), and otherwise HDP|U solves slightly fewer tasks than LRTDP|U .
To gauge the efficiency of heuristic search vs. blind search on MaxProb, compare LRTDP|U vs.
VI in Table 3. Contrary to the intuition that a good initial goal probability estimator is required for
heuristic search to be useful, LRTDP|U is clearly superior. Its advantage does grow with the quality
of the initialization; LM-cut yields the largest coverage increase by far. However, even without
dead-end pruning, i.e., with the trivial initialization of V U , LRTDP|U dominates VI throughout, and
improves coverage in 8 of the 16 domains.

105
104
103

104
103
102

102
101 1
10

105

102

103

104
VI

105

106

101 1
10

107

102

103

104
105
VI (LM-cut)

106

107

Figure 4: Acyclic planning. Number of states visited, for VI (x) vs. LRTDP|U (y), with no pruning
(left) respectively LM-cut pruning (right). Default node selection.
We next shed additional light on this by comparing search space sizes and runtime values.
Tables 4 and 5 provide aggregate data, Figure 4 gives a scatter plot for the canonical comparison
250

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

AO |U
LRTDP|U
LM
M&S

LM
M&S
N

N

IPPC Benchmarks
843.1 843.1 843.1 843.1
0.2 0.2 0.2 0.2
0.8 0.7 0.7 0.7
0.4 0.4 0.4 0.4
3.5 1.7 1.7 1.7
IPPC Benchmarks with Budget Limit
12.7 5.8 2.8 2.8
12 5.2 2.5 2.5 12.3 5.3 2.5 2.5
4.2 2.4 1.8 1.2
4.2 2.2 1.6
1
4 2.1 1.5
1
12.8 3.9 7.2
3
3.3 0.3 0.4 0.1
3.6 0.3 0.4 0.1
1.1K 41.9 92.1 33.2 112.2 1.2 12.2 0.8 117.8 1.4 12.5 1.2
4.1K 213.2 859.9 179 603.1 3.6 133.6 2.6 588
4 106.8 3.2
2.6K 1.5 17.2 1.1 3.1K 2.1 21.3 1.6
4.5 1.7 1.7 1.7
1.9 0.8 0.8 0.8
2 0.8 0.8 0.8
1.0K 130 127.4 127.4 42.6 6.4 6.4 6.4 43.7 6.4 6.4 6.4
2.7K 15.9 2.2 2.2 2.9K 15.9 2.2 2.2
50.6 5.6 1.5 1.5 49.8 5.1 1.2 1.2 50.4 5.1 1.3 1.3
81.9 8.9 2.4 2.4 81.5 8.2 1.9 1.9 81.6 8.3
2
2
1.4K 6.4 6.4 6.4 898.6
3
3
3 896.2 2.9 2.9 2.9
4.1K 229.4 229.4 229.4 1.8K 52.5 52.5 52.5 1.6K 44.5 44.5 44.5
7.4K 610.3 610.3 610.3 4.6K 303.3 303.3 303.3
491.5 30.2 35.8 18.2 491.3 29.9 35.2 17.9 288.2 23.6 27.1 14.1
967.4 104.4 164.6
64 967.1 102.9 161.1 62.3 478.1 75.3 114.9 45.8
Probabilistic Resource-Constrained Benchmarks with Budget Limit
2.8K 6.9 0.5 0.5 2.6K 6.6 0.4 0.4 2.6K 6.4 0.4 0.4
12.4K 122.4 14.1 14.1 12.7K 122.3 16.4 16.4
1.1K 51.8 91.5 22.6 702.9 36.1 58.6 12.4 873.7 38.5 70.8 14.3
2.2K 290.1 512.6 137.6 1.1K 176 281.4 65.9 1.6K 190 366.2 76.3
4.7K 287.1 709.7 205.2 8.3K 338.3 1.0K 242.1
1.1K 49.6 265.4 10.9 660.9
33 183.3 5.7 897.2 38.5 220.7 7.6
3.0K 178.7 894.6 36.6 1.5K 100.4 549.5 14.3 2.1K 120.1 701.9 21.5
Pentesting Benchmarks
19.7 6.3 7.8 6.3 19.5 6.3 7.7 6.3 19.7 6.2 7.7 6.2
238.1 165.1 169.2 165.1 237.2 165.1 169 165.1 238.1 165.1 169.1 165.1
74.3 66.3 66.4 66.3 74.3 66.3 66.4 66.3 74.3 66.3 66.4 66.3
194.3 173.4 173.8 173.4 194.3 173.4 173.8 173.4 194.3 173.4 173.8 173.4


Domain

#

TriaTire
ONLY-H

1
4

Blocksw-b
18
Drive-b
20
Elevator-b
12
ExpBloc-b
18
NON-TRIVIAL 7
ONLY-H
3
Random-b
21
NON-TRIVIAL 4
ONLY-H
2
RecTire-b
18
NON-TRIVIAL 12
TriaTire-b
17
NON-TRIVIAL 6
ONLY-H
1
Zenotra-b
14
NON-TRIVIAL 10
NoMystery-b 11
ONLY-H
1
Rovers-b
21
NON-TRIVIAL 13
ONLY-H
2
TPP-b
9
NON-TRIVIAL 5
Pentest-b
28
NON-TRIVIAL 5
Pentest
3
NON-TRIVIAL 1

VI
LM
M&S
N






HDP|U
LM
M&S
N


2.2 1.8 1.8 1.8
160.3 835.4 835.4 835.4
11.5 4.8 2.3 2.3
4 2.1 1.5 0.9
3.4 0.3 0.4 0.1
150.7 1.2 13.6 0.8
780.4 3.4 125.6 2.2
5.2K 1.9 23.4 1.1
1.9 0.8 0.8 0.8
34.5 5.4 5.4 5.4
2.9K 15.9 2.2 2.2
50.4
5 1.2 1.2
81.5 8.1 1.9 1.9
954 3.4 3.4 3.4
1.8K 67.4 67.4 67.4
6.2K 634.8 634.8 634.8
285.1 23.6 27.4 14.2
468.6 75.3 115.9 46.3
2.7K 6.5 0.4 0.4
12.7K 117.3 14.2 14.2
782.7 35.8 63.2 12.4
1.3K 173.8 318.4 65.9
5.9K 265.5 741 189.5
765.4 31.3 188.1 5.6
1.8K 91.3 561.8
14
19.7 6.2 7.7 6.2
238.1 165.1 169.1 165.1
74.3 66.3 66.4 66.3
194.3 173.4 173.8 173.4

Table 4: Acyclic planning. MaxProb geometric mean search space size (number of states visited) in
multiples of 1000. # gives the size of the instance basis, namely those instances solved
by all shown configurations, skipping instances solved in under 1 second by all configurations. NON-TRIVIAL uses only those instances not solved by VI in < 1 second.
ONLY-H uses those instances commonly solved by AO |U , LRTDP|U , and HDP|U , but
not solved by VI. Rows with empty instance basis are skipped. Default node selection.
between VI and LRTDP|U . Data for AO |L is not shown as its coverage is dominated by VI (cf.
Table 3), and the same goes for its runtime and search space. We include the NON-TRIVIAL rows
in the tables to show behavior on the more interesting instances, where the averages are not skewed
by the many very small instances in most domains. We include the ONLY-H rows to elucidate the
behavior on the most challenging instances beyond reach of VI.
A clear message from Table 4 and Figure 4 is that the heuristic search algorithms, apart from
a few exceptions, visit much fewer states than VI does, even with trivial upper bound initialization where search spaces are reduced in all domains except RectangleTireworld and Pentest. For
instance, using LRTDP|U instead of VI results in a gain of around 1 order of magnitude in many
instances, and larger gains (up to 3 orders of magnitude) also occur in rare cases. By giving the
heuristic search algorithms additional information through earlier dead end detection, the differences become even larger.
251

fiS TEINMETZ & H OFFMANN & B UFFET

Domain

#

TriaTire
ONLY-H

1
4

Blocksw-b
18
Drive-b
20
Elevator-b
12
ExpBloc-b
18
NON-TRIVIAL 7
ONLY-H
3
Random-b
21
NON-TRIVIAL 4
ONLY-H
2
RecTire-b
18
NON-TRIVIAL 12
TriaTire-b
17
NON-TRIVIAL 6
ONLY-H
1
Zenotra-b
14
NON-TRIVIAL 10
NoMystery-b 11
ONLY-H
1
Rovers-b
21
NON-TRIVIAL 13
ONLY-H
2
TPP-b
9
NON-TRIVIAL 5
Pentest-b
28
NON-TRIVIAL 5
Pentest
3
NON-TRIVIAL 1

AO |U
LRTDP|U
HDP|U
LM M&S
 LM M&S
 LM M&S
N

N

N

IPPC Benchmarks
3.5 7.4 4.1
4
0
0 0.1 0.1
0 0 0
0
0 0 0.1 0.1
0.1 0.1 0.5 0.5 0.1 0.3 0.6 0.6
1.4 23.9 12.1 12.1
IPPC Benchmarks with Budget Limit
0.1 0.6 2.5 2.3
1.8 0.8
3 2.8 0.2 0.6 2.3 2.4
0.2 0.5
2 2.1
0 0.2 6.9 14
0.1 0.2
8 15.4
0 0.2 7.1 14.2
0 0.2 6.1 12.3
0.1 0.1 1.8 4.1
0
0 2.2 4.5
0 0 1.8 4.1
0 0 1.7 3.8
6 1 15.5 7.5
1.6
0 16
8 0.8 0.1 14.2 7.1
0.9
0 13.4 6.6
25.3 4.8 36.2 45.7
11.5 0.1 33 45.9
4 0.1 29.7 42.1
5 0.1 27.2 39.6
29.3 0.1 40.9 40.4 21.4 0.1 30.7 34.9 35.1 0.1 30.2 32.5
0.5 0.6 4.8 4.8
0.3 0.3 5.2 5.2 0.3 0.3 4.7 4.8
0.2 0.3 4.4 4.4
13.9 10.1 39.2 43.2
3 0.9 44.4 49.9 1.4 0.8 36.3 43.2
0.8 0.7 35.3 38.2
27.8 11.3 38.1 42.9 30.3 11.1 35.4 37.4 29.8 10.8 34.8 35.2
9 19.4 1.2 1.2
73.6 20.2 1.3 1.3 43.1 17.6 1.3 1.3 131.2 16.4 1.2 1.2
20.4 57.3 2.3 2.3 178.9 61.8 2.4 2.4 106.8 51.4 2.3 2.3 330.1 46.5 2.3 2.3
10.5 0.5 0.6 0.6
14.5 0.4 0.5 0.5 9.5 0.3 0.4 0.4
8.4 0.4 0.4 0.4
27.7 5.9 3.2 3.3
31.3 2.4 2.1
2 14.7
2 1.7 1.6 13.7 2.4 1.8 1.7
153.2 25.4 13.6 13.6 41.5 11.9 5.1 4.4
42 18.2 6.9 7.1
2.7 4.9 15
9
56 5.7 18.9 11.8
13 4.3 15.9 9.2 52.5
5 16.8 9.3
5.6 16.5 27 13.5 163.4 19.3 37.2 18.7 25.3 13.6 30.1 14.5 118.3 16.8 35.2 15.1
Probabilistic Resource-Constrained Benchmarks with Budget Limit
15.6 0.4 0.3 0.3 242.6 0.4 0.3 0.3 27.8 0.4 0.3 0.3 26.2 0.4 0.3 0.3
1623.4 8.9 0.6 0.6 158.8 7.8 0.5 0.4 137.2 7.3 0.4 0.4
9.7 2.3 11.8 17
96.2 2.3 16.5 21.7 12.8
2 11.6 16.1 10.8 1.8 9.9 14.9
20.9 12.9 20.2 33.7 236.6 12 33.3 45.1 24.9 9.7 19.5 30.3 19.2 8.8 15.9 29.1
751.2 19.2 76.9 151.4 127.6 18.6 44.8 126.9 85.4 14.8 34.3 105.1
8.5 1.6 14.9 69.8
63.2 1.3 24.6 70.2 12.7 1.3 16 69.1
9.6 1.1 14.3 65.1
22.4 5.7 18.5 76.2 203.5 4.5 37.3 78.2 31.3 4.2 20.1 76.3 23.2 3.2 17.7 72.4
Pentesting Benchmarks
0 0 6.6 16.5
0.5
0 8.2 19.8
0 0 7.3 18.2
0.3
0 6.5 16.6
3.2 2.2 10.1 92.7
16 4.5 15 108.9
8.5 5.2 15.2 107.6
6.4 4.4 12.7 100
0.9 0.7 3.2 4.4
5.9 2.3 5.7 6.5 3.8
3 5.8 6.3
2.4 2.7 5.1 6.1
2.7
2 6.5 17.2
23 8.1 20.6 23.8
15 10.4 16.6 24.4
9.3 10.6 15.2 24.4
VI
 LM M&S
N 



Table 5: Acyclic planning. MaxProb geometric mean runtime (in CPU seconds). Same setup and
presentation as in Table 4.
As previously hinted, these observations have not been made in this clarity before. While
Kolobov et al. (2011) also report LRTDP to beat VI on MaxProb, they consider only a single domain; they do not experiment with trivially initialized V U ; and they do not use dead-end pruning in
VI, so that LRTDP already benefits from a smaller state space, and the impact of heuristic search
remains unclear.
Even though the search space of the heuristic search algorithms is in many cases only a small
fraction of the whole (dead-end pruned) state space, this is not necessarily reflected in runtime.
On those instances solved by VI, it is typically fast, often faster than heuristic search and rarely
outperformed significantly. This is despite having larger search spaces, i.e., heuristic search does
visit less states but suffers from having to do more updates on these (recall that VI here updates each
visited state exactly once). Significant runtime advantages over VI (in the NON-TRVIAL rows)
are obtained by heuristic search only in ExplodingBlocksb, Randomb, and TriangleTireworldb.
Comparing the heuristic search algorithms, the conclusions are more fine-grained but overall
similar to what we concluded from coverage above. LRTDP|U dominates AO |U almost throughout.
Note that, even though the search space size of AO |U and LRTDP|U almost always is similar, AO |U
requires a lot more time than LRTDP|U . This is because it performs more updates. Across the nontrivial commonly solved instances in the tables, the geometric mean of the number of updates done
252

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

in AO |U is about 4 times higher than that in LRTDP|U . LRTDP|U and HDP|U with non-trivial
value initialization give very similar results, not only in terms of coverage, but also in terms of
runtime and search space size. LRTDP|U is, however, more effective in some of the domains (e.g.,
RectangleTireworld and Zenotravel) if no additional dead-end detection method is used. On the
other hand, HDP|U has a slight edge in the probabilistic resource-constrained domains. One notable
case where LRTDP|U consistently outperforms HDP|U is TriangleTireworld.
The impact of dead-end pruning on VI is typically moderate. The gains for heuristic search are
much more pronounced, thanks to the stronger heuristic function initialization. Especially AO |U
benefits a lot. LRTDP|U and HDP|U benefit as well, but to a smaller extent, partly because they
are already more effective in the first place. Comparing across different dead-end pruning methods,
although M&S with N =  clearly yields the largest search space reductions, and necessarily so as
it recognizes all dead-ends, the overhead of bisimulation computation outweighs the search space
reduction in all but a few cases. In terms of pruning power, M&S with N = 100k and the LM-cut
heuristic are overall roughly similar, yet LM-cut has the edge in runtime.
7.2.2 (2) AT L EAST P ROB AND A PPROX P ROB PARAMETER A NALYSIS
We now turn to the weaker objectives, AtLeastProb and ApproxProb. We fix LM-cut for the (almost
always most effective) dead-end pruning. We examine the power of early termination for different
search algorithms and node selection strategies. This is best viewed as a function of the goal probability threshold  in AtLeastProb, and of the desired goal probability accuracy  in ApproxProb. VI
forms a baseline independent of  (). Consider Figure 5.
VI
LRTDP|U

850

VI

LRTDP|LU

AO |LU

HDP|U

HDP|LU

800

850
825
# solved instances

# solved instances

825

AO |L

AO |U

775
750
725
700
675

AO |L

AO |U

LRTDP|LU

AO |LU

HDP|U

HDP|LU

800
775
750
725
700
675

650

650

625

625
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


LRTDP|U

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0


(a)
(b)
Figure 5: Acyclic planning. Total coverage for AtLeastProb as a function of  in (a), for ApproxProb as a function of  in (b). All configurations use default node selection and LM-cut
dead-end pruning.
For AtLeastProb (Figure 5a), in the interesting region of benchmark instances not feasible for
VI yet sometimes feasible for the other search algorithms, one clear feature is the superiority of
LRTDP over both AO and HDP. As one can see for the smaller values of , LRTDP is able to
update V L much more effectively than HDP, resulting in a larger coverage of LRTDP in the region
253

fiS TEINMETZ & H OFFMANN & B UFFET

of smaller  values. AO |L exhibits strikingly strong behavior for small values of , approaching
(and in one case, surpassing) the performance of LRTDP|U . Evidently, the depth-first expansion
strategy is quite effective for anytime behavior on V L and thus for termination via V L (I)  . It
is way more effective than the heuristic search in AO |LU . As we shall see below (Figure 7), it is
often also more effective than LRTDP. In general, for all algorithms, using V L is a clear advantage
for small . For larger , maintaining V L can become a burden, yet V U is of advantage due to early
termination on V U (I) < . Algorithms using both bounds exhibit an easy-hard-easy pattern.
The spike at the left-hand side in Figure 5 (a), i.e., significantly worse performance for  = 0.1
than for  = 0.2, is an outlier due to the Pentest domains  without these domains, AO |LU ,
LRTDP|LU and HDP|LU exhibit a strict easy-hard-easy pattern. This is because, in contrast to typical probabilistic planning scenarios, in penetration testing the goal probability  the chance of a
successful attack  are typically small, and indeed this is so in our benchmarks. Searches using an
upper bound quickly obtain V U (I) < 0.2, terminating early based on V U (I) <  for  = 0.2. But
it takes a long time to obtain V U (I) < 0.1.
For ApproxProb (Figure 5b), smaller values of  consistently result in worse performance. We
see again the superiority of LRTDP over AO and HDP, and the (relatively, compared to AO |LU )
strong behavior of AO |L in  regions allowing aggressive early termination. Again, the key to
LRTDP beating HDP so clearly is due to LRTDP updating V L much more effectively. HDP|LU can
only improve on HDP|U by a small margin. Nonetheless, we see again the superiority of algorithms
using both bounds over those that dont.
7.2.3 (3) N ODE S ELECTION S TRATEGIES
Figure 6 shows different node selection strategies in AtLeastProb (the relative performance of node
selection strategies is the same in ApproxProb, so we do not include a separate figure for that).
LRTDP|LU (def)
AO |L (BFS)

850

# solved instances

825

AO |LU (def)
AO |LU (h)

AO |L (DFS)
AO |L (h)

AO |LU (o-prob)
HDP|LU (def)

VI

800
775
750
725
700
675
650
625
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


Figure 6: Acyclic planning. Total coverage for AtLeastProb as a function of , varying the node
selection strategy. All configurations use LM-cut dead-end pruning.
For readability, we show only the most competitive base algorithms, AO |L , AO |LU , LRTDP|LU ,
and HDP|LU (as well as the VI baseline). For LRTDP and HDP, we show only default node selection, which consistently works basically as well as the alternatives. For AO |L , we see that the
254

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

depth-first strategy is important (and way beyond breadth-first, which does worse than VI). The
h-bias strategy is marginally, but consistently, better than depth-first. For AO |LU , both the h-bias
and the most-prob-outcome bias are helpful, substantially improving over the default strategy. The
h-bias consistently improves a bit on default AO . The gap-bias and preferred-actions strategies
are not shown as they were consistently slightly worse (apparently, the gap-bias leads to a more
breadth-first style behavior, while preferred actions mainly cause runtime overhead).
7.2.4 (4) A N I LLUSTRATION OF T YPICAL A NYTIME B EHAVIOR
To conclude our discussion of acyclic planning, Figure 7 exemplifies typical anytime behavior, i.e.,
the development of the V L (I) and V U (I) bounds on the initial state value, as a function of runtime,
for LRTDP|LU and AO |L .
1

LRTDP V U LM-cut

LRTDP V L

LRTDP V L LM-cut

AO |

AO |

L

1

L LM-cut

0.8
Probability

Probability

0.8

LRTDP V U

0.6
0.4
0.2

0.6
0.4
0.2

0

0
0

100

200
300
Time (s)

400

500

0

100

(a)

LRTDP V U

LRTDP V U LM-cut

LRTDP V L

LRTDP V L LM-cut

AO |L

AO |L LM-cut

200
300
Time (s)

400

500

(b)

Figure 7: Acyclic planning. Anytime behavior in LRTDP|LU (V and V L ) and AO |L (V L only),
as a function of runtime. Elevators instance 11, without pruning and with LM-cut pruning,
for constrainedness level C = 1.4 (a) respectively C = 1.8 (b). Default node selection.
U

The benefit of LM-cut pruning is evident. Observe that AO |L is way more effective than
LRTDP in quickly improving the lower bound. Indeed, the runs shown here find an optimal policy
very quickly. Across the benchmarks solved by both AO |L and LRTDP, omitting those where both
took < 1 second, in 56% of cases AO |L finds an optimal policy faster than LRTDP. On (geometric) average, AO |L takes 66% of the time taken by LRTDP for this purpose. On the downside,
unless V  (I)  , AO |L must explore the entire state space. Its runs in Figure 7 exhaust memory
for MaxProb. In summary, heuristic search is much stronger in proving that the maximum goal
probability is found, but is often distracting for improving V L quickly.
As both parts of Figure 7 use the same base instance but with different constrainedness levels C,
we can also draw conclusions on the effect of surplus budget. With more budget, more actions can
be applied before reaching absorbing states. This adversely affects the upper bound (consistently
across our experiments), which takes a much longer time to decrease. The lower bound, on the other
hand, often increases more quickly with higher C as it is easier to find goal states.
255

fiS TEINMETZ & H OFFMANN & B UFFET

7.3 Cyclic Planning with FRET
We now consider cyclic planning, pertaining to the standard IPPC benchmarks, and to probabilistic
NoMystery, Rovers, TPP without budget (nor resource-) limit. We run only LRTDP and DFHS, as
AO is restricted to acyclic state spaces. We use the two different variants of FRET described earlier:
FRET-V U as per Kolobov et al. (2011), and our new variant FRET- U . We consider all 3 objectives,
and the same 4 dead-end pruning methods (as LM-cut returns  iff the cheaper heuristic hmax does,
we use hmax here). We do not vary node selection strategies because, like we have seen before, in
LRTDP and DFHS these do not bring a notable advantage over the default strategy. We use the
deterministic-bisimulation (DB) reduced state space with each base algorithm, as some differences
do emerge (in difference to the acyclic case) between VI and the other algorithms, which now need
to run FRET. Again, given DB we do not require any dead-end pruning.
Overall, this yields 305 different possible algorithm configurations. As before, not all of these
are interesting, and we instead organize our experiment in terms of parts focusing on issues of
interest. Specifically, we have parts (1) on MaxProb and (2) on AtLeastProb/ApproxProb as before.
As node selection strategies are not relevant here, we do not have the previous part (3) considering
these. We integrate data illustrating anytime behavior with our discussion of (2). Table 6 gives an
overview of tested configurations.
Experiment
FRET variant
Search Algorithm
Pruning
# Configs
MaxProb search & prun(1)
65
, FRET-V U , FRET- U VI, LRTDP|U , DFHS (5) ALL (4), DB
ing
(2)

VI,
AtLeastProb & ApproxU
U LRTDP| ,
,
FRET-V
,
FRET-
LU
Prob parameters
HDP|LU

LRTDP|U ,
HDP|U ,

hmax

18

Table 6: Overview of algorithms tested on cyclic problems, Section 7.3. Note that VI does not require, and is hence not combined with, FRET; we denote this (not using FRET at all) by 
. In (2), note that the number of configurations gets multiplied by 2 because AtLeastProb
vs. ApproxProb result in different algorithm configurations (using different termination
criteria). All configurations tested use default node selection.
7.3.1 (1) S EARCH A LGORITHMS & P RUNING M ETHODS IN M AX P ROB
Table 7 shows coverage data. As before, the DFHS family is shown at the top, and the remaining
search algorithms, including the most competitive DFHS algorithm which as before is HDP, are
shown at the bottom. We do not vary the FRET variant at the top for space reasons, and as, for
FRET-V U , there were no coverage differences at all across DFHS family members.
Similarly as in the acyclic case, the DFHS configurations stopping exploration at -inconsistent
states give slightly better results than those stopping only at absorbing states. The termination
parameter has almost no effect on coverage: HDP (i.e., DFHSFwdCons
) solves one more task in
Lab
ExplodingBlocks than DFHSFwdCons
,
but
otherwise
the
coverage
is
the
same. Also akin to the
VI
acyclic case, both LRTDP and HDP perform equally well, though now HDP has a slight edge in
combination with FRET- U .
Running the search on a deterministic-bismulation state space is less effective on the cyclic
benchmarks than on the acyclic ones. It gives a clear advantage only in Rovers.
256

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING


Domain

#

Blocksworld
Boxworld
Drive
Elevators
ExplodingBlocks
Random
RectangleTireworld
Tireworld
Zenotravel
NoMystery
Rovers
TPP
P

Domain
Blocksworld
Boxworld
Drive
Elevators
ExplodingBlocks
Random
RectangleTireworld
Tireworld
Zenotravel
NoMystery
Rovers
TPP
P

15 4
15 0
15 4
15 15
15 5
15 6
14 14
15 15
15 3
10 4
10 9
10 8
164 87

#

FRET- U
DFHSVI |U
DFHSFwdCons
|U
DFHSFwd
HDP|U
VI
Lab |U
hmax M&S on 
on  hmax M&S on  hmax M&S on  hmax M&S
N  BS
N  BS
N  BS
N  BS
N 
IPPC Benchmarks
4 4 4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0
15 15 6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6
15 15 5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5
12 5 4 4 5 12 5 4 4 5 14 5 4 4 5 12 5 4 4 5 15 5 4
6 1 0 0 6
6 2 0 0 6
6 1 0 0 6
6 2 0 0 6
6 1 0
14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14
15 15 12 11 15 15 15 12 11 15 15 15 12 11 15 15 15 12 11 15 15 15 12
3 3 1 1 3
3 3 1 1 3
3 3 1 1 3
3 3 1 1 3
3 3 1
Probabilistic Resource-Constrained Benchmarks
4 4 4 0 4
4 4 4 0 4
4 4 4 0 4
4 4 4 1 4
4 4 4
9 9 8 9 9
9 9 8 9 9
9 9 8 9 9
9 9 8 9 9
9 9 8
8 8 6 6 8
8 8 6 6 8
8 8 6 6 8
8 8 6 6 8
8 8 6
105 93 64 60 98 105 94 64 60 98 107 93 64 60 98 105 94 64 61 98 108 93 64
DFHSFwd
VI |U
hmax M&S

4
0
15
15
4
0
14
10
3

4
0
15
15
6
0
14
10
3

4
0
15
15
4
0
14
10
3

10 5
10 5
10 6
164 81

5
5
6
83

5
5
6
81

15
15
15
15
15
15
14
15
15

FRET-V U
FRET- U
LRTDP|U
HDP|U
LRTDP|U
HDP|U
 hmax M&S on  hmax M&S on  hmax M&S on  hmax M&S
N  DB
N  DB
N  DB
N 
IPPC Benchmarks
4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4
0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0
6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6
5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5
4 4 4
6 4 4 4 4
6 4 4 4 5 14 5 4 4 5 15 5 4
0 0 0
0 0 0 0 0
0 0 0 0 4
4 0 0 0 6
6 1 0
14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14
10 11 10 11 10 10 11 10 10 10 10 11 15 15 15 12 11 15 15 15 12
1 0 3
3 3 1 1 3
3 3 1 1 3
3 3 1 1 3
3 3 1
Probabilistic Resource-Constrained Benchmarks
5 5 5
5 5 5 5 5
5 5 5 5 4
4 4 4 1 4
4 4 4
5 9 5
5 5 5 9 5
5 5 5 9 9
9 9 8 9 9
9 9 8
6 6 6
6 6 6 6 6
6 6 6 6 8
8 8 6 6 8
8 8 6
60 64 81 84 81 60 65 81 83 81 60 65 96 105 92 64 61 98 108 93 64

VI
 hmax M&S on
N  DB

on
BS
4
0
6
5
4
0
14
11
1
1
9
6
61

on
DB
4
0
6
5
4
0
14
11
1
1
9
6
61

Table 7: Cyclic planning. MaxProb coverage. Best values, within each table, in boldface. FRETV U is as per Kolobov et al. (2011), FRET- U is our modified version. Top: DFHS variants
member of our DFHS family; DFHSVI is ILAO );
(recall that HDP is the DFHSFwdCons
Lab
showing only the dominating FRET version, FRET- U . Bottom: remaining search algorithms, varying the FRET version, and including also the overall best DFHS variant.
Dead-end pruning variants:  none, else based on heuristic value , for hmax respectively merge-and-shrink (N  size bound N = 100k,  no size bound). on DB: run
on reduced (deterministic-bisimulated) state space. Default node selection.
The most striking result here by far is that FRET- U outperforms both VI and FRET-V U substantially. Note that, in all domains except ExplodingBlocks and Rovers, the advantage over VI is
obtained even without dead-end pruning, i.e., for trivial initialization of V U . This strongly confirms
the power of heuristic search even in the absence of good admissible goal probability estimators.
As before, we shed additional light on the coverage results through search space size and runtime
data. Figure 8 compares the search space sizes for VI vs. FRET- U . The non-trivial initialization
using hmax is useful, but gains of up to 3 orders of magnitude are possible even without it.
Table 8 provides aggregate search space size and runtime data. No data is shown for the configuration using FRET-V U with HDP, as that data is almost identical to that of FRET-V U with LRTDP:
257

fi107

107

106

106
FRET- U (hmax )

FRET- U

S TEINMETZ & H OFFMANN & B UFFET

105
104
103

105
104
103
102

102
101 1
10

102

103

104
VI

105

106

101 1
10

107

102

103

104
105
VI (hmax )

106

107

Figure 8: Cyclic planning. Number of states visited, for VI (x) vs. FRET- U using LRTDP|U (y),
with no pruning (left) respectively hmax pruning (right).
FRET-V U
FRET- U
LRTDP|U
LRTDP|U
M&S
 hmax
M&S
 hmax
M&S

N

N

N

IPPC Benchmarks
2.8
2.7
0 0.1
2.7
2.8
0.1
0.1
3 2.8
0.1
6 42.8
0
0 5.8 33.1
0
0 6.1 42.8
0
2.2
1.7
0
0 2.2
1.8
0
0 2.2
1.9
0
19.3 18.1 15.9 0.4 31.5 17.7 15.7
0 28.8 17.5
3.5
45.1 110.4 82.2 0.8 112.4 102.6 72.9
0 104.1 110.1 14.3
4.7
4.7
3.9
4 4.7
4.7 19.8
4.1
4.7
4.7 20.4
9.1
9
7.6 7.9
9.1
9
53 8.1
9.1
9.2 55.5
14
35
60 55.3 60.2 86.4
0
0 3.8 24.7
0
19.5 55.7 92.1 84.5 89.8 136
0
0 4.7 39.8
0
96.3 283.8
7 12.6 54.3 241.1
0.2
0.2 43.5 227.4
0.2
Probabilistic Resource-Constrained Benchmarks
29.1 69.4 133.9 127 141.4 166.7 627.3 582.4 676.4 618.7 632.1
39.1 42.7 439.8 420.3 435.2 425.1
1.8
1.3
6.2
8.3
1.8
20.1 44.8 140.1 125.8 136.6 156.3 32.9 18.3 63.2 71.3 32.9
31.6 83.5 259.1 241 253.2 299.1 52.5 31.8 118.3 138.8 52.9
IPPC Benchmarks
1.1
1.1
1.1 1.1
1.1
1.1
1.1
1.1
1.1
1.1
1.1
0.3
0.3
0.3 0.2
0.2
0.2
0.3
0.2
0.2
0.2
0.3
0.9
0.9
0.9 0.9
0.9
0.9
0.2
0.2
0.2
0.2
0.2
252.3 39.9 408.5 20.1 242.2 16.9 44.3
0.2
14 0.1 44.4
2.0K 142.4 2.0K 34.6 2.0K 32.4 133.6
0.2 133.6
0.2 133.7
0
0
0.7 0.2
0
0
0.7
0.2
0
0
0.7
0
0
1 0.3
0
0
1 0.3
0
0
1
1.2K 1.2K 1.2K 974.7 974.7 974.7
0.5
0.2
0.2
0.2
2.4
1.7K 1.7K 1.7K 1.4K 1.4K 1.4K
0.4
0.2
0.2
0.2
2.7
309.3 309.3 309.3 309.3 309.3 309.3
2.7
2.7
2.7
2.7
2.7
Probabilistic Resource-Constrained Benchmarks
2.6K 2.6K 2.6K 2.6K 2.6K 2.6K 433 430.8 433 430.8 432.6
2.8K 2.8K 2.8K 2.8K 2.8K 2.8K 15.2 14.8 15.1 14.8 15.2
1.3K 1.3K 1.3K 1.3K 1.3K 1.3K 112.6 89.2 95.7 89.2 112.6
2.3K 2.3K 2.3K 2.3K 2.3K 2.3K 149.2 127.2 138.5 127.2 149.2

VI
 hmax
Domain

#

Blocksworld
Drive
Elevators
ExplodingBlocks
NON-TRIVIAL
RectangleTireworld
NON-TRIVIAL
Tireworld
NON-TRIVIAL
Zenotravel

4
1
5
4
2
6
4
8
7
1

0
0
0
2.6
14.2
3.7
7.2
7.3
11
55.7

0
0
0
0.7
2.7
4
7.9
10.9
16.5
49.3

NoMystery
Rovers
TPP
NON-TRIVIAL

4
5
6
5

21.5
33.1
11.8
21.6

29.8
40.2
14.4
26.2

Blocksworld
Drive
Elevators
ExplodingBlocks
NON-TRIVIAL
RectangleTireworld
NON-TRIVIAL
Tireworld
NON-TRIVIAL
Zenotravel

4
1.1
1.1
1
0.3
0.3
5
0.9
0.9
4 408.5 46.5
2 2.0K 152.6
6
0.7
0.2
4
1
0.3
8 1.2K 1.2K
7 1.7K 1.7K
1 309.3 309.3

NoMystery
Rovers
TPP
NON-TRIVIAL

4
5
6
5

2.6K
2.8K
1.3K
2.3K

2.6K
2.8K
1.3K
2.3K

HDP|U
hmax
M&S
N

0.1
3.1
2.9
0 7.2 33.5
0 2.2
1.8
0 18.9 18.3
0 44.4 107.1
4.1
4.7
4.7
8.1
9.2
9.1
0 3.8 24.9
0 4.6 39.9
0.2
51 233.9
580.4 634.2 628.5
1.3
6.1
8.3
18.5 64.1 70.3
32.3 120.1 137.1
1.1
1.1
0.2
0.2
0.2
0.2
0.2
14
0.2 133.7
0.2
0
0.3
0
0.5
0.5
0.5
0.5
2.7
2.7

1.1
0.2
0.2
0.1
0.2
0
0
0.5
0.5
2.7

418.8 432.6 418.8
14.9 15.1 14.9
89.2 95.7 89.2
127.2 138.5 127.2

Table 8: Cyclic planning. Top: MaxProb geometric mean runtime (in CPU seconds). Bottom:
MaxProb geometric mean search space size (number of states visited) in multiples of 1000.
Similar setup and presentation as in Table 4: # gives the size of the instance basis. The
default are commonly solved instances, skipping trivial ones. NON-TRIVIAL uses only
those instances not solved by VI in < 1 second. (ONLY-H not shown, see text.)

258

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

the search space sizes are exactly the same, and runtimes differ only by a few seconds. In difference
to Tables 4 and 5, we do not include ONLY-H rows, because this would not be interesting here:
FRET-V U hardly solves more instances than VI, so would have to be excluded from these rows; but
then, the data would compare only LRTDP vs. HDP, which perform very similarly anyway.
Most striking in Table 8 is the consistency with which, and the extent by which, FRET- U visits
less states than its competitors (for both LRTDP and HDP). This advantage typically yields better
runtimes as well, with the notable exception of NoMystery, where the larger number of FRET iterations results in a substantial slow-down, despite the much smaller search space: While FRET-V U
with LRTDP only requires 11 FRET iterations on average, in the NoMystery instances commonly
solved with FRET- U and LRTDP, the latter configuration requires over 20000 iterations on average. Similarly when using HDP.
The impact of dead-end pruning is notably smaller than in the acyclic case: search spaces are
reduced substantially in only a single domain, ExplodingBlocks. In the other domains, there either
is no reduction, or a minor/moderate one only.
VI (Kolobov)
VI
VI (hmax )
FRET-V U (Kolobov)
FRET-V U (hmax )
FRET- U (hmax )

104
103

106

Time (s)

States visited

108

105

104

102
101
100

102
1

2

3
4
Problem #

5

101

6

1

2

3
4
Problem #

5

6

(a)
(b)
Figure 9: Cyclic planning. Results on ExplodingBlocks, as shown by Kolobov et al. (2011): FRET
vs VI, (a) number of states visited, (b) runtime in CPU seconds, as a function of the
IPPC instance index. Different variants included for comparison. The data for Kolobov
et al. is taken from their paper (as this code is not available anymore), hence the runtime
comparison is modulo the different computational platforms, and should be treated with
care. All shown FRET configurations use LRTDP|U , with default node selection.
ExplodingBlocks also happens to be the single domain Kolobov et al. (2011) experimented with.
Figure 9 provides a detailed comparison to Kolobov et al.s data, which is the only state of the art
measure provided by previous work. We use here the exact runtime/search space size data reported
by Kolobov et al.; recall that their source code is not available anymore.
Kolobov et al. (2011) ran VI with no pruning vs. FRET-V U using LRTDP with pruning based
on SixthSense (Kolobov et al., 2010). They observed a coverage of 4 for the former and of 6 for
the latter, identical with our results for VI  vs. FRET-V U using LRTDP with hmax . To give more
259

fiS TEINMETZ & H OFFMANN & B UFFET

detail, Figure 9 shows the number of states visited, and the total runtime, in terms of plots over IPPC
instance index as done by Kolobov et al (2011).
Consider first Figure 9 (a), the search space size. The only difference between VI (Kolobov) and
our VI here is the different task/state representation resulting from the respective implementation
framework, the FD framework being somewhat more effective. The substantially better performance
of VI with hmax dead-end pruning shows that the omission of Kolobov et al.s (2011) study, using
dead-end pruning in FRET but not in VI, indeed obfuscates the possible conclusions regarding the
effect of heuristic search vs. the effect of the state pruning itself: with hmax pruning, VI is almost
as effective as FRET-V U using the same pruning. Kolobov et al.s FRET-V U also is very close to
this, except for exploring significantly less states in the large instances. The latter shows, especially
given the more effective representation in FD, that SixthSense is a stronger dead-end detector here
than hmax . That is hardly surprising, considering the information sources in SixthSense  outcomes
of (determinized) classical planning for guidance, and h2 (Graphplan) based validity tests.
On the other hand, SixthSenses information sources are much more time-intensive than hmax ,
which presumably is the reason for the runtime picture in Figure 9 (b). The latter is qualitatively
very similar to (a), except that FRET-V U (Kolobov) is significantly worse, rather than better, on the
largest instance. This last conclusion should be taken with a grain of salt though, given the different
computational environments.
Certainly, given the clarity of FRET- U s advantage in both search space size and runtime, one
can conclude that this variant of FRET substantially improves over the previous state of the art.
7.3.2 (2) AT L EAST P ROB AND A PPROX P ROB PARAMETER A NALYSIS
For the weaker objectives AtLeastProb and ApproxProb, as before we examine coverage as a function of  respectively . Figure 10 shows the data.
For FRET-V U , the behavior in Figure 10 is similar to that for the acyclic case in Figure 5. In
particular, when maintaining both an upper and a lower bound, FRET-V U exhibits an easy-hardeasy pattern due to the advantages of early termination.
For FRET- U , though, the curves are flat over , and the only observation is a small advantage
of using V L in addition to V U . This is due to the scaling of benchmarks, combined with an extreme
performance loss at some point in the scaling: in each domain, there is an instance number x so
that, below x, FRET- U can solve all instances completely (i.e., solving MaxProb), while above x
neither V L (I) nor V U (I) can be improved at all, remaining 0 respectively 1 up to the time/memory
limit. On smaller instances, we do get the expected anytime behavior. Figure 11 exemplifies this.
The easy-hard-easy pattern would thus emerge for smaller runtime/memory limits.14

8. Conclusion
Optimal goal probability analysis in probabilistic planning is a notoriously hard problem, to the
extent that the amount of work addressing it is limited. Our investigation contributes a comprehensive design space of known and adapted algorithms addressing this problem, designing several new
algorithm variants along the way, and establishing an FD implementation basis supporting the tight
integration of MDP heuristic search with classical planning techniques. Our experiments clarify the
14. Figure 11 (b) considers the largest instance feasible when using hmax pruning. Figure 11 (a) considers the secondlargest instance feasible without pruning: on the largest one feasible without pruning, namely instance 05, the maximum goal probability is 1 so the anytime curve for V U is not interesting.

260

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

FRET-V U LRTDP|U

FRET- U LRTDP|U

FRET-V U LRTDP|U

FRET- U LRTDP|U

FRET-V U LRTDP|LU
FRET-V U HDP|U
FRET-V U HDP|LU
VI

FRET- U LRTDP|LU
FRET- U HDP|U
FRET- U HDP|LU

FRET-V U LRTDP|LU
FRET-V U HDP|U
FRET-V U HDP|LU
VI

FRET- U LRTDP|LU
FRET- U HDP|U
FRET- U HDP|LU

110
# solved instances

# solved instances

110

100

90

100

90

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0


(a)

(b)

1

1

0.8

0.8
Probability

Probability

Figure 10: Cyclic planning. Total coverage for AtLeastProb as a function of  in (a), for ApproxProb as a function of  in (b). All configurations use default node selection and hmax
dead-end pruning.

0.6
0.4

0.6
0.4

0.2

0.2

0

0
0

20

40
Time (s)

0

60

(a)

100

200
300
Time (s)

400

(b)
FRET- U

Figure 11: Cyclic planning. Anytime behavior of
with LRTDP|LU and HDP|LU , with
default node selection, (a) without pruning for ExplodingBlocks instance 04, and (b)
with hmax pruning for instance 15.
empirical state of the art, and exhibit substantial improvements thanks to new techniques and technique combinations. They furthermore showcase the opportunities arising from naturally acyclic
problems, and from early termination on criteria weaker than maximum goal probability.
We hope that these encouraging results and new implementation basis will inspire renewed
interest and research in this important problem. There are many promising future directions, of
which we would like to emphasize:
 Advanced admissible goal probability estimators. These could be obtained, e.g. from abstractions interpreted as bounded-parameter MDPs (Givan, Leach, & Dean, 2000). A promis261

fiS TEINMETZ & H OFFMANN & B UFFET

ing approach is to extend state-of-the-art classical-planning abstraction techniques  pattern
databases (Edelkamp, 2001; Haslum, Botea, Helmert, Bonet, & Koenig, 2007), merge-andshrink (Helmert et al., 2014), Cartesian abstractions (Seipp & Helmert, 2013, 2014)  to the
probabilistic setting.
 Hybrids of heuristic search with Monte-Carlo tree search. This appears a promising option
to improve anytime behavior, with respect to the upper and/or lower bound, and thus foster
early termination. Inspiration could be taken here from existing such hybrids, geared toward
other purposes (Keller & Eyerich, 2012; Bonet & Geffner, 2012; Keller & Helmert, 2013).
 Exploiting dominance relations. Goal probability can only be higher in dominating states,
raising the opportunity to prune dominated regions and/or transfer upper/lower bounds across
states. State domination is ubiquitous in limited-budget planning (and resource-constrained
planning). More general domination relations have been shown to exist also in many other
classical planning problems (Torralba & Hoffmann, 2015), and the transfer of these techniques to the probabilistic case, via all-outcomes determinization, should be straightforward.
Last but not least, simulated penetration testing is an application worth algorithms research in its
own right. The basic idea is to exploit the particular structure of such models, specifically their
partially delete-relaxed behavior. A characterizing property of simulated penetration testing is that
any action, once applicable, remains applicable until it is first executed (once the attacker gets into a
position enabling an exploit, that exploit remains enabled). Hence, like in delete-relaxed planning,
to find an optimal solution, navely we will branch over that same action at every state ever after. To
combat this, there are at least three interesting directions. Following Pommerening and Helmerts
(2012) methods for computing h+ , different branching schemes might apply, the challenge being
to maintain value function correctness. Following Gefen and Brafmans (2012) methods for computing h+ , partial-order reduction could be adapted, the challenge being to deal with the action
interference entailed by a shared budget. Finally, methods specific to the probabilistic setting may
apply: intuitively, to preserve optimality, certain actions need to be attempted only if an alternate
goal path failed. This suggests to identify, and branch at, only particular critical points along any
search path.

Acknowledgments
This work was partially supported by the German Research Foundation (DFG), under grant HO
2169/5-1, Critically Constrained Planning via Partial Delete Relaxation, as well as by the Federal Ministry of Education and Research (BMBF) through funding for the Center for IT-Security,
Privacy and Accountability (CISPA) under grant 16KIS0656. We thank Christian Muise for his
Probabilistic-PDDL extension of the FD parser. We thank Andrey Kolobov for discussions. We
thank the anonymous reviewers, whose comments helped to improve the paper.

Appendix A. Depth-First Heuristic Search for Cyclic Problems
The pseudo-code of the family of depth-first heuristic search algorithms (DFHS) for general (cyclic)
probabilistic planning problems is shown in Figure 12.
262

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

procedure GoalProb-DFHS
 := {I};
loop do
[early termination criteria exactly as in GoalProb-AO ]
if (Label and I is not labeled as solved)
or (VI and  U changed after running VI on the  U -greedy graph) then
index := 0
DFHS-Exploration(I)
set IDX of visited states to 
clean stack and visited
else return  U endif /* regular termination */
endloop
procedure DFHS-Exploration(s):
if s 6  then Initialize(s) endif
if s  S>  S or s is labeled solved then
label s solved
return 
endif
f lag := 
if FW then
if V U (s) is not  consistent then f lag := > endif
update V U (s),  U (s), V L (s),  L (s)
if Consist and f lag then return > endif
endif
s.IDX := index; s.lowlink := index
push s onto stack; mark s as visited
index := index + 1
foreach t with P (s,  U (s), t) > 0 do
if t.IDX =  then
f lag := DFHS-Exploration(s)  f lag
if t.IDX <  and t.lowlink < s.lowlink then s.lowlink := t.lowlink endif
else if t is on stack and t.IDX < s.lowlink then s.lowlink := t.IDX endif
done
if f lag or FW then
if V U (s) is not -consistent then f lag := > endif
update V U (s),  U (s), V L (s), and  L (s)
endif
if Label and f lag and s.IDX = s.lowlink then
while forever
t := stack.pop()
label t solved
if t = s then break endif
done
endif
return f lag

Figure 12: Depth-First Heuristic Search (DFHS) for general (cyclic) MaxProb, AtLeastProb, and
ApproxProb.

Appendix B. Landmarks Pruning: Admissible Heuristic vs. Budget Reduction
As stated, Domshlak and Mirkis (2015) problem reformulation, pruning states based on a global
budget reduced using disjunctive action landmarks, is equivalent, regarding the states pruned by
the method on its own, to the much simpler method using the same landmarks for pruning against
263

fiS TEINMETZ & H OFFMANN & B UFFET

the remaining original budget. We now give this argument, previously made only for unit costs
and pairwise disjoint landmarks, for the general setting. We assume a classical planning setup for
simplicity. The arguments in probabilistic and oversubscription setups are essentially the same.
Assume a STRIPS planning task  = (F, A, I, G), with action costs c(a) and with a global
budget b. We use a notation following admissible landmark heuristics as per Karpas and Domshlak
(2009). Let L be a set of disjunctive action landmarks for I, i.e., for every l  L and every
action sequence ~a leading from I to the goal, ~a touches l (there exists a  l used on ~a). Let
furthermore
cp : A  L 7 R+
0 be a cost partitioning, i.e., a function satisfying, for each a  A,
P
that lL c(a, l)P
 c(a). Denote h(l) := minal cp(a, l), and for a subset L0  L of landmarks
denote h(L0 ) := lL0 h(l). Intuitively, each landmark l  L is assigned a weight h(l) via cp, and
the admissible heuristic value h(L) for I is obtained by summing up these weights.
We now describe Domshlak and Mirkis (2015) pruning technique in these terms. Domshlak
and Mirkis formulation is in terms of a compilation into a planning language, which is more complicated, but is equivalent to our formulation here as far as the pruning is concerned.
Domshlak and Mirkis technique maintains the non-used landmarks as part of states. Namely,
for a state s reached on path ~a, l  L is non-used in s iff ~a does not touch l. We denote the set of nonused landmarks in s by L(s). Obviously, the l  L(s) are landmarks for s. Note also that, as L(s)
is part of the state, even if two search paths lead to the same end state but use different landmarks,
their end states are considered to be different. This restriction arises from the compilation approach,
where the book-keeping of landmarks must happen inside the language, i.e., inside states. One
could formulate the pruning technique without this restriction; we get back to this below.
The pruning technique now arises from the interplay of a reduced global budget and reduced
action costs depending on non-used landmarks. Define the reduced global budget as b0 := b  h(L).
For any action a, denote by L(a) the set of landmarks a participates in, i.e., L(a) := {l | l  L, a 
l}. For any state t during search, and an applicable action a, the transition from t to t[[a]] has a
reduced cost, namely the cost c(a)  h(L(a)  L(t)). In words, we reduce the cost of a by the
(summed-up) weight of the non-used landmarks a participates in.
Consider now some state s during search. Denote the remaining reduced budget in s by b0 (s).
Say that we prune s iff b0 (s) < 0.15 Consider any path ~a ending in s. As non-used landmarks
are part of the state, all these paths must touch the
P same subset of landmarks from L, namely
L \ L(s). Denote the actual cost of ~a by c(~a) := a~a c(a). Relative to this cost, the cost saved
thanks to the cost reduction is exactly h(L \ L(s)), the weight of the touched landmarks. Therefore,
b0 (s) = b0  (c(~
a)  h(L \ L(s))) =P(b  h(L))  c(~a) + h(L \ L(s)). By P
definition of h,
P
this equals (b  lL h(l))  c(~a) + lL\L(s) h(l), which equals b  c(~a)  lL(s) h(l) =
b  c(~a)  h(L(s)). Thus, s is pruned, b0 (s) < 0, iff b  c(~a) < h(L(s)). The latter condition is the
same as b(s) < h(L(s)), which is exactly the pruning condition resulting from using h(L(s)) as an
admissible heuristic function pruning against the remaining budget.
In a non-compilation setting, one could, as is indeed customary in admissible landmark heuristics, handle landmarks in a path-dependent manner. That is, non-used landmarks are maintained as
15. Domshlak and Mirkis (2015) do not maintain the remaining budget as part of the state, but instead prune if g(s) >
b0 . This is, obviously, equivalent, except that duplicate detection is more powerful as it compares states based on
their facts F (s) only. For the purpose of our discussion here, this does not make a difference. Note that, in the
probabilistic setting, we do have to distinguish states based on both F (s) and b(s), as goal probability depends on
both so maintaining only the best way of reaching F (s) does not suffice to compute the exact goal probability of the
initial state.

264

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

annotations to states rather than as part of them, and multiple search paths may end in the same state
s but use different landmarks. The set of remaining landmarks L(s) for s then is the union over
those for each individual path; that is, l  L is non-used in s iff there exists at least one path that
does not touch l. This still suffices to show that l is a landmark for s. The landmark heuristic approach as per Karpas and Domshlak (2009) does this kind of book-keeping, and uses the admissible
heuristic value h(L(s)).
If one were to apply Domshlak and Mirkis (2015) reformulation technique without maintaining
landmarks as part of state, then the notion of transition-cost reduction would have to become more
complicated (lest one loses information). This is because, if s is reached on a~1 with a reduced
cost due to touching landmark l1 , but later on we find another path a~2 to s that does not touch l1 ,
then l1 actually still is a valid landmark for s, and therefore there was no need to reduce the cost
on a~1 . To account for this, we would have to revise path costs posthoc, every time a new path
to s becomes available. After these revisions, the cost reduction on each path ~a to s is exactly
h(L \ L(s)): the weight of the non-used landmarks L(s) is no longer subtracted, and the weight of
the other landmarks L \ L(s) is subtracted on every ~a because, by definition, every ~a touches every
l  L \ L(s). So the cost saved on every path ~a to s, relative to ~a, is exactly h(L \ L(s)), from
which point the same arguments as above apply to show that the pruning is equivalent to pruning
via b(s) < h(L(s)). (This is a stronger pruning method than what we would get without posthoc
path cost revision.)
In summary, s based on reduced remaining budget b0 (s) < 0 is equivalent to pruning s based on
original remaining budget vs. the landmark heuristic b(s) < h(L(s)). It should be noted, though,
that such pruning is not the only benefit of Domshlak and Mirkis (2015) reformulation technique.
The technique allows to compute another, complementary, admissible heuristic h on the reformulated task 0 (and this is what Domshlak and Mirkis point out as part of the motivation, and what
they do in practice). From our perspective here, the landmark heuristic and h are used additively
for admissible pruning against the remaining budget, where additivity is achieved with a method
generalizing cost partitionings: in 0 , the cost-reduced variant of each action can be applied only
once. So if h does not abstract away this constraint, and if h uses an action twice, then it employs
the reduced cost only once, yet pays the full cost the second time. Exploring this kind of generalized
cost partitioning in more detail is an interesting research line for future work.

References
Altman, E. (1999). Constrained Markov Decision Processes. CRC Press.
Baier, C., Groer, M., Leucker, M., Bollig, B., & Ciesinski, F. (2004). Controller Synthesis for
Probabilistic Systems (Extended Abstract), pp. 493506. Springer US, Boston, MA.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.
Bertsekas, D. (1995). Dynamic Programming and Optimal Control, (2 Volumes). Athena Scientific.
Bertsekas, D., & Tsitsiklis, J. (1996). Neurodynamic Programming. Athena Scientific.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(12),
533.
265

fiS TEINMETZ & H OFFMANN & B UFFET

Bonet, B., & Geffner, H. (2003a). Faster heuristic search algorithms for planning with uncertainty
and full feedback. In Gottlob, G. (Ed.), Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI03), pp. 12331238, Acapulco, Mexico. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2003b). Labeled RTDP: Improving the convergence of real-time dynamic
programming. In Giunchiglia, E., Muscettola, N., & Nau, D. (Eds.), Proceedings of the 13th
International Conference on Automated Planning and Scheduling (ICAPS03), pp. 1221,
Trento, Italy. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2005). mgpt: A probabilistic planner based on heuristic search. Journal
of Artificial Intelligence Research, 24, 933944.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: A unified approach to heuristic search
in deterministic and non-deterministic settings, and its application to MDPs. In Long, D., &
Smith, S. (Eds.), Proceedings of the 16th International Conference on Automated Planning
and Scheduling (ICAPS06), pp. 142151, Ambleside, UK. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2012). Action selection for MDPs: Anytime AO* versus UCT. In Hoffmann, J., & Selman, B. (Eds.), Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI12), Toronto, ON, Canada. AAAI Press.
Bryce, D., & Buffet, O. (2008). 6th international planning competition: Uncertainty part. In Proceedings of the 6th International Planning Competition (IPC08).
Camacho, A., Muise, C., & McIlraith, S. A. (2016). From FOND to robust probabilistic planning: Computing compact policies that bypass avoidable deadends. In Coles, A., Coles, A.,
Edelkamp, S., Magazzeni, D., & Sanner, S. (Eds.), Proceedings of the 26th International
Conference on Automated Planning and Scheduling (ICAPS16). AAAI Press.
Chatterjee, K., Chmelik, M., Gupta, R., & Kanodia, A. (2015). Optimal cost almost-sure reachability in POMDPs. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the 29th AAAI Conference
on Artificial Intelligence (AAAI15), pp. 34963502. AAAI Press.
Chatterjee, K., Chmelik, M., Gupta, R., & Kanodia, A. (2016). Optimal cost almost-sure reachability in POMDPs. Artificial Intelligence, 234, 2648.
Coles, A. J. (2012). Opportunistic branched plans to maximise utility in the presence of resource
uncertainty. In Raedt, L. D. (Ed.), Proceedings of the 20th European Conference on Artificial
Intelligence (ECAI12), pp. 252257, Montpellier, France. IOS Press.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). A hybrid LP-RPG heuristic for modelling
numeric resource flows in planning. Journal of Artificial Intelligence Research, 46, 343412.
Coles, A. J., Coles, A., Garca Olaya, A., Jimenez, S., Linares Lopez, C., Sanner, S., & Yoon, S.
(2012). A survey of the seventh international planning competition. The AI Magazine, 33(1).
Dai, P., Mausam, Weld, D. S., & Goldsmith, J. (2011). Topological value iteration algorithms.
Journal of Artificial Intelligence Research, 42, 181209.
Dean, T. L., & Givan, R. (1997). Model minimization in markov decision processes. In Kuipers,
B. J., & Webber, B. (Eds.), Proceedings of the 14th National Conference of the American
Association for Artificial Intelligence (AAAI97), pp. 106111, Portland, OR. MIT Press.
266

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

Domshlak, C., & Mirkis, V. (2015). Deterministic oversubscription planning as heuristic search:
Abstractions and reformulations. Journal of Artificial Intelligence Research, 52, 97169.
Drager, K., Finkbeiner, B., & Podelski, A. (2009). Directed model checking with distancepreserving abstractions. International Journal on Software Tools for Technology Transfer,
11(1), 2737.
Edelkamp, S. (2001). Planning with pattern databases. In Cesta, A., & Borrajo, D. (Eds.), Proceedings of the 6th European Conference on Planning (ECP01), pp. 1324. Springer-Verlag.
Gefen, A., & Brafman, R. I. (2012). Pruning methods for optimal delete-free planning. In Bonet,
B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd International
Conference on Automated Planning and Scheduling (ICAPS12). AAAI Press.
Givan, R., Leach, S. M., & Dean, T. (2000). Bounded-parameter Markov decision processes. Artificial Intelligence, 122(1-2), 71109.
Hansen, E. A., & Zilberstein, S. (2001). LAO* : a heuristic search algorithm that finds solutions
with loops. Artificial Intelligence, 129(1-2), 3562.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent construction of pattern database heuristics for cost-optimal planning. In Howe, A., & Holte,
R. C. (Eds.), Proceedings of the 22nd National Conference of the American Association for
Artificial Intelligence (AAAI07), pp. 10071012, Vancouver, BC, Canada. AAAI Press.
Haslum, P., & Geffner, H. (2001). Heuristic planning with time and resources. In Cesta, A., &
Borrajo, D. (Eds.), Proceedings of the 6th European Conference on Planning (ECP01), pp.
121132. Springer-Verlag.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence Research, 26, 191246.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats the difference anyway?. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of
the 19th International Conference on Automated Planning and Scheduling (ICAPS09), pp.
162169. AAAI Press.
Helmert, M., Haslum, P., Hoffmann, J., & Nissim, R. (2014). Merge & shrink abstraction: A method
for generating lower bounds in factored state spaces. Journal of the Association for Computing Machinery, 61(3).
Hoffmann, J. (2015). Simulated penetration testing: From Dijkstra to Turing Test++. In Brafman, R., Domshlak, C., Haslum, P., & Zilberstein, S. (Eds.), Proceedings of the 25th International Conference on Automated Planning and Scheduling (ICAPS15). AAAI Press.
Hoffmann, J., Kissmann, P., & Torralba, A. (2014). Distance? Who Cares? Tailoring merge-andshrink heuristics to detect unsolvability. In Schaub, T. (Ed.), Proceedings of the 21st European
Conference on Artificial Intelligence (ECAI14), Prague, Czech Republic. IOS Press.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253302.
Hou, P., Yeoh, W., & Varakantham, P. (2014). Revisiting risk-sensitive MDPs: New algorithms
and results. In Chien, S., Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings of the 24th
International Conference on Automated Planning and Scheduling (ICAPS14). AAAI Press.
267

fiS TEINMETZ & H OFFMANN & B UFFET

Jimenez, S., Coles, A., & Smith, A. (2006). Planning in probabilistic domains using a deterministic
numeric planner. In Proceedings of the 25th Workshop of the UK Planning and Scheduling
Special Interest Group (PlanSig06).
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Boutilier, C. (Ed.),
Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI09),
pp. 17281733, Pasadena, California, USA. Morgan Kaufmann.
Katz, M., Hoffmann, J., & Helmert, M. (2012). How to relax a bisimulation?. In Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd International Conference on Automated Planning and Scheduling (ICAPS12), pp. 101109. AAAI Press.
Keller, T., & Eyerich, P. (2012). PROST: Probabilistic planning based on UCT. In Bonet, B.,
McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd International
Conference on Automated Planning and Scheduling (ICAPS12). AAAI Press.
Keller, T., & Helmert, M. (2013). Trial-based heuristic tree search for finite horizon MDPs. In
Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings of the 23rd International Conference on Automated Planning and Scheduling (ICAPS13), Rome, Italy. AAAI
Press.
Kolobov, A. (2013). Scalable Methods and Expressive Models for Planning Under Uncertainty.
Ph.D. thesis, University of Washington.
Kolobov, A., Mausam, & Weld, D. S. (2010). Sixthsense: Fast and reliable recognition of dead ends
in MDPs. In Fox, M., & Poole, D. (Eds.), Proceedings of the 24th National Conference of the
American Association for Artificial Intelligence (AAAI10), Atlanta, GA, USA. AAAI Press.
Kolobov, A., Mausam, & Weld, D. S. (2012). A theory of goal-oriented MDPs with dead ends. In
de Freitas, N., & Murphy, K. P. (Eds.), Proceedings of the 28th Conference on Uncertainty in
Artificial Intelligence (UAI12), pp. 438447, Catalina Island, CA, USA. AUAI Press.
Kolobov, A., Mausam, Weld, D. S., & Geffner, H. (2011). Heuristic search for generalized stochastic
shortest path MDPs. In Bacchus, F., Domshlak, C., Edelkamp, S., & Helmert, M. (Eds.),
Proceedings of the 21st International Conference on Automated Planning and Scheduling
(ICAPS11). AAAI Press.
Kurniawati, H., Hsu, D., & Lee, W. S. (2008). SARSOP: Efficient point-based POMDP planning
by approximating optimally reachable belief spaces. In Robotics: Science and Systems IV.
Kuter, U., & Hu, J. (2007). Computing and using lower and upper bounds for action elimination in
MDP planning. In Miguel, I., & Ruml, W. (Eds.), Proceedings of the 7th International Symposium on Abstraction, Reformulation, and Approximation (SARA-07), Vol. 4612 of Lecture
Notes in Computer Science, Whistler, Canada. Springer-Verlag.
Kwiatkowska, M., Parker, D., & Qu, H. (2011a). Incremental quantitative verification for markov
decision processes. In 2011 IEEE/IFIP 41st International Conference on Dependable Systems
Networks (DSN), pp. 359370.
Kwiatkowska, M. Z., Norman, G., & Parker, D. (2011b). Prism 4.0: Verification of probabilistic
real-time systems. In Gopalakrishnan, G., & Qadeer, S. (Eds.), Proceedings of the 23rd International on Conference Computer Aided Verification (CAV11), Vol. 6806 of Lecture Notes in
Computer Science, pp. 585591. Springer.
268

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

Little, I., Aberdeen, D., & Thiebaux, S. (2005). Prottle: A probabilistic temporal planner. In Veloso,
M. M., & Kambhampati, S. (Eds.), Proceedings of the 20th National Conference of the American Association for Artificial Intelligence (AAAI05), pp. 11811186, Pittsburgh, Pennsylvania, USA. AAAI Press.
Little, I., & Thiebaux, S. (2007). Probabilistic planning vs replanning. In ICAPS Workshop on the
International Planning Competition: Past, Present and Future.
Marecki, J., & Tambe, M. (2008). Towards faster planning with continuous resources in stochastic
domains. In Fox, D., & Gomes, C. (Eds.), Proceedings of the 23rd National Conference of the
American Association for Artificial Intelligence (AAAI08), pp. 10491055, Chicago, Illinois,
USA. AAAI Press.
McMahan, H. B., Likhachev, M., & Gordon, G. J. (2005). Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees. In Proceedings of
the 22nd International Conference on Machine Learning (ICML-05).
Meuleau, N., Benazera, E., Brafman, R. I., Hansen, E. A., & Mausam, M. (2009). A heuristic search
approach to planning with continuous resources in stochastic domains. Journal of Artificial
Intelligence Research, 34(1), 2759.
Milner, R. (1990). Operational and algebraic semantics of concurrent processes. In van Leeuwen, J.
(Ed.), Handbook of Theoretical Computer Science, Volume B: Formal Models and Sematics,
pp. 12011242. Elsevier and MIT Press.
Muise, C. J., McIlraith, S. A., & Beck, J. C. (2012). Improved non-deterministic planning by
exploiting state relevance. In Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.),
Proceedings of the 22nd International Conference on Automated Planning and Scheduling
(ICAPS12). AAAI Press.
Nakhost, H., Hoffmann, J., & Muller, M. (2012). Resource-constrained planning: A monte carlo
random walk approach. In Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.),
Proceedings of the 22nd International Conference on Automated Planning and Scheduling
(ICAPS12), pp. 181189. AAAI Press.
Nilsson, N. J. (1971). Problem Solving Methods in Artificial Intelligence. McGraw-Hill.
Nissim, R., Hoffmann, J., & Helmert, M. (2011). Computing perfect heuristics in polynomial time:
On bisimulation and merge-and-shrink abstraction in optimal planning. In Walsh, T. (Ed.),
Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI11),
pp. 19831990. AAAI Press/IJCAI.
Pommerening, F., & Helmert, M. (2012). Optimal planning for delete-free tasks with incremental
lm-cut. In Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the
22nd International Conference on Automated Planning and Scheduling (ICAPS12). AAAI
Press.
Richter, S., & Helmert, M. (2009). Preferred operators and deferred evaluation in satisficing planning. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the 19th
International Conference on Automated Planning and Scheduling (ICAPS09), pp. 273280.
AAAI Press.
269

fiS TEINMETZ & H OFFMANN & B UFFET

Sanner, S. (2010). Relational dynamic influence diagram language (rddl): Language description.
Available at http://users.cecs.anu.edu.au/ssanner/IPPC_2011/RDDL.
pdf.
Santana, P., Thibaux, S., & Williams, B. (2016). RAO*: An algorithm for chance-constrained
POMDPs. In Schuurmans, D., & Wellman, M. (Eds.), Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI16), pp. 33083314. AAAI Press.
Sarraute, C., Buffet, O., & Hoffmann, J. (2012). POMDPs make better hackers: Accounting for
uncertainty in penetration testing. In Hoffmann, J., & Selman, B. (Eds.), Proceedings of the
26th AAAI Conference on Artificial Intelligence (AAAI12), pp. 18161824, Toronto, ON,
Canada. AAAI Press.
Seipp, J., & Helmert, M. (2013). Counterexample-guided Cartesian abstraction refinement. In
Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings of the 23rd International Conference on Automated Planning and Scheduling (ICAPS13), pp. 347351,
Rome, Italy. AAAI Press.
Seipp, J., & Helmert, M. (2014). Diverse and additive cartesian abstraction heuristics. In Chien, S.,
Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings of the 24th International Conference on
Automated Planning and Scheduling (ICAPS14). AAAI Press.
Smith, T., & Simmons, R. G. (2006). Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic. In Gil, Y., & Mooney, R. J. (Eds.), Proceedings of the 21st
National Conference of the American Association for Artificial Intelligence (AAAI06), pp.
12271232, Boston, Massachusetts, USA. AAAI Press.
Steinmetz, M., Hoffmann, J., & Buffet, O. (2016). Revisiting goal probability analysis in probabilistic planning. In Coles, A., Coles, A., Edelkamp, S., Magazzeni, D., & Sanner, S. (Eds.),
Proceedings of the 26th International Conference on Automated Planning and Scheduling
(ICAPS16). AAAI Press.
Tarjan, R. E. (1972). Depth first search and linear graph algorithms. SIAM Journal on Computing,
1(2), 146160.
Teichteil-Konigsbuch, F. (2012). Stochastic safest and shortest path problems. In Hoffmann, J.,
& Selman, B. (Eds.), Proceedings of the 26th AAAI Conference on Artificial Intelligence
(AAAI12), Toronto, ON, Canada. AAAI Press.
Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Incremental plan aggregation for generating policies in MDPs. In van der Hoek, W., Kaminka, G. A., Lesperance, Y., Luck, M., &
Sen, S. (Eds.), Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS10), pp. 12311238. IFAAMAS.
Teichteil-Konigsbuch, F., Vidal, V., & Infantes, G. (2011). Extending classical planning heuristics to
probabilistic planning with dead-ends. In Burgard, W., & Roth, D. (Eds.), Proceedings of the
25th National Conference of the American Association for Artificial Intelligence (AAAI11),
San Francisco, CA, USA. AAAI Press.
Torralba, A., & Hoffmann, J. (2015). Simulation-based admissible dominance pruning. In Yang,
Q. (Ed.), Proceedings of the 24th International Joint Conference on Artificial Intelligence
(IJCAI15), pp. 16891695. AAAI Press/IJCAI.
270

fiG OAL P ROBABILITY A NALYSIS IN P ROBABILISTIC P LANNING

Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: a baseline for probabilistic planning. In
Boddy, M., Fox, M., & Thiebaux, S. (Eds.), Proceedings of the 17th International Conference
on Automated Planning and Scheduling (ICAPS07), pp. 352359, Providence, Rhode Island,
USA. Morgan Kaufmann.
Younes, H. L. S., Littman, M. L., Weissman, D., & Asmuth, J. (2005). The first probabilistic track
of the international planning competition. Journal of Artificial Intelligence Research, 24,
851887.

271

fiJournal of Artificial Intelligence Research 57 (2016) 509-572

Submitted November, 2015; published November, 2016

A Survey of Computational Treatments of Biomolecules
by Robotics-Inspired Methods
Modeling Equilibrium Structure and Dynamics
Amarda Shehu

AMARDA @ GMU . EDU

Department of Computer Science, Department of Bioengineering,
School of Systems Biology
George Mason University, Fairfax, VA, USA

Erion Plaku

PLAKU @ CUA . EDU

Department of Electrical Engineering and Computer Science
Catholic University of America, Washington, DC, USA

Abstract
More than fifty years of research in molecular biology have demonstrated that the ability of
small and large molecules to interact with one another and propagate the cellular processes in the
living cell lies in the ability of these molecules to assume and switch between specific structures
under physiological conditions. Elucidating biomolecular structure and dynamics at equilibrium
is therefore fundamental to furthering our understanding of biological function, molecular mechanisms in the cell, our own biology, disease, and disease treatments. By now, there is a wealth
of methods designed to elucidate biomolecular structure and dynamics contributed from diverse
scientific communities. In this survey, we focus on recent methods contributed from the Robotics
community that promise to address outstanding challenges regarding the disparate length and time
scales that characterize dynamic molecular processes in the cell. In particular, we survey roboticsinspired methods designed to obtain efficient representations of structure spaces of molecules in isolation or in assemblies for the purpose of characterizing equilibrium structure and dynamics. While
an exhaustive review is an impossible endeavor, this survey balances the description of important
algorithmic contributions with a critical discussion of outstanding computational challenges. The
objective is to spur further research to address outstanding challenges in modeling equilibrium
biomolecular structure and dynamics.

1. Introduction
The way in which the chain of amino acid units in a protein molecule is coiled and folded in space
has been worked out for the first time. The protein is myoglobin, the molecule of which contains
2,600 atoms. This is how John Kendrew began his feature article in Scientific American in 1961,
reporting what was the first atomistic model of a protein structure1 obtained via X-ray crystallography (Kendrew, Dickerson, Strandberg, Hart, Davies, Phillips, & Shore, 1960). This model is drawn
in various graphical representations in Figure 1. For the pioneering work on resolving structures of
globular proteins, Kendrew and Perutz were awarded the Nobel Prize in chemistry in 1962. This was
the very same year Watson, Crick, and Wilkins shared the Nobel Prize in physiology or medicine
for using X-ray crystallography data to determine the helical structure of DNA.
1. For the purpose of this survey, we will distinguish between structure and conformation. Structure will refer to a
specific placement of the atoms that comprise a biomolecule in R3 . The concept of conformation is defined in
Section 2.
c
2016
AI Access Foundation. All rights reserved.

fiS HEHU & P LAKU

(a) X-ray model of myoglobin and heme group

(b) Model in atomistic detail (no heme group)

(c) Various models of myoglobin and heme group
Figure 1: (a) The X-ray model of myoglobin and the heme group bound to it determined by Kendrew are
drawn here with the Visual Molecular Dynamics (VMD) software (Humphrey et al., 1996). The model can be
found in the Protein Data Bank (PDB) (Berman et al., 2003), which is a repository of known protein structures,
under PDB entry 1MBN. Drawing the surface of this protein facilitates visually locating the cavity where the
heme group, which helps myoglobin to carry off oxygen to tissue, binds. The heme group is drawn in a
ball-and-stick representation in red. (b) All heavy atoms that comprise the 153-amino acid long myoglobin
chain are drawn in a ball-and-stick representation, color-coded by the amino acid to which they belong. The
backbone that connects atoms of consecutive amino acids in the chain is drawn in white in the NewCartoon
representation in VMD. (c) The X-ray model of myoglobin under PDB entry 1MBN is superimposed over the 12
models obtained for the same protein and the bound heme group from Nuclear Magnetic Resonance (NMR),
deposited in the PDB under PDB entry 1MYF.

The ability to visualize structures of biomolecules in atomistic detail was a shot in the arm to
molecular biology and marked the beginning of a revolution in molecular structural biology; a race
soon ensued across wet laboratories to determine three-dimensional (3d) structures assumed by proteins and other biomolecules under physiological conditions. Since those early days, the set of protein structures resolved in the wet-laboratory, beginning with myoglobin and lysozyme (Kendrew,
Bodo, Dintzis, Parrish, Wyckoff, & Phillips, 1958; Kendrew et al., 1960; Phillips, 1967), has grown
510

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

to over a hundred thousand, now freely available for anyone to download from the PDB (Berman
et al., 2003).
Further pioneering work by Anfinsen, which earned him the Nobel Prize in Chemistry in 1973,
demonstrated that the ability of a protein to carry out its biological function is dependent on its
ability to fold onto a specific 3d structure reversibly (Anfinsen, 1973). The Anfinsen experiments
led to the view that a folded structure corresponds to the global minimum of an underlying energy
surface. They also showed that the information needed for a protein to assume its 3d, biologicallyactive structure is largely encoded in its amino-acid sequence. Since then, any study of biomolecular
function has to consider the role of both sequence and structure (Fersht, 1999).
Figure 1(a), which shows the surface of the biologically-active structure of the myoglobin protein, exposes a central cavity that allows binding of the heme group to myoglobin. Figure 1(b)
traces the amino-acid chain that makes up myoglobin and additionally draws the heavy atoms constituting each amino acid in this protein. These simple images illustrate two important points: first,
that structure plays a central role in function (specifically, complementary geometric and physicochemical features of 3d structures of molecules are key to stable molecular interactions) (Boehr &
Wright, 2008); second, that the ability of the amino-acid chain to fold onto itself makes protein
structures complex. Understanding how and what biologically-active structure a biomolecule assumes in the cell is key not only to elucidating molecular mechanisms in the healthy and diseased
cell, but also determining how to address the abnormal role of a biomolecule in such mechanisms
in order to treat disease. In particular, research has shown that many abnormalities involve proteins with aberrant biological function (Soto, 2008; Uversky, 2009; Fernandez-Medarde & Santos,
2011; Neudecker, Robustelli, Cavalli, Walsh, Lundstrm, Zarrine-Afsar, Sharpe, Vendruscolo, &
Kay, 2012) due to external and internal perturbations (e.g., DNA mutations, copying errors) affecting the ability of these molecules to assume specific structures (Onuchic, Luthey-Schulten, &
Wolynes, 1997; Ozenne, Schneider, Yao, Huang, Salmon, Zweckstetter, Jensen, & Blackledge,
2012; Levy, Jortner, & Becker, 2001; Miao, Sinko, Pierce, Bucher, Walker, & McCammon, 2014;
Gorfe, Grant, & McCammon, 2008; Grant, Gorfe, & McCammon, 2009).
Any treatment of the relationship between structure and function would be incomplete if the
dynamic personality of biomolecules is not taken into account (Jenzler-Wildman & Kern, 2007).
While X-ray models of biomolecular structures seem to suggest rigid molecules with atoms frozen
in space, an increasing number of wet-laboratory, theoretical, and computational studies have shown
that biomolecules are systems of particles in perpetual motion. Indeed, Feynman taught early about
the jiggling and wiggling of atoms (Feynman, Leighton, & Sands, 1963). Cooper and others later
posited that the inherent dynamics of biomolecules could be explained under a general, theoretical
treatment of molecules as thermodynamic systems striving towards their equilibrium, lowest freeenergy state (Cooper, 1984). Thus, the inherent dynamics of biomolecules could be explained
using fundamental physics principles; a statistical mechanics formulation also revealed the inherent
uncertainty at any given time about the particular state of a molecule (Cooper, 1984).
The dynamics of molecular systems was investigated around the same time the first experimental
models of protein structures were emerging. In 1967, Verlet simulated the dynamics of argon and
demonstrated that such simulations were able to reproduce equilibrium properties (Verlet, 1967).
Application of the Verlet algorithm for simulating protein dynamics would have to wait for one
more decade. In 1977, McCammon and Karplus reported on a 9.2 picosecond-long trajectory showing in-vacuum, atomistic fluctuations of the bovine pancreatic trypsin inhibitor around its folded,
active structure (the latter had already been obtained via wet-laboratory techniques) (McCammon,
511

fiS HEHU & P LAKU

Gelin, & Karplus, 1977). Advancements in wet-laboratory techniques, which had spewed about
a dozen models of biologically-active protein structures by the late 70s, facilitated a revolution in
computational structural biology. The pioneering algorithmic work of Verlet, Karplus, McCammon,
Levitt, Warshel, and Lifson (for which Karplus, Levitt, and Warshel shared the 2013 Nobel Prize
in chemistry) provided the earliest frameworks for computational treatments of biomolecules as a
means to investigate equilibrium structure and dynamics (Fersht, 2013).
Since those early days, advances in wet-laboratory techniques have proceeded hand in hand with
advancements in computational techniques, often feeding off each-other. The advent of NMR for
structure determination provided evidence of the ability of biomolecules to fluctuate between different structures even at equilibrium (Kay, 1998, 2005). Figure 1(c) shows, in addition to the X-ray
structure of myoglobin and its bound heme group, twelve models obtained via NMR, showcasing
the intrinsic flexibility of this important biomolecule and its molecular partner in the cell. Nowadays, wet-laboratory techniques, such as NMR and cryo-Electron Microscopy (cryo-EM) can resolve equilibrium structures and quantify equilibrium dynamics. For example, NMR has been used
to identify well-populated intermediate structures along a transition (Aden & Wolf-Watz, 2007).
Hybrid techniques that combine NMR relaxation measurements with X-ray models derived from
room-temperature crystallographic, single-molecule spectroscopy techniques that tune optical radiation to observe one molecule, and others can now elucidate fast and slow dynamic processes lasting
from a few picoseconds to a few milliseconds (Torella, Holden, Santoso, Hohlbein, & Kapanidis,
2011; Fenwick, van den Bedem, Fraser, & Wright, 2014; Karam, Powdrill, Liu, Vasquez, Mah,
Bernatchez, Gotte, & Cosa, 2014; Moerner & Fromm, 2003; Greenleaf, Woodside, & Block, 2007;
Michalet, Weiss, & Jager, 2006; Diekmann & Hoischen, 2014; Hohlbein, Craggs, & Cordes, 2014;
Schlau-Cohen, Wang, Southall, Cogdell, & Moerner, 2013; Moffat, 2003; Schotte, Lim, Jackson,
Smirnov, Soman, Olson, Phillips, Wulff, & Anfinrud, 2003; Roy, Hohng, & Ha, 2008; Fenwick
et al., 2014; Hohlbein et al., 2014; Lee, M., Kim, & Suh, 2013; Socher & Imperiali, 2013; Gall,
Ilioaia, Kruger, Novoderezhkin, Robert, & van Grondelle, 2015).
In particular, wet-laboratory techniques that employ fluorescence-based sensors, can provide
information on dynamic, biological events by effectively monitoring changes in the signals of
strategically-placed fluorophores (Socher & Imperiali, 2013). Depending on the placement of the
fluorophores, the binding of two molecular partners or the switch/transition of one molecule between different structures can be monitored in real time. While such techniques are very promising
and rapidly being adopted to study specific biological systems of interest, the reliance on fluorophores limits the generality of these techniques, as well as the structural detail that can be obtained. At the moment, wet-laboratory techniques obtain an incomplete view of equilibrium dynamics, as they are generally unable to span all the disparate length and time scales involved in
a structural transition of a molecule (Maximova, Moffatt, Ma, Nussinov, & Shehu, 2016); While
atomic motions occur on the picosecond scale, side-chain motions can take a few nanoseconds,
and concerted motions among groups of atoms facilitating structural rearrangements for molecular recognition events can take anywhere from a few microseconds to a few milliseconds (Shaw,
Maragakis, Lindorff-Larsen, Piana, Dror, Eastwood, Bank, Jumper, Salmon, Shan, & Wriggers,
2010; Lindorff-Larsen, Piana, Dror, & Shaw, 2011; Zagrovic, Snow, Shirts, & Pande, 2002; Piana, Lindorff-Larsen, & Shaw, 2012b); in extreme cases, binding of natural and drug molecules to
proteins occurs on the hours scale (Hoelder, Clarke, & Workman, 2012).
Computational treatments of biomolecules are driven by the promise of complementing wetlaboratory treatments in obtaining a comprehensive and detailed characterization of equilibrium
512

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

dynamics. The current most well-known frameworks employed in silico are Molecular Dynamics
(MD) (Verlet, 1967; McCammon et al., 1977) and Monte Carlo (MC) (Hastings, 1970; Metropolis,
Rosenbluth, Rosenbluth, Teller, & Teller, 1953). In principle, the entire equilibrium dynamics of
a molecule can be simulated by simply following the motions of the constitutive atoms along the
physical forces that atoms impose on one another. This is the foundation of the MD framework. In
contrast, in the MC framework, structural perturbation moves applied to atoms or bonds connecting
atoms are not the result of physical forces but instead design decisions. Different local search
strategies can be formulated to make use of such moves and iteratively explore neighborhoods in
the structure space of a biomolecule.
The scope and capabilities of MD- and MC-based treatments of biomolecules have been significantly increased due to improvements in hardware and parallel computation strategies. Specialized architectures, such as Anton, a supercomputer designed for MD simulations, (Piana, LindorffLarsen, Dirks, Salmon, Dror, & Shaw, 2012a; Piana et al., 2012b; Lindorff-Larsen et al., 2011),
GPUs (Stone, Phillips, Freddolino, Hardy, Trabuco, & Schulten, 2007; Harvey, Giupponi, & de
Fabritiis, 2009; Tanner, Phillips, & Schulten, 2012; Gotz, Williamson, Xu, Poole, Le Grand, &
Walker, 2012), and petascale national supercomputers, such as BlueWaters, Titan, Mira, and Stampede (Dubrow, 2015; Zhao, Perilla, Yufenyuy, Meng, Chen, Ning, Ahn, Gronenborn, Schulten,
Aiken, & Zhang, 2013) have allowed characterizing biomolecular structure and dynamics up to the
microsecond time scale. Algorithmic improvements in dynamic load balancing (Fattebert, Richards,
& Glosli, 2012), neighbor searches (Proctor, Lipscomb, Zou, Anderson, & Cho, 2012), and optimal force splitting (Batcho, Case, & Schlick, 2001) allow effectively distributing the simulation of
the dynamics of molecular systems comprised of billions of particles (Perilla, Goh, Cassidy, Liu,
Bernardi, Rudack, Yu, Wu, & Schulten, 2015).
In principle, a full account of the equilibrium dynamics of a biomolecule requires a comprehensive characterization of both the structure space available to the biomolecule at equilibrium as well
as the underlying energy surface that governs accessibility of structures and transitions between
structures at equilibrium. This remains challenging to do via MD and MC-based frameworks, and
algorithmic enhancements of the classic MD and MC frameworks essentially aim to enhance their
sampling of the structure space of a biomolecule. A review of state-of-the-art enhancements can be
found in the work of Maximova et al. (2016).
In this survey paper, we focus instead on emerging contributions from the Robotics community
on how to enhance sampling with complementary algorithmic strategies. Specifically, we review
robotics-inspired methods designed to model structural excursions of a biomolecule at equilibrium
by building conceptually over techniques designed originally for robot motion planning. These
methods have now reached a crucial stage. They have been shown applicable to characterization of
diverse molecular mechanisms in computational structural biology, such as protein-ligand binding,
folding and unfolding of peptides, proteins, and RNA molecules, and transitions of small peptides
and large proteins between thermodynamically-stable and semi-stable structural states. As this survey shows, these methods are capable of addressing challenging computational issues posed in each
of these application settings, but they have yet to be widely adopted by the computational biology
community at large. For various reasons, some of which are discussed in this survey, these methods
are seen as providing an efficient but less detailed and less accurate characterization of biomolecular equilibrium structure and dynamics. This survey provides a critical review of robotics-inspired
methods and lays out outstanding issues that need to be addressed for these methods to be considered reliable tools and be widely adopted for modeling biomolecular structure and dynamics.
513

fiS HEHU & P LAKU

This survey is organized as follows. A background of models of biomolecular energetics and
geometry is provided in Section 2. Section 3 then introduces the main classes of problems in
biomolecular modeling addressed with robotics-inspired methods, summarizes the robot motion
planning frameworks over which such methods build, and concludes with a brief description of
challenges faced by robotics-inspired methods in the context of biomolecular modeling. Section 4
provides examples of design decisions that address such challenges through a comprehensive and
detailed review of robotics-inspired methods for modeling biomolecular structure and dynamics.
Section 5 concludes this survey with a critical summary of remaining challenges and a discussion
of several prospects for future research.

2. Background
The structural excursions that regulate the recognition events in which a biomolecule participates in
the cell can be understood via a theoretical treatment of biomolecules as thermodynamic systems
hopping between energetic states. These hops are fundamentally the result of concerted motions
of the atoms that make up a biomolecule; in any physical system, constitutive particles are in a
state of perpetual motion, fuelled by thermal excitation, all the while subjecting one another to
physical forces (Cooper, 1984). These forces cumulatively drive a molecular system toward lowerenergy states, while thermal excitations kick them off locally-optimal states, providing sufficient
randomness to allow the entire system undergo a biased exploration of its structure space.
In the following we first summarize current knowledge on atomic forces and biomolecular energy functions that are employed in computational treatments of biomolecular structure and dynamics. The rest of this Section provides details on biomolecular geometry, showing how biomolecules
can be treated mechanistically as modular systems composed of numerous, heterogeneous components for the purpose of characterizing their equilibrium structure and dynamics in silico.
2.1 Biomolecular Energetics: Molecular Mechanics
The physical interactions among the particles that make up a molecular system can in principle be
measured via quantum mechanics (QM) methods. QM methods can carry out detailed and accurate
electronic structure calculations but are currently limited in their applicability to molecular systems
composed of no more than a few hundred atoms (Khaliullin, VandeVondele, & Hutter, 2013). Instead, molecular mechanics (MM) methods are now the methods of choice to evaluate structures
of macromolecules, such as proteins, RNA, DNA, and other large molecular systems comprised of
several molecules.
Though it was long known that atoms in a molecule subject one another to physical forces, such
as Coulomb forces and others, it was the work of Levitt and Warshel in the Lifson laboratory at the
Weizmann Institute of Science that propelled the design of consistent (now known as MM) energy
functions for molecules. Lifson argued that it should be possible to come up with a small number of
consistent, transferable parameters that do not depend on the local environment of an atom and allow
analyzing the energetics of small crystalline molecules (Lifson & Warshel, 1968). Kendrew realized
that such consistent energy functions could be used to conduct energetic evaluations on different
placements (that is, structures) of the atoms comprising proteins and nucleic acids. Levitt and
Lifson operationalized this realization to conduct energy refinements of protein structures (Levitt &
Lifson, 1969).
514

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

MM energy functions have now become more detailed and accurate, but they are still estimates
of the true potential energy of a molecule, summing up the possible, physical interactions between
atoms in a molecule. Research on designing MM energy functions is active, and there are now many
different functions offered from different computational chemistry labs across the world. These
functions largely follow a common functional form, and categorize pairwise atomic interactions
into local and non-local interactions; it should be noted that more accurate energy functions are
available that consider n-particle interactions, but these are more computationally expensive and
not widely adopted (Clementi, 2008). Local interactions concern modeling forces due to bonds,
bond angles, and the periodicity of dihedral/torsion angles. Non-local interactions are divided into
electrostatic (measured through the Coulomb potential) and van der Waals (measured through the
Lennard-Jones  LJ  potential) interactions. These different types of interactions are typically
linearly combined together, each with its own weight, to associate a potential energy value with a
particular placement of atoms in a given molecular structure.
The following equation provides an example of the popular CHARMM energy function (Brooks,
Bruccoleri, Olafson, States, Swaminathan, & Karplus, 1983) that is integrated in the NAMD software package for simulation of biomolecular dynamics (Phillips, Braun, Wang, Gumbart, Tajkhorshid, Villa, Chipot, Skeel, Kale, & Schulten, 2005).

ECHARMM =

X

kb  (b  b0 )2

+

kUB  (S  S0 )2

+

X

+

bonds

X
UB

k (  0 )2

valence angles

X

k  (1 + cos(n  ))

+

kimp (  0 )2

+

dihedral angles

X
improper dihedral angles

X
nonbonded atoms i, j

X
nonbonded atomsi, j

ij

h Rmin 
Rminij 6 i
ij 12
2
+
rij
rij

qi  qj
  rij

The k weights are constants, and the 0 subscript indicates equilibrium, ideal values of distances
and angles. The first term effectively penalizes deviations of bond lengths from equilibrium values
with a quadratic potential. The second term, also referred to as the Urey Bradley (UB) or the
1,3 term, introduces a similar penalty for pairs of atoms separated by two covalent bonds, with the
distance between two atoms involved in a 1,3 interaction denoted by S. The third term is a quadratic
potential for valence angles (between two consecutive bonds), denoted by . The fourth term is a
potential calculated over dihedral/torsion angles  and models the presence of steric barriers between
atoms separated by three covalent bonds. In this term, the n and  variables are the multiplicity and
the phase angles, respectively. In CHARMM, improper dihedral angles are specially penalized, as
in the fifth term. The sixth term shows the LJ potential in CHARMM. In the LJ term summing up
515

fiS HEHU & P LAKU

van der Waals interactions between non-bonded atoms, rij measures the Euclidean distance between
two non-bonded atoms (that are not covered by the UB term), and Rminij = (Rmini + Rminj )/2
is the minimum interaction radius between the atoms, measured as half the sum of the known van
der Waals radii Rmini and Rminj (ij is a weight specific to the types of atoms i and j). The LJ
term sums up a weak attraction at long distances and strong repulsion at short distances. The LJ
term in CHARMM has a 126 functional form, with an exponent of 12 for the repulsive sub-term
and an exponent of 6 for the attractive sub-term. The last term in the CHARMM function measures
electrostatic interactions via the Coulomb potential: qi and qj are the known partial charges of atoms
i and j, rij measures the Euclidean distance between atoms i and j, and  is the dielectric constant
encoding the type of environment in which a biomolecule is (vacuum or different types of solvent
environments).
Differences between available potential energy functions are due to different weights, different
exponents used to measure the repulsion versus attraction terms in the van der Waals interaction,
explicit estimation of hydrogen-bonding interactions outside the umbrella of van der Waals interactions, and more (Hornak, Abel, Okur, Strockbine, Roitberg, & Simmerling, 2006). The Amber suite
of energy functions, integrated in the Amber MD simulation package (Case, Darden, Cheatham,
Simmerling, Wang, Duke, Luo, Merz, Pearlman, Crowley, Walker, Zhang, Wang, Hayik, Roitberg,
Seabra, Wong, Paesani, Wu, Brozell, Tsui, Gohlke, Yang, Tan, Mongan, et al., 2014), OPLS (Jorgensen, Maxwell, & Tirado-Reves, 1988), and CHARMM follow a similar functional form. Other
similar functions are CEDAR (Hermans, Berendsen, van Gunsteren, & Postma, 1984) and GROMOS (van Gunsteren, Billeter, Eising, Hunenberger, Kruger, Mark, Scott, & Tironi, 1996), now incorporated in the GROMACS simulation package (Van Der Spoel, Lindahl, Hess, Groenhof, Mark,
& Berendsen, 2005), and others. A review of these functions, known as physics-based function,
can be found in the work of Ponder and Case (2003). Other functions, known as knowledge-based
function, include additional terms derived from conducting statistics over known active structures of
proteins in the PDB. Such functions are best suited for specific applications, such as rapid modeling
of equilibrium structures. Rosetta (Leaver-Fay, Tyka, Lewis, Lange, Thompson, Jacak, Kaufman,
Renfrew, Smith, Sheffler, Davis, Cooper, Treuille, Mandell, Richter, Ban, Fleishman, Corn, Kim,
Lyskov, Berrondo, Mentzer, Popovi, & et. al., 2011) and Quark (Xu & Zhang, 2012) are recent
examples of knowledge-based functions.
Whether physics-based or knowledge-based (or hybrid), all current molecular energy functions
are models and, as such, they contain inherent errors that need to be taken into account when modeling biomolecular structure and dynamics (Hornak et al., 2006). In addition, in all such functions, despite the specific functional form, the most computationally expensive terms are the LJ
and Coulomb terms due to the summation over pairs of atoms. These terms are also the ones that
are most sensitive to small atomic motions. In particular, the 12-6 functional form of the LJ term
provides great complexity and non-linearity to the energy surface that one can associate with the
structure space of a biomolecule. It is quite common to reduce the total energy of a structure by
a few hundred calories solely due to improvements in the LJ term from imperceptible changes in
atomic positions. Moreover, small atomic displacements can lower the value of one term while
increasing that of another in the energy function. From an optimization point of view, the terms that
are linearly combined in an energy function are essentially conflicting optimization objectives. In
computational biology, this issue is known as frustration and results in rugged or rough energy
surfaces (that is, rich in local minima). In the broader AI community, such surfaces would be referred to as multi-modal. While true biomolecular energy surfaces are not overly rugged (known
516

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

as the principle of minimal frustration) (Clementi, 2008; Nevo, Brumfeld, Kapon, Hinterdorfer, &
Reich, 2005), the modeled energy surfaces that one can probe in silico with the current energy functions have been shown exceptionally rugged (Olson & Shehu, 2012; Molloy, Saleh, & Shehu, 2013;
Lois, Blawzdziewicz, & OHern, 2010).
2.2 The Biomolecular Energy Surface
Equilibrium biomolecular dynamics can be visualized as structural excursions on the energy surface. The picture that emerges for proteins is that of a funnel-like, multidimensional energy surface (Onuchic et al., 1997; Dill & Chan, 1997). Projecting the surface onto few coordinates that
capture relevant features of the different structures would allow summarizing and thus visualizing
the energy surface in terms of a landscape (Onuchic & Wolynes, 2004).
The energy landscape shown in two different camera views in Figure 2 illustrates protein energy
landscapes expected to be reconstructed in silico. Horizontal cross-sections of the landscape every
dE units apart correspond to the different energetic states. The cross-sections go down in width as
energy decreases; there are fewer options to place atoms in a molecule as potential energy gets lower
without incurring energetic costs greater than dE. The width of a cross-section, or the structural
diversity of an energetic state, is captured in the notion of entropy. Thermodynamically-stable states
are those with low free energy F , measured as F = hEi  T  S, where hEi is the average potential
energy over structures grouped together in the state, T is temperature, and S is entropy.
The first visual illustration, proposed by Dill and Chan (1997), highlighted the main features
expected of true protein energy landscapes, a single, deep and wide basin corresponding to the
thermodynamically-stable state and few other shallower, narrower basins corresponding to metastable states serving as possible kinetic traps. The landscape shown in Figure 2(a) is a synthetic
one that is closer to the landscapes corresponding to existing MM energy functions; the landscape
is not smooth but rather rich in local minima; in other words, the landscape is highly rugged or
rough. A different camera view in Figure 2(b) emphasizes the presence of multiple, similarly-deep
and wide basins among which current energy functions cannot further distinguish for the purpose of
predicting the most stable state via energetic-based arguments; given the inherent errors, structurefunction arguments cannot depend on small energetic differences.
The energy landscape view was instrumental in linking molecular structure, dynamics, and function. Viewing proteins and other biomolecules in terms of their energy landscapes gave rise to better
understanding folding and binding as diffusion-like processes and not as a series of sequential, deterministic events. Under the new, landscape view (Baldwin, 1995), biomolecules can reach their most
stable state at equilibrium by tumbling down the energy landscape along multiple routes (Bryngelson & Wolynes, 1987; Bryngelson, Onuchic, Socci, & Wolynes, 1995; Onuchic & Wolynes, 2004).
In light of the new view, the intermediate, meta-stable states in which proteins would sometimes be
found in the wet laboratory before transitioning to their most stable state correspond to other wide
basins in the landscape. An illustration of this is provided in Figure 2(b).
The new view inspired a new understanding of dynamic molecular processes, known as conformational selection or population shift (Ma, Kumar, Tsai, & Nussinov, 1999; Tsai, Ma, & Nussinov,
1999b; Tsai, Kumar, Ma, & Nussinov, 1999a). Conformational selection refers to the idea that all
states of an unbound molecular unit are present and accessible by the bound unit. For many unbound/uncomplexed biomolecules, there may be many semi-stable states at equilibrium. The proximity of a ligand or another molecular partner shifts the equilibrium (and thus the probability distri517

fiS HEHU & P LAKU

(a) Illustration of a complex energy landscape

(b) Tilted camera view highlights the presence of multiple energy basins
Figure 2: (a) The shown landscape illustrates what is often reconstructed in silico, rough landscapes rich
in local minima. (b) The tilted camera view emphasizes the presence of multiple energetic basins. A basin
is defined as the neighborhood of a local minimum in a fitness landscape. The interested reader is encouraged to learn more about features of landscapes that arise in optimization problems in Stadlers seminal
review (Stadler, 2002). Given the high dimensionality of the structure space, many methods that probe energy
landscapes cannot guarantee that a particular, sought stable or meta-stable state will be captured among the
probed basins, or that none of the probed basins are artifacts of the energy function employed.

bution over possible states in which a system is found at equilibrium) towards one of the states that
are close to optimal at equilibrium in the unbound/uncomplexed molecule. In other words, the presence of a binding partner can be considered an external perturbation to the unbound/uncomplexed
energy landscape. Internal perturbations refer to changes in a biomolecules composition itself due
to changes to DNA, copy read errors, and other post-translation modifications that can occur. In
many aberrant versions of a biomolecule, energy barriers between stable and semi-stable states can
518

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

drastically change and modify the underlying detailed structural mechanism regulating function,
resulting in dysfunction or even loss of function (Clausen, Ma, Nussinov, & Shehu, 2015).
The principle of conformational selection allows employing analysis of energy landscapes of
unbound molecules to identify structural states of interest for complexation events. The latter can
be found among the meta-stable and stable states of the uncomplexed molecule and can thus be
identified via modeling and simulation of the uncomplexed molecule.
2.3 Computing Structures and Structural Transitions
The two main problems that can be addressed in silico to elucidate biomolecular equilibrium structure and dynamics concern (i) computing the ensemble of structures constituting the stable and
meta-stable states relevant for biological function and (ii) computing the detailed structural transitions between such structures. The first problem is amenable to stochastic optimization, as it fundamentally involves locating deep and wide basins/minima in a nonlinear and multimodal energy
surface. The second problem entails elucidating the different routes employed by a biomolecule as it
switches between two structures. This survey focuses primarily on the second problem, that of computing structural transitions in bound and unbound biomolecules. Specifically, the survey reviews
methods that employ robotics analogies to address this problem. While, in principle, roboticsinspired methods can also provide information on the structure space available to a biomolecule at
equilibrium, other, more powerful stochastic optimization algorithms now exist for this purpose. We
refer interested readers to the review by Shehu (2013), which surveys state-of-the-art evolutionary
algorithms (EAs) capable of extracting efficient, discrete representations of protein energy surfaces.
At a minimum, all algorithms aiming to model biomolecular structures are comprised of three
functional units: (i) a way to represent/model a biomolecular structure; (ii) a way to modify such
models in order to obtain new structures; (iii) and a way to evaluate the energetics of such structures.
The existing MM energy functions summarized above in the context of biomolecular energetics
provide a way to evaluate models of biomolecular structures explored in silico. The functional units
(i) and (iii) are related, as the model chosen for a protein structure determines to a great extent
what moves or perturbation operators can be designed to efficiently and effectively explore the
structure space. Below, we summarize models that are now popular among the different algorithms
employed to model equilibrium structures and dynamics of biomolecules. Details regarding the
moves or perturbation operators designed to interface with such models are provided later in this
survey when reviewing robotics-inspired methods.
2.4 Molecular Models: Selecting Variables of Interest
Covalent bonds link atoms together in a molecule. In protein molecules, atoms are organized into
amino acids, which come in twenty different types in nature. All amino acids contain a common
core of heavy atoms that make up its backbone and a unique set of heavy atoms that make up its
side chain (hydrogen/light atoms are found both in the backbone and side-chain groups). The twenty
naturally-occurring amino acids only differ in their side chain. Figure 3(a) shows the N, CA, C, and
O heavy-atoms that comprise the backbone of an amino acid and further illustrates how amino acids
are connected via covalent, peptide bonds in a serial fashion to form a (polypeptide) chain. What is
referred to as a protein is often just one polypeptide chain; in protein-protein binding polypeptide
chains stay together via non-covalent, weak interactions.
519

fiS HEHU & P LAKU

(a)

(b)

Figure 3: (a) In this chain of six amino acids, backbone atoms are N (gray), CA (black), C (gray), and O
(silver). A peptide bond Ni -Ci+1 links two amino acids (i proceeds from N- to C-terminus, which refer to
backbone N and C atoms not in peptide bonds). Circled atoms comprise the side chain of each shown amino
acid. (b) The three types of internal coordinates are shown here, the bond length di , the valence angle i
between two consecutive angles, and the torsion or dihedral angle i defined by three consecutive bonds. The
dihedral angle is the angle between the two normals corresponding to each of the planes that can be defined
by consecutive bonds j and j + 1 and consecutive bonds j + 1 and j + 2. Depending on which backbone
bonds they are defined, the i dihedral angles are referred to as either  or , and annotated with the position
of the amino acid on which they are defined (in the direction of the N- to the C-terminus). For instance, i
refers to the dihedral angle on the bond connecting the backbone N to the backbone CA atom of amino acid
i, and i refers to the dihedral angle defined on the bond connecting the backbone CA to the backbone C
atom of amino acid i. Characteristic values are observed for the , psi angles among equilibrium protein
structures (Ramachandran et al., 1963).

520

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Figure 3(a) illustrates that side chains dangle off the backbone of a polypeptide chain. Treating
a protein molecule as a model where atoms are represented as balls and bonds between them as
sticks exposes interesting questions regarding how to perform deformations of the model without
breaking covalent bonds. This question is in essence what structure modeling researchers have
to answer when defining variables to represent a molecule so as to be able to capture its intrinsic
flexibility at equilibrium.
2.4.1 C ARTESIAN C OORDINATE -BASED M ODELS
In the most intuitive model of a molecular structure, each Cartesian coordinate of each atom is
selected as a variable. The Cartesian coordinate-based model is the preferred one by MD algorithms, which move individual atoms in a molecule according to the cumulative force that sums
up interactions of an atom with all others in a molecule. However, this model is not ideal. First,
it is redundant, demanding 3N variables for a molecule of N atoms. In small peptide and drug
molecules, the number of atoms may be in the dozens, but even in small proteins, the number of
atoms can easily surpass a hundred; as a result, the variable space has hundreds of dimensions.
Many strategies have been offered to reduce the number of variables by essentially removing
certain atoms from modeling. For example, side-chain atoms are the first to be sacrificed in protein
structure modeling, since it has been demonstrated that the main features of equilibrium protein
structures are captured by the backbone (Rose, Fleming, Banavar, & Maritan, 2006). Once such
reduced structures are modeled, side chains can be modeled via side-chain packing algorithms. In
other studies focusing on modeling molecular interactions, only atoms that comprise the interaction
site are explicitly modeled. These decisions effectively result in reduced models (and thus fewer dimensions of the selected variable space), ranging from CA traces, where only the central CA atom
is modeled in each amino acid, to backbone models, where only the backbone chain is tracked in
3d space (Papoian, Ulander, Eastwood, Luthey-Schulten, & Wolynes, 2004; Matysiak & Clementi,
2004; Das, Matysiak, & Clementi, 2005; Matysiak & Clementi, 2006; Hoang, Trovato, Seno, Banavar, & Maritan, 2007; Rose et al., 2006). There is now a rich literature on reduced models and the
energy functions designed to interface with these models (Clementi, 2008).
Representing a molecular structure in terms of the Cartesian coordinates of all (or a subset of)
the constitutive atoms is appealing, as any new instantiation in the variable space can be readily
evaluated in terms of its energetics. We recall that the central LJ and electrostatic/Coulomb terms
in the energy functions that are now widely adopted in biomolecular structure and dynamics modeling operate over 3d coordinates of atoms. However, the Cartesian coordinate-based model is both
redundant and ineffective. First, the model results in an excessive number of variables, which poses
great challenges for any sampling-based method aimed at probing the structure space one sample at
a time. Second, it is ineffective, as it does not encode in it any of the explicit and implicit geometric
constraints present in molecular structures.
Many efforts in the computational biophysics community target the reduction of coordinates.
The resulting models are referred to as coarse-grained models or representations. The first such
model, employed in an MC simulation of the folding of the bovine pancreatic trypsin inhibitor,
represented each amino-acid residue with one pseudo-atom (Levitt & Warshel, 1975). Work on
coarse-grained and multiscale models (in the latter, different parts of the structure are represented
at different levels of detail/resolution at different time and length scales) has been key to extend
the spatio-temporal reach of MD and MC simulations of biomolecular dynamics. Such work has
521

fiS HEHU & P LAKU

the additional onerous task of designing accompanying energy functions that reproduce known
thermodynamic properties even at lower or mixed structural resolution. Indeed, the 2013 Nobel
Prize to Warshel recognized seminal work by him in multiscale models built with the QM/MM
method (Warshel & Levitt, 1976; Warshel, 2003; Kamerlin, Haranczyk, & Warshel, 2009; Mukherjee & Warshel, 2011, 2012; Dryga, Chakrabarty, Vicatos, & Warshel, 2011; Rychkova, Mukherjee,
Bora, & Warshel, 2013; Mukherjee & Warshel, 2013). The interested reader is directed to the review
by Clementi (2008) on coarse-grained models. The review by Zhou (2014) focuses on multiscale
models.
2.4.2 E NCODING VARIABLE D EPENDENCIES IN C ARTESIAN C OORDINATE -BASED M ODELS
Outside of the realm of modeling chemical reaction processes, such as bond formation and breaking,
when modeling biomolecular equilibrium structures and dynamics, other application setups require
preserving certain structural features that can be formulated as local and non-local constraints. Some
constraints, such as keeping bonded atoms at a distance no more than the ideal/equilibrium length of
their bond, are known as explicit, local constraints. They are trivially extracted from specification of
the chemical composition of a molecule, as they involve neighboring atoms. Equilibrium conditions
place additional, implicit constraints on biomolecular structures. The need to preserve favorable
Lennard-Jones interactions, for instance, places (non-local/long-range) constraints over non-bonded
atoms. Such long-range constraints cannot be effectively captured by a model where there is a
variable for each Cartesian coordinate of each atom. Any perturbation operator that interfaces with
such a model and modifies such variables has no information on invalid or energetically-unfavorable
assignments to subsets of variables, as variable dependencies are not captured in the model. An
external energy model is crucial here in the form of an energy function to evaluate the results of the
perturbation operator and detect variable instantiations resulting in violations.
The Cartesian coordinate-based model can encode variable dependencies. The latter can be
extracted via several techniques, including multivariate analysis techniques that analyze known
equilibrium structures of a biomolecule to identify subsets of atoms that exhibit simultaneous displacements; that is, move in concert. The essential premise of such techniques is that such known
structures are good examples of solutions or near-solutions of the energy function, and that analysis
of such examples will expose variable dependencies. These dependencies can then be employed
to design reduced Cartesian coordinate-based models that readily encode in them the energetic
constraints satisfied by the provided examples (solution or near-solution structures) and effective
perturbation operators that readily yield new near-solution instantiations in the reduced variable
space (Clausen & Shehu, 2015).
Multivariate Analysis Techniques to Obtain Collective Variables The sub-field of statistical
techniques for the identification of collective motions, which are also referred to as collective coordinates or collective variables is rich, and a review is not the subject of our survey. Instead, we point
to recent work in the narrow context of biomolecular modeling, where variance-maximizing techniques, such as Principal Component Analysis (PCA) (Shlens, 2003), Isomap (Tenenbaum, de Silva,
& Langford, 2000), Locally Linear Embedding (Roweis & Saul, 2000), Diffusion Maps (Coifman,
Lafon, Lee, Maggioni, Nadler, Warner, & Zucker, 2005), and others (van der Maaten, Postma, & van
den Herik, 2009) have been employed to analyze biomolecular structures and dynamics (Teodoro,
Phillips, & Kavraki, 2003; Das, Moll, Stamati, Kavraki, & Clementi, 2006; Plaku, Stamati, Clementi,
& Kavraki, 2007; Gorfe et al., 2008; Grant et al., 2009; Hori, Chikenji, & Takada, 2009; Maisuradze,
522

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Liwo, & Scheraga, 2009; Rohrdanz, Zheng, Maggioni, & Clementi, 2011; Zheng, Rohrdanz, Maggioni, & Clementi, 2011) and, more importantly, identify variables that represent collective motions
of atoms (Zheng, Rohrdanz, & Clementi, 2013; Clausen & Shehu, 2015; Clausen et al., 2015; Molloy, Clausen, & Shehu, 2016; Maximova, Plaku, & Shehu, 2015). In addition to these methods,
other ones such as Normal Mode Analysis (NMA) (Ciu & Bahar, 2005) also have a rich history
in computational structural biology (Atilgan, Durell, Jernigan, Demirel, Keskin, & Bahar, 2001;
Delarue & Sanejouand, 2002; Kim, Chirikjian, & Jernigan, 2002b; Zheng & Doniach, 2003; Tama,
Valle, Frank, & Brooks, 2003; Bahar & Rader, 2005; Maragakis & Karplus, 2005; Zheng & Brooks,
2005; Zheng, Brooks, & Hummer, 2007; Yang, Song, Carriquiry, & Jernigan, 2008; Yang, Majek,
& Bahar, 2009; Das, Gur, Cheng, Jo, Bahar, & Roux, 2014). The normal modes extracted from
NMA are also often employed as as effective perturbation operators in robotics-inspired methods (Tama & Sanejouand, 2001; Kim, Jernigan, & Chirikjian, 2002a; Kirillova, Cortes, Stefaniu, &
Simeon, 2008; Schuyler, Jernigan, Wasba, Ramakrishnan, & Chirikjian, 2009; Teknipar & Zheng,
2010; Baron, 2013; Al-Bluwi, Vaisset, Simeon, & Cortes, 2013). Our summary and highlights of
robotics-inspired methods later in this survey describe in greater detail collective variables and their
employment in effective perturbation operators.
2.5 Internal Coordinate- and Angular-Based Models
The internal coordinate model has been offered as an effective alternative to the Cartesian coordinatebased model (Burgess & Scheraga, 1975). In the internal-coordinate model, the only variables
selected are bond lengths, angles between two consecutive bonds, and torsion or dihedral angles
between three consecutive bonds. Figure 3(b) provides an illustration. This model allows for fast
forward kinematics, as changes to Cartesian coordinates as a result of changes to the values of these
variables can be efficiently calculated via accumulation of rigid-body transformations (Craig, 1989;
Zhang & Kavraki, 2002a).
Internal coordinate-based models are now the norm in non-MD based molecular structure modeling. An additional simplification is made for equilibrium protein structures. Analysis of deposited
equilibrium structures of proteins reveals that bond lengths and bond angles are constrained to characteristic values (Engh & Huber, 1991). This is a consequence of the energetic constraints placed
on structures at equilibrium and is exploited to idealize protein geometry in modeling by effectively
removing bond lengths and bond angles from the list of variables in the model. This leaves only
dihedral angles defined over three consecutive bonds as variables (,  backbone angles and at
most four dihedral side-chain angles per amino acid, as shown in Figure 3(a)) and is computationally appealing, as the number of dihedral angles for a polypeptide chain of N atoms is on average
3N/7 (Abayagan, Totrov, & Kuznetsov, 1994). It is worth noting that bond lengths and bond angles do change even at equilibrium, but at a faster pace than other motions. Employing idealized
geometry allows devoting computation to obtaining the slower fluctuations first. Once structures
representative of a molecules equilibrium dynamics are obtained, deviations of bond lengths and
bond angles can be introduced and studied via more detailed models.
2.5.1 B IOMOLECULES AS K INEMATIC C HAINS WITH R EVOLUTE J OINTS
Idealizing protein geometry reveals mechanistic analogies with kinematic chains with revolute
joints. Similarly to how a joint rotation changes positions of following links, so does rotation
by a dihedral angle change positions of following atoms (Craig, 1989). These analogies have been
523

fiS HEHU & P LAKU

employed by robotics researchers to apply algorithms that plan motions for kinematic chains with
revolute joints to the study of protein conformations (Manocha & Zhu, 1994; Singh, Latombe, &
Brutlag, 1999; Apaydin, Singh, Brutlag, & Latombe, 2001; Amato, Dill, & Song, 2003; Apaydin,
Brutlag, Guestrin, Hsu, & Latombe, 2003; Song & Amato, 2004; Cortes, Simeon, & Tran, 2004;
Cortes, Simeon, Guieysse, Remaud-Simeon, & Tran, 2005; Lee, Streinu, & Brock, 2005; Kim
et al., 2002a; Chiang, Apaydin, Brutlag, Hsu, & Latombe, 2007; Shehu & Olson, 2010; Molloy
et al., 2013; Molloy & Shehu, 2013; Haspel, Moll, Baker, Chiu, & E., 2010; Shehu, Clementi, &
Kavraki, 2006). Unlike typical articulated robotic mechanisms, protein chains pose hundreds rather
than a dozen variables (a short backbone of 50 amino acids poses 100 dihedral angles as variables).
The analogies between protein chains and kinematic chains with revolute joints are popular
among robotics researchers proposing robotics-inspired methods for modeling biomolecular structure and dynamics. For instance, torsional angles were employed early to model protein ligand
binding (Singh et al., 1999) and remain popular in modeling the kinetics of folding in small protein
and RNA molecules (Han & Amato, 2001; Amato et al., 2003; Song & Amato, 2004; Thomas,
Song, & Amato, 2005; Thomas, Tang, Tapia, & Amato, 2007; Tang, Thomas, Tapia, Giedroc, &
Amato, 2008; Tapia, Thomas, & Amato, 2010). Such angles have also proved popular in computing
functionally-relevant structures of peptides and proteins (Haspel, Tsai, Wolfson, & Nussinov, 2003;
Shehu et al., 2006; Shehu, Clementi, & Kavraki, 2007; Shehu, Kavraki, & Clementi, 2007, 2008;
Cortes et al., 2004; Shehu, Kavraki, & Clementi, 2009; Shehu, 2009; Shehu & Olson, 2010; Molloy
et al., 2013), as well as in modeling peptides and proteins switching between different functionallyrelevant structures (Cortes et al., 2005; Jaillet, Cortes, & Simeon, 2008; Haspel et al., 2010; Jaillet,
Corcho, Perez, & Cortes, 2011; Molloy et al., 2016; Molloy & Shehu, 2013, 2015; Devaurs, Molloy,
Vaisset, Shehu, Cortes, & Simeon, 2015; Molloy & Shehu, 2016).
Techniques to Obtain Reduced Angular-based Models The number of variables in angularbased models can be reduced further via various techniques. For instance, consecutive dihedral
angles can be bundled together into fragments to capture variable dependencies. This technique,
known as molecular fragment replacement and introduced in the context of MC-based methods for
de novo protein structure prediction (Bradley, Misura, & Baker, 2005), allows operationalizing on
the observation that a limited number of configurations are observed for k-bundles of consecutive
dihedral angles among stable protein structures at equilibrium (Han & Baker, 1996). This technique has been incorporated in robotics-inspired methods for modeling equilibrium protein structure and dynamics (Shehu & Olson, 2010; Molloy et al., 2013; Molloy & Shehu, 2013, 2016). Other
application-specific techniques analyze structures to reduce or prioritize the number of dihedral angles for manipulation by a perturbation operator. Rigidity-based techniques, for instance, analyze a
given structure to detect least-constrained regions and suggest an order for which dihedral angles to
modify first or more often in order to focus computational resources to computing the large structural deformations first (Thorpe & Ming, 2004; Wells, Menor, Hespenheide, & Thorpe, 2005; Fox
& Streinu, 2013). Rigidity-based analysis has been incorporated in robotics-inspired methods for
modeling protein dynamics (Thomas et al., 2007). Other techniques are aimed specifically at modeling structural transitions in large proteins (Raveh, Enosh, Furman-Schueler, & Halperin, 2009;
Haspel et al., 2010). In these, a comparison of the two structures for which a transition is sought
identifies the differently-valued dihedral angles. In a similar fashion as in rigidity-based analysis,
these angles are prioritized and modified more often by perturbation operators in order to capture
possibly large structural deformations in a reasonable amount of time. All these techniques make
524

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

several assumptions about which variables participate in the process of interest, and we highlight
such assumptions and their implications later in this survey.
Biomolecular Structure versus Biomolecular Conformation In light of the various models that
can be employed to represent biomolecular structure, a distinction needs to be made between the
terms structure and conformation. The term structure is meant to refer to the specification of the
Cartesian coordinates of the atoms that comprise a molecule (even if not all atoms are explicitly
modeled, as in a backbone or reduced structure). The term conformation is meant to be more general and refer to the specification of the values of variables selected in the model of a structure; that
is, a conformation is a particular instantiation in the employed variable space. For instance, a conformation is the instantiation of angles if an angular-based model is employed, and forward kinematics
allows obtaining the structure encoded by such a conformation. It is worth noting that the terms
conformation and structure are often used interchangeably and in a slight abuse of terminology
in biomolecular modeling literature. For instance, many algorithms that explicitly modify structures via Cartesian coordinate-based models are referred to as conformational search algorithms;
of course, when such models are employed, a structure can be extracted trivially from a conformation. In this survey, the distinction made above will be observed when referring to structures and
conformations.
In the specific domain of robotics-inspired methods, the term (molecular) conformation is equivalent to (robot) configuration. However, in keeping with the broader computational biology literature, we will employ the term conformation when referring to macromolecules, such as proteins,
RNA, and DNA, reserving the term configuration for small molecules (also referred to as ligands)
that bind to macromolecules. In addition, while the term variable is often referred to as parameter in the broader computational biology and biophysics literature and degree of freedom (dof) in
robotics and AI literature, we will employ the more general, non-domain specific term of variable
and variable space. That is, a (molecular) conformation is an instantiation in the space of selected
variables. Depending on the number of variables selected to model the molecule under investigation, the variable space may be high-dimensional. Mapping a conformation to its corresponding
structure allows associating a structure space to the employed variable space. Moreover, energy
functions allow associating an energy surface to the structure space, and interesting observations
regarding stable and semi-stable structural states and excursions among such states can be made by
analyzing the structure space and low-dimensional projections/embeddings of its underlying energy
surface, hence the energy landscape.
Educational Resources The purpose of the material related above is to provide enough detail on
biomolecular geometry to allow seeing how biomolecules can be treated mechanistically as modular systems composed of numerous, heterogeneous components for the purpose of characterizing
them in silico. Further information for readers of varying levels of background or interest can be
found in online, educational learning modules designed to introduce computer scientists to computational structural biology. One such set of modules, publicly accessible under the cnx project and
highly popular with students and researchers, can be found at http://cnx.org/contents/
9cMfjngH@6.3:ppj-3H2A@14/Structural-Computational-Biolo. In these modules, for instance, interested readers can learn about protein architecture in greater detail. Other
modules mirror the material summarized here on representations and energy functions, and yet others introduce readers to forward and inverse kinematics for modular mechanical systems, making
these modules a good supplement to the survey of robotics-inspired methods presented here.
525

fiS HEHU & P LAKU

3. Summary of Biomolecular Modeling Problems and Robot Motion Planning
Frameworks
We introduce the main classes of problems in biomolecular modeling that are addressed with
robotics-inspired methods. The robot motion planning frameworks over which these methods build
are summarized next. The section concludes with a summary of challenges faced by algorithmic realizations of such frameworks for modeling biomolecular structure and dynamics. These challenges
are a preview of important design decisions that are detailed in Section 4 in the context of reviewing
representative methods.
3.1 Representative Problems in Biomolecular Modeling
Two main classes of molecular mechanisms are studied by robotics-inspired methods, those that
involve more than one molecule associating/complexating with or disassociating from one another,
and those that involve a dynamic, uncomplexated molecule. As application setups, both concern
informing our understanding of dynamic events involving dynamic molecules.
In the first application setup, robotics-inspired methods aim to model and understand proteinligand binding events. Provided unbound structures of the protein receptor and a small ligand
molecule, the objective is to elucidate how the ligand approaches and then binds the protein receptor. In a related problem, the reverse process is addressed. The ligand is bound to the receptor,
and the goal is to determine motions of the ligand and the protein receptor that allow disassociation.
Modeling protein-ligand binding is important not only for a general understanding of our biology
but also for computation-aided drug discovery. While other molecular recognition events such as
protein-protein, protein-DNA, protein-RNA, and protein-membrane binding conceptually fall in the
same category as protein-ligand binding, they are more challenging due to the higher number of
variables needed to model the association or complexation event and are currently beyond the domain of applicability of robotics-inspired methods.
The second application setup concerns modeling and understanding the dynamics of molecules.
Almost exclusively, the focus of robotics-inspired techniques in this category are uncomplexed protein and RNA molecules. The goal is to elucidate the structural deformations or motions that allow
a protein or an RNA molecule to transition between two structural states of interest. These states
can be the unfolded and folded state, in which case the goal is to highlight folding and unfolding
paths, or they can both be stable or semi-stable structures employed by the molecule to recognize
and lock onto different molecules, in which case the goal is to formulate hypothesis regarding the
impact of structure in molecular recognitions in the healthy and diseased cell.
Figures 4-5 illustrates these problems. In Figure 4, a robotics-inspired method can highlight how
the ligand approaches the protein receptor, as well as where it binds onto the receptor and into what
configuration. The variables of interest need at a minimum to include the translational and rotational
variables of the ligand, as well as internal coordinates to capture the potential structural flexibility
of the ligand. The receptor can be considered frozen in 3d space. A more accurate setup would
also consider internal coordinates of the receptor in order to model its possible structural flexibility
upon ligand binding. However, the resulting number of variables would be too large. A roboticsinspired method can not only elucidate the bound ligand configuration and its bound placement
relative to the receptor, but also the possible routes of successive configurations and placements
that the ligand may follow to approach the binding site(s). In addition, as Figure 5(a) illustrates,
a robotics-inspired method can show the possible routes of successive structures employed by a
526

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Figure 4: (a) Where and how does the ligand bind to the protein receptor? Many methods are designed to
elucidate the step-by-step process of how a ligand approaches a protein molecule, where it binds, and with
what configuration.

protein to fold, thus shedding light into the process of protein folding and unfolding. Similar setups
consider RNA molecules. Figure 5(b) illustrates that often robotics-inspired methods are employed
to reveal not only folding or unfolding routes of a protein, but also structural transitions between any
two structures of interest; knowledge of the most probable routes that carry out the transition allows
understanding at a structural level the mechanism by which a biomolecule regulates its biological
activity in the cell.
3.2 Foundations of Robotics-inspired Treatments of Biomolecules
The fundamental assumption of robotics-inspired treatments of biomolecules is that mechanistic
analogies between molecular chains and robot chains allow putting together efficient algorithms for
rapid exploration of molecular structure spaces and modeling of excursions of molecules on such
spaces (Manocha & Zhu, 1994; Singh et al., 1999; Apaydin et al., 2001; Amato et al., 2003; Apaydin
et al., 2003; Song & Amato, 2004; Cortes et al., 2005; Kim et al., 2002a; Chiang et al., 2007;
Kirillova et al., 2008). That is, instead of simulating how a molecule navigates its energy surface
via gradient-based and other local search techniques, more powerful techniques can be put together
by building on algorithms demonstrated to have high exploration capability on robot configuration
spaces. Robotics-inspired treatments of biomolecules draw from techniques for fast forward and
527

fiS HEHU & P LAKU

(a) Protein folding

(b) Structural Transitions
Figure 5: (a) How do proteins fold? Shedding light into the process of protein folding is an important goal,
and many robotics-inspired methods are devoted to elucidating this process step by step by computing the
most probable succession of structures assumed by a protein navigating from the unfolded to the folded state.
(b) How do proteins transition between the diverse structures they use for interacting with different partners in
the cell? Robotics-inspired methods seek to elucidate structural transitions between meta-stable and stable
structural states of a protein.

inverse kinematics and, more importantly, sampling-based algorithms developed in the algorithmic
robotics community to address the robot motion-planning problem (Choset & et al., 2005).
528

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

The objective in robot motion planning is to obtain paths that take a robot from a given, start
configuration to a given, goal configuration. The robot motion planning problem bears mechanistic
analogies to the problem of computing conformations along a transition trajectory of a biomolecule;
in both problems, the driving objective is to uncover what of the underlying (molecular) conformation or (robot) configuration space is employed in motions of an articulated system from a start to a
goal conformation or configuration. Analogies between molecular bonds and robot links and molecular atoms and robot joints help to draw from techniques that perform fast kinematics for kinematic
linkages (Manocha, Zhu, & Wright, 1995; Zhang & Kavraki, 2002b); that is, specifying values for
the variables selected to represent a molecular conformation, rapidly update Cartesian coordinates
of the corresponding structure (Zhang & Kavraki, 2002b). In the inverse kinematics setting, these
techniques allow rapidly obtaining values to underlying variables consistent with Cartesian-based
constraints (Chirikjian, 1993; Manocha & Canny, 1994; Zhang & Kavraki, 2002a; Kolodny, Guibas,
Levitt, & Koehl, 2005).
At a higher level, robotics-inspired methods operationalize on two key observations. The first
observation, originally made for robot configuration spaces, is that solution-containing regions of
a high-dimensional and non-linear variable space can only be found by heuristic rather than exact
approaches; stochastic, or sampling-based techniques can construct the distribution of constraintsatisfying instantiations in a two-stage manner; an initial distribution is first constructed via sampling of the unconstrained variable space. Instantiations are then evaluated through external (energetic) models capable of capturing inter-variable dependencies and penalizing violations of constraints. Violating samples are either removed or down-weighted so the initial, uninformed distribution gradually converges to that containing solutions. The second observation is that transitions of
a system between two given solutions can be modeled via discrete, kinetic models that essentially
embed solutions in graph-like structures amenable to rapid, shortest, or lowest-cost path queries,
provided that lengths or other cost metrics can be associated with a given series of configurations in
a path. Methods that embed solutions in a tree are referred to as tree-based methods, and those that
embed solutions in a graph are referred to as roadmap-based methods.
3.3 The Motion Planning Framework: Tree- and Roadmap-Based Methods
In the context of molecular modeling, tree-based methods grow a tree in conformation space from
a given, start to a given, goal conformation representing the structures bridged by the sought transition. The tree is incrementally extended, with every iteration adding a new conformation node and
a new branch to the tree. Depending on whether the tree is pulled towards configurations sampled
at random over the configuration space or pushed from leaves towards new regions of the configuration space, the method is known as Rapidly Random Exploring Tree (RRT) (LaValle & Kuffner,
2001), or Expansive Spaces Tree (EST) (Hsu, Kindel, Latombe, & Rock, 2002), accordingly. It is
important to note here that the sampling and connectivity go hand in hand, as every sampled conformation is added to the growing tree. The growth of the tree is biased so the goal conformation
can be reached in a reasonable computational time. As a result, tree-based methods are efficient but
limited in their sampling. They are known as single-query methods, as they can only answer one
start-to-goal query at a time; that is, only one path of consecutive conformations that connect the
start to the goal can be extracted. Running them multiple times to sample an ensemble of conformation paths for the same query results in an ensemble with high inter-path correlations due to the
biasing of the conformation tree.
529

fiS HEHU & P LAKU

Roadmap-based methods adapt the Probabilistic Road Map (PRM) framework
(Kavraki, Svestka, Latombe, & Overmars, 1996). Rather than grow a tree in conformation space,
these methods detach the sampling of conformations from the connectivity model that encodes
neighborhood relationships among conformations in the conformation space. Typically, a sampling
stage first provides a discrete representation of the conformation space of interest, with conformations satisfying explicit or implicit geometric and energetic constraints, and then a roadmap building
stage embeds the sampled conformations in a graph/roadmap by connecting each one to its nearest neighbors. Roadmap-based methods can provide richer information regarding a dynamic event,
as multiple paths may exist in the roadmap connecting two structures of interest. Moreover, these
methods support multiple queries, as in principle the same graph can be used to extract paths connecting different given structures. These structures can be specified as conformations and connected
to nearest neighbors in the graph, and then the graph can be queried for optimal paths. In practice,
it is difficult to obtain broad and dense sampling of sufficient regions in the conformation space of
a molecule so as to elucidate diverse excursions between structures of interest.
Below we provide further detail on tree-based and roadmap-based methods in robotics to familiarize the reader with the diverse design decisions employed and even adapted by robotics-inspired
methods for biomolecular modeling.
3.3.1 N ON - HIERARCHICAL T REE -BASED ROBOT M OTION P LANNING M ETHODS
Tree-based methods can be categorized broadly based on the strategies employed to select the vertex
from which to expand the tree. Non-hierarchical strategies consider all the vertices as possible
candidates. Hierarchical strategies place the tree vertices at a bottom layer and introduce additional
high-level layers that group similar vertices together, proceeding from the top to the bottom layer
during the selection process.
RRT is one of the most widely used non-hierarchical tree-based motion planning methods. In
RRT (LaValle & Kuffner, 2001), at each iteration, the tree is expanded towards a randomly-sampled
configuration qrand . The nearest vertex, qnear , in the tree to qrand is determined according to a distance metric. A local planner attempts to connect qnear to qrand . Often the local planner interpolates
over the underlying variables to generate intermediate configurations. In the basic version of RRT,
the iteration stops after one interpolation step. In the connect version, the expansion continues until
qrand is reached or the interpolation results in an invalid configuration, e.g., collision with obstacles.
This process of sampling a configuration and expanding from the nearest neighbor in the tree is repeated until the goal is reached. Figure 6 shows such a tree. By using random sampling and nearest
neighbors, RRT exhibits a Voronoi bias which enables the expansion of the tree toward unexplored
regions. To also bias the search towards the goal, qrand is often selected with probability b (often set
to 0.05) as the goal configuration and with probability 1  b uniformly at random.
Over the years, different RRT variants have been developed in order to improve the exploration.
The adaptive dynamic domain RRT (ADDRRT) (Jaillet, Yershova, LaValle, & Simeon, 2005) associates a sampling radius with each tree vertex and dynamically adjusts the radius based on the
success of the local planner. The reachability-guided RRT (RGRRT) (Shkolnik, Walter, & Tedrake,
2009) relies on the notion of reachable sets to increase the likelihood of successful tree expansions. The obstacle-based RRT (OBRRT) (Rodriguez, Tang, Lien, & Amato, 2006b) increases
sampling near obstacles, PCARRT (Dalibard & Laumond, 2009) relies on PCA, and The selective
retraction-based RRT (SRRRT) (Lee, Kwon, Zhang, & Yoon, 2014) uses bridge sampling and se530

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Figure 6: The tree built by RRT is shown here on a simplistic environment.

lective retraction in order to facilitate expansions inside narrow passages. The utility-guided RRT
(UGRRT) (Burns & Brock, 2007) associates a utility measure with each vertex and uses it to promote expansions that increase the utility. RRT-Blossom (Kalisiak & van de Panne, 2006) creates
a flood-fill behavior to locally explore the area surrounding each vertex. Machine learning has also
been used to derive a distance metric that captures the cost-to-go in order to improve the exploration
in RRT (Palmieri & Arras, 2015). The reachable volume RRT (RVRRT) (McMahon, Thomas, &
Amato, 2015) relies on the notion of reachable volumes in order to restrict sampling to feasible
regions and improve the performance of RRT on highly-constrained problems. The abstractionguided RRT (fRRT) (Kiesel, Burns, & Ruml, 2012) uses A* search on a grid-based decomposition
to bias RRT sampling towards low-cost regions. RRT* (Karaman & Frazzoli, 2011) rewires the
branches in RRT to find optimal solutions with respect to a given cost function.
EST (Hsu et al., 2002) takes a different approach from RRT by pushing the frontier of the
tree towards unexplored areas. Instead of relying on expansions from the nearest neighbor, EST
maintains a probability distribution over the tree vertices. At each iteration, a vertex v is selected
with probability inversely proportional to the density of a small neighborhood around v. This allows
EST to push the tree towards less-explored regions of the configuration space.
3.3.2 H IERARCHICAL T REE -BASED M OTION P LANNING M ETHODS
Hierarchical tree-based methods rely on a scheme which first selects a region and then a vertex from
which to expand the tree. Regions are often defined based on a decomposition of a low-dimensional
projection of the configuration space. The rationale is that, by grouping similar vertices, better
selections can be made at the region level to effectively guide the tree exploration. For instance,
the single-query, bidirectional, lazy collision-checking (SBL) method (Sanchez & Latombe, 2002)
pushes the tree toward sparse regions by using a grid-based decomposition and uniform probability distributions to select non-empty grid cells. The kinodynamic planning by interior-exterior cell
exploration) (KPIECE) method (Sucan & Kavraki, 2012) relies on a multi-level grid decomposition constructed over user-defined or random linear projections. The synergistic combination of
layers of planning (SyCLoP) method (Plaku, Kavraki, & Vardi, 2010) uses discrete search over a
531

fiS HEHU & P LAKU

low-dimensional triangular or grid decomposition to guide the tree exploration along short region
paths to the goal. The guided sampling tree (GUST) method (Plaku, 2015) partitions the motion
tree into equivalence classes and relies on multi-objective criteria based on shortest-path distances,
selection penalties, and progress made to determine equivalence classes which could result in rapid
expansions toward the goal.
The path-directed subdivision tree (PDST) method (Ladd & Kavraki, 2004, 2005) relies on
a grid subdivision of the configuration space. Each cell in the decomposition keeps track of the
tree branches that fall into it. Initially, the tree has only the root vertex and there is only one cell
corresponding to the minimum and maximum values for each variable. At each iteration, a tree
branch is selected for expansion. The cell c that contains the selected tree branch is divided into
two cells, c1 and c2 , along the largest dimension. The tree branches in c are also split according
to the boundaries between c1 and c2 . This ensures the invariant that a tree branch is contained
entirely in one cell. The selected tree branch b is expanded by picking a configuration q along b and
using propagation to add a new branch starting at q. Propagation is problem-dependent and could
correspond to moving in a random direction.
The propagation continues until a collision is found or a maximum number of steps is reached.
The branch is split when it exits the cell boundaries. A key component of the PDST is the weighting
scheme associated with each cell based on its volume, number of branches, and number of previous
selections. When selecting a tree branch, in order to increase the coverage, priority is given to cells
that have large volumes but have not been well-explored. After each iteration, the selected cell
c is penalized in order to ensure that other cells will eventually be selected for expansion. This is
necessary to avoid oversampling and guarantee probabilistic completeness (Ladd & Kavraki, 2005).
PDST has also been combined with artificial potential fields in order to expand the tree from the
region with the lowest potential (Bekris & Kavraki, 2007).
3.4 Roadmap-Based Robot Motion Planning Methods
The introduction of PRM (Kavraki et al., 1996) shifted the focus from complete to probabilisticallycomplete motion-planning algorithms, which guarantee to find a solution, when it exists, with probability approaching one as time tends to infinity. While complete algorithms were limited to simple
problems with 23 variables, PRM made it possible to efficiently solve high-dimensional problems. The underlying idea in PRM is to construct a roadmap which captures the connectivity
of the (obstacle-)free configuration space. The roadmap is populated by generating a number of
collision-free configurations. Each configuration is generated by sampling values for the variables
uniformly at random. The configuration is discarded, if it results in collision. Otherwise, it is added
to the roadmap. To capture the connectivity, neighboring roadmap configurations are connected
with collision-free paths. Figure 7 provides an illustration of a roadmap created by PRM. A common approach is to compute for each roadmap configuration its k-nearest neighbors according to a
distance metric. A path between two configurations is often obtained by linear interpolation. If the
path is collision free, it is added as an edge to the roadmap graph. A path from a given start to a
goal configuration is found by first connecting the start and the goal configurations to the roadmap
and then searching the roadmap graph. A* is often used to efficiently compute the shortest roadmap
path. Additional sampling may be required when the initial roadmap does not contain a path from
the start to the goal.
532

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

b
b

b
b

b
b
b

b
b

b

b
b

b

b

b
b

b

goal

b

start

b
b
b

Figure 7: An illustration of a roadmap that can be used to answer start-to-goal queries.

Over the years, numerous strategies have been proposed to enhance the sampling in PRM.
Obstacle-based PRM (OBPRM) (Amato, Bayazit, Dale, Jones, & Vallejo, 1998) seeks to increase
sampling near obstacles in order to improve the connectivity inside narrow passages. BridgePRM
(Sun, Hsu, Jiang, Kurniawati, & Reif, 2005) has a similar objective but uses a bridge test to generate
samples halfway between two obstacles. Machine learning has also been used in conjunction with
a portfolio of samplers to enhance sampling in narrow passages (Hsu, Sanchez-Ante, & Sun, 2005).
The region-sensitive adaptive PRM (RESAMPL) (Rodriguez, Thomas, Pearce, & Amato, 2006a)
uses the notion of entropy to identify regions that can enhance sampling. TogglePRM (Denny &
Amato, 2013) switches between the free configuration space and the obstacle space in order to facilitate connections in narrow passages. ANC-Spatial (Ekenna, Thomas, & Amato, 2016) uses a
spatial-learning approach to enhance the roadmap connectivity by determining appropriate connection methods for each roadmap vertex. PRM* (Karaman & Frazzoli, 2011) is a variant of PRM that
leads to optimal solutions with respect to a given cost function. The modification is surprisingly
simple, as it requires only using a variable number of nearest neighbors instead of a fixed k.
3.5 Biomolecular Modeling Challenges for Tree- and Roadmap-Based Methods
Both tree- and roadmap-based methods experience the curse of dimensionality in several ways. A
central issue concerns the breadth of sampling in possibly high-dimensional and complex variable
spaces. In particular, in the context of biomolecular modeling, the decision of which variables to
represent is key, as it determines the dimensionality and complexity of the variable/conformation
space. This decision is tightly tied with the application setting or the class of biomolecular systems
considered. As reviewed in Section 2, while in many adaptations the selected variables are all or a
subset of the backbone dihedral angles (Han & Amato, 2001; Amato et al., 2003; Song & Amato,
2004; Thomas et al., 2005, 2007; Jaillet et al., 2008; Tang et al., 2008; Tapia, Tang, Thomas, &
Amato, 2007; Tapia et al., 2010; Jaillet et al., 2011; Shehu & Olson, 2010; Molloy et al., 2013;
Molloy & Shehu, 2013), in others the selected variables capture collective motions of atoms in the
3d Cartesian space (Kim et al., 2002b, 2002a; Kirillova et al., 2008; Schuyler et al., 2009; Al-Bluwi
et al., 2013; Maximova et al., 2015). Whether values for variables are sampled individually or in
533

fiS HEHU & P LAKU

tandem from a-priori compiled databases of good moves (Shehu & Olson, 2010; Molloy et al.,
2013; Molloy & Shehu, 2013), or whether diverse perturbation/sampling operators are employed
that make use of different sets of variables (Gipson, Moll, & Kavraki, 2013; Molloy & Shehu, 2016),
the dimensionality and size of the variable space remains a key challenge for tree- and roadmapbased treatments of biomolecules.
The choice of variables is key to the design of effective sampling or perturbation operators for
generating conformations that satisfy a set of desired geometric and/or energetic constraints for the
biomolecular modeling problem at hand. Samples obtained uniformly at random have very low
probability of being low-energy or in the region of interest for a sought structural transition. In
particular, for long protein chains with hundreds or more backbone dihedral angles, a conformation
sampled at random is highly unlikely to be physically-realistic.
Biased sampling techniques can be used to remedy this issue (Amato et al., 2003; Song &
Amato, 2004), but it is hard to know a priori which perturbation operators will be effective. Recent work recognizes this issue and addresses it by offering diverse sampling operators on possibly
diverse sets of variables (Raveh et al., 2009; Gipson et al., 2013; Molloy & Shehu, 2016). In particular, the work of Molloy and Shehu (2016) implements a probabilistic scheme that selects among a
rich menu of operators making use of angular or Cartesian variables.
It is worth noting that sampling operators may generate samples not in a vacuum but by incremental modifications of existing samples. While earlier generations of tree- and roadmapbased methods typically obtained new conformations by sampling values over the selected variables (Singh et al., 1999), recent methods generate samples in neighborhoods of existing parent
samples (Shehu & Olson, 2010; Molloy & Shehu, 2013; Maximova et al., 2016) (hence, the often
used term perturbation operator). This later strategy has a higher chance of yielding physicallyrealistic conformations, as perturbation operators that perturb a selected sample to obtain a new one
tend to preserve some good structural features in the new sample while introducing enough change
to explore new regions of the variable space (Olson, Hashmi, Molloy, & Shehu, 2012). In the context
of perturbation operators, selection schemes are critical to control sampling. A recent phenomenon
in robotics-inspired methods has been the recognition that selection schemes, which are central to
hierarchical tree-based robot motion planning (as reviewed above), can also be employed in both
tree- and roadmap-based methods to steer biomolecular sampling to regions of interest (Shehu &
Olson, 2010; Molloy & Shehu, 2013; Maximova et al., 2016).
An additional challenge with ensuring high sampling capability is that biomolecules have underlying complex energy surfaces that encode energetic constraints. Therefore, the criterion for
accepting a sampled conformation and adding it to the vertex list of the tree or roadmap needs to either rely on an a-priori set energy threshold or be probabilistic in nature. The latter setting provides
a balance between obtaining low-energy conformations while allowing a particular algorithm to go
over high-energy barriers as needed to sample more of the conformation space (Jaillet et al., 2008,
2011; Molloy & Shehu, 2013; Devaurs et al., 2015; Molloy & Shehu, 2016).
Both roadmap- and tree-based methods rely on local planners or local deformation techniques
to connect neighboring conformations. In tree-based methods that push the tree out in the variable
space by generating child conformations from selected parent conformations via perturbation operators, the child becomes a neighbor of the selected parent; the neighborhood function does not rely
on the notion of a distance in the variable space but is instead tied to the parent-child relationship.
In other methods, a sampled conformation needs to be connected to one or more nearest neighbors. Nearest-neighbor calculations need specification of a distance function between conforma534

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

tions, which is non-trivial in high-dimensional spaces. Most adaptations employ least Root-MeanSquared-Deviation (lRMSD), which is a modification of Euclidean distance after differences due to
rigid-body motions have been removed through optimal superimposition of the protein conformations under comparison (McLachlan, 1972). lRMSD is carried out over Cartesian coordinate-based
instantiations. Other distance functions use L1 or related variants defined over dihedral angles.
It is generally challenging to find computationally-efficient and dynamics-integrating local planners for biomolecular conformations. If conformations q1 and q2 are nearby in variable or structure
space, the local path that is encoded with an edge in the tree or roadmap should encode the process
of diffusion. The local path should provide evidence of the diffusion of the biomolecule from q1 to
q2 in the presence of thermal vibrations. It is not readily obvious how to use dynamics to steer a
biomolecule from q1 to q2 . While in principle MD simulations can be employed in search of such
paths, there is no guarantee that these simulations will reach q2 . While biased MD simulations can
be employed to reach q2 , such simulations modify the energy surface and do not model the actual
dynamics (Ma & Karplus, 1997). Moreover, any corrections to biased MD simulations to model the
actual dynamics prove computationally expensive (Ovchinnikov & Karplus, 2012) in the context of
robotics-inspired methods that evaluate thousands of edges.
As a result, the majority of robotics-inspired methods for biomolecular modeling employ local
planners that carry out linear interpolations over the variables of the conformations that need to be
connected via a tree branch or roadmap edge. Such planners produce unrealistic conformations, and
significant time can be spent correcting geometry and other ensuing energetic violations via energy
minimization. Recent work proposes complex, local planners that are not based on interpolation
but are instead re-formulations of the motion computation problem; that is, the local planners themselves are tree- or roadmap-based methods (Molloy & Shehu, 2016). The latter idea is borrowed
from the similarly challenging setting of motion planning for manipulators, where linear interpolation is also not effective (Nielsen & Kavraki, 2000). When making use of complex local planners,
a prioritized path sampling scheme is needed to prioritize the application of these computationallydemanding planners on the most promising paths in order to control computational cost (Nielsen &
Kavraki, 2000). The work of Molloy and Shehu (2016) provides an implementation of prioritized
path sampling for biomolecular modeling.

4. Robotics-Inspired Methods for Equilibrium Biomolecular Structure and
Dynamics
Table 1 categorizes different robotics-inspired methods by the robot motion planning frameworks
they adapt and the application setups they address. The table is not comprehensive by any means,
but it may be useful to readers selecting to focus on specific applications. In the rest of this Section
we describe these methods in greater detail, paying particular attention to recent, state-of-the-art
methods that showcase the current capabilities of robotics-inspired treatments of biomolecules.
4.1 Tree-Based Methods for Modeling Equilibrium Biomolecular Structure and Dynamics
Tree-based methods have been employed to model biomolecular flexibility and compute conformation paths connecting given structures (Cortes et al., 2005; Shehu, 2009; Shehu & Olson, 2010;
Jaillet et al., 2011; Haspel et al., 2010). Some tree-based methods address decoy sampling for the de
novo protein structure prediction problem (Shehu & Olson, 2010; Olson, Molloy, Hendi, & Shehu,
2012; Molloy et al., 2013) and map the entire energy landscape and pathways connecting stable
535

fiS HEHU & P LAKU

states in molecular loops, peptides, and proteins (Porta, Thomas, Corcho, Canto, & Perez, 2007;
Jaillet et al., 2011; Porta & Jaillet, 2013; Devaurs et al., 2015; Molloy et al., 2016). Others have
focused on specific flexible sub-chains, such as loops, rather than entire protein chains (Cortes et al.,
2004, 2005; Yao, Dhanik, Marz, Propper, Kou, Liu, van den Bedem, Latombe, Halperin-Landsberg,
& Altman, 2008; Barbe, Cortes, Simeon, Monsan, Remaud-Simeon, & Andre, 2011).
Table 1: Categorization of of Tree- and Roadmap-based Methods by Application Setting
Application
Protein
Loop
Motions
Protein-Ligand
Binding

Tree-based Methods
RLG-RRT (Cortes et al., 2005; Cortes,
Jaillet, & Simeon, 2007), ML-RRT (Barbe
et al., 2011)
ML-RRT (Cortes et al., 2007)

Protein Structure
Prediction
Protein and RNA
(Un)Folding

FeLTr (Shehu & Olson, 2010; Molloy
et al., 2013)

Peptide and Protein
Structural
Transitions

NMA-RRT (Kirillova et al., 2008; AlBluwi et al., 2013), PathRover (Enosh,
Raveh, Furman-Schueler, Halperin, &
Ben-Tal, 2008; Raveh et al., 2009), TRRT (Jaillet et al., 2011), PDST (Haspel
et al., 2010), Sprint (Molloy & Shehu,
2013), Multi-T-RRT (Devaurs et al., 2015)
T-RRT (Jaillet et al., 2011), Multi-TRRT (Devaurs et al., 2015)

Peptide and Protein Energy landscape Mapping

Roadmap-based Methods
LoopTK (Yao et al., 2008)

PCR (Singh et al., 1999; Apaydin
et al., 2001), SRS (Apaydin et al.,
2003)

SRS (Apaydin et al., 2003), PRMFP (Amato et al., 2003; Song &
Amato, 2004; Tang, Kirkpatrick,
Thomas, Song, & Amato, 2005;
Thomas et al., 2005, 2007; Tapia
et al., 2007; Tang et al., 2008; Tapia
et al., 2010), MaxFlux-PRM (Yang,
Wu, Li, Han, & Huo, 2007; Li, Yang,
Han, & Huo, 2008)
SRS (Molloy et al., 2016), Spiral (Molloy & Shehu, 2016), SoPRIM (Maximova et al., 2015)

SoPRIM (Maximova et al., 2015)

4.1.1 M ODELING P ROTEIN L OOP M OTIONS AND P ROTEIN -L IGAND D ISASSOCIATION
Early adaptations of the RRT algorithm for biomolecules focused on modeling the equilibrium dynamics of protein loops (Cortes et al., 2005) and protein-ligand interactions (Cortes et al., 2004).
For instance, the method presented in the work of Cortes et al. (2005) proceeds in two stages to
model large-amplitude structural changes in a protein loop at equilibrium. The first stage obtains an
ensemble of collision-free conformations of a loop in a protein structure. This is achieved through
the Random Loop Generator RRT (RLG-RRT) algorithm, which effectively samples the loop conformation space that satisfies kinematic closure constraints. The loop is divided into an active and a
passive part. The passive part is selected to contain 6 variables, whereas the active part contains the
rest of the angular variables selected to represent a loop conformation. The RLG-RRT algorithm
536

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

directly samples values for the variables in the active part via a scheme that increases the probability
of obtaining a conformation that satisfies loop closure. Once the active part of a loop conformation
has been obtained, an exact 6R inverse kinematics technique is applied to solve for the passive variables under the loop closure constraints. Closed loop conformations are added to the tree if they
are collision-free. The tree is run for a fixed amount of time, and a goal region is defined around
the given goal conformation in order to extract many paths from one execution of the RLG-RRT
algorithm. Conformations in extracted paths are then subjected to short energetic minimizations in
order to elucidate energetically-feasible, large-amplitude motions of the loop under investigation.
Analysis of extracted loop motions in the work of Cortes et al. (2004) reveals that results are
comparable with classic molecular modeling methods while obtained with a performance gain of
several orders of magnitude. The work of Cortes et al. (2004) demonstrates the efficacy of the twostage approach for studying the activity-regulating mobility of the 17-residue long loop 7 in the
amylosucrase enzyme from Neisseria polysaccharea.
Ideas similar to RLG-RRT are employed in the LoopTK (toolkit) algorithm (Yao et al., 2008)
to explore the closed, collision-free conformations of flexible loops ranging in length for 5 to 25
amino acids. The algorithm relies on an interplay of sampling and deformation to obtain loops that
satisfy the kinematic closure constraints and are collision-free. The sampling procedure focuses on
obtaining geometrically-diverse, closed loops. The deformation procedure is based on earlier related
work on loop modeling (Lotan, van den Bedem, Deacon, & Latombe, 2004; van den Bedem, Lotan,
Latombe, & Deacon, 2005). The procedure makes use of the null space technique to explore the
self-motion manifold (the constrained, closure space) around a closed loop to resolve steric clashes
while not violating the closure constraints. LoopTK is shown to efficiently handle long loops up
to 25 amino acids and even generate biologically-interesting, calcium-binding conformations. The
toolkit is available at https://simtk.org/home/looptk.
The time demands of RRT-RLG on problems with hundreds of variables are addressed by Cortes
et al. (2007) by proposing the Manhattan-like RRT (ML-RRT) algorithm to efficiently compute
paths for a small protein-bound ligand to exit the protein active site. ML-RRT borrows ideas from
mechanical disassembly and divides the variables into two groups, active and passive. In particular,
the variables that model the internal and rigid-body motions of the ligand are designated as active,
and the subspace of these active variables is sampled as in the RLG-RRT algorithm. The variables
that model the internal motions of amino acids on the protein receptors active site are designated
as passive, and they are slightly perturbed if they hinder motions of the ligand. This decoupling
proves effective, as it allows for possible ensuing collisions between the ligand and the protein to be
addressed in a domino-like scheme, as illustrated in Figure 8.
ML-RRT has been shown to efficiently model motions of small ligands, side chains, loops, and
backbone (Cortes, Le, Lehl, & Simeon, 2010; Barbe et al., 2011). The work of Cortes et al. (2010)
subjects paths extracted from executions of the ML-RRT algorithm to a randomized path smoothing
post-processing technique. The technique is carried out in the composite space of all the parameters
resulting in simultaneous motions of the ligand and the protein in the final path. The work of Barbe
et al. (2011) subjects loop conformations in paths extracted from ML-RRT to minimization of an
MM energy function so as to reveal critical, physically-realistic intermediate conformations and
bottlenecks along the open-to-closed loop motion of the Burkholderia cepacia lipase lid domain.
Adaptations of robot motion planning frameworks to model loop structures and motions represent only a fraction of diverse methods designed for loop modeling. Interested readers are referred
to a survey in the work of Shehu and Kavraki (2012).
537

fiS HEHU & P LAKU

Figure 8: This figure is reproduced from the work of Al-Bluwi et al. (2012). The left panel illustrates the disassembly planning problem for two articulated objects. The ML-RRT algorithm proposed in the work of Cortes
et al. (2007) problem models the escape of a ligand from a proteins binding site as disassembly problem. The
red H-shaped object on the left image can be considered as the ligand in the right image, and the blue sticks
in the left image can be considered as the flexible side chains on the binding site of the receptor protein in the
right image. The figure is reproduced with permission of the Computer Science Review Journal.

4.1.2 M ODELING P EPTIDE AND P ROTEIN S TRUCTURAL T RANSITIONS AND M APPING
P EPTIDE E NERGY L ANDSCAPES
The RLG-RRT algorithm is modified in the work of Enosh et al. (2008) to model structural transitions in proteins and, in particular, model open and close motions in potassium channels. The main
modification to the RLG-RRT algorithm by Enosh et al. concerns the addition of an energetic test
to the collision-free test performed before deciding whether a generated conformation should be
added to the tree. Several novel analysis techniques are introduced. Clustering is conducted over
many paths obtained from several executions of the algorithm in order to identify common intermediate conformations in the paths connecting given start and goal conformations. Path alignment
is employed to obtain the most energetically-favored path among all those computed. A schematic
of the method proposed by Enosh et al. and a visualization of the most energetically-favored path
obtained on the KscA protein are shown in Figure 9.
Another extension of the RRT-based algorithm in the work of Enosh et al. (2008) is presented
by Raveh et al. (2009); the more efficient PathRover algorithm is proposed for modeling structural
transitions on many proteins. PathRover achieves its computational efficiency in two main ways.
First, its application on many proteins is made possible by restricting the number of dihedral angles
used as variables. Three strategies are used to identify a subset of dihedral angles to define the
variable space: careful inspection of structures, relevant literature, computational tools for detecting
hinge regions like NMA, and comparison of structural changes in alternative (native or homologue)
structures. In particular, the FlexProt alignment algorithm (Shatsky, Nussinov, & Wolfson, 2002)
is used to compare start and goal structures and reveal structurally-different regions. Variables are
manually restricted to dihedral angles in such regions. Second, PathRover limits the exploration to
538

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Figure 9: This figure is reproduced from the work of Enosh et al. (2008). The schematic of the method is
shown in (a). The PathRover algorithm is executed multiple times to extract 100 plausible paths connecting
the given open and closed conformations. The paths are clustered and aligned to reveal a path cluster with
minimal energy barrier. This cluster is visualized for the KscA protein in (b), which shows a putative threephase motion between the close and open conformations. This figure is reproduced with permission of the
Biophysical Journal.

539

fiS HEHU & P LAKU

regions of the space consistent with available wet-laboratory data. Branch termination criteria are
employed to stop the tree from being pulled towards regions that do not improve agreement with
wet-laboratory data. The integration of wet-laboratory data aims to circumvent known inaccuracies
of modern biomolecular energy functions.
The RRT-based algorithms summarized above demonstrate the utility of RRT for modeling
structural transitions in proteins. In particular, the PathRover algorithm utilized several schemes
to reduce the number of variables in order to make the problem of modeling structural transitions
tractable (Raveh et al., 2009). The work of Haspel et al. (2010) continued in this spirit via clever,
reduced representations of large proteins. In contrast, work in the Simeon and Cortes labs credited
with introducing RRT-based algorithms to biomolecular modeling focused instead on techniques to
enhance sampling. The Transition-RRT (T-RRT) algorithm proposed by Jaillet et al. (2008) was
shown particularly effective in this regard.
T-RRT and its bi- and multi-tree variants have been recently proposed to explore and obtain comprehensive maps of energy landscapes of small peptides, such as dialanine and Met-Enkephalin (Jaillet et al., 2011; Devaurs et al., 2015). The main modification to the baseline RRT algorithm in
T-RRT concerns the introduction in the acceptance criterion of a state transition test based on the
Metropolis criterion. New conformations are added to the tree if they pass the transition test (hence
the name, T-RRT). The goal in T-RRT is to steer the tree towards exploration of low-energy regions
in order to map energy minima in the potential energy surface while relaxing the transition test as
needed to cross energy barriers that may trap the exploration to a particular local minimum.
The dynamic modification of the state transition test makes use of a reactive temperature scheme.
In the Metropolis criterion, an effective temperature effectively controls the height of energy barriers that can be crossed by two consecutive conformations. In T-RRT, this temperature is increased
when the number of attempts to pull the tree towards low-energy regions reaches a user-specified
threshold; that is, the number of failures to grow the tree is taken as an indication of the presence of
an energy barrier, and the effective temperature is increased in order to relax the state transition test.
As soon as a successful edge is added to the tree, the temperature is then lowered by a pre-specified
factor in order to resume the overall bias of pulling the tree towards local minima. The effect of
this reactive temperature scheme is that the search is balanced between unexplored regions and lowenergy regions of the variable space. Application of T-RRT in the work of Jaillet et al. (2011) shows
that the algorithm can map the entire known energy landscape of the dialanine peptide when run in
an exploration mode. Another setting, where T-RRT is used to obtain paths that connect discovered
minima, also shows that recovered transitions between known stable states of dialanine are in strong
agreement with transitions known from experiment and affirmed in other simulation studies.
Further work addresses the issue of limited sampling when the goal is to obtain accurate representations of energy landscapes of longer peptides, such as Met-Enkephalin (Devaurs et al., 2015).
The T-RRT algorithm is used by Devaurs et al. only to reveal conformation paths connecting
already-identified meta-stable states. These states are identified by an EA known as Basin Hopping (BH), which has been shown to effectively sample local minima of the energy surfaces of
biomolecules (Olson et al., 2012). In the work of Devaurs et al., BH operates over dihedral angles
and provides a sample-based, discrete representation of the energy surface of a peptide. The local
minima are clustered to reveal wide basins corresponding to meta-stable states.
A variant of the T-RRT algorithm, referred to as Multi-T-RRT, is also proposed by Devaurs et al.
to connect all identified states. The algorithm builds n single trees, each rooted at a conformation
representative of a unique meta-stable state. The algorithm proceeds in iterations, at each iteration
540

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Figure 10: In this figure, reproduced from the work of Devaurs et al. (2015), the graph obtained from a single
run of multi-T-RRT with cycles is projected over key dihedral angles in the top panel. The algorithm is seeded
with four meta-stable states of the Met-Enkephalin peptide, drawn as pink triangles, squares, or circles. The
states capture the unfolded state, the folded state, and two intermediates (shown in the bottom panel). Costs
of various paths are shown in the table on the bottom panel. This figure is reproduced with permission of IEEE
Trans Nano BioScience 2015.

randomly selecting one of the n trees for expansion with a conformation q. The conformation
nearest to q in any of the other n  1 trees is identified, and if it is within an extension step-size,
q merges two trees. Iterations continue until all trees are merged in a graph. The graph is queried
for minimum-cost paths connecting any states of interest. The identified meta-stable states and the
minimum-cost paths connecting these states are found to be comparable to those reported by other
studies that employ more computationally-demanding exploration strategies (Devaurs et al., 2015).
A representative result of the information that can be extracted from this combination of BH and
Multi-T-RRT is shown in Figure 10.
One limitation of readily applying T-RRT and its variants to obtain similar, detailed characterization for proteins rather than short peptides is the dimensionality of the conformation space. In the
work of Jaillet et al. (2011), this space has few dimensions due to the limited number of dihedral
angles in small peptides, such as dialanine and Met-Enkephalin. Detail, while desired and possible
on characterizations of short peptides, needs to be sacrificed in order to model large-scale structural transitions in proteins. NMA-RRT (Kirillova et al., 2008), PDST (Haspel et al., 2010), and
Sprint (Molloy & Shehu, 2013) present three different algorithms that make use of representations
of reduced detail to model large-scale structural transitions in proteins.
Normal Mode Analysis (NMA) is used to obtain larger-scale moves (low-frequency modes revealed by the NMA on a conformation) (Kirillova et al., 2008). These moves are then employed to
generate new conformations in the RRT framework. The NMA-RRT algorithm proposed by Kirillova et al. (2008) essentially conducts the RRT search over a low-dimensional variable space of
541

fiS HEHU & P LAKU

the low-frequency modes. Since the normal modes provided by the NMA on a conformation only
allow to get out of the local minimum represented by a conformation, NMA needs to be repeated
regularly during the RRT search in order to explore the breadth of the conformation space. This can
be computationally demanding, and application of NMA-RRT is limited to extraction of minimumcost paths connecting two conformations of interest rather than a comprehensive map of the energy
landscape and its connectivity in proteins. The work of Kirillova et al. shows that precious information can be extracted regarding structural transitions in proteins, such as adenylate kinase, even
when focusing on motions largely driven by normal modes. A complementary study of minimal
energy paths in adenylate kinase via NMA (Maragakis & Karplus, 2005) shows that the modes are
sufficient to capture the structural transition between the open and closed structures in this protein; all known wet-lab structures are found to be within 3.0Aof these mode-based minimal energy
pathways (Maragakis & Karplus, 2005).
A recent extension of NMA-RRT aims to reduce the computational demands of the algorithm.
The extension employs a further reduced representation of a protein chain based on tripeptides and
employs NMA on conformations of such reduced representations. The reactive temperature scheme
in T-RRT is employed to broaden sampling and capture large-scale motions connecting significantly
different structural states in large proteins of several hundred amino acids (Al-Bluwi et al., 2013).
Employing reduced representations has expanded the applicability of tree-based algorithms for
treating large biomolecules. The PDST algorithm is adapted by Haspel et al. (2010) to model a
transition between two structures of interest in large proteins of more than 200 amino acids. The
assumption is made that secondary structures do not unfold in the sought transition, which is largely
valid when modeling domain motions in proteins. Under this assumption, only backbone dihedral
angles on loops connecting secondary structures are selected as variables.
In the work of Haspel et al. (2010), a bias scheme is used on 10% of the iterations to steer
the tree towards the goal conformation. The bias scheme employs a Euclidean distance between
feature vector representations of conformations in the tree. Given a conformation, its corresponding
feature vector contains Euclidean distances between centers of mass of its secondary structure units.
Conformations are evaluated through a detailed coarse-grained energy function that combines terms
from the energy function used in the work of Shehu et al. (2009) and the Amber ff03 energy function.
If a sampled conformation is evaluated to have energy above a set threshold of 100 kcal/mol more
of the energy of the start conformation, the conformation is subjected to 20 steps of steepest descent
and retained only if its energy decreases below this threshold. This is a rather coarse energetic
constraint, but paths collected from 100 runs of the algorithm not only reach the goal conformation
in less time than methods based on Simulated Annealing, but also reveal credible motions consistent
with experimental data on large, well-characterized proteins such as GroEL (Haspel et al., 2010).
The Sprint algorithm proposed by Molloy and Shehu (2013) uses a complementary approach to
simplifying the search space explored for paths connecting given structures of medium-size proteins.
Sprint addresses the issue of sampling in high-dimensional variable spaces by employing a popular
idea from de novo structure prediction. The fragment replacement technique is used to divide a
protein chain into bundles of consecutive dihedral angles, and values for a bundle or fragment are
sampled from an a-priori constructed database of fragment configurations on known, native protein
structures. The fragment replacement technique is used to expand the tree at every iteration.
While the work of Molloy and Shehu (2013) adapts the EST framework via this expansion
procedure to model structural transitions in small- and medium-size proteins, the Fragment Monte
Carlo Tree Exploration (FeLTr) algorithm proposed by Shehu and Olson (2010) uses related con542

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

cepts to sample the space of near-native protein conformations for the purpose of de novo structure
prediction in small proteins.
In both Sprint and FeLTr, a state-transition test is used to steer the tree towards low-energy conformations over time; that is, a probabilistic, Metropolis-like criterion is used to determine whether
a child conformation should be added to the tree. While in FeLTr (Shehu & Olson, 2010) a fixed
scaling parameter (analogous to a fixed effective temperature) is used in the Metropolis-like criterion, Sprint (Molloy & Shehu, 2013) integrates a reactive temperature scheme in order to allow
the tree to go over high-energy regions when no low-energy routes can be found and thus expand
exploration capability. The reactive temperature scheme in Sprint is slightly different from that in
T-RRT (Jaillet et al., 2008). While in T-RRT the effective temperature is increased or decreased by
fixed amounts, Sprint moves temperature along a proportional cooling scheme often employed in
Simulated Annealing Monte Carlo methods (Shehu et al., 2009). Upon failures to expand the tree,
temperature moves up to the next value in the cooling scheme; upon successes, temperature goes
down to the next value in this scheme.
Both FeLTr and Sprint operationalize on the idea that it is easier to push rather than pull
the tree in conformation space when good moves are available or compiled a priori to generate
energetically-feasible child conformations from selected parent conformations in the tree. While
in the work of Molloy and Shehu (2013) the tree is rooted at a given start conformation and the
goal is to get within a tolerance region of the given goal conformation, in the work of Shehu and
Olson (2010) the tree is rooted at an extended conformation, and the termination criterion is a compromise between a desired number of low-energy conformations and running time.
A central idea in both Sprint and FeLTr is that the growth of the tree can be controlled via a
selection mechanism; at each iteration, a conformation in the tree is selected for expansion. The
selection penalizes the tree from growing towards regions of the conformation space that have been
oversampled, thus resulting in enhanced sampling of the conformation space. Two discretization
layers are employed. In FeLTr, the first layer maps conformations in the tree on a 1d grid whose
cells are energy levels of width 2 kcal/mol. In Sprint, the first layer maps conformations on a 1d
grid based on their lRMSD from the goal conformation. The second layer in both algorithms maps
conformations over a geometric projection. The second layer is a 3d grid, where conformations are
associated with 3 shape-based global coordinates (Shehu & Olson, 2010).
The selection mechanism uses both discretization layers. First, it selects an energy level according to a probability distribution function. The latter is defined over weights associated with energy
levels according to some weighting function. Different weighting functions are analyzed for how
strong a global energetic bias needs to be in order to reproduce the native structure (Molloy et al.,
2013). Once an energy level is selected, cells of the geometric projection grid that belong to conformations in the selected energy level are analyzed. A second weighting function over cells of the grid
biases against selecting a cell that has been selected many times before and/or already has many conformations in it. Once a cell is selected, any conformation in it is selected for expansion uniformly
at random, since conformations in a cell are energetically- and geometrically-indistinguishable.
Extensions of FeLTr have explored both the effect of different weighting functions over the
discretization layers and the employment of different projection coordinates (Molloy et al., 2013;
Olson et al., 2012). Different coarse-grained energy functions that are considered state-of-the-art
in de novo structure prediction, including the Rosetta suite of energy functions, are employed in
the framework and directly compared for how they steer the search towards near-native conformations (Molloy et al., 2013). Molloy and Shehu (2013) also investigate the impact of different
543

fiS HEHU & P LAKU

projection schemes and selection mechanisms on both the diversity and energetic profiles of Sprintextracted paths in the context of computing structural transitions.
Applications of Sprint on different start and goal structure pairs of the calmodulin and adenylate
kinase proteins show that the algorithm is able to find paths that reach the goal conformation (Molloy
& Shehu, 2013). Soft global biasing schemes are found to provide the right compromise between
tree depth (that is, lower energies) and diversity of paths (that is, geometrically-diverse conformations). Detailed energetic and structural analysis on computed paths for two hallmark proteins,
such as calmodulin and adenylate kinase, reveals that Sprint yields accurate characterizations of
structural transitions in these proteins. Energetic profiles of extracted paths indicate the presence
of high-energy regions that need to be crossed on specific transitions in calmodulin, in agreement
with wet-laboratory characterizations. Analysis on adenylate kinase shows that known intermediate
structures of this protein are present in the the conformation paths computed by Sprint (Molloy &
Shehu, 2013).
4.2 Roadmap-Based Methods for Modeling Equilibrium Biomolecular Structure and
Dynamics
Roadmap-based methods have been employed to model protein-ligand binding (Singh et al., 1999),
protein and RNA folding and unfolding (Song & Amato, 2004; Chiang et al., 2007; Chiang, Hsu, &
C., 2010), and protein structural transitions (Molloy & Shehu, 2016; Maximova et al., 2015).
4.2.1 M ODELING P ROTEIN -L IGAND B INDING
The adaptation of the roadmap-based motion planning framework for protein-ligand binding by
Singh et al. (1999) is the first occurrence of robotics-inspired treatments of biomolecular structure
and dynamics. The adaptation was simplistic but provided key design issues that were then replicated and extended by many robotics researchers. One of the key simplifications is that the protein
receptor is kept rigid, and the only variables of interest are those allowing to model rigid-body
motions of the ligand around the receptor and internal motions of the ligand. Small ligands are
considered, so that the 6 + p variables to allow modeling of such motions do not go above a dozen.
Sampling proceeds uniformly at random over the 6 + p variables, but ligand configurations added
to the roadmap pass a geometric and energetic criterion. The geometric criterion ensures that ligand
configurations are within some predefined distance of the center of mass of the receptor.
The energetic criterion is probabilistic: two dynamically-updated thresholds, Emin and Emax
values, corresponding to minimum and maximum energy values over sampled configurations, are
recorded. Ligand configurations with energy higher than Emax are rejected. Other configurations
are retained with probability (Emax  E(q))/(Emax  Emin ). The energy function incorporates
both terms to evaluate the internal energy of a ligand configuration as well as terms evaluating
interactions between a ligand configuration and the rigid protein receptor.
Retained ligand configurations are embedded in a nearest-neighbor graph, using lRMSD to measure the distance between two ligand configurations and a user-set parameter, k, for the number of
nearest neighbors. A simple local planner interpolating over all p + 6 variables of two neighboring
configurations is used to estimate the feasibility of q  q 0 and q 0  q edges by generating consecutive configurations. Consecutive configurations qi are generated by the linear interpolation planner
to connect q and q 0 until the distance between two consecutive configurations in the generated series is no higher than 1A. The q  q 0 and q 0  q edges are added to the roadmap only if all qi
544

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

configurations have energies below Emax . Weights are added to retained edges as follows:
0

w(q  q ) = 

s1
X

log[P (qi  qi+1 )]

i=0

where
P (qi  qi+1 ) =

e(Ei+1 Ei )/(KB T )
(e(Ei+1 Ei )/(KB T ) + e(Ei1 Ei )/(KB T ) )

In the above equations, qi1 , qi , qi+1 are three consecutive configurations with corresponding
energies Ei1 , Ei , Ei+1 , KB is the Boltzmann constant, and T is the effective temperature. The
weight of a path qstart
qgoal , which connects a start configuration to a goal configuration, is
then the sum of the weights of the edges in it. The weight of a path initiated at an unbound configuration and terminating in a bound configuration estimates the association rate (the cost of the
ligand approaching and binding to the protein receptor). The weight of the reverse path estimates
the disassociation rate (the cost of the ligand leaving the binding site and diffusing in space).
The resulting roadmap represents a distribution of energetically-credible paths of the ligand approaching and then binding the receptor. In the work of Singh et al. (1999), the bound configuration
of the ligand in the p+6 variable space is presumed not to be known, and RMSD-based clustering of
sampled lowest-energy ligand configurations is employed to reveal a few likely bound candidates.
Analysis reveals that the true bound configuration is indeed present in the top-populated clusters;
however, many false positives are reported, as well. Weights of paths terminating in and initiated
from the lowest-energy ligand configurations are analyzed in order to determine what other characteristics can be used to discriminate between true and false positives. Paths terminating at the true,
bound configurations are found to have high association rates; the reverse paths are found to have
high disassociation rates. This important result elucidates that effective binders are not only those
that allow the ligand to reach the lowest interaction energy but also trap it at the binding site via
high-energy barriers.
4.2.2 M ODELING P ROTEIN AND RNA (U N )F OLDING
Singh et al. (1999) provided a much needed template and has served as the foundation for many
robotics-inspired treatments of biomolecules. In particular, a suite of roadmap-based algorithms
and extensions were designed in the Amato lab to model unfolding of small proteins. A review
of roadmap-based methods to study molecular motions in the Amato lab is available in the work
of Tapia et al. (2010), whereas a review of roadmap-based methods for the specific protein folding
problem is presented in the work of Moll, Schwartz, and Kavraki (2008). A seminal contribution in
this category is the Probabilistic Conformation Roadmap (PCR) algorithm (Apaydin et al., 2001),
which builds upon the template presented by Singh et al. (1999) to study protein folding.
PCR addresses a complex application domain, as the number of variables needed to model the
intrinsic flexibility of protein chains can easily reach 100 or more. In PCR and other extensions that
followed, most notably in the Amato lab, the variables employed are all or a subset of the backbone
dihedral angles of a protein chain (Amato et al., 2003; Song & Amato, 2004; Tang et al., 2005;
Thomas et al., 2005, 2007; Tapia et al., 2007; Tang et al., 2008; Tapia et al., 2010). In such variable
spaces, uniform random sampling is ineffective and likely to result in conformations with severe
internal collisions. For this reason, work in the Amato lab on PCR-based algorithms has gradually
545

fiS HEHU & P LAKU

shifted to sampling strategies based on incremental perturbations of a given native/folded conformation until memory of the folded conformation has been lost. Specifically, backbone dihedral angles
of the folded conformation are perturbed by small amounts by use of a Gaussian distribution until
a minimum number of conformations is obtained for each category (0 to 100% in 10% increments)
of the percentage of native contacts. The lower the number of native contacts is in a conformation,
the more likely that conformation is to belong to the unfolded state. While the acceptance criterion
for sampled conformations is as in the work of Singh et al. (1999), the energy function is different,
as it measures the internal energy of a protein chain. The function contains terms favoring hydrogen
bonds, disulfide bonds, and hydrophobic interactions.
The sampled conformations that pass the energetic/acceptance criterion are embedded in a
nearest-neighbor graph, with the number of nearest neighbor conformation k specified by the user.
In contrast to the original PCR algorithm, directed (u, v) edges in the graph are weighted based on
E(u)E(v)

the Boltzmann-related Metropolis criterionas in: P(u,v) = e KB T , where E(.) is the energy of a
conformation, KB is the Boltzmann constant, and T is an a-priori set temperature determining the
height of energy barriers crossed by an edge. In this early formulation of edge weights, no reactive
temperature schemes are employed as in the later tree- and roadmap-based algorithms for structural
transitions. Instead, T is a user-controlled parameter that determines to a great extent the ability of
the algorithm to navigate the underlying energy surface.
In the works of Song and Amato (2004) and Thomas et al. (2005), all N best paths that end at the
folded conformation and start at conformations with 0 native contacts are extracted and analyzed.
Analysis of such paths has shown that, despite several design decisions intended to simplify the
protein folding problem, PCR-based algorithms can predict the order of secondary structure formation. Agreement with wet-laboratory data has validated the general usage of PCR-based algorithms
to provide a coarse-grained treatment of folding and unfolding pathways for protein chains. Other
works by Amato and collaborators also show the applicability of PCR-based algorithms to study
RNA folding and unfolding (Tapia et al., 2007; Tang et al., 2008; Tapia et al., 2010) .
The sampling strategy of incremental perturbations is effective on protein chains of no more than
60 amino acids (Song & Amato, 2004) but scales poorly on longer chains (Thomas et al., 2005).
Ensuing work improves sampling for protein chains up to 110 amino acids by further reducing the
number of variables modeled to represent conformations (Thomas et al., 2007). Specifically, rigidity analysis is employed to detect least-constrained regions in a given structure. The dihedral angles
belonging to such regions are selected more often for perturbation in the sampling stage. This modification is shown effective in revealing subtle folding differences between protein G and two of its
sequence variants. In particular, the modification is also shown to be promising for capturing other
dynamic events in proteins beyond folding to study large-scale conformational changes involved
in structural transitions of the calmodulin protein. Related ideas have been employed by other researchers to compute temperature-dependent optimal folding paths in peptides and proteins (Yang
et al., 2007; Li et al., 2008). The MaxFlux-PRM algorithm proposed by Yang et al. (2007) to
study structural transitions in the dialanine peptide and folding of a -hairpin is then shown by Li
et al. (2008) to be capable of predicting folding pathways of the engrailed homeodomain protein.
Further work in the Amato lab has focused on exploiting the conformation roadmap to extract
quantities summarizing folding kinetics in protein and RNA molecules. Tapia et al. (2007) introduce
two new analysis techniques, the Map-based Master Equation (MME) and Map-based MC (MMC)
technique. This work shows that treating the roadmap as a map of the folding landscape can be
546

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

exploited to estimate kinetic metrics that are typically only extracted from MD simulation studies.
Metropolis MC simulations can be conducted over the roadmap, moving between roadmap vertices
observing the edge probabilities in the roadmap. Different statistics can then be calculated over the
MMC walks, including folding rates and population kinetics. Tang et al. (2008) show that statistics
summarizing RNA folding predict well the same relative gene expression rate for wild-type MS2
phage RNA and three of its mutants, in good agreement with wet-laboratory data.
4.2.3 S TOCHASTIC ROADMAP S IMULATION AND M ARKOV S TATE M ODELS FOR M ODELING
P ROTEIN AND RNA (U N ) FOLDING AND P ROTEIN S TRUCTURAL T RANSITIONS
The idea that reliable statistics can be extracted from molecular conformation roadmaps was presented earlier by Latombe and colleagues (Apaydin et al., 2003). The stochastic roadmap simulation (SRS) framework is formalized in relation to a key analogy between a roadmap and a Markov
state model (MSM); the concept of a stochastic roadmap with probabilistic edges was presented
earlier (Song & Amato, 2000), but the analogy with an MSM went missing till the 2003 formalization by Latombe and colleagues (Apaydin et al., 2003). The latter laid bare the analogies between a stochastic roadmap and what would later be referred to as a point-based MSM. In such an
MSM, states of the MSM are the single-conformation vertices of the stochastic roadmap, and the
probabilistically-weighted edges connecting vertices in the roadmap are the state-to-state transitions
in the MSM. The analogy brought to focus that a stochastic roadmap better encodes the stochastic nature of biomolecular motions, and the analogy with an MSM could even be used to extract
interesting summary statistics regarding physics-driven stochastic processes.
In addition to recognizing that biased random walks can be carried out over the roadmap and
employed to extract statistics of interest (Tapia et al., 2007), the SRS-MSM analogy highlights that
effective, algebra-based techniques from (Markov chain) transition state theory can be employed to
extract average statistics without launching a single simulation (or random walk over the roadmap).
Folding rates, pfold values,  values, and other estimates of kinetics, such as transition rates, can be
obtained without needing to perform many random walks but by in-order propagation of transition
probabilities. The analogy between a stochastic roadmap and a point-based MSM is shown to result
in correctly-predicted pfold values on small proteins modeled at the secondary structure level with
612 variables (Apaydin, Brutlag, Hsu, & Latombe, 2002; Apaydin et al., 2003). Further work
demonstrated that the transition state ensemble (the set of conformations with pfold =0.5), folding
rates, and  values could be predicted on 16 different proteins but at a fraction of the computational
time that would be needed by a framework launching numerous MC simulations (Chiang, Apaydin,
Brutlag, Hsu, & Latombe, 2006; Chiang et al., 2007) .
While the SRS-MSM analogy permits interesting mathematics, practical issues such as how to
ensure that the transition matrix is not prohibitive in size to allow solving of linear algebra equations
have to be addressed on a case-by-case basis. The formalization presented by Apaydin et al. (2003)
did not discuss practical design decisions such as how to group conformations into states and how
to estimate transition probabilities between two sub-ensembles, but rather on the mathematics that
would be possible by the analogy between a stochastic roadmap and an MSM. Analogies with cellbased MSMs, where states are homogeneous sub-ensembles of conformations rather than single
conformations need addressing practical issues regarding how to organize conformations into states
and how to associate transition probabilities between states.
547

fiS HEHU & P LAKU

Since the seminal work of Apaydin et al. (2003), analogies between SRS and cell-based MSMs
have been largely limited, partly due to the lack of clear objectives in design decisions that are
general in their ability to transform a roadmap into an MSM of manageable size. For instance, the
fundamental assumption was that if conformations were obtained via an MD simulation at some
temperature T , then the probability of an edge representing a transition from a vertex u to a vertex v
could be measured via the Boltzmann-related Metropolis criterion e(E(v)E(u))/(KB T ) . This realization allowed Apaydin, Latombe, and colleagues to see the clear connection between the stochastic (probabilistic) roadmap of structures and a point-based MSM, with vertices seen as states of the
MSM and edges between vertices in the roadmap as transitions between states in the MSM. However, practical considerations as to how to convert these single conformation vertex probabilities
into state-state transition probabilities were not discussed.
The issue of how to associate probabilities in the first place to conformations sampled via other
non-MD algorithms was also not discussed. Two groups of researchers have started operationalizing
on the seminal ideas presented by Apaydin et al. (2003). Work by Latombe and colleagues has focused either on point-based MSMs or on summarizing and uncovering the MD-simulated dynamics
of synthetic and small peptides via cell-based MSMs (Chiang et al., 2007, 2010). Complementary work in the Shehu lab has focused on non-MD approaches and extracting average statistics
to model and compare transitions in healthy and aberrant forms of disease-participating, small- to
medium-size proteins (Molloy et al., 2016).
Chiang et al. (2010) offer a novel representation of states not as individual conformations (Apaydin et al., 2003; Singhal, Snow, & Pande, 2004) or even disjoint regions of conformation space
(Ozkan, Dill, & Bahar, 2002; Chodera, Singhal, Pande, Dill, & Swope, 2007) (as in cell-based
MSMs) but instead as overlapping probabilistic distributions over the conformation space. This
distribution relies on the key recognition that a single conformation does not contain enough information to be uniquely mapped to a state and leads to the presence of hidden states in what is
referred to as a Markov Dynamics Model (MDM) rather than an MSM (Chiang et al., 2010). In
the MDM, emission probabilities of hidden states measure the probability with which a conformation belongs to a state. Both transition and emission probabilities are estimated over trajectories of
conformations obtained from many MD simulation trajectories. A principled criterion based on the
ability of a model to predict long-timescale kinetics allows discriminating between possible MDMs
and selecting an optimal one. The MDM embedded over conformations obtained from MD trajectories simulating folding of the fast-folding villin headpiece subdomain (HP-35 NleNle) is shown
in Figure 11. The MDM presents a highly-interpretable discrete kinetic model of the folding of this
small sub-domain, built over more than 400 MD trajectories, each 1s long. Figure 11 shows that
a few states, 7, 12, 13, 15 and 18, are the most frequently-visited states that significantly influence
the long-term dynamics.
Molloy et al. (2016) present strategies to embed conformations sampled via a non-MD method
in a cell-based MSM. The ability to formulate a cell-based MSM relies on dense sampling of the
conformation space of interest. The latter provides significantly challenging to do in the MD setting
or even in the robotics-inspired setting. Instead, complementary work in the Shehu lab on EAs is
used to obtain a rich ensemble of local minima conformations in healthy and variant sequences of
a given protein (Clausen & Shehu, 2015; Clausen et al., 2015). These conformations are organized
into states via a simple lRMSD-based clustering scheme. A nearest-neighbor graph is then imposed
over the states, but an additional lRMSD constraint is imposed so as to connect only nearby states
via an edge. The assumption is made that transitions are possible between nearby states, and prob548

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Figure 11: In this figure, reproduced from the work of Chiang et al. (2010), (a) shows the MSM connecting
twenty identified states of the villin headpiece peptide. The size of each node in the MSM is proportional to
the probability of the corresponding state in the stationary distribution. The width of each edge is proportional
to the transition probability between corresponding states. States with probability < 0.01 in the stationary
distribution, self-transitions, and edges with transition probability < 0.002 are not drawn to avoid cluttering.
The initial conformations are most likely to belong to state 12, and the native conformation is most likely
to belong to state 15. (b) Representative conformations are shown from states 7, 12, 13, 15, and 18. The
residues forming the important helix 1 in the villin headpiece peptide are drawn in red. (c) The most likely state
transition sequences from states 12 to 15 are shown here. This figure is reproduced under the Bioinformatics
Journals terms of the Creative Commons Attribution Non-Commercial License.

abilities of such transitions can be estimated via a Boltzmann-like probability. The latter makes use
of the concept of the energy of a state. Several schemes are employed to determine the energy of a
state, ranging from the minimum to the average value over energies of conformations grouped in a
state.
549

fiS HEHU & P LAKU

Figure 12: This figure is reproduced from the work of Molloy et al. (2016). Panel (a) shows two wet-laboratory
structures representative of the ON and OFF structural states of the H-Ras catalytic domain. H-Ras switches
between these two states to regulate its biological activity in the cell. The loop regions where the change is
localized are shown in red and blue. The reactant (GTP) and product (GDP) are also drawn where they bind
H-Ras. Panel (b) shows two-dimensional projections of the probed energy surface of the H-Ras wildtype (WT)
and the oncogenic Q61L variant. Sampled conformations are projected on the top two principal components
(PC) obtained via Principal Component Analysis of sampled conformations. The color-coding follows the
Amber ff12SB internal energy values of the all-atom structure corresponding to each sampled conformation.
The ON  OFF minimum-cost paths obtained by querying the stochastic roadmap constructed over sampled
conformations are shown, as well. The costs of these paths are shown in the table in panel (c). The average
number of edges over all possible ON  OFF paths are obtained by treating the roadmap as an MSM. The
actual energy profiles of the minimum-cost paths obtained for the WT and Q61L variant are shown in panel
(d). This figure is reproduced with permission of Robotica 2016.

The result of this process is a stochastic roadmap that can be used to answer lowest-cost path
queries, as traditionally the case in roadmap-based methods, as well as yield average statistics, such
as the average number of edges in a transition, via the analogy of the stochastic roadmap with an
MSM. A path smoothing algorithm based on the conjugate peak refinement technique (Fischer &
Karplus, 1992) provides more detail with state-state paths and improves their energetic profile. The
average statistics, while not direct measurements of transition rates due to the lack of timescale information from non-MD methods, allow conducting comparisons between wildtype (WT) and variant (mutated) sequences of proteins of interest. In the work of Molloy et al. (2016), such statistics
are employed to obtain a structural explanation for the role of specific mutations on the biological
activity in two proteins implicated in human disorders. Figure 12 showcases some representative
results from application of the SRS-based approach in the work of Molloy et al. to the WT and
Q61L variant of the H-Ras protein. Ras sequence mutations have been implicated in various human
cancers (Karnoub & Weinberg, 2008).
550

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Comparison of the energy landscapes, costs of the minimum-cost paths, energy profiles of these
paths, and the expected number in edges over all paths in each H-Ras sequence in Figure 12 provides a structural explanation for the impact of the Q61L mutation on the biological activity of this
enzyme. The mutation introduces an energy barrier between the ON and OFF states, and this barrier
increases both the cost of the minimum cost path and the number of expected edges in transition
paths. Taken together, these results suggest that the Q61L mutation, while preserving the stability
of the ON and OFF states, cannot form the transition state mimic, in agreement with wet-laboratory
studies (Gremer, Gilsbach, Ahmadian, & Wittinghofer, 2008; Gibbs, Schaber, Allard, Sigal, & Scolnick, 1988).
4.2.4 A DDRESSING L IMITED S AMPLING IN ROADMAP -BASED M ETHODS FOR M ODELING
P ROTEIN S TRUCTURAL T RANSITIONS
Sampling remains a key issue in adaptations of roadmap-based methods for biomolecular modeling.
While generally the focus of robotics-inspired methods has been on demonstrating their ability to
reproduce experimental knowledge qualitatively or even quantitatively on specific systems under
investigation, their general applicability has been largely sacrificed. For instance, the roadmapbased methods applied to model the discrete secondary structure formation events in protein folding
and unfolding are largely not applicable to model folding or other transition events in proteins
more than 150 amino acids where the states sought to the bridged by the transition may be farther
than 10A away from each-other. Strategies to reduce the number of variables so as to control
the dimensionality of the variable space have important ramifications. For instance, rigidity-based
techniques base their conclusions of where the most flexible regions are on analysis of a specific
structure. NMA techniques suffer from a similar issue, and regular application of NMA on sampled
conformations adds to the computational time demands of an algorithm. Other techniques that make
assumptions on which regions do or do not participate in a particular transition event rule out the
possibility of potentially complex, cooperative events. Others that bundle variables together and
obtain values for them from pre-compiled databases make similar assumptions on what types of
structural changes facilitate a transition.
Sampling will remain a challenge, but two complementary directions are being explored. The
first direction values broad applicability over specific improvements. Molloy and Shehu (2016)
propose that the community needs a benchmark testing dataset and a baseline approach against
which specific improvements and extensions can be evaluated. In particular, this work ignores
system-specific insights into which variable and which sampling schemes can be more effective
over others but instead compiles a broad set of variables and sampling/perturbation operators that
can be selected via a probabilistic scheme. Different schemes can be employed at different stages
of a roadmap-based method based on the distance of the conformations that need to be connected
and the size of the biomolecule under investigation. A general baseline implementation shows
comparable performance to system-specific methods and promises that further improvements can
guarantee a baseline performance over a broad set of biomolecules and problem instances. Related
ideas building on the concept of a move selector are presented by Gipson et al. (2013).
The second direction sacrifices broad applicability in the interest of improving the predictive
capability of roadmap-based methods to the point that reliable hypotheses can be formulated to further guide wet-laboratory experimentation. Maximova et al. (2015) recognize that roadmap-based
methods do not have to operate in a de novo setting but instead exploit the rich set of wet-laboratory
551

fiS HEHU & P LAKU

Figure 13: This figure is reproduced from the work of Maximova et al. (2015). The left panel shows a
schematic that summarizes all paths within a small energetic threshold of the minimum-cost path connecting
structure pairs of interest in calmodulin. Analysis of these paths reveals that known, wet-laboratory structures
mediate transitions of interest. The PDB ids of these mediating structures are shown along each of the paths.
The right panel shows successive structures in the minimum-cost paths found for transitions of calmodulin
from structure with PDB id 1CLL to that with PDB id 2F3Y and then from structure with PDB id 1CLL to that
with PDB id 1NWD. Numbers indicate model number within an NMR entry. This figure is reproduced with
permission of IEEE Society 2015.

structures to determine the variable space of interest. In particular, the SoPRIM algorithm proposed
by Maximova et al. subjects wet-laboratory structures of different sequences of a protein to a statistical multivariate analysis to determine variables that represent collective motions of atoms. Sampling
focuses on this space of variables and a multiscaling technique converts samples to all-atom structures that are local minima of the Amber ff14SB energy function. The samples are embedded in a
roadmap, and distance constraints ensure that edges are only placed between neighboring samples.
Edges are weighted based on the concept of minimum cost, recording only energetic increases.
In an additional contrast to existing roadmap-based treatments, the work of Maximova et al.
(2015) yields not only the minimum-cost path connecting a given start to a given goal structure, but
allows extracting additional paths with similar costs. The concept of tours is employed, based on
related work in robotics. The tours allow to investigate specific hypotheses regarding the participation of known meta-stable structures in a transition. A set of such structures can be specified, and
all minimum-cost tours that consider all subsets and orders of such structures are reported. Analysis
of tours with costs no higher than a specific threshold over the minimum-cost path reveals precious
information regarding important function-regulation transitions in several proteins, including Ras
and calmodulin. A summary result is shown in Figure 13.
Figure 13 extracts several energetically-credible paths representing the various, equiprobable
routes of transitions of calmodulin from its open, unbound state (represented by structure with PDB
id 1CLL) to two different, closed peptide and protein-bound states (represented by structures with
PDB id 2F3Y and 1NWD). The schematic summary of these paths in Figure 13 highlights that
these open-to-closed transitions in calmodulin may not make use of the calcium-bound structure
(PDB id 1CFD). Indeed, paths that go through this structure have higher energetic cost. A different
intermediate structure emerges from the analysis of paths. This structure (under PDB id 2K0E)
also binds calcium but is slightly different for that under PDB id 1CFD. The succession of structures shown in 13 makes clear that the domain collapse, re-arrangement, and partial unfolding of
the helix that links the N- and C-terminal domains in calmodulin are gradual, as captured in various structures in the NMR ensemble with PDB id 2K0E. This result is in good agreement with
552

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

the wet-laboratory study in the work of Gsponer, Christodoulou, Cavalli, Bui, Richter, Dobson,
and Vendruscolo (2008), which, in addition to contributing the NMR entry under PDB id 2K0E to
the Protein Data Bank, also concludes that correlated motions within the 2K0E Ca(2+)-CaM state
direct the structural fluctuations toward complex-like substates (Gsponer et al., 2008). While the
wet-laboratory study by Gsponer et al. (2008) was restricted to MLCK binding of CaM, the results
obtained by the SoPRIM algorithm (Maximova et al., 2015) suggest that the same mechanism observed by Gsponer et al. (2008) prepares CaM for binding to other peptides (the C-terminal Domain
of Petunia Glutamate Decarboxylase in 1NWD and the IQ domain in 2F3Y). The work of Maximova et al. (2015) points to a general mechanism for the apo-to-closed/complexed dynamics of
calmodulin, where correlated motions within the calcium-bound state direct the fluctuations and
population shift of this protein to its peptide-bound states.

5. Outstanding Challenges and Directions of Research
Robotics-inspired methods are becoming more powerful and diverse in their algorithmic strategies
and the problems they address in biomolecular modeling. While this survey has focused on treeand roadmap-based methods for modeling protein-ligand binding, protein de novo structure prediction, protein and RNA folding and unfolding, structural transitions in peptides and proteins, and
energy landscape mapping, other methods are building on related ideas to efficiently map ligand
migration channel networks in dynamic proteins (Lin & Song, 2011; Na & Song, 2015) or even
model antibody aggregation processes (Hoard, Jacobson, Manavi, & Tapia, 2016). While we have
attempted to provide a broad and deep survey of robotics-inspired methods for biomolecular modeling, an exhaustive survey is not possible. This particular sub-domain at the interface of Robotics and
computational structural biology is rapidly progressing, as demonstrated by the increasing number
of adaptations and applications showcased in this survey over earlier, related reviews of roboticsinspired methods (Al-Bluwi et al., 2012; Gipson, Hsu, Kavraki, & Latombe, 2012). As this survey
showcases, several algorithmic challenges remain. Below we provide a partial list of these challenges and prospects for future research.
5.1 Problem-Specific versus General Treatments
There is a pressing need in the community for benchmarks. While work has been largely driven by
specific biological systems and problems of interest, such data-driven research has often resulted
in specific design decisions that are not easily transferable to other systems and other problems.
For instance, key decisions on how to reduce dimensionality of the variable space and design compliant sampling strategies and perturbation operators on a specific problem instance may not be
applicable to another problem. A realization of the need for baseline, general treatments and benchmarks is leading researchers towards non-specific treatments to establish benchmarks and baseline
performance. Better sharing of problem instances, metrics, and algorithms with known baseline
performance will also be key to allow researchers to build on existing work and expedite progress.
5.2 Sampling
There is a growing realization that sampling will remain a central issue, despite clever reduced representations and sampling strategies. While the community of researchers adapting robot motion planning treatments for biomolecular modeling has been successful at integrating important knowledge
553

fiS HEHU & P LAKU

about biomolecules in model selection, sampling strategies, and energetic evaluations, this community has largely remained isolated from complementary work in AI on stochastic optimization of
continuous, non-linear variable spaces. In particular, there is a growing body of work in the evolutionary computation community on optimization of complex fitness landscapes. Some ideas from
this community have successfully been employed in de novo structure prediction (Shehu, 2013) and
mapping of protein energy landscapes (Clausen & Shehu, 2015; Clausen et al., 2015; Sapin, Carr,
De Jong, & Shehu, 2016). These ideas are also beginning to be incorporated in robotics-inspired
treatments of biomolecular dynamics (Molloy et al., 2016). Better awareness and integration of
effective practices of other communities dealing with similarly challenging high-dimensional problems is likely to address issues in sampling and lead to more powerful robotics-inspired treatments.
In this context, we see great opportunity for AI researchers to make contributions in sampling-based
treatments of biomolecular dynamics.
5.3 Decorrelations of Paths
In particular, applications of tree- and roadmap-based methods for modeling structural transitions
of biomolecules, path correlation is an issue. Path correlations can potentially skew any statistics
of interest and even yield to incorrect conclusions about a structural transition. The culprit in treebased methods is the bias that is applied to steer the conformation tree to the goal conformation.
Even multiple executions of a tree-based method are likely to result in similar paths. To some
extent, this source of path correlations can be addressed. For instance, Molloy and Shehu (2013)
makes use of an additional projection layer to steer the tree towards under-sampled regions of the
conformation space. This is shown to improve path diversity. Yet another culprit that is shared by
tree- and roadmap-based methods is density of sampling. For instance, undersampling of specific
regions may lead to the conclusion that the region is not energetically favorable for the biomolecule
at hand. Further investigation is needed to quantify and reduce path correlations in robotics-inspired
methods. This direction is also ripe for cross-fertilization of ideas from different sub-communities
in AI.
5.4 Injection of Dynamics
A common criticism of robotics-inspired methods is that they are essentially geometric treatments
of biomolecules. While to some extent geometric treatments are accepted in modeling biomolecular structure, they are seen as inadequate in modeling biomolecular dynamics. Modeling dynamics
is largely seen as exclusive to MD simulation frameworks. In a somewhat colloquial and simplistic characterization of robotics-inspired methods, biomolecular dynamics has nothing to do with
robot motion planning. This characterization can be overcome by pointing out that the superficial
analogies are only used to inspire robotics researchers, but the deeper analogies that are exploited
and shown to have an impact are those on selection of models, variables, fast forward and inverse
kinematics, and effective sampling strategies. It is worth noting that the latter are not exclusively
the domain of robotics-inspired researchers. On the contrary, issues of effective variable selection
or representation, variation operators, employment of such operators in sampling strategies, and
others are of broad interest to AI researchers working on optimization problems as part of modeling
abstract, mechanical, or biological systems.
At various places, this survey has highlighted that robotics-inspired methods are capable not
only of reproducing wet-laboratory knowledge and data but also of providing novel findings to direct
554

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

further experimentation in wet laboratories. Still, a valid criticism of robotics-inspired methods
is that edges in trees or roadmaps do not provide a detailed view of the diffusion between the
two conformations they connect. While the survey points out in Section 4 several challenges with
integrating MD trajectories in robotics-inspired framework, it is important that the community think
of ways to do so effectively. Injection of ideas from the AI community at large may prove beneficial
here. A growing body of work in computational biophysics is pointing to effective frameworks of
biomolecular dynamics that integrate thousands or more short MD trajectories in MSMs to capture
biomolecular dynamics. Cross-fertiziliation of ideas from the AI and biophysics communities is
likely to prove fruitful in explicitly integrating MD in robotics-inspired methods.
5.5 Beyond Path Computations: Roadmaps and MSMs
As this survey highlights in Section 4, MSMs have become very popular in computational biophysics literature to organize and extract statistics from many, independent MD simulations of
biomolecular folding or other structural transitions (Jayachandran, Vishal, & Pande, 2006; Chodera
et al., 2007; Noe & Fischer, 2008; Prinz, Keller, & Noe, 2011a; Noe, Doose, Daidone, Lollmann,
Sauer, Chodera, & Smith, 2011; Perez-Hernandez, Paul, Giorgino, De Fabritiis, & Noe, 2013; Weber, Jack, & Pande, 2013; Deng, Dai, & Levy, 2013; Chodera & Noe, 2014; Malmstrom, Lee,
Van Wart, & Amaro, 2014; Song & Zhuang, 2014; Shukla, Hernandez, Weber, & Pande, 2015).
Several survey articles are dedicated to reviewing MSM-based treatments of biomolecular dynamics (Pande, Beachamp, & Bowman, 2010; Gipson et al., 2012; Maximova et al., 2016) review MSMbased treatments of biomolecular dynamics. Works by Chiang et al. (2006), Chiang et al. (2007),
Chiang et al. (2010), and Molloy et al. (2016) provide an important first step in the integration of
MSMs in the analysis of conformation spaces probed via robotics-inspired algorithms. While Chiang et al. (2010) and Molloy et al. (2016) address some of the issues on to convert roadmaps into
MSMs, many others remain, including definition of structural states, possible undersampling of specific states, feedback mechanisms to address undersampling, and rigorous calculation of transition
probabilities. Some of these issues are also contended with in the computational biophysics community, and initial treatments have emerged (Singhal et al., 2004; Singhal & Pande, 2005; Prinz, Wu,
Sarich, Keller, Senne, Held, Chodera, Schutte, & Noe, 2011b; Malmstrom et al., 2014; Da, Sheong,
Silva, & Huang, 2014). We see a great opportunity here for AI researchers, particularly those with
expertise in machine learning, to coordinate efforts with computational biophysicists. Such efforts
will undoubtedly lead to richer and more powerful computational treatments of biomolecular dynamics.
5.6 Cross-Fertilization of Ideas
As this survey shows, work in modeling biomolecular structure and dynamics is highly interdisciplinary, and great progress is achieved when ideas from different communities are combined and
integrated in computational treatments. There is a rich set of scientific questions that can be formulated to understand the role of biomolecular structure and dynamics in human biology and health.
These questions often result in exceptionally challenging computational problems that necessitate
sophisticated algorithmic treatments. Treatments that add to the current knowledge of biomolecular
systems in chemistry, physics, and biophysics are likely to advance not only our modeling capabilities but also make important, general contributions to AI research.
555

fiS HEHU & P LAKU

Acknowledgements
Funding for this work is provided in part by the National Science Foundation. The work A. Shehu is
supported by NSF-CCF1421001, NSF-ACI1440581, and NSF-IIS1144106. The work of E. Plaku
is supported by NSF-ACI1440581, NSF-IIS1449505, and NSF-IIS1548406.

References
Abayagan, R., Totrov, M., & Kuznetsov, D. (1994). ICM - a new method for protein modeling and
design: applications to docking and structure prediction from the distorted native conformation. J Comput Chem, 15(5), 488506.
Aden, J., & Wolf-Watz, M. (2007). NMR identification of transient complexes critical to adenylate
kinase catalysis. J Amer Chem Soc, 129(45), 14003 14012.
Al-Bluwi, I., Simeon, T., & Cortes, J. (2012). Motion planning algorithms for molecular simulations: A survey. Comput Sci Rev, 6(4), 125143.
Al-Bluwi, I., Vaisset, M., Simeon, T., & Cortes, J. (2013). Modeling protein conformational transitions by a combination of coarse-grained normal mode analysis and robotics-inspired methods. BMC Struct Biol, 13(S2), Suppl 1.
Amato, N. M., Bayazit, B., Dale, L., Jones, C., & Vallejo, D. (1998). OBPRM: An obstacle-based
PRM for 3D workspaces. In Workshop Algorithm Found Robot, Vol. 86 of Springer Tracts in
Advanced Robotics, pp. 156168. Springer.
Amato, N. M., Dill, K. A., & Song, G. (2003). Using motion planning to map protein folding
landscapes and analyze folding kinetics of known native structures. J Comput Biol, 10(3-4),
239255.
Anfinsen, C. B. (1973). Principles that govern the folding of protein chains. Science, 181(4096),
223230.
Apaydin, M. S., Brutlag, D. L., Guestrin, C., Hsu, D., & Latombe, J.-C. (2003). Stochastic roadmap
simulation: an efficient representation and algorithm for analyzing molecular motion. J Comput Biol, 10(3-4), 257281.
Apaydin, M. S., Brutlag, D. L., Hsu, D., & Latombe, J.-C. (2002). Stochastic conformational
roadmaps for computing ensemble properties of molecular motion. In Workshop Algorithm
Found Robot, pp. 131147, Nice, France. IEEE.
Apaydin, M. S., Singh, A. P., Brutlag, D. L., & Latombe, J.-C. (2001). Capturing molecular energy
landscapes with probabilistic conformational roadmaps. In Intl Conf Robot Autom (ICRA),
Vol. 1, pp. 932939, Seoul, Korea. IEEE.
Atilgan, A., Durell, S., Jernigan, R., Demirel, M., Keskin, O., & Bahar, I. (2001). Anisotropy of
fluctuation dynamics of proteins with an elastic network model. Biophys J, 80(1), 505515.
Bahar, R., & Rader, A. J. (2005). Coarse-grained normal mode analysis in structural biology. Curr
Opinion Struct Biol, 204(5), 17.
Baldwin, R. L. (1995). The nature of protein folding pathways: the classical versus the new view. J
Biomol NMR, 5(2), 103109.
556

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Barbe, S., Cortes, J., Simeon, T., Monsan, P., Remaud-Simeon, M., & Andre, I. (2011). A mixed
molecular modeling-robotics approach to investigate lipase large molecular motions. Proteins: Struct Funct Bioinf, 79(8), 25172529.
Baron, R. (2013). Fast sampling of A-to-B protein global conformational transitions: From Galileo
Galilei to Monte Carlo anisotropic network modeling. Biophys J, 105(7), 15451546.
Batcho, P., Case, D. A., & Schlick, T. (2001). Optimized particle-mesh ewald/multiple-time step
integration for molecular dynamics simulations. J Chem Phys, 115(9), 40034018.
Bekris, K. E., & Kavraki, L. E. (2007). Greedy but safe replanning under kinodynamic constraints.
In Intl Conf Robot Autom (ICRA), pp. 704710, Rome, Italy. IEEE.
Berman, H. M., Henrick, K., & Nakamura, H. (2003). Announcing the worldwide Protein Data
Bank. Nat Struct Biol, 10(12), 980980.
Boehr, D. D., & Wright, P. E. (2008). How do proteins interact?. Science, 320(5882), 14291430.
Bradley, P., Misura, K. M., & Baker, D. (2005). Toward high-resolution de novo structure prediction
for small proteins. Science, 309(5742), 18681871.
Brooks, B. R., Bruccoleri, R. E., Olafson, B. D., States, D. J., Swaminathan, S., & Karplus, M.
(1983). CHARMM: a program for macromolecular energy, minimization, and dynamics calculations. J Comput Chem, 4(2), 187217.
Bryngelson, J. D., Onuchic, J. N., Socci, N. D., & Wolynes, P. G. (1995). Funnels, pathways, and
the energy landscape of protein folding: a synthesis. Proteins: Struct Funct Genet, 21(3),
167195.
Bryngelson, J. D., & Wolynes, P. G. (1987). Spin glasses and the statistical mechanics of protein
folding. Proc Natl Acad Sci USA, 84(21), 75247528.
Burgess, A. W., & Scheraga, H. A. (1975). Assessment of some problems associated with prediction
of the three-dimensional structure of a protein from its amino-acid sequence. Proc Natl Acad
Sci USA, 72(4), 12211225.
Burns, B., & Brock, O. (2007). Single-query motion planning with utility-guided random trees. In
Intl Conf Robot Autom (ICRA), pp. 33073312, Rome, Italy. IEEE.
Case, D. A., Darden, T. A., Cheatham, T. E. I., Simmerling, C. L., Wang, J., Duke, R. E., Luo, R.,
Merz, K. M., Pearlman, D. A., Crowley, M., Walker, R. C., Zhang, W., Wang, B., Hayik, S.,
Roitberg, A., Seabra, G., Wong, K. F., Paesani, F., Wu, X., Brozell, S., Tsui, V., Gohlke, H.,
Yang, L., Tan, C., Mongan, J., et al. (2014). Amber 14. http://ambermd.org/.
Chiang, T. H., Apaydin, M., Brutlag, D., Hsu, D., & Latombe, J. (2006). Predicting experimental
quantities in protein folding kinetics using stochastic roadmap simulation. In Res Comput
Mol Biol, Vol. 3909 of Lecture Notes in Computer Science, pp. 410424. Springers.
Chiang, T. H., Apaydin, M. S., Brutlag, D. L., Hsu, D., & Latombe, J.-C. (2007). Using stochastic
roadmap simulation to predict experimental quantities in protein folding kinetics: folding
rates and phi-values. J Comput Biol, 14(5), 578593.
Chiang, T. H., Hsu, D., & C., L. J. (2010). Markov dynamic models for long-timescale protein
motion. Bioinformatics, 26(12), 269277.
557

fiS HEHU & P LAKU

Chirikjian, G. S. (1993). General methods for computing hyper-redundant manipulator inverse
kinematics. In Intl Conf Intell Robot Sys (IROS), Vol. 2, pp. 10671073, Yokohama, Japan.
IEEE.
Chodera, J. D., & Noe, F. (2014). Markov state models of biomolecular conformational dynamics.
Curr Opinion Struct Biol, 25, 135144.
Chodera, J. D., Singhal, N., Pande, V. S., Dill, K. A., & Swope, W. C. (2007). Automatic discovery
of metastable states for the construction of markov models of macromolecular conformational
dynamics. J Chem Phys, 126(15), 155101.
Choset, H., & et al. (2005). Principles of Robot Motion: Theory, Algorithms, and Implementations
(1st edition). MIT Press, Cambridge, MA.
Ciu, Q., & Bahar, I. (2005). Normal Mode Analysis: Theory and Applications to Biological and
Chemical Systems (1st edition). CRC Press.
Clausen, R., Ma, B., Nussinov, R., & Shehu, A. (2015). Mapping the conformation space of wildtype
and mutant H-Ras with a memetic, cellular, and multiscale evolutionary algorithm. PLoS
Comput Biol, 11(9), e1004470.
Clausen, R., & Shehu, A. (2015). A data-driven evolutionary algorithm for mapping multi-basin
protein energy landscapes. J Comp Biol, 22(9), 844860.
Clementi, C. (2008). Coarse-grained models of protein folding: Toy-models or predictive tools?.
Curr Opinion Struct Biol, 18(1), 1015.
Coifman, R. R., Lafon, S., Lee, A. B., Maggioni, M., Nadler, B., Warner, F., & Zucker, S. W. (2005).
Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion
maps. Proc Natl Acad Sci USA, 102(21), 74267431.
Cooper, A. (1984). Protein fluctuations and the thermodynamic uncertainty principle. Prog Biophys
Mol Biol, 44(3), 181214.
Cortes, J., Jaillet, L., & Simeon, T. (2007). Molecular disassembly with RRT-like algorithms. In
Intl Conf Robot Autom (ICRA), pp. 33013306, Roma, Italy.
Cortes, J., Le, D. T., Lehl, R., & Simeon, T. (2010). Simulating ligand-induced conformational
changes in proteins using a mechanical disassembly method. Phys Chem Chem Phys, 12(29),
82688276.
Cortes, J., Simeon, T.AND Remauld-Simeon, M., & Tran, V. (2004). Geometric algorithms for the
conformational analysis of long protein loops. J Comput Chem, 25(7), 956967.
Cortes, J., Simeon, T.AND de Angulo, R., Guieysse, D., Remaud-Simeon, M., & Tran, V. (2005). A
path planning approach for computing large-amplitude motions of flexible molecules. Bioinformatics, 21(S1), 116125.
Craig, J. (1989). Introduction to robotics: mechanics and control (2nd edition). Addison-Wesley,
Boston, MA.
Da, L. T., Sheong, F. K., Silva, D. A., & Huang, X. (2014). Application of Markov state models
to simulate long timescale dynamics of biological macromolecules. Adv Exp Med Biol, 805,
2966.
558

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Dalibard, S., & Laumond, J.-P. (2009). Control of probabilistic diffusion in motion planning. In
Workshop Algorithm Found Robot, Vol. 57 of Springer Tracts in Advanced Robotics, pp. 467
481. Springer.
Das, A., Gur, M., Cheng, M. H., Jo, S., Bahar, I., & Roux, B. (2014). Exploring the conformational transitions of biomolecular systems using a simple two-state anisotropic network
model. PLoS Comput Biol, 10(4), e1003521.
Das, P., Matysiak, S., & Clementi, C. (2005). Balancing energy and entropy: A minimalist model
for the characterization of protein folding landscapes. Proc Natl Acad Sci USA, 102(29),
1014110146.
Das, P., Moll, M., Stamati, H., Kavraki, L. E., & Clementi, C. (2006). Low-dimensional free energy
landscapes of protein folding reactions by nonlinear dimensionality reduction. Proc Natl
Acad Sci USA, 103(26), 98859890.
Delarue, M., & Sanejouand, Y. H. (2002). Simplified normal mode analysis of conformational
transitions in DNA-dependent polymerases: the elastic network model. J Mol Biol, 320(5),
10111024.
Deng, N.-J., Dai, W., & Levy, R. M. (2013). How kinetics within the unfolded state affects protein
folding: An analysis based on markov state models and an ultra-long md trajectory. J Phys
Chem B, 117(42), 1278712799.
Denny, J., & Amato, N. M. (2013). Toggle PRM: A coordinated mapping of C-free and C-obstacle
in arbitrary dimension. In Workshop Algorithm Found Robot, Vol. 86 of Springer Tracts in
Advanced Robotics, pp. 297312. Springer.
Devaurs, D., Molloy, K., Vaisset, M., Shehu, A., Cortes, J., & Simeon, T. (2015). Characterizing
energy landscapes of peptides using a combination of stochastic algorithms. IEEE Trans
NanoBioScience, 14(5), 545552.
Diekmann, S., & Hoischen, C. (2014). Biomolecular dynamics and binding studies in the living
cell. Physics of Life Reviews, 11(1), 130.
Dill, K. A., & Chan, H. S. (1997). From Levinthal to pathways to funnels. Nat Struct Biol, 4(1),
1019.
Dryga, A., Chakrabarty, S., Vicatos, S., & Warshel, A. (2011). Realistic simulation of the activation
of voltage-gated ion channels. Proc Natl Acad Sci USA, 109(9), 33353340.
Dubrow, A. (2015). What got done in one year at NSFs Stampede supercomputer. Comput Sci Eng,
17(2), 8388.
Ekenna, C., Thomas, S., & Amato, N. (2016). Adaptive local learning in sampling based motion
planning for protein folding. BMC Syst Biol, 10(Suppl 2).
Engh, R. A., & Huber, R. (1991). Accurate bond and angle parameters for X-ray protein structure
refinement. Acta Crystallogr, A47, 392400.
Enosh, A., Raveh, B., Furman-Schueler, O., Halperin, D., & Ben-Tal, N. (2008). Generation, comparison, and merging of pathways between protein conformations: gating in K-channels. Biophys J, 95(8), 38503860.
559

fiS HEHU & P LAKU

Fattebert, J.-L., Richards, D. F., & Glosli, J. N. (2012). Dynamic load balancing algorithm for
molecular dynamics based on voronoi cells domain decompositions. Comput Phys Communic, 183(12), 26082615.
Fenwick, R. B., van den Bedem, H., Fraser, J. S., & Wright, P. E. (2014). Integrated description of
protein dynamics from room-temperature X-ray crystallography and NMR. Proc Natl Acad
Sci USA, 111(4), E445E454.
Fernandez-Medarde, A., & Santos, E. (2011). Ras in cancer and developmental diseases. Genes
Cancer, 2(3), 344358.
Fersht, A. (2013). Profile of martin karplus, michael levitt, and arieh warshel, 2013 nobel laureates
in chemistry. Proc Natl Acad Sci USA, 110(49), 1965619657.
Fersht, A. R. (1999). Structure and Mechanism in Protein Science. A Guide to Enzyme Catalysis
and Protein Folding (3 edition). W. H. Freeman and Co., New York, NY.
Feynman, R. P., Leighton, R. B., & Sands, M. (1963). The Feynman Lectures on Physics. AddisonWesley, Reading, MA.
Fischer, S., & Karplus, M. (1992). Conjugate peak refinement: an algorithm for finding reaction
paths and accurate transition states in systems with many degrees of freedom. Chem Phys
Lett, 194(3), 252261.
Fox, N., & Streinu, I. (2013). Towards accurate modeling of noncovalent interactions for protein
rigidity analysis. BMC Bioinf, 14(Suppl 18), S3.
Gall, A., Ilioaia, C., Kruger, T. P., Novoderezhkin, V. I., Robert, B., & van Grondelle, R. (2015).
Conformational switching in a light-harvesting protein as followed by single-molecule spectroscopy. Biophys J, 108(11), 27132720.
Gibbs, J. B., Schaber, M. D., Allard, W. J., Sigal, I. S., & Scolnick, E. M. (1988). Purification of Ras
GTPase activating protein from bovine brain. Proc Natl Acad Sci USA, 85(14), 50265030.
Gipson, B., Hsu, D., Kavraki, L. E., & Latombe, J.-C. (2012). Computational models of protein
kinematics and dynamics: Beyond simulation. Annu Rev Anal Chem, 5, 273291.
Gipson, B., Moll, M., & Kavraki, L. E. (2013). SIMS: A hybrid method for rapid conformational
analysis. PLoS One, 8(7), e68826.
Gorfe, A. A., Grant, B. J., & McCammon, J. A. (2008). Mapping the nucleotide and isoformdependent structural and dynamical features of Ras proteins. Structure, 16(6), 885896.
Gotz, A. W., Williamson, M. J., Xu, D., Poole, D., Le Grand, S., & Walker, R. C. (2012). Routine
microsecond molecular dynamics simulations with amber on GPUs. 1. Generalized Born. J
Chem Theory Comput, 8(5), 15421555.
Grant, B. J., Gorfe, A. A., & McCammon, J. A. (2009). Ras conformational switching: Simulating
nucleotide-dependent conformational transitions with accelerated molecular dynamics. PLoS
Comput Biol, 5(3), e1000325.
Greenleaf, W. J., Woodside, M. T., & Block, S. M. (2007). High-resolution, single-molecule measurements of biomolecular motion. Annu Rev Biophys Biomol Struct, 36, 171190.
Gremer, L., Gilsbach, B., Ahmadian, M. R., & Wittinghofer, A. (2008). Fluoride complexes of
oncogenic Ras mutants to study the Ras-RasGap interaction. Biol Chem, 389(9), 11631171.
560

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Gsponer, J., Christodoulou, J., Cavalli, A., Bui, J. M., Richter, B., Dobson, C. M., & Vendruscolo, M.
(2008). A coupled equilibrium shift mechanism in calmodulin-mediated signal transduction.
Structure, 16(5), 736746.
Han, K. F., & Baker, D. (1996). Global properties of the mapping between local amino acid sequence
and local structure in proteins. Proc Natl Acad Sci USA, 93(12), 58145818.
Han, L., & Amato, N. M. (2001). A kinematics-based probabilistic roadmap method for closed chain
systems. In Donald, B. R., Lynch, K. M., & Rus, D. (Eds.), Algorithmic and Computational
Robotics: New Directions, pp. 233246. AK Peters, MA.
Harvey, M. J., Giupponi, G., & de Fabritiis, G. (2009). ACEMD: Accelerating biomolecular dynamics in the microsecond timescale. J Comput Theor Chem, 5(6), 16321639.
Haspel, N., Moll, M., Baker, M. L., Chiu, W., & E., K. L. (2010). Tracing conformational changes
in proteins. BMC Struct Biol, 10(Suppl1), S1.
Haspel, N., Tsai, C., Wolfson, H., & Nussinov, R. (2003). Reducing the computational complexity
of protein folding via fragment folding and assembly. Protein Sci, 12(6), 11771187.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1), 97109.
Hermans, J., Berendsen, H. J. C., van Gunsteren, W. F., & Postma, J. P. M. (1984). A consistent
empirical potential for water-protein interactions. Biopolymers, 23(8), 15131518.
Hoang, T. H., Trovato, A., Seno, F., Banavar, J. R., & Maritan, A. (2007). Geometry and symmetry
presculpt the free-energy landscape of proteins. Proc Natl Acad Sci USA, 101(21), 7960
7964.
Hoard, B., Jacobson, B., Manavi, K., & Tapia, L. (2016). Extending rule-based methods to model
molecular geometry and 3d model resolution. BMC Syst Biol, 10(Suppl 2), 48.
Hoelder, S., Clarke, P. A., & Workman, P. (2012). Discovery of small molecule cancer drugs:
Successes, challenges and opportunities. Mol Oncol, 6(2), 522524.
Hohlbein, J., Craggs, T. D., & Cordes, T. (2014). Alternating-laser excitation: single-molecule
FRET and beyond. Chem Soc Rev, 43(4), 11561171.
Hori, N., Chikenji, G., & Takada, S. (2009). Folding energy landscape and network dynamics of
small globular proteins. Proc Natl Acad Sci USA, 106(1), 7378.
Hornak, V., Abel, R., Okur, A., Strockbine, B., Roitberg, A., & Simmerling, C. (2006). Comparison
of multiple amber force fields and development of improved protein backbone parameters.
Proteins: Struct Funct Bioinf, 65(3), 712725.
Hsu, D., Kindel, R., Latombe, J.-C., & Rock, S. (2002). Randomized kinodynamic motion planning
with moving obstacles. Intl J Robot Res, 21(3), 233255.
Hsu, D., Sanchez-Ante, G., & Sun, Z. (2005). Hybrid PRM sampling with a cost-sensitive adaptive
strategy. In Intl Conf Robot Autom (ICRA), pp. 38853891, Barcelona, Spain.
Humphrey, W., Dalke, A., & Schulten, K. (1996). VMD - Visual Molecular Dynamics. J Mol Graph
Model, 14(1), 3338. http://www.ks.uiuc.edu/Research/vmd/.
Jaillet, L., Corcho, F. J., Perez, J.-J., & Cortes, J. (2011). Randomized tree construction algorithm
to explore energy landscapes. J Comput Chem, 32(16), 34643474.
561

fiS HEHU & P LAKU

Jaillet, L., Cortes, J., & Simeon, T. (2008). Transition-based RRT for path planning in continuous
cost spaces. In Intl Conf Intell Robot Sys (IROS), pp. 2226, Stanford, CA. IEEE/RSJ.
Jaillet, L., Yershova, A., LaValle, S. M., & Simeon, T. (2005). Adaptive tuning of the sampling
domain for dynamic-domain RRTs. In Intl Conf Intell Robot Sys (IROS), pp. 40864091.
IEEE/RSJ.
Jayachandran, G., Vishal, V., & Pande, V. S. (2006). Using massively parallel simulation and Markovian models to study protein folding: examining the dynamics of the villin headpiece. J Chem
Phys, 124(16), 164902164914.
Jenzler-Wildman, K., & Kern, D. (2007). Dynamic personalities of proteins. Nature, 450(7172),
964972.
Jorgensen, W. L., Maxwell, D. S., & Tirado-Reves, J. (1988). Development and testing of the OPLS
all-atom force field on conformational energetics and properties of organic liquids. J Amer
Chem Soc, 118(45), 1122511236.
Kalisiak, M., & van de Panne, M. (2006). RRT-blossom: RRT with a local flood-fill behavior. In
Intl Conf Robot Autom (ICRA), pp. 12371242, Orlando, FL. IEEE.
Kamerlin, S. C., Haranczyk, M., & Warshel, A. (2009). Progresses in ab initio QM/MM free energy
simulations of electrostatic energies in proteins: Accelerated QM/MM studies of pka, redox
reactions and solvation free energies. J Phys Chem B, 113(5), 12531272.
Karam, P., Powdrill, M. H., Liu, H. W., Vasquez, C., Mah, W., Bernatchez, J., Gotte, M., & Cosa,
G. (2014). Dynamics of hepatitis C virus (HCV) RNA-dependent RNA polymerase NS5B in
complex with RNA. J Biol Chem, 289(20), 1439914411.
Karaman, S., & Frazzoli, E. (2011). Sampling-based algorithms for optimal motion planning. Intl
J Robot Res, 30(7), 846894.
Karnoub, A. E., & Weinberg, R. A. (2008). Ras oncogenes: split personalities. Nat Rev Mol Cell
Biol, 9(7), 517531.
Kavraki, L. E., Svestka, P., Latombe, J. C., & Overmars, M. H. (1996). Probabilistic roadmaps for
path planning in high-dimensional configuration spaces. IEEE Trans Robot Automat, 12(4),
566580.
Kay, L. E. (1998). Protein dynamics from NMR. Nat Struct Biol, 5(2-3), 513517.
Kay, L. E. (2005). NMR studies of protein structure and dynamics. J Magn Reson, 173(2), 193207.
Kendrew, J. C., Bodo, G., Dintzis, H. M., Parrish, R. G., Wyckoff, H., & Phillips, D. C. (1958).
A three-dimensional model of the myoglobin molecule obtained by X-ray analysis. Nature,
181(4610), 662666.
Kendrew, J. C., Dickerson, R. E., Strandberg, B. E., Hart, R. G., Davies, D. R., Phillips, D. C., &
Shore, V. C. (1960). Structure of myoglobin: A three-dimensional Fourier synthesis at 2A
resolution. Nature, 185(4711), 422427.
Khaliullin, R. Z., VandeVondele, J., & Hutter, J. (2013). Efficient linear-scaling density functional
theory for molecular systems. J Chem Theory Comput, 9(10), 44214427.
Kiesel, S., Burns, E., & Ruml, W. (2012). Abstraction-guided sampling for motion planning. In
Symp Combinat Search (SOCS), pp. 162163, Niagara Falls, Canada.
562

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Kim, K. M., Jernigan, R. L., & Chirikjian, G. S. (2002a). Efficient generation of feasible pathways
for protein conformational transitions. Biophys J, 83(3), 16201630.
Kim, M. K., Chirikjian, G. S., & Jernigan, R. L. (2002b). Elastic models of conformational transitions in macromolecules. J Mol Graph Model, 21(2), 151160.
Kirillova, S., Cortes, J., Stefaniu, A., & Simeon, T. (2008). An NMA-guided path planning approach
for computing large-amplitude conformational changes in proteins. Proteins: Struct Funct
Bioinf, 70(1), 131143.
Kolodny, R., Guibas, L., Levitt, M., & Koehl, P. (2005). Inverse kinematics in biology: the protein
loop closure problem. Intl J Robot Res, 24(2-3), 151163.
Ladd, A. M., & Kavraki, L. E. (2004). Fast tree-based exploration of state space for robots with dynamics. In Workshop Algorithm Found Robot, pp. 297312. Springer, Utrecht/Zeist, Netherlands.
Ladd, A. M., & Kavraki, L. E. (2005). Motion planning in the presence of drift, underactuation and
discrete system changes. In Robot: Sci and Sys, pp. 233241, Boston, MA.
LaValle, S. M., & Kuffner, J. J. (2001). Randomized kinodynamic planning. Intl J Robot Res, 20(5),
378400.
Leaver-Fay, A., Tyka, M., Lewis, S. M., Lange, O. F., Thompson, J., Jacak, R., Kaufman, K., Renfrew, P. D., Smith, C. A., Sheffler, W., Davis, I. W., Cooper, S., Treuille, A., Mandell, D. J.,
Richter, F., Ban, Y. E., Fleishman, S. J., Corn, J. E., Kim, D. E., Lyskov, S., Berrondo, M.,
Mentzer, S., Popovi, Z., & et. al. (2011). ROSETTA3: an object-oriented software suite for
the simulation and design of macromolecules. Methods Enzymol, 487, 545574.
Lee, A., Streinu, I., & Brock, O. (2005). A methodology for efficiently sampling the conformation
space of molecular structures. J Phys Biol, 2(4), 108S115.
Lee, H. M., M., K. S., Kim, H. M., & Suh, Y. D. (2013). Single-molecule surface-enhanced Raman
spectroscopy: a perspective on the current status. Phys Chem Chem Phys, 15, 52765287.
Lee, J., Kwon, O., Zhang, L., & Yoon, S.-E. (2014). A selective retraction-based RRT planner for
various environments. IEEE Trans Robotics, 30(4), 10021011.
Levitt, M., & Lifson, S. (1969). Refinement of protein conformations using a macromolecular
energy minimization procedure. J Mol Biol, 46(2), 269279.
Levitt, M., & Warshel, A. (1975). Computer simulation of protein folding. Nature, 253(5494),
9496.
Levy, Y., Jortner, J., & Becker, O. M. (2001). Solvent effects on the energy landscapes and folding
kinetics of polyalanine. Proc Natl Acad Sci USA, 98(5), 21882193.
Li, D., Yang, H., Han, L., & Huo, S. (2008). Predicting the folding pathway of engrailed homeodomain with a probabilistic roadmap enhanced reaction-path algorithm. Biophys J, 94(5),
16221629.
Lifson, S., & Warshel, A. (1968). A consistent force field for calculation on conformations, vibrational spectra and enthalpies of cycloalkanes and n-alkane molecules. J Phys Chem, 49,
51165129.
563

fiS HEHU & P LAKU

Lin, T., & Song, G. (2011). Efficient mapping of ligand migration channel networks in dynamic
proteins. Proteins: Struct Funct Bioinf, 79(8), 24752490.
Lindorff-Larsen, K., Piana, S., Dror, R. O., & Shaw, D. E. (2011). How fast-folding proteins fold.
Science, 334(6055), 517520.
Lois, G., Blawzdziewicz, J., & OHern, C. S. (2010). Protein folding on rugged energy landscapes:
Conformational diffusion on fractal networks. Phys Rev E Stat Nonlin Soft Matter Phys, 81(5
Pt 1), 051907.
Lotan, I., van den Bedem, H., Deacon, A. M., & Latombe, J.-C. (2004). Computing protein structures from electron density maps: The mising loop problem. In Erdman, M., Hsu, D., Overmars, M., & van der Stappen, F. (Eds.), Algorithmic Foundations of Robotics VI, pp. 153168.
Springer STAR Series.
Ma, B., Kumar, S., Tsai, C., & Nussinov, R. (1999). Folding funnels and binding mechanisms.
Protein Eng, 12(9), 713720.
Ma, J., & Karplus, M. (1997). Molecular switch in signal transduction: reaction paths of the conformational changes in ras p21. Proc Natl Acad Sci USA, 94(22), 1190511910.
Maisuradze, G. G., Liwo, A., & Scheraga, H. A. (2009). Principal component analysis for protein
folding dynamics. J Mol Biol, 385(1), 312329.
Malmstrom, R. D., Lee, C. T., Van Wart, A. T., & Amaro, R. E. (2014). Application of moleculardynamics based Markov state models to functional proteins. J Chem Theory Comput, 10(7),
26482657.
Manocha, D., & Canny, J. (1994). Efficient inverse kinematics for general 6r manipulator. IEEE
Trans Robot Autom, 10(5), 648657.
Manocha, D., & Zhu, Y. (1994). Kinematic manipulation of molecular chains subject to rigid constraints. In Altman, R. B., Brutlag, D. L., Karp, P. D., Lathrop, R. H., & Searls, D. B. (Eds.),
Intl Conf Intell Sys Mol Biol (ISMB), Vol. 2, pp. 285293, Stanford, CA. AAAI.
Manocha, D., Zhu, Y., & Wright, W. (1995). Conformational analysis of molecular chains using
nano-kinematics. Comput. Appl. Biosci., 11(1), 7186.
Maragakis, P., & Karplus, M. (2005). Large amplitude conformational change in proteins explored
with a plastic network model: adenylate kinase. J Mol Biol, 352(4), 807822.
Matysiak, S., & Clementi, C. (2004). Optimal combination of theory and experiment for the characterization of the protein folding landscape of S6: How far can a minimalist model go?. J
Mol Biol, 343(8), 235248.
Matysiak, S., & Clementi, C. (2006). Minimalist protein model as a diagnostic tool for misfolding
and aggregation. J Mol Biol, 363(1), 297308.
Maximova, T., Moffatt, R., Ma, B., Nussinov, R., & Shehu, A. (2016). Principles and overview of
sampling methods for modeling macromolecular structure and dynamics. PLoS Comput Biol,
12(4), e1004619.
Maximova, T., Plaku, E., & Shehu, A. (2015). Computing transition paths in multiple-basin proteins
with a probabilistic roadmap algorithm guided by structure data. In Intl Conf on Bioinf and
Biomed (BIBM), pp. 3542, Washington, D.C. IEEE.
564

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

McCammon, J. A., Gelin, B. R., & Karplus, M. (1977). Dynamics of folded proteins. Nature,
267(5612), 585590.
McLachlan, A. D. (1972). A mathematical procedure for superimposing atomic coordinates of
proteins. Acta Crystallogr A, 26(6), 656657.
McMahon, T., Thomas, S., & Amato, N. M. (2015). Reachable volume RRT. In Intl Conf Robot
Autom (ICRA), pp. 29772984, Seattle, WA. IEEE.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation
of state calculations by fast computing machines. J Chem Phys, 21(6), 10871092.
Miao, Y., Sinko, W., Pierce, L., Bucher, D., Walker, R. C., & McCammon, J. A. (2014). Improved
reweighting of accelerated molecular dynamics simulations for free energy calculation. J
Chem Theory Comput, 10(7), 26772689.
Michalet, X., Weiss, S., & Jager, M. (2006). Single-molecule fluorescence studies of protein folding
and conformational dynamics. Chem Rev, 106(5), 17851813.
Moerner, W. E., & Fromm, D. P. (2003). Methods of single-molecule fluorescence spectroscopy.
Rev Scientific Instruments, 74(8), 35973619.
Moffat, K. (2003). The frontiers of time-resolved macromolecular crystallography: movies and
chirped X-ray pulses. Faraday Discuss, 122(79-88), 6577.
Moll, M., Schwartz, D., & Kavraki, L. E. (2008). Roadmap methods for protein folding. In Zaki,
M., & Bystroff, C. (Eds.), Protein Structure Prediction, Vol. 413 of Methods Mol Biol, pp.
219239. Springer.
Molloy, K., Clausen, R., & Shehu, A. (2016). A stochastic roadmap method to model protein
structural transitions. Robotica, 34(8), 17051733.
Molloy, K., Saleh, S., & Shehu, A. (2013). Probabilistic search and energy guidance for biased
decoy sampling in ab-initio protein structure prediction. IEEE/ACM Trans Bioinf and Comp
Biol, 10(5), 11621175.
Molloy, K., & Shehu, A. (2013). Elucidating the ensemble of functionally-relevant transitions in
protein systems with a robotics-inspired method. BMC Struct Biol, 13(Suppl 1), S8.
Molloy, K., & Shehu, A. (2015). Interleaving global and local search for protein motion computation. In Harrison, R., Li, Y., & Mandoiu, I. (Eds.), LNCS: Bioinformatics Research and
Applications, Vol. 9096, pp. 175186, Norfolk, VA. Springer International Publishing.
Molloy, K., & Shehu, A. (2016). A general, adaptive, roadmap-based algorithm for protein motion
computation. IEEE Trans. NanoBioSci., 2(15), 158165.
Mukherjee, S., & Warshel, A. (2011). Electrostatic origin of the mechanochemical rotary mechanism and the catalytic dwell of F1-ATPase. Proc Natl Acad Sci USA, 108(51), 2055020555.
Mukherjee, S., & Warshel, A. (2012). Realistic simulations of the coupling between the protomotive
force and the mechanical rotation of the F0-ATPase. Proc Natl Acad Sci USA, 109(3), 14876
14881.
Mukherjee, S., & Warshel, A. (2013). Electrostatic origin of the unidirectionality of walking myosin
v motors. Proc Natl Acad Sci USA, 110(43), 1732617331.
565

fiS HEHU & P LAKU

Na, H., & Song, G. (2015). Quantitative delineation of how breathing motions open ligand migration
channels in myoglobin and its mutants. Proteins: Struct Funct Bioinf, 83(4), 757770.
Neudecker, P., Robustelli, P., Cavalli, A., Walsh, P., Lundstrm, P., Zarrine-Afsar, A., Sharpe, S.,
Vendruscolo, M., & Kay, L. E. (2012). Structure of an intermediate state in protein folding
and aggregation. Science, 336(6079), 362366.
Nevo, R., Brumfeld, V., Kapon, R., Hinterdorfer, P., & Reich, Z. (2005). Direct measurement of
protein energy landscape roughness. EMBO Rep, 6(5), 482486.
Nielsen, C. L., & Kavraki, L. E. (2000). A two level fuzzy PRM for manipulation planning. In Intl
Conf Intell Robot Sys (IROS), Vol. 3, pp. 17161721, Takamatsu, Japan. IEEE/RSJ.
Noe, F., Doose, S., Daidone, I., Lollmann, M., Sauer, M., Chodera, J. D., & Smith, J. C. (2011).
Dynamical fingerprints for probing individual relaxation processes in biomolecular dynamics
with simulations and kinetic experiments. Proc Natl Acad Sci USA, 108(12), 48224827.
Noe, F., & Fischer, S. (2008). Transition networks for modeling the kinetics of conformational
change in macromolecules. Curr Opinion Struct Biol, 18(2), 154162.
Olson, B., Hashmi, I., Molloy, K., & Shehu, A. (2012). Basin hopping as a general and versatile
optimization framework for the characterization of biological macromolecules. Advances in
AI J, 2012(674832).
Olson, B., & Shehu, A. (2012). Evolutionary-inspired probabilistic search for enhancing sampling
of local minima in the protein energy surface. Proteome Sci, 10(Suppl 1), S5.
Olson, B. S., Molloy, K., Hendi, S.-F., & Shehu, A. (2012). Guiding search in the protein conformational space with structural profiles. J Bioinf & Comput Biol, 10(3), 1242005.
Onuchic, J. N., Luthey-Schulten, Z., & Wolynes, P. G. (1997). Theory of protein folding: The energy
landscape perspective. Annu Rev Phys Chem, 48, 545600.
Onuchic, J. N., & Wolynes, P. G. (2004). Theory of protein folding. Curr Opinion Struct Biol, 14,
7075.
Ovchinnikov, V., & Karplus, M. (2012). Analysis and elimination of a bias in targeted molecular
dynamics simulations of conformational transitions: Application to calmodulin. J Phys Chem
B, 116(29), 85848603.
Ozenne, V., Schneider, R., Yao, M., Huang, J. R., Salmon, L., Zweckstetter, M., Jensen, M. R., &
Blackledge, M. (2012). Mapping the potential energy landscape of intrinsically disordered
proteins at amino acid resolution. J Amer Chem Soc, 134(36), 1513815148.
Ozkan, S. B., Dill, K. A., & Bahar, I. (2002). Fast-folding protein kinetics, hidden intermediates,
and the sequential stabilization model. Protein Sci, 11(8), 19581970.
Palmieri, L., & Arras, K. O. (2015). Distance metric learning for RRT-based motion planning with
constant-time inference. In Intl Conf Robot Autom (ICRA), pp. 637643, Seattle, WA. IEEE.
Pande, V. S., Beachamp, K., & Bowman, G. R. (2010). Everything you wanted to know about
Markov state models but were afraid to ask. Nat Methods, 52(1), 99105.
Papoian, G. A., Ulander, J., Eastwood, M. P., Luthey-Schulten, Z., & Wolynes, P. G. (2004). Water
in protein structure prediction. Proc Natl Acad Sci USA, 101(10), 33523357.
566

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Perez-Hernandez, G., Paul, F., Giorgino, T., De Fabritiis, G., & Noe, F. (2013). Identification of slow
molecular order parameters for markov model construction. J Chem Phys, 139(1), 015102.
Perilla, J. R., Goh, B. C., Cassidy, C. K., Liu, B., Bernardi, R. C., Rudack, T., Yu, H., Wu, Z., &
Schulten, K. (2015). Molecular dynamics simulations of large macromolecular complexes.
Curr Opin Struct Biol, 31, 6474.
Phillips, D. C. (1967). The hen egg-white lysozyme molecule. Proc Natl Acad Sci USA, 57(3),
483495.
Phillips, J. C., Braun, R., Wang, W., Gumbart, J., Tajkhorshid, E., Villa, E., Chipot, C., Skeel, R. D.,
Kale, L., & Schulten, K. (2005). Scalable molecular dynamics with NAMD. J Comput Chem,
26(16), 17811802.
Piana, S., Lindorff-Larsen, K., Dirks, R. M., Salmon, J. K., Dror, R. O., & Shaw, D. E. (2012a).
Evaluating the effects of cutoffs and treatment of long-range electrostatics in protein folding
simulations. PLoS ONE, 7(6), e39918.
Piana, S., Lindorff-Larsen, K., & Shaw, D. E. (2012b). Protein folding kinetics and thermodynamics
from atomistic simulation. Proc Natl Acad Sci USA, 109(44), 1784517850.
Plaku, E. (2015). Region-guided and sampling-based tree search for motion planning with dynamics. IEEE Transactions on Robotics, 31(3), 723735.
Plaku, E., Stamati, H., Clementi, C., & Kavraki, L. E. (2007). Fast and reliable analysis of molecular motions using proximity relations and dimensionality reduction. Proteins: Struct Funct
Bioinf, 67(4), 897907.
Plaku, E., Kavraki, L. E., & Vardi, M. Y. (2010). Motion planning with dynamics by a synergistic
combination of layers of planning. IEEE Transactions on Robotics, 26(3), 469482.
Ponder, J. W., & Case, D. A. (2003). Force fields for protein simulations. Adv Protein Chem, 66,
2785.
Porta, J. M., & Jaillet, L. (2013). Exploring the energy landscapes of flexible molecular loops using
higher-dimensional continuation. J Comput Chem, 34(3), 234244.
Porta, J. M., Thomas, F., Corcho, F., Canto, J., & Perez, J. J. (2007). Complete maps of molecularloop conformation spaces. J Comput Chem, 28(13), 21702189.
Prinz, J. H., Keller, B., & Noe, F. (2011a). Probing molecular kinetics with Markov models:
metastable states, transition pathways and spectroscopic observables. Phys Chem Chem Phys,
13(38), 1691216927.
Prinz, J. H., Wu, H., Sarich, M., Keller, B., Senne, M., Held, M., Chodera, J. D., Schutte, C., & Noe,
F. (2011b). Markov models of molecular kinetics: generation and validation. J Chem Phys,
134(17), 174105.
Proctor, A. J., Lipscomb, T. J., Zou, A., Anderson, J. A., & Cho, S. S. (2012). Performance analyses
of a parallel verlet neighbor list algorithm for GPU-optimized MD simulations. In ASE/IEEE
Intl Conf Biomed Comput (BioMedCom), pp. 1419, Alexandria, VA. IEEE.
Ramachandran, G. N., Ramakrishnan, C., & Sasisekharan, V. (1963). Stereochemistry of polypeptide chain configurations. J Mol Biol, 7, 9599.
567

fiS HEHU & P LAKU

Raveh, B., Enosh, A., Furman-Schueler, O., & Halperin, D. (2009). Rapid sampling of molecular
motions with prior information constraints,. PLoS Comput Biol, 5(2), e1000295.
Rodriguez, S., Thomas, S., Pearce, R., & Amato, N. (2006a). RESAMPL: A Region-Sensitive
Adaptive Motion Planner. In Workshop Algorithm Found Robot, Vol. 47 of Springer Tracts
in Advanced Robotics, pp. 285300. Springer, New York, NY.
Rodriguez, S., Tang, X., Lien, J.-M., & Amato, N. M. (2006b). An obstacle-based rapidly-exploring
random tree. In Intl Conf Robot Autom (ICRA), pp. 895900, Orlando, FL. IEEE.
Rohrdanz, M. A., Zheng, W., Maggioni, M., & Clementi, C. (2011). Determination of reaction
coordinates via locally scaled diffusion map. J Chem Phys, 134(12), 124116.
Rose, G. D., Fleming, P. J., Banavar, J. R., & Maritan, A. (2006). A backbone-based theory of
protein folding. Proc Natl Acad Sci USA, 103(45), 1662316633.
Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500), 23232326.
Roy, R., Hohng, S., & Ha, T. (2008). A practical guide to single-molecule FRET. Nat Methods,
5(6), 507516.
Rychkova, A., Mukherjee, S., Bora, R. P., & Warshel, A. (2013). Simulating the pulling of stalled
elongated peptide from the ribosome by the translocon. Proc Natl Acad Sci USA, 110(25),
1019510200.
Sanchez, G., & Latombe, J.-C. (2002). On delaying collision checking in PRM planning: Application to multi-robot coordination. Intl J Robot Res, 21(1), 526.
Sapin, E., Carr, D. B., De Jong, K. A., & Shehu, A. (2016). Computing energy landscape maps and
structural excursions of proteins. BMC Genomics, 17(Suppl 4), 456.
Schlau-Cohen, G. S., Wang, Q., Southall, J., Cogdell, R. J., & Moerner, W. E. (2013). Singlemolecule spectroscopy reveals photosynthetic LH2 complexes switch between emissive
states. Proc Natl Acad Sci USA, 110(27), 1089910903.
Schotte, F., Lim, M., Jackson, T. A., Smirnov, A. V., Soman, J., Olson, J. S., Phillips, G. N., Wulff,
M., & Anfinrud, P. A. (2003). Watching a protein as it functions with 150-ps time-resolved
X-ray crystallography. Science, 300(5627), 19441947.
Schuyler, A. d., Jernigan, R. L., Wasba, P. K., Ramakrishnan, B., & Chirikjian, G. S. (2009). Iterative
cluster-NMA (icnma): a tool for generating conformational transitions in proteins. Proteins:
Struct Funct Bioinf, 74(3), 760776.
Shatsky, M., Nussinov, R., & Wolfson, H. J. (2002). Flexible protein alignment and hinge detection.
Proteins, 48(2), 242256.
Shaw, D. E., Maragakis, P., Lindorff-Larsen, K., Piana, S., Dror, R. O., Eastwood, M. P., Bank, J. A.,
Jumper, J. M., Salmon, J. K., Shan, Y., & Wriggers, W. (2010). Atomic-level characterization
of the structural dynamics of proteins. Science, 330(6002), 341346.
Shehu, A. (2009). An ab-initio tree-based exploration to enhance sampling of low-energy protein
conformations. In Robot: Sci and Sys, pp. 241248, Seattle, WA, USA.
Shehu, A. (2013). Probabilistic search and optimization for protein energy landscapes. In Aluru, S.,
& Singh, A. (Eds.), Handbook of Computational Molecular Biology. Chapman & Hall/CRC
Computer & Information Science Series.
568

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Shehu, A., Clementi, C., & Kavraki, L. E. (2006). Modeling protein conformational ensembles:
From missing loops to equilibrium fluctuations. Proteins: Struct Funct Bioinf, 65(1), 164
179.
Shehu, A., Clementi, C., & Kavraki, L. E. (2007). Sampling conformation space to model equilibrium fluctuations in proteins. Algorithmica, 48(4), 303327.
Shehu, A., & Kavraki, L. E. (2012). Modeling structures and motions of loops in protein molecules.
Entropy J, 14(2), 252290.
Shehu, A., Kavraki, L. E., & Clementi, C. (2007). On the characterization of protein native state
ensembles. Biophys J, 92(5), 15031511.
Shehu, A., Kavraki, L. E., & Clementi, C. (2008). Unfolding the fold of cyclic cysteine-rich peptides. Protein Sci, 17(3), 482493.
Shehu, A., Kavraki, L. E., & Clementi, C. (2009). Multiscale characterization of protein conformational ensembles. Proteins: Struct Funct Bioinf, 76(4), 837851.
Shehu, A., & Olson, B. (2010). Guiding the search for native-like protein conformations with an
ab-initio tree-based exploration. Intl J Robot Res, 29(8), 110611227.
Shkolnik, A., Walter, M., & Tedrake, R. (2009). Reachability-guided sampling for planning under
differential constraints. In Intl Conf Robot Autom (ICRA), pp. 28592865.
Shlens,
J.
(2003).
A
tutorial
on
principal
component
https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition jp.pdf.

analysis.

Shukla, D., Hernandez, C. X., Weber, J. K., & Pande, V. S. (2015). Markov state models provide
insights into dynamic modulation of protein function. Acc Chem Res, 48(2), 414422.
Singh, A. P., Latombe, J.-C., & Brutlag, D. L. (1999). A motion planning approach to flexible ligand
binding. In Schneider, R., Bork, P., Brutlag, D. L., Glasgow, J. I., Mewes, H.-W., & Zimmer,
R. (Eds.), Intl Conf Intell Sys Mol Biol (ISMB), Vol. 7, pp. 252261, Heidelberg, Germany.
AAAI.
Singhal, N., & Pande, V. S. (2005). Error analysis and efficient sampling in markovian state models
for molecular dynamics. J Chem Phys, 123(20), 204909120490913.
Singhal, N., Snow, C. D., & Pande, V. S. (2004). Using path sampling to build better markovian
state models: Predicting the folding rate and mechanism of a tryptophan zipper beta hairpin.
J Chem Phys, 121(1), 415425.
Socher, E., & Imperiali, B. (2013). FRET-CAPTURE: A sensitive method for the detection of
dynamic protein interactions. Chem Biochem, 14(1), 5357.
Song, G., & Amato, N. M. (2000). A motion-planning approach to folding: From paper craft to
protein folding. Tech. rep. TR00-001, Department of Computer Science, Texas A & M University.
Song, G., & Amato, N. M. (2004). A motion planning approach to folding: From paper craft to
protein folding. IEEE Trans Robot Autom, 20(1), 6071.
Song, J., & Zhuang, W. (2014). Simulating the peptide folding kinetic related spectra based on the
Markov state model. In Protein Conformational Dynamics, Vol. 805 of Adv Exp Med Biol,
pp. 199220. Springer.
569

fiS HEHU & P LAKU

Soto, C. (2008). Protein misfolding and neurodegeneration. JAMA Neurology, 65(2), 184189.
Stadler, P. (2002). Fitness landscapes. Appl Math & Comput, 117, 187207.
Stone, J. E., Phillips, J. C., Freddolino, P. L., Hardy, D. J., Trabuco, L. G., & Schulten, K. (2007).
Accelerating molecular modeling applications with graphics processors. J Comput Chem,
28(16), 26182640.
Sucan, I. A., & Kavraki, L. E. (2012). A sampling-based tree planner for systems with complex
dynamics. IEEE Trans Robotics, 28(1), 116131.
Sun, Z., Hsu, D., Jiang, T., Kurniawati, H., & Reif, J. (2005). Narrow passage sampling for probabilistic roadmap planners. IEEE Trans Robotics, 21(6), 11051115.
Tama, F., & Sanejouand, Y. H. (2001). Conformational change of proteins arising from normal
mode calculations. Protein Eng, 14(1), 16.
Tama, F., Valle, M., Frank, J., & Brooks, C. L. (2003). Dynamic reorganization of the functionally
active ribosome explored by normal mode analysis and cryo-electron microscopy. Proc Natl
Acad Sci USA, 100(16), 93199323.
Tang, X., Kirkpatrick, B., Thomas, S., Song, G., & Amato, N. (2005). Using motion planning to
study rna folding kinetics. J Comput Biol, 12(6), 862881.
Tang, X., Thomas, S., Tapia, L., Giedroc, D. P., & Amato, N. (2008). Simulating rna folding kinetics
on approximated energy landscapes. J Mol Biol, 381(4), 10551067.
Tanner, D. E., Phillips, J. C., & Schulten, K. (2012). GPU/CPU algorithm for generalized
born/solvent-accessible surface area implicit solvent calculations. J Chem Theory Comput,
8(7), 25212530.
Tapia, L., Tang, X., Thomas, S., & Amato, N. (2007). Kinetics analysis methods for approximate
folding landscapes. Bioinformatics, 23, i539i548.
Tapia, L., Thomas, S., & Amato, N. (2010). A motion planning approach to studying molecular
motions. Commun Inf Sys, 10(1), 5368.
Teknipar, M., & Zheng, W. (2010). Predicting order of conformational changes during protein
conformational transitions using an interpolated elastic network model. Proteins: Struct Funct
Bioinf, 78(11), 24692481.
Tenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500), 23192323.
Teodoro, M., Phillips, G. N. J., & Kavraki, L. E. (2003). Understanding protein flexibility through
dimensionality reduction. J Comput Biol, 10(3-4), 617634.
Thomas, S., Song, G., & Amato, N. M. (2005). Protein folding by motion planning. J. Phys. Biol.,
2(4), 148.
Thomas, S., Tang, X., Tapia, L., & Amato, N. M. (2007). Simulating protein motions with rigidity
analysis. J. Comput. Biol., 14(6), 839855.
Thorpe, M. F., & Ming, L. (2004). Macromolecular flexibility. Phil. Mag., 84(13-16), 132331137.
Torella, J. P., Holden, S. J., Santoso, Y., Hohlbein, J., & Kapanidis, A. N. (2011). Identifying molecular dynamics in single-molecule FRET experiments with burst variance analysis. Biophys J,
100(6), 15681577.
570

fiC OMPUTATIONAL T REATMENTS OF B IOMOLECULES BY ROBOTICS -I NSPIRED M ETHODS

Tsai, C., Kumar, S., Ma, B., & Nussinov, R. (1999a). Folding funnels, binding funnels, and protein
function. Protein Sci, 8(6), 11811190.
Tsai, C., Ma, B., & Nussinov, R. (1999b). Folding and binding cascades: shifts in energy landscapes.
Proc Natl Acad Sci USA, 96(18), 99709972.
Uversky, V. N. (2009). Intrinsic disorder in proteins associated with neurodegenerative diseases. In
Protein Folding and Misfolding: Neurodegenerative Diseases, Vol. 14 of Focus on Structural
Biology, pp. 51885238. Springer.
van den Bedem, H., Lotan, I., Latombe, J.-C., & Deacon, A. M. (2005). Real-space protein-model
completion: an inverse-kinematics approach. Acta Crystallogr, D61(1), 213.
van der Maaten, L. J. P., Postma, E. O., & van den Herik, H. J. (2009). Dimensionality reduction:
A comparative review. J Mach Learn Res, 10(1-41), 6671.
Van Der Spoel, D., Lindahl, E., Hess, B., Groenhof, G., Mark, A. E., & Berendsen, H. J. (2005).
GROMACS: fast, flexible, and free. J Comput Chem, 26(16), 17011718.
van Gunsteren, W. F., Billeter, S. R., Eising, A. A., Hunenberger, P. H., Kruger, P., Mark, A. E.,
Scott, W. R. P., & Tironi, I. G. (1996). Biomolecular simulation: the gromos96 manual and
user guide. http://www.gromos.net/.
Verlet, L. (1967). Computer experiments on classical fluids. i. thermodynamical properties of
Lennard-Jones molecules. Phys Rev Lett, 159, 98103.
Warshel, A. (2003). Computer simulations of enzyme catalysis: Methods, progress, and insights.
Annu Rev Biophys Biomol Struct, 32, 425443.
Warshel, A., & Levitt, M. (1976). Theoretical studies of enzymatic reactions: Dielectric, electrostatic and steric stabilization of the carbonium ion in the reaction of lysozyme. J Mol Biol,
103(2), 227249.
Weber, J. K., Jack, R. L., & Pande, V. S. (2013). Emergence of glass-like behavior in markov state
models of protein folding dynamics. J Amer Chem Soc, 135(15), 55015504.
Wells, S., Menor, S., Hespenheide, B., & Thorpe, M. F. (2005). Constrained geometric simulation
of diffusive motion in proteins. J Phys Biol, 2(4), 127136.
Xu, D., & Zhang, Y. (2012). Ab initio protein structure assembly using continuous structure fragments and optimized knowledge-based force field. Proteins: Struct Funct Bioinf, 80(7), 1715
1735.
Yang, H., Wu, H., Li, D., Han, L., & Huo, S. (2007). Temperature-dependent probabilistic roadmap
algorithm for calculating variationally optimized conformational transition pathways. J Chem
Theory Comput, 3(1), 1725.
Yang, L., Song, G., Carriquiry, A., & Jernigan, R. L. (2008). Close correspondence between the essential protein motions from principal component analysis of multiple HIV-1 protease structures and elastic network modes. Structure, 16(2), 321330.
Yang, Z., Majek, P., & Bahar, I. (2009). Allosteric transitions of supramolecular systems explored
by network models: Application to chaperonin GroEL. PLoS Comput Biol, 5(4), e1000360.
Yao, P., Dhanik, A., Marz, N., Propper, R., Kou, C., Liu, G., van den Bedem, H., Latombe, J. C.,
Halperin-Landsberg, I., & Altman, R. B. (2008). Efficient algorithms to explore conformation
spaces of flexible protein loops. IEEE/ACM Trans Comput Biol Bioinf, 5(4), 534545.
571

fiS HEHU & P LAKU

Zagrovic, B., Snow, C. D., Shirts, M. R., & Pande, V. S. (2002). Simulation of folding of a small
alpha-helical protein in atomistic detail using worldwide-distributed computing. J Mol Biol,
323(5), 927937.
Zhang, M., & Kavraki, L. E. (2002a). Finding solutions of the inverse kinematics problem in
computer-aided drug design. In Florea, L., Walenz, B., & Hannenhalli, S. (Eds.), Currents in
Computational Molecular Biology, pp. 214215, Washington, D.C. ACM.
Zhang, M., & Kavraki, L. E. (2002b). A new method for fast and accurate derivation of molecular
conformations. Chem Inf Comput Sci, 42(1), 6470.
Zhao, G., Perilla, J. R., Yufenyuy, E. L., Meng, X., Chen, B., Ning, J., Ahn, J., Gronenborn, A. M.,
Schulten, K., Aiken, C., & Zhang, P. (2013). Mature HIV-1 capsid structure by cryo-electron
microscopy and all-atom molecular dynamics. Nature, 497(7451), 643646.
Zheng, W., & Brooks, B. (2005). Identification of dynamical correlations within the myosin motor
domain by the normal mode analysis of an elastic network model. J Mol Biol, 346(3), 745
759.
Zheng, W., Brooks, B. R., & Hummer, G. (2007). Protein conformational transitions explored by
mixed elastic network models. Proteins: Struct Funct Bioinf, 69(1), 4357.
Zheng, W., & Doniach, S. (2003). A comparative study of motor-protein motions by using a simple
elastic-network model. Proc Natl Acad Sci USA, 100(23), 1325313258.
Zheng, W., Rohrdanz, M. A., & Clementi, C. (2013). Rapid exploration of configuration space with
diffusion-map-directed molecular dynamics. J Phys Chem B, 117(42), 1276912776.
Zheng, W., Rohrdanz, M. A., Maggioni, M., & Clementi, C. (2011). Polymer reversal rate calculated
via locally scaled diffusion map. J Chem Phys, 134(14), 144109.
Zhou, H. (2014). Theoretical frameworks for multiscale modeling and simulation. Curr Opinion
Struct Biol, 25, 6776.

572

fiJournal of Artificial Intelligence Research 57 (2016) 307343

Submitted 07/15; published 10/16

Scrubbing During Learning In Real-time Heuristic Search
Nathan R. Sturtevant

sturtevant@cs.du.edu

Department of Computer Science
University of Denver
Denver, Colorado, USA

Vadim Bulitko

bulitko@ualberta.ca

Department of Computing Science
University of Alberta
Edmonton, Alberta, T6G 2E8, Canada

Abstract
Real-time agent-centered heuristic search is a well-studied problem where an agent that
can only reason locally about the world must travel to a goal location using bounded computation and memory at each step. Many algorithms have been proposed for this problem
and theoretical results have also been derived for the worst-case performance with simple
examples demonstrating worst-case performance in practice. Lower bounds, however, have
not been widely studied. In this paper we study best-case performance more generally and
derive theoretical lower bounds for reaching the goal using LRTA*, a canonical example of
a real-time agent-centered heuristic search algorithm. The results show that, given some
reasonable restrictions on the state space and the heuristic function, the number of steps
an LRTA*-like algorithm requires to reach the goal will grow asymptotically faster than
the state space, resulting in scrubbing where the agent repeatedly visits the same state.
We then show that while the asymptotic analysis does not hold for more complex realtime search algorithms, experimental results suggest that it is still descriptive of practical
performance.

1. Introduction
The framework of real-time agent-centered heuristic search models an agent with locally
limited sensing and perception that is trying to reach a goal while interleaving planning and
movement (Koenig, 2001). This well-studied problem has led to numerous algorithms (Korf,
1990; Furcy & Koenig, 2000; Shimbo & Ishida, 2003; Hernandez & Meseguer, 2005; Bulitko
& Lee, 2006; Hernandez & Baier, 2012) and various theoretical analysis (Ishida & Korf,
1991; Koenig & Simmons, 1993; Koenig, Tovey, & Smirnov, 2003; Bulitko & Lee, 2006;
Bulitko & Bulitko, 2009; Sturtevant, Bulitko, & Bjornsson, 2010).
Given the broad work on this problem, it is surprising to find that little work has been
done on lower bounds. It is clear that in some examples an agent can travel directly to
the goal state on an optimal path. Such examples, however, generally require an unreasonably accurate initial heuristic or a favorable domain-specific tie-breaking schema which
are hard to guarantee in practice. Other examples (Koenig & Simmons, 1993; Edelkamp &
Schrodl, 2012) show that the worst-case bound is tight, but these examples do not generally
characterize when the worst-case and best-case performance coincide.
The main contribution of this paper is a non-trivial lower bound on the travel distance
required for an agent to reach the goal in the basic real-time heuristic search framework.
c
2016
AI Access Foundation. All rights reserved.

fiSturtevant & Bulitko

We show theoretically that there exists a tie-breaking schema that forces the agent to revisit
its states (scrub) in polynomially growing state spaces; this is an undesirable property of
real-time heuristic search. Our result shows that this phenomenon is unavoidable when an
agent has limited 1-step lookahead, the state space is polynomial, all edges have unit costs,
and the initial heuristic is consistent and integer valued. We later show that lifting the
lookahead and unit edge cost restrictions allows an agent in some cases to travel directly
from the start to the goal. However, such counter examples are somewhat contrived and,
as our empirical results demonstrate, do not appear to happen frequently in practice. It is
an open question whether similar theoretical results can be derived with fewer restrictions
on the agent and the state space. We also examine exponential state spaces and learning
over multiple trials until convergence. The insights from this theoretical work also provide
explanations into why several approaches taken by other recent literature are effective.
This paper is an extended version of our previously published symposium paper (Sturtevant & Bulitko, 2014). The original paper contained the proof of asymptotic state revisitation under the assumptions of 1-step lookahead, polynomial state spaces, unit edge costs,
integer initial heuristics, and non-optimal tie-breaking. This journal paper adds counterexamples for exponential state spaces, larger lookahead, and spaces with non-unit edge costs
and/or non-integer initial heuristic. Convergence travel is also analyzed, and new experimental results with different tie-breaking rules are added. Finally, we include additional an
discussion around all of these issues.
The rest of the paper is organized as follows. We begin in Section 2 by formally defining
the problem at hand. We will review related work on this problem in Section 3. Our
own theoretical analysis is presented in Section 4. We then discuss the applicability of the
theory to a broader class of state spaces and algorithms in Section 5 and build concrete
counter-examples. The theoretical results are supported by a brief empirical evaluation in
Section 6. We then conclude the paper and outline directions for future work.

2. Problem Formulation
In this paper we use a common definition of the real-time heuristic search problem. We
define a search problem S as the n-tuple (S, E, s0 , sg , h) where S is a finite state of states
and E  S  S is a finite set of edges between them. S and E jointly define the search
graph which is undirected: a, b  S [(a, b)  E = (b, a)  E]. Two states a and b are
immediate neighbors iff there is an edge between them: (a, b)  E; we denote the set of
immediate neighbors of a state s by N (s). A path P is a sequence of states (s0 , s1 , . . . , sn )
such that for all i  {0, . . . , n  1}, (si , si+1 )  E. A route R is a simple path that does not
include duplicate states. Initially we assume that all edge costs are 1 (Assumption 1).
At all times t  {0, 1, . . . } the agent occupies a single state st  S. The state s0 is the
start state and is given as a part of the problem. The agent can change its current state,
that is, move to any immediately neighboring state in N (s). The traversal incurs a travel
cost of c(st , st+1 ). The agent is said to solve the search problem at the earliest time T it
arrives at the goal state: sT = sg . The solution is a path P = (s0 , . . . , sT ): a sequence of
states visited by the agent from the start state until the goal state.
The cumulative cost of all edges in the solution is the travel cost and is the primary
performance metric we are concerned with in this paper. The cost of the shortest possible
308

fiScrubbing in Learning Real-time Heuristic Search

path between states a, b  S is denoted by h (a, b). We abbreviate h (s, sg ) as h (s). The
agent has access to a heuristic h : S  [0, ). The heuristic function is a part of the search
problem specification and is meant to give the agent an estimate of the remaining cost
to go. The heuristic is integer-valued (Assumption 2). The search agent can modify the
heuristic as it sees fit as long as it remains admissible s  S [h(s)  h (s)], consistent a, b 
S [|h(a)  h(b)|  h (a, b)] and integer-valued at all times. (Admissibility and consistency
are assumed throughout the paper.) The heuristic at time t is denoted by ht ; h0 = h. The
total magnitude of all updates to the heuristic function performed by the agent between
t = 0 and t = T is called the total learning amount.
Algorithm 1: Basic Real-time Heuristic Search
input : search problem (S, E, s0 , sg , h), tie-breaking schema 
output: path (s0 , s1 , . . . , sT ), sT = sg
1 t0
2 ht  h
3 while st 6= sg do
4
st+1  arg mins0 N (st ) (1 + ht (s0 ))
5
ht+1 (st )  1 + ht (st+1 )
6
tt+1
7

T t

Numerous heuristic search algorithms have been developed for the problem described
above (e.g., Korf, 1990; Bulitko & Lee, 2006; Koenig & Sun, 2009). Most of them are
based on Algorithm 1. A search agent following the algorithm begins in the start state
s0 and repeatedly loops through the process of choosing the next state and updating its
heuristic until it reaches the goal state sg for the first time (line 3). Within the loop,
in line 4 the agent first computes the immediate neighbor that minimizes the estimated
cost of going to that neighbor (always 1 for now) plus the estimated cost of going from
that neighbor to the goal. The lookahead is 1 and thus only immediate neighbors (i.e.,
N (st )) are considered by the agent (Assumption 3). Ties among neighbors that have the
same minimal heuristic values are broken with a tie-breaking schema  that we provide
(Assumption 4), as detailed later in the paper, which is sufficiently suboptimal to prove
our results. Then, in line 5, the agent updates (or learns) its heuristic in the current state
by making it consistent with the heuristic of the neighbor picked in the previous line. The
agent then changes its current state to the neighbor (i.e., makes a move). While the general
problem allows for the heuristic to decrease, this algorithm will never decrease the heuristic
value of a state, since the heuristic is always consistent.
The problem we consider in this paper is to describe the minimum amount Tmin (S) of
the travel cost that any search agent of the type described above would necessarily incur in
finding a solution to S = (S, E, s0 , sg , h). While the exact value of Tmin (S) can intricately
depend on the particulars of a specific search problem S, we will derive a useful asymptotic
lower bound on Tmin (S). In doing so we are concerned with systematic scrubbing: an
agent is said to scrub systematically on a series of growing state spaces iff the number of
state visits asymptotically dominates the number of states in the space: Tmin (S)  (|S|).
309

fiSturtevant & Bulitko

Formally, consider a series of seach problems {S1 , S2 , . . . S2 , . . . } of progressively increasing
state spaces: |Si | = g(i) (e.g., |Si | = i2 for an open two-dimensional grid Si of i  i cells).
Then the agent scrubs systematically iff its travel time is lower-bounded by a function
Tmin (Si ) = f (i) which asymptotically dominates the state space size: f  (g). We use
the standard definition of  as asymptotic dominance: f (n)  (g(n)) iff k > 0n0 n >
n0 [kg(n)  f (n)]. Since all our functions are positive, we omit the absolute value.
We initially work under the assumption that the agent is only trying to reach the
goal once (Assumption 5), for which we measure first-trial travel, some other work has
considered convergence travel. In convergence travel the agent is teleported back to the
start state after reaching the goal, beginning a new trial, now with the updated heuristic.
The agents learning has converged when a trial does not result in further updates to the
heuristic function (Bulitko & Lee, 2006). If a systematic tie-breaking schema is used,
convergence entails that the agent will follow the same path to the goal on all subsequent
trials. First-trial travel is trivially a lower bound on the convergence travel; we will discuss
this connection in more detail later in the paper.

3. Related Work
Previous work has focused on deriving upper bounds on the agents travel cost. For instance, LRTA* (Algorithm 1) is guaranteed to reach the goal in O(|S|2 ) steps (Ishida &
Korf, 1991). This follows from the analysis of MTS (Ishida & Korf, 1991) if the targets
position is fixed. Examples have been provided to show that the exact bound, |S|2 /2|S|/2,
is tight (Edelkamp & Schrodl, 2012).1 The analysis has been extended to a class of reinforcement learning algorithms, of which LRTA* is a special case (Koenig & Simmons, 1992,
1993, 1996). For state-based value-iteration algorithms, such as the search algorithm listed
above, the upper bound is still O(|S|2 ).
There has also been substantial work on analyzing algorithms with backtracking moves,
introduced with SLA* (Shue & Zamani, 1993a, 1993b) and the more general SLA*T (Shue,
Li, & Zamani, 2001; Zamani & Shue, 2001). Such algorithms behave in the same way as
our algorithm until the total learning exceeds a certain threshold T . After that the agent
backtracks to the previous state whenever it raises a heuristic value. It was shown by Bulitko
and Lee (2006) that the travel cost of SLA*T is upper bounded by h (s0 , sg ) + T where T
is the threshold (learning quota). However, this upper bound holds only if the path being
built by SLA*T is processed after every move and all state revisits are removed from it.
A problem-specific analysis of the minimum learning required to prove the optimal path
was described by Sturtevant et al. (2010) but they did not consider the first-trial performance. They provided a proof sketch that agents that lookahead farther will not converge
to an optimal solution more quickly. Empirical results suggested that algorithms that look
farther ahead have better convergence performance primarily because they perform less
learning over and above the minimum required.

1. The chapter containing these results is authored by Koenig.

310

fiScrubbing in Learning Real-time Heuristic Search

4. Our Analysis
Our goal in this paper is to derive a non-trivial asymptotic lower-bound on the amount of
travel any search agent that uses Algorithm 1 will need to perform to reach the goal state.
We will achieve this goal in stages. Section 4.1 illustrates how tie-breaking can influence
best- and worst-case performance, using two simple examples. We present a high-level
overview of our derivation in Section 4.2 and then detail individual steps in Sections 4.3
through 4.6. We then apply the analysis to the case of polynomially growing state spaces in
Section 4.9 and discuss its implications on state revisits (i.e., scrubbing). These results
require all of our assumptions (1-5). After this presentation is completed, in Section 5 we
discuss extensions of this work to state spaces with non-unit edge costs and larger lookahead.
In Appendix A we consider exponential state spaces.
4.1 Intuition
Consider the five-state grid world in Figure 1. The goal is state 5 on the far right. The
agent starts out in state 2. At each state the agent examines the heuristics of its immediate
neighbors, to the left and right, and goes to the neighbor with the lowest heuristic. If the
neighbors heuristic values are identical then, for the time being, we assume that ties are
broken to the neighbor on the right. This is a favorable or fortunate tie-breaking rule for this
example because it moves the agent towards the goal when the heuristic is not sufficient to
do so by itself. A tie-breaking schema that breaks ties away from the goal, in this example,
would be unfavorable or unfortunate. In this example we temporarily break Assumption 4
of an agent with unfortunate tie breaking. We use several variations on this problem to
illustrate the importance of tie-breaking on solution travel cost.

Figure 1: A one-dimensional grid world of five states numbered at the top. The agent A
is located in state 2 and has two available actions shown in the figure.
In Figure 2 we plot the initial heuristic values of each state in a ten-state version of this
grid, where the goal is on the far right and the agent starts out on the far left. The initial
heuristic values are shown with dark bars, and the learned updates to the heuristic are light
bars stacked on top. Each plate shows a single time step. The agents current position is
indicated with an A. After reaching time step 4, the agent can continue straight to the
goal without further heuristic updates. Due to the fortunate tie-breaking schema in this
particular example, the total amount of learning (i.e., the total area of light bars) is 8 and
the travel cost is 9. Scaling this example to 2k states, the total amount of learning will be
2(k  1) and the path produced will have the optimal cost of 2k  1, with no state revisits.
Thus, the total learning in this example grows linearly with the state space size and no
systematic scrubbing is observed (i.e., Tmin (S) 6 (|S|)).
However, tie breaking can adversely affect the agents performance, and in general there
is no guidance available in a state space to allow favorable tie-breaking. Consider the search
problem in Figure 3 with 13 states. In this example, each tie is broken away from the goal
311

fiSturtevant & Bulitko

Time = 0

Time = 1

A

Time = 2

A

A

Time = 3

Time = 4

A

A

Figure 2: Heuristic learning with favorable tie breaking. The ten states of the problem
are along the horizontal axis. The vertical axis shows the value of the heuristic
function per time step. The darker bars are the initial heuristic values. The
lighter bars are the increases due to learning. The initial heuristic is at the top.
The agents current state is shown with an A.

(i.e., to the left). The travel cost is now 20 and the total amount of learning is 18 indicating
some revisits of the 13 states by the agent, such as at time steps 3, 7, and 9. Scaling this
search problem, the total amount of learning will be asymptotically quadratic in the number
of states. The amount of heuristic learning per move is at most 2 which means that the travel
cost will also be asymptotically quadratic in the number of states and number of revisits
per state will increase with the state space size. Thus, Tmin (S)  (|S|) as |S|2  (|S|)
and we have systematic scrubbing. The difference between these two cases is asymptotically
significant. While favorable tie-breaking can be achieved in specific examples, it cannot be
always guaranteed in general.
4.2 Analysis Overview
Our goal is to quantify the travel required by any agent of the type described in Section 2
to find a solution. The examples in the previous section demonstrated that the travel
cost can depend on the tie-breaking schema used by the agent. Since it may not always
be possible to design a favorable tie-breaking schema for a given problem (or series of
problems), we consider the agents performance with sufficiently suboptimal tie-breaking
schemas (Assumption 4) and develop a meaningful lower bound on the amount of travel.
Our lower bound is asymptotic in the number of states and, unfortunately, demonstrates
that under common conditions the agent will necessarily have to revisit states many times
 an undesirable phenomenon known as scrubbing in real-time heuristic search.
We present the high-level argument immediately below and then detail each step individually in the subsequent sections. First, due to consistency of the heuristic and a bounded
lookahead, the learning performed in each step is constant-bounded. Thus, an asymptotic
lower bound on the learning required to reach the goal is also an asymptotic lower bound
on travel distance (Section 4.3). We show that with a sufficiently suboptimal tie breaking
schema an agent traveling from sa to sb must raise the heuristic of sa to at least that of
312

fiScrubbing in Learning Real-time Heuristic Search

Time = 1

Time = 0

A

A

A

Time = 6

A

A

Time = 10

A

Time = 11

A

A

Time = 12

Time = 7

A

A

Time = 9

Time = 8

A

A

Time = 5

Time = 4

Time = 3

Time = 2

Time = 13

A

A

Figure 3: Heuristic learning with unfortunate tie breaking.

sb (Section 4.4). Given a search graph, a current location, and a goal, we identify a lower
bound on the maximum heuristic value h that the agent must encounter when traveling to
the goal (Section 4.5). Since the heuristic is consistent at all times we use h to compute the
minimum amount of learning (Section 4.6).
We then apply the argument to polynomial state spaces where the number of states
within distance r from a given state grows as (rd ) (e.g., quadratically on commonly used
two-dimensional navigation maps with d = 2).2 In such spaces we show that, given an
appropriately inaccurate initial heuristic, the amount of learning necessarily performed by
the agent can grow as (rd+1 ) which asymptotically dominates the number of states (rd ).
Since learning per step is constant-bounded, the amount of travel will also asymptotically
dominate the number of states. This result indicates that any real-time heuristic search
agent of the type introduced above will necessarily scrub systematically (Corollary 2).
4.3 Learning per Step is Constant Bounded
Here we provide a bound on the learning required to solve a problem the first time  the
cumulative updates to the heuristic function from t = 0 until t = T . In this section we
begin by showing that the learning per step is constant-bounded. This will imply that a
lower bound on learning is also a lower bound on movement.
2. We use the standard definition of  as bounded above and below: f (n)  (g(n)) iff k1 > 0k2 >
0n0 n > n0 [k1 g(n)  f (n)  k2 g(n)], of  as bounded below: f (n)  (g(n)) iff k > 0n0 n >
n0 [kg(n)  f (n)] and of O as bounded above: f (n)  O(g(n)) iff g(n)  (f (n)).

313

fiSturtevant & Bulitko

Lemma 1 The total change in heuristic values during each learning step of Algorithm 1 is
constant bounded independently of the number of states in the state space.
Proof. Algorithm 1 updates the heuristic of a state st in line 5 of the pseudo code based
on the heuristic of a neighboring state st+1 . Because st+1 is an immediate neighbor of st
and because the heuristic is consistent, |ht (st )  ht (st+1 )|  h (st , st+1 ) = 1. By definition,
the new heuristic for st is 1 + ht (st+1 ). Thus, the heuristic of st can increase by at most 2.
Since st is the only state that has its heuristic updated, the maximum change in heuristic
values at each time step is 2, which is constant bounded. 2
We now measure the learning necessary to move between different locations in the world.
4.4 Maintaining a Non-increasing Heuristic Slope
In this section we will prove that there exists a tie-breaking schema,  , that forces the
agent to maintain non-increasing heuristic values for states along its route, which we call
the non-increasing heuristic slope property. While better and worse tie-breaking schemes
may exist for specific problems,  is sufficiently suboptimal to prove our main result.
As defined earlier, the agents route is a simple path from the start state s0 to the
current state st with any loops removed. For instance, if by the time t = 5 the agent has
traversed the path P = (s0 , A, B, A, C, D) then its route R = (s0 , A, C, D). A heuristic
profile is the vector of heuristic values along the route. If the heuristic profile along this
route is {4, 3, 2, 2}, the non-increasing property holds. If the heuristic profile is {2, 3, 4, 3},
the non-increasing properly does not hold.
We will construct a tie-breaking schema such that whenever the non-increasing property
is violated by raising the heuristic at the current state, the agent will be forced to backtrack,
removing the offending state from its route. To illustrate, consider Figure 3. At time step
5 the agent raises the heuristic of its current state and violates the non-increasing property
(the new, larger heuristic value is shown at time step 6). The agent therefore backtracks,
making a move to the left, which removes the offending state from its route. Only after
reaching the state at time step 8 can the agent once again move forward while satisfying
the property.

ht+1

ht

st

1

st

st

1

st

s0

Figure 4: Raising h(st ) above h(st1 ) allows a tie-breaking schema to force the agent to
backtrack.

314

fiScrubbing in Learning Real-time Heuristic Search

Lemma 2 (Forced Backtracking) Consider an agent in a non-goal state st at time t.
Its current route is R = (s0 , . . . , st1 , st ). Then there exists a tie-breaking schema  such
that after updating the heuristic of st from ht (st ) to ht+1 (st ), if the updated value raises
the heuristic profile so that it ceases to be non-increasing, then the agent will backtrack to
its previous state st1 :
ht+1 (st1 ) < ht+1 (st ) = st+1 = st1 .

(1)

Proof. First, as the agent moved from st1 to st , it must hold that ht (st1 ) = 1 + ht (st ),
due to line 5 of Algorithm 1. This is shown in the left side of Figure 4. Due to a consistent,
integer-valued heuristic and unit edge costs, the only way the heuristic can be increasing
along the route after learning is if ht+1 (st ) = ht (st1 ) + 1, shown on the right side of
Figure 4. In this case, the update must have come from some state s0  N (st ) which is a
state with the lowest heuristic among st s neighbors (Figure 4) with ht (s0 ) = ht (st1 ). Even
if s0 is not the same as st1 , it has the same heuristic value. Thus, a tie-breaking schema
will be able to break the ties towards st1 , making the agent backtrack from st to st1 and
maintaining the non-increasing heuristic. This tie-breaking schema is  . 2
Since backtracking removes the offending state from the agents route, we have the
following corollary.
Corollary 1 (Heuristic Slope) At any time during the search, there exists a tie-breaking
schema such that the heuristic along the agents route R = (r1 , . . . , rn ) is non-increasing:
ht (r1 )  ht (r2 )      ht (rn ).

(2)

Proof. We prove this claim by induction on route length n. For n = 1 inequality (2)
trivially holds. Suppose it holds for the route of length n then when the (n + 1)th state
is added to the route, by Lemma 2 it must hold that the heuristic of the added state is
less than or equal to the heuristic of the routes previous end (otherwise the agent would
backtrack and the route would not grow). 2
4.5 A Lower Bound on the Maximum Heuristic Encountered
Assume that at time t the agent added the state sb to its route. This means, according to
Corollary 1, that ht (sa )  ht (sb ) for any state sa already in the route. Informally, before the
agent passes through a state with a high heuristic, it must first raise all previous states in
its route to have at least equally high heuristics. Since heuristic values in Algorithm 1 never
decrease during learning (due to consistency), this also means that ht (sa )  h0 (sb ) which
implies that the agent must have already raised the heuristic of sa by at least h0 (sb )h0 (sa ),
assuming this term is positive. We are interested in identifying the states in a particular
problem which maximize the difference h0 (sb )  h0 (sa ), as they can be used to bound
learning. In this section we show how to find these states. In particular, sb will be the state
with largest heuristic that the agent must encounter en route to the goal. Informally, we
are providing a general definition of a local minima and then proving that the agent must
learn its way out of the local minima by increasing heuristic values.
Figure 5 illustrates this concept with pathfinding on a video-game map. Suppose that
the agent is in state sa and is trying to reach state sg . We observe that the agent must pass
315

fiSturtevant & Bulitko

through one of the states within the bottleneck C1 before reaching the goal. Similarly, the
agent must also pass through one of the states in C2 before reaching the goal.

C1
sg

C3
C4

sa

C2
Figure 5: A two-dimensional pathfinding search problem with the goal state labeled sg and
the start state labeled as sa . C1 , C2 , and C3  C4 are all cut sets.

We say that a set of states C is a cut set with respect to the states sa and sg iff sa 
/ C,
sg 
/ C and all possible routes R = (sa , ..., sg ) have a non-empty intersection with C. In
Figure 5 the sets C1 , C2 and C3  C4 are three different cut sets with respect to sa and
sg . Given two states sa and sg , we denote the set of all their cut sets as C(sa , sg ). Thus
C1  C(sa , sg ), C2  C(sa , sg ) and (C3  C4 )  C(sa , sg ).
We use the notion of cut sets to derive a lower bound on the maximum heuristic value
seen by an agent en route to the goal. For the map in Figure 5, assuming the standard
straight-line heuristic, C1 is not the best cut set because the initial heuristic values of its
states will be relatively small. C2 is better because its initial heuristic values will be larger.
C3  C4 is an even better cut set as it will have the largest minimum initial heuristic values
among C1 , C2 , C3  C4 .
In general, we can find the best lower bound by considering all cut sets and choosing
the one with the maximal minimal initial heuristic:
h(sa , sg ) =

max

min h0 (s)

CC(sa ,sg ) sC

(3)

From this definition and the definition of a cut set it follows that an agent will necessarily
travel through some state s such that h0 (s)  h(sa , sg ). Thus, h(sa , sg ) is a useful lower
bound on the maximum heuristic encountered along a route from sa to sg .
316

fiScrubbing in Learning Real-time Heuristic Search

h(s0 , sg )
h

h0
sg

s0 s

S

Figure 6: An illustration of s , h and h(s0 , sg ).

Note that the state sa can be an arbitrary state in S. Thus, if R(S) is the set of states
in a route generated by the agent while solving the problem S then we can define:
h
i
h = max h(s, sg )  h0 (s)
(4)
sR(S)
h
i
s = arg max h(s, sg )  h0 (s) .
(5)
sR(S)

If there are several states for which h is maximized, s can be set to any of them.
From here it follows that s is a state on the agents route to goal whose heuristic value
has to be raised by at least h :
h  hT (s )  h0 (s ).

(6)

Figure 6 illustrates this with the heuristic profile of a simple one-dimensional state space
where every state between the start state s0 and the goal state sg forms a single-state cut
set. Hence, h(s0 , sg ) is just the highest initial heuristic on the agents route. Thus, the
heuristic of all states to the left of the peak will have to be raised to at least h(s0 , sg ). The
maximum amount of learning, h , happens in the state s .
While all states along a route R(S) may not be known a priori, R(S) must contain the
initial state s0 which, given (4), means that:
h  h(s0 , sg )  h0 (s0 ).

(7)

Furthermore, it is often possible to identify a state in R(S) which yields a lower bound on
h that is higher than h(s0 , sg )  h0 (s0 ). To illustrate: the h shown in Figure 6 is strictly
greater than h(s0 , sg )  h0 (s0 ). In a more practical example, shown in Figure 8, the octile
distance h0 on the eight-connected grid leads to h(s0 , sg )  h0 (s0 ) = 0. But, in practice,
an agent will move to the corner first, allowing us to identify a different state as s . We
analyze this example in detail in Section 4.8.
4.6 Minimum Learning and Minimum Travel Required Overall
We have now established that there exists a state s along the agents route to goal whose
heuristic the agent will necessarily raise by at least h . Per Lemma 1, the amount of
learning per move is constant-bounded, so the number of visits to the state s is (h ).
317

fiSturtevant & Bulitko

Consistency of the heuristic allows us to derive even stronger lower bounds on the total
amount of learning and travel cost. Indeed, raising the heuristic of s by h implies that
the heuristic values of many other states have to be raised as well, contributing to the total
amount of learning, travel and state re-visits.
Specifically, since at any time t  {0, . . . , T } the heuristic ht is consistent, the heuristic
value of any state n  S is upper and lower bounded with respect to an arbitrary state
m  S, according to its distance from it:
ht (m)  h (m, n)  ht (n)  ht (m) + h (m, n).

(8)

Thus, when the agent reaches the goal at time T , the heuristic of any state in the state
space is lower-bounded according to the distance from s :
n  S [hT (n)  hT (s )  h (s , n)] .

(9)

The initial heuristic is consistent and hence upper-bounded:
n  S [h0 (n)  h0 (s ) + h (s , n)] .

(10)

For each state n  S, the difference between the left sides of (9) and (10) is at least as large
as the difference between their right sides:
hT (n)  h0 (n)  hT (s )  2h (s , n)  h0 (s )
which, due to (6), becomes:
 h  2h (s , n)

(11)

We sum the right side of (11) over all states n  S and, since no heuristic value decreases
during learning, we derive the following lower bound on the total amount of learning:
Lmin (S) 

X

max{0, h  2h (s , n)}.

(12)

nS

This is illustrated in Figure 7 where consistency of the heuristic determines the diamond
around h . The area of the diamond is the right side of inequality (12). As the amount of
learning per move is constant bounded (Section 4.3) we can also derive a lower bound on
the amount of travel:
Tmin (S)   (Lmin (S)) .

(13)

Note that in this one-dimensional example the non-increasing heuristic slope property
allows to derive an even higher lower bound on the necessary amount of learning. Specifically, by the time the agent reaches the goal sg , all heuristic values to the left of the peak
have to be raised to at least the level of the dotted line. The filled-in volume is clearly
greater than the area of the diamond in the figure. This indicates a possible direction for
future work  finding a lower bound more aggressive than the one in inequality (12) that
we use in the rest of the paper.
318

fiScrubbing in Learning Real-time Heuristic Search

h

h0
sg

s0 s

Figure 7: A lower bound on Lmin determined by s , h .

4.7 General Conditions for Systematic Scrubbing
Recall that our definition of scrubbing is that the number of state visits asymptotically
dominates the number of states in the space: Tmin (S)  (|S|). Thus, one way to establish
systematic scrubbing on a series of search problems is to compute or, at least, estimate the
sum in Equation 12 and show that it asymptotically dominates the number of states |S|.
Together with the link between Tmin and Lmin given by Equation 13, this would establish
systematic scrubbing.
Generally speaking, the sum may depend intricately on the search problem structure
and the initial heuristic. Thus, from here on we will proceed by analyzing two special cases.
4.8 Special Case: The Corner Map
We cannot make general statements about single problem instances, so instead we parameterize problem instances by their size, allowing us to describe how they scale as the map
size grows. In this case we measure the size by the radius or the length of one edge of the
map. Consider a series of search problems known as the corner map (Figure 8).
s0

s

s

s

n=5
sg

Figure 8: The corner map analyzed by Sturtevant et al. (2010).
This map is parameterized by n: a problem instance Sn for n > 3 has |Sn |  O(n2 )
states. This is a two-dimensional grid where the agent can occupy any vacant cell (white in
the figure). The agent can move to any of its four cardinal neighbors unless they are blocked
by an obstacle (dark grey cells in the figure), all edges have unit cost, and we assume the
Manhattan distance as the initial heuristic h0 . Later in the paper we revisit this example
with diagonal, non-unit-cost moves and an octile distance heuristic.
319

fiSturtevant & Bulitko

If the agent starts in the corner created by the obstacles, that corner state will be s .
Using the analysis in the previous sections, h in Equation 3 has the value of n + 1 in the
two states labeled s in the figure. Correspondingly, the state s defined in Equation 5 and
also shown in the figure has the initial heuristic value of 4 (for any n); this will be raised
to h before the goal state is reached. Thus, h = n + 1  4 = n  3. The lower bound on
Lmin (Inequality 12) becomes:
X
Lmin (Sn ) 
max{0, n  3  2h (s , s)}.
(14)
sSn

Looking at the structure of the corner, we can re-write the right side of (14) by summing
over i = h (s , s) and multiplying by the number of states with each value of h (s , s).
There is one state (s ) with h (s , s) = 0, two states with h (s , s) = 1, and i + 1 states
with h (s , s) = i. For odd values of n, we re-write the sum in (14) as:3
n3

Lmin (Sn ) 

2
X

(i + 1)(n  3  2i) =

i=0

(n  3)(n  1)(n + 1)
.
24

(15)

(n  2)(n  1)n
.
24

(16)

For even values of n the result is:
n4

Lmin (Sn ) 

2
X

(i + 1)(n  3  2i) =

i=0

In either case, the sum, as a function of n, is in the class (n3 ) which puts the learning
amount Lmin in (n3 ). As the amount of learning per move is constant bounded, Tmin 
(n3 ). The number of states on a two-dimensional n  n corner map is O(n2 ) which
is asymptotically dominated by Tmin proving that the agent will scrub systematically on
{S1 , S2 , . . . }.
Specific properties of the corner maps allowed us to derive a complexity class for the
sum in (12) as well as the total number of states on each map. In the following section we
analyze a broader class of search spaces.
4.9 Special Case: Locally Isotropic Polynomial State Spaces
The lower bounds on the amount of learning (12) and total travel (13) hold for a search
problem S. However, the bounds do not explicitly reference the number of states in S and
thus do not immediately allow us to correlate the amount of travel to the number of states
in order to make a claim about state revisitation. To do so we investigate ways of computing
the number of states (r, S, E) which are precisely distance r away from the state s :
(r, S, E) = |{s  S | h (s, s ) = r}|.

(17)

Generally speaking, (r, S, E) depends on the topology of the search graph (S, E) and
can be arbitrarily complex. However, if the state space is isotropic around the state s
3. This is still a lower-bound for this example because we are not considering that the full path to the goal
has to have its heuristic values raised to h(s). Our analysis assumes that only the first state will have
its heuristic raised.

320

fiScrubbing in Learning Real-time Heuristic Search

then the number of states distance r away from s does not depend on the direction and
can be described simply as (r). The term isotropic is usually used to refer to the entire
state space; we use the term locally isotropic to refer to states spaces that are isotropic in
a region within radius r of s .
Then the sum in inequality (12) can be alternatively computed as an integral over the
shortest distance r between the state s and all other states:
X

Z



max{0, h  2h (s , n)} =

h
2

(r)(h  2r)dr.

(18)

0

nS

We can further simplify the computation of the integral (18) for polynomial state spaces
which extend from s for a distance of at least h /2; that is, are locally isotropic. These
are state spaces in which
(r) = rd1

(19)

for r  [0, h /2]. For instance, in two-dimensional navigational maps which are also locally
isotropic to at least radius h /2 around the state s , the degree of the polynomial is d = 2
and (r) = r21 = r, r  [0, h /2]. Note that this is a theoretical abstraction because
states in real-life maps (e.g., Figure 5) are not always locally isotropic; the maps have an
asymmetric structure and may not stretch far enough around s . Also, they may not
be polynomial as the obstacles may non-uniformly reduce the number of available states
distance r away from any state.
Substituting (r) = rd1 in (18), we get:
Z

h
2

(r)(h  2r)dr =

0
h
2

Z

rd1 (h  2r)dr =

0

Z
h 
0

h
2

r

d1

Z
dr  2

h
2

rd1 rdr =

0

fi h
fi h
h  d fifir= 2
2 d+1 fifir= 2
r fi
r fi

d
d+1
r=0
r=0

d


h  h
2
h d+1

d
2
d+1
2


d+1
(h )

(h )d+1
d2d
(d + 1)2d


 1
1

(h )d+1
2d d d + 1

(h )d+1
2d d(d + 1)

=
=
=
=
 ((h )d+1 ), h  .

(20)

Combining (12), (18) and (20) and we conclude that for locally isotropic polynomial spaces
of dimension d that extend for distance at least h /2 around the state s , the minimum
321

fiSturtevant & Bulitko

amount of total learning is lower-bounded as:


Lmin (S)   (h )d+1 , h  .

(21)

As the amount of learning per step is constant-bounded, the same asymptotic lower bound
applies to the travel cost:


Tmin (S)   (h )d+1 , h  .
(22)
Note that under our assumptions, the number of states within the radius r of the state s
grows as:
Z r
Z r
xd1 dx  (rd ), r  .
(23)
(x)dx =
0

0

Now, consider an infinite series of growing search problems {S1 , S2 , . . . }. Suppose each
search problem Si is such that the corresponding state space Si is locally isotropic and
polynomial to radius ri around the corresponding state s . Assume also that the degree
of the polynomial is d  1 and that the radii of the search problems monotonically and
unboundedly increase with i:
r1 < r2 < . . .

(24)

ri   when i  .

(25)

From (23), it follows the state space size is asymptotically lower-bounded by rid :
|Si |  (rid ), i  .

(26)

Suppose also that the state space Si is asymptotically upper-bounded by rid as well:
|Si |  O(rid ), i  .

(27)

Further, suppose the initial heuristic for search problem Si is such that the corresponding
h grows linearly with ri . Finally suppose that the local isotropicity expands far enough
from s : ri  h /2. Together these two conditions are:
h /2  ri  h

(28)

where   1/2 is a fixed constant.
Corollary 2 (Systematic Scrubbing). Given the assumptions in equations 24-28, any
real-time heuristic search algorithm that fits the basic framework formulated earlier in this
paper (Assumptions 1-5) will necessarily scrub systematically.
Proof. As the state space radius ri increases, h for the problem Si will increase
linearly with it due to (28). The minimum amount of travel Tmin asymptotically grows as
a polynomial of degree d + 1:


Tmin (Si )   d+1
,i  
(29)
h


Tmin (Si )   rid+1 , i  
(30)
322

fiScrubbing in Learning Real-time Heuristic Search

due to (22), (25) and the linear relation between h for Si and its radius ri (28).
At the same time, the number of states in the state space will asymptotically grow as a
polynomial of degree d of the radius:
|Si |  (rid ), i  

(31)

due to (26) and (27). Combining (30) and (31) we get that the necessary amount of travel
assymptotically dominates the state space size:
Tmin (Si )  (|Si |), i  

(32)

and the agent will scrub systematically. 2
4.10 Discussion
To informally summarize the results thus far, we have shown that we can systematically
measure the heuristic learning that must be performed in a single state by looking at the
maximum heuristic difference encountered between that state and the goal. Under our
assumptions the total learning required around this state asymptotically dominates the
number of states in the state space. Since the learning per step is constant-bounded, agents
will be forced to scrub.
This analysis, in its most general form (Section 4.9), is based on several important
assumptions. First, we assume a conservative tie-breaking rule that prefers to re-visit states
over exploring new ones. Second, we assume that the state space is locally isotropic around
s . Third, we assume that, in general problems, the heuristic error is growing proportional
to the radius of the state space. Finally, we assume that the agent has 1-step lookahead,
that edges have unit cost, and that heuristic values are integer valued.
We now consider these assumptions in more detail. First, in our experimental results
we will explore several tie-breaking rules, as well as aggressive tie-breaking that prefers
to move to larger g-costs over small g-costs. These results will show that, while a better
tie-breaking rule can improve best-case performance, it can also have a large impact on
worst-case performance.
Second, our definition of systematic scrubbing holds for any problem for which the total
learning (20) grows asymptotically faster than the state space (23). We only proved that
this must occur in polynomial state spaces that are locally isotropic around s ; it would
be interesting future work to develop a broader range of models that have this property.
Experimental results are provided giving evidence that this does occur in practice.
Third, if the heuristic error (h ) in a series of growing state spaces is constant, then
no scrubbing behavior will be guaranteed by our proofs. This requires a highly accurate
heuristic and cannot be assumed in general. It is an interesting open question how the
heuristic error grows on different classes of problems, whether it is linear, logarithmic, or
some other function of the size or radius of the state space. Such analysis is clearly domain
and problem dependent, but even if the heuristic consistently underestimates by just 10% in
locations that are locally isotropic, and is perfect in others, our analysis holds (Corollary 2).
Finally, we will address larger lookahead and non-unit edge and heuristic costs in Section 5.
323

fiSturtevant & Bulitko

4.11 Special Case: Exponential State Spaces
It is not common for real-time agents to traverse exponential state spaces, so we place
our analysis of exponential state spaces in Appendix A. The techniques we used to show
systematic scrubbing in polynomial spaces that are locally isotropic are insufficient to do so
in exponential spaces that are locally isotropic. However, we also do not have a proof that
systematic scrubbing will be absent. Thus, more analysis is needed to make a conclusive
statement on scrubbing in exponential state spaces.

5. Generalizing the Theory
So far, we have proven that systematic scrubbing (Corollary 2) holds for a basic agent
design and a simple state space. In this section we investigate the extent to which the
result is generalizable. We independently relax Assumptions 1 & 2 (unit edge costs and
integer heuristic values) and Assumption 3 (one-step lookahead) showing, with counterexamples, that Corollary 2 does not directly generalize. Some of these counter-examples
contrast with experimental results which suggest that scrubbing does occur in practice. It
is an open problem to succinctly describe conditions for asymptotic scrubbing in these more
complicated scenarios.
Relaxing the single trial assumption (Assumption 5), however, shows that scrubbing is
expected when learning until convergence.
5.1 Non-unit Edge Costs and Non-Integer Initial Heuristics
In this section we address whether Corollary 2 generalizes to search problems with non-unit
edge costs and/or heuristic values. In particular, we relax the assumption of unit edge costs
(Assumption 1), replacing it with an assumption of constant-bounded edge costs. We also
relax the assumption of integer heuristic values (Assumption 2).
We provide a set of counter-examples to systematic scrubbing: specifically constructed
search problems where an agent running LRTA* can move from a region of low heuristic
values to higher heuristic values without maintaining a non-increasing heuristic slope, which
was the key assumption to our previous proofs.

0

c

s0

1

c

s1

2

c

s2

1

c

s3

0
s4

Figure 9: Example chain of states for n = 2.
Consider a chain of 2n + 1 states, illustrated in Figure 9 for n = 2,
{s0 , s1 , s2 , . . . , sn , . . . , s2n } with s0 the start state and s2n the goal. Suppose the initial
heuristic values are monotonically increasing along the chain until state sn (h(s1 ) < h(s2 ) <
. . . h(sn )), and monotonically decreasing afterwards. Let h(i) = i for i  n and h(i) = 2ni
for i > n. This set of growing state spaces is well defined for all n > 0 and, if c = 1, meets
the conditions of Corollary 2. Thus, the example will cause our previously described agent
to scrub.

2
s-2

1.0

1
s-1

1.0

0
324
s0

1.0

1
s1

1.0

1
s2

1.0

2
s3

1.0

2
s4

1.0



1.0

n
s2n

1.0

n-1

s2n+

fiScrubbing in Learning Real-time Heuristic Search

However, if we allow the heuristic values to be non-integer or the edge costs to be
non-unit then scenarios exists in which the agent will climb the heuristic grade without
scrubbing. For instance, if we change the edge costs to be c = 1.5 instead of 1.0, we get
the behavior shown Figure 10. Here the agent starts at state 0; the heuristic of this state
is updated to be 2.5 which is the edge cost (1.5) plus the h-cost of the neighbor (1.0). The
agent then moves to state 1. Notice here that the heuristic to the left (2.5 in state 0) is
slightly larger than the heuristic to the right (2.0 in state 2). After learning, the heuristic
in state 1 is updated to 3.5, and the agent continues to state 2. In the last step shown here,
the agent will begin to follow the gradient to the goal in state 4. Thus, with these edge costs
the agent has no choice but to move between the start and the goal without scrubbing.
4

4

4

4

3

3

3

3

2

2

2

2

1

1

1

A
0
0

1

A
1.5

1

1.5

2

1.5

3

1.5

0
4

0

1.5

1

A
1.5

2

1.5

3

1.5

0
4

0

1.5

1

1.5

2

A
1.5

3

1.5

0
4

0

1.5

1

1.5

2

1.5

3

1.5

4

Figure 10: The agent climbs a heuristic grade without scrubbing.
The intuition can be formalized for a chain of 2n states as follows:
Lemma 3 Consider a chain of states {s0 , s1 , s2 , . . . , s2n } with s0 being the starting state
and s2n being the goal. Suppose the initial heuristic values are monotonically increasing
along the chain until some intermediate state sn : h(s1 ) < h(s2 ) < . . . h(sn ) forming a
heuristic grade of n states. Assume that after sn the heuristic is monotonically decreasing.
Suppose the heuristic grade is of a constant slope i  {0, . . . , 2n  1} [|h(si )  h(si+1 )| = ]
and all edge costs are uniform i  {0, . . . , 2n  1} [c(si , si+1 ) = c]. If the slope  is strictly
below the edge cost c then the agent with a lookahead of 1 will climb the grade to state n
without any scrubbing, regardless of the tie-breaking schema, and reach the goal in (n)
steps.
Proof. The proof is by induction on the state number k. We will show that whenever
the agent is in the state 0  k  n  1 it will set the heuristic of sk to be greater than
the heuristic of sk+2 (i.e., hnew (sk ) > h(sk+2 )) and move to state sk+1 . We call this the
slope-edge property: the slope growing slower than the edges causes the agent to climb the
slope without backtracking.
Base case: k = 0. The agent will increase h(s0 ) to h(s1 ) + c and move to s1 as its only
choice. Starting with c >  we derive:
c > 

(33)

hnew (s0 )  h(s1 ) > h(s2 )  h(s1 )

(34)

hnew (s0 ) > h(s2 ).

(35)

which proves the slope-edge property for k = 0.
Inductive step. Suppose the slope-edge property holds for k = j, j  n  3. Then
it follows that the agent is now in the state sj+1 and hnew (sj ) > h(sj+2 ). From this we
325

fiSturtevant & Bulitko

immediately conclude that the agent will make its next move to sj+2 . Before the move it will
set hnew (sj+1 ) to min {hnew (sj ) + c, h(sj+2 ) + c} = h(sj+2 ) + c. Since h(sj+3 ) = h(sj+2 ) + 
and c > , we conclude that hnew (sj+1 ) > h(sj+3 ) which makes the slope-edge property
hold for k = j + 1.
Once the agent reaches state n, the heuristic is monotonically decreasing towards the
goal, and the agent will follow the slope downward until reaching the goal. 2
The lemma hinges on the interplay between  and c. The interplay can be satisfied by
either non-unit edge costs or non-integer valued initial heuristic. Our example above uses
c = 1.5 and  = 1.0, but the lemma holds for c = 1.0 and  = 0.5, as well as many other
values of c and .
4.0 4.5 5.0 5.5 6.0 7.0 8.0 9.0

4.0 4.5 5.0 5.5 6.0

A 8.0 9.0

3.0

6.5 7.5 8.5

3.0

8.0 7.5 8.5

2.0

6.0 7.0 8.0

2.0

7.5 7.0 8.0

1.0

5.5 6.5 7.5

1.0

7.0 6.5 7.5

G

A 6.0 7.0

G

6.5 6.0 7.0

Figure 11: The agent climbs a heuristic grade without scrubbing (intermediate steps omitted).
This situation can happen in practice, as illustrated in Figure 11. The state with the
agent is marked A and the state with the goal is marked G. All other states are labeled with
their initial heuristic value. Diagonal edges have cost 1.5 and the heuristic is octile distance,
which is the shortest path with no obstacles and only diagonal and cardinal movements
allowed. A real-time heuristic search agent with a lookahead of 1 will walk up along the
wall because the heuristic increases by only  = 0.5 per step while the edge costs along
the agents path are 1. As a result, the agent will walk directly out of the local heuristic
minimum and continue to the goal, traversing a shortest path with no scrubbing.
The example, however, is somewhat fragile. If we extend the map downward, the agent
will no longer necessarily walk directly to the goal. We illustrate this in Figure 12. In
this case the agent begins walking upwards as before, but when it reaches the diagonal line
shown in the right side of the figure, the heuristic no longer changes by 0.5 per step. Because
the slope-edge property is no longer maintained, the agent can no longer move against the
heuristic slope. At this point the agent will have four choices of where to move next (shown
as four gray cells in the figure) and, with unfavorable tie breaking will backtrack. Running
this example shows that, in this particular case, typical scrubbing behavior then follows.
5.2 Larger Lookahead
In this section we show that equipping the agent with a larger lookahead, breaking Assumption 3, in the style of LSS-LRTA* (Koenig & Sun, 2009) can eliminate systematic scrubbing
even in a search problem with unit edge costs.
326

fiScrubbing in Learning Real-time Heuristic Search

6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5

6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5

5.0

7.5 8.5 9.5

5.0

A 8.5 9.5

7.0 8.0 9.0

4.0

8.5 8.0 9.0

3.0

6.5 7.5 8.5

3.0

8.0 7.5 8.5

2.0

6.0 7.0 8.0

2.0

7.5 7.0 8.0

1.0

5.5 6.5 7.5

1.0

7.0 6.5 7.5

G

A 6.0 7.0

G

6.5 6.0 7.0

4.0

 = 1.0
 = 0.5

Figure 12: The agent cannot climb the heuristic grade without scrubbing.

First, we redefine our agent algorithm to allow for larger lookahead from Algorithm 1 to
Algorithm 2. In the terminology of A* the algorithm works as follows: as long as the goal
is not reached (line 3) the agent expands the search space from its current state st . The
expansion is carried out via an OPEN set which is seeded with the current state and at each
moment of time contains all states that have been generated but not expanded. To expand
a state means to generate its neighbors. Such neighbors are placed on the OPEN set unless
they are already in the CLOSED set (line 9). The CLOSED set contains all states that
have been expanded (line 8). States in the OPEN set are expanded in the order of their
minimum f = g + h cost, where the g-cost is the distance from the agents current state st
(line 7). The number of expansions per step is at most l, an algorithm parameter (line 10).
When the expansion process is finished the agent updates the heuristic value for all states
on the CLOSED list (line 12). We call these states the local learning space. Then it changes
its current state to the most promising state on the OPEN list (line 13). In lines 12 and 13
we use a different cost function, cLS (s, s0 ). This is the cost of the shortest path between s
and s0 using only the states inside OPEN and CLOSED.
We now provide an example of a set of problems where, using larger lookahead, an agent
can walk directly out of a local minima and follow an optimal path to the goal without
scrubbing. Similar to the example in Section 5.1, a heuristic barrier will be created behind
the agent that drives it forward and precludes backtracking/scrubbing. Our example is in
Figure 13. In this example all edge costs are 1. Each state is marked with the current
heuristic value, and states are labeled from (b) to (n). The state with the agent is marked
with an A in the lower left corner. The agent has a lookahead of l = 5.
In the first row, the agent will have the gray states in its local learning space. The
darker/red states indicate the OPEN list, which is used as the basis for updating heuristic
values; the agent will also move to one of these states after learning. No matter how ties are
broken (in the f -cost metric used for ordering state expansions), the same five states will be
in the local learning space; the gray states must be expanded. But, because state (h) has a
lower heuristic than state (b), the agent necessarily moves to state (h). Upon reaching state
(h) the space process repeats, except with higher heuristic values. This process is shown in
a similar manner as past examples in Figure 14.
327

fiSturtevant & Bulitko

Algorithm 2: Real-time Heuristic Search with Lookahead
input : search problem (S, E, s0 , sg , h), lookahead l
output: path (s0 , s1 , . . . , sT ), sT = sg
1 t0
2 ht  h
3 while st 6= sg do
4
OPEN  {st }
5
CLOSED  
6
repeat
7
n  best state in OPEN
8
CLOSED  CLOSED  {n}
9
OPEN  OPEN  (N (n) \ CLOSED)
10
until OPEN =   |CLOSED| = l
11
for each state s  CLOSED do
12
ht+1 (s)  0 min (cLS (s, s0 ) + ht (s0 ))
s OPEN

13

st+1  arg

14

tt+1

15

min (cLS (st , s0 ) + ht (s0 ))

s0 OPEN

T t
(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

6

5

4

3

4

5

5

6

7

7

8

9

9

6

7

8

8

7

6

5

6

7

7

8

9

9

6

7

8

8

9

10 10

9

8 A7

8

9

9

A

A

Figure 13: The agent climbs a heuristic grade without scrubbing.
11

11

11

10

10

10

9

9

9

8

8

8

7

7

7

6

6

6

5

5

5

4

4

4

3

3

3

2

2

1

2

1

A

0
1

1.0

2

1.0

3

1.0

4

1.0

5

1.0

6

1.0

7

1.0

8

1.0

9

1.0

10

1.0

11

1.0

12

1.0

1

A

0
13

1

1.0

2

1.0

3

1.0

4

1.0

5

1.0

6

1.0

7

1.0

8

1.0

9

1.0

10

1.0

11

1.0

12

1.0

A

0
13

1

1.0

2

1.0

3

1.0

4

1.0

5

1.0

6

1.0

7

1.0

8

1.0

Figure 14: The agent climbs a heuristic grade without scrubbing.

328

9

1.0

10

1.0

11

1.0

12

1.0

13

fi0

1.0

s0

1

1.0

s1

2

1.0

1

1.0

0

Real-time Heuristic Search
sScrubbing
s3 in Learning
s4
2

This example can be generalized in two ways: first to any value for the lookahead (l),
and second to arbitrary long sequences of states. A given problem instance, however, may
not generalize to a different lookahead or start state. We build an example that shows that
with lookahead l = 3 we can build arbitrarily long sequences of states which a lookahead
agent can traverse without scrubbing, reaching the goal in no more than |S| steps.

2
s-2

1.0

1
s-1

1.0

0

1.0

s0

1

1.0

s1

1

1.0

s2

2

1.0

s3

2

1.0



1.0

n

1.0

s2n+1

s2n

s4

n-1

1.0



1.0

0
s2n+n

Figure 15: General example for l = 3.

Lemma 4 Consider a chain of states {s2 , s1 , s0 , . . . , s3n } with s0 being the starting state
and s3n = s2n+n being the goal. The heuristic of state si is i for i < 0. The heuristic of
state si is di/2e for 0  i  2n. The heuristic of state si is 3n  i for i > 2n as shown in
Figure 15. An agent with a lookahead of 3 will climb the heuristic grade to state 2n without
any scrubbing, regardless of the tie-breaking schema, and reach the goal in 3n steps.
Proof. We prove this by inductively showing an invariant on the heuristic values in the
two neighbors on each side of the agent. This invariant will hold until the agent reaches each
state up to s2n , after which the agent can walk directly to the goal. The invariant is that
the current state has heuristic v, both neighbors of the current state have the same heuristic
value v + 1, the next neighbor to the right has the value v + 1, and the next neighbor to
the left has the value v + 2. Furthermore, along the path the agent never backtracks to the
left, stopping to learn only on the even states s0 , s2 , s4 , . . . , s2n .
Base case: The agent starts at s0 with heuristic 0. By definition, states s1 and s1 both
have heuristics of 1, state s2 has a heuristic of 1, and state s2 has a value of 2. After
learning, h(s1 ) = 3, h(s0 ) = 3 and h(s1 ) = 2. The agent thus moves to state s2 with
heuristic 1. States s1 , s3 , and s4 have heuristic 2, and s0 has heuristic 3. The agent has
only moved to the right thus far, and is in an even-numbered state, so invariant holds for
the agents new location.
Previously unvisited

v+2

si-2

1.0

v+1

si-1

1.0

v

si

1.0

v+1

si+1

1.0

v+1

si+2

1.0

v+2

si+3

1.0

v+2

si+4

Figure 16: Current heuristic values at the induction step.
Inductive step. The current state of the graph at the beginning of the induction step
is shown in Figure 16. We assume the invariant holds for the current agent state si
with heuristic v = di/2e which is i/2 since i is even by the invariant. Because states
329

fiSturtevant & Bulitko

si+1 , si+2 , si+3 , si+4 have yet to be visited or included in the learning, their heuristic values
will be v + 1, v + 1, v + 2, v + 2 respectively. This follows from the initial heuristic values
and the fact that i is even. Furthermore, according to the invariant, states si1 and si+1
both have heuristics of v + 1, state si+2 has a heuristic of v + 1, and state si2 has a value
of v + 2.
After learning, h(si1 ) = v + 3, h(si ) = v + 3 and h(si+1 ) = v + 2. The agent then
must move to si+2 with heuristic v + 1, which meets the even state condition. h(si+1 ) was
just updated to v + 2 and h(si ) to v + 3. States and si+3 and si+4 have yet to be visited
or be part of the local learning space, so they still have their original heuristic values. In
particular, since i is even, they will have the heuristic value d(i + 3)/2e and d(i + 4)/2e
respectively, which are both equal. Thus h(si+3 ) = v + 2 and h(si+4 ) = v + 2. The invariant
still holds, generalizing the example.
The remaining question is what happens after the agent reaches state s2n . At this point
the heuristic values to the left of the agent are higher than those to the right where they
decrease directly with the edge cost until they reach zero. Thus, the agent will always prefer
to move right until it reaches the goal. The agent never backtracks and moves directly from
state s0 to s3n , thus reaching the goal in 3n steps. 2
This example is for a fixed, odd-valued lookahead. When the structure here is wellunderstood, it is not difficult to construct similar examples for other odd-valued lookaheads.
If the lookahead is even, an additional state with one neighbor is needed next to each state
visited by the agent (i.e., a single new state connected to each state s2i ). This extra state
will never be visited, because it is a dead end, and essentially reduces the lookahead by one,
reducing back to the odd-value case for the lookahead.
5.3 Discussion
The examples presented above require particular starting conditions and can be broken
by slightly changing their properties such as the starting state, agent lookahead, and the
heuristic slope. Thus, while these examples confirm that, under our assumptions, the agents
can, under certain circumstances, reach the goal without scrubbing, they do not guarantee
that this will happen in practice on a wide range of real-world problem instances (Section 6).
As it may not always be a priori clear which parameters an LSS-LRTA* style agent needs
to use to avoid scrubbing on a particular problem, we propose several general approaches
to combat scrubbing. They work by removing one or more of the assumptions used in our
theoretical analysis.
The first approach is to learn faster, trying to violate the bound on constant learning
per step. There are several ways to do this. Algorithms such as f -LRTA* (Sturtevant
& Bulitko, 2011) and LSS-LRTA* with swamps (Sharon, Sturtevant, & Felner, 2013) can
prune states from the state space, effectively raising their heuristics to infinity in a single
step. The second approach is to use a better tie-breaking rule. Hernandez and Baier (2012)
have developed tie-breaking rules that work very well on grid worlds, but on a small fraction
of problems result in very poor performance. FALCONS also learns g-costs to do better
tie-breaking (Furcy & Koenig, 2000).
The third approach is to decrease h to decrease the size of the local minima from
which the agent must escape. If we use the 0 initial heuristic there will be no local minima,
330

fiScrubbing in Learning Real-time Heuristic Search

resulting in h = 0 and rendering the theoretical analysis in this paper inapplicable, but
the agent would have no guidance and would wander aimlessly. A better approach is to
multiply the heuristic by some constant 0 <  < 1, reducing the magnitude of h . This is in
contrast to raising the initial heuristic multiplicatively in an attempt to reduce its heuristic
error (Shimbo & Ishida, 2003). The latter method is equivalent to putting a weight on
the g-cost which was introduced in real-time heuristic search by Bulitko (2004) with the
equivalence proven by Bulitko and Lee (2006). The question of how to perform weighting
was recently revisited by Rivera, Baier, and Hernandez (2015).
Finally, we could avoid the value-iteration based heuristic search approach altogether,
using algorithms such as RIBS (Sturtevant et al., 2010) and EDA* (Sharon, Felner, &
Sturtevant, 2014).
5.4 Convergence Travel
The results derived thus far apply just to travel on the first-trial. If one runs Algorithm 1
or Algorithm 2 repeatedly, preserving the learning and using the same start and goal, some
of the heuristic values will eventually converge to the true distance to the goal along at
least one optimal path from the start to the goal (breaking Assumption 5) (Bulitko & Lee,
2006). We can thus build a more general bound for the learning required for convergence.
To start, we derive an analogous result to Lemma 1, showing that the learning per step
is constant-bounded when using larger lookahead.

Lemma 5 Assuming constant-bounded edge costs, constant-bounded branching factor,
and constant-bounded lookahead, the total change in heuristic values during each learning step of Algorithm 2 is constant bounded.

Proof. Algorithm 2 updates the heuristic of a state s in the closed list in line 12 of the
pseudo code based on the heuristic of a state s0 in the open list and the distance between
s and s0 . The number of states on the closed list is constant bounded by the constantbounded lookahead, l. The shortest path (cLS ) between a state s on the open list and a
state s0 on the closed list is bounded from above by the maximum edge cost times l times
the branching factor. These three values are all constants, so the shortest path is constant
bounded. Additionally, the change in heuristic is constant bounded due to consistency.
Thus, the change in heuristic for any state on the closed list is also constant bounded. Put
together, the total change in heuristic values for all states in CLOSED is also constant
bounded. 2
Previously, we used a cut set analysis to determine the minimum learning required
for some state in the state space. Here, we bypass that analysis and look at the optimal
heuristic that must be learned on at least one optimal path between the start to the goal.
Let P (s0 , sg ) be the set of all optimal paths between the start and the goal, and let Pi
 to be the optimal path that requires the minimum
be the ith such path. We define P
learning - that is, it has the smallest difference between the initial and final heuristic after
convergence. Then, we can define alternate versions of h and s for convergence.
331

fiSturtevant & Bulitko


P
= arg

h =
s =

min
(max [h (s, sg )
Pi P (s0 ,sG ) sPi
max [h (s, sg )  h0 (s)]
sP
arg max [h (s, sg )  h0 (s)] .
sP

 h0 (s)])

(36)
(37)
(38)

In this definition, h depends only on the error between the perfect heuristic and the
initial heuristic and the fact that the algorithm converges to the perfect heuristic on one
path. It does not depend on tie-breaking, lookahead, or the edge costs in the environment.

In order to make the connection between Lmin and Tmin
below, we do, however, assume
that edge costs are constant bounded to ensure that the learning per step is also constant
bounded. Thus, we can provide a convergence form of Equation 12:

Lmin (S) 

X

max{0, h  2h (s , n)}.

(39)

nS

We can also show that the convergence travel is bounded from below by the convergence
learning:


Tmin
(S)   (Lmin (S)) .

(40)

Thus, on a series of polynomial state spaces where h grows with the map radius,
systematic scrubbing will necessarily occur for convergence travel.
Corollary 3 (Systematic Scrubbing for Convergence). Consider a series of search
problems {S1 , S2 , . . . } with locally isotropic polynomial state spaces {S1 , S2 , . . . } of the
dimension d (i.e., the number of states at radius r from a given state is rd1 ). Each state
space Si has radius ri = max h (a, b). Suppose the initial heuristic for search problem Si
a,bSi

has heuristic error h  ri where  is a positive constant. Also, suppose that the heuristic
state space Si extends for at least h /2 around the state s . Then any real-time heuristic
search algorithm that runs to convergence will necessarily scrub systematically.
Proof. As the state space radius ri increases, the heuristic error h will increase at

least as ri . The minimum amount of travel Tmin
will asymptotically grow at least as rid+1

(Equation 22 holds trivially for h as well as h given that they both grow as ri ). At
the same time, the number of states in the state space (|Si |) will asymptotically grow no
 (S )  (|S |) and thus the agent
faster than rid (follows from (23)). This means that Tmin
i
i
will scrub systematically. 2
Note that we have not analyzed here any influence of number of trials performed by the
agent. We leave this analysis for future work.
332

fiScrubbing in Learning Real-time Heuristic Search

6. Experimental Results
The goal of this paper is to investigate conditions for scrubbing to occur. We showed, under
certain assumptions, that, given the heuristic learning h required in state s , the agent
must perform asymptotically at least d+1
moves (Tmin ) to solve a problem in polynomial
h
state spaces of dimension d. If h is linear in the radius r of the state space where |S| 
(rd ) then Tmin (S)  (|S|) and systematic scrubbing will occur.
In the experimental section of this paper we focus primarily on elements of the agent
construction, such as lookahead and tie-breaking rules, but also relax the assumption of
unit edge costs. We do not address the questions of whether h is linear in the radius or
whether |S|  (rd ) in general.
Our initial proof required an unfavorable tie-breaking rule, so in our first set of experiments we will explore what happens as we vary the tie-breaking rule and scale the size of
the state space, showing that scrubbing still occurs in practice. Our initial proof also did
not generalize when we relaxed the unit-cost and 1-step lookahead assumptions. So, in our
second experiment we relax these assumptions and measure performance on game maps,
again showing that scrubbing occurs in practice.
To further validate these results and the practical value of our theoretical claims, we
implemented a version of LSS-LRTA* that uses essentially the opposite tie-breaking rule
from  used in our proofs. In particular, it learns g-costs like FALCONS (Furcy & Koenig,
2000) and f -LRTA* (Sturtevant & Bulitko, 2011), and breaks ties by moving towards the
state with the maximum g-cost. We call this algorithm gLSS-LRTA*. Our theoretical tiebreaking rule is conservative, choosing to move back towards the start state (towards states
with minimum g-cost) when possible. gLSS-LRTA* is aggressive, moving away from the
start state when possible.
Evidence both here and elsewhere (Sturtevant, 2012) suggests that the game maps are
two dimensional. To conclude, we look at maze maps taken from the moving AI repository,
which are estimated to be one-dimensional (Sturtevant, 2012). We repeat our experiments
on these maps showing that scrubbing is also occurring.
6.1 Experiments on the Corner Map
Consider the corner map used earlier in the paper (Figure 8). This example contains a
corner-shaped wall with the start s0 in the upper left corner and the goal sg behind the
wall in the bottom right corner. We now consider that the map is 8-connected; diagonal
movement costs 1.5. An octile distance heuristic will mislead the agent into traveling to the
state labeled s (Equation (5)) while trying to reach the goal.
Under the tie-breaking schema  constructed earlier in this paper, the final heuristic
value of that state, hT (s ), will be raised to at least h(s , sg ) = h0 (s), based on the states
marked s in the figure. On this map h(s , sg ) = h0 (s) = n where n is the number of cells
along the side of the map. So under  it will be the case that hT (s )  n.
By recording hT (s )  h(s , sg ) = hT (s )  n we can see how different tie-breaking
schema compare to  . Specifically, a measurement of hT (s )  n < 0 indicates that the
agent did less learning in the state s than it would have under  . A measurement of
hT (s )  n indicates that at least as much learning was performed as is required by  .
333

fi5
0
5
10

Max Difference
Average Difference
Min Difference
20

40

60

80

100

Heuristic Diff. (hT(s)-h0())

Heuristic Diff. (hT(s)-h0())

Sturtevant & Bulitko

0
20
40
60

gLSS(1)
gLSS(10)

80

20

Corner Length/Width (n)

40

60

80

100

Corner Length/Width (n)

(a)

(b)

106

400+2.7x2.5
LSS-LRTA*(1)

106

Dist Traveled

Dist Traveled

Figure 17: Values of hT (s )  n recorded by running LSS-LRTA* and gLSS-LRTA* on
corner maps of different sizes.

104

104
103

10
100
Max Learning ()
Dist Traveled

1

10

400+0.3x2.55
LSS-LRTA*(10)

5

1

10
100
Max Learning ()

400+0.03x2.65
LSS-LRTA*(100)

105
104
103
1

10
100
Max Learning ()

Figure 18: Distance traveled versus maximum learning for any state when solving a problem
instance using LSS-LRTA* with lookahead depths (l) of 1 (top left), 10 (top
right) and 100 (bottom).

As long as hT (s )  n remains constant as the map radius grows, then scrubbing is
occurring. This follows from our analysis in Section 4.8 - subtracting a constant in Equation
(14) will not change the asymptotic complexity of the result.
334

fiScrubbing in Learning Real-time Heuristic Search

For each value of n  {10, 11, . . . , 100} (i.e., nn map with (n2 ) states), we ran 20 trials
of LRTA* with lookahead of 1 and random tie breaking (instead of  ). We also experimented
with fixed tie-breaking (chosen deterministically by operator orderings and the internal data
structures) and got similar results. The average value, maximum value and minimum value
of hT (s )  n on the 20 trials as a function of n are plotted in Figure 17(a). Analyzing the
underlying data, we find that LRTA* with randomized tie-breaking performed less than n
learning in s in 18% of the trials. That is, 18% of the data points fall below the zero line.
We used linear regression to fit a line to the max and min values over different segments of
the data to test if they were growing. Beyond low values of n, the min and max differences
do not seem to grow significantly as the problem size increases. Thus, we can conclude that
scrubbing is occurring in these experiments.
In Figure 17(b) we look at the performance of gLSS-LRTA* on the same corner map
with lookahead 1 and 10. With lookahead 1, gLSS-LRTA* exhibits the same scrubbing
behavior as LSS-LRTA* as the radius of the map scales. However, with the lookahead of
10, gLLS-LRTA* is able to escape the corner map without scrubbing. On the largest map
tested, where the radius of the local minima is 100, gLSS-LRTA*(10) only increased the
heuristic value of s by 15
We suspect that gLSS-LRTA* does not need to raise to the heuristic value of the corner
state as much because on the corner map moving away from the start state is correlated to
moving closer to the goal.
In the next section we look at more complex maps from the game Dragon Age: Origins
to see how LSS-LRTA* and gLSS-LRTA* perform there.
6.2 Pathfinding on Video-Game Maps
In this section we look at pathfinding on video-game maps. Our experiments not only
violate assumptions 1-3 (unit edge costs, integer heuristics, and lookahead of one), but the
maps are also not guaranteed to be locally isotropic around any state. Despite this, we see
that a measure of the heuristic learning correlates with movement that grows asymptotically
faster than the state space. Furthermore, experiments with gLSS-LRTA* show that it has
worse average-case performance that LSS-LRTA*.
We performed these experiments on the Moving AI benchmark set (Sturtevant, 2012),
on all Dragon Age: Origins maps with the optimal solution cost in [400, 404). A total of 600
problems were used over 60 maps. We ran these problems with LSS-LRTA* (Koenig & Sun,
2009) with lookahead depths of 1, 10 and 100 (LSS-LRTA* with lookahead 1 is equivalent
to Algorithm 1). Diagonal movement was allowed with cost 1.5. Ties were broken in a
fixed way in each state, according to the operator ordering and data structures, without
randomization. Note that the optimal solution cost is not predictive of the distance traveled
when solving the problem, so this setup gives a wide range of problem difficulties that are
constrained to take at least 400 steps to solve.
We measured the maximum learning  = max hT (s)  h0 (s) that occurred in any
sR(S)

state as well as the total distance traveled before reaching the goal, and then plotted one
point for each problem instance in our test set. If scrubbing is not occurring in practice,
then all the values of  will be constant-bounded; otherwise we expect a range of values for
335

fi10

400+2.7x2.5
gLSS-LRTA*(1)

6

Dist Traveled

Dist Traveled

Sturtevant & Bulitko

104

1

104
102

10
100
Max Learning ()
Dist Traveled

102

400+0.3x2.45
gLSS-LRTA*(10)

106

1

10
100
Max Learning ()

1000

400+0.05x2.40
gLSS-LRTA*(100)

106
105
104
103
1

10
100
Max Learning ()

1000

Figure 19: Distance travelled versus maximum learning for any state when solving a problem instance using gLSS-LRTA* with lookahead depths (l) of 1 (top left), 10
(top right) and 100 (bottom).

. If the number of states at a given radius in a map grows polynomially, then the total
movement should grow faster than a polynomial with degree two.
The resulting scatter plots are found in Figure 18. We visually fit a polynomial of the
form y = 400 + c1  xc2 to the data, as we knew that all problems have the optimal solution
cost between 400 and 403. (y is the distance traveled and x is .) The values for c1 and c2
for each of the three lookahead depths are shown in the figure, although slightly different
values do not significantly change the curves or the residuals.
These two-dimensional video-game maps are not isotropic around the state s and are
not exactly polynomial due to their topology, have non-unit-cost edges, have non-unit edge
costs, and and may not extend far enough from s to be locally isotropic (Section 4.9). As
a result, our theory does not offer the asymptotic bound of Tmin  ((h )d+1 ) = ((h )3 )
on the travel to hold. Furthermore, the maximum learning amount  we measure is not
h defined earlier in the paper. Yet, the manually fit polynomial curves appear to come
close with the degree of the polynomial being between 2.5 and 2.65. Note that the degree
of the polynomial is greater than two, the maximum dimensionality of the maps. The data
suggests that  is predictive of the total movement required to reach the goal and that
scrubbing is occurring in practice.
Results for the same experiments with gLSS-LRTA* are found in Figure 19. On these
problems the algorithm sometimes raises the heuristic substantially higher than LSS-LRTA*
and also travels substantially further.
336

fiMax Learning ()

Scrubbing in Learning Real-time Heuristic Search

LSS-LRTA*(10)
gLSS-LRTA*(10)

500
250
0

0

25
50
75 100 125
Estimate of h(s0, sg)-h0(s0)

Figure 20: A comparison of the maximum learning in practice () to an estimate of the
learning required between the start and goal states (h(s0 , sg )  h0 (s0 )).

If the different tie-breaking rule achieved better performance, we would expect a smaller
range of values on the x-axis (as compared to Figure 18), because there would be fewer states
with large amounts of learning. Instead, with lookahead 10 and 100 the range of values is
significantly increased, with some states having their heuristic raised by almost 1000. A
two-sample Kolmogorov-Smirnov test comparing the LSS-LRTA* and gLSS-LRTA* results
on the same problems suggests that the differences are significant with lookahead of 10 or
100, but not with lookahead 1. That is gLSS-LRTA* provides worse performance that just
LSS-LRTA*.
Looking deeper into the data with lookahead 10 we find that, although the mean maximum learning is similar (55.29 for LSS-LRTA*(10) versus 59.01 for gLSS-LRTA*(10)), there
is a higher standard deviation for gLSS-LRTA*(10) (68.35 versus 45.79 for LSS-LRTA*).
The data shows that there are more instances where tie-breaking towards states with higher
g-cost can help the agent reach the goal faster (324 instances better versus 240 worse). On
the other hand, when moving towards higher g-costs results in worse performance (distance and learning), it outweighs the gains on the other problems, leading to worse average
performance. With lookahead 100 the difference in instance counts with better and worse
performance is almost equal, but again, the loss of performance from this tie-breaking rule
outweighs the gains. The data suggests that moving away from the start state quickly
can, in the best case, improve performance, but in the worst case it can have significantly
detrimental effects. These results suggest that the corner map is not representative of the
video game maps.
We now compare the learning required between the start and the goal (h(s0 , sg )h0 (s0 )),
a lower-bound on h , to , the actual maximum learning performed by the agent. We
estimate h(s0 , sg )  h0 (s0 ) by measuring over a single shortest path instead of over all
shortest paths. If our lower-bound on h holds for LSS-LRTA*, then all points in Figure 20
would be above the straight line x = y in the figure.
On our 600 pathfinding problems this was indeed the case for LSS-LRTA*(10) on all but
one problem. On that one problem, the algorithm had the maximum amount of learning
337

fiSturtevant & Bulitko

10

6

7.1507x2.3067
LSS-LRTA*(1)

Dist Traveled

Dist Traveled

107

105
104
50
100
Max Learning ()
Dist Traveled

20

10

200

106

1.7745x2.2078
LSS-LRTA*(10)

105
104
103
20

50
100
Max Learning ()

200

0.51333x2.0462
LSS-LRTA*(100)

5

104
103
20

50
100
200
Max Learning ()

Figure 21: Distance travelled versus maximum learning for any state when solving a problem instances on maze maps with corridor size 8 using LSS-LRTA* with lookahead depths (l) of 1 (top left), 10 (top right) and 100 (bottom).

() of 54.5 yet the maximum difference between initial heuristic values along a particular
optimal solution was 55. Repeating the experiments with gLSS-LRTA*(10), the number of
such problems rose from 1 to 36 as illustrated by more markers below the line in the Figure.
Simultaneously, gLSS-LRTA*(10) raised heuristic values of some states substantially higher
than LSS-LRTA*(10).
6.3 Maze Maps
In the previous section we looked at maps from the game Dragon Age: Origins. Analysis of
these maps using regression over the number of states at each level in a breadth-first search
suggests that they are nearly two-dimensional (Sturtevant, 2012). Specifically, polynomial
regression for the equation a + b  x + c  x2 gave an average value of 0.31 for the constant c.
According to this same measure, mazes in the benchmark set appear to be approximately
one-dimensional, as regression gives a value of 0.01 to 0.03 for the constant c in this equation (Sturtevant, 2012). Thus, mazes represent a different class of problems on which we
can re-run our experiments. As before, our theoretical assumptions on the state spaces do
not hold in these experiments.
We duplicated our settings from the experiments in Section 6.2 on 10 maze maps with
corridor width 8 from the Moving AI benchmark set. Because we had fewer maps, we
increased the number of problems solved by solving all probelsm with optimal solution
between 400 and 439, resulting in 1100 total problems solved. The results for LSS-LRTA*
are in Figure 21. We computed the best-fit curve using least squares for the data to the
equation c1  xc2 , which is also shown.
338

fiScrubbing in Learning Real-time Heuristic Search

The fits have correlation of 0.94, 0.94, and 0.91 respectively for lookaheads 1, 10, and
100. On these maps we see that the degree of the fit polynomial is two or greater. As the
maps are one-dimensional and  is not constant, these results suggest that scrubbing is
occurring in practice.
In the initial trials on the maze problems, gLSS-LRTA* had worst-case travel distance
several orders of magnitude higher than LSS-LRTA*. This precluded us from running the
experiments in a reasonable amount of time and suggests that gLSS-LRTA* is not a practical
algorithm on such maps.

7. Conclusions
The primary contribution of this paper is the development of a non-trivial lower bound
on the minimum travel that an LRTA*-like real-time heuristic search agent may have to
perform to reach the goal state. While previous work has provided examples of problems
where state revisitation (i.e., scrubbing) would occur, we provide general conditions that
can be used for analysis. In idealized polynomial state spaces the lower bound grows
asymptotically faster than the state space. This means that the agent will necessarily scrub
 an undesirable behavior in many applications such as real-time pathfinding in video games.
These theoretical results are supported experimentally on real-world search problems.
This result may appear discouraging, as it suggests that common real-time heuristic
search algorithms may not, on their own, be able to avoid scrubbing. While the proofs rely
on a several restrictive assumptions, we expect that our results hold more broadly.
From the proof we suggest four directions for future work trying to improve asymptotic
performance. This include (1) increasing the amount of learning performed in each step, (2)
using different tie breaking rules, (3) decreasing the size of the heuristic local minima and (4)
developing algorithms that do not use value-iteration as the core technique driving agent
behavior. We hope that future researchers will be able to point to these four directions
to explain why their approaches improve performance, and they will be able to identify
underlying assumptions about the state space that makes their approaches successful.
While we have shown that our lower bound does not hold when some of our assumptions
are broken, we continue to look for properties that we could use to extend the lower bounds
to a larger class of problems and agents.

Acknowledgements
The second author received support for this work from the National Research and Engineering Council (NSERC).

Appendix A. Special Case: Locally Isotropic Exponential State Spaces
In this section we consider the case of exponential state spaces. We limit our analysis to
locally isotropic exponential state states spaces where the number of states exactly cost r
away (up to h /2) from a given state s is:
(r) = br , r  [0, h /2]
339

(41)

fiSturtevant & Bulitko

where b is the branching factor of the space. As in Corollary 2, we will assume that the
state space size does not expand substantially beyond h /2 so the total state space size
is asymptotically the same as the size of the state space within the radius h /2 from the
state s (assumption ?).
In such a case we can repeat the derivation in Section 4.9 by substituting (r) = br in
(18):
Z h
2
(r)(h  2r)dr =
0

Z

h
2

br (h  2r)dr =

0

Z
h

h
2

Z

r

b dr  2

0

h
2

br rdr =

0

fi h
Z h
2
br fifir= 2
br rdr.

2
h
ln b fir=0
0

(42)

R h
r
To take the integral 0 2 br rdr in (42) we introduce the function g(r) = lnb b so that g 0 (r) = br
and integrate by parts:
Z
Z
r
b rdr = g 0 (r)rdr =
Z
g(r)r  g(r)r0 dr =
Z r
Z
b
br

dr =
g(r)r  g(r)dr = r
ln b
ln b


br
br
br
1
r

+C =
r
+ C.
(43)
ln b ln2 b
ln b
ln b
With (43) equation (42) becomes:
fi h
Z h
2
br fifir= 2
 2
h
br rdr
fi
ln b r=0
0
fi h

fi h
br fifir= 2
br
1 fifir= 2
h
 2
r
ln b fir=0
ln b
ln b fir=0

fi h
2 fifir= 2
br

h  2r +
ln b
ln b fir=0


2 h

2
2
b 
h +
ln b
ln b
ln2 b

=
=
=
 h 
  b 2 , h  

(44)

Combining (12), (18) and (44) we conclude that for locally isotropic exponential spaces of
branching factor b that extend for distance at least h /2 around the state s , the minimum
amount of total learning is lower-bounded as:
 h 
Lmin (S)   b 2 , h  .
(45)
340

fiScrubbing in Learning Real-time Heuristic Search

As the amount of learning per step is constant-bounded, the same asymptotic lower bound
applies to the travel cost:
 h 
Tmin (S)   b 2 , h  .
(46)
In locally isotropic exponential spaces the number of states within radius h /2 of the state
s grows as:
Z
0

h
2

Z
(r)dr =

h
2

 h 
br dr   b 2 , h  

(47)

0

which is asymptotically the same as the total state space according to the assumption (?)
and asymptotically the same as the lower bound on the minimum amount of
 travel
 (46).
h

This means that in locally isotropic exponential spaces our lower bound  b 2
is not
sufficient to show systematic scrubbing (i.e., to prove an equivalent of Corollary 2).
Note that (46) is a lower asymptotic bound on the amount of travel whereas (47) is both
an upper and lower asymptotic bound on the state space growth. Thus, it may be possible
that Tmin grows asymptotically faster than the state space size but our lower bound derived
above is insufficient to claim so.

References
Bulitko, V. (2004).
Learning for adaptive real-time search.
Tech. rep.
http://arxiv.org/abs/cs.AI/0407016, Computer Science Research Repository (CoRR).
Bulitko, V., & Lee, G. (2006). Learning in real time search: A unifying framework. Journal
of Artificial Intelligence Research, 25, 119157.
Bulitko, V. K., & Bulitko, V. (2009). On backtracking in real-time heuristic search. CoRR,
abs/0912.3228.
Edelkamp, S., & Schrodl, S. (2012). Heuristic Search - Theory and Applications. Academic
Press.
Furcy, D., & Koenig, S. (2000). Speeding up the convergence of real-time search. In National
Conference on Artificial Intelligence (AAAI), pp. 891897.
Hernandez, C., & Baier, J. A. (2012). Avoiding and escaping depressions in real-time
heuristic search. Journal of Artificial Intelligence Research (JAIR), 43, 523570.
Hernandez, C., & Meseguer, P. (2005). LRTA*(k). In International Joint Conference on
Artificial Intelligence (IJCAI), pp. 12381243.
Ishida, T., & Korf, R. (1991). Moving target search. In International Joint Conference on
Artificial Intelligence (IJCAI), pp. 204210.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22 (4), 109132.
Koenig, S., & Simmons, R. G. (1992). Complexity analysis of real-time reinforcement
learning applied to finding shortest paths in deterministic domains. Tech. rep. CMU
CS93106, School of Computer Science, Carnegie Mellon University, Pittsburgh.
341

fiSturtevant & Bulitko

Koenig, S., & Simmons, R. G. (1993). Complexity analysis of real-time reinforcement
learning. In National Conference on Artificial Intelligence (AAAI), pp. 99105.
Koenig, S., & Simmons, R. G. (1996). The effect of representation and knowledge on
goal-directed exploration with reinforcement-learning algorithms. Machine Learning,
22 (1-3), 227250.
Koenig, S., & Sun, X. (2009). Comparing real-time and incremental heuristic search for
real-time situated agents. Journal of Autonomous Agents and Multi-Agent Systems,
18 (3), 313341.
Koenig, S., Tovey, C., & Smirnov, Y. (2003). Performance bounds for planning in unknown
terrain. Artificial Intelligence, 147, 253279.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42 (23), 189211.
Rivera, N., Baier, J. A., & Hernandez, C. (2015). Incorporating weights into real-time
heuristic search. Artificial Intelligence, 225, 123.
Sharon, G., Felner, A., & Sturtevant, N. (2014). Exponential deepening A* for real-time
agent-centered search. In AAAI Conference on Artificial Intelligence, pp. 871877.
Sharon, G., Sturtevant, N. R., & Felner, A. (2013). Online detection of dead states in realtime agent-centered search. In Helmert, M., & Roger, G. (Eds.), Proceedings of the
Sixth Annual Symposium on Combinatorial Search. AAAI Press.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic
search. Artificial Intelligence, 146 (1), 141.
Shue, L.-Y., Li, S.-T., & Zamani, R. (2001). An intelligent heuristic algorithm for project
scheduling problems. In Annual Meeting of the Decision Sciences Institute, San Francisco.
Shue, L.-Y., & Zamani, R. (1993a). An admissible heuristic search algorithm. In International Symposium on Methodologies for Intelligent Systems (ISMIS-93), Vol. 689 of
LNAI, pp. 6975.
Shue, L.-Y., & Zamani, R. (1993b). A heuristic search algorithm with learning capability.
In ACME Transactions, pp. 233236.
Sturtevant, N. R. (2012). Benchmarks for grid-based pathfinding. Transactions on Computational Intelligence and AI in Games, 4 (2), 144  148.
Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). On learning in agent-centered search.
In Autonomous Agents and Multiagent Systems (AAMAS), pp. 333340. International
Foundation for Autonomous Agents and Multiagent Systems.
Sturtevant, N. R., & Bulitko, V. (2011). Learning where you are going and from whence
you came: H- and G-cost learning in real-time heuristic search. In International Joint
Conference on Artificial Intelligence (IJCAI), pp. 365370. AAAI Press.
Sturtevant, N. R., & Bulitko, V. (2014). Reaching the goal in real-time heuristic search:
Scrubbing behavior is unavoidable. In Proceedings of the Symposium on Combinatorial
Search (SoCS), pp. 166174.
342

fiScrubbing in Learning Real-time Heuristic Search

Zamani, R., & Shue, L.-Y. (2001). A heuristic learning algorithm and its application to
project scheduling problems. Tech. rep., Department of Information Systems, University of Wollongong.

343

fiJournal of Artificial Intelligence Research 57 (2016) 151-185

Submitted 05/16; published 10/16

Lightweight Random Indexing for
Polylingual Text Classification
Alejandro Moreo Fernandez
Andrea Esuli

alejandro.moreo@isti.cnr.it
andrea.esuli@isti.cnr.it

Istituto di Scienza e Tecnologie dellInformazione
Consiglio Nazionale delle Ricerche
56124 Pisa, IT

Fabrizio Sebastiani

fsebastiani@qf.org.qa

Qatar Computing Research Institute
Hamad bin Khalifa University
PO Box 5825, Doha, QA

Abstract
Multilingual Text Classification (MLTC) is a text classification task in which documents are
written each in one among a set L of natural languages, and in which all documents must be
classified under the same classification scheme, irrespective of language. There are two main
variants of MLTC, namely Cross-Lingual Text Classification (CLTC) and Polylingual Text
Classification (PLTC). In PLTC, which is the focus of this paper, we assume (differently
from CLTC) that for each language in L there is a representative set of training documents;
PLTC consists of improving the accuracy of each of the |L| monolingual classifiers by
also leveraging the training documents written in the other (|L|  1) languages. The
obvious solution, consisting of generating a single polylingual classifier from the juxtaposed
monolingual vector spaces, is usually infeasible, since the dimensionality of the resulting
vector space is roughly |L| times that of a monolingual one, and is thus often unmanageable.
As a response, the use of machine translation tools or multilingual dictionaries has been
proposed. However, these resources are not always available, or are not always free to use.
One machine-translation-free and dictionary-free method that, to the best of our knowledge, has never been applied to PLTC before, is Random Indexing (RI). We analyse RI in
terms of space and time efficiency, and propose a particular configuration of it (that we
dub Lightweight Random Indexing  LRI). By running experiments on two well known public benchmarks, Reuters RCV1/RCV2 (a comparable corpus) and JRC-Acquis (a parallel
one), we show LRI to outperform (both in terms of effectiveness and efficiency) a number
of previously proposed machine-translation-free and dictionary-free PLTC methods that
we use as baselines.

1. Introduction
With the rapid growth of multicultural and multilingual information accessible on the Internet, how to properly classify texts written in different languages has become a problem
of relevant practical interest. Multilingual Text Classification (MLTC) is a text classification task in which documents are written each in one among a set L = {l1 , . . . , l|L| } of
natural languages, and in which all documents must be classified under the same classification scheme, irrespective of the language. There are two main variants of MLTC, namely
Cross-Lingual Text Classification (CLTC) and Polylingual Text Classification (PLTC).
c
2016
AI Access Foundation. All rights reserved.

fiMoreo, Esuli, & Sebastiani

CLTC is a task characterized by the fact that, for all languages in a subset LT 
L, there are no training documents; the task thus consists of classifying the unlabelled
documents written in the languages in LT (i.e., the target languages) by leveraging the
training documents expressed in the other languages LS = L\LT (i.e., the source languages).
CLTC is thus a transfer learning problem (Pan & Yang, 2010), where one needs to transfer
the knowledge acquired by learning from the training data in LS , to the task of classifying
documents in LT . Most previous work on MLTC indeed focuses on CLTC, and fewer efforts
have been devoted to PLTC, which is instead the focus of this paper.
In PLTC, a representative set of training documents for all languages in L is assumed to
be available. Therefore, a straightforward solution may consist in training |L| independent
monolingual classifiers, one for each language. However, such solution is suboptimal, as
each classifier is obtained by disregarding the additional supervision that could be obtained
by using the training documents written in the other (|L|  1) languages. PLTC thus
consists of leveraging the training documents written in all languages in L to improve
the classification accuracy that could be obtained by simply training the |L| independent,
monolingual classifiers.
However, PLTC entails a number of obstacles that work to the detriment of efficient
representation. To see this, assume we generate a single polylingual vector space (hereafter,
the juxtaposed vector space) by juxtaposing the monolingual vector spaces. The vector
space for a monolingual dataset usually consists of tens or even hundreds of thousands of
features; for the juxtaposed vector space of a polylingual dataset, this dimensionality gets
roughly multiplied by the number of distinct languages under consideration. Such a substantial increase in the feature space would degrade the performance of many classification
algorithms, because of the so-called curse of dimensionality, and would also bring about
a severe degradation in efficiency. Additionally, co-occurrence-based techniques tend to lose
power when representations are polylingual, since terms belonging to different languages
rarely co-occur, if at all (a problem usually referred to as feature disjointness).
As a response, some authors have proposed the use of machine translation (MT) tools
as a device to simultaneously cope with both high dimensionality and feature disjointness in
PLTC. The idea is to reduce the problem to the monolingual case (typically English). That
is, non-English training documents are automatically translated into English, are added to
the English training set, and a monolingual (English) classifier is trained. At classification
time, non-English unlabelled documents are translated into English and are then classified.
(Of course, this idea can also be used in CLTC; in this case, there are no training documents
to translate.) However, these MT-based PLTC (and CLTC) techniques suffer from a number
of drawbacks (Wei, Yang, Lee, Shi, & Yang, 2014): (i) automatically translated texts usually
present different statistical properties with respect to human translations; (ii) MT systems
are not always available for all language pairs; and (iii) training a statistical MT system
from any of the free toolkits available requires collecting large corpora of parallel text in
the domain of interest, which is not always easy.
Thesaurus-based and dictionary-based methods, on the other side, represent a lighter
approach in MLTC. If a multilingual dictionary or thesaurus that encompasses the different languages is available, some kind of unification of the vector representation may
be attempted. This is customarily done by replacing non-English words with their English equivalents in the dictionary, or by replacing all terms with thesaurus codes invariant
152

fiLightweight Random Indexing for Polylingual Text Classification

across languages (e.g., BabelNet synsets  Ehrmann, Cecconi, Vannella, McCrae, Cimiano,
& Navigli, 2014). However, bilingual dictionaries or thesauri are not available for all language pairs, and automatically constructing a domain-dependent bilingual resource requires
a suitable parallel corpus with sentence-level alignment.
1.1 Distributional Representations
For classification purposes, a textual document is usually represented as a vector in a vector
space according to the bag-of-words (BoW) model, i.e., each distinct term corresponds to a
dimension of the vector space. In the juxtaposed vector space, most of the columns in the
document-by-term matrix are thus informative for only one of the languages.
Since each distinct term corresponds to a dimension of the vector space, the BoW model
is agnostic with respect to semantic similarities among terms. That is, the dimension for
term governor is orthogonal to the dimension for the related term president, as it is to
the dimension for the unrelated term transport. The semantic relations among terms can
be uncovered by detecting their co-occurrences, i.e., the contexts in which words tend to be
used together. This idea rests on the distributional hypothesis, according to which words
with similar meanings tend to co-occur in the same contexts (Harris, 1968). By detecting
co-occurrences, it is possible to establish a parallelism between term meaning and geometrical properties in the vector space. Distributed Semantic Models (DSMs  sometimes also
called word space models in Sahlgren, 2006) aim at learning continuous and compact distributed term representations, which have recently been called word embeddings (Mikolov,
Sutskever, Chen, Corrado, & Dean, 2013b). DSMs have gained a lot of attention from the
machine learning community, delivering improved results in many natural language processing tasks (Bengio, Schwenk, Senecal, Morin, & Gauvain, 2006; Bullinaria & Levy, 2007;
Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011). DSM-based methods
can be categorised (see Pennington, Socher, & Manning, 2014; Baroni, Dinu, & Kruszewski,
2014) as belonging (a) to the class of context-counting models, which are often based on
matrix factorization, e.g., Latent Semantic Analysis (LSA  Deerwester, Dumais, Furnas,
Landauer, & Harshman, 1990; Osterlund, Odling, & Sahlgren, 2015), or (b) to the class
of context-predicting models, e.g., methods based on deep learning architectures (Bengio,
2009; Mikolov et al., 2013b).
However, in multilingual contexts huge quantities of plain text for each language should
be processed in order to learn meaningful word representations, which incurs high computational costs. Trying to find such representations for a large multilingual vocabulary
can thus become computationally prohibitive. Some attempts have recently been made
in this direction, by leveraging multilingual external resources such as Wikipedia articles
(Al-Rfou, Perozzi, & Skiena, 2013), or bilingual dictionaries (Gouws & Sgaard, 2015), or
word-aligned parallel corpora (Klementiev, Titov, & Bhattarai, 2012), or sentence-aligned
parallel corpora (Zou, Socher, Cer, & Manning, 2013; Hermann & Blunsom, 2014; Lauly,
Boulanger, & Larochelle, 2014; Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, &
Saha, 2014), or document-aligned parallel corpora (Vulic & Moens, 2015). However, such
external resources may not always be available for all language combinations and, when
they are available (e.g., Wikipedia articles), they may be of uneven quality and quantity for
languages other than English. Alternatively, other approaches require a computationally
153

fiMoreo, Esuli, & Sebastiani

expensive post-processing step to align word representations across languages (Mikolov, Le,
& Sutskever, 2013a; Faruqui & Dyer, 2014).
In this article we discuss efficient representation mechanisms for PLTC that (i) are
MT-free, (ii) do not require external resources, and (iii) do not incur high computational
costs. In particular, we investigate the suitability of Random Indexing (RI  Kanerva,
Kristofersson, & Holst, 2000; Sahlgren, 2005) as an effective representation mechanism of
the original co-occurrence matrix in PLTC. RI is a context-counting model belonging to the
family of random projections methods (Kaski, 1998; Papadimitriou, Raghavan, Tamaki, &
Vempala, 1998), that produces linear projections into a nearly-orthogonal reduced space
where the original distances between vectors are approximately preserved (Hecht-Nielsen,
1994; Johnson, Lindenstrauss, & Schechtman, 1986). RI is expected to deliver fast and
semantically meaningful representations in a reduced space, and can be viewed as a cheaper
approximation of LSA (Sahlgren, 2005). RI is such that each column from the polylingual
matrix produced by it will not depend on any single specific language (as it does instead in
the BoW representation). We hypothesize this could be advantageous in PLTC, since the
entire new space becomes potentially informative for all languages at once, thus making the
problem more easily separable if enough dimensions are considered. While RI has already
been applied to bilingual scenarios (Gorman & Curran, 2006; Sahlgren & Karlgren, 2005),
to the best of our knowledge it has not been tested on the PLTC case so far. In monolingual
TC, RI was found to be competitive, but not superior, to BoW (Sahlgren & Coster, 2004).
In this article we demonstrate that RI outperforms the BoW model in PLTC.
The method we present in this article, that we dub Lightweight Random Indexing (LRI),
is inspired by the works of Achlioptas (2001) and Li, Hastie, and Church (2006) on very
sparse random projections, and goes one step further by pushing sparsity to the limit. LRI is
designed so that the orthogonality of the projection base is maximized, which causes sparsity
to be preserved after the projection. We empirically show that LRI helps Support Vector
Machines (SVMs) to deliver better classification accuracies in PLTC with respect to many
popular alternative vector space models (including the main random projection variants,
LSA-based approaches, and polylingual topic models), while also requiring substantially
less computation effort.
The contribution of this work is twofold. First, we conduct a comparative empirical
study of several PLTC approaches in two representative scenarios: the first is when the
training corpus is comparable at the topic-level (i.e., documents are not direct translations of
each other, but are simply about similar topics; this is here exemplified by the RCV1/RCV2
dataset), and the second is when the training corpus is parallel at the document-level (i.e.,
each text is available in all languages thanks to the intervention of human translators; this
scenario is exemplified by the JRC-Acquis dataset). We show that LRI yields the best results
in both settings, in terms of both effectiveness and efficiency. As a second contribution, we
present an analytical study that can be useful to better understand the nature of random
mapping methods.
The rest of this paper is organized as follows. In Section 2 we discuss related work.
In Section 3 we present the problem statement, describe the Random Indexing method in
detail, and present our proposal. Section 4 reports the results of the experiments we have
conducted. Section 5 presents an analytical study on computational efficiency, while Section
6 concludes.
154

fiLightweight Random Indexing for Polylingual Text Classification

2. Related Work
This section gives an overview of the main approaches to PLTC that have emerged in the
literature. We distinguish three groups of methods, according to whether the problem is approached (i) by leveraging external resources, (ii) by combining the outcome of independent
monolingual classifiers, or (iii) by reducing the dimensionality of the resulting multilingual
feature space. This discussion also includes some references to CLTC techniques that we
consider relevant to PLTC and to our approach.
2.1 Exploiting External Multilingual Resources
Multilingual text classification is a relatively recent area of research, and most previous
efforts within it were devoted to the CLTC subtask. As in CLTC there is no labelled
information for all languages, previous approaches typically relied on automatic translation
mechanisms as a means to fill the gap between the source and the target languages. The
main difference between CLTC and PLTC lies in the fact that PLTC exploits labelled
documents belonging to different languages during learning. Despite this, the two tasks
have a close-knit relation, since in both of them cross-lingual adaptation is generally carried
out by means of external resources, such as parallel corpora, bilingual dictionaries, and
statistical thesauri.
If a suitable (unlabelled) multilingual corpus containing short aligned pieces of texts
is available, correlations among groups of words in the two languages could be explored.
Cross-Lingual Kernel Canonical Correlation Analysis (CL-KCCA) was proposed by Vinokourov, Shawe-Taylor, and Cristianini (2002) as a means to obtain a semantic cross-lingual
representation, by investigating correlations between aligned text fragments. CL-KCCA
takes advantage of kernel functions in order to map aligned texts into a high-dimensional
space in such a manner that the correlations between the mapped aligned texts are jointly
maximized. This cross-lingual representation could then be used for classification, retrieval,
or clustering tasks. CL-KCCA was investigated in combination with Support Vector Machines (SVMs) and applied to cross-lingual patent classification by Li and Shawe-Taylor
(2007). Their method, called SVM 2k, learns two SVM-based classifiers by searching two
linear projections in the original feature space of each language such that the distance of the
projections (instead of the correlation of the projections) of two aligned texts is minimized.
In a similar vein, polylingual topic models (Mimno, Wallach, Naradowsky, Smith, &
McCallum, 2009) have been proposed as an extension of Latent Dirichlet Allocation (LDA
 Blei, Ng, & Jordan, 2003) to the polylingual case. LDA is a generative model which
assigns probability distributions to documents over latent topics, and to latent topics over
terms. These distributions can be viewed as compact representations for documents in a
latent space. Since topics discovered by Polylingual LDA (PLDA) are aligned across all
languages, documents are represented in a common vector space regardless of the language
they are written in. However, PLDA (which we will use as a baseline in the experimental
section) requires a parallel collection of documents aligned at the sentence level.
Bilingual dictionaries can be used in a straightforward manner to carry out a word-byword translation of the feature space. However, dictionary-based translations suffer from
several deficiencies, e.g., context-unaware translations might perform poorly when handling
polysemic words; dictionaries might suffer from a substantial lack of coverage of novel terms
155

fiMoreo, Esuli, & Sebastiani

and domain-dependent terminology; and dictionaries might not be available for all language
pairs, or not be free to use. As a response to these drawbacks, the automatic acquisition
of statistical bilingual dictionaries has been proposed. Wei et al. (2014) explored a cooccurrence-based method to measure the polylingual statistical strength of the correlation
among words in a parallel corpus. These correlations are then taken into account to reinforce the weight of each feature in order to select the most important (highly weighted) ones.
Gliozzo and Strapparava (2006) experimented with bilingual dictionaries and, more interestingly, provided a means to automatically obtain a Multilingual Domain Model (MDM),
a natural extension of domain models to multiple languages, when no additional multilingual resources are available. A domain model defines soft relations between words and
domain topics. In the absence of a multilingual dictionary, a MDM could be automatically
obtained from a comparable corpus by performing Latent Semantic Analysis (explained in
more detail below).
It has been argued that words that are shared across languages play an important role
when searching the semantic latent space. Accordingly, Steinberger, Pouliquen, and Ignat
(2004) exploit language-independent tokens which are shared across the languages, and
propose a simple method to link documents with existing external resources such as thesauri,
nomenclatures, and gazetteers. Finally, de Melo and Siersdorfer (2007) use ontologies to
map original features onto synset-like identifiers, so that the documents are translated into
a language-independent feature space.
MT tools, on the other side, provide more elaborated translations of texts, and represent a promising research field for multilingual tasks. Unfortunately, the above-mentioned
problems regarding availability, accessibility, and performance still hold in this case. The
effect of different translation strategies on CLTC has been investigated by Bel, Koster, and
Villegas (2003), Rigutini, Maggini, and Liu (2005), and Wei, Lin, and Yang (2011).
Even when available, MT tools may be expensive resources. For this reason, in their
experiments Prettenhofer and Stein (2010) restrict the use of an MT tool to a limited budget
of calls. Their Structural Correspondence Learning (SCL) method, initially proposed for
domain adaptation, was indeed applied to CLTC. The key idea of the method consists of
discovering cross-lingual correspondences between pairs of terms (dubbed pivot features)
that are later used to bridge across the two languages. Pivot features play an important
role in bilingual tasks, since they establish pairs of words that behave similarly in the source
and target languages, allowing one to find cross-language structural correspondences. One
such special type of pivot features are obviously the words shared across languages, such as
proper nouns, technical terms, not yet lexicalized terms, or stemmed forms of etymologically
related terms. Nastase and Strapparava (2013) found that etymological ancestors of words
do actually add useful information, allowing to transcend cross-lingual boundaries. This
method however depends on the availability of etymological thesauri (such as Wikipedias
Wiktionary, or Etymological WordNet), and remains restricted to historically interrelated
languages.
In sum, the applicability of the multilingual methods discussed in this section is usually
constrained by the availability of external resources. With the aim of overcoming these limitations, we will restrict our investigations to dictionary-free, MT-free multilingual methods.
156

fiLightweight Random Indexing for Polylingual Text Classification

2.2 Monolingual Classifiers and Multiview Learning
Given the availability of a representative set of labelled documents for each language, a
simple baseline, known as the nave polylingual classifier, could be obtained by delegating
the classification process to individual monolingual classifiers, each built upon separate
monolingual data. Such a solution is sub-optimal, as each classifier does not exploit labelled
information from the other languages, a type of information that might provide insights or
different perspectives on the semantics of the classes.
Garca Adeva, Calvo, and Lopez de Ipina (2005) compared different nave strategies,
considering one single polylingual classifier, i.e., a classifier that works on the juxtaposed
representation (1C), vs. various monolingual ones (NC), and one language-independent
preprocessor (1P) vs. various language-specific ones (NP), using various learning methods
in a bilingual Spanish/Basque benchmark. In their experimentation the combinations NPNC and NP-1C, which we will consider here as baselines, yielded the best results in terms
of running time, memory usage, and accuracy.
Even though training separate language-specific classifiers is a simple way to approach
the PLTC task, there are some strategies that could improve the final accuracy by better
merging the outcomes of each classifier. Multiview learning (Xu, Tao, & Xu, 2013) for TC
deals with parallel texts, i.e., with the case when each document is available in all languages,
where each language is considered as a separate source. It was shown by Amini, Usunier, and
Goutte (2009) that a multiview majority voting algorithm, which returns the label output by
the highest number of language-specific classifiers, outperforms both the nave polylingual
classifier and a multiview Gibbs classifier, which bases its predictions on the mean prediction
of each language-specific classifier. Amini and Goutte (2010) proposed a co-regularization
approach for multiview text classification which minimizes a joint loss function that takes
into account each language-specific classifier loss. However, the availability of a parallel
corpus containing all the documents views is a very strong restriction, that is usually
alleviated by leveraging machine translation tools that automatically generate the missing
documents views.
2.3 Dimensionality Reduction for Multilingual Classification
One of the main challenges in the juxtaposed vector space approach to PLTC concerns
the relevant increase in the number of features that represent the documents, i.e., the dimensionality of the vector space (Rigutini et al., 2005). Feature selection methods attempt
to select a reduced subset of informative features from the original set F so that the size of
this subset is much smaller than |F | and so that the reduced set yields high classification
effectiveness. In TC the problem is usually tackled via a filtering approach, which relies
on a mathematical function meant to measure the contribution of each feature to the classification task. Yang and Pedersen (1997) showed that filtering approaches may improve
the performance of classification, even for aggressive reduction ratios (e.g., removal of 90%
of the features).
Another important dimensionality reduction technique is Latent Semantic Analysis
(LSA  aka Latent Semantic Indexing), which originated from the information retrieval
community (Deerwester et al., 1990), and has been later applied to cross-lingual classification (Gliozzo & Strapparava, 2006; Xiao & Guo, 2013) and cross-lingual problems in general
157

fiMoreo, Esuli, & Sebastiani

(Dumais, Letsche, Littman, & Landauer, 1997). LSA maps the original document-term matrix into a lower dimensional latent semantic space that attempts to capture the (linear)
relations among the original features and the documents. This mapping is carried out by
means of a singular value decomposition (SVD) of the original document-term matrix M .
SVD decomposes M as M = V U T , where  is a diagonal matrix containing all the eigenvalues of M . The approximation Mk = Vk k UkT of the original matrix M can be computed
by taking the k largest eigenvalues of  and setting the remaining ones to 0; Mk is then said
to be rank-k optimal in terms of the Frobenius norm. Vk and Uk are orthogonal matrices
that explain the relations among pairs of terms and pairs of documents, respectively.
Although LSA can successfully be used to discover hidden relations between indirectly
correlated features, as is the case for terms belonging to different languages, it suffers
from high computational costs. Random mappings arise as an alternative to LSA, as
they perform comparably in different machine learning tasks by preserving some important
characteristics of LSA, and by bringing about, at the same time, significant savings in
terms of computational cost (Fradkin & Madigan, 2003). Random Projections (RPs 
Papadimitriou et al., 1998) and Random Mappings (RMs  Kaski, 1998) are two equivalent
formulations deriving from the Johnson-Lindenstrauss lemma (Johnson et al., 1986), which
states that distances in a Euclidean space are approximately preserved if projected onto
a lower-dimensional random space. These formulations are also based on the fundamental
result of Hecht-Nielsen (1994), who proved that there are many more nearly orthogonal
than truly orthogonal directions in high-dimensional spaces.
RP-like methods can be formalized in terms of the projection of the original documentterm matrix M by means of a random matrix , i.e., M|D|n = M|D||F |  |F |n , where
T approximates the identity matrix, |D| and |F | indicate the number of documents and
terms in the collection, and n stands for the reduced dimensionality, which is typically
chosen in advance. The definition of the random-projection matrix  is a fundamental
aspect of the method; Achlioptas (2001) demonstrated that any random distribution with
zero mean and unit variance satisfies the Johnson-Lindenstrauss lemma, and proposed two
simple distributions for the definition of the elements ij = {ij } of the random projection
matrix, by setting the parameter distribution s of Equation 1 to either s = 2 or s = 3:

1
 +1 with probability 2s

0 with probability 1  1s
ij =
s
(1)

1
1 with probability 2s
Achlioptas proved that the configuration in which s = 3 can be used to speed up computation, since in this case only 1/3 of the data is non-zero (sparse random projection),pand
therefore 2/3 of the computations can be skipped. Similarly, Li et al. (2006) set s = |F |
and s = |F |/ log |F | (very sparse random projections) to significantly speed up the computation while still preserving the inner distances.
Random Indexing (RI), first proposed by Kanerva et al. (2000), is an equivalent formulation of RPs that also accommodates Achlioptas theory. Sahlgren (2001) defines RI as an
approximate alternative to LSA for semantic representation. RI maintains a dictionary of
random index vectors for each feature in the original space. Each random index vector consists of an n-dimensional sparse vector with k non-zero values, randomly distributed across
+1 and 1 (the method is explained in detail in Section 3). In the work of Gorman and
158

fiLightweight Random Indexing for Polylingual Text Classification

Curran (2006) different weighting criteria for random index vectors in the dictionary were
proven useful for improving the matrix representation. RI has been tested in different tasks,
such as search (Rangan, 2011), query expansion (Sahlgren, Karlgren, Coster, & Jarvinen,
2002), image and text compression (Bingham & Mannila, 2001), and event detection (Jurgens & Stevens, 2009). Fradkin and Madigan (2003) showed that, since in RI distances
are approximately preserved, distance-based learners such as k-Nearest Neighbours (k-NN)
and SVMs are preferable when learning from randomly indexed instances. Accordingly,
Sahlgren and Coster (2004) applied RI to (monolingual) text classification using SVMs,
and suggested that the random indexing representation (there dubbed Bag of Concepts 
BoCs in Sahlgren & Coster, 2004) performed comparably to the BoW representation. The
performance of RI has also been tested by Sahlgren and Karlgren (2005) and Gorman and
Curran (2006) in the realm of automatic bilingual lexicon acquisition.
The above-discussed works indicate that RI is a promising dimensionality reduction technique for representing polylingual data. Our proposal is inspired by the works of Achlioptas
(2001) and Li et al. (2006) on sparse projections by taking the level of sparsity to the extreme, and extends the application of RI in TC (Sahlgren & Coster, 2004) to PLTC, which,
to the best of our knowledge, has never been done so far. In the following section we will first
describe the method in detail, and then propose a particular setting aimed at overcoming
certain obstacles that could arise in the polylingual setting.

3. Lightweight Random Indexing for Polylingual Text Classification
Text Classification (TC) can be formalized as the task of approximating an unknown target
function  : D  C  {1, +1}, that indicates how documents ought to be classified, by
means of a function  : D  C  {1, +1}, called the classifier, such that  and  coincide
as much as possible in terms of a given evaluation metric. Here D denotes the domain
of documents, C = {c1 , c2 , ..., c|C| } is a set of predefined classes, while values +1 and 1
indicate membership and non-membership of the document in the class, respectively. We
will here consider multilabel classification, that is, the setting in which each document
could belong to zero, one, or several classes at the same time; we will consider the flat
version of the problem, in which no hierarchical relations among classes exist. We adopt
the 1 vs. all strategy, according to which the multilabel classification problem is solved as
|C| independent binary classification problems.
A document collection D can be represented via a matrix M|D||F |
 ~  

d1
w11
w12    w1|F |
 d~2   w21
w22    w2|F | 
 


M = . = .
..
.. 
..
 ..   ..
.
.
. 
w|D|1 w|D|2    w|D||F |
d~|D|

(2)

where |D| and |F | are the number of documents and features in the collection, and real
values wij represent the weight of feature fj in document di , which is usually determined
as a function of the frequency of the feature in the document and in the collection.
Polylingual Text Classification adds one fundamental aspect to TC, i.e., different documents may belong to different languages. Let  : D  L return the language in which
159

fiMoreo, Esuli, & Sebastiani

a given document is written, where L = {l1 , l2 , . . . , l|L| } is the pool of languages, |L| > 1.
S|L|
Let F = i=1 Fi denote the vocabulary of the collection, that can be expressed as the
union of the language-specific vocabularies Fi . The polylingual setting assumes that the
distribution P ((d) = li ) across the training set is approximately uniform, that is, there is
a representative quantity of labelled documents for each language.
There is usually only a small amount of shared features across languages (e.g., proper
nouns)1 , and this implies that hd~0 , d~00 i  0 if (d0 ) 6= (d00 ), where h, i denotes the dot
product. (Incidentally, this means that a direct similarity comparison among documents
expressed in different languages, e.g., using cosine-similarity, would be doomed to fail.) It
is thus possible, for any language li , to perform a reordering of the rows and columns in

M1 M2 0
the matrix that allows the polylingual matrix M to be expressed as M =
,
0 M3 M 4
where [M1 ; M
 2 ] is
 the |{d  D : (d) = li }|  |Fi | monolingual matrix representation for
M2
language li ,
is a |D|   matrix containing all the  words that are shared across two
M3
or more languages, and 0 denotes all-zero matrices.
3.1 Random Indexing
Random Indexing maps each observable problem feature into a random vector in a vector
space in which the number of dimensions is not determined by the number of different unique
features we want to map, but is instead fixed in advance. Originally, RI was proposed for
performing semantic comparisons between terms. Each document was thus mapped into a
random index vector that was then accumulated (via vector addition) into the terms row
of a term-document matrix each time the term occurred in that document. In our case, we
are instead interested in performing semantic comparisons between documents, not terms.
Thus, each term fi is assigned an n-dimensional random index vector, that is accumulated
into the j-th row of a document-term matrix every time the term is found in document dj .
Random index vectors are nearly-orthogonal, and comply with the conditions spelled out
by Achlioptas (2001) (see Section 2.3), i.e., zero-mean distribution with unit variance, so as
to satisfy the Johnson-Lindenstrauss lemma. A random index vector is created by randomly
setting k  n non-zero values, equally distributed between +1 and 1, in an n-dimensional
vector where n is typically on the order of the thousands. Once n is fixed, a recommended
choice of k in the literature is k = n/100. We dub this configuration RI1% , and will use it
in our comparative experiments. As vectors in RI1% are sparse, using sparse data structure
representations could bring about memory savings. The M|D|n = M|D||F |  |F |n matrix
multiplication (see Section 2.3) can be completely skipped, building M|D|n on-the-fly by
scanning each document and accumulating the corresponding random index vectors as each
term is read. This also avoids the need to allocate the entire matrix M|D||F | in memory.
According to Sahlgren (2005), the main advantages of RI can be summarized as follows:
the method (i) is incremental, and provides intermediate results before all the data are read
1. Note that other formulations of the polylingual problem, e.g., the ones by Amini et al. (2009) and
Prettenhofer and Stein (2010), do actually impose that if i 6= j then Fi  Fj = . This means that
shared words across languages, such as proper nouns, are given multiple representations as languagespecific features.

160

fiLightweight Random Indexing for Polylingual Text Classification

in; (ii) avoids the so-called huge matrix step (i.e., allocating the entire M|D||F | matrix
in memory), and (iii) is scalable, since adding new elements to the data does not increase
the dimensionality of the space (e.g., new features are represented via a new random index,
and not via a new dimension).
BoW matrices are typically weighted and normalized to better represent the importance
of the word to each document and to avoid giving long documents more a priori importance,
respectively. Weighting schemes could also be incorporated into the RI formalism in a
simple manner; e.g., each time a random index is added to a document row, it can first be
multiplied by the weight of that term in that document. That this brings about improved
accuracy was shown by Gorman and Curran (2006); however, in the same work it was also
shown that the incremental nature of the algorithm is sacrificed if non-linear weights are
taken into account. In our experiments, as the weighting criterion we use the well-known
tfidf method, expressed as
tfidf (di , fj ) = tf (di , fj )  log

|D|
|d  D : tf (d, fj ) > 0|

(3)

where tf (di , fj ) counts the number of occurrences of feature fj in document di ; weights are
then normalized via cosine normalization, as
wij = qP

tfidf (di , fj )

fk F

(4)

tfidf (di , fk )2

3.2 Lightweight Random Indexing
During preliminary experiments on the application of RI as a method for dimensionality
reduction, we observed that SVMs required more time to train when the training set had
been processed with RI, than with the original high-dimensional vector space (see Section
5.2). We also observed a correlation between training times and the choice of k, while the
choice of n had a smaller impact on efficiency.
Optimizing the choice of k in RI can be though of as a means to achieve two main goals:
(i) being able to encode a large number of different features in a reduced space, and (ii)
increasing the chance that two random index vectors are orthogonal.
With respect to (i), it is easy to show that, if we want to assign a different n-dimensional
index vector with
 k non-zero values to each original feature, RI could encode a maximum
of C(n, k) = nk 2k features (representation capacity). C(n, k) grows rapidly as a function
of either n or k; just as an example, C(5000, 50) 2.5  10135 . Such a huge capacity clearly
exceeds the representation requirements imposed by any current or future dataset. However,
even with small values of k the capacity becomes large enough to encode any reasonable
dataset, e.g., C(5000,2)=49,990,000 distinct features.
With respect to (ii), random-projection-based algorithms rely on the Hecht-Nielsen
(1994) lemma to find nearly orthogonal directions in a reduced space. Two vectors ~u
and ~v inPan inner product space are said to be orthogonal whenever h~u, ~v i = 0, where
h~u, ~v i = i ui vi is the dot product. Random indexes are chosen so as to be sparse in order
to increase the probability that the dot product equals zero, with non-zero products evenly
distributed between +1 and 1, leaving the expected value of the outcome close to zero. By
161

fiMoreo, Esuli, & Sebastiani

Figure 1: Probability of orthogonality of two random index vectors as a function of k and
n.

means of a Monte Carlo algorithm, we estimated the probability of orthogonality between
any two randomly generated vectors for a grid of sample values for n and k. The results,
plotted in Figure 1, reveal that smaller values of k are the main factor in favouring the
orthogonality of two random index vectors, while n has a smaller impact.
If many random index vectors lack orthogonality, the information conveyed by the original distinct features, which are predominantly pair-wise semantically unrelated, gets mixed
up, causing the learner to have more difficulty in learning meaningful separation patterns
from them. The orthogonality of random index vectors plays an even more important role
for features that are shared across languages. As shown in work by Gliozzo and Strapparava (2005), these shared words play a relevant role in bringing useful information across
languages. If their corresponding random index vectors are orthogonal with respect to all
the other vectors, the information they contribute to the process is maximized, instead of
being diluted by other less informative features.
Following the observations above, we propose the use of Random Indexing with a fixed
k = 2; we dub this configuration Lightweight Random Indexing (LRI). Our hypothesis is
that this setting could be advantageous as a mechanism to reduce dimensionality (so as
to mitigate the problem of feature disjointness in PLTC), since it is sufficient in order to
represent large feature vocabularies while also preserving vector orthogonality. Note that
choosing k = 1, when n = |F |, would be equivalent to performing a random permutation of
162

fiLightweight Random Indexing for Polylingual Text Classification

1

2

3

4

5

6

7
8

Output: Dictionary;
// Generate a random index vector for each feature
for i = 0 to (|F |  1) do
// We choose the 1st dimension sequentially
dim1  (i mod n) + 1 ;
// We choose the 2nd dimension uniformly at random
// from the dimensions not chosen in Line 2
dim2  rand({1, ..., n)} \ {dim1 }) ;
// We assign the 1st non-zero value uniformly at random
+1 
val1  rand({ 
, 12 }) ;
2
// Same for the 2nd non-zero value
+1 
val2  rand({ 
, 12 }) ;
2
// We create the sparse random index vector
random index vector  [(dim1 , val1 ), (dim2 , val2 )] ;
// We build the feature-vector mapping
Dictionary.map(fi+1 , random index vector) ;
end
Algorithm 1: Feature Dictionary for Lightweight Random Indexing.

feature indexes in a BoW representation; k = 2 is the minimum value for which an actual
RI is performed.
Algorithm 1 formalizes the process of creating a dictionary, that is, of creating a mapping
consisting of one random vector for each original feature; the mapping is created at training
time and is then used for classifying the unlabelled documents(this means that, in Line 1, F
is the set of features present in the training set). The value 1/ 2 is used instead of 1 in order
to obtain vectors of length one. Note that the two dimensions are selected in a different
manner, with the step at Line 2 ensuring that all latent dimensions are used approximately
the same number of times, and the step at Line 3 ensuring that the dimension chosen in
the previous step is not chosen twice.
Our proposal presents the following advantages with respect to standard RI1% and, in
general, with respect to any RI with k > 2:
 Each index vector has only two non-zero values. The mapping can be allocated in
memory for any number of original features, and the projection is performed very
quickly;
 Given a fixed value of n, it has a higher probability than any other instantiation of
RI of generating truly pairwise orthogonal random vectors;
 Parameter k becomes a constant that needs no tuning.

163

fiMoreo, Esuli, & Sebastiani

4. Experiments
In this section we experimentally compare our Lightweight Random Indexing (LRI) method
to other representation approaches proposed in the literature.
4.1 Baselines and Implementation Details
As the baselines against which to compare LRI we have chosen the following methods, that
we group in three categories according to their common characteristics:
Orthogonal Mappings: methods using a canonical basis for the co-occurrence matrix:
PolyBow: a classifier that operates on the juxtaposed BoW representation (PolyBow
corresponds to the NP-1C setup in Garca Adeva et al., 2005).
FS: Feature Selection on PolyBoW using Information Gain as the term scoring function and Round Robin (Forman, 2004) as the term selection policy.
Majority Voting: a multiview voting algorithm that returns the label output by
the highest number of language-specific classifiers (Amini et al., 2009).
MonoBoW: a lower bound baseline that uses a set of nave monolingual classifiers
(MonoBoW corresponds to the NP-NC setup in Garca Adeva et al., 2005).
MT: an upper bound baseline based on statistical machine translation, which translates all non-English training and test documents into English.
Random Mappings: dimensionality reduction methods relying on random projections:
RI1% : Random Indexing with k = n/100 (Sahlgren & Coster, 2004).
ACH: Achlioptas mapping with ternary distribution obtained by setting s = 3 in
Equation 1 (Achlioptas, 2001).
Non-Random Mappings: dimensionality reduction methods relying on mappings which
are not random:
CL-LSA: Cross-Lingual Latent Semantic Analysis (Dumais et al., 1997).
MDM: Multilingual Domain Models (Gliozzo & Strapparava, 2005).
PLDA: Polylingual Latent Dirichlet Allocation (Mimno et al., 2009).
We will here assume language labels are available in advance2 for both training and testing
documents. Note that RI methods and PolyBoW represent all documents in the same
feature space, irrespective of their language label. Conversely, MonoBoW keeps a separate
language-specific classifier for each language; the class label for a test document is then
decided by the classifier associated to the documents language label. We test PLDA and
Majority Voting only on the JRC-Acquis parallel corpus, since for all documents they
require a separate view in all languages to be available. Majority Voting maintains a
separate classifier for each distinct language (5 in our experiments); each test document is
thus classified after using 5 classification decisions in voting, one for each language-specific
2. This assumption is fair, as current language identification models deliver accuracies very close to 100%

164

fiLightweight Random Indexing for Polylingual Text Classification

view. For singular value decomposition we have used the Rohde (2011) package. We have
used the Haddow, Hoang, Bertoldi, Bojar, and Heafield (2016) implementation to generate
a set of statistical translation systems trained on the sentence-aligned parallel data provided
by the Europarl data release (Koehn, 2005). Note that, since we used the method described
by Gliozzo and Strapparava (2005) to automatically obtain the bilingual model in MDM,
MT is the only method using external knowledge. For PLDA we have used the Richardson
(2008) implementation, which uses Gibbs sampling; we adhere to the common practice
of fixing the budget of iterations to 1,000. We have implemented the LRI method and
the other baseline methods as part of the Esuli, Fagni, and Moreo (2016) framework. We
have used Support Vector Machines (SVMs) as the learning device in all cases, since it has
consistently delivered state-of-the-art results in TC so far; for it we used the well-known
Joachims (2009) implementation of Joachims (2005), with default parameters.
4.2 Evaluation Measures
As the effectiveness measure we use the well-known F1 , the harmonic mean of precision
() and recall () defined as F1 = (2)/( + ) = (T P )/(2T P + F P + F N ) where T P ,
F P , and F N stand for the numbers of true positives, false positives, and false negatives,
respectively. We take F1 = 1 when T P = F P = F N = 0, since the classifier has correctly
classified all examples as negative.
We compute both micro-averaged F1 (denoted by F1 ) and macro-averaged F1 (denoted
by F1M ). F1 is obtained by (i) computing the class-specific values T Pr , F Pr , and F Nr , (ii)
obtaining T P as the summation of the T Pr s (same for F P and F N ), and then applying
the F1 formula. F1M is obtained by first computing the class-specific F1 values and then
averaging them across all classes. The fact that F1M attributes equal importance to all
classes means that low-frequency classes will be as important as high-frequency ones in
determining F1M scores; F1 is instead more influenced by high-frequency classes than by
low-frequency ones. High values of F1M thus tend to indicate that the classifier performs well
also on low-prevalence classes, while high values of F1 may just indicate that the classifier
performs well on high-prevalence classes.
4.3 Datasets
We have performed our experiments on two publicly available corpora, RCV1/RCV2 (a
comparable corpus) and JRC-Acquis (a parallel corpus).
4.3.1 RCV1/RCV2
RCV1 is a publicly available collection consisting of the 804,414 English news stories generated by Reuters from 20 Aug 1996 to 19 Aug 1997 (Lewis, Yang, Rose, & Li, 2004). RCV2
is instead a polylingual collection, containing over 487,000 news stories generated in the
same timeframe in thirteen languages other than English (Dutch, French, German, Chinese,
Japanese, Russian, Portuguese, Spanish, LatinoAmerican Spanish, Italian, Danish, Norwegian, Swedish). The union of RCV1 and RCV2 (hereafter referred to as RCV1/RCV2) is
a corpus comparable at topic-level, as news stories are not direct translations of each other
are but simply refer to the same or to related events in different languages. Since the cor165

fiMoreo, Esuli, & Sebastiani

pus is not parallel, each training document for a given language in general does not have a
counterpart in the other languages.
From RCV1/RCV2 we randomly selected 8,000 news stories for 5 languages (English,
Italian, Spanish, French, German) pertaining to the last 4 months (from 1997-04-19 to
1997-08-19), and we performed a 70%/30% train/test split, thus obtaining a training set
of 28,000 documents (5,600 for each language) and a test set of 12,000 documents (2,400
for each language)3 . In our experiments we have restricted our attention to the 67 classes
(out of 103) with at least one positive training example for each of the five languages.
The average number of classes per document is 2.92, ranging from a minimum of 1 to a
maximum of 11; the number of positive examples per class/language combination ranges
from a minimum of 1 to a maximum of 4,182.
We preprocessed the corpus by removing stop words and by stemming terms using
the Porter stemmer for English, and the Snowball stemmer for the other languages. This
resulted in a total of 123,258 stemmed terms, distributed across languages as shown in Table
1.

English
Italian
Spanish
French
German

English
40,483

Italian
3,420
14,762

Spanish
6,559
3,752
30,077

French
6,370
3,300
6,139
26,961

German
3,921
1,929
3,014
3,441
38,232

Appearing in
1 languages
2 languages
3 languages
4 languages
5 languages

#
106,182
10,474
3,851
1,923
828

Table 1: Feature distribution across languages for the RCV1/RCV2 comparable corpus.
In the leftmost part of the table, the cell in row i and column j represents the
number of features that are shared across the i-specific and the j-specific sections
of the dataset. (The table is symmetric, so for better clarity the entries below the
diagonal have been omitted.) The rightmost part of the table indicates how many
features are shared across x language-specific sections of the dataset.

4.3.2 JRC-Acquis
The JRC-Acquis corpus (version 3.0) is a version of the Acquis Communautaire collection
of parallel legislative texts from European Union law written between the 1950s and 2006
(Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, & Varga, 2006). JRC-Acquis is
publicly available for research purposes, and covers 22 official European languages. The
corpus is parallel at the sentence-level, i.e., each document exists in all 22 languages, as a
sentence-by-sentence translation. The corpus is labelled according to the ontology-based
EuroVoc thesaurus, which consists of more than 6,000 classes; for our experiments we have
restricted our attention to the 21 classes in the top level of the EuroVoc hierarchy.
3. All the information required to replicate the experiments, e.g., IDs of the selected documents, assigned
labels, etc., is publicly available (Moreo, 2016). The source code we used in our experiments is accessible
as part of the Esuli et al. (2016) framework

166

fiLightweight Random Indexing for Polylingual Text Classification

English
Italian
Spanish
French
German

English
150,866

Italian
77,878
150,838

Spanish
80,220
95,515
143,712

French
89,573
90,522
88,561
147,077

German
98,740
78,919
85,434
86,905
228,834

Appearing in
1 languages
2 languages
3 languages
4 languages
5 languages

#
249,216
42,566
33,305
22,171
59,676

Table 2: Feature distribution across languages for the JRC-Acquis parallel corpus; the
meaning of the cells is the same as in Table 1. Note the high number of features (59,676) which appear in all five languages; this is due to the presence of
proper names, which are the same in all languages. Note also the high number of
features (228,834) which are unique to the German language: this is due to the
presence of word compounds, a phenomenon present in the German language but
not in the other four languages.

We have selected the 7,235 texts from 2006 for 5 languages (English, Italian, Spanish,
French, and German) and removed documents without labels, thus obtaining 6,980 documents per language. We have taken the first 70% documents for training (24,430, i.e.,
4,886 for each language) and the remaining 30% (10,470, i.e., 2,094 for each language) for
testing. The average number of classes per document is 3.5, ranging from a minimum of
1 to a maximum of 10; the number of positive examples per class/language combination
ranges from a minimum of 47 to a maximum of 2,011.
The same preprocessing as for RCV1/RCV2 was carried out on this dataset, obtaining 406,934 distinct features distributed across languages as shown in Table 2. Since the
JRC-Acquis corpus is parallel, each language-specific document is guaranteed to have a
counterpart in each of the other languages, which results in a relatively large number of
terms (e.g., proper nouns) appearing in several languages. Note that, despite the fact that
the dataset is parallel at the sentence level, we are interested in indexing entire documents
as a whole, and thus disregard sentence order; we thus consider the corpus as parallel at
the document level.
We use the JRC-Acquis corpus in order to test the performance of LRI in cases in
which the co-occurrence matrix has been compacted, as defined in the work of Dumais et al.
(1997). More precisely, the compact representation of |L| translation-equivalent documents
is a vector consisting of the concatenation of the |L| vectors that each represent one (monolingual) such document. This is different from the juxtaposed representation used in the
previous chapters, where the vector corresponding to one monolingual document has all
zeros in the positions corresponding to the features of the other languages. The compact
matrix can thus be obtained from the matrix resulting from the juxtaposed representations
by compressing |L| rows into a single (compact) row storing their sum.
167

fiMoreo, Esuli, & Sebastiani

4.4 Results
In this section we present the results of our experiments. We first compare LRI to a set
of monolingual classifiers (Section 4.4.1), and then we explore the dimensionality reduction
aspect of the polylingual problem (Section 4.4.2).
4.4.1 Polylingual Information
As a first case of study, we investigate how much the addition of polylingual information
affects the accuracy of a monolingual classifier. In this scenario, we compare LRI and
PolyBoW, which train on documents from all languages, with the lower bound MonoBoW,
which trains only on documents of the same language of test documents, and with the upper
bound MT, that first translates all training and test documents into English. Note that
the MT baseline is not tested in the JRC-Acquis corpus because each of the documents is
already available as a direct translation in all languages. In this experiment the vector space
is not being reduced, i.e., we set n = |F | for LRI so that the vector spaces for PolyBoW
and LRI have the same number of dimensions. Values for LRI were averaged after 10 runs.
The results illustrated in Figure 2 show that the simple addition of examples in different languages (PolyBoW) brings about an improvement in accuracy with respect to the
monolingual solution (MonoBoW). This improvement is likely achieved thanks to the words
shared across languages. However, LRI clearly outperforms PolyBoW. The improvements
of PolyBoW over MonoBoW range from -0.4% to +29.7%, while LRI achieves improvements ranging from +9.7% to +41.1%; when LRI obtains its smallest improvement over
MonoBoW in terms of F1M (on Italian, +9.7%), PolyBoW performs slightly worse than
MonoBoW (-0.4%). The improvements are more marked for F1M than for F1 , indicating
that the improvements especially take place in the more infrequent classes, which have a
substantial impact on F1M but not on F1 .
In general, training on documents coming from all languages (PolyBoW, LRI, and MT)
seems to be preferable to training from language-specific documents only (MonoBoW). This
is particularly so for the MT baseline, which obtained the best results in all cases with the
sole exception of English, where LRI obtained the best result. This exception might be
explained by the fact that automatically translated documents tend to exhibit different
statistical properties with respect to documents written by humans, which means that the
English test documents (which are not translations) might not be in tune with the training
documents (which are mostly the result of automatic translation).
The language-specific classification performance is much more homogeneous in JRCAcquis than in RCV1/RCV2. This can be explained by the fact that JRC-Acquis is a parallel
corpus, and therefore each language benefits from the very same information. There is no
significant difference in performance among the different languages, which means that the
effects due to the different difficulty of the various languages are minor. Instead, differences
in RCV1/RCV2 can be explained by the different amount of information that the training
sets carry on the corresponding test sets. For example, the Spanish classifier is the worst
performer, and is the one that obtains the best benefit (with respect to the MonoBoW
baseline) from the addition of polylingual information (as in PolyBoW, LRI, and MT).
168

fiLightweight Random Indexing for Polylingual Text Classification

Figure 2: Monolingual classification on RCV1/RCV2 (top) and JRC-Acquis (bottom), using F1M (left) and F1 (right) as the evaluation measure.

Note that in this experiment the matrices that PolyBoW and LRI feed to the learning
algorithm are of the same size. The difference between the two methods, which is the likely
cause for their difference in effectiveness, is that in PolyBoW the useful dimensions for a
specific language are packed in a specific portion of the vector space, while LRI spreads
them across the entire vector space, causing all dimensions to become potentially useful
for all languages. Note that this substantial increase in the number of useful dimensions
available for each language allows the model to create more easily separable representations.
We further discuss this aspect in Section 5.3.
4.4.2 Dimensionality Reduction
In the PolyBoW setup the dimensionality of the vector space is substantially increased
when more languages are considered during training. The following experiments explore
the dimensionality reduction aspect of the problem, and address a realistic polylingual
scenario, where both training and test data contain examples for each language.
169

fiMoreo, Esuli, & Sebastiani

n
MonoBoW
PolyBoW
MT
CL-LSA
MDM
ACH
RI1%
LRI

500
0.273
0.365
0.472
0.366
0.426
0.375

1,000
0.353
0.399
0.493
0.389
0.483
0.464

F1M
5,000
0.444
0.513
0.539
0.547

10,000
0.472
0.530
0.543
0.554

full
0.473
0.498
0.509
0.570

500
0.668
0.765
0.769
0.621
0.683
0.679

1,000
0.736
0.777
0.771
0.610
0.705
0.736

F1
5,000
0.786
0.736
0.756
0.792

10,000
0.795
0.755
0.775
0.802

full
0.802
0.804
0.808
0.811

Figure 3: Effects of dimensionality reduction on RCV1/RCV2 (English and Italian). Dotted
lines indicate reference values, e.g., green and red lines represent the performance
of LRI and PolyBoW, respectively, when dimensionality is not reduced. Values
in bold highlights the best performing method for each dimension.

We first run a sample bilingual experiment on RCV1/RCV2 (as the language other
than English we have picked Italian). The total amount of features in this dataset is
51,828. Restricting the experiment to two languages allows us to compare LRI (i) against
methods that were proposed for bilingual representations (MDM), and (ii) against methods
that would be too computationally expensive if considering more languages (such as ACH,
see below). We explore the effect of dimensionality reduction, with the number of selected
features ranging from 500 to 10,000 (Figure 3). We adhere to the common practice that
establishes a number of dimensions ranging from 500 to 1000 in LSA and MDM. Results
for random projection methods (ACH, RI1% , and LRI) are averaged after 10 runs.
LRI obtains good results on both macro- and micro-averaged F1 , while the other methods exhibit alternating performance on the two measures. RI1% obtains comparable results
in terms of F1M but performs poorly on F1 ; in contrast, PolyBoW performs comparably
in terms of F1 but worse in terms of F1M . A two-tailed t-test on paired examples reveals
that the difference in terms of F1M between LRI and RI1% is not statistically significant,
170

fiLightweight Random Indexing for Polylingual Text Classification

n
MonoBoW
PolyBoW
MT
CL-LSA
RI1%
LRI

500
0.254
0.351
0.300
0.270

1,000
0.308
0.384
0.402
0.376

F1M
5,000
0.420
0.482
0.491

10,000
0.445
0.501
0.511

full
0.415
0.483
0.521
0.528

500
0.606
0.728
0.580
0.573

1,000
0.657
0.746
0.649
0.659

F1
5,000
0.747
0.696
0.749

10,000
0.764
0.733
0.766

full
0.753
0.781
0.793
0.786

Figure 4: Accuracy of different PLTC methods on RCV1/RCV2 on 5 languages, for different
levels of dimensionality reduction.

and that LRI significantly outperforms RI1% in F1 and the rest of dimensionality reduction
methods for both evaluation measures, with p < 0.001. Surprisingly, CL-LSA and MDM
perform worse than the nave classifier (MonoBoW) with all features. However, it should
be remarked that they outperform all other baselines with only 500 and 1000 dimensions.
As will be seen in Section 5, apart from the drastic dimensionality reduction, these methods are affected by large computational costs that negatively impact on the run times and
memory resources needed. Consistently with our previous observations (see Figure 2), LRI,
PolyBoW, MonoBoW, and MT are comparable in terms of F1 , but LRI outperforms all
tested algorithms in terms of F1M .
To test the scalability of our method when several languages are involved, we extend the
experiment to five languages (English, Italian, Spanish, French, German) in RCV1/RCV2
(Figure 4). Note that in this case not all algorithms were able to complete their execution
due to memory constraints, hence the incomplete plots and table; concretely, ACH and the
last iterations for RI1% overflowed memory resources when trying to allocate a 28, 000 
123, 258 matrix. More insights about space and time complexity are reported in Section 5.
Results for RI1% and LRI are the average of 10 runs that use different random seeds.
These results confirm the previous observations. RI1% behaves similarly to LRI in terms
of F1M (i.e., with no statistically significant difference) but worse in terms of F1 (p <0.001),
171

fiMoreo, Esuli, & Sebastiani

n
PolyBoW
CL-LSA
PLDA
Majority Vote
RI1%
LRI

500
0.365
0.570
0.456
0.543
0.524

1,000
0.416
0.593
0.463
0.581
0.581

F1M
5,000
0.534
0.655
0.659

10,000
0.570
0.680
0.672

full
0.640
0.656
0.688

500
0.560
0.725
0.644
0.656
0.660

1,000
0.606
0.739
0.650
0.676
0.702

F1
5,000
0.697
0.743
0.764

10,000
0.723
0.770
0.776

full
0.768
0.759
0.789

Figure 5: Accuracy of different PLTC methods on JRC-Acquis on 5 languages, for different
levels of dimensionality reduction.

while PolyBoW behaves in an opposite way, i.e., performs worse than LRI in terms of
F1M (p < 0.001) and comparably in terms of F1 . As a dimensionality reduction method,
LRI thus outperforms the other methods when considering both F1M and F1 ; when no
dimensionality reduction is applied, only the upper bound MT is comparable to LRI in
both F1M and F1 .
Finally, we used JRC-Acquis to reproduce one last polylingual scenario, namely, one
in which texts are aligned at the document level. Even if this situation is not common in
practice (exceptions include, say, proceedings of official events), this scenario is interesting
since such a dataset may serve as a test bed for multiview learning methods (Amini et al.,
2009). Since documents in JRC-Acquis were translated by humans, results are not affected
by any noise MT tools might introduce. Figure 5 shows the results obtained considering the
compacted matrix of JRC-Acquis (a 4, 886  406, 934 matrix), on which we also tested Majority Voting, which combines the classification decisions of the five independently trained
MonoBoW classifiers on the parallel versions of the documents, and PLDA, that first defines
the generative model based on polylingual topics and then trains and tests on the probability distributions over topics assigned to each document. We set the number of polylingual
latent topics to 500 and 1000, respectively.
LRI is clearly superior to PolyBoW in this case. The difference in performance between
LRI and RI1% seems to be lower in this case, especially in terms of F1M ; the t-test revealed
172

fiLightweight Random Indexing for Polylingual Text Classification

however that LRI is superior to RI1% in a statistically significant sense (p <0.001). However,
it should be considered that LRI delivers its best performance without reducing the dimensionality of the polylingual matrix, while RI1% is not able to accomplish the projection due
to memory restrictions; this is something we will expand on in the following section. PLDA,
in turn, succeeded in discovering polylingual topics that were aligned across languages, but
proved less effective in terms of classification performance.

5. Analysis
During our experiments we observed substantial differences in terms of efficiency among
some of the compared methods, particularly ACH, RI1% , and LRI. For example, RI1%
exhausted memory resources for n  10, 000, while LRI was able to represent even the fullsized |D||F | matrix (see Figure 4). Given the strong relationship between the two methods,
we would have expected they delivered similar performance. This anomaly prompted us to
investigate the issue more in depth. This section presents an analytical study in terms of
efficiency of the methods discussed in the previous section.
5.1 Space Efficiency
Data samples in ML are usually represented as a co-occurrence matrix. In TC this matrix
suffers from high-dimensionality, but luckily enough it is also sparse. A sparse, low-density
matrix suggests the use of a non-exhaustive data-structure, in which zero values are not
stored explicitly.
The random projection has a direct impact on sparsity. For each feature contained in a
document, k non-zero values are placed in the projected matrix. For ACH the situation is
worse, since each feature is mapped, on average, into n/3 non-zero values. As an example,
for n = 5, 000 each feature will be mapped into 50 and 1,666 non-zero values in RI1% and
in ACH, respectively.
As an example, we have rerun our RCV1/RCV2 experiments with English and Italian
as the only languages, and examined their matrix density (percentage of non-zero values
over the total matrix size) and memory footprint (absolute number of non-zero values). The
results are displayed in Figure 6.
LRI requires double the space with respect to standard BoW, but succeeds in preserving
sparsity, while RI1% drastically increases the matrix density and produces a large memory
footprint. MDM, LSA, and ACH operate on dense matrices. However, since both MDM and
LSA produce an extreme dimensionality reduction, the overall memory footprint remains
much lower than that of RI1% and, especially, of ACH. When n = |F |, LRI must allocate
about 1, 844103 values (this is indicated as LRI (full) in Figure 6), while RI1% (n = 5000)
must allocate about 28, 463  103 values (requiring 15.42 times more space); ACH (n = 5000)
must allocate 55, 998  103 values (30.35 times more space). Note that even though MDM
and LSA reduce significantly the dimensionality (e.g., from 51,828 to 500, or 1,000), they
need to allocate more values in memory than LRI (full).
As an example, let us suppose that each non-zero value is represented as a double
(typically: 8 bytes in most modern programming languages); this means we roughly need
428MB for ACH and 218MB for RI1% , whereas LRI requires only 15MB. Although the
difference is substantial, (even taking into account that the actual memory needed is higher
173

fiMoreo, Esuli, & Sebastiani

Figure 6: Matrix density (left) and memory footprint (right) in the RCV1/RCV2 EnglishItalian run (11, 200  51, 828 full training matrix size).

if the values are indexed in a hash table) they still do not represent any real problem in
terms of space for most modern computers. However, note that the matrix is not the only
data structure we need to allocate in memory. Also the mapping dictionary, i.e., the data
structure linking each original feature to its random index vector, should be allocated in
memory. The dictionary will be queried as many times as there are terms in any document
we want to classify. If the dictionary is small enough (which it is in LRI), we may be able
to allocate it in cache in order to significantly speed up the indexing of new documents.
Assuming a sparse representation, a random index vector can be described as a list of k
pairs (di , vi ), where di indicates a latent dimension and vi encodes its value. For example,
for k = 2 the random vector (0, 0, +1, 0, 1, 0, ...) could be represented as [(3, 1), (5, 0)],
where a bit set to 1 encodes +1 and a bit set to 0 encodes 1. As from Equation 5,
the space occupation for the dictionary of a random indexing method depends on (i) |F |,
the number of indexes; (ii) k, the number of non-zero values for each index; and (iii) the
number of bits needed to indicate one latent position and to encode all possible non-trivial
values; that is,
Cost(RIk ) = O(|F |  k (log2 n + log2 2))

(5)

It turns out that, given that the expected number of non-zero values for ACH is n/3, using
a dense representation for each index is cheaper. Each position thus indicates one of the
three possible values for the index. The cost in terms of space of the ACH index dictionary
is described by
Cost(ACH) = O(|F |  n  log2 3)
174

(6)

fiLightweight Random Indexing for Polylingual Text Classification

Method
LRI
RI1%
ACH

Index type
sparse
sparse
dense

Index size
2
100
10,000

Index cell
log2 n + log2 2 bits to encode dimi and vali , resp.
log2 n + log2 2 bits to encode dimi and vali , resp.
log2 3 bits to encode ij

Memory required
1.39MB
69.31MB
768.87MB

Table 3: Memory occupation for the feature dictionary for different random mapping methods on the JRC-Acquis dataset (|F | = 406, 934). The meanings of dimi and vali
are as in Algorithm 1. The meaning of ij is as in Equation 1.

Assuming the reduced dimensionality is set to a fixed percentage of the original dimensionality, i.e., n = |F | with 0 <   1, the following hold:
Cost(RI) = O(|F |2 log2 |F |) >
Cost(ACH) = O(|F |2 ) >

(7)

Cost(LRI) = O(|F | log2 |F |)
However, the hidden constants play a key role in practice. As an example, we have computed
the total amount of memory required for each method for storing the index dictionaries for
n = 10, 000 in JRC-Acquis, where |F | = 406, 934; the resulting values are reported in Table
3. As it can be observed, for the index dictionary ACH requires 769MB, while the space
required for the RI-based versions is one to three orders of magnitude smaller. In other
words, the index dictionary for LRI could easily fit in current cache memories, while RI1%
and ACH need to resort to higher-capacity, and thus slower, storage devices.
5.2 Time Efficiency
It is usually the case that sparsity benefits not only space occupation, but also execution
time. As an example, the computational cost of SVD is O(|F |2 |D|) for a document-by-term
matrix; however, the implementation SVDLIBC is specifically optimized for sparse matrices
and requires O(c|F ||D|) steps, where c is the average number of non-zero values in a vector.
In Figure 7 we plot run times for the experiments on the bilingual (English-Italian)
RCV1/RCV2 experiment by paying attention to the time required for (i) obtaining the
transformed index for the training set, (ii) training the learning algorithm (SVM), (iii)
obtaining the transformed index for the test set, and (iv) classifying the test documents.
All the experiments were run on an Intel i7 64bit processor with 12 cores, running at
1,600MHz, and 24GBs RAM memory.
The results show that it takes about 3.5 minutes to generate and test the classifier
that uses the BoW representation. Time is slightly reduced to about 3 minutes when only
5000 features are selected. The total time for LRI is roughly higher by a factor of 2, up
to 7.3 (full) and 6.6 (n = 5000) minutes, respectively. Notwithstanding this, these figures
are still low when compared to the other methods: both training and testing times grow
very substantially for RI and ACH. Regarding latent methods, it should be pointed out
that the time required for preparing the matrices also grow substantially, due to the large
175

fiMoreo, Esuli, & Sebastiani

Figure 7: Run times on RCV1/RCV2 (English and Italian setting).

computational cost inherent in SVD and matrix multiplication, while in the case of random
indexing methods these times are negligible.
By comparing the overall memory footprint (Figure 6, right) with execution times (Figure 7) it seems clear that there is a strong correlation between them. We have investigated
this dependency in our experiments by computing the Pearson correlation between them.
The Pearson correlation quantifies the degree of linear dependence between two variables,
and ranges from 1, meaning perfect negative correlation, to +1, meaning perfect positive
correlation, whereas 0 means that there is not any linear dependency. We found a linear
Pearson correlation of +0.988 and +0.998 between the number of non-zero values in the
matrix and times required for training and testing, respectively, which brings additional
support to our observation: preserving sparsity during the projection favours execution
times in PLTC.
5.3 The Effect of k in Random Indexing
Previous work in RI (see, e.g., Sahlgren & Karlgren, 2005; Sahlgren & Coster, 2004) tend to
set k to about 1% of the dimension of the vector; smaller values of k (about k = 0.1%) have
also been explored (Karlgren, Holst, & Sahlgren, 2008). Other works related to random
projections (see, e.g., Achlioptas, 2001; Li et al., 2006) have noticed that sparse projection
matrices help to speed up computation.
Besides run times, sparsity in the projection matrix also affects the orthogonality of
the random projection, which in turn has an impact on the preservation of the relative
distances. Two random vectors ri and rj are said to be orthogonal if the angle between
them is 90 degrees. Although the probability that any two randomly picked vectors are
orthogonal increases as the dimensionality of the vector space grows (Karlgren et al., 2008),
most random projection approaches choose sparse random vectors, so as to maximize this
probability.
176

fiLightweight Random Indexing for Polylingual Text Classification

Figure 8: Probability distribution of the angle between any two arbitrary vectors in highdimensional space (left), and excess kurtosis as a function of the non-zero values
in a projection matrix of 10,000 dimensions (right).

We could thus establish a parallelism between the degree of orthogonality of any projection matrix and the probability distribution of the angle of any two of its random vectors.
The more this probability distribution is skewed towards 90 degrees, the closer to orthogonal
the projection base is, and the better distances are preserved. We propose to quantify the
orthogonality by means of the excess kurtosis of the distribution of this angle4 . To this aim,
we have studied how the kurtosis of the angle distributions (as estimated via a Monte Carlo
algorithm) varies as a function of the matrix sparsity k for any 10,000-dimension projection
matrix (Figure 8, right).
Figure 8 shows that the orthogonality of the projection, for a fixed dimensionality,
rapidly degrades as the density increases. LRI is thus expected to produce the most nearly
orthogonal indexing, followed by RI and then by ACH.
We have further investigated the relation between orthogonality and PLTC accuracy.
To this aim, we have run a series of experiments on the bilingual version of RCV1/RCV2,
varying (from 2 to 100) the number k of non-zero values and (from 1,000 to 10,000) the
reduced dimensionality n. Figure 9 shows the contour lines (equally valued points in the
3-dimensional representation) for classification performance (here measured in terms of
F1 ), execution time, and probability of pairwise orthogonality (i.e., the probability that
hri , rj i = 0 for any two randomly chosen random index vectors).
The following trends can be directly observed from the results: (i) accuracy improves
as n increases and k decreases; (ii) run times tend to grow when both n and k increase,
and (iii) the higher the dimensionality n and the smaller the parameter k, the higher the
probability of finding two orthogonal random indexes.
4. The excess kurtosis of a random variable X is typically defined as its fourth standardized moment minus
3, i.e., EKurt[X] = 44  3.

177

fiMoreo, Esuli, & Sebastiani

Figure 9: Impact of dimensionality n (on the x axis) and number k of non-zero values (on the
y axis) on classification accuracy (left), execution time (center), and probability
of finding an orthogonal pair of random indexes (right). Darker regions represent
lower values.

In Figure 9, the behaviour of the LRI method we propose is described by the green
horizontal line at the bottom of each plot, while RIs behaviour is described by the blue
diagonal line from coordinates (n = 1, 000, k = 10) to (n = 10, 000, k = 100). The performance in RI improves at the cost of space and time efficiency, and by gradually disrupting
the orthogonality of the base. On the contrary, the following desirable features of LRI are
evident: when dimensionality increases (i) accuracy improves without penalizing execution
times, due to the preservation of sparsity, and (ii) the orthogonality of the base is improved.

6. Conclusions
We have compared several techniques for polylingual text classification, checking their suitability as dimensionality reduction techniques and as techniques for the generation of alternative representations for the co-occurrence matrix, on two PLTC benchmarks (one parallel
and one comparable). Our investigation indicates that reducing the dimensionality of the
data is not sufficient if reasonable efficiency (in terms of both time and space) is required.
Based on this observation we have proposed a variant of Random Indexing, a method originated within the IR community that, to the best of our knowledge, was never tested in
PLTC up to date. Our proposal, Lightweight Random Indexing, yielded the best results not
only in terms of (both time and space) efficiency, but also in terms of classification accuracy,
for which Lightweight Random Indexing obtained the best results both in terms of macroand micro-averaged F1 . Lightweight Random Indexing preserves matrix sparsity, which
means that both memory footprint and training time are not penalized. For example, from
Figures 6 and 7 we may see that Lightweight Random Indexing (in the full configuration
 that is, where the random vectors have the same dimensionality of the original space)
improved over Latent Semantic Analysis (in the n = 1, 000 configuration  that is, where
178

fiLightweight Random Indexing for Polylingual Text Classification

the dimensionality of the reduced space is 1,000) by a margin of +4.37% in terms of F1
with an 89.69% reduction in execution time and an 82.60% reduction in memory footprint.
Even though Lightweight Random Indexing works very well as a dimensionality reduction method, it achieves its best performance when the projection does not reduce
the original dimensionality. Apparently, the BoW representation might be expected to be
preferable in such a case, because it is truly orthogonal. However, in the polylingual BoW
representation most of the features are only informative for a restricted set of the data; e.g.,
a German term has an entire dimension reserved for it in the vector space model, and this
dimension is useful only for documents written in German. Random projections instead
map the feature space into a space that is shared among all languages at once. The effect
is that any dimension of the space becomes informative to represent documents regardless
of the language they were originally written in. This configuration, in which the projection
space is larger than the actual number of different features for a single language, is reminiscent of the kernel-trick effect, because the informative space for each language is enlarged
and thus becomes more easily separable.
In the light of our experiments, Lightweight Random Indexing has important advantages
with respect to previous PLTC approaches. First, the method is machine translationfree, dictionary-free, and does not require any sort of additional resources apart from the
labelled collection. The projected matrix preserves sparsity, which has a direct effect in
reducing both running time and total memory usage. With respect to the original random
indexing technique, Lightweight Random Indexing presents the following advantages: (i) the
probability of finding a pair of truly orthogonal indexes is higher; (ii) it requires less memory
to allocate the index dictionary; and (iii) it avoids the need for tuning the k parameter.
LRI has proven to be very effective in PLTC, and we conjecture it could bring similar
benefits in other related tasks, such as CLTC, cross-lingual information retrieval, as well as
when tackling problems dealing with sparse and heterogeneous sources of data in general. As
discussed above, one of the reasons why k = 2 is a safe configuration is that it still preserves
the representation capacity. However, this might not hold under all circumstances; e.g.,
when processing huge streams of very dynamic data (e.g., streams of tweets), at a certain
point the representation capacity might saturate if the dimensionality of the space has not
been chosen carefully. In these cases, opting for configurations with k > 2 might mitigate
the problem.
Another fact that emerges from our experiments is that dimensionality reduction is not
necessarily a synonym of computational efficiency. The reason is that modern secondary
storage data structures are optimized to operate on sparse data, and when the dimensionality is drastically reduced, matrix density may increase, and the net effect may be a decrease
in efficiency. A true benefit is thus achieved to the extent that the trade-off between sparsity
and separability is preserved; on this dimension, LRI proved extremely effective.
Although results are encouraging, further investigations are still needed to shed some
light on the foundations of random projection methods. A first question is whether there is
any criterion to better choose the random index vectors; given that the current criterion is
random, it seems there might be room for better motivated strategies, possibly by leveraging
class labels or by taking into account the document language labels. Considering that
Random Indexing was originally proposed in the context of the IR community, we wonder
whether the proposed approach could produce similar improvements on IR tasks such as
179

fiMoreo, Esuli, & Sebastiani

query expansion or bilingual lexicon acquisition. Finally, it could be interesting to combine
Lightweight Random Indexing with Reflexive Random Indexing (Cohen, Schvaneveldt, &
Widdows, 2010; Rangan, 2011), a more recent formulation of the model that iteratively
alternates between row indexing and column indexing in the original co-occurrence matrix.

Acknowledgements
Fabrizio Sebastiani is on leave from Consiglio Nazionale delle Ricerche, Italy.

References
Achlioptas, D. (2001). Database-friendly random projections. In Proceedings of the 20th
ACM Symposium on Principles of Database Systems (PODS 2001), pp. 274281,
Santa Barbara, US.
Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representations for
multilingual NLP. In Proceedings of the 17th Conference on Computational Natural
Language Learning (CoNLL 2013), pp. 183192, Sofia, BL.
Amini, M.-R., & Goutte, C. (2010). A co-classification approach to learning from multilingual corpora. Machine Learning, 79 (1/2), 105121.
Amini, M.-R., Usunier, N., & Goutte, C. (2009). Learning from multiple partially observed
views; An application to multilingual text categorization. In Proceedings of the 23rd
Annual Conference on Neural Information Processing Systems (NIPS 2009), pp. 28
36, Vancouver, CA.
Baroni, M., Dinu, G., & Kruszewski, G. (2014). Dont count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL
2014), pp. 238247, Baltomore, US.
Bel, N., Koster, C. H., & Villegas, M. (2003). Cross-lingual text categorization. In Proceedings of the 7th European Conference on Research and Advanced Technology for
Digital Libraries (ECDL 2003), pp. 126139, Trondheim, NO.
Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine
Learning, 2 (1), 1127.
Bengio, Y., Schwenk, H., Senecal, J.-S., Morin, F., & Gauvain, J.-L. (2006). Neural probabilistic language models. In Innovations in Machine Learning, pp. 137186. Springer,
Heidelberg, DE.
Bingham, E., & Mannila, H. (2001). Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the 7th ACM International
Conference on Knowledge Discovery and Data Mining (KDD 2001), pp. 245250, San
Francisco, US.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 9931022.
180

fiLightweight Random Indexing for Polylingual Text Classification

Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations from word
co-occurrence statistics: A computational study. Behavior Research Methods, 39 (3),
510526.
Chandar, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V. C., & Saha,
A. (2014). An autoencoder approach to learning bilingual word representations. In
Proceedings of the 28th Annual Conference on Neural Information Processing Systems
(NIPS 2014), pp. 18531861, Montreal, CA.
Cohen, T., Schvaneveldt, R., & Widdows, D. (2010). Reflective random indexing and indirect inference: A scalable method for discovery of implicit connections. Journal of
Biomedical Informatics, 43 (2), 240256.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) from scratch. Journal of Machine Learning
Research, 12, 24932537.
de Melo, G., & Siersdorfer, S. (2007). Multilingual text classification using ontologies. In
Proceedings of the 29th European Conference on Information Retrieval (ECIR 2007),
pp. 541548, Roma, IT.
Deerwester, S. C., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. A.
(1990). Indexing by latent semantic analysis. Journal of the American Society for
Information Science, 41 (6), 391407.
Dumais, S. T., Letsche, T. A., Littman, M. L., & Landauer, T. K. (1997). Automatic crosslanguage retrieval using latent semantic indexing. In Working Notes of the AAAI
Spring Symposium on Cross-language Text and Speech Retrieval, pp. 1824, Stanford,
US.
Ehrmann, M., Cecconi, F., Vannella, D., McCrae, J. P., Cimiano, P., & Navigli, R. (2014).
Representing multilingual data as linked data: The case of BabelNet 2.0. In Proceedings of the 9th Conference on Language Resources and Evaluation (LREC 2014), pp.
401408, Reykjavik, IS.
Esuli, A., Fagni, T., & Moreo, A. (2016). JaTeCS (Java Text Categorization System). In
Github. Retrieved September 11, 2016, from https://github.com/jatecs/jatecs.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL 2014), pp. 462471, Gothenburg,
SE.
Forman, G. (2004). A pitfall and solution in multi-class feature selection for text classification. In Proceedings of the 21st International Conference on Machine Learning
(ICML 2004), pp. 3845, Banff, CA.
Fradkin, D., & Madigan, D. (2003). Experiments with random projections for machine
learning. In Proceedings of the 9th ACM International Conference on Knowledge
Discovery and Data Mining (KDD 2003), pp. 517522, Washington, US.
Garca Adeva, J. J., Calvo, R. A., & Lopez de Ipina, D. (2005). Multilingual approaches to
text categorisation. European Journal for the Informatics Professional, 6 (3), 4351.
181

fiMoreo, Esuli, & Sebastiani

Gliozzo, A., & Strapparava, C. (2005). Cross-language text categorization by acquiring
multilingual domain models from comparable corpora. In Proceedings of the ACL
Workshop on Building and Using Parallel Texts, pp. 916, Ann Arbor, US.
Gliozzo, A., & Strapparava, C. (2006). Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization. In Proceedings of the 44th Annual
Meeting of the Association for Computational Linguistics (ACL 2006), pp. 553560,
Sydney, AU.
Gorman, J., & Curran, J. R. (2006). Random indexing using statistical weight functions.
In Proceedings of the 4th Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pp. 457464, Sydney, AU.
Gouws, S., & Sgaard, A. (2015). Simple task-specific bilingual word embeddings. In
Proceedings of the North American Chapter of the Association for Computational
Linguistics and Human Language Technologies Conference (NAACL-HLT 2015), pp.
13861390.
Haddow, B., Hoang, H., Bertoldi, N., Bojar, O., & Heafield, K. (2016). MOSES statistical
machine translation system. In Moses website. Retrieved September 11, 2016, from
http://www.statmt.org/moses/.
Harris, Z. S. (1968). Mathematical structures of language. Wiley, New York, US.
Hecht-Nielsen, R. (1994). Context vectors: General-purpose approximate meaning representations self-organized from raw data. In Computational Intelligence: Imitating Life,
pp. 4356. IEEE Press.
Hermann, K. M., & Blunsom, P. (2014). Multilingual models for compositional distributed
semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pp. 5868, Baltimore, US.
Joachims, T. (2009). SVMperf: Support Vector Machine for multivariate performance
measures. In Cornell University website. Retrieved September 11, 2016, from
http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html.
Joachims, T. (2005). A support vector method for multivariate performance measures. In
Proceedings of the 22nd International Conference on Machine Learning (ICML 2005),
pp. 377384, Bonn, DE.
Johnson, W. B., Lindenstrauss, J., & Schechtman, G. (1986). Extensions of Lipschitz maps
into Banach spaces. Israel Journal of Mathematics, 54 (2), 129138.
Jurgens, D., & Stevens, K. (2009). Event detection in blogs using temporal random indexing.
In Proceedings of the Workshop on Events in Emerging Text Types, pp. 916, Borovets,
BG.
Kanerva, P., Kristofersson, J., & Holst, A. (2000). Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd Annual Conference of the Cognitive
Science Society (CogSci 2000), p. 1036, Philadelphia, US.
Karlgren, J., Holst, A., & Sahlgren, M. (2008). Filaments of meaning in word space. In
Proceedings of the 30th European Conference on Information Retrieval (ECIR 2008),
pp. 531538, Glasgow, UK.
182

fiLightweight Random Indexing for Polylingual Text Classification

Kaski, S. (1998). Dimensionality reduction by random mapping: Fast similarity computation
for clustering. In Proceedings of the IEEE International Joint Conference on Neural
Networks (IJCNN 1998), pp. 413418, Anchorage, US.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations of words. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pp. 14591474, Mumbai, IN.
Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In MT
summit, Vol. 5, pp. 7986. Publicly available in http://www.statmt.org/europarl/.
Lauly, S., Boulanger, A., & Larochelle, H. (2014). Learning Multilingual Word Representations using a Bag-of-Words Autoencoder. ArXiv e-prints, arXiv:1401.1803 [cs.CL].
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). Rcv1: A new benchmark collection for
text categorization research. Journal of machine learning research, 5 (Apr), 361397.
Publicly available in http://www.jmlr.org/papers/volume5/lewis04a/lyrl2004_
rcv1v2_README.htm.
Li, P., Hastie, T. J., & Church, K. W. (2006). Very sparse random projections. In Proceedings
of the 12th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD 2006), pp. 287296, Philadelphia, US.
Li, Y., & Shawe-Taylor, J. (2007). Advanced learning algorithms for cross-language patent
retrieval and classification. Information Processing and Management, 43 (5), 1183
1199.
Mikolov, T., Le, Q. V., & Sutskever, I. (2013a). Exploiting Similarities among Languages
for Machine Translation. ArXiv e-prints, arXiv:1309.4168 [cs.CL].
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013b). Distributed
representations of words and phrases and their compositionality. In Proceedings of the
27th Annual Conference on Neural Information Processing Systems (NIPS 2013), pp.
31113119, Lake Tahoe, US.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2009), pp. 880889, Singapore, SN.
Moreo, A. (2016). Data resources for reproducing experiments in polylingual text classification. In Human Language Technologies (HLT) group website. Retrieved September
11, 2016, from http://hlt.isti.cnr.it/pltc.
Nastase, V., & Strapparava, C. (2013). Bridging languages through etymology: The case of
cross-language text categorization. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (ACL 2013), pp. 651659, Sofia, BL.
Osterlund, A., Odling, D., & Sahlgren, M. (2015). Factorization of latent variables in distributional semantic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2015), pp. 227231, Lisbon, PT.
Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on
Knowledge and Data Engineering, 22 (10), 13451359.
183

fiMoreo, Esuli, & Sebastiani

Papadimitriou, C. H., Raghavan, P., Tamaki, H., & Vempala, S. (1998). Latent semantic
indexing: A probabilistic analysis. In Proceedings of the 17th ACM Symposium on
Principles of Database Systems (PODS 1998), pp. 159168, Seattle, US.
Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word
representation. In Proceedings of the Conference on Empirical Methods on Natural
Language Processing (EMNLP 2014), pp. 15321543, Doha, QA.
Prettenhofer, P., & Stein, B. (2010). Cross-language text classification using structural
correspondence learning. In Proceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pp. 11181127, Uppsala, SE.
Rangan, V. (2011). Discovery of related terms in a corpus using reflective random indexing. In Proceedings of the ICAIL 2011 Workshop on Setting Standards for Searching
Electronically Stored Information, Pittsburgh, US.
Richardson, J. (2008). PolyLDA++. In Atlassian Bitbucket. Retrieved September 11, 2016,
from https://bitbucket.org/trickytoforget/polylda.
Rigutini, L., Maggini, M., & Liu, B. (2005). An EM-based training algorithm for crosslanguage text categorization. In Proceedings of the 3rd IEEE/WIC/ACM International Conference on Web Intelligence (WI 2005), pp. 529535, Compiegne, FR.
Rohde, D. (2011). A C library for computing singular value decompositions. In SVDLIBC.
Retrieved September 11, 2016, from http://tedlab.mit.edu/~dr/SVDLIBC/.
Sahlgren, M. (2001). Vector-based semantic analysis: Representing word meanings based
on random labels. In Proceedings of the ESSLLI Workshop on Semantic Knowledge
Acquistion and Categorization, Helsinki, FI.
Sahlgren, M. (2005). An introduction to random indexing. In Proceedings of the Workshop
on Methods and Applications of Semantic Indexing, Copenhagen, DK.
Sahlgren, M. (2006). The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Swedish Institute for Computer Science, University of Stockholm, Stockholm, SE.
Sahlgren, M., & Coster, R. (2004). Using bag-of-concepts to improve the performance of
support vector machines in text categorization. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), Geneva, CH.
Sahlgren, M., & Karlgren, J. (2005). Automatic bilingual lexicon acquisition using random
indexing of parallel corpora. Natural Language Engineering, 11 (3), 327341.
Sahlgren, M., Karlgren, J., Coster, R., & Jarvinen, T. (2002). SICS at CLEF 2002: Automatic query expansion using random indexing. In Working Notes of the CrossLanguage Evaluation Forum Workshop (CLEF 2002), pp. 311320, Roma, IT.
Steinberger, R., Pouliquen, B., & Ignat, C. (2004). Exploiting multilingual nomenclatures
and language-independent text features as an interlingua for cross-lingual text analysis
applications. In Proceedings of the 4th Slovenian Language Technology Conference,
Ljubljana, SL.
184

fiLightweight Random Indexing for Polylingual Text Classification

Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga,
D. (2006). The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th International Conference on Language Resources
and Evaluation (LREC 2006), pp. 21422147, Genova, IT. Publicly available in
https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis.
Vinokourov, A., Shawe-Taylor, J., & Cristianini, N. (2002). Inferring a semantic representation of text via cross-language correlation analysis. In Proceedings of the 16th Annual
Conference on Neural Information Processing Systems (NIPS 2002), pp. 14731480,
Vancouver, CA.
Vulic, I., & Moens, M.-F. (2015). Monolingual and cross-lingual information retrieval models
based on (bilingual) word embeddings. In Proceedings of the 38th International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR
2015), pp. 363372, Santiago, CL.
Wei, C.-P., Lin, Y.-T., & Yang, C. C. (2011). Cross-lingual text categorization: Conquering
language boundaries in globalized environments. Information Processing and Management, 47 (5), 786804.
Wei, C.-P., Yang, C.-S., Lee, C.-H., Shi, H., & Yang, C. C. (2014). Exploiting poly-lingual
documents for improving text categorization effectiveness. Decision Support Systems,
57, 6476.
Xiao, M., & Guo, Y. (2013). A novel two-step method for cross-language representation
learning. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS 2013), pp. 12591267, Lake Tahoe, US.
Xu, C., Tao, D., & Xu, C. (2013). A survey on multi-view learning. ArXiv e-prints,
arXiv:1304.5634 [cs.LG].
Yang, Y., & Pedersen, J. O. (1997). A comparative study on feature selection in text categorization. In Proceedings of the 14th International Conference on Machine Learning
(ICML 1997), pp. 412420, Nashville, US.
Zou, W. Y., Socher, R., Cer, D. M., & Manning, C. D. (2013). Bilingual word embeddings
for phrase-based machine translation. In Proceedings of the Conference on Empirical
Methods on Natural Language Processing (EMNLP 2013), pp. 13931398, Melbourne,
AU.

185

fiJournal of Artificial Intelligence Research 57 (2016) 39-112

Submitted 4/16; published 9/16

PDT Logic: A Probabilistic Doxastic Temporal Logic for
Reasoning about Beliefs in Multi-agent Systems
Karsten Martiny
Ralf Moller

karsten.martiny@uni-luebeck.de
moeller@uni-luebeck.de

Institute of Information Systems,
Universitat zu Lubeck
Lubeck, Germany

Abstract
We present Probabilistic Doxastic Temporal (PDT) Logic, a formalism to represent
and reason about probabilistic beliefs and their temporal evolution in multi-agent systems.
This formalism enables the quantification of agents beliefs through probability intervals
and incorporates an explicit notion of time. We discuss how over time agents dynamically
change their beliefs in facts, temporal rules, and other agents beliefs with respect to any
new information they receive. We introduce an appropriate formal semantics for PDT Logic
and show that it is decidable. Alternative options of specifying problems in PDT Logic
are possible. For these problem specifications, we develop different satisfiability checking
algorithms and provide complexity results for the respective decision problems. The use
of probability intervals enables a formal representation of probabilistic knowledge without
enforcing (possibly incorrect) exact probability values. By incorporating an explicit notion
of time, PDT Logic provides enriched possibilities to represent and reason about temporal
relations.

1. Introduction
Logical analysis of knowledge and belief has been an active topic of research in diverse fields
such as philosophy (Hintikka, 1962), economics (Aumann, 1976), game theory (Harsanyi,
1967, 1968a, 1968b), and computer science (Fagin, Halpern, Moses, & Vardi, 1995). Numerous extensions to modal epistemic logic have been made to reason about knowledge in
multi-agent settings (Fagin et al., 1995; Baltag & Moss, 2004), to add probabilistic knowledge (Fagin & Halpern, 1994; Cripps, Ely, Mailath, & Samuelson, 2008), and to analyze
the dynamic evolution of knowledge (van Ditmarsch, van der Hoek, & Kooi, 2007).
In most realistic scenarios, an agent has only incomplete and inaccurate information
about the actual state of the world, and thus considers several different worlds as actually
being possible. As it receives new information (e.g., it observes some facts that currently
hold), it has to update its beliefs about possible worlds such that they are consistent with
this new information. These updates can for example result in regarding some (previously
considered possible) worlds as impossible or judging some worlds to be more likely than
before. Thus, in addition to analyzing the set of worlds an agent believes to be possible,
it is also useful to quantify these beliefs in terms of probabilities. This provides means to
specify fine-grained distinctions between the range of worlds that an agent considers possible
but highly unlikely, and worlds that seem to be almost certainly the actual world.
c
2016
AI Access Foundation. All rights reserved.

fiMartiny & Moller

When multiple agents are involved in such a setting, an agent may not only have varying
beliefs regarding the facts of the actual world, but also regarding the beliefs of other agents.
In many scenarios, the actions of one agent will not only depend on its belief in ontic facts
(i.e., facts of the actual world), but also on its beliefs in some other agents beliefs.
To illustrate how reasoning about other agents beliefs can yield significant advantages
in practical scenarios, we start with the following informal description of an application
from the cyber security domain (a formal analysis of this example using PDT Logic has
been presented by Martiny, Motzek, & Moller, 2015): Suppose that an adversary is trying
to break into a computer system. This is usually done with an attack graph to detect
and exploit potential vulnerabilities of the system. An attack graph specifies a set of
paths (i.e., sequences of actions) to carry out an attack. Several paths of the attack graph
might be used in parallel, potentially by different agents (for instance, a number of infected
computers controlled by a botnet). Usually, attack patterns specified by one attack graph
are used multiple times, which has two important ramifications: the adversary will learn
from experience which of the paths yield a high probability of successfully breaking into a
system. Defenders in turn will be able to gain knowledge of the attack graph through the
repeated observation of certain patterns. Thus, when a system is under attack, the defender
will have beliefs about both the chosen attack paths and the adversarys belief regarding the
success of the respective path. Thus, the defender can choose countermeasures effectively
by reacting only on paths where these nested beliefs are high and which indeed pose a threat
according the systems mission impact model.
To formalize reasoning about such beliefs in multi-agent settings, we present Probabilistic Doxastic Temporal (PDT) Logic. PDT Logic builds upon recent work on Annotated
Probabilistic Temporal (APT) Logic (Shakarian, Parker, Simari, & Subrahmanian, 2011;
Shakarian, Simari, & Subrahmanian, 2012) and provides a formalism which enables representing and reasoning about dynamically changing quantified temporal multi-agent beliefs
through probability intervals and incorporates a subset of epistemic actions (Baltag & Moss,
2004). Using concepts from APT Logic as a semantic foundation, PDT Logic merges work
on epistemic logic with recent work on temporal logic by Shakarian et al. Apart from
reasoning about imprecise probabilities, this introduces the temporal concept of frequency
functions into epistemic temporal logic.
Quantifying probabilistic knowledge through probability intervals instead of single probability values yields two main advantages. On the one hand, using probability intervals
significantly eases the task of formally representing existing knowledge of a human domain
expert. In most cases, a domain expert can give reasonable probability estimates of her
knowledge, but will inevitably fail at giving correct precise numerical values on these probabilities. Consider for instance a weather forecast: most people find it easy to give coarse
probabilistic quantifications such as the chance of rain is high, while virtually nobody
could quantify this through an exact numerical value. Employing exact numerical values
in a formal representation would then inevitably introduce errors in the probability model.
Thus, the use of probability intervals provides means to express probabilistic knowledge as
precisely as possible without enforcing unrealistic precision. On the other hand, there are
many scenarios where probabilities (and even rough estimates of them) are simply unavailable, while bounds on these values may be known. To illustrate this, consider the scenario
described by Ellsberg (1961):
40

fiPDT Logic

Example 1.1 (The Ellsberg paradox, Ellsberg, 1961). Imagine an urn known to contain
30 red balls and 60 black and yellow balls, the latter in unknown proportion. One ball is
to be drawn at random from the urn; the following actions are considered: Action I is a
bet on red, II is a bet on black.
Now, it is easy to see that any rational agent would believe that action I will be successful
with a probability of 1/3. For action II, no such quantification is possible because the
respective probability is unknown. Yet omitting any probabilistic information about action
II altogether would ignore some available information about the unknown probability value,
namely that it is somewhere between 0 and 2/3. This example exhibits two different types
of uncertainty: the former action is subject to risk, i.e., the outcome is unknown, but
occurs with known probability, while the later action is subject to ambiguity (also known
as Knightian uncertainty), where the probability is unknown (Bradley, 2015). Through
probability intervals, PDT Logic is able to work with such imprecise probabilities. The
width of a probability interval can then give additional information about the certainty of a
probability quantification. Naturally, a narrow interval is associated with a high certainty
of the respective probability and vice versa, a wide interval is associated with low certainty.
PDT Logic employs an explicit notion of time and thereby facilitates the expression
of richer temporal relations. This allows for the analysis of temporal doxastic problems
beyond the scope of previous work. The resulting framework provides means to reason
about the temporal evolution of beliefs in multi-agent systems. Two different applications
of this framework are possible: First, any agent of the respective multi-agent system can
employ this framework online during a run of the system to reason about its own beliefs.
By analyzing nested beliefs as introduced above, this gives an agent also means to reason
about probable evolutions of other agents belief states. Second, this framework can be used
offline by an external observer to analyze whether desired evolutions of a given system are
possible.
The remainder of this work is structured as follows: The next section presents related
work about knowledge in multi-agent systems and APT Logic. Then, in Section 3, the
syntax of PDT Logic is introduced, followed by the definition of a formal semantics. Decision
algorithms and complexity results for PDT Logic are discussed in Section 4. While the
formally defined semantics is based on precise probability values, this section shows that
satisfiability in PDT Logic can be decided even if only imprecise probabilities are given.
Finally, the paper concludes with Section 5.

2. Related Work
Approaches to formalize reasoning about knowledge and belief date back to Hintikkas work
on epistemic logic (Hintikka, 1962). Hintikka proposed to represent knowledge through sets
of states or worlds, together with a binary relation for every agent, to determine which worlds
are indistinguishable for an agent. This approach has sparked multiple branches of research
on epistemic logic, which are still active topics of research today. These branches of research
can be broadly classified into four (not mutually exclusive) areas that are relevant for our
work: multi-agent epistemic logic, probabilistic epistemic logic, epistemic temporal logic,
41

fiMartiny & Moller

and dynamic epistemic logic.1 In the following, we give an overview of the key contributions
in each area and discuss existing approaches that merge these fields of research.
Early research on epistemic logic culminated in the influential work Reasoning about
Knowledge (Fagin et al., 1995), which provides a unified presentation of various preceding
contributions on epistemic logic. This work uses a so-called interpreted systems approach
to represent knowledge in multi-agent systems, where time is represented through runs. A
run is a sequence of a systems global states and it thus identifies the state of a system
for every time point. Among other contributions, this work provides notions for multiagent epistemic modalities such as nested knowledge, distributed knowledge, and common
knowledge.
Several works have extended epistemic logic to represent dynamic evolutions of knowledge. This direction of research is known as Dynamic Epistemic Logic (DEL). A first formal
analysis of the dynamics of knowledge has been presented by Plaza (1989; reprinted as Plaza,
2007). In this contribution, Plaza introduces public communication events (now commonly
known as public announcements) to analyze the dynamic evolution of knowledge in groups
upon truthful public announcements of facts to a group of agents. Independently from
Plaza, a related approach for a public announcement logic was proposed by Gerbrandy and
Groeneveld (1997). Baltag, Moss, and Solecki (1998) and Baltag and Moss (2004) generalize the dynamic approach to epistemic logic to incorporate a variety of complex epistemic
actions. Here, epistemic updates themselves are represented through Kripke models. This
extends dynamic epistemic logic to represent a variety of additional epistemic actions such
as private group announcements (i.e., announcements where agents outside of the receiving
group are unaware of this announcement), lies (i.e., untruthful announcements), and combinations thereof. In PDT Logic, we use public and private group announcements, but we
assume that all announcements are truthful. A thorough treatment of dynamic epistemic
logic is given by van Ditmarsch et al. (2007). Van Eijck (2014) provides a recent overview
of this field.
An alternative approach of modeling the evolution of knowledge is to combine epistemic
logic with some temporal system. One example for this are the aforementioned interpreted
systems from Fagin et al. (1995). Another approach of modeling temporal aspects in epistemic logic was proposed by Parikh and Ramanujam (2003). This approach is known as
Epistemic Temporal Logic (ETL). Here, possible situations are represented through sets of
histories, with local histories for every agent, which represent the respective agents previous
observations. Based on these histories, knowledge based semantics of messages are defined,
and it is shown that messages can vary in meaning, depending on the respective context
of the messages receiver. The temporal model we employ in PDT Logic is closely related
to epistemic temporal logic. Instead of specifying local histories for every agent, we define
the semantics of PDT Logic with respect to a global history. However, the local contexts
1. To simplify the following discussion, we do not explicitly distinguish between epistemic and doxastic
logics in this section, but use epistemic as a general term. Strictly speaking, epistemic formalisms
deal with knowledge, while doxastic formalisms deal with beliefs. The usual axiomatic definition of
knowledge in the literature uses the Truth Axiom, which stipulates that an agent can only know true
facts. Omitting this axiom then leads to the notion of belief. Even though not unanimously accepted
(cf. e.g., Halpern, Samet, & Segev, 2009), this axiom is usually considered as the key distinction between
knowledge and belief.

42

fiPDT Logic

in the sense of ETL can easily be extracted from the global history by filtering this history
for the respective agents observations.
The traditional work on epistemic logic discussed so far does not allow to quantify an
agents degree of belief in certain facts; it can only be specified whether an agent does or
does not know (resp. believe) some fact. To remove this limitation, several approaches have
been proposed to combine logics of knowledge and belief with probabilistic quantifications.
Fagin and Halpern (1994) laid the foundation for this combination in their seminal paper.
They define a belief operator to quantify lower bounds on the probabilities that an agent
assigns to a formula. This is modeled by associating a probability space with each state
and each agent. In their framework, it is generally not guaranteed that formulae define
measurable sets, but they present some properties that can guarantee the measurability of
such sets. In contrast, the semantics defined for PDT Logic always produces events with
measurable probabilities. A special case of the framework introduced by Fagin and Halpern
is presented by Milch and Koller (2000). Just as in PDT Logic, in this formalism it is
assumed (i) that there exists a common prior probability distribution over the set of worlds
and (ii) that each agents local probability distribution at some world is derived from the
global distribution conditioned on the respective set of worlds the agent considers possible.
The additional feature from Milch and Koller is that models are represented as Bayesian
networks to find the probabilities of defined formulae. Van der Hoek (1997) introduces the
logic PF D, which was later extended by de Carvalho Ferreira, Fisher, and van der Hoek
(2008). Like Fagin and Halpern, this framework introduces an operator to quantify the
lower bounds of probabilistic beliefs. Probabilistic values in this work are semantically
restricted to a finite base set of probability values, yielding a logically compact framework
that enables efficient implementations.
A variety of approaches have been proposed to extend probabilistic epistemic logics
to dynamic frameworks: Kooi (2003) restricts the probabilistic epistemic logic from Fagin
and Halpern (1994) to finite settings and combines it with the dynamic epistemic logic
from Gerbrandy and Groeneveld (1997) to create Probabilistic Dynamic Epistemic Logic
(PDEL). This work analyzes the effects on probabilistic beliefs upon public announcements.
As this framework is based on dynamic epistemic logic, it does not have capabilities to
represent temporal relationships; features regarding the past cannot be expressed at all,
and features regarding the future can only be expressed to a limited extent as the result
of certain actions. Van Benthem (2003) extends this framework to analyze the results of
various epistemic actions as described by Baltag et al. (1998). Another extension to this
framework is proposed by van Benthem, Gerbrandy, and Kooi (2009b), where different
sources of probabilities are distinguished. A simplification of this approach is presented by
van Eijck and Schwarzentruber (2014). This paper distinguishes itself from the above work
on probabilistic epistemic logic in that certainty is equated with knowledge. Other works
make an explicit distinction between belief with probability 1 and knowledge. The difference
between these two concepts is often illustrated with repeatedly throwing a fair coin: the
event that the coin shows head at least once is 1 for an infinite number of repetitions.
Yet no agent can know in this example that the coin will eventually show head. As PDT
Logic works only with countable models in finite time frames, we can adopt the view from
van Eijck and Schwarzentruber and consider certainty and knowledge as equivalent in our
models. Deviating from these approaches to extend epistemic logic with probabilities, PDT
43

fiMartiny & Moller

Logic provides a belief operator with probability interval quantifications, so that both lower
and upper bounds on the probability values can be specified explicitly. This provides a
natural means to represent imprecise probabilities as discussed in the introduction.
Another direction of probabilistic extensions is discussed by Halpern and Pucella (2006)
and Doder, Markovic, Ognjanovic, Perovic, and Raskovic (2010), for example. These approaches consider the problem of estimating unknown prior probabilities based on given
evidence. Essentially, the unknown priors are then represented as a set of hypotheses, and
the likelihood of a hypothesis given specific observations is estimated. In these approaches,
all hypotheses represent possible configurations of the world and are thus satisfiable. In
contrast, the aim of PDT Logic is to verify whether any possible assignment of priors exists
such that a given set of formulae is satisfiable.
In dynamic epistemic logic, it is only possible to reason about step-wise changes in
the future. In order to reason about temporal relations, Sack (2008) extends the update
mechanism of dynamic epistemic logic with temporal operators, namely previous-time and
next-time operators. Sack (2009) further extends this approach to probabilistic frameworks by augmenting the work on probabilistic dynamic epistemic logic (Kooi, 2003) with
a previous-time operator and the ability to reason about continuous probabilities. These
approaches enrich dynamic epistemic logic with the ability to reason about events in the
past. Van Benthem, Gerbrandy, Hoshi, and Pacuit (2009a) give a systematic and precise
comparison between ETL (called TEL by van Benthem, Gerbrandy, Hoshi, and Pacuit) and
DEL and it is shown how these approaches can be merged into a single framework.
Shakarian et al. (2011) and Shakarian et al. (2012) introduce APT Logic, a framework
to represent probabilistic temporal evolutions of worlds in threads. APT Logic assigns
prior probabilities to every thread and uses these probabilities to determine probabilities
of events occurring in specific threads. To represent temporal relationships between events,
APT Logic introduces the concept of frequency functions. We utilize the approach of APT
Logic to create a doxastic multi-agent framework that supports explicit reasoning about
temporal relationships through the adoption of frequency functions. While the explicit
notion of time in our formalism increases the complexity of decision problems, it significantly
enhances the expressibility of temporal relations. For instance, in contrast to all approaches
with implicit representations of time, in PDT Logic we are able to specify that events occur
within a certain time interval (cf. the introduction of frequency functions below).

3. PDT Logic: Syntax and Semantics
In this section, we discuss how beliefs in multi-agent systems can be formalized. We start
with defining the syntax of PDT Logic, discuss the employed model of time, and provide
a formal semantics. The proposed formalism enables the expression of different types of
beliefs and can quantify these beliefs using imprecise probabilities. By introducing a suitable
update rule we show how agents beliefs evolve over time and how agents can update their
beliefs such that new information is correctly integrated into their belief state.
44

fiPDT Logic

3.1 Syntax
We assume the existence of a function-free and quantifier-free fragment of first order logic2
language L with finite sets of constant symbols Lcons and predicate symbols Lpred , and an
infinite set of variable symbols Lvar . Every predicate symbol p  Lpred has an arity. A term
is any member of the set Lcons  Lvar . A term is called a ground term if it is a member
of Lcons . If t1 , .., tn are (ground) terms, and p is a predicate symbol in Lpred with arity n,
then p(t1 , ..., tn ) is a (ground) atom. If a is a (ground) atom, then a and a are (ground)
literals. The former is called a positive literal, the latter is called a negative literal. The set
of all ground literals is denoted by Llit . As usual, B denotes the Herbrand Base of L, i.e.,
the set of all ground atoms that can be formed through from Lpred and Lcons .
Time is modeled in discrete steps and we assume that all agents reason about an arbitrarily large, but fixed-size window of time. The set of time points is given by  = {1, ..., tmax }.
The set of agents is denoted by A. Again, we assume that this set may be arbitrarily large,
but of finite size. To describe what agents observe, we define observation atoms as follows.
Definition 3.1 (Observation atoms). For any non-empty group of agents G  A and
ground literal l  Llit , ObsG (l) is an observation atom. The set of all observation atoms is
denoted by Lobs .
Intuitively, the meaning of a statement of the form ObsG (l) is that all agents in the group
G observe that the fact l holds. Note that l may be a negative literal and therefore we can
explicitly specify observations of certain facts being false (such as it is not raining). We
assume that the agents in G not only observe that l holds, but that each agent in G is also
aware that all other agents in G make the same observation. In the line of Baltag and Moss
(2004), observations can be viewed as the effects of private group announcements of a fact
l to a group G (i.e., l becomes common knowledge within G, while all agents outside of G
remain entirely oblivious of the observation): it represents an epistemic action, i.e., it alters
the belief states of all agents in G (as formally defined below), but does not influence the
ontic facts of the respective world.
Definition 3.2 (Formulae). Both atoms and observation atoms are formulae. If F and G
are formulae, then F  G, F  G, and F are formulae. A formula is ground if all atoms of
the formula are ground.
Example 3.1 (Coin toss). Consider two agents 1, 2 and a coin that is tossed. The event
that the coin lands heads is denoted by the primitive proposition Head, and accordingly,
the coin lands tails is denoted by Head. Let us assume that the coin actually lands heads.
Then, all sets of possible observations in this scenario are {Obs{1} (Head)}, {Obs{2} (Head)},
{Obs{1} (Head), Obs{2} (Head)}, {Obs{1,2} (Head)}.
Note that there is a difference between the third and the fourth set: in the former
scenario, both agents observe the outcome of the coin throw but both are unaware that the
other agent actually made the same observation. In the latter scenario, both agents observe
the outcome and are aware that the other agent observes the same. Since we do not allow
2. We use a first order structure for our language definition to have a syntactically convenient way of
representing observations. Apart from this, propositional logic could be used as a base language.

45

fiMartiny & Moller

for nesting of observations (i.e., expressions such as ObsG1 (ObsG2 (l))) in PDT Logic, only
a subset of the epistemic actions discussed by Baltag and Moss (2004) can be represented
in our formalism. While this limits the expressivity of epistemic actions to some extent, we
can ensure that the resulting set of possible observations Lobs is always finite and therefore
we can show that PDT Logic is decidable (as shown in Section 4). Further, note that the
formal concept of observations is not limited to express passive acts of observing facts, but
can instead be used to model a wide range of actions: for instance, in the above example
one could also use Obs{1,2} (Head) to model the act of one agent telling the other about
the outcome of the coin throwthe ramifications of the communication act are exactly the
same as they would be in a shared observation (assuming that agents do not lie).
To express temporal relationships, we define temporal rules following the approach of
APT rules from Shakarian et al. (2011). The definition of temporal rules already relies on
the concept of frequency functions, even though these are defined in the next section. We
still introduce temporal rules now to enable a clearly separated presentation of syntax and
semantics of PDT Logic.
Definition 3.3 (Temporal rules). Let F, G be two ground formulae, t a time interval,
fr (F, G)
and fr a name for a frequency function (as defined below in Section 3.2.5). Then rt
is called a temporal rule.
Frequency functions provide information about temporal connections between events.
fr (F, G) is to be understood as F is followed by G in t
The meaning of an expression rt
time units w.r.t. frequency function fr. Frequency functions enable the specification of
various types of temporal relations. For example, they can be used to determine how often
F is followed by G within t time units or how often F is followed by G exactly after t
time units. The usage of fr in the syntax of temporal rules is used to specify a set of possible
names for the employed types of frequency function.
`,u
`,u
Now, we can define the belief operator Bi,t
0 to express agents beliefs. Intuitively, Bi,t0 ()
means that at time t0 , agent i believes that some fact  is true with a probability p  [`, u].
Particularly, the intuitive meaning of belief in a temporal rule is that agent i believes
fr (F, G), given that F holds at some time point. We call
that G will hold according to rt
the probability interval [`, u] the quantification of agent is belief. We use Ft to denote
that formula F holds at time t and, accordingly, ObsG (l)t to denote that an observation
ObsG (l) occurs at time t. We call these expressions time-stamped formulae and timestamped observation atoms, respectively.
Definition 3.4 (Belief formulae). Let i be an agent, t0 a time point, and [`, u]  [0, 1].
Then, belief formulae are inductively defined as follows:
`,u
1. If F is a ground formula and t is a time point, then Bi,t
0 (Ft ) is a belief formula.
fr (F, G) is a temporal rule, then B `,u (r fr (F, G)) is a belief formula.
2. If rt
i,t0 t
`,u
3. If F and G are belief formulae, then so are Bi,t
0 (F ), F  G , F  G , and F .
`,u
For a belief Bi,t
0 () about something, we call  the belief object. Belief operators are the
`,u
atomic elements in PDT Logic, i.e., any expression Bi,t
0 () (including possibly nested belief

46

fiPDT Logic

formulae) is called an atom. We use script fonts (e.g., F ) to distinguish belief formulae
from standard formulae. Note that we can have both ontic facts and observation atoms
as standard formulae (cf. Definition 3.2) and therefore agents can also have beliefs about
possible observations.
The use of probability intervals [`, u] provides an option to represent imprecise probabilities (Bradley, 2015): When using imprecise probabilities, it is usually assumed that
the degree of belief in some proposition is not represented using a single probability function p(), but instead through a set P of such functions. Then, the belief state P () in a
proposition  is represented through the set
P () = {p() : p  P }.
For this set of probabilities P (), so-called lower and upper envelopes are defined as P () =
inf P () and P () = sup P (), respectively. The belief quantifications in our belief operator
represent such imprecise probabilities and the ` and u values of the probabilistic belief can be
considered as the lower and upper envelopes P and P of the respective imprecise probability.
`,u
Remark 3.1. We decided to index both the belief operators Bi,t
0 () and facts Ft appearing
as belief objects  with time stamps to allow for a concise representation of temporal
relations. Alternatively, one could use the more traditional approach (cf. Sack, 2009 for
example) and introduce previous-time and next-time operators into the language to express
`,u
temporal relationships between t and t0 in Bi,t
0 (Ft ). Then, we could also omit the temporal
0
index t of the belief operator and instead evaluate whether the belief holds at time t0 of
the model. However, these are merely syntactic considerations that do not impact the
underlying formalism. Thus we decided to encode time explicitly into the belief operators
to avoid the introduction of additional temporal operators. Moreover, belief operators can
also be used to express general temporal relationships of the modeled domain. We will
illustrate this point in detail in Section 4.

3.2 Semantics
In this section, we will provide a formal semantics for PDT Logic that captures the intuitions
explained above. To ease understanding of the presentation, we start with the introduction
of an example, to which we will return repeatedly when introducing the various concepts of
the semantics. For an illustration of our formalisms features, we use a simplified exemplary
domain. While the practical use of this example is somewhat limited, it serves to illustrate
how PDT Logic can be applied, and especially how the analysis of multi-agent beliefs can
yield valuable information when deciding on meaningful actions. The resulting insights can
then be easily applied to more sophisticated domains.
Example 3.2 (Trains). Let Alice and Bob be two agents living in two different cities CA
and CB , respectively. Suppose that Alice wants to take a train to visit Bob. Unfortunately,
there is no direct connection between cities CA and CB , so Alice has to change trains at a
third city CC . We assume that train T1 connects CA and CC , and train T2 connects CC and
CB . Both trains usually require 2 time units for their trip, but they might be running late
and arrive one time unit later than scheduled. Alice requires one time unit to change trains
at city CC . If T1 runs on time, she has a direct connection to T2 , otherwise she has to wait
47

fiMartiny & Moller

for two time units until the next train T2 leaves at city CC . If a train is running late, she
can call Bob to let him know. These calls can be modeled as shared observations between
Alice and Bob. For instance, if Alice wants to tell Bob that train T1 is running late (i.e., T1
does not arrive at CC at the expected time), this can be modeled as Obs{AB} (at(T1 , CC ))
at the expected arrival time.
3.2.1 Possible Worlds
Ontic facts and corresponding observations (e.g., as described in the above example) form
worlds (or states in the terminology of Fagin et al., 1995). A world  consists of a set
of ground atoms and a set of observation atoms, i.e.,   2BLobs .3 We use a   and
ObsG (l)   to denote that an atom a, resp. observation atom ObsG (l), holds in world .
Since agents can only observe facts that actually hold in the respective world, we can define
admissibility conditions of worlds w.r.t. the set of observations:
Definition 3.5 (Admissible worlds). A world  is admissible, iff for every observation atom
ObsG (l)  
1. the observed fact holds, i.e., x   if l is a positive literal x, and x 6  if l is a negative
literal x, and
2. for every subgroup G 0  G, ObsG 0 (l)  .
We use adm() to denote that a world  is admissible.
The set of all possible worlds is denoted by  and the set of admissible worlds by .
For the following discussion in this section we assume that some specification of  is given.
While it is possible to employ the usual definition of  as the set of all combinations of
ground atoms and observation atoms ( = 2BLobs ), and  as the maximum subset of 
complying with Definition 3.5, this usually contains a vast number of worlds which are
blatantly impossible according to the respective problem modeled. Therefore, we assume
that a succinct specification of a set of admissible worlds depending on the respective domain
is given. The main reason for this assumption is to simplify the following presentationwe
will describe a method to obtain such a set algorithmically in Section 4.
Remark 3.2. As already discussed in Section 3.1, for group observations ObsG (l) every
agent i  G is aware that all other agents in G have observed this fact. Together with
Definition 3.5, the semantics of observations is then equivalent to the usual semantics of
common knowledge. Fagin et al. (1995) give a definition of common knowledge through the
fixed-point axiom: A fact l is common knowledge among a group G if and only if all members
of G know that l is true and is common knowledge. Thus, we could also equivalently use the
established common knowledge operator CG (l) instead of the previously defined observation
3. Most formalisms in epistemic logic do not encode facts directly into the worlds, but instead use a set of
named states s1 , s2 , ... and some valuation function (si ) to determine which facts hold in world si (cf.
Fagin et al., 1995). This is mainly done to obtain the option of having multiple worlds si , sj where the
same facts hold (i.e., (si ) = (sj )), but the knowledge states of the agents differ. As described below,
in PDT Logic worlds appear within threads, and thus it is possible that worlds with the same valuation
appear at some time point in multiple threads. Thus, in our formalism we can encode facts directly into
the possible worlds and save the valuation function without limiting the epistemic expressivity.

48

fiPDT Logic

atoms ObsG (l). However, the concept of common knowledge is usually used to describe
emergent states of agents knowledge. On the other hand, in the context of our approach,
observations are an extrinsic feature that will result in the emergence of other belief states.
To keep a clear distinction of the intended use of the operator, we will therefore continue
to use ObsG (l) instead of CG (l).
Example 3.3 (Trains continued). For Example 3.2, we have ground terms A, B, CA , CB ,
CC , T1 , and T2 , representing Alice, Bob, three cities, and two trains. Furthermore, we have
atoms on(y, x) indicating that person y is on train x, and at(x, z) indicating that train x
is at city z. Finally, we have observation atoms of the kind ObsG (at(x, z)), indicating that
the agents in G observe that train x is at station z. A possible world can for example be
1 = {at(T1 , CA ), on(A, T1 ), Obs{A} (at(T1 , CA ))}, indicating that train T1 is at city CA and
A has boarded that train.
We define satisfaction of a ground formula F by a world  in the usual way (Lloyd,
1987):
Definition 3.6 (Satisfaction of ground formulae). Let F, F 0 , F 00 be ground formulae and 
a world. Then, F is satisfied by  (denoted  |= F ) if and only if:
case F = a for some ground atom a:
a  .
case F = F 0 for some ground formula F 0 :

case F = F 0  F 00 for formulae F 0 and F 00 :

case F = F 0  F 00 for formulae F 0 and F 00 :

 6|= F 0 .

 |= F 0 and  |= F 00 .

 |= F 0 or  |= F 00 .

We say that a formula F is a tautology if  |= F for all admissible worlds   . We
say that a formula F is a contradiction if there is no world    such that  |= F . We use
the usual symbols > and  to denote tautologies and contradictions, respectively.
3.2.2 Threads
To model temporal evolutions of the problem domain we use the definition of threads from
Shakarian et al. (2011):
Definition 3.7 (Thread). A thread T h is a mapping from the set of time points  to the
set of admissible worlds: T h :   
Thus, a thread is a sequence of worlds and T h(t) identifies the actual world at time t
according to thread T h. The set of all possible threads (i.e., all possible sequences constructible from  and ) is denoted by T . Again, we refrain from directly working with
T , and instead assume that any meaningful problem specification gives information about
possible temporal evolutions of the system. We use T to represent this set of relevant possible threads. For notational convenience, we assume that there is an additional prior world
T h(0) for every thread.
Following Definition 3.6, we use T h |= Ft to denote that thread T h satisfies formulae F
at time t (i.e., T h |= Ft  T h(t) |= F ). Accordingly, we use T |= Ft to denote that every
thread T h  T satisfies formula F at time t.
49

fiObs{A}
at(T1 , CC )
(at(T1 , Cc ))
on(A, T1 ) on(A, T1 ) on(A, T1 ) on(A, T1 )

Obs{A,B}
at(T1 , CC )
(at(T1 , Cc ))
on(A, T1 ) on(A, T1 ) on(A, T1 ) on(A, T1 )

Obs{A}
at(T1 , CC )
(at(T1 , Cc ))
on(A, T1 ) on(A, T1 ) on(A, T1 ) on(A, T1 )

1 2 T h7

1 2 T h6

T h5

T h4

1

1

T hi

T h1

2 T h2
on(A, T1 )
at(T1 , CC )
on(A, T1 )

on(A, T1 ) on(A, T1 )
at(T1 , CA )
on(A, T1 ) on(A, T1 )

3

at(T1 , CC )

at(T1 , CA )

2

on(A, T1 )

on(A, T1 ) on(A, T1 )

1

at(T1 , CC )

at(T1 , CA )

at(T1 , CA )

at(T1 , CA )

at(T1 , CA )

at(T1 , CA )

on(A, T2 )

on(A, T2 )

on(A, T2 )

on(A, T2 )

4
1

5

on(A, T2 ) on(A, T2 )

at(T2 , CC )

at(T2 , CC )

6

on(A, T2 )

at(T2 , CB )

7

Obs{A}
at(T2 , CB )
(at(T2 , CB ))
on(A, T2 ) on(A, T2 ) on(A, T2 ) on(A, T2 )

at(T2 , CC )

8

at(T2 , CB )

at(T2 , CC )
on(A, T2 )

at(T2 , CB )
on(A, T2 )

9

t

Obs{A}
at(T2 , CB )
(at(T2 , CB ))
on(A, T2 ) on(A, T2 )
on(A, T2 )

Obs{A}
at(T2 , CB )
(at(T2 , CB ))
on(A, T2 ) on(A, T2 )
on(A, T2 )

Obs{A,B}
at(T2 , CB )
(at(T2 , CB ))
on(A, T2 ) on(A, T2 )
on(A, T2 )

Obs{A,B}
at(T2 , CB )
(at(T2 , CB ))
on(A, T2 ) on(A, T2 )
on(A, T2 )

at(T2 , CC )

on(A, T2 )

at(T2 , CC )

on(A, T2 )

at(T2 , CC )

on(A, T2 )

at(T2 , CC )

on(A, T2 )

at(T2 , CC )

Obs{A,B}
at(T2 , CB )
(at(T2 , CB ))
on(A, T2 ) on(A, T2 ) on(A, T2 ) on(A, T2 )

Obs{A,B}
at(T1 , CC )
(at(T1 , Cc ))
on(A, T1 ) on(A, T1 ) on(A, T1 ) on(A, T1 )

2 T h3

Obs{A}
at(T1 , CC )
(at(T1 , Cc ))
on(A, T1 ) on(A, T1 ) on(A, T1 ) on(A, T1 )

1 2 T h8

at(T1 , CA )

Obs{A,B}
at(T1 , CC )
(at(T1 , Cc ))
on(A, T1 ) on(A, T1 ) on(A, T1 ) on(A, T1 )

at(T1 , CA )

1 2 T h9

Martiny & Moller

Figure 1: Visualization of the possible threads T hi from Example 3.2. For an easier distinction, shared observations between A and B are marked in blue, single observations
of A are marked in red, and all situations where Alice is on train 1 or train 2
are marked in green and orange, respectively. Note that if a train is running late
(the respective threads are marked with according circles), there are always two
possible threads: one where only A observes this and one where both share the
observation.
50

fiPDT Logic

We assume that the system is synchronous, i.e., the agents have a global clock. Thus,
even if an agent does not observe anything in world T h(t), it is still aware of time passing
and can therefore distinguish between worlds T h(t) and T h(t  1).
Example 3.4 (Trains continued). The description from Example 3.2 (p. 47) yields the
set of possible threads T depicted in Figure 1. Note that this is a manually specified set
of threads containing only threads that comply with the description in Example 3.2. The
set of all possible threads T would contain a vast number of additional threads that are
irrelevant to the described scenario.
3.2.3 Kripke Structures
With the definition of threads, we can use a slightly modified version of Kripke structures
(Kripke, 1963). As usual, we define a Kripke structure as a tuple h, K1 , ..., Kn i, with the
set of admissible worlds  and binary relations Ki on  for every agent i  A. Thus, the
Kripke relation (also called possibility relation) for agent i at world  is defined as
Ki () = { 0 : (,  0 )  Ki }

(1)

Intuitively, (,  0 )  Ki specifies that in world , agent i considers  0 also as a possible
world. In other words, with its current information agent i is unable to distinguish worlds
 and  0 .
We initialize the Kripke structure such that all threads are considered possible at time
t = 0:
[
T h  T : Ki (T h(0)) =
{T h0 (0)}, i  A
(2)
T h0 T

With the evolution of time, each agent can eliminate the worlds that do not comply with its
respective observations. Through the elimination of worlds, an agent will also reduce the
set of threads it considers possible (ifdue to some observationa world  is considered
impossible at a time point t, then all threads T h with T h(t) =  are considered impossible).
We assume that agents have perfect recall and therefore will not consider some thread
possible again if it was considered impossible at one point. Thus, Ki is updated w.r.t. the
agents respective observations, such that it considers all threads possible that both comply
with its current observations and were considered possible at the previous time point:

Ki (T h(t)) = T h0 (t) : T h0 (t  1)  Ki (T h(t  1))
	
{ObsG (l)  T h(t) : i  G} = {ObsG (l)  T h0 (t) : i  G}
(3)
The following two corollaries describe key properties of Ki that follow immediately from
the definitions in (2) and (3):
Corollary 3.1 (Equivalence relation). Ki defines an equivalence relation over the possible
worlds Ki (T h(t)) for time points t   .
Corollary 3.2 (Reduction of considered threads). The set of threads T h0 considered possible
w.r.t. Ki is narrowing to a smaller and smaller subset over time, i.e., {T h0 : T h0 (t) 
Ki (T h(t))}  {T h0 : T h0 (t  1)  Ki (T h(t  1))} for all T h  T and t   .
51

fiMartiny & Moller

Note that updates of Ki are defined such that new information is incorporated instantaneously, i.e., if at time t an agent observes some fact, it updates its possibility relations
already at time t such that it considers every world impossible that does not comply with
the observation of time t.
Example 3.5 (Trains continued). From Figure 1, we obtain that at time 1, the only
possible world is {at(T1 , CA ), on(A, T1 )}, which is contained in all possible threads. Thus,
Ki (T hj (1)) contains exactly this world for all agents i and threads j. Consequently, both
agents consider all threads as possible at time 1.
Now, assume that time evolves for two steps and the actual thread is T h4 (i.e., train
T1 is running late, but A does not inform B about this). Both agents will update their
possibility relations accordingly, yielding
KA (T h4 (3)) = {{Obs{A} (at(T1 , CC )), on(A, T1 )}}
and
KB (T h4 (3)) = {{at(T1 , CC ), on(A, T1 )}, {Obs{A} (at(T1 , CC )), on(A, T1 )}},
i.e., A knows that T1 is not on time, while B is unaware of T1 being late, since he still
considers a situation possible where train T1 is at city CC at time t = 3.
3.2.4 Subjective Posterior Temporal Probabilistic Interpretations
Each agent has probabilistic beliefs about the expected evolution over time. This is expressed through subjective temporal probabilistic interpretations:
Definition 3.8 (Subjective posterior probabilistic temporal interpretation). Given a set of
possible threads T , some thread Th  T , a time point t0 > 0 and an agent i, the function
Th : T  [0, 1] specifies the subjective posterior probabilistic temporal interpretation from
Ii,t
0
agent is point of view at time t0 in thread Th, i.e., a probability distribution over all possible
P
Th (T h) = 1. Since the probabilistic interpretations over possible threads
threads: T hT Ii,t
0
depend on the respective perspective of agent i, Th marks the point of view for a subjective

interpretation. Thus, we call Th the point of view (pov) thread of interpretation I T h0 .
i,t

The concept of point of view threads can be seen as conditional probabilities: A subjecTh specifies agent is probabilistic interpretation
tive posterior probabilistic interpretation Ii,t
0
at time t0 given that Th is the actual thread. Different threads yield different evolutions
of the world andsince every possible thread can be taken as a pov thread may induce
different probabilistic interpretations of an agent. Thus, the notion of pov threads allows
to reason about hypothetical beliefs of an agent, for instance if possible future beliefs are
analyzed or nested beliefs are evaluated.
Th as a vector and occasionally represent a probabilistic
To simplify notation, we see Ii,t
0


T h over a vector of possible threads T as a vector as well, so that the jth
interpretation Ii,t
0


T h refers to the probability assigned to thread T h .
element of Ii,t
0
j



T h (T h). Since
The prior probabilities of each agent for all threads are then given by Ii,0
all threads are indistinguishable a priori, there is only a single prior distribution needed

52

fiPDT Logic

0



0

T h (T h) = I T h (T h)). Furthermore, in order
for each agent (i.e., T h, Th, Th  T : Ii,0
i,0
to be able to reason about nested beliefs (as discussed below), we assume that the prior
probability assessments of all agents are commonly known (i.e., all agents know how all
other agents assess the prior probabilities of each thread). This in turn requires that all
agents have exactly the same prior probability assessment over all possible threads: if two
agents have different, but commonly known prior probability assessments, we essentially
have an instance of Aumanns well-known problem of agreeing to disagree (Aumann,
1976). Intuitively, if differing priors are commonly known, it is common knowledge that
(at least) one of the agents is at fault and should revise its probability assessments. As a
result, we have only one prior probability distribution which is the same from all viewpoints,
denoted by I. Note that I directly corresponds to the concept of temporal probabilistic
interpretations by Shakarian et al. (2011).

Remark 3.3. We could use the prior probability distribution I as an alternative method
to distinguish between the set of all possible threads T and the set of threads T relevant
to a specific problem domain. To do so, we simply assign all unwanted threads T h 6 T a
probability of zero.
Example 3.6 (Trains continued). A meaningful prior interpretation is

I(T ) = 0.7 0.02 0.09 0.02 0.09 0.01 0.02 0.02 0.03 ,
which assigns the highest probability to T h1 (no train running late), lower probabilities to
the threads where one train is running late and A informs B (T h3 and T h5 ), even lower
probabilities to the events that either both trains are running late and A informs B (T h7 ,
T h8 , and T h9 ) or that one train is running late and A does not inform B (T h2 and T h4 ),
and lowest probability to the thread where both trains are running late and A does not
inform B (T h6 ). Note that I represents the prior interpretation for the train example and
thus is the same for every agent i  A and every possible pov thread Th.
Even though we only have a single prior probability distribution over the set of possible
threads, it is still necessary to distinguish the viewpoints of different agents in different
threads, as the following definition of interpretation updates shows.
Whenever an agent updates its Kripke relations according to Equation (3) (p. 51), it
is necessary to update the probabilistic interpretations of that agent to match the new
knowledge. An intuitive way to update the probabilities is conditioning on the remaining
worlds in the agents Kripke structure. We want to point out that conditioning is a suitable
choice in PDT Logic, although it is known to produce undesired or incorrect results in
many cases, most notably in the Monty Hall problem (vos Savant, 1990). Grunwald and
Halpern (2003) discuss that naive conditioning tends to produce errors because updates are
carried out in a simplified space where several events are collapsed since they are seemingly
one event. If one uses so-called sophisticated conditioning instead (i.e., conditioning in the
sophisticated space, which means that all possible events are represented), probabilities are
updated correctly. As the semantics of PDT Logic is based on an exhaustive specification of
all relevant threads, conditioning in a proper specification of all relevant threads is inherently
sophisticated in the sense of Grunwald and Halpern and will therefore produce correct
results. One can easily verify that with the following update rule, well-known probability
53

fiMartiny & Moller

puzzles such as the Monty Hall Problem can be correctly represented in PDT Logic. Thus,
we use the following conditioning-based update rule:
Definition 3.9 (Interpretation update). Let i be an agent, t0 a time point, and Th a pov
thread. Then, if the system is actually in thread Th at time t0 , agent is probabilistic
interpretation over the set of possible threads is given by the update rule:

Th (T h) if T h(t0 )  K (Th(t0 ))
 1  Ii,t
0 1
i
h
Th
T
0
i,t
Ii,t0 (T h) =
(4)
0
if T h(t0 ) 6 Ki (Th(t0 ))
with

1
h
T
i,t0

being a normalization factor to ensure that


X

Th
i,t
0 =

T hT ,

P



T hT

T h (T h) = 1:
Ii,t
0



Th
Ii,t
0 1 (T h)

(5)

T h(t0 )Ki (Th(t0 ))

The invocation of Ki in the update rule yields obvious ramifications about the evolution
of interpretations, as stated in the following corollary:
Corollary 3.3 (Nonzero probabilities). The subjective temporal probabilistic interpretation
Th of an agent i assigns nonzero probabilities exactly to the set of threads that i still
Ii,t
0

considers possible at time t0 , i.e., I T h0 (T h) > 0 iff (T h(t), Th(t))  Ki
i,t

Essentially, the update rule assigns all impossible threads a probability of zero and
scales the probabilities of the remaining threads such that they are proportional to the
probabilities of the previous time point. With a given prior probability distribution I over
Th in a specific pov thread
the set of possible threads, the subjective posterior probabilities Ii,t
0
0

T h for all agents i and all time points t are induced by the respective observations contained


in Th. We use I T h to denote the set of all subjective posterior interpretations I T h0 induced
i,t

in pov thread Th.

Example 3.7 (Trains continued). Applying the update rule from (4) to the situation
described in Example 3.5 (p. 52), with I as given in Example 3.6, yields the updated
interpretation for A:


T h4
IA,3
= 0 0 0 0.4 0 0.2 0 0.4 0



(6)

i.e., A considers exactly those threads possible, where the train is running late and she does
not inform B (threads T h4 , T h6 , and T h8 ). Due to the lack of any new information, B
can only eliminate the situations where A does indeed inform him about being late at time
point 3, and thus Bs interpretation is updated to:

Th4
IB,3
 0.82 0.02 0.10 0.02 0 0.02 0 0.02 0 .
54

(7)

fiPDT Logic

T h(1)

T h(2)

T h(3)

T h(4)

T h(5)

T h(6)

T h(7)

T h(8)

F

G

F

G

G

F

G

F

Figure 2: Example thread T h with  = {1, ..., 8}, adopted from Shakarian et al. (2011).
This figure shows each world that satisfies formula F or formula G.

3.2.5 Frequency Functions
To represent temporal relationships within threads, we adapt the concept of frequency functions as introduced by Shakarian et al. (2011). Frequency functions provide a flexible way of
representing temporal relations between the occurrences of specific events. To illustrate the
motivation behind using frequency functions, consider the exemplary thread T h depicted in
Figure 2. In this thread, one of the events F or G occurs at every time point from t = 1 to
t = 8. As discussed by Shakarian et al., there are multiple ways of characterizing temporal
relationships between the events F and G: For instance, one might specify how often event
F is followed by event G in, say, exactly 2 time points. According to Figure 2, this happens
in one out of four occurrences of F in T h. It might prove meaningful to exclude the final
occurrence of F in T h when determining this frequency, because naturally an occurrence
of F at tmax cannot be followed by a subsequent occurrence of G. Excluding the final
occurrence of F would yield one out of three for the desired frequency. Alternatively, one
could also specify how often F is followed by G within the next two time points. For the
exemplary thread from Figure 2, this would produce frequencies of 1 and 0.75 respectively,
again depending on whether the final occurrence of F is included.
This example illustrates already four different possible definitions of temporal relations
between events. To maintain flexibility in expressing temporal relations, we do not commit to specific definitions in PDT Logic, but instead we adapt an axiomatic definition of
frequency functions:
Definition 3.10 (Frequency functions, adapted from Shakarian et al., 2011). Let T h be
a thread, F , F 0 , G, and G0 be ground formulae, and t  0 be an integer. A frequency
function fr maps quadruples of the form (T h, F, G, t) to [0, 1] such that the following
axioms hold:
(FF1) If (F  G) is a tautology, then fr(T h, F, G, t) = 1.
(FF2) If (F  G) is a contradiction, then fr(T h, F, G, t) = 0.
(FF3) If (F  G) is neither a tautology nor a contradiction, then there exist threads T h1 ,
T h2  T such that fr(T h1 , F, G, t) 6= fr(T h2 , F, G, t).
(FF4) If F  F 0 and G  G0 , then fr(T h, F, G, t) = fr(T h, F 0 , G0 , t).
Axioms (FF1) and (FF2) ensure that in special casesi.e., (G  >), (F  ), or
(F  >, G  )frequency functions behave as temporal implications with premise
F and conclusion G. Axiom (FF3) enforces non-trivial frequency functions by requiring
that in all cases not covered by the first two axioms, there must be at least two threads
55

fiMartiny & Moller

with differing frequency values. Axiom (FF4) ensures that that fr is congruent to logical
equivalence. Examples of frequency functions satisfying these axioms are introduced below.
Remark 3.4. This definition mostly corresponds to the definition of frequency functions
from Shakarian et al. (2011), except that we do not require t > 0. In the work from
Shakarian et al., frequency functions are only intended to express temporal relationships
and therefore t is limited to nonzero values. By additionally allowing t = 0, we obtain
a concise framework that can express both temporal relationships and static constraints
within one time point. This will be exploited in the next section, where decision procedures
for PDT Logic are discussed.
To illustrate the concept of frequency functions, we now present formal definitions for
point and existential frequency functions adapted from Shakarian et al. that represent the
informal descriptions of frequencies from above:
The point frequency function pfr expresses how frequently some event F is followed by
another event G in exactly t time units:
pfr(T h, F, G, t) =

|{t : T h(t) |= F  T h(t + t) |= G}|
|{t : (t  tmax  t)  T h(t) |= F }|

(8)

If the denominator is zero, we define pfr to be 1. The denominator counts the total number of
occurrences of F in a given thread T h and the numerator counts the number of occurrences
of F followed by G after exactly t time units. Thus, the ratio pfr expresses how frequently
F is followed by G in exactly t time units. Note that the denominator only considers
occurrences of F up to time tmax  t. This is done to reflect the previously discussed
intuition that occurrences of F in the last t time points should be excluded from the
frequency, because there is no possibility that they can be followed by a subsequent G after
t time units.
The existential frequency function efr expresses how frequently some event F is followed
by another event G within the next t time units:
efr(T h, F, G, t) =
efn(T h, F, G, t, 0, tmax )
,
|{t : (t  tmax  t)  T h(t) |= F }| + efn(T h, F, G, t, tmax  t, tmax )

(9)

with
efn(T h, F, G, t, t1 , t2 ) =|{t : (t1 < t  t2 )  T h(t) |= F

 t0  [t, min(t2 , t + t)] (T h(t0 ) |= G)}|

The function ef n counts the number of occurrences of F followed by a subsequent occurrence
of G within the next t time units. The first summand of the denominator again counts the
total number of occurrences of F up to the time point tmax  t. In the second summand
of the denominator, additional occurrences of F followed by G within t time units. The
intuition of this definition is again to exclude occurrences of F in the final t time units if
they are not followed by G. Since G may occur within the range t, but this range cannot
be fully considered for the final t time points, only occurrences of F with an according
subsequent occurrence of G are considered for these final time points. Consequently, the
56

fiPDT Logic

ratio efr expresses how frequently some event F is followed by G within the next t time
units without letting single occurrences of F in the final t time points decrease the ratio.
Returning to the exemplary thread T h from Figure 2, we can evaluate the frequency
functions for the given thread: Suppose that we want to determine how often F is followed
by G exactly after two time steps. This can be expressed through a point frequency function:
1
pfr(T h, F, G, 2) = .
3
If instead we want to know how often F is followed by G within the next two time steps,
we can use an existential frequency function:
efr(T h, F, G, 2) =

3
=1
3

It should be noted that frequency functions can be used to model temporal relationships
usually expressed through temporal operators. For instance, pfr with t = 1 reflects
the next operator and efr with t = tmax reflects the future operator. The meaning
of additional temporal operators such as until can be captured through the definition of
additional frequency functions, if required.
3.2.6 Semantics of the Belief Operator
Now, with the definitions of subjective posterior probabilistic temporal interpretations and
the introduction of frequency functions, we can provide a formal semantics for the belief operators defined in Section 3.1. This semantics extends definitions from Shakarian
et al. (2011) for the satisfiability of static interpretations to obtain a formal definition of
probabilistic multi-agent beliefs. We start with providing a definition for the semantics of
atomic belief operators for the three different types of beliefs. Semantics of compound belief
formulae (i.e., involving connectives , , ) are defined below in Definition 3.16.
Definition 3.11 (Belief Semantics of the atomic belief operator). Let i be an agent and
Th be agent is interpretation at time t0 in pov thread Th. Then, it follows from this
Ii,t
0
interpretation that agent i believes at time t0 with a probability in the range [`, u] that
1. (Belief in ground formulae)
Th |= B `,u (F )) iff
a formula F holds at time t (denoted by Ii,t
0
t
i,t0
`

X
T hT ,T h(t)|=F



Th
Ii,t
0 (T h)  u.

(10)

2. (Belief in rules)
fr (F, G) holds (denoted by I Th |= B `,u (r fr (F, G))) iff
a temporal rule rt
i,t0
i,t0 t
`

X
T hT



Th
Ii,t
0 (T h)  fr(T h, F, G, t)  u.

57

(11)

fiMartiny & Moller

3. (Nested beliefs)
` ,u
a belief Bj,tj j () of some other agent j holds at time t0 (denoted by


` ,u

T h |= B `,u (B j j ())) iff
Ii,t
0
j,t
i,t0

`



X
T hT
` ,u
T h |=B j j ()
Ij,t
j,t

Th
Ii,t
0 (T h)  u.

(12)

The intuition behind this semantics is as follows. For beliefs in ground formulae Ft , the
Th (T h) of an agent i at time t0 in pov thread Th are
subjective posterior probabilities Ii,t
0
added for all threads T h that satisfy F at time t. Thus, the sum in (10) represents the
Th assigns to F . If this sum is within the specified boundaries [`, u],
exact probability that Ii,t
0
t
`,u
the respective belief B 0 (Ft ) holds for agent i at time t0 in pov thread Th.
i,t



T h (T h) for every thread are
For beliefs in rules, the subjective posterior probabilities Ii,t
0
fr (F, G). Thus,
weighted with the corresponding frequency fr(T h, F, G, t) from rule rt

T h (T h) in (11) represents the exact probability that I Th assigns to
the weighted sum of Ii,t
0
i,t0
the temporal relation between F and G according to frequency function fr. For beliefs
fr (F, G) only contains information about the type of frequency
in rules, the belief object rt
function fr, while constraints on the respective frequency values are given through the belief
quantification [`, u], i.e., an agent does not have probabilistic beliefs in specific frequency
values.

Remark 3.5. It should be noted that the semantics of beliefs in rules in (11) together
with the axiomatic definition of frequency functions in Definition 3.10 (p. 55) yields certain
fr (F, G). If G is a tautology or F is a contradiction
constraints on satisfiable beliefs in rules rt
(i.e., in Definition 3.10 FF1 is satisfied), it holds for the respective frequency function that
`,u fr
fr(T h, F, G, t) = 1 for every possible thread T h, and thus, any belief Bi,t
0 (rt (F, G)) is
satisfiable if and only if the belief is quantified with u = 1, regardless of the set of threads
Th . Analogously, if F is a tautology and G is a
T or the corresponding interpretation Ii,t
0
`,u fr
contradiction (i.e., FF2 is satisfied), any belief Bi,t
0 (rt (F, G)) is only satisfiable for ` = 0.
` ,u

`,u
j j
For nested beliefs Bi,t
()), the expression is unnested by first determining all
0 (Bj,t
` ,u

` ,u

possible pov threads T h for agent j such that Bj,tj j () is satisfied. If Bj,tj j () corresponds
to a belief in a fact or in a rule, (10) respectively (11) can be used to identify threads T h
T h |= B `j ,uj (). Otherwise, if  represents another belief formula, the belief has
such that Ij,t
j,t
to be unnested recursively until the innermost belief of the expression is obtained. Then, for
T h |= B `j ,uj (), agent is subjective posterior probabilities I Th (T h)
all threads T h with Ij,t
j,t
i,t0
are added again to determine whether the outer belief holds. Note that agent i does not
know the actual beliefs of agent j. However, due to the assumption of common and equal
priors discussed in Section 3.2.4, agent i is able to reason about agent js hypothetical
interpretation updates given that the system is in a specific thread. Thus, agent i is able
to compute (12) without knowing js exact beliefs.
Example 3.8 (Trains continued). We can use a point frequency function to express beliefs
about the punctuality of trains. Assume that both A and B judge the probability of a
58

fiPDT Logic

train running late (i.e., arriving after 3 instead of 2 time units, expressed through the
temporal rule r3pfr (at(T1 , CA ), at(T1 , CC ))) as being at most 0.4. This yields the following
belief formulae
0,0.4 pfr
Bi,0
(r3 (at(T1 , CA ), at(T1 , CC )))

,
0,0.4 pfr
Bi,0
(r3 (at(T2 , CC ), at(T2 , CB )))

i  {A, B}.

(13)

For the temporal rules expressed in these belief formulae, we obtain the following frequencies
from Figure 1 (p. 50):
pfr(T h, at(T1 , CA ), at(T1 , CC ), 3) = 0
pfr(T h, at(T1 , CA ), at(T1 , CC ), 3) = 1
pfr(T h, at(T2 , CC ), at(T1 , CB ) , 3) = 0
pfr(T h, at(T2 , CC ), at(T1 , CB ), 3) = 1

for T h  {T h1 , ..., T h3 }

for T h  {T h4 , ..., T h9 }

for T h  {T h1 , T h4 , T h5 }

for T h  {T h2 , T h3 , T h6 , ..., T h9 }

Combining these frequency values with the prior interpretation

I(T ) = 0.7 0.02 0.09 0.02 0.09 0.01 0.02 0.02 0.03 ,
given in Example 3.6 (p. 53) yields the sum
X
I(T h)  pfr(T h, F, G, 3) = 0.19
T hT

for both F = at(T1 , CA ), G = at(T1 , CC ) and F = at(T2 , CC ), G = at(T2 , CB ). As this sum
is within the belief quantification [`, u] = [0, 0.4], the belief formulae in (13) are valid. Note
that the prior probabilities from Example 3.6 have been specified such that both trains are
late with the same probability, and thus the respective sums for the above frequencies are
the same.
From the above definitions, we can use the belief about some fact F to quantify the
belief about the negation of this fact F :

`,u
Corollary 3.4 (Belief in negated facts). Let Bi,t
0 (Ft ) be an agents quantified temporal belief
about some fact F according to Definition 3.11. Then, the agents belief in the negation of
`0 ,u0
0
0
this fact F is given as Bi,t
0 (F ) with ` = 1  u and u = 1  `.

3.3 Evolution over Time
In order to completely specify a problem in PDT Logic, we introduce the concept of doxastic
systems. In the following, we assume that all syntactical objects are finite.
|A||T |

Definition 3.12 (Doxastic system). Let A be a set of agents, T be a set of threads, A0
be a matrix of prior probability distributions across T for every agent in A, and F be a
|A||T |

set of frequency functions. Then, we call the quadruple D = hA, T , F, A0
system.
59

i a doxastic

fiMartiny & Moller

Note that several of the parameters discussed before are not explicitly specified in a
doxastic system: neither the set of possible worlds , the set of ground atoms B, the set of
observation atoms Lobs , nor the set of time points  are explicitly specified. However, all
relevant information regarding these parameters is already contained in the specification of
T .

Remark 3.6. Since all agents share a common prior, all rows of A0 are the same. Thus,
one could obtain a more parsimonious problem specification by only providing the single
unique row vector of prior probabilities. The choice of using the matrix A0 nonetheless
is for notational purposes only: it will simplify the presentation of interpretation update
operations later on.
|A||T |

Definition 3.13 (Admissibility of doxastic systems). Let D = hA, T , F, A0
i be a
doxastic system. D is called admissible iff every world (implicitly) defined in T is admissible
|A||T |

(according to Definition 3.5, p. 48) and all rows of A0

sum to one.

To identify specific situations in a doxastic system after some time has passed and some
observations occurred, we furthermore define pointed doxastic systems:
|A||T |

Definition 3.14 (Pointed doxastic system, pds). Let D = hA, T , F, A0
i be a doxastic
system and H be a set of time-stamped observation atoms such that all observation atoms
from H occur in at least one of the worlds (implicitly) defined in T . Then we call the pair
hD, Hi a pointed doxastic system.
Definition 3.15 (Admissibility of pointed doxastic systems). Let hD, Hi be a pointed
doxastic system, and T the set of threads from D. hD, Hi is called admissible iff D is
admissible and there exists a thread T h  T such that ObsG (l)t  H : ObsG (l)  T h(t)
(i.e., T must contain at least one thread that complies with all timed observations from H).
Intuitively, the set of timed observations specified in a pds points to a certain situation
in a doxastic system. One could view t(H) = max{t : ObsG (l)t  H} as the present time in
a pds: the most recent observation occurred at t(H), all observations that actually occurred
in the past (t < t(H)) are specified in H (and are thus deterministic in retrospective), and
no further information about future observations t > t(H) is given. In this sense, H specifies
a certain history up to t(H) in a doxastic system and points to the last event of this history.
Example 3.9 (Trains continued). A doxastic system for the train example can be specified
as
D = h{A, B}, {T h1 , ..., T h9 }, {pfr, efr}, A0 i,
with



0.7 0.02 0.09 0.02 0.09 0.01 0.02 0.02 0.03
A0 =
.
0.7 0.02 0.09 0.02 0.09 0.01 0.02 0.02 0.03

To identify the situation described in Example 3.5 (p. 52, T1 is running late), we can specify
the following pointed doxastic system:
hD, {Obs{A} (at(T1 , CC )3 )}i
60

fiPDT Logic

3.3.1 Evolution of Probabilistic Interpretations
In accordance with the prior probability matrix A0 from Definition 3.12, we define an

interpretation matrix ATt h to store the interpretations of all agents A (with n denoting the
number of agents |A|) across all threads T h1 , ..., T hm given that the doxastic system is in
pov thread Th at time t:


Th (T h ) . . . I Th (T h )
I1,t
m
1
1,t



..
..
..

(14)
ATt h = 
.
.
.






T h (T h ) . . . I T h (T h )
In,t
m
1
n,t

With the definition of Ki from Equation (3) (p. 51), the update rule from Equation (4)
(p. 54), and using the prior probability matrix A0 from Definition 3.12, we can provide an

update matrix UtT h to calculate the interpretation matrix for any pov thread Th at any
time point t ( denotes the element-wise multiplication of matrices):






h
ATt h = ATt1
 UtT h

with

(uTt h )ij

=


0
1

 Th

i,t0

if T hj (t) 6 Ki (Th(t))
if T hj (t)  Ki (Th(t))

(15)

(16)



T h a normalization factor as defined in Equation (5) (p. 54).
and i,t
0
The time-stamped observations specified in the history H of a pds hD, Hi induce an
updated set of reachability relations Ki (T h(t)) for every thread T h that complies with
the given observations (for threads T h that do not comply with the given observations
Ki (T h (t)) = ). These updated reachability relations in turn yield the updated interpre
tations in ATt h . The complete state of interpretations at any time point for every possible
pov thread Th1 , ..., Thm can then be specified as a block matrix, which we call the belief
state (bs) of a pds at time t:
 


bs(hD, Hi, t) = ATt h1 , ..., ATt hm
(17)

We use bs(hD, Hi) to denote the sequence of all belief states bs(hD, Hi, t) from t = 1 to
t = tmax .
This definition of belief states can be seen as a specification of conditional probabilities:
the kth entry of bs(hD, Hi, t) specifies the interpretations of all agents across all threads at
time t given that the system is in pov thread Thk . Thusas every thread is considered as
a potential pov threada full specification of an agents belief state for m threads requires
m  m conditional probabilities for every time point t. This is a very general representation
of belief states to allow for an easy evaluation of subjective posterior interpretations at
arbitrary time points and pov threads and for an intuitive definition of belief state updates.
However, this general definition contains some redundant information. By leveraging certain properties of the semantics of PDT Logic, we identify means to obtain compressed
representations of the belief state in the following.
61

fiMartiny & Moller





Corollary 3.5 (Null vectors in ATt hk ). Due to the definition of (16), the ith row of ATt hk is
~0 iff agent is actual observations (as specified in H) do not match the observations specified
in thread T hk .
Proposition 3.6 (Belief state compression). Let hD, Hi be a pointed doxastic system and
let t be a time point such that t  t(H). Then, without any loss of information, the belief
state bs(hD, Hi, t) at time t can be represented through
T
(18)
bs(hD, Hi, t)0 = ~v1,t , ..., ~vn,t
with one probability distribution vector ~vi,t per agent i.


Proof. It follows directly from Corollaries 3.3 (p. 54) and 3.5 that the matrices ATt hk from
bs(hD, Hi, t) with nonzero rows i are exactly those that correspond to threads considered
possible by agent i at time t.
From the properties of Ki given in Corollary 3.1 (p. 51) follows that all worlds T h0 (t) 
Ki for t  t(H) are indistinguishable to agent i and therefore are associated with the same
interpretation. Thus, all nonzero ith rows of the matrices in bs are identical. Defining ~vi,t
as these unique nonzero rows i of bs, we obtain the representation of (18). Information
about impossible pov threads (as described in Corollary 3.5) is still maintained as they are
assigned a probability of 0 in ~vi,t .
It is important to note that this compressed representation is only applicable to time
points t  t(H), because in retrospective an agent is able to classify threads into two
categories: those that comply with the observations so far (i.e., those that are considered
possible), and those that do not. For time points t > t(H) this classification is not possible
because Ki (T h(t)) then depends on future observations and can therefore lead to a branching
of several distinct interpretations depending on the respective observations.
3.3.2 Evolution of Beliefs
In order to analyze the temporal evolution of beliefs, we use the update rule from (15) to
update belief states. Since different possible observations yield different branches in the
evolution of beliefs, we have to update every thread in the belief state individually, using

the respective update matrices UtT h as defined in (16):




bs(hD, Hi, t) = bs(hD, Hi, t  1)  (UtT h1 , ..., UtT hm )

(19)

Furthermore, to analyze satisfiability and validity of arbitrary finite belief expressions
`,u
~
Bi,t
0 () w.r.t. a given pds hD, Hi, we define an auxiliary belief vector b() for different beliefs
`,u
B 0 (). This vector ~b() contains one entry (~b())j for every possible thread T hj  T and
i,t

is defined as follows:
a)

`,u
Bi,t
0 (Ft )

b)

`,u fr
Bi,t
0 (rt (F, G)) :

c)

`,u
`k ,uk
Bi,t
()) :
0 (Bk,t

:

(
1 if T hj (t) |= F
(~b(Ft ))j =
0 if T hj (t) 6|= F

fr
(~b(rt
(F, G)))j = fr(T hj , F, G, t)
(
Th
`k ,uk
1 if Ik,t j |= Bk,t
()
`
,u
k k
~
(b(Bk,t ()))j =
T hj
`k ,uk
0 if Ik,t 6|= Bk,t ()

62

(20)

fiPDT Logic

Note that in the case of nested beliefs, the respective entries (~b())j are set to one if the
inner belief holds in thread T hj , i.e., it is assumed that T hj is the point of view thread for
`k ,uk
() is satisfied in this thread.
agent k and then it is checked whether ks belief Bk,t


Using (19) and (20), we can determine a matrix Pt0 () with the probabilities pTi,th0 k ()
that each agent i assigns at time t0 to some event , for all possible pov threads
Th1 , ..., Thm :4




T h1

T p1,t0
.
Pt0 () = bs(hD, Hi, t0 )  ~b(), ..., ~b() = 
 ..


pTn,th01



. . . pT1,th0m
.. 
..

.
.  ()

. . . pTn,th0m

(21)

For n agents and m threads, this results in a n  m matrix. The rows of this matrix can
be seen as conditional probabilities: agent i believes at time t0 that a fact  is true with

probability pTi,th0 k () given that the system is in pov thread Thk .
Remark 3.7. Computation of Pt0 () is straightforward for cases 20.a) and 20.b). To compute
the probabilities for nested beliefs in 20.c), we start with computing the innermost belief
(which is an instance of case 20.a) or case 20.b) since we assume finite expressions), and
then compute the nested beliefs iteratively.
Using Definition 3.11 (p. 57) and Equation (21), we can provide a definition for the
satisfiability and validity of beliefs:
Definition 3.16 (Validity and satisfiability of beliefs). Let B be a belief formula as defined
in Definition 3.4 (p. 46), hD, Hi a pointed doxastic system, and Pt0 () the corresponding
matrix of probabilities at time t0 as defined in (21). B is satisfiable (valid) w.r.t. hD, Hi iff
`,u
1. For B = Bi,t
0 ():

For at least one (all) thread(s) Thk  T , the entries in row i of Pt0 () satisfy ` 




T hk
T hk
pi,t
0 () and u  pi,t0 ().

`,u
2. For B = Bi,t
0 ():

For at least one (all) thread(s) Thk  T , the entries in row i of Pt0 () satisfy ` >




T hk
T hk
pi,t
0 () or u < pi,t0 ().

3. For B = B1  B2 :
For at least one (all) thread(s) Thk  T , the entries in the corresponding rows of
Pt0 () satisfy both B1 and B2 .
4. For B = B1  B2 :
B1 is satisfiable (valid) or B2 is satisfiable (valid).


4. Since we have to consider every possible pov thread Thk , we have to multiply every matrix ATt h 

T
bs(hD, Hi, t) with ~b(), thus we need to use the vector ~b(), ..., ~b()
with m rows.

63

fiMartiny & Moller

Remark 3.8. The distinction between valid and satisfiable belief formulae is only of interest
for beliefs at time t > t(H). For time points t  t(H) an agents belief is uniquely determined through the given observations (cf. Proposition 3.6), resulting in a single probability
associated to any belief. Therefore, all invalid belief formulae for t  t(H) are unsatisfiable.
From Definition 3.4 (p. 3.4) it follows that the belief object of an atomic belief formula B
as in Definition 3.16-1 can again be any arbitrary belief formula. If the inner belief formula
B 0 is one of the cases defined in Definition 3.16, validity and satisfiability of the entire
`,u
0
expression B = Bi,t
0 (B ) follows inductively from the above definition: If for at least one
(all) thread(s) Thk  T , both the inner belief formula B 0 is satisfied and the limits for the
`,u
0
outer belief of the respective thread are satisfied, the entire belief formula is B = Bi,t
0 (B )
satisfiable (valid).
Definition 3.16 gives rise to an important property of the belief operator, as the following
lemma shows:
`,u
Lemma 3.7 (Distributivity of the belief operator). Let B = Bi,t
0 (1  2 ) be a belief
formula with a belief object (1  2 ) and a connective   {, }. Then, we can express
`,u
`,u
B equivalently as B 0 = Bi,t
0 (1 )  Bi,t0 (2 ).

Proof. This result follows immediately from the validity and satisfiability of beliefs in Definition 3.16:
`,u
The formula B = Bi,t
0 (1  2 ) is satisfiable (valid) iff for at least one (all) thread(s)


T hk  T it holds that T hk |= 1 or Thk |= 2 and the respective entries in Pt0 () satisfy

`,u
Definition 3.16-1. For the former case, Bi,t
0 (1 ) is satisfiable (valid) as well, while for the
`,u
latter case Bi,t
0 (2 ) is satisfiable (valid), which reflects exactly the definition of disjunctive
`,u
`,u
belief formulae from Definition 3.16-4. Thus, B 0 = Bi,t
0 (1 )  Bi,t0 (2 ) is satisfiable (valid)

`,u
iff B = Bi,t
0 (1  2 ) is satisfiable (valid).

`,u
Similarly, the formula B = Bi,t
0 (1  2 ) is satisfiable (valid) iff for at least one (all)

thread(s) T hk  T it holds that both Thk |= 1 and Thk |= 2 hold and the respective
`,u
`,u
entries in Pt0 () satisfy Definition 3.16-1. Then, both Bi,t
0 (1 ) and Bi,t0 (2 ) are satisfiable
`,u
`,u
(valid) and thus, the formula B 0 = Bi,t
0 (1 )  Bi,t0 (2 ) is satisfiable (valid) according
`,u
`,u
to definition Definition 3.16-3. Thus, B 0 = Bi,t
0 (1 )  Bi,t0 (2 ) is satisfiable (valid) iff
`,u
B = Bi,t
0 (1  2 ) is satisfiable (valid).

To illustrate the evolution of beliefs, we finish the train example with an analysis of
expected arrival times.
Example 3.10 (Trains continued). From D, as specified in Example 3.9 (p. 60), we can
infer that Bob (and of course Alice, too) can safely assume at time 1 that Alice will arrive
at time 8 at the latest with a probability in the range [0.9, 1], as expressed in the belief
formula
0.9,1 ef r
BB,t = BB,t
(r7 (on(A, T1 ), (at(T2 , CB )  on(A, T2 ))))

64

(22)

fiPDT Logic

with t = 1. For this rule, we obtain the frequencies
efr(T h, at(T1 , CA ), (at(T2 , CB )  on(A, T2 )), 7) = 1

efr(T h, at(T1 , CA ), (at(T2 , CB )  on(A, T2 )), 7) = 0

for T h  {T h1 , ..., T h5 },

for T h  {T h6 , ..., T h9 },

i.e., in threads T h1 , ..., T h5 from Figure 1 (p. 50), the event (at(T2 , CB )  on(A, T2 )) occurs
within 7 time points following the event on(A, T1 ) from time t = 1 (and thus at time t = 8
at latest), while in threads T h6 , ..., T h9 , the event (at(T2 , CB )  on(A, T2 )) occurs only at
time t = 9, which is outside of the scope of r7efr and thus yields a frequency of zero.
At time point 1, Bob still considers all threads as possible, and thus Bobs subjective
posterior probabilistic interpretation

Th
IB,1
(T ) = 0.7 0.02 0.09 0.02 0.09 0.01 0.02 0.02 0.03
is equal to the prior interpretation given in Example 3.6 (p. 53) for all possible pov threads
Th. Combining this interpretation with the frequencies given above yields the sum
X 
Th
IB,1
(T h)  efr(T h, at(T1 , CA ), (at(T2 , CB )  on(A, T2 )), 7) = 0.92
T hT

and thus formula BB,1 is valid.
Now, consider the previously described situation, where T1 is running late and A does
not inform B about it. This leads to the updated interpretations given in (6) and (7) on
page 54, i.e.,


T h4
IA,3
=(

Th4
IB,3

0

0

0

0.4 0

0.2 0

0.4 0 ),

and

 ( 0.82 0.02 0.10 0.02 0 0.02 0 0.02 0 ).

These updates lead to a significant divergence in the belief of the expected arrival time:
The corresponding sum with respect to Alices updated interpretation is
X
T h4
IA,3
(T h)  efr(T h, at(T1 , CA ), (at(T2 , CB )  on(A, T2 )), 7) = 0.4,
(23)
T hT

(24)
obtained by Alices subjective posterior probability assignment of thread T h4 , which is the
only nonzero summand in the above sum; all other threads T h are either impossible from
T h4
Alices point of view (i.e., IA,3
(T h) = 0 for threads T h  {T h1 , T h2 , T h3 , T h5 , T h7 , T h9 }),
or the corresponding frequency is zero (for threads T h6 and T h8 ). Thus, Alices belief in
arriving at time point 8 at the latest is drastically reduced, as the lower bound ` of Alices
belief may not exceed 0.4. For instance,

0.4,1 ef r
BA,3
r8 (on(A, T1 ), (at(T2 , CB )  on(A, T2 ))) ,
(25)
is now a valid belief formula. The corresponding sum for Bobs belief at time point 3 is
X
T h4
IB,3
(T h)  efr(T h, at(T1 , CA ), (at(T2 , CB )  on(A, T2 )), 7) = 0.96,
(26)
T hT

65

fiMartiny & Moller

obtained by summing over Bobs subjective posterior interpretations for threads T h1 , ..., T h4 ;
the remaining threads again only contribute zero summands because either Bobs probability assignment or the corresponding frequency is zero for those threads. Thus, Bobs
previous belief (expressed in (22)) remains valid at time point t = 3, denoted by BB,3 .
Even though Alices beliefs have changed significantly, she is aware that Bob maintains
beliefs conflicting with her own, as is shown by the following valid expression of nested
beliefs:
1,1
BA,3
(BB,3 )
To verify that this nested belief holds, we need to consider all threads that Alice considers
possible (T h4 , T h6 , T h8 ) and determine what Bobs hypothetical beliefs would be in these
threads. For T h4 , this has already been analyzed in (26). Since threads T h4 , T h6 , and
T h8 are indistinguishable to Bob at time point 3, the same analysis results hold for all
three threads. Consequently, BB3 holds in every thread that Alice considers possible and
therefore the sum for this nested belief is
X
Th
Ii,t
0 (T h) = 1,
T hT

T h |=B
IB,3
B,3

i.e., Alice knows that Bobs belief is outdated.
Finally, consider the pointed doxastic system hD, Obs{AB} (at(T1 , CC ))3 i, i.e., the same
situation as before with the only difference that Alice now shares her observation of the
delayed train with Bob. It immediately follows that Bob updates his beliefs in the same
way as Alice, which in turn yields an update in Alices beliefs about Bobs beliefs so that
now the following expression is valid (because 1 is not a valid lower bound any longer):
1,1
(BB,3 )
BA,3

This example shows how Alice can reason about the influence of her own actions on
Bobs belief state and therefore she can decide on actions that improve Bobs utility (as he
does not have to wait in vain).

4. Satisfiability Checking for PDT Logic
In this section we will describe procedures to check whether there exists a model for some
given set of belief formulae B. For the discussions in this chapter, we assume that all models
and sets of formulae are finite. We start with formally defining the satisfiability checking
problem in PDT Logic. Using the semantics from the previous section, we derive a model
checking algorithm based on fully specified doxastic systems. Afterwards, we show how a set
of belief formulae can be used to specify a problem in PDT Logic andtogether with a given
set of threadshow this can be transformed into a mixed integer linear program in order
to employ existing solvers to decide satisfiability of PDT Logic formulae. Finally, we show
how suitable threads can be derived from a given set of belief formulae automatically. Using
transformations to linear programs is an established approach when deciding satisfiability
of probabilistic logics, as discussed for example by Fagin, Halpern, and Megiddo (1990).
However, if no priors are given, established decision procedures for probabilistic logics are
66

fiPDT Logic

not applicable to PDT Logic due to the formalisms update mechanism (cf. the update rule
from Definition 3.9, p. 54). This update mechanism
If a fully specified doxastic system hD, Hi is given, we can define the problem of checking
whether a set of belief formulae B is satisfiable with respect to this doxastic system as

follows. Recall from Section 3.2.4 that we use I T h to denote the set of all subjective
Th induced by a prior interpretation I in a pov thread Th.
posterior interpretations Ii,t
0
Definition 4.1 (Satisfiability Checking for PDT Logic). Let hD, Hi be a pointed doxastic
system with the set of threads T and according prior interpretation I specified in hD, Hi,
and B be a set of belief formulae. We say that B is satisfiable w.r.t. hD, Hi if there exists
a thread Th in T such that the corresponding interpretations satisfy all belief formulae B
from B:



sat(B, hD, Hi)  Th  T : B  B : I T h |= B
(27)
If such a specification is given, checking satisfiability of B with respect to hD, Hi corresponds to checking whether hD, Hi is a model for B. We continue with introducing a model
checking procedure for this fully specified input. Afterwards, we discuss how satisfiability
of a set of belief formulae B can be decided if no prior probabilities, or neither threads nor
prior probabilities are given.
4.1 A Model Checking Algorithm
A first approach of developing an algorithm to check whether a given set of belief formulae
B is satisfied by a given pointed doxastic system hD, Hi (i.e., checking whether hD, Hi is
a model for B) can be obtained through a direct application of the semantics of the belief
operator given in Definition 3.11 (p. 57). Algorithm 1 shows the resulting model checking
procedure. It starts with computing the belief states for all possible evolutions of the world
from t = 1 to tmax . Afterwards, it iterates through all belief formulae B  B and potential
pov threads Thk to determine whether the interpretation in the respective pov thread is
able to satisfy the current belief formula. If a thread is unable to satisfy some belief formula,
it is excluded from the set of potential pov threads for subsequent checks. If at least one
potential pov thread remains after all belief formulae have been checked (i.e., there is at
least one thread Thk so that all belief formulae B  B are satisfied), hD, Hi is a model for
B.
Theorem 4.1 (Soundness and completeness of Algorithm 1). The decision procedure Algorithm 1 is sound and complete and therefore a model checking procedure for PDT Logic.
Proof. Since the presented algorithm is essentially an inductive application of Definition 3.16
(p. 63), it is easy to see that it yields a sound and complete decision procedure for PDT
`,u
`,u fr
Logic. Basic belief formulae (Bi,t
0 (Ft ) and Bi,t0 (rt (F, G))) return satisfiability results by
directly using the respective semantic definitions from (10) and (11) as calculation rules.
`,u
`,u
0
00
For every possible compound belief formula of PDT Logic (Bi,t
0 (), Bi,t0 (B), B B , and
0
00
B  B ), the procedure provides an appropriate rule according to Definition 3.16 to break
down these formulae iteratively until base formulae are obtained, which can be decided as
above.
67

fiMartiny & Moller

Algorithm 1 Model Checking
procedure ModelChecking(hD, Hi, B)
h1
hm
bs(hD, Hi, 0)  (AT
, ..., AT
)
0
0
for t  1, tmax do
bs(hD, Hi, t)  bs(hD, Hi, t  1)  (UtT h1 , ..., UtT hm )
for B  B do
for Thk  T do
if not Check(bs(hD, Hi), Thk , B)) then
T  T \ {Thk }
if T =  then
return false
return true

. compute all belief states

. check if B is satisfied in Thk
. otherwise remove Thk from threads to check
. exit if no Th can satisfy B
. success if T is nonempty after checking all B  B

function Check(bs(hD, Hi), Thk , B)
switch (B)
. check formulae according to Def. 3.16
`,u
case Bi,t
0 ():
if  = B 0 then
. check nested belief formulae recursively (B 0 is a belief formula)
0

if not Check(bs(hD, Hi), T hk , B )) then
return false
Th
Pt0  bs(hD, Hi, t0 )  ~b()
. use ~b() from (20) to compute Pt0 with elements pi,t0k
Th

Th

Th

. true if pi,t0k  [`, u]

return (`  pi,t0k and u  pi,t0k )
`,u
case Bi,t
0 ():

Pt0  bs(hD, Hi, t0 )  ~b()
Th

Th

Th

. true if pi,t0k 6 [`, u]

return (`  pi,t0k or u  pi,t0k )
case B 0  B 00 :
return (Check(bs(hD, Hi), Thk , B 0 ) and
Check(bs(hD, Hi), Thk , B 00 ))
case B 0  B 00 :
return (Check(bs(hD, Hi), Thk , B 0 ) or
Check(bs(hD, Hi), Thk , B 00 ))

68

fiPDT Logic

The asymptotic complexity of Algorithm 1 depends on the number of belief operators
`,u
Bi,t
0 () contained in B:
Theorem 4.2 (Time complexity of Algorithm 1). Let B be a set of belief formulae and let
k be the number of belief operators contained within B. Then, using Algorithm 1 to check
whether a given pointed doxastic system hD, Hi with m threads is a model for B has time
complexity O(k  m).
Proof. For a given pds with m threads and k belief formulae in B, the main procedure calls
the check function at most m  k times. If B is a base formula with only a single belief
`,u
operator Bi,t
0 (), a single call of the check function will return a result. Otherwise, if a
`,u
belief formula B contains more than one belief operator Bi,t
0 (), the check function will
be called recursively, until base formulae are obtained. Thus, for k belief operators in B,
the satisfaction checks are performed at most k  m times, yielding a time complexity of
O(k  m).

From Theorem 4.2 we immediately obtain a complexity result for the model checking
problem in PDT Logic:
Corollary 4.3 (Complexity of model checking for PDT Logic). The model checking problem
for PDT Logic is in PTIME.
This result shows that model checking of a set of belief formulae w.r.t. a given pointed
doxastic system can be done in polynomial time. If a fully specified pds (and thereby an
exhaustive specification of the set of possible threads T ) is given, this result shows that
Algorithm 1 presents a tractable procedure to perform the model checking task. However,
this approach has a significant drawback as it assumes an exhaustive specification of T
together with precise prior probability assignments I(T ). Although there are some problem
domains that actually come with such a specification (e.g., cf. the cyber security scenario
described in the introduction), this assumption renders Algorithm 1 infeasible for most
problem domains. To overcome this problem, we will proceed with discussing a different
approach, which enables satisfiability checking without requiring a specification of exact
probabilities. Moreover, we show how representative threads with respect to a set of belief
formulae B can be constructed automatically, so that positive satisfiability results can
potentially be obtained without requiring a full materialization of all possible threads T .
4.2 A Compact Problem Specification
Up until now we used a (pointed) doxastic system to specify a problem domain for model
checking a set of belief formulae B in PDT Logic. In the following sections, we show how
we can reformulate the problem such that an extended set of belief formulae together with a
value for tmax is used. The main idea of this approach is that background knowledge regarding the target domain is not given through an explicit specification of possible threads and
according probabilities, but instead through sets of rules in B that describe how the target
domain may evolve over time. This approach has several advantages: In most scenarios,
compared to requiring an exhaustive set of possible threads, specifying a set of rules (which
can be expressed as prior beliefs) gives a more natural means of specifying background
69

fiMartiny & Moller

knowledge of the problem domain (e.g., cf. Example 3.2 on page 47, which actually starts
with a verbal description of rules and only later introduces the corresponding set of possible
threads). Furthermore, using a set of rules to describe a problem domain is a fairly established approach and therefore this approach will provide options to simplify transformation
of existing problem specifications into PDT Logic. Finally, since the set of possible threads
grows exponentially with every additional time point in the set of time points  and every
additional ground atom of the language L, an exhaustive problem specification through
the set of possible threads quickly becomes infeasible, while the same situation could be
described succinctly through a small set of rules. Even though such a succinct specification shifts the exponential nature of this problem from the required input specification to
computational efforts, we show that the exponential effect can be curtailed with heuristics
when constructing possible threads automatically.
4.2.1 Identification of Key Parameters from a Set of Belief Formulae
To simplify the following discussion, we will restrict temporal rules to only use the point
frequency function pfr. Recall that point frequency functions are used to specify that
some event F is followed by another event G after exactly t time points, while existential
frequency functions efr are used to specify that some event F is followed by another event G
within a time interval t. If existential frequency functions are required to specify a problem
domain, we can rewrite them as disjunctions of point frequency functions, as the following
proposition shows. If further frequency functions are defined, the presented techniques can
be easily adapted.
Proposition 4.4 (efr rewriting). An existential frequency function efr can be equivalently
represented as a disjunction of point frequency functions pfr:
efr
(F, G) 
rt

_

pfr
rt
 (F, G)

 0tt

t:

Recall that, according to Definitions 3.12 and 3.14 on page 59, the specification of a pds
consists of a set of agents A, a set of threads T , a set of frequency functions F, a matrix of
|A||T |

prior probability distributions A0
, and a set of time-stamped observations H.
Since we will only use point frequency functions in the following, the set of frequency
functions F is always fixed to {pfr}, and thus there is no need to specify this set separately.
Instead of explicitly specifying the set of agents A, we can just determine it from the
`,u
belief expressions Bi,t
0 () contained in the set of belief formulae B. With a slight abuse of
`,u
`,u
notation, we use Bi,t
0 ()  B to denote that belief operator Bi,t0 () appears somewhere in
a set of belief formulae B. Then, we can define the set of agents AB specified through a set
of belief formulae B as
`,u
AB = {i : Bi,t
(28)
0 ()  B}

Generally, it is possible that the explicit specification of the set of agents A is larger than
the set AB . However, it is obvious that if no beliefs are expressed for some agent i (i.e.,
i  A and i 6 AB ), this agent will not influence satisfiability checking results whatsoever.
Thus, this agent can simply be disregarded and, consequently, it suffices to use the set AB .
70

fiPDT Logic

Similarly, instead of specifying the set of ground atoms of the language L through the
sets of predicates Lpred and constants Lcons , we can define a set of event formulae FB
representing all belief objects occurring in a set of belief formulae B as
n
o
`,u
`,u fr
`,u fr
FB = F : Bi,t
.
(29)
0 (Ft )  B  Bi,t0 (rt (F, G))  B  Bi,t0 (rt (G, F ))  B
This definition gives rise to a potential definition of the set of possible worlds  as
the Herbrand base B FB of FB (resp. the set of admissible worlds    complying with
Definition 3.5 (p. 48). However, as we will show later, there are more options to constrain
the sets of possible worlds to allow for a more concise problem representation.
Note that according to Definition 3.2 (p. 45), formulae may include both atoms and
observation atoms. Consequently, FB does not only specify ontic facts of possible worlds,
but also possible observations of these ontic facts. With this approach, occurrences of
observations are limited to the ones specified in FB . This can be seen as the specification
of a sensor model for groups of agents G  AB .
Remark 4.1. A strict application of (29) would prohibit simple specifications of group observations ObsG (l) with |G| > 1 in B. To ensure that the set of admissible worlds
actually
V
contains worlds with ObsG (l), a full specification of such an observation as G 0 G ObsG 0 (l)
in B would be required (otherwise there might be no world   B FB with  |= ObsG (l)
that satisfies the second property in the definition of possible worlds (cf. Definition 3.5)).
However, the required full specification of an observation for admissible worlds can be determined solely through the simple observation specification ObsG (l). In order to keep the
specification of B as compact as possible,
we allow for simple specifications ObsG (l) and
V
assume that they are expanded with G 0 G ObsG 0 (l) while creating FB .
An alternative approach would be to construct FB only through ontic facts appearing
in B and create a set of admissible worlds by combining all ontic facts with all possible
admissible observations w.r.t. Definition 3.5. These approaches differ in the requirements
of observation specifications: the former requires to specify every possible observation explicitly, while the latter requires to exclude every impossible observation explicitly. Since in
most scenarios the set of observations actually being possible (w.r.t. the problem domain)
is significantly smaller than the set of all admissible observations, the presented approach
will usually yield a more compact problem specification. If desired, one could employ the
latter approach instead without impacting the functionality of the following methods.
Background knowledge regarding the target domainthat was given through an explicit
representation of possible threads beforecan now also be specified as prior beliefs (i.e.,
`,u
beliefs Bi,0
()) in B. Recall from Section 3.2.4 that we assume a commonly known prior


T h which is equal for all agents i  A . As the belief semantics is defined
distribution Ii,t
B


T h (cf. Definition 3.11, p. 57), it follows
with respect to the probabilistic interpretations Ii,t
0

`,u
that every prior belief Bi,0
() is common knowledge as well. Consequently, we can express
background knowledge as prior beliefs of any arbitrary agent i  AB .
`,u fr
As pointed out in Section 3, satisfiability of beliefs in temporal rules Bi,t
0 (rt (F, G))

with certain properties are independent of the respective set of threads T or the associated
interpretation I(T ) (cf. Remark 3.5, p. 58): if the respective frequency function corresponds
71

fiMartiny & Moller

to FF1 or FF2 of Definition 3.10 (i.e., F is a contradiction, G is a tautology, or F is a tautology and G is a contradiction), beliefs are either trivially satisfied for quantifications with
u = 1 (resp. ` = 0) or generally unsatisfiable. In the former case, trivially satisfiable beliefs
can be disregarded without influencing satisfiability results, while for the latter case satisfiability checking can terminate immediately with a negative result. Thus, in the following we
assume that B contains only beliefs in rules that do not correspond to frequency function
axioms FF1 and FF2.
Example 4.1 (Trains revisited). An informal verbal description of the train problem was
given in Example 3.2 (p. 47) with a corresponding formal specification through a set of
possible threads T in Example 3.4 (p. 51)and probability assignments in Example 3.6 (p. 53).
Using the above considerations on the expression of background knowledge as beliefs in rules,
we can reformulate the verbal rules given in Example 3.2 together with the probabilistic
information from Example 3.6 as a set of formal beliefs B with according explanations
below:



1,1
1,1


B1 = BA,0
at(T
,
C
)

B
on(A,
T
)
,
1
1
1
1
A

A,0







.81,.81 pfr


(B20 )
r0 (at(T1 , CA ), punct(T1 ))
BA,0


B
=

2


.81,.81 pfr

 BA,0
r0 (at(T2 , CC ), punct(T2 )) ,
(B200 )









1,1


r3pfr ( punct(T1 )  at(T1 , CA ), at(T2 , CC )  on(A, T2 )) (B30 )
BA,0


B3 =



1,1 pfr

 BA,0
(r5 (punct(T1 )  at(T1 , CA ), at(T2 , CC )  on(A, T2 )) , (B300 )







1,1
B=
r2pfr ( punct(T2 )  at(T2 , CC ), at(T2 , CB )  on(A, T2 )) (B40 )
BA,0

B4 =



1,1


 BA,0
r3pfr (punct(T2 )  at(T2 , CC ), at(T2 , CB )  on(A, T2 )) , (B400 )








1,1


B5 = BA,0
r0pfr (punct(train)  at(train, city), Obs{A} (punct(train))) ,









.93,.93 pfr

B6 = BA,0
r2 (Obs{A} (punct(train)), Obs{AB} (punct(train))) ,









train  {T1 , T2 },




city  {C , C }
A

B













































































Note that all beliefs are expressed for time t = 0, i.e., these are prior beliefs that are by
definition commonly known among all agents. All beliefs expressed in this example are
assigned to A, but they could equivalently be assigned to B or to both.
B1 states that train T1 is at city CA at time t = 1 and that Alice is on that train. B2
states that both agents believe that trains are punctual (denoted by punct(train)) with a
probability of 0.81. The probability values in this example are obtained by summing over the
probabilities given in Example 3.6 for all threads given in Example 3.4 where the respective
belief object is satisfied. To have an equivalent representation of the previous example, we
72

fiPDT Logic

use exact probability values (i.e., ` = u) instead of intervals. Note that punct(train) is an
additional predicate with a variable train that helps to formulate the background knowledge
in a concise way. Formula B2 does not yet specify what the consequences of a non-punctual
train are, only that a train is expected to be punctual with a certain probability. B3 states
that Alice is able to board train T2 after three time steps if train T1 is punctual and that
Alice has to wait for two additional time points otherwise. B4 states that train T2 will
arrive at city CB two time points after being in city CC . Otherwise she will arrive one
time point later. B5 states that Alice will always notice when her train leaves a city not
punctually. This is an example for a sensor model specification as discussed above. Finally,
B6 states that Alice will call Bob with a probability of 0.93 if her train is not punctual.
Example 4.2 (Trains continued). With the definition of the set of belief formulae B from
the above example, we can now also specify the set of event formulae FB required to model
the possible scenarios described through B:






at(T
,
C
),
at(T
,
C
),
at(T
,
C
),
at(T
,
C
),
1
1
2
2
A
B
B
C






 on(A, T ), on(A, T ), punct(T ), punct(T ),

1
2
1
2
FB =


Obs{A} (punct(T1 )), Obs{AB} (punct(T1 )),








 Obs (punct(T2 )), Obs

(punct(T
))
2
{A}
{AB}
To simplify the following discussion, we assume that conjunctive formulae B = B 0 
 B are replaced with individual formulae of the respective conjuncts: B = B \
{B}  {B 0 , B 00 }. This does not impact the satisfiability checking properties of B because
all formulae in B have to be satisfied simultaneously in order to return a positive result
and thus, both B 0 and B 00 have to be satisfied, regardless of their representation as two
individual formulae or as one conjunction.
Now, what remains to be determined is the set of threads T , a corresponding prior
B 00

|A||T |

probability distribution I(T ) (resp. a matrix of prior probability distributions A0
,
where every row is formed by I(T )), and possibly a set of time-stamped observation atoms
H. The tasks of determining T and H can be treated jointly: since the set of relevant
threads needs to be determined anyway, we simply create T such that T |= H.
In the next section we will show how we can transform a set of PDT Logic belief formulae
B together with a given set of threads T into a linear program in order to determine
satisfiability of B with respect to T . Afterwards, we will discuss how a suitable set of
threads T to represent the information contained in B can be constructed automatically.
Using these results, it is possible to model a problem domain in PDT Logic solely through
a set of belief formulae B together with the specification of a maximum time point tmax .
All other key parameters of the domainsuch as the set of agents and the set of ground
atomscan be extracted from B automatically.
4.3 Representing the Satisfiability Problem as a Linear Program

The considerations from the previous section show that most parameters for a problem
specification can be extracted from a given set of belief formulae B. In this section, we
assume that only a set of belief formulae B together with a set of possible threads T is given.
73

fiMartiny & Moller

B is then satisfiable with respect to T (denoted by sat(B, T )) if a prior interpretation I(T )
can be found such that all belief formulae in B are satisfied. By extracting linear constraints
on I(T ) from B, we show how the satisfiability problem can be transformed into a linear
program. Checking satisfiability of B with respect to T is then equivalent to checking
whether the corresponding linear program has a feasible solution.
For a given set of threads with an unknown prior interpretation I(T ), the satisfiability
checking task significantly increases in complexity compared to the model checking task.
Formulation of the satisfiability checking problem in Definition 4.1 (p. 67) might be somewhat delusive: As the existence of a single thread in the context of some interpretation
suffices to verify satisfiability of a set of belief formulae B, it appears intuitive to develop
a method to construct such a threadif possibleand neglect the other threads, or, vice
versa, start with the entire set of threads T and iteratively prune all threads that fail to satisfy any formula from B. In fact, such a pruning approach was used in Algorithm 1 (p.68)
to check whether a given set of threads is a model for a set of belief formulae. Unfortunately,
these approaches are inapplicable if the prior interpretation is unknown. As the semantics of
belief operators (cf. Definition 3.11 (p. 57) relies on subjective posterior probabilistic interpretations (i.e., on probability assignments for multiple threads), it is generally not possible
to find a single thread Th satisfying the satisfiability checking problem from Definition 4.1
without determining probabilities for all threads. Vice versa, it is generally not possible
to discard any thread, because determining whether it satisfies any belief formula can only
be done if its respective probability assignment is known. Instead, we will show that belief
formulae can equivalently be expressed as sets of linear constraints on the unknown prior
interpretation I(T ). Then, checking satisfiability of B is equivalent to checking whether
there is a possible assignment to I(T ) so that all constraints are satisfied.
We will use xk to denote the unknown prior probability of thread T hk , i.e., if T contains
m threads, then its unknown prior probability assignment is represented as
I(T ) = x1 ,    , xm

T

.

(30)

The goal of the following methods is to provide constraints on the xk so that all belief
formulae B  B are satisfied. Since these variables represent a probability distribution
over the set of threads, there are two obvious constraints to begin with:
0  xk  1, k  {1, ..., m}
and

m
X

xk = 1

(31)

(32)

k=1

4.3.1 Representation of Subjective Posterior Probabilities
Since the semantics of beliefs is defined in terms of the respective agents subjective probability assignments in the respective pov thread, we need means to express the subjective
Th of an agent i in terms of the prior probability
posterior probabilistic interpretations Ii,t
0
values xk . These interpretations change at a time point t whenever an observation Obs{i} (l)t
is possible for agent i. If an observation is possible for an agent, we can partition the set of
threads into two sets: one partition containing the set of threads where agent i does observe
74

fiPDT Logic

the respective fact l and one partition where agent i does not observe the respective fact.
The subjective probability assignments need to be updated within each partition to reflect
this information about observation occurrences: Taking every thread within a partition as
a possible pov thread, the probability assignments for all other threads within this partition
need to be scaled according to the update rule in Definition 3.9 and the pov thread specific
probability assignments for all threads outside of the respective partition need to be set to
zero.
Generally, this leads to one vector of subjective probabilities over all threads for every
possible pov thread (cf. the Definition of belief states in Equation (17), p. 61). However, we
can leverage the semantic properties of PDT Logic to obtain a parsimonious representation
of the updated subjective probabilities without representing every pov thread explicitly.
Note that all threads within one partition as described above are indistinguishable to agent
i at the respective time point (i.e., all threads within one partition exhibit exactly the same
set of observations for agent i up to time point t) and therefore receive the same probability
assignment for every possible pov thread within this partition (cf. Proposition 3.6, p. 62).
Consequently, the updated probability assignments for every thread in T can receive only
one of two different types of value assignments: a scaled version of the threads previous
probability assignment according to Definition 3.9 (p. 54), or zero, depending on whether
the agent actually observes the fact l or not. The following proposition shows that we do not
need to consider the cases with zero probabilities in order to perform satisfiability checking
tasks.


T h be the subjective posterior
Proposition 4.5 (Irrelevance of zero-interpretations). Let Ii,t
0
0
probability interpretation at time t for some agent i in pov thread Th (i.e., this interpreta-

tion is determined through the prior interpretation and interpretation updates corresponding
to pov thread Th). If this interpretation assigns a probability of zero to some thread T h (i.e.,
Th (T h) = 0), then satisfiability of any subsequent nontrivial belief B 00 () with t00 > t0 is
Ii,t
0
i,t


T h (T h).
independent of Ii,t
0

`,u
Proof. Every belief Bi,t
0 () with ` > 0 in a fact or in another belief (i.e.,  = Ft or
` ,u

 = Bj,tj j ()) requires that there needs to be at least one thread T h with a nonzero


T h (T h) = 0 can clearly not
probability such that T h |= . Therefore, a thread T h with Ii,t
0
`,u
00
0
prove satisfiability of a belief Bi,t
00 () with t  t . A negative satisfiability result (i.e., B

is unsatisfiable w.r.t. T ) cannot be obtained from such a zero assignment either, because
any consistent interpretation (i.e., the probability assignments of all threads sum to one)
needs to assign a nonzero probability to at least one thread, which could then possibly
`,u
satisfy the belief. The same considerations hold for beliefs Bi,t
0 () with ` = 0 and u < 1:


T h (T h) = 0 satisfies the lower bound ` = 0, the upper bound u < 1
Although a thread with Ii,t
0


T h (T h0 ) > 0 such
requires the existence of another thread T h0 with a nonzero probability Ii,t
0


T h (T h) = 0 can only prove satisfiability of beliefs B `,u ()
that T h0 |= . Consequently, Ii,t
0
i,t0
with ` = 0 and u = 1. These are trivial beliefs that are satisfied by every thread and
every possible probability assignment and thus, their satisfiability can be proven without
Th (T h) = 0, too.
Ii,t
0

75

fiMartiny & Moller

`,u fr
Analogous considerations hold for beliefs in rules: A belief Bi,t
0 (rt (F, G)) with ` > 0
requires the existence of a thread with a nonzero probability such that fr(T h, F, G, t) > 0,
Th (T h) = 0 cannot prove satisfiability of this belief. Satisfiaand thus a thread T h with Ii,t
0
`,u fr
bility of a belief Bi,t
0 (rt (F, G)) with ` = 0 and u < 1 depends on the respective frequencies
0
fr(T h , F, G, t) in additional threads T h0 with nonzero probabilities.

As a result of this proposition, we can merge the nonzero entries from both cases (agent
i observes the fact l and agent i does not observe the fact l) into a single probability
distribution vector for each agent i and time point t. This yields a modified version of the
update rule from Definition 3.9. We will use this modified update rule to determine linear
constraints on the unknown prior probabilities xk .
Definition 4.2 (Modified update rule). Let i be an agent, t0 be a time point where some observation Obs{i} (l) can occur and T h be a thread. Then, a compressed subjective posterior
probability assignment Ii,t0 (T h) for agent i at time t0 for thread T h is given through
Ii,t0 (T h) =

1
 Ii,t0 1 (T h)
Th
i,t
0

(33)

T h again being a normalization factor to ensure that the probabilities of all threads
with i,t
0
that agent i considers possible sum to one:
X
Th
Ii,t0 (T h0 )
i,t
0 =
T h0 (t0 )Ki (T h(t0 ))

Example 4.3 (Modified update rule). To illustrate the modified update rule, we return to
the situation described in Example 3.7 (p. 54). In this example we assumed that train T1
is running late and A does not inform B about it. This resulted in the following updated
interpretation for A:

Th8
Th6
Th4
= 0 0 0 0.4 0 0.2 0 0.4 0
= IA,3
= IA,3
IA,3
In the given example, two additional hypothetical partitions of the set of threads T are
possible for Alice at time point t = 3 . If train T1 is running late and A does inform
B about it, threads T h5 , T h7 , and T h9 are indistinguishable to A, yielding the updated
subjective interpretation

Th5
Th7
Th9
IA,3
= IA,3
= IA,3
= 0 0 0 0 0.14 0 0.65 0 0.21
If T1 is on time, Alice considers threads T h1 , T h2 , and T h3 as possible. The corresponding
subjective interpretation is then

Th1
Th2
Th3
IA,3
= IA,3
= IA,3
= 0.86 0.03 0.11 0 0 0 0 0 0
These three different subjective interpretations have nonzero entries exactly for the threads
that are in the partitions of the respective pov thread. Since the partitions are not overlapping, we can merge the nonzero entries into a single probability vector

IA,3 = 0.86 0.03 0.11 0.4 0.14 0.2 0.65 0.4 0.21 .
76

fiPDT Logic

Note that in this modified update rule, the update for each pov thread does not specify interpretations over all threads anymore, but instead only the reflexive interpretations
for each thread T h, given that T h is the pov thread, are used. As discussed above, for
the satisfiability problem this is still a sufficient representation of posterior probabilities,
because all other potential pov threads Th in the respective partition are indistinguishable
to agent i and therefore yield exactly the same interpretations. It should be noted however that Ii,t0 (T h) is not a probabilistic vector anymore, i.e., its elements do not sum to
one. Compared to the representation of belief states from Section 3.3.1 (p. 61), information
about distinguishable worlds is lost. Thus, reconstruction of an agents belief state from this
representation is only possible with an additional specification of the respective relations
Ki .
Returning to the problem representation from (30) (p. 74), we can use the modified update rule to obtain an inductive definition of subjective posterior probabilities based on the
T
respective (unknown) prior probabilities xk . If I(T ) = x1 ,    , xm is the prior interpretation over the set of threads, agent is compressed subjective posterior interpretations
Ii,t0 at the time point t0 of the first possible interpretation can be represented as
Ii,t0 (T ) =



1
1i,t0

 x1 ,    ,

1
m
i,t0

 xm



0

T

,

(34)

k determined through
with the update factors i,t
0

1
i,t
0



i,t
1,1



.
m T

i,t
x
,



,
x
=

0
1
m
 ..


..
.

0

i,t
m,1   

i,t0

j,k


0
i,t
1,m
.. 

. ,
0

i,t
m,m

(
1 if T hk (t0 )  Ki (T hj (t0 ))
=
0 if T hk (t0 ) 6 Ki (T hj (t0 ))
0

with a (symmetric) matrix of indicators i,t
j,k denoting whether agent i considers thread
0
T hk possible in thread T hj at time t . Using (34) as the base case, we can then define
interpretation updates for the next possible observation at time t00 inductively as
Ii,t00 (T ) =



1
1i,t00



1
1i,t0

 x1 ,    ,

1

m
i,t00



1

m
i,t0

 xm

T

(35)

To simplify notation, in the following we use a single factor aki,t0 to represent the agk  k  ...) for all observations that can occur at
gregated sequence of scaling factors (i,t
i,t2
1
time points t1 , t2 , ... between t = 1 and t = t0 for agent i, i.e., agent is subjective posterior
interpretations Ii,t0 (T ) at time t0 are given as
Ii,t0 (T ) = a1i,t0  x1 ,    , am
i,t0  xm

T

.

(36)

Note that potential interpretation updates for an agent i can occur at some time point t
if and only if some observation Obs{i} (l) is possible at that time point. Hence, for any time
interval between two possible observations, the subjective interpretations are constant:
77

fiMartiny & Moller

Proposition 4.6 (Piecewise constant interpretations). Let t1 and t2 with t1 < t2 be two
time points such that observations for an agent i are possible at t1 and t2 , but at no time
point t in between t1 and t2 . Then, the compressed subjective interpretation Ii,t0 (T ) is
constant for all time points t1  t < t2 :
t  [t1 , t2  1] : Ii,t (T ) = Ii,t1 (T )
This proposition states that all constraints identified in the following section do not
only restrict the subjective interpretations at single time points, but instead restrict the
interpretations for the respective time interval between any two possible observations.
4.3.2 Extracting Linear Constraints from Belief Formulae
Now that we have established representation (36) of subjective posterior interpretations in
terms of the unknown prior probabilities xk , we can use this representation to extract linear
constraints on the xk from the set of belief formulae B.
We assume that the distributive property of the belief operator from Lemma 3.7 (p. 64)
`,u
is applied whenever possible, i.e., belief formulae Bi,t
0 (B1  B2 ) with   {, } are
`,u
`,u
separated into Bi,t
0 (B1 )Bi,t0 (B2 ). Furthermore, without loss of generality, we can assume
that conjunctive formulae B = B1  B2 are replaced through B \ {B}  {B1 , B2 } and that
trivial beliefs (with ` = 0 and u = 1) are removed from B.
Moreover, we assume that all belief formulae B  B are represented in negation normal
form (NNF), i.e., the negation operator is only applied to atoms. Since any arbitrary logic
formula can equivalently be expressed as a formula in NNF (cf. e.g., Baaz, Egly, Leitsch,
Goubault-Larrecq, & Plaisted, 2001), this assumption does not restrict B either.
With these assumptions, the following types of belief formulae B can occur in B:
`,u
 atomic belief formulae B = Bi,t
0 ()
`,u
 negated atomic belief formulae B = Bi,t
0 ()

 disjunctive belief formulae B = B1  B2
For each of these types, we will now show how the respective formula can be expressed as
a set of linear constraints on the prior probabilities xk .
Atomic Belief Formulae Using the parsimonious representation of subjective posterior
interpretations Ii,t0 (T h) given through the modified update rule in Definition 4.2 requires
an adaption when deciding satisfiability of belief formulae. Before, satisfaction of a belief
formula in a given pov thread could be determined by summing over the respective subjective interpretations of all threads in which the belief object is satisfied. Threads that
an agent does not consider possible anymore w.r.t. the given pov thread are automatically
excluded as they have a probability assignment of zero. In the compressed representation,
the respective probability assignments for threads considered impossible are overloaded with
different probability assignments given that the agent is in another pov thread, as illustrated
in Example 4.3. We obtain an adapted version of satisfiability testing by explicitly ensuring
that only those interpretations of threads are summed that are still considered possible
w.r.t. the respective pov thread. As this additional constraint only excludes summands
78

fiPDT Logic

with zero-values, the original semantics is still maintained. Thus, we use equivalence classes
1 , C 2 , ...} to represent the set of distinguishable situations for agent i at time t0 .
Ci,t0 = {Ci,t
0
i,t0
Naturally, two threads T h1 , T h2 are indistinguishable and therefore in the same equivalence
class for agent i at time t0 , if they exhibit exactly the same observations for agent i for all
time points t  {1, .., t0 }. All threads outside of a particular equivalence class receive a
probability of zero for every pov thread Th within the respective equivalence class andas
discussed in the previous sectiontherefore do not contribute to the satisfiability properties. Then, in the belief semantics from Definition 3.11 (p. 57), instead of summing over all
k : (Th  C k )
threads T h  T with certain properties, we can restrict the range to T h  Ci,t
i,t
while maintaining the original semantics. Naturally, a belief formula is then satisfiable if
there exists at least one equivalence class that satisfies the respective beliefs. For instance,
`,u
a belief in a fact Bi,t
0 (Ft ) is satisfiable with respect to an agent is compressed subjective
posterior interpretation Ii,t0 at time t0 iff
k
Ci,t
0  Ci,t0 : ` 

X
n:

k T h (t)|=F )
(T hn Ci,t
n
0

ani,t0  xn  u

(37)

Such a constraint can equivalently be expressed as a set of linear inequalities with
conjunctive and disjunctive connectives, leading to an alternative representation of the
satisfiability problem.
Corollary 4.7 (Alternative satisfiability representation for atomic beliefs). Let Ii,t0 (T ) =
T
T
a1i,t0  x1 ,    , am
and Ij,t (T ) = a1j,t  x1 ,    , am
be the compressed
j,t  xm
i,t0  xm
representation of agent i and js respective subjective posterior probabilities at time t0 and
t, respectively, as given in (36), and let Ci,t0 and Cj,t be the sets of worlds that agent i and
agent j can distinguish at the respective time point. Then, an atomic belief expression B is
satisfiable w.r.t. Ii,t0 (T ) for
`,u
1. belief in a fact B = Bi,t
0 (Ft ) iff



_
k C
Ci,t
0
i,t0

X
k
(T hn Ci,t
0

n:
T hn (t)|=F )

ani,t0



 xn  ` 



X
k
(T hn Ci,t
0

n:
T hn (t)|=F )

ani,t0


 xn  u

!
(38)

`,u pfr
2. belief in a rule B = Bi,t
0 (rt (F, G)) iff

_



k C
Ci,t
0
i,t0

X
k )
n: (T hn Ci,t
0




X
n:

k )
(T hn Ci,t
0


ani,t0  xn  pfr(T hn , F, G, t)  `
ani,t0  xn  pfr(T hn , F, G, t) 

79


u

!
(39)

fiMartiny & Moller

` ,u

`,u
j j
3. nested belief B = Bi,t
()) iff
0 (Bj,t

_



k C
Ci,t
0
i,t0

X
k )
n: (T hn Cj,t
k
k }6=)
T hn |=  ({Cj,t Ci,t
0




X
k )
n: (T hn Cj,t
k C k }6=)
T hn |=  ({Cj,t
i,t0




X

X
n:

anj,t  xn 

ani,t0  xn 

k C k }
T hn {Cj,t
0
i,t0



uj

ani,t0  xn  `

k C k }
n: T hn {Cj,t
i,t0
 T hn |=




anj,t  xn  `j

u







!
(40)

 T hn |=

As discussed above, the representations for satisfiability of beliefs in facts (38) and beliefs
in rules (39) are obtained directly by replacing the range of threads T in the sum with the
k considered possible by agent i at time t0 . The inequalities for
respective set of threads Cj,t
0
nested beliefs (40) are obtained by ensuring in the first two lines that in every situation
that agent i conceives as a possible situation for agent j (expressed through the constraint
k )  C k  C k 6= ), agent js belief in the respective fact  (expressed
n : (T hn  Cj,t
j,t
i,t0
through the constraint T hn |= ) is within [`j , uj ]. The latter two lines ensure that for
these respective situations, the outer belief of agent i is satisfied, as well. Note that the
belief object  in (40) might contain additional belief operators, i.e., beliefs with multiple
levels of nesting are expressed. In this case, evaluation of T h |=  in the first two lines
of (40) yields additional constraints of type (38)(40), such that the formula is evaluated
recursively.
Negated Atomic Belief Formulae To satisfy a negated atomic belief formula B =
`,u
Bi,t
0 (), the accumulated probabilities of all threads that satisfy the belief object  in
k must be either lower than ` or higher than u, i.e., the individual
an equivalence class Ci,t
0
disjuncts
in (38)(40) have to be negated. By pushing the negations inward and using
X
(   ) as a representative for the respective sums defined in (38) and (39) to express


satisfiability of atomic beliefs, we can represent negations of the according beliefs expressed
in (38) and (39) as
!
 X
  X

_

(   ) < `  
(   ) < u .
(41)
k C
Ci,t
0
i,t0





If nested beliefs as defined in (40) contain negated belief operators, this can be expressed
accordingly by replacing the conjunctive constraints on ` and u (resp. `j and uj ) with the
corresponding disjunctive constraints (41) for negated atomic belief formulae.
80

fiPDT Logic

Disjunctive Belief Formulae With the above inequalities, the required constraints for
a disjunctive formula B = B1  B2 can easily be expressed as an additional disjunction of
inequalities. Let C1 and C2 be the sets of inequalities to express satisfiability of B1 and B2
according to (38)(41), respectively. Then, the constraints for B can be expressed as
C1  C2

(42)

Example 4.4 (Trains continued). In Example 4.1 (p. 72), a set of belief formulae B has
been given for the train example. To illustrate the extraction of linear constraints from
this set, we continue to use the set of threads depicted in Figure 1 (p. 50) with a minor
modification: to reflect the model specified in B of Example 4.1, we assume that the
predicate punct(train) is explicitly encoded in the respective threads. Moreover, for the
sake of the example we assume that the prior probabilistic interpretations are yet unknown.
We use x1 , ..., x9 to denote these unknown probabilities. Note that for our example, we
are only dealing with prior beliefs, i.e., we only have one equivalence class C = T and
all scaling factors ani,t0 are equal to one. This significantly eases the presentation of this
example. Of course, in general we have to deal with both multiple equivalence classes and
multiple varying scaling factors. As this highly increases complexity of the presentation, we
refrain from giving explicit examples for these cases. The constraints from B are extracted
as follows:

.81,.81 pfr
r0 (at(T1 , CA ), punct(T1 )) :
 For belief B20 = BA,0
pfr(T h, at(T1 , CA ), punct(T1 ), 0) = 1 for T h  {T h1 , ..., T h3 }

pfr(T h, at(T1 , CA ), punct(T1 ), 0) = 0 for T h  {T h4 , ..., T h9 }

and thus application of rule (39) yields the constraints
x1  x2  x3  0.81
x1 + x2 + x3 

0.81

In this special case where ` = u, we can simplify this constraint to
x1 + x2 + x3 =

0.81

Since all of the rules exhibit this property, we slightly deviate from (39) and only
give the equivalent equality constraints for subsequent rules in order to simplify the
presentation.

.81,.81 pfr
Accordingly, for belief B200 = BA,0
r0 (at(T2 , CC ), punct(T2 )) we obtain:
pfr(T h, at(T2 , CC ), punct(T2 ), 0) = 1 for T h  {T h1 , T h4 , T h5 }

pfr(T h, at(T2 , CC ), punct(T2 ), 0) = 0 for T h  {T h2 , T h3 , T h6 ..., T h9 }

with the corresponding constraints
x1 + x4 + x5 =
81

0.81

fiMartiny & Moller


.93,.93 pfr
 For belief B6 = BA,0
r2 (Obs{A} (punct(train)), Obs{AB} (punct(train))) :
for T h  {T h1 , T h3 , T h5 , T h9 } :

pfr(T h, punct(train), Obs{AB} (punct(train)), 2) = 1,

for T h  {T h2 , T h4 , T h6 } :

pfr(T h, punct(train), Obs{AB} (punct(train)), 2) = 0,

for T h  {T h7 , T h8 } :

pfr(T h, punct(train), Obs{AB} (punct(train)), 2) = 0.5

and thus application of rule (39) yields the constraint
x1 + x3 + x5 + 0.5  x7 + 0.5  x8 + x9 = 0.93
 For the remaining beliefs, the respective belief objects are satisfied in every thread
and thus we only obtain the redundant constraints
9
X

xk = 1.

k=1

One can easily verify that prior probabilistic interpretation given in Example 3.6, i.e.,

x = 0.7 0.02 0.09 0.02 0.09 0.01 0.02 0.02 0.03
indeed is a solution with respect to the above constraints. Of course, for the given
example, this solution was expected, as B was defined such that it exactly reflects the
situation described in the examples from the previous section.
4.3.3 Transformation into a Disjunctive Program
For every belief formula B  B, the above extractions of linear constraints yield a set of
inequalities of the form
ai,1 x1 + ai,2 x2 + ... + ai,m xm  bi ,

(43)

with xj representing the unknown prior probabilities of threads T h1 , ..., T hm , the coefficients
ai,j set to the respective values of ani,t0 if they contribute to this constraint and set to zero
otherwise, and the value b1 set to the respective limit obtained through ` or u.
As Corollary 4.7 shows, every belief formula B  B yields a disjunctive set of inequality
constraints, i.e., every belief formula B introduces branches in the set of linear constraints.
By collecting all inequalities of the form (43) that constrain a single branch, we can express
the constraints in matrix form:
Ax  b,
(44)
with



a1,1
 ..
A= .
an,1


..
.



 
 
a1,m
x1
b1
..  , x =      , and b =   
. 
xm
bm
an,m
82

fiPDT Logic

This form of representation has a close connection to linear programming (LP). Linear
programming (e.g., Murty, 1983) is a solution method to optimization problems where some
linear function of a set of continuous variables xk is to be optimized with respect to a given
set of linear constraints. While the task of satisfiability checking does not require any
optimization and thus actually solving a linear program is not required for this work, we
will exploit similarities between our sets of linear constraints and LP in order to show how
the satisfiability problem can be solved.
The standard form of an LP problem (Murty, 1983) gives a set of constraints exactly in
the form of (44). Every solution x that satisfies these constraints is called feasible and the
entire solution space for (44) is called feasible region. Thus, checking whether a set of belief
formulae B is satisfiable is equivalent to checking whether a corresponding LP problem
has a non-empty feasible region. For standard LP problems with constraints of the form
(44), the feasible region is a convex polytope, which allows performing this check with little
computational effort (Garey & Johnson, 1979).
Unfortunately, extracting linear constraints from a set of belief formulae B as described
in Section 4.3.2 does not yield a single set of constraints in the form of (44), but instead
a disjunction of different sets of constraints. This gives rise to the representation of the
satisfiability checking problem as a disjunctive program (DP) (Balas, 1998):
Corollary 4.8 (Satisfiability Checking as a Disjunctive Program). Let B be a set of belief
formulae, let T be a set of threads and let D be the set of all disjunctive branches d of linear
constraints extracted from B and T according to the extraction rules (38)-(42). Then, the
satisfiability checking problem can be formulated as a disjunctive program (Balas, 1998):
_
Ad x  bd
(45)
dD

B is satisfiable with respect to T , denoted by sat(B, T ), if (45) has a solution.
A disjunctive program is called bounded, if the range of every variable xk is restricted
through lower and upper bounds. Since we will rely on the bounded property subsequently,
we state the following result:
Lemma 4.9 (Satisfiability Checking as a Bounded DP). Let B be a set of belief formulae
and T be a set of threads. Checking satisfiability of B with respect to T can be represented
as a bounded disjunctive program.
Proof. This is a straightforward result: Corollary 4.8 shows that satisfiability checking
for PDT Logic can be represented as a disjunctive program in the form of (45). Since
every variable xk in (45) represents a probability value, all xk are naturally bounded by
0  xk  1.
In a disjunctive program, the feasible region cannot be guaranteed to be convex anymore,
nor can it be guaranteed that the solution space even represents a connected region. This
significantly increases the complexity of determining whether a nonempty solution space
exists. To analyze this problem in more detail and to show connections to established
solution approaches, we will discuss in the next section how a disjunctive program in the
form (45) can be further transformed.
83

fiMartiny & Moller

4.3.4 Transformation into a 0-1 Mixed Integer Linear Program
The concept of linear programs with continuous variables xk subject to linear constraints of
the form (43) can be extended to so-called mixed integer linear programs (MILPs) (Schrijver,
1986). Opposed to standard linear programming, for MILPs it is not required that all
variables xk have a continuous domain. Instead, MILPs can use a mix of both continuous
and integer variables. There are several equivalent ways of representing a MILP, we adopt
the representation from Fischetti, Glover, and Lodi (2005), which specifies the constraints
of a MILP as
Ax  b

xj integer

j  I

with an index set I indicating which of the variables xj are integer variables. A special
case of MILPs are 0-1 mixed integer linear programs (Williams, 2009), where the integer
variables xj are restricted to binary values:
Ax  b

xj  {0, 1}

(46)
j  I

By augmenting the set of variables x with binary switching variables xj for every possible
disjunction, it is possible to represent disjunctive programs in the form of (45) as 0-1 MILPs
in the form of (46) (Balas, 1985). This leads to a central result for satisfiability checking in
PDT Logic:
Theorem 4.10 (Satisfiability Checking as 0-1 MILP). Let B be a set of belief formulae
and T be a set of threads. The problem of checking satisfiability of B with respect to T
can be transformed into a corresponding 0-1 mixed integer linear program M so that B is
satisfiable with respect to T iff M has a feasible solution.
Proof. Lemma 4.9 shows that satisfiability checking for PDT Logic can be represented as
a bounded disjunctive program, such that a set of belief formulae B is satisfiable iff the
corresponding bounded disjunctive program has a feasible solution. The proof of Theorem
4.4 from Balas (1985) shows that every bounded disjunctive program can be equivalently
represented as a 0-1 mixed integer program M . Consequently, satisfiability checking for
PDT Logic is equivalent to checking whether M has a feasible solution.
We can leverage Theorem 4.10 to obtain complexity results for the satisfiability problem
in PDT Logic:
Theorem 4.11 (Complexity of PDT SAT w.r.t. a given set of threads). Checking satisfiability of a set of PDT Logic belief formulae B with respect to a given set of threads T is
NP-complete.
Proof. It is generally known that checking whether a bounded 0-1 mixed integer linear
program has a feasible solution is NP-complete (cf. Bienstock, 1996). As Theorem 4.10
shows that satisfiability checking in PDT Logic with respect to a given set of threads T can
be reformulated as a 0-1 MILP with bounded variables xk (cf. Lemma 4.9), it follows that
84

fiPDT Logic

satisfiability checking for a set of belief formulae B with respect to a given set of threads
T is in NP.
Arbitrary propositional formulae F (cf. Definition 3.2, p. 45) can be expressed in PDT
1,1
Logic by using them as a belief object for a strict prior belief Bi,0
(F ). Since it is well known
that the boolean satisfiability problem (SAT) is NP-complete (Cook, 1971), it follows that
any problem in NP can be transformed to a satisfiability checking problem in PDT Logic.
Hence, the satisfiability checking problem in PDT Logic is NP-hard and consequently NPcomplete.
The NP-completeness result shows that the problem is in NP and therefore we immediately obtain another important property of the satisfiability problem in PDT Logic:
Corollary 4.12 (Decidability of PDT SAT). Checking satisfiability of a set of PDT Logic
belief formulae B is decidable.
MILPs have been subject to extensive research for decades, and thus an ample variety
of solving methods has been proposed (e.g., Balas, Ceria, & Cornuejols, 1993, Balas, Ceria,
& Cornuejols, 1996, Balas & Perregaard, 2002, to name some of the most notable work on
MILP solving, and especially Fischetti et al., 2005 and Bertacco, Fischetti, & Lodi, 2007 to
find feasible solutions of MILPs). This research gave rise to various efficient implementations
of MILP solvers, both commercial (e.g., ILOG, 2016, Gurobi Optimization, Inc., 2016) and
non-profit products (e.g., Gnu Project, 2016, Computational Infrastructure For Operations
Research (COIN-OR) Project, 2016). For a given set of threads, PDT Logic satisfiability
checking can be reformulated as a 0-1 MILP problem, and thus any of these state-of-the-art
MILP solvers can be exploited for relatively fast satisfiability checks for most instances of
PDT Logic belief formulae B with respect to a given set of threads T .
The results from this section show how satisfiability of a set of PDT Logic belief formulae B can be decided with respect to a given set of threads, even if no specific prior
probability assignment is specified. As the overall goal of this section is the design of a
decision procedure that requires only a set of belief formulae B as an input, we continue
the discussion of satisfiability testing with the development of a method to automatically
construct a set of threads representing the background knowledge specified in B.
4.4 Prior Constraints on Possible Threads
To determine whether the set of belief formulae B is satisfiable, we need to obtain a set
of possible threads that reflects the background knowledge specified in B. In this section,
we describe how we can identify certain constraints on the set of possible threads T prior
to actually starting to generate threads that represent the information specified in B. To
identify such prior constraints, we discuss different properties of the belief formulae contained in B. Using these properties, we can create a taxonomy of belief formulae depending
on the respective impact on the set of possible threads T . Beliefs with certain properties
can then be used to constrain the search space for sets of possible threads prior to actually search for these sets. After discussing prior constraints in this section, we use these
results in Section 4.5 to develop a decision procedure for PDT Logic that requires neither
a specification of probabilities nor a specification of possible threads.
85

fiMartiny & Moller

4.4.1 A Taxonomy of Belief Formulae
The set of belief formulae B may contain beliefs with various features that will have different
impacts on the sets of admissible worlds at specific time points t. We will discuss these
features below and show how they yield a taxonomy of belief formulae. This taxonomy
allows for the classification of beliefs into three different types with respect to their impact
on the sets of admissible worlds. In particular, we can identify beliefs that are independent
of any specific probability assignment and of any Kripke relations Ki . This classification is
for technical purposes: beliefs that depend neither on specific probability assignments nor
on specific Kripke relations can be used to derive initial constraints on the sets of possible
worlds at some or all time points t  tmax . We use B to denote the set of all worlds
admissible with respect to a set of belief formulae B, and we use B (t) to denote the set
of admissible worlds with respect to a set of belief formulae B at time t.
Recall that there are three different kinds of beliefs: beliefs in facts, beliefs in rules, and
beliefs in beliefs. As before, we differentiate between prior beliefs that hold at time point
t = 0 (and are therefore commonly known among all agents) and posterior beliefs that hold
at time points t > 0.
`,u pfr
We can further distinguish beliefs in rules Bi,t
0 (rt (F, G)) with respect to t: we call
pfr
pfr
(F, G) a dynamic rule if t > 0. Accordingly,
(F, G) a static rule if t = 0 and we call rt
rt
we can separate beliefs in rules into beliefs in static rules and beliefs in dynamic rules,
respectively. These beliefs differ with respect to their temporal impact: a static rule will
constrain the possible worlds instantaneously, i.e., r0pfr (F, G) states that there can be no
world  such that both  |= F and  6|= G hold. A dynamic rule on the other hand requires
that whenever a world  with  |= F occurs, there must be another world  0 with  0 |= G
after t time steps.
Finally, we can classify beliefs with respect to their probabilistic quantifications: we call
`,u
a belief Bi,t
0 () strict, if both ` = u = 0 or ` = u = 1. For the sake of simplicity, in the
following we assume without loss of generality that strict beliefs are always represented with
0,0
1,1
` = u = 1. Any strict belief Bi,t
() can easily be rewritten as Bi,t
().5 We call a belief
trivial if ` = 0 and u = 1. Obviously, these beliefs are trivially satisfied by any arbitrary
interpretation, thus they do not impact satisfiability checking results at all and therefore
can be removed from B.
Remark 4.2. From the definition of the belief semantics (Definition 3.11, p. 57) it follows for
1,1
the special case of strict beliefs Bi,t
() that (i) agent i considers the occurrence of the belief
objects complement  as impossible and (ii) that this occurrence is indeed impossible.
Thus, strict beliefs comply with the common definitions of knowledge as justified true belief
and belief that is stable with respect to the truth (cf. e.g., Shoham & Leyton-Brown, 2009,
page 433). Consequently, we could also refer to a strict belief as knowledge and equivalently
use the established knowledge operator Ki () instead of Bi1,1 ().
1,1
Remark 4.3. Note that the concept of strict beliefs only applies to positive beliefs Bi,t
().
1,1
For the negation of such a belief, Bi,t (), it follows from Definition 3.16 (p. 63) that there
fr
fr
5. If the belief object  is a temporal rule rt
(F, G), we represent  as rt
(F, G). This is possible
because we do not need to consider frequency functions that correspond to axioms FF1 and FF2 from
Definition 3.10 (p. 55) and only use point frequency functions pfr. If other frequency functions are used,
their negations need to be defined accordingly.

86

fiPDT Logic

is at least one thread that does not satisfy the belief object , which in turn implies ` < 1.
1,1
Consequently, these beliefs Bi,t
() are considered as non-strict in the following discussion.

Using these features, we can create a taxonomy of beliefs as depicted in Figure 3 to
identify prior constraints on the set of possible threads. This taxonomy is obtained by
successively distinguishing between strict and non-strict, prior and posterior beliefs, between
beliefs in facts, rules and nested beliefs, and finally between beliefs in static and dynamic
rules. Nested beliefs are only considered as strict (prior) beliefs, if all involved beliefs are
strict (prior), otherwise they are considered as non-strict (posterior). If a nested belief is
actually strict and prior, we can unnest this belief and consider only the innermost belief
expression: since prior beliefs are commonly known and therefore identical for all agents
i  AB , it is evident that for any strict belief of any agent i, all other agents know that agent
i has this strict belief. Consequently, strict prior beliefs can be nested to an arbitrary depth
without introducing any further constraints: they are satisfied exactly if the innermost
belief is satisfied. Thus, we do not need to consider nested strict prior beliefs explicitly.
This taxonomy gives rise to three different types of belief formulae with respect to their
impact on the sets of admissible worlds:
Definition 4.3 (Belief formula typification). A set of belief formulae B can be categorized
into three different types of beliefs:
 Type 0: These are beliefs that restrict the set of admissible worlds B (t) at every
time point t   . Thus, type 0 beliefs have the highest impact because they can be
exploited to prune the set of admissible worlds B globally. An evaluation of these
beliefs relies neither on a specific probability assignment nor on any given Kripke
structures Ki .
 Type 1: These are beliefs that restrict sequences of possible worlds. Moreover, they
can potentially restrict the sets of admissible worlds B (t) at specific time points.
Thus, type 1 beliefs have less impact than type 0 beliefs because they can only be
exploited to prune the sets of admissible worlds B (t) locally. Again, an evaluation
of these beliefs relies neither on a specific probability assignment nor on any given
Kripke structures Ki .
 Type 2: This type encompasses all remaining beliefs in B that are neither type 0 nor
type 1 beliefs. These beliefs are situation-specific and cannot be used to prune the
sets of admissible worlds a priori. Satisfiability of these beliefs depends on a suitable
probability assignment or on the evaluation of Kripke structures in the respective
threads.
We use Tk (B) to denote the set of type k beliefs from B.
The main goal of this belief formula taxonomy is to identify constraints on possible
worlds  and possible threads T h that can be evaluated prior to searching for a suitable
probability assignment, namely by using the belief formulae in T0 (B) and T1 (B) to prune
the search space of possible sets of threads T that may show satisfiability of B. It should
be noted that the existence of a thread T h  T violating a belief from T0 (B) or T1 (B)
technically does not preclude satisfiability of B with respect to T , as there is a special
87

fiMartiny & Moller

all beliefs

non-strict beliefs
`<1

strict beliefs
`=1

If all ` = 1, only
the innermost beprior beliefs
lief is of interest

posterior beliefs
t0 > 0

0

t =0
belief in 0 beliefs
1,1
` ,u0
Bi,0
(Bj,t
())

belief in beliefs
`,u
1,1
B1,1
0 (Bj,t ())
TF
1 (B)

belief in rules
1,1 fr
Bi,0
(rt (F, G))

belief in facts
1,1
Bi,0
(Ft )

disjunctive belief
formulae
1,1
1,1
Bi,0
(1 )  Bi,0
(2 )    

belief in dynamic rules
t > 0

belief in facts
1,1
Bi,t
0 (Ft )
belief in rules
1,1 fr
Bi,t
0 (rt (F, G))

disjunctive belief
formulae
1,1
1,1
Bi,0
(1 )  Bi,0
(2 )    

belief in dynamic rules
t > 0

T1 (B)

belief in static rules
t = 0
T0 (B)

Type 0: These beliefs
have the highest impact,
because they restrict every
world at every time point.

belief in static rules
t = 0

Type 1: These beliefs restrict threads independently of any probabiliy assignment.
Moreover, they can potentially restrict possible worlds at individual time points.

Figure 3: Taxonomy of belief formulae

88

T2 (B)

Type 2: All remaining beliefs; they can be
treated the same way.

fiPDT Logic

case of a suitable probability assignment: If there is a thread T h  T such that some
belief B  T0 (B) or B  T1 (B) is not satisfied, there could still be suitable probability
assignments I(T ) such that sat(B, T ) holds iff I(T h) = 0. The effect of excluding such a
thread T h from T or assigning a prior probability I(T h) of zero is the same (cf. Remark 3.3,
p. 53), i.e., the respective thread is marked as impossible. Since we aim at reducing both
the search space of possible threads and the input to the satisfiability check sat(B, T ), we
exploit belief formulae in T0 (B) and T1 (B) to exclude impossible threads prior to searching
for suitable probability assignments.
Type 0 belief formulae As depicted in Figure 3, the set of type 0 belief formulae is
1,1 pfr
formed by formulae with strict prior beliefs in static rules Bi,0
(r0 (F, G)) from B. Since
prior beliefs represent the background knowledge and since it follows from the definition of
strict beliefs that they cannot be violated in any world, it is clear that the rule r0pfr (F, G)
has to be always satisfied. As this is a static rule, it has to be satisfied in every world
  B . We define the set of type 0 beliefs as
1,1 pfr
T0 (B) = {B  B : B = Bi,0
(r0 (F, G))}

(47)

with arbitrary formulae F and G.
Type 1 belief formulae The set of type 1 beliefs contains all strict prior beliefs that
are not in the set T0 (B). The contributions of this set T1 (B) are twofold: As T1 (B) only
comprises strict prior beliefs, every thread in a potential set of threads T has to satisfy all
beliefs B  T1 (B). Moreover, constraints from T1 (B) may constrain the sets of worlds
B (t) at individual time points t   regardless of any specific thread. According to
Figure 3, we define the set of type 1 beliefs as

1,1
T1 (B) = B  B :
B = Bi,0
(Ft )
1,1 pfr
 B = (Bi,0
(rt (F, G))  t > 0)

1,1
1,1
 B = (Bi,0
(1 )  Bi,0
(2 )     )

	

(48)

For a potential set of possible threads T , the beliefs specified in this set T1 (B) have to
be satisfied by every thread T h  T . Note that satisfiability of beliefs in dynamic rules and
disjunctive belief formulae generally depends on worlds  at multiple time points and thus
satisfiability of T1 (B) cannot be ensured by only constraining sets of worlds at single time
points. However, by analyzing strict prior beliefs in facts and their potential interplay with
dynamic rules we can derive constraints for the sets of worlds B (t) at specific time points
t   as follows.
1,1
Strict prior beliefs in facts B = Bi,0
(Ft ) restrict the set of admissible worlds B (t) at
time t by enforcing that F holds at every world   B (t). In the following, we use TF1 (B)
to denote such strict prior beliefs in facts F at certain time points t. Moreover, we use
1,1
B |= Ft as a shorthand for Bi,0
(Ft )  B to denote that B enforces F at time t.
Through interplay with existing constraints on sets possible worlds B (t) at individual
time points t, strict beliefs in dynamic rules can yield additional constraints: For a belief
1,1 pfr
formula B = Bi,0
(rt (F, G)), t > 0, additional constraints might be derived, depending
on the type of belief in the respective rules premise F : if (T0 (B)  TF1 (B)) |= Ft is given,
89

fiMartiny & Moller

1,1
we can extract a strict prior belief in a fact B 0 = Bi,0
(Gt+t ), which then again restricts
the set of possible worlds at time point t + t and is therefore added to TF1 (B).
Since dynamic rules can be considered as temporal implications (cf. Definition 3.10 from
Section 3), these rules can also be applied backwards to obtain additional constraints: If a
1,1 pfr
belief formula B = Bi,0
(rt (F, G)), t > 0 is given and the rules negated conclusion G
is already enforced at some time point t (i.e., (T0 (B)  TF1 (B)) |= Gt ), the rules premise
1,1
F cannot be satisfied at time t  t. Thus, we can add the belief B 0 = Bi,0
(Ftt ) as an
F
additional constraint to T1 (B).
Extending the set of type 1 beliefs through dynamic rules may lead to a chained ex1,1 pfr
tension: if we have a belief in a dynamic rule Bi,0
(rt (F, G)) and a corresponding belief
1,1
1,1
F
Bi,0 (Ft )  T1 (B), this will lead to the additional belief Bi,0
(Gt+t )  TF1 (B), which
1,1 pfr
in turn might trigger another dynamic rule Bi,0
(rt (G, G0 )). Analogously, any additional
belief in TF1 (B) could also trigger further backward rule applications.
To capture all constraints that emerge from forward and backward chaining of strict
dynamic rules, we define the set TF1 (B) as the following fix-point set:6

TF1 (B) =

1,1
{Bi,0
(Ft )  B}
1,1
 {Bi,0
(Gt+t ) :

1,1 pfr
t > 0  Bi,0
(rt (F, G))  B

 (T0 (B)  T1 (B)) |= Ft }

1,1
 {Bi,0
(Ftt ) :

1,1 pfr
t > 0  Bi,0
(rt (F, G))  B

 (T0 (B)  T1 (B)) |= Gt }

(49)

After having determined all constraints on individual time points, we can reduce this
1,1
set TF1 (B) such that it contains at most one belief Bi,0
(Ft ) for every time point t. If
1,1
1,1
F
T1 (B) contains multiple beliefs Bi,0 (Ft ), Bi,0 (Gt ) regarding the same time point t, we
1,1
can replace them by a joint belief Bi,0
(Ft0 ) with F 0 = F  G. Note that this substitution
uses Lemma 3.7 (p. 64) to merge different belief expressions into one expression with a
conjunctive belief object. We still assume that belief formulae with conjunctions of belief
operators are separated into atomic belief formulae.
Type 2 belief formulae The set of type 2 belief formulae consists of all beliefs in B that
are neither type 0 nor type 1 beliefs. Thus we define this set as
T2 (B) = (B \ T0 (B)) \ T1 (B)

(50)

6. For this representation, we have only considered the influence of temporal rules for the set TF
1 (B). In
1,1
1,1
principle, information from disjunctive formulae B = Bi,0
(1 )      Bi,0
(n ) in T1 (B) could yield
additional constraints on the sets B (t): If TF
1 (B) enforces n1 disjuncts in B to be false, the remaining
disjunct must be satisfied. As the belief objects of the respective disjuncts might be dynamic rules again,
a formal representation of this consideration would result in a rather intricate specification. Since we have
to ensure that any potential thread satisfies all beliefs in T1 (B) anyways, omitting disjunctive formulae
in the construction of TF
1 (B) does not impact satisfiability results. Yet an actual implementation of the
described procedures could exploit this consideration to obtain additional pruning conditions in special
cases.

90

fiPDT Logic

Example 4.5 (Trains continued). Continuing with the set of belief formulae B from Example 4.1 (p. 72) and assuming that conjunctive formulae B = B 0  B 00 are treated as
individual formulae B 0 and B 00 , we obtain the following sets of typed belief formulae:
 1,1 pfr
	
T0 (B) = BA,0
r0 (punct(train)  at(train, city), Obs{A} (punct(train)))
(B5 )
 1,1

T1 (B) = BA,0
at(T1 , CA )1 ,

1,1
BA,0
on(A, T1 )1 ,

(B10 )
(B100 )


1,1
BA,0
r3pfr ( punct(T1 )  at(T1 , CA ), at(T2 , CC )  on(A, T2 )) ,

1,1 pfr
BA,0
(r5 (punct(T1 )  at(T1 , CA ), at(T2 , CC )  on(A, T2 )) ,

1,1
BA,0
r2pfr ( punct(T2 )  at(T2 , CC ), at(T2 , CB )  on(A, T2 ))
	
1,1
BA,0
r3pfr (punct(T2 )  at(T2 , CC ), at(T2 , CB )  on(A, T2 )) ,
 1,1

TF1 (B) = BA,0
at(T1 , CA )1 ,
	
1,1
BA,0
on(A, T1 )1 ,

(B30 )
(B300 )
(B40 )
(B400 )
(B10 )
(B100 )

T2 (B) = B \ T0 (B) \ T1 (B)

.81,.81 pfr
= {BA,0
r0 (at(T1 , CA ), punct(T1 )) ,

.81,.81 pfr
BA,0
r0 (at(T2 , CC ), punct(T2 )) ,

.93,.93 pfr
BA,0
r2 (Obs{A} (punct(train)), Obs{AB} (punct(train))) }

(B20 )
(B200 )
(B6 )

The taxonomy of belief formulae provides means to construct sets of admissible worlds
B (t) for every time point t   . Type 0 beliefs (i.e., beliefs with the highest impact)
constrain the global set of possible worlds B . Certain beliefs of type 1materialized in
the set TF1 (B)can then give additional constraints for specific time points t, such that
only subsets B (t)  B need to be considered as possible worlds for time t. The sets
T0 (B) and T1 (B) together provide satisfiability conditions that are independent of any
specific probability assignments. Then, only beliefs of type 2 need to be considered as
probabilistic constraints to check whether B can be satisfied with respect to T , i.e., the
satisfiability problem sat(B, T ) from the previous section can be reduced to sat(T2 (B), T ),
if unsatisfiability of B has not yet been shown through constraints in T0 (B) and T1 (B).
Since the prior constraints define necessary conditions for any potential thread, they give
rise to a definition of thread soundness with respect to a given set of belief formulae B:
Definition 4.4 (Thread soundness). Let B be a set of belief formulae, and let T0 (B) and
T1 (B) be the set of type 0 and type 1 belief formulae in this set, respectively. Then, a
thread T h is sound with respect to B (denoted snd(T h, B)) if it satisfies all belief formulae
from T0 (B) and T1 (B):
snd(T h, B)  B  (T0 (B)  T1 (B)) : T h |= B
91

(51)

fiMartiny & Moller

Accordingly, we use snd(T , B) to denote that all threads T h  T are sound.
Note that this definition only relies on strict prior beliefs and the soundness property can
therefore be verified for every thread individually, without having to consider other threads
or probability assignments. Thus, a simplified version of the model checking procedure from
Section 4.1 can be used to verify soundness. The intuition behind this property is that we
can verify it easily prior to checking sat(B, T ) and can therefore obtain a reduced version
of the satisfiability problem:
Theorem 4.13 (Reduced satisfiability checking). Let B be a set of belief formulae, let
T2 (B) be the set of type 2 beliefs in B according to (50), and let T be a set of sound
threads. Then, B is satisfiable with respect to T iff T2 (B) is satisfiable with respect to T :
sat(B, T )  snd(T , B)  sat(T2 (B), T )

(52)

Proof. This follows directly from Definition 4.4: snd(T , B) is defined so that it satisfies all
belief formulae in the sets T0 (B) and T1 (B). Consequently, these sets resemble tautologies
with respect to T and therefore do not have any impact on the satisfiability checking
properties. Thus, instead of checking B for satisfiability, it suffices to check the set (B \
T0 (B)) \ T1 (B), which is exactly the definition of T2 (B).
4.4.2 Constraining Possible Worlds at Individual Time Points
Using the classification of beliefs in B into the three different types, we can now continue
with constructing sets of possible worlds B (t) for every time point t   . The main goal of
this section is an identification of obvious pruning conditions for possible worlds at specific
time points. Since we are in the process of searching for a set of possible threads that
satisfies a set of belief formulae B, any constraints on the sets B (t) have the potential to
significantly reduce the later used search space. Thus, the results of this section highlight
possible optimizations for an implementation of a PDT Logic sat solver. Even if the following constraints are notor only partiallyapplied, the search for possible threads as
described in subsequent Section 4.5 can be carried out, yet with a potentially larger search
space.
Since the set of type 0 beliefs has to be satisfied in every admissible world, we can define
the global set of admissible worlds B as follows:
Definition 4.5 (Global set of admissible worlds). Let B be a set of belief formulae, with
the corresponding sets of belief objects FB and type 0 beliefs T0 (B). Then, the set of
admissible worlds B w.r.t. B is given as
n

o
1,1 pfr
B =   B FB : adm()  Bi,0
(r0 (F, G))  T0 (B) :  |= (F  G) .
(53)
Remark 4.4. This definition uses adm() to ensure that all worlds   B are admissible
as defined in the external Definition 3.5 (p. 48). Alternatively, we could use the existing
formalism to encode these admissibility conditions directly as strict prior beliefs in B:
1,1 pfr
1,1 pfr
Bi,0
(r0 (ObsG (l), l)) and G 0  G : Bi,0
(r0 (ObsG (l), ObsG 0 (l))) represent conditions 1
and 2 of Definition 3.5, respectively. However, since these conditions are independent of the
respective problem being modeled, we do not include them in the problem-specific belief
set B, but use them as external constraints.
92

fiPDT Logic

Example 4.6 (Trains continued). The global set of worlds B admissible with respect to
B from Example 4.1 (p. 72) can be automatically constructed from all combinations of
events from FB shown in Example 4.2 (p. 73), given that these combinations are admissible
with respect to Definition 3.5 and satisfy the type 0 beliefs in T0 (B) from Example 4.4
(p. 81). We refrain from enumerating all of these worlds explicitly and instead describe
which worlds are excluded from the Herbrand base B FB of FB : From FB it follows that the
only possible shared observation between A and B is the fact that a train is not punctual
(Obs{AB} (punct(train))). In every possible world where this observation occurs, admissibility conditions require that both agents A and B observe that the respective train is
not punctual and that the train is indeed not punctual. Furthermore, the beliefs in T0 (B)
require that there is a corresponding observation for A at every possible world where a
train is not punctual (which incidentally also enforces admissibility conditions for these
observations).
Next, we can build upon the set of globally admissible worlds B and use the set of
type 1 beliefs to further prune the set of admissible worlds B (t) at individual time points
t:
Definition 4.6 (Local sets of admissible worlds). Let B be a set of belief formulae with
the corresponding sets of admissible worlds B , TF1 (B) be the set of materialized strict
prior beliefs induced by T0 (B) and T1 (B), and  be a set of time points. Then, the set of
admissible worlds B (t) w.r.t. B at time t   is given as
n

o
1,1
F
B (t) =   B : Bi,0 (Ft )  T1 (B) :  |= F
.
(54)
Example 4.7 (Trains continued). To obtain the scenario from the original Example 3.2,
we assume tmax = 9. From the set TF1 (B) identified in Example 4.5, we can restrict the set
of worlds at time 1 to
n
o
B (1) =   B :  |= (at(T1 , CA )  on(A, T1 ))
For all other time points, there are no options for further restrictions, thus the respective
local sets B (t) of possible worlds for all time points t 6= 1 remain at B .
Using Definition 4.6, we can now formulate constraints for the set of sound threads T :
T h  T , t   : T h(t)  B (t).

(55)

Note that this constraint provides a necessary but not sufficient condition for thread
soundness. To illustrate this, consider Example 4.5 again: the set TF1 (B) requires that
{at(T1 , CA ), on(A, T1 )} holds at every possible world at time t = 1 and thus we can constrain B (1) as shown in Example 4.7, because any thread violating this constraint is
inherently unsound. On the other hand, a thread according to (55) may contain the
fact, say punct(T1 )  T h(1), whichaccording to B30 only yields a sound thread if
{at(T2 , CC ), on(A, T2 )}  T h(4) holds as well. Thus, (55) provides general constraints on
the set of threads with respect to beliefs from T0 (B) and TF1 (B), while additional beliefs
from T1 (B) can discard individual threads by catching any potential unsatisfiable interplay
of possible worlds at different time points.
93

fiMartiny & Moller

Of course, in general it is possible that the methods discussed so far result in special
cases: for one thing, it is possible that B induces a set T0 (B)  TF1 (B) of inconsistent
beliefs, i.e., it will contain beliefs that contradict each other. Then, B or B (t) for some
t will be empty. This precludes the creation of any set of threads T such that I(T ) |= B.
In this case, satisfiability checking can terminate immediately with a negative result. For
another, it is possible that the above simplification process will result in an empty set
T2 (B). In this case, there are no probabilistic constraints that could impact satisfiability of
B and thus it is unnecessary to search for a suitable probability assignment. In this case, it
needs to be checked whether any of the threads in compliance with (55) is sound according
to Definition 4.4. If such a thread can be found, satisfiability checking can terminate
immediately with a positive result, otherwise B is unsatisfiable. Verifying soundness of a
single thread can be done with a simplified version of the model checking procedure from
Section 4.1 and is therefore in PTIME (cf. Corollary 4.3). However, as the number threads
satisfying condition (55) can grow exponentially with the number of ground atoms and the
number of time points, the problem of finding a sound thread is more complex:
Theorem 4.14 (Complexity of finding a sound thread). Let B be a set of belief formulae
such that all included formulae are grounded. Deciding whether there exists a sound thread
with respect to B, as defined in Definition 4.4, is NP-complete.
Proof. According to Definition 4.4, a set is sound if it satisfies all formulae from the set
T0 (B)  T1 (B). By treating the belief objects atoms F at all time points t as individual
variables Ft , we can transform beliefs in facts and belief in rules from T0 (B)  T1 (B) into
a boolean sat problem as follows:7
1,1
Bi,0
(Ft )

 Ft

1,1 pfr
Bi,0
(rt (F, G)) 

tmax
^t
t=0

(Ft  Gt+t )

Accordingly, disjunctive belief formulae can then be expressed through transforming every
disjunct individually. This transformation requires at most tmax conjuncts for every belief
operator and can therefore be performed in linear time. Since the boolean sat problem is
known to be NP-complete (Cook, 1971), it follows that searching for a sound thread with
respect to B is in NP.
NP-hardness of this problem has already been shown in the proof of Theorem 4.11
(p. 84) and consequently it follows that searching for a sound thread with respect to B is
NP-complete.
It should be noted that this result analyzes the worst-case complexity of the problem,
but in practice finding a sound thread is usually not dominated by this worst case. In most
cases, a sound thread can be found easily by employing the principle of least effort: For
1,1 pfr
belief in temporal rules Bi,0
(rt (F, G)), choosing worlds  such that  |= F ensures that
consequences of this rule do not have to be evaluated at other time points. Accordingly, for
7. This transformation is only defined for temporal rules with point frequency functions pfr. If other
frequency functions are used, the transformation has to be adapted accordingly.

94

fiPDT Logic

disjunctive rules a disjunct should be selected such that no temporal rule is triggered by this
fact. Of course, this is only a heuristic that may not give a sound thread immediately for
every input B, but it represents a feasible approach for most problems. We will illustrate
this approach with an example subsequently.
In this work, we only consider ground formulae for PDT Logic. In general, the formalism
as introduced in Section 3 allows the treatment of non-ground formulae as well. However,
for non-ground formulae the complexity result from Theorem 4.14 does not hold, because
transformation into a boolean sat problem is then exponential in the number of possible
groundings. Finding a sound thread then requires the use of sophisticated grounding procedures, (e.g., Dal Palu, Dovier, Pontelli, & Rossi, 2009 and Faber, Leone, & Perri, 2012),
which is beyond the scope of this work.
Now that sets of possible worlds are identified for every time point t   , we can proceed
with creating sets of representative threads with respect to these constraints. The aim of
the following discussion is the successive generation of a set of representative threads T
such that sat(B, T ) can be decided.
4.5 Representative Threads
Using Definition 4.4 and constraint (55) gives rise to a potential definition of the set of
possible threads T by constructing all possible combinations of sound world sequences from
B (t) for all t   . However, this would still result in an unnecessarily large set of possible
threads. Instead of constructing all of these threads explicitly, we will heuristically create
representative threads that represent excerpts from the situations modeled by T2 (B). This
approach uses heuristics to successively expand the set of representative threads. As soon as
a suitable set of threads (i.e., a model for B) is found, the decision procedure can terminate
with a positive result. If a set of representative threads does not show satisfiability of
B, additional threads are created until either a positive satisfiability result is obtained
or all possible threads have been created. Consequently, the heuristic search for models
constitutes a complete decision procedure for PDT Logic.
For the following discussion, we assume that the set T2 (B) is nonempty, i.e., there are
additional constraints that need to be satisfied by the generated set of threads. Otherwise,
if the set T2 (B) was empty, satisfiability could already be determined by checking whether
a sound thread with respect to B exists, as discussed in the previous section and there
would be no need to generate any specific set of threads.
`,u
`0 ,u0
For all beliefs in facts Bi,t
0 (Ft ) from B, the dual belief in the negated fact Bi,t0 (Ft )
with `0 = 1  u and u0 = 1  ` (cf. Corollary 3.4, p. 59) has to be satisfied as well. For
`,u fr
beliefs in rules Bi,t
0 (rt (F, G)), satisfiability depends on the accumulated subjective posterior interpretations of all threads weighted with their respective frequencies. The goal of
`,u
the following procedure is to successively create threads for every belief in a fact Bi,t
0 (Ft )
in T2 (B), such that we obtain representatives for the set of threads that (i) satisfy the
respective fact Ft and for the set of threads that satisfy Ft , and (ii) exhibit varying fre`,u fr
quencies for all beliefs in temporal rules Bi,t
0 (rt (F, G))  T2 (B). Consequently, belief
formulae can be considered as splitting rules and their application to generate representative threads results in a procedure similar to tableau-based methods. However, beliefs in
temporal rules can induce splits both forward and backward in time and thusunlike con95

fiMartiny & Moller

ventional tableau-based methodsthe following procedure does not create a tree structure,
but instead a set of sequences that represent possible threads. A key difference between
the generation of representative threads and other logical sat solvers is that in PDT Logic
it is virtually impossible to discard any generated potential thread: the probabilistic nature of the semantics requires that not only threads are considered where a given formula
holds, but also threads where it does not. Thus, even threads violating the objects of given
belief formulae are usually required to show satisfiability of a corresponding set of belief
formulae B. The following discussion provides a general outline for a decision procedure
in PDT Logic if only a set of belief formulae B is given. An actual implementation of
these methods is possible, but to obtain feasible run times for practical problems, various
optimization techniques from research on logic reasoning implementations would need to be
implemented, which is beyond the scope of this work.
4.5.1 Generating Representative Threads
`,u
Since the existence of any non-strict belief in a fact Bi,t
0 (Ft ) requires the existence of at
least two threadsone, where the respective belief object is satisfied and one, where it is
not8 we start with creating two threads from hB (1), ..., B (tmax )i such that we obtain a
set T = hT h1 , T h2 i with T h1 |=  and T h2 |=  for all belief objects  = Ft contained
in the set T2 (B) to obtain a minimal set of set threads T such that all belief formulae
B  T2 (B) can potentially be satisfied. This set will then subsequently be expanded with
additional threads until either a suitable set of threads to show satisfiability of T2 (B) is
found, or until no more additional threads can be created.
To allow for a concise notation, in the following we adapt the frequency notation for all
belief objects and use (1  ) to denote that  is true, (0  ) to denote that  is false, and
generally (x  ) to denote that  holds with frequency x. Of course, values 0 < x < 1 can
only occur for belief objects that represent temporal rules. With this notation, we try to
create initial sound threads such that
^
T h1 |= (1  j ), and
(56)
j

T h2 |=

^
j

(0  j )

(57)

holds for the respective belief objects j of all belief formulae Bj  T2 (B).9
This initial set T = {T h1 , T h2 } is meant to represent the two extreme choices for possible
threads with respect to T2 (B) to provide a suitable starting point for the subsequently
employed search heuristic. In general, it is not necessarily possible to create such extreme
threads in compliance with (56) and (57) for every possible set of belief formulae T2 (B). For
`,u
`,u
instance, T2 (B) might contain conflicting beliefs in facts Bi,t
0 (Ft ) and Bi,t0 (Ft ). Obviously,
no single thread can satisfy both belief objects simultaneously, but it might still be possible
`,u
8. Technically, a non-strict belief Bi,t
0 () could be satisfied with a single thread T h such that T h |=  if
the beliefs quantification has an upper bound u = 1. This might give rise to further optimizations for
an actual implementation, but for the sake of simplicity, we do not consider this case explicitly.
`,u
`,u
0
00
9. This notation is slightly simplified: for disjunctive belief formulae Bj = Bi,t
0 (j )  Bi,t0 (j ), we use j
0
00
as an abbreviation for j  j .

96

fiPDT Logic

to create a set of threads such thattogether with a suitable probability assignmentboth
beliefs can be satisfied. Thus, (56) and (57) characterize the intended goal when creating
the initial threads T h1 , T h2 , but do not represent hard constraints on these threads.
To find suitable threads that match these constraints, we employ the principle of least
`,u
effort by adding as few facts as possible to each thread: For every belief in a fact Bi,t
0 (Ft ),
we add the explicit constraints F  T h1 (t) and F 6 T h2 (t), such that T h1 represents the
thread where all belief objects are true and T h2 represents the set where all belief objects are
`,u fr
false. For beliefs in rules Bi,t
0 (rt (F, G)) we add G  T h1 (t + t) (resp. F  T h1 (t  t))
whenever another constraint enforces F  T h1 (t) (resp. G  T h2 (t)). If no occurrence of
fr (F, G) is trivially satisfied with frequency
F respectively G is enforced in T h1 , a rule rt
1 (i.e., there are no occurrences where F is not followed by G in t steps) and no further
constraints need to be added. Analogously, for T h2 we need to ensure that F holds at
least once and that whenever F  T h2 (t) holds, G  T h2 (t + t) holds, as well. For
`,u
`,u
disjunctive belief formulae Bi,t
0 (1 )  Bi,t0 (2 ), we need to ensure that belief object 1 or
2 holds in thread T h1 , as described above, and that 1  2 holds in thread T h2 . If
possible, the respective belief object 1 or 2 for thread T h1 should be chosen such that no
additional beliefs are triggered (we say that a belief is triggered by a fact F , if the existence
of F enforces another constraint through a belief in a temporal rule or a disjunctive belief
formula). Nested belief formulae are treated as above with respect to their innermost belief
object. If some constraint cannot be applied because it is in conflict with previously added
constraints from T2 (B), it is simply skipped in this stage. As the creation of T h1 and T h2 is
only the initialization step for a heuristic search of possible set threads, skipped constraints
will still be considered later in subsequent expansions.
Whenever a constraint regarding a fact F is added to T h1 or T h2 , it is necessary to
check whether this triggers additional rules from set of type 1 beliefs T1 (B). If necessary,
resulting facts are added to the respective threads. This application works analogously
to the construction of the set TF1 (B) as described in Section 4.4.1. Finally, if all belief
formulae have been processed, we search for a sound thread with respect to the created
constraints. Usually, a sound thread can be found easily by choosing all facts that are
yet unconstrained in T h1 and T h2 such that they do not trigger any additional beliefs.
Especially, for possible worlds T h(t) that are unconstrained, we can choose T h(t) =  if B
does not contain any belief in rules with purely negative preconditions or disjunctive belief
formulae that are not satisfiable by . More generally, the principle of least effort should
be employed such that worlds  are selected so that no further belief formulae need to be
considered. Such a selection is impossible if and only if the addition of both F and F to
some world triggers additional beliefs. Then, the consequences of adding the respective fact
need to be evaluated, as well. The resulting set T = {T h1 , T h2 } then provides a minimal
set of representative threads that that can be used to check sat(T2 (B), T ).
In the following, we show how the principle of least effort can be used to obtain representative threads as efficiently as possible. The constraints used in the following example
provide the minimal number of constraints that need to be enforced to obtain representative threads for the desired threads T h1 and T h2 . For all worlds  without any specific
constraints, we simply use  = . One can easily verify that this indeed yields threads in
compliance with (56) and (57).

97

fiMartiny & Moller

Example 4.8 (Trains continued). We continue the train example with the sets of typed
belief formulae specified in Example 4.5 (p.91). In Example 4.7 (p. 93), it was shown that
the set of worlds at time 1 B (1) is restricted such that {at(T1 , CA ), on(A, T1 )}   for
every world   B (1). The set T2 (B) contains three non-strict belief formulae, namely

.81,.81 pfr
T2 (B) = {BA,0
r0 (at(T1 , CA ), punct(T1 )) ,
(B20 )

.81,.81 pfr
BA,0
r0 (at(T2 , CC ), punct(T2 )) ,
(B200 )

.93,.93 pfr
BA,0
r2 (Obs{A} (punct(train)), Obs{AB} (punct(train))) }
(B6 )
By evaluating these belief formulae, we obtain constraints on the possible worlds in
threads T h1 and T h2 . A visualization of the following steps is given in Figure 4.
Analysis of belief formula B20 results in the constraints punct(T1 )  T h1 (1) and
punct(T1 ) 6 T h2 (1). These facts in turn trigger rules B30 and B300 , respectively:

1,1
BA,0
r3pfr ( punct(T1 )  at(T1 , CA ), at(T2 , CC )  on(A, T2 )) and

1,1 pfr
BA,0
(r5 (punct(T1 )  at(T1 , CA ), at(T2 , CC )  on(A, T2 )) ,

(B30 )
(B300 )

resulting in the additional constraints {at(T2 , CC ), on(A, T2 )}  T h1 (4) and {at(T2 , CC ),
on(A, T2 )}  T h2 (6).
Application of belief formula B200 then yields the additional facts punct(T2 )  T h1 (4)
and punct(T2 ) 6 T h2 (6). Again, this triggers rules from T1 (B):

1,1
(B40 )
r2pfr ( punct(T2 )  at(T2 , CC ), at(T2 , CB )  on(A, T2 )) and
BA,0

1,1 pfr
(B400 )
(r3 (punct(T2 )  at(T2 , CC ), at(T2 , CB )  on(A, T2 )) ,
BA,0
resulting in the additional constraints T h1 (6) = at(T2 , CB ), on(A, T2 ) and T h2 (9) =
at(T2 , CB ), on(A, T2 ).
Note that belief formula

1,1
BA,0
r0pfr (punct(train)  at(train, city), Obs{A} (punct(train)))
(B5 )
from T0 (B) provides a global constraint on the set of possible worlds B such that
Obs{A} (punct(train)) holds in every world where punct(train) holds, and thus we obtain
for thread T h2 the additional facts Obs{A} (punct(T1 ))  T h2 (1) and Obs{A} (punct(T1 )) 
T h2 (6).
Finally, rule

.93,.93 pfr
BA,0
r2 (Obs{A} (punct(train)), Obs{AB} (punct(train)))
(B6 )

98

fiPDT Logic

00
T h2 B2
B5
0
T h1 B2

at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )

B300

B200
B5

B30

B200

at(T2 , CC ), on(A, T2 )
punct(T2 )



1

B40


4

at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CB ), on(A, T2 )

6

B400



at(T2 , CB ), on(A, T2 )

9

t

1

Figure 4: Visualization of the representative thread set generation for the train example.
Both threads start with the given facts at(T1 , CA ), on(A, T1 ). Applications of
formulae from T2 (B)such that T h1 contains positive belief objects and T h2
contains negative belief objectsare marked in blue, additional constraints from
T0 (B) and T1 (B) are marked in red.

does not change the created threads T h1 , T h2 : in T h1 the rules precondition is never
enforced to be satisfied and thus the resulting frequency is one, while the lack of any
observation in T h2 even though there are nonpunctual trainsensures that the resulting
frequency is zero.
When trying to solve the resulting problem sat(T2 (B), {T h1 , T h2 }), the non-strict belief
formulae yield the following constraints on T h1 :
B20 :
B200

:

B6 :

0.81  I(T h1 )  0.81

0.81  I(T h1 )  0.81

0.93  I(T h1 )  0.93

Clearly, these constraints cannot be satisfied simultaneously and therefore the set T =
{T h1 , T h2 } is insufficient to show satisfiability of T2 (B) (and therefore B).
If the created set of threads fails to show satisfiability of T2 (B), additional threads can
be created to continue searching for an expanded set T such that T2 (B) can be satisfied
with respect to T . Based on an existing thread T h, an additional thread T h0 can be created
by ensuring that one conjunct for T h1 or T h2 in (56) and (57) is not satisfied anymore,
i.e., from a given thread T h with existing constraints (xk  k ), a new thread T h0 can be
obtained through the substitution
^
^
T h |= (xj  j )  T h0 |=
(xj  j )  x0k  k , x0k 6= xk .
(58)
j

j6=k

Every such substitution of one conjunct with a new constraint provides a choice point to
direct the continuation of the search for a suitable set of threads. The constraint notation
in 58 is used to provide a formal characterization of choice points. In practice, a new thread
T h0 satisfying the above constraint can usually be created easily through the addition of new
or the modification of existing facts in T h as follows. To simplify the following discussion,
we assume that the expansion keeps a history of expansion steps and resulting consequences,
such that all effects of adding an additional F can be undone if the respective fact F is
changed for a newly created thread.
99

fiMartiny & Moller

Definition 4.7 (Principle of least effort (ple) expansion). Let T be a set of threads and let
T2 (B) be a set of type 2 belief formulae. A principle of least effort expansion creates an
expanded set T 0 = T  {T h0 } according to a single application of one of the following rules.
`,u
 For a (possibly negated) belief in an a fact Bi,t
0 (Ft )  T2 (B): If there exists a thread

T h  T such that F  T h(t) (resp. F 6 T h(t)) is not yet enforced, T h0 is created as
a duplication of T h with the additional constraint F  T h(t) (resp. F 
6 T h(t)).

`,u fr
 For a belief in a temporal rule Bi,t
0 (rt (F, G))  T2 (B): If there exists a thread

T h  T such that F  T h(t) but G  T h(t + t) (resp. G 
6 T h(t + t)) is not
0
yet enforced, T h is created as a duplication of T h with the additional constraint
G  T h(t + t) (resp. G 6 T h(t + t)).

`,u
`,u
 For a disjunctive belief formula B = (Bi,t
0 (1 )  Bi,t0 (2 )     )  T2 (B): If possible,
`,u
expansion is carried out with respect to one belief Bi,t
0 () as described in the two
previous steps.

 Nested beliefs are again treated with respect to their innermost belief object.
 If the new thread T h0 is created from T h through the addition of F  T h0 for some
fact F and time point t and F 6 T h was enforced in the original thread T h, the
consequences of adding F 6 T h are undone in the new thread T h0 .

Then, for the created thread T h0 , additional belief formulae from T1 (B) that are
triggered by this modification need to be evaluated to obtain a sound thread, as
described above for the creation of initial threads T h1 , T h2 .

The intuition behind this ple-expansion is to create additional threads that satisfy an
alternative set of belief objects  contained in the set T2 (B) with as little effort as possible.
In general, it is possible to add constraints on arbitrary facts at arbitrary time points and
then continue with a successive expansion based on this thread. However, this would result
in a rather aimless exploration of the exponential search space. Following the ple-expansion
instead helps to direct the search for a suitable model guided by the rules specified in
T2 (B). To illustrate this, consider Figure 4 from the previous example: Possible pleexpansions could for example result in an additional thread by altering the punctuality of
train T2 . Clearly, the resulting situations are intended in this model, as they were already
considered in the original thread specification (cf. Figure 1, p. 50). On the other hand, by
deviating from the ple-expansion, one could add additional factssay at(T1 , CA ), on(A, T1 )
at arbitrary time points t > 1. This could then give rise to multiple subsequent expansions
of the resulting thread and may actually serve to generate a model for B, while such a
situation was not intended by the specification of B. The example about train punctuality
also illustrates the requirement of an undo operation: The fact punct(T2 )  T h1 (4) produced
the additional constraint {at(T2 , CB ), on(A, T2 )} at time t = 6. Clearly, this constraint
should not be enforced any longer ifbased on T h1 a new thread T h0 is created such that
punct(T2 ) 6 T h0 (4).
With information about violated constrains from the linear program corresponding to
sat(T2 (B), {T h1 , T h2 }), we can perform a dependency-directed selection of choice points:
100

fiPDT Logic

`,u
If the lower bound of a belief Bi,t
0 (k ) cannot be satisfied with the current set of threads,
0
an additional thread T h can be created with the existing constraints on T h1 or T h2 and
substituting the respective constraint on k , as shown in (58).
The dependency of choice points on violated lower bounds can best be illustrated through
the results from the previous example: Clearly, the upper bounds induced by B20 and B200
and the lower bound induced by B6 hinder satisfiability of T2 (B) with respect to the created
threads. Using the belief object of formula B20 (or B200 ) to create an additional thread T h3
yields the updated constraint

B20 :

0.81  I(T h1 ) + x  I(T h3 )  0.81

with a factor x depending on the frequency of the respective belief object in T h3 , while the
constraint induced by B6 remains unchanged. As a result, the new constraint only allows
for lower values of I(T h1 ), and thus the lower bound induced by B6 remains unsatisfiable.
Using the belief object of formula B6 to create an additional thread instead yields the
constraint
B6 :

0.93  I(T h1 ) + x  I(T h3 )  0.93,

whichthrough nonzero values for x and I(T h3 )potentially allows for lower values on
I(T h1 ). Note that this example only uses atomic belief formulae. For disjunctive belief
`,u
`,u
formulae B = (Bi,t
0 (1 )Bi,t0 (2 )   ), any of the respective belief objects with a violated
lower bound can be used to direct the selection of subsequent choice points (given that no
other disjunct of B is satisfiable, of course).
Combining information about violated lower bounds with the principle of least effort
provides a multi-stage heuristic to proceed with a dependency-directed selection of choice
points:
Definition 4.8 (Dependency-directed search heuristic). Let T2 (B) be a set of type 2 belief
formulae and let T be a set of threads such that sat(T2 (B), T ) holds. Then, to enable a
dependency-directed search for an expanded set T 0  T such that sat(T2 (B), T 0 ) holds, T
is expanded with an additional thread T h0 6 T according to the following rules.
1. If the existing set of threads T fails to satisfy lower bounds of constraints induced by
a belief formula B with belief object  and an additional thread T h0 can be obtained
through one ple-expansion with respect to , T is expanded to T 0 = T  {T h0 }.
2. Otherwise, if no dependency-directed ple-expansion is possible, another ple-expansion
is applied to T , if possible.
3. Finally, if no ple-expansion is possible in T , an additional thread T h0 can be created
by adding the constraint F  T h(t) (resp. F 6 T h(t)) for arbitrary facts F that are
not yet constrained in T h(t).
The intuition behind this heuristic is that information about violated probabilistic constraints should be used to select a suitable next expansion step, if possible. Otherwise,
other possible ple-expansion steps should be performed to use rules from T2 (B) to guide the
101

fiMartiny & Moller

search. Only if no further ple-expansions are possible, additional constraints should be employed to continue the search. Restricting possible expansions with respect to criterion 1 to
one step follows the principle of least effort, again: To illustrate this, consider Example 4.8:
It was shown that the created set of threads {T h1 , T h2 } fails to satisfy the lower bound of
belief formula B6 . In thread T h1 , there is no world T h1 (t) |= Obs{A} (punct(train)) such
that the precondition of the rule in B6 is satisfied. Consequently, there is no single step
ple-expansion of T h1 that could change the constraints induced by B6 . On the other hand,
T h2 provides two such choice points and should therefore be preferred for expansion. Note
that the soundness requirement will determine choices for all unconstrained facts. Thus,
in general the proposed expansion may produce threads that are already contained in T
by constraining facts that have been determined before. We will not consider this scenario
explicitly but instead assume that in such cases, further expansion steps are performed until
an additional thread is created.
4.5.2 A Thread Generation Example
To illustrate the expansion of a set of threads T with respect to the dependency-directed
search heuristic from Definition 4.8, in the following we resume the train example.
Example 4.9 (Trains continued). In the previous example, a set of threads T = {T h1 , T h2 }
has been created that fails to show satisfiability of T2 (B). Consequently, the heuristic from
Definition 4.8 should be used to iteratively expand this set until an expanded set of threads
T 0 is created such that a model for B is obtained or no further expansions of T 0 are possible.
Belief formula

.93,.93 pfr
r2 (Obs{A} (punct(train)), Obs{AB} (punct(train)))
B6 = BA,0
has already been identified as a belief formula which yields constrains with an unsatisfiable
lower bound and this should therefore be used to guide the subsequent expansion. As
already discussed before, no single-step ple-expansion of T h1 is possible to influence the
constraints induced by B6 . Therefore we continue with an expansion based on thread T h2 .
A visualization of the following steps is given in Figure 5.
There are two worlds in T h2 where Obs{A} (punct(train)) is satisfied, namely
Obs{A} (punct(T1 ))  T h2 (1) and Obs{A} (punct(T2 ))  T h2 (6). Both of these occurrences allow for an ple-expansion. We choose T h2 (1) to perform the expansion. This yields
a new thread T h3 with the additional constraint Obs{A,B} (punct(T1 ))  T h3 (3), while all
constraints from T h2 remain intact, since there are no constraints that need to be undone
by adding Obs{A,B}  T h3 (3).
The expanded set T 0 = T  {T h3 } can then be used to check sat(T2 (B), T 0 ). In thread
T h3 , the rule contained in B6 is satisfied in one of two occurrences of Obs{A} (punct(train))
and therefore yields a frequency of 0.5. Consequently, through transformation into a linear
program we obtain the constraints
B20 :
B200

:

B6 :

0.81 

I(T h1 )

 0.81

0.93 

I(T h1 ) + 0.5  I(T h3 )

 0.93

0.81 

I(T h1 )

102

 0.81

fiPDT Logic

T h4

T h3

00
T h2 B2
B5
0
T h1 B2

at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
1

Obs{A,B} (punct(T1 ))

Obs{A,B} (punct(T1 ))
B6
B300

B200
B5

B30



at(T2 , CC ), on(A, T2 )
punct(T2 )

B200
3

B40

4
1

Obs{A,B} (punct(T2 ))

at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CB ), on(A, T2 )



6

B6

at(T2 , CB ), on(A, T2 )

B400



8

9

t

Figure 5: Visualization of ple-expansions for the train example. Applications of formulae
from T2 (B) are marked in blue, additional constraints from T0 (B) and T1 (B)
are marked in red. Expansion steps are marked in green.

Apparently, B6 allows for lower values of I(T h1 ) for the this set T 0 . From the constraints
induced by B20 (resp. B200 ) we still obtain I(T h1 ) = 0.81. Then, the constraint induced
by B6 requires I(T h3 ) = 0.24 (since 0.81 + 0.5  0.24 = 0.93). This is still no suitable
probability assignment since the sum over all priors exceeds one. Consequently, the thread
set expansion continues. The above constraints show thataccording to condition 1 of the
search heuristicthread T h3 is now a suitable candidate for further expansion with respect
to the belief object of B6 .
Thus, based on T h3 , we create an additional thread T h4 through ple-expansion. In this
case, the only possible expansion step is Obs{A,B}  T h4 (8), which results in a frequency of
one for the rule contained in B6 . Thus, testing sat(Tk (B), T 0 ) with the further expanded
set T 0 yields the following constraints:
B20 :
B200

:

B6 :

0.81 

I(T h1 )

 0.81

0.93 

I(T h1 ) + 0.5  I(T h3 ) + 1  I(T h4 )

 0.93

0.81 

I(T h1 )

 0.81

These constraints are now satisfiable, for instance with

I(T 0 ) = 0.81, 0.07, 0, 0.12 .
Thus, sat(T2 (B), T 0 ) returns a positive result and satisfiability checking of B can terminate
with this result.
This result concludes satisfiability testing of the set of belief formulae B originally
specified in Example 4.1 (p. 72). Nevertheless, for illustration purposes we show the result
of further applications of ple-expansion steps in Figure 6. Changes in the additionally
created threads are obtained through a further respectively different application of a belief
formula from T2 (B), marked in blue in the respective threads. Worlds T h(t) that remain
unconstrained after a saturated application of ple-expansions are marked with /. All of
these worlds then give rise to further expansions according to step 3 of the search heuristic.
103

fi104

0
T h1 B2

00
T h2 B2
B5

T h3

T h4

T h5

T h6

T h7

T h8

T h9

1

at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )

at(T1 , CA ), on(A, T1 )
punct(T1 )

at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )
Obs{A} (punct(T1 ))
at(T1 , CA ), on(A, T1 )
punct(T1 )

B30

2

/

/

/

/

/

/

B200
B5

B6

3

/

/

B300

B200

Obs{A,B} (punct(T1 ))

Obs{A,B} (punct(T2 ))

/

/

/

/

Obs{A,B} (punct(T2 ))
B6

/

4

B200

00
/ B2
B5

/

1

5

at(T2 , CC ), on(A, T2 )
B40
punct(T2 )
/

/

/

/

/

at(T2 , CC ), on(A, T2 )
punct(T2 )
/
Obs{A} (punct(T2 ))
at(T2 , CC ), on(A, T2 )
punct(T2 )
/
Obs{A} (punct(T2 ))

/

/

/

6

at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CC ), on(A, T2 )
punct(T2 )
Obs{A} (punct(T2 ))
at(T2 , CB ), on(A, T2 )

B400

B6

Obs{A,B} (punct(T2 ))

at(T2 , CC ), on(A, T2 )
punct(T2 )

at(T2 , CC ), on(A, T2 )
punct(T2 )

/

7

/

/

/

/

/

B400

8

/

/

/

Obs{A,B} (punct(T2 ))
B6

9

/
t

at(T2 , CB ), on(A, T2 )

at(T2 , CB ), on(A, T2 )

at(T2 , CB ), on(A, T2 )

at(T2 , CB ), on(A, T2 )

Obs{A,B} (punct(T2 ))
B6

/

/

/

/

/

at(T2 , CB ), on(A, T2 )

/

at(T2 , CB ), on(A, T2 )

at(T2 , CB ), on(A, T2 )

at(T2 , CB ), on(A, T2 )

B40

/

Martiny & Moller

Figure 6: Visualization of continued ple-expansions for the train example. Applications of
formulae from T2 (B) are marked in blue, additional constraints from T0 (B) and
T1 (B) are marked in red. Expansion steps originating from T h1 and T h2 are
marked in green and orange, respectively. Unconstrained worlds are marked with
/.

fiPDT Logic

Some comments on the resulting set of threads from this example are necessary. Comparing the final threads depicted in Figure 6 with the original set of threads introduced in
Figure 1 shows that the expansion result largely corresponds to the original specification
(except differing thread labels). There are some notable differences however.
 First of all, there is the additional predicate punct(train), which was introduced in
Example 4.1 (p. 72) to allow for a concise specification of the background knowledge.
As the concept of nonpunctual trains (and especially the respective ramifications)
are implicitly encoded in Figure 1 as well, this does not change the properties of the
modeled example.
 With the explicit representation of train punctuality, observations of nonpunctual
trains can be expressed explicitly in this example, while the previous example uses
the ramifications of nonpunctual trains to model observations. Since rules B3 and
B4 assert that ramifications of punctual respectively nonpunctual trains are common
knowledge among Alice and Bob, both modeling alternatives preserve the intended
meaning of the example.
 Another difference is the timing of Alices observations. In the original example we
assumed that such an observation occurs at the time point when a train was supposed
to arrive at the destination city. In the current example we assume that Alice already
observes that a train is not punctual when leaving the departure city. The reason
for this change is solely for illustration purposes: specifying in rule B5 that Alice
immediately observes a nonpunctual train yields a type 0 belief and thus serves to
illustrate how additional facts can be obtained through global constraints. Since rule
B6 ensures that potential calls to Bob (i.e., shared observations) occur two time points
after Alices original observation, the intended model of the original example is still
maintained.
 The above points are only concerned with specific details of the modeled domain.
Comparing the set threads from Figure 1 with the threads from Figure 6 also shows a
more general modeling problem: for instance, analyzing the worlds at time point 2 in
Figure 6 shows that Alice is not (necessarily) on train T1 , while she is on this train both
at the previous time point and later time points. Naturally, one should expect that
Alice is on the train at all intermediate time points between boarding and exiting the
train. This is an instance of the frame problem (e.g., Reiter, 2001) that occurs when
specifying dynamic systems through logic formulae. Generally, the frame problem is
concerned with finding a suitable set of axioms to describe adequate evolutions of the
world. From a modeling perspective, evolutions where Alice vanishes and reappears
while on a train ride are obviously no adequate evolutions of the world. An application
of the final step of the search heuristic could then yield a tremendous blow-up of the
considered set of threads. For the modeled problem, this would clearly result in
unintended models, but the resulting models could still serve to show satisfiability of
the respective set of belief formulae B, even though this result might not be desired.
This problem could be fixed by adding successor state axioms in the style of Reiter,
e.g., specifying that if Alice is on a train, she remains there for the next time point
unless she explicitly exits the train.
105

fiMartiny & Moller

4.5.3 Properties of the Representative Thread Generation
In this section, we provide results to connect the set of representative threads to the satisfiability problem of PDT Logic and discuss the complexity of generating representative
threads.
Theorem 4.15 (PDT Logic Decision Procedure). Let B be a set of PDT Logic belief
formulae, and let T = {T h1 , T h2 } be the initial set of threads with length tmax obtained
from B according to Equations (56) and (57). Iteratively expanding this set according to
the search heuristic from Definition 4.8 and testing sat(T2 (B), T 0 ) for the expanded sets T 0
until (i) sat(T2 (B), T 0 ) returns a positive result, or (ii) T 0 is fully expanded with respect to
the search heuristic yields a sound and complete decision procedure for sat(B, tmax ).
Proof. Both the initial set of threads T = {T h1 , T h2 } and the expanded sets T 0 obtained through ple-expansion steps are defined such that only sound threads according
to Definition 4.4 are considered. Theorem 4.13 (p. 92) states that the decision problem
sat(T2 (B), T ) is equivalent to sat(B, T ) if the set T contains only threads that are sound
with respect to B. A positive result for sat(B, T ) for threads with length tmax shows
that T is a model for B and thus sat(B, tmax ) follows. Consequently, a positive result for
sat(T2 (B), T 0 ) always proofs that B is satisfiable for tmax time points.
On the other hand, if no model for B has been found and it is not possible to create
additional threads according to the search heuristic from Definition 4.8 (p. 101), the search
space is fully explored. From this it follows that no model for B with tmax time points
exists and therefore B is unsatisfiable for tmax time points. Consequently, it follows that
the PDT Logic decision procedure is sound.
With these properties, the completeness result is straightforward: For any arbitrary
input B and tmax , either a model can be found or non-existence of such a model can be
proven through a full exploration of the search space, and thus completeness of the procedure
follows.
In the following, we analyze the complexity of generating representative threads for a
set of belief formulae B.
Theorem 4.16 (Complexity of representative thread generation). Let B be a set of belief
formulae. Creating a set of expanded representative threads T 0 for B is in EXPSPACE.
Proof. The maximum number of possible threads for a given set of belief formulae B is
determined through the size |FB | and the maximum time point tmax . Recall from Equation (29) (p. 71) that we use FB to identify all event formulae from B and use this as the set
of ground atoms to construct possible worlds. Since every PDT Logic formula contains at
most two event formulae, we obtain the constraint |FB |  2  |B|. The largest set of possible
threads is then obtained as the sequences of combinations of all possible worlds over all
time points, yielding 22tmax |B| possible threads. In the worst case, all |FB |  2  |B| representative threads are created before obtaining a satisfiability result. Consequently, creating
all possible representative threads is in the complexity class DSPACE(2p(n) ), which is the
class EXPSPACE.
From this theorem, we immediately obtain complexity results for the satisfiability problem sat(B, tmax ).
106

fiPDT Logic

Corollary 4.17 (Complexity of PDT SAT without a given set of threads). Checking satisfiability of a set of PDT Logic belief formulae B without a specification of possible threads
is in EXPSPACE.
Proof. The generation of representative threads is in EXPSPACE, as shown in Theorem 4.16. For a given set of threads Theorem 4.11 shows that satisfiability checking in
PDT Logic is in NP. Thus, this does not further increase complexity of the PDT sat problem without a given set of threads and it follows that this problem is in EXPSPACE.
Some comments on these results are necessary. Since the decision procedure outlined
in Theorem 4.15 yields an exponential expansion of possible threads T 0 which all need
to be fed into the decision problem sat(T2 (B), T 0 )the exponential space requirement is
evident. However, as we have illustrated with the example, positive satisfiability results can
possibly be already obtained through small sets of possible threads T 0 with a diminutive
size compared to the entire search space. Moreover, the discussion of the train example has
shown that a major part of the search space stems from insufficient rule specifications. This
is not a specific problem of our formalism nor the presented decision procedure, but a general
problem of rule-based modeling approaches, namely the aforementioned frame problem. An
incomplete model specification then leads to the generation of unintended models, which
serve to show satisfiability of the modeled problem, but have not been intended by the
respective modeler. This could lead to the worst caseboth from a complexity and from
a model perspectivethat after an exponential execution of the decision procedure, the
result only shows that the input specification does not specify the intended model. The
problem can be addressed on the modeling side by providing additional axioms to ensure
that no unintended model is generated. However, this leads to a significant increase in the
specification size and it is difficult to ensure through rule specifications that indeed every
unintended model is prevented.
The ple-expansion steps could be used as a heuristic to discriminate between intended
and unintended models: As shown in the train example, only applying ple-expansion steps
results in a relatively small set of threads, which indeed corresponds to the intention of
the model, while any further expansions inherently leads to an exponential growth of the
set of threads and introduces only additional unintended models. Thus, omitting the final
step of the search heuristic would give a significantly faster termination of the decision
procedure, even though the resulting procedure cannot prove unsatisfiable sets of formulae
any longer. However, one could use the expansion procedure to create the set of intended
threads first andpossibly after an inspection by the modelercontinue to use this set to
perform satisfiability checks with respect to the intended model.
The runtime of the expansion procedure and resulting satisfiability checks is clearly
tilted towards the positive side: If a set of belief formulae is satisfiable, there is a good
chance that satisfiability can be shown in a small number of steps. Negative results on
the other hand can only be obtained after an exhaustive exploration of the search space.
However, for many applications negative satisfiability results are required. For instance,
checking entailment B |= B can be checked through the reformulation sat(B  B).
For applications relying on such a reformulation, the presented procedure is unfavorable
because positive entailment results can never be obtained efficiently. One could overcome
this problem as sketched above by generating a set of intended threads first and then use
107

fiMartiny & Moller

this set to perform subsequent satisfiability testsonce a set of threads is given, the decision
problems complexity significantly decreases, as shown in Section 4.3.

5. Conclusion
In this work, by extending APT Logic to dynamic scenarios with multiple agents, we have
developed a general framework to represent and reason about the belief change in multiagent systems. Next to lifting the single-agent case of APT Logic to multiple agents, we
have also provided a suitable semantics to the temporal evolution of beliefs. The resulting
framework extends previous work on dynamic multi-agent epistemic logics by enabling the
quantification of agents beliefs through probability intervals. An explicit notion of temporal relationships is provided through temporal rules building on the concept of frequency
functions.
The quantification of beliefs with probability intervals instead of precise values has
the advantage that when domain experts model a problem, they can not only provide
background knowledge about the problem domain, but can also specify their certainty in
the respective specifications. Narrow interval quantifications reflect a high certainty and
vice versa. This can be a significant advantage compared to other probabilistic approaches,
because in most approaches, sharp probability values are required which a human can
usually not express with precise values and thus has to rely on guesses. Specifying precise
values, when these are actually not precisely known can yield misleading results. PDT Logic
is not exposed to this problem, because it is not required to guess sharp values to specify a
problem.
We have shown that there are two alternative ways of specifying problems in PDT Logic,
either through explicit enumerations of possible threads or through a set of appropriate
rules. Both approaches exhibit their specific advantages and drawbacks: For many problem
domains, requiring an exhaustive enumeration of all possible threads poses a severe obstacle
for modeling the respective scenarios, as the combinatorial blow-up renders the specification
practically unmanageable. On the other hand, there are problem domains (e.g., attack
graphs in cyber security scenarios) that come with such an explicit specification anyways.
For these types of problems, we have shown that it is possible to check satisfiability of these
models very efficiently.
To overcome modeling disadvantages of the thread-based approach, we have also shown
how a problem domain can be solely specified through a set of PDT Logic belief formulae.
For most problem domains, this is a more natural way of specifying the problem. Also, this
provides means to easily adapt many existing problemsthat are specified in other formal
languages as sets of rulesto PDT Logic. On the other hand, waiving the requirement
of an exhaustive thread specification and according probabilities extremely increases the
problem complexity of checking satisfiability of a set of PDT Logic formulae. Nevertheless,
even when only imprecise probabilities are given, the resulting problem remains decidable
and the increased complexity might be curtailed through search heuristics.
Combinations of both approaches are possible as well: If an exhaustive specification
of possible threads is given, but probability intervals are only specified through beliefs
with imprecise probabilities, the satisfiability problem can be transformed into a 0-1 mixed
integer linear program. As there are a variety of efficient solvers available for this class of
108

fiPDT Logic

problems, this transformation provides a means to exploit existing optimizations to check
satisfiability of PDT Logic formulae.

References
Aumann, R. J. (1976). Agreeing to Disagree. The Annals of Statistics, 4 (6), 12361239.
Baaz, M., Egly, U., Leitsch, A., Goubault-Larrecq, J., & Plaisted, D. (2001). Normal Form
Transformations. In Robinson, A., & Voronkov, A. (Eds.), Handbook of Automated
Reasoning, chap. 5, pp. 273  333. MIT Press.
Balas, E. (1985). Disjunctive Programming and a Hierarchy of Relaxations for Discrete
Optimization Problems. SIAM Journal on Algebraic Discrete Methods, 6 (3), 466
486.
Balas, E. (1998). Disjunctive Programming: Properties of the Convex Hull of Feasible
Points. Discrete Applied Mathematics, 89 (1), 344.
Balas, E., Ceria, S., & Cornuejols, G. (1993). A Lift-and-project Cutting Plane Algorithm
for Mixed 0-1 Programs. Mathematical Programming, 58 (3), 295324.
Balas, E., Ceria, S., & Cornuejols, G. (1996). Mixed 0-1 Programming by Lift-and-project
in a Branch-and-cut Framework. Management Science, 42 (9), 12291246.
Balas, E., & Perregaard, M. (2002). Lift-and-project for Mixed 0-1 Programming: Recent
Progress. Discrete Applied Mathematics, 123 (1), 129154.
Baltag, A., & Moss, L. S. (2004). Logics for Epistemic Programs. Synthese, 139 (2), 165224.
Baltag, A., Moss, L. S., & Solecki, S. (1998). The Logic of Public Announcements, Common
Knowledge, and Private Suspicions. In Proceedings of the Seventh Conference on
Theoretical Aspects of Rationality and Knowledge, TARK 98, pp. 4356.
Bertacco, L., Fischetti, M., & Lodi, A. (2007). A Feasibility Pump Heuristic for general
Mixed-Integer Problems. Discrete Optimization, 4 (1), 6376.
Bienstock, D. (1996). Computational Study of a Family of Mixed-Integer Quadratic Programming Problems. Mathematical Programming, 74 (2), 121140.
Bradley, S. (2015). Imprecise probabilities. In Zalta, E. N. (Ed.), The Stanford Encyclopedia
of Philosophy (Summer 2015 edition).
Computational Infrastructure For Operations Research (COIN-OR) Project, T. (2016).
CBC (Coin-or branch and cut) user guide. http://www.coin-or.org/Cbc/index.html.
accessed: 2016-04-15.
Cook, S. A. (1971). The Complexity of Theorem-proving Procedures. In Proceedings of the
Third Annual ACM Symposium on Theory of Computing, STOC 71.
Cripps, M. W., Ely, J. C., Mailath, G. J., & Samuelson, L. (2008). Common Learning.
Econometrica, 76 (4), 909933.
Dal Palu, A., Dovier, A., Pontelli, E., & Rossi, G. (2009). Gasp: Answer set programming
with lazy grounding. Fundamenta Informaticae - Advances in Computational Logic,
96 (3), 297322.
109

fiMartiny & Moller

de Carvalho Ferreira, N., Fisher, M., & van der Hoek, W. (2008). Specifying and Reasoning
about Uncertain Agents. International Journal of Approximate Reasoning, 49 (1),
3551.
Doder, D., Markovic, Z., Ognjanovic, Z., Perovic, A., & Raskovic, M. (2010). A Probabilistic Temporal Logic That Can Model Reasoning about Evidence. In Foundations of
Information and Knowledge Systems: 6th International Symposium, FoIKS 2010.
Ellsberg, D. (1961). Risk, Ambiguity, and the Savage Axioms. The Quarterly Journal of
Economics, 75 (4), 643669.
Faber, W., Leone, N., & Perri, S. (2012). The intelligent grounder of DLV. In Correct
Reasoning: Essays on Logic-Based AI in Honour of Vladimir Lifschitz. Springer.
Fagin, R., & Halpern, J. Y. (1994). Reasoning about Knowledge and Probability. Journal
of the ACM, 41 (2), 340367.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). A Logic for Reasoning about Probabilities.
Information and Computation, 87 (1), 78128.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning About Knowledge.
MIT Press.
Fischetti, M., Glover, F., & Lodi, A. (2005). The Feasibility Pump. Mathematical Programming, 104 (1), 91104.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability; A Guide to the Theory
of NP-Completeness. W. H. Freeman & Co.
Gerbrandy, J., & Groeneveld, W. (1997). Reasoning About Information Change. Journal
of Logic, Language and Information, 6 (2), 147169.
Gnu

Project, T. (2016).
GLPK: GNU Linear Programming
http://www.gnu.org/software/glpk/glpk.html. accessed: 2016-04-15.

Kit.

Grunwald, P. D., & Halpern, J. Y. (2003). Updating Probabilities. Journal of Artificial
Intelligence Research, 19 (1), 243278.
Gurobi Optimization, Inc. (2016).
Gurobi optimizer reference
http://www.gurobi.com/documentation/. accessed: 2016-04-15.

manual.

Halpern, J. Y., & Pucella, R. (2006). A Logic for Reasoning about Evidence. Journal of
Artificial Intelligence Research, 26 (1), 134.
Halpern, J. Y., Samet, D., & Segev, E. (2009). Defining Knowledge in Terms of Belief: The
Modal Logic Perspective. The Review of Symbolic Logic, 2 (3), 469487.
Harsanyi, J. C. (1967). Games with Incomplete Information Played by Bayesian Players.
Part I. The Basic Model. Management Science, 14 (3), 159182.
Harsanyi, J. C. (1968a). Games with Incomplete Information Played by Bayesian Players.
Part II. Bayesian Equilibrium Points. Management Science, 14 (5), 320324.
Harsanyi, J. C. (1968b). Games with Incomplete Information Played by Bayesian Players.
Part III. The Basic Probability Distribution of the Game. Management Science, 14 (7),
486502.
110

fiPDT Logic

Hintikka, J. (1962). Knowledge and Belief: An Introduction to the Logic of the Two Notions.
Cornell University Press.
ILOG,
I.
(2016).
CPLEX
Optimizer.
01.ibm.com/software/commerce/optimization/cplex-optimizer/.
04-15.

http://wwwaccessed: 2016-

Kooi, B. P. (2003). Probabilistic Dynamic Epistemic Logic. Journal of Logic, Language and
Information, 12 (4), 381408.
Kripke, S. A. (1963). Semantical Considerations on Modal Logic. Acta Philosophica Fennica,
16, 8394.
Lloyd, J. W. (1987). Foundations of Logic Programming, 2nd Edition. Springer.
Martiny, K., Motzek, A., & Moller, R. (2015). Formalizing Agents Beliefs for Cyber-Security
Defense Strategy Planning. In CISIS 2015 - Proceedings of the 8th International Conference on Computational Intelligence in Security for Information Systems, Burgos,
Spain, 15-17 June, 2015.
Milch, B., & Koller, D. (2000). Probabilistic Models for Agents Beliefs and Decisions. In
Proceedings of the Sixteenth Annual Conference on Uncertainty in Artificial Intelligence, UAI 00. Morgan Kaufmann Publishers Inc.
Murty, K. G. (1983). Linear Programming. John Wiley & Sons.
Parikh, R., & Ramanujam, R. (2003). A Knowledge Based Semantics of Messages. Journal
of Logic, Language and Information, 12 (4), 453467.
Plaza, J. (1989). Logics of public communications. In Proceedings of the Fourth International
Symposium on Methodologies for Intelligent Systems: Poster session program, ISMIS
89. Oak Ridge National Laboratory.
Plaza, J. (2007). Logics of Public Communications. Synthese, 158 (2), 165179.
Reiter, R. (2001). Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems. MIT Press.
Sack, J. (2008). Temporal Languages for Epistemic Programs. Journal of Logic, Language
and Information, 17 (2), 183216.
Sack, J. (2009). Extending Probabilistic Dynamic Epistemic Logic. Synthese, 169 (2), 241
257.
Schrijver, A. (1986). Theory of Linear and Integer Programming. John Wiley & Sons.
Shakarian, P., Parker, A., Simari, G., & Subrahmanian, V. S. (2011). Annotated Probabilistic Temporal Logic. ACM Transactions on Computational Logic, 12 (2), 14:114:44.
Shakarian, P., Simari, G. I., & Subrahmanian, V. S. (2012). Annotated Probabilistic Temporal Logic: Approximate Fixpoint Implementation. ACM Transactions on Computational Logic, 13 (2), 13:113:33.
Shoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations. Cambridge University Press.
van Benthem, J. (2003). Conditional Probability Meets Update Logic. Journal of Logic,
Language and Information, 12 (4), 409421.
111

fiMartiny & Moller

van Benthem, J., Gerbrandy, J., Hoshi, T., & Pacuit, E. (2009a). Merging Frameworks for
Interaction. Journal of Philosophical Logic, 38 (5), 491526.
van Benthem, J., Gerbrandy, J., & Kooi, B. (2009b). Dynamic Update with Probabilities.
Studia Logica, 93 (1), 6796.
van der Hoek, W. (1997). Some Considerations on the Logic PFD: A Logic combining
Modality and Probability. Journal of Applied Non-Classical Logics, 7 (3), 287307.
van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2007). Dynamic Epistemic Logic.
Springer.
van Eijck, J. (2014). Dynamic epistemic logics. In Johan van Benthem on Logical and
Informational Dynamics, chap. 7, pp. 175202. Springer.
van Eijck, J., & Schwarzentruber, F. (2014). Epistemic Probability Logic Simplified. In
Gore, R., Kooi, B. P., & Kurucz, A. (Eds.), Advances in Modal Logic 10, invited and
contributed papers from the tenth conference on Advances in Modal Logic,, AiML
14. College Publications.
vos Savant, M. (1990). Ask Marilyn. Parade Magazine, 16.
Williams, H. P. (2009). Logic and Integer Programming. Springer.

112

fi