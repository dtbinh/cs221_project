journal of artificial intelligence research                  

submitted        published      

paramils  an automatic algorithm configuration framework
frank hutter
holger h  hoos
kevin leyton brown

hutter   cs   ubc   ca
hoos   cs   ubc   ca
kevinlb   cs   ubc   ca

university of british columbia       main mall
vancouver  bc  v t z   canada

thomas stutzle

stuetzle   ulb   ac   be

universite libre de bruxelles  code  iridia
av  f  roosevelt    b      brussels  belgium

abstract
the identification of performance optimizing parameter settings is an important part of the development and application of algorithms  we describe an automatic framework for this algorithm
configuration problem  more formally  we provide methods for optimizing a target algorithms
performance on a given class of problem instances by varying a set of ordinal and or categorical parameters  we review a family of local search based algorithm configuration procedures and
present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations  we describe the results of a comprehensive experimental evaluation
of our methods  based on the configuration of prominent complete and incomplete algorithms for
sat  we also present what is  to our knowledge  the first published work on automatically configuring the c plex mixed integer programming solver  all the algorithms we considered had default
parameter settings that were manually identified with considerable effort  nevertheless  using our
automated algorithm configuration procedures  we achieved substantial and consistent performance
improvements 

   introduction
many high performance algorithms have parameters whose settings control important aspects of
their behaviour  this is particularly the case for heuristic procedures used for solving computationally hard problems   as an example  consider c plex  a commercial solver for mixed integer
programming problems   cplex version    has about    parameters that affect the solvers search
mechanism and can be configured by the user to improve performance  there are many acknowledgements in the literature that finding performance optimizing parameter configurations of heuristic algorithms often requires considerable effort  see  e g   gratch   chien        johnson       
diao  eskesen  froehlich  hellerstein  spainhower   surendra        birattari        adenso diaz
  laguna         in many cases  this tedious task is performed manually in an ad hoc way  automating this task is of high practical relevance in several contexts 
 development of complex algorithms setting the parameters of a heuristic algorithm is a
highly labour intensive task  and indeed can consume a large fraction of overall development
   our use of the term heuristic algorithm includes methods without provable performance guarantees as well as
methods that have such guarantees  but nevertheless make use of heuristic mechanisms  in the latter case  the use
of heuristic mechanisms often results in empirical performance far better than the bounds guaranteed by rigorous
theoretical analysis 
   http   www ilog com products cplex 
c
    
ai access foundation  all rights reserved 

fih utter   h oos   l eyton  b rown   s t utzle

time  the use of automated algorithm configuration methods can lead to significant time
savings and potentially achieve better results than manual  ad hoc methods 
 empirical studies  evaluations  and comparisons of algorithms a central question in comparing heuristic algorithms is whether one algorithm outperforms another because it is fundamentally superior  or because its developers more successfully optimized its parameters  johnson         automatic algorithm configuration methods can mitigate this problem of unfair
comparisons and thus facilitate more meaningful comparative studies 
 practical use of algorithms the ability of complex heuristic algorithms to solve large and
hard problem instances often depends critically on the use of suitable parameter settings 
end users often have little or no knowledge about the impact of an algorithms parameter
settings on its performance  and thus simply use default settings  even if it has been carefully
optimized on a standard benchmark set  such a default configuration may not perform well on
the particular problem instances encountered by a user  automatic algorithm configuration
methods can be used to improve performance in a principled and convenient way 
a wide variety of strategies for automatic algorithm configuration have been explored in the literature  briefly  these include exhaustive enumeration  hill climbing  gratch   dejong         beam
search  minton         genetic algorithms  terashima marn  ross   valenzuela rendon        
experimental design approaches  coy  golden  runger   wasil         sequential parameter optimization  bartz beielstein         racing algorithms  birattari  stutzle  paquete   varrentrapp 
      birattari        balaprakash  birattari   stutzle         and combinations of fractional experimental design and local search  adenso diaz   laguna         we discuss this and other
related work more extensively in section    here  we note that while some other authors refer to the
optimization of an algorithms performance by setting its  typically few and numerical  parameters
as parameter tuning  we favour the term algorithm configuration  or simply  configuration   this is
motivated by the fact that we are interested in methods that can deal with a potentially large number
of parameters  each of which can be numerical  ordinal  e g   low  medium  or high  or categorical  e g   choice of heuristic   categorical parameters can be used to select and combine discrete
building blocks of an algorithm  e g   preprocessing and variable ordering heuristics   consequently 
our general view of algorithm configuration includes the automated construction of a heuristic algorithm from such building blocks  to the best of our knowledge  the methods discussed in this article
are yet the only general ones available for the configuration of algorithms with many categorical
parameters 
we now give an overview of what follows and highlight our main contributions  after formally stating the algorithm configuration problem in section    in section   we describe paramils
 first introduced by hutter  hoos   stutzle         a versatile stochastic local search approach for
automated algorithm configuration  and two of its instantiations  basicils and focusedils 
we then introduce adaptive capping of algorithm runs  a novel technique that can be used to
enhance search based algorithm configuration procedures independently of the underlying search
strategy  section     adaptive capping is based on the idea of avoiding unnecessary runs of the
algorithm to be configured by developing bounds on the performance measure to be optimized 
we present a trajectory preserving variant and a heuristic extension of this technique  after discussing experimental preliminaries in section    in section   we present empirical evidence showing that adaptive capping speeds up both basicils and focusedils  we also show that basicils
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

outperforms random search and a simple local search  as well as further evidence that focusedils
outperforms basicils 
we present extensive evidence that paramils can find substantially improved parameter configurations of complex and highly optimized algorithms  in particular  we apply our automatic
algorithm configuration procedures to the aforementioned commercial optimization tool c plex 
one of the most powerful  widely used and complex optimization algorithms we are aware of  as
stated in the c plex user manual  version       page       a great deal of algorithmic development effort has been devoted to establishing default ilog c plex parameter settings that achieve
good performance on a wide variety of mip models  we demonstrate consistent improvements
over this default parameter configuration for a wide range of practically relevant instance distributions  in some cases  we were able to achieve an average speedup of over an order of magnitude
on previously unseen test instances  section     we believe that these are the first results to be
published on automatically configuring c plex or any other piece of software of comparable complexity 
in section   we review a wide range of  separately published  paramils applications  specifically  we survey work that has considered the optimization of complete and incomplete heuristic
search algorithms for the problems of propositional satisfiability  sat   most probable explanation
 mpe   protein folding  university time tabling  and algorithm configuration itself  in three of these
cases  paramils was an integral part of the algorithm design process and allowed the exploration of
very large design spaces  this could not have been done effectively in a manual way or by any other
existing automated method  thus  automated algorithm configuration in general and paramils in
particular enables a new way of  semi  automatic design of algorithms from components 
section   presents related work and  finally  section    offers discussion and conclusions  here
we distill the common patterns that helped paramils to succeed in its various applications  we also
give advice to practitioners who would like to apply automated algorithm configuration in general
and paramils in particular  and identify promising avenues of research for future work 

   problem statement and notation
the algorithm configuration problem we consider in this work can be informally stated as follows 
given an algorithm  a set of parameters for the algorithm and a set of input data  find parameter
values under which the algorithm achieves the best possible performance on the input data 
to avoid potential confusion between algorithms whose performance is optimized and algorithms used for carrying out that optimization task  we refer to the former as target algorithms
and to the latter as configuration procedures  or simply configurators   this setup is illustrated in
figure    different algorithm configuration problems have also been considered in the literature  including setting parameters on a per instance basis and adapting the parameters while the algorithm
is running  we defer a discussion of these approaches to section   
in the following  we define the algorithm configuration problem more formally and introduce
notation that we will use throughout this article  let a denote an algorithm  and let p            pk be
parameters of a  denote the domain of possible values for each parameter pi as i   throughout
this work  we assume that all parameter domains are finite sets  this assumption can be met by
discretizing all numerical parameters to a finite number of values  furthermore  while parameters
   

fih utter   h oos   l eyton  b rown   s t utzle

figure    a configuration scenario includes an algorithm to be configured and a collection of problem instances  a configuration procedure executes the target algorithm with specified parameter settings
on some or all of the instances  receives information about the performance of these runs  and uses
this information to decide which subsequent parameter configurations to evaluate 

may be ordered  we do not exploit such ordering relations  thus  we effectively assume that all
parameters are finite and categorical  
our problem formulation allows us to express conditional parameter dependencies  for example 
one algorithm parameter might be used to select among search heuristics  with each heuristics
behaviour controlled by further parameters   in this case  the values of these further parameters
are irrelevant if the heuristic is not selected  paramils exploits this and effectively searches the
space of equivalence classes in parameter configuration space  in addition  our formulation supports
constraints on feasible combinations of parameter values  we use             k to denote
the space of all feasible parameter configurations  and a   denoting the instantiation of algorithm
a with parameter configuration    
let d denote a probability distribution over a space  of problem instances  and denote an element of  as   d may be given implicitly  as through a random instance generator or a distribution
over such generators  it is also possible  and indeed common  for  to consist of a finite sample of
instances  in this case  we define d as the uniform distribution over  
there are many ways of measuring an algorithms performance  for example  we might be interested in minimizing computational resources consumed by the given algorithm  such as runtime 
memory or communication bandwidth   or in maximizing the quality of the solution found  since
high performance algorithms for computationally challenging problems are often randomized  their
behaviour can vary significantly between multiple runs  thus  an algorithm will not always achieve
the same performance  even when run repeatedly with fixed parameters on a single problem instance  our overall goal must therefore be to choose parameter settings that minimize some cost
statistic of the algorithms performance across the input data  we denote this statistic as c    for
example  we might aim to minimize mean runtime or median solution cost 
with this intuition in mind  we now define the algorithm configuration problem formally 
definition    algorithm configuration problem   an instance of the algorithm configuration problem is a   tuple ha    d  max   o  mi  where 
 a is a parameterized algorithm 
  is the parameter configuration space of a 
   we are currently extending our algorithm configuration procedures to natively support other parameter types 

   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

 d is a distribution over problem instances with domain  
 max is a cutoff time  or captime   after which each run of a will be terminated if still running 
 o is a function that measures the observed cost of running a   on an instance    with
captime   r  examples are runtime for solving the instance  or cost of the solution found 
 m is a statistical population parameter  such as expectation  median  or variance  
any parameter configuration    is a candidate solution of the algorithm configuration
problem  for each configuration   o denotes the distribution of costs induced by function o 
applied to instances  drawn from distribution d and multiple independent runs for randomized
algorithms  using captime    max   the cost of a candidate solution  is defined as
c      m o   

   

the statistical population parameter m of the cost distribution o   an optimal solution      minimizes c   
   arg min c   
   


an algorithm configuration procedure is a procedure for solving the algorithm configuration
problem  unfortunately  at least for the algorithm configuration problems considered in this article  we cannot optimize c in closed form since we do not have access to an algebraic representation of the function  we denote the sequence of runs executed by a configurator as r  
          s        o              n   n   sn   n   on     the ith run is described by five values 
 i   denotes the parameter configuration being evaluated 
 i   denotes the instance on which the algorithm is run 
 si denotes the random number seed used in the run  we keep track of seeds to be able to block
on them  see section        
 i denotes the runs captime  and
 oi denotes the observed cost of the run
note that each of     s    and o can vary from one element of r to the next  regardless of whether
or not other elements are held constant  we denote the ith run of r as r i   and the subsequence
of runs using parameter configuration   i e   those runs with i     as r   the configuration
procedures considered in this article compute empirical estimates of c   based solely on r   but
in principle other methods could be used  we compute these cost estimates both online  during
runtime of a configurator  as well as offline  for evaluation purposes 
definition    cost estimate   given an algorithm configuration problem ha    d  max   o  mi 
we define a cost estimate of a cost c   based on a sequence of runs r             s        o            
 n   n   sn   n   on    as c   r     m  oi   i       where m is the sample statistic analogue to
the statistical population parameter m 
for example  when c   is the expected runtime over a distribution of instances and random number
seeds  c   r  is the sample mean runtime of runs r  
all configuration procedures in this paper are anytime algorithms  meaning that at all times they
keep track of the configuration currently believed to have the lowest cost  we refer to this configuration as the incumbent configuration  or in short the incumbent  inc   we evaluate a configurators
performance at time t by means of its incumbents training and test performance  defined as follows 
   

fih utter   h oos   l eyton  b rown   s t utzle

definition    training performance   when at some time t a configurator has performed a sequence of runs r             s        o              n   n   sn   n   on    to solve an algorithm configuration problem ha    d  max   o  mi  and has thereby found incumbent configuration inc   then its
training performance at time t is defined as the cost estimate c inc   r  
the set of instances              n   discussed above is called the training set  while the true cost
of a parameter configuration cannot be computed exactly  it can be estimated using training performance  however  the training performance of a configurator is a biased estimator of its incumbents
true cost  because the same instances are used for selecting the incumbent as for evaluating it  in
order to achieve unbiased estimates during offline evaluation  we set aside a fixed set of instances
              t     called the test set  and random number seeds  s             s t    both unknown to the
configurator  and use these for evaluation 
definition    test performance   at some time t  let a configurators incumbent for an algorithm
configuration problem ha    d  max   o  mi be inc  this is found by means of executing a sequence of runs on the training set   furthermore  let r      inc        s     max   o              inc   t   
s t   max   ot    be a sequence of runs on the t instances and random number seeds in the test set
 which is performed offline for evaluation purposes   then the configurators test performance at
time t is defined as the cost estimate c inc   r    
throughout this article  we aim to minimize expected runtime   see section       for a discussion
of that choice   thus  a configurators training performance is the mean runtime of the runs it
performed with the incumbent  its test performance is the mean runtime of the incumbent on the
test set  note that  while the configurator is free to use any i  max   test performance is always
computed using the maximal captime  max  
it is not obvious how an automatic algorithm configurator should choose runs in order to best
minimize c   within a given time budget  in particular  we have to make the following choices 
   which parameter configurations     should be evaluated 
   which problem instances     should be used for evaluating each         and how
many runs should be performed on each instance 
   which cutoff time i should be used for each run 
hutter  hoos and leyton brown        considered this design space in detail  focusing on the
tradeoff between the  fixed  number of problem instances to be used for the evaluation of each
parameter configuration and the  fixed  cutoff time used for each run  as well as the interaction of
these choices with the number of configurations that can be considered  in contrast  here  we study
adaptive approaches for selecting the number of problem instances  section      and the cutoff
time for the evaluation of a parameter configuration  section     we also study which configurations
should be selected  sections     and      

   paramils  iterated local search in parameter configuration space
in this section  we address the first and most important of the previously mentioned dimensions
of automated algorithm configuration  the search strategy  by describing an iterated local search
framework called paramils  to start with  we fix the other two dimensions  using an unvarying
benchmark set of instances and fixed cutoff times for the evaluation of each parameter configuration  thus  the stochastic optimization problem of algorithm configuration reduces to a simple
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

optimization problem  namely to find the parameter configuration that yields the lowest mean runtime on the given benchmark set  then  in section      we address the second question of how many
runs should be performed for each configuration 
    the paramils framework
consider the following manual parameter optimization process 
   begin with some initial parameter configuration 
   experiment with modifications to single parameter values  accepting new configurations whenever they result in improved performance 
   repeat step   until no single parameter change yields an improvement 
this widely used procedure corresponds to a manually executed local search in parameter configuration space  specifically  it corresponds to an iterative first improvement procedure with a search
space consisting of all possible configurations  an objective function that quantifies the performance
achieved by the target algorithm with a given configuration  and a neighbourhood relation based on
the modification of one single parameter value at a time  i e   a one exchange neighbourhood  
viewing this manual procedure as a local search algorithm is advantageous because it suggests
the automation of the procedure as well as its improvement by drawing on ideas from the stochastic
local search community  for example  note that the procedure stops as soon as it reaches a local optimum  a parameter configuration that cannot be improved by modifying a single parameter value  
a more sophisticated approach is to employ iterated local search  ils  lourenco  martin   stutzle 
      to search for performance optimizing parameter configurations  ils is a prominent stochastic
local search method that builds a chain of local optima by iterating through a main loop consisting of
    a solution perturbation to escape from local optima      a subsidiary local search procedure and
    an acceptance criterion to decide whether to keep or reject a newly obtained candidate solution 
paramils  given in pseudocode as algorithm    is an ils method that searches parameter configuration space  it uses a combination of default and random settings for initialization  employs
iterative first improvement as a subsidiary local search procedure  uses a fixed number  s  of random moves for perturbation  and always accepts better or equally good parameter configurations 
but re initializes the search at random with probability prestart    furthermore  it is based on a
one exchange neighbourhood  that is  we always consider changing only one parameter at a time 
paramils deals with conditional parameters by excluding all configurations from the neighbourhood of a configuration  that differ only in a conditional parameter that is not relevant in  
    the basicils algorithm
in order to turn paramils as specified in algorithm framework   into an executable configuration
procedure  it is necessary to instantiate the function better that determines which of two parameter settings should be preferred  we will ultimately propose several different ways of doing this 
here  we describe the simplest approach  which we call basicils  specifically  we use the term
basicils n   to refer to a paramils algorithm in which the function better         is implemented
as shown in procedure    simply comparing estimates cn of the cost statistics c     and c     that
are based on n runs each 
   our original parameter choices hr  s  prestart i   h           i  from hutter et al         were somewhat arbitrary 
though we expected performance to be quite robust with respect to these settings  we revisit this issue in section     

   

fih utter   h oos   l eyton  b rown   s t utzle

algorithm framework    paramils     r  prestart   s 
outline of iterated local search in parameter configuration space  the specific variants of paramils
we study  basicils n  and focusedils  are derived from this framework by instantiating procedure
better  which compares          basicils n  uses bettern  see procedure     while focusedils
uses betterf oc  see procedure     the neighbourhood nbh   of a configuration  is the set of all
configurations that differ from  in one parameter  excluding configurations differing in a conditional
parameter that is not relevant in  
input   initial configuration      algorithm parameters r  prestart   and s 
output   best parameter configuration  found 
  for i              r do
 
  random    
 
if better       then     
 
 
 

ils  iterativefirstimprovement      
while not terminationcriterion   do
  ils  

 

         perturbation
for i              s do   random     nbh   

 

         basic local search
  iterativefirstimprovement    

 
  

         acceptancecriterion
if better   ils   then ils   
with probability prestart do ils  random    

  

return overall best inc found 

  
  
  
  
  

procedure iterativefirstimprovement   
repeat
     
foreach      n bh      in randomized order do
if better            then         break 

  
  

until       
return  

basicils n   is a simple and intuitive approach since it evaluates every parameter configuration
by running it on the same n training benchmark instances using the same random number seeds 
like many other related approaches  see  e g   minton        coy et al         adenso diaz  
laguna         it deals with the stochastic part of the optimisation problem by using an estimate
based on a fixed training set of n instances  when benchmark instances are very heterogeneous or
procedure    bettern         
procedure used in basicils n   and randomsearch n   to compare two parameter configurations  procedure objective   n   returns the user defined objective achieved by a   on the first n instances
and keeps track of the incumbent solution  inc   it is detailed in procedure   on page     
input
  parameter configuration     parameter configuration  
output
  true if   does better than or equal to   on the first n instances  false otherwise
side effect   adds runs to the global caches of performed algorithm runs r  and r    potentially
updates the incumbent inc
  cn       objective     n  
  cn       objective     n  
  return cn       cn     

   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

when the user can identify a rather small representative subset of instances  this approach can find
good parameter configurations with low computational effort 
    focusedils  adaptively selecting the number of training instances
the question of how to choose the number of training instances  n   in basicils n   has no straightforward answer  optimizing performance using too small a training set leads to good training
performance  but poor generalization to previously unseen test benchmarks  on the other hand 
we clearly cannot evaluate every parameter configuration on an enormous training setif we did 
search progress would be unreasonably slow 
focusedils is a variant of paramils that deals with this problem by adaptively varying the
number of training samples considered from one parameter configuration to another  we denote
the number of runs available to estimate the cost statistic c   for a parameter configuration  by
n     having performed different numbers of runs using different parameter configurations  we
face the question of comparing two parameter configurations  and    for which n     n       
one option would be simply to compute the empirical cost statistic based on the available number
of runs for each configuration  however  this can lead to systematic biases if  for example  the first
instances are easier than the average instance  instead  we compare  and    based on n    runs
on the same instances and seeds  this amounts to a blocking strategy  which is a straight forward
adaptation of a known variance reduction technique  see     for a more detailed discussion 
this approach to comparison leads us to a concept of domination  we say that  dominates   
when at least as many runs have been conducted on  as on      and the performance of a   on the
first n       runs is at least as good as that of a      on all of its runs 
definition    domination     dominates   if and only if n       n      and cn           
cn           
now we are ready to discuss the comparison strategy encoded in procedure betterf oc          
which is used by the focusedils algorithm  see procedure     this procedure first acquires one
additional sample for the configuration i having smaller n  i    or one run for both configurations if
they have the same number of runs  then  it continues performing runs in this way until one configuration dominates the other  at this point it returns true if   dominates     and false otherwise  we
also keep track of the total number of configurations evaluated since the last improving step  i e  
since the last time betterf oc returned true   we denote this number as b  whenever betterf oc         
returns true  we perform b bonus runs for   and reset b to    this mechanism ensures that we
perform many runs with good configurations  and that the error made in every comparison of two
configurations   and   decreases on expectation 
it is not difficult to show that in the limit  focusedils will sample every parameter configuration
an unbounded number of times  the proof relies on the fact that  as an instantiation of paramils 
focusedils performs random restarts with positive probability 
lemma    unbounded number of evaluations   let n  j    denote the number of runs focusedils
has performed with parameter configuration  at the end of ils iteration j to estimate c    then 
for any constant k and configuration     with finite      limj p  n  j     k      
proof  after each ils iteration of paramils  with probability prestart     a new configuration is
picked uniformly at random  and with probability       this is configuration   the probability of
   

fih utter   h oos   l eyton  b rown   s t utzle

procedure    betterf oc         
procedure used in focusedils to compare two parameter configurations  procedure objective   n  
returns the user defined objective achieved by a   on the first n instances  keeps track of the incumbent solution  and updates r  a global cache of algorithm runs performed with parameter configuration    it is detailed in procedure   on page      for each   n      length r    b is a global
counter denoting the number of configurations evaluated since the last improvement step 
input
  parameter configuration     parameter configuration  
output
  true if   dominates     false otherwise
side effect  adds runs to the global caches of performed algorithm runs r  and r    updates the
global counter b of bonus runs  and potentially the incumbent inc
  b b  
  if n       n      then
 
min      max   
 
if n        n      then b  b    
else min      max   
repeat
i  n  min      
ci  max    objective max   i     if n  min     n  max    adds a new run to rmax  
ci  min    objective min   i     adds a new run to rmin  
   until dominates         or dominates        
   if dominates         then
 
 
 
 
 

         perform b bonus runs 
  
  
  

cn      b       objective     n        b     adds b new runs to r   
b  
return true

  

else return false

  
  
  

procedure dominates        
if n        n      then return false
return objective     n        objective     n      

visiting  in an ils iteration is thus p  prestart
     hence  the number of runs performed with  is
  
lower bounded by a binomial random
 variable b k  j  p   then  for any constant k   k we obtain
limj b k  j  p    limj jk pk     p jk      thus  limj p  n  j     k      
definition    consistent estimator   cn    is a consistent estimator for c   iff
       lim p   cn     c            
n 

when cn    is a consistent estimator of c    cost estimates become more and more reliable
as n approaches infinity  eventually eliminating overconfidence and the possibility of mistakes in
comparing two parameter configurations  this fact is captured in the following lemma 
lemma    no mistakes for n     let         be any two parameter configurations with
c       c      then  for consistent estimators cn   limn  p  cn       cn           
proof  write c  as shorthand for c      c  for c      c  for cn       and c  for cn       define
m        c    c    as the midpoint between c  and c    and    c   m   m  c      as
its distance from each of the two points  since cn is a consistent estimator for c  the estimate
c  comes arbitrarily close to the real cost c    that is  limn  p   c   c             since
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

 m  c        the estimate c  cannot be greater than or equal to m  limn  p  c   m      
similarly  limn  p  c    m       since
p  c   c      p  c   c   c   m    p  c   c   c    m 
  p  c   c   c   m    p  c   c   c    m  c    m 
 p  c   m    p  c    m  
we have limn  p  c   c     limn   p  c   m    p  c    m               
combining our two lemmata we can now show that in the limit  focusedils is guaranteed to
converge to the true best parameter configuration 
theorem    convergence of focusedils   when focusedils optimizes a cost statistic c based on
a consistent estimator cn   the probability that it finds the true optimal parameter configuration  
approaches one as the number of ils iterations goes to infinity 
proof  according to lemma    n    grows unboundedly for each     for each         as
n      and n      go to infinity  lemma   states that in a pairwise comparison  the truly better
configuration will be preferred  thus eventually  focusedils visits all finitely many parameter
configurations and prefers the best one over all others with probability arbitrarily close to one 
we note that in many practical scenarios cost estimators may not be consistentthat is  they
may fail to closely approximate the true performance of a given parameter configuration even for
a large number of runs of the target algorithm  for example  when a finite training set    is used
during configuration rather than a distribution over problem instances  d  then even for large n   cn
will only accurately reflect the cost of parameter configurations on the training set    for small
training sets    the cost estimate based on  may differ substantially from the true cost as defined
by performance across the entire distribution  d  the larger the training set    the smaller the
expected difference  it vanishes as training set size goes to infinity   thus  it is important to use
large training sets  which are representative of the distribution of interest  whenever possible 

   adaptive capping of algorithm runs
now we consider the last of our dimensions of automated algorithm configuration  the cutoff time
for each run of the target algorithm  we introduce an effective and simple capping technique that
adaptively determines the cutoff time for each run  the motivation for this capping technique comes
from a problem encountered by all configuration procedures considered in this article  often the
search for a performance optimizing parameter setting spends a lot of time with evaluating a parameter configuration that is much worse than other  previously seen configurations 
consider  for example  a case where parameter configuration   takes a total of    seconds to
solve n       instances  i e   it has a mean runtime of     seconds per instance   and another parameter configuration   takes     seconds to solve the first of these instances  in order to compare
the mean runtimes of   and   based on this set of instances  knowing all runtimes for     it is
not necessary to run   on all     instances  instead  we can already terminate the first run of  
after       seconds  this results in a lower bound on   s mean runtime of            since
the remaining    instances could take no less than zero time  this lower bound exceeds the mean
runtime of     and so we can already be certain that the comparison will favour     this insight
provides the basis for our adaptive capping technique 
   

fih utter   h oos   l eyton  b rown   s t utzle

    adaptive capping in basicils
in this section  we introduce adaptive capping for basicils  we first introduce a trajectory preserving
version of adaptive capping  tp capping  that provably does not change basicilss search trajectory and can lead to large computational savings  we then modify this strategy heuristically to
perform more aggressive adaptive capping  aggr capping   potentially yielding even better performance in practice 
      t rajectory  preserving c apping
observe that all comparisons between parameter configurations in paramils are pairwise  in
basicils n    these comparisons are based on procedure bettern           where   is either the
best configuration encountered in this ils iteration or the best configuration of the last ils iteration  without adaptive capping  these comparisons can take a long time  since a poor parameter
configuration  can easily take more than an order of magnitude longer than good configurations 
for the case of optimizing the mean of non negative cost functions  such as runtime or solution
cost   we implement a bounded evaluation of a parameter configuration  based on n runs and a
given performance bound in procedure objective  see procedure     this procedure sequentially
performs runs for parameter configuration  and after each run computes a lower bound on cn   
based on the i  n runs performed so far  specifically  for our objective of mean runtime we
sum the runtimes of each of the i runs  and divide this sum by n   since all runtimes must be
nonnegative  this quantity lower bounds cn     once the lower bound exceeds the bound passed
as an argument  we can skip the remaining runs for   in order to pass the appropriate bounds to
procedure objective  we need to slightly modify procedure bettern  see procedure   on page     
for adaptive capping  procedure objective now has a bound as an additional third argument  which
is set to  in line   of bettern   and to cn      in line   
because this approach results in the computation of exactly the same function bettern as used in
the original version of basicils  the modified procedure follows exactly the same search trajectory
it would have followed without capping  but typically requires much less runtime  hence  within
the same amount of overall running time  this new version of basicils tends to be able to search a
larger part of the parameter configuration space  although in this work we focus on the objective of
minimizing mean runtime for decision algorithms  we note that our adaptive capping approach can
be applied easily to other configuration objectives 
      aggressive c apping
as we demonstrate in section      the use of trajectory preserving adaptive capping can result in
substantial speedups of basicils  however  sometimes this approach is still less efficient than it
could be  this is because the upper bound on cumulative runtime used for capping is computed from
the best configuration encountered in the current ils iteration  where a new ils iteration begins
after each perturbation   as opposed to the overall incumbent  after a perturbation has resulted in
a new parameter configuration   the new iterations best configuration is initialized to   in the
frequent case that this new  performs poorly  the capping criterion does not apply as quickly as
when the comparison is performed against the overall incumbent 
to counteract this effect  we introduce a more aggressive capping strategy that can terminate
the evaluation of a poorly performing configuration at any time  in this heuristic extension of our
adaptive capping technique  we bound the evaluation of any parameter configuration by the per   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

procedure    objective   n  optional parameter bound 
procedure that computes cn     either by performing new runs or by exploiting previous cached runs 
an optional third parameter specifies a bound on the computation to be performed  when this parameter
is not specified  the bound is taken to be   for each   n    is the number of runs performed for  
i e   the length of the global array r   when computing runtimes  we count unsuccessful runs as   
times their cutoff time 
input
  parameter configuration   number of runs  n   optional bound bound
output
  cn    if cn     bound  otherwise a large constant  maxpossibleobjective  plus the
number of instances that remain unsolved when the bound was exceeded
side effect  adds runs to the global cache of performed algorithm runs  r   updates global
incumbent  inc
         maintain invariant  n  inc    n    for any 
 
 

if   
  inc and n  inc     n then
cn  inc    objective inc   n       adds n  n  inc   runs to rinc
         for aggressive capping  update bound 

 

if aggressive capping then bound  min bound  bm  cn  inc   
         update the run results in tuple r  

for i       n do
sum runtime  sum of runtimes in r              r  i        tuple indices starting at   
 i  max max   n  bound sum runtime 
if n     i then    i   i   oi    r  i 
if n     i and   i   i and oi   unsuccessful  or  i    i and oi    unsuccessful  
then o i  oi    previous run is longer yet unsuccessful or shorter yet successful  can re use result
 
else
  
o i  objective from a newly executed run of a   on instance i with seed si and captime i
 
 
 
 
 

  
  
  
  

r  i      i    i   o i  
if   n   sum runtime   o i     bound then return maxpossibleobjective    n       i
if n   n  inc   and  sum of runtimes in r      sum of runtimes in rinc   then inc  
return   n   sum of runtimes in r  

formance of the incumbent parameter configuration multiplied by a factor that we call the bound
multiplier  bm  when a comparison between any two parameter configurations  and    is performed and the evaluations of both are terminated preemptively  the configuration having solved
more instances within the allowed time is taken to be the better one   this behaviour is achieved
by line    in procedure objective  which keeps track of the number of instances solved when exceeding the bound   ties are broken to favour moving to a new parameter configuration instead of
staying with the current one 
depending on the bound multiplier  the use of this aggressive capping mechanism may change
the search trajectory of basicils  for bm    the heuristic method reduces to our trajectorypreserving method  while a very aggressive setting of bm     means that once we know a parameter
configuration to be worse than the incumbent  we stop its evaluation  in our experiments we set
bm      meaning that once the lower bound on the performance of a configuration exceeds twice
the performance of the incumbent solution  its evaluation is terminated   in section      we revisit
this choice of bm      configuring the parameters of paramils itself  
   

fih utter   h oos   l eyton  b rown   s t utzle

    adaptive capping in focusedils
the main difference between basicils and focusedils is that the latter adaptively varies the number of runs used to evaluate each parameter configuration  this difference complicates  but does
not prevent the use of adaptive capping  this is because focusedils always compares pairs of parameter configurations based on the same number of runs for each configuration  even though this
number can differ from one comparison to the next 
thus  we can extend adaptive capping to focusedils by using separate bounds for every number
of runs  n   recall that focusedils never moves from one configuration    to a neighbouring
configuration       without performing at least as many runs for    as have been performed for  
since we keep track of the performance of  with any number of runs m  n     a bound for the
evaluation of    is always available  therefore  we can implement both trajectory preserving and
aggressive capping as we did for basicils 
as for basicils  for focusedils the inner workings of adaptive capping are implemented in
procedure objective  see procedure     we only need to modify procedure betterf oc  see procedure
  on page      to call objective with the right bounds  this leads to the following changes in
procedure betterf oc   subprocedure dominates on line    now takes a bound as an additional
argument and passes it on to the two calls to objective in line     the two calls of dominates in
line    and the one call in line    all use the bound cmax   the three direct calls to objective in
lines       and    use bounds   cmax   and   respectively 

   experimental preliminaries
in this section we give background information about the computational experiments presented in
the following sections  first  we describe the design of our experiments  next  we present the
configuration scenarios  algorithm benchmark data combinations  studied in the following section 
finally  we describe the low level details of our experimental setup 
    experimental design
here we describe our objective function and the methods we used for selecting instances and seeds 
      c onfiguration o bjective   p enalized average runtime
in section    we mentioned that algorithm configuration problems arise in the context of various
different cost statistics  indeed  in our past work we explored several of them  maximizing solution
quality achieved in a given time  minimizing the runtime required to reach a given solution quality 
and minimizing the runtime required to solve a single problem instance  hutter et al         
in this work we focus on the objective of minimizing the mean runtime over instances from
a distribution d  this optimization objective naturally occurs in many practical applications  it
also implies a strong correlation between c   and the amount of time required to obtain a good
empirical estimate of c    this correlation helps to make our adaptive capping scheme effective 
one might wonder whether means are the right way to aggregate runtimes  in some preliminary
experiments  we found that minimizing mean runtime led to parameter configurations with overall good runtime performance  including rather competitive median runtimes  while minimizing
median runtime yielded less robust parameter configurations that timed out on a large  but       
fraction of the benchmark instances  however  when we encounter runs that do not terminate within
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

the given cutoff time the mean is ill defined  in order to penalize timeouts  we define the penalized
average runtime  par  of a set of runs with cutoff time max to be the mean runtime over those
runs  where unsuccessful runs are counted as p  max with penalization constant p     in this
study  we use p      
      s electing i nstances and s eeds
as mentioned previously  often only a finite set  of instances is available upon which to evaluate
our algorithm  this is the case in the experiments we report here  throughout our study  all configuration experiments are performed on a training set containing half of the given benchmark instances 
the remaining instances are solely used as a test set to evaluate the found parameter configurations 
for evaluations within paramils that are based on n runs  we selected the n instances and
random number seeds to be used by following a common blocking technique  see  e g   birattari
et al         ridge   kudenko         we ensured that whenever two parameter configurations
were compared  their cost estimates were based on exactly the same instances and seeds  this
serves to avoid noise effects due to differences between instances and the use of different seeds  for
example  it prevents us from making the mistake of considering configuration  to be better than
configuration    just because  was tested on easier instances 
when dealing with randomized target algorithms  there is also a tradeoff between the number
of problem instances used and the number of independent runs performed on each instance  in
the extreme case  for a given sample size n   one could perform n runs on a single instance or a
single run on n different instances  this latter strategy is known to result in minimal variance of
the estimator for common optimization objectives such as minimization of mean runtime  which
we consider in this study  or maximization of mean solution quality  see  e g   birattari        
consequently  we only performed multiple runs per instance when we wanted to acquire more
samples of the cost distribution than there were instances in the training set 
based on these considerations  the configuration procedures we study in this article have been
implemented to take a list of hinstance  random number seedi pairs as one of their inputs  empirical
estimates cn    of the cost statistic c   to be optimized were determined from the first n hinstance 
seedi pairs in that list  each list of hinstance  seedi pairs was constructed as follows  given a training
set consisting of m problem instances  for n  m   we drew a sample of n instances uniformly at
random and without replacement and added them to the list  if we wished to evaluate an algorithm
on more samples than we had training instances  which could happen in the case of randomized
algorithms  we repeatedly drew random samples of size m as described before  where each such
batch corresponded to a random permutation of the n training instances  and added a final sample
of size n mod m   m   as in the case n  m   as each sample was drawn  it was paired with
a random number seed that was chosen uniformly at random from the set of all possible seeds and
added to the list of hinstance  seedi pairs 
      c omparison of c onfiguration p rocedures
since the choice of instances  and to some degree of seeds  is very important for the final outcome
of the optimization  in our experimental evaluations we always performed a number of independent
runs of each configuration procedure  typically      we created a separate list of instances and seeds
for each run as explained above  where the kth run of each configuration procedure uses the same
kth list of instances and seeds   note  however  that the disjoint test set used to measure performance
of parameter configurations was identical for all runs  
   

fih utter   h oos   l eyton  b rown   s t utzle

configuration scenario
s a p s  swgcp
s p e a r  swgcp
s a p s  qcp
s p e a r  qcp
c p l e x  r e g i o n s    

type of benchmark instances   citation
graph colouring  gent  hoos  prosser   walsh       
graph colouring  gent  hoos  prosser   walsh       
quasigroup completion  gomes   selman       
quasigroup completion  gomes   selman       
combinatorial auctions  cats   leyton brown  pearson   shoham       

table    overview of our five b r o a d configuration scenarios 
algorithm
s aps
s pear

c plex

parameter type
continuous
categorical
integer
continuous
categorical
integer
continuous

  parameters of type
 
  
 
  
  
 
 

  values considered
 
   
  
  
  
  
  

total   configurations    
     
          

          

table    parameter overview for the algorithms we consider  more information on the parameters for each algorithm is given in the text 
a detailed list of all parameters and the values we considered can be found in an online appendix at
http   www cs ubc ca labs beta projects paramils algorithms html 
we performed a paired statistical test to compare the final results obtained in the runs of two
configuration procedures  a paired test was required since the kth run of both procedures shared the
same kth list of instances and seeds  in particular  we performed a two sided paired max wilcoxon
test with the null hypothesis that there was no difference in the performances  considering p values
below      to be statistically significant  the p values reported in all tables were derived using this
test  p values shown in parentheses refer to cases where the procedure we expected to perform better
actually performed worse 
    configuration scenarios
in section    we analyze our configurators based on five configuration scenarios  each combining a
high performance algorithm with a widely studied benchmark dataset  table   gives an overview
of these  which we dub the b r o a d scenarios  the algorithms and benchmark instance sets used
in these scenarios are described in detail in sections       and        respectively  in these five
b r o a d configuration scenarios  we set fairly aggressive cutoff times of five seconds per run of the
target algorithm and allowed each configuration procedure to execute the target algorithm for an
aggregate runtime of five cpu hours  these short cutoff times and fairly short times for algorithm
configuration were deliberately chosen to facilitate many configuration runs for each b r o a d scenario  in contrast  in a second set of configuration scenarios  exclusively focusing on c plex   we
set much larger cutoff times and allowed more time for configuration  we defer a description of
these scenarios to section   
      target a lgorithms
our three target algorithms are listed in table   along with their configurable parameters 
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

s aps the first target algorithm used in our experiments was s aps  a high performance dynamic
local search algorithm for sat solving  hutter  tompkins   hoos        as implemented in ubcsat  tompkins   hoos         when introduced in       s aps was a state of the art solver  and
it still performs competitively on many instances  we chose to study this algorithm because it is
well known  it has relatively few parameters  and we are intimately familiar with it  s apss four
continuous parameters control the scaling and smoothing of clause weights  as well as the probability of random walk steps  the original default parameters were set manually based on experiments
with prominent benchmark instances  this manual experimentation kept the percentage of random
steps fixed and took up about one week of development time  having subsequently gained more
experience with s apss parameters for more general problem classes  hutter  hamadi  hoos  
leyton brown         we chose promising intervals for each parameter  including  but not centered
at  the original default  we then picked seven possible values for each parameter spread uniformly
across its respective interval  resulting in      possible parameter configurations  these are exactly
the same values as used by hutter et al          as the starting configuration for paramils  we used
the center point of each parameters domain 
s pear the second target algorithm we considered was s pear  a recent tree search algorithm
for solving sat problems  s pear is a state of the art sat solver for industrial instances  and
with appropriate parameter settings it is the best available solver for certain types of hardware and
software verification instances  hutter  babic  hoos   hu         furthermore  configured with
paramils  s pear won the quantifier free bit vector arithmetic category of the      satisfiability
modulo theories competition  s pear has    parameters  including ten categorical  four integer 
and twelve continuous parameters  and their default values were manually engineered by its developer   manual tuning required about one week   the categorical parameters mainly control
heuristics for variable and value selection  clause sorting  resolution ordering  and enable or disable
optimizations  such as the pure literal rule  the continuous and integer parameters mainly deal with
activity  decay  and elimination of variables and clauses  as well as with the interval of randomized
restarts and percentage of random choices  we discretized the integer and continuous parameters
by choosing lower and upper bounds at reasonable values and allowing between three and eight
discrete values spread relatively uniformly across the resulting interval  including the default  which
served as the starting configuration for paramils  the number of discrete values was chosen according to our intuition about the importance of each parameter  after this discretization  there
were           possible parameter configurations  exploiting the fact that nine of the parameters are
conditional  i e   only relevant when other parameters take certain values  reduced this to           
configurations 
c plex the third target algorithm we used was the commercial optimization tool c plex         a
massively parameterized algorithm for solving mixed integer programming  mip  problems  out of
its     user specifiable parameters  we identified    parameters that affect c plexs search trajectory  we were careful to omit all parameters that change the problem formulation  e g   by changing
the numerical accuracy of a solution   many c plex parameters deal with mip strategy heuristics  such as variable and branching heuristics  probing  dive type and subalgorithms  and with
the amount and type of preprocessing to be performed  there are also nine parameters governing
how frequently a different type of cut should be used  those parameters have up to four allowable
magnitude values and the value choose automatically  note that this last value prevents the parameters from being ordinal   a considerable number of other parameters deal with simplex and
   

fih utter   h oos   l eyton  b rown   s t utzle

barrier optimization  and with various other algorithm components  for categorical parameters with
an automatic option  we considered all categorical values as well as the automatic one  in contrast  for continuous and integer parameters with an automatic option  we chose that option instead
of hypothesizing values that might work well  we also identified some numerical parameters that
primarily deal with numerical issues  and fixed those to their default values  for other numerical
parameters  we chose up to five possible values that seemed sensible  including the default  for the
many categorical parameters with an automatic option  we included the automatic option as a choice
for the parameter  but also included all the manual options  finally  we ended up with    configurable parameters  leading to            possible configurations  exploiting the fact that seven of
the c plex parameters were only relevant conditional on other parameters taking certain values  we
reduced this to            distinct configurations  as the starting configuration for our configuration
procedures  we used the default settings  which have been obtained by careful manual configuration
on a broad range of mip instances 
      b enchmark i nstances
we applied our target algorithms to three sets of benchmark instances  sat encoded quasi group
completion problems  sat encoded graph colouring problems based on small world graphs  and
mip encoded winner determination problems for combinatorial auctions  each set consisted of
     instances  partitioned evenly into training and test sets 
qcp our first benchmark set contained        instances of the quasi group completion problem  qcp   which has been widely studied by ai researchers  we generated these qcp instances
around the solubility phase transition  using the parameters given by gomes and selman        
specifically  the order n was drawn uniformly from the interval           and the number of holes
h  open entries in the latin square  was drawn uniformly from              n       the resulting
qcp instances were converted into sat cnf format  for use with the complete solver  s pear 
we sampled      of these sat instances uniformly at random  these had on average      variables  standard deviation        and        clauses  standard deviation           and      of them
were satisfiable  for use with the incomplete solver  s aps  we randomly sampled      instances
from the subset of satisfiable instances  determined using a complete algorithm   their number of
variables and clauses were very similar to those used with s pear 
sw gcp our second benchmark set contained        instances of the graph colouring problem
 gcp  based on the small world  sw  graphs of gent et al          of these  we sampled     
instances uniformly at random for use with s pear  these had on average      variables  standard
deviation       and        clauses  standard deviation         and      of them were satisfiable 
for use with s aps  we randomly sampled      satisfiable instances  again  determined using a
complete sat algorithm   whose number of variables and clauses were very similar to those used
with s pear 
regions    for our third benchmark set we generated      instances of the combinatorial auction
winner determination problem  encoded as mixed integer linear programs  milps   we used the
regions generator from the combinatorial auction test suite  leyton brown et al          with
the goods parameter set to     and the bids parameter set to      the resulting milp instances
contained     variables and     inequalities on average  with a standard deviation of     variables
and     inequalities 
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

scenario
s a p s  swgcp
s p e a r  swgcp
s a p s  qcp
s p e a r  qcp
c p l e x  r e g i o n s    

default
     
    
     
    
    

test performance  penalized average runtime  in cpu seconds 
mean  stddev  for    runs
run with best training performance
basicils
focusedils
basicils
focusedils
                     
    
    
         
        
   
   
                     
    
    
          
         
    
    
        
          
    
    

fig 
  a 
  b 
  c 
  d 
  e 

table    performance comparison of the default parameter configuration and the configurations found
with basicils and focusedils  both with aggr capping and bm       for each configuration scenario  we list test performance  penalized average runtime over      test instances  in cpu seconds  of the algorithm default  mean  stddev of test performance across
   runs of basicils        focusedils  run for five cpu hours each   and the test performance of the run of basicils and focusedils that was best in terms of training performance  boldface indicates the better of basicils and focusedils  the algorithm configurations found in focusedilss run with the best training performance are listed in an online appendix at http   www cs ubc ca labs beta projects paramils results html  column fig  gives a reference to a scatter plot comparing the performance of those configurations
against the algorithm defaults 

    experimental setup
we carried out all of our experiments on a cluster of    dual    ghz intel xeon pcs with  mb
cache and  gb ram  running opensuse linux       we measured runtimes as cpu time on
these reference machines  all our configuration procedures were implemented as ruby scripts  and
we do not include the runtime of these scripts in the configuration time  in easy configuration
scenarios  where most algorithm runs finish in milliseconds  the overhead of our scripts can be
substantial  indeed  the longest configuration run we observed took    hours to execute five hours
worth of target algorithm runtime  in contrast  for the harder c plex scenarios described in section
  we observed virtually no overhead 

   empirical evaluation of basicils  focusedils and adaptive capping
in this section  we use our b r o a d scenarios to empirically study the performance of basicils n  
and focusedils  as well as the effect of adaptive capping  we first demonstrate the large speedups
paramils achieved over the default parameters and then study the components responsible for this
success 
    empirical comparison of default and optimized parameter configurations
in this section  for each of our five b r o a d configuration scenarios  we compare the performance of
the respective algorithms default parameter configuration against the final configurations found by
basicils      and focusedils  table   and especially figure   show that the configurators led to
very substantial speedups 
in table    we report the final performance achieved by    independent runs of each configurator  for each independent configuration run  we used a different set of training instances and seeds
 constructed as described in section         we note that there was often a rather large variance
in the performances found in different runs of the configurators  and that the configuration found
   

fih utter   h oos   l eyton  b rown   s t utzle

 

 

  

runtime  s   autotuned

runtime  s   autotuned

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  
 

 

  

 

  

 

  

  

 

  

 

 

  

 

  

runtime  s   default

  

 a  s a p s  swgcp 
   s vs     s      vs no timeouts
 

 

  

 

  

 

 

  

 

  

 

  

 

  

 

  

  

 

  

 

  

 

  

 

  

 

  

runtime  s   default

 c  s a p s  qcp 
  s vs     s      vs   timeouts

 

  

 

  

 

 

  

 

  

 

  

 

  

 

  

 

  
 

 

  

  

runtime  s   autotuned

runtime  s   autotuned

 

  

 b  s p e a r  swgcp 
  s vs   s    vs   timeouts

  

  

 

  

runtime  s   default

 

  

runtime  s   autotuned

 

  

 

  

 

  

 

  

 

  

 

  

 

  
 

  

 

  

 

  

 

  

 

  

  

runtime  s   default

 d  s p e a r  qcp 
   s vs     s    vs   timeouts

 

 

  

 

  

 

  

 

  

 

  

 

  

 

  

runtime  s   default

 

  

 e  c p l e x  r e g i o n s     
    s vs     s  no timeouts

figure    comparison of default vs automatically determined parameter configurations for our five b r o a d
configuration scenarios  each dot represents one test instance  timeouts  after one cpu hour  are
denoted by circles  the dashed line at five cpu seconds indicates the cutoff time of the target
algorithm used during the configuration process  the subfigure captions give mean runtimes for
the instances solved by both of the configurations  default vs optimized   as well as the number of
timeouts for each 

in the run with the best training performance also tended to yield better test performance than the
others  for that reason  we used that configuration as the result of algorithm configuration   note
that choosing the configuration found in the run with the best training set performance is a perfectly
legitimate procedure since it does not require knowledge of the test set  of course  the improvements thus achieved come at the price of increased overall running time  but the independent runs
of the configurator can easily be performed in parallel  
in figure    we compare the performance of this automatically found parameter configuration
against the default configuration  when runs are allowed to last up to an hour  the speedups are
more obvious in this figure than in table    since the penalized average runtime in that table counts
runtimes larger than five seconds as fifty seconds  ten times the cutoff of five seconds   whereas the
data in the figure uses a much larger cutoff time  the larger speedups are most apparent for scenarios
s a p s  swgcp  s a p s  qcp  and s p e a r  qcp  their corresponding speedup factors in mean runtime are
now           and     respectively  see figure    
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

algorithm    randomsearch n     
outline of random search in parameter configuration space  inc denotes the incumbent parameter
configuration  bettern compares two configurations based on the first n instances from the training
set 
input   number of runs to use for evaluating parameter configurations  n   initial configuration
    
output   best parameter configuration inc found 
  inc     
  while not terminationcriterion   do
 
  random    
 
if bettern    inc   then
 
inc   
 

return inc

    empirical comparison of basicils and simple baselines
in this section  we evaluate the effectiveness of basicils n   against two of its components 
 a simple random search  used in basicils for initialization  we dub it randomsearch n   and
provide pseudocode for it in algorithm     and
 a simple local search  the same type of iterative first improvement search used in basicils n  
 we dub it simplels n    
to evaluate one component at a time  in this section and in section     we study our algorithms
without adaptive capping  we then investigate the effect of our adaptive capping methods in section
    
if there is sufficient structure in the search space  we expect basicils to outperform randomsearch  if there are local minima  we expect basicils to perform better than simple local search 
our experiments showed that basicils did indeed offer the best performance 
here  we are solely interested in comparing how effectively the approaches search the space
of parameter configurations  and not how the found parameter configurations generalize to unseen
test instances   thus  in order to reduce variance in our comparisons  we compare the configuration
methods in terms of their performance on the training set 
in table    we compare basicils against randomsearch for our b r o a d configuration scenarios 
on average  basicils always performed better  and in three of the five scenarios  the difference was
statistically significant as judged by a paired max wilcoxon test  see section         table   also
lists the performance of the default parameter configuration for each of the scenarios  we note that
both basicils and randomsearch consistently achieved substantial  and statistically significant 
improvements over these default configurations 
next  we compared basicils against its second component  simplels  this basic local search
is identical to basicils  but stops at the first local minimum encountered  we used it in order to
study whether local minima pose a problem for simple first improvement search  table   shows that
in the three configuration scenarios where basicils had time to perform multiple ils iterations 
its training set performance was statistically significantly better than that of simplels  thus  we
conclude that the search space contains structure that can be exploited with a local search algorithm
as well as local minima that can limit the performance of iterative improvement search 
   

fih utter   h oos   l eyton  b rown   s t utzle

scenario
s a p s  swgcp
s p e a r  swgcp
s a p s  qcp
s p e a r  qcp
c p l e x  r e g i o n s    

training performance  penalized average runtime  in cpu seconds 
default randomsearch     
basicils     
     
          
          
     
          
          
     
           
          
    
          
          
    
          
          

p value
    
    
        
     
        

table    comparison of randomsearch      and basicils       both without adaptive capping  the table
shows training performance  penalized average runtime over n       training instances  in cpu
seconds   note that both approaches yielded substantially better results than the default configuration  and that basicils performed statistically significantly better than randomsearch in three
of the five b r o a d configuration scenarios as judged by a paired max wilcoxon test  see section
       
scenario
s a p s  swgcp
s a p s  qcp
s p e a r  qcp

simplels     
performance
         
          
         

basicils     
performance
avg    ils iterations
          
   
          
   
          
    

p value
        
        
     

table    comparison of simplels      and basicils       both without adaptive capping  the table shows
training performance  penalized average runtime over n       training instances  in cpu seconds   in configuration scenarios s p e a r  swgcp and c p l e x  r e g i o n s      basicils did not complete its first ils iteration in any of the    runs  the two approaches were thus identical and are not
listed here  in all other configuration scenarios  basicils found significantly better configurations
than simplels 

    empirical comparison of focusedils and basicils
in this section we investigate focusedilss performance experimentally  in contrast to our previous comparison of randomsearch  simplels  and basicils using training performance  we now
compare focusedils against basicils using test performance  this is becausein contrast to basicils and simplelsfocusedils grows the number of target algorithm runs used to evaluate a
parameter configuration over time  even different runs of focusedils  using different training sets
and random seeds  do not use the same number of target algorithm runs to evaluate parameter configurations  however  they all eventually aim to optimize the same cost statistic  c  and therefore
test set performance  an unbiased estimator of c  provides a fairer basis for comparison than training performance  we only compare focusedils to basicils  since basicils already outperformed
randomsearch and simplels in section     
figure   compares the test performance of focusedils and basicils n   with n         and
     using a single target algorithm run to evaluate each parameter configuration  basicils   
was fast  but did not generalize well to the test set at all  for example  in configuration scenario
s a p s  swgcp  basicils    selected a parameter configuration whose test performance turned out to
be even worse than the default  on the other hand  using a large number of target algorithm runs for
each evaluation resulted in a very slow search  but eventually led to parameter configurations with
good test performance  focusedils aims to achieve a fast search and good generalization to the test
set  for the configuration scenarios in figure    focusedils started quickly and also led to the best
final performance 
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

 

 

mean runtime  s   test

mean runtime  s   test

  

 

  

 

  

 

  

basicils   
basicils    
basicils     
focusedils
 

  

 

 

   

 

 

  

   

  

basicils   
basicils    
basicils     
focusedils
 

  

cpu time used for tuner  s 

 

 

  

  

cpu time used for tuner  s 

 a  s a p s  swgcp

 b  c p l e x  r e g i o n s    

figure    comparison of basicils n   with n          and     vs focusedils  both without adaptive
capping  we show the median of test performance  penalized average runtime across       test
instances  across    runs of the configurators for two scenarios  performance in the other three
b r o a d scenarios was qualitatively similar  basicils    was the fastest to move away from the
starting parameter configuration  but its performance was not robust at all  basicils     was a
rather good compromise between speed and generalization performance  but given enough time
was outperformed by basicils       focusedils started finding good configurations quickly
 except for scenario s p e a r  qcp  where it took even longer than basicils      to improve over
the default  and always was amongst the best approaches at the end of the configuration process 

scenario
s a p s  swgcp
s p e a r  swgcp
s a p s  qcp
s p e a r  qcp
c p l e x  r e g i o n s    

test performance  penalized average runtime  in cpu seconds 
default basicils     
focusedils
     
          
          
    
          
          
     
          
          
    
          
          
    
          
          

p value
        
      
     
      
        

table    comparison of basicils      and focusedils  both without adaptive capping  the table shows
test performance  penalized average runtime over       test instances  in cpu seconds   for each
configuration scenario  we report test performance of the default parameter configuration  mean 
stddev of the test performance reached by    runs of basicils      and focusedils  and the pvalue for a paired max wilcoxon test  see section        for the difference of the two configurators
performance 

we compare the performance of focusedils and basicils      for all configuration scenarios in table    for three s aps and c plex scenarios  focusedils performed statistically significantly better than basicils       these results are consistent with our past work in which focusedils achieved statistically significantly better performance than basicils       hutter et al  
       however  we found that in both configuration scenarios involving the s pear algorithm 
basicils      actually performed better on average than focusedils  albeit not statistically significantly  we attribute this to the fact that for a complete  industrial solver such as s pear  the two
benchmark distributions qcp and swgcp are quite heterogeneous  we expect focusedils to have
problems in dealing with highly heterogeneous distributions  due to the fact that it frequently tries
to extrapolate performance based on a few runs per parameter configuration 
   

fih utter   h oos   l eyton  b rown   s t utzle

 

   
basicils  no capping
basicils  tp capping

mean runtime  s   train

mean runtime  s   train

  

 

  

 

  

 

  

 

 

  

  

basicils  no capping
basicils  tp capping
 

   

 

   

   
  

 

  

cpu time used for tuner  s 

 

  

 

  

cpu time used for tuner  s 

 a  s a p s  swgcp  significant 

 b  c p l e x  r e g i o n s      significant 

figure    speedup of basicils by adaptive capping for two configuration scenarios  we performed   
runs of basicils      without adaptive capping and with tp capping  for each time step  we
computed training performance of each run of the configurator  penalized average runtime over
n       training instances  and plot the median over the    runs 
scenario
s a p s  swgcp
s p e a r  swgcp
s a p s  qcp
s p e a r  qcp
c p l e x  r e g i o n s    

training performance  penalized average runtime 
no capping
tp capping
p value
          
          
        
          
          
    
          
          
        
                       
    
          
          
        

avg    ils iterations
no capping tp capping
 
  
 
 
 
  
 
 
 
 

table    effect of adaptive capping for basicils       we show training performance  penalized average runtime on n       training instances  in cpu seconds   for each configuration scenario  we
report mean  stddev of the final training performance reached by    runs of the configurator without capping and with tp capping  the p value for a paired max wilcoxon test for their difference
 see section         as well as the average number of ils iterations performed by the respective
configurator 

    empirical evaluation of adaptive capping in basicils and focusedils
we now present experimental evidence that the use of adaptive capping has a strong impact on the
performance of basicils and focusedils 
figure   illustrates the extent to which tp capping sped up basicils for two configuration scenarios  in both cases  capping helped to improve training performance substantially  for s a p s  swgcp 
basicils found the same solutions up to about an order of magnitude faster than without capping 
table   quantifies the speedups for all five b r o a d configuration scenarios  tp capping enabled
up to four times as many ils iterations  in s a p s  swgcp  and improved average performance in all
scenarios  the improvement was statistically significant in all scenarios  except s p e a r  qcp 
aggressive capping further improved basicils performance for one scenario  for scenario
s a p s  swgcp  it increased the number of ils iterations completed within the configuration time
from    to      leading to a significant improvement in performance  in the first ils iteration of
basicils  both capping techniques are identical  the best configuration in that iteration is always
the incumbent   thus  we did not observe a difference on configuration scenarios s p e a r  swgcp and
c p l e x  r e g i o n s      for which none of the    runs of the configurator finished its first ils iteration 
for the remaining two configuration scenarios  the differences were insignificant 
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

scenario
s a p s  swgcp
s p e a r  swgcp
s a p s  qcp
s p e a r  qcp
c p l e x  r e g i o n s    

number of ils iterations performed
no capping
tp capping
p value
       
       
        
      
      
      
       
       
    
       
       
    
      
      
    

aggr capping
       
      
       
       
      

p value
        
      
     
        
        

number of runs performed for the incumbent parameter configuration
scenario
no capping
tp capping
p value
aggr capping
p value
s a p s  swgcp
        
                                     
        
        
      
        
     
s p e a r  swgcp
s a p s  qcp
                   
     
         
     
s p e a r  qcp
        
         
    
         
     
        
        
    
        
    
c p l e x  r e g i o n s    

table    effect of adaptive capping on search progress in focusedils  as measured by the number of ils
iterations performed and the number of runs performed for the incumbent parameter configuration 
for each configuration scenario  we report mean  stddev of both of these measures across   
runs of the configurator without capping  with tp capping  and with aggr capping  as well as the
p values for paired max wilcoxon tests  see section        for the differences between no capping
and tp capping  and between no capping and aggr capping 

we now evaluate the usefulness of capping for focusedils  training performance is not a useful
quantity in the context of comparing different versions of focusedils  since the number of target
algorithm runs this measure is based on varies widely between runs of the configurator  instead  we
used two other measures to quantify search progress  the number of ils iterations performed and
the number of target algorithm runs performed for the incumbent parameter configuration  table  
shows these two measures for our five b r o a d configuration scenarios and the three capping schemes
 none  tp  aggr   focusedils with tp capping achieved higher values than without capping for all
scenarios and both measures  although only some of the differences were statistically significant  
aggressive capping increased both measures further for all scenarios  and most of the differences
between no capping and aggressive capping were statistically significant  figure   demonstrates that
for two configuration scenarios focusedils with capping reached the same solution qualities more
quickly than without capping  however  after finding the respective configurations  focusedils
showed no further significant improvement 
recall that the experiments in section     and     compared our various configurators without adaptive capping  one might wonder how these comparisons change in the presence of adaptive
capping  indeed  adaptive capping also worked out of the box for randomsearch and enabled it to
evaluate between     and    times as many configurations than without capping  this improvement
significantly improved the simple algorithm randomsearch to the point where its average performance came within    of the one of basicils for two domains  s a p s  swgcp and s p e a r  swgcp 
compare the much larger differences without capping reported in table     for s p e a r  qcp  there
was still a     difference in average performance  but this result was not significant  finally  for
s a p s  qcp and c p l e x  r e g i o n s     the difference was still substantial and significant      and    
difference in average performance  with p values          and         respectively  
adaptive capping also reduced the gap between basicils and focusedils  in particular  for
s a p s  swgcp  where  even without adaptive capping  focusedils achieved the best performance we
have encountered for this scenario  basicils caught up when using adaptive capping  similarly 
   

fih utter   h oos   l eyton  b rown   s t utzle

 

 
focusedils  no capping
focusedils  tp capping
focusedils  aggr capping

mean runtime  s   test

mean runtime  s   test

  

 

  

 

  

 

  

 

  

 

  

 

  

focusedils  no capping
focusedils  tpcapping
focusedils  aggr capping
   

 

   

   
  

 

  

cpu time used for tuner  s 

 

  

 

  

 

  

 

  

cpu time used for tuner  s 

 a  s a p s  swgcp

 b  c p l e x  r e g i o n s    

figure    speedup of focusedils by adaptive capping for two configuration scenarios  we performed   
runs of focusedils without adaptive capping  with tp capping and with aggr capping  for each
time step  we computed the test performance of each run of the configurator  penalized average
runtime over      test instances  and plot the median over the    runs  the differences at the
end of the trajectory were not statistically significant  however  with capping the time required to
achieve that quality was lower in these two configuration scenarios  in the other three scenarios 
the gains due to capping were smaller 

for c p l e x  r e g i o n s      focusedils already performed very well without adaptive capping while
basicils did not  here  basicils improved based on adaptive capping  but still could not rival
focusedils  for the other scenarios  adaptive capping did not affect the relative performance much 
compare tables    without capping  and    with capping  for details 

   case study  configuring c plex for real world benchmarks
in this section  we demonstrate that paramils can improve the performance of the commercial optimization tool c plex for a variety of interesting benchmark distributions  to our best knowledge 
this is the first published study on automatically configuring c plex 
we use five c plex configuration scenarios  for these  we collected a wide range of mip benchmarks from public benchmark libraries and other researchers  and split each of them       into
disjoint training and test sets  we detail them in the following 
 regions    this set is almost identical to the regions    set  described in section      
and used throughout the paper   but its instances are much larger  we generated       milp
instances with the generator provided with the combinatorial auction test suite  leytonbrown et al          based on the regions option with the goods parameter set to     and
the bids parameter set to        these instances contain an average of       variables and    
inequalities  with respective standard deviations of     and     
 mja this set comprises     machine job assignment instances encoded as mixed integer
quadratically constrained programs  miqcp   it was obtained from the berkeley computational optimization lab  and was introduced by akturk  atamturk and s  gurel        
these instances contain an average of       variables and       constraints  with respective
standard deviations of       and       
   http   www ieor berkeley edu atamturk bcol   where this set is called conic sch

   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

 cls this set comprises     capacitated lot sizing instances encoded as mixed integer linear
programs  milp   it was also obtained from the berkeley computational optimization lab
and was introduced by atamturk and munoz         all     instances contain     variables
and     constraints 
 mik this set of     milp encoded mixed integer knapsack instances was also obtained
from the berkeley computational optimization lab and was originally introduced by atamturk
        these instances contain an average of     variables and     constraints  with respective standard deviations of     and     
 qp this set of quadratic programs originated from rna energy parameter optimization  andronescu  condon  hoos  mathews   murphy         mirela andronescu generated       instances for our experiments  these instances contain            variables and           
constraints  since the instances are polynomial time solvable quadratic programs  we set a
large number of inconsequential c plex parameters concerning the branch and cut mechanism to their default values  ending up with    categorical    integer and   continuous parameters to be configured  for a discretized parameter configuration space of size             
to study paramilss behavior for these harder problems  we set significantly longer cutoff times
for these c plex scenarios than for the b r o a d scenarios from the previous section  specifically  we
used a cutoff time of     cpu seconds for each run of the target algorithm during training  and
allotted two cpu days for every run of each of the configurators  as always  our configuration
objective was to minimize penalized average runtime with a penalization constant of    
in table    we compare the performance of c plexs default parameter configuration with the
final parameter configurations found by basicils      and focusedils  both with aggressive capping and bm       note that  similar to the situation described in section      in some configuration
scenarios  e g   c p l e x  cls  c p l e x  mik  there was substantial variance between the different runs of
the configurators  and the run with the best training performance yielded a parameter configuration
that was also very good on the test set  while basicils outperformed focusedils in   of these  
scenarios in terms of mean test performance across the ten runs  focusedils achieved the better test
performance for the run with the best training performance for all but one scenario  in which it performed almost as well   for scenarios c p l e x  r e g i o n s     and c p l e x  cls  focusedils performed
substantially better than basicils 
note that in all c plex configuration scenarios we considered  both basicils and focusedils
found parameter configurations that were better than the algorithm defaults  sometimes by over
an order of magnitude  this is particularly noteworthy since ilog expended substantial effort to
determine strong default c plex parameters  in figure    we provide scatter plots for all five scenarios  for c p l e x  r e g i o n s      c p l e x   c o n i c   s c h   c p l e x  cls  and c p l e x  mik  speedups were quite
consistent across instances  with average speedup factors reaching from   for c p l e x   c o n i c   s c h
to    for c p l e x  mik   finally  for c p l e x  qp we see an interesting failure mode of paramils  the
optimized parameter configuration achieved good performance with the cutoff time used for the
   for configuration scenario c p l e x  mik  nine out of ten runs of focusedils yielded parameter configurations with
average runtimes smaller than two seconds  one run  however  demonstrated an interesting failure mode of focusedils with aggressive capping  capping too aggressively caused every c plex run to be unsuccessful  such that
focusedils selected a configuration which did not manage to solve a single instance in the test set  counting unsuccessful runs as ten times the cutoff time  this resulted in an average runtime of                seconds for this run 
 for full details  see section     of hutter        

   

fih utter   h oos   l eyton  b rown   s t utzle

scenario
c p l e x  r e g i o n s    
cp l e x c o n i c s c h
c p l e x  cls
c p l e x  mik
c p l e x  qp

test performance  penalized average runtime  in cpu seconds 
mean  stddev  for    runs
run with best training performance
default
basicils
focusedils basicils
focusedils
  
      
         
  
    
    
          
         
    
    
   
        
        
  
    
    
      
          
    
    
   
        
        
   
   

fig 
  a 
  b 
  c 
  d 
  e 

table    experimental results for our c plex configuration scenarios 

for each configuration scenario  we list test performance  penalized average runtime over test instances  of the algorithm default  mean  stddev of test performance across ten runs of basicils     
  focusedils  run for two cpu days each   and the test performance of the run of
basicils and focusedils that is best in terms of training performance  boldface indicates the better of basicils and focusedils  the algorithm configurations found in
focusedilss run with the best training performance are listed in an online appendix
at http   www cs ubc ca labs beta projects paramils results html 
column
fig  gives a reference to a scatter plot comparing the performance of those configurations against
the algorithm defaults 

configuration process      cpu seconds  see figure   f    but this performance did not carry over
to the higher cutoff time we used in our tests       cpu seconds  see figure   e    thus  the parameter configuration found by focusedils did generalize well to previously unseen test data  but
not to larger cutoff times 

   review of other paramils applications
in this section  we review a number of other applications of paramilssome of them dating back
to earlier stages of its development  others very recentthat demonstrate its utility and versatility 
    configuration of saps  gls  and sat j
hutter et al          in the first publication on paramils  reported experiments on three target algorithms to demonstrate the effectiveness of the approach  the sat algorithm saps  which has
  numerical parameters   the local search algorithm gls  for solving the most probable explanation  mpe  problem in bayesian networks  which has   numerical parameters  hutter  hoos  
stutzle         and the tree search sat solver sat j  which has   categorical and   numerical
parameters  http   www sat j org   they compared the respective algorithms default performance 
the performance of the calibra system  adenso diaz   laguna         and the performance
of basicils and focusedils  out of the four configuration scenarios studied  focusedils significantly outperformed calibra on two and performed better on average on the third  for the fourth
one  configuring sat j   calibra was not applicable due to the categorical parameters  while
focusedils significantly outperformed basicils 
overall  automated parameter optimization using paramils achieved substantial improvements
over the previous default settings  gls  was sped up by a factor        tuned parameters found
solutions of better quality in    seconds than the default found in one hour   saps by factors of  
and     on saps qwh and saps sw  respectively  and sat j by a factor of    
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

 

 

 

  

 

  

 

  

 

  

 

  

runtime  s   autotuned

 

  

  

 

  

 

  

 

  

 

  

 

  

 

  
 

  

 

  

 

  

 

  

 

  

 

  

runtime  s   default

 

  

 

  

 

  

 

  

 

  

 

 

  

runtime  s   default

 

  

 

  

 

  

 

  

  

 

  

 

  

 

  

 

  

  

runtime  s   default

 d  c p l e x  mik 
  s vs    s  no timeouts

 

 

  

 
 

  

 

  

 

  

 

  

 

  

runtime  s   default

 

  

 c  c p l e x  cls 
   s vs     s  no timeouts
 

  

 

  

 

  

 

  

 

  

 

  

 

 

  

 

  

 

  

 

  

 

  

 

  
 

 

  

 

runtime  s   autotuned

runtime  s   autotuned

 

  

 

  

  

 

 

 

  

  

  

  

 

  

 

 b  c p l e x   c o n i c   s c h  
    s vs       s  no timeouts

  

 

  

  
 

  

 a  c p l e x  r e g i o n s     
  s vs     s  no timeouts

runtime  s   autotuned

 

  

runtime  s   autotuned

runtime  s   autotuned

  

  
 

  

 

  

 

  

 

  

 

  

 

  

runtime  s   default

 e  c p l e x  qp 
   s vs    s    vs    timeouts

 

 

  

  

 

  

 

  

 

  

 

  

 

  

runtime  s   default

 

  

 f  c p l e x  qp  with test cutoff of    
seconds 
  s vs   s      vs     timeouts

figure    comparison of default vs automatically determined parameter configuration for our five c plex
configuration scenarios  each dot represents one test instance  time outs  after one cpu hour 
are denoted by red circles  the blue dashed line at     cpu seconds indicates the cutoff time
of the target algorithm used during the configuration process  the subfigure captions give mean
runtimes for the instances solved by both of the configurations  default vs optimized   as well as
the number of timeouts for each 

    configuration of spear for industrial verification problems
hutter et al         applied paramils to a specific real world application domain  configuring
the    parameters of the tree search dpll solver s pear to minimize its mean runtime on a set
of practical verification instances  in particular  they considered two sets of industrial problem
instances  bounded model checking  bmc  instances from zarpas        and software verification
 swv  instances generated by the c alysto static checker  babic   hu        
the instances from both problem distributions exhibited a large spread in hardness for s pear 
for the swv instances  the default configuration solved many instances in milliseconds but failed
to solve others in days  this was despite the fact that s pear was specifically developed for this type
of instances  that its developer had generated the problem instances himself  and thus had intimate
domain knowledge   and that a week of manual performance tuning had been expended in order to
optimize the solvers performance 
s pear was first configured for good general performance on industrial sat instances from
previous sat competitions  this already led to substantial improvements over the default perfor   

fih utter   h oos   l eyton  b rown   s t utzle

mance in the      sat competition   while the s pear default solved    instances and ranked   th
in the first round of the competition  an automatically configured version solved    instances and
ranked  th  and a further optimized version solved    instances  ranking  th  above minisat   the
speedup factors due to this general optimization were    and     on the swv and bmc datasets 
respectively 
optimizing on the specific instance sets yielded further  and much larger improvements  a factor
of over     for swv and     for bmc   most encouragingly  the best parameter configuration found
for the software verification instances did not take longer than    seconds to solve any of the swv
problem instances  compared to multiple timeouts after a cpu day for the original default values  
key to good performance in that application was to perform multiple independent runs of focusedils  and to select the found configuration with best training performance  as also done in
sections     and   of this article  
    configuration of satenstein
khudabukhsh  xu  hoos and leyton brown         used paramils to perform automatic algorithm
design in the context of stochastic local search algorithms for sat  specifically  they introduced a
new framework for local search sat solvers  called satenstein  and used paramils to choose
good instantiations of the framework for given instance distributions  satenstein spans three broad
categories of sls based sat solvers  walksat based algorithms  dynamic local search algorithms
and g  wsat variants  all of these are combined in a highly parameterized framework solver with
a total of    parameters and            unique instantiations 
focusedils was used to configure satenstein on six different problem distributions  and the
resulting solvers were compared to eleven state of the art sls based sat solvers  the results
showed that the automatically configured versions of satenstein outperformed all of the eleven
state of the art solvers in all six categories  sometimes by a large margin 
the sat enstein work clearly demonstrated that automated algorithm configuration methods
can be used to construct new algorithms by combining a wide range of components from existing algorithms in novel ways  and thereby go beyond simple parameter tuning  due to the low
level of manual work required by this approach  we believe this automated design of algorithms
from components will become a mainstream technique in the development of algorithms for hard
combinatorial problems 
key to the successful application of focusedils for configuring sat enstein was the careful
selection of homogeneous instance distributions  most instances of which could be solved within a
comparably low cutoff time of    seconds per run  again  the configuration with the best training
quality was selected from ten parallel independent runs of focusedils per scenario 
    self configuration of paramils
as a heuristic optimization procedure  paramils is itself controlled by a number of parameters 
the number of random configurations  r  to be sampled at the beginning of search  the perturbation
strength  s  and the probability of random restarts  prestart   furthermore  our aggressive capping
mechanism makes use of an additional parameter  the bound multiplier  bm  throughout this article 
we have used the manually determined default values hr  s  prestart   bmi   h              i 
   see http   www cril univ artois fr sat    s pear was not allowed to participate in the second round
of this competition since its source code is not publicly available 

   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

in recent work  see section     of hutter         we evaluated whether focusedilss performance could be improved by using paramils to automatically find a better parameter configuration 
in this self configuration task  configuration scenarios play the role of instances  and the configurator to be optimized plays the role of the target algorithm  to avoid confusion  we refer to this
configurator as the target configurator  here  we set fairly short configuration times of one cpu
hour for the target configurator  however  this was still significantly longer than the cutoff times
we used in any of our other experiments  such that parallelization turned out to be crucial to finish the experiment in a reasonable amount of time  because basicils is easier to parallelize than
focusedils  we chose basicils      as the meta configurator 
although the notion of having an algorithm configurator configure itself was intriguing  in this
case  it turned out to only yield small improvements  average performance improved for four out
of the five scenarios and degraded for the remaining one  however  none of the differences was
statistically significant 
    other applications of paramils
thachuk  shmygelska and hoos         used basicils in order to determine performance optimizing
parameter settings of a new replica exchange monte carlo algorithm for protein folding in the  dhp and  d hp models   even though their algorithm has only four parameters  two categorical and
two continuous   basicils achieved substantial performance improvements  while the manuallyselected configurations were biased in favour of either short or long protein sequences  basicils
found a configuration which consistently yielded good mean runtimes for all types of sequences 
on average  the speedup factor achieved was approximately      and for certain classes of protein
sequences up to    while all manually selected configurations performed worse than the previous
state of the art algorithm for this problem on some instances  the robust parameter configurations
selected by basicils yielded uniformly better performance 
in very recent work  fawcett  hoos and chiarandini        used several variants of paramils
 including a version that has been slightly extended beyond the ones presented here  to design
a modular stochastic local search algorithm for the post enrollment course timetabling problem 
they followed a design approach that used automated algorithm configuration in order to explore
a large design space of modular and highly parameterised stochastic local search algorithms  this
quickly led to a solver that placed third in track   of the  nd international timetabling competition
 itc      and subsequently produced an improved solver that is shown to achieve consistently
better performance than the top ranked solver from the competition 

   related work
many researchers before us have been dissatisfied with manual algorithm configuration  and various
fields have developed their own approaches for automatic parameter tuning  we start this section
with the most closely related workapproaches that employ direct search to find good parameter
configurationsand then describe other methods  finally  we discuss work on related problems 
such as finding the best parameter configuration or algorithm on a per instance basis  and approaches
that adapt their parameters during an algorithms execution  see also hoos        for further related
work on automated algorithm design  
   basicils was used  because focusedils had not yet been developed when that study was conducted 

   

fih utter   h oos   l eyton  b rown   s t utzle

    direct search methods for algorithm configuration
approaches for automated algorithm configuration go back to the early     s  when a number of
systems were developed for adaptive problem solving  one of these systems is composer  gratch
  dejong         which performs a hill climbing search in configuration space  taking moves if
enough evidence has been gathered to render a neighbouring configuration statistically significantly
better than the current configuration  composer was successfully applied to improving the five
parameters of an algorithm for scheduling communication between a collection of ground based
antennas and spacecrafts  gratch   chien        
around the same time  the multi tac system was introduced by minton               multitac takes as input generic heuristics  a specific problem domain  and a distribution over problem instances  it adapts the generic heuristics to the problem domain and automatically generates
domain specific lisp programs implementing them  a beam search is then used to choose the best
lisp program where each program is evaluated by running it on a fixed set of problem instances
sampled from the given distribution 
another search based approach that uses a fixed training set was introduced by coy et al         
their approach works in two stages  first  it finds a good parameter configuration i for each instance ii in the training set by a combination of experimental design  full factorial or fractional
factorial  and gradient descent  next  it combines the parameter configurations             n thus determined by setting each parameter to the average of the values taken in all of them  note that this
averaging step restricts the applicability of the method to algorithms with only numerical parameters 
a similar approach  also based on a combination of experimental design and gradient descent 
using a fixed training set for evaluation  is implemented in the calibra system of adenso diaz
and laguna         calibra starts by evaluating each parameter configuration in a full factorial
design with two values per parameter  it then iteratively homes in to good regions of parameter
configuration space by employing fractional experimental designs that evaluate nine configurations
around the best performing configuration found so far  the grid for the experimental design is
refined in each iteration  once a local optimum is found  the search is restarted  with a coarser
grid   experiments showed calibras ability to find parameter settings for six target algorithms
that matched or outperformed the respective originally proposed parameter configurations  its main
drawback is the limitation to tuning numerical and ordinal parameters  and to a maximum of five
parameters  when we first introduced paramils  we performed experiments comparing its performance against calibra  hutter et al          these experiments are reviewed in section     
terashima marn et al         introduced a genetic algorithm for configuring a constraint satisfaction algorithm for large scale university exam scheduling  they constructed and configured an
algorithm that works in two stages and has seven configurable categorical parameters  they optimized these choices with a genetic algorithm for each of    problem instances  and for each of them
found a configuration that improved performance over a modified brelaz algorithm  however  note
that they performed this optimization separately for each instance  their paper did not quantify how
long these optimizations took  but stated that issues about the time for delivering solutions with
this method are still a matter of research 
work on automated parameter tuning can also be found in the numerical optimization literature  in particular  audet and orban        proposed the mesh adaptive direct search algorithm 
designed for purely continuous parameter configuration spaces  this algorithm is guaranteed to converge to a local optimum of the cost function  parameter configurations were evaluated on a fixed
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

set of large unconstrained regular problems from the cuter collection  using as optimization objectives runtime and number of function evaluations required for solving a given problem instance 
performance improvements of around     over the classical configuration of four continuous parameters of interior point methods were reported 
algorithm configuration is a stochastic optimization problem  and there exists a large body of
algorithms designed for such problems  see  e g   spall         however  many of the algorithms in
the stochastic optimization literature require explicit gradient information and are thus inapplicable
to algorithm configuration  some algorithms approximate the gradient from function evaluations
only  e g   by finite differences   and provably converge to a local minimum of the cost function
under mild conditions  such as continuity  still  these methods are primarily designed to deal with
numerical parameters and only find local minima  we are not aware of any applications of general
purpose algorithms for stochastic optimization to algorithm configuration 
    other methods for algorithm configuration
sequential parameter optimization  spo   bartz beielstein        is a model based parameter optimization approach based on the design and analysis of computer experiments  dace  see  e g  
santner  williams   notz         a prominent approach in statistics for blackbox function optimization  spo starts by running the target algorithm with parameter configurations from a latin
hypercube design on a number of training instances  it then builds a response surface model based
on gaussian process regression and uses the models predictions and predictive uncertainties to determine the next parameter configuration to evaluate  the metric underlying the choice of promising
parameter configurations is the expected improvement criterion used by jones  schonlau and welch
        after each algorithm run  the response surface is refitted  and a new parameter configuration is determined based on the updated model  in contrast to the previously mentioned methods 
spo does not use a fixed training set  instead  it starts with a small training set and doubles its size
whenever a parameter configuration is determined as incumbent that has already been incumbent in
a previous iteration  a recent improved mechanism resulted in a more robust version  spo   hutter  hoos  leyton brown   murphy         the main drawbacks of spo and its variants  and in
fact of the entire dace approach  are its limitation to continuous parameters and to optimizing
performance for single problem instances  as well as its cubic runtime scaling in the number of data
points 
another approach is based on adaptations of racing algorithms in machine learning  maron  
moore        to the algorithm configuration problem  birattari et al               developed a procedure dubbed f race and used it to configure various stochastic local search algorithms  f race
takes as input an algorithm a  a finite set of algorithm configurations   and an instance distribution d  it iteratively runs the target algorithm with all surviving parameter configurations on a
number of instances sampled from d  in the simplest case  each iteration runs all surviving configurations on one instance   a configuration is eliminated from the race as soon as enough statistical
evidence is gathered against it  after each iteration  a non parametric friedman test is used to check
whether there are significant differences among the configurations  if this is the case  the inferior
configurations are eliminated using a series of pairwise tests  this process is iterated until only
one configuration survives or a given cutoff time is reached  various applications of f race have
demonstrated very good performance  for an overview  see birattari         however  since at the
start of the procedure all candidate configurations are evaluated  this approach is limited to situations
in which the space of candidate configurations can practically be enumerated  in fact  published ex   

fih utter   h oos   l eyton  b rown   s t utzle

periments with f race have been limited to applications with only around      configurations  a
recent extension presented by balaprakash et al         iteratively performs f race on subsets of
parameter configurations  this approach scales better to large configuration spaces  but the version
described by balaprakash et al         handles only algorithms with numerical parameters 
    related algorithm configuration problems
up to this point  we have focused on the problem of finding the best algorithm configuration for
an entire set  or distribution  of problem instances  related approaches attempt to find the best
configuration or algorithm on a per instance basis  or to adapt algorithm parameters during the
execution of an algorithm  approaches for setting parameters on a per instance basis have been
described by patterson and kautz         cavazos and oboyle         and hutter et al         
furthermore  approaches that attempt to select the best algorithm on a per instance basis have been
studied by leyton brown  nudelman and shoham         carchrae and beck         gebruers 
hnich  bridge and freuder         gagliolo and schmidhuber         and xu  hutter  hoos and
leyton brown         in other related work  decisions about when to restart an algorithm are made
online  during the run of an algorithm  horvitz  ruan  gomes  kautz  selman   chickering       
kautz  horvitz  ruan  gomes   selman        gagliolo   schmidhuber         so called reactive
search methods perform online parameter modifications  battiti  brunato   mascia         this
last strategy can be seen as complementary to our work  even reactive search methods tend to have
parameters that remain fixed during the search and can hence be configured using offline approaches
such as paramils 
    relation to other local search methods
since paramils performs an iterated local search with a one exchange neighbourhood  it is very
similar in spirit to local search methods for other problems  such as sat  selman  levesque  
mitchell        hoos   stutzle         csp  minton  johnston  philips   laird         and mpe
 kask   dechter        hutter et al          since paramils is a local search method  existing
theoretical frameworks  see  e g   hoos        mengshoel         could in principle be used for
its analysis  the main factor distinguishing our problem from the ones faced by standard local
search algorithms is the stochastic nature of our optimization problem  for a discussion of local
search for stochastic optimization  see  e g   spall         furthermore  there exists no compact
representation of the objective function that could be used to guide the search  to illustrate this 
consider local search for sat  where the candidate variables to be flipped can be limited to those
occurring in currently unsatisfied clauses  in general algorithm configuration  on the other hand 
such a mechanism cannot be used  because the only information available about the target algorithm
is its performance in the runs executed so far  while  obviously  other  stochastic  local search
methods could be used as the basis for algorithm configuration procedures  we chose iterated local
search  mainly because of its conceptual simplicity and flexibility 

    discussion  conclusions and future work
in this work  we studied the problem of automatically configuring the parameters of complex 
heuristic algorithms in order to optimize performance on a given set of benchmark instances  we
extended our earlier algorithm configuration procedure  paramils  with a new capping mechanism
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

and obtained excellent results when applying the resulting enhanced version of paramils to two
high performance sat algorithms as well as to c plex and a wide range of benchmark sets 
compared to the carefully chosen default configurations of these target algorithms  the parameter configurations found by paramils almost always performed much better when evaluated on
sets of previously unseen test instances  for some configuration scenarios by as much as two orders
of magnitude  the improvements over c plexs default parameter configuration are particularly
noteworthy  though we do not claim to have found a new parameter configuration for c plex that
is uniformly better than its default  rather  given a somewhat homogeneous instance set  we find a
configuration specific to that set that typically outperforms the default  sometimes by a factor as high
as     note that we achieved these results even though we are not intimately familiar with c plex
and its parameters  we chose the parameters to optimize as well as the values to consider based
on a single person day of studying the c plex user manual  the success of automated algorithm
configuration even under these extreme conditions demonstrates the potential of the approach 
the paramils source code and executable are freely available at
http   www cs ubc ca labs beta projects paramils  
along with a quickstart guide and data for the configuration scenarios studied in this article  
in order to apply paramils  or other such automated algorithm configuration methods  a practitioner must supply the following ingredients 
 a parameterized algorithm a it must be possible to set as configurable parameters externally  e g   in a command line call  often  a search for hard coded parameters hidden in the
algorithms source code can lead to a large number of additional parameters to be exposed 
 domains for the parameters algorithm configurators must be provided with the allowable
values for each parameter  depending on the configurator  it may be possible to include additional knowledge about dependencies between parameters  such as the conditional parameters
supported by paramils  for the use of paramils  numerical parameters must be discretized
to a finite number of choices  depending on the type of parameter  a uniform spacing of
values or some other spacing  such as uniform on a log scale  is typically reasonable 
 a set of problem instances the more homogeneous the problem set of interest is  the better
we can expect any algorithm configuration procedure to perform on it  while it is possible
to configure an algorithm for good performance on rather heterogeneous instance sets  e g  
on industrial sat instances  as we did with s pear as reported in section       the results for
homogeneous subsets of interest will improve when we configure on instances from that subset  whenever possible  the set of instances should be split into disjoint training and test sets
in order to safeguard against over tuning  when configuring on a small and or heterogeneous
benchmark set  paramils  or any other configuration procedure  might not find configurations that perform well on an independent test set 
 an objective function while we used median performance in our first study on paramils
 hutter et al          we have since found cases where optimizing median performance led
to parameter configurations with good median but poor overall performance  in these cases 
optimizing for mean performance yielded more robust parameter configurations  however 
when optimizing mean performance one has to define the cost for unsuccessful runs  in this
article  we have penalized such runs by counting them as ten times the cutoff time  how to
deal with unsuccessful runs in a more principled manner is an open research question 
   paramils continues to be actively developed  it is currently maintained by chris fawcett 

   

fih utter   h oos   l eyton  b rown   s t utzle

 a cutoff time for unsuccessful runs the smaller the cutoff time for each run of the target
algorithm is chosen  the more quickly any configuration procedure will be able to explore the
configuration space  however  choosing too small a cutoff risks the failure mode we experienced with our c p l e x  qp scenario  recall that there  choosing     seconds as a timeout
yielded a parameter configuration that was very good when judged with that cutoff time  see
figure   f    but performed poorly for longer cutoffs  see figure   e    in all of our other experiments  parameter configurations performing well with low cutoff times turned out to scale
well to harder problem instances as well  in many configuration scenarios  in fact  we noticed
that our automatically found parameter configurations showed much better scaling behaviour
than the default configuration  we attribute this to our use of mean runtime as a configuration
objective  the mean is often dominated by the hardest instances in a distribution  however  in
manual tuning  algorithm developers typically pay more attention to easier instances  simply
because repeated profiling on hard instances takes too long  in contrast  a patient automatic
configurator can achieve better results because it avoids this bias 
 computational resources the amount of  computational  time required for the application
of automated algorithm configuration clearly depends on the target application  if the target
algorithm takes seconds to solve instances from a homogeneous benchmark set of interest 
in our experience a single five hour configuration run will suffice to yield good results and
for some domains we have achieved good results with configuration times as short as half an
hour  in contrast  if runs of the target algorithm are slow and only performance with a large
cutoff time can be expected to yield good results on the instances of interest  then the time
requirements of automated algorithm configuration grow  we also regularly perform multiple
parallel configuration runs and pick the one with best training performance in order to deal
with variance across configuration runs 
overall  we firmly believe that automated algorithm configuration methods such as paramils
will play an increasingly prominent role in the development of high performance algorithms and
their applications  the study of such methods is a rich and fruitful research area with many interesting questions remaining to be explored 
in ongoing work  we are currently developing methods that adaptively adjust the domains of
integer valued and continuous parameters during the configuration process  similarly  we plan to
enhance paramils with dedicated methods for dealing with continuous parameters that do not require discretization by the user  another direction for further development concerns the strategic
selection of problem instances used during evaluation of configurations and of instance specific cutoff times used in this context  by heuristically preventing the configuration procedure from spending inordinate amounts of time trying to evaluate poor parameter settings on very hard problem
instances  it should be possible to improve its scalability 
we believe that there is significant room for combining aspects of the methods studied here with
concepts from related work on this and similar algorithm configuration problems  in particular 
we believe it would be fruitful to integrate statistical testing methodsas used  e g   in f race
into paramils  furthermore  we see much potential in the use of response surface models and
active learning  and believe these can be combined with our approach  finally  while the algorithm
configuration problem studied in this article is of significant practical importance  there is also much
to be gained from studying methods for related problems  in particular  instance specific algorithm
configuration and the online adjustment of parameters during the run of an algorithm 
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

acknowledgments
we thank kevin murphy for many helpful discussions regarding this work  we also thank domagoj
babic  the author of s pear  and dave tompkins  the author of the ubcsat s aps implementation
we used in our experiments  we thank the researchers who provided the instances or instance
generators used in our work  in particular gent et al          gomes and selman         leytonbrown et al          babic and hu         zarpas         le berre and simon         akturk
et al          atamturk and munoz         atamturk         and andronescu et al          lin
xu created the specific sets of qcp and swgcp instances we used  thanks also to chris fawcett
and ashique khudabukhsh for their comments on a draft of this article  finally  we thank the
anonymous reviewers as well as rina dechter and adele howe for their valuable feedback  thomas
stutzle acknowledges support from the f r s  fnrs  of which he is a research associate  holger
hoos acknowledges support through nserc discovery grant        

references
adenso diaz  b    laguna  m          fine tuning of algorithms using fractional experimental design and
local search  operations research               
akturk  s  m   atamturk  a     gurel  s          a strong conic quadratic reformulation for machine job
assignment with controllable processing times  research report bcol        university of californiaberkeley 
andronescu  m   condon  a   hoos  h  h   mathews  d  h     murphy  k  p          efficient parameter
estimation for rna secondary structure prediction  bioinformatics      i  i   
atamturk  a          on the facets of the mixedinteger knapsack polyhedron  mathematical programming 
           
atamturk  a    munoz  j  c          a study of the lot sizing polytope  mathematical programming     
       
audet  c    orban  d          finding optimal algorithmic parameters using the mesh adaptive direct search
algorithm  siam journal on optimization                
babic  d    hu  a  j          structural abstraction of software verification conditions  in w  damm  h  h 
 ed    computer aided verification    th international conference  cav       volume      of lecture
notes in computer science   pp           springer verlag  berlin  germany 
balaprakash  p   birattari  m     stutzle  t          improvement strategies for the f race algorithm  sampling design and iterative refinement  in bartz beielstein  t   aguilera  m  j  b   blum  c   naujoks 
b   roli  a   rudolph  g     sampels  m   eds     th international workshop on hybrid metaheuristics  mh      pp          
bartz beielstein  t          experimental research in evolutionary computation  the new experimentalism  natural computing series  springer verlag  berlin  germany 
battiti  r   brunato  m     mascia  f          reactive search and intelligent optimization  volume    of
operations research computer science interfaces  springer verlag  available online at http   reactivesearch org thebook 
birattari  m          the problem of tuning metaheuristics as seen from a machine learning perspective 
phd thesis  universite libre de bruxelles  brussels  belgium 
birattari  m   stutzle  t   paquete  l     varrentrapp  k          a racing algorithm for configuring metaheuristics  in langdon  w  b   cantu paz  e   mathias  k   roy  r   davis  d   poli  r   balakrishnan  k  
honavar  v   rudolph  g   wegener  j   bull  l   potter  m  a   schultz  a  c   miller  j  f   burke  e  
  jonoska  n   eds    proceedings of the genetic and evolutionary computation conference  gecco        pp         morgan kaufmann publishers  san francisco  ca  usa 
   

fih utter   h oos   l eyton  b rown   s t utzle

carchrae  t    beck  j  c          applying machine learning to low knowledge control of optimization
algorithms  computational intelligence                
cavazos  j    oboyle  m  f  p          method specific dynamic compilation using logistic regression  in
cook  w  r   ed    proceedings of the acm sigplan international conference on object oriented programming  systems  languages  and applications  oopsla       pp            new york  ny  usa 
acm press 
coy  s  p   golden  b  l   runger  g  c     wasil  e  a          using experimental design to find effective
parameter settings for heuristics  journal of heuristics             
diao  y   eskesen  f   froehlich  s   hellerstein  j  l   spainhower  l     surendra  m          generic online
optimization of multiple configuration parameters with application to a database server  in brunner  m   
keller  a   eds      th ifip ieee international workshop on distributed systems  operations and management  dsom      volume      of lecture notes in computer science   pp        springer verlag 
berlin  germany 
fawcett  c   hoos  h  h     chiarandini  m          an automatically configured modular algorithm for post
enrollment course timetabling  technical report tr          university of british columbia  department
of computer science 
gagliolo  m    schmidhuber  j          dynamic algorithm portfolios  in amato  c   bernstein  d     zilberstein  s   eds    ninth international symposium on artificial intelligence and mathematics  ai math     
gagliolo  m    schmidhuber  j          learning restart strategies  in veloso  m  m   ed    proceedings
of the twentieth international joint conference on artificial intelligence  ijcai     volume     pp     
      morgan kaufmann publishers  san francisco  ca  usa 
gebruers  c   hnich  b   bridge  d     freuder  e          using cbr to select solution strategies in constraint programming  in munoz avila  h    ricci  f   eds    proceedings of the  th international conference on case based reasoning  iccbr     volume      of lecture notes in computer science   pp 
         springer verlag  berlin  germany 
gent  i  p   hoos  h  h   prosser  p     walsh  t          morphing  combining structure and randomness 
in hendler  j    subramanian  d   eds    proceedings of the sixteenth national conference on artificial
intelligence  aaai      pp            orlando  florida  aaai press   the mit press  menlo park  ca 
usa 
gomes  c  p    selman  b          problem structure in the presence of perturbations  in kuipers  b   
webber  b   eds    proceedings of the fourteenth national conference on artificial intelligence  aaai    
 pp           aaai press   the mit press  menlo park  ca  usa 
gratch  j    chien  s  a          adaptive problem solving for large scale scheduling problems  a case
study  journal of artificial intelligence research            
gratch  j    dejong  g          composer  a probabilistic solution to the utility problem in speed up
learning  in rosenbloom  p    szolovits  p   eds    proceedings of the tenth national conference on
artificial intelligence  aaai      pp           aaai press   the mit press  menlo park  ca  usa 
hoos  h  h          a mixture model for the behaviour of sls algorithms for sat  in proceedings of the
eighteenth national conference on artificial intelligence  aaai       pp            edmonton  alberta 
canada 
hoos  h  h          computer aided design of high performance algorithms  technical report tr         
university of british columbia  department of computer science 
hoos  h  h    stutzle  t          stochastic local search  foundations   applications  morgan kaufmann
publishers  san francisco  ca  usa 
horvitz  e   ruan  y   gomes  c  p   kautz  h   selman  b     chickering  d  m          a bayesian
approach to tackling hard computational problems  in breese  j  s    koller  d   eds    proceedings
of the seventeenth conference on uncertainty in artificial intelligence  uai      pp           morgan
kaufmann publishers  san francisco  ca  usa 
   

fiparam ils  a n automatic a lgorithm c onfiguration f ramework

hutter  f          automated configuration of algorithms for solving hard computational problems  phd
thesis  university of british columbia  department of computer science  vancouver  canada 
hutter  f   babic  d   hoos  h  h     hu  a  j          boosting verification by automatic tuning of decision
procedures  in proceedings of formal methods in computer aided design  fmcad      pp         
washington  dc  usa  ieee computer society 
hutter  f   hamadi  y   hoos  h  h     leyton brown  k          performance prediction and automated
tuning of randomized and parametric algorithms  in benhamou  f   ed    principles and practice of constraint programming  cp       twelfth international conference  volume      of lecture notes in
computer science   pp           springer verlag  berlin  germany 
hutter  f   hoos  h     leyton brown  k          tradeoffs in the empirical evaluation of competing algorithm designs  technical report tr          university of british columbia  department of computer
science 
hutter  f   hoos  h  h   leyton brown  k     murphy  k  p          an experimental investigation of
model based parameter optimisation  spo and beyond  in proceedings of the genetic and evolutionary
computation conference  gecco         pp          
hutter  f   hoos  h  h     stutzle  t          efficient stochastic local search for mpe solving  in proceedings
of the nineteenth international joint conference on artificial intelligence  ijcai      pp          
hutter  f   hoos  h  h     stutzle  t          automatic algorithm configuration based on local search 
in howe  a    holte  r  c   eds    proceedings of the twenty second national conference on artificial
intelligence  aaai      pp             aaai press   the mit press  menlo park  ca  usa 
hutter  f   tompkins  d  a  d     hoos  h  h          scaling and probabilistic smoothing  efficient dynamic
local search for sat  in hentenryck  p  v   ed    principles and practice of constraint programming 
cp       eighth international conference  volume      of lecture notes in computer science   pp 
         springer verlag  berlin  germany 
johnson  d  s          a theoreticians guide to the experimental analysis of algorithms  in goldwasser 
m  h   johnson  d  s     mcgeoch  c  c   eds    data structures  near neighbor searches  and methodology  fifth and sixth dimacs implementation challenges   pp           american mathematical society  providence  ri  usa 
jones  d  r   schonlau  m     welch  w  j          efficient global optimization of expensive black box
functions  journal of global optimization             
kask  k    dechter  r          stochastic local search for bayesian networks  in the seventh international
workshop on artificial intelligence and statistics  aistats    
kautz  h   horvitz  e   ruan  y   gomes  c  p     selman  b          dynamic restart policies  in dechter 
r   kearns  m     sutton  r   eds    proceedings of the eighteenth national conference on artificial
intelligence  aaai      pp           aaai press   the mit press  menlo park  ca  usa 
khudabukhsh  a   xu  l   hoos  h  h     leyton brown  k          satenstein  automatically building local search sat solvers from components  in proceedings of the twenty first international joint conference
on artificial intelligence  ijcai      pp          
le berre  d    simon  l          fifty five solvers in vancouver  the sat      competition  in hoos  h  h 
  mitchell  d  g   eds    theory and applications of satisfiability testing  proceedings of the seventh
international conference  sat     volume      of lecture notes in computer science   pp          
springer verlag 
leyton brown  k   nudelman  e     shoham  y          learning the empirical hardness of optimization
problems  the case of combinatorial auctions  in hentenryck  p  v   ed    principles and practice of
constraint programming  cp       eighth international conference  volume      of lecture notes in
computer science   pp           springer verlag  berlin  germany 
leyton brown  k   pearson  m     shoham  y          towards a universal test suite for combinatorial
auction algorithms  in jhingran  a   mason  j  m     tygar  d   eds    ec     proceedings of the  nd
   

fih utter   h oos   l eyton  b rown   s t utzle

acm conference on electronic commerce   pp          new york  ny  usa  acm 
lourenco  h  r   martin  o     stutzle  t          iterated local search  in f  glover   g  kochenberger
 eds    handbook of metaheuristics  pp           kluwer academic publishers  norwell  ma  usa 
maron  o    moore  a          hoeffding races  accelerating model selection search for classification
and function approximation  in cowan  j  d   tesauro  g     alspector  j   eds    advances in neural
information processing systems    nips      volume     pp         morgan kaufmann publishers  san
francisco  ca  usa 
mengshoel  o  j          understanding the role of noise in stochastic local search  analysis and experiments  artificial intelligence                   
minton  s          an analytic learning system for specializing heuristics  in bajcsy  r   ed    proceedings of
the thirteenth international joint conference on artificial intelligence  ijcai      pp           morgan
kaufmann publishers  san francisco  ca  usa 
minton  s          automatically configuring constraint satisfaction programs  a case study  constraints 
          
minton  s   johnston  m  d   philips  a  b     laird  p          minimizing conflicts  a heuristic repair
method for constraint satisfaction and scheduling problems  artificial intelligence                
patterson  d  j    kautz  h          auto walksat  a self tuning implementation of walksat  in electronic
notes in discrete mathematics  endm     
ridge  e    kudenko  d          sequential experiment designs for screening and tuning parameters of
stochastic heuristics  in paquete  l   chiarandini  m     basso  d   eds    workshop on empirical methods
for the analysis of algorithms at the ninth international conference on parallel problem solving from
nature  ppsn    pp        
santner  t  j   williams  b  j     notz  w  i          the design and analysis of computer experiments 
springer verlag  new york 
selman  b   levesque  h  j     mitchell  d          a new method for solving hard satisfiability problems 
in rosenbloom  p    szolovits  p   eds    proceedings of the tenth national conference on artificial
intelligence  aaai      pp           aaai press   the mit press  menlo park  ca  usa 
spall  j  c          introduction to stochastic search and optimization  new york  ny  usa  john wiley  
sons  inc 
terashima marn  h   ross  p     valenzuela rendon  m          evolution of constraint satisfaction strategies in examination timetabling  in proceedings of the genetic and evolutionary computation conference
 gecco         pp           morgan kaufmann 
thachuk  c   shmygelska  a     hoos  h  h          a replica exchange monte carlo algorithm for protein
folding in the hp model  bmc bioinformatics            
tompkins  d  a  d    hoos  h  h          ubcsat  an implementation and experimentation environment
for sls algorithms for sat   max sat  in theory and applications of satisfiability testing  proceedings of the seventh international conference  sat     volume        pp           springer verlag 
berlin  germany 
xu  l   hutter  f   hoos  h  h     leyton brown  k          satzilla  portfolio based algorithm selection
for sat  journal of artificial intelligence research             
zarpas  e          benchmarking sat solvers for bounded model checking  in bacchus  f    walsh  t 
 eds    theory and applications of satisfiability testing  proceedings of the eighth international conference  sat     volume      of lecture notes in computer science   pp           springer verlag 

   

fi