journal artificial intelligence research                 

submitted        published      

content modeling using latent permutations
harr chen
s r k  branavan
regina barzilay
david r  karger

harr csail mit edu
branavan csail mit edu
regina csail mit edu
karger csail mit edu

computer science artificial intelligence laboratory
massachusetts institute technology
   vassar street  cambridge  massachusetts       usa

abstract
present novel bayesian topic model learning discourse level document structure  model leverages insights discourse theory constrain latent topic assignments way reflects underlying organization document topics  propose
global model topic selection ordering biased similar across
collection related documents  show space orderings effectively represented using distribution permutations called generalized mallows model 
apply method three complementary discourse level tasks  cross document alignment 
document segmentation  information ordering  experiments show incorporating permutation based model applications yields substantial improvements
performance previously proposed methods 

   introduction
central problem discourse analysis modeling content structure document 
structure encompasses topics addressed order topics
appear across documents single domain  modeling content structure particularly
germane domains exhibit recurrent patterns content organization  news
encyclopedia articles  models aim induce  example  articles
cities typically contain information history  economy  transportation 
descriptions history usually precede transportation 
previous work  barzilay   lee        elsner  austerweil    charniak        demonstrated content models learned raw unannotated text  useful
variety text processing tasks summarization information ordering  however  expressive power approaches limited  taking markovian view
content structure  model local constraints topic organization  shortcoming substantial since many discourse constraints described literature global
nature  graesser  gernsbacher    goldman        schiffrin  tannen    hamilton        
paper  introduce model content structure explicitly represents two
important global constraints topic selection   first constraint posits document follows progression coherent  nonrecurring topics  halliday   hasan        
following example above  constraint captures notion single topic 
   throughout paper  use topic refer interchangeably discourse unit
language model views topic 

c
    
ai access foundation  rights reserved 

fichen  branavan  barzilay    karger

history  expressed contiguous block within document  rather spread
disconnected sections  second constraint states documents domain
tend present similar topics similar orders  bartlett        wray         constraint
guides toward selecting sequences similar topic ordering  placing history transportation  constraints universal across genres human
discourse  applicable many important domains  ranging newspaper text
product reviews  
present latent topic model related documents encodes discourse
constraints positing single distribution entirety documents content ordering  specifically  represent content structure permutation topics 
naturally enforces first constraint since permutation allow topic repetition 
learn distribution permutations  employ generalized mallows model
 gmm   model concentrates probability mass permutations close canonical
permutation  permutations drawn distribution likely similar  conforming second constraint  major benefit gmm compact parameterization
using set real valued dispersion values  dispersion parameters allow model
learn strongly bias documents topic ordering toward canonical permutation  furthermore  number parameters grows linearly number topics 
thus sidestepping tractability problems typically associated large discrete space
permutations 
position gmm within larger hierarchical bayesian model explains
set related documents generated  document  model posits topic
ordering drawn gmm  set topic frequencies drawn multinomial distribution  together  draws specify documents entire topic structure 
form topic assignments textual unit  traditional topic models  words
drawn language models indexed topic  estimate model posterior 
perform gibbs sampling topic structures gmm dispersion parameters
analytically integrating remaining hidden variables 
apply model three complex document level tasks  first  alignment
task  aim discover paragraphs across different documents share topic 
experiments  permutation based model outperforms hidden topic markov
model  gruber  rosen zvi    weiss        wide margin gap averaged     percentage points f score  second  consider segmentation task  goal
partition document sequence topically coherent segments  model yields
average pk measure             percentage point improvement competitive
bayesian segmentation method take global constraints account  eisenstein   barzilay         third  apply model ordering task  is  sequencing
held set textual units coherent document  previous two applications  difference model state of the art baseline substantial 
model achieves average kendalls        compared value      
hmm based content model  barzilay   lee        
success permutation based model three complementary tasks demonstrates flexibility effectiveness  attests versatility general document
   example domain first constraint violated dialogue  texts domains follow
stack structure  allowing topics recur throughout conversation  grosz   sidner        

   

ficontent modeling using latent permutations

structure induced model  find encoding global ordering constraints
topic models makes suitable discourse level analysis  contrast local
decision approaches taken previous work  furthermore  evaluation scenarios  full model yields significantly better results simpler variants either
use fixed ordering order agnostic 
remainder paper proceeds follows  section    describe approach relates previous work topic modeling statistical discourse processing 
provide problem formulation section     followed overview content
model section      heart model distribution topic permutations 
provide background section      employing formal description
models probabilistic generative story section      section   discusses estimation models posterior distribution given example documents using collapsed gibbs
sampling procedure  techniques applying model three tasks alignment 
segmentation  ordering explained section    evaluate models performance tasks section   concluding touching upon directions
future work section    code  data sets  annotations  raw outputs
experiments available http   groups csail mit edu rbg code mallows  

   related work
describe two areas previous work related approach  algorithmic
perspective work falls broad class topic models  earlier work topic
modeling took bag words view documents  many recent approaches expanded
topic models capture structural constraints  section      describe extensions highlight differences model  linguistic side  work
relates research modeling text structure statistical discourse processing  summarize work section      drawing comparisons functionality supported
model 
    topic models
probabilistic topic models  originally developed context language modeling 
today become popular range nlp applications  text classification document browsing  topic models posit latent state variable controls generation
word  parameters estimated using approximate inference techniques
gibbs sampling variational methods  traditional topic models latent dirichlet allocation  lda   blei  ng    jordan        griffiths   steyvers         documents
treated bags words  word receives separate topic assignment words
assigned topic drawn shared language model 
bag words representation sufficient applications  many cases
structure unaware view limited  previous research considered extensions
lda models two orthogonal directions  covering intrasentential extrasentential
constraints 

   

fichen  branavan  barzilay    karger

      modeling intrasentential constraints
one promising direction improving topic models augment constraints
topic assignments adjoining words within sentences  example  griffiths  steyvers 
blei  tenenbaum        propose model jointly incorporates syntactic
semantic information unified generative framework constrains syntactic classes
adjacent words  approach  generation word controlled two hidden
variables  one specifying semantic topic specifying syntactic class 
syntactic class hidden variables chained together markov model  whereas semantic
topic assignments assumed independent every word 
another example intrasentential constraints  wallach        proposes way
incorporate word order information  form bigrams  lda style model 
approach  generation word conditioned previous word
topic current word  word topics generated perdocument topic distributions lda  formulation models text structure
level word transitions  opposed work griffiths et al         structure
modeled level hidden syntactic class transitions 
focus modeling high level document structure terms semantic content 
such  work complementary methods impose structure intrasentential
units  possible combine model constraints adjoining words 
      modeling extrasentential constraints
given intuitive connection notion topic lda notion topic
discourse analysis  natural assume lda like models useful discourselevel tasks segmentation topic classification  hypothesis motivated research
models topic assignment guided structural considerations  purver  kording 
griffiths    tenenbaum        gruber et al         titov   mcdonald         particularly
relationships topics adjacent textual units  depending application 
textual unit may sentence  paragraph  speaker utterance  common property
models bias topic assignments cohere within local segments text 
models category vary terms mechanisms used encourage local topic
coherence  instance  model purver et al         biases topic distributions
adjacent utterances  textual units  similar  model generates utterance
mixture topic language models  parameters topic mixture distribution assumed follow type markovian transition process specifically  high
probability utterance u topic distribution previous utterance
u    otherwise  new topic distribution drawn u  thus  textual units topic
distribution depends previous textual unit  controlled parameter indicating
whether new topic distribution drawn 
similar vein  hidden topic markov model  htmm   gruber et al         posits
generative process sentence  textual unit  assigned single topic 
sentences words drawn single language model  model
purver et al   topic transitions adjacent textual units modeled markovian
fashion specifically  sentence topic sentence   high probability 
receives new topic assignment drawn shared topic multinomial distribution 

   

ficontent modeling using latent permutations

htmm model  assumption single topic per textual unit allows
sections text related across documents topic  contrast  purver et al s model
tailored task segmentation  utterance drawn mixture topics 
thus  model capture utterances topically aligned across related
documents  importantly  htmm model purver et al  able
make local decisions regarding topic transitions  thus difficulty respecting longrange discourse constraints topic contiguity  model instead takes global view
topic assignments textual units explicitly generating entire documents topic
ordering one joint distribution  show later paper  global view yields
significant performance gains 
recent multi grain latent dirichlet allocation  mglda  model  titov   mcdonald        studied topic assignments level sub document textual units 
mglda  set local topic distributions induced sentence  dependent
window local context around sentence  individual words drawn either
local topics document level topics standard lda  mglda represents
local context using sliding window  window frame comprises overlapping short
spans sentences  way  local topic distributions shared sentences
close proximity 
mglda represent complex topical dependencies models purver
et al  gruber et al   window incorporate much wider swath local
context two adjacent textual units  however  mglda unable encode longer
range constraints  contiguity ordering similarity  sentences close
proximity loosely connected series intervening window frames 
contrast  work specifically oriented toward long range constraints  necessitating
whole document notion topic assignment 
    modeling ordering constraints statistical discourse analysis
global constraints encoded model closely related research discourse
information ordering applications text summarization generation  barzilay 
elhadad    mckeown        lapata        karamanis  poesio  mellish    oberlander       
elsner et al          emphasis body work learning ordering constraints
data  goal reordering new text domain  methods build
assumption recurring patterns topic ordering discovered analyzing
patterns word distribution  key distinction prior methods approach
existing ordering models largely driven local constraints limited ability
capture global structure  below  describe two main classes probabilistic ordering
models studied discourse processing 
      discriminative models
discriminative approaches aim directly predict ordering given set sentences 
modeling ordering sentences simultaneously leads complex structure prediction
problem  practice  however  computationally tractable two step approach taken 
first  probabilistic models used estimate pairwise sentence ordering preferences  next 
local decisions combined produce consistent global ordering  lapata       

   

fichen  branavan  barzilay    karger

althaus  karamanis    koller         training data pairwise models constructed
considering pairs sentences document  supervision labels based
actually ordered  prior work demonstrated wide range features
useful classification decisions  lapata        karamanis et al         ji  
pulman        bollegala  okazaki    ishizuka         instance  lapata       
demonstrated lexical features  verb pairs input sentences  serve
proxy plausible sequences actions  thus effective predictors well formed
orderings  second stage  local decisions integrated global order
maximizes number consistent pairwise classifications  since finding
ordering np hard  cohen  schapire    singer         various approximations used
practice  lapata        althaus et al         
two step discriminative approaches effectively leverage information
local transitions  provide means representing global constraints 
recent work  barzilay lapata        demonstrated certain global properties captured discriminative framework using reranking mechanism 
set up  system learns identify best global ordering given set n possible
candidate orderings  accuracy ranking approach greatly depends quality
selected candidates  identifying candidates challenging task given large
search space possible alternatives 
approach presented work differs existing discriminative models two
ways  first  model represents distribution possible global orderings  thus 
use sampling mechanisms consider whole space rather limited
subset candidates ranking models  second difference arises
generative nature model  rather focusing ordering task  order aware
model effectively captures layer hidden variables explain underlying structure
document content  thus  effectively applied wider variety applications 
including sentence ordering already observed  appropriately adjusting
observed hidden components model 
      generative models
work closer technique generative models treat topics hidden variables 
one instance work hidden markov model  hmm  based content model  barzilay   lee         model  states correspond topics state transitions represent
ordering preferences  hidden states emission distribution language model
words  thus  similar approach  models implicitly represent patterns
level topical structure  hmm used ranking framework select
ordering highest probability 
recent work  elsner et al         developed search procedure based simulated annealing finds high likelihood ordering  contrast ranking based approaches  search procedure cover entire ordering space  hand 
show section      define ordering objective maximized
efficiently possible orderings prediction model parameters
learned  specifically  bag p paragraphs  o pk  calculations paragraph
probabilities necessary  k number topics 

   

ficontent modeling using latent permutations

another distinction proposed model prior work way global
ordering constraints encoded  markovian model  possible induce global
constraints introducing additional local constraints  instance  topic contiguity
enforced selecting appropriate model topology  e g   augmenting hidden states
record previously visited states   however  global constraints  similarity
overall ordering across documents  much challenging represent  explicitly
modeling topic permutation distribution  easily capture kind global
constraint  ultimately resulting accurate topic models orderings  show
later paper  model substantially outperforms approach barzilay lee
information ordering task applied hmm based content model 

   model
section  describe problem formulation proposed model 
    problem formulation
content modeling problem formalized follows  take input corpus
 d          dd   related documents  specification number topics k  
document comprised ordered sequence nd paragraphs  pd             pd nd   
output  predict single topic assignment zd p             k  paragraph p  
z values reflect underlying content organization document
related content discussed within document  across separate documents 
receive z value 
formulation shares similarity standard lda setup common
set topics assigned across collection documents  difference lda
words topic assignment conditionally independent  following bag words view
documents  whereas constraints topics assigned let us connect word
distributional patterns document level topic structure 
    model overview
propose generative bayesian model explains corpus documents
produced set hidden variables  high level  model first selects
frequently topic expressed document  topics ordered 
topics determine selection words paragraph  notation used
subsequent sections summarized figure   
document nd paragraphs  separately generate bag topics td
topic ordering   unordered bag topics td   contains nd elements  expresses
many paragraphs document assigned k topics  equivalently 
td viewed vector occurrence counts topic  zero counts
topics appear all  variable td constructed taking nd samples
   nonparametric extension model would learn k 
   well structured documents  paragraphs tend internally topically consistent  halliday   hasan 
       predicting one topic per paragraph sufficient  however  note approach
applied modifications levels textual granularity sentences 

   

fichen  branavan  barzilay    karger



parameters distribution
topic counts



parameters distribution
topic orderings



vector topic counts

v

vector inversion counts



topic ordering

z

paragraph topic assignment



language model parameters
topic

dirichlet    
j           k   
j gmm          
k           k 
k dirichlet    

td
vd
 
zd  

w document words

document
multinomial  
gmm  
compute  vd  
compute z td    

paragraph p
word w p
w multinomial zd p  

k number topics
number documents corpus
nd number paragraphs
document
np number words paragraph p

algorithm  compute 
input  inversion count vector v

algorithm  compute z
input  topic counts t  permutation

create empty list
    k
j   k    
  k   v j 
 i       i 
 v j   j

create empty list z
end  
k   k  
    t  k  
z end   k 
end end    

output  permutation

output  paragraph topic vector z

figure    plate diagram generative process model  along table
notation reference purposes  shaded circles figure denote observed
variables  squares denote hyperparameters  dotted arrows indicate
constructed deterministically v according algorithm compute  
z constructed deterministically according compute z 
   

ficontent modeling using latent permutations

distribution topics   multinomial representing probability topic
expressed  sharing documents captures notion certain topics
likely across documents corpus 
topic ordering variable permutation numbers   k
defines order topics appear document  draw generalized
mallows model  distribution permutations explain section     
see  particular distribution biases permutation selection close single
centroid  reflecting discourse constraint preferring similar topic structures across
documents 
together  documents bag topics td ordering determine topic assignment
zd p paragraphs  example  corpus k      seven paragraph
document td                                        would induce topic sequence
zd                          induced topic sequence zd never assign topic
two unconnected portions document  thus satisfying constraint topic contiguity 
assume topic k associated language model k   words
paragraph assigned topic k drawn topics language model k  
portion similar standard lda topic relates language model 
however  unlike lda  model enforces topic coherence entire paragraph rather
viewing paragraph mixture topics 
turning formal discussion generative process  first provide
background permutation model topic ordering 
    generalized mallows model permutations
central challenge approach presented modeling distribution possible topic orderings  purpose use generalized mallows model  gmm   fligner
  verducci        lebanon   lafferty        meila  phadnis  patterson    bilmes       
klementiev  roth    small         exhibits two appealing properties context
task  first  model concentrates probability mass canonical ordering
small perturbations  permutations  ordering  characteristic matches
constraint documents domain exhibit structural similarity  second 
parameter set scales linearly number elements ordered  making
sufficiently constrained tractable inference 
first describe standard mallows model orderings  mallows        
mallows model takes two parameters  canonical ordering dispersion parameter  
sets probability ordering proportional ed     
d     represents distance metric orderings   frequently  metric
kendall distance  minimum number swaps adjacent elements needed
transform ordering canonical ordering   thus  orderings close
canonical ordering high probability  many elements
moved less probability mass 
generalized mallows model  first introduced fligner verducci         refines
standard mallows model adding additional set dispersion parameters 
parameters break apart distance d     orderings set independent
components  component separately vary sensitivity perturbation 

   

fichen  branavan  barzilay    karger

tease apart distance function components  gmm distribution considers
inversions required transform canonical ordering observed ordering  first
discuss inversions parameterized gmm  turn distributions
definition characteristics 
      inversion representation permutations
typically  permutations represented directly ordered sequence elements
example            represents permuting initial order placing third element
first  followed first element  second  gmm utilizes alternative
permutation representation defined vector  v            vk    inversion counts
respect identity permutation             k   term vj counts number times
value greater j appears j permutation  note jth inversion
count vj take integer values   k j inclusive  thus inversion count
vector k   elements  vk always zero  instance  given standard
form permutation                     v              appear    v     
numbers appear it  entire inversion count vector would                 
likewise  previous example permutation              maps inversion counts           
sum components entire inversion count vector simply orderings
kendall distance canonical ordering 
significant appeal inversion representation every valid  distinct vector
inversion counts corresponds distinct permutation vice versa  see this 
note permutation straightforwardly compute inversion counts 
conversely  given sequence inversion counts  construct unique corresponding
permutation  insert items permutation  working backwards item k 
assume already placed items j     k proper order  insert
item j  note exactly vj items j     k must precede it  meaning
must inserted position vj current order  see compute  algorithm
figure     since one place j inserted fulfills inversion
counts  induction shows exactly one permutation constructed satisfy
given inversion counts 
model  take canonical topic ordering always identity ordering
            k   topic numbers task completely symmetric linked
extrinsic meaning  fixing global ordering specific arbitrary value
sacrifice representational power  general case gmm  canonical ordering
parameter distribution 
      probability mass function
gmm assigns probability mass particular order based order permuted canonical ordering  precisely  associates distance every
permutation  canonical ordering distance zero permutations many
inversions respect canonical ordering larger distance  distance assignment based k   real valued dispersion parameters p
             k     distance
permutation inversion counts v defined j j vj   gmms probability

   

ficontent modeling using latent permutations

mass function exponential distance 
p

e j j vj
gmm v     
  
 

k 

j  

    

q

j

ej vj
 
j  j  

   

j  j   normalization factor value 
j  j    

  e kj   j
 
  ej

   

setting j equal single value recovers standard mallows model kendall
distance function  factorization gmm independent probabilities per
inversion count makes distribution particularly easy apply  use gmmj refer
jth multiplicand probability mass function  marginal distribution
vj  
gmmj  vj   j    

ej vj
 
j  j  

   

due exponential form distribution  requiring j     constrains gmm
assign highest probability mass vj zero  i e   distributional mode
canonical identity permutation  higher value j assigns probability mass vj
close zero  biasing j fewer inversions 
      conjugate prior
major benefit gmm membership exponential family distributions 
means particularly amenable bayesian representation  admits
natural independent conjugate prior parameter j  fligner   verducci        
gmm   j   vj         e j vj   log j  j      

   

prior distribution takes two parameters   vj     intuitively  prior states
  previous trials  total number inversions observed   vj     distribution
easily updated observed vj derive posterior distribution 
vj different range  inconvenient set prior hyperparameters
vj   directly  work  instead assign common prior value parameter j  
denote     set vj   maximum likelihood estimate
j     differentiating likelihood gmm respect j   straightforward
verify works setting 
vj    

e 

k j  
 
 kj   
 
   
  e

   

   

fichen  branavan  barzilay    karger

    formal generative process
fully specify details content model  whose plate diagram appears
figure    observe corpus documents  document ordered
sequence nd paragraphs paragraph represented bag words  number
topics k assumed pre specified  model induces set hidden variables
probabilistically explain words corpus produced  final desired
output posterior distributions paragraphs hidden topic assignment variables 
following  variables subscripted   fixed prior hyperparameters 
   topic k  draw language model k dirichlet      lda 
topic specific word distributions 
   draw topic distribution dirichlet      expresses likely topic
appear regardless position 
   draw topic ordering distribution parameters j gmm           j    
k    parameters control rapidly probability mass decays
inversions topic  separate j every topic allows us learn
topics likely reordered others 
   document nd paragraphs 
 a  draw bag topics td sampling nd times multinomial   
 b  draw topic ordering   sampling vector inversion counts vd gmm   
applying algorithm compute  figure   vd  
 c  compute vector topic assignments zd document ds paragraphs
sorting td according   algorithm compute z figure    
 d  paragraph p document d 
i  sample word w p according language model p  w
multinomial zd p   
    properties model
section describe rationale behind using gmm represent ordering
component content model 
representational power gmm concentrates probability mass around one centroid permutation  reflecting preferred bias toward document structures similar topic orderings  furthermore  parameterization gmm using vector
dispersion parameters allows flexibility strongly model biases toward
single ordering one extreme       one ordering nonzero probability         orderings equally likely  comprised
   multiple permutations contribute probability single documents topic assignments zd  
topics appear td   result  current formulation biased toward
assignments fewer topics per document  practice  find negatively impact model
performance 

   

ficontent modeling using latent permutations

independent dispersion parameters              k     distribution assign different penalties displacing different topics  example  may learn middle
sections  in case cities  sections economy culture  likely
vary position across documents early sections  such introduction
history  
computational benefits parameterization gmm using vector dispersion parameters compact tractable  since number parameters grows
linearly number topics  model efficiently handle longer documents
greater diversity content 
another computational advantage model seamless integration larger
bayesian model  due membership exponential family existence
conjugate prior  inference become significantly complex
gmm used hierarchical context  case  entire document generative
model accounts topic frequency words within topic 
one final beneficial effect gmm breaks symmetry topic assignments fixing distribution centroid  specifically  topic assignments
invariant relabeling  probability underlying permutation would
change  contrast  many topic models assign probability relabeling
topic assignments  model thus sidesteps problem topic identifiability  issue model may multiple maxima likelihood due
underlying symmetry hidden variables  non identifiable models
standard lda may cause sampling procedures jump maxima produce
draws difficult aggregate across runs 
finally  show section   benefits gmm extend theoretical empirical  representing permutations using gmm almost always leads
superior performance compared alternative approaches 

   inference
variables aim infer paragraph topic assignments z  determined bag topics ordering document  thus  goal estimate
joint marginal distributions given document text integrating
remaining hidden parameters 
p  t      w  
   
accomplish inference task gibbs sampling  geman   geman        bishop 
       gibbs sampler builds markov chain hidden variable state space whose
stationary distribution actual posterior joint distribution  new sample
drawn distribution single variable conditioned previous samples
variables  collapse sampler integrating hidden
variables model  effect reducing state space markov chain  collapsed
sampling previously demonstrated effective lda variants  griffiths
  steyvers        porteous  newman  ihler  asuncion  smyth    welling        titov  

   

fichen  branavan  barzilay    karger

p  td i            p  td i     t d i        p  wd   td     wd   zd      


n  t d i    t     

p  wd   z  wd       
 t d i      k 

p  vd j   v          p  vd j   v   j   p  wd   td     wd   zd      
  gmmj  v  j   p  wd   z  wd       

p


vd j   vj    
p  j            gmm  j  
  n      
n    
figure    collapsed gibbs sampling inference procedure estimating models
posterior distribution  plate diagram  variable resampled
shown double circle markov blanket highlighted black 
variables  impact variable resampled  grayed out 
variables   shown dotted circles  never explicitly depended
re estimated  marginalized sampler  diagram
accompanied conditional resampling distribution respective variable 

   

ficontent modeling using latent permutations

mcdonald         typically preferred explicit gibbs sampling hidden
variables smaller search space generally shorter mixing time 
sampler analytically integrates three sets hidden variables  bags
topics t  orderings   permutation inversion parameters   burn in period 
treat last samples draw posterior  samples
marginalized variables necessary  estimated based topic
assignments show section      figure   summarizes gibbs sampling steps
inference procedure 
document probability preliminary step  consider calculate probability
single documents words wd given documents paragraph topic assignments zd
remaining documents topic assignments  note probability decomposable product probabilities individual paragraphs paragraphs
different topics conditionally independent word probabilities  let wd zd indicate words topic assignments documents d  w vocabulary
size  probability words then 
k z

p  wd   z  wd        
p  wd   zd   k   p  k   z  wd       dk

 

k   k
k


dcm  wd i   zd i   k     wd i   zd i   k       

   

k  

dcm   refers dirichlet compound multinomial distribution  result
integrating multinomial parameters dirichlet prior  bernardo   smith        
dirichlet prior parameters                w    dcm assigns following
probability series observations x    x            xn   
p
w
  j j  
 n  x  i     
p
dcm x      q
 
   
  x    j j  
j  j  
i  

n  x  i  refers number times word appears x  here     gamma
function  generalization factorial real numbers  algebra shows
dcms posterior probability density function conditioned series observations  
 y            yn   computed updating counts often word appears
y 
dcm x   y      dcm x      n  y              w   n  y  w    

   

equations     used compute conditional distributions hidden
variables  turn individual random variable resampled 
bag topics first consider resample td i   ith topic draw document
conditioned parameters fixed  note topic ith
paragraph  reorder topics using   generated separately  
p  td i            p  td i     t d i        p  wd   td     wd   zd      


n  t d i    t     

p  wd   z  wd       
 t d i      k 
   

    

fichen  branavan  barzilay    karger

td updated reflect td i   t  zd deterministically computed last step
using compute z figure   inputs td   first step reflects application
bayes rule factor term wd   drop superfluous terms
conditioning  second step  former term arises dcm  updating
parameters   observations t d i  equation   dropping constants 
latter document probability term computed using equation    new td i selected
sampling probability computed possible topic assignments 
ordering parameterization permutation series inversion values vd j
reveals natural way decompose search space gibbs sampling  document
d  resample vd j j     k   independently successively according
conditional distribution 
p  vd j   v          p  vd j   v   j   p  wd   td     wd   zd      
  gmmj  v  j   p  wd   z  wd       

    

updated reflect vd j   v  zd computed deterministically according
td   first term refers equation    second computed using equation   
probability computed every possible value v  ranges   k j 
term vd j sampled according resulting probabilities 
gmm parameters j     k    resample j posterior distribution 
p


vd j   vj    
p  j            gmm  j  
  n      
    
n    
gmm  evaluated according equation    normalization constant
distribution unknown  meaning cannot directly compute invert cumulative distribution function sample distribution  however  distribution
univariate unimodal  expect mcmc technique slice
sampling  neal        perform well  practice  matlabs built in slice sampler
provides robust draw distribution  
computational issues inference  directly computing document probabilities
basis equation   results many redundant calculations slow runtime
iteration considerably  improve computational performance proposed
inference procedure  apply memoization techniques sampling  within
single iteration  document  gibbs sampler requires computing documents
probability given topic assignments  equation    many times  computation
frequently conditions slight variations topic assignments  nave approach
would compute probability every paragraph time document probability
desired  performing redundant calculations topic assignment sequences shared
subsequences repeatedly considered 
instead  use lazy evaluation build three dimensional cache  indexed tuple
 i  j  k   follows  time document probability requested  broken independent subspans paragraphs  subspan takes one contiguous topic assignment  possible due way equation   factorizes independent per topic
   particular  use slicesample function matlab statistics toolbox 

   

ficontent modeling using latent permutations

multiplicands  subspan starting paragraph i  ending paragraph j  assigned topic k  cache consulted using key  i  j  k   example  topic assignments
zd                         would result cache lookups                                 
cached value unavailable  correct probability computed using equation  
result stored cache location  i  j  k   moreover  record values every
intermediate cache location  i  l  k  l   j    values computed
subproblems evaluating equation    i  j  k   cache reset proceeding
next document since conditioning changes documents  document 
caching guarantees o nd  k  paragraph probability calculations 
practice  individual gibbs steps small  bound loose
caching mechanism reduces computation time several orders magnitude 
maintain caches word topic paragraph topic assignment frequencies 
allowing us rapidly compute counts used equations       form caching
used griffiths steyvers        

   applications
section  describe model applied three challenging discourselevel tasks  aligning paragraphs similar topical content documents  segmenting
document topically cohesive sections  ordering new unseen paragraphs
coherent document  particular  show posterior samples produced
inference procedure section   used derive solution tasks 
    alignment
alignment task wish find paragraphs document topically
relate paragraphs documents  essentially  cross document clustering
task alignment assigns paragraph document one k topically related
groupings  instance  given set cell phone reviews  one group may represent text
fragments discuss price  another group consists fragments reception 
model readily employed task  view topic assignment
paragraph z cluster label  example  two documents d  d 
topic assignments zd                          zd                           paragraph   d 
grouped together paragraphs     d    paragraphs     d   
  d    remaining paragraphs assigned topics     form separate
per document clusters 
previously developed methods cross document alignment primarily driven
similarity functions quantify lexical overlap textual units  barzilay   elhadad        nelken   shieber         methods explicitly model document
structure  specify global constraints guide search optimal
alignment  pairs textual units considered isolation making alignment decisions  contrast  approach allows us take advantage global structure shared
language models across related textual units without requiring manual specification
matching constraints 

   

fichen  branavan  barzilay    karger

    segmentation
segmentation well studied discourse task goal divide document
topically cohesive contiguous sections  previous approaches typically relied lexical
cohesion is  similarity word choices within document subspan guide
choice segmentation boundaries  hearst        van mulbregt  carp  gillick  lowe   
yamron        blei   moreno        utiyama   isahara        galley  mckeown  foslerlussier    jing        purver et al         malioutov   barzilay        eisenstein   barzilay 
       model relies notion determining language models topics 
connecting topics across documents constraining topics appear allow
better learn words indicative topic cohesion 
output samples models inference procedure map straightforwardly
segmentations contiguous spans paragraphs assigned topic number taken one segment  example  seven paragraph document topic
assignments zd                         would segmented three sections  comprised
paragraph    paragraphs      paragraphs      note segmentation ignores specific values used topic assignments  heeds paragraph
boundaries topic assignments change 
    ordering
third application model problem creating structured documents
collections unordered text segments  text ordering task important step
broader nlp tasks text summarization generation  task  assume
provided well structured documents single domain training examples 
trained  model used induce ordering previously unseen collections
paragraphs domain 
training  model learns canonical ordering topics documents within
collection  via language models associated topic  gmm
concentrates probability mass around canonical             k  topic ordering  expect
highly probable words language models lower  numbered topics tend appear early
document  whereas highly probable words language models higher  numbered
topics tend appear late document  thus  structure new documents according
intuition paragraphs words tied low topic numbers placed earlier
paragraphs words relating high topic numbers 
formally  given unseen document comprised unordered set paragraphs
 p            pn    order paragraphs according following procedure  first  find
probable topic assignment zi independently paragraph pi   according
parameters learned training phase 
zi   arg max p  zi   k   pi      
k

  arg max p  pi   zi   k  k  p  zi   k     

    

k

second  sort paragraphs topic assignment zi ascending order since          k 
gmms canonical ordering  yields likely ordering conditioned single
estimated topic assignment paragraph  due possible ties topic assignments 
   

ficontent modeling using latent permutations

resulting document may partial ordering  full ordering required  ties
broken arbitrarily 
key advantage proposed approach closed form computationally
efficient  though training phase requires running inference procedure section   
model parameters learned  predicting ordering new set p paragraphs
requires computing pk probability scores  contrast  previous approaches
able rank small subset possible document reorderings  barzilay  
lapata         performed search procedure space orderings find
optimum  elsner et al          
objective function equation    depends posterior estimates given
training documents  since collapsed gibbs sampler integrates two hidden
variables  need back values known posterior samples
z  easily done computing point estimate distribution based
word topic topic document assignment frequencies  respectively  done griffiths
steyvers         probability mass kw word w language model topic k
given by 
n  k  w     
kw  
 
    
n  k    w  
n  k  w  total number times word w assigned topic k  n  k 
total number words assigned topic k  according posterior sample z 
derive similar estimate k   prior likelihood topic k 
k  

n  k     
 
n   k 

    

n  k  total number paragraphs assigned topic k according sample
z  n total number paragraphs entire corpus 

   experiments
section  evaluate performance model three tasks presented
section    cross document alignment  document segmentation  information ordering 
first describe preliminaries common three tasks  covering data sets 
reference comparison structures  model variants  inference algorithm settings shared
evaluation  provide detailed examination model performs
individual task 
    general evaluation setup
data sets experiments use five data sets  briefly described  for additional
statistics  see table    
   approach describe finding probable paragraph ordering according
data likelihood  optimal ordering derived hmm based content model 
proposed ordering technique essentially approximates objective using per paragraph maximum
posteriori estimate topic assignments rather full posterior topic assignment distribution 
approximation makes much faster prediction algorithm performs well empirically 

   

fichen  branavan  barzilay    karger

articles
corpus
citiesen
citiesen   
citiesfr

large cities wikipedia
language documents sections
english
   
    
english
   
    
french
   
    

paragraphs
    
    
    

vocabulary
      
      
      

tokens
     
     
     

articles chemical elements wikipedia
corpus
language documents sections
elements
english
   
   

paragraphs
    

vocabulary
      

tokens
     

cell phone reviews phonearena com
corpus
language documents sections
phones
english
   
   

paragraphs
    

vocabulary
      

tokens
     

table    statistics data sets used evaluations  values except vocabulary
size document count per document averages 

citiesen  articles english wikipedia worlds     largest cities
population  common topics include history  culture  demographics  articles typically substantial size share similar content organization patterns 
citiesen      articles english wikipedia worlds     largest cities
population  collection superset citiesen  many lower ranked
cities well known english wikipedia editors thus  compared citiesen
articles shorter average exhibit greater variability content selection
ordering 
citiesfr   articles french wikipedia     cities citiesen 
elements  articles english wikipedia chemical elements periodic table   including topics biological role  occurrence  isotopes 
phones  reviews extracted phonearena com  popular cell phone review website  topics corpus include design  camera  interface  reviews
written expert reviewers employed site  opposed lay users  
heterogeneous collection data sets allows us examine behavior
model diverse test conditions  sets vary articles generated 
language articles written  subjects discuss  result 
patterns topic organization vary greatly across domains  instance  within phones
corpus  articles formulaic  due centralized editorial control website 
establishes consistent standards followed expert reviewers  hand 
wikipedia articles exhibit broader structural variability due collaborative nature
       elements http   en wikipedia org wiki periodic table  including undiscovered element     
   phones set     documents short express reviews without section headings  include
input model  evaluate them 

   

ficontent modeling using latent permutations

wikipedia editing  allows articles evolve independently  wikipedia articles
within category often exhibit similar section orderings  many idiosyncratic
inversions  instance  citiesen corpus  geography history sections
typically occur toward beginning document  history appear either
geography across different documents 
corpus consider manually divided sections authors 
including short textual heading section  sections              discuss
author created sections headings used generate reference annotations
alignment segmentation tasks  note use headings evaluation 
none heading information provided methods consideration 
tasks alignment segmentation  evaluation performed datasets presented
table    ordering task  however  data used training  evaluation
performed using separate held out set documents  details held out dataset
given section       
model variants evaluation  besides comparing baselines literature 
consider two variants proposed model  particular  investigate
impact mallows component model alternately relaxing tightening
way constrains topic orderings 
constrained   variant  require documents follow exact canonical ordering topics  is  topic permutation inversions allowed  though
documents may skip topics before  case viewed special case
general model  mallows inversion prior   approaches infinity 
implementation standpoint  simply fix inversion counts v zero
inference   
uniform  variant assumes uniform distribution topic permutations 
instead biasing toward small related set  again  special case full
model  inversion prior   set zero  strength prior   approaching
infinity  thus forcing item always zero 
note variants still enforce long range constraint topic contiguity 
vary full model capture topic ordering similarity 
evaluation procedure parameter settings evaluation model
variants  run collapsed gibbs sampler five random seed states  take
      th iteration chain sample  results presented average
five samples 
dirichlet prior hyperparameters bag topics   language models   set
     gmm  set prior dispersion hyperparameter      effective
    first glance  constrained model variant appears equivalent hmm state
transition either      however  case topics may appear zero times
document  resulting multiple possible transitions state  furthermore  transition
probabilities would dependent position within document example  earlier absolute
positions within document  transitions high index topics unlikely  would require
subsequent paragraphs high index topic 

   

fichen  branavan  barzilay    karger

sample size prior       times number documents  values minimally
tuned  similar results achieved alternative settings       parameters  
  control strength bias toward structural regularity  trading
constrained uniform model variants  values chosen middle ground
two extremes 
model takes parameter k controls upper bound number
latent topics  note algorithm select fewer k topics document 
k determine number segments document  general  higher k
results finer grained division document different topics  may result
precise topics  may split topics together  report results
evaluation using k         
    alignment
first evaluate model task cross document alignment  goal
group textual units different documents topically cohesive clusters  instance 
cities related domains  one cluster may include transportation related paragraphs  turning results first present details specific evaluation setup
targeted task 
      alignment evaluation setup
reference annotations generate sufficient amount reference data evaluating alignments use section headings provided authors  assume two
paragraphs aligned section headings identical  headings
constitute noisy annotations wikipedia datasets  topical content may
labeled different section headings different articles  e g   citiesen  places
interest one article landmarks another   call reference structure
noisy headings set 
clear priori effect noise section headings may evaluation accuracy  empirically estimate effect  use manually annotated
alignments experiments  specifically  citiesen corpus  manually annotated articles paragraphs consistent set section headings  providing us
additional reference structure evaluate against  clean headings set  found
approximately    topics expressed one document 
metrics quantify alignment output compute recall precision score
candidate alignment reference alignment  recall measures  unique
section heading reference  maximum number paragraphs heading
assigned one particular topic  final score computed summing
section heading dividing total number paragraphs  high recall indicates
paragraphs section headings generally assigned topic 
conversely  precision measures  topic number  maximum number paragraphs topic assignment share section heading  precision summed
topic normalized total number paragraphs  high precision means
paragraphs assigned single topic usually correspond section heading 

   

ficontent modeling using latent permutations

recall precision trade finely grained topics tend
improve precision cost recall  extremes  perfect recall occurs every
paragraph assigned topic  perfect precision paragraph
topic 
present one summary f score results  harmonic mean
recall precision 
statistical significance setup measured approximate randomization  noreen 
       nonparametric test directly applied nonlinearly computed metrics
f score  test used prior evaluations information extraction
machine translation  chinchor        riezler   maxwell        
baselines

task  compare two baselines 

hidden topic markov model  htmm   gruber et al          explained section    model represents topic change adjacent textual units markovian fashion  htmm capture local constraints  would allow topics
recur non contiguously throughout document  use publicly available implementation    priors set according recommendations made original
work 
clustering  use repeated bisection algorithm find clustering paragraphs maximizes sum pairwise cosine similarities items
cluster    clustering implemented using cluto toolkit    note
approach completely structure agnostic  treating documents bags paragraphs rather sequences paragraphs  types clustering techniques
shown deliver competitive performance cross document alignment
tasks  barzilay   elhadad        
      alignment results
table   presents results alignment evaluation  datasets  best
performance achieved model variants  statistically significant usually
substantial margin 
comparative performance baseline methods consistent across domains
surprisingly  clustering performs better complex htmm model  observation consistent previous work cross document alignment multidocument
summarization  use clustering main component  radev  jing    budzikowska 
      barzilay  mckeown    elhadad         despite fact htmm captures
dependencies adjacent paragraphs  sufficiently constrained  manual examination actual topic assignments reveals htmm often assigns topic
disconnected paragraphs within document  violating topic contiguity constraint 
one domain full gmm based approach yields best performance compared variants  one exception phone domain  constrained
    http   code google com p openhtmm 
    particular clustering technique substantially outperforms agglomerative graph partitioningbased clustering approaches task 
    http   glaros dtc umn edu gkhome views cluto 

   

fik     

k     

k     

k     

chen  branavan  barzilay    karger

clustering
htmm
constrained
uniform
model
clustering
htmm
constrained
uniform
model

citiesen
clean headings
recall
prec
f score
     
           
     
           
     
           
     
           
           
     
     
           
     
           
     
           
     
           
           
     

citiesen
noisy headings
recall
prec
f score
     
           
     
           
     
           
     
           
           
     
     
           
     
           
     
           
     
           
           
     

citiesen   
noisy headings
recall
prec
f score
     
           
     
           
     
           
     
           
           
     
     
           
     
           
     
           
     
           
           
     

clustering
htmm
constrained
uniform
model
clustering
htmm
constrained
uniform
model

citiesfr
noisy headings
recall
prec
f score
     
           
     
           
     
     
     
     
           
           
     
     
           
     
           
     
           
     
           
           
     

elements
noisy headings
recall
prec
f score
     
           
     
           
     
           
     
           
           
     
     
           
     
           
     
           
     
           
           
     

phones
noisy headings
recall
prec
f score
     
           
     
           
           
     
     
           
     
     
     
     
           
     
           
           
     
     
           
     
     
     

table    comparison alignments produced model series baselines
model variations        topics  evaluated clean noisy sets
section headings  higher scores better  within k  methods
model significantly outperforms indicated p        
p        

   

ficontent modeling using latent permutations

baseline achieves best result k small margin  results
expected  given fact domain exhibits highly rigid topic structure across
documents  model permits permutations topic ordering  gmm 
flexible highly formulaic domains 
finally  observe evaluations based manual noisy annotations exhibit
almost entirely consistent ranking methods consideration  see clean
noisy headings results citiesen table     consistency indicates noisy
headings sufficient gaining insight comparative performance different
approaches 
    segmentation
next consider task text segmentation  test whether model able
identify boundaries topically coherent text segments 
      segmentation evaluation setup
reference segmentations described section      datasets used
evaluation manually divided sections authors  annotations
used create reference segmentations evaluating models output  recall
section       built clean reference structure citiesen set  structure encodes clean segmentation document adjusts granularity
section headings consistent across documents  thus  compare
segmentation specified citiesen clean section headings 
metrics segmentation quality evaluated using standard penalty metrics pk
windowdiff  beeferman  berger    lafferty        pevzner   hearst         pass
sliding window documents compute probability words end
windows improperly segmented respect other  windowdiff
stricter  requires number segmentation boundaries endpoints
window correct well   
baselines first compare bayesseg  eisenstein   barzilay           bayesian
segmentation approach current state of the art task  interestingly 
model reduces approach every document considered completely isolation 
topic sharing documents  connecting topics across documents makes
much difficult inference problem one tackled eisenstein barzilay 
time  algorithm cannot capture structural relatedness across documents 
since bayesseg designed operated specification number segments 
provide baseline benefit knowing correct number segments
document  provided system  run baseline using
    statistical significance testing standardized usually reported segmentation task 
omit tests results 
    evaluate corpora used work  since model relies content similarity across
documents corpus 

   

fibayesseg
u i
u i
constrained
uniform
model
constrained
uniform
model

citiesen
clean headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
           
   
     
     
   
     
     
   
     
     
    
     
     
    
           
    

citiesen
noisy headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
     
     
   
     
     
   
           
   
     
     
    
     
     
    
           
    

citiesen   
noisy headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
     
     
   
     
     
   
           
   
     
     
   
     
     
    
           
    

bayesseg
u i
u i
constrained
uniform
model
constrained
uniform
model

citiesfr
noisy headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
     
     
   
           
   
           
   
     
     
   
     
     
    
           
    

elements
noisy headings
pk
wd
  segs
     
     
   
     
     
   
     
     
   
     
     
   
     
     
   
           
   
     
     
   
     
     
   
           
   

phones
noisy headings
pk
wd
  segs
     
     
   
     
     
   
     
     
   
           
   
     
     
   
     
     
   
           
    
     
     
   
     
     
    

k      k     

k      k     

chen  branavan  barzilay    karger

table    comparison segmentations produced model series baselines
model variations        topics  evaluated clean noisy
sets section headings  lower scores better  bayesseg u i given
true number segments  segments counts reflect reference structures
segmentations  contrast  u i automatically predicts number segments 

   

ficontent modeling using latent permutations

authors publicly available implementation    priors set using built in mechanism
automatically re estimates hyperparameters 
compare method algorithm utiyama isahara        
commonly used point reference evaluation segmentation algorithms 
algorithm computes optimal segmentation estimating changes predicted
language models segments different partitions  used publicly available
implementation system    require parameter tuning held out
development set  contrast bayesseg  algorithm mechanism predicting
number segments  take pre specified number segments 
comparison  consider versions algorithm u i denotes case
correct number segments provided model u i denotes model
estimates optimal number segments 
      segmentation results
table   presents segmentation experiment results  every data set model outperforms bayesseg u i baselines substantial margin regardless k  result
provides strong evidence learning connected topic models related documents leads
improved segmentation performance 
best performance generally obtained full version model  three
exceptions  two cases  citiesen k      using clean headings windowdiff
metric  citiesfr k      pk metric   variant performs better
full model minute margin  furthermore  instances 
corresponding evaluation k      using full model leads best overall
results respective domains 
case variant outperforms full model notable margin
phones data set  result unexpected given formulaic nature dataset
discussed earlier 
    ordering
final task evaluate model finding coherent ordering
set textual units  unlike previous tasks  prediction based hidden variable
distributions  ordering observed document  moreover  gmm model uses
information inference process  therefore  need divide data sets
training test portions 
past  ordering algorithms applied textual units various granularities  commonly sentences paragraphs  ordering experiments operate
level relatively larger unit sections  believe granularity suitable
nature model  captures patterns level topic distributions
rather local discourse constraints  ordering sentences paragraphs
studied past  karamanis et al         barzilay   lapata        two types
models effectively combined induce full ordering  elsner et al         
    http   groups csail mit edu rbg code bayesseg 
    http   www  nict go jp x x    members mutiyama software html textseg

   

fichen  branavan  barzilay    karger

corpus
citiesen
citiesfr
phones

set
training
testing
training
testing
training
testing

documents
   
  
   
  
   
  

sections
    
    
    
   
   
   

paragraphs
    
    
    
    
    
    

vocabulary
      
      
      
      
      
      

tokens
     
     
     
     
     
     

table    statistics training test sets used ordering experiments  values
except vocabulary average per document  training set statistics
reproduced table   ease reference 

      ordering evaluation setup
training test data sets use citiesen  citiesfr phones data sets
training documents parameter estimation described section    introduce
additional sets documents domains test sets  table   provides statistics
training test set splits  note out of vocabulary terms test sets
discarded    
even though perform ordering section level  collections still pose
challenging ordering task  example  average number sections citiesen test
document       comparable      sentences  the unit reordering  per document
national transportation safety board corpus used previous work  barzilay   lee 
      elsner et al         
metrics report kendalls rank correlation coefficient ordering experiments  metric measures much ordering differs reference order
underlying assumption reasonable sentence orderings fairly similar
it  specifically  permutation sections n  section document    
computed
d    
         n  
    
 

d     is  before  kendall distance  number swaps adjacent textual
units necessary rearrange reference order  metric ranges     inverse
orders     identical orders   note random ordering yield zero score expectation  measure widely used evaluating information ordering  lapata 
      barzilay   lee        elsner et al         shown correlate human
assessments text quality  lapata        
baselines model variants ordering method compared original
hmm based content modeling approach barzilay lee         baseline delivers
    elements data set limited     articles  preventing us splitting reasonably sized
training test sets  therefore consider ordering experiments  citiesrelated sets  test documents shorter cities lesser population 
hand  phones test set include short express reviews thus exhibits higher
average document length 

   

ficontent modeling using latent permutations

hmm based content model
constrained
k     
model
constrained
k     
model

citiesen
     
     
     
     
     

citiesfr
     
     
     
     
     

phones
     
     
     
     
     

table    comparison orderings produced model series baselines
model variations        topics  evaluated respective test sets 
higher scores better 

state of the art performance number datasets similar spirit model
aims capture patterns level topic distribution  see section     again 
use publicly available implementation   parameters adjusted according
values used previous work  content modeling implementation provides a 
search procedure use find optimal permutation 
include comparison local coherence models  barzilay   lapata       
elsner et al          models designed sentence level analysis particular 
use syntactic information thus cannot directly applied section level ordering 
state above  models orthogonal topic based analysis  combining two
approaches promising direction future work 
note uniform model variant applicable task  since
make claims preferred underlying topic ordering  fact  document likelihood perspective  proposed paragraph order reverse order would
probability uniform model  thus  model variant consider
constrained 
      ordering results
table   summarizes ordering results gmm  hmm based content models  across
data sets  model outperforms content modeling large margin  instance 
citiesen dataset  gap two models reaches      difference
expected  previous work  content models applied short formulaic texts 
contrast  documents collection exhibit higher variability original collections 
hmm provide explicit constraints generated global orderings  may
prevent effectively learning non local patterns topic organization 
observe constrained variant outperforms full model 
difference two small  fairly consistent across domains  since
possible predict idiosyncratic variations test documents topic orderings 
constrained model better capture prevalent ordering patterns consistent
across domain 
    http   people csail mit edu regina code html

   

fichen  branavan  barzilay    karger

    discussion
experiments three separate tasks reveal common trends results 
first  observe single unified model document structure readily
successfully applied multiple discourse level tasks  whereas previous work proposed
separate approaches task  versatility speaks power topic driven
representation document structure  second  within task model outperforms
state of the art baselines substantial margins across wide variety evaluation scenarios  results strongly support hypothesis augmenting topic models
discourse level constraints broadens applicability discourse level analysis tasks 
looking performance model across different tasks  make notes
importance individual topic constraints  topic contiguity consistently
important constraint  allowing model variants outperform alternative baseline
approaches  cases  introducing bias toward similar topic ordering  without requiring identical orderings  provides benefits encoded model 
flexible models achieve superior performance segmentation alignment tasks 
case ordering  however  extra flexibility pay off  model distributes
probability mass away strong ordering patterns likely occur unseen data 
identify properties dataset strongly affect performance
model  constrained model variant performs slightly better full model
rigidly formulaic domains  achieving highest performance phones data set 
know priori domain formulaic structure  worthwhile choose
model variant suitably enforces formulaic topic orderings  fortunately  adaptation
achieved proposed framework using prior generalized mallows
model recall constrained variant special case full model 
however  performance model invariant respect data set characteristics  across two languages considered  model baselines exhibit
comparative performance task  moreover  consistency holds
general interest cities articles highly technical chemical elements articles  finally  smaller citiesen larger citiesen    data sets  observe
results consistent 

   conclusions future work
paper  shown unsupervised topic based approach capture content
structure  resulting model constrains topic assignments way requires global
modeling entire topic sequences  showed generalized mallows model
theoretically empirically appealing way capturing ordering component
topic sequence  results demonstrate importance augmenting statistical models
text analysis structural constraints motivated discourse theory  furthermore 
success gmm suggests could applied modeling ordering
constraints nlp applications 
multiple avenues future extensions work  first  empirical results
demonstrated certain domains providing much flexibility model may
fact detrimental predictive accuracy  cases  tightly constrained
variant model yields superior performance  interesting extension current
   

ficontent modeling using latent permutations

model would allow additional flexibility prior gmm drawing
another level hyperpriors  technical perspective  form hyperparameter
re estimation would involve defining appropriate hyperprior generalized mallows
model adapting estimation present inference procedure 
additionally  may cases assumption one canonical topic ordering
entire corpus limiting  e g   data set consists topically related articles
multiple sources  editorial standards  model extended
allow multiple canonical orderings positing additional level hierarchy
probabilistic model  i e   document structures generated mixture several
generalized mallows models  distributional mode  case 
model would take additional burden learning topics permuted
multiple canonical orderings  change model would greatly complicate
inference re estimating generalized mallows model canonical ordering general nphard  however  recent advances statistics produced efficient approximate algorithms
theoretically guaranteed correctness bounds  ailon  charikar    newman       
exact methods tractable typical cases  meila et al         
generally  model presented paper assumes two specific global constraints
content structure  domains satisfy constraints plentiful 
domains modeling assumptions hold  example  dialogue well
known topics recur throughout conversation  grosz   sidner         thereby violating
first constraint  nevertheless  texts domains still follow certain organizational
conventions  e g  stack structure dialogue  results suggest explicitly incorporating domain specific global structural constraints content model would likely
improve accuracy structure induction 
another direction future work combine global topic structure model
local coherence constraints  previously noted  model agnostic toward
relationships sentences within single topic  contrast  models local coherence
take advantage wealth additional knowledge  syntax  make decisions
information flow across adjoining sentences  linguistically rich model would provide
powerful representation levels textual structure  could used even
greater variety applications considered here 

bibliographic note
portions work previously presented conference publication  chen  branavan 
barzilay    karger         article significantly extends previous work  notably
introducing new algorithm applying models output information ordering
task  section    considering new data sets experiments vary genre 
language  size  section    

acknowledgments
authors acknowledge funding support nsf career grant iis        
grant iis          nsf graduate fellowship  office naval research  quanta 
nokia  microsoft faculty fellowship  thank many people offered

   

fichen  branavan  barzilay    karger

suggestions comments work  including michael collins  aria haghighi  yoong
keok lee  marina meila  tahira naseem  christy sauper  david sontag  benjamin snyder 
luke zettlemoyer  especially grateful marina meila introducing us
mallows model  paper greatly benefited thoughtful feedback
anonymous reviewers  opinions  findings  conclusions  recommendations expressed
paper authors  necessarily reflect views funding
organizations 

   

ficontent modeling using latent permutations

references
ailon  n   charikar  m     newman  a          aggregating inconsistent information 
ranking clustering  journal acm         
althaus  e   karamanis  n     koller  a          computing locally coherent discourses 
proceedings acl 
bartlett  f  c          remembering  study experimental social psychology  cambridge university press 
barzilay  r     elhadad  n          sentence alignment monolingual comparable corpora 
proceedings emnlp 
barzilay  r   elhadad  n     mckeown  k          inferring strategies sentence ordering
multidocument news summarization  journal artificial intelligence research 
         
barzilay  r     lapata  m          modeling local coherence  entity based approach 
computational linguistics              
barzilay  r     lee  l          catching drift  probabilistic content models  applications generation summarization  proceedings naacl hlt 
barzilay  r   mckeown  k     elhadad  m          information fusion context
multi document summarization  proceedings acl 
beeferman  d   berger  a     lafferty  j  d          statistical models text segmentation 
machine learning             
bernardo  j  m     smith  a  f          bayesian theory  wiley series probability
statistics 
bishop  c  m          pattern recognition machine learning  springer 
blei  d  m     moreno  p  j          topic segmentation aspect hidden markov
model  proceedings sigir 
blei  d  m   ng  a     jordan  m          latent dirichlet allocation  journal machine
learning research             
bollegala  d   okazaki  n     ishizuka  m          bottom up approach sentence
ordering multi document summarization  proceedings acl coling 
chen  h   branavan  s   barzilay  r     karger  d  r          global models document
structure using latent permutations  proceedings naacl hlt 
chinchor  n          statistical significance muc   results  proceedings  th
conference message understanding 
cohen  w  w   schapire  r  e     singer  y          learning order things  journal
artificial intelligence research             
eisenstein  j     barzilay  r          bayesian unsupervised topic segmentation  proceedings emnlp 
elsner  m   austerweil  j     charniak  e          unified local global model
discourse coherence  proceedings naacl hlt 
   

fichen  branavan  barzilay    karger

fligner  m     verducci  j          distance based ranking models  journal royal
statistical society  series b                 
fligner  m  a     verducci  j  s          posterior probabilities consensus ordering 
psychometrika               
galley  m   mckeown  k  r   fosler lussier  e     jing  h          discourse segmentation
multi party conversation  proceedings acl 
geman  s     geman  d          stochastic relaxation  gibbs distributions bayesian
restoration images  ieee transactions pattern analysis machine intelligence             
graesser  a   gernsbacher  m     goldman  s   eds            handbook discourse
processes  erlbaum 
griffiths  t  l     steyvers  m          finding scientific topics  proceedings national
academy sciences                
griffiths  t  l   steyvers  m   blei  d  m     tenenbaum  j  b          integrating topics
syntax  advances nips 
grosz  b  j     sidner  c  l          attention  intentions  structure discourse 
computational linguistics                 
gruber  a   rosen zvi  m     weiss  y          hidden topic markov models  proceedings
aistats 
halliday  m  a  k     hasan  r          cohesion english  longman 
hearst  m          multi paragraph segmentation expository text  proceedings
acl 
ji  p  d     pulman  s          sentence ordering manifold based classification
multi document summarization  proceedings emnlp 
karamanis  n   poesio  m   mellish  c     oberlander  j          evaluating centeringbased metrics coherence text structuring using reliably annotated corpus 
proceedings acl 
klementiev  a   roth  d     small  k          unsupervised rank aggregation distancebased models  proceedings icml  pp         
lapata  m          probabilistic text structuring  experiments sentence ordering 
proceedings acl 
lapata  m          automatic evaluation information ordering  kendalls tau  computational linguistics                 
lebanon  g     lafferty  j          cranking  combining rankings using conditional probability models permutations  proceedings icml 
malioutov  i     barzilay  r          minimum cut model spoken lecture segmentation 
proceedings acl 
mallows  c  l          non null ranking models  biometrika             

   

ficontent modeling using latent permutations

meila  m   phadnis  k   patterson  a     bilmes  j          consensus ranking
exponential model  proceedings uai 
neal  r  m          slice sampling  annals statistics             
nelken  r     shieber  s  m          towards robust context sensitive sentence alignment
monolingual corpora  proceedings eacl 
noreen  e  w          computer intensive methods testing hypotheses  introduction  wiley 
pevzner  l     hearst  m  a          critique improvement evaluation metric
text segmentation  computational linguistics           
porteous  i   newman  d   ihler  a   asuncion  a   smyth  p     welling  m          fast
collapsed gibbs sampling latent dirichlet allocation  proceedings sigkdd 
purver  m   kording  k   griffiths  t  l     tenenbaum  j  b          unsupervised topic
modelling multi party spoken discourse  proceedings acl coling 
radev  d   jing  h     budzikowska  m          centroid based summarization multiple documents  sentence extraction  utility based evaluation user studies 
proceedings anlp naacl summarization workshop 
riezler  s     maxwell  j  t          pitfalls automatic evaluation significance testing mt  proceedings acl workshop intrinsic extrinsic
evaluation measures machine translation and or summarization 
schiffrin  d   tannen  d     hamilton  h  e   eds            handbook discourse
analysis  blackwell 
titov  i     mcdonald  r          modeling online reviews multi grain topic models 
proceedings www 
utiyama  m     isahara  h          statistical model domain independent text segmentation  proceedings acl 
van mulbregt  p   carp  i   gillick  l   lowe  s     yamron  j          text segmentation
topic tracking broadcast news via hidden markov model approach  proceedings
icslp 
wallach  h  m          topic modeling  beyond bag words  proceedings icml 
wray  a          formulaic language lexicon  cambridge university press  cambridge 

   


