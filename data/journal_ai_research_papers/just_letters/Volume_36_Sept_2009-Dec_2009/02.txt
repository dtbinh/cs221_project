journal of artificial intelligence research                 

submitted        published      

content modeling using latent permutations
harr chen
s r k  branavan
regina barzilay
david r  karger

harr csail mit edu
branavan csail mit edu
regina csail mit edu
karger csail mit edu

computer science and artificial intelligence laboratory
massachusetts institute of technology
   vassar street  cambridge  massachusetts       usa

abstract
we present a novel bayesian topic model for learning discourse level document structure  our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics  we propose
a global model in which both topic selection and ordering are biased to be similar across a
collection of related documents  we show that this space of orderings can be effectively represented using a distribution over permutations called the generalized mallows model  we
apply our method to three complementary discourse level tasks  cross document alignment 
document segmentation  and information ordering  our experiments show that incorporating our permutation based model in these applications yields substantial improvements
in performance over previously proposed methods 

   introduction
a central problem of discourse analysis is modeling the content structure of a document 
this structure encompasses the topics that are addressed and the order in which these topics
appear across documents in a single domain  modeling content structure is particularly
germane for domains that exhibit recurrent patterns in content organization  such as news
and encyclopedia articles  these models aim to induce  for example  that articles about
cities typically contain information about history  economy  and transportation  and that
descriptions of history usually precede those of transportation 
previous work  barzilay   lee        elsner  austerweil    charniak        has demonstrated that content models can be learned from raw unannotated text  and are useful in
a variety of text processing tasks such as summarization and information ordering  however  the expressive power of these approaches is limited  by taking a markovian view on
content structure  they only model local constraints on topic organization  this shortcoming is substantial since many discourse constraints described in the literature are global in
nature  graesser  gernsbacher    goldman        schiffrin  tannen    hamilton        
in this paper  we introduce a model of content structure that explicitly represents two
important global constraints on topic selection   the first constraint posits that each document follows a progression of coherent  nonrecurring topics  halliday   hasan        
following the example above  this constraint captures the notion that a single topic  such
   throughout this paper  we will use topic to refer interchangeably to both the discourse unit and
language model views of a topic 

c
    
ai access foundation  all rights reserved 

fichen  branavan  barzilay    karger

as history  is expressed in a contiguous block within the document  rather than spread over
disconnected sections  the second constraint states that documents from the same domain
tend to present similar topics in similar orders  bartlett        wray         this constraint
guides toward selecting sequences with similar topic ordering  such as placing history before transportation  while these constraints are not universal across all genres of human
discourse  they are applicable to many important domains  ranging from newspaper text to
product reviews  
we present a latent topic model over related documents that encodes these discourse
constraints by positing a single distribution over the entirety of a documents content ordering  specifically  we represent content structure as a permutation over topics  this
naturally enforces the first constraint since a permutation does not allow topic repetition 
to learn the distribution over permutations  we employ the generalized mallows model
 gmm   this model concentrates probability mass on permutations close to a canonical
permutation  permutations drawn from this distribution are likely to be similar  conforming to the second constraint  a major benefit of the gmm is its compact parameterization
using a set of real valued dispersion values  these dispersion parameters allow the model
to learn how strongly to bias each documents topic ordering toward the canonical permutation  furthermore  the number of parameters grows linearly with the number of topics 
thus sidestepping tractability problems typically associated with the large discrete space of
permutations 
we position the gmm within a larger hierarchical bayesian model that explains how a
set of related documents is generated  for each document  the model posits that a topic
ordering is drawn from the gmm  and that a set of topic frequencies is drawn from a multinomial distribution  together  these draws specify the documents entire topic structure  in
the form of topic assignments for each textual unit  as with traditional topic models  words
are then drawn from language models indexed by topic  to estimate the model posterior 
we perform gibbs sampling over the topic structures and gmm dispersion parameters while
analytically integrating out the remaining hidden variables 
we apply our model to three complex document level tasks  first  in the alignment
task  we aim to discover paragraphs across different documents that share the same topic 
in our experiments  our permutation based model outperforms the hidden topic markov
model  gruber  rosen zvi    weiss        by a wide margin  the gap averaged     percentage points in f score  second  we consider the segmentation task  where the goal is to
partition each document into a sequence of topically coherent segments  the model yields
an average pk measure of        a      percentage point improvement over a competitive
bayesian segmentation method that does not take global constraints into account  eisenstein   barzilay         third  we apply our model to the ordering task  that is  sequencing
a held out set of textual units into a coherent document  as with the previous two applications  the difference between our model and a state of the art baseline is substantial 
our model achieves an average kendalls  of        compared to a value of       for the
hmm based content model  barzilay   lee        
the success of the permutation based model in these three complementary tasks demonstrates its flexibility and effectiveness  and attests to the versatility of the general document
   an example of a domain where the first constraint is violated is dialogue  texts in such domains follow
the stack structure  allowing topics to recur throughout a conversation  grosz   sidner        

   

ficontent modeling using latent permutations

structure induced by our model  we find that encoding global ordering constraints into
topic models makes them more suitable for discourse level analysis  in contrast to the local
decision approaches taken by previous work  furthermore  in most of our evaluation scenarios  our full model yields significantly better results than its simpler variants that either
use a fixed ordering or are order agnostic 
the remainder of this paper proceeds as follows  in section    we describe how our approach relates to previous work in both topic modeling and statistical discourse processing 
we provide a problem formulation in section     followed by an overview of our content
model in section      at the heart of this model is the distribution over topic permutations 
for which we provide background in section      before employing it in a formal description
of the models probabilistic generative story in section      section   discusses the estimation of the models posterior distribution given example documents using a collapsed gibbs
sampling procedure  techniques for applying our model to the three tasks of alignment 
segmentation  and ordering are explained in section    we then evaluate our models performance on each of these tasks in section   before concluding by touching upon directions
for future work in section    code  data sets  annotations  and the raw outputs of our
experiments are available at http   groups csail mit edu rbg code mallows  

   related work
we describe two areas of previous work related to our approach  from the algorithmic
perspective our work falls into a broad class of topic models  while earlier work on topic
modeling took the bag of words view of documents  many recent approaches have expanded
topic models to capture some structural constraints  in section      we describe these extensions and highlight their differences from our model  on the linguistic side  our work
relates to research on modeling text structure in statistical discourse processing  we summarize this work in section      drawing comparisons with the functionality supported by
our model 
    topic models
probabilistic topic models  originally developed in the context of language modeling  have
today become popular for a range of nlp applications  such as text classification and document browsing  topic models posit that a latent state variable controls the generation of
each word  their parameters are estimated using approximate inference techniques such as
gibbs sampling and variational methods  in traditional topic models such as latent dirichlet allocation  lda   blei  ng    jordan        griffiths   steyvers         documents are
treated as bags of words  where each word receives a separate topic assignment and words
assigned to the same topic are drawn from a shared language model 
while the bag of words representation is sufficient for some applications  in many cases
this structure unaware view is too limited  previous research has considered extensions of
lda models in two orthogonal directions  covering both intrasentential and extrasentential
constraints 

   

fichen  branavan  barzilay    karger

      modeling intrasentential constraints
one promising direction for improving topic models is to augment them with constraints
on topic assignments of adjoining words within sentences  for example  griffiths  steyvers 
blei  and tenenbaum        propose a model that jointly incorporates both syntactic and
semantic information in a unified generative framework and constrains the syntactic classes
of adjacent words  in their approach  the generation of each word is controlled by two hidden
variables  one specifying a semantic topic and the other specifying a syntactic class  the
syntactic class hidden variables are chained together as a markov model  whereas semantic
topic assignments are assumed to be independent for every word 
as another example of intrasentential constraints  wallach        proposes a way to
incorporate word order information  in the form of bigrams  into an lda style model  in
this approach  the generation of each word is conditioned on both the previous word and
the topic of the current word  while the word topics themselves are generated from perdocument topic distributions as in lda  this formulation models text structure at the
level of word transitions  as opposed to the work of griffiths et al         where structure
is modeled at the level of hidden syntactic class transitions 
our focus is on modeling high level document structure in terms of its semantic content 
as such  our work is complementary to methods that impose structure on intrasentential
units  it should be possible to combine our model with constraints on adjoining words 
      modeling extrasentential constraints
given the intuitive connection between the notion of topic in lda and the notion of topic in
discourse analysis  it is natural to assume that lda like models can be useful for discourselevel tasks such as segmentation and topic classification  this hypothesis motivated research
on models where topic assignment is guided by structural considerations  purver  kording 
griffiths    tenenbaum        gruber et al         titov   mcdonald         particularly
relationships between the topics of adjacent textual units  depending on the application  a
textual unit may be a sentence  paragraph  or speaker utterance  a common property of
these models is that they bias topic assignments to cohere within local segments of text 
models in this category vary in terms of the mechanisms used to encourage local topic
coherence  for instance  the model of purver et al         biases the topic distributions
of adjacent utterances  textual units  to be similar  their model generates each utterance
from a mixture of topic language models  the parameters of this topic mixture distribution is assumed to follow a type of markovian transition process  specifically  with high
probability an utterance u will have the same topic distribution as the previous utterance
u     otherwise  a new topic distribution is drawn for u  thus  each textual units topic
distribution only depends on the previous textual unit  controlled by a parameter indicating
whether a new topic distribution is drawn 
in a similar vein  the hidden topic markov model  htmm   gruber et al         posits
a generative process where each sentence  textual unit  is assigned a single topic  so that
all of the sentences words are drawn from a single language model  as with the model of
purver et al   topic transitions between adjacent textual units are modeled in a markovian
fashion  specifically  sentence i has the same topic as sentence i    with high probability 
or receives a new topic assignment drawn from a shared topic multinomial distribution 

   

ficontent modeling using latent permutations

in both htmm and our model  the assumption of a single topic per textual unit allows
sections of text to be related across documents by topic  in contrast  purver et al s model
is tailored for the task of segmentation  so each utterance is drawn from a mixture of topics 
thus  their model does not capture how utterances are topically aligned across related
documents  more importantly  both htmm and the model of purver et al  are only able
to make local decisions regarding topic transitions  and thus have difficulty respecting longrange discourse constraints such as topic contiguity  our model instead takes a global view
on topic assignments for all textual units by explicitly generating an entire documents topic
ordering from one joint distribution  as we show later in this paper  this global view yields
significant performance gains 
the recent multi grain latent dirichlet allocation  mglda  model  titov   mcdonald        has also studied topic assignments at the level of sub document textual units 
in mglda  a set of local topic distributions is induced for each sentence  dependent on a
window of local context around the sentence  individual words are then drawn either from
these local topics or from document level topics as in standard lda  mglda represents
local context using a sliding window  where each window frame comprises overlapping short
spans of sentences  in this way  local topic distributions are shared between sentences in
close proximity 
mglda can represent more complex topical dependencies than the models of purver
et al  and gruber et al   because the window can incorporate a much wider swath of local
context than two adjacent textual units  however  mglda is unable to encode longer
range constraints  such as contiguity and ordering similarity  because sentences not in close
proximity are only loosely connected through a series of intervening window frames  in
contrast  our work is specifically oriented toward these long range constraints  necessitating
a whole document notion of topic assignment 
    modeling ordering constraints in statistical discourse analysis
the global constraints encoded by our model are closely related to research in discourse
on information ordering with applications to text summarization and generation  barzilay 
elhadad    mckeown        lapata        karamanis  poesio  mellish    oberlander       
elsner et al          the emphasis of that body of work is on learning ordering constraints
from data  with the goal of reordering new text from the same domain  these methods build
on the assumption that recurring patterns in topic ordering can be discovered by analyzing
patterns in word distribution  the key distinction between prior methods and our approach
is that existing ordering models are largely driven by local constraints with limited ability
to capture global structure  below  we describe two main classes of probabilistic ordering
models studied in discourse processing 
      discriminative models
discriminative approaches aim directly to predict an ordering for a given set of sentences 
modeling the ordering of all sentences simultaneously leads to a complex structure prediction
problem  in practice  however  a more computationally tractable two step approach is taken 
first  probabilistic models are used to estimate pairwise sentence ordering preferences  next 
these local decisions are combined to produce a consistent global ordering  lapata       

   

fichen  branavan  barzilay    karger

althaus  karamanis    koller         training data for pairwise models is constructed
by considering all pairs of sentences in a document  with supervision labels based on how
they are actually ordered  prior work has demonstrated that a wide range of features
are useful in these classification decisions  lapata        karamanis et al         ji  
pulman        bollegala  okazaki    ishizuka         for instance  lapata        has
demonstrated that lexical features  such as verb pairs from the input sentences  serve as
a proxy for plausible sequences of actions  and thus are effective predictors of well formed
orderings  during the second stage  these local decisions are integrated into a global order
that maximizes the number of consistent pairwise classifications  since finding such an
ordering is np hard  cohen  schapire    singer         various approximations are used in
practice  lapata        althaus et al         
while these two step discriminative approaches can effectively leverage information
about local transitions  they do not provide any means for representing global constraints 
in more recent work  barzilay and lapata        demonstrated that certain global properties can be captured in the discriminative framework using a reranking mechanism  in
this set up  the system learns to identify the best global ordering given a set of n possible
candidate orderings  the accuracy of this ranking approach greatly depends on the quality
of selected candidates  identifying such candidates is a challenging task given the large
search space of possible alternatives 
the approach presented in this work differs from existing discriminative models in two
ways  first  our model represents a distribution over all possible global orderings  thus 
we can use sampling mechanisms that consider this whole space rather than being limited
to a subset of candidates as with ranking models  the second difference arises out of the
generative nature of our model  rather than focusing on the ordering task  our order aware
model effectively captures a layer of hidden variables that explain the underlying structure
of document content  thus  it can be effectively applied to a wider variety of applications 
including those where sentence ordering is already observed  by appropriately adjusting the
observed and hidden components of the model 
      generative models
our work is closer in technique to generative models that treat topics as hidden variables 
one instance of such work is the hidden markov model  hmm  based content model  barzilay   lee         in their model  states correspond to topics and state transitions represent
ordering preferences  each hidden states emission distribution is then a language model
over words  thus  similar to our approach  these models implicitly represent patterns at
the level of topical structure  the hmm is then used in the ranking framework to select an
ordering with the highest probability 
in more recent work  elsner et al         developed a search procedure based on simulated annealing that finds a high likelihood ordering  in contrast to ranking based approaches  their search procedure can cover the entire ordering space  on the other hand 
as we show in section      we can define an ordering objective that can be maximized very
efficiently over all possible orderings during prediction once the model parameters have
been learned  specifically  for a bag of p paragraphs  only o pk  calculations of paragraph
probabilities are necessary  where k is the number of topics 

   

ficontent modeling using latent permutations

another distinction between our proposed model and prior work is in the way global
ordering constraints are encoded  in a markovian model  it is possible to induce some global
constraints by introducing additional local constraints  for instance  topic contiguity can
be enforced by selecting an appropriate model topology  e g   by augmenting hidden states
to record previously visited states   however  other global constraints  such as similarity in
overall ordering across documents  are much more challenging to represent  by explicitly
modeling the topic permutation distribution  we can easily capture this kind of global
constraint  ultimately resulting in more accurate topic models and orderings  as we show
later in this paper  our model substantially outperforms the approach of barzilay and lee
on the information ordering task to which they applied the hmm based content model 

   model
in this section  we describe our problem formulation and proposed model 
    problem formulation
our content modeling problem can be formalized as follows  we take as input a corpus
 d          dd   of related documents  and a specification of a number of topics k   each
document d is comprised of an ordered sequence of nd paragraphs  pd             pd nd    as
output  we predict a single topic assignment zd p              k  for each paragraph p  
these z values should reflect the underlying content organization of each document 
related content discussed within each document  and across separate documents  should
receive the same z value 
our formulation shares some similarity with the standard lda setup in that a common
set of topics is assigned across a collection of documents  the difference is that in lda
each words topic assignment is conditionally independent  following the bag of words view
of documents  whereas our constraints on how topics are assigned let us connect word
distributional patterns to document level topic structure 
    model overview
we propose a generative bayesian model that explains how a corpus of d documents can
be produced from a set of hidden variables  at a high level  the model first selects how
frequently each topic is expressed in the document  and how the topics are ordered  these
topics then determine the selection of words for each paragraph  notation used in this and
subsequent sections is summarized in figure   
for each document d with nd paragraphs  we separately generate a bag of topics td and
a topic ordering d   the unordered bag of topics td   which contains nd elements  expresses
how many paragraphs of the document are assigned to each of the k topics  equivalently 
td can be viewed as a vector of occurrence counts for each topic  with zero counts for
topics that do not appear at all  variable td is constructed by taking nd samples from a
   a nonparametric extension of this model would be to also learn k 
   in well structured documents  paragraphs tend to be internally topically consistent  halliday   hasan 
       so predicting one topic per paragraph is sufficient  however  we note that our approach can be
applied with no modifications to other levels of textual granularity such as sentences 

   

fichen  branavan  barzilay    karger



 parameters of distribution over
topic counts



 parameters of distribution over
topic orderings

t

 vector of topic counts

v

 vector of inversion counts



 topic ordering

z

 paragraph topic assignment



 language model parameters of
each topic

  dirichlet    
for j           k    
j  gmm          
for k           k 
k  dirichlet    
for each
td 
vd 
d  
zd  

w  document words

document d
multinomial  
gmm  
compute  vd  
compute z td   d  

for each paragraph p in d
for each word w in p
w  multinomial zd p  

k  number of topics
d  number of documents in corpus
nd  number of paragraphs in
document d
np  number of words in paragraph p

algorithm  compute 
input  inversion count vector v

algorithm  compute z
input  topic counts t  permutation 

create an empty list 
     k
for j   k    down to  
for i   k    down to v j 
 i        i 
 v j    j

create an empty list z
end   
for k   k to  
for i     to t  k  
z end    k 
end  end    

output  permutation 

output  paragraph topic vector z

figure    the plate diagram and generative process for our model  along with a table of
notation for reference purposes  shaded circles in the figure denote observed
variables  and squares denote hyperparameters  the dotted arrows indicate that
 is constructed deterministically from v according to algorithm compute   and
z is constructed deterministically from t and  according to compute z 
   

ficontent modeling using latent permutations

distribution over topics   a multinomial representing the probability of each topic being
expressed  sharing  between documents captures the notion that certain topics are more
likely across most documents in the corpus 
the topic ordering variable d is a permutation over the numbers   through k that
defines the order in which topics appear in the document  we draw d from the generalized
mallows model  a distribution over permutations that we explain in section      as we will
see  this particular distribution biases the permutation selection to be close to a single
centroid  reflecting the discourse constraint of preferring similar topic structures across
documents 
together  a documents bag of topics td and ordering d determine the topic assignment
zd p for each of its paragraphs  for example  in a corpus with k      a seven paragraph
document d with td                         and d                would induce the topic sequence
zd                          the induced topic sequence zd can never assign the same topic to
two unconnected portions of a document  thus satisfying the constraint of topic contiguity 
we assume that each topic k is associated with a language model k   the words of a
paragraph assigned to topic k are then drawn from that topics language model k   this
portion is similar to standard lda in that each topic relates to its own language model 
however  unlike lda  our model enforces topic coherence for an entire paragraph rather
than viewing a paragraph as a mixture of topics 
before turning to a more formal discussion of the generative process  we first provide
background on the permutation model for topic ordering 
    the generalized mallows model over permutations
a central challenge of the approach we have presented is modeling the distribution over possible topic orderings  for this purpose we use the generalized mallows model  gmm   fligner
  verducci        lebanon   lafferty        meila  phadnis  patterson    bilmes       
klementiev  roth    small         which exhibits two appealing properties in the context
of this task  first  the model concentrates probability mass on some canonical ordering
and small perturbations  permutations  of that ordering  this characteristic matches our
constraint that documents from the same domain exhibit structural similarity  second 
its parameter set scales linearly with the number of elements being ordered  making it
sufficiently constrained and tractable for inference 
we first describe the standard mallows model over orderings  mallows         the
mallows model takes two parameters  a canonical ordering  and a dispersion parameter  
it then sets the probability of any other ordering  to be proportional to ed      where
d     represents some distance metric between orderings  and   frequently  this metric
is the kendall  distance  the minimum number of swaps of adjacent elements needed to
transform ordering  into the canonical ordering   thus  orderings which are close to the
canonical ordering will have high probability  while those in which many elements have been
moved will have less probability mass 
the generalized mallows model  first introduced by fligner and verducci         refines
the standard mallows model by adding an additional set of dispersion parameters  these
parameters break apart the distance d     between orderings into a set of independent
components  each component can then separately vary in its sensitivity to perturbation 

   

fichen  branavan  barzilay    karger

to tease apart the distance function into components  the gmm distribution considers the
inversions required to transform the canonical ordering into an observed ordering  we first
discuss how these inversions are parameterized in the gmm  then turn to the distributions
definition and characteristics 
      inversion representation of permutations
typically  permutations are represented directly as an ordered sequence of elements 
for example            represents permuting the initial order by placing the third element
first  followed by the first element  and then the second  the gmm utilizes an alternative
permutation representation defined by a vector  v            vk    of inversion counts with
respect to the identity permutation             k   term vj counts the number of times when
a value greater than j appears before j in the permutation  note that the jth inversion
count vj can only take on integer values from   to k  j inclusive  thus the inversion count
vector has only k    elements  as vk is always zero  for instance  given the standard
form permutation                     v      because       and   appear before    and v     
because no numbers appear before it  the entire inversion count vector would be                 
likewise  our previous example permutation              maps to inversion counts           
the sum of all components of an entire inversion count vector is simply that orderings
kendall  distance from the canonical ordering 
a significant appeal of the inversion representation is that every valid  distinct vector
of inversion counts corresponds to a distinct permutation and vice versa  to see this 
note that for each permutation we can straightforwardly compute its inversion counts 
conversely  given a sequence of inversion counts  we can construct the unique corresponding
permutation  we insert items into the permutation  working backwards from item k 
assume that we have already placed items j     through k in the proper order  to insert
item j  we note that exactly vj of items j     to k must precede it  meaning that it
must be inserted after position vj in the current order  see the compute  algorithm in
figure     since there is only one place where j can be inserted that fulfills the inversion
counts  induction shows that exactly one permutation can be constructed to satisfy the
given inversion counts 
in our model  we take the canonical topic ordering to always be the identity ordering
            k   because the topic numbers in our task are completely symmetric and not linked
to any extrinsic meaning  fixing the global ordering to a specific arbitrary value does not
sacrifice any representational power  in the general case of the gmm  the canonical ordering
is a parameter of the distribution 
      probability mass function
the gmm assigns probability mass to a particular order based on how that order is permuted from the canonical ordering  more precisely  it associates a distance with every
permutation  where the canonical ordering has distance zero and permutations with many
inversions with respect to this canonical ordering have larger distance  the distance assignment is based on k    real valued dispersion parameters p
             k     the distance of a
permutation with inversion counts v is then defined to be j j vj   the gmms probability

   

ficontent modeling using latent permutations

mass function is exponential in this distance 
p

e j j vj
gmm v     
  
 

k 
y
j  

where     

q

j

ej vj
 
j  j  

   

j  j   is a normalization factor with value 
j  j    

   e kj   j
 
   ej

   

setting all j equal to a single value  recovers the standard mallows model with a kendall
 distance function  the factorization of the gmm into independent probabilities per
inversion count makes this distribution particularly easy to apply  we will use gmmj to refer
to the jth multiplicand of the probability mass function  which is the marginal distribution
over vj  
gmmj  vj   j    

ej vj
 
j  j  

   

due to the exponential form of the distribution  requiring that j     constrains the gmm
to assign highest probability mass to each vj being zero  i e   the distributional mode is the
canonical identity permutation  a higher value for j assigns more probability mass to vj
being close to zero  biasing j to have fewer inversions 
      conjugate prior
a major benefit of the gmm is its membership in the exponential family of distributions 
this means that it is particularly amenable to a bayesian representation  as it admits a
natural independent conjugate prior for each parameter j  fligner   verducci        
gmm   j   vj          e j vj   log j  j      

   

this prior distribution takes two parameters   and vj     intuitively  the prior states that
over   previous trials  the total number of inversions observed was   vj     this distribution
can be easily updated with the observed vj to derive a posterior distribution 
because each vj has a different range  it is inconvenient to set the prior hyperparameters
vj   directly  in our work  we instead assign a common prior value for each parameter j  
which we denote as     then we set each vj   such that the maximum likelihood estimate of
j is     by differentiating the likelihood of the gmm with respect to j   it is straightforward
to verify that this works out to setting 
vj    

e 

k j  
 
  kj   
 
    
  e

   

   

fichen  branavan  barzilay    karger

    formal generative process
we now fully specify the details of our content model  whose plate diagram appears in
figure    we observe a corpus of d documents  where each document d is an ordered
sequence of nd paragraphs and each paragraph is represented as a bag of words  the number
of topics k is assumed to be pre specified  the model induces a set of hidden variables
that probabilistically explain how the words of the corpus were produced  our final desired
output is the posterior distributions over the paragraphs hidden topic assignment variables 
in the following  variables subscripted with   are fixed prior hyperparameters 
   for each topic k  draw a language model k  dirichlet      as with lda  these are
topic specific word distributions 
   draw a topic distribution   dirichlet      which expresses how likely each topic is
to appear regardless of position 
   draw the topic ordering distribution parameters j  gmm           for j     to
k     these parameters control how rapidly probability mass decays for having
more inversions for each topic  a separate j for every topic allows us to learn that
some topics are more likely to be reordered than others 
   for each document d with nd paragraphs 
 a  draw a bag of topics td by sampling nd times from multinomial   
 b  draw a topic ordering d   by sampling a vector of inversion counts vd  gmm   
and then applying algorithm compute  from figure   to vd  
 c  compute the vector of topic assignments zd for document ds paragraphs by
sorting td according to d   as in algorithm compute z from figure    
 d  for each paragraph p in document d 
i  sample each word w in p according to the language model of p  w 
multinomial zd p   
    properties of the model
in this section we describe the rationale behind using the gmm to represent the ordering
component of our content model 
 representational power the gmm concentrates probability mass around one centroid permutation  reflecting our preferred bias toward document structures with similar topic orderings  furthermore  the parameterization of the gmm using a vector of
dispersion parameters  allows for flexibility in how strongly the model biases toward
a single ordering  at one extreme       only one ordering has nonzero probability  while at the other        all orderings are equally likely  because  is comprised
   multiple permutations can contribute to the probability of a single documents topic assignments zd  
if there are topics that do not appear in td   as a result  our current formulation is biased toward
assignments with fewer topics per document  in practice  we do not find this to negatively impact model
performance 

   

ficontent modeling using latent permutations

of independent dispersion parameters              k     the distribution can assign different penalties for displacing different topics  for example  we may learn that middle
sections  in the case of cities  sections such as economy and culture  are more likely
to vary in position across documents than early sections  such as introduction and
history  
 computational benefits the parameterization of the gmm using a vector of dispersion parameters  is compact and tractable  since the number of parameters grows
linearly with the number of topics  the model can efficiently handle longer documents
with greater diversity of content 
another computational advantage of this model is its seamless integration into a larger
bayesian model  due to its membership in the exponential family and the existence
of its conjugate prior  inference does not become significantly more complex when the
gmm is used in a hierarchical context  in our case  the entire document generative
model also accounts for topic frequency and the words within each topic 
one final beneficial effect of the gmm is that it breaks the symmetry of topic assignments by fixing the distribution centroid  specifically  topic assignments are not
invariant to relabeling  because the probability of the underlying permutation would
change  in contrast  many topic models assign the same probability to any relabeling
of the topic assignments  our model thus sidesteps the problem of topic identifiability  the issue where a model may have multiple maxima with the same likelihood due
to the underlying symmetry of the hidden variables  non identifiable models such as
standard lda may cause sampling procedures to jump between maxima or produce
draws that are difficult to aggregate across runs 
finally  we will show in section   that the benefits of the gmm extend from the theoretical to the empirical  representing permutations using the gmm almost always leads to
superior performance compared to alternative approaches 

   inference
the variables that we aim to infer are the paragraph topic assignments z  which are determined by the bag of topics t and ordering  for each document  thus  our goal is to estimate
the joint marginal distributions of t and  given the document text while integrating out
all remaining hidden parameters 
p  t      w  
   
we accomplish this inference task through gibbs sampling  geman   geman        bishop 
       a gibbs sampler builds a markov chain over the hidden variable state space whose
stationary distribution is the actual posterior of the joint distribution  each new sample
is drawn from the distribution of a single variable conditioned on previous samples of the
other variables  we can collapse the sampler by integrating over some of the hidden
variables in the model  in effect reducing the state space of the markov chain  collapsed
sampling has been previously demonstrated to be effective for lda and its variants  griffiths
  steyvers        porteous  newman  ihler  asuncion  smyth    welling        titov  

   

fichen  branavan  barzilay    karger

p  td i   t           p  td i   t   t d i        p  wd   td   d   wd   zd      


n  t d i    t     

p  wd   z  wd       
 t d i      k 

p  vd j   v           p  vd j   v   j   p  wd   td   d   wd   zd      
  gmmj  v  j   p  wd   z  wd       

p


d vd j   vj    
p  j            gmm  j  
  n      
n    
figure    the collapsed gibbs sampling inference procedure for estimating our models
posterior distribution  in each plate diagram  the variable being resampled is
shown in a double circle and its markov blanket is highlighted in black  other
variables  which have no impact on the variable being resampled  are grayed out 
variables  and   shown in dotted circles  are never explicitly depended on or
re estimated  because they are marginalized out by the sampler  each diagram is
accompanied by the conditional resampling distribution for its respective variable 

   

ficontent modeling using latent permutations

mcdonald         it is typically preferred over the explicit gibbs sampling of all the hidden
variables because of the smaller search space and generally shorter mixing time 
our sampler analytically integrates out all but three sets of hidden variables  bags of
topics t  orderings   and permutation inversion parameters   after a burn in period 
we treat the last samples of t and  as a draw from the posterior  when samples of the
marginalized variables  and  are necessary  they can be estimated based on the topic
assignments as we show in section      figure   summarizes the gibbs sampling steps of
our inference procedure 
document probability as a preliminary step  consider how to calculate the probability
of a single documents words wd given the documents paragraph topic assignments zd and
the remaining documents and their topic assignments  note that this probability is decomposable into a product of probabilities over individual paragraphs where paragraphs with
different topics have conditionally independent word probabilities  let wd and zd indicate the words and topic assignments to documents other than d  and w be the vocabulary
size  the probability of the words in d is then 
k z
y
p  wd   z  wd        
p  wd   zd   k   p  k   z  wd       dk

 

k   k
k
y

dcm  wd i   zd i   k     wd i   zd i   k       

   

k  

where dcm   refers to the dirichlet compound multinomial distribution  the result of
integrating over multinomial parameters with a dirichlet prior  bernardo   smith        
for a dirichlet prior with parameters                 w    the dcm assigns the following
probability to a series of observations x    x            xn   
p
w
  j j   y
 n  x  i    i  
p
dcm x      q
 
   
  x    j j  
j  j  
i  

where n  x  i  refers to the number of times word i appears in x  here     is the gamma
function  a generalization of the factorial for real numbers  some algebra shows that the
dcms posterior probability density function conditioned on a series of observations y  
 y            yn   can be computed by updating each i with counts of how often word i appears
in y 
dcm x   y      dcm x      n  y              w   n  y  w    

   

equations   and   will be used to compute the conditional distributions of the hidden
variables  we now turn to how each individual random variable is resampled 
bag of topics first we consider how to resample td i   the ith topic draw for document
d conditioned on all other parameters being fixed  note this is not the topic of the ith
paragraph  as we reorder topics using d   which is generated separately  
p  td i   t           p  td i   t   t d i        p  wd   td   d   wd   zd      


n  t d i    t     

p  wd   z  wd       
 t d i      k 
   

    

fichen  branavan  barzilay    karger

where td is updated to reflect td i   t  and zd is deterministically computed in the last step
using compute z from figure   with inputs td and d   the first step reflects an application
of bayes rule to factor out the term for wd   we then drop superfluous terms from the
conditioning  in the second step  the former term arises out of the dcm  by updating
the parameters   with observations t d i  as in equation   and dropping constants  the
latter document probability term is computed using equation    the new td i is selected
by sampling from this probability computed over all possible topic assignments 
ordering the parameterization of a permutation d as a series of inversion values vd j
reveals a natural way to decompose the search space for gibbs sampling  for each document
d  we resample vd j for j     to k    independently and successively according to its
conditional distribution 
p  vd j   v           p  vd j   v   j   p  wd   td   d   wd   zd      
  gmmj  v  j   p  wd   z  wd       

    

where d is updated to reflect vd j   v  and zd is computed deterministically according to
td and d   the first term refers to equation    the second is computed using equation   
this probability is computed for every possible value of v  which ranges from   to k  j 
and term vd j is sampled according to the resulting probabilities 
gmm parameters for each j     to k     we resample j from its posterior distribution 
p


d vd j   vj    
p  j            gmm  j  
  n      
    
n    
where gmm  is evaluated according to equation    the normalization constant of this
distribution is unknown  meaning that we cannot directly compute and invert the cumulative distribution function to sample from this distribution  however  the distribution
itself is univariate and unimodal  so we can expect that an mcmc technique such as slice
sampling  neal        should perform well  in practice  matlabs built in slice sampler
provides a robust draw from this distribution  
computational issues during inference  directly computing document probabilities on
the basis of equation   results in many redundant calculations that slow the runtime of
each iteration considerably  to improve the computational performance of our proposed
inference procedure  we apply some memoization techniques during sampling  within a
single iteration  for each document  the gibbs sampler requires computing the documents
probability given its topic assignments  equation    many times  but each computation
frequently conditions on only slight variations of those topic assignments  a nave approach
would compute a probability for every paragraph each time a document probability is
desired  performing redundant calculations when topic assignment sequences with shared
subsequences are repeatedly considered 
instead  we use lazy evaluation to build a three dimensional cache  indexed by tuple
 i  j  k   as follows  each time a document probability is requested  it is broken into independent subspans of paragraphs  where each subspan takes on one contiguous topic assignment  this is possible due to the way equation   factorizes into independent per topic
   in particular  we use the slicesample function from the matlab statistics toolbox 

   

ficontent modeling using latent permutations

multiplicands  for a subspan starting at paragraph i  ending at paragraph j  and assigned topic k  the cache is consulted using key  i  j  k   for example  topic assignments
zd                         would result in cache lookups at                       and            if a
cached value is unavailable  the correct probability is computed using equation   and the
result is stored in the cache at location  i  j  k   moreover  we also record values at every
intermediate cache location  i  l  k  for l   i to j     because these values are computed as
subproblems while evaluating equation   for  i  j  k   the cache is reset before proceeding to
the next document since the conditioning changes between documents  for each document 
this caching guarantees that there are at most o nd  k  paragraph probability calculations 
in practice  because most individual gibbs steps are small  this bound is very loose and the
caching mechanism reduces computation time by several orders of magnitude 
we also maintain caches of word topic and paragraph topic assignment frequencies 
allowing us to rapidly compute the counts used in equations   and     this form of caching
is the same as what is used by griffiths and steyvers        

   applications
in this section  we describe how our model can be applied to three challenging discourselevel tasks  aligning paragraphs of similar topical content between documents  segmenting
each document into topically cohesive sections  and ordering new unseen paragraphs into
a coherent document  in particular  we show that the posterior samples produced by our
inference procedure from section   can be used to derive a solution for each of these tasks 
    alignment
for the alignment task we wish to find how the paragraphs of each document topically
relate to paragraphs of other documents  essentially  this is a cross document clustering
task  an alignment assigns each paragraph of a document into one of k topically related
groupings  for instance  given a set of cell phone reviews  one group may represent text
fragments that discuss price  while another group consists of fragments about reception 
our model can be readily employed for this task  we can view the topic assignment
for each paragraph z as a cluster label  for example  for two documents d  and d  with
topic assignments zd                          and zd                           paragraph   of d  is
grouped together with paragraphs   through   of d    and paragraphs   and   of d  with  
and   of d    the remaining paragraphs assigned to topics   and   form their own separate
per document clusters 
previously developed methods for cross document alignment have been primarily driven
by similarity functions that quantify lexical overlap between textual units  barzilay   elhadad        nelken   shieber         these methods do not explicitly model document
structure  but they specify some global constraints that guide the search for an optimal
alignment  pairs of textual units are considered in isolation for making alignment decisions  in contrast  our approach allows us to take advantage of global structure and shared
language models across all related textual units without requiring manual specification of
matching constraints 

   

fichen  branavan  barzilay    karger

    segmentation
segmentation is a well studied discourse task where the goal is to divide a document into
topically cohesive contiguous sections  previous approaches have typically relied on lexical
cohesion  that is  similarity in word choices within a document subspan  to guide the
choice of segmentation boundaries  hearst        van mulbregt  carp  gillick  lowe   
yamron        blei   moreno        utiyama   isahara        galley  mckeown  foslerlussier    jing        purver et al         malioutov   barzilay        eisenstein   barzilay 
       our model relies on this same notion in determining the language models of topics 
but connecting topics across documents and constraining how those topics appear allow it
to better learn the words that are most indicative of topic cohesion 
the output samples from our models inference procedure map straightforwardly to
segmentations  contiguous spans of paragraphs that are assigned the same topic number are taken to be one segment  for example  a seven paragraph document d with topic
assignments zd                         would be segmented into three sections  comprised of
paragraph    paragraphs   and    and paragraphs   through    note that the segmentation ignores the specific values used for topic assignments  and only heeds the paragraph
boundaries at which topic assignments change 
    ordering
a third application of our model is to the problem of creating structured documents from
collections of unordered text segments  this text ordering task is an important step in
broader nlp tasks such as text summarization and generation  for this task  we assume
we are provided with well structured documents from a single domain as training examples 
once trained  the model is used to induce an ordering of previously unseen collections of
paragraphs from the same domain 
during training  our model learns a canonical ordering of topics for documents within
the collection  via the language models associated with each topic  because the gmm
concentrates probability mass around the canonical             k  topic ordering  we expect that
highly probable words in the language models of lower  numbered topics tend to appear early
in a document  whereas highly probable words in the language models of higher  numbered
topics tend to appear late in a document  thus  we structure new documents according to
this intuition  paragraphs with words tied to low topic numbers should be placed earlier
than paragraphs with words relating to high topic numbers 
formally  given an unseen document d comprised of an unordered set of paragraphs
 p            pn    we order paragraphs according to the following procedure  first  we find
the most probable topic assignment zi independently for each paragraph pi   according to
parameters  and  learned during the training phase 
zi   arg max p  zi   k   pi      
k

  arg max p  pi   zi   k  k  p  zi   k     

    

k

second  we sort the paragraphs by topic assignment zi in ascending order  since          k 
is the gmms canonical ordering  this yields the most likely ordering conditioned on a single
estimated topic assignment for each paragraph  due to possible ties in topic assignments 
   

ficontent modeling using latent permutations

the resulting document may be a partial ordering  if a full ordering is required  ties are
broken arbitrarily 
a key advantage of this proposed approach is that it is closed form and computationally
efficient  though the training phase requires running the inference procedure of section   
once the model parameters are learned  predicting an ordering for a new set of p paragraphs
requires computing only pk probability scores  in contrast  previous approaches have
only been able to rank a small subset of all possible document reorderings  barzilay  
lapata         or performed a search procedure through the space of orderings to find an
optimum  elsner et al          
the objective function of equation    depends on posterior estimates of  and  given
the training documents  since our collapsed gibbs sampler integrates out these two hidden
variables  we need to back out the values of  and  from the known posterior samples of
z  this can easily be done by computing a point estimate of each distribution based on the
word topic and topic document assignment frequencies  respectively  as is done by griffiths
and steyvers         the probability mass kw of word w in the language model of topic k
is given by 
n  k  w     
kw  
 
    
n  k    w  
where n  k  w  the total number of times word w was assigned to topic k  and n  k  is the
total number of words assigned to topic k  according to the posterior sample of z  we can
derive a similar estimate for k   the prior likelihood of topic k 
k  

n  k     
 
n   k 

    

where n  k  is the total number of paragraphs assigned to topic k according to the sample
of z  and n is the total number of paragraphs in the entire corpus 

   experiments
in this section  we evaluate the performance of our model on the three tasks presented in
section    cross document alignment  document segmentation  and information ordering 
we first describe some preliminaries common to all three tasks  covering the data sets 
reference comparison structures  model variants  and inference algorithm settings shared by
each evaluation  we then provide a detailed examination of how our model performs on
each individual task 
    general evaluation setup
data sets in our experiments we use five data sets  briefly described below  for additional
statistics  see table    
   the approach we describe is not the same as finding the most probable paragraph ordering according to
data likelihood  which is how the optimal ordering is derived for the hmm based content model  our
proposed ordering technique essentially approximates that objective by using a per paragraph maximum
a posteriori estimate of the topic assignments rather than the full posterior topic assignment distribution 
this approximation makes for a much faster prediction algorithm that performs well empirically 

   

fichen  branavan  barzilay    karger

articles about
corpus
citiesen
citiesen   
citiesfr

large cities from wikipedia
language documents sections
english
   
    
english
   
    
french
   
    

paragraphs
    
    
    

vocabulary
      
      
      

tokens
     
     
     

articles about chemical elements from wikipedia
corpus
language documents sections
elements
english
   
   

paragraphs
    

vocabulary
      

tokens
     

cell phone reviews from phonearena com
corpus
language documents sections
phones
english
   
   

paragraphs
    

vocabulary
      

tokens
     

table    statistics of the data sets used in our evaluations  all values except vocabulary
size and document count are per document averages 

 citiesen  articles from the english wikipedia about the worlds     largest cities by
population  common topics include history  culture  and demographics  these articles are typically of substantial size and share similar content organization patterns 
 citiesen      articles from the english wikipedia about the worlds     largest cities
by population  this collection is a superset of citiesen  many of the lower ranked
cities are not well known to english wikipedia editors  thus  compared to citiesen
these articles are shorter on average and exhibit greater variability in content selection
and ordering 
 citiesfr   articles from the french wikipedia about the same     cities as in citiesen 
 elements  articles from the english wikipedia about chemical elements in the periodic table   including topics such as biological role  occurrence  and isotopes 
 phones  reviews extracted from phonearena com  a popular cell phone review website  topics in this corpus include design  camera  and interface  these reviews are
written by expert reviewers employed by the site  as opposed to lay users  
this heterogeneous collection of data sets allows us to examine the behavior of the
model under diverse test conditions  these sets vary in how the articles were generated 
the language in which the articles were written  and the subjects they discuss  as a result 
patterns in topic organization vary greatly across domains  for instance  within the phones
corpus  the articles are very formulaic  due to the centralized editorial control of the website 
which establishes consistent standards followed by the expert reviewers  on the other hand 
wikipedia articles exhibit broader structural variability due to the collaborative nature of
   all     elements at http   en wikipedia org wiki periodic table  including undiscovered element     
   in the phones set     documents are very short express reviews without section headings  we include
them in the input to the model  but did not evaluate on them 

   

ficontent modeling using latent permutations

wikipedia editing  which allows articles to evolve independently  while wikipedia articles
within the same category often exhibit similar section orderings  many have idiosyncratic
inversions  for instance  in the citiesen corpus  both the geography and history sections
typically occur toward the beginning of a document  but history can appear either before
or after geography across different documents 
each corpus we consider has been manually divided into sections by their authors 
including a short textual heading for each section  in sections       and        we discuss
how these author created sections with headings are used to generate reference annotations
for the alignment and segmentation tasks  note that we only use the headings for evaluation 
none of the heading information is provided to any of the methods under consideration  for
the tasks of alignment and segmentation  evaluation is performed on the datasets presented
in table    for the ordering task  however  this data is used for training  and evaluation is
performed using a separate held out set of documents  the details of this held out dataset
are given in section       
model variants for each evaluation  besides comparing to baselines from the literature 
we also consider two variants of our proposed model  in particular  we investigate the
impact of the mallows component of the model by alternately relaxing and tightening the
way it constrains topic orderings 
 constrained   in this variant  we require all documents to follow the exact same canonical ordering of topics  that is  no topic permutation inversions are allowed  though
documents may skip topics as before  this case can be viewed as a special case of
the general model  where the mallows inversion prior   approaches infinity  from
an implementation standpoint  we simply fix all inversion counts v to zero during
inference   
 uniform  this variant assumes a uniform distribution over all topic permutations 
instead of biasing toward a small related set  again  this is a special case of the full
model  with inversion prior   set to zero  and the strength of that prior   approaching
infinity  thus forcing each item of  to always be zero 
note that both of these variants still enforce the long range constraint of topic contiguity 
and vary from the full model only in how they capture topic ordering similarity 
evaluation procedure and parameter settings for each evaluation of our model
and its variants  we run the collapsed gibbs sampler from five random seed states  and take
the       th iteration of each chain as a sample  results presented are the average over
these five samples 
dirichlet prior hyperparameters for the bag of topics   and language models   are set
to      for the gmm  we set the prior dispersion hyperparameter   to    and the effective
    at first glance  the constrained model variant appears to be equivalent to an hmm where each state i
can transition to either i or i      however  this is not the case  some topics may appear zero times
in a document  resulting in multiple possible transitions from each state  furthermore  the transition
probabilities would be dependent on position within the document  for example  at earlier absolute
positions within a document  transitions to high index topics are unlikely  because that would require
all subsequent paragraphs to have a high index topic 

   

fichen  branavan  barzilay    karger

sample size prior   to be     times the number of documents  these values are minimally
tuned  and similar results are achieved for alternative settings of   and     parameters  
and   control the strength of the bias toward structural regularity  trading off between the
constrained and uniform model variants  the values we have chosen are a middle ground
between those two extremes 
our model also takes a parameter k that controls the upper bound on the number of
latent topics  note that our algorithm can select fewer than k topics for each document 
so k does not determine the number of segments in each document  in general  a higher k
results in a finer grained division of each document into different topics  which may result
in more precise topics  but may also split topics that should be together  we report results
in each evaluation using both k      and    
    alignment
we first evaluate the model on the task of cross document alignment  where the goal is to
group textual units from different documents into topically cohesive clusters  for instance 
in the cities related domains  one such cluster may include transportation related paragraphs  before turning to the results we first present details of the specific evaluation setup
targeted to this task 
      alignment evaluation setup
reference annotations to generate a sufficient amount of reference data for evaluating alignments we use section headings provided by the authors  we assume that two
paragraphs are aligned if and only if their section headings are identical  these headings
constitute noisy annotations in the wikipedia datasets  the same topical content may be
labeled with different section headings in different articles  e g   for citiesen  places of
interest in one article and landmarks in another   so we call this reference structure the
noisy headings set 
it is not clear a priori what effect this noise in the section headings may have on evaluation accuracy  to empirically estimate this effect  we also use some manually annotated
alignments in our experiments  specifically  for the citiesen corpus  we manually annotated each articles paragraphs with a consistent set of section headings  providing us an
additional reference structure to evaluate against  in this clean headings set  we found
approximately    topics that were expressed in more than one document 
metrics to quantify our alignment output we compute a recall and precision score of
a candidate alignment against a reference alignment  recall measures  for each unique
section heading in the reference  the maximum number of paragraphs with that heading
that are assigned to one particular topic  the final score is computed by summing over each
section heading and dividing by the total number of paragraphs  high recall indicates that
paragraphs of the same section headings are generally being assigned to the same topic 
conversely  precision measures  for each topic number  the maximum number of paragraphs with that topic assignment that share the same section heading  precision is summed
over each topic and normalized by the total number of paragraphs  high precision means
that paragraphs assigned to a single topic usually correspond to the same section heading 

   

ficontent modeling using latent permutations

recall and precision trade off against each other  more finely grained topics will tend
to improve precision at the cost of recall  at the extremes  perfect recall occurs when every
paragraph is assigned the same topic  and perfect precision when each paragraph is its own
topic 
we also present one summary f score in our results  which is the harmonic mean of
recall and precision 
statistical significance in this setup is measured with approximate randomization  noreen 
       a nonparametric test that can be directly applied to nonlinearly computed metrics
such as f score  this test has been used in prior evaluations for information extraction and
machine translation  chinchor        riezler   maxwell        
baselines

for this task  we compare against two baselines 

 hidden topic markov model  htmm   gruber et al          as explained in section    this model represents topic change between adjacent textual units in a markovian fashion  htmm can only capture local constraints  so it would allow topics to
recur non contiguously throughout a document  we use the publicly available implementation    with priors set according to the recommendations made in the original
work 
 clustering  we use a repeated bisection algorithm to find a clustering of the paragraphs that maximizes the sum of the pairwise cosine similarities of the items in each
cluster    this clustering was implemented using the cluto toolkit    note that
this approach is completely structure agnostic  treating documents as bags of paragraphs rather than sequences of paragraphs  these types of clustering techniques
have been shown to deliver competitive performance for cross document alignment
tasks  barzilay   elhadad        
      alignment results
table   presents the results of the alignment evaluation  on all of the datasets  the best
performance is achieved by our model or its variants  by a statistically significant and usually
substantial margin 
the comparative performance of the baseline methods is consistent across domains 
surprisingly  clustering performs better than the more complex htmm model  this observation is consistent with previous work on cross document alignment and multidocument
summarization  which use clustering as their main component  radev  jing    budzikowska 
      barzilay  mckeown    elhadad         despite the fact that htmm captures some
dependencies between adjacent paragraphs  it is not sufficiently constrained  manual examination of the actual topic assignments reveals that htmm often assigns the same topic
for disconnected paragraphs within a document  violating the topic contiguity constraint 
in all but one domain the full gmm based approach yields the best performance compared to its variants  the one exception is in the phone domain  there the constrained
    http   code google com p openhtmm 
    this particular clustering technique substantially outperforms the agglomerative and graph partitioningbased clustering approaches for our task 
    http   glaros dtc umn edu gkhome views cluto 

   

fik     

k     

k     

k     

chen  branavan  barzilay    karger

clustering
htmm
constrained
uniform
our model
clustering
htmm
constrained
uniform
our model

citiesen
clean headings
recall
prec
f score
     
            
     
            
     
            
     
            
           
     
     
            
     
            
     
            
     
            
           
     

citiesen
noisy headings
recall
prec
f score
     
            
     
            
     
            
     
            
           
     
     
            
     
            
     
            
     
            
           
     

citiesen   
noisy headings
recall
prec
f score
     
            
     
            
     
            
     
            
           
     
     
            
     
            
     
            
     
            
           
     

clustering
htmm
constrained
uniform
our model
clustering
htmm
constrained
uniform
our model

citiesfr
noisy headings
recall
prec
f score
     
            
     
            
     
     
     
     
            
           
     
     
            
     
            
     
            
     
            
           
     

elements
noisy headings
recall
prec
f score
     
            
     
            
     
            
     
            
           
     
     
            
     
            
     
            
     
            
           
     

phones
noisy headings
recall
prec
f score
     
            
     
            
           
     
     
            
     
     
     
     
            
     
            
           
     
     
            
     
     
     

table    comparison of the alignments produced by our model and a series of baselines and
model variations  for both    and    topics  evaluated against clean and noisy sets
of section headings  higher scores are better  within the same k  the methods
which our model significantly outperforms are indicated with  for p         and
 for p        

   

ficontent modeling using latent permutations

baseline achieves the best result for both k by a small margin  these results are to be
expected  given the fact that this domain exhibits a highly rigid topic structure across all
documents  a model that permits permutations of topic ordering  such as the gmm  is too
flexible for such highly formulaic domains 
finally  we observe that the evaluations based on manual and noisy annotations exhibit
an almost entirely consistent ranking of the methods under consideration  see the clean and
noisy headings results for citiesen in table     this consistency indicates that the noisy
headings are sufficient for gaining insight into the comparative performance of the different
approaches 
    segmentation
next we consider the task of text segmentation  we test whether the model is able to
identify the boundaries of topically coherent text segments 
      segmentation evaluation setup
reference segmentations as described in section      all of the datasets used in this
evaluation have been manually divided into sections by their authors  these annotations
are used to create reference segmentations for evaluating our models output  recall from
section       that we also built a clean reference structure for the citiesen set  that structure encodes a clean segmentation of each document because it adjusts the granularity
of section headings to be consistent across documents  thus  we also compare against the
segmentation specified by the citiesen clean section headings 
metrics segmentation quality is evaluated using the standard penalty metrics pk and
windowdiff  beeferman  berger    lafferty        pevzner   hearst         both pass a
sliding window over the documents and compute the probability of the words at the end
of the windows being improperly segmented with respect to each other  windowdiff is
stricter  and requires that the number of segmentation boundaries between the endpoints
of the window be correct as well   
baselines we first compare to bayesseg  eisenstein   barzilay           a bayesian
segmentation approach that is the current state of the art for this task  interestingly  our
model reduces to their approach when every document is considered completely in isolation 
with no topic sharing between documents  connecting topics across documents makes for
a much more difficult inference problem than the one tackled by eisenstein and barzilay 
at the same time  their algorithm cannot capture structural relatedness across documents 
since bayesseg is designed to be operated with a specification of a number of segments 
we provide this baseline with the benefit of knowing the correct number of segments for
each document  which is not provided to our system  we run this baseline using the
    statistical significance testing is not standardized and usually not reported for the segmentation task 
so we omit these tests in our results 
    we do not evaluate on the corpora used in their work  since our model relies on content similarity across
documents in the corpus 

   

fibayesseg
u i
u i
constrained
uniform
our model
constrained
uniform
our model

citiesen
clean headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
           
   
     
     
   
     
     
   
     
     
    
     
     
    
           
    

citiesen
noisy headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
     
     
   
     
     
   
           
   
     
     
    
     
     
    
           
    

citiesen   
noisy headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
     
     
   
     
     
   
           
   
     
     
   
     
     
    
           
    

bayesseg
u i
u i
constrained
uniform
our model
constrained
uniform
our model

citiesfr
noisy headings
pk
wd
  segs
     
     
    
     
     
    
     
     
   
     
     
   
           
   
           
   
     
     
   
     
     
    
           
    

elements
noisy headings
pk
wd
  segs
     
     
   
     
     
   
     
     
   
     
     
   
     
     
   
           
   
     
     
   
     
     
   
           
   

phones
noisy headings
pk
wd
  segs
     
     
   
     
     
   
     
     
   
           
   
     
     
   
     
     
   
           
    
     
     
   
     
     
    

k      k     

k      k     

chen  branavan  barzilay    karger

table    comparison of the segmentations produced by our model and a series of baselines
and model variations  for both    and    topics  evaluated against clean and noisy
sets of section headings  lower scores are better  bayesseg and u i are given the
true number of segments  so their segments counts reflect the reference structures
segmentations  in contrast  u i automatically predicts the number of segments 

   

ficontent modeling using latent permutations

authors publicly available implementation    its priors are set using a built in mechanism
that automatically re estimates hyperparameters 
we also compare our method with the algorithm of utiyama and isahara         which
is commonly used as a point of reference in the evaluation of segmentation algorithms 
this algorithm computes the optimal segmentation by estimating changes in the predicted
language models of segments under different partitions  we used the publicly available
implementation of the system    which does not require parameter tuning on a held out
development set  in contrast to bayesseg  this algorithm has a mechanism for predicting
the number of segments  but can also take a pre specified number of segments  in our
comparison  we consider both versions of the algorithm  u i denotes the case when the
correct number of segments is provided to the model and u i denotes when the model
estimates the optimal number of segments 
      segmentation results
table   presents the segmentation experiment results  on every data set our model outperforms the bayesseg and u i baselines by a substantial margin regardless of k  this result
provides strong evidence that learning connected topic models over related documents leads
to improved segmentation performance 
the best performance is generally obtained by the full version of our model  with three
exceptions  in two cases  citiesen with k      using clean headings on the windowdiff
metric  and citiesfr with k      on the pk metric   the variant that performs better than
the full model only does so by a minute margin  furthermore  in both of those instances 
the corresponding evaluation with k      using the full model leads to the best overall
results for the respective domains 
the only case when a variant outperforms our full model by a notable margin is the
phones data set  this result is not unexpected given the formulaic nature of this dataset
as discussed earlier 
    ordering
the final task on which we evaluate our model is that of finding a coherent ordering of a
set of textual units  unlike the previous tasks  where prediction is based on hidden variable
distributions  ordering is observed in a document  moreover  the gmm model uses this
information during the inference process  therefore  we need to divide our data sets into
training and test portions 
in the past  ordering algorithms have been applied to textual units of various granularities  most commonly sentences and paragraphs  our ordering experiments operate at
the level of a relatively larger unit  sections  we believe that this granularity is suitable
to the nature of our model  because it captures patterns at the level of topic distributions
rather than local discourse constraints  the ordering of sentences and paragraphs has been
studied in the past  karamanis et al         barzilay   lapata        and these two types
of models can be effectively combined to induce a full ordering  elsner et al         
    http   groups csail mit edu rbg code bayesseg 
    http   www  nict go jp x x    members mutiyama software html textseg

   

fichen  branavan  barzilay    karger

corpus
citiesen
citiesfr
phones

set
training
testing
training
testing
training
testing

documents
   
  
   
  
   
  

sections
    
    
    
   
   
   

paragraphs
    
    
    
    
    
    

vocabulary
      
      
      
      
      
      

tokens
     
     
     
     
     
     

table    statistics of the training and test sets used for the ordering experiments  all values
except vocabulary are the average per document  the training set statistics are
reproduced from table   for ease of reference 

      ordering evaluation setup
training and test data sets we use the citiesen  citiesfr and phones data sets
as training documents for parameter estimation as described in section    we introduce
additional sets of documents from the same domains as test sets  table   provides statistics
on the training and test set splits  note that out of vocabulary terms in the test sets are
discarded    
even though we perform ordering at the section level  these collections still pose a
challenging ordering task  for example  the average number of sections in a citiesen test
document is       comparable to the      sentences  the unit of reordering  per document
of the national transportation safety board corpus used in previous work  barzilay   lee 
      elsner et al         
metrics we report the kendalls  rank correlation coefficient for our ordering experiments  this metric measures how much an ordering differs from the reference order  the
underlying assumption is that most reasonable sentence orderings should be fairly similar
to it  specifically  for a permutation  of the sections in an n  section document      is
computed as
d    
           n   
    
 

where d     is  as before  the kendall  distance  the number of swaps of adjacent textual
units necessary to rearrange  into the reference order  the metric ranges from     inverse
orders  to    identical orders   note that a random ordering will yield a zero score in expectation  this measure has been widely used for evaluating information ordering  lapata 
      barzilay   lee        elsner et al         and has been shown to correlate with human
assessments of text quality  lapata        
baselines and model variants our ordering method is compared against the original
hmm based content modeling approach of barzilay and lee         this baseline delivers
    the elements data set is limited to     articles  preventing us from splitting it into reasonably sized
training and test sets  therefore we do not consider it for our ordering experiments  for the citiesrelated sets  the test documents are shorter because they were about cities of lesser population  on the
other hand  for phones the test set does not include short express reviews and thus exhibits higher
average document length 

   

ficontent modeling using latent permutations

hmm based content model
constrained
k     
our model
constrained
k     
our model

citiesen
     
     
     
     
     

citiesfr
     
     
     
     
     

phones
     
     
     
     
     

table    comparison of the orderings produced by our model and a series of baselines and
model variations  for both    and    topics  evaluated on the respective test sets 
higher scores are better 

state of the art performance in a number of datasets and is similar in spirit to our model
 it also aims to capture patterns at the level of topic distribution  see section     again 
we use the publicly available implementation   with parameters adjusted according to the
values used in their previous work  this content modeling implementation provides an a 
search procedure that we use to find the optimal permutation 
we do not include in our comparison local coherence models  barzilay   lapata       
elsner et al          these models are designed for sentence level analysis  in particular 
they use syntactic information and thus cannot be directly applied for section level ordering 
as we state above  these models are orthogonal to topic based analysis  combining the two
approaches is a promising direction for future work 
note that the uniform model variant is not applicable to this task  since it does not
make any claims to a preferred underlying topic ordering  in fact  from a document likelihood perspective  for any proposed paragraph order the reverse order would have the same
probability under the uniform model  thus  the only model variant we consider here is
constrained 
      ordering results
table   summarizes ordering results for the gmm  and hmm based content models  across
all data sets  our model outperforms content modeling by a very large margin  for instance 
on the citiesen dataset  the gap between the two models reaches      this difference is
expected  in previous work  content models were applied to short formulaic texts  in
contrast  documents in our collection exhibit higher variability than the original collections 
the hmm does not provide explicit constraints on generated global orderings  this may
prevent it from effectively learning non local patterns in topic organization 
we also observe that the constrained variant outperforms our full model  while the
difference between the two is small  it is fairly consistent across domains  since it is not
possible to predict idiosyncratic variations in the test documents topic orderings  a more
constrained model can better capture the prevalent ordering patterns that are consistent
across the domain 
    http   people csail mit edu regina code html

   

fichen  branavan  barzilay    karger

    discussion
our experiments with the three separate tasks reveal some common trends in the results 
first  we observe that our single unified model of document structure can be readily and
successfully applied to multiple discourse level tasks  whereas previous work has proposed
separate approaches for each task  this versatility speaks to the power of our topic driven
representation of document structure  second  within each task our model outperforms
state of the art baselines by substantial margins across a wide variety of evaluation scenarios  these results strongly support our hypothesis that augmenting topic models with
discourse level constraints broadens their applicability to discourse level analysis tasks 
looking at the performance of our model across different tasks  we make a few notes
about the importance of the individual topic constraints  topic contiguity is a consistently
important constraint  allowing both of our model variants to outperform alternative baseline
approaches  in most cases  introducing a bias toward similar topic ordering  without requiring identical orderings  provides further benefits when encoded in the model  our more
flexible models achieve superior performance in the segmentation and alignment tasks  in
the case of ordering  however  this extra flexibility does not pay off  as the model distributes
its probability mass away from strong ordering patterns likely to occur in unseen data 
we can also identify the properties of a dataset that most strongly affect the performance
of our model  the constrained model variant performs slightly better than our full model
on rigidly formulaic domains  achieving highest performance on the phones data set  when
we know a priori that a domain is formulaic in structure  it is worthwhile to choose the
model variant that suitably enforces formulaic topic orderings  fortunately  this adaptation
can be achieved in the proposed framework using the prior of the generalized mallows
model  recall that the constrained variant is a special case of the full model 
however  the performance of our model is invariant with respect to other data set characteristics  across the two languages we considered  the model and baselines exhibit the
same comparative performance for each task  moreover  this consistency also holds between
the general interest cities articles and the highly technical chemical elements articles  finally  between the smaller citiesen and larger citiesen    data sets  we observe that our
results are consistent 

   conclusions and future work
in this paper  we have shown how an unsupervised topic based approach can capture content
structure  our resulting model constrains topic assignments in a way that requires global
modeling of entire topic sequences  we showed that the generalized mallows model is a
theoretically and empirically appealing way of capturing the ordering component of this
topic sequence  our results demonstrate the importance of augmenting statistical models
of text analysis with structural constraints motivated by discourse theory  furthermore 
our success with the gmm suggests that it could be applied to the modeling of ordering
constraints in other nlp applications 
there are multiple avenues of future extensions to this work  first  our empirical results
demonstrated that for certain domains providing too much flexibility in the model may
in fact be detrimental to predictive accuracy  in those cases  a more tightly constrained
variant of our model yields superior performance  an interesting extension of our current
   

ficontent modeling using latent permutations

model would be to allow additional flexibility in the prior of the gmm by drawing it from
another level of hyperpriors  from a technical perspective  this form of hyperparameter
re estimation would involve defining an appropriate hyperprior for the generalized mallows
model and adapting its estimation into our present inference procedure 
additionally  there may be cases when the assumption of one canonical topic ordering
for an entire corpus is too limiting  e g   if a data set consists of topically related articles
from multiple sources  each with its own editorial standards  our model can be extended
to allow for multiple canonical orderings by positing an additional level of hierarchy in the
probabilistic model  i e   document structures can be generated from a mixture of several
generalized mallows models  each with its own distributional mode  in this case  the
model would take on the additional burden of learning how topics are permuted between
these multiple canonical orderings  such a change to the model would greatly complicate
inference as re estimating a generalized mallows model canonical ordering is in general nphard  however  recent advances in statistics have produced efficient approximate algorithms
with theoretically guaranteed correctness bounds  ailon  charikar    newman        and
exact methods that are tractable for typical cases  meila et al         
more generally  the model presented in this paper assumes two specific global constraints
on content structure  while domains that satisfy these constraints are plentiful  there are
domains where our modeling assumptions do not hold  for example  in dialogue it is well
known that topics recur throughout a conversation  grosz   sidner         thereby violating
our first constraint  nevertheless  texts in such domains still follow certain organizational
conventions  e g  the stack structure for dialogue  our results suggest that explicitly incorporating domain specific global structural constraints into a content model would likely
improve the accuracy of structure induction 
another direction of future work is to combine the global topic structure of our model
with local coherence constraints  as previously noted  our model is agnostic toward the
relationships between sentences within a single topic  in contrast  models of local coherence
take advantage of a wealth of additional knowledge  such as syntax  to make decisions about
information flow across adjoining sentences  such a linguistically rich model would provide
a powerful representation of all levels of textual structure  and could be used for an even
greater variety of applications than we have considered here 

bibliographic note
portions of this work were previously presented in a conference publication  chen  branavan 
barzilay    karger         this article significantly extends our previous work  most notably
by introducing a new algorithm for applying our models output to the information ordering
task  section    and considering new data sets for our experiments that vary in genre 
language  and size  section    

acknowledgments
the authors acknowledge the funding support of the nsf career grant iis         and
grant iis          the nsf graduate fellowship  the office of naval research  quanta 
nokia  and the microsoft faculty fellowship  we thank the many people who offered

   

fichen  branavan  barzilay    karger

suggestions and comments on this work  including michael collins  aria haghighi  yoong
keok lee  marina meila  tahira naseem  christy sauper  david sontag  benjamin snyder 
and luke zettlemoyer  we are especially grateful to marina meila for introducing us to
the mallows model  this paper also greatly benefited from the thoughtful feedback of the
anonymous reviewers  any opinions  findings  conclusions  or recommendations expressed
in this paper are those of the authors  and do not necessarily reflect the views of the funding
organizations 

   

ficontent modeling using latent permutations

references
ailon  n   charikar  m     newman  a          aggregating inconsistent information 
ranking and clustering  journal of the acm         
althaus  e   karamanis  n     koller  a          computing locally coherent discourses 
in proceedings of acl 
bartlett  f  c          remembering  a study in experimental and social psychology  cambridge university press 
barzilay  r     elhadad  n          sentence alignment for monolingual comparable corpora 
in proceedings of emnlp 
barzilay  r   elhadad  n     mckeown  k          inferring strategies for sentence ordering
in multidocument news summarization  journal of artificial intelligence research 
         
barzilay  r     lapata  m          modeling local coherence  an entity based approach 
computational linguistics              
barzilay  r     lee  l          catching the drift  probabilistic content models  with applications to generation and summarization  in proceedings of naacl hlt 
barzilay  r   mckeown  k     elhadad  m          information fusion in the context of
multi document summarization  in proceedings of acl 
beeferman  d   berger  a     lafferty  j  d          statistical models for text segmentation 
machine learning             
bernardo  j  m     smith  a  f          bayesian theory  wiley series in probability and
statistics 
bishop  c  m          pattern recognition and machine learning  springer 
blei  d  m     moreno  p  j          topic segmentation with an aspect hidden markov
model  in proceedings of sigir 
blei  d  m   ng  a     jordan  m          latent dirichlet allocation  journal of machine
learning research             
bollegala  d   okazaki  n     ishizuka  m          a bottom up approach to sentence
ordering for multi document summarization  in proceedings of acl coling 
chen  h   branavan  s   barzilay  r     karger  d  r          global models of document
structure using latent permutations  in proceedings of naacl hlt 
chinchor  n          statistical significance of muc   results  in proceedings of the  th
conference on message understanding 
cohen  w  w   schapire  r  e     singer  y          learning to order things  journal of
artificial intelligence research             
eisenstein  j     barzilay  r          bayesian unsupervised topic segmentation  in proceedings of emnlp 
elsner  m   austerweil  j     charniak  e          a unified local and global model for
discourse coherence  in proceedings of naacl hlt 
   

fichen  branavan  barzilay    karger

fligner  m     verducci  j          distance based ranking models  journal of the royal
statistical society  series b                 
fligner  m  a     verducci  j  s          posterior probabilities for a consensus ordering 
psychometrika               
galley  m   mckeown  k  r   fosler lussier  e     jing  h          discourse segmentation
of multi party conversation  in proceedings of acl 
geman  s     geman  d          stochastic relaxation  gibbs distributions and the bayesian
restoration of images  ieee transactions on pattern analysis and machine intelligence             
graesser  a   gernsbacher  m     goldman  s   eds            handbook of discourse
processes  erlbaum 
griffiths  t  l     steyvers  m          finding scientific topics  proceedings of the national
academy of sciences                
griffiths  t  l   steyvers  m   blei  d  m     tenenbaum  j  b          integrating topics
and syntax  in advances in nips 
grosz  b  j     sidner  c  l          attention  intentions  and the structure of discourse 
computational linguistics                 
gruber  a   rosen zvi  m     weiss  y          hidden topic markov models  in proceedings
of aistats 
halliday  m  a  k     hasan  r          cohesion in english  longman 
hearst  m          multi paragraph segmentation of expository text  in proceedings of
acl 
ji  p  d     pulman  s          sentence ordering with manifold based classification in
multi document summarization  in proceedings of emnlp 
karamanis  n   poesio  m   mellish  c     oberlander  j          evaluating centeringbased metrics of coherence for text structuring using a reliably annotated corpus  in
proceedings of acl 
klementiev  a   roth  d     small  k          unsupervised rank aggregation with distancebased models  in proceedings of the icml  pp         
lapata  m          probabilistic text structuring  experiments with sentence ordering  in
proceedings of acl 
lapata  m          automatic evaluation of information ordering  kendalls tau  computational linguistics                 
lebanon  g     lafferty  j          cranking  combining rankings using conditional probability models on permutations  in proceedings of icml 
malioutov  i     barzilay  r          minimum cut model for spoken lecture segmentation 
in proceedings of acl 
mallows  c  l          non null ranking models  biometrika             

   

ficontent modeling using latent permutations

meila  m   phadnis  k   patterson  a     bilmes  j          consensus ranking under the
exponential model  in proceedings of uai 
neal  r  m          slice sampling  annals of statistics             
nelken  r     shieber  s  m          towards robust context sensitive sentence alignment
for monolingual corpora  in proceedings of eacl 
noreen  e  w          computer intensive methods for testing hypotheses  an introduction  wiley 
pevzner  l     hearst  m  a          a critique and improvement of an evaluation metric
for text segmentation  computational linguistics           
porteous  i   newman  d   ihler  a   asuncion  a   smyth  p     welling  m          fast
collapsed gibbs sampling for latent dirichlet allocation  in proceedings of sigkdd 
purver  m   kording  k   griffiths  t  l     tenenbaum  j  b          unsupervised topic
modelling for multi party spoken discourse  in proceedings of acl coling 
radev  d   jing  h     budzikowska  m          centroid based summarization of multiple documents  sentence extraction  utility based evaluation and user studies  in
proceedings of anlp naacl summarization workshop 
riezler  s     maxwell  j  t          on some pitfalls in automatic evaluation and significance testing for mt  in proceedings of the acl workshop on intrinsic and extrinsic
evaluation measures for machine translation and or summarization 
schiffrin  d   tannen  d     hamilton  h  e   eds            the handbook of discourse
analysis  blackwell 
titov  i     mcdonald  r          modeling online reviews with multi grain topic models 
in proceedings of www 
utiyama  m     isahara  h          a statistical model for domain independent text segmentation  in proceedings of acl 
van mulbregt  p   carp  i   gillick  l   lowe  s     yamron  j          text segmentation and
topic tracking on broadcast news via a hidden markov model approach  in proceedings
of icslp 
wallach  h  m          topic modeling  beyond bag of words  in proceedings of icml 
wray  a          formulaic language and the lexicon  cambridge university press  cambridge 

   

fi