journal of artificial intelligence research                  

submitted        published      

domain and function  a dual space model
of semantic relations and compositions
peter d  turney

peter turney nrc cnrc gc ca

national research council canada
ottawa  ontario  canada  k a  r 

abstract
given appropriate representations of the semantic relations between carpenter and wood
and between mason and stone  for example  vectors in a vector space model   a suitable
algorithm should be able to recognize that these relations are highly similar  carpenter is
to wood as mason is to stone  the relations are analogous   likewise  with representations
of dog  house  and kennel  an algorithm should be able to recognize that the semantic
composition of dog and house  dog house  is highly similar to kennel  dog house and kennel
are synonymous   it seems that these two tasks  recognizing relations and compositions 
are closely connected  however  up to now  the best models for relations are significantly
different from the best models for compositions  in this paper  we introduce a dual space
model that unifies these two tasks  this model matches the performance of the best
previous models for relations and compositions  the dual space model consists of a space
for measuring domain similarity and a space for measuring function similarity  carpenter
and wood share the same domain  the domain of carpentry  mason and stone share the
same domain  the domain of masonry  carpenter and mason share the same function  the
function of artisans  wood and stone share the same function  the function of materials 
in the composition dog house  kennel has some domain overlap with both dog and house
 the domains of pets and buildings   the function of kennel is similar to the function of
house  the function of shelters   by combining domain and function similarities in various
ways  we can model relations  compositions  and other aspects of semantics 

   introduction
the distributional hypothesis is that words that occur in similar contexts tend to have similar
meanings  harris        firth         many vector space models  vsms  of semantics use
a wordcontext matrix to represent the distribution of words over contexts  capturing the
intuition behind the distributional hypothesis  turney   pantel         vsms have achieved
impressive results at the level of individual words  rapp         but it is not clear how to
extend them to the level of phrases  sentences  and beyond  for example  we know how to
represent dog and house with vectors  but how should we represent dog house  
one approach to representing dog house is to treat it as a unit  the same way we handle
individual words  we call this the holistic or noncompositional approach to representing
phrases  the holistic approach may be suitable for some phrases  but it does not scale up 
with a vocabulary of n individual words  we can have n   two word phrases  n   threeword phrases  and so on  even with a very large corpus of text  most of these possible
phrases will never appear in the corpus  people are continually inventing new phrases  and
we are able to understand these new phrases although we have never heard them before 
we are able to infer the meaning of a new phrase by composition of the meanings of the
c
    
national research council canada  reprinted with permission 

fiturney

component words  this scaling problem could be viewed as an issue of data sparsity  but
it is better to think of it as a problem of linguistic creativity  chomsky        fodor  
lepore         to master natural language  algorithms must be able to represent phrases
by composing representations of individual words  we cannot treat all n grams  n      the
way we treat unigrams  individual words   on the other hand  the holistic approach is ideal
for idiomatic expressions  e g   kick the bucket  for which the meaning cannot be inferred
from the component words 
the creativity and novelty of natural language require us to take a compositional approach to the majority of the n grams that we encounter  suppose we have vector representations of dog and house  how can we compose these representations to represent dog
house   one strategy is to represent dog house by the average of the vectors for dog and
house  landauer   dumais         this simple proposal actually works  to a limited degree
 mitchell   lapata               however boat house and house boat would be represented
by the same average vector  yet they have different meanings  composition by averaging
does not deal with the order sensitivity of phrase meaning  landauer        estimates that
    of the meaning of english text comes from word choice and the remaining     comes
from word order 
similar issues arise with the representation of semantic relations  given vectors for
carpenter and wood  how can we represent the semantic relations between carpenter and
wood   we can treat carpenter  wood as a unit and search for paraphrases of the relations
between carpenter and wood  turney      b   in a large corpus  we could find phrases such
as the carpenter cut the wood  the carpenter used the wood  and wood for the carpenter  this
variation of the holistic approach can enable us to recognize that the semantic relations
between carpenter and wood are highly similar to the relations between mason and stone 
however  the holistic approach to semantic relations suffers from the same data sparsity
and linguistic creativity problems as the holistic approach to semantic composition 
we could represent the relation between carpenter and wood by averaging their vectors 
this might enable us to recognize that carpenter is to wood as mason is to stone  but it
would incorrectly suggest that carpenter is to wood as stone is to mason  the problem of
order sensitivity arises with semantic relations just as it arose with semantic composition 
many ideas have been proposed for composing vectors  landauer   dumais       
kintsch        mitchell   lapata         erk and pado        point out two problems
that are common to several of these proposals  first  often they do not have the adaptive
capacity to represent the variety of possible syntactic relations in a phrase  for example  in
the phrase a horse draws  horse is the subject of the verb draws  whereas it is the object of
the verb in the phrase draws a horse  the composition of the vectors for horse and draws
must be able to adapt to a variety of syntactic contexts in order to properly model the
given phrases  second  a single vector is too weak to handle a long phrase  a sentence  or
a document  a single vector can only encode a fixed amount of structural information if
its dimensionality is fixed  but there is no upper limit on sentence length  and hence on the
amount of structure to be encoded  erk   pado        p        a fixed dimensionality
does not allow information scalability 
simple  unweighted  averaging of vectors lacks adaptive capacity  because it treats all
kinds of composition in the same way  it does not have the flexibility to represent different
modes of composition  a good model must have the capacity to adapt to different situations 
   

fidomain and function  a dual space model

for example  with weighted averaging  the weights can be tuned for different syntactic
contexts  mitchell   lapata              
information scalability means that the size of semantic representations should grow in
proportion to the amount of information that they are representing  if the size of the
representation is fixed  eventually there will be information loss  on the other hand  the
size of representations should not grow exponentially 
one case where the problem of information scalability arises is with approaches that
map multiple vectors into a single vector  for example  if we represent dog house by adding
the vectors for dog and house  mapping two vectors into one   there may be information
loss  as we increase the number of vectors that are mapped into a single vector  we will
eventually reach a point where the single vector can no longer contain the information from
the multiple vectors  this problem can be avoided if we do not try to map multiple vectors
into a single vector 
suppose we have a k dimensional vector with floating point elements of b bits each  such
a vector can hold at most kb bits of information  even if we allow b to grow  if k is fixed 
we will eventually have information loss  in a vector space model of semantics  the vectors
have some resistance to noise  if we perturb a vector with noise below some threshold  
there is no significant change in the meaning that it represents  therefore we should think
of the vector as a hypersphere with a radius of   rather than a point  we may also put
bounds  r   r  on the range of the values of the elements in the vector   there is a finite
number n of hyperspheres of radius  that can be packed into a bounded k dimensional
space  conway   sloane         according to information theory  if we have a finite set
of n messages  then we need at most log   n   bits to encode a message  likewise  if we
have a finite set of n vectors  then a vector represents at most log   n   bits of information 
therefore the information capacity of a single vector in bounded k dimensional space is
limited to log   n   bits 
past work suggests that recognizing relations and compositions are closely connected
tasks  kintsch              mangalath  quesada    kintsch         the goal of our research
is a unified model that can handle both compositions and relations  while also resolving the
issues of linguistic creativity  order sensitivity  adaptive capacity  and information scalability 
these considerations have led us to a dual space model  consisting of a domain space for
measuring domain similarity  i e   topic  subject  or field similarity  and a function space
for measuring function similarity  i e   role  relationship  or usage similarity  
in an analogy a   b    c   d  a is to b as c is to d  for example  traffic is to street as water is
to riverbed   a and b have relatively high domain similarity  traffic and street come from the
domain of transportation  and c and d have relatively high domain similarity  water and
riverbed come from the domain of hydrology   on the other hand  a and c have relatively
high function similarity  traffic and water have similar roles in their respective domains 
they are both things that flow  and b and d have relatively high function similarity  street
and riverbed have similar roles in their respective domains  they are both things that carry
things that flow   by combining domain and function similarity in appropriate ways  we
   in models where the vectors are normalized to unit length  e g   models that use cosine to measure
similarity   the elements must lie within the range          if any element is outside this range  then
the length of the vector will be greater than one  in general  floating point representations have minimum
and maximum values 

   

fiturney

can recognize that the semantic relations between traffic and street are analogous to the
relations between water and riverbed 
for semantic composition  the appropriate way to combine similarities may depend on
the syntax of the composition  lets focus on noun modifier composition as an example 
in the noun modifier phrase ab  for instance  brain doctor   the head noun b  doctor  is
modified by an adjective or noun a  brain   suppose we have a word c  neurologist  that
is synonymous with ab  the functional role of the noun modifier phrase ab is determined
by the head noun b  a brain doctor is a kind of doctor  and b has a relatively high degree
of function similarity with c  doctor and neurologist both function as doctors   both a and
b have a high degree of domain similarity with c  brain  doctor  and neurologist all come
from the domain of clinical neurology   by combining domain and function similarity  we
can recognize that brain doctor is synonymous with neurologist 
briefly  the proposal is to compose similarity measures instead of composing vectors 
that is  we apply various mathematical functions to combine cosine similarity measures 
instead of applying the functions directly to the vectors  this addresses the information
loss problem  because we preserve the vectors for the individual component words   we do
not map multiple vectors into a single vector   since we have two different spaces  we also
have flexibility to address the problem of adaptive capacity   this model is compositional 
so it resolves the linguistic creativity problem  we deal with order sensitivity by combining
similarity measures in ways that recognize the effects of word order 
it might be argued that what we present here is not a model of semantic composition  but
a way to compare the words that form two phrases in order to derive a measure of similarity
of the phrases  for example  in section     we derive a measure of similarity for the phrases
environment secretary and defence minister  but we do not actually provide a representation
for the phrase environment secretary  on the other hand  most past work on the problem
of semantic composition  reviewed in section      yields a representation for the composite
phrase environment secretary that is different from the union of the representations of the
component words  environment and secretary 
this argument is based on the assumption that the goal of semantic composition is to
create a single  general purpose  stand alone representation of a phrase  as a composite 
distinct from the union of the representations of the component words  this assumption
is not necessary and our approach does not use this assumption  we believe that this
assumption has held back progress on the problem of semantic composition 
we argue that what we present here is a model of semantic composition  but it is composition of similarities  not composition of vectors  vectors can represent individual words  but
similarities inherently represent relations between two  or more  things  composing vectors
can yield a stand alone representation of a phrase  but composing similarities necessarily
yields a linking structure that connects a phrase to other phrases  similarity composition
does not result in a stand alone representation of a phrase  but practical applications do
not require stand alone representations  whatever practical tasks can be performed with
stand alone representations of phrases  we believe can be performed equally well  or better 
with similarity composition  we discuss this issue in more depth in section   
   two similarity spaces give us more options for similarity composition than one space  just as two types
of characters    and    give us more options for generating strings than one type of character    alone  

   

fidomain and function  a dual space model

the next section surveys related work on the modeling of semantic composition and
semantic relations  section   describes how we build domain and function space  to test
the hypothesis that there is value in having two separate spaces  we also create mono space 
which is the merger of the domain and function spaces  we then present four sets of experiments with the dual space model in section    we evaluate the dual space approach with
multiple choice analogy questions from the sat  turney      b   multiple choice nounmodifier composition questions derived from wordnet  fellbaum         phrase similarity rating problems  mitchell   lapata         and similarity versus association problems
 chiarello  burgess  richards    pollock         we discuss the experimental results in
section    section   considers some theoretical questions about the dual space model  limitations of the model are examined in section    section   concludes 
this paper assumes some familiarity with vector space models of semantics  for an
overview of semantic vsms  see the papers in the handbook of latent semantic analysis
 landauer  mcnamara  dennis    kintsch         the review in mitchell and lapatas
       paper  or the survey by turney and pantel        

   related work
here we examine related work with semantic composition and relations  in the introduction  we mentioned four problems with semantic models  which yield four desiderata for a
semantic model 
   linguistic creativity  the model should be able to handle phrases  in the case of
semantic composition  or word pairs  in the case of semantic relations  that it has
never seen before  when it is familiar with the component words 
   order sensitivity  the model should be sensitive to the order of the words in a
phrase  for composition  or a word pair  for relations   when the order affects the
meaning 
   adaptive capacity  for phrases  the model should have the flexibility to represent
different kinds of syntactic relations  for word pairs  the model should have the
flexibility to handle a variety of tasks  such as measuring the degree of relational
similarity between two pairs  see section      versus measuring the degree of phrasal
similarity between two pairs  see section      
   information scalability  for phrases  the model should scale up with neither loss of
information nor exponential growth in representation size as the number of component
words in the phrases increases  for n ary semantic relations  turney      a   the
model should scale up with neither loss of information nor exponential growth in
representation size as n  the number of terms in the relations  increases 
we will review past work in the light of these four considerations 
    semantic composition
let ab be a phrase  such as a noun modifier phrase  and assume that we have vectors a and
b that represent the component words a and b  one of the earliest proposals for semantic
composition is to represent ab by the vector c that is the average of a and b  landauer  
   

fiturney

dumais         if we are using a cosine measure of vector similarity  taking the average of a
set of vectors  or their centroid  is the same as adding the vectors  c   a b  vector addition
works relatively well in practice  mitchell   lapata               although it lacks order
sensitivity  adaptive capacity  and information scalability  regarding order sensitivity and
adaptive capacity  mitchell and lapata              suggest using weights  c   a b  and
tuning the weights to different values for different syntactic relations  in their experiments
 mitchell   lapata         weighted addition performed better than unweighted addition 
kintsch        proposes a variation of additive composition
in which c is the sum of a 
p
b  and selected neighbours ni of a and b  c   a   b   i ni   the neighbours are vectors for
other words in the given vocabulary  i e   other rows in the given wordcontext matrix   the
neighbours are chosen in a manner that attempts to address order sensitivity and adaptive
capacity  but there is still a problem with information scalability due to fixed dimensionality 
utsumi        presents a similar model  but with a different way of selecting neighbours 
mitchell and lapata        found that a simple additive model peformed better than an
additive model that included neighbours 
mitchell and lapata              suggest element wise multiplication as a composition
operation  c   a fi b  where ci   ai  bi   like vector addition  element wise multiplication suffers from a lack of order sensitivity  adaptive capacity  and information scalability 
nonetheless  in an experimental evaluation of seven compositional models and two noncompositional models  element wise multiplication had the best performance  mitchell  
lapata        
another approach is to use a tensor product for composition  smolensky        aerts
  czachor        clark   pulman        widdows         such as the outer product 
c   a  b  the outer product of two vectors  a and b   each with n elements  is an n  n
matrix  c   the outer product of three vectors is an n  n  n third order tensor  this
results in an information scalability problem  the representations grow exponentially large
as the phrases grow longer   furthermore  the outer product did not perform as well as
element wise multiplication in mitchell and lapatas        experiments  recent work with
tensor products  clark  coecke    sadrzadeh        grefenstette   sadrzadeh        has
attempted to address the issue of information scalability 
circular convolution is similar to the outer product  but the outer product matrix is
compressed back down to a vector  c   a   b  plate        jones   mewhort        
this avoids information explosion  but it results in information loss  circular convolution
performed poorly in mitchell and lapatas        experiments 
baroni and zamparelli        and guevara        suggest another model of composition
for adjective noun phrases  the core strategy that they share is to use a few holistic vectors
to train a compositional model  with partial least squares regression  plsr   we can learn
a linear model that maps the vectors for the component nouns and adjectives to linear
approximations of the holistic vectors for the phrases  the linguistic creativity problem is
avoided because the linear model only needs a few holistic vectors for training  there is no
need to have holistic vectors for all plausible adjective noun phrases  given a phrase that is
not in the training data  the linear model predicts the holistic vector for the phrase  given
   there are ways to avoid the exponential growth  for example  a third order tensor with a rank of   on
all three modes may be compactly encoded by its three component vectors  kolda and bader       
discuss compact tensor representations 

   

fidomain and function  a dual space model

the component vectors for the adjective and the noun  this works well for adjective noun
phrases  but it is not clear how to generalize it to other parts of speech or to longer phrases 
one application for semantic composition is measuring the similarity of phrases  erk  
pado        mitchell   lapata         kernel methods have been applied to the closely
related task of identifying paraphrases  moschitti   quarteroni         but the emphasis
with kernel methods is on syntactic similarity  rather than semantic similarity 
neural network models have been combined with vector space models for the task of
language modeling  bengio  ducharme  vincent    jauvin        socher  manning    ng 
      socher  huang  pennington  ng    manning         with impressive results  the goal
of a language model is to estimate the probability of a phrase or to decide which of several
phrases is the most likely  vsms can improve the probability estimates of a language model
by measuring the similarity of the words in the phrases and smoothing probabilities over
groups of similar words  however  in a language model  words are considered similar to
the degree that they can be exchanged without altering the probability of a given phrase 
without regard to whether the exchange alters the meaning of the phrase  this is like
function similarity  which measures the degree to which words have similar functional roles 
but these language models are missing anything like domain similarity 
erk and pado        present a model that is similar to ours in that it has two parts  a
vector space for measuring similarity and a model of selectional preferences  their vector
space is similar to domain space and their model of selectional preferences plays a role
similar to function space  an individual word a is represented by a triple  a   ha  r  r  i 
consisting of the words vector  a  its selectional preferences  r  and its inverse selectional
preferences  r    a phrase ab is represented by a pair of triples  ha    b   i  the triple a  is
a modified form of the triple a that represents the individual word a  the modifications
adjust the representation to model how the meaning of a is altered by its relation to b in
the phrase ab  likewise  the triple b   is a modified form of the triple b that represents b 
such that b   takes into account how a affects b 
when a is transformed to a  to represent the influence of b on the meaning of a  the
vector a in a is transformed to a new vector a  in a    let rb be a vector that represents
the typical words that are consistent with the selectional preferences of b  the vector a 
is the composition of a with rb   erk and pado        use element wise multiplication for
composition  a    a fi rb   the intention is to make a more like a typical vector x that would
be expected for a phrase xb  likewise  for b  in b     we have b    b fi ra
erk and pados        model and related models  thater  furstenau    pinkal       
address linguistic creativity  order sensitivity  adaptive capacity  and information scalability 
but they are not suitable for measuring the similarity of semantic relations  consider the
analogy traffic is to street as water is to riverbed  let ha    b   i represent traffic  street and
let hc     d  i represent water  riverbed  the transformation of a  b  c  and d to a    b    
c     and d  reinforces the connection between traffic and street and between water and
riverbed  but it does not help us recognize the relational similarity between traffic  street
and water  riverbed  of course  these models were not designed for relational similarity  so
this is not surprising  however  the goal here is to find a unified model that can handle
both compositions and relations 
   

fiturney

    semantic relations
for semantic relations  we can make some general observations about order sensitivity  let
a   b and c   d be two word pairs and let simr  a   b  c   d     be a measure of the degree
of similarity between the relations of a   b and c   d  if a   b    c   d is a good analogy  then
simr  a   b  c   d  will have a relatively high value  in general  a good model of relational
similarity should respect the following equalities and inequalities 

simr  a   b  c   d    simr  b   a  d   c 

   

simr  a   b  c   d    simr  c   d  a   b 

   

simr  a   b  c   d     simr  a   b  d   c 

   

simr  a   b  c   d     simr  a   d  c   b 

   

for example  given that carpenter  wood and mason  stone make a good analogy  it follows
from equation   that wood  carpenter and stone  mason make an equally good analogy  also 
according to equation    mason  stone and carpenter  wood make a good analogy  on the
other hand  as suggested by equation    carpenter  wood is not analogous to stone  mason 
likewise  as indicated by equation    it is a poor analogy to assert that carpenter is to stone
as mason is to wood 
rosario and hearst        present an algorithm for classifying word pairs according
to their semantic relations  they use a lexical hierarchy to map word pairs to feature
vectors  any classification scheme implicitly tell us something about similarity  two word
pairs that are in the same semantic relation class are implicitly more relationally similar
than two word pairs in different classes  when we consider the relational similarity that is
implied by rosario and hearsts        algorithm  we see that there is a problem of order
sensitivity  equation   is violated 
let simh  x  y     be a measure of the degree of hierarchical similarity between the
words x and y  if simh  x  y  is relatively high  then x and y share a common hypernym
relatively close to them in the given lexical hierarchy  in essence  the intuition behind
rosario and hearsts        algorithm is  if both simh  a  c  and simh  b  d  are high  then
simr  a   b  c   d  should also be high  that is  if simh  a  c  and simh  b  d  are high enough 
then a   b and c   d should be assigned to the same relation class 
for example  consider the analogy mason is to stone as carpenter is to wood  the common hypernym of mason and carpenter is artisan  we can see that simh  mason  carpenter 
is high  the common hypernym of stone and wood is material  hence simh  stone  wood  is
high  it seems that a good analogy is indeed characterized by high values for simh  a  c  and
simh  b  d   however  the symmetry of simh  x  y  leads to a problem  if simh  b  d  is high 
then simh  d  b  must also be high  but this implies that simr  a   d  c   b  is high  that is  we
incorrectly conclude that mason is to wood as carpenter is to stone  see equation    
some later work with classifying semantic relations has used different algorithms  but
the same underlying intuition about hierarchical similarity  rosario  hearst    fillmore 
      nastase   szpakowicz        nastase  sayyad shirabad  sokolova    szpakowicz 
       we use a similar intuition here  since similarity in function space is closely related
   

fidomain and function  a dual space model

to hierarchical similarity  simh  x  y   as we will see later  section       however  including
domain space in the relational similarity measure saves us from violating equation   
let simf  x  y     be function similarity as measured by the cosine of vectors x and y
in function space  let simd  x  y     be domain similarity as measured by the cosine of
vectors x and y in domain space  like past researchers  rosario   hearst        rosario
et al         nastase   szpakowicz        veale        nastase et al          we look for
high values of simf  a  c  and simf  b  d  as indicators that simr  a   b  c   d  should be high  but
we also look for high values of simd  a  b  and simd  c  d   continuing the previous example 
we do not conclude that mason is to wood as carpenter is to stone  because wood does not
belong in the domain of masonry and stone does not belong in the domain of carpentry 
let d be a determiner  e g   the  a  an   hearst        showed how patterns of the form
d x such as d y  a bird such as a crow  or d y is a kind of x  the crow is a kind
of bird  can be used to infer that x is a hypernym of y  bird is a hypernym of crow  
a pairpattern matrix is a vsm in which the rows are word pairs and the columns are
various x       y patterns  turney  littman  bigham  and shnayder        demonstrated
that a pairpattern vsm can be used to measure relational similarity  suppose we have a
pair pattern matrix x in which the word pair a   b corresponds to the row vector xi and c   d
corresponds to xj   the approach is to measure the relational similarity simr  a   b  c   d  by
the cosine of xi and xj  
at first the patterns in these pairpattern matrices were generated by hand  turney
et al         turney   littman         but later work  turney      b  used automatically
generated patterns  other authors have used variations of this technique  nakov   hearst 
            davidov   rappoport        bollegala  matsuo    ishizuka        o seaghdha
  copestake         all of these models suffer from the linguistic creativity problem 
because the models are noncompositional  holistic   they cannot scale up to handle the
huge number of possible pairs  even the largest corpus cannot contain all the pairs that a
human speaker might use in daily conversation 
turney      b  attempted to handle the linguistic creativity problem within a holistic
model by using synonyms  for example  if a corpus does not contain traffic and street within
a certain window of text  perhaps it might contain traffic and road  if it does not contain
water and riverbed  perhaps it has water and channel  however  this is at best a partial
solution  turneys      b  algorithm required nine days to process     multiple choice sat
analogy questions  using the dual space model  without specifying in advance what word
pairs it might face  we can answer the     questions in a few seconds  see section      
compositional models scale up better than holistic models 
mangalath et al         presented a model for semantic relations that represents word
pairs with vectors of ten abstract relational categories  such as hyponymy  meronymy  taxonomy  and degree  the approach is to construct a kind of second order vector space in
which the elements of the vectors are degrees of similarity  calculated from cosines with a
first order wordcontext matrix 
for instance  carpenter  wood can be represented by a second order vector composed of
ten cosines calculated from first order vectors  in this second order vector  the value of the
element corresponding to  say  meronymy would be the cosine of two first order vectors  x
and y  the vector x would be the sum of the first order vectors for carpenter and wood 
the vector y would be the sum of several vectors for words that are related to meronymy 
   

fiturney

such as part  whole  component  portion  contains  constituent  and segment  the cosine of
x and y would indicate the degree to which carpenter and wood are related to meronymy 
mangalath et al s        model suffers from information scalability and order sensitivity
problems  information loss takes place when the first order vectors are summed and also
when the high dimensional first order space is reduced to a ten dimensional second order
space  the order sensitivity problem is that the second order vectors violate equation   
because the pairs c   d and d   c are represented by the same second order vector 
a natural proposal is to represent a word pair a   b the same way we would represent
a phrase ab  that is  whatever compositional model we have for phrases could also be
applied to word pairs  however any problems that the compositional model has with order
sensitivity or information scalability carry over to word pairs  for example  if we represent
a   b by c   a   b or c   a fi b  then we violate equation    because a   b   b   a and
a fi b   b fi a 

   three vector spaces
in this section  we describe three vector space models  all three spaces consist of word
context matrices  in which the rows correspond to words and the columns correspond to the
contexts in which the words occur  the differences among the three spaces are in the kinds
of contexts  domain space uses nouns for context  function space uses verb based patterns
for context  and mono space is a merger of the domain and function contexts  mono space
was created in order to test the hypothesis that it is useful to separate the domain and
function spaces  mono space serves as a baseline 
    constructing the wordcontext matrices
building the three spaces involves a series of steps  there are three main steps  each of
which has a few substeps  the first and last steps are the same for all three spaces  the
differences in the spaces are the result of differences in the second step 
   find terms in contexts  input  a corpus and a lexicon  output  terms in contexts 
     extract terms from the lexicon and find their frequencies in the corpus 
     select all terms above a given frequency as candidate rows for the frequency
matrix 
     for each selected term  find phrases in the corpus that contain the term within
a given window size 
     use a tokenizer to split the phrases into tokens 
     use a part of speech tagger to tag the tokens in the phrases 
   build a termcontext frequency matrix  input  terms in contexts  output  a
sparse frequency matrix 
     convert the tagged phrases into contextual patterns  candidate columns  
     for each contextual pattern  count the number of terms  candidate rows  that
generated the pattern and rank the patterns in descending order of their counts 
     select the top nc contextual patterns as the columns of the matrix 
   

fidomain and function  a dual space model

     from the initial set of rows  from step       drop any row that does not match
any of the top nc contextual patterns  yielding the final set of nr rows 
     for each row  term  and each column  contextual pattern   count the number
of phrases  from step      containing the given term and matching the given
pattern  and output the resulting numbers as a sparse frequency matrix 
   weight the elements and smooth the matrix  input  a sparse frequency matrix 
output  the singular value decomposition  svd  of the weighted matrix 
     convert the raw frequencies to positive pointwise mutual information  ppmi 
values 
     apply svd to the ppmi matrix and output the svd component matrices 
the input corpus in step   is a collection of web pages gathered from university websites
by a webcrawler   the corpus contains approximately       words  which comes to about
    gigabytes of plain text  to facilitate finding term frequencies and sample phrases  we
indexed this corpus with the wumpus search engine  buttcher   clarke          the rows
for the matrices were selected from terms  words and phrases  in the wordnet lexicon  
we found that selecting terms from wordnet resulted in subjectively higher quality than
simply selecting terms with high corpus frequencies 
in step      we extract all unique words and phrases  n grams  from the index sense file
in wordnet      skipping n grams that contain numbers  only letters  hyphens  and spaces
are allowed in the n grams   we find the n gram corpus frequencies by querying wumpus
with each n gram  all n grams with a frequency of at least     and at least   characters
are candidate rows in step      for each selected n gram  we query wumpus to find a
maximum of        phrases in step       the phrases are limited to a window of   words
to the left of the n gram and   words to the right  for a total window size of      n words 
we use opennlp       to tokenize and part of speech tag the phrases  steps     and       
the tagged phrases come to about    gigabytes  
in step      we generate contextual patterns from the part of speech tagged phrases 
different kinds of patterns are created for the three different kinds of spaces  the details
of this step are given in the following subsections  each phrase may yield several patterns 
the three spaces each have more than         rows  with a maximum of        phrases
per row and several patterns per phrase  this can result in millions of distinct patterns  so
we filter the patterns in steps     and      we select the top nc patterns that are shared
by the largest number of rows  given the large number of patterns  they may not all fit
in ram  to work with limited ram  we use the linux sort command  which is designed
to efficiently sort files that are too large to fit in ram  for each row  we make a file of
the distinct patterns generated by that row  we then concatenate all of the files for all of
  
  
  
  

the corpus was collected by charles clarke at the university of waterloo 
wumpus is available at http   www wumpus search org  
wordnet is available at http   wordnet princeton edu  
the limit of        phrases per n gram is required to make wumpus run in a tolerable amount of time 
finding phrases is the most time consuming step in the construction of the spaces  we use a solid state
drive  ssd  to speed up this step 
   opennlp is available at http   incubator apache org opennlp  
   the tagged phrases are available from the author on request 

   

fiturney

the rows and alphabetically sort the patterns in the concatenated file  in the sorted file 
identical patterns are adjacent  which makes it easy to count the number of occurrences of
each pattern  after counting  a second sort operation yields a ranked list of patterns  from
which we select the top nc  
it is possible that some of the candidate rows from step     might not match any of
the patterns from step      these rows would be all zeros in the matrix  so we remove
them in step      finally  we output a sparse frequency matrix f with nr rows and nc
columns  if the i th row corresponds to the n gram wi and the j th column corresponds to
the contextual pattern cj   then the value of the element fij in f is the number of phrases
containing wi  from step      that generate the pattern cj  in step       in step      we use
svdlibc      to calculate the singular value decomposition  so the format of the output
sparse matrix in step     is chosen to meet the requirements of svdlibc   
in step      we apply positive pointwise mutual information  ppmi  to the sparse frequency matrix f  this is a variation of pointwise mutual information  pmi   church  
hanks        turney        in which all pmi values that are less than zero are replaced
with zero  niwa   nitta        bullinaria   levy         let x be the matrix that results
when ppmi is applied to f  the new matrix x has the same number of rows and columns
as the raw frequency matrix f  the value of an element xij in x is defined as follows 
fij
pij   pnr pnc

j   fij

i  

   

pnc

j   fij
pi   pnr pnc

   

pnr
f
pncij
  pnr i  

   

i  

pj

i  



j   fij
j   fij

pij
pi pj



pmiij   log

pmiij if pmiij    
xij  
  otherwise

   
   

in this definition  pij is the estimated probability that the word wi occurs in the context
cj   pi is the estimated probability of the word wi   and pj is the estimated probability of
the context cj   if wi and cj are statistically independent  then pij   pi pj  by the definition
of independence   and thus pmiij is zero  since log          the product pi pj is what we
would expect for pij if wi occurs in cj by pure random chance  on the other hand  if there
is an interesting semantic relation between wi and cj   then we should expect pij to be larger
than it would be if wi and cj were indepedent  hence we should find that pij   pi pj   and
thus pmiij is positive  if the word wi is unrelated to  or incompatible with  the context cj  
we may find that pmiij is negative  ppmi is designed to give a high value to xij when there
is an interesting semantic relation between wi and cj   otherwise  xij should have a value of
zero  indicating that the occurrence of wi in cj is uninformative 
    svdlibc is available at http   tedlab mit edu dr svdlibc  

   

fidomain and function  a dual space model

finally  in step      we apply svdlibc to x  svd decomposes x into the product of
three matrices uvt   where u and v are in column orthonormal form  i e   the columns
are orthogonal and have unit length  ut u   vt v   i  and  is a diagonal matrix of
singular values  golub   van loan         if x is of rank r  then  is also of rank r  let
k   where k   r  be the diagonal matrix formed from the top k singular values  and let uk
and vk be the matrices produced by selecting the corresponding columns from u and v 
the matrix uk k vkt is the matrix of rank k that best approximates the original matrix x 
in the sense that it minimizes the approximation errors  that is  x   uk k vkt minimizes
kx  xkf over all matrices x of rank k  where k       kf denotes the frobenius norm  golub
  van loan         the final output is the three matrices  uk   k   and vk   that form the
truncated svd  x   uk k vkt  
    domain space
the intuition behind domain space is that the domain or topic of a word is characterized by
the nouns that occur near it  we use a relatively wide window and we ignore the syntactic
context in which the nouns appear 
for domain space  in step      each tagged phrase generates at most two contextual
patterns  the contextual patterns are simply the first noun to the left of the given n gram
 if there is one  and the first noun to the right  if there is one   since the window size is  
words on each side of the n gram  there are usually nouns on both sides of the n gram  the
nouns may be either common nouns or proper nouns  opennlp uses the penn treebank
tags  santorini         which include several different categories of noun tags  all of the
noun tags begin with a capital n  so we simply extract the first words to the left and right
of the n gram that have tags that begin with n  the extracted nouns are converted to lower
case  if the same noun appears on both sides of the n gram  only one contextual pattern
is generated  the extracted patterns are always unigrams  in a noun compound  only the
component noun closest to the n gram is extracted 
table   shows some examples for the n gram boat  note that the window of   words
does not count punctuation  so the number of tokens in the window may be greater than
the number of words in the window  we can see from table   that the row vector for
the n gram boat in the frequency matrix f will have nonzero values  for example  in the
columns for lake and summer  assuming that these contextual patterns make it through the
filtering in step      
for step      we set nc to         in step      after we drop rows that are all zero 
we are left with nr equal to          after ppmi  which sets negative elements to zero 
we have             nonzero values  for a matrix density of        table   shows the
contextual patterns for the first five columns and the last five columns  the columns are in
order of their ranks in step       the count column of the table gives the number of rows
 n grams  that generate the pattern  that is  these are the counts mentioned in step      
the last patterns all begin with c because they have the same counts and ties are broken
by alphabetical order 
   

fiturney

tagged phrases
would md visit vb big nnp lake nnp and cc take vb our prp 
boat nn on in this dt huge jj beautiful jj lake nn     there ex
was vbd

patterns
lake

 

the dt large jj paved jj parking nn lot nn in in the dt boat nn
ramp nn area nn and cc walk vb south rb along in the dt

lot
ramp

 

building vbg permit nn       anyway rb     we prp should md
have vb a dt boat nn next jj summer nn with in skiing nn
and cc tubing nn paraphernalia nns    

permit
summer

 

table    examples of step     in domain space for the n gram boat  the three tagged
phrases generate five contextual patterns 

column
 
 
 
 
 

pattern
time
part
years
way
name

count
      
      
      
      
      

column
      
      
      
      
      

pattern
clu
co conspirator
conciseness
condyle
conocer

count
   
   
   
   
   

table    contextual patterns for the first and last columns in domain space  clu is an
abbreviation for chartered life underwriter and other terms  condyle is a round
bump on a bone where it forms a joint with another bone  and conocer is the
spanish verb to know  in the sense of being acquainted with a person 

    function space
the concept of function space is that the function or role of a word is characterized by the
syntactic context that relates it to the verbs that occur near it  we use a more narrow
window for function space than domain space  based on the intuition that proximity to a
verb is important for determining the functional role of the given word  a distant verb is
less likely to characterize the function of the word  we generate relatively complex patterns
for function space  to try to capture the syntactic patterns that connect the given word to
the nearby verbs 
in step      each tagged phrase generates up to six contextual patterns  for a given
tagged phrase  the first step is to cut the window down to   tokens before the given n gram
and   tokens after it  if any of the remaining tokens to the left of the n gram are punctuation  the punctuation and everything to the left of the punctuation is removed  if any of
the remaining tokens to the right of the n gram are punctuation  the punctuation and everything to the right of the punctuation is removed  lets call the remaining tagged phrase
a truncated tagged phrase 
next we replace the given n gram in the truncated tagged phrase with a generic marker 
   

fidomain and function  a dual space model

x  we then simplify the part of speech tags by reducing them all to their first character
 santorini         for example  all of the various verb tags  vb  vbd  vbg  vbn  vbp 
vbz  are reduced to v  if the truncated tagged phrase contains no v tag  it generates
zero contextual patterns  if the phrase contains a v tag  then we generate two types of
contextual patterns  general patterns and specific patterns 
for the general patterns  the verbs  every token with a v tag  have their tags removed
 naked verbs  and all other tokens are reduced to naked tags  tags without words   for the
specific patterns  verbs  modals  tokens with m tags   prepositions  tokens with i tags   and
to  tokens with t tags  have their tags removed and all other tokens are reduced to naked
tags   see table   for examples  
for both general and specific patterns  to the left of x  we trim any leading naked tags 
to the right of x  we trim any trailing naked tags  a t tag can only be to  so we replace
any remaining naked t tags with to  a sequence of n tags  n n or n n n  is likely a
compound noun  so we reduce the sequence to a single n 
for a given truncated tagged phrase  we now have two patterns  one general pattern
and one specific pattern  if either of these patterns has tokens on both the left and right
sides of x  we make two more patterns by duplicating the x and then splitting the pattern
at the point between the two xs  if one of the new patterns does not have a verb  we drop
it  thus we may now have up to three specific patterns and three general patterns for the
given truncated tagged phrase  if the specific and general patterns are the same  only one
of them is generated 
table   shows some examples for the n gram boat  note that every pattern must contain
the generic marker  x  and at least one verb 
truncated tagged phrases
the dt canals nns by in boat nn
and cc wandering vbg the dt

patterns
x c wandering
by x c wandering

types
general
specific

 

a dt charter nn fishing vbg boat nn
captain nn named vbn jim nnp

fishing x n named
fishing x
x n named

general
general
general

 

used vbn from in a dt
and cc lowered vbd to to

used i d x c lowered
used i d x
x c lowered
used from d x c lowered to
used from d x
x c lowered to

general
general
general
specific
specific
specific

 

boat nn

table    examples of step     in function space for the n gram boat  the three truncated
tagged phrases generate eleven contextual patterns 

for step      we set nc to         in step      after rows that are all zero are dropped 
nr is          after ppmi  there are            nonzero values  yielding a matrix density
of        table   shows the contextual patterns for the first and the last five columns  the
   

fiturney

last patterns all begin with s because they have the same counts and ties are broken by
alphabetical order 
column
 
 
 
 
 

pattern
x is
x n is
is d x
is x
x was

count
      
      
      
      
      

column
      
      
      
      
      

pattern
since d x n was
sinking i d x
supplied with x
supports d x n of
suppressed i d x

count
   
   
   
   
   

table    contextual patterns for the first and last columns in function space 
the contextual patterns for function space are more complex than the patterns for
domain space  the motivation for this greater complexity is the observation that mere
proximity is not enough to determine functional roles  although it seems sufficient for determining domains  for example  consider the verb gives  if there is a word x that occurs near
gives  x could be the subject  direct object  or indirect object of the verb  to determine the
functional role of x  we need to know which case applies  the syntactic context that connects x to gives provides this information  the contextual pattern x gives implies that
x is the subject  gives x implies x is an object  likely the direct object  and gives to x
suggests that x is the indirect object  modals and prepositions supply further information
about the functional role of x in the context of a given verb  the verb gives appears in   
different contextual patterns  i e      of the        columns in function space correspond to
syntactic patterns that contain gives  
many of the row vectors in the function space matrix correspond to verbs  it might
seem surprising that we can characterize the function of a verb by its syntactic relation to
other verbs  but consider an example  such as the verb run  the row vector for run in the
ppmi matrix for function space has       nonzero values  that is  run is characterized by
      different contextual patterns 
note that appearing in a contextual pattern is different from having a nonzero value
for a contextual pattern  the character string for the word run appears in    different
contextual patterns  such as run out of x  the row vector for the word run has nonzero
values for       contextual patterns  columns   such as had to x 
    mono space
mono space is simply the merger of domain space and function space  for step      we
take the union of the        domain space columns and the        function space columns 
resulting in a total nc of         columns  in step      we have a total nr of        
rows  the mono matrix after ppmi has             nonzero values  yielding a density of
       the values in the mono frequency matrix f equal the corresponding values in the
domain and function matrices  some of the rows in the mono space matrix do not have
corresponding rows in the function space matrix  for these rows  the corresponding values
are zeros  but there are nonzero elements in these rows  which correspond to values in the
domain matrix  
   

fidomain and function  a dual space model

    summary of the spaces
table   summarizes the three matrices  in the following four sets of experiments  we use
the same three matrices  the domain  function  and mono matrices  in all cases  we do not
generate different matrices for each set of experiments  three of the four sets of experiments
involve datasets that have been used in past by other researchers  we made no special
effort to ensure that the words in these three datasets have corresponding rows in the three
matrices  the intention is that these three matrices should be adequate to handle most
applications without any special customization 
space
domain
function
mono

rows  nr  
       
       
       

columns  nc  
      
      
       

nonzeros  after ppmi 
           
          
           

density  after ppmi 
     
     
     

table    summary of the three spaces 

    using the spaces to measure similarity
in the following experiments  we measure the similarity of two terms  a and b  by the cosine
of the angle  between their corresponding row vectors  a and b 
sim a  b    cos a  b   

a
b

kak kbk

    

the cosine of the angle between two vectors is the inner product of the vectors  after they
have been normalized to unit length  the cosine ranges from   when the vectors point in
opposite directions   is     degrees  to    when they point in the same direction   is  
degrees   when the vectors are orthogonal   is    degrees   the cosine is zero  with raw
frequency vectors  which necessarily cannot have negative elements  the cosine cannot be
negative  but weighting and smoothing often introduce negative elements  ppmi weighting
does not yield negative elements  but truncated svd can generate negative elements  even
when the input matrix has no negative values 
the semantic similarity of two terms is given by the cosine of the two corresponding rows
in uk pk  see section       there are two parameters in uk pk that need to be set  the
parameter k controls the number of latent factors and the parameter p adjusts the weights
of the factors  by raising the corresponding singular values in pk to the power p  the
parameter k is well known in the literature  landauer et al          but p is less familiar 
the use of p was suggested by caron         in the following experiments  section     we
explore a range of values for p and k 
suppose we take a word w and list all of the other words in descending order of their
cosines with w  using uk pk to calculate the cosines  when p is high  as we go down the list 
the cosines of the nearest neighbours of w decrease slowly  when p is low  they decrease
quickly  that is  a high p results in a broad  fuzzy neighbourhood and a low p yields a sharp 
crisp neighbourhood  the parameter p controls the sharpness of the similarity measure 
   

fiturney

to reduce the running time of svdlibc  we limit the number of singular values to
      which usually results in less than      singular values  for example  the svd for
domain space has      singular values  as long as k is not greater than       we can
experiment with a range of k values without rerunning svdlibc  we can generate uk pk
from u     p     by simply deleting the       k columns with the smallest singular values 
in the experiments  we vary k from     to      in increments of         values for k 
and we vary p from   to    in increments of         values for p   when p is    we
give more weight to the factors with smaller singular values  when p is     the factors with
larger singular values have more weight  caron        observes that most researchers use
either p     or p      that is  they use either uk or uk k  
let simf  a  b     be function similarity as measured by the cosine of vectors a and b
in function space  let simd  a  b     be domain similarity as measured by the cosine of
vectors a and b in domain space  when a similarity measure combines both simd  a  b  and
simf  a  b   there are four parameters to tune  kd and pd for domain space and kf and pf for
function space 
for one space  it is feasible for us to explore all              combinations of parameter
values  but two spaces have                    combinations of values  to make the search
tractable  we initialize the parameters to the middle of their ranges  kf   kd       and
pf   pd      and then we alternate between tuning simd  a  b   i e   kd and pd   while holding
simf  a  b   i e   kf and pf   fixed and tuning simf  a  b  while holding simd  a  b  fixed  we stop
the search when there is no improvement in performance on the training data  in almost
all cases  a local optimum is found in one pass  that is  after we have tuned the parameters
once  there is no improvement when we try to tune them a second time  thus we typically
evaluate              parameter values    because we tune one similarity  tune the other 
and then try the first again to see if further improvement is possible    
we could use a standard numerical optimization algorithm to tune the four parameters  but the algorithm we use here takes advantage of background knowledge about the
optimization task  we know that small variations in the parameters make small changes
in performance  so there is no need to make a very fine grained search  and we know that
simd  a  b  and simf  a  b  are relatively independent  so we can optimize them separately 
the rows in the matrices are based on terms in the wordnet index sense file  in this
file  all nouns are in their singular forms and all verbs are in their stem forms  to calculate
sim a  b   we first look for exact matches for a and b in the terms that correspond to the
rows of the given matrix  domain  function  or mono   if an exact match is found  then
we use the corresponding row vector in the matrix  otherwise  we look for alternate forms
of the terms  using the validforms function in the wordnet  querydata perl interface to
wordnet    this automatically converts plural nouns to their singular forms and verbs to
their stem forms  if none of the alternate forms is an exact match for a row in the matrix 
we map the term to a zero vector of length k 

    we use perl data language  pdl  for searching for parameters  calculating cosines  and other operations
on vectors and matrices  see http   pdl perl org  
    wordnet  querydata is available at http   search cpan org dist wordnet querydata  

   

fidomain and function  a dual space model

    composing similarities
our approach to semantic relations and compositions is to combine the two similarities 
simd  a  b  and simf  a  b   in various ways  depending on the task at hand or the syntax
of the phrase at hand  in general  we want the combined similarity to be high when the
component similarities are high  and we want the values of the component similarities to be
balanced  to achieve balance  we use the geometric mean to combine similarities  instead
of the arithmetic mean  the geometric mean is not suitable for negative numbers  and the
cosine can be negative in some cases  hence we define the geometric mean as zero if any of
the component similarities are negative 

geo x    x            xn    

 x  x        xn    n if xi     for all i              n
  otherwise

    

    element wise multiplication
one of the most successful approaches to composition  so far  has been element wise multiplication  c   a fi b  where ci   ai  bi  mitchell   lapata               this approach
only makes sense when the elements in the vectors are not negative  when the elements in
a and b are positive  relatively large values of ai and bi reinforce each other  resulting in a
large value for ci   this makes intuitive sense  but when ai and bi are both highly negative 
ci will be highly positive  although intuition says ci should be highly negative  mitchell and
lapata              designed their wordcontext matrices to ensure that the vectors had
no negative elements 
the values in the matrix uk pk are typically about half positive and half negative  we
use element wise multiplication as a baseline in some of the following experiments  for a
fair baseline  we cannot simply apply element wise multiplication to row vectors in uk pk  
one solution would be to use the ppmi matrix  x  which has no negative elements  but this
would not allow element wise multiplication to take advantage of the smoothing effect of
svd  our solution is to use row vectors from x   uk k vkt   although the ppmi matrix 
x  is sparse  see table     x and uk pk have a density of      
let a  and b  be the vectors in x that correspond to the terms a and b  these row
vectors benefit from smoothing due to truncated svd  but their elements are almost all
positive  if there are any negative elements  we set them to zero  let c    a  fi b    after
we apply element wise multiplication to the vectors  we then multiply by vk kp    so that
the resulting vector c   c  vk p 
can be compared with other row vectors in the matrix
k
p
uk k  
p 
t
x vk p 
k      uk k vk   vk k  

 
 
 

uk k vkt vk kp 
uk k p 
k
uk pk

    
    
    
    

note that  since vk is column orthonormal  vkt vk equals ik   the k  k identity matrix 
   

fiturney

similarly  if a is a row vector in uk pk   we can find its counterpart a  in x by multiplying
t
a with  p
k vk  
t
 uk pk   k p vkt     uk pk  p
k vk

    

  uk k vkt

    

  x

    

let nn x   nn for nonnegative  be a function that converts negative elements in a vector
x to zero 

nn hx            xn i    hy            yn i

xi if xi    
yi  
  otherwise

    
    

our version of element wise multiplication may be expressed as follows 
p 
 p t
t
c    nn a p
k vk   fi nn bk vk    vk k

    

another way to deal with element wise multiplication would be to use nonnegative
matrix factorization  nmf   lee   seung        instead of svd  we have not yet found
an implementation of nmf that scales to the matrix sizes that we have here  table     in
our past experiments with smaller matrices  svd and nmf have similar performance 

   experiments with varieties of similarities
this section presents four sets of experiments  the first set of experiments presents a dualspace model of semantic relations and evaluates the model with multiple choice analogy
questions from the sat  the second set presents a model of semantic composition and
evaluates it with multiple choice questions that are constructed from wordnet  the third
set applies a dual space model to the phrase similarity dataset of mitchell and lapata
        the final set uses three classes of word pairs from chiarello et al         to test a
hypothesis about the dual space model  that domain space and function space capture the
intuitive concepts of association and similarity 
    similarity of relations
here we evaluate the dual space model applied to the task of measuring the similarity of
semantic relations  we use a set of     multiple choice analogy questions from the sat
college entrance exam  turney      b   table   gives an example of one of the questions 
the task is to select the choice word pair that is most analogous  most relationally similar 
to the stem word pair 
let a   b represent the stem pair  e g   lull  trust   we answer the sat questions by
selecting the choice pair c   d that maximizes the relational similarity  simr  a   b  c   d   defined
as follows 
   

fidomain and function  a dual space model

stem 
choices 

solution 

   
   
   
   
   
   

lull trust
balk fortitude
betray loyalty
cajole compliance
hinder destination
soothe passion
cajole compliance

table    an example of a question from the     sat analogy questions  lulling a person
into trust is analogous to cajoling a person into compliance 

sim   a   b  c   d    geo simf  a  c   simf  b  d  

    

sim   a   b  c   d    geo simd  a  b   simd  c  d  

    

sim   a   b  c   d    geo simd  a  d   simd  c  b  

sim   a   b  c   d  if sim   a   b  c   d   sim   a   b  c   d 
simr  a   b  c   d   
  otherwise

    
    

the intent of sim  is to measure the function similarity across the two pairs  the domain
similarity inside the two pairs is measured by sim    whereas the domain similarity across the
two pairs is given by sim    the relational similarity  simr   is simply the function similarity 
sim    subject to the constraint that the domain similarity inside pairs  sim    must not be
less than the domain similarity across pairs  sim   
figure   conveys the main ideas behind equations    to     we want high function
similarities  indicated by  f  for a   c and b   d  as measured by sim    we also prefer
relatively high domain similarities  marked with  d  for a   b and c   d  measured by sim    
in contrast to relatively low domain similarities   d  for a   d and c   b  as given by sim      
using the example in table    we see that lulling a person into trust is analogous to
cajoling a person into compliance  since the functional role of lull is similar to the functional
role of cajole  both involve manipulating a person  and the functional role of trust is similar
to the functional role of compliance  both are states that a person can be in   this is
captured by sim    the constraint sim   a   b  c   d   sim   a   b  c   d  implies that the
domain similarities of lull  trust  the domain of confidence and loyalty  and cajole  compliance
 the domain of obedience and conformity  should be greater than or equal to the domain
similarities of lull  compliance and cajole  trust 
analogy is a way of mapping knowledge from a source domain to a target domain
 gentner         if a in the source domain is mapped to c in the target domain  then a
should play the same role in the source domain as c plays in the target domain  this is
the theory behind sim    if a and b are in the source domain and c and d are in the target
    we recently came across this same rectangular structure in lepage and shin ichis        paper on
morphological analogy  see their figure     although our algorithm and our task differ considerably
from the algorithm and task of lepage and shin ichi         we have independently discovered the same
underlying structure in analogical reasoning 

   

fiturney

d

a
d

b
d

f

c

f

d

d

simr  a   b  c   d 
relational similarity

figure    a diagram of the reasoning behind equations    to      f represents high
function similarity   d means high domain similarity  and  d indicates low
domain similarity 

domain  then the internal domain similarity of a and b and the internal domain similarity of
c and d should not be less than the cross domain similarities  this motivates the constraint
sim   sim    our definition is a natural expression of gentners        theory of analogy 
recall the four equations that we introduced in section      we repeat these equations
here for convenience 

simr  a   b  c   d    simr  b   a  d   c 

    

simr  a   b  c   d    simr  c   d  a   b 

    

simr  a   b  c   d     simr  a   b  d   c 

    

simr  a   b  c   d     simr  a   d  c   b 

    

inspection will show that the definition of relational similarity in equation    satisfies the
requirements of equations             and     this can be understood by considering
figure    equation    tells us that we can rotate figure   about its vertical axis without
altering the network of similarities  due to the symmetry of the figure  equation    tells
us that we can rotate figure   about its horizontal axis without altering the network of
similarities 
on the other hand  we cannot swap c and d while holding a and b fixed  because this
would change both the  f and  d links  although it would not change the  d links  
in other words  sim  and sim  would be changed  although sim  would not be affected 
therefore equation    is satisfied 
also  we cannot swap b and d while holding a and c fixed  because this would change
the  d and  d links  although it would not change the  f links   in other words  sim 
and sim  would be changed  although sim  would not be affected  therefore equation   
   

fidomain and function  a dual space model

is satisfied  we can see that sim  by itself would violate equation     due to the symmetry
of cosines  simf  b  d    simf  d  b   the constraint sim   a   b  c   d   sim   a   b  c   d  breaks
this symmetry 
another way to break the symmetry  so that equation    is satisfied  would be to use a
similarity measure that is inherently asymmetric  such as skew divergence  in equation    
the symmetry is broken in a natural way by considering how domain and function similarity
apply to analogies  so there is no need to introduce an inherently asymmetric measure  also 
note that the symmetries of equations    and    are desirable  we do not wish to break
these symmetries 
it would have been reasonable to include simd  a  c  and simd  b  d  in sim    but we decided
to leave them out  it seems to us that the function similarities simf  a  c  and simf  b  d  
which should have high values in a good analogy  might cause simd  a  c  and simd  b  d 
to be relatively high  even though they cross domains  if people observe a certain kind
of abstract function similarity frequently  that function similarity might become a popular
topic for discussion  which could result in a high domain similarity 
for example  carpenter  wood is analogous to mason  stone  the domain of carpenter  wood
is carpentry and the domain of mason  stone is masonry  the functional role of carpenter
is similar to the functional role of mason  in that both are artisans  although carpenter
and mason belong to different domains  their high degree of abstract function similarity
may result in discussions that mention them together  such as discussions about specialized trades  skilled manual labour  the construction industry  and workplace injuries  in
other words  high function similarity between two words may cause a rise in their domain
similarity  therefore we did not include simd  a  c  and simd  b  d  in sim   
when all five choices for a sat question have a relational similarity of zero  we skip the
question  we use ten fold cross validation to set the parameters for the sat questions  the
same parameter values are selected in nine of the ten folds  kd        pd        kf       
and pf        after the parameters are determined  all     sat questions can be answered
in a few seconds  equation    correctly answers     of the questions  skips   questions  and
incorrectly answers     questions  achieving an accuracy of       
      comparison with past work
for comparison  the average score for senior highschool students applying to us universities
is        the acl wiki lists many past results with the     sat questions    table  
shows the top ten results at the time of writing  in this table  dual space refers to the dualspace model using equation     four of the past results achieved an accuracy of      
or higher  all four used holistic approaches and hence are not able to address the issue of
linguistic creativity  the best previous algorithm attains an accuracy of            correct 
  skipped      incorrect   turney      b   the difference between       and       is not
statistically significant at the     confidence level  according to fishers exact test 
the majority of the algorithms in table   are unsupervised  but dual space  pairclass
 turney      b   and bagpack  herdagdelen   baroni        use limited supervision  pairclass and bagpack answer a given sat question by learning a binary classification model
that is specific to the given question  the training set for a given question consists of one
    see http   aclweb org aclwiki index php title sat analogy questions 

   

fiturney

algorithm
lsa predication
know best
k means
bagpack
vsm
dual space
bmi
pairclass
pert
lra
human

reference
mangalath et al        
veale       
bicici and yuret       
herdagdelen and baroni       
turney and littman       
bollegala et al        
turney      b 
turney      a 
turney      b 
average us college applicant

accuracy
    
    
    
    
    
    
    
    
    
    
    

    confidence
        
        
        
        
        
        
        
        
        
        
        

table    the top ten results with the     sat questions  from the acl wiki  the    
confidence intervals are calculated using the binomial exact test 

positive training example  the stem pair for the question  and ten randomly selected pairs
as  assumed  negative training examples  the induced binary classifier is used to assign
probabilities to the five choices and the most probable choice is the guess  dual space uses
the training set only to tune four numerical parameters  these three algorithms are best
described as weakly supervised 
      sensitivity to parameters
to see how sensitive the dual space model is to the values of the parameters  we perform
two exhaustive grid searches  one with a coarse  wide grid and another with a fine  narrow
grid  for each point in the grids  we evaluate the dual space model using the whole set
of     sat questions  the narrow grid search is centred on the parameter values that
were selected in nine of the ten folds in the previous experiment  kd        pd       
kf        and pf        both searches evaluate   values for each parameter  yielding a total
of          parameter settings  table   shows the values that were explored in the two grid
searches and table   presents the minimum  maximum  average  and standard deviation of
the accuracy for the two searches 
grid
coarse

fine

parameter
kd
pd
kf
pf
kd
pd
kf
pf

   
    
   
    
   
    
   
   

   
    
   
    
   
    
   
   

values
        
   
   
        
   
   
   
   
    
   
   
   
   
   

    
   
    
   
    
   
   
   

table    the range of parameter values for the two grid searches 
   

fidomain and function  a dual space model

grid
coarse
fine

minimum
    
    

accuracy
maximum average
    
    
    
    

standard deviation
   
   

table    the sensitivity of the dual space model to the parameter settings 
the accuracy attained by the heuristic search  described in section      with ten fold
cross validation         table     is near the best accuracy of the fine grid search using the
whole set of     sat questions         table     this is evidence that the heuristic search is
effective  accuracy with the coarse search varies from       to        which demonstrates
the importance of tuning the parameters  on the other hand  accuracy with the fine search
spans a narrower range and has a lower standard deviation  which suggests that the dualspace model is not overly sensitive to relatively small variations in the parameter values 
that is  the parameters are reasonably stable   that nine of the ten folds in cross validation
select the same parameters is further evidence of stability  
      parts of speech
since domain space is based on nouns and function space is based on verbs  it is interesting
to know how the performance of the dual space model varies with different parts of speech 
to answer this  we manually labeled all     sat questions with part of speech labels  the
labels for a single pair can be ambiguous  but the labels become unambiguous in the context
of the whole question  for example  lull  trust could be noun  verb  but in the context of
table    it must be verb  noun 
table    splits out the results for the various parts of speech  none of the differences
in this table are statistically significant at the     confidence level  according to fishers
exact test  a larger and more varied set of questions will be needed to determine how part
of speech affects the dual space model 
parts of speech
noun noun
noun adjective or adjective noun
noun verb or verb noun
adjective adjective
verb adjective or adjective verb
verb verb
verb adverb or adverb verb
all

right
  
  
  
 
  
  
 
   

accuracy
    
    
    
    
    
    
   
    

wrong
  
  
  
  
 
 
 
   

skipped
 
 
 
 
 
 
 
 

total
   
  
  
  
  
  
 
   

table     performance of the dual space model with various parts of speech 

      order sensitivity
it seems that function space is doing most of the work in equation     if we use sim  alone 
dropping the constraint that sim   sim    then accuracy drops from       to        this
   

fiturney

drop is not statistically significant  we hypothesize that the small drop is due to the design
of the sat test  which is primarily intended to test a students understanding of functional
roles  not domains 
to verify this hypothesis  we reformulated the sat questions so that they would test
both function and domain comprehension  the method is to first expand each choice pair
c   d by including the stem pair a   b  resulting in the full explicit analogy a   b    c   d  for each
expanded choice  a   b    c   d  we then generate another choice  a   d    c   b  table    shows
the reformulation of table    due to symmetry  sim  must assign the same similarity to
both a   b    c   d and a   d    c   b  this new ten choice test evaluates both function and domain
similarities 
choices 

solution 

   
   
   
   
   
   
   
   
   
    
   

lull trust  balk fortitude
lull fortitude  balk trust
lull loyalty  betray trust
lull trust  betray loyalty
lull compliance  cajole trust
lull trust  cajole compliance
lull destination  hinder trust
lull trust  hinder destination
lull trust  soothe passion
lull passion  soothe trust
lull trust  cajole compliance

table     an expanded sat question  designed to test both function and domain comprehension  choices     and     have the same similarity according to sim   

the task with the expanded ten choice sat questions is the same as with the original
five choice questions  to select the best analogy  the solution in table    is the same as the
solution in table    except that the stem pair is explicit in table     the only signficant
change is that five new distractors have been added to the choices  we answer the ten choice
questions by selecting the choice a   b    c   d that maximizes simr  a   b  c   d  
on the ten choice reformulated sat test  simr  equation     attains an accuracy of
       whereas sim  alone  equation     only achieves        the difference is statistically
significant at the     confidence level  according to fishers exact test  this more stringent
test supports the claim that function similarity is insufficient by itself 
as a further test of the value of two separate spaces  we use a single space for both
simd and simf in equation     the model still has four parameters it can tune  kd   pd   kf  
and pf   but the same matrix is used for both similarities  the best result is an accuracy of
      on the ten question reformulated sat test  using function space for both simd and
simf   this is significantly below the       accuracy of the dual space model when simd is
based on domain space and simf is based on function space      confidence level  fishers
exact test  
table    summarizes the results  in the cases where the matrix for simd is not used 
the model is based on sim  alone  equation      in all other cases  the model is based
on simr  equation      for both the five choice and ten choice sat questions  the original
   

fidomain and function  a dual space model

dual space model is more accurate than any of the modified models  the significant column
indicates whether the accuracy of a modified model is significantly less than the original
dual space model      confidence level  fishers exact test   the more difficult ten choice
questions clearly show the value of two distinct spaces 
algorithm
dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space

accuracy
    
    
    
    
    
    
    
    
    
    
    
    
    
    

significant
no
yes
yes
no
yes
yes
yes
yes
yes
yes
yes
yes

questions
five choice
five choice
five choice
five choice
five choice
five choice
five choice
ten choice
ten choice
ten choice
ten choice
ten choice
ten choice
ten choice

matrix for simd
domain space
function space
mono space
domain space
not used
not used
not used
domain space
function space
mono space
domain space
not used
not used
not used

matrix for simf
function space
function space
mono space
domain space
function space
mono space
domain space
function space
function space
mono space
domain space
function space
mono space
domain space

table     accuracy with the original five choice questions and the reformulated ten choice
questions  in the modified models  we intentionally use the wrong matrix  or
no matrix  for simd or simf   the modified models show that accuracy decreases
when only one space is used 

      summary
the dual space model performs as well as the current state of the art holistic model and
addresses the issue of linguistic creativity  the results with the reformulated sat questions
support the claim that there is value in having two separate spaces 
as we mentioned in section      the task of classifying word pairs according to their
semantic relations  rosario   hearst        rosario et al         nastase   szpakowicz 
      is closely connected to the problem of measuring relational similarity  turney      b 
applied a measure of relational similarity to relation classification by using cosine similarity
as a measure of nearness in a nearest neighbour supervised learning algorithm  the dualspace model  equation     is also suitable for relation classification with a nearest neighbour
algorithm 
    similarity of compositions
in this second set of experiments  we apply the dual space model to noun modifier compositions  given vectors for dog  house  and kennel  we would like to be able to recognize that
dog house and kennel are synonymous  we compare the dual space model to the holistic
approach  vector addition  and element wise multiplication  the approaches are evaluated
   

fiturney

using multiple choice questions that are automatically generated from wordnet  using the
wordnet  querydata perl interface to wordnet  table    gives an example of one of the
noun modifier questions 
stem 
choices 

solution 

   
   
   
   
   
   
   
   

dog house
kennel
dog
house
canine
dwelling
effect
largeness
kennel

table     an example of a multiple choice noun modifier composition question 
in these questions  the stem is a bigram and the choices are unigrams  choice     is
the correct answer      is the modifier  and     is the head noun  choice     is a synonym
or hypernym of the modifier and     is a synonym or hypernym of the head noun  if no
synonyms or hypernyms can be found  a noun is randomly chosen  the last two choices     
and      are randomly selected nouns  choices     and     can be either nouns or adjectives 
but the other choices must be nouns 
the stem bigram and the choice unigrams must have corresponding rows in function
space  the space with the least number of rows   the stem bigram must have a noun sense
in wordnet  it may also have senses for other parts of speech   the solution unigram      
must be a member of the synset  synonym set  for the first noun sense of the stem bigram
 the most frequent or dominant sense of the bigram  when the bigram is used as a noun  
and it cannot be simply the hyphenation  dog house  or concatenation  doghouse  of the
stem bigram 
these requirements result in a total of      seven choice questions  which we randomly
split into     for training  parameter tuning  and      for testing    the questions are deliberately designed to be difficult  in particular  all of the approaches are strongly attracted
to choices     and      furthermore  we did not attempt to ensure that the stem bigrams are
compositional  some of them may be idiomatic expressions that no compositional approach
could possibly get right  we did not want to bias the questions by imposing theories about
distinguishing compositions and idioms in their construction 
let ab represent a noun modifier bigram  dog house  and let c represent a unigram
 kennel   we answer the multiple choice questions by selecting the unigram that maximizes
the compositional similarity  simc  ab  c   defined as follows 

sim   ab  c    geo simd  a  c   simd  b  c   simf  b  c  

sim   ab  c  if a    c and b    c
simc  ab  c   
  otherwise
    the questions are available as an online appendix at http   jair org  

   

    
    

fidomain and function  a dual space model

equations    and    are illustrated in figure   
a

  

b

d

d f

  

c
simc  ab  c 
noun modifier compositional similarity

figure    a diagram of equations    and    
the thinking behind sim  is that c  kennel   should have high domain similarity with
both the modifier a  dog  and the head noun b  house   furthermore  the function of the
bigram ab  dog house  is determined by the head noun b  house   so the head noun should
have high function similarity with c  kennel    we add the constraints a    c and b    c
because sim  by itself tends to have high values for sim   ab  a  and sim   ab  b     it seems
plausible that humans use constraints like this  we reason that dog house cannot mean the
same thing as house  because then the extra word dog in dog house would serve no purpose 
it would be meaningless noise   
the constraints a    c and b    c could be expressed in terms of similarities  such as
simd  a  c    t and simd  b  c    t  where t is a high threshold  e g   t         but this would
add another parameter to the model  we decided to keep the model relatively simple 
when all seven choices for a noun modifier question have a compositional similarity of
zero  we skip the question  on the training set  the best parameter settings are kd       
pd        kf        and pf        on the testing set  equation    correctly answers    
questions  skips    questions  and incorrectly answers      yielding an accuracy of       
      comparison with other approaches
mitchell and lapata        compared many different approaches to semantic composition in
their experiments  but they only considered one task  the task we examine in section      
in this paper  we have chosen to compare a smaller number of approaches on a larger
number of tasks  we include element wise multiplication in these experiments  because this
approach had the best performance in mitchell and lapatas        experiments  vector
    in spite of these constraints  it is still worthwhile to include the head noun and the modifier as distractors
in the multiple choice questions  because it enables us to experimentally evaluate the impact of these
distractors on the various algorithms when the constraints are removed  see table      also  future users
of this dataset may find a way to avoid these distractors without explicit constraints 
    in philosophy of language  grice        argued that proper interpretation of language requires us to
charitably assume that speakers generally do not insert random words into their speech 

   

fiturney

addition is included due to its historical importance and its simplicity  although mitchell
and lapata        found that weighted addition was better than unweighted addition  we
do not include weighted addition in our experiments  because it did not perform as well as
element wise multiplication in mitchell and lapatas        experiments  we include the
holistic model as a noncompositional baseline 
table    compares the dual space model with the holistic model  element wise multiplication  and vector addition  for the latter three models  we try all three spaces 
algorithm
dual space
holistic
holistic
holistic
multiplication
multiplication
multiplication
addition
addition
addition

space
domain and function
mono
domain
function
mono
domain
function
mono
domain
function

accuracy
    
    
    
    
    
    
    
    
    
    

table     results for the noun modifier questions 
in this table  dual space refers to the dual space model using equation     in the holistic
model  ab is represented by its corresponding row vector in the given space  recall from
section     that  in step      the rows in the matrices correspond to n grams in wordnet 
where n may be greater than one  thus  for example  dog house has a corresponding
row vector in all three of the spaces  the holistic model simply uses this row vector as
the representation of dog house  for element wise multiplication  ab is represented using
equation     with the vector addition model  ab is represented by a   b  where the vectors
are normalized to unit length before they are added  all four models use the constraints
a    c and b    c  all four models use the training data for parameter tuning 
the difference between the dual space model         and the best variation of elementwise multiplication         is not statistically significant at the     confidence level  according to fishers exact test  however  the difference between the dual space model
        and the best variation of vector addition         is significant 
      limitations of the holistic approach
for all three spaces  the holistic model is significantly better than all other models  but its
inability to address the issue of linguistic creativity is a major limitation  the      multiplechoice questions that we have used in these experiments were intentionally constructed with
the requirement that the stem bigram must have a corresponding row in function space  see
above   this was done so that we could use the holistic model as a baseline  however  it
gives the misleading impression that the holistic model is a serious competitor with the
compositional approaches  by design  table    shows what the holistic model can achieve
under ideal  but unrealistic  conditions 
   

fidomain and function  a dual space model

mitchell and lapatas        dataset  used in the experiments in section      illustrates
the limitations of the holistic model  the dataset consists of     distinct pairs of bigrams 
composed of     distinct bigrams  of the     bigrams           occur in wordnet  of
the     pairs of bigrams          contain bigrams that both occur in wordnet  given
the matrices we use here  with rows based on wordnet   the holistic approach would be
reduced to random guessing for     of the pairs in mitchell and lapatas        dataset 
it might be argued that the failure of the holistic approach with mitchell and lapatas
       dataset is due to our decision to base the rows of the matrices on terms from wordnet 
however  suppose we attempt to build a holistic model for all frequent bigrams  the web
 t   gram corpus  brants   franz        includes a list of all bigrams that appeared   
or more times in a terabyte of text  a total of             bigrams  using a compositional
approach  the matrices we use here can represent the majority of these bigrams  on the
other hand  the holistic approach would require a matrix with             rows  which is
considerably beyond the current state of the art 
one possibility is to build a matrix for the holistic approach as needed  for a given
input set of n grams  instead of building a large  static  multipurpose matrix  there are
two problems with this idea  first  it is slow  turney      b  used this approach for the
sat analogy questions  but it required nine days to run  whereas the dual space model can
process the sat questions in a few seconds  given a static  multipurpose matrix  second  it
requires a large corpus  and the corpus size must grow exponentially with n  the length of
the phrases  longer phrases are more rare  so larger corpora are needed to gather sufficient
data to model the phrases  larger corpora also result in longer processing times 
for a given application  it may be wise to have a predefined list of bigrams with holistic
representations  but it would not be wise to expect this list to be sufficient to cover most
bigrams that would be seen in practice  the creativity of human language use requires
compositional models  chomsky        fodor   lepore         although the holistic model
is included as a baseline in the experiments  it is not a competitor for the other models  it
can only supplement the other models 
      impact of constraints
if we use sim  alone  equation      dropping the constraints a    c and b    c  then accuracy
drops signficantly  from       to        however  all of the models benefit greatly from
these constraints  in table     we take the best variation of each model from table    and
look at what happens when the constraints are dropped 

algorithm
dual space
holistic
multiplication
addition

space
domain and function
mono
domain
domain

constraints
    
    
    
    

accuracy
no constraints
    
    
   
   

difference
     
     
     
     

table     the impact of the constraints  a    c and b    c  on accuracy 

   

fiturney

      element wise multiplication
in section      we argued that c   a fi b is not suitable for row vectors in the matrix uk pk
and we suggested equation    as an alternative  when we use c   a fi b with domain
space  instead of equation     performance drops significantly  from       to       
      impact of idioms
some of the gap between the holistic model and the other models may be due to idiomatic
bigrams in the testing questions  one of the most successful approaches to determining
whether a multiword expression  mwe  is compositional or noncompositional  idiomatic 
is to compare its holistic vector representation with its compositional vector representation
 for example  a high cosine between the two vectors suggests that the mwe is compositional 
not idiomatic   biemann   giesbrecht        johannsen  alonso  rishj    sgaard        
however  this approach is not suitable here  because we do not want to assume that the
gap is entirely due to idiomatic bigrams  instead  we would like to estimate how much of
the gap is due to idiomatic bigrams 
wordnet contains some clues that we can use as indicators that a bigram might be less
compositional than most bigrams  allowing that compositionality is a matter of degree  
one clue is whether the wordnet gloss of the bigram contains either the head noun or the
modifier  for example  the gloss of dog house is outbuilding that serves as a shelter for a
dog  which contains the modifier  dog  this suggests that dog house may be compositional 
we classified each of the      testing set questions as head  the first five characters in
the head noun of the bigram match the first five characters in a word in the bigrams gloss  
modifier  the first five characters in the modifier of the bigram match the first five characters
in a word in the bigrams gloss   both  both the head and the modifier match   or neither
 neither the head nor the modifier match   the four classes are approximately equally
distributed in the testing questions      head      modifier      both  and     neither   we
match on the first five characters to allow for cases like brain surgeon  which has the gloss
someone who does surgery on the nervous system  especially the brain   this bigram is
classified as both  because the first five characters of surgeon match the first five characters
of surgery 
table    shows how the accuracy of the models varies over the four classes of questions  for the three compositional models  dual space  multiplication  addition   the neither class is significantly less accurate than the other three classes  fishers exact test 
    confidence   but the difference is not significant for the holistic model  for the three
compositional models  the neither class is     to     less accurate than the other classes 
this supports the view that a significant fraction of the wrong answers of the compositional
models are due to noncompositional bigrams 
another clue for compositionality in wordnet is whether the head noun is a hypernym
of the bigram  for example  surgeon is a hypernym of brain surgeon  we classified each
of the      testing set questions as hyper  the head noun is a member of the synset of the
immediate hypernym for the first noun sense of the bigram  we do not look further up in the
hypernym hierarchy and we do not look at other senses of the bigram  or not  not hyper  
in the testing set      questions are hyper and     are not 
   

fidomain and function  a dual space model

algorithm
dual space
holistic
multiplication
addition

space
domain and function
mono
domain
domain

both
    
    
    
    

head
    
    
    
    

accuracy
modifier neither
    
    
    
    
    
    
    
    

all
    
    
    
    

table     the variation of accuracy for different classes of bigram glosses 
table    gives the accuracy of the models for each of the classes  this table has the
same general pattern as table     the three compositional models have significantly lower
accuracy for the not class  with decreases from    to     there is no significant difference
for the holistic model 
algorithm
dual space
holistic
multiplication
addition

space
domain and function
mono
domain
domain

accuracy
hyper not
         
         
         
         

all
    
    
    
    

table     the variation of accuracy for different classes of bigram hypernyms 

      order sensitivity
note that vector addition and element wise multiplication lack order sensitivity  but equation    is sensitive to order  simc  ab  c     simc  ba  c   we can see the impact of this by
reformulating the noun modifier questions so that they test for order sensitivity  first we
expand each choice unigram c by including the stem bigram ab  resulting in the explicit
comparison ab  c  for each expanded choice  ab  c  we then generate another choice 
ba  c  this increases the number of choices from seven to fourteen  due to symmetry 
vector addition and element wise multiplication must assign the same similarity to both
ab  c and ba  c 
table    compares the dual space model with element wise multiplication and vector addition  using the reformulated fourteen choice noun modifier questions  the holistic model
is not included in this table because there are no rows in the matrices for the reversed ba
bigrams  which may be seen as another illustration of the limits of the holistic model   on
this stricter test  the dual space model is significantly more accurate than both element wise
multiplication and vector addition  fishers exact test      confidence  
for the dual space model to perform well with the fourteen choice questions  we need
both simd and simf   if we drop simd from equation     function alone in table      then we
are ignoring the modifier and only paying attention to the head noun  accuracy drops from
      down to        if we drop simf from equation     domain alone in table      then
the equation becomes symmetrical  so the same similarity is assigned to both ab  c and
   

fiturney

algorithm
dual space
multiplication
modified dual space
modified dual space
addition

space
domain and function
domain
function alone
domain alone
domain

accuracy
    
    
    
    
    

table     results for the reformulated fourteen choice noun modifier questions 
ba  c  accuracy drops from       down to          the dual space model is significantly
more accurate than either of these modified dual space models  fishers exact test     
confidence  
      summary
with the reformulated fourteen choice noun modifier questions  table      the dual space is
significantly better than element wise multiplication and vector addition  with the original
seven choice questions  table      the difference is not as large  because these questions do
not test for order  unlike element wise multiplication and vector addition  the dual space
model addresses the issue of order sensitivity  unlike the holistic model  the dual space
addresses the issue of linguistic creativity 
    similarity of phrases
in this subsection  we apply the dual space model to measuring the similarity of phrases 
using mitchell and lapatas        dataset of human similarity ratings for pairs of phrases 
the dataset includes three types of phrases  adjective noun  noun noun  and verb object 
there are     pairs of each type               pairs of phrases   each pair of phrases was
rated by    human subjects  the ratings use a   point scale  in which   signifies the lowest
degree of similarity and   signifies the highest degree  table    gives some examples 
let ab represent the first phrase in a pair of phrases  environment secretary  and let cd
represent the second phrase  defence minister   we rate the similarity of the phrase pairs
by simp  ab  cd   defined as follows 
simp  ab  cd    geo simd  a  c   simd  b  d   simf  a  c   simf  b  d  

    

this equation is based on the instructions to the human participants  mitchell   lapata 
      appendix b   which imply that both function and domain similarity must be high
for a phrase pair to get a high similarity rating  figure   illustrates the reasoning behind
this equation  we want high domain and function similarities between the corresponding
components of the phrases ab and cd 
    it is only a coincidence that both modified dual space models have an accuracy of       on the fourteenchoice questions  although their aggregate accuracy is the same  on individual questions  the two models
typically select different choices 

   

fidomain and function  a dual space model

participant
   
   
   
   
   
   
   
   
   

phrase type
adjective noun
adjective noun
adjective noun
noun noun
noun noun
noun noun
verb object
verb object
verb object

group
 
 
 
 
 
 
 
 
 

phrase pair
certain circumstance  particular case
large number  great majority
further evidence  low cost
environment secretary  defence minister
action programme  development plan
city centre  research work
lift hand  raise head
satisfy demand  emphasise need
like people  increase number

similarity
 
 
 
 
 
 
 
 
 

table     examples of phrase pair similarity ratings from mitchell and lapatas       
dataset  similarity ratings vary from    lowest  to    highest  

a

b

d f

d f

c

d
simp  ab  cd 
phrasal similarity

figure    a diagram of equation    

      experimental setup
mitchell and lapata        divided their dataset into a development set  for tuning parameters  and an evaluation set  for testing the tuned models   the development set has
  ratings for each phrase pair and the evaluation set has    ratings for each phrase pair 
the development and evaluation sets contain the same phrase pairs  but with judgments by
different participants  thus there are               rated phrase pairs in the development
set and                  ratings in the evaluation set   
for a more challenging evaluation  we divide the dataset by phrase pairs rather than
by participants  our development set has     phrase pairs with    ratings each and the
evaluation set has     phrase pairs with    ratings each  for each of the three phrase types 
we randomly select    phrase pairs for the development set              phrase pairs  and
    the information in this paragraph is based on section     of the paper by mitchell and lapata       
and personal communication with jeff mitchell in june       

   

fiturney

   for the evaluation set              phrase pairs   thus there are                 
ratings in the development set and                  in the evaluation set 
mitchell and lapata        use spearmans rank correlation coefficient  spearmans
rho  to evaluate the performance of various vector composition algorithms on the task of
emulating the human similarity ratings  for a given phrase type  the     phrase pairs are
divided into   groups of    pairs each  for each group in the evaluation set     people
gave similarity ratings to the pairs in the given group  each group of    pairs was given
to a different group of    people  the score of an algorithm for a given phrase type is the
average of three rho values  one rho for each of the three groups  with    people rating   
pairs in a group  there are              ratings  these human ratings are represented by
a vector of     numbers  an algorithm only generates one rating for each pair in a group 
yielding    numbers  to make the algorithms ratings comparable to the human ratings  the
algorithms ratings are duplicated    times  yielding a vector of     numbers  spearmans
rho is then calculated with these two vectors of     ratings  for   phrase types with   rho
values each and     ratings per rho value  we have       ratings   
we believe that this evaluation method underestimates the performance of the algorithms  combining ratings from different people into one vector of     numbers does not
allow the correlation to adapt to different biases  if one person gives consistently low ratings
and another person gives consistently high ratings  but both people have the same ranking 
and this ranking matches the algorithms ranking  then the algorithm should get a high
score  for a more fair evaluation  we score an algorithm by calculating one rho value for
each human participant for the given phrase type  and then we calculate the average of the
rho values for all of the participants 
for a given phrase type  the     phrase pairs are divided into   groups of    pairs each 
for the development set  we randomly select    phrase pairs from each of the   groups
            phrase pairs per phrase type   this leaves    phrase pairs in each of the  
groups for the evaluation set             phrase pairs per phrase type   each human
participants ratings are represented by a vector of    numbers  an algorithms ratings are
also represented by a vector of    numbers  a rho value is calculated with these two vectors
of    numbers as input  for a given phrase type  the algorithms score is the average of   
rho values     participants per group    groups      rho values   for   phrase types with
   rho values each and    ratings per rho value  we have       ratings 
      comparison with other approaches
table    compares the dual space model to vector addition and element wise multiplication 
we use the development set to tune the parameters for all three approaches  for vector
addition  ab is represented by a   b and cd is represented by c   d  the similarity of ab and
cd is given by the cosine of the two vectors  element wise multiplication uses equation   
to represent ab and cd  the dual space model uses equation    
the average correlation of the dual space model        is significantly below the average
correlation of vector addition using function space         element wise multiplication with
mono space        is also significantly below vector addition using function space         the
    the information in this paragraph is based on personal communication with jeff mitchell in june       
mitchell and lapatas        paper does not describe how spearmans rho is applied 

   

fidomain and function  a dual space model

algorithm
human
dual space
addition
addition
addition
multiplication
multiplication
multiplication

correlation for
ad nn nn nn
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

each phrase type
vb ob avg
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

comment
leave one out correlation between subjects
domain and function space
mono space
domain space
function space
mono space
domain space
function space

table     performance of the models on the evaluation dataset 
difference between the dual space model        and element wise multiplication with mono
space        is not signficant  the average correlation for an algorithm is based on     rho
values    phrase types    groups     participants       rho values       participants  
we calculate the statistical significance using a paired t test with a     significance level 
based on     pairs of rho values 
      order sensitivity
mitchell and lapatas        dataset does not test for order sensitivity  given a phrase pair
ab  cd  we can test for order sensitivity by adding a new pair ab  dc  we assume that
all such new pairs would be given a rating of   by the human participants  in table     we
show what happens when this transformation is applied to the examples in table     to
save space  we only give the examples for participant number     
participant
   
   
   
   
   
   

phrase type
adjective noun
adjective noun
adjective noun
adjective noun
adjective noun
adjective noun

group
 
 
 
 
 
 

phrase pair
certain circumstance  particular case
certain circumstance  case particular
large number  great majority
large number  majority great
further evidence  low cost
further evidence  cost low

similarity
 
 
 
 
 
 

table     testing for order sensitivity by adding new phrase pairs 
table    gives the results with the new  expanded dataset  with this more stringent
dataset  the dual space model performs significantly better than both vector addition and
vector multiplication  unlike element wise multiplication and vector addition  the dualspace model addresses the issue of order sensitivity 
we manually inspected the new pairs that were automatically rated   and found that a
rating of   was reasonable in all cases  although some cases could be disputed  for example 
the original noun noun pair tax charge  interest rate generates the new pair tax charge 
rate interest and the original verb object pair produce effect  achieve result generates the
new pair produce effect  result achieve  it seems that we have a natural tendency to correct
   

fiturney

algorithm
human
dual space
addition
addition
addition
multiplication
multiplication
multiplication

correlation for
ad nn nn nn
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

each phrase type
vb ob avg
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

comment
leave one out correlation between subjects
domain and function space
mono space
domain space
function space
mono space
domain space
function space

table     performance when the dataset is expanded to test for order sensitivity 
these incorrectly ordered pairs in our minds and then assign them higher ratings than they
deserve  we predict that human ratings of these new pairs would vary greatly  depending
on the instructions that were given to the human raters  if the instructions emphasized the
importance of word order  the new pairs would get low ratings  this prediction is supported
by the results of semeval      task    jurgens  mohammad  turney    holyoak        
where the instructions to the raters emphasized the importance of word order and wrongly
ordered pairs received low ratings 
      summary
when the dataset does not test for order sensitivity  vector addition performs slightly better
than the dual space model  when the dataset tests for order sensitivity  the dual space
model surpasses both vector addition and element wise multiplication by a large margin 
    domain versus function as associated versus similar
chiarello et al         created a dataset of     word pairs that they labeled similar only 
associated only  or similar associated     pairs in each of the three classes   table   
shows some examples from their dataset  these labeled pairs were created for cognitive
psychology experiments with human subjects  in their experiments  they found evidence
that processing associated words engages the left and right hemispheres of the brain in
ways that are different from processing similar words  that is  it seems that there is a
fundamental neurological difference between these two types of semantic relatedness   
we hypothesize that similarity in domain space  simd  a  b   is a measure of the degree to
which two words are associated and similarity in function space  simf  a  b   is a measure of
the degree to which two words are similar  to test this hypothesis  we define similar only 
simso  a  b   associated only  simao  a  b   and similar associated  simsa  a  b   as follows 

ratio x  y   

x y if x     and y    
  otherwise

    

    there is some controversy among cognitive scientists over the distinction between semantic similarity
and association  mcrae  khalkhali    hare        

   

fidomain and function  a dual space model

word pair
table bed
music art
hair fur
house cabin
cradle baby
mug beer
camel hump
cheese mouse
ale beer
uncle aunt
pepper salt
frown smile

class label
similar only
similar only
similar only
similar only
associated only
associated only
associated only
associated only
similar associated
similar associated
similar associated
similar associated

table     examples of word pairs from chiarello et al          labeled similar only 
associated only  or similar associated  the full dataset is in their appendix 

simso  a  b    ratio simf  a  b   simd  a  b  

    

simao  a  b    ratio simd  a  b   simf  a  b  

    

simsa  a  b    geo simd  a  b   simf  a  b  

    

the intention is that simso is high when simf is high and simd is low  simao is high when
simd is high and simf is low  and simsa is high when both simd and simf are high  this is
illustrated in figure   
a

a

a

d f

d f

d f

b

b

b

simso  a  b 

simao  a  b 

simsa  a  b 

similar only

associated only

similar associated

figure    diagrams of equations         and    

   

fiturney

      evaluation
from the experiments in the three preceding subsections  we have three sets of parameter
settings for the dual space model  table    shows these parameter values  in effect  these
three sets of parameter setttings give us three variations of the similarity measures  simso  
simao   and simsa   we will evaluate the three variations to see how well they correspond to
the labels in chiarello et al s        dataset 
similarity
simr  a   b  c   d 
simc  ab  c 
simp  ab  cd 

description
similarity of relations
similarity of noun modifier compositions
similarity of phrases

section
   
   
   

kd
   
   
   

pd
    
   
   

kf
   
   
   

pf
   
   
   

table     parameter settings for the dual space model 
for a given similarity measure  such as simso   we can sort the     word pairs in descending order of their similarities and then look at the top n pairs to see how many of them
have the desired label  in the case of simso   we would like to see that the majority of the
top n have the label similar only  table    shows the percentage of pairs that have the
desired labels for each of the three variations of the three similarity measures  note that
random guessing would yield      since the three classes of pairs have the same size 

source of parameters
simr  a   b  c   d 

simc  ab  c 

simp  ab  cd 

n
  
  
  
  
  
  
  
  
  

percentage of top n with desired label
similar only associated only similar associated
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

table     percentage of the top n word pairs with the desired labels 
for all three sets of parameter settings  table    displays a high density of the desired
labels at the tops of the sorted lists  the density slowly decreases as we move down the
lists  this is evidence that the three similarity measures are capturing the three classes of
chiarello et al         
as another test of the hypothesis  we use the three similarity measures to create feature
vectors of three elements for each word pair  that is  the word pair a   b is represented by
the feature vector hsimso  a  b   simao  a  b   simsa  a  b i  we then use supervised learning with
ten fold cross validation to classify the feature vectors into the three classes of chiarello
et al          for the learning algorithm  we use logistic regression  as implemented in
   

fidomain and function  a dual space model

weka    the results are summarized in table     these results lend further support to the
hypothesis that similarity in domain space  simd  a  b   is a measure of the degree to which
two words are associated and similarity in function space  simf  a  b   is a measure of the
degree to which two words are similar 

source of parameters
simr  a   b  c   d 
simc  ab  c 
simp  ab  cd 

accuracy
    
    
    

similar only
     
     
     

f measure
associated only similar associated
     
     
     
     
     
     

average
     
     
     

table     performance of logistic regression with the three similarity measures as features 

in table     similar only seems more sensitive to the parameter settings than associatedonly and similar associated  we hypothesize that this is because function similarity is
more difficult to measure than domain similarity  note that the construction of function
space  section      is more complex than the construction of domain space  section      
intuitively  it seems easier to identify the domain of a thing than to identify its functional
role  gentners        work suggests that children master domain similarity before they
become competent with function similarity 

   discussion of experiments
this section discusses the results of the previous section 
    summary of results
in section      we used     multiple choice analogy questions to evaluate the dual space
model of relational similarity  simr  a   b  c   d   the difference between the performance of
the dual space model        accuracy  and the best past result        accuracy   using a
holistic model  was not statistically significant  experiments with a reformulated version
of the questions  designed to test order sensitivity  supported the hypothesis that both
domain and function space are required  function space by itself is not sensitive to order
and merging the two spaces  mono space  causes a significant drop in performance 
in section      we automatically generated       multiple choice noun modifier composition questions with wordnet  to evaluate the dual space model of noun modifier compositional similarity  simc  ab  c   the difference between the performance of the dual space
model        accuracy  and the state of the art element wise multiplication model       
accuracy  was not statistically significant  the best performance was obtained with a holistic model          but this model does not address the issue of linguistic creativity  further
experiments suggest that a significant fraction of the gap between the holistic model and
the other models is due to noncompositional phrases  a limitation of the element wise multiplication model is lack of sensitivity to order  experiments with a reformulated version
    weka is available at http   www cs waikato ac nz ml weka  

   

fiturney

of the questions  designed to test order sensitivitiy  demonstrated a statistically significant
advantage to the dual space model over the element wise multiplication and vector addition
models 
in section      we used mitchell and lapatas        dataset of     pairs of phrases to
evaluate the dual space model of phrasal similarity  simp  ab  cd   a reformulated version of
the dataset  modified to test order sensitivitiy  showed a statistically significant advantage
to the dual space model over the element wise multiplication and vector addition models 
in section      we used chiarello et al s        dataset of     word pairs  labeled
similar only  associated only  or similar associated  to test the hypothesis that similarity
in domain space  simd  a  b   is a measure of the degree to which two words are associated
and similarity in function space  simf  a  b   is a measure of the degree to which two words
are similar  the experimental results support the hypothesis  this is interesting because
chiarello et al         argue that there is a fundamental neurological difference in the way
people process these two kinds of semantic relatedness 
the experiments support the claim that the dual space model can address the issues of
linguistic creativity  order sensitivity  and adaptive capacity  furthermore  the dual space
model provides a unified approach to both semantic relations and semantic composition 
    corpus based similarity versus lexicon based similarity
the results in section     suggest that function similarity may correspond to the kind of
taxonomical similarity that is often associated with lexicons  such as wordnet  resnik 
      jiang   conrath        leacock   chodrow        hirst   st onge         the
word pairs in table    that are labeled similar only are the kinds of words that typically
share a common hypernym in a taxonomy  for example  table bed share the hypernym
furniture  we believe that this is correct  but it does not necessarily imply that lexiconbased similarity measures would be better than a corpus based approach  such as we have
used here 
of the various similarities in section    arguably relational similarity  simr  a   b  c   d  
makes the most use of function similarity  by itself  function similarity achieves       on the
sat questions  original five choice version  see table      however  the best performance
achieved on the sat questions using wordnet is        veale         the difference is
statistically significant at the     confidence level  based on fishers exact test 
consider the analogy traffic is to street as water is to riverbed  one of the sat questions
involves this analogy  with traffic  street as the stem pair and water  riverbed as the correct
choice  both simr  a   b  c   d   equation     and function similarity by itself  equation    
make the correct choice  we can recognize that traffic and water have a high degree of
function similarity  in fact  this similarity is used in hydrodynamic models of traffic flow
 daganzo         however  we must climb the wordnet hierachy all the way up to entity
before we find a shared hypernym for traffic and water  we believe that no manually
generated lexicon can capture all of the functional similarity that can be discovered in a
large corpus 

   theoretical considerations
this section examines some theoretical questions about the dual space model 
   

fidomain and function  a dual space model

    vector composition versus similarity composition
in the dual space model  a phrase has no stand alone  general purpose representation  as a
composite phrase  apart from the representations of the component words  the composite
meaning is constructed in the context of a given task  for example  if the task is to measure
the similarity of the relation in dog  house to the relation in bird  nest  then we compose the
meanings of dog and house one way  see section       if the task is to measure the similarity
of the phrase dog house to the word kennel  then we compose the meanings of dog and
house another way  see section       if the task is to measure the similarity of the phrase
dog house to the phrase canine shelter  then we compose the meanings of dog and house a
third way  see section       the composition is a construction that explicitly ties together
the two things that are being compared  and it depends on the nature of the comparison
that is desired  the task that is to be performed  we hypothesize that no single stand alone 
task independent representation can be constructed that is suitable for all purposes 
as we noted in the introduction  composition of vectors can result in a stand alone
representation of a phrase  but composing similarities necessarily yields a linking structure
that connects a phrase to other phrases  these linking structures can be seen in figures
  to    intuitively  it seems that an important part of how we understand a phrase is by
connecting it to other phrases  part of our understanding of dog house is its connection to
kennel  dictionaries make these kinds of connections explicit  from this perspective  the
idea of an explicit linking structure seems natural  given that making connnections among
words and phrases is an essential aspect of meaning and understanding 
    general form of similarities in the dual space model
in this subsection  we present a general scheme that ties together the various similarities
that were defined in section    this scheme includes similarities between chunks of text
of arbitrary size  the scheme encompasses phrasal similarity  relational similarity  and
compositional similarity 
let t be a chunk of text  an ordered set of words   ht    t            tn i  where each ti is a
word  we represent the semantics of t by t   hd  fi  where d and f are matrices  each
row vector di in d  i                 n  is the row vector in domain space that represents the
domain semantics of the word ti   each row vector fi in f  i                 n  is the row vector in
function space that represents the function semantics of the word ti   to keep the notation
simple  the parameters  kd and pd for domain space and kf and pf for function space  are
implicit  assume that the row vectors in d and f are normalized to unit length  note
that the size of the representation t scales linearly with n  the number of words in t  hence
we have information scalability  for large values of n  there will inevitably be duplicate
words in t  so the representation could easily be compressed to sublinear size without loss
of information 
let t  and t  be two chunks of text with representations t    hd    f  i and t   
hd    f  i  where t  contains n  words and t  has n  words  let d  and d  have the same
parameters  kd and pd   and let f  and f  have the same parameters  kf and pf   then d 
is n   kd   d  is n   kd   f  is n   kf   and f  is n   kf   note that d  dt
  is an n   n 
matrix of the cosines between any two row vectors in d    that is  the element in the i th
   

fiturney

t
row and j th column of d  dt
  is cos di   dj    likewise  d  d  is an n   n  matrix of the
cosines between any row vector in d  and any row vector in d   
suppose that we wish to measure the similarity  sim t    t     between the two chunks of
text  t  and t    in this paper  we have restricted the similarity measures to the following
general form 
t
t
t
t
t
sim t    t      f  d  dt
    d   d     d   d     f  f    f  f    f  f   

    

in other words  the only input to the composition function f is cosines  and the implicit
parameters  kd   pd   kf   and pf    f does not operate directly on any of the row vectors in d   
d    f    and f    in contrast to much of the work discussed in section      the composition
operation is shifted out of the representations  t  and t    and into the similarity measure 
f   the exact specification of f depends on the task at hand  when t  and t  are sentences 
we envision that the structure of f will be determined by the syntactic structures of the
two sentences   
consider relational similarity  section      

sim   a   b  c   d    geo simf  a  c   simf  b  d  

    

sim   a   b  c   d    geo simd  a  b   simd  c  d  

    

sim   a   b  c   d    geo simd  a  d   simd  c  b  

sim   a   b  c   d  if sim   a   b  c   d   sim   a   b  c   d 
simr  a   b  c   d   
  otherwise

    
    

this fits the form of equation    when we have t    ha  bi and t    hc  di  we can see that
t
t
sim  is based on cosines from f  ft
    sim  is based on cosines from d  d  and d  d    and
t
sim  is based on cosines from d  d   
consider compositional similarity  section      

sim   ab  c    geo simd  a  c   simd  b  c   simf  b  c  

sim   ab  c  if a    c and b    c
simc  ab  c   
  otherwise

    
    

this can be seen as an instance of equation    in which t    ha  bi and t    hci  in this
t
case  sim  is based on cosines from d  dt
  and f  f    the constraints  a    c and b    c 
t
can be expressed in terms of cosines from d  d    as simd  a  c       and simd  b  c       
 equivalently  we could use cosines from f  ft
     similar analyses apply to the similarities
in sections     and      these similarities are also instances of equation    
although the representations t  and t  have sizes that are linear functions of the numbers of phrases in t  and t    the size of the composition in equation    is a quadratic
function of the numbers of phrases in t  and t    however  specific instances of this general
equation may be less than quadratic in size  and it may be possible to limit the growth
    note that there is no requirement for the two chunks of text  t  and t    to have the same number of
words  that is  n  does not necessarily equal n    in section      n     n   

   

fidomain and function  a dual space model

to a linear function  also  in general  quadratic growth is often acceptable in practical
applications  garey   johnson        
with function words  e g   prepositions  conjunctions   one option would be to treat
them the same as any other words  they would be represented by vectors and their similarities would be calculated in function and domain spaces  another possibility would be
to use function words as hints to guide the construction of the composition function f   the
function words would not correspond to vectors  instead they would contribute to determining the linking structure that connects the two given chunks of text  the first option
appears more elegant  but the choice between the options should be made empirically 
    automatic composition of similarities
in section    we manually constructed the functions that combined the similarity measures 
using our intuition and background knowledge  manual construction will not scale up to the
task of comparing any two arbitrarily chosen sentences  however  there are good reasons
for believing that the construction of composition functions can be automated 
turney      a  presents an algorithm for solving analogical mapping problems  such as
the analogy between the solar system and the rutherford bohr model of the atom  given
a list of terms from the solar system domain   planet  attracts  revolves  sun  gravity  solar system  mass   and a list of terms from the atomic domain   revolves  atom  attracts 
electromagnetism  nucleus  charge  electron   it can automatically generate a one to one
mapping from one domain to the other   solar system  atom  sun  nucleus  planet
 electron  mass  charge  attracts  attracts  revolves  revolves  gravity  electromagnetism   on twenty analogical mapping problems  it attains an accuracy of       
compared to an average human accuracy of       
the algorithm scores the quality of a candidate analogical mapping by composing the
similarities of the mapped terms  the composition function is addition and the individual
component similarities are holistic relational similarities  the algorithm searches through
the space of possible mappings for the mapping that maximizes the composite similarity
measure  that is  analogical mapping is treated as an argmax problem  where the argument
to be maximized is a mapping function  in effect  the output of the algorithm  an analogical
mapping  is an automically generated composition of similarities  the mapping structures
found by the algorithm are essentially the same as the linking structures that we see in
figures   to   
we believe that a variation of turneys      a  algorithm could be used to automatically compose similarities in the dual space model  for example  it should be possible to
identify paraphrases using automatic similarity composition  the proposal is to search for
a composition that maximizes composite similarity  subject to various constraints  such as
constraints based on the syntax of the sentences   turney      a  points out that analogical
mapping could be used to align the words in two sentences  but does not experimentally
evaluate this suggestion 
recent work  lin   bilmes        has shown that argmax problems can be solved efficiently and effectively if they can be framed as monotone submodular function maximization
problems  we believe that automatic composition of similarities can fit naturally into this
framework  which would result in highly scalable algorithms for semantic composition 
   

fiturney

regarding information scalability  the dual space model does not suffer from information
loss  unlike approaches that represent compositions with vectors of fixed dimensionality  
because the sizes of the representations grow as the lengths of the phrases grow  the growth
might be quadratic  but it is not exponential  there are questions about how to automate
composition of similarities  which may have an impact on the computational complexity of
scaling to longer phrases  but there is evidence that these questions are tractable 

   limitations and future work
one area for future work is to experiment with longer phrases  more than two words  and
sentences  as discussed in section      an interesting topic for research is how parsing might
be used to constrain the automatic search for similarity composition functions 
here we have focused on two spaces  domain and function  but it seems likely to us
that a model with more spaces would yield better performance  we are currently experimenting with a quad space model that includes domain  noun based contextual patterns  
function  verb based   quality  adjective based   and manner  adverb based  spaces  the
preliminary results with quad space are promising  quad space seems to be related to
pustejovskys        four part qualia structure 
another issue we have avoided here is morphology  as discussed in section      we used
the validforms function in the wordnet  querydata perl interface to wordnet to map
morphological variations of words to their base forms  this implies that  for example  a
singular noun and its plural form should have the same semantic representation  this is
certainly a simplification and a more sophisticated model would use different representations
for different morphological forms of a word 
we have also avoided the issue of polysemy  it should be possible to extend past work
with polysemy in vsms to the dual space model  schutze        pantel   lin        erk
  pado        
in this paper  we have treated the holistic model and the dual space model as if they are
competitors  but there are certain cases  such as idiomatic expressions  where the holistic
approach is required  likewise  the holistic approach is limited by its inability to handle
linguistic creativity  these considerations suggest that the holistic and dual space models
must be integrated  this is another topic for future work 
arguably it is a limitation of the dual space model that there are four parameters to
tune  kd   pd   kf   and pf    on the other hand  perhaps any model with adaptive capacity
must have some parameters to tune  further research is needed 
a number of design decisions were made in the construction of domain and function
space  especially in the conversion of phrases to contextual patterns  sections     and      
these decisions were guided by our intuitions  we expect that the exploration and experimental evaluation of this design space will be a fruitful area for future research 
the construction of function space  section      is specific to english  it may generalize readily to other indo european languages  but some other languages may present a
challenge  this is another topic for future research 
most of our composite similarities use the geometric mean to combine domain and
function similarities  but we see no reason to restrict the possible composition functions 
   

fidomain and function  a dual space model

equation    allows any composition function f   exploring the space of possible composition
functions is another topic for future work 
another question is how formal logic and textual entailment can be integrated into this
approach  the dual space model seems to be suitable for recognizing paraphrases  but
there is no obvious way to handle entailment  more generally  we have focused on various
kinds of similarity  but when we scale up from phrases  red ball  to sentences  the ball is
red   we encounter truth and falsity  gardenfors        argues that spatial models are a
bridge between low level connectionist models and high level symbolic models  he claims
that spatial models are best for questions about similarity and symbolic models are best
for questions about truth  we do not yet know how to join these two kinds of models 

   conclusions
the goal in this research has been to develop a model that unifies semantic relations and
compositions  while also addressing linguistic creativity  order sensitivity  adaptive capacity  and information scalability  we believe that the dual space model achieves this goal 
although there is certainly room for improvement and further research 
there are many kinds of wordcontext matrices  based on various notions of context 
sahlgren        gives a good overview of the types of context that have been explored
in past work  the novelty of the dual space model is that it includes two distinct and
complementary wordcontext matrices that work together synergistically 
with two distinct spaces  we have two distinct similarity measures  which can be
combined in many different ways  with multiple similarity measures  similarity composition becomes a viable alternative to vector composition  for example  instead of multiplying vectors  such as c   a fi b  we can multiply similarities  such as simsa  a  b   
geo simd  a  b   simf  a  b    the results here suggest that this is a fruitful new way to look
at some of the problems of semantics 

acknowledgments
thanks to george foster  yair neuman  david jurgens  and the reviewers of jair for
their very helpful comments on an earlier version of this paper  thanks to charles clarke
for the corpus used to build the three spaces  to stefan buttcher for wumpus  to the
creators of wordnet for making their lexicon available  to the developers of opennlp 
to doug rohde for svdlibc  to jeff mitchell and mirella lapata for sharing their data
and answering questions about their evaluation methodology  to christine chiarello  curt
burgess  lorie richards  and alma pollock for making their data available  to jason rennie
for the wordnet  querydata perl interface to wordnet  and to the developers of perl data
language 

references
aerts  d     czachor  m          quantum aspects of semantic analysis and symbolic
artificial intelligence  journal of physics a  mathematical and general      l   
l    
   

fiturney

baroni  m     zamparelli  r          nouns are vectors  adjectives are matrices  representing adjective noun constructions in semantic space  in proceedings of the     
conference on empirical methods in natural language processing  emnlp       
pp           
bengio  y   ducharme  r   vincent  p     jauvin  c          a neural probabilistic language
model  journal of machine learning research              
bicici  e     yuret  d          clustering word pairs to answer analogy questions  in
proceedings of the fifteenth turkish symposium on artificial intelligence and neural
networks  tainn        akyaka  mugla  turkey 
biemann  c     giesbrecht  e          distributional semantics and compositionality      
shared task description and results  in proceedings of the workshop on distributional
semantics and compositionality  disco        pp        portland  oregon 
bollegala  d   matsuo  y     ishizuka  m          measuring the similarity between implicit
semantic relations from the web  in proceedings of the   th international conference
on world wide web  www        pp         
brants  t     franz  a          web  t   gram version    linguistic data consortium 
philadelphia 
bullinaria  j     levy  j          extracting semantic representations from word cooccurrence statistics  a computational study  behavior research methods         
       
buttcher  s     clarke  c          efficiency vs  effectiveness in terabyte scale information retrieval  in proceedings of the   th text retrieval conference  trec       
gaithersburg  md 
caron  j          experiments with lsa scoring  optimal rank and basis   in proceedings
of the siam computational information retrieval workshop  pp          raleigh 
nc 
chiarello  c   burgess  c   richards  l     pollock  a          semantic and associative
priming in the cerebral hemispheres  some words do  some words dont       sometimes 
some places  brain and language            
chomsky  n          the logical structure of linguistic theory  plenum press 
church  k     hanks  p          word association norms  mutual information  and lexicography  in proceedings of the   th annual conference of the association of computational linguistics  pp        vancouver  british columbia 
clark  s   coecke  b     sadrzadeh  m          a compositional distributional model of
meaning  in proceedings of the  nd symposium on quantum interaction  pp         
oxford  uk 
clark  s     pulman  s          combining symbolic and distributional models of meaning 
in proceedings of the aaai spring symposium on quantum interaction  pp       
stanford  ca 
conway  j  h     sloane  n  j  a          sphere packings  lattices and groups  springer 
   

fidomain and function  a dual space model

daganzo  c  f          the cell transmission model  a dynamic representation of highway
traffic consistent with the hydrodynamic theory  transportation research part b 
methodological                 
davidov  d     rappoport  a          unsupervised discovery of generic relationships using
pattern clusters and its evaluation by automatically generated sat analogy questions 
in proceedings of the   th annual meeting of the acl and hlt  acl hlt      pp 
        columbus  ohio 
erk  k     pado  s          a structured vector space model for word meaning in context 
in proceedings of the      conference on empirical methods in natural language
processing  emnlp      pp          honolulu  hi 
fellbaum  c   ed            wordnet  an electronic lexical database  mit press 
firth  j  r          a synopsis of linguistic theory           in studies in linguistic
analysis  pp       blackwell  oxford 
fodor  j     lepore  e          the compositionality papers  oxford university press 
gardenfors  p          conceptual spaces  the geometry of thought  mit press 
garey  m  r     johnson  d  s          computers and intractability  a guide to the theory
of np completeness  freeman 
gentner  d          structure mapping  a theoretical framework for analogy  cognitive
science                
gentner  d          language and the career of similarity  in gelman  s     byrnes  j 
 eds    perspectives on thought and language  interrelations in development  pp 
        cambridge university press 
golub  g  h     van loan  c  f          matrix computations  third edition   johns
hopkins university press  baltimore  md 
grefenstette  e     sadrzadeh  m          experimenting with transitive verbs in a discocat  in proceedings of the gems      workshop on geometrical models of natural
language semantics 
grice  h  p          studies in the way of words  harvard university press  cambridge 
ma 
guevara  e          a regression model of adjective noun compositionality in distributional
semantics  in proceedings of the      workshop on geometrical models of natural
language semantics  gems        pp       
harris  z          distributional structure  word                  
hearst  m          automatic acquisition of hyponyms from large text corpora  in proceedings of the   th conference on computational linguistics  coling      pp         
herdagdelen  a     baroni  m          bagpack  a general framework to represent semantic
relations  in proceedings of the eacl      geometrical models for natural language
semantics  gems  workshop  pp       
   

fiturney

hirst  g     st onge  d          lexical chains as representations of context for the detection
and correction of malapropisms  in fellbaum  c   ed    wordnet  an electronic
lexical database  pp          mit press 
jiang  j  j     conrath  d  w          semantic similarity based on corpus statistics
and lexical taxonomy  in proceedings of the international conference on research in
computational linguistics  rocling x   pp        tapei  taiwan 
johannsen  a   alonso  h  m   rishj  c     sgaard  a          shared task system
description  frustratingly hard compositionality prediction  in proceedings of the
workshop on distributional semantics and compositionality  disco        pp    
    portland  oregon 
jones  m  n     mewhort  d  j  k          representing word meaning and order information in a composite holographic lexicon  psychological review           
jurgens  d  a   mohammad  s  m   turney  p  d     holyoak  k  j          semeval     
task    measuring degrees of relational similarity  in proceedings of the first joint
conference on lexical and computational semantics   sem   pp          montreal 
canada 
kintsch  w          metaphor comprehension  a computational theory  psychonomic bulletin   review                
kintsch  w          predication  cognitive science                 
kolda  t     bader  b          tensor decompositions and applications  siam review 
               
landauer  t  k          on the computational basis of learning and cognition  arguments
from lsa  in ross  b  h   ed    the psychology of learning and motivation  advances
in research and theory  vol      pp        academic press 
landauer  t  k     dumais  s  t          a solution to platos problem  the latent semantic analysis theory of the acquisition  induction  and representation of knowledge 
psychological review                  
landauer  t  k   mcnamara  d  s   dennis  s     kintsch  w          handbook of latent
semantic analysis  lawrence erlbaum  mahwah  nj 
leacock  c     chodrow  m          combining local context and wordnet similarity for
word sense identification  in fellbaum  c   ed    wordnet  an electronic lexical
database  mit press 
lee  d  d     seung  h  s          learning the parts of objects by nonnegative matrix
factorization  nature              
lepage  y     shin ichi  a          saussurian analogy  a theoretical account and its
application  in proceedings of the   th international conference on computational
linguistics  coling        pp         
lin  h     bilmes  j          a class of submodular functions for document summarization 
in the   th annual meeting of the association for computational linguistics  human
language technologies  acl hlt   pp         
   

fidomain and function  a dual space model

mangalath  p   quesada  j     kintsch  w          analogy making as predication using
relational information and lsa vectors  in proceedings of the   th annual meeting
of the cognitive science society  p        austin  tx 
mcrae  k   khalkhali  s     hare  m          semantic and associative relations in adolescents and young adults  examining a tenuous dichotomy  in reyna  v   chapman 
s   dougherty  m     confrey  j   eds    the adolescent brain  learning  reasoning 
and decision making  pp        apa  washington  dc 
mitchell  j     lapata  m          vector based models of semantic composition  in proceedings of acl     hlt  pp          columbus  ohio  association for computational
linguistics 
mitchell  j     lapata  m          composition in distributional models of semantics 
cognitive science                   
moschitti  a     quarteroni  s          kernels on linguistic structures for answer extraction  in proceedings of the   th annual meeting of the association for computational
linguistics on human language technologies  short papers  p          columbus 
oh 
nakov  p     hearst  m          using verbs to characterize noun noun relations  in proceedings of the   th international conference on artificial intelligence  methodology 
systems  and applications  aimsa        pp          varna  bulgaria 
nakov  p     hearst  m          ucb  system description for semeval task    in proceedings of the fourth international workshop on semantic evaluations  semeval       
pp          prague  czech republic 
nastase  v   sayyad shirabad  j   sokolova  m     szpakowicz  s          learning nounmodifier semantic relations with corpus based and wordnet based features  in proceedings of the   st national conference on artificial intelligence  aaai      pp 
       
nastase  v     szpakowicz  s          exploring noun modifier semantic relations  in
proceedings of the fifth international workshop on computational semantics  iwcs    pp          tilburg  the netherlands 
niwa  y     nitta  y          co occurrence vectors from corpora vs  distance vectors from
dictionaries  in proceedings of the   th international conference on computational
linguistics  pp          kyoto  japan 
o seaghdha  d     copestake  a          using lexical and relational similarity to classify
semantic relations  in proceedings of the   th conference of the european chapter of
the association for computational linguistics  eacl      athens  greece 
pantel  p     lin  d          discovering word senses from text  in proceedings of the eighth
acm sigkdd international conference on knowledge discovery and data mining 
pp          edmonton  canada 
plate  t          holographic reduced representations  ieee transactions on neural networks                
pustejovsky  j          the generative lexicon  computational linguistics                 
   

fiturney

rapp  r          word sense discovery based on sense descriptor dissimilarity  in proceedings of the ninth machine translation summit  pp         
resnik  p          using information content to evaluate semantic similarity in a taxonomy 
in proceedings of the   th international joint conference on artificial intelligence
 ijcai      pp          san mateo  ca  morgan kaufmann 
rosario  b     hearst  m          classifying the semantic relations in noun compounds
via a domain specific lexical hierarchy  in proceedings of the      conference on
empirical methods in natural language processing  emnlp      pp       
rosario  b   hearst  m     fillmore  c          the descent of hierarchy  and selection in
relational semantics  in proceedings of the   th annual meeting of the association
for computational linguistics  acl      pp         
sahlgren  m          the word space model  using distributional analysis to represent syntagmatic and paradigmatic relations between words in high dimensional vector spaces 
ph d  thesis  department of linguistics  stockholm university 
santorini  b          part of speech tagging guidelines for the penn treebank project  tech 
rep   department of computer and information science  university of pennsylvania 
  rd revision   nd printing  
schutze  h          automatic word sense discrimination  computational linguistics         
      
smolensky  p          tensor product variable binding and the representation of symbolic
structures in connectionist systems  artificial intelligence         
socher  r   huang  e  h   pennington  j   ng  a  y     manning  c  d          dynamic
pooling and unfolding recursive autoencoders for paraphrase detection  in advances
in neural information processing systems  nips        pp         
socher  r   manning  c  d     ng  a  y          learning continuous phrase representations
and syntactic parsing with recursive neural networks  in proceedings of the nips     
deep learning and unsupervised feature learning workshop 
thater  s   furstenau  h     pinkal  m          contextualizing semantic representations
using syntactically enriched vector models  in proceedings of the   th annual meeting
of the association for computational linguistics  pp         
turney  p  d          mining the web for synonyms  pmi ir versus lsa on toefl  in
proceedings of the twelfth european conference on machine learning  ecml     
pp          freiburg  germany 
turney  p  d       a   expressing implicit semantic relations without supervision  in
proceedings of the   st international conference on computational linguistics and
  th annual meeting of the association for computational linguistics  coling acl     pp          sydney  australia 
turney  p  d       b   similarity of semantic relations  computational linguistics         
       
turney  p  d       a   the latent relation mapping engine  algorithm and experiments 
journal of artificial intelligence research             
   

fidomain and function  a dual space model

turney  p  d       b   a uniform approach to analogies  synonyms  antonyms  and associations  in proceedings of the   nd international conference on computational
linguistics  coling        pp          manchester  uk 
turney  p  d     littman  m  l          corpus based learning of analogies and semantic
relations  machine learning                  
turney  p  d   littman  m  l   bigham  j     shnayder  v          combining independent
modules to solve multiple choice synonym and analogy problems  in proceedings of
the international conference on recent advances in natural language processing
 ranlp      pp          borovets  bulgaria 
turney  p  d     pantel  p          from frequency to meaning  vector space models of
semantics  journal of artificial intelligence research             
utsumi  a          computational semantics of noun compounds in a semantic space model 
in proceedings of the   st international joint conference on artificial intelligence
 ijcai      pp           
veale  t          wordnet sits the sat  a knowledge based approach to lexical analogy  in
proceedings of the   th european conference on artificial intelligence  ecai       
pp          valencia  spain 
widdows  d          semantic vector products  some initial investigations  in proceedings
of the  nd symposium on quantum interaction  oxford  uk 

   

fi