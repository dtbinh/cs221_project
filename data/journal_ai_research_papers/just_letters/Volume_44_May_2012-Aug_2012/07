journal artificial intelligence research                  

submitted        published      

riffled independence efficient inference
partial rankings
jonathan huang

jhuang   stanford edu

james h  clark center
stanford university  stanford ca        usa

ashish kapoor

akapoor microsoft com

microsoft research
one microsoft way
redmond wa             usa

carlos guestrin

guestrin cs cmu edu

gates hillman complex  carnegie mellon university 
     forbes avenue  pittsburgh  pa        usa

abstract
distributions rankings used model data multitude real world settings
preference analysis political elections  modeling distributions presents
several computational challenges  however  due factorial size set rankings
item set  challenges quite familiar artificial intelligence
community  compactly represent distribution combinatorially large
space  efficiently perform probabilistic inference representations 
respect ranking  however  additional challenge refer
human task complexity users rarely willing provide full ranking long list
candidates  instead often preferring provide partial ranking information 
simultaneously addressing challenges i e   designing compactly representable model amenable efficient inference learned using partial
ranking data difficult task  necessary would scale problems
nontrivial size  paper  show recently proposed riffled independence
assumptions cleanly efficiently address challenges  particular 
establish tight mathematical connection concepts riffled independence
partial rankings  correspondence allows us develop efficient
exact algorithms performing inference tasks using riffled independence based representations partial rankings  somewhat surprisingly  shows efficient inference
possible riffle independent models  in certain sense  observations
take form partial rankings  finally  using inference algorithm  introduce
first method learning riffled independence based models partially ranked data 

   probabilistic modeling ranking data  three challenges
rankings arise number machine learning application settings preference analysis movies books  lebanon   mao        political election analysis  gormley
  murphy        huang   guestrin         many problems  great interest
build statistical models ranking data order make predictions  form recommendations  discover latent trends structure construct human comprehensible data
summaries 
c
    
ai access foundation  rights reserved 

fihuang  kapoor   guestrin

modeling distributions rankings difficult problem  however  due fact
number items ranked increases  number possible rankings increases
factorially  combinatorial explosion forces us confront three central challenges
dealing rankings  first  need deal storage complexity compactly represent distribution space rankings   algorithmic complexity efficiently answer probabilistic inference queries given distribution 
finally  must contend refer human task complexity 
challenge stemming fact difficult accurately elicit full ranking
large list candidates human user  choosing list n  options easy task
users typically prefer provide partial information  take american psychological
association  apa  elections  example  allow voters rank order candidates
favorite least favorite       election  five candidates  therefore
         ways rank five candidates  despite small candidate list  voters
election preferred specify top k favorite candidates rather writing
full rankings ballots  see figure     example  roughly third voters
simply wrote single favorite candidate      election 
three intertwined challenges storage  algorithmic  human task complexity
central issues probabilistic modeling rankings  models efficiently
handle three sources complexity limited applicability  paper  examine
flexible intuitive class models rankings based generalization probabilistic
independence called riffled independence  proposed recent work  huang   guestrin 
             previous papers focused primarily representational  storage
complexity  issues  concentrate inference incomplete observations  i e   partial
rankings   showing addition storage complexity  riffle independence based models
efficiently address issues algorithmic human task complexity 
fact two issues algorithmic human task complexity intricately linked
riffle independent models  considering partial rankings  give users flexibility
provide much little information care give  context partial
ranking data  relevant inference queries take form partial rankings 
example  might want predict voters second choice candidate given information
first choice  one main contributions paper show inference
partial ranking queries performed particularly efficiently riffle independent
models 
main contributions work follows  
reveal natural fundamental connection riffle independent models
partial rankings  particular  show collection partial rankings
item set form complete characterization space observations upon
   note common wonder one would care represent distribution rankings
number sample rankings never nearly large  problem number samples always
much smaller n  however  means rankings never observed  limiting ability
estimate probability arbitrary ranking  way overcome paucity samples
exploit representational structure  much alignment solving storage complexity
issue 
   paper extended presentation paper  huang  kapoor    guestrin        appeared
     conference uncertainty artificial intelligence  uai  well results first
authors dissertation  huang        

   

fiefficient inference partial rankings

first
choice

second
choice

third
choice

fourth
choice

fifth
choice

 
votes

 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

   

   

  

 

   

   

   

   

    

 

 

 

   

   

  

 

 

   

   

   

   

 

 

 

 

 

   

figure    example partial ranking data  taken american psychological association
election dataset       

one efficiently condition riffle independent model  result  show
ranked items satisfy riffled independence relationship  conditioning
partial rankings done efficiently  running time o n h     h  denotes
number model parameters 
prove that  sense  which formalize   impossible efficiently condition
riffle independent models observations take form partial rankings 
propose first algorithm capable efficiently estimating structure
parameters riffle independent models heterogeneous collections partially
ranked data 
show results real voting preference data evidencing effectiveness
methods 

   riffled independence rankings
ranking    items item set one to one mapping rank
set r               n  denoted using vertical bar notation                          n  
say ranks item i   or over  item i  rank i  less
rank i    example  might  corn  p eas  apples  oranges  ranking
corn p eas apples oranges encodes preference corn peas turn preferred apples on  collection possible rankings item set denoted
 or sn implicit  
since n  rankings n items  intractable estimate even explicitly
represent arbitrary distributions sn without making structural assumptions
underlying distribution  many possible simplifying assumptions one
make  focus approach proposed recent papers  huang   guestrin 
            ranks items assumed satisfy intuitive generalized notion
probabilistic independence known riffled independence  paper  argue
riffled independence assumptions particularly effective settings one would
make queries taking form partial rankings  remainder section 
review riffled independence 
   

fihuang  kapoor   guestrin

riffled independence assumption posits rankings item set generated independently generating rankings smaller disjoint item subsets  say 
b  partition   piecing together full ranking interleaving  or riffle shuffling 
smaller rankings together  example  rank item set foods  one might first
rank vegetables fruits separately  interleave two subset rankings form
full ranking  formally define riffled independence  use notions relative rankings
interleavings 
definition    relative ranking map   given ranking subset  
relative ranking items a      ranking  sa    i     j 
 i     j  
definition    interleaving map   given ranking partition disjoint
sets b  interleaving b  denoted  ab      binary  mapping
rank set r               n   a  b  indicating whether rank occupied
b  rankings  denote interleaving ranking vertical bar notation 
 ab         ab                 ab     n  
example    consider partitioning item set vegetables    corn  p eas 
fruits b    apples  oranges   well full ranking four items   
corn oranges p eas apples  case  relative ranking vegetables     
corn p eas relative ranking fruits b      oranges apples  interleaving vegetables fruits ab      a b a b 
definition    riffled independence   let h distribution consider subset
items complement b  sets b said riffle independent
h decomposes  or factors  as 
h     mab  ab     fa  a     gb  b     
distributions mab   fa gb   defined interleavings relative rankings
b respectively  words  b riffle independent relative rankings
b  well interleaving mutually independent  refer mab
interleaving distribution fa gb relative ranking distributions 
riffled independence found approximately hold number real datasets
 huang   guestrin         relationships identified data  instead
exhaustively representing n  ranking probabilities  one represent factors
mab   fa gb   distributions smaller sets 
    hierarchical riffle independent models
relative ranking factors fa gb distributions rankings 
reduce parameter space  natural consider hierarchical decompositions item sets
nested collections partitions  like hierarchical clustering   example  figure    
shows hierarchical decomposition vegetables riffle independent fruits among
healthy foods  healthy foods are  turn  riffle independent subset
desserts   doughnuts   m s  
   

fiefficient inference partial rankings

 c p a o d m 

 d m 

 c p a o 

doughnuts  m ms

 c p 

 a o 

corn  peas

apples  oranges

figure    example hierarchy six food items 
simplicity  restrict consideration binary hierarchies  defined tuples
form h    ha   hb    ha hb either     null  case h called leaf 
    hierarchies item sets b respectively  second case  b
assumed form nontrivial partitioning item set 
definition    say distribution h factors riffle independently respect
hierarchy h    ha   hb   item sets b riffle independent respect h 
fa gb factor riffle independently respect subhierarchies ha hb  
respectively 
bayesian networks  hierarchies represent families distributions obeying
certain set  riffled  independence constraints parameterized locally  draw
model  one generates full rankings recursively starting drawing rankings
leaf sets  working tree  sequentially interleaving rankings reaching
root  parameters hierarchical models simply interleaving relative
ranking distributions internal nodes leaves hierarchy  respectively 
general  number total parameters required represent hierarchical riffle
independent model  as bayesian networks  still scale exponentially number

items  example  number interleavings p items n p items np  
often case however  much fewer parameters necessary  example  thin
models  huang   guestrin         number items factored model
stage hierarchy never small constant k  always represented
 degree k  polynomial number parameters  use  h  refer number
parameters necessary representing distribution factors according hierarchy
h 
decomposing distributions rankings small pieces  like bayesian networks
done distributions   hierarchical models allow better interpretability 
efficient probabilistic representation  low sample complexity  efficient map optimization 
and  show paper  efficient inference 
example    figure   a   reproduce hierarchical structure learned using
fully ranked subset apa data consisting      training examples huang
guestrin         five candidates election      william bevan      ira
iscoe      charles kiesler      max siegle      logan wright  marden         strikingly 
structure learned using algorithm  maximum likelihood  knows nothing
underlying politics apa  leaf nodes correspond exactly
political coalitions dominated apa      election research psychologists
   

fihuang  kapoor   guestrin

a        

b       

c    
community
psychologists

mb c  
   

b c b b b

   

b b c b b

   

b b b c b

   

b b b b c

   



md e  



fc  

d d e e

   

 

    

d e d e

   

d e e d

   

d     

e     

e d d e

   

research
psychologists

clinical
psychologists

e d e d

   

e e d d

   

 a  hierarchical structure learned
via mle using      full rankings
apa dataset 


c b b b b



fd  



fe  

   

   

   

   

   

   

   

   

 b  riffle independent model parameters learned via mle using
     full rankings apa dataset 

figure    example hierarchical model apa election  candidates enumerated
as      william bevan      ira iscoe      charles kiesler      max siegle     
logan wright  marden        

 candidates       clinical psychologists  candidates       community
psychologists  candidate    
figure   b   plot corresponding parameter distributions learned via
maximum likelihood  three relative ranking distributions  corresponding
political party  well two interleaving distributions  one interleaving research
clinical psychologists  one interleaving community psychologist
remaining candidates   since parameter distribution constrained sum   
total    free parameters 
    model estimation
paper estimate riffle independent models based methods introduced
earlier work  given hierarchial structure model  maximum likelihood parameter
estimates hierarchical riffle independent model straightforward compute via frequency estimates  estimate correct structure model challenging
problem  key insight lies noticing two subsets b riffle independent 
j  k b  independence relation  i    j     k   must hold 
structure learning algorithms operate hunting tripletwise independence
relations within data  defer interested readers details  huang   guestrin 
      
   

fiefficient inference partial rankings

note earlier work  assumed algorithms access dataset
consisting i i d  full rankings provided users  current work  relax
assumptions allowing users provide partially ranked data  one assumption throughout  however  user full ranking mind items  particular 
current work address incomplete ranking problem  users might
seen items  we discuss possible extensions incomplete ranking setting
section   

   decomposable observations
given prior distribution  h  rankings observation o  bayes rule tells us
posterior distribution  h  o   proportional l o   h    l o  
likelihood function  operation conditioning h observation typically computationally intractable since requires multiplying two n  dimensional functions  unless
one exploit structural decompositions problem  section  describe decomposition certain class likelihood functions space rankings
observations factored simpler parts  observation decomposable
way  show one efficiently condition riffle independent prior distribution
o  simplicity paper  focus primarily subset observations whose likelihood
functions encode membership subset rankings sn  
definition    subset observations   subset observation binary observation whose
likelihood proportional indicator function subset sn i e  

 
 
l o    
  otherwise
running example  consider class first place observations throughout
chapter  we consider far general observation models later sections   first
place observation  corn ranked first  example  associated collection
rankings placing item corn first place  o        corn         interested
computing posterior h   o   thus first place scenario  given voters
top choice would infer preferences remaining candidates 
given partitioning item set two subsets b  sometimes possible
decompose  or factor   subset observation involving items smaller subset observations involving a  b interleavings b independently  decompositions
often exploited efficient inference 
example   
consider first place observation
  corn ranked first 
decomposed two independent observations observation
relative ranking vegetables  observation interleaving vegetables
fruits 
   

fihuang  kapoor   guestrin

oa   corn ranked first among vegetables 
oa b   first place occupied vegetable 
condition case  one updates relative ranking distribution
vegetables  a  zeroing rankings vegetables place corn first
place  updates interleaving distribution zeroing interleavings
place vegetable first place  normalizes resulting distributions 
example nondecomposable observation observation
  corn third place 
see decompose  with respect vegetables fruits   enough
notice interleaving vegetables fruits independent relative
ranking vegetables  if  example  element interleaves  vegetables 
b  fruits  ab      a b a b  since  corn       relative ranking
vegetables constrained      p eas corn  since interleavings
relative rankings independent  see cannot decomposable 
formally  use riffle independent factorizations define decomposability respect
hierarchy h item set 
definition    decomposability   given hierarchy h item set  subset observation decomposes respect h likelihood function l o   factors riffle
independently respect h 
subset observations prior decompose according hierarchy 
show  as example    posterior decomposes 
proposition     let h hierarchy item set  given prior distribution h
subset observation decompose respect h  posterior distribution
h  o  factors riffle independently respect h 
proof  denote likelihood function corresponding l  in proof 
matter assumed subset observation result holds arbitrary
likelihoods  
use induction size item set n       base case n     trivially
true  next consider general case n      posterior distribution  bayes rule 
written h  o  l   h    two cases  h leaf node 
posterior h  trivially factors according h  done  otherwise  l h
factor  assumption  according h    ha   hb   following way 
l     ml  ab    fl  a    gl  b      h     mh  ab    fh  a    gh  b     
multiplying grouping terms  see posterior factors as 
h  o     ml mh   ab      fl fh   a      gl gh   b     
show h  o  factors respect h  need demonstrate  by definition   
distributions  fl fh    gl gh    after normalizing  factor respect ha
   

fiefficient inference partial rankings

hb   respectively  since fl fh factor according hierarchy ha assumption
 a    n since h leaf  invoke inductive hypothesis show
posterior distribution  proportional fl fh must factor according ha  
similarly  distribution proportional gl gh must factor according hb  

   complete decomposability
condition proposition     prior observation must decompose respect exactly hierarchy  sufficient one efficient inference  might
first glance seem restrictive render proposition useless practice  overcome
limitation hierarchy specific decomposability  explore special family observations  which call completely decomposable  property decomposability
depend specifically particular hierarchy  implying particular
observations  efficient inference always possible  provided efficient representation
prior distribution possible  
illustrate observation decompose respect multiple hierarchies
item set  consider first place observation  corn ranked first  argued
example   decomposable observation  notice however decomposability
particular observation depend items partitioned
hierarchy  specifically  instead vegetables fruits  sets    corn  apples 
b    p eas  oranges  riffle independent  similar decomposition would continue
hold  decomposing observation relative ranking items  corn
first among items a   observation interleaving b  first place
occupied element a  
formally capture notion observation decompose respect
arbitrary underlying hierarchies  define complete decomposability 
definition     complete decomposability   say subset observation completely decomposable decomposes respect every possible hierarchy item
set   denote collection possible completely decomposable  subset  observations c  see figure   illustration set c 
conceptually  completely decomposable observations correspond indicator functions
riffle independent possible  complete decomposability guarantee
observation one always exploit available factorized structure prior
distribution order efficiently condition o 
proposition     let h binary hierarchy item set  given prior h
factorizes respect h  completely decomposable observation o  posterior
h  o  decomposes respect h 
proof  proposition    follows simple corollary proposition    
example     simplest example completely decomposable observation uniform observation ounif     includes possible rankings corresponds
uniform indicator function unif rankings  given hierarchy h  unif shown
decompose riffle independently respect h  factor uniform 
hence ounif completely decomposable 
   

fihuang  kapoor   guestrin

h 

h 

h 

completely
decomposable
observations

h 

h 

h 

figure    diagram illustrating collection completely decomposable observations  c 
shaded region  labeled hi   represents family subset observations
sn decompose respect hierarchy hi   collection c
seen intersection shaded regions  subset observations
lie inside intersection ones conditioning performed
linear time  in number model parameters  

uniform observation course particularly interesting context bayesian
inference  hand  given stringent conditions definition    
obvious nontrivial completely decomposable observations even exist  nonetheless 
exist nontrivial examples  such first place observations   next
section  exhibit rich general class completely decomposable observations 

   complete decomposability partial ranking observations
section discuss mathematical problem fully characterizing class
completely decomposable observations  main contribution section show
completely decomposable observations correspond precisely partial rankings
item set 
partial rankings  begin discussion introducing partial rankings  allow
items tied respect ranking dropping verticals vertical bar
representation  
definition     partial ranking observation   let                r ordered collection
subsets partition  i e     j      j   partial ranking
observation   corresponding partition collection rankings rank items
   remarked ailon         note term partial ranking used confused
two standard objects      partial order  namely  reflexive  transitive anti symmetric binary

   

fiefficient inference partial rankings

items j   j  denote partial ranking               r say
type                         r     denote collection partial rankings
 over n items  p 
partial ranking defined viewed coset subgroup  
s  s  sr   given type full ranking   one
partial ranking type containing   thus therefore equivalently denote partial
ranking               r   element               r   note coset
notation allows multiple rankings refer partial ranking  
space partial rankings defined captures rich natural class
observations  particular  partial rankings encompass number commonly occurring
special cases  traditionally modeled isolation  work  as well
recent works lebanon   lafferty        lebanon   mao        used
unified setting 
example     partial ranking observations include 
 first place  top   observations   first place observations correspond partial
rankings type       n     observation corn ranked first
written corn peas apples oranges 
 top k observations   top k observations partial rankings type                  n
k   generalize first place observations specifying items mapping
first k ranks  leaving n k remaining items implicitly ranked behind  example 
observation corn ranked first peas ranked second written
corn peas apples oranges 
 desired less desired dichotomy   partial rankings type    k  n k  correspond
subset k items preferred desired remaining subset n k items 
example  partial rankings type  k  n k  might arise approval voting
voters mark subset approved candidates  implicitly indicating disapproval
remaining n k candidates 
 ratings   finally  partial rankings come form rating data where 
example  restaurants rated as                corresponding partial ranking
would thus tie restaurants rated number stars  ranking
restaurants stars restaurants fewer stars 
 trivial observations   partial rankings type    n  refer trivial observations
whose likelihood functions uniform entire space rankings    trivial
observation rankings item set    corn  p eas  apples   example 
simply written simply corn  p eas  apples 
show partial ranking observations decompose  exhibit explicit factorization respect hierarchy h items  simplicity  begin considering
single layer case  items partitioned two leaf sets b  factorization depends following notions consistency relative rankings interleavings
partial ranking 
relation      ranking subset  which discuss section   incomplete rankings  
search engines  example  although top k elements returned  remaining n k
implicitly assumed ranked behind  and therefore  search engines return partial rankings  

   

fihuang  kapoor   guestrin

definition     restriction consistency   given partial ranking                 r
subset   define restriction partial ranking items
obtained intersecting a  hence restriction is 
 s  a     a   a         r a 
given ranking  items a  say consistent partial ranking
member restriction a   s  a  
definition     interleaving consistency   given interleaving ab two sets a  b
partition   say ab consistent partial ranking              r  with
type   first   entries ab contain number bs    
second   entries ab contain number bs     on  given
partial ranking   denote collection consistent interleavings  s  ab  
example  consider partial ranking
  corn  apples p eas  oranges 
places single vegetable single fruit first two ranks  single vegetable
single fruit last two ranks  alternatively  partially specifies interleaving
ab ab  full interleavings a b b a b a b a consistent  by dropping
vertical lines  a a b b consistent  since places two vegetables first two
ranks  
using notions consistency partial ranking  show partial ranking
observations decomposable respect binary partitioning  i e   single layer
hierarchy  item set 
proposition     single layer hierarchy   partial ranking observation
binary partitioning item set  a  b   indicator function     factors riffle
independently as 
     mab  ab     fa  a     gb  b     

     

factors mab   fa gb indicator functions consistent interleavings
relative rankings   s  ab    s  a  s  b   respectively 
single layer decomposition proposition    turned recursive decomposition partial ranking observations arbitrary binary hierarchies  establishes
main result  particular  given partial ranking prior distribution
factorizes according hierarchy h  first condition topmost interleaving distribution zeroing parameters corresponding interleavings consistent
  normalizing distribution  need condition subhierarchies
ha hb relative rankings b consistent   respectively 
since consistent sets   s  a  s  b   partial rankings themselves 
algorithm conditioning partial ranking applied recursively
subhierarchies ha hb   precise  show that 
theorem     every partial ranking completely decomposable  p c  
   

fiefficient inference partial rankings

prcondition  prior hprior   hierarchy h  observation                 r  
isleaf h 
forall

hprior   
hpost   
 
 
otherwise
normalize  hpost    
return  hpost   
else
forall

mprior      s  ab
mpost    
 
 
otherwise
normalize  mpost    
f  a   prcondition  fprior   ha    s  a    
g b   prcondition  gprior   hb    s  b    
return  mpost   fpost   gpost   

algorithm    pseudocode prcondition  algorithm recursively conditioning hierarchical riffle independent prior distribution partial ranking observations  see definitions   
    s  a    s  b    s  ab   runtime prcondition o n  h     h 
number model parameters  input  parameter distributions prior hprior represented explicit tabular form  observation form partial ranking  output 
parameter distributions posterior hpost represented explicit tabular form 

since proof theorem    fairly straight forward given form factorization  equation       deferred appendix  consequence theorem   
proposition     conditioning partial ranking observations performed efficiently 
see algorithm   details recursive conditioning algorithm 
running time complexity conditioning partial ranking  recursion
algorithm   operates parameter distribution once  setting probabilities
interleavings relative rankings distribution either zero not 
normalizing  decide whether zero probability not  one must check partial
ranking consistency either interleaving relative ranking  requires
o n  time  therefore  total  algorithm   requires o n  h   time   h 
total number model parameters  notice complexity conditioning depends
linearly complexity prior whenever prior distribution compactly
represented  efficient inference partial ranking observations possible 
stated section     h  general scale exponentially n  thin chain models 
number items factored model stage never
small constant k  verifying interleaving relative ranking consistency performed
constant time  implying conditioning operation linear number model
parameters  guaranteed polynomial n 
example     example  consider conditioning apa distribution example   observation candidate   ranked first place 
represented partial ranking                 recall candidate   charles
kiesler  research psychologist 
figure   a  show structure parameters prior distribution
apa election data  highlighting particular interleavings relative rankings
   

fihuang  kapoor   guestrin



mb c  



c b b b b

   

c b b b b

mb c  
 

b c b b b

   

b c b b b

   

b b c b b

   

b b c b b

   

b b b c b

   

b b b c b

   

b b b b c

   

b b b b c

   



md e  



fc  



md e  



fc  

d d e e

   

 

    

d d e e

   

 

    

d e d e

   

d e d e

   

d e e d

   

d e e d

   

e d d e

   

e d d e

 

e d e d

   

e d e d

 

e e d d

   

e e d d

 



fd  



fe  



fd  



fe  

   

   

   

   

   

 

   

   

   

   

   

   

   

    

   

   

 a  structure parameters prior distribution  with consistent relative rankings interleavings highlighted  

 b  structure parameters posterior distribution conditioning 

figure    example conditioning apa hierarchy  from example    first place
observation candidate   ranked first place 

consistent o  example  possible interleavings research psychologists
 d  clinical psychologists  e   interleavings consistent
rank research psychologist first among research clinical psychologists 
therefore three consistent interleavings  d d e e  d e d e  d e e d 
conditioning sets relative rankings interleavings consistent
zero normalizes resulting parameter distribution  resulting riffle
independent representation posterior distribution shown figure   b  

    impossibility result
interesting consider completely decomposable observations exist beyond partial
rankings  one main contributions show observations 
theorem     converse theorem      every completely decomposable observation takes
form partial ranking  c p  
together  theorems       form significant insight nature rankings 
showing notions partial rankings riffled independence deeply connected 
fact  result shows even possible define partial rankings via complete
decomposability 
practical matter  theorem    shows algorithm based simple
multiplicative updates parameters exactly condition observations
take form partial rankings  computational complexity conditioning
observations partial rankings remains open  conjecture approximate
inference approaches may necessary efficiently handling complex observations 
   

fiefficient inference partial rankings

    proof impossiblity result  theorem    
turn proving theorem     since proof significantly longer less obvious
proof converse  theorem      sketch main ideas drive proof
refer interested readers details appendix 
recall definition linear span set vectors vector space
intersection linear subspaces containing set vectors  prove theorem    
introduce analogous concepts span set rankings 
definition     rspan pspan   let x sn collection rankings  define
pspan x  intersection partial rankings containing x  similarly  define
rspan x  intersection completely decomposable observations containing x 
formally 
 
 
pspan x   
  rspan x   
o 
o xo  oc

 xs

example  x    corn p eas apples  apples p eas corn   checked
partial ranking three items containing items x entire set itself 
thus pspan x    corn  p eas  apples 
proof strategy establish two claims      pspan set always
partial ranking      fact  rspan pspan set x exactly
sets  since claim     fact partial rankings involve riffled
independence  defer related proofs appendix  thus have 
lemma     x sn   pspan x  partial ranking 
proof  see appendix 
following discussion instead sketch proof claim      first show  however 
theorem    must hold indeed true claims         hold 
proof   of theorem      given c  want show p  claim
     rspan o    pspan o   since element c  however 
  rspan o   thus   pspan o   finally lemma     claim      guarantees
pspan o  partial ranking  conclude p 
proceed establish claim rspan x    pspan x   following
proposition lists several basic properties rspan use several
proofs  follow directly definition write proofs 
proposition    
i   monotonicity  x  x rspan x  
ii   subset preservation  x  x   x x     rspan x  rspan x     
iii   idempotence  x  rspan rspan x     rspan x  
one inclusion proof rspan x    pspan x  follows directly fact
p c  theorem     
   

fihuang  kapoor   guestrin

formpspan x 
x  x    
      xt disagree relative ordering items a    a 
xt  
foreach xt
add partial ranking obtained deleting vertical bar items
a  a  xt  
    
return  any element xt    

algorithm    pseudocode computing pspan x   formpspan x  takes set partial
rankings  or full rankings  x input outputs partial ranking  algorithm iteratively
deletes vertical bars elements x agreement  note necessary
keep track t  ease notation proofs  algorithm
direct way computing pspan x   again  simplifies proof main theorem 

lemma     subset orderings  x  rspan x  pspan x  
proof  fix subset x sn let element rspan x   would show
element pspan x   consider partial ranking p covers x
 i e       x   want see   theorem     p c 
therefore  c  since rspan x       x  conclude 
definition rspan    since holds partial ranking covering x 
pspan x  
remains task establishing reverse inclusion 
proposition     subset orderings  x  rspan x  pspan x  
prove proposition     consider problem computing partial ranking span
 pspan  given set rankings x  algorithm    show simple procedure based
iteratively finding rankings x disagree pairwise ranking two items 
replacing rankings partial ranking vertical bar two
elements removed  show algorithm provably outputs correct
result 
proposition     given set rankings x input  algorithm   outputs pspan x  
proof  see appendix 
final step able prove proposition     prove following two
technical lemmas relate computation pspan algorithm   riffled independence  really form heart argument  particular  completely
decomposable observation c  lemma    shows ranking contained
force rankings contained o 
lemma     let c suppose exist       disagree relative
ranking items i  j   ranking obtained swapping relative ranking
items i  j within   must contained o 
   

fiefficient inference partial rankings

proof  let h indicator distribution corresponding observation o 
show swapping relative ranking items i  j   result ranking
assigned nonzero probability h  thus showing new ranking contained o 
let    i  j  b    a  since c  h must factor riffle independently according
partition  a  b   thus 
h       m ab       f  a       g b           
h       m ab       f  a       g b           
since     disagree relative ranking items a  factorization implies
particular f  a   i j      f  a   j i       since h          must
m ab        f  a        g b       positive probability 
therefore swap relative ranking a    obtain new ranking positive
probability since terms decomposition new ranking positive
probability 
lemma    provides conditions removing vertical bar one
rankings x change support completely riffle independent distribution  illustrate example  consider completely decomposable observation
contains partial ranking   corn  p eas apples  oranges subset 
lemma    guarantees that  if  addition  exists element disagrees
relative ordering of  say  p eas oranges  fact partial ranking
      corn  p eas  apples  oranges  with bar removed   must
subset o  formally 
lemma     let              i  i            k partial ranking item set  
                 i i            k   partial ranking sets i  
merged  let a  ij   j a  kj i   j   element c
additionally exists ranking disagrees relative
ordering a    a        o 
proof  key strategy proof lemma    argue large subsets rankings
must contained completely decomposable observation decomposing rankings
transpositions invoking technical lemma  lemma     repeatedly 
see appendix details 
use lemma    show reverse inclusion proposition   
holds  establishing two sets rspan x  pspan x  fact equal thereby
proving desired result  c p 
proof   of proposition     iteration t  algorithm   producess
set partial rankings 
xt   denote union partial rankings time xt xt   note
x    x xt   pspan x   idea proof show iteration
t  following set inclusion holds  rspan xt   rspan xt     indeed holds 
   

fihuang  kapoor   guestrin

final iteration   shown that 
pspan x    xt  

 proposition    

rspan xt   

 monotonicity  proposition    

rspan x    

 since rspan xt   rspan xt     shown below  

rspan x 

 x    x  see algorithm   

would prove proposition 
remains show rspan xt   rspan xt     claim xt rspan xt    
let xt   xt    since xt  rspan xt     rspan xt   
proof done  otherwise  xt  xt    second case  use fact
iteration t  vertical bar i   deleted partial ranking              i  i            k  which subset xt    form partial ranking
                 i i            k    which subset xt    furthermore  order
vertical bar deleted algorithm  must existed partial
ranking  and therefore full ranking     disagreed relative ordering items a    a  opposite sides bar  since xt  xt  assume
     
would apply lemma     note c xt  o 
o  since xt    application lemma    shows
    therefore o 
shown fact holds observation c xt 
o  therefore taking intersection supports c  see xt
rspan xt     taking rspan sides yields 
rspan xt   rspan rspan xt     
rspan xt    

 subset preservation  proposition    

 idempotence  proposition    

    going beyond subset observations
though stated results far subset observations  comment
theory would look considered general likelihood functions 
order avoid confusion  refer general class functions call completely decomposable functions  instead completely decomposable subset observations
definition    
definition     function h   sn r called completely decomposable function
factors riffle independently respect every hierarchy item set   denote
e
collection possible completely decomposable functions c 
e nearly same  quite simple restate theorem   
discuss  c c
respect general case completely decomposable functions 
theorem  every partial ranking indicator function completely decomposable function 
   

fiefficient inference partial rankings

unfortunately  proof converse  theorem     easily generalize 
instead used show support    sn   h         every completely
decomposable function partial ranking  natural  however  suspect full
converse indeed exist every completely decomposable function proportional
indicator function partial ranking  fact  suspected converse almost
holds  have 
theorem  h completely decomposable function supported partial ranking
             r  i    
                 r  h proportional indicator
function  
proof  see appendix 
example     completely decomposable functions  possible away
assumption  i    
    i  example  function defined as 

      corn p eas apples
      p eas corn apples  
h    

 
otherwise
supported partial ranking   corn  p eas apples  where           
proportional indicator function  i e   uniform rankings
assigned positive probability  
however  still possible show h completely decomposable function 
prove so  necessary establish three things   corn  p eas   apples 
riffle independent   corn  apples   p eas  riffle independent 
 p eas  apples   corn  riffle independent  example  respect partitioning sets    corn  apples  b    p eas   see
h     m ab     f  a     g b     
where 

   
   
m ab    

 

ab   a b a
ab   b a a  
ab   a a b


f  a    

 
 

 ac    a c
 
otherwise

g b       

therefore   i        possible completely decomposable functions
uniform supports 
    conditioning noisy observations
conclude section remark handling noise observations 
assumed paper observed partial rankings always consistent users
underlying full ranking  situations one may wish model noisier
setting  partial rankings may misreported small probability 
natural model accounts noise  example  might be 

 
l o    
 
     

 o   otherwise
   

fihuang  kapoor   guestrin

prior distribution factorizes respect hierarchy h  conditioning
noisy likelihood equation     results posterior distribution written
weighted mixture prior distribution posterior would resulted
conditioning noise free observation  component posterior
distribution factorizes respect h  mixture factor general  and
factor according theory   result  iteratively conditioning multiple
partial rankings according noisy likelihood function would quickly lead
unmanageable number mixture components  therefore believe approximate
inference methods conditioning multiple noisy partial ranking observations fruitful
area research 

   model estimation partially ranked data
many ranking based applications  datasets predominantly composed partial rankings rather full rankings due fact humans  partial rankings typically
easier faster specify  addition  many datasets heterogeneous  containing partial
ranking different types  example  american psychological assoication well
irish house parliament elections  voters allowed specify top k candidate
choices value k  see figures   a    b    section use efficient
inference algorithm proposed section   estimating riffle independent model
partially ranked data  estimating model using partially ranked data typically
considered difficult estimating one using full rankings  common practice  e g   see huang   guestrin        simply ignore partial rankings
dataset  ability method incorporate available data however  lead
significantly improved model accuracy well wider applicability method 
section  propose first efficient method estimating structure parameters
hierarchical riffle independent model heterogeneous datasets consisting arbitrary
partial ranking types  central approach idea given someones partial preferences  use efficient algorithms developed previous section infer full
preferences consequently apply previously proposed algorithms designed
work full rankings 
    censoring interpretations partial rankings
model estimation problem full rankings stated follows  given i i d  training
examples                m   consisting full rankings  drawn hierarchical riffle independent distribution h  recover structure parameters h 
partial ranking setting  assume i i d  draws  training
example  i  undergoes censoring process producing partial ranking consistent  i   
example  censoring might allow ranking top k items  i 
observed  allow arbitrary types partial rankings arise via censoring 
make common assumption partial ranking type resulting censoring  i 
depend  i  itself 
   

fiefficient inference partial rankings

    algorithm
treat model estimation partial rankings problem missing data problem 
many problems  could determine full ranking corresponding observation data  could apply algorithms work completely observed
data setting  since full rankings given  utilize expectation maximization  em 
approach use inference compute posterior distribution full rankings
given observed partial ranking  case  apply algorithms huang
guestrin              designed estimate hierarchical structure
model parameters dataset full rankings 
given initial model h collection training examples  o      o              o m   
consisting partial rankings  em based approach alternates following two
steps convergence achieved 
 e step   observation  o i     i   i    training examples  use
inference compute posterior distribution full ranking could
generated o i  via censoring  h  o i     i   i     since observations take
form partial rankings hence completely decomposable  use efficient
algorithms section   perform e step 
 m step   m step  one maximizes expected log likelihood training
data respect model  hierarchical structure model
provided  known beforehand  m step performed using standard methods optimizing parameters  structure unknown  use structural
em approach  analogous methods graphical models literature
structure learning incomplete data  friedman              
unfortunately   riffled independence  structure learning algorithm huang
guestrin        unable directly use posterior distributions computed
e step  instead  observing sampling riffle independent models
done efficiently exactly  as opposed to  example  mcmc methods   simply
sample full rankings posterior distributions computed e step
pass full rankings structure learning algorithm huang guestrin
        number samples necessary  instead scaling factorially  scales
according number samples required detect riffled independence  which
mild assumptions polynomial n  huang   guestrin        

   related work
rankings permutations recently become active area research machine
learning due part hinge role play information retrieval preference
elicitation  algorithms ranksvm  joachims        rankboost  freund 
iyer  schapire    singer         example  successful large scale ranking
problems appear web search  main aims work differ web
scale settings however instead seeking single optimal ranking respect
objective function  seek understanding large collection rankings via density
estimation  following  outline two major lines research influenced
work 
   

fihuang  kapoor   guestrin

    additive multiplicative decompositions
paper builds particular upon thread recent work tractable models permutation data based function decompositions  kondor  howard  jebara       
huang  guestrin  guibas              considered additive decompositions distribution weighted sum fourier basis functions  papers show low frequency
fourier assumptions often effective coping representational complexity
working distributions permutations  show particular conditioning
prior distributions low frequency likelihood functions often arise multiobject
tracking problems performed especially efficiently 
unfortunately  low frequency assumptions applicable distributions defined
rankings  address ranking problems specifically  huang guestrin       
      introduced concept riffled independence useful generalization probabilistic independence rankings  using multiplicative decompositions based riffled
independence  showed possible learn hierarchical structure model
given fully ranked dataset  previous papers topic riffled independence
focused problems related efficiently representing distributions  main focus
current paper lies efficient reasoning inference tackling human task complexity
considering partial rankings 
interesting note natural efficient condition fourier based
representation low frequency observations  involving small number items 
 alice third place  multiplicative decomposition based riffled independence
would able efficiently condition observation  hand 
multiplicative decompositions allow us condition top k observations efficiently  independently size k   whereas top k observations would difficult handle
fourier theoretic setting  except small k  
    mallows models
work fits larger body research well known mallows distribution
rankings  parameterized by 
h                

     

function refers kendalls tau distance metric rankings  mallows
distribution  equation      always shown special case hierarchical riffle independent model items sequentially factored model one
one  huang         see figure    
mallows models  as well similar distance based models  advantage
compactly represent distributions large n  admit conjugate prior
distributions  meila  phadnis  patterson    bilmes         estimating parameters
popular problem statisticians recovering optimal   data known
consensus ranking rank aggregation problem known n p  hard  bartholdi 
tovey    trick         many authors focused approximation algorithms instead 
gaussian distributions  mallows models tend lack flexibility  lebanon
mao        propose nonparametric model ranked  and partially ranked  data based
placing weighted mallows kernels top training examples  which  show 
   

fiefficient inference partial rankings

 corn peas apples oranges doughnuts 

 corn 

 peas apples oranges doughnuts 

 peas 

 apples oranges doughnuts 
 apples 

 oranges doughnuts 
 oranges 

 doughnuts 

figure    mallows model always factors according refer chain
structure items factored one one  mallows distribution five items food item set mode  or central ranking 
    corn p eas apples oranges doughnuts  example  must factor according hierarchical structure 

realize far richer class distributions  learned efficiently  however 
address inference problem  immediately clear many mallows models
papers whether one efficiently perform inference operations marginalization
conditioning models  riffle independent models  hand  encompass
class distributions rich well interpretable  additionally 
identified precise conditions efficient conditioning possible  the conditions
observations take form partial rankings  
several recent works model partial rankings using mallows based models 
busse  orbanz  buhmann        learned finite mixtures mallows models topk data  also using em approach   lebanon mao         mentioned 
developed nonparametric model based mallows models handle arbitrary
types partial rankings  settings  central problem marginalize mallows
model full rankings consistent particular partial ranking 
efficiently  papers rely fact  first shown fligner   verducci       
marginalization step performed closed form  closed form equation fligner
verducci         however  seen special case setting since mallows
models always shown factor riffle independently according chain structure 
specifically  compute sum rankings consistent partial ranking
  necessary condition   compute normalization constant
resulting function  conditioning step performed using methods
described paper  normalization constant computed multiplying
normalization constant factor hierarchical decomposition  thus  instead
resorting complicated mathematics inversion combinatorics  theory
complete decomposability offers simple conceptual way understand mallows models
conditioned efficiently partial ranking observations 
   

fihuang  kapoor   guestrin

finally recent related work  lu boutilier        considered even general
class observations based dag  directed acyclic graph  based observations
probabilities rankings consistent dag relative ranking relations
set zero  lu boutilier show particular conditioning problem
dag based class observations  p  hard  additionally propose efficient
rejection sampling method performing probabilistic inference within general class
dag observations prove sampling method exact class partial
rankings discussed paper 

   experiments
section  demonstrate method learning hierarchical riffle independent models partial rankings simulated data well real datasets taken different
domains  experiments  initialize distributions uniform  use random restarts 
    datasets
addition roughly      full rankings  apa dataset        top k rankings
  candidates  previous work  used full rankings apa data  huang
  guestrin               able use entire dataset  figure   a  plots 
k                 number ballots apa data length k 
likewise  meath dataset  gormley   murphy        taken     
irish parliament election        top k rankings    candidates  apa
data  used full rankings meath data previous work  use
entire dataset  figure   b  plots  k                  number ballots
meath data length k  particular  note vast majority ballots dataset
consist partial rather full rankings  half electorate preferring list
favorite three four candidates  run inference  algorithm   
     top k examples meath data    seconds dual     ghz pentium machine
unoptimized python implementation  using brute force inference  estimate
job would require roughly one hundred years 
extracted third dataset database searchtrails collected white
drucker         browsing sessions roughly      users logged           many cases  users unlikely read articles news story twice 
often possible think order user reads collection
articles top k ranking articles concerning particular story topic  ability
model visit orderings would allow us make long term predictions user browsing
behavior  even recommend curriculums articles users  ran algorithms
roughly     visit orderings eight popular posts www huffingtonpost com
concerning sarah palin  popular subject      u s  presidential election  since
user visited every article  full rankings data thus
even exist option learning using subset full rankings 
   

fiefficient inference partial rankings

number votes

number votes

    
    

    
    
    
    

      

      

 

 

 

 

 
 
 
 
number candidates

 

 

          
k

 a  apa election data

 b  irish election data

figure    histograms top k ballot lengths apa irish election datasets  whereas
majority electorate provided full rankings apa election data
 probably due fact five candidates   vast majority
voters irish election data provided top   top   choices 
       

       
       

   

      

   

      
      

     

   
   

    

 a  structure learned using
subset full rankings
 out     given training
examples 

     

   
   

    

 b  structure learned using
training examples   iteration em

    
research

   
community
psychologists

    
clinical

 c  structure learned using
training examples structural convergence    iterations 

figure    structure learning subset apa dataset      rankings  randomly
sampled  including full partial rankings  

    apa structure learning results
due unordinarily large number full rankings apa data  gains made
additionally using partially ranked data insignificant  better illustrate benefits
partial rankings  subsampled dataset     rankings  including full partial
rankings  present results smaller dataset  performing structure learning using
full rankings     training examples  consisting roughly     examples  
one obtains structure figure   a   seen match correct
structure figure   a  learned using      full rankings  figures   b    c 
   

fihuang  kapoor   guestrin

training time  seconds 

test log likelihood

x     
  
  
  
  

  
em

flat em

 

training time  seconds 

test log likelihood

  
    
    
    
    
  
k  

  
 

 
em

flat em uniform
fill in

 b  training time comparison
em approach flatem
uniform fill in methods 

x   

k  

  

uniform
fill in

 a  test set log likelihood comparison
em approach flatem
uniform fill in methods 

k  

  

   
 
   

 

k  

 c  test set log likelihoods  training
top t rankings larger
fixed k 

k  

k  

k  

k  

 d  training times  training
top t rankings larger fixed
k 

figure    apa experimental results experiment repeated     bootstrapped
resamplings data

plot results em algorithm former displaying resulting structure
single em iteration latter result structural convergence  occurs
third iteration  showing method learn correct structure given
    training examples 
compared em algorithm two alternative baseline approaches
refer plots flatem uniform fill in  flatem algorithm
em algorithm except two details      performs conditioning exhaustively
instead exploiting factorized model structure      performs m step without
sampling  uniform fill in approach treats every top k ranking training set
uniform collection votes full rankings consistent top k ranking 
accomplished using one iteration em algorithm 
figure   a  plot test set loglikelihoods corresponding approach  em
flatem almost identical results performing much better
uniform fill in approach  hand  figure   b   compares running times
three approaches  shows flatem far costly  for datasets 
cannot even run reasonable amount time  
   

fiefficient inference partial rankings

 st iteration

 nd iteration

                 

 rd iteration

                 
                 

   

               

   

               
   

             

   

             

               

   
             

           

   

           

   
       

       

     

log likelihood           
 a 

       

   

       

     

log likelihood           
 b 

log likelihood           
 c 

figure     iterations structure em sarah palin data structural changes
iteration highlighted red  structural convergence occurs three
iterations  note structure discovered using visit orders 
text information pages incorporated learning process 
figure best viewed color 

verify partial rankings indeed make difference apa data  plot
results estimating model subsets apa training data consisting top k
rankings length larger fixed k  figures   c    d  show log likelihood
running times k              k     entire training set k    
subset training data consisting full rankings  results show 
including partial rankings indeed help average improving test log likelihood
 with diminishing returns  
    structure discovery em larger n 
experiments led several observations using em learning partial
rankings  first  observe typical runs converge fixed structure quickly 
three em iterations  figure    shows progress em sarah palin
data  whose structure converges third iteration  expected  log likelihood
increases iteration  remark structure becomes interpretable
example  leaf set           corresponds three posts palins wardrobe
election  posts leaf set           related verbal gaffes
made palin campaign  notice structure discovered purely using
data visit orders text information used experiments 
   

fihuang  kapoor   guestrin

     

x    

test log likelihood

  em iterations
convergence

  

  
  
  

  

em
decomposable
conditioning

    

 lebanon   mao     

     
     

 
 

     

 

                            
k

                         
  partial rankings training set
 in addition full rankings 

 a 

 b 

figure      a   number em iterations required convergence training set
contains rankings length longer k   b   density estimation synthetic
data  plot test loglikelihood learning     full rankings
         additional partial rankings 

     training
examples

 

test log likelihood

x   
    



    

      training
examples

 

x   
     

 lm   

 lm   

     



    

    



     

 lm   


    

 lm   

     

    

full

mixed  full partial 

full

mixed  full partial 

figure     density estimation small       examples  large subsets        examples  meath data  compare method work lebanon
mao        two settings      training available data     training
subset full rankings 

secondly  number em iterations required reach convergence log likelihood
depends types partial rankings observed  ran algorithm subsets
meath dataset  time training        rankings length larger
   

fiefficient inference partial rankings

fixed k  figure    a  shows number iterations required convergence
function k  with    bootstrap trials k   observe fastest convergence
datasets consisting almost full rankings slowest convergence consisting
almost empty rankings  almost    iterations necessary one trains using rankings
types  finally remark model obtained first iteration em
interesting thought result pretending voter completely
ambivalent regarding n k unspecified candidates 
    value partial rankings
verify larger n using partial rankings addition full rankings
allows us achieve better density estimates  first learned models synthetic data
drawn hierarchy  training using     full rankings plus varying numbers partial
ranking examples  ranging            repeat setting    bootstrap
trials  evaluation  compute log likelihood testset      examples 
speed  learn structure h fix h learn parameters trial 
figure    b   plots test log likelihood function number partial
rankings made available training set  shows indeed able learn
accurate distributions data form partial rankings made
available 
    comparing nonparametric model
comparing performance riffle independent models approaches possible previous work since able handle partial rankings  using
methods developed current paper  however  compare riffle independent models
state of the art nonparametric estimator lebanon mao         to
hereby refer lm   estimator  data  setting regularization parameter c            via validation set   figure    b  shows  naturally 
data drawn synthetically riffle independent model  em method significantly outperforms lm   estimator  remark theory  lm   guaranteed
catch performance  under appropriate conditions  given enough training examples 
meath data  approximately riffle independent  trained subsets
size               testing remaining data   subset  evaluated em
algorithm learning riffle independent model lm   estimator    
using full ranking data      using data  before  methods better
partial rankings made available 
smaller training set  riffle independent model performs well better
lm   estimator  larger training set         see nonparametric
method starts perform slightly better average  advantage nonparametric
model guaranteed consistent  converging correct model given
enough data  advantage riffle independent models  however  simple 
interpretable  highlight global structures hidden within data 
   

fihuang  kapoor   guestrin

   future directions
remain several possible extensions current work  list open
questions extensions following 
    inference incomplete rankings
shown paper one exploit riffled independence structure condition
observation takes form partial ranking  space
partial rankings rich useful many settings  cover important class
observations  incomplete rankings  defined ranking  or partial
ranking  subset itemset   example  theorem    shows conditioning problem pairwise observations form apples preferred bananas
nondecomposable  note top k rankings considered complete rankings since
implicitly rank items last n k positions 
then  tractably condition incomplete rankings  one possible approach
convert fourier representation using methods  huang   guestrin        
conditioning pairwise ranking observation using fourier domain conditioning
algorithm proposed  huang et al          fourier domain approach would useful one particularly interested low order marginal probabilities posterior
distributions 
fourier approach viable  another option may assume
posterior distribution takes particular riffle independent structure  in way
mean field methods graphical models literature would assume factorized
posterior   research question interest is  hierarchical structure used
purposes approximating posterior 
    reexamining data independence assumptions
paper  assumed throughout training examples independent
identically distributed  however practice always safe assumptions
number factors impact validity both  example  internet survey
user must perform series preference ranking tasks sequence  concern
users prior ranking tasks may bias results future rankings 
another source bias lies reference ranking may displayed 
user asked rearrange items dragging dropping  one hand  showing
everyone reference ranking may bias resulting data  hand 
showing every user different reference ranking may mean training examples
exactly identically distributed 
yet another form bias lies partial ranking types reported data 
formulate em algorithm  assumed users preferences influence
whether chooses to  say  report full ranking instead top   ranking  practice 
however  partial ranking types user preferences often correlated  irish elections  example  typically one sinn fein candidate  rank
sinn fein first typically likely reported top   choice 
   

fiefficient inference partial rankings

understanding  identifying  finally  learning spite different types biases
may occur eliciting preference data remains fundamental problem ranking 
    probabilistic modeling strategic voting
interesting consider differences actual vote distributions considered
paper approximate riffle independent distributions  take apa dataset 
example  optimal approximation riffle independent hierarchy reflects
underlying political coalitions within organization  upon comparison
approximation empirical distribution  however  marked differences arise 
example  riffle independent approximation underestimates number votes obtained
candidate    a research psychologist  ultimately election 
one possible explanation discrepancy may lie idea voters tend vote
strategically apa elections  placing stronger candidates opposing political coalitions
lower ranking  rather revealing true preferences  interesting line
future work lies detecting studying presence strategic voting election
datasets  open questions include     verifying mathematically whether strategic voting
indeed exist in  say  apa election data      so  strategic voting effect
strong enough overwhelm riffled independence structure learning algorithms 
    strategic voting manifest partial ranking votes 

    conclusion
probabilistic reasoning problems  often case certain data types suggest
certain distribution representations  example  sparse dependency structure data
often suggests markov random field  or graphical model  representation  friedman 
             low order permutation observations  depending items
time   recent work  huang et al         kondor        shown fourier domain
representation appropriate  preference ranking scenarios  one must contend
human task complexity difficulty involved human rank long list items
often leads partially  instead fully ranked data  paper  shown
data takes form partial rankings  hierarchical riffle independent models
natural representation 
conjugate priors  showed riffle independent model guaranteed
retain factorization structure conditioning partial ranking  which performed efficiently   surprisingly  work shows observations
take form partial rankings amenable simple multiplicative update based
conditioning algorithms  finally  showed possible learn hierarchical riffle
independent models partially ranked data  significantly extending applicability
previous work 

acknowledgments
project formulated largely conducted internship jonathan huang
microsoft research  additional work supported part onr muri
n              aro muri w   nf         carlos guestrin funded
   

fihuang  kapoor   guestrin

part nsf career iis         thank eric horvitz  ryen white  dan liebling 
yi mao discussions 

appendix a  proofs
appendix  provide supplementary proofs theoretical results
paper 
a   proof theorem   
prove theorem     as well later results   refer rank sets 
definition     given partial ranking type   denote rank set occupied
ri   note ri depends
written r                    
p
r                                     rr     r 
i                n  
refer following basic fact regarding rank sets 
proposition                  r i   i     ri  
proof   of theorem     use induction size itemset  cases n       
trivial since every distribution s  s  factors riffle independently  consider
general case n     
fix partial ranking                 r type binary partition item
set subsets b  show indicator function factors as 
     m ab     f  a     g b     

 a   

factors m  f g indicator functions set consistent interleavings 
 s  ab   sets consistent relative rankings   s  a  s  b   respectively 
equation a   true  shown must decompose respect
top layer h  show decomposes hierarchically  must show
relative ranking factors fa gb decompose respect ha hb  
subhierarchies item sets b  establish second step  assuming
equation a   holds   note fa gb indicator functions restricted partial
rankings   s  a  s  b   partial rankings smaller item sets
b  inductive hypothesis  and fact b assumed strictly
smaller sets   shows functions fa gb factor according
respective subhierarchies 
turn establishing equation a    suffices prove following two
statements equivalent 
i  ranking consistent partial ranking  i e     
ii  following three conditions hold 
 a  interleaving ab    consistent  i e   ab     s  ab   
 b  relative ranking    consistent  i e       s  a   
 c  relative ranking b    consistent  i e   b     s  b   
   

fiefficient inference partial rankings

 i ii   first show implies conditions  a    b   c  
 a    i 
 j ri   ab  j    a     j ri      j  a  
   k   k a  

 by definition   

 by proposition    

   i a  
argument  replacing b  shows i   j
ri   ab  j    b     i b   two conditions  by definition     show
ab consistent  
 b     by definition     ranks items items j
  j  intersecting a  see ranks item
item j i  j  definition       ranks item
item j i  j  finally definition    again 
see    consistent partial ranking  
 c   same argument  b   
 ii i   assume conditions  a    b    c  hold  show  
proposition    sufficient show item k    k  ri  
prove claim  show induction item k a   k  ri
 and similarly k b   k  ri   
base case  base case  i       assume k   a  goal show
 k  r    condition  a   ab     s  ab   definition    
means that     a     j r     ab     j    a     j r       j  a  
words       a  items lie rank set r                    
show item k maps rank r    must show relative
ranking elements a  k among first m  condition  b       s  a  
implying item subset   occupies first positions relative
ranking a  since k   a  item k among first items ranked   
therefore  k  r    similar argument shows k   b implise
 k  r   
inductive case  show k a   k  ri   condition  b  
    s  a   implying item subset  and hence  item k  occupies
first    i a  positions relative ranking beyond items i 
j    j
a   inductive hypothesis mutual exclusivity  items  together
i 
i 
j    j b  occupy ranks j   rj   therefore  k  r    i 
hand  condition  a  assures us  i a     j ri      j  a 
words  ranks ri occupied exactly items a  therefore 
 k  ri   again  similar argument shows k b implies  k  ri  

a   pspan set always partial ranking
reason pspan set rankings  first introduce basic concepts
regarding combinatorics partial rankings  collection partial rankings
   

fihuang  kapoor   guestrin

forms partially ordered set  poset      obtained    
dropping vertical lines  example  s                hasse diagram
graph node corresponds partial ranking node x connected
node via edge x exists partial ranking z x z
 see lebanon   mao         top hasse diagram partial ranking               n
 i e     bottom hasse diagram lie full rankings  see figure   
example partial ranking lattice s   
lemma      lebanon   mao        given two partial rankings        
exists unique supremum      a node ssup sup ssup sup
    ssup sup   node greater ssup sup    similarly 
exists unique infimum      
lemma     given two partial rankings         relation     holds
lies     hasse diagram 
proof  lies     hasse diagram      trivial since
obtained dropping vertical bars       given lie
      would show         let sinf inf unique infimum
    guaranteed lemma     definition hasse diagram 
obtained dropping verticals vertical bar representation
sinf inf   since lie       must vertical bar
dropped     dropped  if exist bar 
       hence must exist pair items i  j separated single vertical
bar unseparated       therefore exists      j     i 
even though exists   conclude        
lemma     lemma    main body   x sn   pspan x  partial ranking 
proof  consider subset x sn   partial ranking containing every element x
must upper bound every element x hasse diagram lemma    
lemma     must exist unique least upper bound  supremum  x  ssup sup  
common upper bound x  must ancestor ssup sup
hence ssup sup   therefore see partial ranking containing x must
superset ssup sup   hand  ssup sup partial ranking containing x 
since pspan x  intersection partial rankings containing x  pspan x   
ssup sup therefore pspan x  must partial ranking 
a   proofs claim rspan x    pspan x 
simplify notation remaining proofs  introduce following definition 
definition     ties   given partial ranking              r   say items a 
a  tied  written a  a    respect a    a  i 
following basic properties tie relation straightforward 
proposition    
   

fiefficient inference partial rankings

   
    

    

    

    

    

    

     

     

     

     

     

     

figure     hasse diagram lattice partial rankings s   
i  respect fixed partial ranking   tie relation    equivalence relation
item set  i e   reflexive  symmetric transitive  
ii  exist     disagree relative ranking items a  a   
a  a  respect  
iii        a  a  respect   a  a  respect      
iv  a  a  respect    a       a       a    item a 
  a  a  a   
proposition     given set rankings x input  algorithm   outputs pspan x  
proof  prove three things  together prove proposition      algorithm
terminates      stage elements x contained pspan x      
upon termination  pspan x  contained element x 
   first note algorithm must terminate finitely many iterations
loop since stage least one vertical bar removed partial ranking 
vertical bars removed elements x 
disagreements relative ordering 
   show stage algorithm  every element xt subset
pspan x   initialization  course  x    simply singleton
set consisting element x  therefore pspan x  
suppose pspan x  every xt   replaced
xt     want show pspan x  well  algorithm   
j               j  j            r   written            j
j            r   vertical bar j j   deleted due existence
partial ranking xt       xt disagrees relative
ordering items a    a  opposite sides bar  since    
subsets pspan x  assumption  know a  a  respect pspan x 
 proposition     ii   suppose a  a  i    x
i    x a  a  respect pspan x   iii 
proposition     moreover   i  transitivity   see x respect
pspan x   two elements i     iv  proposition    
items lying   i             i  thus tied respect pspan x  therefore
removing bar items a  a   producing  example    results
partial ranking subset pspan x  
   

fihuang  kapoor   guestrin

   finally  upon termination  ranking x contained element
xt   would exist two items a    a  whose relative ranking
disagree upon  contradiction  therefore  every element xt contains
every element x thus pspan x  every xt  

lemma     let              i  i            k partial ranking item set  
                 i i            k   partial ranking sets i  
merged  let a  ij   j a  kj i   j   element c
additionally exists ranking disagrees relative
ordering a    a        o 
proof  fix completely decomposable work h  indicator
distribution corresponding o  let       prove lemma  need establish
h        let   element    k     k  k   i i     
since supp h  assumption  h          
since   match items except i     exists sequence
rankings                       adjacent rankings sequence differ
pairwise exchange items b    b  i     show step
along sequence  h        implies h  t          prove h       
suppose h        t   differ relative ranking
items b    b  i    without loss generality  assume  b       b   
t    b      t    b     
idea following paragraph use previous lemma  lemma     prove
t   positive probability so  necessary argue
exists ranking   h             b         b     i e     disagrees
relative ranking b    b     let element   a    rearrange
a  ranked first among elements   a  i     rearrange
a  ranked last among elements i     note still element
possible rearrangements therefore h        assume  b       b   
since otherwise shown wanted show  thus relative ordering
a    a    b    b  within a   b   b   a    note treat case items a    a    b    b 
distinct  argument follows cases a    b  a    b   
since disagrees relative ordering a    a  assumption  and
hence disagrees    apply lemma    conclude swapping relative ordering
a    a  within  obtaining a   b   b   a    results ranking      h          
finally  observe   must disagree relative ranking a    b   
invoking lemma    shows swap relative ordering a    b  within
 obtaining a   a   b   b    result ranking   h           element   ranks
b  b    wanted show 
shown exist rankings disagree relative ordering b 
b  positive probability h  applying lemma    shows swap
relative ordering items b    b  within obtain t   h  t         
concludes proof 
   

fiefficient inference partial rankings

a   uniformity c functions partial ranking
thus far shown element c must supported partial ranking 
following  show  up certain class exceptions   element must
assign uniform probability members partial ranking 
theorem     h completely decomposable function supported partial ranking
             r  i    
                 r  h uniform  i e  
 
q
h    
 i     


establish theorem     must establish two supporting results      lemma   
factors h r smaller completely decomposable functions  nonzero everywhere domain      theorem    establishes uniformity completely
decomposable function nonzero everywhere domain 
lemma     completely decomposable q
function  h  supported partial ranking
             r   must factor as  h     ri   h  i     factor distribution
h  i    completely decomposable function si  
proof  since h completely decomposable   i   riffle independent
  i   i  since h supported partial ranking               r   however 
interleaving complement deterministic therefore conclude fact
 i   fully independent
  i    since  i     i   i 
qr
factorization  h     i   h  i    
turn establishing factor h  i    completely decomposable
observation  fix      without loss generality  consider partition set  
subsets b  would see sets b riffle independent
respect h        since h assumed completely decomposable  know
riffle independent complement  b       words  b   b      
variables     ab   b  the relative ranking a  interleaving
remaining items  relative ranking remaining items  respectively  mutually
independent  observe     interleaving b  ab   deterministic
function interleaving ab     relative ranking b  b   deterministic
function b   thus proving   ab b mutually independent hence
b riffle independent 
theorem     let h completely decomposable function h       sn
n      two rankings       differ single transposition 
h       h     
proof strategy theorem    involve examining ratio two
probabilities h     h      define operation transforming     new
rankings       ratio rankings preserved  i e   h     h      
h      h        performing sequence ratio preserving operations  show that 
h    
h    
 
 
h    
h    
theorem    easily follows 
   

fihuang  kapoor   guestrin

use two types operations transform ranking new ranking     
changing interleaving two sets b within ranking        changing
relative ranking set within ranking   precisely  given ranking
partitioning item set subsets b  uniquely index triplet
     b      a b           b   b     two operations
defined follows 
   changing interleaving a  b within     yields new ranking  
indexed         b   
   or      yields new
   changing relative ranking  or b  within
b
 
 
 
ranking indexed      b    or      b    

use operations obtain         interested conditions
transformation ratio preserving  i e   h     h       h      h       
following lemma provides sufficient conditions ratio preservation 
lemma     let h completely decomposable function consider       sn
h          partitioning item set subsets b  have 
       match interleaving b  i e   a b        ab       
h     
h    
 
 
h       h            formed changing interleaving sets
b within     new interleaving    
       match relative ranking  or b   i e                or
h     
h    
  h              formed changing
b        b         h 
  
 
 
relative ranking set  or b  within     new relative ranking
 
 or b   
proof  since proofs parts     nearly identical  prove part   here 
since h c  sets b riffle independent assumption  hence
factorizations 
h    
m     f   a   g  b  
 
 
h    
m     f   a   g  b  

    match interleaving sets b           
thus interleaving terms  m     m     numerator
denominator 
hand  examine ratio h      h       see
interleaving terms must cancel 
h    
m      f   a   g  b  
 
 
h    
m      f   a   g  b  

therefore that 
f   a   g pib
h     
h    
   
 
 
 

b
h    
h     
f      g    

   

fiefficient inference partial rankings

established lemma     turn establishing three short claims  using
lemma  allow us prove finally prove theorem     interesting note
require n      strictly  claim iii swap order j
numerator denominator  third item k proof thought
playing role dummy variable analogous temporary storage variables one
might use implementing swap function  necessity third item precisely
result hold special case n     
proposition     let h   sn r completely decomposable function n    
h       sn   following equivalences  where
ratios  entries explicitly written assumed match identically
numerator denominator  
i 

ii 

h i j         k         
h i j k         
 
 
h j i         k         
h j i k         
h        i         j         
h i j         
 
 
h        j         i         
h j i         

iii 

h j i k         
h i j k         
 
 
h j i k         
h i j k         

proof 
i  equality holds since     match interleaving sets    k 
b     k   thus change interleaving b    
item k inserted rank   preserving ratio 
ii  equality holds ii since     match interleaving sets    i  j 
b     i  j   thus change interleaving b  
  items j occupy first two ranks preserving ratio
h     h     
iii  following use     refer arguments numerator
denominator  respectively  preceding line 
h i j k         
h i k j         
 
 
h j i k         
h k i j         
h j i k         
 
 
h j k i         
h i j k         
 
 
h i k j         
h k j i         
 
 
h k i j         
h j i k         
 
 
h i j k         

 since       match relative ranking  j  k  
 since       match interleaving  j    j  
 since       match relative ranking  i  j  
 since       match relative ranking  i  k  
 since       match interleaving  k    k   

   

fihuang  kapoor   guestrin

proof   of theorem     want show two rankings differ single transposition 
assigned equal probability h  suppose   obtained
  swapping ranks items j  additionally  let k item besides j
 such item must exist since n       following  use proposition    show
h     h       h     h      before  entries explicitly written
assumed match identically numerator denominator 
h        i         j         
h i j         
h    
 
 
   by prop      part ii 
h    
h        j         i         
h j i         
h i j         k         
h i j k         
 
 
   by prop      part i 
h j i         k         
h j i k         
h j i k         
   by prop      part iii 
 
h i j k         
h j i         k         
 
   by prop      part i 
h i j         k         
h j i         
h        j         i         
 
 
   by prop      part ii 
h i j         
h        i         j         
h    
 
 
h    

since assumed h     h          must conclude h       h     
finally  assemble supporting results prove theorem    
proof   of theorem     lemma     completely decomposable function h must factor
as 
r

h    
h  i    
 a   
i  

factor distribution h  i    completely decomposable function si  
assumption   i          i        corresponding factor h  i    must trivially
uniform  otherwise   i        latter case  apply theorem   
h  i    show must assign equal probability two rankings differ
single transposition  however  given rankings       si   obtain sequence
transpositions transforms       therefore  theorem    fact implies
factor h  i    constant inputs  proved factor equation a  
constant  conclude h must constant support 

references
ailon  n          aggregation partial rankings  p ratings top m lists  proceedings
eighteenth annual acm siam symposium discrete algorithms  soda    
new orleans  louisiana 
bartholdi  j  j   tovey  c  a     trick  m          voting schemes
difficult tell won  social choice welfare       
   

fiefficient inference partial rankings

busse  l  m   orbanz  p     buhmann  j          cluster analysis heterogeneous rank
data    th annual international conference machine learning  corvallis 
oregon 
fligner  m  a     verducci  j  s          distance based ranking models  journal
royal statistical society     
freund  y   iyer  r   schapire  r  e     singer  y          efficient boosting algorithm
combining preferences  journal machine learning research  jmlr             
friedman  n          learning belief networks presence missing values hidden variables  proceedings fourteenth international conference machine
learning  icml     pp          san francisco  ca  usa  morgan kaufmann publishers inc 
friedman  n          bayesian structural em algorithm    th conference
uncertainty artificial intelligence  uai     madison  wisconsin 
gormley  c     murphy  b          latent space model rank data  proceedings
     conference statistical network analysis  icml    pp         berlin 
heidelberg  springer verlag 
huang  j   kapoor  a     guestrin  c          efficient probabilistic inference partial
ranking queries    th conference uncertainty artificial intelligence  uai
    barcelona  spain 
huang  j          probabilistic reasoning learning permutations  exploiting structural decompositions symmetric group  ph d  thesis  carnegie mellon university 
huang  j     guestrin  c          riffled independence ranked data  bengio  y  
schuurmans  d   lafferty  j   williams  c  k  i     culotta  a   eds    advances
neural information processing systems     nips     pp          mit press 
huang  j     guestrin  c          learning hierarchical riffle independent groupings
rankings  proceedings   th annual international conference machine
learning  icml     pp          haifa  israel 
huang  j     guestrin  c          uncovering riffled independence structure ranked
data  electronic journal statistics            
huang  j   guestrin  c     guibas  l          efficient inference distributions permutations  platt  j   koller  d   singer  y     roweis  s   eds    advances neural
information processing systems     nips     pp          mit press  cambridge 
ma 
huang  j   guestrin  c     guibas  l  j          fourier theoretic probabilistic inference
permutations  journal machine learning research  jmlr               
joachims  t          optimizing search engines using clickthrough data  proceedings
eighth acm sigkdd international conference knowledge discovery data
mining  kdd     pp          new york  ny  usa  acm 
kondor  r   howard  a     jebara  t          multi object tracking representations
symmetric group  meila  m     shen  x   eds    proceedings eleventh
   

fihuang  kapoor   guestrin

international conference artificial intelligence statistics march             
san juan  puerto rico  vol  volume   jmlr  w cp 
kondor  r          group theoretical methods machine learning  ph d  thesis  columbia
university 
lebanon  g     lafferty  j          conditional models ranking poset  s  becker 
s  t     obermayer  k   eds    advances neural information processing systems
    nips     pp          cambridge  ma  mit press 
lebanon  g     mao  y          non parametric modeling partially ranked data  platt 
j  c   koller  d   singer  y     roweis  s   eds    advances neural information
processing systems     nips     pp          cambridge  ma  mit press 
lu  t     boutilier  c          learning mallows models pairwise preferences 
  th annual international conference machine learning  icml     bellevue 
washington 
marden  j  i          analyzing modeling rank data  chapman   hall 
meila  m   phadnis  k   patterson  a     bilmes  j          consensus ranking
exponential model  tech  rep       university washington  statistics department 
white  r     drucker  s          investigating behavioral variability web search 
proceedings   th international conference world wide web  www    
banff  alberta  canada  acm 

   


