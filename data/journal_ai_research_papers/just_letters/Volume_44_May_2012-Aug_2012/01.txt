journal of artificial intelligence research                 

submitted         published       

solving limited memory influence diagrams
denis deratani maua
cassio polpo de campos
marco zaffalon

denis idsia ch
cassio idsia ch
zaffalon idsia ch

istituto dalle molle di studi sullintelligenza artificiale  idsia 
galleria    manno       switzerland

abstract
we present a new algorithm for exactly solving decision making problems represented as
influence diagrams  we do not require the usual assumptions of no forgetting and regularity 
this allows us to solve problems with simultaneous decisions and limited information  the
algorithm is empirically shown to outperform a state of the art algorithm on randomly
generated problems of up to     variables and      solutions  we show that these problems
are np hard even if the underlying graph structure of the problem has low treewidth and
the variables take on a bounded number of states  and that they admit no provably good
approximation if variables can take on an arbitrary number of states 

   introduction
influence diagrams  howard   matheson        are graphical models aimed at the representation of problems of decision making under uncertainty  traditionally  they are designed
to handle situations involving a single  non forgetful decision maker  limited memory influence diagrams  hereafter limids  are generalizations of influence diagrams that allow for
decision making with limited information  as in the case of simultaneous decisions  bounded
memory controllers and non communicating cooperative agents  zhang  qi    poole       
lauritzen   nilsson        poupart   boutilier        detwarasiti   shachter         more
precisely  limids relax the regularity and no forgetting assumptions of influence diagrams 
namely  that there is a complete temporal ordering over the decision variables  and that any
disclosed information  i e   decisions and observations made  is remembered and considered
for future decisions  these assumptions might not only be hard to meet in some applications  but they might lead to an exponential growth in the size of policies  and consequently
to intractability 
solving a  limited memory  influence diagram refers to finding an optimal plan of action 
that is  a combination of decision rules  or policies  that associate any possible observation
to an action  optimality is understood as maximizing expected utility  this task has been
empirically and theoretically shown to be very hard  de campos   ji         in fact  we
show here that solving a limid is np hard even if we admit only singly connected diagrams
with bounded number of states per variable   and that devising an algorithm that produces
provably good approximate solutions within any fixed factor is unlike to exist even for
diagrams of low treewidth 
   a diagram is singly connected if the underlying  undirected  graph contains no cycles 
     ai access foundation  all rights reserved 

fimaua  de campos    zaffalon

lauritzen and nilsson        have shown that limids that satisfy certain graphstructural conditions  which no forgetting and regularity imply  can be solved exactly by
a dynamic programming procedure with complexity exponential in the treewidth  hence 
solving such limids is computationally similar to performing probabilistic inference in
bayesian networks  koller   friedman         in fact  the single policy updating  spu 
algorithm of lauritzen and nilsson        performs a local search in the space of policies
and at each step performs a probabilistic inference to evaluate each candidate solution 
however  many problems fail to meet the conditions necessary for spu achieving optimality  and in these cases spu might converge to a local optimum that is much inferior to the
actual  global  optimum  to circumvent this problem  de campos and ji        formulated
the credal reformulation  cr  algorithm that maps a limid into a mixed integer linear
programming problem  they showed that the cr algorithm is able to solve small problems exactly and to obtain good approximations for medium sized problems by relaxing the
integrality constraints 
we show in this paper that limids can be solved exactly by a variable elimination
scheme that simultaneously propagates sets of  partial  solutions  although the algorithm
runs in exponential time in the worst case  which is just to be expected  as the problem
is np hard   we show that for many problem instances it is possible to obtain an optimal
solution efficiently by pruning solutions that are pareto dominated by others  at the heart
of the algorithms efficiency is the property that at any moment during variable elimination
local pareto dominance implies global pareto dominance  that is  that a partial solution
that is pareto dominated by another partial solution cannot be part of an optimal solution 
and hence can be safely discarded  we show experimentally that the pruning of paretodominated local solutions can enormously save computational resources  and enable us to
compute exact solutions for much bigger problems than previous algorithms  in fact  the
algorithm is orders of magnitude faster than the cr algorithm on randomly generated
diagrams containing up to     variables and      strategies 
the paper is organized as follows  section   describes the limid formalism and presents
new results about the complexity of solving a limid  the variable elimination algorithm
for computing exact solutions is presented in section    and evaluated in section    at last 
sections   and   contain related work and a final discussion  to improve readability  some
of the proofs and supporting results are given in the appendix 

   limited memory influence diagrams
in this section  we describe the limid formalism  state the complexity of solving a limid
instance  and show that any limid can be transformed into an equivalent  in terms of
maximum expected utility  diagram whose utilities are nonnegative and decision variables
have no parents  such limids are the input of our algorithm in the next section  we start
with an example of a decision problem with limited information  which we use throughout
the rest of the paper to illustrate and motivate concepts  although this example  which
is essentially a team coordination problem  is rather simple  it can easily be extended to
account for more realistic scenarios 
  

fisolving limids

    the fire dispatching problem
a particular fire station contains a group of firefighters divided in three units  the fire
dispatcher decides which units to dispatch for each reported accident  each dispatched
unit costs    utile  and units not dispatched cost no utiles  in case of fire  the higher
the number of dispatched teams the higher the chances of minimum damage  which implies
saving lives and preventing third party financial losses   to make things simple  we consider
that an accident can be handled either appropriately  in which case we say it is a success 
or inappropriately  in which case we say it is a failure  ideally  the dispatcher wants to
maximize the chance of success while minimizing the number of dispatched teams  and
hence the cost of the operation   a successful operation is rewarded with     utiles  while
a failure gets zero utiles 
    variables and domains
in the formalism of  limited memory  influence diagrams  the quantities and events of
interest are represented by three distinct types of variables or nodes   chance variables
represent events on which the decision maker has no control  such as outcomes of tests or
consequences of actions  decision variables represent the options available to a decision
maker  finally  value variables represent additive parcels of the utility associated to a state
of the world  the set of all variables considered relevant for a problem is denoted by u 
each variable x in u has an associated domain x   which is the finite non empty set of
values that x can assume  the elements of x are called states  we assume the existence
of the empty domain        which contains a single element  which is not in any
other domain  decision and chance variables are assumed to have domains different from
the empty domain  whereas value variables are always associated to the empty domain 
in the fire dispatching problem  we can represent the act of dispatching or not the
unit i by a decision variable ti   hence we have three decision variables t    t    and t 
with domains t    t    t     a  w   where a stands for act and means the unit is
dispatched  while w stands for wait and means the unit is not dispatched  the outcome
of the incident after the assignment of units is represented by a binary chance variable o
with domain o    s  f    representing success and failure  respectively   and evaluated
by a value variable v  which is associated to     there are also individual costs per unit
dispatched  which are modeled by three value variables v    v  and v    the set of relevant
variables for the problem is then u    t    v    t    v    t    v    o  v   
the domain x of a set of variables x    x            xn    u is given by the cartesian
product x       xn of the variable domains  thus  an element u  u defines a
state of the world  that is  a realization of all actions and events of interest  if x and y are
sets of variables such that y  x  u  and x is an element of the domain x   we write
xy to denote the projection of x onto the smaller domain y   that is  xy  y contains
only the components of x that are compatible with the variables in y  by convention 
x     the cylindrical extension of y  y to x is the set y x    x  x   xy   y  
often  we write x     xn to denote the set  x            xn   and  if clear from the context 
x to denote the singleton  x   for instance  if x    t    o  and y    t     then x  
   we make no distinction between a node in the graphical representation of a decision problem and its
corresponding variable 

  

fimaua  de campos    zaffalon

  a  s    w  s    a  f     w  f     also  if x    w  s   x then xy   w and xo   s  the
cylindrical extension of s  o to x is given by sx     a  s    w  s   
    operations over real valued functions
some operations over real valued functions need to be defined  let f and g be functions
over domains x and y   respectively  the product f g is defined as the function over the
domain xy such that  f g  w    f  wx  g wy   for any w of its domain  sum of functions
is defined analogously   f   g  w    f  wx     g wy    notice that product and sum of
functions are associative and commutative  and that product distributes over sum  that is 
f g   gf   f   g   gp  f   and f  g   h    f g   f h  if f is a function over x   and y  u 
the sum marginalp y f returns
w of its
p a function over x y such that for any element
p
domain we have   y f   w    xwx f  x   notice that if y  x     then y f   f   also 
the sum marginal operation
inherits
commutativity
p
p p
p pand associativity from addition of real
numbers  and hence xy f   x y y f   y x x f  
if  fxy  yy is a set containing functions fxy of domain x   one for each element of y  
y
we write fxy to denote the function that for all w  xy satisfies fxy  w    fxw  wx   
for instance  if x and y are two binary valued variables with domains x    x    x   
y
y
y
and y    y     y      and fx  and fx  are two functions over x such that fx   x          
y 
y 
y 
y is such that
fx  x           fx  x        and fx  x         then the function fx
y

y

y
fx
 x    y       fx   x           

y

y
fx
 x    y       fx   x         

y
fx
 x    y       fx   x           

y

y
fx
 x    y       fx   x         

when clear from the context  we write   to denote a function that returns one to all
values in its domain and   to denote a function that returns always zero  for x  x   the
indicator function ix returns one if x   x and zero otherwise 
if f and g are functions over a domain x and k is a real number  the expressions f  g
and f   k denote that f  x   g x  and f  x    k  respectively  for all x  x  e g   in the
y
previous example we have fx          finally  any function over a domain containing a
single element  e g   the empty domain  is identified with the real number it returns 
    definition
a limid l consists of a direct acyclic graph  dag  over the set of variables u annotated
with variable types  decision  chance and value   together with a collection of  conditional 
probability mass functions  one per chance variable  and utility functions  one per value
variable   the value nodes in the graph are assumed to have no children  the precise
meaning of the arcs varies according to the type of node to which they point  arcs entering
chance and value nodes denote stochastic and functional dependency  respectively  arcs
entering decision nodes describe information awareness at the time the decision is made 
for each variable x in u  we denote by pax the set of parents of x  that is  the set of
nodes of from which there is an arc pointing to x  similarly  we let chx denote the set of
children of x  i e   nodes to which there is an arc from x   and fax   pax   x  denote
its family  we let c  d and v be a partition of u in sets of chance  decision and value
variables  respectively  each chance variable c in c has an associated set  p
c     pac  
   

fisolving limids

v 

v 

v 

t 

t 

t 

uv   a    uv   a    uv   a     
uv   w    uv   w    uv   w     

o

pto   t   t   s  t    t    t      i a a a 
uv  s       
uv  f      

v
figure    a limid representing the fire dispatching problem 
of  conditional  probability mass functions p
c quantifying the decision makers beliefs about
states x  c conditional on a state  of its parents  if c has no parents  it has a single
probability mass function assigned   using the notation introduced in the previous section 
we equivalently represent the set of probability mass functions associated to a variable c by
pa
a function pc c   we assume any chance variable x  c to be stochastically independent of
its non descendant non parents given its parents  each value variable v  v is associated
with a real valued utility function uv over pav   which quantifies the  additive  contribution
of the states of its parents to the overall utility  thus  thepoverall utility of a state x  cd
is given by the sum of utility functions  that is  u x    v v uv  xpav   
    a limid for the fire dispatching problem
figure   depicts a limid for the fire dispatching problem  in the graph  chance  decision
and value variables are represented by ovals  rectangles and diamonds  respectively  the
value variables v    v  and v  have associated utility functions uv    uv  and uv    respectively 
representing the cost per unit dispatched  the utility of the outcome is quantified by the
function uv associated to the value variable v   the chance variable o has associated
a function pto   t   t  that quantifies the conditional probabilities p  o   o t    tt    t   
tt    t    tt    of success  o   s  or failure  o   f   given a joint decision t  t   t   t   
according to the model in the figure  dispatching the three units results in certain success 
whereas dispatching less than three units leads to a failure 
    policies and strategies
for any decision variable d  d with at least one parent  a policy d specifies an action
for each possible state configuration of its parents  that is  d   pad  d   if d has no
parents  then d is a state in d   the set of all policies d for a variable d is denoted by
d   for instance  a policy t  for the first unit in the running example is a state from t   
the space of policies for t  is given by t     a  w  
let    dd d denote the space of possible combination of policies  an element
s    d  dd   is said to be a strategy for l  given a policy d and a state   pad   let

p
d denote a probability mass function for d conditional on pad    such that pd   id     
if d has no parents  then pd   id is an unconditional probability mass function over d  
   

fimaua  de campos    zaffalon

pa

to simplify notation  we sometimes write pd d irrespective of whether d has any parent 
pa
there is a one to one correspondence between functions pd d and policies d  d such
pad
that specifying a policy d is equivalent to specifying pd and vice versa  we denote the
pa
set of all functions pd d obtained in this way by pd   so  for instance  pt     ia   iw   
a strategy s induces a joint probability mass function over the variables in c  d by
y pa y pa
ps  
pc c
pd d  
   
cc

dd

and has an associated expected utility
es  l   

x

ps

cd

x

uv  

   

v v

notice that the two sums in eq      have different semantics  the outer  leftmost  sum
denotes the sum marginal of the s
set of variables c d  whereas the inner  rightmost  denotes
the overall utility function over v v pav that results from the sum of functions uv  
in the fire dispatching problem  there are eight possible strategies consisting of a decision
to act or wait for each of the units  for example  s    t    t    t       a  w  a  is a possible
strategy  the policy t    a that dispatches unit t  induces a probability mass function
pt    ia on t    likewise  the policy t    w induces a function pt    iw   and the policy
t    a induces pt    ia   the strategy s    a  w  a  then induces the joint probability
mass function such that for x  o t   t   t 
t   t   t 
ps  x    po
 x pt   xt   pt   xt   pt   xt     

and has an expected utility of
x
es  l   
ps  uv    uv    uv    uv  
o t   t   t 

 

x

h
i
ps  x  uv   xt      uv   xt      uv   xt      uv  xo        

xo t   t   t 

the optimal strategy s    a  a  a  that dispatches all units  on the other hand  has an
expected utility of es  l        
    theoretical complexity
the treewidth of a graph measures its resemblance to a tree and is given by the number
of vertices in the largest clique of the corresponding triangulated moral graph minus one
 bodlaender         as in bayesian networks  the complexity of solving a limid is strongly
affected by its treewidth  given a limid l of treewidth   we can evaluate the expected
utility of any given strategy s in time and space at most exponential in   koller   friedman 
       hence  if  is bounded by a constant  computing es  l  takes  at most  polynomial
time in the input size 
the primary task of a limid is to find a strategy s with maximal expected utility 
that is  to find s   such that
es  l   es  l 
   

for all s 

   

fisolving limids

the value es  l  is called the maximum expected utility of l and it is denoted by meu l  
for most real problems  enumerating all the strategies is prohibitively costly  in fact 
computing the meu in bounded treewidth diagrams is np hard  de campos   ji        
and  as the following result implies  it remains np hard in even simpler limids 
theorem    given a singly connected limid with treewidth equal to two  and with variables having at most three states  deciding whether there is a strategy with expected utility
greater than a given k is np complete 
the proof  based on a reduction from the partition problem  garey   johnson        
is given in the appendix 
under the usual assumptions of complexity theory  when a problem is np hard to solve
the best available options are  i  trying to devise an algorithm that runs efficiently on
many instances but has exponential worst case complexity  or  ii  trying to develop an
approximation algorithm that for all instances provides in polynomial time a solution that
is provably within a certain range of the optimal solution  in section    we take option  i  
and present an algorithm that efficiently computes optimal solutions for many limids  but
runs in exponential time for many others  in the following we state a result that suggests
that alternative  ii  is most likely unfeasible  even if we consider only diagrams of bounded
treewidth 
given       a  approximation algorithm  for solving a limid  obtains a strategy s
such that
meu l 
 es  l   
   

if we set             for           then a  approximation algorithm finds a solution
whose induced relative error is at most   that is 
meu l   es  l 
  
meu l 

   

the following result indicates that provably good approximation algorithms do not exist
unless p np 
theorem    given a singly connected limid l with bounded treewidth   unless p np 
there is no polynomial time  approximation algorithm  for any            where  is the
number of numerical parameters  i e   probabilities and utilities  required to specify l 
we defer the proof to the appendix  the result asserts that any algorithm that finds
solutions to limids in polynomial time cannot guarantee a relative error smaller than
       even if the set of inputs is restricted to limids of bounded treewidth  hence  any
polynomial time algorithm for limids must eventually produce very poor solutions  with
relative error close to one for large models  an exception is when both treewidth and the
number of states per variable are bounded  in such cases  we have shown constructively
in an early work  maua  de campos    zaffalon        that there is a  approximation
algorithm that runs in polynomial time 
   

fimaua  de campos    zaffalon

    constraining limids to nonnegative utilities
in principle  the utilities associated to value variables in a limid can take on any real
value  this complicates the ordering between functions that we use in the algorithm we
devise here  fortunately  we can easily and efficiently transform any limid l into an
equivalent limid l  where all utilities are nonnegative and whose optimal strategies s are
also optimal strategies in l  moreover  obtaining es  l  from es  l    for any strategy s is
straightforward 
let l be a limid and let k denote the smallest utility value associated to any of the
value variables  that is  for all v  v it follows that k  uv   and there is v such that
uv  x    k for some x  pav   the following transformation generates a new limid l 
whose value variables are associated only to nonnegative values 
transformation    for each value variable v  v  substitute its associated utility function
uv with a new utility function u v   uv  k 
the transformation shifts the utility functions so that uv     and makes uv  x      for
at least one v and x  pav   since it affects only value variables  any strategy for l  the
limid before the transformation  is also a valid strategy for l   the transformed limid  
the expected utilities of a strategy s in l and l  are related according to the following
result 
proposition    for any strategy s  es  l    es  l      k v  
proof  the expected utility of s with respect to l  is given by
es  l     

x

 

x
x

v

 

x

x

ps  x 

x

x

u v  xpav  

v

x
ps  x 
 uv  xpav    k 
ps  x 

x

uv  xpav    k v 

x

ps  x 

x

v

  es  l   k v   
where the last step follows from

p

x ps  x 

    

an optimal strategy s for l satisfies es  l   es  l  for all s  and hence proposition  
ensures that es  l    es  l    k v   es  l    k v    es  l   which implies that s is also an
optimal strategy for l    similarly  if s is an optimal strategy for l    we have by the same
proposition that es  l      es  l   k v   es  l   k v    es  l    for all s  and therefore s
is also optimal for l  the following corollary summarizes these results 
corollary    a strategy s for l  is an optimal strategy if and only if it is also an optimal
strategy for l 
   

fisolving limids

consider the running example once more  the smallest utility value is k      the
utilities associated to the value variables of the transformed limid l  are given by
u v  s       

u v  f      

u v   a     

u v   w     

u v   a     

u v   w     

u v   a     

u v   w       

the strategy s    a  w  a  has expected utility es  l      es  l   k v                
the optimal strategy s    a  a  a  obtains es  l          
in the rest of the paper  we consider only limids with nonnegative utilities  which due
to proposition   does not incur any loss of generality 
    decision nodes with many parents versus parentless decision nodes
a policy for a decision variable with no parents corresponds to a choice of one of its states 
hence  the space of policies of such nodes contains a number of policies that is polynomial in
the input  on the other hand  the cardinality of the space of policies for decision nodes with
many parents is exponential in the number of states of the parents  to see this  consider a
ten state decision variable d  if d has no parents then the space of policies d contains   
 
policies  however  if d has four ternary parent nodes  the space d contains           
policies 
one might then wonder whether limids whose decision nodes have many parents are
more difficult to solve than limids with parentless decision nodes  we will show that 
at least from a theoretical perspective  this is not the case  and that any limid can be
efficiently mapped into a meu equivalent limid where decision nodes have no parents 
we then show how an optimal strategy for the original diagram can be produced from an
optimal strategy for the transformed diagram  this is particularly relevant for algorithms
that search the space of policies  as is the case of the algorithm we devise here  and it allow
us  without loss of generality  to focus on limids whose decision nodes have no parents 
before formally describing the transformation and showing that it produces a diagram
with equal meu  let us first give an idea of how and why it works  to this end  consider a
limid l and a decision node d with at least one parent  e g   the diagram in figure   a   
and let              m denote the configurations in pad   a policy d maps a configuration
pa
 i to a decision d  d   a function pd d associated to a policy d can be seen as a
i
set of probability mass functions pt            ptm where pti   p
d   id   i     that is  each
function pti represents a choice of a state of d for a fixed configuration  i of the parents 
recall that a policy associated to a parentless variable is simply a choice of a state  the
transformation replaces the decision variable d with m decision variables t            tm and m
chance variables x            xm such that each policy ti corresponds to a decision d  i   of the
original variables policy  see diagram in figure   b    the chain x       xm of chance
variables is responsible for making only the policy ti active when the parents assume
configuration  i   as it occurs with d   by either blocking or allowing information to
flow according to the value of the parents of d  thus  the parents of d act as a selector that
   

fimaua  de campos    zaffalon

pad

pad
d

x 

x 

chd

t 

t 



 a 

xm

chd

tm
 b 

figure    a piece of a diagram before  a  and after  b  transformation   

decides which of the probability mass functions pti associated to decision nodes t            tm is
going to be used  so that the transformed diagram acts as the original one  the probability
x
 ti  pad
mass functions of pxi 
are set to ensure that xi   ti if pad    i and xi   xi 
i
otherwise 
transformation    consider a limid l and a decision node d with at least one parent 
and let              m denote the configurations in pad   remove d and add m    pad  
chance nodes x            xm and m decision nodes t            tm with domains xi   ti   d
 for i              m   add an arc from every parent of d to each of x            xm   an arc from
every xi to xi     with i   m  and an arc from every ti to xi   i              m  add an arc
from xm to each child of d  associate to x  the function


if xpad      and xx    xt 
  
t  pa
px   d  x      
if xpad      and xx     xt 


  m if xpad        
for each node xi   i              m  associate a function


   if  xpad     i



   if  xpad    
x
 ti  pad
i
pxi 
 x 
 
i
pa
d

   if  x
  i



   if  xpad   
i

and
and
and
and

xxi
xxi
xxi
xxi

  xxi   
   xxi   
  xti  
   xti    

pa

finally  the functions px x for each child x of d have d substituted by xm in their domain 
without altering the numerical values 
figure   depicts a decision node with many parents  on the left  and the new sub diagram
generated by transformation    on the right   it is not difficult to see that the treewidth
of the transformed diagram is increased by at most three  because the subgraph containing
the new nodes  the parents of d and the children of d is triangulated and contains cliques
with at most  pad   xi   xi    di    variables   also  the transformation for two different
decision variables affect different parts  and hence transforming a diagram in a diagram
with parentless decisions does not increase the treewidth by more than three  the following
result states that also optimality of strategies is preserved by the transformation 
   since the treewidth is given by the size of largest clique in the triangulated moral graph minus one   pad  
is a lower bound on the treewidth of the original graph 

   

fisolving limids

proposition    let l  be the result of applying transformation   on a decision variable
d in a limid l  s  denote a strategy for l    and t            tm denote the corresponding
policies for t            tm in s    let also d be a policy for d such that d   i     ti for all
 i  pad   finally  let s be a strategy for l obtained by substituting t            tm in s  with
d  and keeping the remaining policies   then s is an optimal strategy for l if and only if
s  is an optimal strategy for l   
the proof is in the appendix  for each decision variable d in the original limid  the
transformed model contains m chance variables specifying m d    values  and m decision
nodes with  d   states  if the treewidth of the original diagram is bounded  then m is
bounded and the transformation takes polynomial time   in the example of a ten state
decision variable with four ternary parents  the transformation replaces the decision variable
d with         decision variables whose space of policies contain    elements each  besides
the    chance variables  the combined space of policies  that is  t       t   contains
also      elements  so that the total search space is still  doubly  exponential in the input 
however  algorithms can take advantage of the smaller local policy spaces to reach better
solutions  and this is particularly true for the algorithm we devise later on 
in the rest of the paper we assume without loss of generality that decision nodes have
no parents and utilities are nonnegative 

   solving limids
in this section  we describe a new algorithm for solving limids exactly by propagating
multiple non dominated solutions  we start by defining the basic algebraic structure of
our algorithm  which is given by the framework of valuation algebra  we show that this
framework alone  similar to the one used by spu  might lead to poor accuracy  we thus
extend the framework with sets of valuations that attempt to improve accuracy by increasing
complexity  efficiency is obtained by pruning sets so that their cardinality is kept as small
as possible without affecting accuracy 
    valuation algebra
the basic ingredients of our algorithmic framework for representing and handling information in limids are the so called valuations  which encode information  probabilities  utilities
and policies  about the elements of a domain  each valuation is associated to a subset of the
variables in u  called its scope  more concretely  a valuation  with scope x is a pair  p  u 
of nonnegative real valued functions p and u over the domain x   we refer to p and u as
the probability and utility part  respectively  of   often  we write x to make explicit the
scope x of a valuation   for any x  u  we denoted the set of all possible
s valuations with
scope x by x   the set of all possible valuations is thus given by    xu x   the set
 is closed under two basic operations of combination and marginalization  combination
represents the aggregation of information and is defined as follows 
definition    if     p  u  and     q  v  are valuations with scopes x and y  respectively 
its combination    is the valuation  pq  pv   qu  with scope x  y 
   if the treewidth is not bounded then the output of any algorithm  that is  an optimal strategy  might
take space exponential in the input 

   

fimaua  de campos    zaffalon

marginalization  on the other hand  acts by coarsening information 
definition    if     p  u  is a valuationp
with scope
p x  and y is a set of variables such that
y  x  the marginal y is the valuation   x y p  x y u  with scope y  in this case  we say
that z   x   y has been eliminated from   which we denote by z  
notice that our definitions of combination and marginalization slightly differ from previous works on influence diagrams  e g   lauritzen   nilsson         which usually require
a division of the utility part by the probability part  the removal of the division operation
turns out to be an important feature when we discuss maximality of valuations later on 
but otherwise our definition is equivalent to valuations with division  in the sense that one
could easily reformulate message passing algorithms like spu using our definition 
in terms of computational complexity  combining two valuations  and  with scopes
x and y  respectively  requires   xy   multiplications and  xy   additions of numbers 
computing y   where y  x  costs  xy   operations of addition  in other words  the cost
of combining or marginalizing a valuation is exponential in the cardinality of its scope  and
linear in the cardinality of its domain   hence  we wish to work with valuations whose
scope is as small as possible  the following result shows that our framework respects the
necessary conditions for computing efficiently with valuations  in the sense of keeping the
scope of valuations obtained from combinations and marginalizations of other valuations
minimal  
proposition     the system    u      satisfies the following three axioms of a  weak 
labeled valuation algebra  shenoy   shafer        kohlas        
 a   combination is commutative and associative  that is  for any             we have
that
             
                         
 a   marginalization is transitive  that is  for z  z and y  x  z we have that
y
 x
  y
z  
z  

 a   marginalization distributes over combination  that is  for x  x   y  y and
x  z  x  y we have that
 x  y  z   x  yyz  
proof   a   follows directly from commutativity  associativity and distributivity of product
and sum of real valued functions  and  a   follows directly from commutativity of the summarginal operation  to show  a    consider any two valuations  p  u  and  q  v  with scopes
x and y  respectively  and a set z such that x  z  x  y  by definition of  and   we
have that


x
x
  p  u    q  v  z   
pq 
 pv   qu   
xy z

   

xy z

fisolving limids

since x  y   z   y   z  and p and u are functions over x   it follows that

 

x
x
x
x
x

q
v u
q  p
 pv   qu    p
pq 
xy z

y z

xy z

y z

y z



x x
v  
   p  u   
q 
y z

y z

which equals  p  y    q  v yz  
the following result by kohlas        section      is a direct consequence of  a   that
we shall use to prove the correctness of our algorithm 
lemma     if x  x   y  y   z  y and z  x     then  x  y  z   x  z
y  
the primary goal of a valuation algebra is the computation of marginal valuations of
the form            m     let  x            xn   be the set of variables appearing in the
scopes of             m   the marginal  can be computed efficiently by a variable elimination
procedure  that receives a set                 m   and a permutation  of the variables
   
 k  
x            xn   and for i              n replaces all valuations i           i i whose scope contains


   
 k    xi  
variable  xi   with the marginal i   i      i i
  algorithm   describes the
procedure  the algorithm returns a valuation j      k   where j           k  n   which
equals  by axioms  a   a    kohlas        section      
algorithm   variableelimination x   
input  a permutation  of the variables x    x            xn   and a set of valuations   
             m   over subsets of x
output  the marginal valuation         m  
   let    
   for i    to n do
   
 k 
  
let i           i denote the valuations in i  whose scope contains  xi  


   
 k   xi  
  
compute i   i      i
   

 k 

let i  i    i      i           i  
   end for
   return the combination of all valuations in n

  

the complexity of the variable elimination procedure is given by the size of the largest
valuation i generated in the loop  this valuation might have a size exponential in the
size of valuations             m given as input  but  as we discuss later on  there are certain
conditions under which the size of i is bounded and the procedure takes time polynomial
in the input 
   variable elimination algorithms are also known in the literature as fusion algorithms  shenoy   shafer 
      and bucket elimination  dechter        

   

fimaua  de campos    zaffalon

    computing expected utilities
we can use the valuation algebra framework introduced to compute the expected utility of a
given strategy using variable elimination  let s    d  dd   be a strategy for a limid
l whose expected utility we want to compute  and  be a permutation of the variables in
c  d  we assume that the decision nodes in l have no parents  otherwise we need first
to apply transformation     so that the strategy s is simply a configuration in d   the
procedure in algorithm   computes the expected utility induced by the strategy s  the
pa
procedure calls variable elimination with a set  that contains a valuation c    pc c     
for each chance variable  a valuation v       uv   for each value variable  and a valuation
d    id      for each decision variable  we have the following result 
algorithm   expectedutility l    s 
input  a limid l whose decision nodes have no parents  a permutation  of the variables
in c  d  and a strategy s    d  dd  
output  the expected utility of s
   let   
   for c  c do
pa
  
add c    pc c      to 
   end for
   for v  v do
  
add v       uv   to 
   end for
   for d  d do
  
add d    id      to 
    end for
    let s  variableelimination c  d     
    return the utility part of s

proposition     the procedure described in algorithm     returns the expected utility of
the strategy s 
proof  let s be the output of the variable elimination algorithm  according to axioms
 a   a    we have that s       where
 
   
   
 
o pa
o
o

 
pc c     
 id      
    uv    
cc

dd

v v

let p and u denote the probability and
of s   by definition of
p utility part  respectively 
q
pa
combination  we have that     ps   ps v v uv    where ps   pxcd px x as in      since
ps isp
a probability
p distribution over c  d  it follows that p   xcd ps  x       finally 
u   cd ps v v uv   which equals es  l  by     
consider the limid of the fire dispatching problem  figure    and the strategy s  
 a  w  a  whose expected utility we want to compute using the procedure above  we assume
   

fisolving limids

that the utilities are nonnegative  i e   we have already applied transformation     according to the procedure in algorithm    we first generate the set     o    pto   t   t        v   
    u v     v        u v     v        u v     v       u v    t     ia       t     iw       t   
 ia        for x    o  x    t    x    t    x    t    let  be a permutation of the variables
such that  xi     xi for i                 the variable elimination algorithm with  and  as
input produces the valuations
     v  o  o

     v   t      t 

     v   t      t 

     v   t      t 

during its loop  and outputs the valuation s               similarly  to compute the
expected utility of the optimal strategy s    a  a  a  we run variable elimination with
    o    pto   t   t        v        u v     v        u v     v        u v     v       u v    t   
 ia       t     ia       t     ia        which then outputs s            
in general  the described procedure can take time exponential in the input  however 
when l has bounded treewidth it can be shown that there exists a permutation  for
which the procedure takes time polynomial in the input   e g   koller   friedman       
section          hence  if the space of strategies is sufficiently small  we can find an optimal
strategy by simply ranking strategies according to their expected utilities  however  we do
not expect this to be feasible for any realistic diagram as the space of strategies increases
exponentially with the number of decision nodes  assuming they have no parents   even in
diagrams of bounded treewidth and bounded number of states per variable 
    local search algorithms
as a first attempt to design a fast algorithm to solve limids  one might suggest a local
search scheme that starts with a random solution and repeatedly explores its neighborhood
in order to find a solution with higher expected utility  if the treewidth of the diagram is
bounded  the expected utility of each neighbor solution can be efficiently computed  so the
complexity of the algorithm is given by the size of the neighborhood  a possible approach
is then to define the neighborhood of a solution to be the strategies obtained by changing
a single policy  which gives a local search space polynomial in the input  algorithm  
describes a greedy procedure that at each step looks for a new policy that improves on
the current best solution  the algorithm is guaranteed to find a strategy which is locally
optimal in its neighborhood  that is  it cannot be improved by changing only one of its
policies  lauritzen and nilsson        stated sufficient conditions that a diagram has to
satisfy in order to guarantee that the solution produced by a local search procedure is
 globally  optimal  unfortunately  as the following example shows  these conditions are
violated by even structurally very simple chain diagrams  and in such cases a local search
procedure might output local optima of very poor accuracy 
consider the limid of our running example  and suppose we start with strategy s   
 a  w  a   which has expected utility    at the first step we might try to improve the policy
for t    producing strategy s    w  w  a  whose expected utility is    since this is higher
than the expected utility of the initial solution  we set sbest  s and update the highest
expected utility found  next  we try to search for a better policy for t    and we generate
strategy s    w  a  a   this strategy has an expected utility of    which is less than the
   

fimaua  de campos    zaffalon

algorithm   greedypolicysearch l    s   
input  a limid l  a permutation  of the variables in c  d  and an initial strategy
s     d  dd
output  a locally optimum strategy sbest
   let sbest  s  and esbest  l   expectedutility l    s   
   repeat
  
generate a new candidate strategy s by replacing a single policy d in sbest
  
compute es  l   expectedutility l    s 
  
if es  l    esbest  l  then
  
set sbest  s and esbest  l   es  l 
  
end if
   until current solution cannot be further improved in this way
   return sbest

x 

d 

d 

x 

x 

dn



xn

r

figure    a chain structure diagram with n decision variables 
expected utility of the best solution found so far  finally  we look for a better policy for t   
which leads us to strategy s    w  w  w   whose expected utility is    since this is better
than our current best solution we set sbest  s and update the associated expected utility 
since no change of a single policy can improve this strategy  the algorithm halts with a
solution whose expected utility is   against a maximum expected utility of     achieved by
strategy s    a  a  a   more than     relative error   or  in terms of the original diagram
 by means of proposition     an expected utility of zero against a maximum expected utility
of     
the procedure we have just described is very similar to the spu algorithm  and it
illustrates the pitfalls of a local search  in fact  spu will output just the same local optimum
 but it would start with a uniform policy for every decision variable   note that the solution
obtained by the greedy local search on the example degrades as the ratio of the utility of
success  achieved only by strategy  a  a  a   and the utility of failure increases  for instance 
if the utility of success were increased to u v  s       and the utility of failure remained the
same  that is  if u v  f        the algorithm would reach a solution whose expected utility is
   an error of     relative to the maximum expected utility of     moreover  cases where
spu performs poorly are not rare  for instance  the plots in figure   show spus relative
performance in chain diagrams like the one in figure    each diagram was generated by
independently sampling each conditional distribution associated to a chance node from a
symmetric dirichlet distribution with parameter   m  where m is the number of variable
states  the maximum expected utility of each diagram was computed using the algorithm
we devise here  which took less than   seconds on any diagram in the experiment 
each  blue  point in the plots in figure   depict the relative error of spu on a given
diagram  the  red  line indicates the third quartile of each fixed configuration  the di   

fisolving limids

   

   
   
   
   
   
   
   
 

relative error

   
   
   
   
   
 
  
  
  
  
  
number of decision nodes

  
  
  
  
  
number of states per variable

figure    relative performance of spu on randomly generated chain diagrams  each  blue 
circle depicts an experiment and the  red  line depicts the third quartile 
agrams in the left hand side plot were obtained with the number of states fixed at    
while the diagrams on the right had the number of decision variables fixed in ten  for
example  we see from the third quartile line on the right hand side plot that in     of the
chain diagrams of    states and ten decision variables  spu returned a strategy s such that
 meu l   es  l    meu l        also  there were cases where spu obtains up to    
relative error  on the other hand  we see that in the majority of the cases the solution
returned by spu achieved a relative error of less than      all in all  these experiments
show that local search is effective in many cases  but may produce very poor results 
    ordered valuations
can we also exploit the redundancy in the computation of expected utility of neighboring
strategies to decide whether a candidate solution improves the current solution without
having to run variable elimination completely  for instance  when evaluating the quality
of a new candidate strategy that differs from the current best strategy only by the policy
associated to t    can we have any insight by inspecting two valuations   produced by
variable elimination in the example in section     using two different strategies  fortunately 
the answer is yes  and to show this we need the concept of ordered valuations 
let us define a partial order  i e   a reflexive  antisymmetric and transitive relation  over
  the set of all possible valuations  as follows 
definition     for any two valuations     p  u  and     q  v  in   we say that 
dominates   conversely  we say that  is dominated by    and we write     if  and
 have equal scope  p  q  and u  v 
if  and  have scope x  deciding whether  dominates  costs at most   x   operations
of comparison of numbers  the following result shows that the algebra of valuations is
monotonic with respect to dominance 
proposition     the system    u        satisfies the following two additional axioms
of an ordered valuation algebra  haenni        
   

fimaua  de campos    zaffalon

 a   combination is monotonic with respect to dominance  that is 
if x  x and y  y then  x  y     x  y    
 a   marginalization is monotonic with respect to dominance  that is 
y
if x  x then y
x  x  

proof   a    consider two valuations  px   ux   and  qx   vx   with scope x such that  px   ux   
 qx   vx    and two valuations  py   uy   and  qy   vy   with scope y satisfying  py   uy     qy   vy   
by definition of   we have that px  qx   ux  vx   py  qy and uy  vy   since all
functions are nonnegative  it follows that px py  qx qy   px uy  qx vy and py ux  qy vx  
hence   px   ux     py   uy      px py   px uy   py ux     qx qy   qx vy   qy vx      qx   vx     qy   vy   
 a    let y be a subset of x  it follows from monotonicity of  with respect to addition of
real numbers that

 

x
x
x
x
 px   ux  y   
px  
ux   
qx  
vx     qx   vx  y  
x y

x y

x y

x y

hence  the result follows 
axioms  a   and  a   assert that combination and marginalization preserve the partial
ordering between valuations  this allow us to detect suboptimal strategies early in the
variable elimination procedure  consider comparing the strategies s    w  w  w  and s   
 w  a  w  for the limid of our running example  at the third iteration of the loop  i e  
when i       the variable elimination procedure produces valuations s     ps    us    and
 
 
 
s     ps    us    for the strategies s and s    respectively  such that
ps   a       

ps   w       

us   a       

us   w       

 

ps   w       

 

 

us   w       

ps   a       

 

us   a       
 

 

 

thus  s   s    since s     st   v   s   t  and s     st   v   s   t    we know
 
by axioms  a   and  a   that s   s    and hence that es   l   es  l   therefore  there is
no need to continue the execution of variable elimination for s    as its expected value cannot
be higher than that of s 
unfortunately  suboptimal solutions do not always produce valuations that are dominated by an optimal one during variable elimination  as an example  consider strategies
s    a  w  a  and s    a  a  a   at the third step  variable elimination generates valuations



s     ps    us    and s     ps    us    such that
ps   a       

ps   w       

us   a       

us   w       




ps   w       



us   w       

ps   a       



us   a         
   

fisolving limids



thus  even though s is an optimal strategy  s    s   
the algorithm we devise later on exploits that fact that some suboptimal solutions can
be early detected and eliminated from the search space  because some suboptimal solutions
might not be eliminated during variable elimination  the algorithm runs in exponential time
in the worst case  but this is just to be expected  as the problem is np hard  fortunately 
our experiments with random problems suggest that situations like this are not frequent 
    sets of valuations
the multiple runs of variable elimination for different inputs but same elimination ordering  i e   a permutation of the variables  are represented by sets in the framework of the
algorithm we devise later on  for instance  we might consider the set   of all valuations
  produced by variable elimination at the third iteration of the loop for every possible
strategy  due to the monotonicity of combination and marginalization with respect to  
we can immediately halt the computation of valuations from   that are dominated by
some other  that is  we can remove dominated valuations from     this is formalized by
the concept of maximal valuations and the operator max 
definition     given a finite set of valuations     we say that    is maximal if for
all    such that    it holds that     the operator max returns the set max  
of maximal valuations of  
if x is a set with m valuations of scope x  the set of maximal valuations max x   can
be obtained by m  comparisons     where       x  x  
when all valuations in the set x have the same scope x  we say that x also have scope
x  we can extend combination and marginalization to sets of valuations as follows 
definition     if x and y are any two sets of valuations in  
x  y    x  y   x  x   y  y  
denotes the set obtained from all combinations of a valuation in x and a valuation in y  
definition     if x  x is a set of valuations with scope x and y  x 
y
y
x    x   x  x  

denote the set of valuations obtained by element wise marginalization of valuations to y 
it can be checked that sets of valuations with combination and marginalization defined
element wise satisfy axioms  a   a    and therefore form a valuation algebra  hence 
lemma    applies also for sets of valuations with marginalization and combination defined
as above 
lemma     if x  x and y  y are two sets of valuations with scope x and y 
respectively  and z is a set of variables such that z  y and z  x     then  x  y  z  
x  yz  
proof  the result follows from element wise application of lemma    to  x  y  z 
 x  y  z  
   

fimaua  de campos    zaffalon

    solving limids exactly
we are now ready to describe the multiplepolicyupdating  mpu  algorithm  which
solves arbitrary limids exactly  the algorithm assumes that the decision nodes have no
variables  and that utilities are nonnegative  hence transformations   and   have to be
applied before running the algorithm in case some of these assumptions fail 
consider a limid l and a permutation  of the variables in c  d  and let n    c  d  
pa
the algorithm is initialized by generating a set s  that contains a singleton   pc c      
for each chance variable c  a singleton      uv    for each value variable v and a set of
valuations   pd        which contains one element  pd      per policy d   for each decision
variable d  then a set valued variable elimination is performed for the sets of valuations
in s    with dominated valuations being discarded after each set marginalization  finally 
an optimal solution is obtained from the utility part of the single maximal valuation in the
set combination of all sets of valuations in sn obtained after the variable elimination  the
procedure is detailed in algorithm   
algorithm   multiplepolicyupdating l   
input  a limid l and a permutation  of the variables in c  d
output  the maximum expected utility
   let s   
   for c  c do
pa
  
add a singleton   pc c       to s 
   end for
   for v  v do
  
add a singleton      uv    to s 
   end for
   for d  d do
  
add a set   id        d  d   to s 
    end for
    for i    to n do
   
 k 
   
let i           i denote
whose
h the sets in si 
 scope contains  xi  
i
   
 k   xi  
   
compute i   max i      i
   

 k 

let si  si    i      i           i  
    end for
    let s denote the set combination of all sets in sn
    return the utility part u of  p  u   max s 

   

since all variables have been eliminated at the end of the loop  the valuations in sets
in s have empty scope and have both their probability and utility parts identified with
numbers  hence  the algorithm outputs an expected utility  i e   a real number  at line    
let us illustrate the algorithm with an example  once more  consider the limid l of the
fire dispatching problem after applying transformation   and assume the same elimination
ordering of the variables used in the example in section      we start with an empty set
   

fisolving limids

s    o is the only chance variable  and we add a set
o     pto   t   t       
to s    then we add the sets
v         u v      

v         u v      

v         u v      

v        u v   

to s  due to the value variables v    v    v  and v   respectively  the decision variable t 
causes a set
t      ia        iw      
with scope t  to be included in s    and similarly  for variables t  and t    that is  we add
the sets
t      ia        iw        

t      ia        iw        

with scopes t  and t    respectively  to s    obtaining s     o   v    v    v    v   t    t    t    
in the first iteration  i      of the variable elimination loop  lines        we have that


    max  v  o  o     p    u      
where p  and u  are functions over t   t   t  such that p      and u   a  a  a         and
u   x      for all x     a  a  a   note that   is a singleton since only singletons were
involved is its computation  in the second iteration we have that


w
    max  v   t      t      pa    ua      pw
    u      
w
a
w
a
where pa    ua    pw
    u  are functions over t   t  such that p    p       u   a  a        
a
w
u   x      for all x     a  a   and u       note that we have labeled the functions
according to the policies t  that generated them  this allows us to easily extract the
optimal strategy at the end of the algorithm 
in the third iteration we need to compute


    max  v   t      t 
 a a 

  max   p 
 a a 

    p 
 a a 

 a a 

 a w 

 a a 

  u 

 a a 

 a w 

    p 

 w w 

 a w 

  u 

 w w 

  u 

    p 

  u 

 a w 

 w w 

 w w 

 w w 

    p 

 w w 

  u 

   

    
 a a 

where p    u    p 
  u 
  p 
  u 
are functions over t  such that p 
 
 a w 
 w w 
 a a 
 a a 
 a w 
 w w 
p 
  p 
     u   a         u   w       u 
     and u 
     note that
the valuation associated to the policies t    w and t    a do not appear in   because
 a w   a w 
they generate a valuation equal to  p 
  u 
   this implies that strategies  a  w  w 
and  w  a  w  have the same expected utility  and also strategies  a  w  a  and  w  a  a  
   

fimaua  de campos    zaffalon

in the last iteration  we generate the set


    max  v   t      t 
 a a a 

 a a a 

  max   p 

  u 

 a a a 

 a a a 

    p 

  u 

 a a a 

 a a a 

 w w a 

    p 

 w w a 

  u 

 a a w 

    p 

 a a w 

  u 

 w w w 

    p 

 w w w 

  u 

   

    
 w w a 

 w w a 

 a a w 

 a a w 

 w w w 

 w w w 

where p 
  u 
  p 
  u 
  p 
  u 
  p 
  u 
are functions over
 a a a 
 w w a 
 a a w 
 w w w 
 a a a 
the empty set such that p 
  p 
  p 
  p 
     u 
      
 w w a 
 a a w 
 w w w 
u 
     u 
     and u 
    
 a a a 
finally  we have that s          so the algorithm returns u 
       which is the
expected utility of the optimal strategy  a  a  a   as one can see  the optimal strategy is
easily recovered by labeling valuations with their corresponding policies 
differently from other message passing algorithms that obtain approximate solutions
to limids by  repeatedly  propagating a single valuation  e g   the spu algorithm   the
mpu algorithm computes exact solutions by propagating several maximal valuations that
correspond to partial combinations of local decision rules  the efficiency of the algorithm in
handling the propagation of many valuations derives from the early removal of valuations
performed by the max operation in the propagation step 
consider the set l    s   s     where each s is given by
 

 
o

s  

pa

pc c    



 


cc

 
o

dd

 id      

  

 
o

    uv  

v v

such that the functions id are consistent with the policies in s  it is not difficult to see that
 

 
o

l  

 
pa
pc c    

cc



 
o

  id        d  d   

  

 
o

     uv   

v v

dd




 

 

o

x 

 

x s 

hence  by proposition    we have that each s in l is a valuation with probability part
one and utility part equal to the expected utility of some strategy s in   since the relation
 induces a strict  linear  order over l   the meu of the diagram equals the utility part of
the  single  valuation in max l    the
n variable elimination procedure in the propagation
step is responsible
nfor obtaining max  sn     max l   more efficiently by distributing
max and  over x s  x   which allows for a significant reduction in the cardinalities of
sets and scopes of valuations produced 
we now formally prove the correctness of the algorithm  we start by showing that max
distributes over marginalization and combination 
lemma      distributivity of maximality   if x  x and y  y are two finite sets of
ordered valuations and z  x  the following holds 
   

fisolving limids

 i  max x  max y      max x  y   
 ii  max max x  z     max z
x   
proof  part  i  has been shown by fargier  rollon  and wilson        lemma   iv    we
use a similar proof to show that part  ii  also holds  first  we show that max z
x   
z
z
max max x      assume  to show a contradiction  that there is an element x  max z
x   
where x  x   which is not an element of max max x  z    by definition of max x    there
z
z
z
is x  max x   such that x  x   hence   a   implies z
x  x   and because x  x
z
z
z
z
it follows that z
  max max x  z  
x   x   and therefore x  max x     since x 
there is z  max max x  z   such that z
x  z   but this contradicts our initial assumpz
tion since z  x  
let us now show that max xz    max max x  z    assume by contradiction that
z
z
there is z  max max x  z     max z
x    since z  x   there is z  max x   such
z
that z  z   but we have shown that max x    max max x  z    hence z   z and
z  max z
x    a contradiction 
at any iteration i of the propagation step  the combination of all sets in the current pool
of sets si produces the set of maximal valuations of the initial factorization marginalized
to xi             xn  
lemma     for i                 n   it follows that

 x       xi   


o
o


max 

  
   max 
s 

si

where for each i  si is the collection of sets of valuations generated at the i th iteration of
the propagation step of mpu 
proof  by induction on i  the basis  i      follows trivially 
assume the result holds at i  that is 



 x       xi   
o
o


max 

  
   max 
si

s 

by eliminating xi   from both sides and then applying the max operation we get to

 x       xi   xi   

xi   
o

 o 




max max 



   max max 
 




s 

si

   

fimaua  de campos    zaffalon

applying lemma    ii  to both sides and  a   to the left hand side yields


 x       xi     
xi   
 o 

 o 

max 


   max 

s 

si



  max 

xi   
o

  




o



bi  

si  bi  



  max 



xi   
 o

  max 




o

bi  

si  bi  


  max 


o



  i 

si  bi  


  max 


o

  

si  

where the passage from the first to the second identity follows from element wise application
of  a   and lemma     the third follows from the second by lemma    i   and the last two
follow from the definitions of i and si     respectively 
we are now able to show the correctness of the algorithm in solving limids exactly 
theorem     given a limid l  mpu outputs meu l  

n
proof  the algorithm returns the utility part of a valuation  p  u  in max
   which 
s
n
n
 
by lemma    for i   n  equals max

  by definition of s    any valuation 
s 

n
in
s   satisfies
 
 

 
o
cc


pa
pc c    

 


 
o

dd

 id      

 

 
o

    uv    

v v

for some combination of decisions  d 
n  d   which corresponds to a strategy in   and
there is exactly one valuation  
s   for each strategy in   hence  by propo
n
sition     the set
contains a pair     es  l   for every strategy s inducing
s  
a distinct expected utility  moreover  since functions with empty scope correspond to

n
numbers  the relation  specifies a total ordering over the valuations in
 
s  
 be a strategy associated to  p  u   since
which implies a single maximal
element 
let
s
n
 
 p  u   max

  it follows from maximality that es  l   es  l  for all s  and
s 
hence u   meu l  
   

fisolving limids

    complexity analysis
as with any variable elimination  the complexity of the algorithm depends on the permutation  given as input  the time complexity of the algorithm is given by the cost of creating
the sets of valuations in the initialization step plus the overall cost of the combination and
marginalization operations performed during the propagation step  regarding the initialization step  the loops for chance and value variables generate singletons  and thus take time
linear in the input  since decision nodes have no parents  each set d added due to a decision variable d contains d    d   valuations  let    maxdd d be the cardinality of
the largest domain of a decision variable  then the initialization loop for decision variables
takes o  d   time  which is polynomial in the input  let us now analyze the propagation
step  the running time of propagating  sets of  valuations is exponential in the maximum
number of variables in the scope of the valuations generated during the loop step  this
number depends on the permutation  chosen and is in the best case equal to the treewidth
of the diagram plus one  although finding an optimal permutation  i e   one that leads to
a minimum maximum number of variables per scope  is an np hard task  we can generate
permutations  using the standard heuristics for variable elimination in bayesian networks 
such as minimizing the number of fill ins or the cardinality of the domain of the neighbor
set  which have been empirically shown to produce good elimination orderings  jensen  
nielsen        koller   friedman        
consider a permutation  that induces a maximum number of variables per scope of
  and a diagram with bounded number of states per variable   then the cost of each
combination or marginalization is bounded by a constant  and the complexity depends
only on the number of operations performed  moreover  we have in this case that    
let  denote the cardinality of the largest set i   for i              n  thus  computing i
requires at most   u    operations of combination  because
that is the maximum number
n
of sets that we might need to combine to compute bi  in the propagation step  and
 operations of marginalization  in the worst case   is equal to  d   o  d     that is  all
sets associated to decision variables have been combined without discarding any valuation 
hence  the worst case complexity of the propagation step is exponential in the number of
decision variables  even if the width of the elimination ordering and the number of states per
variable are bounded  note however that this is a very pessimistic scenario and  on average 
the removal of non maximal elements greatly reduces the complexity  as the experiments in
section   show 
    reverse topological ordering
the valuations used by mpu specify twice as many numbers as the cardinality of the domain
of their associated scope  it is possible to decrease the number of numerical parameters per
valuation the algorithm needs to handle by a factor of two by constraining the elimination
of variables to follow a reverse topological ordering according to the diagram  that is  by
requiring each variable to be processed only after all its descendants have been processed 
as the following result shows  any reverse topological ordering produces valuations whose
probability part equals one in all coordinates 

   

fimaua  de campos    zaffalon

a

b

v 

d

c

e

v 

f

figure    a limid in which a reverse topological ordering increases treewidth 

proposition     if  defines a reverse topological ordering over the variables in c  d 
then for i              n the valuations in i have probability part p      where   is the function
that always returns the unity 
proof  we show the result by induction on i  regarding the basis  we have from the reverse
topological ordering that x  is a variable containing only value nodes as children  hence 
pax
b     x           uv      v  chx     where by definition x  equals   px          if
pax

pax

x  is a chance node  and   px           px     px    if it is a decision node  it follows
p
pax p
pax p
that     max    x  px      x  px    v chx uv      since for any   pax   p
x 
 
 
p
pax
 
is a probability mass function over x    we have that p  
     assume by
x  px 
n
inductive hypothesis
that
the
result
holds
for
  
 
 
 
 
i

  
and
let

 
x
bi  s    then
n
i   max   bi s     x    by inductive hypothesis all valuations in a set  in bi   s 
have probability part p      hence  by definition of combination  the valuations in x
contain also probability part equal to one  the reverse topological ordering implies that
by the time variable xi is processed in the propagation step  all its children have been
pax
processed  hence  the only element of bi  s  is the set xi   which equals   pxi i       if xi
pax

pax

is a chance node    pxi i        pxi i  pxi   if xi is a decision node  and      uxi    if it is a
value node  thus  we have that i   max xi  x    the case when xi is a value node is
immediate  since any valuation in i is the result of a combination of two valuations with
probability part equal to one  if xi is not a value node then


 x


pax x pax
pax
i   max 
pxi i  
pxi i ux    pxi i       faxi       ux    x 
xi

xi



 x


pax
pax
  max    
pxi i ux    pxi i       xi       ux    x   
xi

since p
xi is a probability mass function for any   pax  
i

the result states that if we assume a reverse topological elimination ordering  then mpu
needs to care only about the utility part of the valuations  unfortunately  constraining the
elimination order might increase the complexity of the algorithm  as the following example
shows 
consider the limid in figure    where all variables are assumed binary  we omit the
specification of probabilities and utilities as they are not relevant for the matter   after
   

fisolving limids

the initialization  we have that s     a   b   c   d   e   f   v    v     using a reverse
topological elimination ordering implies we first have to eliminate e  which generates the
set


    max  e   v   v   e        u      

whose single element     u    has scope  a  c  d  f   and size          eliminating variables
in the ordering f  c  b  a  d  e  on the other hand  generates the following sets 


    max  f  v   c  f     p    u      


    max  c  e     c     p    u      

 n
o
 d   d 
    max  b  d  b    p    u      d  d  

 n
o
 d   d 
    max  a  v      a    p    u      d  d  
 n
o

 d   d 
    max        d    p    u      d  d  
 n
o

 d 
 
   
u
 
 
d


 
    max e
d
 
 
the scopes of the valuations in                   and   are  respectively   e  c    d  e  
 d  a    e  d    e  and     as one can see  the largest valuation generated using ordering
f  c  b  a  d  e contains two variables in its scope and therefore has size         this is
a four fold decrease in size compared to the size of the set   generated using the reverse
topological ordering 
notice however that even though using reverse topological ordering might increase the
size of the valuations generated during variable elimination  it does not necessarily results
in higher complexity for the mpu  this is because the overall complexity of the algorithm
depends not only on the size of the largest valuation generated but also on the cardinality of
the generated sets  and it is possible that a reverse topological ordering induces significantly
smaller sets  as it produces valuations whose probability parts are always equal to one  which
might increase the number of dominated elements 

   experiments
we evaluate the performance of the algorithm on random limids generated in the following
way  each limid is parameterized by the number of decision nodes d    d   the number
of chance nodes c    c   the maximum cardinality of the domain of the family of a chance
variable c   maxc  fac    and the maximum cardinality of the domain of the family of
a decision variable d   maxd  fad    we set the number of value nodes v to be d     
for each variable xi   i              c   d   v  we sample xi to contain from   to   states 
then we repeatedly add an arc from a decision node with no children to a value node
with no parents  so that each decision node has at least one value node as children   this
step guarantees that all decisions are relevant for the computation of the meu  finally  we
repeatedly add an arc that neither makes the domain of a variable greater than the given
bounds nor makes the treewidth more than     until no arcs can be added without exceeding
   

fimaua  de campos    zaffalon

the bounds   note that this generates diagrams where decision and chance variables have
at most log  d    and log  c    parents  respectively  once the dag is obtained  we
randomly sample the probability mass functions and utility functions associated to chance
and value variables  respectively 
we compare mpu against the cr algorithm of de campos and ji        in      limids
randomly generated by the described procedure with parameters    d         c     
   d     and     c      mpu was implemented in c   and tested in the
same computer as cr   table   contrasts the running times of each algorithm  averages 
standard deviation  for different configurations of randomly generated limids  each row
contains the percentage of solved diagrams  scr and smpu   and time performance  tcr
and tmpu   of each of the algorithms for n diagrams randomly generated using parameters
d  c  v  d   and c   for each fixed parameter configuration  mpu outperforms cr by
orders of magnitude  line    contains the only case in which the average running time of
cr is lower than mpus  but note that in this case cr solve it only one instance  whereas
mpu solved     of the instances   also  cr was unable to solve most of the diagrams with
more than    variables  whereas mpu could solve diagrams containing up to     variables
and with d      both algorithms failed to solve diagrams with d       a diagram is
consider unsolved by an algorithm if the algorithm was not able to reach the exact solution
within the limit of    hours  all in all  mpu appears to scale well on the number of nodes
 i e   on d  c and v  but poorly on the domain cardinality of the family of decision variables
 i e   on d   
a good succinct measure of the hardness of solving a limid is the total number of
strategies     which represents the size of the search space in a brute force approach    
can also be loosely interpreted as the total number of alternatives  over all decision variables 
in the problem instance  figure   depicts running time against number of strategies in a
log log scale for the two algorithms on the same test set of random diagrams  for each
algorithm  only solved instances are shown  which covers approximately     of the cases
for mpu  and     for cr  we note that mpu solved all cases that cr solved  but not the
opposite   again  we see that mpu is orders of magnitude faster than cr  within the limit
of    hours  mpu was able to compute diagrams containing up to      strategies  whereas
cr solved diagrams with at most      strategies 
the reduction in complexity obtained by the removal of non maximal valuations during
the propagation step can be checked in figure    which shows the maximum cardinality of
a set i generated in the propagation step in contrast to the number of strategies  for each
diagram  a point in the figure  solved by mpu  the cardinality of the sets remains bounded
above by     while we vary the number of strategies  which equals the largest cardinality
of a propagated set in the worst case where no valuation is discarded   this shows that the
worst case analysis in section     is very pessimistic 
   since current algorithms for checking whether the treewidth of a graph exceeds a fixed k are too slow
for k     bodlaender         we resort to a greedy heuristic that resulted in diagrams whose actual
treewidth ranged from   to    
   we used the cr implementation available at http   www idsia ch  cassio id mip  and cplex
 http   www ilog com  as mixed integer programming solver  our implementation of mpu can be
downloaded at http   www idsia ch  cassio mpu  

   

fisolving limids

n

d

c

v

d

c

scr    

tcr  s 

smpu    

tmpu  s 

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

 
 
 
 
 
 
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  

 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
  
 
  
  
 
  
  
  
  
  
  
  
 
  
  
  
  
  
  
 
  
  
  
  
 
  
  
 
  
 
  
 

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

   
   
   
  
  
   
  
  
  
  
 
 
 
   
  
  
  
 
 
 
   
  
  
  
  
  
  
 
  
 
  
 
  

     
     
     
      
        
        
          
          
          
          

     

  
          
           
           



     
          
          
          
            
         
          

          

          

          

   
   
   
   
   
   
   
   
  
   
  
  
 
   
   
  
   
  
  
 
   
   
   
   
   
   
  
   
   
   
   
  
   

           
          
           
          
        
           
          
        
       
        
         
          

           
         
         
      
         
          

           
        
      
         
     
          
       
     
         
         
        
          
         

table    performance of mpu and cr on randomly generated limids  numbers are
rounded down  

   

fimaua  de campos    zaffalon

   

mpu
cr

running time  s 

   
   
   
   
   
   
   
   

    
    
    
number of strategies     

maximum set cardinality  maxi  i   

figure    running time of mpu and cr on randomly generated limids 
   
   
   
   
   
   
   
   

    
    
    
number of strategies     

figure    maximum number of valuations in a set during the propagation step of mpu 

   related work
influence diagrams were introduced by howard and matheson        as a concise language
for the specification of utility based decision problems  there is a substantial literature that
formalizes influence diagrams and develop algorithms under the premises of no forgetting
and regularity  cooper        qi   poole        shachter   peot         we point the
interested reader to the works of jensen and nielsen        and koller and friedman        
zhang et al         studied families of limids that could be solved by dynamic programming  such as limids respecting no forgetting and regularity  the spu algorithm
of lauritzen and nilsson        solves these cases in polynomial time if the diagram has
   

fisolving limids

bounded treewidth  to the best of our knowledge  the only attempt to  globally  solve arbitrary limids exactly without recurring to an exhaustive search on the space of strategies
is the cr algorithm of de campos and ji        against which we compare our algorithm 
shenoy and shafer        introduced the framework of valuation algebras  which states
the basic algebraic requirements for efficient computation with valuations  more recently 
haenni        incorporated partially ordered preferences in the algebra to enable approximate computation  fargier et al         then extended the framework with a preference
degree structure in order to capture the common algebraic structure of optimization problems based on a partial order  the algebra we develop in section   can be partly casted in
this framework 
the pfu framework of pralet  verfaillie  and schiex        subsumes many formalisms
of probabilistic reasoning  constraint satisfaction and decision making  when it comes
to decision problems the framework is geared towards sequential decision making under
equivalent assumptions of non forgetting  although the authors mention the possibility of
extending it to limited information decision scenarios 
the variable elimination algorithm we develop here is conceptually close to the message
passing algorithm of dubus  gonzales  and perny         their algorithm  however  does
not handle uncertainty and target primarily the obtention of pareto efficient solutions for
a specific class of multi objective optimization problems 
there is a close relation between maximum a posteriori  map  inference in bayesian
networks and limids whose decision variables have no parents  in this sense  the algorithm of de campos         which solves map by propagating pareto efficient probability
potentials in a join tree  relates to ours 

   conclusion
solving limited memory influence diagrams is a very hard task  the complexity results
presented here show that the problem is np hard even for diagrams with bounded treewidth
and number of states per variable  and that obtaining provably good approximations in
polynomial time is unlikely if the number of states is not small 
despite the theoretical hardness of the problem  we developed an algorithm that in
spite of its exponential worst case complexity performed empirically well on a large set of
randomly generated problems  the algorithms efficiency is based on the early removal of
suboptimal solutions  which helps the algorithm to drastically reduce the search space 
designing good heuristics for elimination orderings with our algorithm seems to be
a more complex task than with standard variable elimination algorithms  e g   for belief
updating in bayesian networks   because there is a second component  the cardinality of a
set  that together with domain cardinalities we wish to minimize  in fact  some preliminary
experimentation has shown that favoring set cardinality at expense of domain cardinality
might be a good approach  unlike standard variable elimination  given an elimination
ordering and a limid  it does not seem to be possible to determine the true complexity
of mpu in advance  i e   prior to running the algorithm   it is an open question whether
mpus complexity can be estimated beforehand  and which heuristics for finding elimination
orderings perform better 
   

fimaua  de campos    zaffalon

acknowledgments
this work was partially supported by the swiss national science foundation  snsf 
grants no                                   and                hasler foundation grant
no         and the canton ticino computational life sciences project  we thank the
reviewers for pointing us to related work and making a number of comments that helped
us improve the readability of the paper  a short version of this paper appeared in nips
    maua   de campos        

appendix a  missing proofs
this section contains long proofs and supporting results that were left out of the main part
of the text to improve readability 
the following two lemmas are used in the proof of theorem   later on 
lemma     if     is a real number and i is a nonnegative integer then       i     
i
     
proof  since         we have that       i               i           i     and
i
it is sufficient to show that      i         from the binomial theorem we have that
i

      

i   i

 

 

   i
x
 
k  

k

  i   k  

for k               i   we have that
 i
 
 i   i          i  k     
 
   i  k  
k
k 
hence 
i

      

i   i

 

i

 
 

x
x
x
i k i  k
k

       
   
  
 k      
k  

 i

and therefore      i     

k  

k  

 
 

lemma     if    x      then  x     x    x  
proof  we obtain the result by approximating the functions on the left  and right hand side
of the inequalities by their truncated taylor expansions f  x  and g x   respectively  and
 
then showing that  x     x   f  x   g x    x   the n th order taylor expansion of
the left hand side around zero is given by
tn  x       

n  
x
 ln     k
k  

  k  

x k  

clearly  the series converges and hence  x     x    limn tn  x   moreover  for any
n  the residual rn  x     x     x   tn  x  is positive because the terms of the sum are
   

fisolving limids

all non negative  thus 
f  x    t   x       

 ln       
x   x     x   
 

in a similar fashion  we apply the variable change y   x  on the right hand side and
obtain its taylor expansion around zero  given by
tn   y       
   

n
x
 ln    k
k  
n
x
k  

k 

yk

 ln    k  k
x  
k 

which also converges and has positive residual  hence 
 

 x   lim tn   x 
n

      x  ln      x  ln   


x
 ln    k 
k  

     x  ln      x  ln   


x
k  

      x  ln     

 ln     
  

 

k 
 

 
x k 

  k 

x    g x   

the inequality is obtained by noticing that  ln    k   k         x       ln    and that
the geometric series

x
k  

 
  k 


 
  
  x   k
 
ln   
  x   k
   
     
 
   
 
 
 
 
 
 
  
k  

k  

finally  since x            ln       we have that


ln   
 
g x        x ln    x  
  


  
ln   
 
      x ln   
ln     
  
  
 
 ln      
   
x   f  x   
 
 

 

hence   x  g x   f  x    x     x  and the result holds 
the following result shows that solving limids is np hard even if we assume bounded
treewidth and number of states per variable 
   

fimaua  de campos    zaffalon

x 

d 

d 

x 

x 

dn



xn

r

figure    limid used to solve the partition problem in the proof of theorem   

proof of theorem    given a strategy s  deciding whether es  l    k can be done in polynomial time  koller   friedman         so the problem is in np  hardness is shown using
a reduction from the partition problem  which is np complete  garey   johnson       
and can be stated as follows  pgiven a set
p of n positive integers a            an   is there a set
i  a               n  such that ii ai   ia i ai   we assume that n     
p
p
let a      ia ai   an even partition is a subset i  a that achieves ii ai   a 
to solve partition  we consider the rescaled problem  dividing every element
by a   so that
p
vp
i   ai  a    are the elements and we look for a partition such that
ii vi      because
v
 
   
ia i
consider the following limid with topology as in figure    there are n binary decision
nodes labeled d            dn   each decision di can take on states d  and d    the chain of
chance nodes has n     ternary variables x    x            xn with states x  y  and z  there is
an arc from xn to the single value node r  for notational purposes  we specify a function
f over the domain  x  y  z  as a triple  f  x   f  y   f  z    the value node has an associated
utility function ur              for i              n  each chance node xi has an associated set
of conditional probability mass functions given by
d   x
   ti         ti   
px
i

d   x
            
px
i

pdx i y             

pdx i y       ti      ti   

pdx i z             

pdx i z             
dx

for ti          we specify these variables later on   note that pxii i   w      for every
w  faxi such that wxi    wxi  and wxi    z  finally  we define px                    
given a strategy s    d            dn    let i    i   di   d    be the index set of policies
in s such that di      d    we have that
es  l   

x

px 

cd

n
y

 
dx
pxii i  pdi

i  


 

x

x

px 


xn

ur

n
y


d xi 

pxii

i  

cd  xn  

let
ps   px 

n
y

d xi 

pxii

i  

   

pdi

pdi  ur  

fisolving limids

and
x

pxn  

n
y

px 

d xi 

pxii

i  

cd  xn  

x

pdi  

ps  

cd  xn  
d xn 

for w  cd such that wxn   x  i e   for w  xcd   it follows that pxnn
  if and only if wxn    x  but for wxn 

 wfaxn     

d
xn 
  x we have that pxn 
 wfaxn        
n 
dx
also  for any i              n   pxii i   wfaxi  

if and only if wxn    x and so recursively 
equals ti if i  i and   otherwise  hence 
  q
 
ti   if wxi   x for i              n   
ps  w      ii
  
otherwise 
and

n

 y
ti  
ps  w   
 
cd

x

pxn  x   

ii

wx

likewise  it holds for w  y cd that
  q
ps  w   

 
 

ia i ti  

  

if wxi   y for i              n   
otherwise 

and therefore
pxn  x   

n
  y
ti  
 
ia i

since pxn is a probability mass function on xn   pxn  z       pxn  x   pxn  y   and
x
pxn ur
es  l   
xn

     pxn  x   pxn  y 
 y
  y
  
ti 
ti  
 
 
ii

ia i

let us assume initially that ti    vi   the reduction from the original problem in
this way is not polynomial  and we will use it only as an upper bound for the outcome of
the reduction we obtain later  it is not difficult
to see
p
p that es  l  is a concave function of
v            vn that achieves its maximum at ii vi   ia i vi      since each strategy s
defines a partition of a and vice versa  there is an even partition if and only if meu l   
                        
we will now show a reduction that encodes the numbers ti in time and space polynomial
in b  the number of bits used to encode the original problem  this part is in close analogy
with the last part of the proof of hardness of map in bayesian networks by de campos
       theorem     
by setting ti to represent  vi with  b     bits of precision  rounding up if necessary  
that is  by choosing ti so that  vi  ti    vi   i   where    i      b      we have that
   

fimaua  de campos    zaffalon

 vi  ti    vi      b      which implies  by using lemma    with    vi    and
 b
i    b  that  vi  ti    vi     
assume that an even partition i exists  then 
p
y
 b
 b
 b
ti      n ii vi        n        
ii

y

 b n

t i     

p

ia i

vi

 b n

      

 b

     

 

ia i

and

 b


  
       b
 b
 
      
  
 
meu l      
 
 

   

 b

let r be equal to   
encoded with  b     bits of precision  and rounded up   that is 
 b
 b
 
 
  b   
 
r  
  
  which implies  by lemma    with      b    and i    b 
that
 b
 b
 b
  b
 b
  
 r        
    
      
   
the reduction is done by verifying whether meu l       r    we already know that an
even partition has an associated strategy which obtains an expected utility greater than
   r    because of equality     and the fact that r is rounded up  let us consider the
case where an even partition does not exist  we want to show that in this case meu l  
 b
          which by inequality     implies meu l       r    since there is not an even
partition  any strategy induces
a partition such
p
p that  for some integer a  c  a different
from zero  we have that ii ai   ac and ia i ai   a c  because the original numbers
ai are positive integers that add up to  a  it follows that
y
y
ti  
ti    c a     c a   
ii

ia i

the right hand side of the equality is a function on c   a          a       which is symmetric
with respect to the y axis  i e   f  c    f  c   and monotonically increasing for c     
therefore  it obtains its minimum at c      hence 
y
y
ti  
ti     a       a   
ii

ia i

since n     implies a     because the numbers ai are positive integers   we have by
lemma    that
 
   a       a      a  
each number ai is encoded with at least log  ai bits  and therefore b  log   a          
log   an     log   a     an    the latter is greater than or equal to log   a         an    and
hence is also greater than log  a  thus  we have that a   b   which implies a     b and
 
 b
therefore   a     b and    a       hence 
 b

   a       a     

 

   since the number of bits used to encode the partition problem must be greater than or equal to n  we
have that n  b  n b     and hence   j   b n    jb   for any j     

   

fisolving limids

thus  if an even partition does not exist we have that


 b
y
  
  y
ti     
meu l      
ti  
     r    
 
 
ii

ia i

to summarize  we have built a limid l in polynomial time since each ti was specified
dx
using o b  bits and there are n functions pxii i    each encoding    numbers  which are
either      or ti    and  n     variables with bounded number of states  we have shown that
there is a one to one correspondence between partitions of a in the original problem and
strategies of l  and that for a given rational r   f  b  encoded with o b  bits the existence
of an even partition is equivalent to meu l       r   
the following lemma is used in the proof of theorem    a similar result has been shown
by park and darwiche        lemma    
lemma     for any x    it follows that x            ln       x  
proof  let f  x    ln       x      x         then
f    x    

x 

 
 
   
 
  x x   x      

which is strictly negative for x    since x   x   x   x      hence  f  x  is a monotonically
decreasing function for x     because limx f  x       f  x  is strictly positive in       
thus  the result follows from ln       x       x         since x    
we now show that approximately solving limids of bounded treewidth for any given
minimum performance is np hard 
proof of theorem    we will show that for any fixed          the existence of a polynomial

time    approximation algorithm for solving a limid would imply the existence of a
polynomial time algorithm for the cnf sat problem  which is known to be impossible
unless p np  garey   johnson         a very similar reduction was used by park and
darwiche        theorem    to show an analogous inapproximability result for maximum a
posteriori inference in bayesian networks  notice that for any          there is         

such that        hence the existence of an  approximation algorithm implies the existence

of a    approximation  and it suffices for the desired result to show that the latter cannot
be true  unless p np  
a clause is a disjunction of literals  each literal being either a boolean variable or its
negation  we say that a clause is satisfied if  given an assignment of truth values to its
variables  at least one of the literals evaluates to    thus  we can decide if a truth value
assignment satisfies a clause in time linear in the number of variables  the cnf sat
problem is defined as follows  given a set of clauses c            cm over  subsets of   boolean
variables x            xn   is there an assignment of truth values to the variables that satisfies
all the clauses 
for a positive integer q that we specify later on  consider the limid obtained as follows
 the topology is depicted in figure     for each boolean variable xi we add q binary
   

fimaua  de campos    zaffalon

b 
dn 

sn 

dn 

bq
dnq

sn 

 
 
 
d  



b 

 
 
 

s  

d  

snq

 
 
 
d q

s  

s  

u

s q
s q

s  

figure    graph structure of the limid used in the proof of theorem   

decision variables di            diq and q chance variables si            siq with domain                m  
additionally  there are q clause selector variables s             s q taking values on                m  
q binary variables b             b q   and a value node u with b q as parent  as illustrated in
figure    the limid consists of q replicas of a polytree shaped diagram over variables
d j           dnj   s j           snj   b j   and the probability mass functions for the variables b             b q
are chosen so as to make the expected utility equal the product of the expected utilities of
each replica  in any of the replicas  i e   for j              q    a variable dij  i              n 
represents an assignment of truth value for xi and has no parents  the selector variables
s j represent the choice of a clause to process  that is  s j   k denotes clause ck is being
processed  and by summing out s j we process all clauses  each variable sij   for i  
j
           n and j              q  has dij and si 
as parents  the variables b j have snj and  if
j      b j  as parents  for all j  we assign uniform probabilities to s j   that is  ps j     m 
 

for j              q  we set the probabilities associated to variables s j           snj so that if ck is
the clause selected by s j then sij is set to zero if ck is satisfied by di but not by any of
j
d            di    and sij   si 
otherwise  formally  for x   s j  dj  s j   we have that
i



  




j j
d s
  
p ji i   x   
si
  




  

i

i 

j

j

if xsi   xsi       
j

j

if xsi     and xsi    k    and xi   xdi satisfies ck  
j

j

if xsi   xsi    k    and xi   xdi does not satisfy ck  
otherwise 

notice that for s j the first case never occurs since s j takes values on             m   for any
j
joint state configuration x of s j           snj   d j           dnj such that xs    k              m   i e  
j
clause ck is being processed  and xsn      it follows that
 
n
j
y
dij si 
ps j
p j
pdj  x 
 

i  

si

i

equals   m only if for some     i  n clause ck is satisfied by xi   xdi but not
j
by any of x    xd            xi    xdi    variables s j           si 
all assume value k  i e  
   

fisolving limids

j

j

j

j

xs         xsi    k   and xsi        xsn      otherwise  it equals    hence  for
any  partial  strategy sj    dj           dnj   we have for x     that
 




j
pss j  x 
n

 x

 

s j      s j

ps j

 

n 

 

n
y
i  

j
dij si 

p

sij



sat  sj  
pd j 
 x 
 
 
i
m


j
d j      dn

where sat  sj   denotes the number of clauses satisfied by the truth value assignment of
j

sn b
x            xn according to sj   each variable b j is associated to a function pb
j
for x  fabj  

j
b j   xb j  and xsn

    
   if x
j j 
j
j
psbnj b  x       if xb     and xsn       


   otherwise 

j 

such that

 

where for b   we assume xb      hence  we have for any joint state configuration x of
b             b q   sn            snq that

q
 
b          xb q     and xsn

       xsn     
   if x
j j 
 
q
 

psbnj b   x       if xb        xb     and xsn      


j  
   otherwise 




q
y

finally  we set the utility functionu associated to u to return   if b q     and  
j j 
q
 
q
 
otherwise  in this way  u qj   psbnj b
 x  equals   if xb        xb     and xsn  
q

     xsn     and zero otherwise  thus  for any strategy s    s            sq    where sj  
dj           dnj   it follows that
 

es  l   

x

u

cd

 

q
y

u

b        b q
       s q
sn
n

 

 

j 

ps j

 

j  

x

x

j

psbnj b
q
y

j

psbnj b

u

b        b q j  
       s q
sn
n
q
y
j
pss j      
n
j  

i  

j 

j  

q
y

n
y

p

j
dij si 

sij

x

i

ps j

j
s j      sn 
j
j
d       dn
j

sn b
pb
j

j 

pd j

 

n
y
i  

p

j
dij si 

sij

pd j
i

j

pss j

n

q
  y
sat  sj    
mq
j  

if the instance of cnf sat problem is satisfiable then there is an optimum strategy s
such that sat  sj     m for all j  and meu l       on the other hand  if the instance
   

fimaua  de campos    zaffalon

is not satisfiable  we have for all j and strategy s that sat  sj    m     and hence
meu l    m    q  mq   for some given           let q be a positive integer chosen so

that       mq   m     q   we show later on that q can be obtained from a polynomial

on the input  if the cnf sat instance is satisfiable  a    approximation algorithm for
meu l  returns a value es  l  such that
q 


m  q
meu l 
m
 
 
es  l  
 
m  
m
  
where the rightmost strict inequality follows from m  m         m     m  on the other
hand  if the cnf sat instance is not satisfiable  the approximation returns


m  q
es  l   meu l  
 
m


hence  we can use a    approximation algorithm to solve cnf sat by checking whether its
output e l     m  q  mq   since q and m are positive integers  the test bound  m  q  mq
can be obtained in polynomnial time 
it remains to show that the reduction is polynomial in the input  the limid contains
q  n          variables  each requiring the specification of at most   m       numbers in
      m      so   the number of numerical parameters in l  is polynomially bounded by
q m         n           therefore  it suffices to show that q is a polynomial on m and n  by
definition  q obeys


  q
 

  
    q m      n        
m
which is equivalent to


 
q ln    
m



  q    m         n           ln  

  m         n          

ln  
 
ln     m
   
 
  m         n          

ln  
 
ln     m

 q    

q 

since  by lemma     m            ln       m  and     ln     it suffices to choose q such
that
  
q     m       m         n              
   



in other words  q is polynomially bounded by m    n     therefore  if meu l  can

be approximated in polynomial time with an ratio no greater than   then we can solve
cnf sat in polynomial time 
the next result show that transformation   preserves expected utility of strategies  and
that strategies can be easily mapped back and forward between original and transformed
diagrams 
   

fisolving limids

pad    j

x 

x 

t 

t 



xj 

xj

xj  

tj 

tj

tj  



xm

chd

tm

figure     reasoning of the proof of transformation   

t  pa

x

 t  pa

proof of proposition    by looking at the definition of the functions px   d and pxii  i d  
for i              m  we can see that when pad selects  j   that is  conditional on pad    j  
the variable xj is independent of xj  and for i              m  i    j  the variable xi is
independent of ti   in other words  we have that
t

pr xj  xj    tj   pad    j     pr xj  tj   pad    j     pxjj  
and  for any i    j 
x

pr xi  xi    ti   pad    j     pr xi  xi    pad    j     pxii   
we can visualize this situation by removing the arc from xj  to xj and all the arcs from
ti to xi for i    j in the diagram of figure   b   the arcs leaving pad can also be removed
as we are conditioning on a value of pad    which results in the diagram in figure     note
that in principle the case for j     deserves special attention  as the x  does not depend
on any other xi variable  and the function associated to x  slightly differ from others 
nevertheless  a similar reasoning can be applied  we will omit the case for j     for the
sake of simplicity 
it follows from the previous reasoning that


pxjm   pr xm  pad    j  
m
x
x
y
t
x
 
 px  pt    pxjj ptj  
pxii  pti
t       tm x       xm 

 

x

ptj

tj

x

i   i  j
t

pxjj

m
y

x

i j  

xj      xm 

x

pxi 
i

x       xj 

 
 

x
tj

ptj

x
xj      xm 

t

pxjj

m
y

x

pxii   

i j  

   

px 

j 
y

x

pxi 
i

i  

x

m
y

pti

t       tm  tj i   i  j

 z

  

 

fimaua  de campos    zaffalon

x

when pad    j   each function pxi 
for i    j equals the indicator function ixi  xi    by
i
t

design  and pxjj   ixj  tj   we thus have that

pxjm

 

x

x

ptj

tj

m
y

ixj  tj

ixi  xi   

i j  

xj      xm 

for each term of the outer sum over tj   the inner sum over xj           xm  differs from zero
only when tj   xj   xj          xm   in which case it equals one  hence  we have that
x

pxjm  
ptj ixm  tj
tj

  ptj  
pa

now consider a strategy s     t            tm           for l    and let pxmd be a function that

equals pxjm for every value  j  pad   let also d be a policy for the original decision
variable d in l such that d   j     tj for all j  and s be a strategy for l obtained by
substituting policies t            tm with d in s    finally  let pt            ptm be the distributions
pa
induced by the policies for t            tm in s    and pd d be the distribution induced by d  
pad
since for each value  j of pad we have that pxm   j     ptj   and since by design xm   d  
pa
pa
it follows that pxmd   pd d   hence  for each combination of policies t            tm in l  we
can derive a corresponding policy in l  the converse is also true  for each policy d we can
pa
pa
generate t            tm such that pxmd   pd d  simply choose ti   d   j   for all i   thus 
there is a one to one correspondence between policies t            tm and policies d   and a
pa
pa
one to one correspondence between the induced functions pxmd and pd d  
it remains to show that a combination of policies t            tm and a corresponding policy
d induce the same expected utility  let c   and d  denote  respectively  the set of chance
and decision variables in l    and c and d the set of chance and decision variables in l 
also  for a given strategy s for l  let
y
pa
p s  
px x  
cd  d 
pa

note that the above function is independent of the choice of policy d   and that ps   p s pd d  
given any strategy s  for l  we have that
x
x
es   l     
ps 
uv
c   d 

 

x

v v

p s

t  pa
px   d

c   d 

m
y

x
 t  pa
pxii  i d

i  

m
y

 
pti

 

p s

x

uv

v v

cd  d 

x x
c    c d   d

x

x

cd  d  xm

pa

pxmd p s

x

m
y

x

pxii 

i  

 z

 
 

t  pad

px  

uv

v v

i  

 
x

x

p
pad
  xm pxm

uv

v v

   

 ti  pad

m
y

pti

i  

 

fisolving limids

 

x

x

pa

pd d p s

x

ps

cd

x

uv

v v

cd  d  d

 

x

uv

v v

  es  l   
where s is the strategy for l obtained from s  by substituting t            tm with the corresponding policy d  

references
bodlaender  h  l          a linear time algorithm for finding tree decompositions of small
treewidth  siam journal on computing                   
cooper  g  f          a method for using belief networks as influence diagrams  fourth
workshop on uncertainty in artificial intelligence 
de campos  c  p          new results for the map problem in bayesian networks  in
proceedings of the   nd international joint conference on artificial intelligence  pp 
         
de campos  c  p     ji  q          strategy selection in influence diagrams using imprecise probabilities  in proceedings of the   th conference in uncertainty in artificial
intelligence  pp         
dechter  r          bucket elimination  a unifying framework for reasoning  artificial
intelligence                  
detwarasiti  a     shachter  r  d          influence diagrams for team decision analysis 
decision analysis            
dubus  j  p   gonzales  c     perny  p          multiobjective optimization using gai
models  in proceedings of the   st international joint conference on artificial intelligence  pp           
fargier  h   rollon  e     wilson  n          enabling local computation for partially
ordered preferences  constraints             
garey  m  r     johnson  d  s          computers and intractability  a guide to the theory
of np completeness  w  h  freeman 
haenni  r          ordered valuation algebras  a generic framework for approximating
inference  international journal of approximate reasoning              
howard  r  a     matheson  j  e          influence diagrams  in readings on the principles
and applications of decision analysis  pp          strategic decisions group 
jensen  f  v     nielsen  t  d          bayesian networks and decision graphs   nd
edition   information science and statistics  springer 
kohlas  j          information algebras  generic structures for inference  springer verlag 
new york  usa 
   

fimaua  de campos    zaffalon

koller  d     friedman  n          probabilistic graphical models  principles and techniques  mit press 
lauritzen  s  l     nilsson  d          representing and solving decision problems with
limited information  management science               
maua  d  d     de campos  c  p          solving decision problems with limited information  in advances in neural information processing systems     pp         
maua  d  d   de campos  c  p     zaffalon  m          solving limited memory influence
diagrams  arxiv          v   cs ai  
park  j  d     darwiche  a          complexity results and approximation strategies for
map explanations  journal of artificial intelligence research             
poupart  p     boutilier  c          bounded finite state controllers  in advances in neural
information processing systems     nips  
pralet  c   verfaillie  g     schiex  t          an algebraic graphical model for decision with
uncertainties  feasibilities  and utilities  journal of artificial intelligence research     
       
qi  r     poole  d          a new method for influence diagram evaluation  computational
intelligence             
shachter  r  d     peot  m  a          decision making using probabilistic inference methods  in proceedings of the  th conference on uncertainty in artificial intelligence 
pp         
shenoy  p     shafer  g          axioms for probability and belief function propagation 
in proceedings of the  th conference on uncertainty in artificial intelligence  pp 
       
zhang  n  l   qi  r     poole  d          a computational theory of decision networks 
international journal of approximate reasoning                

   

fi