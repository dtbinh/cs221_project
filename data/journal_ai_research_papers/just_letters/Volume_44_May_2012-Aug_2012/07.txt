journal of artificial intelligence research                  

submitted        published      

riffled independence for efficient inference
with partial rankings
jonathan huang

jhuang   stanford edu

james h  clark center
stanford university  stanford ca        usa

ashish kapoor

akapoor microsoft com

microsoft research
one microsoft way
redmond wa             usa

carlos guestrin

guestrin cs cmu edu

gates hillman complex  carnegie mellon university 
     forbes avenue  pittsburgh  pa        usa

abstract
distributions over rankings are used to model data in a multitude of real world settings
such as preference analysis and political elections  modeling such distributions presents
several computational challenges  however  due to the factorial size of the set of rankings
over an item set  some of these challenges are quite familiar to the artificial intelligence
community  such as how to compactly represent a distribution over a combinatorially large
space  and how to efficiently perform probabilistic inference with these representations 
with respect to ranking  however  there is the additional challenge of what we refer to as
human task complexity  users are rarely willing to provide a full ranking over a long list
of candidates  instead often preferring to provide partial ranking information 
simultaneously addressing all of these challenges  i e   designing a compactly representable model which is amenable to efficient inference and can be learned using partial
ranking data  is a difficult task  but is necessary if we would like to scale to problems
with nontrivial size  in this paper  we show that the recently proposed riffled independence
assumptions cleanly and efficiently address each of the above challenges  in particular  we
establish a tight mathematical connection between the concepts of riffled independence and
of partial rankings  this correspondence not only allows us to then develop efficient and
exact algorithms for performing inference tasks using riffled independence based representations with partial rankings  but somewhat surprisingly  also shows that efficient inference
is not possible for riffle independent models  in a certain sense  with observations which do
not take the form of partial rankings  finally  using our inference algorithm  we introduce
the first method for learning riffled independence based models from partially ranked data 

   probabilistic modeling of ranking data  three challenges
rankings arise in a number of machine learning application settings such as preference analysis for movies and books  lebanon   mao        and political election analysis  gormley
  murphy        huang   guestrin         in many of these problems  it is of great interest
to build statistical models over ranking data in order to make predictions  form recommendations  discover latent trends and structure and to construct human comprehensible data
summaries 
c
    
ai access foundation  all rights reserved 

fihuang  kapoor   guestrin

modeling distributions over rankings is a difficult problem  however  due to the fact that
as the number of items being ranked increases  the number of possible rankings increases
factorially  this combinatorial explosion forces us to confront three central challenges when
dealing with rankings  first  we need to deal with storage complexity  how can we compactly represent a distribution over the space of rankings   then there is algorithmic complexity  how can we efficiently answer probabilistic inference queries given a distribution 
finally  we must contend with what we refer to as human task complexity  which is a
challenge stemming from the fact that it can be difficult to accurately elicit a full ranking over
a large list of candidates from a human user  choosing from a list of n  options is no easy task
and users typically prefer to provide partial information  take the american psychological
association  apa  elections  for example  which allow their voters to rank order candidates
from favorite to least favorite  in the      election  there were five candidates  and therefore
         ways to rank those five candidates  despite the small candidate list  most voters
in the election preferred to only specify their top k favorite candidates rather than writing
down full rankings on their ballots  see figure     for example  roughly a third of voters
simply wrote down their single favorite candidate in this      election 
these three intertwined challenges of storage  algorithmic  and human task complexity
are the central issues of probabilistic modeling for rankings  and models that do not efficiently
handle all three sources of complexity have limited applicability  in this paper  we examine
a flexible and intuitive class of models for rankings based on a generalization of probabilistic
independence called riffled independence  proposed in our recent work  huang   guestrin 
             while our previous papers have focused primarily on representational  storage
complexity  issues  we now concentrate on inference and incomplete observations  i e   partial
rankings   showing that in addition to storage complexity  riffle independence based models
can efficiently address issues of algorithmic and human task complexity 
in fact the two issues of algorithmic and human task complexity are intricately linked
for riffle independent models  by considering partial rankings  we give users more flexibility
to provide as much or as little information as they care to give  in the context of partial
ranking data  the most relevant inference queries also take the form of partial rankings  for
example  we might want to predict a voters second choice candidate given information about
his first choice  one of our main contributions in this paper is to show that inference for
such partial ranking queries can be performed particularly efficiently for riffle independent
models 
the main contributions of our work are as follows  
 we reveal a natural and fundamental connection between riffle independent models
and partial rankings  in particular  we show that the collection of partial rankings
over an item set form a complete characterization of the space of observations upon
   note that it is common to wonder why one would care to represent a distribution over all rankings if the
number of sample rankings is never nearly as large  this problem that the number of samples is always
much smaller than n  however  means that most rankings are never observed  limiting our ability to
estimate the probability of an arbitrary ranking  the only way to overcome the paucity of samples is to
exploit representational structure  which is very much in alignment with solving the storage complexity
issue 
   this paper is an extended presentation of our paper  huang  kapoor    guestrin        which appeared
in the      conference on uncertainty in artificial intelligence  uai  as well as results from the first
authors dissertation  huang        

   

fiefficient inference with partial rankings

first
choice

second
choice

third
choice

fourth
choice

fifth
choice

  of
votes

 

 

 

 

 

  

 

 

 

 

 

  

 

 

 

   

   

  

 

   

   

   

   

    

 

 

 

   

   

  

 

 

   

   

   

   

 

 

 

 

 

   

figure    example partial ranking data  taken from the american psychological association
election dataset       

which one can efficiently condition a riffle independent model  as a result  we show
that when ranked items satisfy the riffled independence relationship  conditioning on
partial rankings can be done efficiently  with running time o n h    where  h  denotes
the number of model parameters 
 we prove that  in a sense  which we formalize   it is impossible to efficiently condition
riffle independent models on observations that do not take the form of partial rankings 
 we propose the first algorithm that is capable of efficiently estimating the structure
and parameters of riffle independent models from heterogeneous collections of partially
ranked data 
 we show results on real voting and preference data evidencing the effectiveness of our
methods 

   riffled independence for rankings
a ranking    of items in an item set  is a one to one mapping between  and a rank
set r               n  and is denoted using vertical bar notation as                           n  
we say that  ranks item i  before  or over  item i  if the rank of i  is less than the
rank of i    for example   might be  corn  p eas  apples  oranges  and the ranking
corn p eas apples oranges encodes a preference of corn over peas which is in turn preferred over apples and so on  the collection of all possible rankings of item set  is denoted
by s  or just sn when  is implicit  
since there are n  rankings of n items  it is intractable to estimate or even explicitly
represent arbitrary distributions on sn without making structural assumptions about the
underlying distribution  while there are many possible simplifying assumptions that one can
make  we focus on an approach that we have proposed in recent papers  huang   guestrin 
            in which the ranks of items are assumed to satisfy an intuitive generalized notion
of probabilistic independence known as riffled independence  in this paper  we argue that
riffled independence assumptions are particularly effective in settings where one would like
to make queries taking the form of partial rankings  in the remainder of this section  we
review riffled independence 
   

fihuang  kapoor   guestrin

the riffled independence assumption posits that rankings over the item set  are generated by independently generating rankings of smaller disjoint item subsets  say  a and
b  which partition   and piecing together a full ranking by interleaving  or riffle shuffling 
these smaller rankings together  for example  to rank our item set of foods  one might first
rank the vegetables and fruits separately  then interleave the two subset rankings to form a
full ranking  to formally define riffled independence  we use the notions of relative rankings
and interleavings 
definition    relative ranking map   given a ranking   s and any subset a    the
relative ranking of items in a  a     is a ranking    sa   such that  i     j  if and
only if  i     j  
definition    interleaving map   given a ranking   s and a partition of  into disjoint
sets a and b  the interleaving of a and b in   denoted  ab     is a  binary  mapping
from the rank set r               n  to  a  b  indicating whether a rank in  is occupied by a
or b  as with rankings  we denote the interleaving of a ranking by its vertical bar notation 
 ab         ab                 ab     n  
example    consider a partitioning of an item set  into vegetables a    corn  p eas 
and fruits b    apples  oranges   as well as a full ranking over these four items    
corn oranges p eas apples  in this case  the relative ranking of vegetables in  is a     
corn p eas and the relative ranking of fruits in  is b      oranges apples  the interleaving of vegetables and fruits in  is ab      a b a b 
definition    riffled independence   let h be a distribution over s and consider a subset
of items a   and its complement b  the sets a and b are said to be riffle independent
if h decomposes  or factors  as 
h     mab  ab      fa  a      gb  b     
for distributions mab   fa and gb   defined over interleavings and relative rankings of a and
b respectively  in other words  a and b are riffle independent if the relative rankings of
a and b  as well as their interleaving are mutually independent  we refer to mab as the
interleaving distribution and fa and gb as the relative ranking distributions 
riffled independence has been found to approximately hold in a number of real datasets
 huang   guestrin         when such relationships can be identified in data  then instead
of exhaustively representing all n  ranking probabilities  one can represent just the factors
mab   fa and gb   which are distributions over smaller sets 
    hierarchical riffle independent models
the relative ranking factors fa and gb are themselves distributions over rankings  to further
reduce the parameter space  it is natural to consider hierarchical decompositions of item sets
into nested collections of partitions  like hierarchical clustering   for example  figure    
shows a hierarchical decomposition where vegetables are riffle independent of fruits among
the healthy foods  and these healthy foods are  in turn  riffle independent of the subset of
desserts   doughnuts  m  m s  
   

fiefficient inference with partial rankings

 c p a o d m 

 d m 

 c p a o 

doughnuts  m ms

 c p 

 a o 

corn  peas

apples  oranges

figure    an example of a hierarchy over six food items 
for simplicity  we restrict consideration to binary hierarchies  defined as tuples of the
form h    ha   hb    where ha and hb are either     null  in which case h is called a leaf 
or     hierarchies over item sets a and b respectively  in this second case  a and b are
assumed to form a nontrivial partitioning of the item set 
definition    we say that a distribution h factors riffle independently with respect to a
hierarchy h    ha   hb   if item sets a and b are riffle independent with respect to h 
and both fa and gb factor riffle independently with respect to subhierarchies ha and hb  
respectively 
like bayesian networks  these hierarchies represent families of distributions obeying a
certain set of  riffled  independence constraints and can be parameterized locally  to draw
from such a model  one generates full rankings recursively starting by drawing rankings of
the leaf sets  then working up the tree  sequentially interleaving rankings until reaching the
root  the parameters of these hierarchical models are simply the interleaving and relative
ranking distributions at the internal nodes and leaves of the hierarchy  respectively 
in general  the number of total parameters required to represent a hierarchical riffle
independent model can  as with bayesian networks  still scale exponentially in the number

of items  for example  the number of interleavings of p items with n  p items is np  
it is often the case however  that much fewer parameters are necessary  for example  thin
models  huang   guestrin         in which the number of items factored out of the model at
each stage of the hierarchy is never more than a small constant k  can always be represented
with a  degree k  polynomial number of parameters  we will use  h  to refer to the number
of parameters necessary for representing a distribution which factors according to hierarchy
h 
by decomposing distributions over rankings into small pieces  like bayesian networks
have done for other distributions   these hierarchical models allow for better interpretability 
efficient probabilistic representation  low sample complexity  efficient map optimization 
and  as we show in this paper  efficient inference 
example    in figure   a   we reproduce the hierarchical structure that was learned using
a fully ranked subset of the apa data consisting of      training examples in huang and
guestrin         there were five candidates in the election      william bevan      ira
iscoe      charles kiesler      max siegle  and     logan wright  marden         strikingly 
the structure that is learned using an algorithm  maximum likelihood  which knows nothing
about the underlying politics of the apa  has leaf nodes which correspond exactly to the
political coalitions that dominated the apa in the      election  the research psychologists
   

fihuang  kapoor   guestrin

a        

b       

c    
community
psychologists

mb c  
   

b c b b b

   

b b c b b

   

b b b c b

   

b b b b c

   



md e  



fc  

d d e e

   

 

    

d e d e

   

d e e d

   

d     

e     

e d d e

   

research
psychologists

clinical
psychologists

e d e d

   

e e d d

   

 a  hierarchical structure learned
via mle using      full rankings
from the apa dataset 


c b b b b



fd  



fe  

   

   

   

   

   

   

   

   

 b  riffle independent model parameters learned via mle using
     full rankings from the apa dataset 

figure    example hierarchical model for the apa election  candidates are enumerated
as      william bevan      ira iscoe      charles kiesler      max siegle  and    
logan wright  marden        

 candidates   and     the clinical psychologists  candidates   and     and the community
psychologists  candidate    
in figure   b   we plot the corresponding parameter distributions that are learned via
maximum likelihood  there are three relative ranking distributions  each corresponding to a
political party  as well as two interleaving distributions  one for the interleaving of research
and clinical psychologists  and one for the interleaving of the community psychologist and all
remaining candidates   since each parameter distribution is constrained to sum to    there
are a total of    free parameters 
    model estimation
in this paper we estimate riffle independent models based on the methods introduced in our
earlier work  given the hierarchial structure of a model  the maximum likelihood parameter
estimates of a hierarchical riffle independent model are straightforward to compute via frequency estimates  but how to estimate the correct structure of a model is a more challenging
problem  the key insight lies in noticing that if two subsets a and b are riffle independent 
then for any i  a and j  k  b  the independence relation  i     j     k   must hold 
our structure learning algorithms operate by hunting for these tripletwise independence
relations within the data  we defer interested readers to the details in  huang   guestrin 
      
   

fiefficient inference with partial rankings

note that in our earlier work  we assumed that our algorithms have access to a dataset
consisting of i i d  full rankings provided by users  in the current work  we will relax our
assumptions by allowing for users to provide partially ranked data  one assumption throughout  however  is that each user has a full ranking in mind over the items  in particular  our
current work does not address the incomplete ranking problem  in which users might not
have seen all of the items  we discuss possible extensions to the incomplete ranking setting
in section   

   decomposable observations
given a prior distribution  h  over rankings and an observation o  bayes rule tells us that
the posterior distribution  h  o   is proportional to l o    h    where l o   is the
likelihood function  this operation of conditioning h on an observation o is typically computationally intractable since it requires multiplying two n  dimensional functions  unless
one can exploit structural decompositions of the problem  in this section  we describe a decomposition for a certain class of likelihood functions over the space of rankings in which the
observations are factored into simpler parts  when an observation o is decomposable in
this way  we show that one can efficiently condition a riffle independent prior distribution on
o  for simplicity in this paper  we focus primarily on subset observations whose likelihood
functions encode membership with some subset of rankings in sn  
definition    subset observations   a subset observation o is a binary observation whose
likelihood is proportional to the indicator function of some subset of sn  i e  

  if   o
 
l o    
  otherwise
as a running example  we will consider the class of first place observations throughout
the chapter  we will consider far more general observation models in later sections   the first
place observation o  corn is ranked first  for example  is associated with the collection of
rankings placing the item corn in first place  o        corn         we are interested in
computing the posterior h    o   thus in the first place scenario  we are given a voters
top choice and we would like to infer his preferences over the remaining candidates 
given a partitioning of the item set  into two subsets a and b  it is sometimes possible
to decompose  or factor   a subset observation involving items in  into smaller subset observations involving a  b and the interleavings of a and b independently  such decompositions
can often be exploited for efficient inference 
example   
 consider the first place observation
o   corn is ranked first 
which can be decomposed into two independent observations  an observation on the
relative ranking of vegetables  and an observation on the interleaving of vegetables and
fruits 
   

fihuang  kapoor   guestrin

 oa   corn is ranked first among vegetables 
 oa b   first place is occupied by a vegetable 
to condition on o in this case  one updates the relative ranking distribution over
vegetables  a  by zeroing out rankings of vegetables which do not place corn in first
place  and updates the interleaving distribution by zeroing out interleavings which do
not place a vegetable in first place  then normalizes the resulting distributions 
 an example of a nondecomposable observation is the observation
o   corn is in third place 
to see that o does not decompose  with respect to vegetables and fruits   it is enough
to notice that the interleaving of vegetables and fruits is not independent of the relative
ranking of vegetables  if  for example  an element   o interleaves a  vegetables 
and b  fruits  as ab      a b a b  then since  corn       the relative ranking
of vegetables is constrained to be a      p eas corn  since the interleavings and
relative rankings are not independent  we see that o cannot be decomposable 
formally  we use riffle independent factorizations to define decomposability with respect
to a hierarchy h of the item set 
definition    decomposability   given a hierarchy h over the item set  a subset observation o decomposes with respect to h if its likelihood function l o   factors riffle
independently with respect to h 
when subset observations and the prior decompose according to the same hierarchy  we
can show  as in example    that the posterior also decomposes 
proposition     let h be a hierarchy over the item set  given a prior distribution h and
a subset observation o which both decompose with respect to h  the posterior distribution
h  o  also factors riffle independently with respect to h 
proof  denote the likelihood function corresponding to o by l  in this proof  it does not
matter that o is assumed to be a subset observation  the result holds for arbitrary
likelihoods  
we use induction on the size of the item set n       the base case n     is trivially
true  next consider the general case where n      the posterior distribution  by bayes rule 
can be written h  o   l    h    there are now two cases  if h is a leaf node  then
the posterior h  trivially factors according to h  and we are done  otherwise  l and h both
factor  by assumption  according to h    ha   hb   in the following way 
l     ml  ab    fl  a    gl  b      and h     mh  ab    fh  a    gh  b     
multiplying and grouping terms  we see that the posterior factors as 
h  o     ml  mh   ab       fl  fh   a       gl  gh   b     
to show that h  o  factors with respect to h  we need to demonstrate  by definition   
that the distributions  fl  fh   and  gl  gh    after normalizing  factor with respect to ha and
   

fiefficient inference with partial rankings

hb   respectively  since fl and fh both factor according to the hierarchy ha by assumption
and  a    n since h is not a leaf  we can invoke the inductive hypothesis to show that the
posterior distribution  which is proportional to fl  fh must also factor according to ha  
similarly  the distribution proportional to gl  gh must factor according to hb  

   complete decomposability
the condition of proposition     that the prior and observation must decompose with respect to exactly the same hierarchy  is a sufficient one for efficient inference  but it might at
first glance seem so restrictive as to render the proposition useless in practice  to overcome
this limitation of hierarchy specific decomposability  we explore a special family of observations  which we call completely decomposable  for which the property of decomposability
does not depend specifically on a particular hierarchy  implying in particular that for these
observations  efficient inference is always possible  provided that efficient representation of
the prior distribution is also possible  
to illustrate how an observation can decompose with respect to multiple hierarchies over
the item set  consider again the first place observation o  corn is ranked first  we argued
in example   that o is a decomposable observation  notice however that decomposability
for this particular observation does not depend on how the items are partitioned by the
hierarchy  specifically  if instead of vegetables and fruits  the sets a    corn  apples  and
b    p eas  oranges  are riffle independent  a similar decomposition of o would continue
to hold  with o decomposing as an observation on the relative ranking of items in a  corn is
first among items in a   and an observation on the interleaving of a and b  first place is
occupied by some element of a  
to formally capture this notion that an observation can decompose with respect to
arbitrary underlying hierarchies  we define complete decomposability 
definition     complete decomposability   we say that a subset observation o is completely decomposable if it decomposes with respect to every possible hierarchy over the item
set   we denote the collection of all possible completely decomposable  subset  observations as c  see figure   for an illustration of the set c 
conceptually  completely decomposable observations correspond to indicator functions
that are as riffle independent as possible  complete decomposability is a guarantee for an
observation o that one can always exploit any available factorized structure of the prior
distribution in order to efficiently condition on o 
proposition     let h be any binary hierarchy over the item set  given a prior h which
factorizes with respect to h  and a completely decomposable observation o  the posterior
h  o  also decomposes with respect to h 
proof  proposition    follows as a simple corollary to proposition    
example     the simplest example of a completely decomposable observation is the uniform observation ounif   s   which includes all possible rankings and corresponds to a
uniform indicator function unif over rankings  given any hierarchy h  unif can be shown
to decompose riffle independently with respect to h  where each factor is also uniform  and
hence ounif is completely decomposable 
   

fihuang  kapoor   guestrin

h 

h 

h 

completely
decomposable
observations

h 

h 

h 

figure    a diagram illustrating the collection of completely decomposable observations  c 
each shaded region  labeled hi   above represents the family of subset observations
over sn which decompose with respect to the hierarchy hi   the collection c can be
seen as the intersection over all such shaded regions  and subset observations which
lie inside of this intersection are ones for which conditioning can be performed in
linear time  in the number of model parameters  

the uniform observation is of course not particularly interesting in the context of bayesian
inference  but on the other hand  given the stringent conditions in definition     it is not
obvious that nontrivial completely decomposable observations can even exist  nonetheless 
there do exist nontrivial examples  such as the first place observations   and in the next
section  we exhibit a rich and general class of completely decomposable observations 

   complete decomposability of partial ranking observations
in this section we discuss the mathematical problem of fully characterizing the class of
completely decomposable observations  our main contribution in this section is to show
that completely decomposable observations correspond precisely to partial rankings of the
item set 
partial rankings  we begin our discussion by introducing partial rankings  which allow
for items to be tied with respect to a ranking  by dropping verticals from the vertical bar
representation of  
definition     partial ranking observation   let                r be an ordered collection
of subsets which partition   i e   i i    and i  j    if i    j   the partial ranking
observation   corresponding to this partition is the collection of rankings which rank items
   as remarked by ailon         we note that the term partial ranking used here should not be confused
with two other standard objects      partial order  namely  a reflexive  transitive anti symmetric binary

   

fiefficient inference with partial rankings

in i before items in j if i   j  we denote this partial ranking as               r and say
that it has type                          r     we denote the collection of all partial rankings
 over n items  as p 
each partial ranking as defined above can be viewed as a coset of the subgroup s  
s   s       sr   given the type  and any full ranking   s   there is only one
partial ranking of type  containing   thus we will therefore equivalently denote the partial
ranking               r as s   where  is any element of               r   note that this coset
notation allows for multiple rankings  to refer to the same partial ranking s  
the space of partial rankings as defined above captures a rich and natural class of
observations  in particular  partial rankings encompass a number of commonly occurring
special cases  which have traditionally been modeled in isolation  but in our work  as well
as recent works such as lebanon   lafferty        lebanon   mao        can be used in a
unified setting 
example     partial ranking observations include 
  first place  or top   observations   first place observations correspond to partial
rankings of type        n      the observation that corn is ranked first can be
written as corn peas apples oranges 
  top k observations   top k observations are partial rankings with type                   n
k   these generalize the first place observations by specifying the items mapping to the
first k ranks  leaving all n  k remaining items implicitly ranked behind  for example 
the observation that corn is ranked first and peas is ranked second can be written as
corn peas apples oranges 
  desired less desired dichotomy   partial rankings of type     k  n  k  correspond to
a subset of k items being preferred or desired over the remaining subset of n  k items 
for example  partial rankings of type  k  n  k  might arise in approval voting in which
voters mark the subset of approved candidates  implicitly indicating disapproval of the
remaining n  k candidates 
  ratings   finally  partial rankings can come in the form of rating data where  for
example  restaurants are rated as         or        a corresponding partial ranking
would thus tie restaurants that are rated with the same number of stars  while ranking
restaurants with more stars above restaurants with fewer stars 
  trivial observations   partial rankings of type     n  refer to trivial observations
whose likelihood functions are uniform on the entire space of rankings  s   the trivial
observation for rankings of the item set     corn  p eas  apples   for example  can
simply be written simply as corn  p eas  apples 
to show how partial ranking observations decompose  we will exhibit an explicit factorization with respect to a hierarchy h over items  for simplicity  we begin by considering the
single layer case  in which the items are partitioned into two leaf sets a and b  our factorization depends on the following notions of consistency of relative rankings and interleavings
with a partial ranking 
relation  and     a ranking of a subset of   which we discuss in section   as incomplete rankings   in
search engines  for example  although only the top k elements of  are returned  the remaining n  k
are implicitly assumed to be ranked behind  and therefore  search engines return partial rankings  

   

fihuang  kapoor   guestrin

definition     restriction consistency   given a partial ranking s                  r and
any subset a    we define the restriction of s  to a as the partial ranking on items in
a obtained by intersecting each i with a  hence the restriction of s  to a is 
 s  a      a    a         r  a 
given a ranking  a of items in a  we say that a is consistent with the partial ranking
s  if a is a member of the restriction of s  to a   s  a  
definition     interleaving consistency   given an interleaving ab of two sets a  b which
partition   we say that ab is consistent with a partial ranking s               r  with
type   if the first   entries of ab contain the same number of as and bs as     and the
second   entries of ab contain the same number of as and bs as     and so on  given a
partial ranking s   we denote the collection of consistent interleavings as  s  ab  
for example  consider the partial ranking
s    corn  apples p eas  oranges 
which places a single vegetable and a single fruit in the first two ranks  and a single vegetable
and a single fruit in the last two ranks  alternatively  s  partially specifies an interleaving
ab ab  the full interleavings a b b a and b a b a are consistent with s   by dropping
vertical lines  while a a b b is not consistent  since it places two vegetables in the first two
ranks  
using the notions of consistency with a partial ranking  we show that partial ranking
observations are decomposable with respect to any binary partitioning  i e   single layer
hierarchy  of the item set 
proposition     single layer hierarchy   for any partial ranking observation s  and any
binary partitioning of the item set  a  b   the indicator function of s   s    factors riffle
independently as 
s       mab  ab      fa  a      gb  b     

     

where the factors mab   fa and gb are the indicator functions for consistent interleavings
and relative rankings   s  ab    s  a and  s  b   respectively 
the single layer decomposition of proposition    can be turned into a recursive decomposition for partial ranking observations over arbitrary binary hierarchies  which establishes
our main result  in particular  given a partial ranking s  and a prior distribution which
factorizes according to a hierarchy h  we first condition the topmost interleaving distribution by zeroing out all parameters corresponding to interleavings which are not consistent
with s   and normalizing the distribution  we then need to condition the subhierarchies
ha and hb on relative rankings of a and b which are consistent with s   respectively 
since these consistent sets   s  a and  s  b   are partial rankings themselves  the same
algorithm for conditioning on a partial ranking can be applied recursively to each of the
subhierarchies ha and hb   to be precise  we show that 
theorem     every partial ranking is completely decomposable  p  c  
   

fiefficient inference with partial rankings

prcondition  prior hprior   hierarchy h  observation s                  r  
if isleaf h  then
forall  do

hprior    if   s 
hpost    
 
 
otherwise
normalize  hpost    
return  hpost   
else
forall  do

mprior     if    s  ab
mpost     
 
 
otherwise
normalize  mpost    
f  a   prcondition  fprior   ha    s  a    
g b   prcondition  gprior   hb    s  b    
return  mpost   fpost   gpost   

algorithm    pseudocode for prcondition  an algorithm for recursively conditioning a hierarchical riffle independent prior distribution on partial ranking observations  see definitions   
and    for  s  a    s  b   and  s  ab   the runtime of prcondition is o n   h    where  h 
is the number of model parameters  input  all parameter distributions of the prior hprior represented in explicit tabular form  and an observation s  in the form of a partial ranking  output 
all parameter distributions of the posterior hpost represented in explicit tabular form 

since the proof of theorem    is fairly straight forward given the form of the factorization  equation       it is deferred to the appendix  as a consequence of theorem    and
proposition     conditioning on partial ranking observations can be performed efficiently 
see algorithm   for details on our recursive conditioning algorithm 
what is the running time complexity of conditioning on a partial ranking  the recursion
of algorithm   operates on each parameter distribution once  setting the probabilities of
the interleavings or relative rankings in each such distribution to either zero or not  then
normalizing  to decide whether to zero out a probability or not  one must check a partial
ranking for consistency against either an interleaving or relative ranking  which requires at
most o n  time  therefore  in total  algorithm   requires o n   h   time  where  h  is
the total number of model parameters  notice that the complexity of conditioning depends
linearly on the complexity of the prior  whenever the prior distribution can be compactly
represented  efficient inference for partial ranking observations is also possible  as we have
stated in section     h  can in general scale exponentially in n  but for thin chain models 
in which the number of items factored out of the model at each stage is never more than
a small constant k  verifying interleaving or relative ranking consistency can be performed
in constant time  implying that the conditioning operation is linear in the number of model
parameters  and guaranteed to be polynomial in n 
example     in this example  we consider conditioning the apa distribution from example   on the observation o that candidate   is ranked in first place  which can also
be represented as the partial ranking o                 recall that candidate   was charles
kiesler  who was a research psychologist 
in figure   a  we show again the structure and parameters of the prior distribution for
the apa election data  highlighting in particular the interleavings and relative rankings which
   

fihuang  kapoor   guestrin



mb c  



c b b b b

   

c b b b b

mb c  
 

b c b b b

   

b c b b b

   

b b c b b

   

b b c b b

   

b b b c b

   

b b b c b

   

b b b b c

   

b b b b c

   



md e  



fc  



md e  



fc  

d d e e

   

 

    

d d e e

   

 

    

d e d e

   

d e d e

   

d e e d

   

d e e d

   

e d d e

   

e d d e

 

e d e d

   

e d e d

 

e e d d

   

e e d d

 



fd  



fe  



fd  



fe  

   

   

   

   

   

 

   

   

   

   

   

   

   

    

   

   

 a  structure and parameters of the prior distribution  with consistent relative rankings and interleavings highlighted  

 b  structure and parameters of the posterior distribution after conditioning 

figure    example of conditioning the apa hierarchy  from example    on the first place
observation that candidate   is ranked in first place 

are consistent with o  for example  of the possible interleavings of research psychologists
 d  with clinical psychologists  e   the interleavings that are consistent with o are those
which rank a research psychologist first among the research and clinical psychologists  there
are therefore only three consistent interleavings  d d e e  d e d e  and d e e d 
conditioning on o sets all relative rankings and interleavings which are not consistent
with o to zero and normalizes each resulting parameter distribution  the resulting riffle
independent representation of the posterior distribution is shown in figure   b  

    an impossibility result
it is interesting to consider what completely decomposable observations exist beyond partial
rankings  one of our main contributions is to show that there are no such observations 
theorem     converse of theorem      every completely decomposable observation takes
the form of a partial ranking  c  p  
together  theorems    and    form a significant insight into the nature of rankings 
showing that the notions of partial rankings and riffled independence are deeply connected 
in fact  our result shows that it is even possible to define partial rankings via complete
decomposability 
as a practical matter  theorem    shows that there is no algorithm based on simple
multiplicative updates to the parameters which can exactly condition on observations which
do not take the form of partial rankings  the computational complexity of conditioning on
observations which are not partial rankings remains open  we conjecture that approximate
inference approaches may be necessary for efficiently handling more complex observations 
   

fiefficient inference with partial rankings

    proof of the impossiblity result  theorem    
we now turn to proving theorem     since this proof is significantly longer and less obvious
than the proof for its converse  theorem      we sketch the main ideas that drive the proof
here and refer interested readers to details in the appendix 
recall that the definition of the linear span of a set of vectors in a vector space is the
intersection of all linear subspaces containing that set of vectors  to prove theorem     we
introduce analogous concepts of the span of a set of rankings 
definition     rspan and pspan   let x  sn be any collection of rankings  we define
pspan x  to be the intersection of all partial rankings containing x  similarly  we define
rspan x  to be the intersection of all completely decomposable observations containing x 
more formally 
 
 
pspan x   
s   and rspan x   
o 
o xo  oc

s  xs 

for example  if x    corn p eas apples  apples p eas corn   it can be checked that
the only partial ranking of all three items containing both items of x is the entire set itself 
thus pspan x    corn  p eas  apples 
our proof strategy is to establish two claims      that the pspan of any set is always
a partial ranking  and     that in fact  the rspan and pspan of a set x are exactly the
same sets  since claim     is a fact about partial rankings and does not involve riffled
independence  we defer all related proofs to the appendix  thus we have 
lemma     for any x  sn   pspan x  is a partial ranking 
proof  see appendix 
the following discussion will instead sketch a proof of claim      we first show  however 
that theorem    must hold if it is indeed true that claims     and     hold 
proof   of theorem      given some o  c  we want to show that o  p  by claim
     rspan o    pspan o   since o is an element of c  however  we also have that
o   rspan o   and thus that o   pspan o   finally lemma     claim      guarantees
that pspan o  is a partial ranking  and so we conclude that o  p 
we now proceed to establish the claim that rspan x    pspan x   the following
proposition lists several basic properties of the rspan that we will use in several of the
proofs  they all follow directly from definition so we do not write out the proofs 
proposition    
i   monotonicity  for any x  x  rspan x  
ii   subset preservation  for any x  x   such that x  x     rspan x   rspan x     
iii   idempotence  for any x  rspan rspan x     rspan x  
one inclusion of our proof that rspan x    pspan x  follows directly from the fact
that p  c  theorem     
   

fihuang  kapoor   guestrin

formpspan x 
x   x  t    
while s   s       xt which disagree on the relative ordering of items a    a  do
xt    
foreach s   xt do
add any partial ranking obtained by deleting a vertical bar from s  between items
a  and a  to xt  
t  t     
return  any element of xt    

algorithm    pseudocode for computing pspan x   formpspan x  takes a set of partial
rankings  or full rankings  x as input and outputs a partial ranking  this algorithm iteratively
deletes vertical bars from elements of x until they are in agreement  note that it is not necessary
to keep track of t  but we do so here to ease notation in the proofs  nor is this algorithm the most
direct way of computing pspan x   but again  it simplifies the proof of our main theorem 

lemma     for any subset of orderings  x  rspan x   pspan x  
proof  fix a subset x  sn and let  be any element of rspan x   we would like to show
 to be an element of pspan x   consider any partial ranking s   p which covers x
 i e       s  for all     x   we want to see that   s   by theorem     p  c 
and therefore  s   c  since   rspan x   and     s  for all     x  we conclude 
by definition of rspan  that   s   since this holds for any partial ranking covering x 
  pspan x  
what remains is the task of establishing the reverse inclusion 
proposition     for any subset of orderings  x  rspan x   pspan x  
to prove proposition     we consider the problem of computing the partial ranking span
 pspan  of a given set of rankings x  in algorithm    we show a simple procedure based
on iteratively finding rankings in x which disagree on the pairwise ranking of two items 
and replacing those rankings by a partial ranking in which a vertical bar between those two
elements have been removed  we show that this algorithm provably outputs the correct
result 
proposition     given a set of rankings x as input  algorithm   outputs pspan x  
proof  see appendix 
as a final step before being able to prove proposition     we prove the following two
technical lemmas which relate the computation of the pspan in algorithm   to riffled independence  and really form the heart of our argument  in particular  for a completely
decomposable observation o  c  lemma    below shows how a ranking contained in o
can force other rankings to also be contained in o 
lemma     let o  c and suppose there exist        o which disagree on the relative
ranking of items i  j    then the ranking obtained by swapping the relative ranking of
items i  j within any    o must also be contained in o 
   

fiefficient inference with partial rankings

proof  let h be the indicator distribution corresponding to the observation o  we will
show that swapping the relative ranking of items i  j in   will result in a ranking which is
assigned nonzero probability by h  thus showing that this new ranking is contained in o 
let a    i  j  and b    a  since o  c  h must factor riffle independently according
to the partition  a  b   thus 
h       m ab        f  a        g b            and
h       m ab        f  a        g b           
since   and   disagree on the relative ranking of items in a  this factorization implies in
particular that both f  a   i j      and f  a   j i       since h          it must also
be that each of m ab        f  a        and g b       have positive probability  we can
therefore swap the relative ranking of a  a   to obtain a new ranking which has positive
probability since all of the terms in the decomposition of this new ranking have positive
probability 
lemma    below provides conditions under which removing a vertical bar from one of
the rankings in x will not change the support of a completely riffle independent distribution  to illustrate with an example  consider a completely decomposable observation o
which contains the partial ranking s    corn  p eas apples  oranges as a subset  what
lemma    guarantees is that  if  in addition  there exists any element  in o which disagrees
with s  on the relative ordering of  say  p eas and oranges  then in fact the partial ranking
s        corn  p eas  apples  oranges  with the bar removed from s   must also be a
subset of o  formally 
lemma     let s               i  i            k be a partial ranking on item set   and
s                   i  i            k   the partial ranking in which the sets i and i   are
merged  let a   ij   j and a   kj i   j   if o is any element of c such that s   o
and there additionally exists a ranking   o which disagrees with s  on the relative
ordering of a    a    then s       o 
proof  the key strategy in our proof of lemma    is to argue that large subsets of rankings
must be contained in a completely decomposable observation o by decomposing rankings
into transpositions and invoking the technical lemma from above  lemma     repeatedly 
see the appendix for details 
we now can use lemma    to show that the reverse inclusion of proposition    also
holds  establishing that the two sets rspan x  and pspan x  are in fact equal and thereby
proving the desired result  that c  p 
proof   of proposition     at each iteration t  algorithm   producess
a set of partial rankings 
xt   we denote the union of all partial rankings at time t as xt  s xt s   note that
x    x and xt   pspan x   the idea of our proof will be to show that at each iteration
t  the following set inclusion holds  rspan xt    rspan xt     if indeed this holds  then
   

fihuang  kapoor   guestrin

after the final iteration t   we will have shown that 
pspan x    xt  

 proposition    

 rspan xt   

 monotonicity  proposition    

 rspan x    

 since rspan xt    rspan xt     shown below  

 rspan x 

 x    x  see algorithm   

which would prove the proposition 
it remains now to show that rspan xt    rspan xt     we claim that xt  rspan xt    
let   xt   if   xt    then since xt   rspan xt     we have   rspan xt    and
the proof is done  otherwise    xt  xt    in this second case  we use the fact that
at iteration t  the vertical bar between i and i   was deleted from the partial ranking s               i  i            k  which is a subset of xt    to form the partial ranking
s                   i  i            k    which is a subset of xt    furthermore  in order for the
vertical bar to have been deleted by the algorithm  there must have existed some partial
ranking  and therefore some full ranking      that disagreed with s  on the relative ordering of items a    a  on opposite sides of the bar  since   xt  xt  we can assume that
  s       
we now would like to apply lemma     note that for any o  c such that xt   o 
we also have s   o  since s   xt    an application of lemma    then shows that
s       o and therefore that   o 
we have shown in fact that   o holds for any observation o  c such that xt  
o  and therefore taking the intersection of supports over all o  c  we see that xt 
rspan xt     taking the rspan of both sides yields 
rspan xt    rspan rspan xt     
 rspan xt    

 subset preservation  proposition    

 idempotence  proposition    

    going beyond subset observations
though we have stated all of our results so far for subset observations  we now comment
on what our theory would look like if we had considered general likelihood functions  in
order to avoid confusion  we here refer to a more general class of functions that we call completely decomposable functions  instead of the completely decomposable subset observations
of definition    
definition     a function h   sn  r is called a completely decomposable function if it
factors riffle independently with respect to every hierarchy over the item set   we denote
e
the collection of all possible completely decomposable functions as c 
e are very nearly the same  it is quite simple to restate theorem   
as we discuss  c and c
with respect to the general case of completely decomposable functions 
theorem  every partial ranking indicator function is a completely decomposable function 
   

fiefficient inference with partial rankings

unfortunately  the proof of its converse  theorem     does not easily generalize  and
instead can only be used to show that the support     sn   h         of every completely
decomposable function is a partial ranking  it is natural  however  to suspect that a full
converse does indeed exist  that every completely decomposable function is proportional to
the indicator function of some partial ranking  in fact  this suspected converse only almost
holds  we have 
theorem  if h is any completely decomposable function supported on a partial ranking
s               r where  i    
    for all i              r  then h is proportional to the indicator
function on s  
proof  see appendix 
example     for completely decomposable functions  it is not possible to do away with the
assumption that  i    
    for all i  as an example  the function defined below as 

     if    corn p eas apples
    if    p eas corn apples  
h    

 
otherwise
is supported on the partial ranking s    corn  p eas apples  where            and is not
proportional to any indicator function  i e   it is not uniform on rankings which are not
assigned positive probability  
however  it is still possible to show that h is a completely decomposable function  to
prove so  it is necessary to establish only three things  that  corn  p eas  and  apples 
are riffle independent  that  corn  apples  and  p eas  are riffle independent  and that
 p eas  apples  and  corn  are riffle independent  for example  with respect to the partitioning into sets a    corn  apples  and b    p eas   we see that
h     m ab      f  a      g b     
where 

    
   
m ab    

 

if ab   a b a
if ab   b a a  
if ab   a a b


f  a    

 
 

if  ac    a c
 
otherwise

g b       

therefore  when  i        it is possible to have completely decomposable functions which
are not uniform on their supports 
    conditioning on noisy observations
we conclude this section with a remark on handling noise in observations  while we have
assumed in this paper that observed partial rankings are always consistent with a users
underlying full ranking  there are situations in which one may wish to model a noisier
setting  where the partial rankings may be misreported with some small probability  a
natural model that accounts for noise  for example  might be 

    if   o
l o    
 
     

 o   otherwise
   

fihuang  kapoor   guestrin

if a prior distribution factorizes with respect to a hierarchy h  then conditioning on the
noisy likelihood of equation     results in a posterior distribution which can be written
as a weighted mixture of the prior distribution and the posterior that would have resulted
from conditioning on a noise free observation  while each component of this posterior
distribution factorizes with respect to h  the mixture itself does not factor in general  and
should not factor according to our theory   as a result  iteratively conditioning on multiple
partial rankings according to the noisy likelihood function above would quickly lead to
an unmanageable number of mixture components  we therefore believe that approximate
inference methods for conditioning on multiple noisy partial ranking observations is a fruitful
area for further research 

   model estimation from partially ranked data
in many ranking based applications  datasets are predominantly composed of partial rankings rather than full rankings due to the fact that for humans  partial rankings are typically
easier and faster to specify  in addition  many datasets are heterogeneous  containing partial
ranking of different types  for example  in the american psychological assoication as well
as the irish house of parliament elections  voters are allowed to specify their top k candidate
choices for any value of k  see figures   a  and   b    in this section we use the efficient
inference algorithm proposed in section   for estimating a riffle independent model from
partially ranked data  because estimating a model using partially ranked data is typically
considered to be more difficult than estimating one using only full rankings  a common practice  e g   see huang   guestrin        has been to simply ignore the partial rankings in a
dataset  the ability of a method to incorporate all of the available data however  can lead
to significantly improved model accuracy as well as wider applicability of that method  in
this section  we propose the first efficient method for estimating the structure and parameters
of a hierarchical riffle independent model from heterogeneous datasets consisting of arbitrary
partial ranking types  central to our approach is the idea that given someones partial preferences  we can use the efficient algorithms developed in the previous section to infer his full
preferences and consequently apply previously proposed algorithms which are designed to
work with full rankings 
    censoring interpretations of partial rankings
the model estimation problem for full rankings is stated as follows  given i i d  training
examples                  m   consisting of full rankings  drawn from a hierarchical riffle independent distribution h  recover the structure and parameters of h 
in the partial ranking setting  we again assume i i d  draws  but that each training
example   i  undergoes a censoring process producing a partial ranking consistent with   i   
for example  censoring might only allow for the ranking of the top k items of   i  to be
observed  while we allow for arbitrary types of partial rankings to arise via censoring  we
make a common assumption that the partial ranking type resulting from censoring   i  does
not depend on   i  itself 
   

fiefficient inference with partial rankings

    algorithm
we treat the model estimation from partial rankings problem as a missing data problem  as
with many such problems  if we could determine the full ranking corresponding to each observation in the data  then we could apply algorithms which work in the completely observed
data setting  since full rankings are not given  we utilize an expectation maximization  em 
approach in which we use inference to compute a posterior distribution over full rankings
given the observed partial ranking  in our case  we then apply the algorithms from huang
and guestrin              which were designed to estimate the hierarchical structure of a
model and its parameters from a dataset of full rankings 
given an initial model h and a collection of training examples  o      o              o m   
consisting of partial rankings  our em based approach alternates between the following two
steps until convergence is achieved 
  e step   for each observation  o i    s  i    i    in the training examples  we use
inference to compute a posterior distribution over the full ranking  that could have
generated o i  via censoring  h  o i    s  i    i     since the observations take the
form of partial rankings and are hence completely decomposable  we use the efficient
algorithms in section   to perform the e step 
  m step   in the m step  one maximizes the expected log likelihood of the training
data with respect to the model  when the hierarchical structure of the model has been
provided  or is known beforehand  our m step can be performed using standard methods for optimizing parameters  when the structure is unknown  we use a structural
em approach  which is analogous to methods from the graphical models literature for
structure learning from incomplete data  friedman              
unfortunately  the  riffled independence  structure learning algorithm of huang and
guestrin        is unable to directly use the posterior distributions computed from
the e step  instead  observing that sampling from riffle independent models can be
done efficiently and exactly  as opposed to  for example  mcmc methods   we simply
sample full rankings from the posterior distributions computed in the e step and
pass these full rankings into the structure learning algorithm of huang and guestrin
        the number of samples that are necessary  instead of scaling factorially  scales
according to the number of samples required to detect riffled independence  which
under mild assumptions is polynomial in n  huang   guestrin        

   related work
rankings and permutations have recently become an active area of research in machine
learning due in part to the hinge role that they play in information retrieval and preference
elicitation  algorithms such as the ranksvm  joachims        and rankboost  freund 
iyer  schapire    singer         for example  have been successful in the large scale ranking
problems that appear in web search  the main aims of our work differ from these web
scale settings however  instead of seeking a single optimal ranking with respect to some
objective function  we seek an understanding of a large collection of rankings via density
estimation  in the following  we outline two major lines of research which have influenced
our work 
   

fihuang  kapoor   guestrin

    additive and multiplicative decompositions
our paper builds in particular upon a thread of recent work on tractable models for permutation data based on function decompositions  kondor  howard  and jebara        and
huang  guestrin  and guibas              considered additive decompositions of a distribution into a weighted sum of fourier basis functions  these papers show that low frequency
fourier assumptions can often be effective for coping with the representational complexity
of working with distributions over permutations  they show in particular that conditioning
prior distributions on the low frequency likelihood functions that often arise in multiobject
tracking problems can be performed especially efficiently 
unfortunately  low frequency assumptions are not as applicable for distributions defined
over rankings  and to address ranking problems specifically  huang and guestrin       
      introduced the concept of riffled independence as a useful generalization of probabilistic independence for rankings  using multiplicative decompositions based on riffled
independence  we showed that it is possible to learn the hierarchical structure of a model
given a fully ranked dataset  while our previous papers on the topic of riffled independence
focused more on problems related to efficiently representing distributions  the main focus of
our current paper lies in efficient reasoning inference and tackling human task complexity
by considering partial rankings 
it is interesting to note that while it is natural and efficient to condition a fourier based
representation on low frequency observations  involving a very small number of items  such
as o  alice is in third place  a multiplicative decomposition based on riffled independence
would not be able to efficiently condition on the same observation  on the other hand 
multiplicative decompositions allow us to condition on top k observations efficiently  independently of the size of k   whereas top k observations would be difficult to handle in a
fourier theoretic setting  except for very small k  
    mallows models
our work also fits into a larger body of research about the well known mallows distribution
over rankings  parameterized by 
h          d        

     

where the function d refers to the kendalls tau distance metric on rankings  a mallows
distribution  equation      can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by
one  huang         see figure    
mallows models  as well as other similar distance based models  have the advantage
that they can compactly represent distributions for very large n  and admit conjugate prior
distributions  meila  phadnis  patterson    bilmes         estimating parameters has been
a popular problem for statisticians  recovering the optimal   from data is known as the
consensus ranking or rank aggregation problem and is known to be n p  hard  bartholdi 
tovey    trick         many authors have focused on approximation algorithms instead 
like gaussian distributions  mallows models tend to lack flexibility  and so lebanon and
mao        propose a nonparametric model of ranked  and partially ranked  data based
on placing weighted mallows kernels on top of training examples  which  as they show  can
   

fiefficient inference with partial rankings

 corn peas apples oranges doughnuts 

 corn 

 peas apples oranges doughnuts 

 peas 

 apples oranges doughnuts 
 apples 

 oranges doughnuts 
 oranges 

 doughnuts 

figure    a mallows model always factors according to what we refer to as a chain
structure in which items are factored out one by one  the mallows distribution over five items from our food item set with mode  or central ranking  at
    corn p eas apples oranges doughnuts  for example  must factor according to the above hierarchical structure 

realize a far richer class of distributions  and can be learned efficiently  however  they do
not address the inference problem  and it is not immediately clear in many mallows models
papers whether one can efficiently perform inference operations like marginalization and
conditioning in such models  riffle independent models  on the other hand  encompass a
class of distributions which is both rich as well as interpretable  and additionally  we have
identified precise conditions under which efficient conditioning is possible  the conditions
being that the observations take the form of partial rankings  
there are several recent works to model partial rankings using mallows based models 
busse  orbanz  and buhmann        learned finite mixtures of mallows models from topk data  also using an em approach   lebanon and mao         as we have mentioned 
developed a nonparametric model based on mallows models which can handle arbitrary
types of partial rankings  in both settings  a central problem is to marginalize a mallows
model over all full rankings which are consistent with a particular partial ranking  to do so
efficiently  both papers rely on the fact  first shown in fligner   verducci        that this
marginalization step can be performed in closed form  this closed form equation of fligner
and verducci         however  can be seen as a very special case of our setting since mallows
models can always be shown to factor riffle independently according to a chain structure 
specifically  to compute the sum over rankings which are consistent with a partial ranking
s   it is necessary to condition on s   and to compute the normalization constant of the
resulting function  the conditioning step can be performed using the methods that we have
described in this paper  and the normalization constant can be computed by multiplying
the normalization constant of each factor of the hierarchical decomposition  thus  instead
of resorting to the more complicated mathematics of inversion combinatorics  our theory of
complete decomposability offers a simple conceptual way to understand why mallows models
can be conditioned efficiently on partial ranking observations 
   

fihuang  kapoor   guestrin

finally in recent related work  lu and boutilier        considered an even more general
class of observations based on dag  directed acyclic graph  based observations in which
probabilities of rankings which are not consistent with a dag of relative ranking relations
are set to zero  lu and boutilier show in particular that the conditioning problem for
their dag based class of observations is  p  hard  they additionally propose an efficient
rejection sampling method for performing probabilistic inference within the general class
of dag observations and prove that the sampling method is exact for the class of partial
rankings that we have discussed in this paper 

   experiments
in this section  we demonstrate our method for learning hierarchical riffle independent models from partial rankings on simulated data as well as real datasets taken from different
domains  in all experiments  we initialize distributions to be uniform  and do not use random restarts 
    datasets
in addition to roughly      full rankings  the apa dataset has over        top k rankings of
  candidates  in previous work  we had used only the full rankings of the apa data  huang
  guestrin               but now we are able to use the entire dataset  figure   a  plots 
for each k                  the number of ballots in the apa data of length k 
likewise  the meath dataset  gormley   murphy        which was taken from the     
irish parliament election has over        top k rankings of    candidates  as with the apa
data  we had used only the full rankings of the meath data in previous work  but here we use
the entire dataset  figure   b  plots  for each k                   the number of ballots in the
meath data of length k  in particular  note that the vast majority of ballots in the dataset
consist of partial rather than full rankings  with over half of the electorate preferring to list
only their favorite three or four candidates  we can run inference  algorithm    on over
     top k examples for the meath data in    seconds on a dual     ghz pentium machine
with an unoptimized python implementation  using brute force inference  we estimate
that the same job would require roughly one hundred years 
we extracted a third dataset from a database of searchtrails collected by white and
drucker         in which browsing sessions of roughly      users were logged during           in many cases  users are unlikely to read articles about the same news story twice 
and so it is often possible to think of the order in which a user reads through a collection
of articles as a top k ranking over articles concerning a particular story topic  the ability
to model visit orderings would allow us to make long term predictions about user browsing
behavior  or even recommend curriculums over articles for users  we ran our algorithms on
roughly     visit orderings for the eight most popular posts from www huffingtonpost com
concerning sarah palin  a popular subject during the      u s  presidential election  since
no user visited every article  there are no full rankings in the data and thus there does not
even exist the option of learning using only the subset of full rankings 
   

fiefficient inference with partial rankings

number of votes

number of votes

    
    

    
    
    
    

      

      

 

 

 

 

 
 
 
 
number of candidates

 

 

          
k

 a  apa election data

 b  irish election data

figure    histograms of top k ballot lengths in the apa and irish election datasets  whereas
the majority of the electorate provided full rankings in the apa election data
 probably due to the fact that there were only five candidates   the vast majority
of voters in the irish election data provided only their top   or top   choices 
       

       
       

   

      

   

      
      

     

   
   

    

 a  structure learned using
only the subset of full rankings
 out of the     given training
examples 

     

   
   

    

 b  structure learned using all
training examples after   iteration of em

    
research

   
community
psychologists

    
clinical

 c  structure learned using all
training examples after structural convergence    iterations 

figure    structure learning with a subset of the apa dataset      rankings  randomly
sampled  including both full and partial rankings  

    apa structure learning results
due to the unordinarily large number of full rankings in the apa data  the gains made by
additionally using partially ranked data are insignificant  to better illustrate the benefits of
partial rankings  we subsampled a dataset of     rankings  including both full and partial
rankings  and present results with this smaller dataset  performing structure learning using
only the full rankings of these     training examples  consisting of roughly     examples  
one obtains the structure in figure   a   which can be seen to not match the correct
structure of figure   a  which was learned using      full rankings  figures   b  and   c 
   

fihuang  kapoor   guestrin

training time  seconds 

test log likelihood

x     
  
  
  
  

  
em

flat em

 

training time  seconds 

test log likelihood

  
    
    
    
    
  
k  

  
 

 
em

flat em uniform
fill in

 b  training time comparison of our
em approach against the flatem and
uniform fill in methods 

x   

k  

  

uniform
fill in

 a  test set log likelihood comparison
of our em approach against the flatem
and uniform fill in methods 

k  

  

   
 
   

 

k  

 c  test set log likelihoods  training
only with top t rankings with t larger
than a fixed k 

k  

k  

k  

k  

 d  training times  training only with
top t rankings with t larger than a fixed
k 

figure    apa experimental results  each experiment repeated with     bootstrapped
resamplings of the data

plot the results of our em algorithm with the former displaying the resulting structure after
just a single em iteration and the latter the result after structural convergence  which occurs
by the third iteration  showing that our method can learn the correct structure given just
    training examples 
we compared our em algorithm against two alternative baseline approaches that we
refer to in our plots as flatem and uniform fill in  the flatem algorithm is the same as
the em algorithm above except for two details      it performs conditioning exhaustively
instead of exploiting the factorized model structure  and     it performs the m step without
sampling  the uniform fill in approach treats every top k ranking in the training set as
a uniform collection of votes for all of the full rankings consistent with that top k ranking 
and is accomplished by using just one iteration of our em algorithm 
in figure   a  we plot test set loglikelihoods corresponding to each approach  with em
and flatem having almost identical results and both performing much better than the
uniform fill in approach  on the other hand  figure   b   which compares running times
of the three approaches  shows that flatem can be far more costly  for most datasets  it
cannot even be run in a reasonable amount of time  
   

fiefficient inference with partial rankings

 st iteration

 nd iteration

                 

 rd iteration

                 
                 

   

               

   

               
   

             

   

             

               

   
             

           

   

           

   
       

       

     

log likelihood           
 a 

       

   

       

     

log likelihood           
 b 

log likelihood           
 c 

figure     iterations of structure em for the sarah palin data with structural changes at
each iteration highlighted in red  structural convergence occurs after just three
iterations  note that this structure was discovered using only visit orders  and
that no text information from pages was incorporated in the learning process 
this figure is best viewed in color 

to verify that partial rankings do indeed make a difference in the apa data  we plot
the results of estimating a model from the subsets of apa training data consisting of top k
rankings with length larger than some fixed k  figures   c  and   d  show the log likelihood
and running times for k              with k     being the entire training set and k    
being the subset of training data consisting only of full rankings  as our results show 
including partial rankings does indeed help on average for improving test log likelihood
 with diminishing returns  
    structure discovery with em with larger n 
our experiments have led to several observations about using em for learning with partial
rankings  first  we observe that typical runs converge to a fixed structure quickly  with no
more than three em iterations  figure    shows the progress of em on the sarah palin
data  whose structure converges by the third iteration  as expected  the log likelihood
increases at each iteration  and we remark that the structure becomes more interpretable
 for example  the leaf set           corresponds to the three posts about palins wardrobe
before the election  while the posts from the leaf set           were related to verbal gaffes
made by palin during the campaign  notice that this structure is discovered purely using
data about visit orders and that no text information was used in our experiments 
   

fihuang  kapoor   guestrin

     

x    

test log likelihood

  of em iterations before
convergence

  

  
  
  

  

em with
decomposable
conditioning

    

 lebanon   mao     

     
     

 
 

     

 

                            
k

                         
  of partial rankings in training set
 in addition to full rankings 

 a 

 b 

figure      a   number of em iterations required for convergence if the training set only
contains rankings of length longer than k   b   density estimation from synthetic
data  we plot test loglikelihood when learning from     full rankings and between
  and        additional partial rankings 

     training
examples

 

test log likelihood

x   
    

ours

    

      training
examples

 

x   
     

 lm   

 lm   

     

ours

    

    

ours

     

 lm   
ours

    

 lm   

     

    

full only

mixed  full partial 

full only

mixed  full partial 

figure     density estimation from small       examples  and large subsets        examples  of the meath data  we compare our method against the work by lebanon
and mao        in two settings      training on all available data and     training
on the subset of full rankings 

secondly  the number of em iterations required to reach convergence in log likelihood
depends on the types of partial rankings observed  we ran our algorithm on subsets of
the meath dataset  each time training on m        rankings all with length larger than
   

fiefficient inference with partial rankings

some fixed k  figure    a  shows the number of iterations required for convergence as a
function of k  with    bootstrap trials for each k   we observe fastest convergence for
datasets consisting of almost full rankings and slowest convergence for those consisting of
almost empty rankings  with almost    iterations necessary if one trains using rankings of
all types  finally we remark that the model obtained after the first iteration of em is
interesting and can be thought of as the result of pretending that each voter is completely
ambivalent regarding the n  k unspecified candidates 
    the value of partial rankings
we now verify again with larger n that using partial rankings in addition to full rankings
allows us to achieve better density estimates  we first learned models from synthetic data
drawn from a hierarchy  training using     full rankings plus varying numbers of partial
ranking examples  ranging between            we repeat each setting with    bootstrap
trials  and for evaluation  we compute the log likelihood of a testset with      examples 
for speed  we learn a structure h only once and fix h to learn parameters for each trial 
figure    b   which plots the test log likelihood as a function of the number of partial
rankings made available to the training set  shows that we are indeed able to learn more
accurate distributions as more and more data in the form of partial rankings are made
available 
    comparing to a nonparametric model
comparing the performance of riffle independent models to other approaches was not possible in previous work since we had not been able to handle partial rankings  using the
methods developed in our current paper  however  we compare riffle independent models
with the state of the art nonparametric estimator of lebanon and mao         to which we
hereby refer as the lm   estimator  on the same data  setting their regularization parameter to be c         or    via a validation set   figure    b  shows  naturally  that when the
data are drawn synthetically from a riffle independent model  then our em method significantly outperforms the lm   estimator  we remark that in theory  the lm   is guaranteed
to catch up in performance  under appropriate conditions  given enough training examples 
for the meath data  which is only approximately riffle independent  we trained on subsets
of size       and         testing on remaining data   for each subset  we evaluated our em
algorithm for learning a riffle independent model against the lm   estimator when    
using only full ranking data  and     using all data  as before  both methods do better
when partial rankings are made available 
for the smaller training set  the riffle independent model performs as well or better than
the lm   estimator  for the larger training set of         we see that the nonparametric
method starts to perform slightly better on average  the advantage of a nonparametric
model being that it is guaranteed to be consistent  converging to the correct model given
enough data  the advantage of riffle independent models  however  is that they are simple 
interpretable  and can highlight global structures hidden within the data 
   

fihuang  kapoor   guestrin

   future directions
there remain several possible extensions to the current work  we list a few such open
questions and extensions in the following 
    inference with incomplete rankings
we have shown in this paper that one can exploit riffled independence structure to condition
on an observation if and only if it takes the form of a partial ranking  while the space of
partial rankings is both rich and useful in many settings  it does not cover an important class
of observations  that of incomplete rankings  which are defined to be a ranking  or partial
ranking  of a subset of the itemset   for example  theorem    shows that the conditioning problem for pairwise observations of the form apples are preferred over bananas is
nondecomposable  note that top k rankings are considered to be complete rankings since
they implicitly rank all other items in the last n  k positions 
how then  can we tractably condition on incomplete rankings  one possible approach
is to convert to a fourier representation using the methods from  huang   guestrin        
then conditioning on a pairwise ranking observation using the fourier domain conditioning
algorithm proposed in  huang et al          this fourier domain approach would be useful if one were particularly interested in low order marginal probabilities of the posterior
distributions 
when the fourier approach is not viable  another option may be to assume that the
posterior distribution takes on a particular riffle independent structure  in the same way
that mean field methods from the graphical models literature would assume a factorized
posterior   the research question of interest is  which hierarchical structure should be used
for the purposes of approximating the posterior 
    reexamining data independence assumptions
in this paper  we have assumed throughout that training examples are independent and
identically distributed  however in practice these are not always safe assumptions as a
number of factors can impact the validity of both  for example  in an internet survey in
which a user must perform a series of preference ranking tasks in sequence  a concern is that
the users prior ranking tasks may bias the results of his future rankings 
another source of bias lies in the reference ranking that may be displayed  in which the
user is asked to rearrange items by dragging and dropping  on the one hand  showing
everyone the same reference ranking may bias the resulting data  but on the other hand 
showing every user a different reference ranking may mean that the training examples are
not exactly identically distributed 
yet another form of bias lies in the partial ranking types that are reported in data  to
formulate our em algorithm  we have assumed that a users preferences does not influence
whether he chooses to  say  report a full ranking instead of a top   ranking  in practice 
however  partial ranking types and user preferences are often correlated  in the irish elections  for example  where there is typically only one sinn fein candidate  those who rank
sinn fein first are typically more likely to have only reported their top   choice 
   

fiefficient inference with partial rankings

understanding  identifying  and finally  learning in spite of the different types of biases
that may occur in eliciting preference data remains a fundamental problem in ranking 
    probabilistic modeling of strategic voting
it is interesting to consider the differences between the actual vote distributions considered in
this paper against the approximate riffle independent distributions  take the apa dataset 
for example  in which the optimal approximation by a riffle independent hierarchy reflects
the underlying political coalitions within the organization  upon comparison between the
approximation and the empirical distribution  however  some marked differences arise  for
example  the riffle independent approximation underestimates the number of votes obtained
by candidate    a research psychologist  who ultimately won the election 
one possible explanation for the discrepancy may lie in the idea that voters tend to vote
strategically in apa elections  placing stronger candidates of opposing political coalitions
lower in the ranking  rather than revealing their true preferences  an interesting line of
future work lies in detecting and studying the presence of such strategic voting in election
datasets  open questions include     verifying mathematically whether strategic voting does
indeed exist in  say  the apa election data  and     if so  why the strategic voting effect is
not strong enough to overwhelm our riffled independence structure learning algorithms  and
    how strategic voting can manifest itself in partial ranking votes 

    conclusion
in probabilistic reasoning problems  it is often the case that certain data types suggest
certain distribution representations  for example  sparse dependency structure in the data
often suggests a markov random field  or other graphical model  representation  friedman 
             for low order permutation observations  depending on only a few items at a
time   recent work  huang et al         kondor        has shown that a fourier domain
representation is appropriate  for preference ranking scenarios  one must contend with
human task complexity  the difficulty involved for a human to rank a long list of items
and often leads to partially  instead of fully ranked data  in this paper  we have shown that
when data takes the form of partial rankings  then hierarchical riffle independent models are
a natural representation 
as with conjugate priors  we showed that a riffle independent model is guaranteed to
retain its factorization structure after conditioning on a partial ranking  which can be performed in efficiently   most surprisingly  our work shows that observations which do not
take the form of partial rankings are not amenable to simple multiplicative update based
conditioning algorithms  finally  we showed that it is possible to learn hierarchical riffle
independent models from partially ranked data  significantly extending the applicability of
previous work 

acknowledgments
this project was formulated and largely conducted during an internship by jonathan huang
at microsoft research  additional work was supported in part by onr under muri
n              and aro under muri w   nf         carlos guestrin was funded
   

fihuang  kapoor   guestrin

in part by nsf career iis         we thank eric horvitz  ryen white  dan liebling  and
yi mao for discussions 

appendix a  proofs
in this appendix  we provide supplementary proofs of some of the theoretical results in this
paper 
a   proof of theorem   
to prove theorem     as well as later results   we will refer to rank sets 
definition     given a partial ranking of type   we denote the rank set occupied by
i by ri   note that ri depends only
on  and can be written as r                    
p
r                                     rr     r 
i   i              n  
and we will refer to the following basic fact regarding rank sets 
proposition       s               r if and only if for each i   i     ri  
proof   of theorem     we use induction on the size of the itemset  the cases n        are
trivial since every distribution on s  or s  factors riffle independently  we now consider the
more general case of n     
fix a partial ranking s                  r of type  and a binary partition of the item
set into subsets a and b  we will show that the indicator function s  factors as 
s       m ab      f  a      g b     

 a   

where factors m  f and g are the indicator functions for the set of consistent interleavings 
 s  ab   and the sets of consistent relative rankings   s  a and  s  b   respectively  if
equation a   is true  then we will have shown that s  must decompose with respect
to the top layer of h  to show that s  decomposes hierarchically  we must also show
that the relative ranking factors fa and gb decompose with respect to ha and hb   the
subhierarchies over the item sets a and b  to establish this second step  assuming that
equation a   holds   note that fa and gb are indicator functions for the restricted partial
rankings   s  a and  s  b   which themselves are partial rankings over smaller item sets
a and b  the inductive hypothesis  and the fact that a and b are assumed to be strictly
smaller sets than   then shows that the functions fa and gb both factor according to their
respective subhierarchies 
we now turn to establishing equation a    it suffices to prove that the following two
statements are equivalent 
i  the ranking  is consistent with the partial ranking s   i e     s   
ii  the following three conditions hold 
 a  the interleaving ab    is consistent with s   i e   ab      s  ab    and
 b  the relative ranking a    is consistent with s   i e   a      s  a    and
 c  the relative ranking b    is consistent with s   i e   b      s  b   
   

fiefficient inference with partial rankings

  i  ii   we first show that   s  implies conditions  a    b  and  c  
 a  if   s   then for each i 
 j  ri   ab  j    a     j  ri       j   a  
   k  i   k  a  

 by definition   

 by proposition    

   i  a  
the same argument  replacing a with b  shows that for each i  we have  j 
ri   ab  j    b     i  b   these two conditions  by definition     show that
ab is consistent with s  
 b  if   s   then  by definition      ranks items in i before items in j for any
i   j  intersecting each i with a  we also see that  ranks any item in i  a
before any item in j  a for all i  j  by definition    a    also ranks any item
in i  a before any item in j  a for all i  j  and finally by definition    again 
we see that a    is consistent with the partial ranking s  
 c   same argument as  b   
  ii  i   we now assume conditions  a    b   and  c  to hold  and show that   s  
by proposition    it is sufficient to show that if an item k  i   then  k   ri   to
prove this claim  we show by induction on i that if an item k  i  a  then  k   ri
 and similarly if k  i  b  then  k   ri   
base case  in the base case  i       we assume that k    a  and the goal is to show
that  k   r    by condition  a   we have that ab      s  ab   by definition    
this means that      a     j  r     ab     j    a     j  r        j   a   in
words  there are m       a  items from a which lie in rank set r                     to
show that an item k  a maps to a rank in r    we now must show that in the relative
ranking of elements in a  k is among the first m  by condition  b   a      s  a  
implying that the item subset    a occupies the first m positions in the relative
ranking of a  since k     a  item k is among the first m items ranked by a   
and therefore  k   r    a similar argument shows that k     b implise that
 k   r   
inductive case  we now show that if k  i  a  then  k   ri   by condition  b  
a      s  a   implying that the item subset i a  and hence  item k  occupies the
first m    i  a  positions in the relative ranking of a beyond the items i 
j    j 
a   by the inductive hypothesis and mutual exclusivity  these items  together with
i 
i 
j    j  b  occupy ranks j   rj   and therefore  k   r  for some    i  on the
other hand  condition  a  assures us that  i  a     j  ri       j   a   or in
other words  that the ranks in ri are occupied by exactly m items of a  therefore 
 k   ri   again  a similar argument shows that k  i  b implies that  k   ri  

a   the pspan of a set is always a partial ranking
to reason about the pspan of a set of rankings  we first introduce some basic concepts
regarding the combinatorics of partial rankings  the collection of partial rankings over 
   

fihuang  kapoor   guestrin

forms a partially ordered set  poset  where s       s  if s  can be obtained from s      by
dropping vertical lines  for example  on s    we have that              the hasse diagram
is the graph in which each node corresponds to a partial ranking and a node x is connected
to node y via an edge if x  y and there exists no partial ranking z such that x  z  y
 see lebanon   mao         at the top of the hasse diagram is the partial ranking               n
 i e   all of s   and at the bottom of the hasse diagram lie the full rankings  see figure   
for an example of the partial ranking lattice on s   
lemma      lebanon   mao        given any two partial rankings s   s        there
exists a unique supremum of s  and s       a node ssup sup such that s   ssup sup
and s       ssup sup   and any other such node is greater than ssup sup    similarly  there
exists a unique infimum of s  and s       
lemma     given two partial rankings s   s        the relation s       s  holds if and
only s  lies above s      in the hasse diagram 
proof  if s  lies above s      in the hasse diagram  then s       s  is trivial since s 
can be obtained by dropping vertical bars of s        now given that s  does not lie above
s        we would like to show that s        s   let sinf inf be the unique infimum of
s  and s      as guaranteed by lemma     by the definition of the hasse diagram  both
s  and s  can be obtained by dropping verticals from the vertical bar representation
of sinf inf   since s  does not lie above s        there must be a vertical bar that was
dropped by s      which was not dropped by s   if there does not exist such a bar  then
s       s    and hence there must exist a pair of items i  j separated by a single vertical
bar in s  but unseparated in s        therefore there exists   s      such that  j     i 
even though there exists no such   s   we conclude that s        s  
lemma     lemma    in main body   for any x  sn   pspan x  is a partial ranking 
proof  consider any subset x  sn   a partial ranking containing every element in x
must be an upper bound of every element of x in the hasse diagram by lemma     by
lemma     there must exist a unique least upper bound  supremum  of x  ssup sup   such
that for any common upper bound s  of x  s  must also be an ancestor of ssup sup and
hence ssup sup  s   we therefore see that any partial ranking containing x must be a
superset of ssup sup   on the other hand  ssup sup is itself a partial ranking containing x 
since pspan x  is the intersection of partial rankings containing x  we have pspan x   
ssup sup and therefore that pspan x  must be a partial ranking 
a   proofs for the claim that rspan x    pspan x 
to simplify the notation in some of the remaining proofs  we introduce the following definition 
definition     ties   given a partial ranking s               r   we say that items a  and
a  are tied  written a   a    with respect to s  if a    a   i for some i 
the following basic properties of the tie relation are straightforward 
proposition    
   

fiefficient inference with partial rankings

   
    

    

    

    

    

    

     

     

     

     

     

     

figure     the hasse diagram for the lattice of partial rankings on s   
i  with respect to a fixed partial ranking s   the tie relation    is an equivalence relation
on the item set  i e   is reflexive  symmetric and transitive  
ii  if there exist       s  which disagree on the relative ranking of items a  and a   
then a   a  with respect to s  
iii  if s   s        and a   a  with respect to s   then a   a  with respect to s       
iv  if a   a  with respect to s   and  a       a       a    for some item a    and
some   s   then a   a   a   
proposition     given a set of rankings x as input  algorithm   outputs pspan x  
proof  we prove three things  which together prove the proposition      that the algorithm
terminates      that at each stage the elements of x are contained in pspan x   and    
that upon termination  pspan x  is contained in each element of x 
   first we note that the algorithm must terminate in finitely many iterations of the while
loop since at each stage at least one vertical bar is removed from a partial ranking 
and when all of the vertical bars have been removed from the elements of x  there are
no disagreements on relative ordering 
   we now show that at any stage in the algorithm  every element of xt is a subset of
the pspan x   at initialization  of course  if s   x    then it is simply a singleton
set consisting of an element of x  and therefore s   pspan x  
suppose now that s   pspan x  for every s   xt   if s  is replaced by s 
in xt     then we want to show that s   pspan x  as well  from algorithm   
for some j  if s               j  j            r   s  can be written as            j 
j            r   where the vertical bar between j and j   is deleted due to the existence
of some partial ranking in xt   s       xt which disagrees with s  on the relative
ordering of items a    a  on opposite sides of the bar  since s  and s      are both
subsets of pspan x  by assumption  we know that a   a  with respect to pspan x 
 proposition     ii   suppose now that a   i and a   i    then for any x  i
and y  i    we have x  a  and y  a  with respect to pspan x  by  iii  of
proposition     moreover  by  i  transitivity   we see that x  y with respect to
pspan x   for any two elements of i and i    by  iv  of proposition     all the
items lying in i   i             i  are thus tied with respect to pspan x  and therefore
removing any bar between items a  and a   producing  for example  s   results in a
partial ranking which is a subset of pspan x  
   

fihuang  kapoor   guestrin

   finally  upon termination  if some ranking   x is not contained in some element
s   xt   then there would exist two items a    a  whose relative ranking  and s 
disagree upon  which is a contradiction  therefore  every element s   xt contains
every element of x and thus pspan x   s  for every s   xt  

lemma     let s               i  i            k be a partial ranking on item set   and
s                   i  i            k   the partial ranking in which the sets i and i   are
merged  let a   ij   j and a   kj i   j   if o is any element of c such that s   o
and there additionally exists a ranking   o which disagrees with s  on the relative
ordering of a    a    then s       o 
proof  we will fix a completely decomposable o and again work with h  the indicator
distribution corresponding to o  let   s        to prove the lemma  we need to establish
that h        let    be any element of s  such that     k     k  for all k    i i     
since s   supp h  by assumption  we have that h          
since    and  match on all items except for those in i  i     there exists a sequence
of rankings                         m    such that adjacent rankings in this sequence differ only
by a pairwise exchange of items b    b   i  i     we will now show that at each step
along this sequence  h  t       implies that h  t          which will prove that h       
suppose now that h  t       and that  t and  t   differ only by the relative ranking of
items b    b   i  i    without loss of generality  we will assume that  t  b       t  b    and
 t    b       t    b     
the idea of the following paragraph is to use the previous lemma  lemma     to prove
that  t   has positive probability and to do so  it will be necessary to argue that there
exists some ranking    such that h          and     b          b     i e      disagrees with  t
on the relative ranking of b    b     let  be any element of s   if a   i   rearrange 
such that a  is ranked first among elements of i   if a   i     further rearrange  such
that a  is ranked last among elements of i     note that  is still an element of s  after
the possible rearrangements and therefore h        we can assume that  b       b   
since otherwise we will have shown what we wanted to show  thus the relative ordering of
a    a    b    b  within  is a   b   b   a    note that we treat the case where the items a    a    b    b 
are distinct  but the same argument follows in the cases when a    b  or a    b   
now since  disagrees with s  on the relative ordering of a    a  by assumption  and
hence disagrees with    we apply lemma    to conclude that swapping the relative ordering
of a    a  within   obtaining a   b   b   a    results in a ranking       such that h          
finally  observe that  and    must now disagree on the relative ranking of a    b    and
invoking lemma    again shows that we can swap the relative ordering of a    b  within 
 obtaining a   a   b   b    to result in a ranking    such that h           this element    ranks
b  before b    which is what we wanted to show 
we have shown that there exist rankings which disagree on the relative ordering of b 
and b  with positive probability under h  again applying lemma    shows that we can swap
the relative ordering of items b    b  within  t to obtain  t   such that h  t          which
concludes the proof 
   

fiefficient inference with partial rankings

a   uniformity of c functions over a partial ranking
we have thus far shown that any element of c must be supported on some partial ranking 
in the following  we show that  up to a certain class of exceptions   such an element must
assign uniform probability to all members of this partial ranking 
theorem     if h is any completely decomposable function supported on a partial ranking
s               r where  i    
    for all i              r  then h is uniform on s   i e  
 
q
h    
 i   for all   s   
i

to establish theorem     we must establish two supporting results      lemma    which
factors h into r smaller completely decomposable functions  each of which is nonzero everywhere on its domain  and     theorem    which establishes uniformity for any completely
decomposable function which is nonzero everywhere on its domain 
lemma     any completely decomposable q
function  h  supported on the partial ranking
s               r   must factor as  h     ri   h  i     where each factor distribution
h  i    is itself a completely decomposable function on si  
proof  since h is completely decomposable  we have that  i   is riffle independent of
  i   for each i  since h is supported on the partial ranking s                r   however 
the interleaving of i with its complement is deterministic and therefore we conclude in fact
that  i   is fully independent
of   i    since  i      i   for each i  we have the
qr
factorization  h     i   h  i    
we now turn to establishing that each factor h  i    is itself a completely decomposable
observation  fix i      without loss of generality  and consider any partition of the set  
into subsets a  b  we would like to see that the sets a and b are riffle independent of each
other with respect to h        since h is assumed to be completely decomposable  we know
that a is riffle independent of its complement  b       in other words  if b   b      
then the variables a     ab   b  the relative ranking of a  the interleaving of a with all
remaining items  and the relative ranking of all remaining items  respectively  are mutually
independent  we then observe that     the interleaving of a and b  ab   is a deterministic
function of the interleaving of ab and     the relative ranking of b  b   is a deterministic
function of b   thus proving that a   ab and b are mutually independent and hence that
a and b are riffle independent 
theorem     let h a completely decomposable function such that h       for all   sn
for n      then for any two rankings       which differ by a single transposition  we have
h       h     
our proof strategy for theorem    will involve examining the ratio between the two
probabilities h     and h      we then define an operation transforming   and   into new
rankings    and    such that the ratio between the rankings is preserved  i e   h     h      
h      h        by performing a sequence of such ratio preserving operations  we show that 
h    
h    
 
 
h    
h    
from which theorem    easily follows 
   

fihuang  kapoor   guestrin

we will use two types of operations which transform a ranking into a new ranking     
changing the interleaving of two sets a and b within a ranking   and      changing the
relative ranking of a set a within a ranking   more precisely  given a ranking  and a
partitioning of the item set into subsets a and b  we can uniquely index  as a triplet
   a   b    where    a b     a   a     and b   b     the two operations are
defined as follows 
   changing the interleaving of a  b within  to      yields the new ranking    which is
indexed by       a   b   
   or       yields the new
   changing the relative ranking of a  or b  within  to a
b
 
 
 
ranking  which is indexed by    a   b    or    a   b    

if we use the above operations to obtain from    and      we are interested in conditions
under which this transformation is ratio preserving  i e   h     h       h      h        the
following lemma provides sufficient conditions for ratio preservation 
lemma     let h be any completely decomposable function and consider        sn such
that h          then for any partitioning of the item set into subsets a and b  we have 
   if   and   match on the interleaving of a and b  i e   a b        ab        then
h     
h    
 
 
h       h        where   and   are formed by changing the interleaving of the sets a
and b within   and   to be any new interleaving     
   if   and   match on the relative ranking of a  or b   i e   a        a       or
h     
h    
  h        where    and    are formed by changing the
b        b         then h 
  
 
 
relative ranking of set a  or b  within   and   to be any new relative ranking a
 
 or b   
proof  since the proofs of parts   and   are nearly identical  we just prove part   here 
since h  c  the sets a and b are riffle independent by assumption  and hence we have the
factorizations 
h    
m      f   a    g  b  
 
 
h    
m      f   a    g  b  

if   and   match on the interleaving of the sets a and b  then we have that           
and thus the interleaving terms  m     and m     are the same in both the numerator and
denominator 
on the other hand  if we examine the ratio between h      and h       we also see that
the interleaving terms must cancel 
h    
m       f   a    g  b  
 
 
h    
m       f   a    g  b  

we therefore have that 
f   a    g pib
h     
h    
   
 
 
 
a
b
h    
h     
f       g    

   

fiefficient inference with partial rankings

having now established lemma     we turn to establishing three short claims  using
the lemma  that will allow us to prove finally prove theorem     it is interesting to note
that we require n      strictly  in claim iii below in which we swap the order of i and j
in numerator and denominator  the third item k in our proof below can be thought of as
playing the role of a dummy variable analogous to the temporary storage variables that one
might use in implementing a swap function  the necessity of this third item is precisely why
our result does not hold in the special case that n     
proposition     let h   sn  r be a completely decomposable function with n     with
h       for all   sn   we have the following equivalences  where in each of the below
ratios  entries which have not been explicitly written out are assumed to match identically
in both the numerator and denominator  
i 

ii 

h i j         k         
h i j k         
 
 
h j i         k         
h j i k         
h        i         j         
h i j         
 
 
h        j         i         
h j i         

iii 

h j i k         
h i j k         
 
 
h j i k         
h i j k         

proof 
i  equality holds in i since   and   match on the interleaving of the sets a    k  with
b     k   thus we can change the interleaving of a and b in both   and   so
that item k is inserted in rank   while preserving the ratio 
ii  equality holds in ii since   and   match on the interleaving of the sets a    i  j 
with b     i  j   thus we can change the interleaving of a and b in both   and
  so that items i and j occupy the first two ranks while preserving the ratio between
h     and h     
iii  in the following we use   and   to refer to the arguments in the numerator and
denominator  respectively  of the preceding line 
h i j k         
h i k j         
 
 
h j i k         
h k i j         
h j i k         
 
 
h j k i         
h i j k         
 
 
h i k j         
h k j i         
 
 
h k i j         
h j i k         
 
 
h i j k         

 since       match on the relative ranking of  j  k  
 since       match on the interleaving of  j  with   j  
 since       match on the relative ranking of  i  j  
 since       match on the relative ranking of  i  k  
 since       match on the interleaving of  k  with   k   

   

fihuang  kapoor   guestrin

proof   of theorem     we want to show that if two rankings differ by a single transposition 
then they are assigned equal probability under h  suppose then that   is obtained from
  by swapping the ranks of items i and j  additionally  let k be any item besides i and j
 such an item must exist since n       in the following  we use proposition    to show that
h     h       h     h      as before  entries which have not been explicitly written out
are assumed to match identically in both the numerator and denominator 
h        i         j         
h i j         
h    
 
 
   by prop      part ii 
h    
h        j         i         
h j i         
h i j         k         
h i j k         
 
 
   by prop      part i 
h j i         k         
h j i k         
h j i k         
   by prop      part iii 
 
h i j k         
h j i         k         
 
   by prop      part i 
h i j         k         
h j i         
h        j         i         
 
 
   by prop      part ii 
h i j         
h        i         j         
h    
 
 
h    

since we have assumed h     and h          we must conclude that h       h     
finally  we assemble all of our supporting results to prove theorem    
proof   of theorem     by lemma     a completely decomposable function h must factor
as 
r
y
h    
h  i    
 a   
i  

where each factor distribution h  i    is itself a completely decomposable function on si  
by assumption   i         if  i        then its corresponding factor h  i    must trivially
be uniform  otherwise  we have that  i        in this latter case  we apply theorem    to
h  i    to show that it must assign equal probability to any two rankings that differ by a
single transposition  however  given any rankings        si   we can obtain a sequence of
transpositions that transforms   into     and therefore  theorem    in fact implies that the
factor h  i    is constant on all inputs  having proved that each factor in equation a   is
constant  we conclude that h must be constant on its support 

references
ailon  n          aggregation of partial rankings  p ratings and top m lists  in proceedings
of the eighteenth annual acm siam symposium on discrete algorithms  soda    
new orleans  louisiana 
bartholdi  j  j   tovey  c  a     trick  m          voting schemes for which it can be
difficult to tell who won  social choice and welfare       
   

fiefficient inference with partial rankings

busse  l  m   orbanz  p     buhmann  j          cluster analysis of heterogeneous rank
data  in the   th annual international conference on machine learning  corvallis 
oregon 
fligner  m  a     verducci  j  s          distance based ranking models  journal of the
royal statistical society     
freund  y   iyer  r   schapire  r  e     singer  y          an efficient boosting algorithm for
combining preferences  journal of machine learning research  jmlr             
friedman  n          learning belief networks in the presence of missing values and hidden variables  in proceedings of the fourteenth international conference on machine
learning  icml     pp          san francisco  ca  usa  morgan kaufmann publishers inc 
friedman  n          the bayesian structural em algorithm  in the   th conference on
uncertainty in artificial intelligence  uai     madison  wisconsin 
gormley  c     murphy  b          a latent space model for rank data  in proceedings
of the      conference on statistical network analysis  icml    pp         berlin 
heidelberg  springer verlag 
huang  j   kapoor  a     guestrin  c          efficient probabilistic inference with partial
ranking queries  in the   th conference on uncertainty in artificial intelligence  uai
    barcelona  spain 
huang  j          probabilistic reasoning and learning on permutations  exploiting structural decompositions of the symmetric group  ph d  thesis  carnegie mellon university 
huang  j     guestrin  c          riffled independence for ranked data  in bengio  y  
schuurmans  d   lafferty  j   williams  c  k  i     culotta  a   eds    advances in
neural information processing systems     nips     pp          mit press 
huang  j     guestrin  c          learning hierarchical riffle independent groupings from
rankings  in proceedings of the   th annual international conference on machine
learning  icml     pp          haifa  israel 
huang  j     guestrin  c          uncovering the riffled independence structure of ranked
data  electronic journal of statistics            
huang  j   guestrin  c     guibas  l          efficient inference for distributions on permutations  in platt  j   koller  d   singer  y     roweis  s   eds    advances in neural
information processing systems     nips     pp          mit press  cambridge 
ma 
huang  j   guestrin  c     guibas  l  j          fourier theoretic probabilistic inference
over permutations  journal of machine learning research  jmlr               
joachims  t          optimizing search engines using clickthrough data  in proceedings of
the eighth acm sigkdd international conference on knowledge discovery and data
mining  kdd     pp          new york  ny  usa  acm 
kondor  r   howard  a     jebara  t          multi object tracking with representations
of the symmetric group  in meila  m     shen  x   eds    proceedings of the eleventh
   

fihuang  kapoor   guestrin

international conference on artificial intelligence and statistics march             
san juan  puerto rico  vol  volume   of jmlr  w cp 
kondor  r          group theoretical methods in machine learning  ph d  thesis  columbia
university 
lebanon  g     lafferty  j          conditional models on the ranking poset  in s  becker 
s  t     obermayer  k   eds    advances in neural information processing systems
    nips     pp          cambridge  ma  mit press 
lebanon  g     mao  y          non parametric modeling of partially ranked data  in platt 
j  c   koller  d   singer  y     roweis  s   eds    advances in neural information
processing systems     nips     pp          cambridge  ma  mit press 
lu  t     boutilier  c          learning mallows models with pairwise preferences  in
the   th annual international conference on machine learning  icml     bellevue 
washington 
marden  j  i          analyzing and modeling rank data  chapman   hall 
meila  m   phadnis  k   patterson  a     bilmes  j          consensus ranking under the
exponential model  tech  rep       university of washington  statistics department 
white  r     drucker  s          investigating behavioral variability in web search  in
proceedings of the   th international conference on world wide web  www    
banff  alberta  canada  acm 

   

fi