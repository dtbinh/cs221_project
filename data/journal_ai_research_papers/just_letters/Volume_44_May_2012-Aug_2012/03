journal artificial intelligence research                  

submitted        published      

domain function  dual space model
semantic relations compositions
peter d  turney

peter turney nrc cnrc gc ca

national research council canada
ottawa  ontario  canada  k a  r 

abstract
given appropriate representations semantic relations carpenter wood
mason stone  for example  vectors vector space model   suitable
algorithm able recognize relations highly similar  carpenter
wood mason stone  relations analogous   likewise  representations
dog  house  kennel  algorithm able recognize semantic
composition dog house  dog house  highly similar kennel  dog house kennel
synonymous   seems two tasks  recognizing relations compositions 
closely connected  however  now  best models relations significantly
different best models compositions  paper  introduce dual space
model unifies two tasks  model matches performance best
previous models relations compositions  dual space model consists space
measuring domain similarity space measuring function similarity  carpenter
wood share domain  domain carpentry  mason stone share
domain  domain masonry  carpenter mason share function 
function artisans  wood stone share function  function materials 
composition dog house  kennel domain overlap dog house
 the domains pets buildings   function kennel similar function
house  the function shelters   combining domain function similarities various
ways  model relations  compositions  aspects semantics 

   introduction
distributional hypothesis words occur similar contexts tend similar
meanings  harris        firth         many vector space models  vsms  semantics use
wordcontext matrix represent distribution words contexts  capturing
intuition behind distributional hypothesis  turney   pantel         vsms achieved
impressive results level individual words  rapp         clear
extend level phrases  sentences  beyond  example  know
represent dog house vectors  represent dog house  
one approach representing dog house treat unit  way handle
individual words  call holistic noncompositional approach representing
phrases  holistic approach may suitable phrases  scale up 
vocabulary n individual words  n   two word phrases  n   threeword phrases  on  even large corpus text  possible
phrases never appear corpus  people continually inventing new phrases 
able understand new phrases although never heard before 
able infer meaning new phrase composition meanings
c
    
national research council canada  reprinted permission 

fiturney

component words  scaling problem could viewed issue data sparsity 
better think problem linguistic creativity  chomsky        fodor  
lepore         master natural language  algorithms must able represent phrases
composing representations individual words  cannot treat n grams  n     
way treat unigrams  individual words   hand  holistic approach ideal
idiomatic expressions  e g   kick bucket  meaning cannot inferred
component words 
creativity novelty natural language require us take compositional approach majority n grams encounter  suppose vector representations dog house  compose representations represent dog
house   one strategy represent dog house average vectors dog
house  landauer   dumais         simple proposal actually works  limited degree
 mitchell   lapata               however boat house house boat would represented
average vector  yet different meanings  composition averaging
deal order sensitivity phrase meaning  landauer        estimates
    meaning english text comes word choice remaining     comes
word order 
similar issues arise representation semantic relations  given vectors
carpenter wood  represent semantic relations carpenter
wood   treat carpenter  wood unit search paraphrases relations
carpenter wood  turney      b   large corpus  could find phrases
carpenter cut wood  carpenter used wood  wood carpenter 
variation holistic approach enable us recognize semantic relations
carpenter wood highly similar relations mason stone 
however  holistic approach semantic relations suffers data sparsity
linguistic creativity problems holistic approach semantic composition 
could represent relation carpenter wood averaging vectors 
might enable us recognize carpenter wood mason stone 
would incorrectly suggest carpenter wood stone mason  problem
order sensitivity arises semantic relations arose semantic composition 
many ideas proposed composing vectors  landauer   dumais       
kintsch        mitchell   lapata         erk pado        point two problems
common several proposals  first  often adaptive
capacity represent variety possible syntactic relations phrase  example 
phrase horse draws  horse subject verb draws  whereas object
verb phrase draws horse  composition vectors horse draws
must able adapt variety syntactic contexts order properly model
given phrases  second  single vector weak handle long phrase  sentence 
document  single vector encode fixed amount structural information
dimensionality fixed  upper limit sentence length  hence
amount structure encoded  erk   pado        p        fixed dimensionality
allow information scalability 
simple  unweighted  averaging vectors lacks adaptive capacity  treats
kinds composition way  flexibility represent different
modes composition  good model must capacity adapt different situations 
   

fidomain function  dual space model

example  weighted averaging  weights tuned different syntactic
contexts  mitchell   lapata              
information scalability means size semantic representations grow
proportion amount information representing  size
representation fixed  eventually information loss  hand 
size representations grow exponentially 
one case problem information scalability arises approaches
map multiple vectors single vector  example  represent dog house adding
vectors dog house  mapping two vectors one   may information
loss  increase number vectors mapped single vector 
eventually reach point single vector longer contain information
multiple vectors  problem avoided try map multiple vectors
single vector 
suppose k dimensional vector floating point elements b bits each 
vector hold kb bits information  even allow b grow  k fixed 
eventually information loss  vector space model semantics  vectors
resistance noise  perturb vector noise threshold  
significant change meaning represents  therefore think
vector hypersphere radius   rather point  may put
bounds  r   r  range values elements vector   finite
number n hyperspheres radius packed bounded k dimensional
space  conway   sloane         according information theory  finite set
n messages  need log   n   bits encode message  likewise 
finite set n vectors  vector represents log   n   bits information 
therefore information capacity single vector bounded k dimensional space
limited log   n   bits 
past work suggests recognizing relations compositions closely connected
tasks  kintsch              mangalath  quesada    kintsch         goal research
unified model handle compositions relations  resolving
issues linguistic creativity  order sensitivity  adaptive capacity  information scalability 
considerations led us dual space model  consisting domain space
measuring domain similarity  i e   topic  subject  field similarity  function space
measuring function similarity  i e   role  relationship  usage similarity  
analogy   b    c    a b c d  example  traffic street water
riverbed   b relatively high domain similarity  traffic street come
domain transportation  c relatively high domain similarity  water
riverbed come domain hydrology   hand  c relatively
high function similarity  traffic water similar roles respective domains 
things flow  b relatively high function similarity  street
riverbed similar roles respective domains  things carry
things flow   combining domain function similarity appropriate ways 
   models vectors normalized unit length  e g   models use cosine measure
similarity   elements must lie within range          element outside range 
length vector greater one  general  floating point representations minimum
maximum values 

   

fiturney

recognize semantic relations traffic street analogous
relations water riverbed 
semantic composition  appropriate way combine similarities may depend
syntax composition  lets focus noun modifier composition example 
noun modifier phrase ab  for instance  brain doctor   head noun b  doctor 
modified adjective noun  brain   suppose word c  neurologist 
synonymous ab  functional role noun modifier phrase ab determined
head noun b  a brain doctor kind doctor  b relatively high degree
function similarity c  doctor neurologist function doctors  
b high degree domain similarity c  brain  doctor  neurologist come
domain clinical neurology   combining domain function similarity 
recognize brain doctor synonymous neurologist 
briefly  proposal compose similarity measures instead composing vectors 
is  apply various mathematical functions combine cosine similarity measures 
instead applying functions directly vectors  addresses information
loss problem  preserve vectors individual component words   we
map multiple vectors single vector   since two different spaces 
flexibility address problem adaptive capacity   model compositional 
resolves linguistic creativity problem  deal order sensitivity combining
similarity measures ways recognize effects word order 
might argued present model semantic composition 
way compare words form two phrases order derive measure similarity
phrases  example  section     derive measure similarity phrases
environment secretary defence minister  actually provide representation
phrase environment secretary  hand  past work problem
semantic composition  reviewed section      yields representation composite
phrase environment secretary different union representations
component words  environment secretary 
argument based assumption goal semantic composition
create single  general purpose  stand alone representation phrase  composite 
distinct union representations component words  assumption
necessary approach use assumption  believe
assumption held back progress problem semantic composition 
argue present model semantic composition  composition similarities  composition vectors  vectors represent individual words 
similarities inherently represent relations two  or more  things  composing vectors
yield stand alone representation phrase  composing similarities necessarily
yields linking structure connects phrase phrases  similarity composition
result stand alone representation phrase  practical applications
require stand alone representations  whatever practical tasks performed
stand alone representations phrases  believe performed equally well  or better 
similarity composition  discuss issue depth section   
   two similarity spaces give us options similarity composition one space  two types
characters       give us options generating strings one type character    alone  

   

fidomain function  dual space model

next section surveys related work modeling semantic composition
semantic relations  section   describes build domain function space  test
hypothesis value two separate spaces  create mono space 
merger domain function spaces  present four sets experiments dual space model section    evaluate dual space approach
multiple choice analogy questions sat  turney      b   multiple choice nounmodifier composition questions derived wordnet  fellbaum         phrase similarity rating problems  mitchell   lapata         similarity versus association problems
 chiarello  burgess  richards    pollock         discuss experimental results
section    section   considers theoretical questions dual space model  limitations model examined section    section   concludes 
paper assumes familiarity vector space models semantics 
overview semantic vsms  see papers handbook latent semantic analysis
 landauer  mcnamara  dennis    kintsch         review mitchell lapatas
       paper  survey turney pantel        

   related work
examine related work semantic composition relations  introduction  mentioned four problems semantic models  yield four desiderata
semantic model 
   linguistic creativity  model able handle phrases  in case
semantic composition  word pairs  in case semantic relations 
never seen before  familiar component words 
   order sensitivity  model sensitive order words
phrase  for composition  word pair  for relations   order affects
meaning 
   adaptive capacity  phrases  model flexibility represent
different kinds syntactic relations  word pairs  model
flexibility handle variety tasks  measuring degree relational
similarity two pairs  see section      versus measuring degree phrasal
similarity two pairs  see section      
   information scalability  phrases  model scale neither loss
information exponential growth representation size number component
words phrases increases  n ary semantic relations  turney      a  
model scale neither loss information exponential growth
representation size n  number terms relations  increases 
review past work light four considerations 
    semantic composition
let ab phrase  noun modifier phrase  assume vectors
b represent component words b  one earliest proposals semantic
composition represent ab vector c average b  landauer  
   

fiturney

dumais         using cosine measure vector similarity  taking average
set vectors  or centroid  adding vectors  c   a b  vector addition
works relatively well practice  mitchell   lapata               although lacks order
sensitivity  adaptive capacity  information scalability  regarding order sensitivity
adaptive capacity  mitchell lapata              suggest using weights  c   a b 
tuning weights different values different syntactic relations  experiments
 mitchell   lapata         weighted addition performed better unweighted addition 
kintsch        proposes variation additive composition
c sum a 
p
b  selected neighbours ni b  c     b   ni   neighbours vectors
words given vocabulary  i e   rows given wordcontext matrix  
neighbours chosen manner attempts address order sensitivity adaptive
capacity  still problem information scalability due fixed dimensionality 
utsumi        presents similar model  different way selecting neighbours 
mitchell lapata        found simple additive model peformed better
additive model included neighbours 
mitchell lapata              suggest element wise multiplication composition
operation  c   b  ci   ai bi   vector addition  element wise multiplication suffers lack order sensitivity  adaptive capacity  information scalability 
nonetheless  experimental evaluation seven compositional models two noncompositional models  element wise multiplication best performance  mitchell  
lapata        
another approach use tensor product composition  smolensky        aerts
  czachor        clark   pulman        widdows         outer product 
c   b  outer product two vectors  a b   n elements  n n
matrix  c   outer product three vectors n n n third order tensor 
results information scalability problem  representations grow exponentially large
phrases grow longer   furthermore  outer product perform well
element wise multiplication mitchell lapatas        experiments  recent work
tensor products  clark  coecke    sadrzadeh        grefenstette   sadrzadeh       
attempted address issue information scalability 
circular convolution similar outer product  outer product matrix
compressed back vector  c     b  plate        jones   mewhort        
avoids information explosion  results information loss  circular convolution
performed poorly mitchell lapatas        experiments 
baroni zamparelli        guevara        suggest another model composition
adjective noun phrases  core strategy share use holistic vectors
train compositional model  partial least squares regression  plsr   learn
linear model maps vectors component nouns adjectives linear
approximations holistic vectors phrases  linguistic creativity problem
avoided linear model needs holistic vectors training 
need holistic vectors plausible adjective noun phrases  given phrase
training data  linear model predicts holistic vector phrase  given
   ways avoid exponential growth  example  third order tensor rank  
three modes may compactly encoded three component vectors  kolda bader       
discuss compact tensor representations 

   

fidomain function  dual space model

component vectors adjective noun  works well adjective noun
phrases  clear generalize parts speech longer phrases 
one application semantic composition measuring similarity phrases  erk  
pado        mitchell   lapata         kernel methods applied closely
related task identifying paraphrases  moschitti   quarteroni         emphasis
kernel methods syntactic similarity  rather semantic similarity 
neural network models combined vector space models task
language modeling  bengio  ducharme  vincent    jauvin        socher  manning    ng 
      socher  huang  pennington  ng    manning         impressive results  goal
language model estimate probability phrase decide several
phrases likely  vsms improve probability estimates language model
measuring similarity words phrases smoothing probabilities
groups similar words  however  language model  words considered similar
degree exchanged without altering probability given phrase 
without regard whether exchange alters meaning phrase 
function similarity  measures degree words similar functional roles 
language models missing anything domain similarity 
erk pado        present model similar two parts 
vector space measuring similarity model selectional preferences  vector
space similar domain space model selectional preferences plays role
similar function space  individual word represented triple    ha  r  r  i 
consisting words vector  a  selectional preferences  r  inverse selectional
preferences  r    phrase ab represented pair triples  ha    b   i  triple a 
modified form triple represents individual word a  modifications
adjust representation model meaning altered relation b
phrase ab  likewise  triple b   modified form triple b represents b 
b   takes account affects b 
transformed a  represent influence b meaning a 
vector transformed new vector a  a    let rb vector represents
typical words consistent selectional preferences b  vector a 
composition rb   erk pado        use element wise multiplication
composition  a    rb   intention make typical vector x would
expected phrase xb  likewise  b  b     b    b ra
erk pados        model related models  thater  furstenau    pinkal       
address linguistic creativity  order sensitivity  adaptive capacity  information scalability 
suitable measuring similarity semantic relations  consider
analogy traffic street water riverbed  let ha    b   represent traffic  street
let hc     d  represent water  riverbed  transformation a  b  c  a    b    
c     d  reinforces connection traffic street water
riverbed  help us recognize relational similarity traffic  street
water  riverbed  course  models designed relational similarity 
surprising  however  goal find unified model handle
compositions relations 
   

fiturney

    semantic relations
semantic relations  make general observations order sensitivity  let
  b c   two word pairs let simr  a   b  c   d    measure degree
similarity relations   b c   d    b    c   good analogy 
simr  a   b  c   d  relatively high value  general  good model relational
similarity respect following equalities inequalities 

simr  a   b  c   d    simr  b   a    c 

   

simr  a   b  c   d    simr  c   d    b 

   

simr  a   b  c   d     simr  a   b    c 

   

simr  a   b  c   d     simr  a   d  c   b 

   

example  given carpenter  wood mason  stone make good analogy  follows
equation   wood  carpenter stone  mason make equally good analogy  also 
according equation    mason  stone carpenter  wood make good analogy 
hand  suggested equation    carpenter  wood analogous stone  mason 
likewise  indicated equation    poor analogy assert carpenter stone
mason wood 
rosario hearst        present algorithm classifying word pairs according
semantic relations  use lexical hierarchy map word pairs feature
vectors  classification scheme implicitly tell us something similarity  two word
pairs semantic relation class implicitly relationally similar
two word pairs different classes  consider relational similarity
implied rosario hearsts        algorithm  see problem order
sensitivity  equation   violated 
let simh  x  y    measure degree hierarchical similarity
words x y  simh  x  y  relatively high  x share common hypernym
relatively close given lexical hierarchy  essence  intuition behind
rosario hearsts        algorithm is  simh  a  c  simh  b  d  high 
simr  a   b  c   d  high  is  simh  a  c  simh  b  d  high enough 
  b c   assigned relation class 
example  consider analogy mason stone carpenter wood  common hypernym mason carpenter artisan  see simh  mason  carpenter 
high  common hypernym stone wood material  hence simh  stone  wood 
high  seems good analogy indeed characterized high values simh  a  c 
simh  b  d   however  symmetry simh  x  y  leads problem  simh  b  d  high 
simh  d  b  must high  implies simr  a   d  c   b  high  is 
incorrectly conclude mason wood carpenter stone  see equation    
later work classifying semantic relations used different algorithms 
underlying intuition hierarchical similarity  rosario  hearst    fillmore 
      nastase   szpakowicz        nastase  sayyad shirabad  sokolova    szpakowicz 
       use similar intuition here  since similarity function space closely related
   

fidomain function  dual space model

hierarchical similarity  simh  x  y   see later  section       however  including
domain space relational similarity measure saves us violating equation   
let simf  x  y    function similarity measured cosine vectors x
function space  let simd  x  y    domain similarity measured cosine
vectors x domain space  past researchers  rosario   hearst        rosario
et al         nastase   szpakowicz        veale        nastase et al          look
high values simf  a  c  simf  b  d  indicators simr  a   b  c   d  high 
look high values simd  a  b  simd  c  d   continuing previous example 
conclude mason wood carpenter stone  wood
belong domain masonry stone belong domain carpentry 
let determiner  e g   the  a  an   hearst        showed patterns form
x  a bird crow  kind x  the crow kind
bird  used infer x hypernym  bird hypernym crow  
pairpattern matrix vsm rows word pairs columns
various x       patterns  turney  littman  bigham  shnayder        demonstrated
pairpattern vsm used measure relational similarity  suppose
pair pattern matrix x word pair   b corresponds row vector xi c  
corresponds xj   approach measure relational similarity simr  a   b  c   d 
cosine xi xj  
first patterns pairpattern matrices generated hand  turney
et al         turney   littman         later work  turney      b  used automatically
generated patterns  authors used variations technique  nakov   hearst 
            davidov   rappoport        bollegala  matsuo    ishizuka        seaghdha
  copestake         models suffer linguistic creativity problem 
models noncompositional  holistic   cannot scale handle
huge number possible pairs  even largest corpus cannot contain pairs
human speaker might use daily conversation 
turney      b  attempted handle linguistic creativity problem within holistic
model using synonyms  example  corpus contain traffic street within
certain window text  perhaps might contain traffic road  contain
water riverbed  perhaps water channel  however  best partial
solution  turneys      b  algorithm required nine days process     multiple choice sat
analogy questions  using dual space model  without specifying advance word
pairs might face  answer     questions seconds  see section      
compositional models scale better holistic models 
mangalath et al         presented model semantic relations represents word
pairs vectors ten abstract relational categories  hyponymy  meronymy  taxonomy  degree  approach construct kind second order vector space
elements vectors degrees similarity  calculated cosines
first order wordcontext matrix 
instance  carpenter  wood represented second order vector composed
ten cosines calculated first order vectors  second order vector  value
element corresponding to  say  meronymy would cosine two first order vectors  x
y  vector x would sum first order vectors carpenter wood 
vector would sum several vectors words related meronymy 
   

fiturney

part  whole  component  portion  contains  constituent  segment  cosine
x would indicate degree carpenter wood related meronymy 
mangalath et al s        model suffers information scalability order sensitivity
problems  information loss takes place first order vectors summed
high dimensional first order space reduced ten dimensional second order
space  order sensitivity problem second order vectors violate equation   
pairs c     c represented second order vector 
natural proposal represent word pair   b way would represent
phrase ab  is  whatever compositional model phrases could
applied word pairs  however problems compositional model order
sensitivity information scalability carry word pairs  example  represent
  b c     b c   b  violate equation      b   b  
b   b a 

   three vector spaces
section  describe three vector space models  three spaces consist word
context matrices  rows correspond words columns correspond
contexts words occur  differences among three spaces kinds
contexts  domain space uses nouns context  function space uses verb based patterns
context  mono space merger domain function contexts  mono space
created order test hypothesis useful separate domain
function spaces  mono space serves baseline 
    constructing wordcontext matrices
building three spaces involves series steps  three main steps 
substeps  first last steps three spaces 
differences spaces result differences second step 
   find terms contexts  input  corpus lexicon  output  terms contexts 
     extract terms lexicon find frequencies corpus 
     select terms given frequency candidate rows frequency
matrix 
     selected term  find phrases corpus contain term within
given window size 
     use tokenizer split phrases tokens 
     use part of speech tagger tag tokens phrases 
   build termcontext frequency matrix  input  terms contexts  output 
sparse frequency matrix 
     convert tagged phrases contextual patterns  candidate columns  
     contextual pattern  count number terms  candidate rows 
generated pattern rank patterns descending order counts 
     select top nc contextual patterns columns matrix 
   

fidomain function  dual space model

     initial set rows  from step       drop row match
top nc contextual patterns  yielding final set nr rows 
     row  term  column  contextual pattern   count number
phrases  from step      containing given term matching given
pattern  output resulting numbers sparse frequency matrix 
   weight elements smooth matrix  input  sparse frequency matrix 
output  singular value decomposition  svd  weighted matrix 
     convert raw frequencies positive pointwise mutual information  ppmi 
values 
     apply svd ppmi matrix output svd component matrices 
input corpus step   collection web pages gathered university websites
webcrawler   corpus contains approximately       words  comes
    gigabytes plain text  facilitate finding term frequencies sample phrases 
indexed corpus wumpus search engine  buttcher   clarke          rows
matrices selected terms  words phrases  wordnet lexicon  
found selecting terms wordnet resulted subjectively higher quality
simply selecting terms high corpus frequencies 
step      extract unique words phrases  n grams  index sense file
wordnet      skipping n grams contain numbers  only letters  hyphens  spaces
allowed n grams   find n gram corpus frequencies querying wumpus
n gram  n grams frequency least     least   characters
candidate rows step      selected n gram  query wumpus find
maximum        phrases step       phrases limited window   words
left n gram   words right  total window size      n words 
use opennlp       tokenize part of speech tag phrases  steps           
tagged phrases come    gigabytes  
step      generate contextual patterns part of speech tagged phrases 
different kinds patterns created three different kinds spaces  details
step given following subsections  phrase may yield several patterns 
three spaces         rows  maximum        phrases
per row several patterns per phrase  result millions distinct patterns 
filter patterns steps          select top nc patterns shared
largest number rows  given large number patterns  may fit
ram  work limited ram  use linux sort command  designed
efficiently sort files large fit ram  row  make file
distinct patterns generated row  concatenate files
  
  
  
  

corpus collected charles clarke university waterloo 
wumpus available http   www wumpus search org  
wordnet available http   wordnet princeton edu  
limit        phrases per n gram required make wumpus run tolerable amount time 
finding phrases time consuming step construction spaces  use solid state
drive  ssd  speed step 
   opennlp available http   incubator apache org opennlp  
   tagged phrases available author request 

   

fiturney

rows alphabetically sort patterns concatenated file  sorted file 
identical patterns adjacent  makes easy count number occurrences
pattern  counting  second sort operation yields ranked list patterns 
select top nc  
possible candidate rows step     might match
patterns step      rows would zeros matrix  remove
step      finally  output sparse frequency matrix f nr rows nc
columns  i th row corresponds n gram wi j th column corresponds
contextual pattern cj   value element fij f number phrases
containing wi  from step      generate pattern cj  in step       step      use
svdlibc      calculate singular value decomposition  format output
sparse matrix step     chosen meet requirements svdlibc   
step      apply positive pointwise mutual information  ppmi  sparse frequency matrix f  variation pointwise mutual information  pmi   church  
hanks        turney        pmi values less zero replaced
zero  niwa   nitta        bullinaria   levy         let x matrix results
ppmi applied f  new matrix x number rows columns
raw frequency matrix f  value element xij x defined follows 
fij
pij   pnr pnc

j   fij

i  

   

pnc

j   fij
pi   pnr pnc

   

pnr
f
pncij
  pnr i  

   

i  

pj

i  



j   fij
j   fij

pij
pi pj



pmiij   log

pmiij pmiij    
xij  
  otherwise

   
   

definition  pij estimated probability word wi occurs context
cj   pi estimated probability word wi   pj estimated probability
context cj   wi cj statistically independent  pij   pi pj  by definition
independence   thus pmiij zero  since log          product pi pj
would expect pij wi occurs cj pure random chance  hand 
interesting semantic relation wi cj   expect pij larger
would wi cj indepedent  hence find pij   pi pj  
thus pmiij positive  word wi unrelated  or incompatible with  context cj  
may find pmiij negative  ppmi designed give high value xij
interesting semantic relation wi cj   otherwise  xij value
zero  indicating occurrence wi cj uninformative 
    svdlibc available http   tedlab mit edu dr svdlibc  

   

fidomain function  dual space model

finally  step      apply svdlibc x  svd decomposes x product
three matrices uvt   u v column orthonormal form  i e   columns
orthogonal unit length  ut u   vt v   i  diagonal matrix
singular values  golub   van loan         x rank r  rank r  let
k   k   r  diagonal matrix formed top k singular values  let uk
vk matrices produced selecting corresponding columns u v 
matrix uk k vkt matrix rank k best approximates original matrix x 
sense minimizes approximation errors  is  x   uk k vkt minimizes
kx xkf matrices x rank k  k       kf denotes frobenius norm  golub
  van loan         final output three matrices  uk   k   vk   form
truncated svd  x   uk k vkt  
    domain space
intuition behind domain space domain topic word characterized
nouns occur near it  use relatively wide window ignore syntactic
context nouns appear 
domain space  step      tagged phrase generates two contextual
patterns  contextual patterns simply first noun left given n gram
 if one  first noun right  if one   since window size  
words side n gram  usually nouns sides n gram 
nouns may either common nouns proper nouns  opennlp uses penn treebank
tags  santorini         include several different categories noun tags 
noun tags begin capital n  simply extract first words left right
n gram tags begin n  extracted nouns converted lower
case  noun appears sides n gram  one contextual pattern
generated  extracted patterns always unigrams  noun compound 
component noun closest n gram extracted 
table   shows examples n gram boat  note window   words
count punctuation  number tokens window may greater
number words window  see table   row vector
n gram boat frequency matrix f nonzero values  for example 
columns lake summer  assuming contextual patterns make
filtering step      
step      set nc         step      drop rows zero 
left nr equal          ppmi  which sets negative elements zero 
            nonzero values  matrix density        table   shows
contextual patterns first five columns last five columns  the columns
order ranks step       count column table gives number rows
 n grams  generate pattern  that is  counts mentioned step      
last patterns begin c counts ties broken
alphabetical order 
   

fiturney

tagged phrases
would md visit vb big nnp lake nnp and cc take vb our prp 
boat nn on in this dt huge jj beautiful jj lake nn     there ex
was vbd

patterns
lake

 

the dt large jj paved jj parking nn lot nn in in the dt boat nn
ramp nn area nn and cc walk vb south rb along in the dt

lot
ramp

 

building vbg permit nn       anyway rb     we prp should md
have vb a dt boat nn next jj summer nn with in skiing nn
and cc tubing nn paraphernalia nns    

permit
summer

 

table    examples step     domain space n gram boat  three tagged
phrases generate five contextual patterns 

column
 
 
 
 
 

pattern
time
part
years
way
name

count
      
      
      
      
      

column
      
      
      
      
      

pattern
clu
co conspirator
conciseness
condyle
conocer

count
   
   
   
   
   

table    contextual patterns first last columns domain space  clu
abbreviation chartered life underwriter terms  condyle round
bump bone forms joint another bone  conocer
spanish verb know  sense acquainted person 

    function space
concept function space function role word characterized
syntactic context relates verbs occur near it  use narrow
window function space domain space  based intuition proximity
verb important determining functional role given word  distant verb
less likely characterize function word  generate relatively complex patterns
function space  try capture syntactic patterns connect given word
nearby verbs 
step      tagged phrase generates six contextual patterns  given
tagged phrase  first step cut window   tokens given n gram
  tokens it  remaining tokens left n gram punctuation  punctuation everything left punctuation removed 
remaining tokens right n gram punctuation  punctuation everything right punctuation removed  lets call remaining tagged phrase
truncated tagged phrase 
next replace given n gram truncated tagged phrase generic marker 
   

fidomain function  dual space model

x  simplify part of speech tags reducing first character
 santorini         example  various verb tags  vb  vbd  vbg  vbn  vbp 
vbz  reduced v  truncated tagged phrase contains v tag  generates
zero contextual patterns  phrase contains v tag  generate two types
contextual patterns  general patterns specific patterns 
general patterns  verbs  every token v tag  tags removed
 naked verbs  tokens reduced naked tags  tags without words  
specific patterns  verbs  modals  tokens tags   prepositions  tokens tags  
 tokens tags  tags removed tokens reduced naked
tags   see table   examples  
general specific patterns  left x  trim leading naked tags 
right x  trim trailing naked tags  tag to  replace
remaining naked tags to  sequence n tags  n n n n n  likely
compound noun  reduce sequence single n 
given truncated tagged phrase  two patterns  one general pattern
one specific pattern  either patterns tokens left right
sides x  make two patterns duplicating x splitting pattern
point two xs  one new patterns verb  drop
it  thus may three specific patterns three general patterns
given truncated tagged phrase  specific general patterns same  one
generated 
table   shows examples n gram boat  note every pattern must contain
generic marker  x  least one verb 
truncated tagged phrases
the dt canals nns by in boat nn
and cc wandering vbg the dt

patterns
x c wandering
x c wandering

types
general
specific

 

a dt charter nn fishing vbg boat nn
captain nn named vbn jim nnp

fishing x n named
fishing x
x n named

general
general
general

 

used vbn from in a dt
and cc lowered vbd to to

used x c lowered
used x
x c lowered
used x c lowered
used x
x c lowered

general
general
general
specific
specific
specific

 

boat nn

table    examples step     function space n gram boat  three truncated
tagged phrases generate eleven contextual patterns 

step      set nc         step      rows zero dropped 
nr          ppmi             nonzero values  yielding matrix density
       table   shows contextual patterns first last five columns 
   

fiturney

last patterns begin counts ties broken
alphabetical order 
column
 
 
 
 
 

pattern
x
x n
x
x
x

count
      
      
      
      
      

column
      
      
      
      
      

pattern
since x n
sinking x
supplied x
supports x n
suppressed x

count
   
   
   
   
   

table    contextual patterns first last columns function space 
contextual patterns function space complex patterns
domain space  motivation greater complexity observation mere
proximity enough determine functional roles  although seems sufficient determining domains  example  consider verb gives  word x occurs near
gives  x could subject  direct object  indirect object verb  determine
functional role x  need know case applies  syntactic context connects x gives provides information  contextual pattern x gives implies
x subject  gives x implies x object  likely direct object  gives x
suggests x indirect object  modals prepositions supply information
functional role x context given verb  verb gives appears   
different contextual patterns  i e             columns function space correspond
syntactic patterns contain gives  
many row vectors function space matrix correspond verbs  might
seem surprising characterize function verb syntactic relation
verbs  consider example  verb run  row vector run
ppmi matrix function space       nonzero values  is  run characterized
      different contextual patterns 
note appearing contextual pattern different nonzero value
contextual pattern  character string word run appears    different
contextual patterns  run x  row vector word run nonzero
values       contextual patterns  columns   x 
    mono space
mono space simply merger domain space function space  step     
take union        domain space columns        function space columns 
resulting total nc         columns  step      total nr        
rows  mono matrix ppmi             nonzero values  yielding density
       values mono frequency matrix f equal corresponding values
domain function matrices  rows mono space matrix
corresponding rows function space matrix  rows  corresponding values
zeros  but nonzero elements rows  correspond values
domain matrix  
   

fidomain function  dual space model

    summary spaces
table   summarizes three matrices  following four sets experiments  use
three matrices  the domain  function  mono matrices  cases 
generate different matrices set experiments  three four sets experiments
involve datasets used past researchers  made special
effort ensure words three datasets corresponding rows three
matrices  intention three matrices adequate handle
applications without special customization 
space
domain
function
mono

rows  nr  
       
       
       

columns  nc  
      
      
       

nonzeros  after ppmi 
           
          
           

density  after ppmi 
     
     
     

table    summary three spaces 

    using spaces measure similarity
following experiments  measure similarity two terms  b  cosine
angle corresponding row vectors  b 
sim a  b    cos a  b   


b

kak kbk

    

cosine angle two vectors inner product vectors 
normalized unit length  cosine ranges   vectors point
opposite directions       degrees     point direction    
degrees   vectors orthogonal      degrees   cosine zero  raw
frequency vectors  necessarily cannot negative elements  cosine cannot
negative  weighting smoothing often introduce negative elements  ppmi weighting
yield negative elements  truncated svd generate negative elements  even
input matrix negative values 
semantic similarity two terms given cosine two corresponding rows
uk pk  see section       two parameters uk pk need set 
parameter k controls number latent factors parameter p adjusts weights
factors  raising corresponding singular values pk power p 
parameter k well known literature  landauer et al          p less familiar 
use p suggested caron         following experiments  section    
explore range values p k 
suppose take word w list words descending order
cosines w  using uk pk calculate cosines  p high  go list 
cosines nearest neighbours w decrease slowly  p low  decrease
quickly  is  high p results broad  fuzzy neighbourhood low p yields sharp 
crisp neighbourhood  parameter p controls sharpness similarity measure 
   

fiturney

reduce running time svdlibc  limit number singular values
      usually results less      singular values  example  svd
domain space      singular values  long k greater      
experiment range k values without rerunning svdlibc  generate uk pk
u     p     simply deleting      k columns smallest singular values 
experiments  vary k          increments         values k 
vary p      increments         values p   p   
give weight factors smaller singular values  p     factors
larger singular values weight  caron        observes researchers use
either p     p      is  use either uk uk k  
let simf  a  b    function similarity measured cosine vectors b
function space  let simd  a  b    domain similarity measured cosine
vectors b domain space  similarity measure combines simd  a  b 
simf  a  b   four parameters tune  kd pd domain space kf pf
function space 
one space  feasible us explore             combinations parameter
values  two spaces                   combinations values  make search
tractable  initialize parameters middle ranges  kf   kd      
pf   pd      alternate tuning simd  a  b   i e   kd pd   holding
simf  a  b   i e   kf pf   fixed tuning simf  a  b  holding simd  a  b  fixed  stop
search improvement performance training data  almost
cases  local optimum found one pass  is  tuned parameters
once  improvement try tune second time  thus typically
evaluate             parameter values    tune one similarity  tune other 
try first see improvement possible    
could use standard numerical optimization algorithm tune four parameters  algorithm use takes advantage background knowledge
optimization task  know small variations parameters make small changes
performance  need make fine grained search  know
simd  a  b  simf  a  b  relatively independent  optimize separately 
rows matrices based terms wordnet index sense file 
file  nouns singular forms verbs stem forms  calculate
sim a  b   first look exact matches b terms correspond
rows given matrix  domain  function  mono   exact match found 
use corresponding row vector matrix  otherwise  look alternate forms
terms  using validforms function wordnet  querydata perl interface
wordnet    automatically converts plural nouns singular forms verbs
stem forms  none alternate forms exact match row matrix 
map term zero vector length k 

    use perl data language  pdl  searching parameters  calculating cosines  operations
vectors matrices  see http   pdl perl org  
    wordnet  querydata available http   search cpan org dist wordnet querydata  

   

fidomain function  dual space model

    composing similarities
approach semantic relations compositions combine two similarities 
simd  a  b  simf  a  b   various ways  depending task hand syntax
phrase hand  general  want combined similarity high
component similarities high  want values component similarities
balanced  achieve balance  use geometric mean combine similarities  instead
arithmetic mean  geometric mean suitable negative numbers 
cosine negative cases  hence define geometric mean zero
component similarities negative 

geo x    x            xn    

 x  x        xn    n xi                  n
  otherwise

    

    element wise multiplication
one successful approaches composition  far  element wise multiplication  c   b  ci   ai bi  mitchell   lapata               approach
makes sense elements vectors negative  elements
b positive  relatively large values ai bi reinforce other  resulting
large value ci   makes intuitive sense  ai bi highly negative 
ci highly positive  although intuition says ci highly negative  mitchell
lapata              designed wordcontext matrices ensure vectors
negative elements 
values matrix uk pk typically half positive half negative 
use element wise multiplication baseline following experiments 
fair baseline  cannot simply apply element wise multiplication row vectors uk pk  
one solution would use ppmi matrix  x  negative elements 
would allow element wise multiplication take advantage smoothing effect
svd  solution use row vectors x   uk k vkt   although ppmi matrix 
x  sparse  see table     x uk pk density      
let a  b  vectors x correspond terms b  row
vectors benefit smoothing due truncated svd  elements almost
positive  negative elements  set zero  let c    a  b   
apply element wise multiplication vectors  multiply vk kp   
resulting vector c   c  vk p 
compared row vectors matrix
k
p
uk k  
p 

x vk p 
k      uk k vk   vk k  

 
 
 

uk k vkt vk kp 
uk k p 
k
uk pk

    
    
    
    

note that  since vk column orthonormal  vkt vk equals ik   k k identity matrix 
   

fiturney

similarly  row vector uk pk   find counterpart a  x multiplying

 p
k vk  

 uk pk   k p vkt     uk pk  p
k vk

    

  uk k vkt

    

  x

    

let nn x   nn nonnegative  function converts negative elements vector
x zero 

nn hx            xn i    hy            yn

xi xi    
yi  
  otherwise

    
    

version element wise multiplication may expressed follows 
p 
 p

c    nn a p
k vk   nn bk vk    vk k

    

another way deal element wise multiplication would use nonnegative
matrix factorization  nmf   lee   seung        instead svd  yet found
implementation nmf scales matrix sizes  table    
past experiments smaller matrices  svd nmf similar performance 

   experiments varieties similarities
section presents four sets experiments  first set experiments presents dualspace model semantic relations evaluates model multiple choice analogy
questions sat  second set presents model semantic composition
evaluates multiple choice questions constructed wordnet  third
set applies dual space model phrase similarity dataset mitchell lapata
        final set uses three classes word pairs chiarello et al         test
hypothesis dual space model  domain space function space capture
intuitive concepts association similarity 
    similarity relations
evaluate dual space model applied task measuring similarity
semantic relations  use set     multiple choice analogy questions sat
college entrance exam  turney      b   table   gives example one questions 
task select choice word pair analogous  most relationally similar 
stem word pair 
let   b represent stem pair  e g   lull  trust   answer sat questions
selecting choice pair c   maximizes relational similarity  simr  a   b  c   d   defined
follows 
   

fidomain function  dual space model

stem 
choices 

solution 

   
   
   
   
   
   

lull trust
balk fortitude
betray loyalty
cajole compliance
hinder destination
soothe passion
cajole compliance

table    example question     sat analogy questions  lulling person
trust analogous cajoling person compliance 

sim   a   b  c   d    geo simf  a  c   simf  b  d  

    

sim   a   b  c   d    geo simd  a  b   simd  c  d  

    

sim   a   b  c   d    geo simd  a  d   simd  c  b  

sim   a   b  c   d  sim   a   b  c   d  sim   a   b  c   d 
simr  a   b  c   d   
  otherwise

    
    

intent sim  measure function similarity across two pairs  domain
similarity inside two pairs measured sim    whereas domain similarity across
two pairs given sim    relational similarity  simr   simply function similarity 
sim    subject constraint domain similarity inside pairs  sim    must
less domain similarity across pairs  sim   
figure   conveys main ideas behind equations        want high function
similarities  indicated f    c b   d  measured sim    prefer
relatively high domain similarities  marked d    b c    measured sim    
contrast relatively low domain similarities   d    c   b  as given sim      
using example table    see lulling person trust analogous
cajoling person compliance  since functional role lull similar functional
role cajole  both involve manipulating person  functional role trust similar
functional role compliance  both states person in  
captured sim    constraint sim   a   b  c   d  sim   a   b  c   d  implies
domain similarities lull  trust  the domain confidence loyalty  cajole  compliance
 the domain obedience conformity  greater equal domain
similarities lull  compliance cajole  trust 
analogy way mapping knowledge source domain target domain
 gentner         source domain mapped c target domain 
play role source domain c plays target domain 
theory behind sim    b source domain c target
    recently came across rectangular structure lepage shin ichis        paper
morphological analogy  see figure     although algorithm task differ considerably
algorithm task lepage shin ichi         independently discovered
underlying structure analogical reasoning 

   

fiturney






b


f

c

f





simr  a   b  c   d 
relational similarity

figure    diagram reasoning behind equations        f represents high
function similarity  means high domain similarity  indicates low
domain similarity 

domain  internal domain similarity b internal domain similarity
c less cross domain similarities  motivates constraint
sim  sim    definition natural expression gentners        theory analogy 
recall four equations introduced section      repeat equations
convenience 

simr  a   b  c   d    simr  b   a    c 

    

simr  a   b  c   d    simr  c   d    b 

    

simr  a   b  c   d     simr  a   b    c 

    

simr  a   b  c   d     simr  a   d  c   b 

    

inspection show definition relational similarity equation    satisfies
requirements equations                 understood considering
figure    equation    tells us rotate figure   vertical axis without
altering network similarities  due symmetry figure  equation    tells
us rotate figure   horizontal axis without altering network
similarities 
hand  cannot swap c holding b fixed 
would change f links  although would change links  
words  sim  sim  would changed  although sim  would affected 
therefore equation    satisfied 
also  cannot swap b holding c fixed  would change
links  although would change f links   words  sim 
sim  would changed  although sim  would affected  therefore equation   
   

fidomain function  dual space model

satisfied  see sim  would violate equation     due symmetry
cosines  simf  b  d    simf  d  b   constraint sim   a   b  c   d  sim   a   b  c   d  breaks
symmetry 
another way break symmetry  equation    satisfied  would use
similarity measure inherently asymmetric  skew divergence  equation    
symmetry broken natural way considering domain function similarity
apply analogies  need introduce inherently asymmetric measure  also 
note symmetries equations       desirable  wish break
symmetries 
would reasonable include simd  a  c  simd  b  d  sim    decided
leave out  seems us function similarities simf  a  c  simf  b  d  
high values good analogy  might cause simd  a  c  simd  b  d 
relatively high  even though cross domains  people observe certain kind
abstract function similarity frequently  function similarity might become popular
topic discussion  could result high domain similarity 
example  carpenter  wood analogous mason  stone  domain carpenter  wood
carpentry domain mason  stone masonry  functional role carpenter
similar functional role mason  artisans  although carpenter
mason belong different domains  high degree abstract function similarity
may result discussions mention together  discussions specialized trades  skilled manual labour  construction industry  workplace injuries 
words  high function similarity two words may cause rise domain
similarity  therefore include simd  a  c  simd  b  d  sim   
five choices sat question relational similarity zero  skip
question  use ten fold cross validation set parameters sat questions 
parameter values selected nine ten folds  kd        pd        kf       
pf        parameters determined      sat questions answered
seconds  equation    correctly answers     questions  skips   questions 
incorrectly answers     questions  achieving accuracy       
      comparison past work
comparison  average score senior highschool students applying us universities
       acl wiki lists many past results     sat questions    table  
shows top ten results time writing  table  dual space refers dualspace model using equation     four past results achieved accuracy      
higher  four used holistic approaches hence able address issue
linguistic creativity  best previous algorithm attains accuracy            correct 
  skipped      incorrect   turney      b   difference            
statistically significant     confidence level  according fishers exact test 
majority algorithms table   unsupervised  dual space  pairclass
 turney      b   bagpack  herdagdelen   baroni        use limited supervision  pairclass bagpack answer given sat question learning binary classification model
specific given question  training set given question consists one
    see http   aclweb org aclwiki index php title sat analogy questions 

   

fiturney

algorithm
lsa predication
know best
k means
bagpack
vsm
dual space
bmi
pairclass
pert
lra
human

reference
mangalath et al        
veale       
bicici yuret       
herdagdelen baroni       
turney littman       
bollegala et al        
turney      b 
turney      a 
turney      b 
average us college applicant

accuracy
    
    
    
    
    
    
    
    
    
    
    

    confidence
        
        
        
        
        
        
        
        
        
        
        

table    top ten results     sat questions  acl wiki     
confidence intervals calculated using binomial exact test 

positive training example  stem pair question  ten randomly selected pairs
 assumed  negative training examples  induced binary classifier used assign
probabilities five choices probable choice guess  dual space uses
training set tune four numerical parameters  three algorithms best
described weakly supervised 
      sensitivity parameters
see sensitive dual space model values parameters  perform
two exhaustive grid searches  one coarse  wide grid another fine  narrow
grid  point grids  evaluate dual space model using whole set
    sat questions  narrow grid search centred parameter values
selected nine ten folds previous experiment  kd        pd       
kf        pf        searches evaluate   values parameter  yielding total
         parameter settings  table   shows values explored two grid
searches table   presents minimum  maximum  average  standard deviation
accuracy two searches 
grid
coarse

fine

parameter
kd
pd
kf
pf
kd
pd
kf
pf

   
    
   
    
   
    
   
   

   
    
   
    
   
    
   
   

values
        
   
   
        
   
   
   
   
    
   
   
   
   
   

    
   
    
   
    
   
   
   

table    range parameter values two grid searches 
   

fidomain function  dual space model

grid
coarse
fine

minimum
    
    

accuracy
maximum average
    
    
    
    

standard deviation
   
   

table    sensitivity dual space model parameter settings 
accuracy attained heuristic search  described section      ten fold
cross validation         table     near best accuracy fine grid search using
whole set     sat questions         table     evidence heuristic search
effective  accuracy coarse search varies              demonstrates
importance tuning parameters  hand  accuracy fine search
spans narrower range lower standard deviation  suggests dualspace model overly sensitive relatively small variations parameter values 
is  parameters reasonably stable   that nine ten folds cross validation
select parameters evidence stability  
      parts speech
since domain space based nouns function space based verbs  interesting
know performance dual space model varies different parts speech 
answer this  manually labeled     sat questions part of speech labels 
labels single pair ambiguous  labels become unambiguous context
whole question  example  lull  trust could noun  verb  context
table    must verb  noun 
table    splits results various parts speech  none differences
table statistically significant     confidence level  according fishers
exact test  larger varied set questions needed determine part
speech affects dual space model 
parts speech
noun noun
noun adjective adjective noun
noun verb verb noun
adjective adjective
verb adjective adjective verb
verb verb
verb adverb adverb verb


right
  
  
  
 
  
  
 
   

accuracy
    
    
    
    
    
    
   
    

wrong
  
  
  
  
 
 
 
   

skipped
 
 
 
 
 
 
 
 

total
   
  
  
  
  
  
 
   

table     performance dual space model various parts speech 

      order sensitivity
seems function space work equation     use sim  alone 
dropping constraint sim  sim    accuracy drops             
   

fiturney

drop statistically significant  hypothesize small drop due design
sat test  primarily intended test students understanding functional
roles  domains 
verify hypothesis  reformulated sat questions would test
function domain comprehension  method first expand choice pair
c   including stem pair   b  resulting full explicit analogy   b    c   d 
expanded choice    b    c   d  generate another choice       c   b  table    shows
reformulation table    due symmetry  sim  must assign similarity
  b    c        c   b  new ten choice test evaluates function domain
similarities 
choices 

solution 

   
   
   
   
   
   
   
   
   
    
   

lull trust  balk fortitude
lull fortitude  balk trust
lull loyalty  betray trust
lull trust  betray loyalty
lull compliance  cajole trust
lull trust  cajole compliance
lull destination  hinder trust
lull trust  hinder destination
lull trust  soothe passion
lull passion  soothe trust
lull trust  cajole compliance

table     expanded sat question  designed test function domain comprehension  choices         similarity according sim   

task expanded ten choice sat questions original
five choice questions  select best analogy  solution table   
solution table    except stem pair explicit table     signficant
change five new distractors added choices  answer ten choice
questions selecting choice   b    c   maximizes simr  a   b  c   d  
ten choice reformulated sat test  simr  equation     attains accuracy
       whereas sim  alone  equation     achieves        difference statistically
significant     confidence level  according fishers exact test  stringent
test supports claim function similarity insufficient itself 
test value two separate spaces  use single space
simd simf equation     model still four parameters tune  kd   pd   kf  
pf   matrix used similarities  best result accuracy
      ten question reformulated sat test  using function space simd
simf   significantly       accuracy dual space model simd
based domain space simf based function space      confidence level  fishers
exact test  
table    summarizes results  cases matrix simd used 
model based sim  alone  equation      cases  model based
simr  equation      five choice ten choice sat questions  original
   

fidomain function  dual space model

dual space model accurate modified models  significant column
indicates whether accuracy modified model significantly less original
dual space model      confidence level  fishers exact test   difficult ten choice
questions clearly show value two distinct spaces 
algorithm
dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space

accuracy
    
    
    
    
    
    
    
    
    
    
    
    
    
    

significant

yes
yes

yes
yes
yes
yes
yes
yes
yes
yes

questions
five choice
five choice
five choice
five choice
five choice
five choice
five choice
ten choice
ten choice
ten choice
ten choice
ten choice
ten choice
ten choice

matrix simd
domain space
function space
mono space
domain space
used
used
used
domain space
function space
mono space
domain space
used
used
used

matrix simf
function space
function space
mono space
domain space
function space
mono space
domain space
function space
function space
mono space
domain space
function space
mono space
domain space

table     accuracy original five choice questions reformulated ten choice
questions  modified models  intentionally use wrong matrix  or
matrix  simd simf   modified models show accuracy decreases
one space used 

      summary
dual space model performs well current state of the art holistic model
addresses issue linguistic creativity  results reformulated sat questions
support claim value two separate spaces 
mentioned section      task classifying word pairs according
semantic relations  rosario   hearst        rosario et al         nastase   szpakowicz 
      closely connected problem measuring relational similarity  turney      b 
applied measure relational similarity relation classification using cosine similarity
measure nearness nearest neighbour supervised learning algorithm  dualspace model  equation     suitable relation classification nearest neighbour
algorithm 
    similarity compositions
second set experiments  apply dual space model noun modifier compositions  given vectors dog  house  kennel  would able recognize
dog house kennel synonymous  compare dual space model holistic
approach  vector addition  element wise multiplication  approaches evaluated
   

fiturney

using multiple choice questions automatically generated wordnet  using
wordnet  querydata perl interface wordnet  table    gives example one
noun modifier questions 
stem 
choices 

solution 

   
   
   
   
   
   
   
   

dog house
kennel
dog
house
canine
dwelling
effect
largeness
kennel

table     example multiple choice noun modifier composition question 
questions  stem bigram choices unigrams  choice    
correct answer      modifier      head noun  choice     synonym
hypernym modifier     synonym hypernym head noun 
synonyms hypernyms found  noun randomly chosen  last two choices     
     randomly selected nouns  choices         either nouns adjectives 
choices must nouns 
stem bigram choice unigrams must corresponding rows function
space  the space least number rows   stem bigram must noun sense
wordnet  it may senses parts speech   solution unigram      
must member synset  synonym set  first noun sense stem bigram
 the frequent dominant sense bigram  bigram used noun  
cannot simply hyphenation  dog house  concatenation  doghouse 
stem bigram 
requirements result total      seven choice questions  randomly
split     training  parameter tuning       testing    questions deliberately designed difficult  particular  approaches strongly attracted
choices          furthermore  attempt ensure stem bigrams
compositional  may idiomatic expressions compositional approach
could possibly get right  want bias questions imposing theories
distinguishing compositions idioms construction 
let ab represent noun modifier bigram  dog house  let c represent unigram
 kennel   answer multiple choice questions selecting unigram maximizes
compositional similarity  simc  ab  c   defined follows 

sim   ab  c    geo simd  a  c   simd  b  c   simf  b  c  

sim   ab  c     c b    c
simc  ab  c   
  otherwise
    questions available online appendix http   jair org  

   

    
    

fidomain function  dual space model

equations       illustrated figure   


  

b



f

  

c
simc  ab  c 
noun modifier compositional similarity

figure    diagram equations       
thinking behind sim  c  kennel   high domain similarity
modifier  dog  head noun b  house   furthermore  function
bigram ab  dog house  determined head noun b  house   head noun
high function similarity c  kennel    add constraints    c b    c
sim  tends high values sim   ab  a  sim   ab  b     seems
plausible humans use constraints this  reason dog house cannot mean
thing house  extra word dog dog house would serve purpose 
would meaningless noise   
constraints    c b    c could expressed terms similarities 
simd  a  c    simd  b  c    t  high threshold  e g           would
add another parameter model  decided keep model relatively simple 
seven choices noun modifier question compositional similarity
zero  skip question  training set  best parameter settings kd       
pd        kf        pf        testing set  equation    correctly answers    
questions  skips    questions  incorrectly answers      yielding accuracy       
      comparison approaches
mitchell lapata        compared many different approaches semantic composition
experiments  considered one task  the task examine section      
paper  chosen compare smaller number approaches larger
number tasks  include element wise multiplication experiments 
approach best performance mitchell lapatas        experiments  vector
    spite constraints  still worthwhile include head noun modifier distractors
multiple choice questions  enables us experimentally evaluate impact
distractors various algorithms constraints removed  see table      also  future users
dataset may find way avoid distractors without explicit constraints 
    philosophy language  grice        argued proper interpretation language requires us
charitably assume speakers generally insert random words speech 

   

fiturney

addition included due historical importance simplicity  although mitchell
lapata        found weighted addition better unweighted addition 
include weighted addition experiments  perform well
element wise multiplication mitchell lapatas        experiments  include
holistic model noncompositional baseline 
table    compares dual space model holistic model  element wise multiplication  vector addition  latter three models  try three spaces 
algorithm
dual space
holistic
holistic
holistic
multiplication
multiplication
multiplication
addition
addition
addition

space
domain function
mono
domain
function
mono
domain
function
mono
domain
function

accuracy
    
    
    
    
    
    
    
    
    
    

table     results noun modifier questions 
table  dual space refers dual space model using equation     holistic
model  ab represented corresponding row vector given space  recall
section     that  step      rows matrices correspond n grams wordnet 
n may greater one  thus  example  dog house corresponding
row vector three spaces  holistic model simply uses row vector
representation dog house  element wise multiplication  ab represented using
equation     vector addition model  ab represented   b  vectors
normalized unit length added  four models use constraints
   c b    c  four models use training data parameter tuning 
difference dual space model         best variation elementwise multiplication         statistically significant     confidence level  according fishers exact test  however  difference dual space model
        best variation vector addition         significant 
      limitations holistic approach
three spaces  holistic model significantly better models 
inability address issue linguistic creativity major limitation       multiplechoice questions used experiments intentionally constructed
requirement stem bigram must corresponding row function space  see
above   done could use holistic model baseline  however 
gives misleading impression holistic model serious competitor
compositional approaches  design  table    shows holistic model achieve
ideal  but unrealistic  conditions 
   

fidomain function  dual space model

mitchell lapatas        dataset  used experiments section      illustrates
limitations holistic model  dataset consists     distinct pairs bigrams 
composed     distinct bigrams      bigrams           occur wordnet 
    pairs bigrams          contain bigrams occur wordnet  given
matrices use  with rows based wordnet   holistic approach would
reduced random guessing     pairs mitchell lapatas        dataset 
might argued failure holistic approach mitchell lapatas
       dataset due decision base rows matrices terms wordnet 
however  suppose attempt build holistic model frequent bigrams  web
 t   gram corpus  brants   franz        includes list bigrams appeared   
times terabyte text  total             bigrams  using compositional
approach  matrices use represent majority bigrams 
hand  holistic approach would require matrix             rows 
considerably beyond current state art 
one possibility build matrix holistic approach needed  given
input set n grams  instead building large  static  multipurpose matrix 
two problems idea  first  slow  turney      b  used approach
sat analogy questions  required nine days run  whereas dual space model
process sat questions seconds  given static  multipurpose matrix  second 
requires large corpus  corpus size must grow exponentially n  length
phrases  longer phrases rare  larger corpora needed gather sufficient
data model phrases  larger corpora result longer processing times 
given application  may wise predefined list bigrams holistic
representations  would wise expect list sufficient cover
bigrams would seen practice  creativity human language use requires
compositional models  chomsky        fodor   lepore         although holistic model
included baseline experiments  competitor models 
supplement models 
      impact constraints
use sim  alone  equation      dropping constraints    c b    c  accuracy
drops signficantly               however  models benefit greatly
constraints  table     take best variation model table   
look happens constraints dropped 

algorithm
dual space
holistic
multiplication
addition

space
domain function
mono
domain
domain

constraints
    
    
    
    

accuracy
constraints
    
    
   
   

difference
     
     
     
     

table     impact constraints     c b    c  accuracy 

   

fiturney

      element wise multiplication
section      argued c   b suitable row vectors matrix uk pk
suggested equation    alternative  use c   b domain
space  instead equation     performance drops significantly              
      impact idioms
gap holistic model models may due idiomatic
bigrams testing questions  one successful approaches determining
whether multiword expression  mwe  compositional noncompositional  idiomatic 
compare holistic vector representation compositional vector representation
 for example  high cosine two vectors suggests mwe compositional 
idiomatic   biemann   giesbrecht        johannsen  alonso  rishj    sgaard        
however  approach suitable here  want assume
gap entirely due idiomatic bigrams  instead  would estimate much
gap due idiomatic bigrams 
wordnet contains clues use indicators bigram might less
compositional bigrams  allowing compositionality matter degree  
one clue whether wordnet gloss bigram contains either head noun
modifier  example  gloss dog house outbuilding serves shelter
dog  contains modifier  dog  suggests dog house may compositional 
classified      testing set questions head  the first five characters
head noun bigram match first five characters word bigrams gloss  
modifier  the first five characters modifier bigram match first five characters
word bigrams gloss    both head modifier match   neither
 neither head modifier match   four classes approximately equally
distributed testing questions      head      modifier      both      neither  
match first five characters allow cases brain surgeon  gloss
someone surgery nervous system  especially brain   bigram
classified both  first five characters surgeon match first five characters
surgery 
table    shows accuracy models varies four classes questions  three compositional models  dual space  multiplication  addition   neither class significantly less accurate three classes  fishers exact test 
    confidence   difference significant holistic model  three
compositional models  neither class         less accurate classes 
supports view significant fraction wrong answers compositional
models due noncompositional bigrams 
another clue compositionality wordnet whether head noun hypernym
bigram  example  surgeon hypernym brain surgeon  classified
     testing set questions hyper  the head noun member synset
immediate hypernym first noun sense bigram  look
hypernym hierarchy look senses bigram   not hyper  
testing set      questions hyper     not 
   

fidomain function  dual space model

algorithm
dual space
holistic
multiplication
addition

space
domain function
mono
domain
domain


    
    
    
    

head
    
    
    
    

accuracy
modifier neither
    
    
    
    
    
    
    
    


    
    
    
    

table     variation accuracy different classes bigram glosses 
table    gives accuracy models classes  table
general pattern table     three compositional models significantly lower
accuracy class  decreases        significant difference
holistic model 
algorithm
dual space
holistic
multiplication
addition

space
domain function
mono
domain
domain

accuracy
hyper
         
         
         
         


    
    
    
    

table     variation accuracy different classes bigram hypernyms 

      order sensitivity
note vector addition element wise multiplication lack order sensitivity  equation    sensitive order  simc  ab  c     simc  ba  c   see impact
reformulating noun modifier questions test order sensitivity  first
expand choice unigram c including stem bigram ab  resulting explicit
comparison ab c  expanded choice  ab c  generate another choice 
ba c  increases number choices seven fourteen  due symmetry 
vector addition element wise multiplication must assign similarity
ab c ba c 
table    compares dual space model element wise multiplication vector addition  using reformulated fourteen choice noun modifier questions  holistic model
included table rows matrices reversed ba
bigrams  which may seen another illustration limits holistic model  
stricter test  dual space model significantly accurate element wise
multiplication vector addition  fishers exact test      confidence  
dual space model perform well fourteen choice questions  need
simd simf   drop simd equation     function alone table     
ignoring modifier paying attention head noun  accuracy drops
             drop simf equation     domain alone table     
equation becomes symmetrical  similarity assigned ab c
   

fiturney

algorithm
dual space
multiplication
modified dual space
modified dual space
addition

space
domain function
domain
function alone
domain alone
domain

accuracy
    
    
    
    
    

table     results reformulated fourteen choice noun modifier questions 
ba c  accuracy drops                dual space model significantly
accurate either modified dual space models  fishers exact test     
confidence  
      summary
reformulated fourteen choice noun modifier questions  table      dual space
significantly better element wise multiplication vector addition  original
seven choice questions  table      difference large  questions
test order  unlike element wise multiplication vector addition  dual space
model addresses issue order sensitivity  unlike holistic model  dual space
addresses issue linguistic creativity 
    similarity phrases
subsection  apply dual space model measuring similarity phrases 
using mitchell lapatas        dataset human similarity ratings pairs phrases 
dataset includes three types phrases  adjective noun  noun noun  verb object 
    pairs type              pairs phrases   pair phrases
rated    human subjects  ratings use   point scale    signifies lowest
degree similarity   signifies highest degree  table    gives examples 
let ab represent first phrase pair phrases  environment secretary  let cd
represent second phrase  defence minister   rate similarity phrase pairs
simp  ab  cd   defined follows 
simp  ab  cd    geo simd  a  c   simd  b  d   simf  a  c   simf  b  d  

    

equation based instructions human participants  mitchell   lapata 
      appendix b   imply function domain similarity must high
phrase pair get high similarity rating  figure   illustrates reasoning behind
equation  want high domain function similarities corresponding
components phrases ab cd 
    coincidence modified dual space models accuracy       fourteenchoice questions  although aggregate accuracy same  individual questions  two models
typically select different choices 

   

fidomain function  dual space model

participant
   
   
   
   
   
   
   
   
   

phrase type
adjective noun
adjective noun
adjective noun
noun noun
noun noun
noun noun
verb object
verb object
verb object

group
 
 
 
 
 
 
 
 
 

phrase pair
certain circumstance particular case
large number great majority
evidence low cost
environment secretary defence minister
action programme development plan
city centre research work
lift hand raise head
satisfy demand emphasise need
people increase number

similarity
 
 
 
 
 
 
 
 
 

table     examples phrase pair similarity ratings mitchell lapatas       
dataset  similarity ratings vary    lowest     highest  



b

f

f

c


simp  ab  cd 
phrasal similarity

figure    diagram equation    

      experimental setup
mitchell lapata        divided dataset development set  for tuning parameters  evaluation set  for testing tuned models   development set
  ratings phrase pair evaluation set    ratings phrase pair 
development evaluation sets contain phrase pairs  judgments
different participants  thus               rated phrase pairs development
set                 ratings evaluation set   
challenging evaluation  divide dataset phrase pairs rather
participants  development set     phrase pairs    ratings
evaluation set     phrase pairs    ratings each  three phrase types 
randomly select    phrase pairs development set             phrase pairs 
    information paragraph based section     paper mitchell lapata       
personal communication jeff mitchell june       

   

fiturney

   evaluation set             phrase pairs   thus                
ratings development set                 evaluation set 
mitchell lapata        use spearmans rank correlation coefficient  spearmans
rho  evaluate performance various vector composition algorithms task
emulating human similarity ratings  given phrase type      phrase pairs
divided   groups    pairs each  group evaluation set     people
gave similarity ratings pairs given group  group    pairs given
different group    people  score algorithm given phrase type
average three rho values  one rho three groups     people rating   
pairs group              ratings  human ratings represented
vector     numbers  algorithm generates one rating pair group 
yielding    numbers  make algorithms ratings comparable human ratings 
algorithms ratings duplicated    times  yielding vector     numbers  spearmans
rho calculated two vectors     ratings    phrase types   rho
values     ratings per rho value        ratings   
believe evaluation method underestimates performance algorithms  combining ratings different people one vector     numbers
allow correlation adapt different biases  one person gives consistently low ratings
another person gives consistently high ratings  people ranking 
ranking matches algorithms ranking  algorithm get high
score  fair evaluation  score algorithm calculating one rho value
human participant given phrase type  calculate average
rho values participants 
given phrase type      phrase pairs divided   groups    pairs each 
development set  randomly select    phrase pairs   groups
           phrase pairs per phrase type   leaves    phrase pairs  
groups evaluation set            phrase pairs per phrase type   human
participants ratings represented vector    numbers  algorithms ratings
represented vector    numbers  rho value calculated two vectors
   numbers input  given phrase type  algorithms score average   
rho values     participants per group   groups      rho values     phrase types
   rho values    ratings per rho value        ratings 
      comparison approaches
table    compares dual space model vector addition element wise multiplication 
use development set tune parameters three approaches  vector
addition  ab represented   b cd represented c   d  similarity ab
cd given cosine two vectors  element wise multiplication uses equation   
represent ab cd  dual space model uses equation    
average correlation dual space model        significantly average
correlation vector addition using function space         element wise multiplication
mono space        significantly vector addition using function space        
    information paragraph based personal communication jeff mitchell june       
mitchell lapatas        paper describe spearmans rho applied 

   

fidomain function  dual space model

algorithm
human
dual space
addition
addition
addition
multiplication
multiplication
multiplication

correlation
ad nn nn nn
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

phrase type
vb ob avg
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

comment
leave one out correlation subjects
domain function space
mono space
domain space
function space
mono space
domain space
function space

table     performance models evaluation dataset 
difference dual space model        element wise multiplication mono
space        signficant  average correlation algorithm based     rho
values    phrase types   groups    participants       rho values       participants  
calculate statistical significance using paired t test     significance level 
based     pairs rho values 
      order sensitivity
mitchell lapatas        dataset test order sensitivity  given phrase pair
ab cd  test order sensitivity adding new pair ab dc  assume
new pairs would given rating   human participants  table    
show happens transformation applied examples table    
save space  give examples participant number     
participant
   
   
   
   
   
   

phrase type
adjective noun
adjective noun
adjective noun
adjective noun
adjective noun
adjective noun

group
 
 
 
 
 
 

phrase pair
certain circumstance particular case
certain circumstance case particular
large number great majority
large number majority great
evidence low cost
evidence cost low

similarity
 
 
 
 
 
 

table     testing order sensitivity adding new phrase pairs 
table    gives results new  expanded dataset  stringent
dataset  dual space model performs significantly better vector addition
vector multiplication  unlike element wise multiplication vector addition  dualspace model addresses issue order sensitivity 
manually inspected new pairs automatically rated   found
rating   reasonable cases  although cases could disputed  example 
original noun noun pair tax charge interest rate generates new pair tax charge
rate interest original verb object pair produce effect achieve result generates
new pair produce effect result achieve  seems natural tendency correct
   

fiturney

algorithm
human
dual space
addition
addition
addition
multiplication
multiplication
multiplication

correlation
ad nn nn nn
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

phrase type
vb ob avg
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

comment
leave one out correlation subjects
domain function space
mono space
domain space
function space
mono space
domain space
function space

table     performance dataset expanded test order sensitivity 
incorrectly ordered pairs minds assign higher ratings
deserve  predict human ratings new pairs would vary greatly  depending
instructions given human raters  instructions emphasized
importance word order  new pairs would get low ratings  prediction supported
results semeval      task    jurgens  mohammad  turney    holyoak        
instructions raters emphasized importance word order wrongly
ordered pairs received low ratings 
      summary
dataset test order sensitivity  vector addition performs slightly better
dual space model  dataset tests order sensitivity  dual space
model surpasses vector addition element wise multiplication large margin 
    domain versus function associated versus similar
chiarello et al         created dataset     word pairs labeled similar only 
associated only  similar associated     pairs three classes   table   
shows examples dataset  labeled pairs created cognitive
psychology experiments human subjects  experiments  found evidence
processing associated words engages left right hemispheres brain
ways different processing similar words  is  seems
fundamental neurological difference two types semantic relatedness   
hypothesize similarity domain space  simd  a  b   measure degree
two words associated similarity function space  simf  a  b   measure
degree two words similar  test hypothesis  define similar only 
simso  a  b   associated only  simao  a  b   similar associated  simsa  a  b   follows 

ratio x  y   

x y x        
  otherwise

    

    controversy among cognitive scientists distinction semantic similarity
association  mcrae  khalkhali    hare        

   

fidomain function  dual space model

word pair
table bed
music art
hair fur
house cabin
cradle baby
mug beer
camel hump
cheese mouse
ale beer
uncle aunt
pepper salt
frown smile

class label
similar only
similar only
similar only
similar only
associated only
associated only
associated only
associated only
similar associated
similar associated
similar associated
similar associated

table     examples word pairs chiarello et al          labeled similar only 
associated only  similar associated  full dataset appendix 

simso  a  b    ratio simf  a  b   simd  a  b  

    

simao  a  b    ratio simd  a  b   simf  a  b  

    

simsa  a  b    geo simd  a  b   simf  a  b  

    

intention simso high simf high simd low  simao high
simd high simf low  simsa high simd simf high 
illustrated figure   






f

f

f

b

b

b

simso  a  b 

simao  a  b 

simsa  a  b 

similar only

associated only

similar associated

figure    diagrams equations            

   

fiturney

      evaluation
experiments three preceding subsections  three sets parameter
settings dual space model  table    shows parameter values  effect 
three sets parameter setttings give us three variations similarity measures  simso  
simao   simsa   evaluate three variations see well correspond
labels chiarello et al s        dataset 
similarity
simr  a   b  c   d 
simc  ab  c 
simp  ab  cd 

description
similarity relations
similarity noun modifier compositions
similarity phrases

section
   
   
   

kd
   
   
   

pd
    
   
   

kf
   
   
   

pf
   
   
   

table     parameter settings dual space model 
given similarity measure  simso   sort     word pairs descending order similarities look top n pairs see many
desired label  case simso   would see majority
top n label similar only  table    shows percentage pairs
desired labels three variations three similarity measures  note
random guessing would yield      since three classes pairs size 

source parameters
simr  a   b  c   d 

simc  ab  c 

simp  ab  cd 

n
  
  
  
  
  
  
  
  
  

percentage top n desired label
similar only associated only similar associated
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

table     percentage top n word pairs desired labels 
three sets parameter settings  table    displays high density desired
labels tops sorted lists  density slowly decreases move
lists  evidence three similarity measures capturing three classes
chiarello et al         
another test hypothesis  use three similarity measures create feature
vectors three elements word pair  is  word pair   b represented
feature vector hsimso  a  b   simao  a  b   simsa  a  b i  use supervised learning
ten fold cross validation classify feature vectors three classes chiarello
et al          learning algorithm  use logistic regression  implemented
   

fidomain function  dual space model

weka    results summarized table     results lend support
hypothesis similarity domain space  simd  a  b   measure degree
two words associated similarity function space  simf  a  b   measure
degree two words similar 

source parameters
simr  a   b  c   d 
simc  ab  c 
simp  ab  cd 

accuracy
    
    
    

similar only
     
     
     

f measure
associated only similar associated
     
     
     
     
     
     

average
     
     
     

table     performance logistic regression three similarity measures features 

table     similar only seems sensitive parameter settings associatedonly similar associated  hypothesize function similarity
difficult measure domain similarity  note construction function
space  section      complex construction domain space  section      
intuitively  seems easier identify domain thing identify functional
role  gentners        work suggests children master domain similarity
become competent function similarity 

   discussion experiments
section discusses results previous section 
    summary results
section      used     multiple choice analogy questions evaluate dual space
model relational similarity  simr  a   b  c   d   difference performance
dual space model        accuracy  best past result        accuracy   using
holistic model  statistically significant  experiments reformulated version
questions  designed test order sensitivity  supported hypothesis
domain function space required  function space sensitive order
merging two spaces  mono space  causes significant drop performance 
section      automatically generated       multiple choice noun modifier composition questions wordnet  evaluate dual space model noun modifier compositional similarity  simc  ab  c   difference performance dual space
model        accuracy  state of the art element wise multiplication model       
accuracy  statistically significant  best performance obtained holistic model          model address issue linguistic creativity 
experiments suggest significant fraction gap holistic model
models due noncompositional phrases  limitation element wise multiplication model lack sensitivity order  experiments reformulated version
    weka available http   www cs waikato ac nz ml weka  

   

fiturney

questions  designed test order sensitivitiy  demonstrated statistically significant
advantage dual space model element wise multiplication vector addition
models 
section      used mitchell lapatas        dataset     pairs phrases
evaluate dual space model phrasal similarity  simp  ab  cd   reformulated version
dataset  modified test order sensitivitiy  showed statistically significant advantage
dual space model element wise multiplication vector addition models 
section      used chiarello et al s        dataset     word pairs  labeled
similar only  associated only  similar associated  test hypothesis similarity
domain space  simd  a  b   measure degree two words associated
similarity function space  simf  a  b   measure degree two words
similar  experimental results support hypothesis  interesting
chiarello et al         argue fundamental neurological difference way
people process two kinds semantic relatedness 
experiments support claim dual space model address issues
linguistic creativity  order sensitivity  adaptive capacity  furthermore  dual space
model provides unified approach semantic relations semantic composition 
    corpus based similarity versus lexicon based similarity
results section     suggest function similarity may correspond kind
taxonomical similarity often associated lexicons  wordnet  resnik 
      jiang   conrath        leacock   chodrow        hirst   st onge        
word pairs table    labeled similar only kinds words typically
share common hypernym taxonomy  example  table bed share hypernym
furniture  believe correct  necessarily imply lexiconbased similarity measures would better corpus based approach 
used here 
various similarities section    arguably relational similarity  simr  a   b  c   d  
makes use function similarity  itself  function similarity achieves      
sat questions  original five choice version  see table      however  best performance
achieved sat questions using wordnet        veale         difference
statistically significant     confidence level  based fishers exact test 
consider analogy traffic street water riverbed  one sat questions
involves analogy  traffic  street stem pair water  riverbed correct
choice  simr  a   b  c   d   equation     function similarity  equation    
make correct choice  recognize traffic water high degree
function similarity  fact  similarity used hydrodynamic models traffic flow
 daganzo         however  must climb wordnet hierachy way entity
find shared hypernym traffic water  believe manually
generated lexicon capture functional similarity discovered
large corpus 

   theoretical considerations
section examines theoretical questions dual space model 
   

fidomain function  dual space model

    vector composition versus similarity composition
dual space model  phrase stand alone  general purpose representation 
composite phrase  apart representations component words  composite
meaning constructed context given task  example  task measure
similarity relation dog  house relation bird  nest  compose
meanings dog house one way  see section       task measure similarity
phrase dog house word kennel  compose meanings dog
house another way  see section       task measure similarity phrase
dog house phrase canine shelter  compose meanings dog house
third way  see section       composition construction explicitly ties together
two things compared  depends nature comparison
desired  task performed  hypothesize single stand alone 
task independent representation constructed suitable purposes 
noted introduction  composition vectors result stand alone
representation phrase  composing similarities necessarily yields linking structure
connects phrase phrases  linking structures seen figures
     intuitively  seems important part understand phrase
connecting phrases  part understanding dog house connection
kennel  dictionaries make kinds connections explicit  perspective 
idea explicit linking structure seems natural  given making connnections among
words phrases essential aspect meaning understanding 
    general form similarities dual space model
subsection  present general scheme ties together various similarities
defined section    scheme includes similarities chunks text
arbitrary size  scheme encompasses phrasal similarity  relational similarity 
compositional similarity 
let chunk text  an ordered set words   ht    t            tn i  ti
word  represent semantics   hd  fi  f matrices 
row vector di d                  n  row vector domain space represents
domain semantics word ti   row vector f                  n  row vector
function space represents function semantics word ti   keep notation
simple  parameters  kd pd domain space kf pf function space 
implicit  assume row vectors f normalized unit length  note
size representation scales linearly n  number words t  hence
information scalability  large values n  inevitably duplicate
words t  representation could easily compressed sublinear size without loss
information 
let t  t  two chunks text representations t    hd    f  t   
hd    f  i  t  contains n  words t  n  words  let d  d 
parameters  kd pd   let f  f  parameters  kf pf   d 
n  kd   d  n  kd   f  n  kf   f  n  kf   note d  dt
  n  n 
matrix cosines two row vectors d    is  element i th
   

fiturney


row j th column d  dt
  cos di   dj    likewise  d  d  n  n  matrix
cosines row vector d  row vector d   
suppose wish measure similarity  sim t    t     two chunks
text  t  t    paper  restricted similarity measures following
general form 





sim t    t      f  d  dt
                f  f    f  f    f  f   

    

words  input composition function f cosines  and implicit
parameters  kd   pd   kf   pf    f operate directly row vectors d   
d    f    f    contrast much work discussed section      composition
operation shifted representations  t  t    similarity measure 
f   exact specification f depends task hand  t  t  sentences 
envision structure f determined syntactic structures
two sentences   
consider relational similarity  section      

sim   a   b  c   d    geo simf  a  c   simf  b  d  

    

sim   a   b  c   d    geo simd  a  b   simd  c  d  

    

sim   a   b  c   d    geo simd  a  d   simd  c  b  

sim   a   b  c   d  sim   a   b  c   d  sim   a   b  c   d 
simr  a   b  c   d   
  otherwise

    
    

fits form equation    t    ha  bi t    hc  di  see


sim  based cosines f  ft
    sim  based cosines d  d  d  d   

sim  based cosines d  d   
consider compositional similarity  section      

sim   ab  c    geo simd  a  c   simd  b  c   simf  b  c  

sim   ab  c     c b    c
simc  ab  c   
  otherwise

    
    

seen instance equation    t    ha  bi t    hci 

case  sim  based cosines d  dt
  f  f    constraints     c b    c 

expressed terms cosines d  d    simd  a  c       simd  b  c       
 equivalently  could use cosines f  ft
     similar analyses apply similarities
sections          similarities instances equation    
although representations t  t  sizes linear functions numbers phrases t  t    size composition equation    quadratic
function numbers phrases t  t    however  specific instances general
equation may less quadratic size  may possible limit growth
    note requirement two chunks text  t  t    number
words  is  n  necessarily equal n    section      n     n   

   

fidomain function  dual space model

linear function  also  general  quadratic growth often acceptable practical
applications  garey   johnson        
function words  e g   prepositions  conjunctions   one option would treat
words  would represented vectors similarities would calculated function domain spaces  another possibility would
use function words hints guide construction composition function f  
function words would correspond vectors  instead would contribute determining linking structure connects two given chunks text  first option
appears elegant  choice options made empirically 
    automatic composition similarities
section    manually constructed functions combined similarity measures 
using intuition background knowledge  manual construction scale
task comparing two arbitrarily chosen sentences  however  good reasons
believing construction composition functions automated 
turney      a  presents algorithm solving analogical mapping problems 
analogy solar system rutherford bohr model atom  given
list terms solar system domain   planet  attracts  revolves  sun  gravity  solar system  mass   list terms atomic domain   revolves  atom  attracts 
electromagnetism  nucleus  charge  electron   automatically generate one to one
mapping one domain other   solar system atom  sun nucleus  planet
electron  mass charge  attracts attracts  revolves revolves  gravity electromagnetism   twenty analogical mapping problems  attains accuracy       
compared average human accuracy       
algorithm scores quality candidate analogical mapping composing
similarities mapped terms  composition function addition individual
component similarities holistic relational similarities  algorithm searches
space possible mappings mapping maximizes composite similarity
measure  is  analogical mapping treated argmax problem  argument
maximized mapping function  effect  output algorithm  an analogical
mapping  automically generated composition similarities  mapping structures
found algorithm essentially linking structures see
figures     
believe variation turneys      a  algorithm could used automatically compose similarities dual space model  example  possible
identify paraphrases using automatic similarity composition  proposal search
composition maximizes composite similarity  subject various constraints  such
constraints based syntax sentences   turney      a  points analogical
mapping could used align words two sentences  experimentally
evaluate suggestion 
recent work  lin   bilmes        shown argmax problems solved efficiently effectively framed monotone submodular function maximization
problems  believe automatic composition similarities fit naturally
framework  would result highly scalable algorithms semantic composition 
   

fiturney

regarding information scalability  dual space model suffer information
loss  unlike approaches represent compositions vectors fixed dimensionality  
sizes representations grow lengths phrases grow  growth
might quadratic  exponential  questions automate
composition similarities  may impact computational complexity
scaling longer phrases  evidence questions tractable 

   limitations future work
one area future work experiment longer phrases  more two words 
sentences  discussed section      interesting topic research parsing might
used constrain automatic search similarity composition functions 
focused two spaces  domain function  seems likely us
model spaces would yield better performance  currently experimenting quad space model includes domain  noun based contextual patterns  
function  verb based   quality  adjective based   manner  adverb based  spaces 
preliminary results quad space promising  quad space seems related
pustejovskys        four part qualia structure 
another issue avoided morphology  discussed section      used
validforms function wordnet  querydata perl interface wordnet map
morphological variations words base forms  implies that  example 
singular noun plural form semantic representation 
certainly simplification sophisticated model would use different representations
different morphological forms word 
avoided issue polysemy  possible extend past work
polysemy vsms dual space model  schutze        pantel   lin        erk
  pado        
paper  treated holistic model dual space model
competitors  certain cases  idiomatic expressions  holistic
approach required  likewise  holistic approach limited inability handle
linguistic creativity  considerations suggest holistic dual space models
must integrated  another topic future work 
arguably limitation dual space model four parameters
tune  kd   pd   kf   pf    hand  perhaps model adaptive capacity
must parameters tune  research needed 
number design decisions made construction domain function
space  especially conversion phrases contextual patterns  sections          
decisions guided intuitions  expect exploration experimental evaluation design space fruitful area future research 
construction function space  section      specific english  may generalize readily indo european languages  languages may present
challenge  another topic future research 
composite similarities use geometric mean combine domain
function similarities  see reason restrict possible composition functions 
   

fidomain function  dual space model

equation    allows composition function f   exploring space possible composition
functions another topic future work 
another question formal logic textual entailment integrated
approach  dual space model seems suitable recognizing paraphrases 
obvious way handle entailment  generally  focused various
kinds similarity  scale phrases  red ball  sentences  the ball
red   encounter truth falsity  gardenfors        argues spatial models
bridge low level connectionist models high level symbolic models  claims
spatial models best questions similarity symbolic models best
questions truth  yet know join two kinds models 

   conclusions
goal research develop model unifies semantic relations
compositions  addressing linguistic creativity  order sensitivity  adaptive capacity  information scalability  believe dual space model achieves goal 
although certainly room improvement research 
many kinds wordcontext matrices  based various notions context 
sahlgren        gives good overview types context explored
past work  novelty dual space model includes two distinct
complementary wordcontext matrices work together synergistically 
two distinct spaces  two distinct similarity measures 
combined many different ways  multiple similarity measures  similarity composition becomes viable alternative vector composition  example  instead multiplying vectors  c   b  multiply similarities  simsa  a  b   
geo simd  a  b   simf  a  b    results suggest fruitful new way look
problems semantics 

acknowledgments
thanks george foster  yair neuman  david jurgens  reviewers jair
helpful comments earlier version paper  thanks charles clarke
corpus used build three spaces  stefan buttcher wumpus 
creators wordnet making lexicon available  developers opennlp 
doug rohde svdlibc  jeff mitchell mirella lapata sharing data
answering questions evaluation methodology  christine chiarello  curt
burgess  lorie richards  alma pollock making data available  jason rennie
wordnet  querydata perl interface wordnet  developers perl data
language 

references
aerts  d     czachor  m          quantum aspects semantic analysis symbolic
artificial intelligence  journal physics a  mathematical general      l   
l    
   

fiturney

baroni  m     zamparelli  r          nouns vectors  adjectives matrices  representing adjective noun constructions semantic space  proceedings     
conference empirical methods natural language processing  emnlp       
pp           
bengio  y   ducharme  r   vincent  p     jauvin  c          neural probabilistic language
model  journal machine learning research              
bicici  e     yuret  d          clustering word pairs answer analogy questions 
proceedings fifteenth turkish symposium artificial intelligence neural
networks  tainn        akyaka  mugla  turkey 
biemann  c     giesbrecht  e          distributional semantics compositionality      
shared task description results  proceedings workshop distributional
semantics compositionality  disco        pp        portland  oregon 
bollegala  d   matsuo  y     ishizuka  m          measuring similarity implicit
semantic relations web  proceedings   th international conference
world wide web  www        pp         
brants  t     franz  a          web  t   gram version    linguistic data consortium 
philadelphia 
bullinaria  j     levy  j          extracting semantic representations word cooccurrence statistics  computational study  behavior research methods         
       
buttcher  s     clarke  c          efficiency vs  effectiveness terabyte scale information retrieval  proceedings   th text retrieval conference  trec       
gaithersburg  md 
caron  j          experiments lsa scoring  optimal rank basis   proceedings
siam computational information retrieval workshop  pp          raleigh 
nc 
chiarello  c   burgess  c   richards  l     pollock  a          semantic associative
priming cerebral hemispheres  words do  words dont       sometimes 
places  brain language            
chomsky  n          logical structure linguistic theory  plenum press 
church  k     hanks  p          word association norms  mutual information  lexicography  proceedings   th annual conference association computational linguistics  pp        vancouver  british columbia 
clark  s   coecke  b     sadrzadeh  m          compositional distributional model
meaning  proceedings  nd symposium quantum interaction  pp         
oxford  uk 
clark  s     pulman  s          combining symbolic distributional models meaning 
proceedings aaai spring symposium quantum interaction  pp       
stanford  ca 
conway  j  h     sloane  n  j  a          sphere packings  lattices groups  springer 
   

fidomain function  dual space model

daganzo  c  f          cell transmission model  dynamic representation highway
traffic consistent hydrodynamic theory  transportation research part b 
methodological                 
davidov  d     rappoport  a          unsupervised discovery generic relationships using
pattern clusters evaluation automatically generated sat analogy questions 
proceedings   th annual meeting acl hlt  acl hlt      pp 
        columbus  ohio 
erk  k     pado  s          structured vector space model word meaning context 
proceedings      conference empirical methods natural language
processing  emnlp      pp          honolulu  hi 
fellbaum  c   ed            wordnet  electronic lexical database  mit press 
firth  j  r          synopsis linguistic theory           studies linguistic
analysis  pp       blackwell  oxford 
fodor  j     lepore  e          compositionality papers  oxford university press 
gardenfors  p          conceptual spaces  geometry thought  mit press 
garey  m  r     johnson  d  s          computers intractability  guide theory
np completeness  freeman 
gentner  d          structure mapping  theoretical framework analogy  cognitive
science                
gentner  d          language career similarity  gelman  s     byrnes  j 
 eds    perspectives thought language  interrelations development  pp 
        cambridge university press 
golub  g  h     van loan  c  f          matrix computations  third edition   johns
hopkins university press  baltimore  md 
grefenstette  e     sadrzadeh  m          experimenting transitive verbs discocat  proceedings gems      workshop geometrical models natural
language semantics 
grice  h  p          studies way words  harvard university press  cambridge 
ma 
guevara  e          regression model adjective noun compositionality distributional
semantics  proceedings      workshop geometrical models natural
language semantics  gems        pp       
harris  z          distributional structure  word                  
hearst  m          automatic acquisition hyponyms large text corpora  proceedings   th conference computational linguistics  coling      pp         
herdagdelen  a     baroni  m          bagpack  general framework represent semantic
relations  proceedings eacl      geometrical models natural language
semantics  gems  workshop  pp       
   

fiturney

hirst  g     st onge  d          lexical chains representations context detection
correction malapropisms  fellbaum  c   ed    wordnet  electronic
lexical database  pp          mit press 
jiang  j  j     conrath  d  w          semantic similarity based corpus statistics
lexical taxonomy  proceedings international conference research
computational linguistics  rocling x   pp        tapei  taiwan 
johannsen  a   alonso  h  m   rishj  c     sgaard  a          shared task system
description  frustratingly hard compositionality prediction  proceedings
workshop distributional semantics compositionality  disco        pp    
    portland  oregon 
jones  m  n     mewhort  d  j  k          representing word meaning order information composite holographic lexicon  psychological review           
jurgens  d  a   mohammad  s  m   turney  p  d     holyoak  k  j          semeval     
task    measuring degrees relational similarity  proceedings first joint
conference lexical computational semantics   sem   pp          montreal 
canada 
kintsch  w          metaphor comprehension  computational theory  psychonomic bulletin   review                
kintsch  w          predication  cognitive science                 
kolda  t     bader  b          tensor decompositions applications  siam review 
               
landauer  t  k          computational basis learning cognition  arguments
lsa  ross  b  h   ed    psychology learning motivation  advances
research theory  vol      pp        academic press 
landauer  t  k     dumais  s  t          solution platos problem  latent semantic analysis theory acquisition  induction  representation knowledge 
psychological review                  
landauer  t  k   mcnamara  d  s   dennis  s     kintsch  w          handbook latent
semantic analysis  lawrence erlbaum  mahwah  nj 
leacock  c     chodrow  m          combining local context wordnet similarity
word sense identification  fellbaum  c   ed    wordnet  electronic lexical
database  mit press 
lee  d  d     seung  h  s          learning parts objects nonnegative matrix
factorization  nature              
lepage  y     shin ichi  a          saussurian analogy  theoretical account
application  proceedings   th international conference computational
linguistics  coling        pp         
lin  h     bilmes  j          class submodular functions document summarization 
  th annual meeting association computational linguistics  human
language technologies  acl hlt   pp         
   

fidomain function  dual space model

mangalath  p   quesada  j     kintsch  w          analogy making predication using
relational information lsa vectors  proceedings   th annual meeting
cognitive science society  p        austin  tx 
mcrae  k   khalkhali  s     hare  m          semantic associative relations adolescents young adults  examining tenuous dichotomy  reyna  v   chapman 
s   dougherty  m     confrey  j   eds    adolescent brain  learning  reasoning 
decision making  pp        apa  washington  dc 
mitchell  j     lapata  m          vector based models semantic composition  proceedings acl     hlt  pp          columbus  ohio  association computational
linguistics 
mitchell  j     lapata  m          composition distributional models semantics 
cognitive science                   
moschitti  a     quarteroni  s          kernels linguistic structures answer extraction  proceedings   th annual meeting association computational
linguistics human language technologies  short papers  p          columbus 
oh 
nakov  p     hearst  m          using verbs characterize noun noun relations  proceedings   th international conference artificial intelligence  methodology 
systems  applications  aimsa        pp          varna  bulgaria 
nakov  p     hearst  m          ucb  system description semeval task    proceedings fourth international workshop semantic evaluations  semeval       
pp          prague  czech republic 
nastase  v   sayyad shirabad  j   sokolova  m     szpakowicz  s          learning nounmodifier semantic relations corpus based wordnet based features  proceedings   st national conference artificial intelligence  aaai      pp 
       
nastase  v     szpakowicz  s          exploring noun modifier semantic relations 
proceedings fifth international workshop computational semantics  iwcs    pp          tilburg  netherlands 
niwa  y     nitta  y          co occurrence vectors corpora vs  distance vectors
dictionaries  proceedings   th international conference computational
linguistics  pp          kyoto  japan 
seaghdha  d     copestake  a          using lexical relational similarity classify
semantic relations  proceedings   th conference european chapter
association computational linguistics  eacl      athens  greece 
pantel  p     lin  d          discovering word senses text  proceedings eighth
acm sigkdd international conference knowledge discovery data mining 
pp          edmonton  canada 
plate  t          holographic reduced representations  ieee transactions neural networks                
pustejovsky  j          generative lexicon  computational linguistics                 
   

fiturney

rapp  r          word sense discovery based sense descriptor dissimilarity  proceedings ninth machine translation summit  pp         
resnik  p          using information content evaluate semantic similarity taxonomy 
proceedings   th international joint conference artificial intelligence
 ijcai      pp          san mateo  ca  morgan kaufmann 
rosario  b     hearst  m          classifying semantic relations noun compounds
via domain specific lexical hierarchy  proceedings      conference
empirical methods natural language processing  emnlp      pp       
rosario  b   hearst  m     fillmore  c          descent hierarchy  selection
relational semantics  proceedings   th annual meeting association
computational linguistics  acl      pp         
sahlgren  m          word space model  using distributional analysis represent syntagmatic paradigmatic relations words high dimensional vector spaces 
ph d  thesis  department linguistics  stockholm university 
santorini  b          part of speech tagging guidelines penn treebank project  tech 
rep   department computer information science  university pennsylvania 
  rd revision   nd printing  
schutze  h          automatic word sense discrimination  computational linguistics         
      
smolensky  p          tensor product variable binding representation symbolic
structures connectionist systems  artificial intelligence         
socher  r   huang  e  h   pennington  j   ng  a  y     manning  c  d          dynamic
pooling unfolding recursive autoencoders paraphrase detection  advances
neural information processing systems  nips        pp         
socher  r   manning  c  d     ng  a  y          learning continuous phrase representations
syntactic parsing recursive neural networks  proceedings nips     
deep learning unsupervised feature learning workshop 
thater  s   furstenau  h     pinkal  m          contextualizing semantic representations
using syntactically enriched vector models  proceedings   th annual meeting
association computational linguistics  pp         
turney  p  d          mining web synonyms  pmi ir versus lsa toefl 
proceedings twelfth european conference machine learning  ecml     
pp          freiburg  germany 
turney  p  d       a   expressing implicit semantic relations without supervision 
proceedings   st international conference computational linguistics
  th annual meeting association computational linguistics  coling acl     pp          sydney  australia 
turney  p  d       b   similarity semantic relations  computational linguistics         
       
turney  p  d       a   latent relation mapping engine  algorithm experiments 
journal artificial intelligence research             
   

fidomain function  dual space model

turney  p  d       b   uniform approach analogies  synonyms  antonyms  associations  proceedings   nd international conference computational
linguistics  coling        pp          manchester  uk 
turney  p  d     littman  m  l          corpus based learning analogies semantic
relations  machine learning                  
turney  p  d   littman  m  l   bigham  j     shnayder  v          combining independent
modules solve multiple choice synonym analogy problems  proceedings
international conference recent advances natural language processing
 ranlp      pp          borovets  bulgaria 
turney  p  d     pantel  p          frequency meaning  vector space models
semantics  journal artificial intelligence research             
utsumi  a          computational semantics noun compounds semantic space model 
proceedings   st international joint conference artificial intelligence
 ijcai      pp           
veale  t          wordnet sits sat  knowledge based approach lexical analogy 
proceedings   th european conference artificial intelligence  ecai       
pp          valencia  spain 
widdows  d          semantic vector products  initial investigations  proceedings
 nd symposium quantum interaction  oxford  uk 

   


