journal of artificial intelligence research                

submitted       published     

speeding up the convergence of value iteration
in partially observable markov decision processes
nevin l  zhang
weihong zhang

department of computer science
hong kong university of science   technology
clear water bay road  kowloon  hong kong  china

lzhang cs ust hk
wzhang cs ust hk

abstract

partially observable markov decision processes  pomdps  have recently become popular among many ai researchers because they serve as a natural model for planning under
uncertainty  value iteration is a well known algorithm for finding optimal policies for
pomdps  it typically takes a large number of iterations to converge  this paper proposes
a method for accelerating the convergence of value iteration  the method has been evaluated on an array of benchmark problems and was found to be very effective  it enabled
value iteration to converge after only a few iterations on all the test problems 

   introduction
pomdps model sequential decision making problems where effects of actions are nondeterministic and the state of the world is not known with certainty  they have attracted
many researchers in operations research and artificial intelligence because of their potential applications in a wide range of areas  monahan       cassandra     b   one of which is
planning under uncertainty  unfortunately  there is still a significant gap between this potential and actual applications  primarily due to the lack of effective solution methods  for
this reason  much recent effort has been devoted to finding ecient algorithms for pomdps
 e g   parr and russell       hauskrecht     b  cassandra     a  hansen       kaelbling
et al        zhang et al        
value iteration is a well known algorithm for pomdps  smallwood and sondik      
puterman        it starts with an initial value function and iteratively performs dynamic
programming  dp  updates to generate a sequence of value functions  the sequence converges to the optimal value function  value iteration terminates when a predetermined
convergence condition is met 
value iteration performs typically a large number of dp updates before it converges and
dp updates are notoriously expensive  in this paper  we develop a technique for reducing
the number of dp updates 
dp update takes  the finite representation of  a value function as input and returns  the
finite representation of  another value function  the output value function is closer to the
optimal than the input value function  in this sense  we say that dp update improves its
input  we propose an approximation to dp update called point based dp update  pointbased dp update also improves its input  but possibly to a lesser degree than standard dp
update  on the other hand  it is computationally much cheaper  during value iteration  we
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fizhang   zhang
perform point based dp update a number of times in between two standard dp updates 
the number of standard dp updates can be reduced this way since point based dp update
improves its input  the reduction does not come with a high cost since point based dp
update takes little time 
the rest of this paper is organized as follows  in the next section we shall give a brief
review of pomdps and value iteration  the basic idea behind point based dp update will
be explained in section    after some theoretical preparations in section    we shall work
out the details of point based dp update in section    empirical results will be reported
in section   and possible variations evaluated in section    finally  we shall discuss related
work in section   and provide some concluding remarks in section   

   pomdps and value iteration
    pomdps
a partially observable markov decision process  pomdp  is a sequential decision model
for an agent who acts in a stochastic environment with only partial knowledge about the
state of its environment  the set of possible states of the environment is referred to as
the state space and is denoted by s   at each point in time  the environment is in one of
the possible states  the agent does not directly observe the state  rather  it receives an
observation about it  we denote the set of all possible observations by z   after receiving the
observation  the agent chooses an action from a set a of possible actions and executes that
action  thereafter  the agent receives an immediate reward and the environment evolves
stochastically into a next state 
mathematically  a pomdp is specified by  the three sets s   z   and a  a reward function
r s  a   a transition probability function p  s  js  a   and an observation probability function
p  z js    a   the reward function characterizes the dependency of the immediate reward on
the current state s and the current action a  the transition probability characterizes the
dependency of the next state s  on the current state s and the current action a  the
observation probability characterizes the dependency of the observation z at the next time
point on the next state s  and the current action a 

    policies and value functions
since the current observation does not fully reveal the identity of the current state  the agent
needs to consider all previous observations and actions when choosing an action  information about the current state contained in the current observation  previous observations 
and previous actions can be summarized by a probability distribution over the state space
 astrom        the probability distribution is sometimes called a belief state and denoted
by b  for any possible state s  b s  is the probability that the current state is s  the set of
all possible belief states is called the belief space  we denote it by b 
a policy prescribes an action for each possible belief state  in other words  it is a
mapping from b to a  associated with a policy  is its value function v    for each belief
state b  v   b  is the expected total discounted reward that the agent receives by following
  

fispeeding up value iteration in pomdps
the policy starting from b  that is

 
x
t r   

v   b    e b 

t  

t

   

where rt is the reward received at time t and
       is the discount factor  it is known
that there exists a policy  such that v   b v   b  for any other policy  and any belief
state b  puterman        such a policy is called an optimal policy  the value function of an
optimal policy is called the optimal value function  we denote it by v    for any positive
number   a policy  is  optimal if

v   b      v   b   b   b 

    value iteration

to explain value iteration  we need to consider how belief state evolves over time  let b
be the current belief state  the belief state at the next point in time is determined by the
current belief state  the current action a  the next observation z   we denote it by baz   for
any state s    baz  s    is given by

baz  s     

ps p  z  s js  a b s 

 
   
p  z jb  a 
p
where p  z  s  js  a  p  z js    a p  s  js  a  and p  z jb  a   s s  p  z  s  js  a b s  is the renormal 

ization constant  as the notation suggests  the constant can also be interpreted as the
probability of observing z after taking action a in belief state b 
define an operator t that takes a value function v and returns another value function
tv as follows 
x
tv  b    max
 r b  a     p  z jb  a v  baz     b   b
   
a
z

p
where r b  a    s r s  a b s  is the expected immediate reward for taking action a in belief
state b  for a given value function v   a policy  is said to be v  improving if
x p  zjb  a v  ba    b   b 
 b    arg max
 
r
 
b 
a
 
 

z
a
z

   

value iteration is an algorithm for finding  optimal policies  it starts with an initial
value function v  and iterates using the following formula 
vn   tvn    
it is known  e g   puterman       theorem      that vn converges to v  as n goes to
infinity  value iteration terminates when the bellman residual maxb jvn  b    vn    b j falls
below           when it does  a vn  improving policy is  optimal  e g   puterman       
since there are infinitely many belief states  value functions cannot be explicitly represented  fortunately  the value functions that one encounters in the process of value iteration
admit implicit finite representations  before explaining why  we first introduce several technical concepts and notations 
  

fizhang   zhang

 
 
 
 

      

      

figure    illustration of technical concepts 

    technical and notational considerations

for convenience  we view functions over the state space vectors of size jsj  we use lower
case greek letters ff and fi to refer to vectors and script letters v and u to refer to sets of
vectors  in contrast  the upper case letters v and u always refer to value functions  that is
functions over the belief space b  note that a belief state is a function over the state space
and hence can be viewed as a vector 
a set v of vectors induces a value function as follows 

f  b    max
ffb  b   b 
ff v

p

where ffb is the inner product of ff and b  that is ffb  s ff s b s   for convenience  we
shall abuse notation and use v to denote both a set of vectors and the value function induced
by the set  under this convention  the quantity f  b  can be written as v  b  
a vector in a set is extraneous if its removal does not affect the function that the set
induces  it is useful otherwise  a set of vectors is parsimonious if it contains no extraneous
vectors 
given a set v and a vector ff in v   define the open witness region r ff  v   and closed
witness region r ff  v   of ff w r t v to be regions of the belief space b respectively given by

r ff  v     fb   bjffb   ff  b   ff    vnfffgg
r ff  v     fb   bjffb  ff  b   ff    vnfffgg
in the literature  a belief state in the open witness region r ff  v   is usually called a witness
point for ff since it testifies to the fact that ff is useful  in this paper  we shall call a belief
state in the closed witness region r ff  v   a witness point for ff 
figure   diagrammatically illustrates the aforementioned concepts  the line at the
bottom depicts the belief space of a pomdp with two states  the point at the left end
represents the probability distribution that concentrates all its masses on one of the states 
while the point at the right end represents the one that concentrates all its masses on the
other state  there are four vectors ff    ff    ff    and ff    the four slanting lines represent
  

fispeeding up value iteration in pomdps

v     

vi 

  
  
  
  
  
  
  



         

do f
u

dp update v   
maxb ju  b    v  b j 
if  r     v u  
g while   r     
return u  

r

figure    value iteration for pomdps 
the linear functions ffi b  i             of b  the value function induced by the four vectors
is represented by the three bold line segments at the top  vector ff  is extraneous as its
removal does not affect the induced function  all the other vectors are useful  the first
segment of the line at the bottom is the witness region of ff    the second segment is that of
ff    and the last segment is that of ff   

    finite representation of value functions and value iteration

a value function v is represented by a set of vectors if it equals the value function induced
by the set  when a value function is representable by a finite set of vectors  there is a
unique parsimonious set of vectors that represents the function  littman et al      a  
sondik        has shown that if a value function v is representable by a finite set
of vectors  then so is the value function tv   the process of obtaining the parsimonious
representation for tv from the parsimonious representation of v is usually referred to as
dynamic programming  dp  update  let v be the parsimonious set of vectors that represents
v   for convenience  we use t v to denote the parsimonious set of vectors that represents
tv  
in practice  value iteration for pomdps is not carried out directly in terms of value
functions themselves  rather  it is carried out in terms of sets of vectors that represent the
value functions  figure     one begins with an initial set of vectors v   at each iteration 
one performs a dp update on the previous parsimonious set v of vectors and obtains a new
parsimonious set of vectors u   one continues until the bellman residual maxb ju  b   v  b j 
which is determined by solving a sequence of linear programs  falls below a threshold 

   point based dp update  the idea
this section explains the intuitions behind point based dp update  we begin with the
so called backup operator 

    the backup operator
let v be a set of vectors and b be a belief state  the backup operator constructs a new

vector in three steps 

  

fizhang   zhang
   for each action a and each observation z   find the vector in v that has maximum inner
product with bza   the belief state for the case when z is observed after executing action
a in belief state b  if there are more than one such vector  break ties lexicographically
 littman        denote the vector found by fia z  
   for each action a  construct a vector fia by 
x
fia  s    r s  a     p  s    zjs  a fia z  s     s   s  
z s 

   find the vector  among the fia  s  that has maximum inner product with b  if there
are more than one such vector  break ties lexicographically  denote the vector found
by backup b  v   
it has been shown  smallwood and sondik       littman       that backup b  v   is a
member of t v   the set of vectors obtained by performing dp update on v   moreover  b
is a witness point for backup b  v   
the above fact is the corner stone of several dp update algorithms  the one pass
algorithm  sondik        the linear support algorithm  cheng        and the relaxed region
algorithm  cheng       operate in the following way  they first systematically search for
witness points of vectors in t v and then obtain the vectors using the backup operator  the
witness algorithm  kaelbling et al        employs a similar idea 

    point based dp update

systematically searching for witness points for all vectors in t v is computationally expensive  point based dp update does not do this  instead  it uses heuristics to come up with
a collection of belief points and backs up on those points  it might miss witness points for
some of the vectors in t v and hence is an approximation of standard dp update 
obviously  backing up on different belief states might result in the same vector  in other
words  backup b  v   and backup b    v   might be equal for two different belief states b and
b   as such  it is possible that one gets only a few vectors after many backups  one issue in
the design of point based dp update is to avoid this  we address this issue using witness
points 
point based dp update assumes that one knows a witness point for each vector in its
input set  it backs up on those points   the rationale is that witness points for vectors in
a given set  scatter all over the belief space  and hence the chance of creating duplicate
vectors is low  our experiments have confirmed this intuition 
the assumption made by point based dp update is reasonable because its input is
either the output of a standard dp update or another point based dp update  standard
dp update computes  as by products  a witness point for each of its output vectors  as will
be seen later  point based dp update also shares this property by design 

    the use of point based dp update

as indicated in the introduction  we propose to perform point based dp update a number
of times in between two standard dp updates  to be more specific  we propose to modify
   as will be seen later  point based dp update also backs up on other points 

  

fispeeding up value iteration in pomdps

vi  

  
  
  
  
  
  
  

v     



         

do f
u

dp update v   
maxb ju  b    v  b j 
if  r     v point based vi u     
g while   r     
return u  

r

u     
   do f
  
v u 
  
u point based dpu v  
   g while  stop u   v      false  
   return v  
point based vi 

figure    modified value iteration for pomdps 
value iteration in the way as shown in figure    note that the only change is at line
   instead of assigning u directly to v   we pass it to a subroutine point based vi and
assign the output of the subroutine to v   the subroutine functions in the same way as
value iteration  except that it performs point based dp updates rather than standard dp
updates  hence we call it point based value iteration 
figure   illustrates the basic idea behind modified value iteration in contrast to value
iteration  when the initial value function is properly selected   the sequence of value functions produced by value iteration converges monotonically to the optimal value function 
convergence usually takes a long time partially because standard dp updates  indicated
by fat upward arrows  are computationally expensive  modified value iteration interleaves
standard dp updates with point based dp updates  which are indicated by the thin upward
arrows  point based dp update does not improve a value function as much as standard dp
update  however  its complexity is much lower  as a consequence  modified value iteration
can hopefully converge in less time 
the idea of interleaving standard dp updates with approximate updates that back up
at a finite number of belief points is due to cheng         our work differs from cheng s
method mainly in the way we select the belief points  a detailed discussion of the differences
will be given in section   
the modified value iteration algorithm raises three issues  first  what stopping criterion
do we use for point based value iteration  second  how can we guarantee the stopping
criterion can eventually be satisfied  third  how do we guarantee the convergence of the
modified value iteration algorithm itself  to address those issues  we introduce the concept
of uniformly improvable value functions 
   we will show how in section     

  

fizhang   zhang

 
 
 

 
 
 

standard update

point based update

value iteration

modified value iteration

figure    illustration of the basic idea behind modified value iteration 

   uniformly improvable value functions

suppose v and u are two value functions  we say that u dominates v and write v u if
v  b u  b  for every belief state b  a value function v is said to be uniformly improvable if
v tv   a set u of vectors dominates another set v of vectors if the value function induced
by u dominates that induced by v   a set of vectors is unformly improvable if the value
function it induces is 

lemma   the operator t is isotone in the sense that for any two value functions v and
u   v u implies tv tu    
this lemma is obvious and is well known in the mdp community  puterman       
nonetheless  it enables us to explain the intuition behind the term  uniformly improvable  
suppose v is a uniformly improvable value function and suppose value iteration starts
with v   then the sequence of value functions generated is monotonically increasing and
converges to the optimal value function v    this implies v tv v    that is  tv  b  is
closer to v   b  than v  b  for all belief states b 
the following lemma will be used later to address the issues listed at the end of the
previous section 

lemma   consider two value functions v and u   if v is uniformly improvable and
v u tv   then u is also uniformly improvable 
proof  since v u   we have tv tu by lemma    we also have the condition u tv  
consequently  u tu   that is  u is uniformly improvable   
corollary   if value function v is uniformly improvable  so is tv    

   point based dp update  the algorithm

point based dp update is an approximation of standard dp update  when designing
point based dp update  we try to strike a balance between quality of approximation and
  

fispeeding up value iteration in pomdps
computational complexity  we also need to guarantee that the modified value iteration
algorithm converges 

    backing up on witness points of input vectors
let v be a set of vectors on which we are going to perform point based dp update  as
mentioned earlier  we can assume that we know a witness point for each vector in v   denote

the witness point for a vector ff by w ff   point based dp update first backs up on these
points and thereby obtains a new set of vectors  to be more specific  it begins with the
following subroutine 

v   
   u   
   for each fi   v
  
ff backup w fi    v   
  
if ff    u
  
w ff 
w fi   
  
u u   fffg 
   return u  
backupwitnesspoints 

in this subroutine  line   makes sure that the resulting set u contains no duplicates and
line   takes note of the fact that w fi   is also a witness point for ff  w r t t v   

    retaining uniform improvability

to address convergence issues  we assume that the input to point based dp update is
uniformly improvable and require its output to be also uniformly improvable  we will
explain later how the assumption can be facilitated and how the requirement guarantees
convergence of the modified value iteration algorithm  in this subsection  we discuss how
the requirement can be fulfilled 
point based dp update constructs new vectors by backing up on belief points and the
new vectors are all members of t v   hence the output of point based dp update is trivially
dominated by t v   if the output also dominates v   then it must be uniformly improvable
by lemma    the question is how to guarantee that the output dominates v  
consider the set u resulted from backupwitnesspoints  if it does not dominate v   then
there must exist a belief state b such u  b  v  b   consequently  there must exist a vector
fi in v such that u  b  fi b  this gives us the following subroutine for testing whether
u dominates v and for  when this is not the case  adding vectors to u so that it does 
the subroutine is called backuplppoints because belief points are found by solving linear
programs 

u   v   
   for each fi   v
  
do f
backuplppoints 

  
  
  

b

if b  

dominancecheck 
  null 
backup 
  

ff

fi  u   

b  v

  

fizhang   zhang
  
  
  

w 

ff 

b 

u u   fffg 
g while  b    null  

the subroutine examines vectors in v one by one  for each fi in v   it calls another subroutine
try to find a belief point b such that u  b  fi b  if such a point is found 
it backs up on it  resulting in a new vector ff  line     by the property of the backup
operator  b is a witness point of ff w r t t v  line     there cannot be any vector in u that
equals ff   consequently  the vector is simply added to u without checking for duplicates
 line     the process repeats for fi until dominancecheck returns null  that is when there
are no belief points b such that u  b  fi b  when backuplppoints terminates  we have
u  b fi b for any vector fi in v and any belief point b  hence u dominates v  
the subroutine dominancecheck fi  u   first checks whether there exists a vector ff in u
that pointwise dominates fi   that is ff s fi  s  for all states s  if such an ff exists  it returns
null right away  otherwise  it solves the following linear program lp fi  u    it returns the
solution point b when the optimal value of the objective function is positive and returns
null otherwise  
dominancecheck to

fi  u   
   variables  x  b s  for each state s
   maximize  x 
   constraints 
ps fi s b s   x  ps ff s b b  for all ff u
  
ps b s       b s     for all states s 
  
lp 

    the algorithm

here is the complete description of point based dp update  it first backs up on the witness
points of the input vectors  then  it solves linear programs to identify more belief points and
backs up on them so that its output dominates its input and hence is uniformly improvable 
point based dpu v   
   u backupwitnesspoints v  
   backuplppoints u   v  
   return u  
in terms of computational complexity  point based dp update performs exactly jvj
backups in the first step and no more than jt vj backups in the second step  it solves linear
programs only in the second step  the number of linear programs solved is upper bounded
by jt vj jvj and is usually much smaller than the bound  the numbers of constraints in
the linear programs are upper bounded by jt vj     
   since b is a witness of ff w r t t v   we have ffb t v  b   since v is uniformly improvable  we also
have t v  b v  b   together with the obvious fact that v  b fi b and the condition fi b u  b   we have
ffb u  b   consequently  there cannot be any vector in u that equals ff 
   in our actual implementation  the solution point b is used for backup even when the optimal value of
the objective function is negative  in this case  duplication check is needed 

  

fispeeding up value iteration in pomdps
there are several algorithms for standard dp update  among them  the incremental
pruning algorithm  zhang and liu       has been shown to be the most ecient both
theoretically and empirically  cassandra et al         empirical results  section    reveal
that point based dp update is much less expensive than incremental pruning on a number
of test problems  it should be noted  however  that we have not proved this is always the
case 

    stopping point based value iteration

consider the do while loop of point based vi  figure     starting from an initial set of
vectors  it generates a sequence of sets  if the initial set is uniformly improvable  then the
value functions represented by the sets are monotonically increasing and are upper bounded
by the optimal value function  as such  they converge to a value function  which is not
necessarily the optimal value function   the question is when to stop the do while loop 
a straightforward method would be to compute the distance maxb ju  b   v  b j between
two consecutive sets u and v and stop when the distance falls below a threshold  to compute
the distance  one needs to solve juj jvj linear programs  which is time consuming  we use
a metric that is less expensive to compute  to be more specific  we stop the do while loop
when
max
ju  w ff     v  w ff  j     
ff u
in words  we calculate the maximum difference between u and v at the witness points of
vectors in u and stop the do while loop when this quantity is no larger than     here 
is the threshold on the bellman residual for terminating value iteration and   is a number
between   and    in our experiments  we set it at     

    convergence of modified value iteration
let vn and vn  be sets of vectors respectively generated by vi  figure    and vi   figure

   at line   in iteration n  suppose the initial set is uniformly improvable  using lemma  
and corollary    one can prove by induction that vn and vn  are uniformly improvable for
all n and their induced value functions increase with n  moreover  vn  dominates vn and is
dominated by the optimal value function  it is well known that vn converges to the optimal
value function  therefore  vn  must also converge to the optimal value function 
the question now is how to make sure that the initial set is uniformly improvable  the
following lemma answers this question 

lemma   let m  mins a r s  a   c   m         and ffc be the vector whose components
are all c  then the singleton set fffc g is uniformly improvable 
proof  use v to denote the value function induced by the singleton set  for any belief

state b  we have

tv  b    max
 r b  a    
a
  

x p  zjb  a v  ba  
z

z

fizhang   zhang
  max
 r b  a    
a

x p  zjb  a c 
z

  max
 r b  a    m        
a
 m   m       
  m          v  b  
therefore the value function  and hence the singleton set  is uniformly improvable   
experiments  section    have shown that vi  is more ecient vi on a number of test
problems  it should be noted  however  that we have not proved this is always the case 
moreover  complexity results by papadimitriou and tsitsiklis        implies that the task
of finding  optimal policies for pomdps is pspace complete  hence  the worst case
complexity should remain the same 

    computing the bellman residual

in the modified value iteration algorithm  the input v to standard dp update is always
uniformly improvable  as such  its output u dominates its input  this fact can be used to
simplify the computation of the bellman residual  as a matter of fact  the bellman residual
maxb ju  b  v  b j reduces maxb  u  b  v  b   
to compute the latter quantity  one goes through the vectors in u one by one  for each
vector  one solves the linear program lp ff  v    the quantity is simply the maximum of
the optimal values of the objective functions of the linear programs  without uniformly
improvability  we would have to repeat the process one more time with the roles of v and
u exchanged 

   empirical results and discussions

experiments have been conducted to empirically determine the effectiveness of point based
dp update in speeding up value iteration  eight problems are used in the experiments 
in the literature  the problems are commonly referred to as  x co  cheese   x   part
painting  tiger  shuttle  network  and aircraft id  we obtained the problem files from
tony cassandra  information about their sizes is summarized in the following table 
problem jsj jzj jaj
 x co   
    
 x    
 
 
tiger  
 
 
network  
 
 

problem jsj jzj jaj
cheese   
 
 
painting  
 
 
shuttle  
 
 
aircraft id   
 
 

the effectiveness of point based dp update is determined by comparing the standard
value iteration algorithm vi and the modified value iteration algorithm vi   the implementation of standard value iteration used in our experiments is borrowed from hansen 
modified value iteration is implemented on top of hansen s code   the discount factor is
set at      and round off precision is set at        all experiments are conducted on an
ultrasparc ii machine 
   the implementation is available on request 

  

fispeeding up value iteration in pomdps
table   shows the amounts of time vi and vi  took to compute      optimal policies
for the test problems  we see that vi  is consistently more ecient than vi  especially on
the larger problems  it is about                            and    times faster than vi on the
first seven problems respectively  for the aircraft id problem  vi  was able to compute a
     optimal policy in less than   hours  while vi was only able to produce a    optimal
policy after    hours 
 x co cheese  x  paint tiger shuttle network aircraft
vi
   
                                   
vi 
   
        
   
   
  
          
table    time for computing      optimal policies in seconds 
various other statistics are given in table   to highlight computational properties of
vi  and to explain its superior performance  the numbers of standard dp updates carried
out by vi and vi  are shown at rows   and    we see that vi  performed no more than  
standard updates on the test problems  while vi performed more than      this indicates
that point based update is very effective in cutting down the number of standard updates
required to reach convergence  as a consequence  vi  spent much less time than vi in
standard updates  row   and     
problem
 x co cheese  x  paint tiger shuttle network
dpu  
   
               
   
   
vi
time
    
                            
     
dpu  
 
 
 
 
 
 
 
time
   
   
   
   
   
  
  
vi  pbdpu  
   
               
   
   
time
    
         
   
   
  
   
quality ratio
   
   
   
   
   
    
   
complexity ratio
   
   
                    
    
table    detailed statistics 
row   shows the numbers of point based updates carried out by vi   we see that those
numbers are actually larger than the numbers of standard updates performed by vi  this
is expected  to see why  recall that point based update is an approximation of standard
update  let v be a set of vectors that is uniformly improvable  use t   v to denote the
sets of vectors resulted from performing point based update on v   for any belief state b 
we have v  b t   v  b t v  b   this means that point based update improves v but not as
much as standard update  consequently  the use of point based update increases the total
   note that times shown there do not include time for testing the stopping condition 

  

fizhang   zhang
number of iterations  i e the number of standard updates plus the number of point based
updates 
intuitively  the better point based update is as an approximation of standard update 
the less the difference between the total number iterations that vi  and vi need take  so 
the ratio between those two numbers in a problem can be used  to certain extent  as a
measurement of the quality of point based update in that problem  we shall refer to it as
the quality ratio of point based update  row   shows the quality ratios in the seven test
problems  we see that the quality of point based update is fairly good and stable across all
the problems 
row   shows  for each test problem  the ratio between the average time of a standard
update performed by vi and that of a point based update performed by vi   those ratios
measure  to certain extent  the complexity of point based update relative to standard update
and hence will be referred to as the complexity ratios of point based update  we see that 
as predicted by the analysis in section      point based update is consistently less expensive
than standard update  the differences are more than     times in the last four problems 
in summary  the statistics suggest that the quality of point based update relative to
standard update is fairly good and stable and its complexity is much lower  together with
the fact that point based update can drastically reduces the number of standard updates 
those explain the superior performance of vi  
to close this section  let us note that while vi finds policies with quality  very close to
the predetermined criterion  vi  usually finds much better ones  table     this is because
vi checks policy quality after each  standard  update  while vi  does not do this after
point based updates 

problem  x co cheese  x  paint tiger shuttle network
vi
                     
                 
vi 
                                         
table    quality of policies found by vi and vi  

   variations of point based dp update
we have studied several possible variations of point based update  most of them are based
on ideas drawn from the existing literature  none of the variations were able to significantly
enhance the effectiveness of the algorithm in accelerating value iteration  nonetheless a brief
discussion of some of them is still worthwhile  the discussion provides further insights about
the algorithm and shows how it compares to some of the related work to be discussed in
detail in the next section 
the variations can be divide into two categories  those aimed at improving the quality
of point based update and those aimed at reducing complexity  we shall discuss them one
by one 
   quality of a policy is estimated using the bellman residual 

  

fispeeding up value iteration in pomdps

    improving the quality of point based dp update
a natural way to improve the quality of point based update is to back up on additional
belief points  we have explored the use of randomly generated points  cassandra     a  
additional by product points  and projected points  hauskrecht        here additional byproduct points refer to points generated at various stages of standard update  excluding the
witness points that are already being used  projected points are points that are reachable
in one step from points that have given rise to useful vectors 
table   shows  for each test problem  the number of standard updates and the amount
of time that vi  took with and without using projected points  we see that the use of
projected points did reduce the number of standard updates by one in  x co  cheese  and
shuttle  however  it increased the time complexity in all test problems except for network 
the other two kinds of points and combinations of the three did not significantly improve
vi  either  on the contrary  they often significantly degraded the performance of vi  
w o
with
w o
with

 x co cheese  x  paint tiger shuttle network aircraft
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
       
   
   
  
          
   
       
   
   
  
          

table    number of standard dp updates and time that vi  took with and without
using projected points 
a close examination of experimental data reveals a plausible explanation  point based
update  as it stands  can already reduce the number of standard updates down to a just few
and among them the last two or three are the most time consuming  as such  the possibility
of further reducing the number standard updates is low and even when it is reduced  the
effect is roughly to shift the most time consuming standard updates earlier  consequently 
it is unlikely to achieve substantial gains  on the other hand  the use of additional points
always increases overheads 

    reducing the complexity of point based dp update
solving linear programs is the most expensive operation in point based update  an obvious
way to speed up is to avoid linear programs  point based update solves linear programs
and backs up on the belief points found so as to guarantee uniform improvability  if the
linear programs are to be skipped  there must be some other way to guarantee uniform
improvability  there is an easy solution to this problem  suppose v is the set of vectors
that we try to update and it is uniformly improvable  let u be the set obtained from v by
backing up only on the witness points  which can be done without solving linear programs 
the set u might or might not be uniformly improvable  however  the union v   u is
guaranteed to be uniformly improvable  therefore we can reprogram point based update
  

fizhang   zhang
to return the union in hope to reduce complexity  the resulting variation will be called
non lp point based dp update 
another way to reduce complexity is to simplify the backup operator  section      using
the idea behind modified policy iteration  e g   puterman        when backing up from
a set of vectors v at a belief point  the operator considers all possible actions and picks
the one that is optimal according to the v  improving policy  to speed up  one can simply
use the action found for the belief point by the previous standard update  the resulting
operator will be called the mpi backup operator  where mpi stands for modified policy
iteration  if v is the output of the previous standard update  the two actions often are the
same  however  they are usually different if v is the result of several point based updates
following the standard update 
table   shows  for each test problem  the number of standard updates and the amount of
time that vi  took when non lp point based update was used  together with the standard
backup operator   comparing the statistics with those for point based update  tables  
and     we see that the number of standard updates is increased on all test problems and the
amount of time is also increased except for the first three problems  here are the plausible
reasons  first  it is clear that non lp point based update does not improve a set of vectors
as much as point based update  consequently  it is less effective in reducing the number of
standard updates  second  although it does not solve linear programs  non lp point based
update produces extraneous vectors  this means that it might need to deal with a large
number of vectors at later iterations and hence might not be as ecient as point based
update after all 
 x co cheese  x  paint tiger shuttle network aircraft
 
 
 
 
 
 
  
 
    
        
   
   
  
          
table    number of standard dp updates and time that vi  took when non lp pointbased update is used 
extraneous vectors can be pruned  as a matter of fact  we did prune vectors that are
pointwise dominated by others  hence extraneous  in our experiments  this is inexpensive 
pruning of other extraneous vectors  however  requires the solution of linear programs and
is expensive  in zhang et al          we have discussed how this can be done the most
ecient way  still the results were not as good as those in table    in that paper  we
have also explored the combination of non lp point based update with the mpi backup
operator  once again  the results were not as good as those in table    the reason is that
the mpi backup operator further compromises the quality of point based update 
the quality of non lp point based update can be improved by using the gauss seidel
asynchronous update  denardo        suppose we are updating a set v   the idea is to 
after a vector is created by backup  add a copy of the vector to the set v right away  the
hope is to increase the components of later vectors  we have tested this idea when preparing
zhang et al         and found that the costs almost always exceed the benefits  a reason
  

fispeeding up value iteration in pomdps
is that asynchronous update introduces many more extraneous vectors than synchronous
update 
in conclusion  point based is conceptually simple and clean  when compared to its more
complex variations  it seems to be the most effective in accelerating value iteration 

   related work
work presented in this paper has three levels  point based dp update at the bottom  pointbased value iteration in the middle  and modified value iteration at the top  in this section 
we discuss previous relevant work at each of the three levels 

    point based dp update and standard dp update

as mentioned in section      point based update is closely related to several exact algorithms for standard update  namely one pass  sondik        linear support  cheng       
and relaxed region  cheng        they all backup on a finite number of belief points 
the difference is that these exact algorithms generate the points systematically  which is
expensive  while point based update generate the points heuristically 
there are several other exact algorithms for standard dp update  the enumeration reduction algorithms  monahan       eagle       and incremental pruning  zhang
and liu       cassandra et al        first generate a set of vectors that are not parsimonious and then prune extraneous vectors by solving linear programs  point based dp
update never generates extraneous vectors  it might generate duplicate vectors  however 
duplicates are pruned without solving linear programs  the witness algorithm  kaelbling
et al        has two stages  in the first stage  it considers actions one by one  for each
action  it constructs a set of vectors based on a finite number of systematically generated
belief points using an operator similar to the backup operator  in the second stage  vectors
for different actions are pooled together and extraneous vectors are pruned 
there are proposals to carry out standard update approximately by dropping vectors
that are marginally useful  e g   kaelbling et al        hansen        here is one idea
along this line that we have empirically evaluated  recall that to achieve  optimality  the
stopping threshold for the bellman residual should be              our idea is to drop
marginally useful vectors at various stages of standard update while keeping the overall
error under    and to stop when the bellman residual falls below     it is easy to see
that  optimality is still guaranteed this way  we have also tried to start with a large error
tolerance in hope to prune more vectors and gradually decrease the tolerance level to    
reasonable improvements have been observed especially when one does not need quality
of policy to be high  however such approximate updates are much more expensive than
point based updates  in the context of the modified value iteration algorithm  they are
more suitable alternatives to standard updates than point based update 

    point based value iteration and value function approximation

point based value iteration starts with a set of vectors and generates a sequence of vector
sets by repeatedly applying point based update  the last set can be used to approximate
the optimal value function 
  

fizhang   zhang
various methods for approximating the optimal value function have been developed
previously   we will compare them against point based value iteration along two dimensions      whether they map one set of vectors to another  that is whether the can be
interleaved with standard updates  and     if they do  whether they can guarantee convergence when interleaved with standard updates 
lovejoy        proposes to approximate the optimal value function v  of a pomdp
using the optimal value function of the underlying markov decision process  mdp   the
latter is a function over the state space  so v  is being approximated by one vector 
littman et al       b  extend this idea and approximate v  using jaj vectors  each of
which corresponds to a q function of the underlying mdp  a further extension is recently
introduced by zubek and dietterich         their idea is to base the approximation not on
the underlying mdp  rather on a so called even odd pomdp that is identical to the original
pomdp except that the state is fully observable at even time steps  platzman       
suggests approximating v  using the value functions of one or more fixed suboptimal policies
that are constructed heuristically  those methods do not start with a set of vectors and
hence do not map a set of vectors to another  however  they can easily be adapted to do so 
however  they all put a predetermined limit on the number of output vectors  consequently 
convergence is not guaranteed when they are interleaved with standard updates 
fast informed bound  hauskrecht     a   q function curve fitting  littman et al      b  
and softmax curve fitting  parr and russell       do map a set of vectors to another  however  they differ drastically from point based value iteration and from each other in their
ways of deriving the next set of vectors from the current one  regardless of the size of the
current set  fast informed bound and q function curve fitting always produces jaj vectors 
one for each action  in softmax curve fitting  the number of vectors is also determined a
priori  although it is not necessarily related to the number of actions  those methods can be
interleaved with standard dp updates  unlike point based value iteration  they themselves
may not converge  hauskrecht        even in cases where they do converge themselves 
the algorithms resulting from interleaving them with standard updates do not necessarily
converge due to the a priori limits on the number of vectors 
grid based interpolation extrapolation methods  lovejoy       brafman       hauskrecht
    b  approximate value functions by discretizing the belief space using a fixed or variable
grid and by maintaining values only for the grid points  values at non grid points are estimated by interpolation extrapolation when needed  such methods cannot be interleaved
with standard dp updates because they do not work with sets of vectors 
there are grid based methods that work with sets of vectors  lovejoy s method to lower
bound the optimal value function  lovejoy        for instance  falls into this category  this
method is actually identical to point based value iteration except for the way it derives the
next set of vectors from the current one  instead of using point based update  it backs up on
grid points in a regular grid  convergence of this method is not guaranteed  the algorithm
resulting from interleaving it with standard updates may not converge either 
   hauskrecht        has conducted an extensive survey on previous value function approximation methods
and has empirically compared them in terms of  among other criteria  complexity and quality  it would
be interesting to also include point based value iteration in the empirical comparison  this is not done
in the present paper because our focus is on using point based value iteration to speed value iteration 
rather than using as a value function approximation method 

  

fispeeding up value iteration in pomdps
the incremental linear function method  hauskrecht       roughly corresponds to a
variation of point based value iteration that uses non lp point based update  section     
augmented with the gauss seidel asynchronous update  the method does not have access to
witness point  so it starts  for the purpose of backup  with extreme points of the belief space
and supplement them with projected points  this choice of points appears poor because it
leads to a large number of vectors and consequently the backup process is  usually stopped
well before  convergence  hauskrecht       

    previous work related to modified value iteration
the basic idea of our modified value iteration algorithm vi  is to add  in between two
consecutive standard updates  operations that are inexpensive  the hope is that those
operations can significantly improve the quality of a vector set and hence reduce the number
of standard updates 
several previous algorithms work in the same fashion  the differences lie in the operations that are inserted between standard updates  the reward revision algorithm  white
et al        constructs  at each iteration  a second pomdp based on the current set of
vectors  it runs value iteration on the second pomdp for a predetermined number of steps 
the output is used to modify the current set of vectors and the resulting set of vectors is
fed to the next standard update 
why is reward revision expected to speed up value iteration  let v be the value function
represented by the current set of vectors  the second pomdp is constructed in such way
that it shares the same optimal value function as the original pomdp if v is optimal 
as such  one would expect the two pomdps to have similar optimal value functions if v
is close to optimal  consequently  running value iteration on the second pomdp should
improve the current value function  and it is inexpensive to do so because the second
pomdp is fully observable 
reward revision is conceptually much more complex than vi  and seems to be less
ecient  according to white et al          reward revision can  on average  reduce the
number of standard updates by     and computational time by      from tables   and
   we see that the differences between vi  and vi are much larger 
the iterative discretization procedure  idp  proposed by cheng        is very similar to
vi   there are two main differences  while vi  uses point based update  idp uses non lp
point based update  while point based update in vi  backs up on witness points and belief
points found by linear programs  non lp point based update in idp backs up on extreme
points of witness regions found as by products by cheng s linear support or relaxed region
algorithms 
cheng has conducted extensive experiments to determine the effectiveness of idp in
accelerating value iteration  it was found that idp can cut the number of standard updates
by as much as     and the amount of time by as much as      those are much less
significant than the reductions presented in tables   and   
hansen s policy iteration  pi  algorithm maintains a policy in the form of a finite state
controller  each node in the controller represents a vector  at each iteration  a standard
update is performed on the set of vectors represented in the current policy  the resulting
  

fizhang   zhang
set of vectors is used to improve the current policy  and the improved policy is evaluated
by solving a system of linear equations  this gives rise to a third set of vectors  which is
fed to the next standard update 
we compared the performance of hansen s pi algorithm to vi   table   shows  for each
test problem  the number of standard updates and the amount of time the algorithm took 
comparing with the statistics for vi   table     we see that pi performed more standard
updates than vi   this indicates that policy improvement evaluation is less effective than
point based value iteration in cutting down the number of standard updates  in terms of
time  pi is more ecient than vi  on the first three problems but significantly less ecient
on all other problems 
 x co cheese  x  paint tiger shuttle network aircraft
 
 
 
  
  
 
  
 
   
       
   
   
  
            
table    number of standard updates and time that pi took to compute      optimal
policies 
it might be possible to combine vi  and pi  to be more specific  one can probably
insert a policy improvement evaluation step between two point based updates in pointbased value iteration  figure     this should accelerate point based value iteration and
hence vi   this possibility and its benefits are yet to be investigated 

   conclusions and future directions
value iteration is a popular algorithm for finding  optimal policies for pomdps  it typically performs a large number of dp updates before convergence and dp updates are
notoriously expensive  in this paper  we have developed a technique called point based dp
update for reducing the number of standard dp updates  the technique is conceptually
simple and clean  it can easily be incorporated into most existing pomdp value iteration algorithms  empirical studies have shown that point based dp update can drastically
cut down the number of standard dp updates and hence significantly speeding up value
iteration  moreover  point based dp update compares favorably with its more complex
variations that we can think also  it also compares favorably with policy iteration 
the algorithm presented this paper still requires standard dp updates  this limits its
capability of solving large pomdps  one future direction is to investigate the properties
of point based value iteration as an approximation algorithm by itself  another direction is
to design ecient algorithms for standard dp updates in special models  we are currently
exploring the latter direction 
   in hansen s writings  policy improvement includes dp update as a substep  here dp update is not
considered part of policy improvement 

  

fispeeding up value iteration in pomdps

acknowledgments
research is supported by hong kong research grants council grant hkust       e 
the authors thank tony cassandra and eric hansen for sharing with us their programs 
we are also grateful for the three anonymous reviewers who provided insightful comments
and suggestions on an earlier version of this paper 

references

astrom  k  j          optimal control of markov decision processes with the incomplete
state estimation  journal of computer and system sciences              
brafman  r  i          a heuristic variable grid solution for pomdps  in proceedings of
the fourteenth national conference on artificial intelligence aaai              
cassandra  a  r   littman  m  l   and zhang  n  l          incremental pruning  a
simple  fast  exact method for partially observable markov decision processes  in
proceedings of thirteenth conference on uncertainty in artificial intelligence        
cassandra  a  r       a   exact and approximate algorithms for partially observable
markov decision processes  phd thesis  department of computer science  brown
university 
cassandra  a  r       b   a survey of pomdp applications  in working notes of aaai
     fall symposium on planning with partially observable markov decision processes        
denardo  e  v          dynamic programming  models and applications prentice hall 
eagle  j  n         the optimal search for a moving target when the search path is
constrained  operations research                   
cheng  h  t         algorithms for partially observable markov decision processes  ph d
thesis  university of british columbia 
hansen  e  a          solving pomdps by searching in policy space  in proceedings of
fourteenth conference on uncertainty in artificial intelligence          
hauskrecht  m      a   incremental methods for computing bounds in partially observable
markov decision processes  in proceedings of the fourteenth national conference on
artificial intelligence  aaai              
hauskrecht  m      b   planning and control in stochastic domains with imperfect information  phd thesis  department of electrical engineering and computer science 
massachusetts institute of technology 
hauskrecht  m          value function approximations for partially observable markov
decision processes  journal of artificial intelligence research            
  

fizhang   zhang
littman  m  l   cassandra  a  r  and kaelbling  l  p       a   ecient dynamicprogramming updates in partially observable markov decision processes  technical
report cs        brown university 
littman  m  l   cassandra  a  r  and kaelbling  l  p       b   learning policies for partially observable environments  scaling up  in proceedings of the fifteenth conference
on machine learning          
littman  m  l          algorithms for sequential decision making  ph d thesis  department of computer science  brown university 
kaelbling  l  p   littman  m  l  and cassandra  a  r         planning and acting in
partially observable stochastic domains  artificial intelligence  vol     
lovejoy  w  s          computationally feasible bounds for partially observed markov
decision processes  operations research              
lovejoy  w  s          suboptimal policies with bounds for parameter adaptive decision
processes  operations research              
monahan  g  e          a survey of partially observable markov decision processes  theory  models  and algorithms  management science               
parr  r   and russell  s          approximating optimal policies for partially observable
stochastic domains  in proceedings of the fourteenth international joint conference
on artificial intelligence           
papadimitriou  c  h   tsitsiklis  j  n         the complexity of markov decision processes 
mathematics of operations research                 
platzman  l  k         optimal infinite horizon undiscounted control of finite probabilistic systems  siam journal of control and optimization              
puterman  m  l          markov decision processes  in d  p  heyman and m  j  sobel
 eds    handbooks in or   ms   vol              elsevier science publishers 
smallwood  r  d  and sondik  e  j          the optimal control of partially observable
processes over a finite horizon  operations research                
sondik  e  j          the optimal control of partially observable markov processes  phd
thesis  stanford university 
sondik  e  j          the optimal control of partially observable markov processes over
the infinite horizon  operations research                
white  c  c  iii and scherer  w  t          solution procedures for partially observed
markov decision processes  operations research                 
zhang  n  l   lee  s  s   and zhang  w         a method for speeding up value iteration
in partially observable markov decision processes  in proc  of the   th conference on
uncertainties in artificial intelligence 
  

fispeeding up value iteration in pomdps
zhang  n  l  and w  liu         a model approximation scheme for planning in stochastic
domains  journal of artificial intelligence research             
zubek  v  b  and dietterich  t  g         a pomdp approximation algorithm that anticipates the need to observe  to appear in proceedings of the pacific rim conference
on artificial intelligence  pricai        lecture notes in computer science  new
york  springer verlag 

  

fi