journal of artificial intelligence research                 

submitted       published     

technical paper recommendation  a study in combining
multiple information sources
chumki basu

cbasu cs rutgers edu

haym hirsh

hirsh cs rutgers edu

department of computer science  rutgers university      frelinghuysen road 
piscataway nj            and
telcordia technologies  inc       south street 
morristown nj           
department of computer science  rutgers university      frelinghuysen road 
piscataway nj           

william w  cohen

wcohen whizbang com

craig nevill manning

nevill cs rutgers edu

whizbang  labs  whizbang labs   east       henry street 
pittsburgh pa      

department of computer science  rutgers university      frelinghuysen road 
piscataway nj           

abstract

the growing need to manage and exploit the proliferation of online data sources is opening up new opportunities for bringing people closer to the resources they need  for instance 
consider a recommendation service through which researchers can receive daily pointers to
journal papers in their fields of interest  we survey some of the known approaches to the
problem of technical paper recommendation and ask how they can be extended to deal
with multiple information sources  more specifically  we focus on a variant of this problem
  recommending conference paper submissions to reviewing committee members   which
offers us a testbed to try different approaches  using whirl   an information integration system   we are able to implement different recommendation algorithms derived from
information retrieval principles  we also use a novel autonomous procedure for gathering
reviewer interest information from the web  we evaluate our approach and compare it
to other methods using preference data provided by members of the aaai    conference
reviewing committee along with data about the actual submissions 

   introduction
we can define the paper recommendation problem as follows 
given a representation of my interests  find me relevant papers 

in fact  if we replace papers in the above definition with the name of some other artifact
of choice  we have yet another instantiation of a recommendation problem  what then
makes paper recommendation all that interesting 
   

fibasu  hirsh  cohen    nevill manning

the ability to automatically filter a large set of papers and find those that are most
aligned with one s research interests has its advantages  with the growing number of
publications  many of them online  it is dicult to keep up with the latest research  even
if it s within one s field  with the timeliness of information becoming all the more critical 
it is also desirable for a paper to reach its target audience with minimal latency  although
a straightforward approach to finding relevant papers may look for close matches between
a person s interests and a paper s content  what is less clear is how to represent both the
interests of the researchers and the contents of the papers 
another feature that sets paper recommendation apart is that there is a variant problem
which must be dealt with on a regular basis by numerous conference chairs  conferences
offer a venue where a large number of fairly specific papers must be distributed to a smaller
number of reviewers  all within a very tight timeframe  even with the scope of the problem
being constrained to some degree by topic  conference organizers and or reviewers still must
expend a great deal of time and effort before they can begin the reviewing process  this
would suggest that there can be real value in finding ways of automating the filtering process
that would make it less burdensome to the potential consumers 
we consider algorithms for recommending focused sets of technical papers  we use
conference reviewing as a platform to explore a series of questions relating to the recommendation process  there has been new interest in the ai community for this problem
recently since it was proposed as a  challenge  task at ijcai     geller         our focus
on conference reviewing turns out to be a natural choice since we can obtain data about a
set of papers  i e   the conference submissions  and we can also obtain information about
the preferences of a set of reviewers for these submissions  in the following section  we discuss related work that addresses the conference reviewing problem  we also consider how
other work in the area of recommender systems   e g   recommending articles to newsgroup
readers or recommending web pages to web site visitors   can contribute to this task 
however  our focus is on varying the sources of information in our data representations 
thereby allowing us to formulate different recommendation algorithms based on how we recombine these sources when computing similarity  we show that there is indeed a difference
in performance when we vary the amount and source of data  compared to the baseline of
using a single source of information in our data representations  we also compare these
recommendation algorithms against each other  against collaborative filtering  and against
the random assignment of papers to reviewers  we apply our methods to experimental data
involving reviewer preferences and conference abstracts for the aaai    conference   

   what we know about paper recommendation
we already know that by recommending papers to reviewers  and more generally  to the
arbitrary researcher  we are trying to be selective in choosing those papers that will ultimately reach the consumer based on relevance to interests or expertise  however  finding
papers for conference reviewers is necessarily a more complex task  since papers may be
assigned to reviewers based on other criteria  for instance  reviewer load balancing and
conict resolution of reviewer author aliations may be two such criteria  in addition  the
   the data were obtained with permission from aaai  the aaai reviewers  and when appropriate  from
the authors of the submitted papers 

   

fitechnical paper recommendation  a study in combining multiple information sources

reviewer s own reviewing preferences may be inuenced by considerations such as a paper s
readability and novelty  for example  a preference for novelty may lead a reviewer to choose
a paper simply because it is not relevant to his or her interests 
our methods are not suited to address these latter issues for a number of reasons  first 
for confidentiality purposes  we lack information related to the author identity or aliation
of the submitted conference papers  secondly  since constraint satisfaction is not our main
concern   we are primarily interested in finding the best papers for each person without
regard to whether multiple people receive the same paper   we do not incorporate other
criteria into our selection procedure  we also do not have a way to represent the  novelty 
of a paper with respect to any consumer  and thereby do not have a means for recognizing
it  finally  our methods do not distinguish between the notion of interest and expertise
with respect to reviewers  for the more general recommendation problem  the researcher
may want to retrieve papers in areas outside of his or her expertise  in which case a separate
representation for each would be needed 
previous work in the area of assigning conference papers to reviewers had approached
the problem as one of content based information retrieval  dumais and nielsen        used
data provided by    members of the reviewing committee for the hypertext     conference  these reviewers not only submitted abstracts of their papers and or interests  but also
provided complete relevance assessments for the     papers submitted to the conference 
using an information retrieval method known as latent semantic indexing  lsi   they compared the reviewer abstracts with the submissions  ranking the submissions from most to
least similar to each reviewer  from their results  they noticed  based on the performance
metric that evaluates the number of relevant articles returned in the top     that they
could achieve an average of     improvement using their automated methods compared to
random assignment of articles to reviewers 
while these results are encouraging  we believe that the widespread availability of online
resources introduces opportunities for exploring some new issues  what if the reviewers
weren t asked to supply interest information  can the process of gleaning reviewer interest
data be automated with simple methods  how well do we do at retrieving relevant papers
using this  approximation  of reviewer interests  the automatic collection of reviewer
interest information from the web  which effectively removes the reviewer from loop  is a
novel aspect of our research 
yarowsky and florian        attempted a similar task for the acl    conference  however  their primary focus was on classification   the assignment of every paper to exactly
one of six conference committees  they used    papers which were submitted to the acl
conference in electronic form and also requested committee members to provide representative papers  when the number of papers returned by these members was insucient  they
augmented the collection with other papers downloaded from online sources  they used
content based retrieval  within the context of the vector space model  salton         as one
of their routing strategies  the main algorithm first computed a centroid for each reviewer
based on representative papers and then computed a centroid for each committee as the
sum of its reviewer centroids  then  each paper was classified  assigned to a committee  by
computing its cosine similarity with the committee centroids and choosing the one with the
highest rank  amongst other approaches  they experimented with a naive bayes classifier
and the assessment of similarity between reviewing committee members and authors cited
   

fibasu  hirsh  cohen    nevill manning

in the papers  based on their system performance relative to human judges on the same
task  evaluated against the actual assignments provided by the program chair of the conference   they extrapolated that automated methods could be as effective as human judges 
especially in cases where the judges may be less experienced 
when we are dealing with large conferences with several hundred papers covering a variety of areas  the information load is even greater for conference organizers and reviewers
alike  in these cases  getting evaluative relevance judgments for all submitted  or even accepted  papers from the reviewers is not feasible   as an example  for the aaai conference 
reviewers do not even have to state their preferences for all the papers they can potentially
review  instead  they can stop scanning the list as soon as they have filled up their quota
of  bids   papers they expressed interest in reviewing   therefore  we focus on building
an extensible framework for recommendation   defining a process whereby we can systematically incorporate more information in formulating recommendation algorithms  for the
purpose of generating better recommendations 
content based information retrieval  also known as content based filtering  is a popular
recommendation method  consider systems that recommend web pages such as syskill  
webert  pazzani   billsus         there are a number of other systems such as webwatcher
and fab that do content based filtering  mainly as part of a hybrid approach that also
involves collaborative filtering  whereas content based filtering looks only at the contents
of an artifact  e g   the words on a web page   collaborative filtering will also consider the
opinions of other like minded people with respect to these artifacts  collaborative filtering
has been used to recommend netnews articles  konstan  miller  maltz  herlocker  gordon 
  riedl         movies  hill  stead  rosenstein    furnas        basu  hirsh    cohen 
       music  cohen   fan        shardanand   maes         and even jokes  gupta 
digiovanni  narita    goldberg         since both content based and collaborative methods
use data that are orthogonal to one another  there are opportunities to come up with hybrid
approaches that use combinations of the data  our own work on movie recommendation
provides another example of how to design a hybrid system  hybrid systems exploit data
from multiple sources with the expectation that they can do better by compensating for
the limiting factor of data sparseness associated with any single source 
in our current study  we would like to identify different sources of information to describe
both papers and reviewers  with the expectation that the individual pieces themselves  along
with knowledge of how to combine them  can make a difference in the recommendations 
although we do share the common goal of combining data from multiple sources with the
hybrid recommendation approaches  the algorithms that we develop are strictly contentbased  for evaluative purposes  we also compare our algorithms against the results of
applying collaborative filtering methods to the set of reviewer preferences 

   representing papers and reviewers
our approach to recommendation is to represent each entity using a variety of information
sources  to enumerate different combinations of these sources  and to evaluate the effectiveness of these combinations using ranked retrieval methods  for the paper recommendation
problem  we have two types of entities   papers and their consumers  reviewers  in our
case   for each entity  we can represent the salient features of that entity as a sequence
   

fitechnical paper recommendation  a study in combining multiple information sources

of one or more information sources  in addition  we also need another type of information
source that relates a reviewer to a paper  namely  the reviewers  actual preferences for the
papers  we begin with a discussion of our choice of information sources   some of these
choices are based on data that are typically used to assign papers to reviewers  and are
usually provided explicitly by the papers  authors  while other choices rely more on implicit
knowledge mined from the semi structured data available on the web 

    paper information sources

all of our experiments were based on a compilation of submitted abstracts obtained from
aaai for the aaai    conference  there were     papers submitted to this conference 
aaai gave us a collection of     papers to use in our experiments   the abstracts of    
accepted papers and the abstracts of     papers that had been rejected but whose authors
had granted aaai permission to provide the abstract for this work  also excluded were
any papers that had been authored by any of the authors of this paper 
for each submission we obtained its title  abstract  and a set of user assigned keywords
from a prespecified list  therefore  each paper has associated with it a set of three information sources all of which were provided by the papers  authors  although one may consider
the body of the paper as another source  this information was not available to the reviewers
 nor us   so we do not use it as a source 

    reviewer information sources

so far  we have seen an example where an entity such as a paper can be represented by
multiple information sources mainly because it is composed of distinct units such as a title 
abstract  etc  however  there is another case where we may want to multiply represent an
entity  consider trying to automatically compose a representation of a reviewer s interests 
we may try first to go to the reviewer s home page  from there  we may decide to look
around for the reviewer s papers  each of these sources can offer a different point of view
of the reviewer s interests  and therefore  can be considered as a separate unit  we focus on
these sources   the reviewer s entry level home page and the papers that are referenced from
the home page   as a substitute for asking the reviewer to provide interest information 
we believe both home pages and online papers are credible information sources since it
is likely that a fair number of conference reviewers have stated their research interests in
either or both sources  since one of our paper information sources is the paper abstract 
we decided to represent the reviewer as an  abstract of interests   in the case of home
pages  the entire text of the reviewers s entry level home page was taken as an abstract of
the reviewer s interests  in the case of postscript files  we define an abstract to be the first
    words extracted from the paper 
we extracted all of this information from the web using pre existing utilities  to find
reviewers  home pages  we fed the names and aliations of the members of the review
committee into ahoy   a home page finding engine  shakes  langheinrich    etzioni        
when ahoy returned at least one match  we supplied the url as a starting point for
w mir   an http service that retrieves files from the contents of web sites  we used
   http   ahoy cs washington edu      
   http   www math uio no janl w mir 

   

fibasu  hirsh  cohen    nevill manning

w mir to download only html files and postscript files accessible from the entry level
home page and residing on the same site   since all of a person s papers may not directly
be available from one site  we additionally retrieved cross references to other sites which
contained postscript files  also using w mir  the postscript files were then converted to
ascii using prescript  nevill manning  reed    witten        
all postscript files retrieved for a reviewer are treated uniformly  although it would
be desirable to attempt to do so in future work  we make no attempt to determine the
timeliness of a paper  especially with respect to a reviewer s current interests  we also do
not distinguish between journal papers  conference papers  and even lecture notes  it is
for this reason that we do not attempt to do any detailed analysis of the contents of these
files  e g   to automatically extract titles  abstracts  etc    instead  we rely on heuristics
such as looking at the first n words to approximate a paper s abstract  although detailed
analysis is likely to be valuable in the paper recommendation process  our immediate goal
is to obtain a gross sense of the usability of various sources of semi structured information 

    reviewer preferences
to evaluate our queries we need some  ground truth    some set of data specifying what
papers each reviewer had selected as suitable for him or her to review  with this information 
we can evaluate how different approaches perform in making the same choices  we note that
this is only an approximation to the full set of abstracts that the reviewer might have liked
  the reviewing process only requires a reviewer to find some minimum quota of papers 
and once that quota is reached  a reviewer need not look at other papers to find more  we
view this optimistically as yielding a close approximation to what a reviewer s full set of
preferences would be  since reviewers are able to peruse abstracts by keywords and often
attempt to inspect at least the subset of papers labeled by keywords in the areas in which
they are knowledgeable 
in our experiments ground truth comes from the actual preferences stated by      of
the      aaai    reviewers who gave aaai permission to release their preference information for the papers we considered in this work  we point out that this data only reects
the reviewers  initial preferences for reviewing  we do not have data on what papers the
reviewers actually received following the aaai reviewer assignment process 
of course  one potential limitation of this data is that it is based only on a portion of data
that may not be representative of the entire data for the conference  for example  we have
preference data for approximately half of all of the reviewers and are predicting preferences
for a collection of papers whose distribution is skewed towards the accepted papers  there is
also the issue of whether aaai researchers are representative of the much larger community
of researchers at large   we can ask a similar question of the user populations of other
conferences as well   however  we consider these as acceptable limitations resulting from
our use of conference reviewing as a platform for paper recommendation 
   at the moment  we focus on postscript for convenience  but there is no reason to limit ourselves to just
one file format  the main constraint is being able to extract the words from a document 

   

fitechnical paper recommendation  a study in combining multiple information sources

   recommendation methodology

in this section  we examine both collaborative and content based methods of recommendation  these methods allow us to explore the use of different subsets of the data described
in the previous section 

    recommending with reviewer and paper information sources

in the following sections  we outline a content based recommendation framework that uses
data describing the papers as well as data describing the reviewers to make recommendations  the reviewer preference data is then used for evaluation purposes  but not as input
to the recommendation process 
in order to locate papers that closely match reviewer interest data we rely on ad hoc
similarity metrics commonly used in the information retrieval community  we will describe
these methods further in the section on whirl  in brief  for each reviewer we compare
the given reviewer representation with the appropriate paper information source s   each
of these comparisons can be implemented as a query that returns a rank ordered list of
papers  we can consequently compute precision at top n   or the proportion of the papers
returned that were actually preferred by the reviewer  for each query  our final score for
each query is the average of this value  computed over a subset of    reviewers  from the
larger set of reviewers who gave us their permission  
our recommendation algorithms take different paper and reviewer information sources
as inputs  since our data can be plotted along two dimensions  let reviewer be the set of
information sources describing reviewers and paper the set of information sources describing
papers  we can construct a reviewer  paper matrix where each entry in this matrix is
a score measuring the effectiveness of using the respective sources   reviewer   paper    to
compute similarity between reviewers and papers when performing a ranked retrieval  for
instance  given the paper and reviewer representations we have described  we can construct
a      matrix  which gives us   possible evaluations or scores  we will refer to this matrix
as the recommendation sources matrix 
conceptually  we can extend the recommendation sources matrix along each dimension 
by considering combinations of the rows and columns  we refer to the augmented matrix
as the source combinations matrix  we can now define a recommendation algorithm as
a combination method or procedure applied to one or more rows columns of the source
combinations matrix  this introduces another dimension for comparison   the combination
method itself   which we consider by looking at replicates of the source combinations matrix 
now  we can pose the following questions for experimental analysis 
i

j

 do recommendation algorithms that incorporate more information lead to better performance 
 if so  does the method of combining data used by the algorithm make a difference 
      whirl

for all of our queries  we use whirl  a system specifically designed for informationintegration tasks  cohen      b  cohen   hirsh         for these tasks  it is often necessary to manipulate in a general way information obtained from many heterogeneous online
   

fibasu  hirsh  cohen    nevill manning

sources  each potentially having its own data organization and terminology  in particular 
whirl makes it possible to integrate information that can be decomposed and represented
in a clean  modular way  for example  we would like to have information about home pages
and postscript papers represented separately  using the information integration tool to
resolve these sources of information 
whirl is a conventional dbms that has been extended to use ad hoc similarity metrics
developed in the information retrieval community  using these metrics  it can reason about
pieces of text culled from heterogeneous sources based on the similarity of values rather than
on strict equality  whirl computes similarity using the  vector space  representation to
model text  salton         each text object is represented by a vector of term weights
 where the terms have been stemmed using porter s algorithm  porter         based on
the tfidf weighting scheme  similarity between two vectors is computed using the cosine
similarity metric  the answers to a query are presented by rank ordering the generated
tuples  with tuples having more similar pairs of attribute fields appearing first 
for example  using whirl  we can pose the following query 
select reviewer name  paper id
from paper and reviewer
where reviewer descriptor sim paper abstract

this query will return a list of reviewer names and paper ids for papers whose abstracts
were similar to the reviewer s interest descriptor  rather than returning only those tuples for
which the descriptor and abstract fields are identical  as would be performed by a traditional
database join  this query returns name and id pairs for those tuples whose fields contain
similar terms  ordered according to decreasing value of similarity  the advantage of doing
ad hoc joins without requiring the textual fields to be identical to one another is important
when the text comes from multiple sources and thereby may use different terminology  it is
also important from the perspective of comparing the relative importance of different fields
to one another in an ecient way 
to use whirl all data must be stored in the form of whirl relations  for our
data we constructed two relations  each one representing different information sources  for
each conference submission  we form a paper relation containing its id  abstract  keywords 
and title  for every reviewer  we form a reviewer relation which contains a single tuple
with attributes representing the reviewer s name and some representation of the reviewer s
interests  for example  based on the reviewer s home page  
so far  we have discussed how we can use whirl to formulate queries involving a single
information source for both reviewers and papers  however  an advantage of the whirl
approach lies in the simplicity with which we can extend these queries to incorporate multiple sources  the primary advantage of using whirl in our work is the ease with which
we can measure the impact of conjunctive queries incorporating data from multiple sources 
we form conjunctive queries by adding multiple conditions to a where clause 
select reviewer name  paper id
from paper and reviewer
where reviewer descriptor sim paper abstract
and reviewer descriptor sim paper keywords
   

fitechnical paper recommendation  a study in combining multiple information sources

when a whirl where clause contains multiple conditions  the similarity scores
of the individual conjuncts are combined by taking their product as though they were
independent probabilities  since similarity scores are not independent probabilities  we
only use it as a convenient way to combine scores  albeit one that offers a straightforward
approach to combination which has been previously studied  cohen      a   in the above
query  whirl would assign a score that reects both the similarity of the submitted
paper s abstract and the reviewer s descriptor  as well as the similarity of the submitted
paper s keywords and the reviewer s descriptor 
      combining information sources by query expansion

what does it mean for a recommendation algorithm to combine data from multiple information sources  this means enumerating the information sources that can be used as possible
inputs to the algorithm  and then defining a way to use these sources to compute similarity 
for instance  suppose we look at   reviewer source and   paper sources for a given collection
of reviewers and papers  to decide whether a paper is likely to interest the reviewer  we
can compute the similarity between the reviewer source and each of the paper sources and
combine the two similarity scores  alternatively  we can compute a single similarity score
by first combining the two paper sources into a single representation and then computing
its similarity with respect to the reviewer source 
the idea of combining two sources into a single representation can be implemeted by
appending terms from the sources  in information retrieval  terms from relevant sources
are often appended to a baseline representation of a query during the process of query
reformulation  this is usually referred to as query expansion  since our methods bear a
resemblance to query expansion  we make this analogy  these expansion methods will be
further described in the following sections  of course  we do not have prior knowledge
of the relevance of our sources  and in this sense  we differ from the information retrieval
implementation of query expansion 
when we compare the relative performance of recommendation algorithms  we have
multiple dimensions along which to compare the results  we can differentiate the results
based on the methods used to combine the data and compute similarity or we can differentiate between the results based on which information sources were used in the comparison 
in other words  on the same set of inputs  does one method of query expansion perform
better than another  if we want to compare the merit of a single source  we can consider
two groups of algorithms   those that include a given source as input to the algorithm  and
those that exclude this source  if we simply count the number of times algorithms that
include this source outperform algorithms that exclude it  we can determine the relative
merit of the source 
      the concatenation method

one way to  add  information from a new data source is to append the terms appearing in
the source to the original whirl query  for this type of query  we always have a single
whirl conjunct but each of the textual fields appearing in the conjunct can  grow  with
the addition of new terms  we call this method  queryconcat 
   

fibasu  hirsh  cohen    nevill manning

suppose  for example  that we start with the base query from the previous section that
only compares reviewer descriptors with paper abstracts  now  suppose we want to compare
reviewer descriptors not only to the paper abstracts but also to the paper keywords  one
way to do this is to use the queryconcat method  we form a new field representing the
union of the words appearing in the paper abstract and paper keywords fields which we can
substitute in the original query  let paper descriptor   paper abstract   paper keywords 
our new query is 
select reviewer name  paper id
from paper and reviewer
where reviewer descriptor sim paper descriptor

similarly  we can replace paper descriptor in the where clause to represent different
combinations of the fields  paper abstract  paper keywords and paper title using the union
operator 
      the conjunction method

as we previously stated  an important motivation for using whirl is its ability to execute
conjunctive queries  which we can also use to combine information sources in the recommendation process  for this type of query  instead of adding terms to any particular text field 
we add conjuncts to the original where  we refer this method of reformulating queries
as queryconjunct 
we enumerate the query combinations that we considered for queryconjunct as follows 
using the same sources as for queryconcat  we can begin the queries as before 
select reviewer name  paper id
from paper and reviewer
where

but now  replacing the body of the where clause with the following 
a  reviewer descriptor sim paper abstract
k  reviewer descriptor sim paper keywords
t  reviewer descriptor sim paper title
ak  reviewer descriptor sim paper abstract
and reviewer descriptor sim paper keywords
at  reviewer descriptor sim paper abstract
and reviewer descriptor sim paper title
kt  reviewer descriptor sim paper keywords
and reviewer descriptor sim paper title
akt  reviewer descriptor sim paper abstract
and reviewer descriptor sim paper keywords
and reviewer descriptor sim paper title

we assign the labels  a  abstract   k  keywords   and t  title  to the queries to identify
the paper sources used   we use these labels in a comparable fashion for the queryconcat
method  representing the information sources that are concatenated together  
   

fitechnical paper recommendation  a study in combining multiple information sources

for each of the above queries  we can also vary the source of data used to represent the
reviewers  the first variant accounts for the case where the reviewer s descriptor contains the
words from the reviewer s home page  the second accounts for the case where the descriptor
contains the union of the first     words extracted from each postscript file obtained from
the reviewer s web pages 
we decided to try yet another combination to see whether using both representations
for reviewers would improve performance  for simplicity  we chose to test this hypothesis
with an expanded conjunctive query involving a single extra conjunct  we constructed
a reviewer table that contains two attributes  papers  consisting of the abstracts of the
reviewer s postscript papers  and homepage  consisting of the reviewer s home page   we
then ran each of the above queries  but now with an additional conjunct appearing in each
where clause 
reviewer homepage sim paper keywords

we chose to use keywords as the paper descriptor based on our intuitions that a paper s
keywords and a reviewer s homepage would have a greater number of words in common 

    recommending with reviewer preferences

since we have evaluations from the reviewers on a common set of papers  one approach
for recommending papers would be to take this information and use it for collaborative
filtering  we note that for the actual conference reviewing problem  collaborative filtering
as a method for assigning papers may not be practical  although we have the benefit of
using all of the preferences for a set of reviewers in our study  this information will generally
not be available to the reviewers as they are making their selections  thereby making it more
dicult to base predictions on the preferences of others  nevertheless  it is worthwhile to
measure the impact of using reviewer preferences for the purpose of recommending papers 
the recommendation methodology for the collaborative filtering approaches is implemented as follows  each reviewer is presented with a recommended paper in an online
manner  after the paper is presented the reviewer tells the system if the paper was relevant  if it was  then the paper is assigned a rating of   and the paper is said to be rated
positively  if the paper was not relevant  it is assigned a rating of   and is said to be rated
negatively  let rating  r  p   represent the rating that has been assigned to paper p by
reviewer r  when the paper is not relevant  the reviewer also provides a single relevant
paper as a positive example in order to condition future recommendations  since we know
which papers were liked by the reviewers  we can simulate this process with the data that
we have  we experiment with two collaborative filtering algorithms  knn  hill et al        
cohen   fan        and extended direct bayes  cohen   fan         we let p   p       p   
represent the papers that have been previously rated by the reviewer in t     trials  the
knn algorithm uses the following distance metric to locate other reviewers  r   closest to
the current reviewer with respect to the papers that have already been rated 
t

i

dist r  r     jrating  r  p     rating  r   p  j         jrating  r  p       rating  r   p    j
t

t

we can then compute a score for an arbitrary paper  p   with respect to the ratings of
the k closest reviewers  r      r   as follows 
k

   

fibasu  hirsh  cohen    nevill manning

score p     rating  r   p           rating  r   p  
k

according to the above methodology  the highest scoring paper will be presented to the
reviewer as the next recommendation 
extended direct bayes can be viewed as an ad hoc extension of a direct bayesian approach to recommendation  we define r p   p   to represent the laplace corrected estimate
of the prior probability that the reviewer will give p a positive rating   r p   p   can be
thought of as measuring the  relatedness  between two papers   now consider an arbitrary
trial t and let p   p      p    represent the papers that have been rated positively by the
reviewer on previous trials and consider an arbitrary trial t 
we can now use the following scoring function to rank each paper p  
i

j

j

i

j

t

score p               r p  p               r p  p      
t

the subtrahend in the above expression represents the probability that p is not related
to any p  assuming that the p  s are independent  
i

i

    evaluation methodology

in the following sections  we evaluate the performance of our recommendation algorithms 
for collaborative filtering  we compute recommendations for a reviewer until we run out
of positive examples to use as feedback  for each reviewer s list of recommendations  we
measure precision in the top n   this gives us the proportion of the items returned in the
top n for a given reviewer that were actually preferred by the reviewer  although it is
possible to use other evaluation metrics  we compute precision at different levels of papers
returned since it is well suited to the conference reviewing task  since a reviewer may get a
list of about    papers to review  we would like to simulate this by recommending the top
   papers returned by our methods  by computing precision  we measure the percentage of
papers in this list that would have matched the reviewer s preferences  this metric is also
commonly used in the literature  for instance  dumais and nielsen        mostly used this
measure  i e   the number of relevant articles in the top     when reporting their results
since this constituted a reasonable reviewer load  we additionally report results of precision
at top     for the knn algorithm  we set k      for our experiments 
our recommendation algorithms can be seen as a choice of a query expansion method
crossed against a choice of the input data sources  for each of the methods queryconjunct
and queryconcat  we ran      queries detailed in the previous section  this resulted in   
runs per reviewer  per method  each run returned an ordered list of paper ids  for each run 
we again measure precision in the top n  for n      and n        in our discussion  we
refer to a run using abstracts based on a reviewer s papers as a p run  similarly  h runs will
be based on a reviewer s home page  finally  ph runs combine both sources of information
 using the extra conjunct   the results we will report represent precision values averaged
across the reviewers  in order for us to compare performance across different information
sources  we need to do our evaluation using the same population of reviewers  not all of
the reviewers who provided preference data had home pages and or papers available online 
therefore  we performed a set of runs using    reviewers randomly chosen from the set of
   

fitechnical paper recommendation  a study in combining multiple information sources

source s 
p top   
h top   
ph top   
p top   
h top   
ph top   

a
     
     
     
     
     
     

k
     
     
     
     
     
     

t
     
     
     
     
     
     

ak
     
     
     
     
     
     

at
     
     
     
     
     
     

kt
     
     
     
     
     
     

akt
     
     
     
     
     
     

table    average precision scores at top    and top    papers returned using queryconjunct 
reviewers who had both home pages and papers available online  and report results averaged
across these    reviewers 
as we mentioned earlier  reviewer choices may be inuenced by a variety of factors
ranging from a person s curiosity to a paper s readability  many of these factors are dicult
to model  furthermore  human judges may assign papers to reviewers according to criteria
other than relevance of paper contents to reviewer interests  and their individual opinions
may vary  therefore  it is highly unlikely that our proposed methods will achieve     
precision  unfortunately  given the nature of the problem  we have not been able to get
an assessment of how human judges would have done at the same task  nevertheless  we
can evaluate our recommendation framework built on content based information retrieval
principles and compare relative performance to other reasonable baseline approaches 

   results
there are a number of questions we would like to keep in mind as we analyze the results 
in the course of our experiments we vary both the amount of information input to our
algorithms and the method of query expansion used by the algorithms  one of the questions
we would like to answer is what algorithm or set of algorithms is most suited to the task
at hand  we also ask whether the choice of inputs results in measurable differences in
performance  the tabulation of results which provides the basis for analyzing our contentbased algorithms is presented in table   and table    the baseline method against which we
compare all algorithms is random assignment  this method assigns each reviewer a random
collection of papers  with this method  we can expect a precision of       in other words 
this means that if we were to select papers randomly  on average  each reviewer would like
fewer than   out of    of the papers selected 
table   and table   are replicates of the source combinations matrix we had discussed
earlier  since we ran two trials for top n papers returned  each table is actually the
concatenated representation of the matrices for the top    and top    experiments  in
the first three rows of table   and table    we report precision figures of the top   
papers returned for the queryconjunct method and the queryconcat method  respectively 
   

fibasu  hirsh  cohen    nevill manning

precision at top    and top   
   
compare dat
x

    
    
    
queryconjunct method

    
   
    
    
    
    
   
    
    
    

   

    
   
queryconcat method

    

   

figure    a comparison of two query methods
similarly  we show the results for top    papers returned in the bottom three rows of the
tables  since we can view the rows as representing the reviewer sources used in a query and
the columns representing the paper sources  we can measure the impact of adding data in
two ways  by reading across a row  across groups of columns representing n information
sources  we can gauge how the results vary as more paper data are included in the queries 
similarly  by reading down a column  we can gauge the differences in the results as more
reviewer data are included in the queries 
given this information  what can we say about the performance of our recommendation
algorithms that used different methods of query expansion  we can compare the relative
performance of the two methods queryconjunct and queryconcat based on the values listed
in table   and table    note that in all cases performance of these methods exceeds
that of random selection  with accuracies a factor   to   times better  in figure    we
record this information as a data point for every query that uses two or more sources of
information  since the methods differ in how they combine data from two or more sources 
it is meaningless to plot points that refer to queries using a single source   in this figure  the
x axis represents queries expanded using the queryconcat method and the y axis represents
queries expanded using the queryconjunct method  if a point falls on the x   y line  then
the two methods yielded the same performance for a query using the same information
sources  all points that fall in the area above the x   y line mark those queries where
queryconjunct had higher precision than queryconcat  the data reveal that in almost all
cases  queryconjunct had higher precision than queryconcat  thereby making queryconjunct
the dominant of the two query expansion methods and the preferred method of the two for
the task at hand 
our expectation is that as we increase the source data we should notice an increase
in precision  specifically  we note that for queryconjunct  the query that uses the most
information for a paper submission in a majority of cases performs statistically significantly
   

fitechnical paper recommendation  a study in combining multiple information sources

source s 
p top   
h top   
ph top   
p top   
h top   
ph top   

a
     
     
     
     
     
     

k
     
     
     
     
     
     

t
     
     
     
     
     
     

ak
     
     
     
     
     
     

at
     
     
     
     
     
     

kt
     
     
     
     
     
     

akt
     
     
     
     
     
     

table    average precision scores at top    and top    papers returned using queryconcat 
better  than queries that use less information and in no case performs statistically significantly worse 
we should note that adding information will not always lead to monotonically better
results  notice that for queryconjunct  in the case of top    papers returned  hkt is indistinguishable from hakt  we also note that pht performs better  though not statistically
significantly better  than phkt  there are similar cases for queryconcat  how do we explain these gaps  if these are indeed gaps  i e   they are true statistical differences  then we
may consider as an explanation that adding information may also be increasing the amount
of noise in our representations  consider  for example  that keywords from a fixed list can
often be a poor match to the real subject matter of a paper  in these special cases  the use
of keywords as a source could lead to a degradation in retrieval performance 
analogous to our analysis of the paper sources  we can now examine any column of
table   or table   and measure the effect of adding more information to the reviewer representation  for queryconjunct  a majority of the time  we find that queries incorporating
more information  ph entries  perform statistically significantly better than single source
queries  p and h entries  
so far  we have illustrated how we can move across groups of columns or blocks of
rows in the source combinations matrix  adding sources to the queries until there is no
improvement  how significant are the gains that we can realize when we do this  focusing
on queryconjunct  for every reviewer source  we consider queries that contained data from
a single paper source and had the lowest precision  we pair each of these queries with the
corresponding query in the same row of the matrix that made use of all of the paper sources
and report the resulting improvement in precision in table    for the top    results  we
note that in the best case  we can gain an improvement in precision of     when going from
a single source to a multi source query  and for the top    results  we gain an improvement
   all comparisons between two queries qi and qj were made using a two tailed sign test  specifically  we
consider the set rij of reviewers r for which precision qi   r     precision qj   r  and then estimate the
probability
pij   p rob precision qi   r     precision qj   r   j r   rij  
we consider a difference to be statistically significant if one can reject with confidence        the null
hypothesis that pij was generated by j rij j independent ips of a fair coin 

   

fibasu  hirsh  cohen    nevill manning

single source queries improvement after adding two sources
pt top    
   
ha top    
   
phk top    
   
pt top    
   
ha top    
   
phk top    
   

table    a comparision of single source vs  multi source queries 
methods s 

top    top   
knn
           
extendeddirectbayes            

table    average precision scores at top    and top    papers returned using collaborative filtering methods 
of      these results do support our intuitions that by incorporating more information in
our queries  the quality of the retrieval results improves  since we have a different paper
source for the single source queries in each row of table    we also note that the impact of
any given paper source is dependent on the reviewer representation that we use 
can we still come up with an assessment of which sources are significant for the conference reviewing task  for queryconjunct  we present a series of figures  figure   to figure   
that illustrate the impact of each source by plotting precision values of queries that exclude
the source along the x axis and precision values of queries that include the source along the
y axis  for both n      and n        if a point falls on the x   y line  then the queries
have exactly the same performance   the choice of source is irrelevant  all points that fall
in the area above the x   y line mark those queries that had higher precision compared to
their query counterparts which did not contain the source 
by simply counting the number of times the queries that include a source outperform
the queries that did not include the source  we have one way of ranking the sources in
decreasing order of importance  in this case  queries that include the abstract source for
papers and the home page source for reviewers have the highest rates of success  when
compared to the other information sources for papers and reviewers  respectively  
now  the natural question to ask is whether the trends that we noticed for queryconjunct
also hold for queryconcat  the answer is no  which also means that queryconcat does not
give us a definitive answer to the question of whether more information is really better 
just as we have noticed that query performance is linked to both the reviewer and paper
sources  we also find that it is linked to the query expansion method 
   

fitechnical paper recommendation  a study in combining multiple information sources

precision at top    and top   
   
a dat
x

    
    
    

queries with abstract

    
   
    
    
    
    
   
    
    
    

   

    
   
queries without abstract

    

   

figure    the role of abstract as an information source

precision at top    and top   
   
k dat
x

    
    

queries with keywords

    
    
   
    
    
    
    
   
    
    
    

   

    
   
queries without keywords

    

   

figure    the role of keywords as an information source

   

fibasu  hirsh  cohen    nevill manning

precision at top    and top   
   
t dat
x

    
    
    

queries with titles

    
   
    
    
    
    
   
    
    
    

   

    
   
queries without titles

    

   

figure    the role of title as an information source

precision at top    and top   
   
p dat
x

    
    

queries with reviewer papers

    
    
   
    
    
    
    
   
    
    
    

   

    
   
queries without reviewer papers

    

   

figure    the role of papers as an information source

   

fitechnical paper recommendation  a study in combining multiple information sources

precision at top    and top   
   
h dat
x

    

queries with reviewer homepages

    
    
    
   
    
    
    
    
   
    
    
    

   

    
   
queries without reviewer homepages

    

   

figure    the role of homepage as an information source
in table    we show the results of the collaborative filtering runs  we report averages
of the precision values computed for the top n  for n     and n      papers returned
based on the reviewer recommendation lists  since we stop recommending after we have
exhausted the set of positive examples for a reviewer  the reviewer recommendation lists are
of varying lengths  in those cases where the size of the list is less than n   we still compute
precision at top n   assuming the remaining items are incorrect predictions  both methods
for collaborative filtering exceed random selection by a significant margin 
for top    papers returned  the collaborative recommendation methods are competitive with the best performance of queryconcat  this is already an interesting observation 
since not only do the methods differ  but each method is using different data to make recommendations  we can further state than when we use queryconjunct and all information
sources to recommend    papers  on average almost four papers coincide with the reviewer s
preferences  compared to random selection  collaborative filtering  and queryconcat  this
method yields more papers of interest to reviewers 
in summary  what have we learned from our experiments  we have found that within
the context of peer reviewing of papers  we can make the recommendation process less
 people intensive   most recommendation systems require their users to provide samples
of their preferences which are then used to extrapolate future behaviors  collaborative
methods go even further by using preference information across multiple users to predict
the preferences of a single user  by automatically collecting reviewer interest information
from web sources and precomputing similarities between these profiles and paper content 
we require less input from the reviewers  furthermore  our content based retrieval methods
can exceed the performance of collaborative methods in this task 
we also believe that our recommendation framework provides an extensible way of
formulating queries that provides more control over the information content of the queries 
we can control not only how much information we include in our queries but also how we
incorporate that information  as new data become available  we can evaluate which data
   

fibasu  hirsh  cohen    nevill manning

sources and or combinations are more effective  thereby fine tuning the query formulation
process 

   related work on query reformulation

since our work on expanding queries using whirl can be viewed as a type of query
reformulation  we review some related work in the information retrieval community on this
topic  salton        describes the process of query reformulation as that of  moving  a
given query towards the relevant items and away from the nonrelevant ones  in the context
of the vector space model of retrieval  this means that given a query expression of the form
 salton        
q      q     q          qt 

where q is a number between   and   representing the weight assigned to term   we want
to arrive at a new query expression 
i

i

q      q     q          qt 
 

 

 

 

such that the weights are adjusted so that new terms can be introduced into the vector
representation  while other terms can effectively be removed by reducing their respective
weights to   
harman        describes the operational procedure underlying this process as the merging of document and query vectors  more specifically  this means that query terms not in
the original query but appearing in the relevant documents are added to the initial query
expression  the expansion occurs using both positive and negative weights  depending on
whether the terms appears in a relevant or non relevant document 
the above description assumes that we have relevance judgments for documents that
the system can return  practically speaking  this type of information is hard to come by 
therefore  people have been seeking to compensate for this lack of information by expanding
queries using a variety of techniques such as the use of thesauri and relevance feedback  in
the latter case  query reformulation is part of an iterative and interactive process whereby
users are presented with the results of a retrieval and are asked to supply feedback regarding
the relative importance of the results 
comparing our approach with these methods of query reformulation  we make a couple of
observations  first  query reformulation can be driven by knowledge we have precomputed
about a data colection  given that entities such as papers have abstracts  keywords  and
titles  does it make sense to vary the amount of this information in the queries  if we have
the equivalent of table   for a collection  we can do a table lookup at run time to determine
which formulations are the most promising 
we note that the way we construct queries for the queryconjunct method combines
aspects of both the boolean and vector space models of query formulation into a hybrid approach  in the case of boolean queries  relevance feedback can lead to new query expressions
consisting of term conjuncts such as  salton        
 term and term and term  
notice that if we replace any term with vector in the above expression  we have a
query expression formulated according to our queryconjunct method 
i

j

i

k

i

   

fitechnical paper recommendation  a study in combining multiple information sources

   conclusions

in this paper  we have shown that we can collect information about reviewers automatically
from the web  and we can use it as a part of a recommendation framework to route papers
to reviewers  we treat the problem as one of decomposing reviewer interest and paper
contents into information sources  and then of combining the information sources using
different query formulations  in our experiments  we compared two ways of formulating
queries using content based information retrieval and one collaborative approach  we have
found that the recommendation algorithm using conjunctive queries outperforms the other
approaches  we have also looked at using different subsets of the information sources in our
algorithms  and in the case of our optimal algorithm  we found that using more information
generally lead to better performance 
in a practical setting  the recommendation method of choice is likely to depend on a
number of factors ranging from the availability of information to ease of use  on the one
hand  our framework provides a more exible alternative to simple keyword based searches
and a less intrusive alternative to collaborative methods  on the other hand  our methods
assume that we can obtain data that are reliable  accurate  and timely  based on our results 
we are optimistic that the web can provide credible information sources that can be used
successfully in the recommendation process 

   acknowledgments

we extend our thanks to aaai  the aaai reviewers  the aaai paper authors  members
of the rutgers machine learning research group  and the reviewers of this paper for their
inputs in this work 
we note that the following are the property of their respective companies as listed 
whirl  at t labs   research   lsi  telcordia technologies  inc   

references

basu  c   hirsh  h     cohen  w          recommendation as classification  using social
and content based information in recommendation  in proceedings of aaai    
cohen  w       a   integration of heterogeneous databases without common domains using
queries based on textual similarity  in proceedings of acm sigmod    
cohen  w       b   the whirl approach to information integration  in ieee intelligent
systems  ieee press 
cohen  w     fan  w          web collaborative filtering  recommending music by crawling the web  in proceedings of www      
cohen  w     hirsh  h          joins that generalize  text classification using whirl  in
proceedings of kdd    
dillon  m     desper  j          automatic relevance feedback in boolean retrieval systems 
journal of documentation     
   

fibasu  hirsh  cohen    nevill manning

dumais  s     nielsen  j          automating the assignment of submitted manuscripts to
reviewers  in proceedings of acm sigir    
geller  j          challenge  how ijcai      can prove the value of ai by using ai  in
proceedings of ijcai    
gupta  d   digiovanni  m   narita  h     goldberg  k          jester      a new lineartime collaborative filtering algorithm applied to jokes  in workshop on recommender
systems at acm sigir    
harman  d          relevance feedback revisited  in proceedings of acm sigir    
hill  w   stead  l   rosenstein  m     furnas  g          recommending and evaluating
choices in a virtual community of use  in proceedings of chi    
konstan  j   miller  b   maltz  d   herlocker  l   gordon  l     riedl  j          grouplens 
applying collaborative filtering to usenet news   vol     
nevill manning  c   reed  t     witten  i          extracting text from postscript  software
practice and experience         
pazzani  m     billsus  d          learning and revising user profiles  the identification of
interesting web sites  machine learning              
porter  m          an algorithm for sux stripping  program              
salton  g          automatic text processing  addison wesley 
salton  g          improving retrieval performance by relevance feedback  in readings in
information retrieval 
shakes  j   langheinrich  m     etzioni  o          dynamic reference sifting  a case study
in the homepage domain  in proceedings of www    
shardanand  u     maes  p          social information filtering  algorithms for automating
 word of mouth   in proceedings of chi    
yarowsky  d     florian  r          taking the load off the conference chairs  towards a
digital paper routing assistant  in proceedings of the      joint sigdat conference
on empirical methods in nlp and very large corpora 

   

fi