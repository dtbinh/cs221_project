journal artificial intelligence research                

submitted       published     

speeding convergence value iteration
partially observable markov decision processes
nevin l  zhang
weihong zhang

department computer science
hong kong university science   technology
clear water bay road  kowloon  hong kong  china

lzhang cs ust hk
wzhang cs ust hk

abstract

partially observable markov decision processes  pomdps  recently become popular among many ai researchers serve natural model planning
uncertainty  value iteration well known algorithm finding optimal policies
pomdps  typically takes large number iterations converge  paper proposes
method accelerating convergence value iteration  method evaluated array benchmark problems found effective  enabled
value iteration converge iterations test problems 

   introduction
pomdps model sequential decision making problems effects actions nondeterministic state world known certainty  attracted
many researchers operations research artificial intelligence potential applications wide range areas  monahan       cassandra     b   one
planning uncertainty  unfortunately  still significant gap potential actual applications  primarily due lack effective solution methods 
reason  much recent effort devoted finding ecient algorithms pomdps
 e g   parr russell       hauskrecht     b  cassandra     a  hansen       kaelbling
et al        zhang et al        
value iteration well known algorithm pomdps  smallwood sondik      
puterman        starts initial value function iteratively performs dynamic
programming  dp  updates generate sequence value functions  sequence converges optimal value function  value iteration terminates predetermined
convergence condition met 
value iteration performs typically large number dp updates converges
dp updates notoriously expensive  paper  develop technique reducing
number dp updates 
dp update takes  the finite representation of  value function input returns  the
finite representation of  another value function  output value function closer
optimal input value function  sense  say dp update improves
input  propose approximation dp update called point based dp update  pointbased dp update improves input  possibly lesser degree standard dp
update  hand  computationally much cheaper  value iteration 
c      ai access foundation morgan kaufmann publishers  rights reserved 

fizhang   zhang
perform point based dp update number times two standard dp updates 
number standard dp updates reduced way since point based dp update
improves input  reduction come high cost since point based dp
update takes little time 
rest paper organized follows  next section shall give brief
review pomdps value iteration  basic idea behind point based dp update
explained section    theoretical preparations section    shall work
details point based dp update section    empirical results reported
section   possible variations evaluated section    finally  shall discuss related
work section   provide concluding remarks section   

   pomdps value iteration
    pomdps
partially observable markov decision process  pomdp  sequential decision model
agent acts stochastic environment partial knowledge
state environment  set possible states environment referred
state space denoted   point time  environment one
possible states  agent directly observe state  rather  receives
observation it  denote set possible observations z   receiving
observation  agent chooses action set possible actions executes
action  thereafter  agent receives immediate reward environment evolves
stochastically next state 
mathematically  pomdp specified by  three sets   z   a  reward function
r s  a   transition probability function p  s  js  a   observation probability function
p  z js    a   reward function characterizes dependency immediate reward
current state current action a  transition probability characterizes
dependency next state s  current state current action a 
observation probability characterizes dependency observation z next time
point next state s  current action a 

    policies value functions
since current observation fully reveal identity current state  agent
needs consider previous observations actions choosing action  information current state contained current observation  previous observations 
previous actions summarized probability distribution state space
 astrom        probability distribution sometimes called belief state denoted
b  possible state s  b s  probability current state s  set
possible belief states called belief space  denote b 
policy prescribes action possible belief state  words 
mapping b a  associated policy value function v   belief
state b  v  b  expected total discounted reward agent receives following
  

fispeeding value iteration pomdps
policy starting b 

 
x
r   

v  b    e b 

t  



   

rt reward received time
      discount factor  known
exists policy v  b v  b  policy belief
state b  puterman        policy called optimal policy  value function
optimal policy called optimal value function  denote v   positive
number   policy  optimal

v  b    v  b   b   b 

    value iteration

explain value iteration  need consider belief state evolves time  let b
current belief state  belief state next point time determined
current belief state  current action a  next observation z   denote baz  
state s    baz  s    given

baz  s     

ps p  z  s js  a b s 

 
   
p  z jb  a 
p
p  z  s  js  a  p  z js    a p  s  js  a  p  z jb  a   s s  p  z  s  js  a b s  renormal 

ization constant  notation suggests  constant interpreted
probability observing z taking action belief state b 
define operator takes value function v returns another value function
tv follows 
x
tv  b    max
 r b  a    p  z jb  a v  baz     b   b
   

z

p
r b  a    r s  a b s  expected immediate reward taking action belief
state b  given value function v   policy said v  improving
x p  zjb  a v  ba    b   b 
 b    arg max
 
r
 
b 

 
 

z

z

   

value iteration algorithm finding  optimal policies  starts initial
value function v  iterates using following formula 
vn   tvn    
known  e g   puterman       theorem      vn converges v n goes
infinity  value iteration terminates bellman residual maxb jvn  b    vn    b j falls
          does  vn  improving policy  optimal  e g   puterman       
since infinitely many belief states  value functions cannot explicitly represented  fortunately  value functions one encounters process value iteration
admit implicit finite representations  explaining why  first introduce several technical concepts notations 
  

fizhang   zhang

 
 
 
 

      

      

figure    illustration technical concepts 

    technical notational considerations

convenience  view functions state space vectors size jsj  use lower
case greek letters refer vectors script letters v u refer sets
vectors  contrast  upper case letters v u always refer value functions 
functions belief space b  note belief state function state space
hence viewed vector 
set v vectors induces value function follows 

f  b    max
ffb  b   b 
ff v

p

ffb inner product b  ffb  ff s b s   convenience 
shall abuse notation use v denote set vectors value function induced
set  convention  quantity f  b  written v  b  
vector set extraneous removal affect function set
induces  useful otherwise  set vectors parsimonious contains extraneous
vectors 
given set v vector v   define open witness region r ff  v   closed
witness region r ff  v   w r t v regions belief space b respectively given

r ff  v     fb   bjffb   ff  b   ff    vnfffgg
r ff  v     fb   bjffb ff  b   ff    vnfffgg
literature  belief state open witness region r ff  v   usually called witness
point since testifies fact useful  paper  shall call belief
state closed witness region r ff  v   witness point ff 
figure   diagrammatically illustrates aforementioned concepts  line
bottom depicts belief space pomdp two states  point left end
represents probability distribution concentrates masses one states 
point right end represents one concentrates masses
state  four vectors ff    ff    ff    ff    four slanting lines represent
  

fispeeding value iteration pomdps

v     

vi 

  
  
  
  
  
  
  



         

f
u

dp update v   
maxb ju  b    v  b j 
 r     v u  
g   r     
return u  

r

figure    value iteration pomdps 
linear functions ffi b  i             b  value function induced four vectors
represented three bold line segments top  vector ff  extraneous
removal affect induced function  vectors useful  first
segment line bottom witness region ff    second segment
ff    last segment ff   

    finite representation value functions value iteration

value function v represented set vectors equals value function induced
set  value function representable finite set vectors 
unique parsimonious set vectors represents function  littman et al      a  
sondik        shown value function v representable finite set
vectors  value function tv   process obtaining parsimonious
representation tv parsimonious representation v usually referred
dynamic programming  dp  update  let v parsimonious set vectors represents
v   convenience  use v denote parsimonious set vectors represents
tv  
practice  value iteration pomdps carried directly terms value
functions themselves  rather  carried terms sets vectors represent
value functions  figure     one begins initial set vectors v   iteration 
one performs dp update previous parsimonious set v vectors obtains new
parsimonious set vectors u   one continues bellman residual maxb ju  b   v  b j 
determined solving sequence linear programs  falls threshold 

   point based dp update  idea
section explains intuitions behind point based dp update  begin
so called backup operator 

    backup operator
let v set vectors b belief state  backup operator constructs new

vector three steps 

  

fizhang   zhang
   action observation z   find vector v maximum inner
product bza   belief state case z observed executing action
belief state b  one vector  break ties lexicographically
 littman        denote vector found fia z  
   action a  construct vector fia by 
x
fia  s    r s  a    p  s    zjs  a fia z  s     s    
z s 

   find vector  among fia  s  maximum inner product b 
one vector  break ties lexicographically  denote vector found
backup b  v   
shown  smallwood sondik       littman       backup b  v  
member v   set vectors obtained performing dp update v   moreover  b
witness point backup b  v   
fact corner stone several dp update algorithms  one pass
algorithm  sondik        linear support algorithm  cheng        relaxed region
algorithm  cheng       operate following way  first systematically search
witness points vectors v obtain vectors using backup operator 
witness algorithm  kaelbling et al        employs similar idea 

    point based dp update

systematically searching witness points vectors v computationally expensive  point based dp update this  instead  uses heuristics come
collection belief points backs points  might miss witness points
vectors v hence approximation standard dp update 
obviously  backing different belief states might result vector 
words  backup b  v   backup b    v   might equal two different belief states b
b   such  possible one gets vectors many backups  one issue
design point based dp update avoid this  address issue using witness
points 
point based dp update assumes one knows witness point vector
input set  backs points   rationale witness points vectors
given set  scatter belief space  hence chance creating duplicate
vectors low  experiments confirmed intuition 
assumption made point based dp update reasonable input
either output standard dp update another point based dp update  standard
dp update computes  by products  witness point output vectors 
seen later  point based dp update shares property design 

    use point based dp update

indicated introduction  propose perform point based dp update number
times two standard dp updates  specific  propose modify
   seen later  point based dp update backs points 

  

fispeeding value iteration pomdps

vi  

  
  
  
  
  
  
  

v     



         

f
u

dp update v   
maxb ju  b    v  b j 
 r     v point based vi u     
g   r     
return u  

r

u     
   f
  
v u 
  
u point based dpu v  
   g  stop u   v      false  
   return v  
point based vi 

figure    modified value iteration pomdps 
value iteration way shown figure    note change line
   instead assigning u directly v   pass subroutine point based vi
assign output subroutine v   subroutine functions way
value iteration  except performs point based dp updates rather standard dp
updates  hence call point based value iteration 
figure   illustrates basic idea behind modified value iteration contrast value
iteration  initial value function properly selected   sequence value functions produced value iteration converges monotonically optimal value function 
convergence usually takes long time partially standard dp updates  indicated
fat upward arrows  computationally expensive  modified value iteration interleaves
standard dp updates point based dp updates  indicated thin upward
arrows  point based dp update improve value function much standard dp
update  however  complexity much lower  consequence  modified value iteration
hopefully converge less time 
idea interleaving standard dp updates approximate updates back
finite number belief points due cheng         work differs cheng s
method mainly way select belief points  detailed discussion differences
given section   
modified value iteration algorithm raises three issues  first  stopping criterion
use point based value iteration  second  guarantee stopping
criterion eventually satisfied  third  guarantee convergence
modified value iteration algorithm itself  address issues  introduce concept
uniformly improvable value functions 
   show section     

  

fizhang   zhang

 
 
 

 
 
 

standard update

point based update

value iteration

modified value iteration

figure    illustration basic idea behind modified value iteration 

   uniformly improvable value functions

suppose v u two value functions  say u dominates v write v u
v  b u  b  every belief state b  value function v said uniformly improvable
v tv   set u vectors dominates another set v vectors value function induced
u dominates induced v   set vectors unformly improvable value
function induces is 

lemma   operator isotone sense two value functions v
u   v u implies tv tu    
lemma obvious well known mdp community  puterman       
nonetheless  enables us explain intuition behind term  uniformly improvable  
suppose v uniformly improvable value function suppose value iteration starts
v   sequence value functions generated monotonically increasing
converges optimal value function v   implies v tv v   is  tv  b 
closer v  b  v  b  belief states b 
following lemma used later address issues listed end
previous section 

lemma   consider two value functions v u   v uniformly improvable
v u tv   u uniformly improvable 
proof  since v u   tv tu lemma    condition u tv  
consequently  u tu   is  u uniformly improvable   
corollary   value function v uniformly improvable  tv    

   point based dp update  algorithm

point based dp update approximation standard dp update  designing
point based dp update  try strike balance quality approximation
  

fispeeding value iteration pomdps
computational complexity  need guarantee modified value iteration
algorithm converges 

    backing witness points input vectors
let v set vectors going perform point based dp update 
mentioned earlier  assume know witness point vector v   denote

witness point vector w ff   point based dp update first backs
points thereby obtains new set vectors  specific  begins
following subroutine 

v   
   u   
     v
  
backup w fi    v   
  
   u
  
w ff 
w fi   
  
u u   fffg 
   return u  
backupwitnesspoints 

subroutine  line   makes sure resulting set u contains duplicates
line   takes note fact w fi   witness point  w r t v   

    retaining uniform improvability

address convergence issues  assume input point based dp update
uniformly improvable require output uniformly improvable 
explain later assumption facilitated requirement guarantees
convergence modified value iteration algorithm  subsection  discuss
requirement fulfilled 
point based dp update constructs new vectors backing belief points
new vectors members v   hence output point based dp update trivially
dominated v   output dominates v   must uniformly improvable
lemma    question guarantee output dominates v  
consider set u resulted backupwitnesspoints  dominate v  
must exist belief state b u  b  v  b   consequently  must exist vector
v u  b  fi b  gives us following subroutine testing whether
u dominates v for  case  adding vectors u does 
subroutine called backuplppoints belief points found solving linear
programs 

u   v   
     v
  
f
backuplppoints 

  
  
  

b

b  

dominancecheck 
  null 
backup 
  



fi  u   

b  v

  

fizhang   zhang
  
  
  

w 

ff 

b 

u u   fffg 
g  b    null  

subroutine examines vectors v one one  v   calls another subroutine
try find belief point b u  b  fi b  point found 
backs it  resulting new vector  line     property backup
operator  b witness point w r t v  line     cannot vector u
equals ff   consequently  vector simply added u without checking duplicates
 line     process repeats dominancecheck returns null 
belief points b u  b  fi b  backuplppoints terminates 
u  b fi b vector v belief point b  hence u dominates v  
subroutine dominancecheck fi  u   first checks whether exists vector u
pointwise dominates   ff s fi  s  states s  exists  returns
null right away  otherwise  solves following linear program lp fi  u    returns
solution point b optimal value objective function positive returns
null otherwise  
dominancecheck

fi  u   
   variables  x  b s  state
   maximize  x 
   constraints 
ps fi s b s  x  ps ff s b b  ff u
  
ps b s       b s    states s 
  
lp 

    algorithm

complete description point based dp update  first backs witness
points input vectors  then  solves linear programs identify belief points
backs output dominates input hence uniformly improvable 
point based dpu v   
   u backupwitnesspoints v  
   backuplppoints u   v  
   return u  
terms computational complexity  point based dp update performs exactly jvj
backups first step jt vj backups second step  solves linear
programs second step  number linear programs solved upper bounded
jt vj jvj usually much smaller bound  numbers constraints
linear programs upper bounded jt vj     
   since b witness w r t v   ffb t v  b   since v uniformly improvable 
v  b v  b   together obvious fact v  b fi b condition b u  b  
ffb u  b   consequently  cannot vector u equals ff 
   actual implementation  solution point b used backup even optimal value
objective function negative  case  duplication check needed 

  

fispeeding value iteration pomdps
several algorithms standard dp update  among them  incremental
pruning algorithm  zhang liu       shown ecient
theoretically empirically  cassandra et al         empirical results  section    reveal
point based dp update much less expensive incremental pruning number
test problems  noted  however  proved always
case 

    stopping point based value iteration

consider do while loop point based vi  figure     starting initial set
vectors  generates sequence sets  initial set uniformly improvable 
value functions represented sets monotonically increasing upper bounded
optimal value function  such  converge value function  which
necessarily optimal value function   question stop do while loop 
straightforward method would compute distance maxb ju  b   v  b j
two consecutive sets u v stop distance falls threshold  compute
distance  one needs solve juj jvj linear programs  time consuming  use
metric less expensive compute  specific  stop do while loop

max
ju  w ff     v  w ff  j    
ff u
words  calculate maximum difference u v witness points
vectors u stop do while loop quantity larger    
threshold bellman residual terminating value iteration   number
     experiments  set     

    convergence modified value iteration
let vn vn  sets vectors respectively generated vi  figure    vi   figure

   line   iteration n  suppose initial set uniformly improvable  using lemma  
corollary    one prove induction vn vn  uniformly improvable
n induced value functions increase n  moreover  vn  dominates vn
dominated optimal value function  well known vn converges optimal
value function  therefore  vn  must converge optimal value function 
question make sure initial set uniformly improvable 
following lemma answers question 

lemma   let m  mins a r s  a   c   m         ffc vector whose components
c  singleton set fffc g uniformly improvable 
proof  use v denote value function induced singleton set  belief

state b 

tv  b    max
 r b  a   

  

x p  zjb  a v  ba  
z

z

fizhang   zhang
  max
 r b  a   


x p  zjb  a c 
z

  max
 r b  a    m        

  m       
  m          v  b  
therefore value function  hence singleton set  uniformly improvable   
experiments  section    shown vi  ecient vi number test
problems  noted  however  proved always case 
moreover  complexity results papadimitriou tsitsiklis        implies task
finding  optimal policies pomdps pspace complete  hence  worst case
complexity remain same 

    computing bellman residual

modified value iteration algorithm  input v standard dp update always
uniformly improvable  such  output u dominates input  fact used
simplify computation bellman residual  matter fact  bellman residual
maxb ju  b  v  b j reduces maxb  u  b  v  b   
compute latter quantity  one goes vectors u one one 
vector  one solves linear program lp ff  v    quantity simply maximum
optimal values objective functions linear programs  without uniformly
improvability  would repeat process one time roles v
u exchanged 

   empirical results discussions

experiments conducted empirically determine effectiveness point based
dp update speeding value iteration  eight problems used experiments 
literature  problems commonly referred  x co  cheese   x   part
painting  tiger  shuttle  network  aircraft id  obtained problem files
tony cassandra  information sizes summarized following table 
problem jsj jzj jaj
 x co   
    
 x    
 
 
tiger  
 
 
network  
 
 

problem jsj jzj jaj
cheese   
 
 
painting  
 
 
shuttle  
 
 
aircraft id   
 
 

effectiveness point based dp update determined comparing standard
value iteration algorithm vi modified value iteration algorithm vi   implementation standard value iteration used experiments borrowed hansen 
modified value iteration implemented top hansen s code   discount factor
set      round off precision set        experiments conducted
ultrasparc ii machine 
   implementation available request 

  

fispeeding value iteration pomdps
table   shows amounts time vi vi  took compute      optimal policies
test problems  see vi  consistently ecient vi  especially
larger problems                                times faster vi
first seven problems respectively  aircraft id problem  vi  able compute
     optimal policy less   hours  vi able produce    optimal
policy    hours 
 x co cheese  x  paint tiger shuttle network aircraft
vi
   
                                   
vi 
   
        
   
   
  
          
table    time computing      optimal policies seconds 
various statistics given table   highlight computational properties
vi  explain superior performance  numbers standard dp updates carried
vi vi  shown rows      see vi  performed  
standard updates test problems  vi performed      indicates
point based update effective cutting number standard updates
required reach convergence  consequence  vi  spent much less time vi
standard updates  row       
problem
 x co cheese  x  paint tiger shuttle network
dpu  
   
               
   
   
vi
time
    
                            
     
dpu  
 
 
 
 
 
 
 
time
   
   
   
   
   
  
  
vi  pbdpu  
   
               
   
   
time
    
         
   
   
  
   
quality ratio
   
   
   
   
   
    
   
complexity ratio
   
   
                    
    
table    detailed statistics 
row   shows numbers point based updates carried vi   see
numbers actually larger numbers standard updates performed vi 
expected  see why  recall point based update approximation standard
update  let v set vectors uniformly improvable  use   v denote
sets vectors resulted performing point based update v   belief state b 
v  b t   v  b t v  b   means point based update improves v
much standard update  consequently  use point based update increases total
   note times shown include time testing stopping condition 

  

fizhang   zhang
number iterations  i e number standard updates plus number point based
updates 
intuitively  better point based update approximation standard update 
less difference total number iterations vi  vi need take  so 
ratio two numbers problem used  certain extent 
measurement quality point based update problem  shall refer
quality ratio point based update  row   shows quality ratios seven test
problems  see quality point based update fairly good stable across
problems 
row   shows  test problem  ratio average time standard
update performed vi point based update performed vi   ratios
measure  certain extent  complexity point based update relative standard update
hence referred complexity ratios point based update  see that 
predicted analysis section      point based update consistently less expensive
standard update  differences     times last four problems 
summary  statistics suggest quality point based update relative
standard update fairly good stable complexity much lower  together
fact point based update drastically reduces number standard updates 
explain superior performance vi  
close section  let us note vi finds policies quality  close
predetermined criterion  vi  usually finds much better ones  table    
vi checks policy quality  standard  update  vi 
point based updates 

problem  x co cheese  x  paint tiger shuttle network
vi
                     
                 
vi 
                                         
table    quality policies found vi vi  

   variations point based dp update
studied several possible variations point based update  based
ideas drawn existing literature  none variations able significantly
enhance effectiveness algorithm accelerating value iteration  nonetheless brief
discussion still worthwhile  discussion provides insights
algorithm shows compares related work discussed
detail next section 
variations divide two categories  aimed improving quality
point based update aimed reducing complexity  shall discuss one
one 
   quality policy estimated using bellman residual 

  

fispeeding value iteration pomdps

    improving quality point based dp update
natural way improve quality point based update back additional
belief points  explored use randomly generated points  cassandra     a  
additional by product points  projected points  hauskrecht        additional byproduct points refer points generated various stages standard update  excluding
witness points already used  projected points points reachable
one step points given rise useful vectors 
table   shows  test problem  number standard updates amount
time vi  took without using projected points  see use
projected points reduce number standard updates one  x co  cheese 
shuttle  however  increased time complexity test problems except network 
two kinds points combinations three significantly improve
vi  either  contrary  often significantly degraded performance vi  
w o

w o


 x co cheese  x  paint tiger shuttle network aircraft
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
       
   
   
  
          
   
       
   
   
  
          

table    number standard dp updates time vi  took without
using projected points 
close examination experimental data reveals plausible explanation  point based
update  stands  already reduce number standard updates
among last two three time consuming  such  possibility
reducing number standard updates low even reduced 
effect roughly shift time consuming standard updates earlier  consequently 
unlikely achieve substantial gains  hand  use additional points
always increases overheads 

    reducing complexity point based dp update
solving linear programs expensive operation point based update  obvious
way speed avoid linear programs  point based update solves linear programs
backs belief points found guarantee uniform improvability 
linear programs skipped  must way guarantee uniform
improvability  easy solution problem  suppose v set vectors
try update uniformly improvable  let u set obtained v
backing witness points  done without solving linear programs 
set u might might uniformly improvable  however  union v   u
guaranteed uniformly improvable  therefore reprogram point based update
  

fizhang   zhang
return union hope reduce complexity  resulting variation called
non lp point based dp update 
another way reduce complexity simplify backup operator  section      using
idea behind modified policy iteration  e g   puterman        backing
set vectors v belief point  operator considers possible actions picks
one optimal according v  improving policy  speed up  one simply
use action found belief point previous standard update  resulting
operator called mpi backup operator  mpi stands modified policy
iteration  v output previous standard update  two actions often
same  however  usually different v result several point based updates
following standard update 
table   shows  test problem  number standard updates amount
time vi  took non lp point based update used  together standard
backup operator   comparing statistics point based update  tables  
    see number standard updates increased test problems
amount time increased except first three problems  plausible
reasons  first  clear non lp point based update improve set vectors
much point based update  consequently  less effective reducing number
standard updates  second  although solve linear programs  non lp point based
update produces extraneous vectors  means might need deal large
number vectors later iterations hence might ecient point based
update all 
 x co cheese  x  paint tiger shuttle network aircraft
 
 
 
 
 
 
  
 
    
        
   
   
  
          
table    number standard dp updates time vi  took non lp pointbased update used 
extraneous vectors pruned  matter fact  prune vectors
pointwise dominated others  hence extraneous  experiments  inexpensive 
pruning extraneous vectors  however  requires solution linear programs
expensive  zhang et al          discussed done
ecient way  still results good table    paper 
explored combination non lp point based update mpi backup
operator  again  results good table    reason
mpi backup operator compromises quality point based update 
quality non lp point based update improved using gauss seidel
asynchronous update  denardo        suppose updating set v   idea to 
vector created backup  add copy vector set v right away 
hope increase components later vectors  tested idea preparing
zhang et al         found costs almost always exceed benefits  reason
  

fispeeding value iteration pomdps
asynchronous update introduces many extraneous vectors synchronous
update 
conclusion  point based conceptually simple clean  compared
complex variations  seems effective accelerating value iteration 

   related work
work presented paper three levels  point based dp update bottom  pointbased value iteration middle  modified value iteration top  section 
discuss previous relevant work three levels 

    point based dp update standard dp update

mentioned section      point based update closely related several exact algorithms standard update  namely one pass  sondik        linear support  cheng       
relaxed region  cheng        backup finite number belief points 
difference exact algorithms generate points systematically 
expensive  point based update generate points heuristically 
several exact algorithms standard dp update  enumeration reduction algorithms  monahan       eagle       incremental pruning  zhang
liu       cassandra et al        first generate set vectors parsimonious prune extraneous vectors solving linear programs  point based dp
update never generates extraneous vectors  might generate duplicate vectors  however 
duplicates pruned without solving linear programs  witness algorithm  kaelbling
et al        two stages  first stage  considers actions one one 
action  constructs set vectors based finite number systematically generated
belief points using operator similar backup operator  second stage  vectors
different actions pooled together extraneous vectors pruned 
proposals carry standard update approximately dropping vectors
marginally useful  e g   kaelbling et al        hansen        one idea
along line empirically evaluated  recall achieve  optimality 
stopping threshold bellman residual             idea drop
marginally useful vectors various stages standard update keeping overall
error    stop bellman residual falls     easy see
 optimality still guaranteed way  tried start large error
tolerance hope prune vectors gradually decrease tolerance level    
reasonable improvements observed especially one need quality
policy high  however approximate updates much expensive
point based updates  context modified value iteration algorithm 
suitable alternatives standard updates point based update 

    point based value iteration value function approximation

point based value iteration starts set vectors generates sequence vector
sets repeatedly applying point based update  last set used approximate
optimal value function 
  

fizhang   zhang
various methods approximating optimal value function developed
previously   compare point based value iteration along two dimensions      whether map one set vectors another  whether
interleaved standard updates      do  whether guarantee convergence interleaved standard updates 
lovejoy        proposes approximate optimal value function v pomdp
using optimal value function underlying markov decision process  mdp  
latter function state space  v approximated one vector 
littman et al       b  extend idea approximate v using jaj vectors 
corresponds q function underlying mdp  extension recently
introduced zubek dietterich         idea base approximation
underlying mdp  rather so called even odd pomdp identical original
pomdp except state fully observable even time steps  platzman       
suggests approximating v using value functions one fixed suboptimal policies
constructed heuristically  methods start set vectors
hence map set vectors another  however  easily adapted so 
however  put predetermined limit number output vectors  consequently 
convergence guaranteed interleaved standard updates 
fast informed bound  hauskrecht     a   q function curve fitting  littman et al      b  
softmax curve fitting  parr russell       map set vectors another  however  differ drastically point based value iteration
ways deriving next set vectors current one  regardless size
current set  fast informed bound q function curve fitting always produces jaj vectors 
one action  softmax curve fitting  number vectors determined
priori  although necessarily related number actions  methods
interleaved standard dp updates  unlike point based value iteration 
may converge  hauskrecht        even cases converge themselves 
algorithms resulting interleaving standard updates necessarily
converge due priori limits number vectors 
grid based interpolation extrapolation methods  lovejoy       brafman       hauskrecht
    b  approximate value functions discretizing belief space using fixed variable
grid maintaining values grid points  values non grid points estimated interpolation extrapolation needed  methods cannot interleaved
standard dp updates work sets vectors 
grid based methods work sets vectors  lovejoy s method lower
bound optimal value function  lovejoy        instance  falls category 
method actually identical point based value iteration except way derives
next set vectors current one  instead using point based update  backs
grid points regular grid  convergence method guaranteed  algorithm
resulting interleaving standard updates may converge either 
   hauskrecht        conducted extensive survey previous value function approximation methods
empirically compared terms of  among criteria  complexity quality  would
interesting include point based value iteration empirical comparison  done
present paper focus using point based value iteration speed value iteration 
rather using value function approximation method 

  

fispeeding value iteration pomdps
incremental linear function method  hauskrecht       roughly corresponds
variation point based value iteration uses non lp point based update  section     
augmented gauss seidel asynchronous update  method access
witness point  starts  purpose backup  extreme points belief space
supplement projected points  choice points appears poor
leads large number vectors consequently backup process  usually stopped
well before  convergence  hauskrecht       

    previous work related modified value iteration
basic idea modified value iteration algorithm vi  add  two
consecutive standard updates  operations inexpensive  hope
operations significantly improve quality vector set hence reduce number
standard updates 
several previous algorithms work fashion  differences lie operations inserted standard updates  reward revision algorithm  white
et al        constructs  iteration  second pomdp based current set
vectors  runs value iteration second pomdp predetermined number steps 
output used modify current set vectors resulting set vectors
fed next standard update 
reward revision expected speed value iteration  let v value function
represented current set vectors  second pomdp constructed way
shares optimal value function original pomdp v optimal 
such  one would expect two pomdps similar optimal value functions v
close optimal  consequently  running value iteration second pomdp
improve current value function  inexpensive second
pomdp fully observable 
reward revision conceptually much complex vi  seems less
ecient  according white et al          reward revision can  average  reduce
number standard updates     computational time      tables  
   see differences vi  vi much larger 
iterative discretization procedure  idp  proposed cheng        similar
vi   two main differences  vi  uses point based update  idp uses non lp
point based update  point based update vi  backs witness points belief
points found linear programs  non lp point based update idp backs extreme
points witness regions found by products cheng s linear support relaxed region
algorithms 
cheng conducted extensive experiments determine effectiveness idp
accelerating value iteration  found idp cut number standard updates
much     amount time much      much less
significant reductions presented tables     
hansen s policy iteration  pi  algorithm maintains policy form finite state
controller  node controller represents vector  iteration  standard
update performed set vectors represented current policy  resulting
  

fizhang   zhang
set vectors used improve current policy  improved policy evaluated
solving system linear equations  gives rise third set vectors 
fed next standard update 
compared performance hansen s pi algorithm vi   table   shows 
test problem  number standard updates amount time algorithm took 
comparing statistics vi   table     see pi performed standard
updates vi   indicates policy improvement evaluation less effective
point based value iteration cutting number standard updates  terms
time  pi ecient vi  first three problems significantly less ecient
problems 
 x co cheese  x  paint tiger shuttle network aircraft
 
 
 
  
  
 
  
 
   
       
   
   
  
            
table    number standard updates time pi took compute      optimal
policies 
might possible combine vi  pi  specific  one probably
insert policy improvement evaluation step two point based updates pointbased value iteration  figure     accelerate point based value iteration
hence vi   possibility benefits yet investigated 

   conclusions future directions
value iteration popular algorithm finding  optimal policies pomdps  typically performs large number dp updates convergence dp updates
notoriously expensive  paper  developed technique called point based dp
update reducing number standard dp updates  technique conceptually
simple clean  easily incorporated existing pomdp value iteration algorithms  empirical studies shown point based dp update drastically
cut number standard dp updates hence significantly speeding value
iteration  moreover  point based dp update compares favorably complex
variations think also  compares favorably policy iteration 
algorithm presented paper still requires standard dp updates  limits
capability solving large pomdps  one future direction investigate properties
point based value iteration approximation algorithm itself  another direction
design ecient algorithms standard dp updates special models  currently
exploring latter direction 
   hansen s writings  policy improvement includes dp update substep  dp update
considered part policy improvement 

  

fispeeding value iteration pomdps

acknowledgments
research supported hong kong research grants council grant hkust       e 
authors thank tony cassandra eric hansen sharing us programs 
grateful three anonymous reviewers provided insightful comments
suggestions earlier version paper 

references

astrom  k  j          optimal control markov decision processes incomplete
state estimation  journal computer system sciences              
brafman  r  i          heuristic variable grid solution pomdps  proceedings
fourteenth national conference artificial intelligence aaai              
cassandra  a  r   littman  m  l   zhang  n  l          incremental pruning 
simple  fast  exact method partially observable markov decision processes 
proceedings thirteenth conference uncertainty artificial intelligence        
cassandra  a  r       a   exact approximate algorithms partially observable
markov decision processes  phd thesis  department computer science  brown
university 
cassandra  a  r       b   survey pomdp applications  working notes aaai
     fall symposium planning partially observable markov decision processes        
denardo  e  v          dynamic programming  models applications prentice hall 
eagle  j  n         optimal search moving target search path
constrained  operations research                   
cheng  h  t         algorithms partially observable markov decision processes  ph
thesis  university british columbia 
hansen  e  a          solving pomdps searching policy space  proceedings
fourteenth conference uncertainty artificial intelligence          
hauskrecht  m      a   incremental methods computing bounds partially observable
markov decision processes  proceedings fourteenth national conference
artificial intelligence  aaai              
hauskrecht  m      b   planning control stochastic domains imperfect information  phd thesis  department electrical engineering computer science 
massachusetts institute technology 
hauskrecht  m          value function approximations partially observable markov
decision processes  journal artificial intelligence research            
  

fizhang   zhang
littman  m  l   cassandra  a  r  kaelbling  l  p       a   ecient dynamicprogramming updates partially observable markov decision processes  technical
report cs        brown university 
littman  m  l   cassandra  a  r  kaelbling  l  p       b   learning policies partially observable environments  scaling up  proceedings fifteenth conference
machine learning          
littman  m  l          algorithms sequential decision making  ph thesis  department computer science  brown university 
kaelbling  l  p   littman  m  l  cassandra  a  r         planning acting
partially observable stochastic domains  artificial intelligence  vol     
lovejoy  w  s          computationally feasible bounds partially observed markov
decision processes  operations research              
lovejoy  w  s          suboptimal policies bounds parameter adaptive decision
processes  operations research              
monahan  g  e          survey partially observable markov decision processes  theory  models  algorithms  management science               
parr  r   russell  s          approximating optimal policies partially observable
stochastic domains  proceedings fourteenth international joint conference
artificial intelligence           
papadimitriou  c  h   tsitsiklis  j  n         complexity markov decision processes 
mathematics operations research                 
platzman  l  k         optimal infinite horizon undiscounted control finite probabilistic systems  siam journal control optimization              
puterman  m  l          markov decision processes  d  p  heyman m  j  sobel
 eds    handbooks   ms   vol              elsevier science publishers 
smallwood  r  d  sondik  e  j          optimal control partially observable
processes finite horizon  operations research                
sondik  e  j          optimal control partially observable markov processes  phd
thesis  stanford university 
sondik  e  j          optimal control partially observable markov processes
infinite horizon  operations research                
white  c  c  iii scherer  w  t          solution procedures partially observed
markov decision processes  operations research                 
zhang  n  l   lee  s  s   zhang  w         method speeding value iteration
partially observable markov decision processes  proc    th conference
uncertainties artificial intelligence 
  

fispeeding value iteration pomdps
zhang  n  l  w  liu         model approximation scheme planning stochastic
domains  journal artificial intelligence research             
zubek  v  b  dietterich  t  g         pomdp approximation algorithm anticipates need observe  appear proceedings pacific rim conference
artificial intelligence  pricai        lecture notes computer science  new
york  springer verlag 

  


