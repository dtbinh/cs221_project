journal of artificial intelligence research                 

submitted        published      

text rewriting improves semantic role labeling
kristian woodsend
mirella lapata

k woodsend ed ac uk
mlap inf ed ac uk

institute for language  cognition and computation
school of informatics  university of edinburgh
   crichton street  edinburgh eh   ab

abstract
large scale annotated corpora are a prerequisite to developing high performance nlp
systems  such corpora are expensive to produce  limited in size  often demanding linguistic
expertise  in this paper we use text rewriting as a means of increasing the amount of labeled
data available for model training  our method uses automatically extracted rewrite rules
from comparable corpora and bitexts to generate multiple versions of sentences annotated
with gold standard labels  we apply this idea to semantic role labeling and show that
a model trained on rewritten data outperforms the state of the art on the conll     
benchmark dataset 

   introduction
recent years have witnessed increased interest in the automatic identification and labeling
of the semantic roles conveyed by sentential constituents  gildea   jurafsky         the
goal of the semantic role labeling task is to discover the relations that hold between a
predicate and its arguments in a given input sentence  e g   who did what to whom 
when  where  and how  
   

 mrs  yeargin a   gave v  the questions and answers a   two days before the
examination tmp to  two low ability geography classes arg   

in sentence      a  represents the agent or giver  a  represents the theme or thing given 
a  represents the recipient  tmp is a temporal modifier indicating when the action took
place  and v determines the boundaries of the predicate  the semantic roles in the example
are labeled in the style of propbank  palmer  gildea    kingsbury         a broad coverage
human annotated corpus of semantic roles and their syntactic realizations  under the propbank annotation framework each predicate is associated with a set of core roles  named a  
a   a   and so on  whose interpretations are specific to that predicate  and a set of adjunct
roles such as location or time whose interpretation is common across predicates  e g   two
days before the examination in sentence     above  
this type of semantic information is shallow but relatively straightforward to infer automatically and useful for the development of broad coverage  domain independent language
understanding systems  indeed  the analysis produced by existing semantic role labelers has
been shown to benefit a wide spectrum of applications ranging from information extraction
 surdeanu  harabagiu  williams    aarseth        and question answering  shen   lapata 
   more precisely  a  and a  have a common interpretation across predicates as proto agent and protopatient in the sense described by dowty        
c
    
ai access foundation  all rights reserved 

fiwoodsend   lapata

source
   the retreating guerrillas were soon pursued by the government forces 
   a survey conducted by the gallup poll
last summer indicated that one in four
americans takes cues from the stars or believes in ghosts 
   the examiner who was kind let the student finish his lunch 
   because she didnt know the rules  she
died 
   mexico city  the biggest city in the world 
has many interesting archaeological sites 
   the arrival of the train was unexpected 

target
government forces soon pursued the retreating guerrillas 
a survey was conducted by the gallup
poll last summer  it indicated that one in
four americans takes cues from the stars
or believes in ghosts 
the kind examiner let the student finish
his lunch 
she died  because she didnt know the
rules 
mexico city has many interesting archaeological sites 
the trains arrival was unexpected 

table    examples of syntactic rewriting 

       to machine translation  wu   fung        and summarization  melli  wang  liu 
kashani  shi  gu  sarkar    popowich        
most srl systems to date conceptualize the semantic role labeling task as a supervised
learning problem and rely on role annotated data for model training  supervised methods
deliver reasonably good performance  with f  scores in the low eighties on standard test
collections for english  they rely primarily on syntactic features  such as path features 
in order to identify and classify roles  this has been a mixed blessing as the path from an
argument to the predicate can be very informative but also quite complicated  many paths
through the parse tree are likely to occur a relatively small number of times  or not at all 
resulting in very sparse information for the classifier to learn from  even if the training
data includes examples for a specific predicate and set of arguments  unless a test sentence
contains them in the same syntactic structure  then as far as the classifier is concerned  the
labeling of items within the two sentences is unrelated 
our idea is to use use rewrite rules in order to create several syntactic variants for
a sentence  thus alleviating the training requirements for semantic role labeling  rewrite
rules are typically synchronous grammar rules defining how a sequence of source terminals
and nonterminals rewrites to a sequence of target terminals and nonterminals  such rules
are most often extracted from monolingual corpora containing parallel translations of the
same source text  barzilay   mckeown        pang  knight    marcu         bilingual
corpora consisting of documents and their translations  bannard   callison burch      a 
callison burch         or comparable corpora such as wikipedia revision histories  coster
  kauchak        woodsend   lapata         examples of rewrites are given in table   
these include transforming passive to active sentences  see sentence pair     in table    
splitting a long and complicated sentence into several shorter ones  see     in table    
removing redundant parts of a sentence  see     in table     reordering parts in a sentence
 see     in table     deleting appositives  see     in table     transforming a prepositional
phrase into a genitive  see     in table     and so on 
   

fitext rewriting improves semantic role labeling

we automatically extract syntactic rewrite rules from corpora and use them to generate
multiple versions of role annotated sentences whilst preserving their original semantic roles 
we therefore expand the training data with a wide range of syntactic variations for each
predicate argument combination and then learn a semantic role labeler on the expanded
dataset  the approach we describe essentially increases the size of the training data by
creating many different syntactic variations for different predicates and their roles 
rewrite rules have been previously deployed in a variety of text to text generation applications ranging from summarisation  galley   mckeown        yamangil   nelken       
cohn   lapata        ganitkevitch  callison burch  napoles    van durme         to
question answering  wang  smith    mitamura         information retrieval  park  croft 
  smith         simplification  zhu  bernhard    gurevych        woodsend   lapata 
      feblowitz   kauchak         and machine translation  callison burch        marton  callison burch    resnik        ganitkevitch  cao  weese  post    callison burch 
       however  the application of text rewriting as a means of increasing the amount of
labeled data available for model training is novel to our knowledge  we show experimentally  that syntactic transformations improve srl performance beyond the state of the art
when using the conll      benchmark dataset and the best scoring system  bjorkelund 
hafdell    nugues         importantly  our approach can be used in combination with any
srl learner or role annotated data  moreover  it is not specifically tied to the srl task
or the employed learning model and dataset  rewrite rules could be used to expand the
training data for other tasks that make use of syntactic features such as semantic parsing
 kwiatkowski        and textual entailment  mehdad  negri    federico        wang  
manning        
in the following  we present an overview of related work  section    and then describe
how rewrite rules are automatically extracted and filtered for correctness  section     section   details our experiments and section   reports our results 

   related work
the idea of transforming sentences to make them more amenable to nlp technology dates
back to chandrasekar  doran  and srinivas        who argue that simpler sentences would
decrease the likelihood for an incorrect parse  to this end  they employ mostly hand crafted
syntactic rules aimed at splitting long and complicated sentences into simpler ones  klebanov  knight  and marcu        preprocess texts into easy access sentences  i e   sentences
consisting of one finite verb and its dependents in order to facilitate information seeking applications such as summarization or information retrieval in accessing factual information 
in a similar vein  vickrey and koller        devise a large number of hand written rules
in order to simplify sentences for semantic role labeling  they present a log linear model
which jointly learns to select the best simplification  out of a possibly exponential space
of candidates  and role labeling  kundu and roth        use textual transformations for
domain adaptation  rather than training a new model on out of domain data  they propose
to rewrite the out of domain text so that it is more similar to the training domain  they
pilot their idea in semantic role labeling using hand written rewrite rules and show that it
compares favorably with approaches that retrain their model on the target domain 
   

fiwoodsend   lapata

other work has focused on the idea of automatically expanding the data available for a
given task without  however  applying any transformations  furstenau and lapata       
combine labeled and unlabeled data by projecting semantic role annotations from a labeled
source sentence onto an unlabeled target sentence  they find novel instances for classifier
training based on their lexical and structural similarity to manually labeled seed instances 
zanzotto and pennacchiotti        increase the datasets for textual entailment by mining
wikipedia revision histories 
contrary to previous work  we automatically extract general rewrite rules from various data sources including wikipedia revision histories  comparable articles  and bilingual
corpora  given a sentence in the  semantic role  annotated data  we create several syntactic transformations  many of which may be erroneous  we only maintain for model
training the transformations whose role labels are preserved under a syntactic rewrite  we
identify which transformations are label preserving automatically  without requiring any
srl specific knowledge  our approach differs from that of vickrey and koller        in
three important aspects   a  we employ automatic rules which are not simplification specific   b  we do not attempt to select the best rewrite  any transformations that preserve
the gold standard role labels are used for training  c  we do not have a model that jointly
rewrites sentences and labels their semantic roles  we only rewrite the training data which
is then available to any model for the srl task  our work shares with that of kundu
and roth        the idea of transforming the sentences while preserving the gold standard
role labels  however  we do not transform the test data to make it look like the training
data  this unavoidably requires specialized knowledge of the differences between the two
domains  which our more general model does not have 
as mentioned earlier  we use synchronous grammars to extract the set of possible syntactic rewrites  synchronous context free grammars  scfgs  aho   ullman        are
a generalization of the context free grammar  cfg  formalism to simultaneously produce
strings in two languages  they have been used extensively in syntax based statistical mt
 wu        yamada   knight        chiang        graehl   knight        and related
generation tasks such as sentence compression  galley   mckeown        cohn   lapata 
            ganitkevitch et al          sentence simplification  zhu et al         feblowitz
  kauchak        woodsend   lapata         and summarization  woodsend   lapata 
       rather than focusing on one type of transformation  e g   simplification or compression   we learn the full spectrum of rewrite operations and then select rules appropriate for
the task at hand  furthermore  our results show that rewrite rules improve semantic role
labeling performance across the board  irrespectively of the specific variant of synchronous
grammar or corpus used  we experiment with conventional  weighted  scfgs  aho  
ullman        and tree substitution grammars  eisner        and employ transformation
rules extracted from wikipedia revision histories  zhu et al         woodsend   lapata 
      and bitexts  ganitkevitch  van durme    callison burch        

   method
in this section we describe the general idea behind our algorithm and then move on to
present our specific implementation  we define a transformation g to be a function that
maps an example sentence s into a modified sentence s    let g be the set of known
   

fitext rewriting improves semantic role labeling

transformation functions  g    g    g            gn    suppose now that there are labels associated
with example s  in the context of this paper  these are semantic role labels  labels could be
defined over spans of tokens  but here we use the conll       formalism where it is the
head word of the span that is labelled  the transformation function is therefore a mapping
between tokens t in sentence s to tokens t  in s    we do not require that the mapping
involves all the tokens of s or s    but we do require that the mappings are one to one 
a label preserving transformation is a transformation gi mapping from  some of the 
tokens t in example s to tokens t  in s    such that the  correct  labels of t  are identical to
the labels of its source tokens t for all the token mappings defined in gi   in other words  those
labels that could be preserved  have been preserved  and no others have been introduced 
let g be the set of label preserving transformation functions  with g  g  the problem
that we address in this paper is therefore two fold  firstly  to find automatically a set of
possible transformation functions g which due to its automated nature will unavoidably be
an error prone process  secondly  to identify  again automatically  those transformations g
which are actually label preserving  more specifically  those transformations that rewrite
a training instance s into s  through varying its syntactic structure  and yet preserve the
semantic roles of those arguments that appear in the new version s   
algorithm   describes our approach which boils down to three steps   a  extracting
transformations   b  refining transformations  and  c  generating and labeling an extended
corpus  a standard gold annotated corpus is used to train an initial semantic role labeling
model  see lines    in algorithm     meanwhile  a set of candidate transformation functions g are extracted from some suitable comparable or parallel corpus  line     this full
set of transformation functions is used to rewrite the gold corpus  creating a much extended
corpus which inevitably will contain grammatically or semantically incorrect sentences  the
extended corpus is next automatically labeled using the original srl model after preprocessing through a normal srl pipeline  whose details we discuss in section       without
knowledge of the transformation functions involved 
we could in theory use this extended corpus as the basis of training a further srl
model  however  it will contain many errors  and is unlikely to yield useful information
to guide the model  one approach could be to manually correct the rewrites that have
been generated automatically  but this would be very time and resource intensive  instead 
we do the corrections automatically  and create an extended corpus where the rewrites do
not impair the quality of the training data  we therefore learn which rules yield accurate
rewrites  i e   rewrites which preserve the labels of the gold standard  our intuition is
that  given a large number of possible rewrites  the srl model will in general label the
accurate rewrites correctly and mis label the erroneous sentences  due to it finding them
more confusing  we thus compare the semantic role labels produced by the model with the
labels for corresponding predicate argument pairs in the gold corpus  and provide them as
samples to train a binary classifier  here an svm  which learns to predict which rewrites
are likely to be successful and which are problematic  lines      in algorithm    
each rewritten sentence is classed as a positive sample if the srl model is able to label
the transformation to the same standard or better than it was able to label the original
sentence  i e  the labels that the srl model predicts for the transformed sentence match
those it predicted for the original  or have now been corrected with respect to the mapped
gold labels  if  however  a semantic role is no longer predicted correctly  or missed  or an
   

fiwoodsend   lapata

algorithm   learn srl model mextended by extending a gold training corpus cgold through
transformation functions g 
   mgold  srl model trained on cgold
   cmodel  label cgold using mgold
extract transformations 
   g  all transformation functions that can be extracted from pairs of aligned sentences
in comparable corpora
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   

refine transformations 
initialize svm training data dsvm  
for s  sentences in cgold do
for g  applicable transformations in g do
s   rewrite s using g
label s  using mgold
if srl labels of s  match labels of s in cgold or equivalent s in cmodel then
y  true
else
y  false
end if
add  s    y  to dsvm
end for
end for
train svm using dsvm
g   g  g   g has positive svm weight 
generate extended corpus 
initialize refined rewrite corpus crefined  
for s  sentences in cgold do
for g  applicable transformations in g do
s   rewrite s using g
project labels from s to s  using g
add s  to crefined
end for
end for
cextended  cgold  crefined

retrain srl model 
mextended  srl model trained on cextended
    return mextended

   

   

fitext rewriting improves semantic role labeling

erroneous role introduced  this is classified as a negative sample  as such a sample is likely
to harm the training of a new srl model 
once the svm has identified the refined set of transformation functions g  line      these
transformations are used to create an extended training corpus  this time  knowledge of
the transformation function is involved to project the labels that correspond to the original
gold corpus  lines        in the case of srl  the labels describe the predicate and its
arguments  this extended corpus supplements the original gold standard corpus  line     
and the combination is then used to create a further srl model  line     
it is worth noting that our method does not impinge on the actual process of learning an
srl model  as it is concerned with the preparation of training data  we therefore believe
it can be applied to a range of srl modeling approaches  and that gains in performance
we achieve are largely orthogonal to those that could be made by improving other aspects
of the learning process  see section     for empirical evidence  
    learning transformations
conceptually a wide range of text rewriting transformation functions could be included in
the set g  such as paraphrasing  simplification or translation into another language  here 
we focus on transformation functions that can be expressed in synchronous context free
grammars  aho   ullman         synchronous rules operate on parse tree constituents in
a context free manner  and typically modify the syntax  the transformations we consider
can be sub categorized into 
   statement extraction  constituents of a sub tree of the parse tree are identified  extracted from their context and rewritten as a complete sentence  typically shorter and
simpler  although not necessarily so 
   compression  the original sentence is rewritten by compressing constituents of the
parse tree  typically by deleting nodes 
   insertion  new elements are added to the parse tree  as significant chunks of new
text would have semantic role information of their own  in practice these insertions
are often additional punctuation to clarify the scope of phrases  or a simple structure
such as it is         to aid in statement extraction 
   substitution  through using a lexicalized synchronous grammar  text can be replaced
with new text  and paraphrases represented 
we obtain a set of possible transformations g from monolingual comparable corpora
drawn from wikipedia and bitexts  see section   for details   in the following we describe
the grammar formalisms and resources we consider 
      transformations from monolingual corpora
we extract transformation rules from corpora that are only broadly comparable  using an
unsupervised process  these corpora are constructed from wikipedia revision histories  and
comparable wikipedia articles  as a result  it cannot be guaranteed that the aligned source
and target sentences are truly related  nor can it be expected that the source sentence will
   

fiwoodsend   lapata

fully generate the target sentence  in practice this means that in addition to not requiring
a strictly synchronous structure over the source and target sentences  we cannot assume
an alignment between source and target root nodes  or require a surjective alignment of all
target nodes to nodes in the source parse tree  to be able to describe structural mismatches
and non isomorphic tree pairs  the grammar rules can comprise trees of arbitrary depth 
and fragments can be mapped  we represent transformation functions using the synchronous
tree substitution grammar formalism  eisner        
a synchronous tree substitution grammar  stsg  defines the space of valid pairs of
source and target parse trees  rules specify how to map tree fragments of the source parse
tree into fragments in the target tree  recursively and free of context  following cohn and
lapata         a stsg is a   tuple  g    ns   nt   s   t   p  rs   rt   where n are the
non terminals and  are the terminals  with the subscripts s and t indicating source and
target respectively  p are the productions and rs  ns and rt  nt are the distinguished
root symbols 
typically  each production is a rewrite rule for two aligned non terminals x  ns and
y  nt in the source and target 
hx  y i  h    i 
where  and  are elementary trees rooted with the symbols x and y respectively  while
in a synchronous context free grammar  and  would be limited to one level elementary
trees  an stsg imposes no such limits and the elementary trees can be arbitrarily deep 
a one to one alignment between the frontier nodes  non terminal leaves of the elementary
trees  in  and  is specified by  
in our experiments  we investigate two stsg variants  the strictly synchronous tree
substitution grammar t   cohn   lapata         which was originally developed for the
task of text compression  but does support a full range of transformation operations and
the quasi synchronous tree substitution grammar  qtsg  of woodsend and lapata        
which has been used in text simplification and summarization  woodsend   lapata        
in t  tokens are first aligned using a probabilistic aligner which has been initially provided
with identity mappings for the entire vocabulary  in our experiments we used the berkeley
aligner  liang  taskar    klein         however any aligner with broadly similar output
could have been used instead  synchronous rules comprising trees of arbitrary depth are
extracted from the pair of input cfg parse trees  consistent with the alignment  across
the complete corpus of aligned trees  t  filters the extracted rules to provide the maximally
general rule set  consisting of rules with the smallest depth  which are still capable of
synchronously deriving any of the original aligned tree pairs  after removing identity rules 
the resulting grammar forms the transformation functions g 
unlike t   qtsg works with only a partial alignment of tokens  based on identity 
non terminal nodes in the parse trees are then aligned to be consistent with the token
alignment  this can have the result that sections of both the source and target parse trees
remain unaligned  then  like t   synchronous rules comprising trees of minimum necessary
depth are extracted from the pair of input trees  consistent with the alignment  and as
before  identity rules are removed to form g 
as an example  figure   shows two comparable parse trees aligned at the token level 
the synchronous rules extracted from this alignment by t  and qtsg are shown in table   
   

fitext rewriting improves semantic role labeling

root
s

nnp

 

vp

np
nns

vp

vbp

s

vbn

vp
vp

to

np

vb

pp

np

np

in

nn

dt

advp

modern

scholars

come

have

some

scholars

to

question

question

the

the

existence

existence

nns
np

at

least

jj

cd

nns

the

first

nine

emperors

the

first

nine

emperors

dt

jj

cd

nns

 

 

np
pp

np
nnp

jjs

in

nn

dt

of

of

in

dt

np

vbp
vp

 
s
root

figure    example of sentence alignment showing source  above  and target  below  trees 

where it is possible to extract rules from nodes at the child level  then the rules that t 
and qtsg extract at the parent level will be identical  in cases where a sub tree has been
compressed   in the example  have come to question is compressed to question   qtsg
extracts the full sub tree until frontier nodes align  while t  will extract several rules of the
smallest depth 
      transformations from bitexts
we also obtain transformation rules from the paraphrase database  ppdb  ganitkevitch
et al          a collection of english  and spanish  paraphrases derived from large bilingual
parallel corpora  a variety of paraphrases  lexical  phrasal  and syntactic  are obtained
following bannard and callison burchs      b  bilingual pivoting method 
   

fiwoodsend   lapata

rules extracted by t 
    s np
vp
   i

hs  si



h s np

hnp  npi



h np  nnp modern  nns

hvp  vpi



h vp vbp

hvp  vpi



h vp vbn

hvp  vpi



h vp to

hvp  vpi



h vp  vb question  np

hnp  npi



h np np

hnp  npi



h np dt

hpp  ppi



h pp in

hnp  npi



h np advp

hs  si



h s np

hnp  npi



h np nnp

hvp  vpi



h vp vbp

hnp  npi



h np np

hnp  npi



h np dt

hpp  ppi



h pp in

hnp  npi



h np advp

vp

 

 

vp



vp

pp

 

np



 

 

jj

np


 

 

 

 i

 

 i

pp

 

np
cd

 

 

nn

 

 

 i

 i

 

 i

 

nns

 

 

 i

 

    np dt

 

 

 

jj

 

cd

 

nns

 

 i

 

 s  vp to



    pp in
 

 

    np  dt some  nns

jj

 

 

pp

 

    np dt

dt

 i

 

 

 i

 

    vp  vbp question  np

    np np

 

 

rules extracted by qtsg
    s np
vp
   i

 vp vbn

nn

 

 

    np dt

 

 

     vp vp

    pp in

 

pp

 

 

 

 

    np np

 

nns



 

    vp vp

dt



vp

 

 

    np  dt some  nns

    vp vp

 

 

nn

 

 

 s vp





 

 

 

nn

 

np
cd

 

 



 

 i

 vp vb

 

np

 

        vp vbp

 

np

 

 i

 i

 

 i

 i
nns

 

    np dt

 

jj

 

cd

 

nns

 

 i

table    synchronous tree grammar rules extracted by t  and qtsg from the aligned
sentences in figure    boxed indices are short hand notation for the alignment   

the intuition is that two english strings e  and e  that translate to the same foreign
string f can be assumed to have the same meaning  the method then pivots over f to
extract he    e  i as a pair of paraphrases  an example is shown in figure    taken from
ganitkevitch et al          the method extracts a wide range of possible paraphrases some
of which are unavoidably noisy due to inaccurate word alignments  paraphrases are ranked
by computing p e   e    as shown below 
x
p e   e    
p e   f  p f  e   
   
f

where p ei  f   and p f  ei   are translation probabilities estimated from the bitext  koehn 
och    marcu        
rewrite rules in ppdb are obtained using a generalization of the method sketched
above to extract syntactic paraphrases  ganitkevitch et al          using techniques from
syntactic machine translation  koehn         scfg rules are first extracted from englishforeign sentence pairs  for a foreign phrase the corresponding english phrase is found via the
   

fitext rewriting improves semantic role labeling

      farmers were thrown into jail

in ireland    

    fnf landwirte

festgenommen

  weil    

    oder wurden

festgenommen

  gefoltert    

    or have been

imprisoned

  tortured    

figure    paraphrases extracted via bilingual pivoting 
word alignments  this phrase pair is turned into a scfg rule by assigning a left hand side
nonterminal symbol  corresponding to the syntactic constituent that dominates the english
phrase  to introduce nonterminals into the right hand sides of the rule  corresponding subphrases in the english and foreign phrases are replaced with nonterminal symbols  doing
this for all sentence pairs in a bilingual parallel corpus results in a translation grammar
that serves as the basis for syntactic machine translation  a translation grammar can be
converted into a paraphrase grammar as follows  let r  and r  denote translation rules
where the left hand side nonterminals hx  y i and foreign language strings  match 
r    hx  y i  h      i

   

r    hx  y i  h      i
a paraphrase rule rp is then created by pivoting over f  
rp   hx  y i  h        i

   

although not shown in equations     and      the rules of the scfg are associated
with a set of features that are combined in a log linear model to estimate the derivation
probabilities 
      manual transformations
our experiments primarily make use of automatically learned transformations since these
can be adapted to different tasks  domains or languages  however  for the proposed approach it is not necessary that transformation functions are acquired automatically  such
functions could be also crafted by hand  we thus also investigated the effectiveness of
rewrites generated by the system of heilman and smith         henceforth h s   which
uses a sophisticated hand crafted rule based algorithm to extract simplified declarative sentences in english from syntactically complex ones  these rules are similar to those engineered by vickrey and koller        but deterministic in that they will only generate
a unique rewrite for a given sentence  the algorithm operates on the standard phrase
   

fiwoodsend   lapata

structure tree of an input sentence  it extracts new sentence trees from the input tree for
the following  non restrictive appositives and relative clauses  subordinate clauses with a
subject and finite verb  and participial phrases that modify noun phrases  verb phrases  or
clauses  in addition  the algorithm splits conjoined s  sbar  or vp nodes  and extracts new
sentence trees for each conjunct  each output tree is further processed to move any leading
prepositional phrases and quotations to be the last children of the main verb phrase  and
the following are removed  noun modifiers offset by commas  non restrictive appositives 
non restrictive relative clauses  parenthetical phrases  participial phrases   verb modifiers
offset by commas  subordinate clauses  participial phrases  prepositional phrases   leading
modifiers of the main clause  nodes that precede the subject  
table   shows examples of rules extracted using the t   qtsg and ppdb grammar
formalisms applied to a sentence from the conll dataset  the final column of table   indicates whether the transformation could be classed as statement extraction  compression 
insertion  or substitution  as reflected in the table  t  captures compression transformations by deleting nodes in the parse tree  qtsg rules are a range of mainly syntactic
transformations  and ppdb transformations are substitutions of words or short phrases 
    refining transformations
as mentioned earlier  the transformation rules obtained from our synchronous grammars
could be used to rewrite the gold standard sentences  unfortunately  due to the nature of
the corpora from which the rules are obtained and the automatic extraction process  many
of the rules will contain errors which will impair rather than improve the quality of the
training data  our idea is to extrapolate which rules to trust by observing how the srl
labeler handles the rewritten sentences  if it has mis labeled them  it is possible that the
rewrite is not correct or that the original labels have not been preserved 
each rewritten sentence is classed as a positive sample if the srl model predicts the
same labels for the transformed sentence as those it predicted for the original  or the labels
have now been corrected with respect to the gold labels  if  however  a semantic role is no
longer predicted correctly  or missed  or an erroneous role introduced  this is classified as a
negative sample  as such a sample is likely to harm the training of a new srl model  to
capture the full impact of a candidate transformation function  a sentence is provided as
a positive sample to the classifier only if all the labels  i e   all predicates and arguments 
from the source sentence have been successfully projected onto the rewrite  table   shows
examples of positive and negative samples of t   qtsg  and ppdb rewrites  note that
no refining was used on the h s outputs 
to decide which transformation function to include in the refined set  we used a linear
kernel svm  vapnik        as a binary classifier  but other classifiers or indeed suitable
statistical tests for contingency could be used  the input to the svm learner is a set
of l training samples  x    y              xl   yl    xi  rn   y           xi is an n dimensional
feature vector representing the ith sample  and yi is the label for that sample  the learning
process involves solving a convex optimization problem to find a large margin separation
hyperplane between positive and negative samples  in order to cope with inseparable data 
some misclassification is allowed  the amount of which is determined by a parameter c 
which can be thought of as a penalty for each misclassified training sample  from one
   

fitext rewriting improves semantic role labeling

grammar
original

t 

examples
bell  based in los angeles  makes and distributes electronic  computer and
building products 
bell  based  makes and distributes electronic  computer and building products 
hpp  ppi  h pp in  np      pp  i

comp

bell  based in los angeles  makes and distributes 
hnp  npi  h np adjp  nns      np  i

comp

based in los angeles  makes and distributes electronic  computer and building
products 
hnp  npi  h np nnp       np  i

comp

bell  based in angeless  makes and distributes electronic  computer and building products 
hnp  npi  h np nnp  nnp     np nnp
 pos s  i

comp

bell makes and distributes electronic  computer and building products 
hnp  npi  h np np
  vp       np np  i

comp

it makes and distributes electronic  computer and building products 
hs  si
 h s np  vp
      s  np it  vp
   i

ins

 

qtsg

 

 

 

 

bell was based in los angeles 
hnp  si
 h np np
  vp
 

 

 

     s np

 

 

 

 vp  vbd was  vp

 

    i

ext

bell  based in los  makes and distributes electronic  computer and building
products 
hnp  npi  h np nnp
nnp      np nnp  i

comp

los angeles makes and distributes electronic  computer and building products 
hnp  npi  h np np     vp vbn   pp in  np         np np  i

comp

bell  founded in los angeles  makes and distributes electronic  computer and
building products 
hvp  vpi  h vp  x based  pp     vp  x founded  pp  i

sub

bell  building in los angeles  makes and distributes electronic  computer and
building products 
hvp  vpi  h vp  x based  in
np     vp  x building  in
np  i

sub

bell  based during los angeles  makes and distributes electronic  computer
and building products 
hvp  vpi  h vp vbn
in  np     vp vbn
 x during  np
 i

sub

 

 

 

ppdb

type

 

 

 

 

 

 

 

 

 

 

 

table    examples of transformation rules extracted using t   qtsg and ppdb grammar
formalisms  applied to the sentence marked original  the final column indicates
whether the rule is statement extraction  ext   compression  comp   insertion
 ins  or substitution  sub   as before  boxed indices are short hand notation for
the alignment   

view  the dual problem   the result is a set of support vectors  the associated weights i  
and a constant b  from another view  the primal problem   the result is a vector w that
defines the separation hyperplane  with a dimension that depends on the particular kernel
   

fiwoodsend   lapata

original bell  based in los angeles  makes and distributes electronic  computer and building products 
t 

qtsg

bell  based  makes and distributes electronic  computer and building products 
bell  based in los angeles  makes and distributes 
based in los angeles  makes and distributes electronic  computer and building products 
bell  based in angeless  makes and distributes electronic  computer and building products 

 
 
 


bell makes and distributes electronic  computer and building products 
it makes and distributes electronic  computer and building products 
bell was based in los angeles 
bell  based in los  makes and distributes electronic  computer and building products 
bell  based in angeles  makes and distributes electronic  computer and building products 
los angeles makes and distributes electronic  computer and building products 

 
 
 

 


ppdb

bell  founded in los angeles  makes and distributes electronic  computer and building products  
bell  building in los angeles  makes and distributes electronic  computer and building products  
bell  based during los angeles  makes and distributes electronic  computer and building products  

h s

bell makes  bell distributes  bell is based in los angeles 

original for its employees to sign up for the options  a college also must approve the plan 
t 

for it  a college also must approve the plan 
a college also must approve the plan 
for its employees to sign up for the options  this a also must approve this the 
for its employees to sign up for  a college also must approve the plan 


 

 

qtsg

for its employees to sign up for the options  a college also must approve 
for its employees to sign up for the options  a college also must approve plan 
for its employees to sign up for all of the options  a college also must approve the plan 


 


ppdb

for
for
for
for

 

 


h s

a college must approve the plan for its employees to sign up for the options 

its
its
its
its

employees
employees
employees
employees

to
to
to
to

sign
sign
sign
sign

up
up
up
up

for
for
for
for

the
the
the
the

options 
options 
options 
options 

a
a
a
a

college
college
college
college

also
also
also
also

must adopt the plan 
must agree to the plan 
must endorse the plan 
needs to approve the plan 

original that went over the permissible line for warm and fuzzy feelings 
t 

that
that
that
that

went over the permissible line for feelings 
went over for warm and fuzzy feelings 
went over it for it 
went 

 


 

qtsg

that went over the line for warm and fuzzy feelings 
that went over the permissible line for feelings 
that went over permissible for warm and fuzzy feelings 

 



ppdb

that went over the permissible line for hot and fuzzy feelings 
that went during the permissible line for warm and fuzzy feelings 


 

h s

that went over the permissible line for warm and fuzzy feelings 

table    examples of rewrites generated by t   qtsg  and ppdb for a source sentence
 original  from the conll      training set  symbols    indicate whether the
sample was classified as positive  i e   argument label preserving  and forms part
of extended training corpus  or not 

   

fitext rewriting improves semantic role labeling

function used in the svm  in the case of the linear kernel function  wpis ndimensional  as
the feature vectors  and there is a straight forward relationship w   lj   yj j xj between
primal and dual variables  effectively assigning weights to the explicitly specified features 
other kernel functions allow for interaction between variables  for instance when using
binary valued features  a degree  polynomial kernel function implies that the classifier
considers all available pairs of features as well 
we used the identity of the transformation functions involved as the features of each
sample  so the size of the feature space n   kgk  and features were binary valued  other
features could be easily incorporated in this setting  perhaps capturing information on the
structure of the source sentence or the transformation function  and this might achieve good
results in conjunction with a polynomial kernel  but we did not pursue this avenue further 
instead we used a linear kernel  and due to the simple structure of our features  the svm
assigned a weight to each transformation function independent of the source sentence  we
chose which transformation functions should form the refined set based on whether their
corresponding weight was above a global threshold value  and we set the threshold value by
maximizing the performance of the resulting srl model on the development set 
    labeling the extended corpus
once the svm has identified the refined set of transformation functions g  these transformations are used to create an extended training corpus  using the alignment information
in the transformation functions to trace the position of tokens from the original sentence to
the rewrite  the semantic role labels of the gold corpus are projected onto the corresponding predicate argument pairs in the rewritten corpus  assuming the svm has correctly
identified the transformation function involved as indeed label preserving  and that the
transformation functions can be applied in the current context  the semantic role labeling
of the rewrite will now be of the same quality standard as the source  both conditions are
however unlikely to be true  resulting in a degradation in the quality of the rewrite corpus 
the corpus of rewrites is appended to the original gold standard corpus to create a new
larger training corpus  which is then used to create a further srl model 

   experimental setup
in this section we present our experimental setup for assessing the performance of our approach  we give details on the corpora and grammars we used to create the transformations 
and model parameters used to identify those that preserve labels  we explain how an existing srl system was modified through our approach  and how we evaluated the effects of
increasing the training data with our transformations 
    grammar extraction
we extracted synchronous grammars from two monolingual comparable corpora drawn
from wikipedia  a corpus of         aligned sentences created by pairing simple english
wikipedia with english wikipedia  kauchak         and a corpus of        paired sentences from comparing consecutive revisions of articles in simple english wikipedia  woodsend   lapata         these corpora provide a large repository of monolingual  comparable
   

fiwoodsend   lapata

grammar
t 
qtsg

aligned
      
     

revisions
     
   

table    non identical rules extracted from each wikipedia corpus  with rules appearing
only one or two times removed 

sentences  taken from real world writing  advantageously  simple english wikipedia encourages contributors to employ simpler grammar than the ordinary english wikipedia  the
corpora therefore naturally contain many examples of syntactic variation such as reordering and sentence splitting  as well as paraphrasing and changes to content  table   lists
the number of non identical rules each grammar formalism extracted from the wikipedia
corpora  once rules with an instance count of only one or two were removed 
in addition to these grammars extracted from simple english wikipedia  we worked
with the monolingual synchronous grammar included in the paraphrase database  ganitkevitch et al          where paraphrases were extracted from bilingual parallel corpora 
the english portion of ppdb contains over     million paraphrase pairs  including    
million paraphrase patterns capturing syntactic transformations with varying confidence 
to form a synchronous grammar  we used highest scoring         paraphrases from the
subset of constituent syntactic paraphrases  where all nonterminals were labeled with penn
treebank constituents  
    semantic role labeler
the method presented in this paper crucially relies on a semantic role labeler for refining
the transformations and performing the semantic analysis in general  we used the publicly available system of bjorkelund et al          out of the those that competed in the
conll      srl only challenge  it was ranked first for english language  and second overall  to the best of our knowledge  this system represents the state of the art for english
srl parsing  the system architecture consists of a four stage pipeline of classifiers  for
predicate identification  although this module is not required in evaluation   for predicate
sense disambiguation  a binary classifier for argument identification  and finally argument
classification using a multiclass classifier  beam search is used to identify the arguments of
each predicate and to label them  according to local classifiers using features which relate
mainly to dependency parse information linking predicates to potential arguments and their
siblings  in addition  a global reranker can be used to select the best combination of candidates  see section   for details   the srl system requires tokenized input with lemma 
pos tag and dependency parse information  this information was already provided in the
gold standard training corpus  see immediately below   to create equivalent information
for the transformed text and evaluation files  we used the mate tools pipeline  bjorkelund
et al          retrained  like the srl model itself  on just the training partition of the data 
we used the english language benchmark datasets from the conll      shared task to
train and evaluate the srl models  we identified and labeled semantic arguments for nouns
   

fitext rewriting improves semantic role labeling

corpus
training
  h s
  ppdb
  t 
  qtsg
  t    qtsg
  ppdb   t 
  ppdb   qtsg
  ppdb   t    qtsg
development
test in domain
test out of domain

sentences
      
      
       
       
       
       
       
       
       
     
     
   

tokens
       
       
         
         
         
          
          
          
          
      
      
     

table    statistics on corpora used to train and evaluate the srl models 
and verbs  hajic  ciaramita  johansson  kawahara  mart  marquez  meyers  nivre  pado 
stepanek  stranak  surdeanu  xue    zhang         we used the training  development 
test and out of domain test partitions as they were provided  and some statistics on these
data sets are shown in table    specifically  we show the increase on the training data
effected by our method when using transformations obtained from t   qtsg  ppdb  and
their combinations  for comparison we also use the manual transformations available from
heilman and smith         to train the srl model  and also the previous stages in the nlp
pipeline   we used data from the training partition only  while the development partition
was used to identify the best subset of g transformations   
we used liblinear  fan  chang  hsieh  wang    lin        to train the svm  and
the hyper parameters of the svm were tuned by cross validation on the training set to
maximise the area under roc curve  using the automatic grid search utility of the python
package scikit learn  pedregosa  varoquaux  gramfort  michel  thirion  grisel  blondel 
prettenhofer  weiss  dubourg  vanderplas  passos  cournapeau  brucher  perrot    duchesnay         an assessment of the cross validation accuracy  in terms of f  score and area
under roc curve  of the svm for each grammar is shown in table    the results show
that ppdb rewrites are the most accurate to employ  perhaps because the rules are the
most heavily lexicalized of all the grammars  t  grammar is the most unpredictable to use 
although the svm scores considerably higher than chance 
test sets were used solely for evaluation  making use of the indicators in the data
files as to which words were argument bearing predicates  results were generated using
the conll      evaluation script unmodified  we only report results on semantic roles
 i e   not in combination with syntactic dependencies which tends to yield higher scores 
using both the in domain and out of domain evaluation data  in the evaluation script 
semantic propositions are evaluated by converting them to semantic dependencies between
   the result of this re training was that the performance reported here is worse than for the models
available on the mate website  which have been trained on all partitions of the conll      data
 training  development and test  

   

fiwoodsend   lapata

grammar
ppdb
t 
qtsg

f 
    
    
    

area under roc
    
    
    

table    statistics on the svms performance for each grammar  obtained through crossvalidation on the training set 

a predicate and each of its arguments  and labeling the dependency with the labels of the
corresponding argument  additionally  a dependency is created from a virtual root node to
each predicate and labeled with the predicate sense  to be comparable with other published
results  in general we report the scores that combine predicate sense and argument role label
predictions  in tables        and     however  we focus on arguments only  and remove the
predicate sense scores 

   results
in this section we provide empirical evidence on the performance of our approach  our
experiments were primarily designed to answer the following questions  does text rewriting
generally improve srl performance  does it matter which transformation rules to use 
i e   are some rules better than others  are the transformation rules useful on out ofdomain data  which srl labels are mostly affected by rewriting  does performance vary
depending on the size of the original training data  are the results sensitive to the learner
being employed  we first examine the effect of different  transformation  grammars on
the srl task both on in domain and out of domain test data  and then move on to assess
which labels are mostly affected by our method  finally  we present results on the effect of
combining our approach with a global reranker and training with different sized datasets 
    transformation rules improve f  across the board
table    left half  shows srl performance  measured in terms of precision  recall  and f  
on the in domain conll      test set  for the training corpora rewritten by the h s
system  the t   qtsg  and ppdb grammars  all of the resulting srl models significantly
 p         improve over a model trained on the original corpus  we used stratified shuffling  noreen        to examine whether differences in f  were significant  pado        
recall shows the largest increase  particularly with the acquired synchronous grammars 
indicating that the increased training data is resulting in better coverage  generally this is
not at the expense of precision which in all cases apart from ppdb has increased as well 
significant gains are also seen in the acquired grammars compared to the h s system  with
the exception of t  where there is greater variation in its performance 
we also combined the rewrites produced by the different grammars  see t  qtsg 
ppdb t   ppdb qtsg and ppdb t  qtsg in table    but this did not significantly
improve performance over the individual grammars  although still significantly better than
the original model and the h s system   suggesting that the grammars are capturing very
   

fitext rewriting improves semantic role labeling

original
h s
ppdb
t 
qtsg
ppdb t 
ppdb qtsg
t  qtsg
ppdb qtsg t 
 label projection

p
     
     
     
     
     
     
     
     
     
     

in domain
r
f 
           
            
            
            
            
            
            
            
            
            

out of domain
p
r
f 
                 
                 
                  
                 
                  
                  
                  
                  
                  
                  

table    semantic evaluation results on conll      in domain and out of domain test
sets  combining predicate word sense and argument role labels   results for the
models trained on the original training set  a baseline extension to the training set 
extensions due to each grammar and all combinations   label projection  results
from training on the ppdb qtsg t  training corpus  but without rewriting
the labels using gold corpus information   difference from original is significant
at p          difference from h s is significant at p        

proportion of sentences
produced by this grammar

h s
qtsg

also produced
ppdb
   
   

by this grammar
t 
qtsg
   
    
   

table    sentence rewrite overlap     in the refined rewrite corpora produced by h s 
ppdb  t  and qtsg 

similar information  for instance  t  and qtsg are extracted from the same corpora
of aligned sentence pairs  the degree of overlap in the rewrite corpora produced by the
grammars is shown in table    although the degree of overlap in exact sentences is low 
the relative performance of the resulting models is closer  discussed below   overall  the
best performing system uses transformations obtained from qtsg and ppdb  which is not
surprising as the rules extracted from these grammars present minimal overlap 
benefits also transfer to out of domain text for the acquired grammars  improving the
overall performance even more than for the in domain data  see right half in table     the
f  score of the qtsg model is over    higher than the original model  and recall for
the model combining all the acquired grammars has increased by       meanwhile  the
rewrites of the h s system do not seem to improve coverage  resulting in a drop in recall
and f  score 
   

fiwoodsend   lapata

original
ppdb
t 
qtsg

p
     
     
     
     

in domain
r
f 
           
           
           
           

out of domain
p
r
f 
                 
                 
                 
                 

table     results on conll      in domain and out of domain test sets  training the srl
model only on rewrites that were labeled as positive 

svm
thresholds count
none

      
  
      
  
    
  
all
  
      
 
      
 

p
     
     
     
     
     
     
     

quality
r
     
     
     
     
     
     
     

f 
     
     
     
     
     
     
     

table     effect of selecting transforms by svm on the quality of the resulting model
 precision  recall and f  measures on labeling the development set  

in addition  we examined whether filtering the set of acquired transformation functions
is indeed beneficial  in the approach we have proposed  transformations are applied to the
training corpus twice  the first time as input to an svm to identify the more reliable rewrite
rules  and in a second pass the reduced set of rules is applied to the whole training corpus 
an alternative approach would be to apply the transforms only once  and then train the srl
model  we thus took the rewrites that are labeled positive in steps     of algorithm   and
corrected the labels to gold standard  step      srl models were subsequently trained using
the extended training corpus  created by concatenating the original training dataset with
these rewrites  table    shows srl performance for different grammars  ppdb  t   and
qtsg  on the test set  although precision and f  have increased over the original model 
the gains are much reduced compared to the results obtained using the svm  table     it
appears that the extra rewrites obtained by applying generally reliable transforms to the
whole training set increases coverage  and so improves the performance of the models 
table    shows how altering the quality threshold  and removing indicator features
for the number of times each transformation function was extracted  affects performance 
results are shown for the qtsg grammar on the  in domain  development set  we observed
similar patterns for all other grammars and grammar combinations   the svm quality
threshold varied from very positive  no transformations accepted  to very negative  all
   

fitext rewriting improves semantic role labeling

original
h s
ppdb
t 
qtsg
ppdb t 
ppdb qtsg
t  qtsg
ppdb qtsg t 

p
     
     
     
     
     
     
     
     
     

in domain
r
f 
           
            
            
            
            
            
            
            
            

out of domain
p
r
f 
                 
                 
                  
                  
                  
                  
                  
                  
                  

table     performance in the labeling of semantic arguments  predicate word sense information removed    difference from original is significant at p          difference
from h s is significant at p        

original
h s
ppdb
t 
qtsg
ppdb t 
ppdb qtsg
t  qtsg
ppdb qtsg t 

in domain
p
r
f 
                 
                 
                 
                 
                 
                 
                 
                 
                 

out of domain
p
r
f 
                 
                 
                 
                 
                 
                 
                 
                 
                 

table     accuracy of identification but not classification  labeling  of semantic arguments 

transformations   these findings indicate that constructing g to be transformations with
a positive svm weight  threshold of         gives better results that no transformations 
or any more permissive threshold 
    transformation rules improve semantic role assignment for verbal and
nominal predicates
the results in table   combine accuracy in predicting the sense of predicates and accuracy in
labeling their arguments  generally  the models are better at assigning the correct predicate
sense  an interesting result is that much of the gain in performance seen here by rewriting
the training corpus comes through improving semantic role assignment  it appears that
   

fiwoodsend   lapata



rammnr



ramloc





ramcau





ra 

















ra 
ra 
ca 





















































































































ca 
amtmp









amprd
ampnc

argument



ramtmp







amneg





ammod





ammnr







amloc



















































amdir















amcau



















































amdis

amadv







a 



a 

























a 





















a 



























a 







jj

nn

vbp

vbz

a 

nnp nns

vb

    

   






amext

change in f 



vbd vbg vbn

  
  

   
occurrences




  
   

     



     

predicate

figure    changes in f  score for the ppdb t  qtsg model over original  measured
by pairs of predicate pos tag and argument 

introducing syntactic variation in the training data provides the model with wider coverage
in syntactic dependency paths between predicate and arguments 
table    shows results for the same models and data sets as above  but focusing on
the argument labels only  the acquired grammars show the biggest improvements  with
over    improvement in recall in each case  and gains in f  score between      and      
the same models and data sets were used in table     with the results here for argument
identification only  not classification  unlabelled arguments   there are improvements over
   

fitext rewriting improves semantic role labeling



rammnr



ramloc





ramcau





ra 

















ra 
ra 
ca 











































































   







  







































ca 
amtmp









amprd
ampnc

argument



ramtmp







amneg





ammod





ammnr







amloc






















































amdir















amcau



















































amext
amdis

amadv







a 



a 

























a 





















a 



























a 







jj

nn

vbp

vbz

a 

nnp nns

vb



vbd vbg vbn

change in f 
    

  

   
occurrences




  
   

     



     

predicate

figure    relative performance in terms of f  score of the qtsg  red  and ppdb  blue 
models  by pairs of predicate pos tag and argument 

original in both recall and f   they are not as large as before  showing that the overall
gains are a result of improvements in both argument identification and classification 
a breakdown of the gains in f  score by predicate pos tag and argument is shown in
figure    illustrating the relative improvements of the model trained on all acquired grammars  ppdb t  qtsg  to the model trained on the original conll training data  this
further analysis reveals that most of the gain came from increased precision and recall in
predicting the core arguments  there are additional gains in the modifiers of nominal pred   

fiwoodsend   lapata

dependency path distance
proportion of test set
srl model 
original
ppdb
t 
qtsg
ppdb qtsg t 

  
     

 
     

 
    

 
    

 
    

 
    

  
    

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

     
     
     
    
     

     
    
     
     
     

table     f  scores for labeled arguments where distance between predicate and argument
is measured as the number of arcs in the dependency graph  results are from
the conll in domain test set  lower rows show the change in f  score over the
original srl model 

icates  there was some improvement and some losses in the very common core arguments
 a  and a   of the verbal predicates  but the more striking gains were seen for the other
core argument labels  this seems consistent with the models learning from wider syntactic
coverage  figure   shows a similar breakdown of the gains in f  score by predicate postag and argument  this time comparing the improvements seen from the qtsg corpus with
those resulting from ppdb  the differences are less pronounced  with ppdb improving the
core arguments more  and qtsg improving performance in labeling modifiers 
we also investigated the effect of the label projection mechanism itself  we used the
rewrites produced by all grammars  ppdb t  qtsg  to extend the training set  however  instead of using projected labels  we used the the original model mgold  trained on the
training partition of conll       to label the refined corpus  we then retrained on the
extended corpus and used this retrained model to label the test corpus  in other words  we
removed step    in algorithm    this can be considered as a form of self training  results
on both the test and out of domain sets show that using automatically generated labels
instead of projected ones seriously impairs the resulting model  with f  scores decreasing
by almost    on the test set and    on the out of domain set  see last row of table    
    transformation rules improve performance of relations involving long
dependency paths
the dependency path  the sequence of arcs through the syntactic dependency tree  between
a predicate and its argument is typically short  table    shows that in the gold labeled
test set  three quarters of the arguments are direct dependency heads or children of the
predicate  or in the case of nominal predicates  the argument is the predicate itself  existing
srl models are highly accurate over these shorter pathsthe original srl model has an
f  score of almost    but prediction accuracy drops considerably as the dependency path
grows  as can be seen in table     adding rewrites to the training set improves prediction
accuracy for almost all combinations of transformation grammar and dependency path
distance  and the largest gains are seen when the number of arcs in the dependency path
   

fitext rewriting improves semantic role labeling

original
h s
ppdb
t 
qtsg
t  qtsg
ppdb t 
ppdb qtsg
ppdb qtsg t 

p
     
     
     
     
     
     
     
     
     

in domain
r
f 
           
           
            
           
            
            
            
            
            

out of domain
p
r
f 
                 
                 
                  
                 
                  
                  
                  
                  
                  

table     results on the conll test sets for models combining extended training data and
global reranker   difference from original is significant at p          difference
from h s is significant at p        

is between three and six  improvements in f  score are observed for individual grammars
and their combination  ppdb qtsg t   
    transformation rules improve performance even when a global
reranker is used
the srl system we used  bjorkelund et al         can optionally incorporate a global
reranker  toutanova  haghighi    manning         the reranker re scores the complete
predicate argument structure  using features from all stages of the local pipeline and additional features representing the sequence of core argument labels for the current predicate 
table    presents evaluation results for a global reranker trained with the extended corpora
produced by our method  compared to the model trained on the original corpus  adding
the reranker does provide significant improvement   training on the extended data gives
further increases in performance  these are now smaller  though still significant  than was
the case in table    this indicates that the global reranker is compensating for some  but
not all  of the new information contained in the extended training data 
    transformation rules improve performance across  small and large 
datasets
we also investigated the accuracy of the labeler as a function of the size of the original
training data  for each size  subsets of the original training data were created  with replacement  and used to train the srl model  and the performance of each resulting model
measured using the development set  for each training subset  we applied algorithm    the
original srl model was trained only on the subset  we created an extended corpus from
   the scores reported here are higher than the official conll      ones  in domain p        r       
f         out of domain p        r        f         through using the mate tools nlp pipeline for
the dependency parse  rather than the dependency information provided in the test set 

   

fiwoodsend   lapata




  

  











  




  












  

  

recall

precision











rewrites



source


  



  



  

  






  

  
   

   

    

     

     



   

source sentences

   

    

     

     

source sentences

figure    srl model performance as a function of the size of the training data  with and
without additional rewrites  error bars show standard error over    experiments 

the subset using the grammar  an svm was trained each time to refine the transformations
to those that preserved labels  and the srl model retrained on the original plus refined
rewritten version of the corpus subset 
in particular  we wanted to investigate if the rewritten text provided a performance
benefit when there was only a small amount of training data  and any such benefit would be
subsumed if more labeled training data was provided  the learning curves in figure   show
the contrary  while increasing the quantity of source training data undoubtedly improves
the quality of the srl model  we found that including the rewritten training data in addition
consistently improves both precision and recall measures  the learning curves in figure  
use the qtsg grammar as the set of transformation functions  we obtained similar results
with ppdb and t   and all grammar combinations   however we omit them for the sake
of brevity 

   conclusions
in this paper we investigated the potential of text rewriting as a means of increasing the
amount of training data available for supervised nlp tasks  our method automatically
extracts rewrite rules from comparable corpora and uses them to generate multiple syntactic variants for sentences annotated with gold standard labels  application of our method
to semantic role labeling reveals that syntactic transformations improve srl performance
   

fitext rewriting improves semantic role labeling

qtsg

ppdb

hnp  npi



h np dt

hnp  npi



h np np

hnp  si



h np np

hs  si



h s even if it vbz

hadjp  adjpi



h adjp just as jj

hpp  ppi



h pp in the past month    pp in the last month i

 
 
 

jj



  np
pp

nns

    np dt

cc np



 

 



 

np

 

nns

    np np

    s it is np
 

 

 

pp

 

 

 

 i

 i

  i

    s even though it vbz

    adjp equally jj

 

 

np

 

 i

 i

table     examples of qtsg and ppdb synchronous grammar rules given high importance
during refinement  boxed indices indicate alignment   

beyond the sate of the art on the conll      benchmark dataset  specifically  we experimentally show that  a  rewrite rules  whether automatic or hand written  consistently
improve srl performance  although automatic variants tend to perform best   b  syntactic transformations improve srl performance both within  and out of domain  and  c 
improvements are observed across learners  even when using a global reranker 
in the future we would like to explore better ways of identifying the best  i e   performance enhancing  rewrite rules which may be task and grammar specific  table   
illustrates the rules deemed important  i e   given high weight  by our svm classifier for
the srl task  for instance  we could undertake more detailed feature engineering  including tree based and ngram features to capture the grammaticality of the rewritten sentences 
throughout this paper we have argued that transformation rules can be used to enhance
performance in the srl task  conversely  some of the work described here might be of
relevance to other nlp tasks employing rewriting  for example  the idea of identifying
label preserving transformations  could be used to learn which rules are meaning preserving
and consequently safe to use in tasks such simplification or sentence compression  machine
translation  textual entailment  and semantic parsing are additional application areas which
stand to benefit from more accurate rewrite rules  much of the methodology reported here
could be adapted to machine translation either for training with larger datasets  callisonburch  koehn    osborne         for domain adaptation  irvine  quirk    daume iii 
       or evaluation  kauchak   barzilay       
finally  beyond supervised srl  we would like to adapt our method to unsupervised
semantic role induction  lang   lapata        titov   klementiev         investigate alternative synchronous grammar extraction methods  e g   based on dependency information  
and obtain rewrite rules from larger comparable corpora 

acknowledgments
we are grateful to the anonymous referees whose feedback helped to substantially improve
the present paper  we acknowledge the financial support of epsrc  ep k          in
the framework of the chist era readers project 
   

fiwoodsend   lapata

references
aho  a  v     ullman  j  d          syntax directed translations and the pushdown assembler  journal of computer and system sciences              
bannard  c     callison burch  c       a   paraphrasing with bilingual parallel corpora 
in proceedings of the   rd annual meeting of the association for computational linguistics  pp          ann arbor 
bannard  c     callison burch  c       b   paraphrasing with bilingual parallel corpora 
in proceedings of the   rd acl  pp          ann arbor  mi 
barzilay  r     mckeown  k          extracting paraphrases from a parallel corpus  in
proceedings of the acl eacl  pp        toulouse  france 
bjorkelund  a   hafdell  l     nugues  p          multilingual semantic role labeling  in
proceedings of the thirteenth conference on computational natural language learning  conll        shared task  pp        boulder  colorado  software retrieved
from https   code google com p mate tools  
callison burch  c          paraphrasing and translation  ph d  thesis  university of edinburgh 
callison burch  c          syntactic constraints on paraphrases extracted from parallel
corpora  in proceedings of the      conference on empirical methods in natural
language processing  pp          honolulu  hawaii 
callison burch  c   koehn  p     osborne  m          improved statistical machine translation using paraphrases  in proceedings of the human language technology conference
of the naacl  main conference  pp        new york city  usa 
chandrasekar  r   doran  c     srinivas  b          motivations and methods for text
simplification  in proceedings of the   th international conference on computational
linguistics  pp            copenhagen  denmark 
chiang  d          hierarchical phrase based translation  computational linguistics 
               
cohn  t     lapata  m          sentence compression as tree transduction  journal of
artificial intelligence research             
cohn  t     lapata  m          an abstractive approach to sentence compression  acm
trans  intell  syst  technol                    
coster  w     kauchak  d          simple english wikipedia  a new text simplification
task  in proceedings of the   th annual meeting of the association for computational
linguistics  human language technologies  pp          portland  oregon  usa 
dowty  d          thematic proto roles and argument selection  language             
    
eisner  j          learning non isomorphic tree mappings for machine translation  in
proceedings of the acl interactive poster demonstration sessions  pp          sapporo  japan 
   

fitext rewriting improves semantic role labeling

fan  r  e   chang  k  w   hsieh  c  j   wang  x  r     lin  c  j          liblinear 
a library for large linear classification  journal of machine learning research    
         
feblowitz  d     kauchak  d          sentence simplification as tree transduction  in
proceedings of the second workshop on predicting and improving text readability
for target reader populations  pp       sofia  bulgaria 
furstenau  h     lapata  m          semi supervised semantic role labeling via structural
alignment  computational linguistics                 
galley  m     mckeown  k          lexicalized markov grammars for sentence compression  in proceedings of the naacl hlt  pp          rochester  ny 
ganitkevitch  j   callison burch  c   napoles  c     van durme  b          learning
sentential paraphrases from bilingual parallel corpora for text to text generation 
in proceedings of the      conference on empirical methods in natural language
processing  pp            edinburgh  scotland  uk 
ganitkevitch  j   cao  y   weese  j   post  m     callison burch  c          joshua     
packing  pro  and paraphrases  in proceedings of the seventh workshop on statistical
machine translation  pp          montreal  canada 
ganitkevitch  j   van durme  b     callison burch  c          ppdb  the paraphrase
database  in proceedings of the      conference of the north american chapter of
the association for computational linguistics  human language technologies  pp 
        atlanta  georgia  we used the prepackaged small constituent syntactic
subset of ppdb  retrieved from http   paraphrase org 
gildea  d     jurafsky  d          automatic labeling of semantic roles  computational
linguistics                 
graehl  j     knight  k          training tree transducers  in hlt naacl       main
proceedings  pp          boston  ma 
hajic  j   ciaramita  m   johansson  r   kawahara  d   mart  m  a   marquez  l   meyers 
a   nivre  j   pado  s   stepanek  j   stranak  p   surdeanu  m   xue  n     zhang  y 
        the conll      shared task  syntactic and semantic dependencies in multiple
languages  in proceedings of the thirteenth conference on computational natural
language learning  conll        shared task  pp       boulder  colorado 
heilman  m     smith  n          extracting simplified statements for factual question
generation  in proceedings of the  rd workshop on question generation  pp       
carnegie mellon university  pa  software available at http   www ark cs cmu edu 
mheilman questions  
irvine  a   quirk  c     daume iii  h          monolingual marginal matching for translation model adaptation  in proceedings of the      conference on empirical methods
in natural language processing  pp            seattle  washington  usa 
kauchak  d          improving text simplification language modeling using unsimplified
text data  in proceedings of the   st annual meeting of the association for computational linguistics  volume    long papers   pp            sofia  bulgaria  we used
   

fiwoodsend   lapata

the version     sentence aligned corpus  retrieved from http   www cs middlebury 
edu  dkauchak simplification  
kauchak  d     barzilay  r          paraphrasing for automatic evaluation  in proceedings
of the human language technology conference of the naacl  main conference  pp 
        new york city  usa 
klebanov  b  b   knight  k     marcu  d          text simplification for informationseeking applications  in meersman  r     tari  z   eds    on the move to meaningful
internet systems       coopis  doa  and odbase  pp          springer berlin
heidelberg 
koehn  p          statistical machine translation  cambridge university press 
koehn  p   och  f  j     marcu  d          statistical phrase based translation  in proceedings of the hlt naacl  pp        edmonton  canada 
kundu  g     roth  d          adapting text instead of the model  an open domain
approach  in proceedings of the fifteenth conference on computational natural language learning  pp          portland  oregon  usa 
kwiatkowski  t          probabilistic grammar induction from sentences and structured
meanings  ph d  thesis  university of edinburgh 
lang  j     lapata  m          unsupervised semantic role induction via split merge clustering  in proceedings of the   th annual meeting of the association for computational
linguistics  human language technologies  pp            portland  oregon  usa 
liang  p   taskar  b     klein  d          alignment by agreement  in proceedings of the
hlt naacl  pp          new york  ny 
marton  y   callison burch  c     resnik  p          improved statistical machine translation using monolingually derived paraphrases  in proceedings of the      conference
on empirical methods in natural language processing  pp          singapore 
mehdad  y   negri  m     federico  m          towards cross lingual textual entailment  in
human language technologies  the      annual conference of the north american
chapter of the association for computational linguistics  pp          los angeles 
california 
melli  g   wang  y   liu  y   kashani  m  m   shi  z   gu  b   sarkar  a     popowich  f 
        description of squash  the sfu question answering summary handler for
the duc      summarization task  in proceedings of the human language technology conference and the conference on empirical methods in natural language
processing document understanding workshop  vancouver  canada 
noreen  e          computer intensive methods for testing hypotheses  an introduction 
wiley 
pado  s          users guide to sigf  significance testing by approximate randomisation 
retrieved from http   www nlpado de  sebastian software sigf shtml 
palmer  m   gildea  d     kingsbury  p          the proposition bank  an annotated
corpus of semantic roles  computational linguistics                
   

fitext rewriting improves semantic role labeling

pang  b   knight  k     marcu  d          syntax based alignment of multiple translations 
extracting paraphrases and generating new sentences  in proceedings of the naacl 
pp          edmonton  canada 
park  j  h   croft  b     smith  d  a          a quasi synchronous dependence model for
information retrieval  in proceedings of the   th acm international conference on
information and knowledge management  pp        glasgow  united kingdom 
pedregosa  f   varoquaux  g   gramfort  a   michel  v   thirion  b   grisel  o   blondel 
m   prettenhofer  p   weiss  r   dubourg  v   vanderplas  j   passos  a   cournapeau 
d   brucher  m   perrot  m     duchesnay  e          scikit learn  machine learning
in python  journal of machine learning research               
shen  d     lapata  m          using semantic roles to improve question answering  in
proceedings of the      joint conference on empirical methods in natural language
processing and computational natural language learning  emnlp conll   pp    
    prague  czech republic 
surdeanu  m   harabagiu  s   williams  j     aarseth  p          using predicate argument
structures for information extraction  in proceedings of the annual meeting of the
association for computational linguistics  pp       sapporo  japan 
titov  i     klementiev  a          a bayesian approach to unsupervised semantic role
induction  in proceedings of the   th conference of the european chapter of the
association for computational linguistics  pp        avignon  france 
toutanova  k   haghighi  a     manning  c          joint learning improves semantic role
labeling  in proceedings of the   rd annual meeting of the association for computational linguistics  acl     pp          ann arbor  michigan 
vapnik  v          the nature of statistical learning theory  springer verlag new york 
inc 
vickrey  d     koller  d          sentence simplification for semantic role labeling  in
proceedings of acl     hlt  pp          columbus  ohio 
wang  m     manning  c          probabilistic tree edit models with structured latent
variables for textual entailment and question answering  in proceedings of the   rd
international conference on computational linguistics  coling        pp           
beijing  china 
wang  m   smith  n  a     mitamura  t          what is the jeopardy model  a quasisynchronous grammar for qa  in proceedings of the      joint conference on empirical methods in natural language processing and computational natural language
learning  emnlp conll   pp        prague  czech republic 
woodsend  k     lapata  m          learning to simplify sentences with quasi synchronous
grammar and integer programming  in proceedings of the      conference on empirical methods in natural language processing  pp          edinburgh  scotland  uk 
we used the wikipedia revisions corpus  retrieved from http   homepages inf ed 
ac uk kwoodsen wiki html 
woodsend  k     lapata  m          multiple aspect summarization using integer linear
programming  in proceedings of the      joint conference on empirical methods
   

fiwoodsend   lapata

in natural language processing and computational natural language learning  pp 
        jeju island  korea 
wu  d          stochastic inversion transduction grammars and bilingual parsing of
parallel corpora  computational linguistics                 
wu  d     fung  p          semantic roles for smt  a hybrid two pass model  in proceedings of human language technologies  the annual conference of the north american
chapter of the association for computational linguistics  companion volume  short
papers  pp        boulder  colorado 
yamada  k     knight  k          a syntax based statistical translation model  in proceedings of the   th annual meeting of the association for computational linguistics 
pp          toulouse  france 
yamangil  e     nelken  r          mining wikipedia revision histories for improving
sentence compression  in proceedings of acl     hlt  short papers  pp         
columbus  ohio 
zanzotto  f  m     pennacchiotti  m          expanding textual entailment corpora
fromwikipedia using co training  in proceedings of the  nd workshop on the peoples web meets nlp  collaboratively constructed semantic resources  pp       
beijing  china  coling      organizing committee 
zhu  z   bernhard  d     gurevych  i          a monolingual tree based translation model
for sentence simplification  in proceedings of the   rd international conference on
computational linguistics  pp            beijing  china 

   

fi