journal of artificial intelligence research                 

submitted        published      

research note
bdd ordering heuristics for classical planning
peter kissmann
jorg hoffmann

kissmann   cs   uni   saarland   de
hoffmann   cs   uni   saarland   de

saarland university  saarbrucken  germany

abstract
symbolic search using binary decision diagrams  bdds  can often save large amounts of memory due to its concise representation of state sets  a decisive factor for this methods success is the
chosen variable ordering  generally speaking  it is plausible that dependent variables should be
brought close together in order to reduce bdd sizes  in planning  variable dependencies are typically captured by means of causal graphs  and in preceding work these were taken as the basis for
finding bdd variable orderings  starting from the observation that the two concepts of dependency are actually quite different  we introduce a framework for assessing the strength of variable
ordering heuristics in sub classes of planning  it turns out that  even for extremely simple planning
tasks  causal graph based variable orders may be exponentially worse than optimal 
experimental results on a wide range of variable ordering variants corroborate our theoretical
findings  furthermore  we show that dynamic reordering is much more effective at reducing bdd
size  but it is not cost effective due to a prohibitive runtime overhead  we exhibit the potential of
middle ground techniques  running dynamic reordering until simple stopping criteria hold 

   introduction
finding good variable orderings is an important task in many areas of artificial intelligence  such as
constraint satisfaction problems  csps   sat  and planning  for some heuristic search approaches 
but especially when applying symbolic search   in many cases  an efficient ordering is determined
by evaluating a graphical representation of the underlying problem  for csps  for example  the
constraint graph can be used to determine a variable ordering for backtracking based approaches 
typical approaches take the minimum width  freuder         maximum degree  or maximum cardinality  dechter   meiri        of nodes in the constraint graph into account  an alternative
approach considers the bandwidth of the constraint graph under a given ordering  which is the maximal distance in that ordering of any two nodes that are adjacent in the graph  the idea is to find an
ordering that minimizes the bandwidth  zabih        
in sat  a widely used approach to determine the variable order in conflict driven clause learning
 cdcl  is variable state independent decaying sum  vsids   moskewicz  madigan  zhao  zhang 
  malik         this is based on the weights of propositional variables  i e   how often such a
variable occurs in the clauses  recently  rintanen        noted that for applying sat solvers to
planning tasks  a different ordering might be more efficient  giving better coverage in the typical
benchmarks of the international planning competition  ipc   this ordering takes the structure of
planning tasks into account  trying to support  sub goals as early on as possible 
in planning  variable dependencies are typically represented by the causal graph  e g   knoblock 
      jonsson   backstrom        brafman   domshlak        helmert         capturing variable
dependencies in terms of co occurences in action descriptions  this kind of graph has turned out
c
    
ai access foundation  all rights reserved 

fik issmann   h offmann

to be useful for a great variety of purposes  including problem decomposition  knoblock        
system design  williams   nayak         complexity analysis  e g  jonsson   backstrom       
domshlak   dinitz        brafman   domshlak        katz   domshlak        gimenez  
jonsson        chen   gimenez         derivation of heuristic functions  helmert              
and search topology analysis  hoffmann      b      a   for our purposes here  the causal graphs
most relevant application is the derivation of variable orderings  that has been done for bdds  to
which we return in detail below  as well as for merge and shrink heuristics  helmert  haslum   
hoffmann        helmert  haslum  hoffmann    nissim         in merge and shrink  a complete
variable ordering corresponds to a  linear  merging strategy  an order in which variables are merged
into a global abstraction  in a recent extension to non linear merging strategies  sievers  wehrle 
  helmert         the order of the merges is instead given by a tree  such a merge tree bears
some similarity to the concept of vtrees  which are used as a generalization of variable orderings for
sentential decision diagram  sdds   darwiche         fan  muller  and holte        have shown
that efficient merge trees can be determined by means of the causal graph  to do so  they use mincuts in the causal graph  putting the two resulting sets of variables into two different branches of
the merge tree and recursively continue in the subgraphs 
in this paper  we are concerned with symbolic search based on binary decision diagrams  bdds 
 bryant        for optimal planning  a variable ordering here refers to the order in which variables
are queried within the bdds  a key ingredient for the practical efficiency of the approach  in
planning  not much work has been invested into finding good variable orderings  but in model
checking  where symbolic search originated  mcmillan         many different variable ordering
schemes have been proposed in the past  e g   malik  wang  brayton    sangiovanni vincentelli 
      minato  ishiura    yajima         again  many of those are based on the evaluation of
a graphical representation of the problem  often  bringing dependent variables close together
results in smaller bdds  this can be straightforwardly applied to planning  by defining variable
dependencies via the causal graph  that is exactly how gamer  a state of the art symbolic search
planner  determines its variable ordering  kissmann   edelkamp        
the starting point of our investigation is a feeling of discomfort with the double use of the word
dependency in the above  in causal graphs  such a dependency means that the corresponding
variables appear in at least one common action  so changing the value of one variable may require
changing the other variable as well  bdds  on the other hand  represent boolean functions  
if many assignments to a subset p of all variables immediately determine the truth value of  
independently of the value of the other variables  then the variables in p should be grouped closely
together  in planning   typically represents a layer of states sharing the same distance to the initial
state  forward search  or the goal  backward search   so the concept of dependence here relates
to determining whether or not a state is a member of such a layer  what  if anything  does this have
to do with causal graph dependencies 
we do not have a conclusive answer to that question  but we contribute a number of insights
suggesting that the two concepts of dependence do not have much in common  we consider the
issue from both a theoretical and a practical perspective  on the theoretical side  we introduce a
simple formal framework for assessing the strength of variable ordering heuristics in sub classes of
planning  applying that framework to causal graph based variable orders  we show that these may
be exponentially worse than optimal orderings  even for extremely simple planning tasks 
on the practical side  we experiment with a wide range of variable ordering schemes  several
ones based on the causal graph  and also a range of techniques adapted from the model checking
   

fibdd o rdering h euristics for c lassical p lanning

literature  to get an idea of how good these ordering schemes are  on the grand scale of things 
we use an upper and a lower delimiter  for the latter  we use random variable orderings  not
too surpisingly  most ordering schemes are better than random  surprisingly  not all of them are 
indeed  fast downwards level heuristic  helmert        turns out to be much worse than the
average random bdd variable ordering 
as the upper delimiter  we employ dynamic reordering techniques that minimize bdd size
online  during the construction process  compared to static up front variable ordering schemes 
such reordering has a much better basis for taking decisions  but is much more time consuming  it
is thus expected for the bdd size results to be much better  the extent to which that happens in our
experiments is remarkable  however  static orderings are hardly ever even a tiny bit better  whereas
the advantage of dynamic reordering easily and frequently goes up to three orders of magnitude 
while it has been successfully employed in at least one domain in non deterministic planning
 cimatti  pistore  roveri    traverso         dynamic reordering usually is prohibitively slow and
not cost effective  still  its prowess at reducing bdd size  combined with our pessimistic outlook
on static ordering schemes  suggests that it may be the better alternative  an initial experiment
indicates that this could  indeed  be the case  with very simple adaptive stopping criteria  running
dynamic reordering only up to a certain point  we obtain better results than with any of the static
ordering schemes 
the remainder of the paper is organized as follows  section   gives the necessary background
on our planning framework and the use of bdds  section   introduces our theoretical framework
and investigates the properties of causal graph based ordering schemes in a range of well known
planning sub classes  section   presents our experiments regarding the quality of causal graph
based ordering schemes  and section   presents our experiments with adaptive stopping criteria for
dynamic reordering  section   concludes the paper with a brief discussion and outlook 
this research note is an extension of the authors previous short conference paper  kissmann
  hoffmann         the present paper contains comprehensive details regarding the technical
background and the variable orderings we implemented  and it includes full proofs  the experiments
with adaptive stopping criteria for dynamic reordering  section    are new 

   background
for bdd based planning  as argued e g  by edelkamp and helmert         it is important to have
a small encoding of the given planning task  so we use a finite domain variable representation as
the basis for our investigation  a finite domain representation  fdr  planning task is a tuple
   hv  a  i  gi  where v is the set of state variables and each v  v is associated with its
finite domain d v   a is a finite set of actions where each a  a is a pair hpre a   eff a i of partial
assignments to v with pre a being the precondition and eff a the effect of action a  the initial state
i is a complete assignment to v   the goal g is a partial assignment to v   by v pa   for a partial
assignment pa  we denote the variables v  v where pa v  is defined 
an action a  a is applicable in a state s iff pre a  s  for the resulting successor state s  it
holds that s   v    eff a  v  for all v  v eff a   and s   v    s v  for all v  v   v eff a    a plan
is a sequence of actions whose successive application starting in the initial state results in a state sg
with g  sg   a plan is optimal if no plan of shorter length exists 
binary decision diagrams  bdds  as introduced by bryant        represent boolean functions
  a bdd  is a directed acyclic graph with one root and two terminal nodes  the   sink and the
   

fik issmann   h offmann

x 

x 

x 

x 

x 

x 

x 

 

 

x 

x 

x 

 

 a  full obdd 

 

 b  reduced obdd 

figure    example bdds for the function      x   x     x     dashed arrows denote low edges 
solid ones high edges 
  sink  each internal node corresponds to a binary variable p and has two successors  one following
the high edge taken if p is true and one following the low edge taken if p is false  for any assignment
to all variables the sink reached corresponds to the value of the function  represented by  
as is common in practice  here we use reduced ordered bdds  an ordered bdd  obdd 
is a bdd in which the ordering of the binary variables on any path is fixed  a reduced obdd
applies two reduction rules to result in a canonical representation   i  remove any node with identical
successor along the high and the low edge   ii  merge nodes of the same variable that have the same
successor along the high edge and the same successor along the low edge  figure   illustrates
example bdds for the function      x   x     x    with the ordering hx    x    x  i  in figure  a
we have the full obdd without any reduction  when considering the nodes for x    we note that
the rightmost one can be removed due to rule  i   and the other three can be merged due to rule  ii  
applying these rules to preceding layers as well  we end up with the reduced obdd in figure  b 
we consider bdd based planning in terms of symbolic search  mcmillan        as implemented in gamer  kissmann   edelkamp         the finite domain variables v of the fdr task
are encoded by replacing each v  v with a binary counter  v  using dlog   d v  e bits  for a task
representable by n bits we need  n bdd variables in two sets  one set x representing the current
state variables  another set x  representing the successor state variables  each action a  a is represented by a transition relation bdd  ta  x  x     which captures the changes due to the application
of a but also the frame  i e   the variables that do not change 
ta  x  x      pre a  x   eff a  x     frame v   v eff a    x  x   
w
with frame v     x  x      vv   v x   v x    modeling the
w frame  it is possible to create a monolithic transition relation over all actions  i e   t  x  x      aa ta  x  x     however  this typically
is not feasible in terms of memory  thus  we store the transition relations of all actions separately
 burch  clarke    long        
in order to calculate the successors of a set of states s  represented in the current state variables 
we use the image function
image s   

 

x  s x   ta  x  x     x   x  

aa

   

fibdd o rdering h euristics for c lassical p lanning

the conjunction makes sure that only applicable actions are considered  and sets the corresponding
successor state variables  the existential quantification removes the current state variables  the
operator  x   x  stands for a swapping of the current and successor state variables  so that in the
end the successor states are again represented in the current state variables  i e   they are the new
current states  finally  the disjunction ensures that the successors based on all actions are calculated 
for the case of backward search  the pre image calculating the predecessors of a set given in the
successor variables looks similar  only that the successor state variables are quantified instead of the
current state variables 
using these two functions  symbolic breadth first search is straightforward  starting at the initial
state  or the set of goal states   iterate the image  or the pre image   until a goal  or the initial state 
is reached  storing the entire set of reached states we can ensure completeness  during search  each
layer l of states  a subset of states with identical distance to the initial state  forward search  or
the goal  backward search   is then represented by a bdd for its characteristic function 
based on a given variable ordering  the size of the bdd  i e   the number of nodes needed to
represent the corresponding function  can differ exponentially  so that finding good orderings is
crucial in practice  as the size also has an influence on the runtime  e g   the time and memory
requirements for the conjunction of two bdds is polynomial in the product of the sizes of the two
bdds   smaller size is important not only in terms of memory but also in terms of runtime  bdd
packages typically contain dynamic reordering algorithms  which can reduce the bdd sizes based
on the current situation  however  as previous work has argued  kissmann   edelkamp        
and as our experiments here reconfirm  the runtime overhead of dynamic reordering is prohibitive
in planning  the alternative is to use static variable ordering schemes instead  we define such
schemes as functions  mapping any planning task  to a non empty set    of variable orderings  i e   orderings of the planning tasks finite domain variables v   we use a set    here  as
opposed to a single ordering  because the variable ordering schemes we consider here contain ambiguity  i e   they impose only some constraints on the final variable ordering as opposed to fixing a
unique complete ordering 
before the first bdd is created  the set of possible orderings is determined in a pre processing
step  and the actual ordering hv            vn i   o     is chosen arbitrarily  i e   we do not consider
this step here   the calculated ordering is defined over the set of multi valued variables  thus  to
get the final bdd binary variable order we replace each finite domain variable vi in o with its binary
counter  vi    this means that the bdd treats these counters like inseparable fixed blocks   note
that the bits of the counters are not represented at the level of the planning tasks   so that it is
impossible for  to make an informed choice for a separation of such a block   in addition to these
blocks we store the current and successor state variables in an interleaved fashion  burch  clarke 
long  mcmillan    dill        
for any layer l and ordering o of the planning tasks finite domain variables  the ordered bdd
is unique  we denote its size  i e   the number of nodes  by bddsize o  l   by bddsize   l    
mino bddsize o  l  we denote the size of the bdd for an optimal variable ordering  finding
such an optimal ordering is np hard  bryant        
the state of the art ordering scheme in symbolic planning is based on the causal graph cg
of the planning task  knoblock        domshlak   dinitz         cg is a directed graph with
nodes v and an arc  v  v     iff v    v   and there exists an action a  a such that  v  v      v eff a   
v pre a    v eff a    in other words  we have an arc from v to v   if both appear as an effect of some
action or v appears in the precondition of an action that has v   in its effect 
   

fik issmann   h offmann

gamers scheme 
denoted ga   maps  to the set of orderings o   hv            vn i that minimize
p
the expression  vi  vj  cg  i  j     the idea is that variables vi   vj that are adjacent in the cg
are dependent and should be brought close together in the ordering by minimizing their distance
 i  j   this bears some similarity to the minimal bandwidth variable ordering in csps  zabih 
       though there the maximum of the distances is to be minimized  while we minimize the sum 
in practice  gamer approximates ga by a limited amount of local search in the space of orderings 
as finding an optimal solution is np hard  kissmann   edelkamp         for this  it starts several
searches with a random ordering  swaps two variables and checks if the sum decreased  if it did 
the search continues with the new ordering  otherwise it will stick to the old one  in the end  the
generated ordering with the smallest sum is used  the original hope was that there is a connection
between the two notions of dependency  this was supported by the fact that the new ordering
resulted in improved coverage in the used benchmark set compared to what was used before 
apart from ga   we also consider the scheme cg   which is only defined for an acyclic cg   it
maps  to the set of topological orderings of the nodes in cg   we consider this to be of theoretical
interest since it is the straightforward way to trust the causal graph completely  i e   to take the
dependencies as derived from the causal graph and order the bdd variables accordingly 

   whats in a causal graph  theory
as we have pointed out in the introduction  it is doubtful whether the concept of dependency in the
causal graph has any real relation with the concept of dependency relevant to bdd size  we now
frame this in terms of a classification of the guarantees offered  or rather  the guarantees not offered 
by ga and cg in restricted classes of planning tasks 
we first introduce our theoretical framework  then outline our results for cg and ga  
    classification framework
we classify ordering schemes  relative to a given scalable family of planning tasks  as follows 
definition    classification of ordering schemes   let f    n   be an infinite family of fdr
planning tasks parameterized by n  where the size of n is bounded by a polynomial in n  let
d   forward  backward  be a search direction  a variable ordering scheme  is 
 i  perfect in f for d if for all n  f  all d layers l in n   and all o   n    we have
bddsize o  l    bddsize   l  
 ii  safe in f for d if there exists a polynomial p s t  for all n  f  all d layers l in n   and all
o   n    we have bddsize o  l   p bddsize   l   
 iii  viable in f for d if there exists a polynomial p s t  for all n  f and all d layers l in n  
there exists o   n   with bddsize o  l   p bddsize   l   
in other words  a perfect  guarantees to deliver only optimal orderings  a safe  guarantees
at most polynomial overhead  and a viable  always delivers at least one good ordering but runs
the risk of super polynomial overhead  if  is not viable  then it actively deceives the planner  in
the sense that all variable orderings suggested are super polynomially bad in some task and layer 
note that our interpretation of viability is generous in that  while at least one good ordering
must be delivered  that ordering may differ for different search directions and layers  so that the
   

fibdd o rdering h euristics for c lassical p lanning

x

x 

x 

x 

x 

g 

x 

 a  chains

g 

g 

g 

g 

 b  forks

g chain
x 

x 

x 

x 

x 

g

g fork

g dag

g

g ifork

g

 d  relations  arrows mean  

 c  inverted forks

figure    causal graph special cases and their relation 
disambiguation over  n   is left with the job of determining which ordering actually is the good
one  one could define this notion more strictly  but as our results will be negative anyhow we stick
to this optimistic version 
we extend the classification to arbitrary sub classes c of fdr  whose sizes still are bounded
by a polynomial  by the worst case over all families f contained in c  if c contains at least one f
where  is not perfect  then  is not perfect in c  if c contains at least one f where  is not safe 
then  is not safe in c  if c contains at least one f where  is not viable  then  is not viable in c 
as we are interested in variable orderings derived from the causal graph  it is natural to consider
sub classes of fdr characterized by their causal graphs  for a set of directed graphs g  by fdr g 
we denote the class of fdr planning tasks whose causal graphs are elements of g  we investigate
some widely considered causal graph special cases  namely 
 chains  g chain    where we can find an order x            xn of the variables so that there are only
arcs from each xi to xi   for    i  n     cf  figure  a  
 forks  g fork    where we have one variable x  and a set of variables gi   with an arc from x to
each gi  cf  figure  b  
 inverted forks  g ifork    where we have a set of variables xi   and one variable g  with an arc
from each xi to g  cf  figure  c  
 directed acyclic graphs  dags  g dag   
as simple limiting cases  we also consider causal graphs without any arcs  g     as well as arbitrary
causal graphs  g     figure  d illustrates the relations between the cases considered 
bad cases are inherited in the hierarchy of figure  d  if g  g     then for any ordering scheme
the classification within fdr g     is at least as bad as that in fdr g   simply because the culprit
worst case  not perfect not safe not viable  family f of fdr planning tasks in fdr g  will be
contained in fdr g     as well 
    classification results
we start our investigation with empty causal graphs  i e   causal graphs with no arcs 
   

fik issmann   h offmann

  

  

  

  

  

  

  

  

 a  dtg for variable x 

 b  dtg for variable y 

figure    dtgs for the two variables of the planning task used in the proof of theorem   
theorem    for both search directions  any ordering scheme is safe in fdr g     ga and cg are
not perfect 
proof  if the causal graph has no arcs  then all variables move independently  i e   each action
may have only a single variable in the precondition  and the same variable in the effect  so any
forward backward layer with distance d contains exactly the states in which the sum of individual
distances  from a variables initial value to a variables goal value  equals d  for any variable v of
the task  the number of vertices  more precisely  of copies of its binary counter  v   needed is thus
bounded by the number of possible individual distance sums of the variables preceding v  hence
bdd size is polynomially bounded regardless of the variable ordering 
to see that ga and cg are not perfect  consider the following simple example  we design an
fdr task n that uses   variables x and y  each with a domain of size    represented by the values
            and     for forward search  initially x      and y      holds  for the x variable we
have an action setting it to    if it is currently     another setting it to    from     and two setting it
to    from    or     respectively  for the y variable we have an action setting it to    if it is currently
    another setting it from    to    and another setting it from    to     thus  for the values of the
x variable we have distances of          and    respectively  from the initial value of x  and for the y
variable we have distances of          and    respectively  from ys initial value  figure   illustrates
the domain transition graphs  dtgs  for variables x and y  a similar task having the same distances
to the goal values can be defined for backward search 
each variable is represented by two bdd variables  x    x  and y    y    if we keep the order
within the x and y variables fixed  we have two possible orderings  x before y or vice versa  for
a distance of   from the initial  or goal  state  we get the bdds illustrated in figure    ordering
x before y results in a slightly larger bdd  thus  ga and cg   which correspond to all possible
orderings  are not perfect  which concludes the proof 
even though the schemes ga and cg do not constrain the set of possible orderings in any way 
theorem   can be seen as a good case for the connection of causal graphs and bdd orderings 
empty causal graphs entail that any ordering is safe  the connection doesnt seem to carry any
further than this trivial case  though  in all other sub classes considered  the space of bdd orderings
contains exponentially bad ones  indeed  that is true not only for the set of all bdd orderings  but
   

fibdd o rdering h euristics for c lassical p lanning

y 

x 

x 

x 

y 

y 

x 

x 

y 

y 

x 

x 

 

 

 

 

y 

 a  x before y 

 b  y before x 

figure    bdds showing that not all orderings are perfect in the proof of theorem    solid arrows
represent high edges  dashed ones low edges 
x 

x 

y 

x 

x 

x 

x 

x 

x 

y 

y 

y 

y 

y 

x 

y 

y 

y 

y 

 

x 

 

 

 a  good variable ordering  hx    y    x    y    x    y  i

 

 b  bad variable ordering  hx    x    x    y    y    y  i

figure    bdds with different variable orderings for q     with n       x   y      x   y    
 x   y     solid arrows denote high edges  dashed ones low edges 
also for the subsets delivered by ga and cg   the classification of these schemes is very bad in
almost all considered cases  with a little bit of hope only for chain causal graphs 
our negative results employ boolean functions in quadratic form  these have the variables
 x    y            xn   yn    and take the form  x  oplow y   ophi       ophi  xn oplow yn    where either ophi 
     and oplow     or vice versa  we denote these functions by q ophi   oplow    for each of
these functions  the ordering hx    y            xn   yn i  i e   bringing pairs of xi and yi together  yields a
bdd whose size is polynomial in n  while the ordering hx            xn   y            yn i  i e   splitting the
variables in two blocks  one with all x and one with all y variables  yields a bdd of exponential
size   wegener        proves this for q     as depicted in figure    similar arguments apply for
the other quadratic forms  

   

fik issmann   h offmann

g

x 

x 

y 

y 

x 

g

x 

 

g

y 

y 

x 

x 

y 

y 

 

 

 

 a  g at the front 

g

 b  g within a pair 

figure    bdds representing g  q     with different positions of the g variable  solid arrows
represent high edges  dashed ones low edges 
theorem    for both search directions  ga and cg are not safe in fdr g ifork   
w
proof  to prove the claim for backward search  consider the function q       ni    xi  yi   
we design an fdr task n that uses  n     boolean variables   g  x    y            xn   yn   including an
additional variable g that the goal requires to be true  there are n actions achieving g  each of which
requires a pair  xi  yi   to be true as the precondition  clearly  wn  fdr g ifork    the backward
layer with a distance of   from the goal is characterized by g  ni    xi  yi   
an optimal ordering for q     consists of pairs of  xi   yi   or  yi   xi    adding the g variable 
an optimal ordering places it either at the front  as depicted in figure  a  or at the end  these cases
require exactly one node representing the g variable  placing the g variable anywhere else requires
as many nodes representing g as there are nodes  different from the   sink  reached by edges passing
through that layer  in this case  there are two g nodes if g is placed between two pairs  and up to
three nodes if it is placed between two nodes constituting a pair  see figure  b for the latter case  
any ordering following ga  n   places g in the middle and the x and y variables in an arbitrary
order around it  any ordering following cg  n   places g at the end and the x and y variables in an
arbitrary order before it  in both cases  all x variables may be placed before all y variables  resulting
in an exponential overhead which concludes the proof for backward search 
for forward search  we consider the same function q      and construct n   which has the
same variables  g  x    y            xn   yn   but where the domains of  x    y            xn   yn   are ternary 
unknown  true      or false     all x and y variables are initially unknown  and can be set to either
true or false if they are currently unknown  there are n actions achieving g  exactly as above  then
in the states with initial state distance  n     all x and w
y variables are either true or false and the
states are exactly those that satisfy g  q       g  ni    xi        yi       as the causal
   

fibdd o rdering h euristics for c lassical p lanning

d 

x 

dx 

dy 

y 

dy 

y 

dyn

yn

d 

x 

dx 

d 

dn

xn

dxn

dn  

figure    dtg for variable z used in the proof of theorem    the dashed edges correspond to
preconditions for changes in the value of the corresponding variable 
graph remains unchanged  the set of possible orderings following ga  n   and cg  n   remains
the same as in backward search as well  so that again some orders result in exponential overhead
which concludes the proof for forward search 
note that  while the proof construction shows that some orders possible in ga and cg are
super polynomially bad  other possible orders are good  hence  while as claimed we prove that ga
and cg are not safe in fdr g ifork    it might be the case that ga and cg are viable in fdr g ifork   
we leave this as an open question 
theorem    for both search directions  ga and cg are not safe in fdr g fork   
v
proof  for both search directions  we use the same function q       ni    xi yi    and the same
fdr task n with boolean variables  x    y            xn   yn   plus an additional variable z with domain
 d    dx    dy    d    dx    dy            dn   dxn   dyn   dn      the actions are such that  for    i  n 
z can move from di to either dxi or dyi   and from each of these to di    see figure     an action
preconditioned on dxi achieves xi   an action preconditioned on dyi achieves yi   initially  z   d 
and all xi   yi are false  the goal requires that z   dn   and all xi   yi are true  in forward search  the
states with initial state distance  n are exactly those where z   dn   and q     is true  and in
backward search the states with goal state distance  n are exactly those where z   d  and q    
is true 
any ordering following ga  n   places z in the middle and the x and y variables arbitrarily
around it  any ordering following cg  n   places z at the beginning and the x and y variables
arbitrarily after it  thus  there is no constraint on the variables  x    y            xn   yn    so that placing
   

fik issmann   h offmann

x 

x 

y 

x 

y 

y 

g

figure    causal graph for the planning task used in the proof of theorem   
all x variables before all y variables is an ordering compatible with both schemes  and results in
exponential overhead 
again  the proof shows that ga and cg are not safe  but makes no statement regarding viability 
note also that the task in the proof construction is unsolvable  it is easy to modify the task to be
solvable without breaking the proof argument for the forward search direction  we did not investigate whether the same is true of the backward search direction as well  in practice  while proving
unsolvability has not traditionally been a popular objective in planning  state space exhaustion is
one of the traditional purposes bdds are deemed to be good for 
for dag causal graphs  we prove that there are cases where all orderings admitted by ga and
cg
 are super polynomially bad 
theorem    for both search directions  ga and cg are not viable in fdr g dag   
proof  for the backward search claim  we use the combination of a chain causal graph and an
inverted fork as illustrated in figure    we design an fdr task n that uses  n     boolean
variables   g  x    y            xn   yn    including a variable g that the goal requires to be true  there are
n actions achieving g  each of which requires a pair  xi  yi   to be true as the precondition  this
part of the task is the same as in the proof of theorem     we add actions ensuring that in our two
schemes all x variables will be placed before all y variables  or vice versa   one action has an empty
precondition and sets x  to true in its effect  another one requires xn to be true in the precondition
and sets y  to true in its effect  the rest have xi   or yi    in the precondition and set xi  or yi   to
true in the effect  all states with a goal distance of   are thus characterized by g  q     
any order induced by ga places g in the middle  and either places all x variables in increasing
order before g and all y variables in increasing order after g  or places all y variables in decreasing
order before g and all x variables in decreasing order after g  cg induces an order starting with
all x variables in increasing order  followed by all y variables in increasing order  followed by g 
thus  in all cases  the x variables are placed separately from the y variables  resulting in exponential
overhead which proves the claim for the backward search direction 
for forward search we use the same approach as in the proof of theorem    namely to extend
the domain of all x and y variables to  true      false     unknown   all x and y variables are
initialized to the value unknown  there are n actions setting g to true  all requiring a pair of  xi yi  
to be true  the additional actions are as follows  two require x  to be unknown and set it to true
or false  respectively  two require xn to be true and y  to be unknown and set y  to true or false 
respectively  two require xn to be false and y  to be unknown and set y  to true or false  respectively 
in the same manner we have four actions for each xi and yi     i  n   requiring xi   yi    to be
true respectively false  and requiring xi  yi   to be unknown  and setting xi  yi   to true respectively
false  thus  all states
w with an initial state distance of  n     can be characterized by the function
g  q       g  ni    xi        yi       the variable orders induced by ga and cg are the
same as in backward search  resulting in exponential overhead  concluding the proof 
   

fibdd o rdering h euristics for c lassical p lanning

x 

y 

y 

x 

y 

x 

figure    causal graph for the planning task used in the proof of theorem   
x 

x 

y 

y 

x 

x 

y 

y 

y 

y 

y 

x 

x 

x 

x 

y 

x 

y 

y 

y 

y 

 

 

 

 a  optimal ordering

x 

y 

y 

y 

y 

y 

y 

y 

y 

 

 b  exponential ordering

w
l
figure     bdds representing  i   yi   i    xi  yi    as used in the proof of theorem    solid
arrows represent high edges  dashed ones low edges 
from this we immediately get  recall that cg is defined only for acyclic causal graphs  
corollary    ga is not viable in fdr g    
we close our investigation with a somewhat positive result for chain causal graphs 
theorem    for both search directions  ga and cg are not perfect in fdr g chain    there exists
an ordering scheme that is not viable in fdr g chain   
proof  the first part of the claim is inherited from fdr g     i e   as a corollary of theorem   
for the second part of the claim  existence of a non viablew
ordering scheme  we consider first
the backward search direction  using the function q       ni    xi  yi    we design an fdr
task n that uses  n boolean variables   x    y            xn   yn    the goal requires all y variables to be
false  we have an action without precondition to set x  to true  actions with preconditions requiring
yi  to be false setting xi to true  and actions preconditioned on xi being true setting yi to false 
the causal graph is depicted in figure    clearly  n  fdr g chain   
the states with distance   from the goal are the ones where all except onelyi are false  and for
n
the
single
true
ln
wn yi we have xi true as well  this is characterized by the formula i   yi  q      
i   yi  i    xi  yi    it is easy to see that the exclusive or part of this formula does not change
the relevant properties of bdds for the quadratic form  i e   we still have orderings with polynomial
and other orderings with exponential number of nodes  e g   those placing all x variables before all y
   

fik issmann   h offmann

safe 
g chain
trivially
safe
g

not
viable
g dag

not safe
g fork

not
viable
g

not safe
g ifork

figure     overview of our classification results  these hold for each of ga and cg   and for each search
direction 

variables  see figure    for illustration   any ordering scheme including only such latter orderings
is not viable 
for the forward search direction case  we construct a planning task where all x and y variables
are ternary  unknown  true      false      and are unknown initially  the value of x  can be set
freely  yi can be set to true or false if xi is true  and can only be set to true if xi is false  xi  
can be set freely once yi has been set
v to either true or false  in  n steps  we can reach exactly the
states characterized by q       ni    xi        yi       a bdd representing q     is of
exponential size if  e g   all x variables are placed before all y variables  and any ordering scheme
including only such orderings is not viable 
note that  for both planning task families  n   just described  both ga and cg and force the
xi and yi variable to be ordered in pairs  resulting in bdds of minimal size  see figure   a   in that
sense  these two planning task families constitute our only truly positive result  within them  the
ordering information in the causal graph keeps us from making exponentially bad mistakes  that
positive message would be much stronger if ga and cg were safe for all families of tasks with
chain causal graphs  it remains an open question whether that is so 
figure    gives an overview of our results  the evidence speaks rather clearly against a strong
connection between causal graph dependencies and dependencies as relevant for bdd size  note
that the causal graph underlying theorem    non viability for fdr g dag    has a very simple
form combining a chain with an inverted fork  and that theorem    non safety for fdr g ifork   
relies on planning tasks which fall into a known syntactically identified tractable class for optimal
planning  katz   domshlak         note also that being not safe already is quite bad in practice 
incurring an exponential risk unless we have a clever way of choosing an ordering within   
 which  at the moment  we do not have  

   whats in a causal graph  practice
while we have shown poor worst case performance of causal graph based variable ordering schemes
in theory  practice might be another matter  to assess the latter  we implemented a comprehensive
set of causal graph based variable ordering schemes  comprising    such schemes in total  and ran
them in comparison to practical good bad delimiters  as the bad delimiter  we used random
orderings  as the good delimiter  we used the off the shelf dynamic reordering algorithm of
gamers bdd package cudd  which is based on sifting  rudell        
a few words are in order regarding how sifting works  the variable with the greatest number
of nodes in the current bdd is chosen  it is first moved towards the end of the ordering  then
   

fibdd o rdering h euristics for c lassical p lanning

towards the beginning of the ordering  by iteratively swapping its position with the next variable in
the corresponding direction  once all positions have been tried  the variable is moved to the position
where the bdd size was smallest  this done  the next variable is chosen  until all variables have
been processed  for better comparability with our ordering schemes  we restrict the algorithm to
keep the variables representing each  v  together 
as previously indicated  dynamic reordering consumes too much runtime to be cost effective 
in the present experiments  where we are interested only in bdd size  we give dynamic reordering ample runtime  in section    we will identify simple adaptive criteria for stopping dynamic
reordering automatically during the search  taking advantage of its size reduction capacity without
suffering too much from its runtime consumption 
we ran the benchmarks of the      international planning competition  ipc     and we used
gamer as the base implementation for all planners  running them on one core of an intel xeon
x     cpu with      ghz  unless otherwise stated  we used the ipc   settings  namely a timeout
of    minutes and a memory limit of   gb 
    ordering schemes
we ran six schemes based directly on the causal graph 
gamer is gamers original ordering scheme  which approximates ga  
gamerpre is like gamer but on a causal graph extended by arcs between pairs of precondition
variables  the idea here is to capture not only dependency for forward search  but also for
backward search  i e   when inverting the actions 
wgamer is like gamer but with arcs weighted by the number of relevant actions  i e   the number
of actions inducing the corresponding arcs 
wgamerpre is like gamerpre with weighted arcs 
cglevel is fast downwards  helmert        level heuristic  which approximates cg   it orders
the variables by strongly connected components and  within these components  considers the
weighted causal graph and orders variables with smallest incoming weight first  similar to
wgamer  the weights correspond to the number of actions that induce an arc 
cgsons is another approximation of cg   it always selects a variable v all of whose parents have already been selected  or at least one of whose parents has already been selected  or an arbitrary
variable if no such v exists 
additionally  we used six ordering schemes we adopted from the model checking literature 
based on a structure called the abstract syntax tree  ast   e g   maisonneuve         that is a
directed graph containing a root node for the overall task and subtrees for all actions  each subtree
consists of nodes representing the subformulas of the specified action  i e   subformulas for the
actions precondition and its effect   the variables of the task are the leaves of the ast  the leaves
are merged  i e   we have only one node for each variable of the task  edges point from a node
representing some function to all corresponding subtrees 
we construct the ast based on the pddl input  consider the following example actions  similar to those in the floortile domain  we have predicates at r  t   denoting the tile t robot r is
   

fik issmann   h offmann

a

a 

a 







at r    t   



painted  t   

at r    t   

figure     example ast 
currently on and painted  t  denoting whether tile t has already been painted  we have two actions
a    paint r    t    t    with precondition  at r    t     painted  t     and effect  painted  t     denoting that a robot r  can paint tile t  if it currently is on t  and t  has not been painted  similarly 
action a    paint r    t    t    with precondition  at r    t   painted  t     and effect  painted  t    
denotes that robot r  can paint tile t  if it currently is on t  and t  has not been painted 
figure    illustrates the corresponding ast  we have the root for all actions a  and one subtree
for each of the two actions a  and a    for both actions the preconditions and effects are encoded
but we retain only one copy of each variable in the leaves  here relevant only for painted  t     
using their first authors names for reference  the additional ordering schemes are the following 
butler  butler  ross  kapur    mercer        is an extension of an approach by fujita  fujisawa 
and kawato         the latter proposed to perform a depth first search  dfs  in the ast 
starting at the root node  and to order the variables in the order in which they are reached the
first time  butler et al  extended this to a setting with several roots  if we remove the overall
root and retain only the subtrees for the various actions we arrive at exactly the same setting  
their approach starts the dfs at the action containing the highest number of variables  within
the tree it advances in a similar manner  it always continues with the subtree that contains the
highest number of variables among all subtrees of the current node  the retrieved ordering is
then again in the order in which the variables are reached the first time 
chung   chung  hajj    patel        is a two step approach  in the first step it assigns values
to all nodes of the ast  starting at the leaves  assigning them a value of    it assigns each
inner node the maximum of the values assigned to its successors plus    in the second step it
performs a dfs starting at the root  which is guided by the values of the nodes  visiting those
successors with highest value first  the order in which the variables are reached for the first
time is then chosen as the variable ordering 
chung   chung et al         determines the shortest distance between each pair of variables  which
can be calculated by considering all edges in the ast as undirected  additionally  the total
distances  i e   the sum of the minimal distances to all other variables  are stored for all variables  a variable with smallest total distance is chosen first  the next one is the one closest
   

fibdd o rdering h euristics for c lassical p lanning

to the last variable inserted into the ordering  in case of a tie the distance to the preceding
variables is also taken into account 
maisonneuve  maisonneuve        is a greedy approach starting with an empty sequence  in each
step it temporarily extends the current sequence by a variable not yet in the sequence  for
this variable  a weight is determined  which is the number of variables from this extended
sequence that appear in an action  summed over all actions  the variable is then removed
from the sequence and the next one added  when all weights are calculated the variable with
highest weight is appended to the sequence  and the next iteration starts  calculating the new
weights for the remaining variables  in the end  the last sequence contains all variables and
thus corresponds to the variable ordering 
malik  malik et al         assigns a level value  the maximal level of all its predecessors plus   
to each node within the ast  the root is assigned value    the variables are then ordered
according to their level values  those with highest values coming first 
minato  minato et al         calculates weights for all nodes in the ast  the weight of the root
node of each action is set to    that of all successors of a node to w m if w is the nodes weight
and m the number of successors of that node  one of the variables with highest weight is then
chosen first and all its nodes removed  along with all ingoing edges and  recursively  those
nodes with no remaining successors   for the reduced graph the weights are recalculated and
the procedure continues until finally all variables are in the ordering 
    bad delimiter
to get the bad delimiter we ran      random orderings  where each such ordering corresponds
to one run of the ipc   benchmark tasks  using a random variable ordering for each instance  to
make this feasible we used a time out of one minute  our backward search implementation is not
viable for such a short time out  so that we use only forward search here   for comparison to this
data  the same settings    minute time out  only forward search  was used with the twelve static
ordering schemes  initially we ran all ordering schemes and the random orderings on all tasks  after
    random runs we removed all tasks from the benchmark set that were not solved at least once
during the previous  random or static ordering  runs  retaining    tasks  figure    shows coverage 
i e   the number of solved planning tasks  on the x axis  and the fraction of random orderings having
that coverage on the y axis  the coverage data for the ordering schemes are shown as vertical lines 
malik and cglevel lie in and below the middle of the gaussian distribution  respectively  in
other words  malik is as bad as  and cglevel is even worse than  the average random ordering 
matters are not as bleak for the other ten ordering schemes  which are close together and lie clearly
above the gaussian distribution  compared to a best of over the random orders  however  all the
ordering schemes appear rather humble  consider table    in particular  consider nr
    giving the
number of instances solved by ordering scheme  but not by any random order  and consider n r
  
giving the number of instances not solved by the scheme but solved by some random order  as
 r
table   shows  nr
  is strictly smaller than n in all but three of the ordering schemes   and is
strictly larger  by a single task  only for one of the schemes  namely butler   the average over nr
 
is      while that over n r
is
     

   

fik issmann   h offmann

percentage of random orderings

  

cglevel
malik
maisonneuve wgamerpre
minato wgamer
cgsons
gamer gamerpre
chung  chung 
butler

  
  
  
 
 
 
 
 
  

  

  

  
coverage

  

  

  

type

butler

cglevel

cgsons

chung 

chung 

gamer

gamerpre

maisonneuve

malik

minato

wgamer

wgamerpre

best scheme

figure     coverage for random orders vs  ordering schemes  schemes are ordered top to bottom
from worst to best coverage  x  y means that both schemes  x and y   result in the same coverage 

nr
 
n r

n r
 
nr


 
 
  
 

 
  
  
 

 
 
  
 

 
 
  
 

 
 
  
 

 
 
  
 

 
 
  
 

 
  
  
 

 
  
  
 

 
 
  
 

 
 
  
 

 
  
  
 

 
 
  
 

table    differences in solved instances for the    ipc   tasks    minute timeout   r means
solved by no random ordering   r by at least one random ordering   not solved by the corresponding ordering scheme    solved by the corresponding ordering scheme 
    good delimiter
here we performed bidirectional blind search  i e   the most competitive setup in general  figure   
contains one data point for every pair  i    of ipc   benchmark instance i and ordering scheme
 that were solved by both  a  gamer using dynamic reordering starting from an arbitrary variable
order  the one returned by gamers grounding process   and  b  gamer using ordering scheme 
 without dynamic reordering   the time out is   hours for  a   and    minutes for  b   the x value
of each data point is the size of the largest bdd constructed for i by  a   the y value is the size of the
largest bdd constructed for i by  b   we allowed a much higher time out for dynamic reordering
because such reordering is not runtime effective  the question we are asking here is merely which
of the two methods yields smaller bdds  figure    shows that dynamic reordering is universally
much better at this  giving us sizes that are up to three orders of magnitude smaller than those of the
schemes  for a total of      instances  solved by both an ordering scheme and dynamic reordering  
in      cases the bdd sizes are smaller by a factor of up to    when using dynamic reordering  in
   

fibdd o rdering h euristics for c lassical p lanning

peak size ordering schemes

   
   
   
   
   
   

   
   
peak size dynamic reordering

   

   

dynamic
reordering

wgamerpre

wgamer

minato

malik

maisonneuve

gamerpre

gamer

chung 

chung 

cgsons

cglevel

butler

figure     bdd size for dynamic reordering vs  ordering schemes 

domain
barman
 
 
 
 
 
 
 
 
 
 
 
 
    
elevators                                    
      
floortile
 
 
 
 
 
 
 
 
 
 
 
 
    
      
nomystery                                    
openstacks                                    
      
parc printer
 
 
 
 
 
 
 
 
 
 
 
 
    
pegsol                                    
      
scanalyzer
 
 
 
 
 
 
 
 
 
 
 
 
    
sokoban                                    
      
tidybot   
          
       
       
 
     
transport
 
 
 
 
 
 
 
 
 
 
 
 
     
                               
      
visitall   
woodworking                        
          
      
total                                                               

table    coverage in the ipc   tasks  for dynamic reordering  the numbers in parentheses represent the coverage with a    minute timeout 
    cases they are smaller by a factor between    and      and in    cases they are smaller by a
factor of more than     
table   shows the coverage of the different schemes on the ipc   tasks  we can make a
similar observation to that of the one minute  only forward search results  namely that cglevel
and malik are clearly behind the others  the last column shows the coverage of gamer using
dynamic reordering  and provides two numbers  first the coverage with the   hours timeout  second
the coverage with the same timeout as the schemes  i e      minutes  from this it becomes clear that
applying dynamic reordering for the entire search time is not feasible in practice when limiting the
runtime 
   

fi   
   
   
   
   
   
 

total runtime
reordering time
transition relation creation

 

 

 

 
 
reorderings

time  s 

time  s 

k issmann   h offmann

  

  

    
    
    
   
   
   
   
 

total runtime
reordering time
transition relation creation

 

 a  visitall  task    

 

 

 
    
reorderings

  

  

  

 b  pegsol  task    

figure     total runtime and time spent in reordering for limited number of reordering steps for two
example ipc   tasks  all reorderings until the vertical line were performed during the transition
relation creation 

   limited dynamic reordering
given the much more memory efficient behavior of dynamic reordering  a possible approach is
to run dynamic reordering for a limited time only  hoping to get an ordering that is good enough
for the remainder of the search  reordering is automatically started when the number of allocated
bdd nodes reaches a certain threshold  by default  the first threshold is      nodes   which is
dynamically adapted after each reordering  by default  the next threshold is set to   times the number
of nodes after reordering   a simple way to control dynamic reordering is to limit the number of
reordering steps  and to turn dynamic reordering off once the desired number of reorderings has
been performed 
for different reordering limits  the total runtime for a task often looks similar to the situation
depicted in figure   a  with too few reorderings it takes a long time to solve the task due to a
bad initial ordering  also  the first reorderings can sometimes hurt  as they are performed at the
very beginning or during construction of the transition relation  before enough information on good
orderings is available  however  with too many reorderings solving takes a long time due to an
immense overhead in reordering time  which grows exponentially with each step 
an important different behavioral pattern is depicted in figure   b  in some domains  such as
pegsol or sokoban  the minimum of the curve is at the very beginning  without any reordering  
and the total runtime only increases afterward  mainly based on the increase in reordering time  
an explanation for this behavior might be that the initial ordering is already pretty good  so that
dynamic reordering cannot improve much and its overhead is incurred in vain  the same also often
happens in the easier tasks of a domain  so that learning a good setting based on the simpler tasks
seems impossible 
attempting to exploit these observations to design adaptive stopping criteria  geared at finding
a good point for stopping dynamic reordering  given only the available observations  e g   number
of bdd nodes before after reordering  reordering times  current total runtimes   we experimented
with the following approaches 
first  we noticed that early on the reordering time increases from step to step by a small factor 
but later on that factor increases  in some preliminary runs we saw that often the area of smallest
runtime coincides with the situation when the increase in reordering time reaches some threshold 
often between      and       see  e g   figure   a and compare it to the runtime minimum in
figure   a for the same planning task   we call this the factor criterion 
   

fibdd o rdering h euristics for c lassical p lanning

   

factor
percentage

factor

 
   
 
   
 
 

 

 

 
 
  
  
reorderings
 a  factor of the time for the last reordering and
the previous one 

  
  
  
  
  
  
  
 

percentage

 

 

 

 
 
  
  
reorderings
 b  percentage of the time for the last reordering
of the total runtime so far 

figure     factor and percentage criterion for limited number of reordering steps for task     of
the ipc   visitall domain 
second  another observation from these preliminary runs is that the percentage of the time spent
in the last reordering step on the total current runtime often follows a u like curve  and the minimum
of that curve often lies close to the same number of reorderings as the total runtime minimum  see 
e g   figure   b and compare to the runtime minimum in figure   a for the same task   we employ
a percentage criterion  which stops reordering after a  possibly local  minimum has been reached 
i e   we compare the percentage of the current step with that of the previous step  when the current
one is greater we stop reordering 
finally  a simple combination of both criteria is to stop reordering as soon as one of them tells
us to do so 
to evaluate these adaptive stopping criteria  we ran all tasks of the ipc   domains with different limits for the number of reorderings  ranging from   to      based on those runs we calculated
the results we would achieve with the adaptive stopping criteria  see table    the best possible
coverage  i e   the number of tasks solved by at least one setting with limited number of reorderings 
is      while without any reordering we found     solutions  the adaptive stopping criteria yield
coverage between     and      performance is reasonable for the factor criterion  but is quite bad
for the percentage criterion and the combination of both criteria  recall that the percentage criterion aims at stopping reordering before incurring prohibitive overhead  indeed  with this criterion 
reordering is often stopped earlier than with the factor criterion  in some cases this was detrimental 
particularly in the woodworking domain where this strategy happens to fall into a dramatic local
peak of the total runtime curve  resulting in   problem instances no longer being solved 
as  in several cases  dynamic reordering during the transition relation creation was counterproductive  we also ran delayed reordering  where dynamic reordering is started only after the bdds
for the transition relation have been created  the results are in table    the case without reordering
is unchanged with respect to table    the best possible result is now slightly worse than before 
with coverage      for the adaptive stopping criteria  the picture changes substantially  in contrast
to table    the percentage criterion now excels  delivering coverage just   short of the best possible 
regarding the factor criteria  overly small or large factors now are bad and the best behavior    short
of best possible  is obtained in the middle 
   in all those cases the highest number of reorderings we could observe was clearly below     either the planner ran
out of time or memory  or finished before the last reorderings could be performed  in all cases we used gamerpre as
the initial ordering  which turned out to be among the best in a preliminary set of experiments 

   

fik issmann   h offmann

domain
barman
elevators
floortile
nomystery
openstacks
parc printer
pegsol
scanalyzer
sokoban
tidybot
transport
visitall
woodworking
total

no
reord
 
  
 
  
  
 
  
 
  
  
 
  
  
   

best
possible
 
  
 
  
  
 
  
 
  
  
 
  
  
   

    
 
  
 
  
  
 
  
 
  
  
 
  
  
   

factor criterion
            
 
 
 
  
  
  
 
 
 
  
  
  
  
  
  
 
 
 
  
  
  
 
 
 
  
  
  
  
  
  
 
 
 
  
  
  
  
  
  
           

percentage
criterion
 
  
 
  
  
 
  
 
  
  
 
  
 
   

    
 
  
 
  
  
 
  
 
  
  
 
  
 
   

both criteria
        
 
 
  
  
 
 
  
  
  
  
 
 
  
  
 
 
  
  
  
  
 
 
  
  
 
 
       

   
 
  
 
  
  
 
  
 
  
  
 
  
 
   

table    coverage results for different stopping criteria  immediate reordering 

domain
barman
elevators
floortile
nomystery
openstacks
parc printer
pegsol
scanalyzer
sokoban
tidybot
transport
visitall
woodworking
total

no
reord
 
  
 
  
  
 
  
 
  
  
 
  
  
   

best
possible
 
  
 
  
  
 
  
 
  
  
 
  
  
   

    
 
  
 
  
  
 
  
 
  
  
 
  
  
   

factor criterion
            
 
 
 
  
  
  
 
 
 
  
  
  
  
  
  
 
 
 
  
  
  
 
 
 
  
  
  
  
  
  
 
 
 
  
  
  
  
  
  
           

percentage
criterion
 
  
 
  
  
 
  
 
  
  
 
  
  
   

    
 
  
 
  
  
 
  
 
  
  
 
  
  
   

both criteria
        
 
 
  
  
 
 
  
  
  
  
 
 
  
  
 
 
  
  
  
  
 
 
  
  
  
  
       

   
 
  
 
  
  
 
  
 
  
  
 
  
  
   

table    coverage results for different stopping criteria  delayed reordering  i e   reordering started
only after creation of the transition relation bdds 
to shed some light on these observations  figure    shows coverage as a function of more
different factor values  for both the case of immediate reordering  figure   a  and that of delayed
reordering  figure   b   in figure   a  we see that the percentage criterion stops reordering too
early  without it  the coverage resulting from stopping reordering based solely on the factor criterion
can get as high as      however  before the ascend in the coverage actually starts the percentage
criterion stops reordering  thus cutting off many solutions  in figure   b  up to a factor of    
both curves are identical  both mainly increasing with increasing factor  after that  the combination
criterion rises another little bit  while the factor criterion alone drops substantially  the combined
criterion avoids that drop because  at some point  here  at a factor of roughly       the percentage
criterion stops reordering at least as early as the factor criterion 
   

fi   
   
   
   
   
   
   

factor
both
coverage

coverage

bdd o rdering h euristics for c lassical p lanning

 

   

 

   
 
factor
 a  immediate reordering 

   

 

   
   
   
   
   
   
   

factor
both

 

   

 

   
 
factor
 b  delayed reordering 

   

 

figure     coverage as a function of factor  for the factor criterion alone  and for its combination
with the percentage criterion  denoted as both  

   conclusion
it is tempting to equate the variable dependencies in bdd based symbolic search with those
identified in causal graphs  and previous research has done so unquestioningly  looking a little more
closely at this issue  we have shown that causal graph based variable orderings are exponentially bad
even in severely restricted sub classes of planning  empirically  fast downwards level heuristic is
worse than random  and all ordering schemes lag far behind off the shelf reordering 
one may wonder about the meaning of the theoretical results  how could a static ordering
scheme not incur exponential overhead in the worst case  we agree with that view in principle 
but we did not expect this to happen in planning tasks so restricted as to be tractable for domainindependent optimal planning  it remains to be seen to what extent our classification framework is
suitable to characterize the properties of other ordering schemes and or planning fragments 
our impression at this point is that static ordering schemes are so limited as to be hopeless 
prior to actually building the bdds  it appears impossible to extract any reliable information about
which form they will take  the way forward  then  is to use dynamic reordering techniques in a
more targeted manner  our initial experiments in that direction did not meet with an immediate
breakthrough  but certainly they show promise  especially considering the primitive nature of the
method and of the stopping criteria employed  promising future directions include more flexible
on off strategies for dynamic reordering  machine learning for deciding when to toggle the switch 
and planning specific reordering techniques exploiting the particular structure of the bdds at hand 

acknowledgments
we thank the anonymous reviewers of the icaps      short version and those of a previous version
of this article  whose comments helped tremendously to improve the paper 

references
brafman  r     domshlak  c          structure and complexity in planning with unary operators 
journal of artificial intelligence research             
bryant  r  e          graph based algorithms for boolean function manipulation  ieee transactions on computers                
   

fik issmann   h offmann

burch  j  r   clarke  e  m     long  d  e          symbolic model checking with partitioned
transition relations  in halaas  a     denyer  p  b   eds    proceedings of the international
conference on very large scale integration  vlsi      vol  a   of ifip transactions  pp 
      edinburgh  scotland  north holland 
burch  j  r   clarke  e  m   long  d  e   mcmillan  k  l     dill  d  l          symbolic model
checking for sequential circuit verification  ieee transactions on computer aided design of
integrated circuits and systems                
butler  k  m   ross  d  e   kapur  r     mercer  m  r          heuristics to compute variable
orderings for efficient manipulation of ordered binary decision diagrams  in proceedings
of the   th conference on design automation  dac      pp          san francisco  ca 
usa  acm 
chen  h     gimenez  o          causal graphs and structurally restricted planning  journal of
computer and system sciences                
chung  p  y   hajj  i  n     patel  j  h          efficient variable ordering heuristics for shared
robdd  in proceedings of the      ieee international symposium on circuits and systems
 iscas      pp            chicago  il  usa  ieee 
cimatti  a   pistore  m   roveri  m     traverso  p          weak  strong  and strong cyclic planning
via symbolic model checking  artificial intelligence                
darwiche  a          sdd  a new canonical representation of propositional knowledge bases  in
walsh  t   ed    proceedings of the   nd international joint conference on artificial intelligence  ijcai     pp          aaai press ijcai 
dechter  r     meiri  i          experimental evaluation of preprocessing techniques in constraint
satisfaction problems  in sridharan  n  s   ed    proceedings of the   th international joint
conference on artificial intelligence  ijcai      pp          detroit  mi  morgan kaufmann 
domshlak  c     dinitz  y          multi agent offline coordination  structure and complexity  in
cesta  a     borrajo  d   eds    recent advances in ai planning   th european conference
on planning  ecp      lecture notes in artificial intelligence  pp        toledo  spain 
springer verlag 
edelkamp  s     helmert  m          exhibiting knowledge in planning problems to minimize
state encoding length  in biundo  s     fox  m   eds    recent advances in ai planning 
 th european conference on planning  ecp     lecture notes in artificial intelligence  pp 
        durham  uk  springer verlag 
fan  g   muller  m     holte  r          non linear merging strategies for merge and shrink based
on variable interactions  in edelkamp  s     bartak  r   eds    proceedings of the  th annual
symposium on combinatorial search  socs     aaai press 
freuder  e  c          a sufficient condition for backtrack free search  journal of the association
for computing machinery              
fujita  m   fujisawa  h     kawato  n          evaluation and improvements of boolean comparison method based on binary decision diagrams  in proceedings of the      international
conference on computer aided design  iccad      pp      ieee computer society press 
   

fibdd o rdering h euristics for c lassical p lanning

gimenez  o     jonsson  a          the complexity of planning problems with simple causal
graphs  journal of artificial intelligence research             
helmert  m          a planning heuristic based on causal graph analysis  in koenig  s   zilberstein 
s     koehler  j   eds    proceedings of the   th international conference on automated
planning and scheduling  icaps     pp          whistler  canada  morgan kaufmann 
helmert  m          the fast downward planning system  journal of artificial intelligence research             
helmert  m   haslum  p     hoffmann  j          flexible abstraction heuristics for optimal sequential planning  in boddy  m   fox  m     thiebaux  s   eds    proceedings of the   th
international conference on automated planning and scheduling  icaps     pp         
providence  rhode island  usa  morgan kaufmann 
helmert  m   haslum  p   hoffmann  j     nissim  r          merge   shrink abstraction  a method
for generating lower bounds in factored state spaces  journal of the association for computing machinery        
hoffmann  j       a   analyzing search topology without running any search  on the connection
between causal graphs and h    journal of artificial intelligence research             
hoffmann  j       b   where ignoring delete lists works  part ii  causal graphs  in bacchus  f  
domshlak  c   edelkamp  s     helmert  m   eds    proceedings of the   st international
conference on automated planning and scheduling  icaps     pp         aaai press 
jonsson  p     backstrom  c          incremental planning  in european workshop on planning 
katz  m     domshlak  c          new islands of tractability of cost optimal planning  journal of
artificial intelligence research             
katz  m     domshlak  c          implicit abstraction heuristics  journal of artificial intelligence
research            
kissmann  p     edelkamp  s          improving cost optimal domain independent symbolic planning  in burgard  w     roth  d   eds    proceedings of the   th national conference of the
american association for artificial intelligence  aaai      pp          san francisco  ca 
usa  aaai press 
kissmann  p     hoffmann  j          whats in it for my bdd  on causal graphs and variable orders in planning  in borrajo  d   fratini  s   kambhampati  s     oddi  a   eds    proceedings
of the   rd international conference on automated planning and scheduling  icaps     pp 
        rome  italy  aaai press 
knoblock  c          automatically generating abstractions for planning  artificial intelligence 
              
maisonneuve  v          automatic heuristic based generation of mtbdd variable orderings for
prism models  internship report  oxford university computing laboratory 
malik  s   wang  a   brayton  r     sangiovanni vincentelli  a          logic verification using
binary decision diagrams in a logic synthesis environment  in proceedings of the      international conference on computer aided design  iccad      pp      ieee computer
society press 
   

fik issmann   h offmann

mcmillan  k  l          symbolic model checking  kluwer academic publishers 
minato  s   ishiura  n     yajima  s          shared binary decision diagram with attributed edges
for efficient boolean function manipulation  in proceedings of the   th acm ieee design
automation conference  dac      pp        orlando  fl  usa  ieee computer society
press 
moskewicz  m   madigan  c   zhao  y   zhang  l     malik  s          chaff  engineering an
efficient sat solver  in proceedings of the   th conference on design automation  dac     las vegas  nevada  usa  ieee computer society 
rintanen  j          planning as satisfiability  heuristics  artificial intelligence            
rudell  r          dynamic variable ordering for ordered binary decision diagrams  in lightner 
m  r     jess  j  a  g   eds    proceedings of the      ieee acm international conference
on computer aided design  iccad      pp        santa clara  ca  usa  ieee computer
society 
sievers  s   wehrle  m     helmert  m          generalized label reduction for merge and shrink
heuristics  in proceedings of the   th aaai conference on artificial intelligence  aaai    
quebec city  quebec  canada  aaai press 
wegener  i          branching programs and binary decision diagrams  siam 
williams  b  c     nayak  p  p          a reactive planner for a model based executive  in pollack 
m   ed    proceedings of the   th international joint conference on artificial intelligence
 ijcai      pp            nagoya  japan  morgan kaufmann 
zabih  r          some applications of graph bandwidth to constraint satisfaction problems  in
proceedings of the  th national conference of the american association for artificial intelligence  aaai      pp        boston  ma  mit press 

   

fi