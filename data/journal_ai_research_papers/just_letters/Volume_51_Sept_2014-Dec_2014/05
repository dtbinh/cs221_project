journal artificial intelligence research          

submitted        published      

using meta mining support data mining workflow
planning optimization
phong nguyen
melanie hilario

phong nguyen unige ch
melanie hilario unige ch

department computer science
university geneva
switzerland

alexandros kalousis

alexandros kalousis hesge ch

department business informatics
university applied sciences
western switzerland 
department computer science
university geneva
switzerland

abstract
knowledge discovery databases complex process involves many different
data processing learning operators  todays knowledge discovery support systems
contain several hundred operators  major challenge assist user designing
workflows valid ideally optimize performance measure
associated user goal  paper present system  system relies
meta mining module analyses past data mining experiments extracts metamining models associate dataset characteristics workflow descriptors view
workflow performance optimization  meta mining model used within data mining
workflow planner  guide planner workflow planning  learn metamining models using similarity learning approach  extract workflow descriptors
mining workflows generalized relational patterns accounting domain
knowledge provided data mining ontology  evaluate quality data mining
workflows system produces collection real world datasets coming
biology show produces workflows significantly better alternative
methods workflow selection planning 

   introduction
learning models extracting knowledge data using data mining extremely
complex process requires combining number data mining  dm  operators  selected large pools available operators  combined data mining workflow  dm
workflow assembly individual data transformations analysis steps  implemented
dm operators  composing dm process data analyst chooses address
his her dm task  workflows recently emerged new paradigm representing
managing complex computations accelerating pace scientific progress   meta  
analysis becoming increasingly challenging growing number complexity
available operators  gil et al         
c
    
ai access foundation  rights reserved 

finguyen  hilario   kalousis

todays second generation knowledge discovery support systems  kdss  allow complex
modeling workflows contain several hundreds operators  rapidminer platform  klinkenberg  mierswa    fischer         extended version weka  hall
et al         r  r core team         proposes actually     operators 
complex data control flows  e g  bagging boosting operators 
several sub workflows interleaved  consequence  possible number
workflows modeled within systems order several millions 
ranging simple elaborated workflows several hundred operators  therefore data analyst carefully select among operators ones
meaningfully combined address his her knowledge discovery problem  however  even
sophisticated data miner overwhelmed complexity modeling 
rely his her experience biases well thorough experimentation
hope finding best operator combination  advance new generation
kdss provide even advanced functionalities  becomes important provide
automated support user workflow modeling process  issue
identified one top ten challenges data mining  yang   wu        

   state art dm workflow design support
last decade  rather limited number systems proposed provide
automated user support design dm workflows  bernstein  provost  hill       
propose ontology based intelligent discovery assistant  ida  plans valid dm workflows valid sense executed without failure according basic
descriptions input dataset attribute types  presence missing values  number
classes  etc  describing dm ontology input conditions output effects
dm operators  according three main steps kd process  pre processing  modeling post processing  fayyad  piatetsky shapiro    smyth         ida systematically
enumerates workflow planner possible valid operator combinations  workflows 
fulfill data input request  ranking workflows computed according
user defined criteria speed memory consumption measured
past experiments 
zakova  kremen  zelezny  lavrac        propose kd ontology support automatic design dm workflows relational dm  ontology  dm relational algorithms
datasets modeled semantic web language owl dl  providing semantic
reasoning inference querying dm workflow repository  similar ida 
kd ontology describes dm algorithms data input output specifications 
authors developed translator ontology representation planning domain definition language  pddl   mcdermott et al          produce
abstract directed acyclic graph workflows using ff style planning algorithm  hoffmann 
       demonstrate approach genomic product engineering  cad  usecases complex workflows produced make use relational data structure
background knowledge 
recently  e lico project  featured another ida built upon planner
constructs dm plans following hierarchical task networks  htn  planning approach 
   http   www e lico eu

   

fiusing meta mining support dm workflow planning optimization

specification htn given data mining workflow  dmwf  ontology  kietz 
serban  bernstein    fischer         predecessors  e lico ida designed
identify operators whose preconditions met given planning step order plan
valid dm workflows exhaustive search space possible dm plans 
none three dm support systems discussed consider eventual
performance workflows plan respect dm task supposed
address  example  goal plan design workflows solve classification
problem  would consider measure classification performance  accuracy 
deliver workflows optimize it  discussed dm support systems deliver
extremely large number plans  dm workflows  typically ranked simple
heuristics  workflow complexity expected execution time  leaving user
loss best workflow terms expected performance dm task
he she needs address  even worse  planning search space large
systems even fail complete planning process  see example discussion
kietz  serban  bernstein  fischer        
considerable work tries support user  view performance
maximization  specific part dm process  modeling learning 
number approaches proposed  collectively identified meta learning  brazdil 
giraud carrier  soares    vilalta        kalousis        kalousis   theoharis        hilario        soares   brazdil         main idea meta learning given new
dataset system able rank pool learning algorithms respect
expected performance dataset  so  one builds meta learning model
analysis past learning experiments  searching associations algorithms performances dataset characteristics  however  already mentioned  meta learning
approaches address learning modeling part applicable complete
process level 
work hilario  nguyen  do  woznica  kalousis         first effort
lift meta learning ideas level complete dm workflows  proposed novel metalearning framework called meta mining process oriented meta learning applied
complete dm process  associate workflow descriptors dataset descriptors 
applying decision tree algorithms past experiments  order learn couplings
workflows datasets lead high predictive performance  workflow descriptors
extracted using frequent pattern mining accommodating background knowledge 
given data mining optimization  dmop  ontology  dm tasks  operators  workflows  performance measures relationships  however predictive performance
system rather low  due limited capacity decision trees capturing
relations dataset workflow characteristics essential
performance prediction 
address limitation  presented work nguyen  wang  hilario 
kalousis      b  approach learns called heterogeneous similarity measures 
associating dataset workflow characteristics  learn similarity measures
dataset space  workflow space  dataset workflow space  similarity
measures reflect respectively  similarity datasets given similarity
relative workflow performance vectors workflows applied them 
similarity workflows given performance based similarity different datasets 
   

finguyen  hilario   kalousis

dataset workflow similarity based expected performance latter applied
former  however two meta mining methods described limited select
from  rank  set given workflows according expected performance  i e 
cannot plan new workflows given input dataset 
retrospectively  presented work nguyen  kalousis  hilario       
initial blueprint approach dm workflow planning view workflow performance optimization  suggested planner guided metamining model ranks partial candidate workflows planning step  gave
preliminary evaluation proposed approach interesting results  nguyen  kalousis 
  hilario      a   however  meta mining module rather trivial  uses dataset
pattern based workflow descriptors nearest neighbor search dataset descriptors identify similar datasets dataset want plan
workflows  within neighborhood  ranks partial workflows using support
workflow patterns workflows perform best datasets neighborhood  pattern based ranking workflows cumbersome heuristic 
system learning associations dataset workflow characteristics explicitly optimize expected workflow performance  must guide workflow
planning  approach deployed kietz et al         
paper  follow line work first sketched work nguyen et al 
        couple tightly together workflow planning meta mining module
develop dm workflow planning system given input dataset designs workflows
expected optimize performance given dataset  meta mining
module applies heterogeneous similarity learning method presented nguyen et al 
     b  learn associations dataset workflow descriptors lead optimal
performance  exploits learned associations guide planner workflow
construction planning  best knowledge  first system
kind  i e  able design dm workflows specifically tailored
characteristics input dataset view optimizing dm task performance measure 
evaluate system number real world datasets show workflows
plans significantly better workflows delivered number baseline methods 
rest paper structured follows  section    present global architecture system  together brief description planner  section  
describe detail meta mining module  including dataset workflow characteristics uses  learning model  learned model used workflow planning 
section    provide detailed experimental results evaluate approach different
settings  finally  conclude section   

   system description
section  provide general description system  start defining
notations use throughout paper give brief overview
different components system  two important components  planner
meta miner described subsequent sections        respectively  
close section providing formal representation dm workflows
use planner meta miner 
   

fiusing meta mining support dm workflow planning optimization

symbol

e
wl    o            ol  
wcl    i p  wl            i p p   wl   t
xu    x            xd  t
r x  w 
g


   o             
cl
sl

meaning
workflow operator 
workflow data type 
ground dm workflow sequence l operators 
fixed length  p   dimensional vector description
workflow wl
d dimensional vector description dataset 
relative performance rank w workflow x
dataset
dm goal 
htn task 
htn method 
htn abstract operator n possible operators 
set candidate workflows abstract operator o 
set candidate workflows selected cl  

table    summary notations used 
    notations
provide basic notations subsequently introduce additional notations needed  use term dm experiment designate execution
dm workflow w w dataset x x   describe dataset x d dimensional
column vector xu    x          xd  t   describe detail section       dataset descriptions use  experiment characterized performance measure 
example mining problem addressing classification problem one
performance measure accuracy  performance measures workflows
applied given dataset x extract relative performance rank r x  w  r 
workflow w x dataset  statistically comparing performance
differences different workflows given dataset  more section        
matrix x   n contains descriptions n datasets used training system  given workflow w two representations  wl denote
l length operator sequence constitutes workflow  note different workflows
different lengths  fixed length representation  wcl denote
fixed length  p   dimensional binary vector representation w workflow  feature
wcl indicates presence absence relational feature pattern workflow 
essentially wcl propositional representation workflow  describe
detail section       extract propositional representation  finally w denotes
collection workflows  used training system  w
corresponding  p   matrix contains propositional representations  depending context different workflow notations used interchangeably  table  
summarizes important notations 
    system architecture operational pipeline
provide figure   high level architectural description system  three blue
shaded boxes are   i  data mining experiment repository  dmer  stores
   

finguyen  hilario   kalousis

input data

   user interface
goal
input md

  

input md

dmer

optimal plans   

training md

metaminer

intelligent discovery assistant  ida 

 partial  candidate workflows
ai
planner

dm workflow
ontology  dmwf 

workflow ranking
  

online
mode

metamined
model

offline
mode

  

dm optimization
ontology  dmop 

software
data flow

figure    meta mining systems components pipeline 
base level resources  i e  training datasets  workflows  well performance results
application latter former  essentially dmer contains training data
used derive models necessary workflow planning
design   ii  user interface user interacts system  specifying
data mining tasks input datasets  iii  intelligent discovery assistant  ida 
component actually plans data mining workflows optimize
performance measure given dataset data mining task user provided
input  ida constitutes core system contains planning component
meta mining component interact closely order deliver optimal workflows
given input problem  describe two components detail sections      
respectively  system operates two modes  offline online mode  offline
mode meta mining component analyses past base level data mining experiments 
stored dmer  learn meta mining model associates dataset workflow
characteristics view performance optimization  online mode meta miner
interacts planner guide planning workflows using meta mining
model 
go briefly different steps systems life cycle  first
need collect dmer sufficient number base level data mining experiments 
i e  applications different data mining workflows different datasets  step    
experiments used step   meta miner generate meta mining model 
precisely extract base level experiments number characteristics
describe datasets workflows performance latter achieved
applied former  meta data meta miner learns associations dataset
workflow characteristics lead high performance  learning heterogeneous similarity measure outputs high similarity workflow dataset
former expected achieve high performance applied latter
 more details section      
model learned  offline phase completed online phase
start  online phase system directly interacts user  given user
select data mining task g g  classification  regression  clustering  etc  well
   

fiusing meta mining support dm workflow planning optimization

input dataset task applied  might specify
number top k optimal workflows planned  step     given new task
input dataset action transfered ida ai planner starts
planning process  step planning process ai planner generates list
valid actions  partial candidate workflows  passes ranking meta miner
according expected performance given dataset  step    meta miner
ranks using learned meta mining model planning continues topranked partial candidate workflows data mining task resolved  end
planning process  top k workflows presented user order given
expected performance input dataset  greedy planning approach
step select top k current solutions  principle let aiplanner first generate possible workflows meta miner rank them 
resulting plans would ranked according local greedy approach 
would ranked globally thus optimally respect meta mining model 
however general feasible due complexity planning process
combinatorial explosion number plans 
    dm workflow representation
give formal definition dm workflow represent it  dm
workflows directed acyclic typed graphs  dags   nodes correspond operators
edges nodes data input output objects  fact hierarchical dags
since dominating nodes operators contain sub workflows  typical
example cross validation operator whose control flow given parallel execution
training sub workflows  complex operator boosting  formally  let 
set available operators appear dm workflow  e g  classification operators  j    svms  etc  includes dominating operators
defined one sub workflows dominate  operator
defined name labelling function  o   data types e e
inputs outputs  direct sub workflows dominating operator 
e set available data types appear dm workflow  namely
data types various i o objects appear dm workflow
models  datasets  attributes  etc 
graph structure dm workflow pair  o   e    contains subworkflows any  set vertices correspond operators used
dm workflow sub workflow s   e e set pairs nodes   oi   oj   
directed edges  correspond data types output input objects 
passed operator oi operator oj   following graph structure  topological sort
dm workflow permutation vertices graph edge  oi   oj  
implies oi appears oj   i e  complete ordering nodes directed
acyclic graph given node sequence 
wl    o        ol  
   

   

finguyen  hilario   kalousis

legend
input   output edges
sub input   output edges
x
x

end

retrieve

basic nodes
output

composite nodes

result
x validation
split

training set

join

test set

weight information gain

weights
example set
apply model
performance
select weights
labelled data
training set
performance
naive bayes
model

figure    example dm workflow performance estimation combination
feature selection classification algorithms 

subscript wl denotes length l  i e  number operators  topological
sort  topological sort dm workflow structurally represented rooted 
labelled ordered tree  bringmann        zaki         depth first search
graph structure maximum depth given expanding recursively subworkflows dominating operators  thus topological sort workflow tree
representation reduction original directed acyclic graph nodes
edges fully ordered 
example hierarchical dag representing rapidminer dm workflow given
figure    graph corresponds dm workflow cross validates feature selection
method followed classification model building step naivebayes classifier  xvalidation typical example dominating operator workflow
two basic blocks  training block arbitrary workflow receives
input dataset outputs model  testing block receives input model
dataset outputs performance measure  particular  training subworkflow  feature weights computed informationgain operator 
number features selected selectbyweights operator  followed
final model building naivebayes operator  testing block  typical
sub workflow consists application learned model testing set
applymodel operator  followed performance estimation given performance
operator  topological sort graph given ordered tree given figure   
   

fiusing meta mining support dm workflow planning optimization

retrieve
 

weight
information gain
 

select
weights
 

x validation
   

naive
bayes
 

end
 

apply
model
 

performance
 

figure    topological order dm workflow given figure   

    workflow planning
workflow planner use based work kietz et al                designs
dm workflows using hierarchical task network  htn  decomposition crisp dm process model  chapman et al          however planner workflow enumeration
generating possible plans  i e  consider expected workflow performance
planning process scale large set operators explode
search space  order address limitations  presented preliminary results
planner coupled frequent pattern meta mining algorithm
nearest neighbor algorithm rank partial workflows step workflow planning  nguyen et al             a   system deployed kietz et al         
approach followed prioritize partial workflows according support
frequent patterns contained achieved set workflows performed
well set datasets similar input dataset want plan
workflow  however learning associations dataset workflow
characteristics  approach follow here 
section       briefly describe htn planner kietz et al               
section       describe use prediction expected performance
partial workflow applied given dataset guide htn planner  give
complete description meta mining module learns associations dataset
workflow characteristics expected achieve high performance section   
      htn planning
given goal g g  ai planner decompose top down manner goal
elements two sets  tasks methods   task achieve g 
subset associated methods share data input output
 i o  object specification address it  turn  method
defines sequence operators  and or abstract operators  see below   and or sub tasks 
executed order achieve m  recursively expanding tasks  methods
operators given goal g  ai planner sequentially construct htn plan
terminal nodes correspond operators  non terminal nodes htn
task method decompositions  dominating operators  x validation instance
dominate training testing sub workflows   example htn plan given
   

finguyen  hilario   kalousis

evaluateattributeselectionclassification
x validation
in  dataset
out  performance

attributeselection
classificationtraining

classification
training

attributeselection
training

attribute
weighting
operator
in  dataset
out  attributeweights

select
weights
in  dataset
in  attributeweights
out  dataset

predictive
supervised
learner
in  dataset
out  predictive
model

attributeselection
classificationtesting

model
application
apply
model
in  dataset
in  predictive
model
out  labelled
dataset

model
evaluation
in  labelled
dataset
out  performance

figure    htn plan dm workflow given figure    non terminal nodes
htn tasks methods  except dominating operator x validation  abstract
operators bold simple operators italic  annotated
i o specification 

figure    plan corresponds feature selection classification workflow
figure   exactly topological sort  of operators  given figure   
sets goals g  tasks   methods   operators o  relations 
described dmwf ontology  kietz et al          there  methods operators
annotated pre  post conditions used ai planner 
additionally  set operators enriched shallow taxonomic view
operators share i o object specification grouped common
ancestor 
   o             

   

defines abstract operator  i e  operator choice point alternative among set
n syntactically similar operators  example  abstract attributeweighting operator
given figure   include feature weighting algorithms informationgain
relieff  similarly abstract predictive supervised learner operator contain
classification algorithms naivebayes linear svm 
htn planner plan operators applied attribute  e g 
continuous attribute normalization operator  discretization operator  uses cyclic
planning structures apply subsets attributes  case use attributegrouping functionality require use cyclic planning structures 
precisely operator selected application applied appropriate
attributes  overall htn grammar contains descriptions    different tasks
   

fiusing meta mining support dm workflow planning optimization

    operators  planner plan  numbers limited
modeling effort required describe tasks operators  inherent
limitation htn grammar planner  kietz et al         developed module
open source ontology editor protege called e proplan  facilitate modelling
new operators describe task method decomposition grammars dm problems
dmwf ontology 
      workflow selection task
ai planner designs construction htn plan several partial workflows 
derived substituting abstract operator one n
operators  number planned workflows order several thousands
leave user loss workflow choose her his problem 
even worse  possible planning process never succeeds find valid plan
terminate due complexity search space 
support user selection dm workflows  use post planning approach designed workflows evaluated according evaluation
measure order find k ones globally best given mining problem  however  global approach computational expensive planning
phase mentioned above  instead  follow planning approach
locally guide ai planner towards design dm workflows optimized
given problem  avoiding thus need explore whole planning search space 
clearly  following local approach cost one pay potential reduction
performance since workflows explored evaluated  potentially missing good
ones due greedy nature plan construction 
adopt heuristic hill climbing approach guide planner  abstract
operator need determine n candidate operators expected
achieve best performance given dataset  formally  define by 
cl     wlo    wl  sl   o o  kn

   

set k n partial candidate workflows length l generated
expansion abstract operator adding one n candidate operators
one k candidate workflows length l   constitute set sl  workflows
selected previous planning step  s  empty set see below   let xu vector
description input dataset want plan workflows address
data mining goal g optimize performance measure  moreover let wclo binary
vector provides propositional representation wlo workflow respect
set generalized relational frequent workflow patterns contains    construct
set sl selected workflows current planning step according to 
sl     arg max r xu   wclo  g  k

   

 wlo cl  

   available http   www e lico eu eproplan html
   provide detailed description extract wclo descriptors section       
moment note propositional representations fixed length representations 
depend l 

   

finguyen  hilario   kalousis

r xu   wclo  g  estimated performance workflow propositional description wclo applied dataset description xu   i e  planning step
select best current partial workflows according estimated expected performance  meta mining model  learn past experiments  delivers
estimations expected performance  section     describe derive
meta mining models use get estimates expected performance 
stress producing performance estimate r xu   wclo  g 
meta mining model uses workflow description wclo   candidate operator descriptions  pattern based descriptions capture dependencies interactions
different operators workflows  again wclo representation
section         rather crucial point since well known fact construct data mining workflows need consider relations biases different
algorithms use within them  bias combinations better others 
pattern based descriptions use provide precisely type information  i e 
information operator combinations appearing within workflows 
next section provide complete description meta miner module 
including characterize datasets  workflows performance latter applied
former  course learn meta mining models use planning 

   meta miner
meta miner component operates two modes  offline mode learns
past experimental data meta mining model provides expected performance
estimates r xu   wclo  g   on line mode interacts ai planner step
planning process  delivering r xu   wclo  g  estimates used planner
select workflows planning step according eq     
rest section organised follows  subsection       explain
describe datasets  i e  derive xu dataset descriptors  subsection      
derive propositional representation wclo data mining workflows subsection       rank workflows according performance given dataset  i e 
r xu   wclo  g   finally subsection     explain build models past mining
experiments provide expected performance estimations r xu   wclo  g 
use models within planner 
    meta data performance measures
section describe meta data  namely dataset workflow descriptions 
performance measures used meta miner learn meta mining models
associate dataset workflow characteristics view planning dm workflows
optimize performance measure 
      dataset characteristics
idea characterize datasets full fledged research problem early
inception meta learning  michie  spiegelhalter  taylor    campbell       
kopf  taylor    keller        pfahringer  bensusan    giraud carrier         soares  
   

fiusing meta mining support dm workflow planning optimization

brazdil        hilario   kalousis        peng  flach  soares    brazdil        kalousis 
gama    hilario         following state of art dataset characteristics  characterize
dataset x x three following groups characteristics 
statistical information theoretic measures  group refers data characteristics
defined statlog  michie et al         king  feng    sutherland       
metal projects   soares   brazdil         includes number instances 
number classes  proportion missing values  proportion continuous   categorical
features  noise signal ratio  class entropy  mutual information  mainly describe
attribute statistics class distributions given dataset sample 
geometrical topological measures  group concerns new measures try
capture geometrical topological complexity class boundaries  ho   basu 
             includes non linearity  volume overlap region  maximum fishers
discriminant ratio  fraction instance class boundary  ratio average intra inter
class nearest neighbour distance 
landmarking model based measures  group related measures asserted
fast machine learning algorithms  called landmarkers  pfahringer et al         
derivative based learned models  peng et al          includes error
rates pairwise  p values obtained landmarkers  nn decisionstump 
histogram weights learned relief svm  extended last group
new landmarking methods based weight distribution feature weighting
algorithms relief svm build twenty different histogram
representations discretized feature weights 
overall  system makes use total       numeric characteristics describe
dataset  denote vectorial representation dataset x x xu  
far exhaustive dataset characteristics used  including
characteristics subsampling landmarks  leite   brazdil         main goal
work produce comprehensive set dataset descriptors design dm
workflow planning system given set dataset characteristics  coupled workflow
descriptors  plan dm workflows optimize performance measure 
      workflow characteristics
seen section     workflows graph structures quite complex
containing several nested sub structures  often difficult analyze
spaghetti like structure information
subtask addressed workflow component  van der aalst   giinther 
       process mining addresses problem mining generalized patterns
workflow structures  bose   der aalst        
characterize dm workflows follow process mining approach 
extract generalized  relational  frequent patterns tree representations 
use derive propositional representations them  possible generalizations
   http   www metal kdd org 

   

finguyen  hilario   kalousis

dm algorithm
dataprocessing
algorithm

predictivemodelling
algorithm

featureweighting
algorithm

classificationmodelling
algorithm

learnerfreefw
algorithm

univariatefw
algorithm

multivariatefw
algorithm

missingvalues
tolerant
algorithm

irrelevant
tolerant
algorithm

exactcos
based
algorithm

c   

naive
bayes

svm

entropybasedfw
algorithm

ig

relieff
is followed by

is implemented by

figure    part dmops algorithm taxonomies  short dashed arrows represent
is followed by relation dm algorithms  long dashed arrows represent is implemented by relation dm operators dm algorithms 

 a 

 b 

x validation

 c 

x validation

feature
weighting
algorithm

classification
modeling
algorithm

feature
weighting
algorithm

classification
modeling
algorithm

univariatefw
algorithm

irrelevant
tolerant
algorithm

learnerfreefw
algorithm

missingvalues
tolerant
algorithm

entropybasedfw
algorithm

x validation
feature
weighting
algorithm

classification
w
algorithm
exactcos
based
algorithm

figure    three workflow patterns cross level concepts  thin edges depict workflow
decomposition  double lines depict dmops concept subsumption 

described domain knowledge which  among knowledge  given data
mining ontology  use data mining optimization  dmop  ontology  hilario
et al                briefly  ontology provides formal conceptualization dm
domain describing dm algorithms defining relations terms dm tasks 
models workflows  describes learning algorithms c     naivebayes svm 
according learning behavior bias variance profile  sensitivity
type attributes  etc  instance  algorithms cited tolerant irrelevant attributes  c    naivebayes algorithms tolerant missing values 
whereas svm naivebayes algorithms exact cost function  algorithm characteristics families classified taxonomies dmop primitive
   

fiusing meta mining support dm workflow planning optimization

concept dm algorithm  moreover  dmop specifies workflow relations  algorithm order 
is followed by relation relates workflow operators dm algorithms
is implemented by relation  figure   shows snapshot dmops algorithm taxonomies ground operators bottom related dm algorithms implement 
mine generalized relational patterns dm workflows  follow method
presented hilario et al          first  use dmop ontology annotate set w
workflows  then  extract set generalized patterns using frequent pattern
mining algorithm  concretely  operator contained parse tree training dm
workflow wl w   insert tree branch operator taxonomic concepts 
ordered top bottom  implemented operator  given
dmop  result new parse tree additional nodes dmops
concepts  call parse tree augmented parse tree  reorder nodes
augmented parse tree satisfy dmops algorithm taxonomies relations 
example feature selection algorithm typically composed feature weighting algorithm
followed decision rule selects features according heuristics  result
set augmented reordered workflow parse trees  representation 
apply tree mining algorithm  zaki        extracts set p frequent patterns 
pattern corresponds tree appears frequently within augmented parse trees 
mine patterns support higher equal five  principle go
low support one  exploding dimensionality description workflows 
probably features poor discriminatory power  nevertheless since metamining models rely metric learning  able learn importance different
meta features  would able cope scenario  note extract
workflow characteristics could used different techniques graph
mining directly graph structures defined workflows ontology 
main reason computational cost latter approaches  well
fact frequent pattern mining propositionalization known work well 
figure   give examples mined patterns  note extracted patterns
generalized  sense contain entities defined different abstraction levels 
provided dmop ontology  relational describe
relations  order relations  structures appear within workflow 
contain properties entities described dmop ontology 
example pattern  c  figure   states feature weighting algorithm  abstract
concept  followed  relation  classification algorithm exact cost function
 property   within cross validation 
use set p frequent workflow patterns describe dm workflow wl w
patterns p p wl workflow contains  propositional description
workflow wl given  p   length binary vector 
wcl    i p  wl            i p p   wl   t        p  

   

denotes induced tree relation  zaki        i pi wl   returns one
frequent pattern  pi   appears within workflow zero otherwise 
use propositional workflow representation together tabular representation datasets characteristics learn meta mining models describe
next section  although could used tree even graph properties represent
   

finguyen  hilario   kalousis

workflows  propositionalization standard approach used extensively successfully
learning problems learning instances complex structures  kramer  lavrac 
  flach        
also  propositional workflow representation easily deal parameter values
different operators appear within workflows  so  discretize
range values continuous parameter ranges low  medium  high 
ranges depending nature parameter  treat discretized values
simply property operators  resulting patterns parameter aware 
include information parameter range mined operators
used support parameter setting planning dm workflows 
however within paper explore possibility 
      performance based ranking dm workflow
characterize performance number workflows applied given dataset
use relative performance rank schema derive using statistical significance
tests  given estimations performance measure different workflows
given dataset use statistical significance test compare estimated performances
every pair workflows  within given pair one workflows significantly
better gets one point gets zero points 
significance difference workflows get half point  final performance rank
workflow given dataset simply sum points pairwise
performance comparisons  higher better  denote relative performance
rank workflow wc applied dataset xu r xu   wc    note workflow
applicable  executed  dataset x  set rank score default value zero
means workflow appropriate  if yet executed  given dataset 
planning goal g classification task  use evaluation measure
experiments classification accuracy  estimated ten fold cross validation 
significance testing using mcnemars test  significance level      
next section describe build meta mining models
past data mining experiments using meta data performance measures
described far use models support dm workflow planning 
    learning meta mining models workflow planning
starting describe detail build meta mining models let us take
step back give abstract picture type meta mining setting
address  previous sections  described two types learning instances  datasets
x x workflows w w  given set datasets set workflows stored
dmer  meta miner build these  two training matrices x w 
x   n dataset matrix  ith row description xui ith dataset 
w    p   workflow matrix  j th row description wcj jth workflow 
preference matrix r   n m  rij   r xui   wcj    i e  gives
relative performance rank workflow wj applied dataset xi respect
workflows  see rij measure appropriateness match
wj workflow xi dataset  ith line r matrix contains vector
   

fiusing meta mining support dm workflow planning optimization

relative performance ranks workflows applied xui dataset 
meta miner take input x  w r matrices output model
predicts expected performance  r xu   wc    workflow w applied dataset x 
construct meta mining model using similarity learning  exploiting two basic
strategies initially presented context dm workflow selection  nguyen et al       b  
give high level presentation them  details interested user
refer original paper  first strategy learn homogeneous similarity
measures  measuring similarity datasets similarity workflows  use
derive r xu   wc  g  estimates  second learn heterogeneous similarity measures
directly estimate appropriateness workflow dataset  i e  produce
direct estimates r xu   wc  g  
      learning homogeneous similarity measures
goal provide meta mining models good predictors performance
workflow applied dataset  simplest approach want learn good
similarity measure dataset space deem two datasets similar set
workflows applied result similar relative performance  i e 
order workflows according performance achieve dataset
two datasets similar workflow orders similar  thus learned similarity
measure dataset space good predictor similarity datasets
determined relative performance order workflows  completely
symmetrical manner consider two workflows similar achieve similar
relative performance scores set datasets  thus case workflows learn
similarity measure workflow space good predictor similarity
relative performance scores set datasets 
briefly  learn two mahalanobis metric matrices  mx   mw   datasets
workflows respectively  optimizing two following convex metric learning optimization
problems 
min f      rrt xmx xt    f   tr mx  
mx

s t 

   

mx  


min f      rt r wmw wt    f   tr mw  
mw

s t 

   

mw  

     f frobenius matrix norm  tr   matrix trace    parameter
controlling trade off empirical error metric complexity control overfitting  rrt   n n matrix reflects similarity relative workflow performance
vectors different dataset pairs learned dataset similarity metric
reflect  rt r   matrix gives respective similarities workflows 
details learning problem solve it  see work nguyen et al 
     b  
   

finguyen  hilario   kalousis

note far model computes expected relative performance
r xu   wclo    case homogeneous metric learning compute on line
mode planning phase  describe right away following
paragraph 
planning homogeneous similarity metrics  p   use two
learned mahalanobis matrices  mx   mw   compute dataset similarity workflow similarity finally compute estimates r xu   wclo   planning
step 
concretely  prior planning determine similarity input dataset xu  for
want plan optimal dm workflows  training datasets xui x using
mx dataset metric measure dataset similarities  mahalanobis similarity
two datasets  xu   xui   given
sx  xu   xui     xtu mx xui

   

then  planning planning step determine similarity candidate
workflow wlo cl training workflows wcj w 
sw  wclo   wcj     wctlo mw wcj  

   

finally derive r xu   wclo   estimate weighted average elements
r matrix  weights given similarity input dataset xu
training datasets  similarities candidate workflow wclo training
workflows  formally expected rank given by 
p
p
wcj w xui wcj r xui   wcj  g 
xui x
p
p
    
r xu   wclo  g   
wc w xui wcj
xu x
j



xui wcj gaussian weights given xui   exp sx  xu   xui   x   wcj  
exp sw  wclo   wcj   w    x w kernel widths control size neighbors
data workflow spaces respectively  smart   kaelbling        forbes   andre 
      
using rank performance estimates delivered eq       select planning step best candidate workflows set  sl   according eq      call resulting
planning strategy p   p  expected performance set selected candidate
workflows sl greedily increases deliver k dm complete workflows
expected achieve best performance given dataset 
      learning heterogeneous similarity measure
p  planning strategy makes use two similarity measures learned independently other  one defined feature space  simplistic
assumption model interactions workflows datasets 
know certain types dm workflows appropriate datasets certain
types characteristics  order address limitation  define heterogeneous
metric learning problem directly estimate similarity appropriateness
   

fiusing meta mining support dm workflow planning optimization

workflow given dataset given r xu   wc   relative performance
measure 
since learning mahalanobis metric equivalent learning linear transformation
rewrite two mahalanobis metric matrices described previously mx   uut
mw   vvt   u   v    p   respective linear transformation matrices
dimensionality   min rank x   rank w   
learn heterogeneous similarity
measure datasets workflows using two linear transformations solve
following optimization problem 
min f      r xuvt wt    f     rrt xuut xt    f
u v

    rt r wvvt wt    f  

    


   u   f     v   f  
 

using alternating gradient descent algorithm  first optimize u keeping
v fixed vice versa  optimization problem non convex algorithm
converge local minimum  first term similar low rank matrix factorization
srebro  rennie  jaakkola         however factorization learn
function dataset workflow feature spaces result address
samples training instances  known cold start problem
recommendation systems  case dm workflow planning problem strong
requirement need able plan workflows datasets never
seen training  able qualify workflows seen
training  second third terms define metrics reflect performancebased similarities datasets workflows respectively  along lines homogeneous
metrics given previously   together give directly similarity appropriateness
dm workflow dataset estimating expected relative predictive performance
as 
r xu   wclo  g    xu uvt wctlo

    

see heterogeneous similarity metric performing projection dataset
workflow spaces common latent space compute standard similarity
projections  details  see work nguyen et al       b  
planning heterogeneous similarity measure  p   planning heterogeneous similarity measure  strategy denote p   much simpler planning homogeneous similarity measures  given input dataset described xu
step planning make use relative performance estimate r xu   wclo  g 
delivered eq      select set best workflows sl set partial workflows cl using selection process described eq      unlike planning strategy p 
computes r xu   wclo  g  weighted average help two independently learned similarity metrics  p  relies heterogeneous metric directly computes
r xu   wclo  g   modeling thus explicitly interactions dataset workflow characteristics 
note p  p  planning strategies able construct
workflows even pools ground operators include operators
never experimented baseline experiments  provided operators well
   

finguyen  hilario   kalousis

described within dmop  meta mining models planner uses
prioritize workflows rely wclo descriptions workflow generalized
descriptions workflows operators 
next section evaluate ability two planning strategies
introduced plan dm workflows optimize predictive performance compare
number baseline strategies different scenarios 

   experimental evaluation
evaluate approach data mining task classification  reasons
rather practical  classification supervised task means
ground truth compare results produced classification
algorithm  using different evaluation measures accuracy  error  precision etc 
mining tasks clustering  performance evaluation comparison bit
problematic due lack ground truth  extensively studied 
extensively used many application fields  resulting plethora benchmark datasets 
easily reuse construct base level experiments well evaluate
system  moreover  extensively addressed context meta learning 
providing baseline approaches compare approach  finally approach
requires different algorithms operators use well described
dmop ontology  due historical predominance classification task algorithms
well extensive use real world problems  started developing dmop them 
result task classification corresponding algorithms well described 
said this  emphasize approach limited task
classification  applied mining task define evaluation
measure  collect set benchmark datasets perform base level experiments  provide descriptions task respective algorithms dmop 
train evaluate approach  collected set benchmark classification
datasets  applied set classification data mining workflows 
base level experiments learn meta mining models used
planner plan data mining workflows  challenge system new datasets
used training meta mining models  datasets
plan new classification workflows achieve high level predictive performance 
explore two distinct evaluation scenarios  first one  constrain
system plans dm workflows selecting operators restricted operator
pool  namely operators experimented base level experiments 
thus operators characterized dmop ontology tested
base level experiments  call tested operators  second scenario
allow system choose operators never experimented
characterized dmop ontology  call operators untested
operators  goal second scenario evaluate extend system
effectively use untested operators workflows designs 
   

fiusing meta mining support dm workflow planning optimization

type
fs tested
fs tested
fs tested

abbr 
ig
chi
rf

parameters
  features selected k     
  features selected k     
  features selected k     

svmrfe

  features selected k     

fs untested
cl tested

operator
information gain
chi square
relieff
recursive feature
elimination svm
information gain ratio
one nearest neighbor

igr
 nn

  features selected k     

cl tested

c   

c   

cl tested

cart

cart

fs tested

cl tested
cl tested

naivebayes normal
density estimation
logistic regression
linear kernel svm

cl tested

gaussian kernel svm

svmr

cl untested
cl untested
cl untested
cl untested

linear discriminant analysis
rule induction
random decision tree
perceptron neural network

lda
ripper
rdt
nnet

cl tested

 pruning confidence c
 min  inst  per leaf
 pruning confidence c
 min  inst  per leaf

      
  
      
  

nb
lr
svml

 complexity c    
 complexity c    
 gamma      

table    table operators used design dm workflows    datasets  type
corresponds feature selection  fs  classification  cl  operators  operators experimented marked tested  otherwise untested 

    base level datasets dm workflows
construct base level experiments  collected    real world datasets genomic
microarray proteomic data related cancer diagnosis prognosis  mostly
national center biotechnology information    typical datasets 
datasets use characterized high dimensionality small sample size 
relatively low number classes  often two  average       instances 
         attributes       classes 
build base level experiments  applied datasets workflows consisted either single classification algorithm  combination feature selection
classification algorithm  although htn planner use  kietz et al              
able generate much complex workflows     different tasks     
operators  limit planning classification  feature selection
classification  workflows simply respective tasks  algorithms operators
well annotated dmop  annotation important characterization
workflows construction good meta mining models used guide
planning  nevertheless  system directly usable planning scenarios com   http   www ncbi nlm nih gov 

   

finguyen  hilario   kalousis

plexity  describe htn grammar  provided appropriate tasks 
algorithms operators annotated dmop ontology 
used four feature selection algorithms together seven classification algorithms
build set base level training experiments  given table    noted
tested  mentioned previously  plan operators parameters
discretizing range values parameters treating properties
operators  another alternative use inner cross validation automatically select
set parameter values  strictly speaking  case  would selecting
standard operator cross validated variant  nevertheless  would incur significant
computational cost 
overall  seven workflows contained classification algorithm 
   workflows combination feature selection classification algorithm 
resulting total    workflows applied    datasets corresponds      baselevel dm experiments  performance measure use accuracy estimate
using ten fold cross validation  algorithms  used implementations provided
rapidminer dm suite  klinkenberg et al         
already said  two evaluation settings  first  scenario    constrain
system plan workflows using tested operators  second  scenario   
allow system select untested operators  additional operators
given table    denoted untested  total number possible workflows
setting    
    meta learning   default methods
compare performance system two baseline methods default
strategy  two baseline methods simple approaches fall classic metalearning stream instead selecting individual algorithms select
workflows  thus cannot plan dm workflows used setting
workflows choose seen model construction phase 
first meta learning method use  call eucl  standard
approach meta learning  kalousis   theoharis        soares   brazdil        
makes use euclidean based similarity dataset characteristics select n
similar datasets input dataset xu want select workflows
averages workflow rank vectors produce average rank vector 
n
  x
rxui   xui  arg max xtu xui  n
n
xui x

    



uses order different workflows  thus method simply ranks workflows
according average performance achieve neighborhood input
dataset  second meta learning method call metric makes use learned
dataset similarity measure given eq     select n similar datasets
input dataset averages well respective workflow rank vectors 
n
  x
rxui   xui  arg max xtu mx xui  n
n
xui x


   

    

fiusing meta mining support dm workflow planning optimization

   

   

   

   

chi c  
rf nbn
svmrfe c  
chi cart
rf c  
svmr
 nn
ig c  
rf  nn
svmrfe cart
c  
chi svmr
rf cart
chi svml
ig cart
rf svmr
rf svml
svmrfe  nn
chi  nn
ig  nn
svmrfe svmr
cart
chi nbn
ig svmr
svmrfe lr
chi lr
rf lr
svmrfe nbn
ig svml
svml
ig lr
svmrfe svml
nbn
ig nbn
lr

   

figure    percentage times workflow among top   workflows different datasets 

default recommendation strategy  simply use average rxui workflow
rank vectors collection training datasets 
 x
rxui   xui x
n
n

    



rank select workflows  note rather difficult baseline
beat  see case plot figure   percentage times
   dm workflows appears among top   worfklows    datasets  top
workflow  consisting lr algorithm  among top      
datasets  next two workflows  nbn ig nbn  among top  
almost     datasets  words select top   workflows using default
strategy roughly     datasets lr correctly them 
nbn ig nbn percentage around      thus set dataset
quite similar respect workflows perform better them  making
default strategy rather difficult one beat 
    evaluation methodology
estimate performance planned workflows evaluation scenarios
use leave one dataset out  using time    datasets build meta mining
models one dataset plan 
evaluate method measuring well list  l  top k ranked workflows  delivers given dataset  correlates true list    top k ranked
   

finguyen  hilario   kalousis

workflows dataset using rank correlation measure  place true quotes
general case  i e  restrict choice operators specific
set  cannot know true best workflows unless exhaustively examine
exponential number them  however since select restricted list operators
set best  precisely  measure rank correlation
two lists l   use kendall distance p penalty  denote
k  p   l     fagin  kumar    sivakumar         kendall distance gives number
exchanges needed bubble sort convert one list other  assigns penalty
p pair workflows one workflow one list other 
set p        k        l    normalized  propose define normalized
kendall similarity ks l t  as 
 

k        l   
ks l       
u

    

 

   

pktakes values         u upper bound k  l    given u      k  k     
  i   i  derived direct application lemma     work fagin et al         
assume two lists share element  qualify method 
m  including two baselines  kendall similarity gain  kg m   i e  gain  or loss 
achieves respect default strategy given datasets  compute as 

kg m  l     

ks m  l   
 
ks def   l   

    

method  report average kendall similarity gain overall datasets 
kg m   note that  scenario    default strategy based average ranks
   workflows  scenario    default strategy based average ranks   
workflows  experiment order set baseline 
addition see well top k ranked list workflows  given method
suggests given dataset  correlates true list  compute average accuracy
top k workflows suggests achieve given dataset  report average
overall datasets 
    meta mining model selection
iteration leave one dataset out evaluation planning performance 
rebuild meta mining model tune parameter mahalanobis metric
learning using inner ten fold cross validation  select value maximizes
spearman rank correlation coefficient predicted workflow rank vectors
real rank vectors  heterogenous metric  used parameter setting defined
nguyen et al       b   two meta learning methods  fixed number n
nearest neighbors five  reflecting prior belief appropriate neighborhood size 
planning  set manually dataset kernel width parameter kx       
workflow kernel width parameter kw        result small dataset workflow
neighborhoods respectively  again  two parameters tuned simply set
prior belief respective neighborhood size 
   

fiusing meta mining support dm workflow planning optimization

 b  scenario    tested untested operators 

    

  

  

  

  

  

kg

    

    

 

    
    

    

 

p 
p 
def  

    

    
    

p 
p 
metric
eucl
def  

    

kg

    

    

 a  scenario    tested operators 

  

 

k

 

  

  

  

  

  

  

k

figure    average correlation gain kg different methods baseline
   bio datasets  x axis  k               number top k workflows
suggested user  p  p  two planning strategies  metric
eucl baseline methods defx default strategy computed set
x workflows 

    experimental results
following sections give results experimental evaluation different
methods presented far two evaluation scenarios described above 
      scenario    building dm workflows pool tested
operators
scenario  evaluate quality dm workflows constructed two
planning strategies p  p  compare two baseline methods well
default strategy  leave one dataset out evaluate workflow
recommendations given method  figure   a  give average kendall gain
kg method default strategy compute top k lists
k                  clearly p  strategy one gives largest improvements
respect default strategy         gain  compared method 
establish statistical significance results k  counting number
datasets method better worse default  using mcnemars
test  summarize figure   statistical significance results given p values
different ks give detailed results table   appendix methods
k               see p   far best method significantly better
default       values k  close significant         p value     
ten    times never significantly worse  methods  p 
managed beat default      cases k 
   

finguyen  hilario   kalousis

   
 

  

  

  

  

  

  

 

  

  

  

  

  

metric    wins   losses 

eucl    wins   losses 

  

   
   
   

   

   

   

pvalue

   

   

k

   

k

   

pvalue

   

pvalue

   

   

   
   

   

pvalue

   

   

p     wins   losses 

   

p      wins   losses 

 

  

  

  

  

  

  

 

k

  

  

  

  

  

  

k

figure    p values mcnemars test number times kendal similarity
method better default given k  scenario    positive pvalue means wins losses  negative opposite  solid lines
p            dash dotted p           x wins  losses
header indicates number times k         method
significantly better worse default 

examine average accuracy top k workflows suggested
method achieve  advantage p  even striking  average accuracy      
      higher default strategy  k     k     respectively  see
table   a   k      p  achieves higher average accuracy default   
   datasets  under performs compared default     using
mcnemars test statistical significance       i e  p  significantly better
default strategy comes average accuracy top k     workflows
plans  results similar k      fact eight top k lists  k               p 
significantly better default five values k  close significantly better once 
never significantly worse  higher values k  k                significantly
better    times  close significantly better three times  never significantly worse 
stops significantly better k       large k values  average taken
almost workflows  thus expect important differences lists
produced different methods  figure     visualize statistical significance
results different values k              give detailed results table  
appendix 
   

fiusing meta mining support dm workflow planning optimization

   
 

  

  

  

  

  

  

 

  

  

  

  

  

metric    wins   losses 

eucl    wins   losses 

  

   
   
   

   

   

   

pvalue

   

   

k

   

k

   

pvalue

   

pvalue

   

   

   
   

   

pvalue

   

   

p     wins   losses 

   

p      wins   losses 

 

  

  

  

  

  

  

 

k

  

  

  

  

  

  

k

figure     p values mcnemars test number times average accuracy method better default given k  scenario   
figure interpretation figure  

p  never significantly better default k               k               
significantly better nine values k  close significantly better three times 
close significantly worse once  metric baseline never significantly better
default k               k                significantly better four values
k and close significantly better four times  results ec quite poor  terms
average accuracy  similar default  terms number times
performs better default  cases  less number
times performs worse default  figure    give results
statistical significance results different values k detailed results
table   appendix  p  method directly learns exploits
workflow planning associations dataset workflow characteristics
experimental results clearly demonstrate strategy best pays off 
      scenario    building dm workflows pool tested
non tested operators
second scenario  evaluate performance two planning strategies  p 
p   pool available operators planning limited
operators already experimented base level experimented with 
extended include additional operators described dmop ontology 
   

finguyen  hilario   kalousis

 a  scenario    tested operators

p 
p 
metric
eucl
def  

avg  acc
      
      
      
      
      

k  
w l
     
     
     
     

p value
     
    
    
    

avg  acc
      
      
      
      
      

k  
w l
     
     
     
     

p value
     
    
    
    

 b  scenario    tested untested operators

p 
p 
def  

avg  acc
      
      
      

k  
w l
     
     

p value
    
    

avg  acc
      
      
      

k  
w l
     
     

p value
    
    

table    average accuracy top k workflows suggested method  w indicates
number datasets method achieved top k average accuracy larger
respective default  l number datasets smaller
default  p value result mcnemars statistical significance
test    indicates method statistically better default 

already described exact setting section      reminder number
possible workflows     before  estimate performances using leave onedataset out  note two baseline methods  metric eucl  applicable
setting  since deployed workflows already
experimented baseline experiments  here  already explained section     
default strategy correspond average rank    possible workflows
denote def    note highly optimistically biased default method
since relies execution    possible workflows base level datasets  unlike
p  p  get see    workflows model building  operators
therein  plan larger pool 
figure   b   give average kendall gain kg p  p  def  
baseline  similarly first evaluation scenario  p  advantage p  since
demonstrates higher gains default  note though performance gains
smaller previously  terms number k values p 
 close be  significantly better default  six eight 
different k               def   significantly better p  close
significantly better  concerns p   significant difference
performance def    value k  values k       p  systematically
under performs compared def    due advantage latter comes
seeing performance    workflows base level dataset  visualize
statistical significance results top row figure     give detailed results
table   appendix values k              
   

fiusing meta mining support dm workflow planning optimization

   
  

  

  

  

  

  

 

  

  

  

  

  

k

p     wins    losses 

p     wins   losses 

  

   
   
   
   

   

   

   

pvalue

   

   

k

   

 

pvalue

   

pvalue

   

   

   
   

   

pvalue

   

   

p     wins   losses 

   

p     wins   loss 

 

  

  

  

  

  

  

 

k

  

  

  

  

  

  

k

figure     top row p values mcnemars test number times kendall
similarity method better default given k  scenario    bottom
row average accuracy  figure interpretation figure   

concerns performance p  respect average accuracy
top k workflows suggests  slight advantage def   small
values k  four  significantly better compared def   once  k     
k         two methods significant difference  k               p 
worse     times significant manner  p  picture slightly different  average
accuracy significantly different def    exception three k values
significantly worse  visualize statistical significance results bottom
row figure     give detailed results table    seems fact p 
learns directly associations datasets workflow characteristics puts
disadvantage want plan operators tested training
phase  scenario  p  strategy weights preferences dataset similarity
workflow similarity seems cope better untested operators  nevertheless
still possible outperform default strategy significant manner  keeping
however mind def   optimistic default strategy based
experimentation possible workflows training dataset 
    discussion
previous sections  evaluated two workflow planning strategies two settings 
planning tested operators  planning tested untested operators 
   

finguyen  hilario   kalousis

first scenario  p  planning strategy makes use heterogeneous metric
learning model  directly connects dataset workflow characteristics  clearly stands
out  outperforms default strategy terms kendall similarity gain 
statistically significant  close statistically significant  manner    values k
                 terms average accuracy top k workflows  outperforms   
values k statistically significant  close statistically significant  manner 
methods  including p   follow large performance difference p  
allow planners include workflows operators
used baseline experiments  annotated dmop ontology  p s
performance advantage smaller  terms kendall similarity gain  statistically
significant  close to  k                   respect average accuracy top k
lists  better default small lists  k         k      
fact significantly worse  p  fairs better second scenario  however performance
different default method  keep mind though baseline used
second scenario quite optimistic one 
fact  see able generalize plan well datasets 
evidenced good performance p  first setting  however  comes
generalizing datasets operators case second scenario
performance planned workflows good  exception top
workflows  take look new operators added second scenario 
feature selection algorithm  information gain ratio  four classification algorithms 
namely linear discriminant algorithm  ripper rule induction algorithm  neural net
algorithm  random tree  them  information gain ratio seen
base level set experiments algorithm  information gain  rather
similar learning bias it  ripper rule induction sequential covering algorithm 
closest operators set training operators two decision tree
algorithms recursive partitioning algorithms  respect dmop ontology 
ripper shares certain number common characteristics decision trees  however
meta mining model contains information set covering learning bias performs
different datasets  might lead selected given dataset based
common features decision trees  learning bias fact appropriate
dataset  similar observations hold algorithms  example lda
shares number properties svml lr  however learning bias  maximizing
between to within class distances ratio  different learning biases
two  meta mining model contains information bias associates
dataset characteristics 
overall  extent system able plan  tested untested
operators  workflows achieve good performance  depends extend
properties latter seen training meta mining models
within operators experimented with  well whether
unseen properties affect critically final performance  case operators
well characterized experimentally  scenario    performance
workflows designed p  strategy good  note necessary
operators workflows applied datasets  enough sufficient set
experiments operator  heterogeneous metric learning algorithm handle
   

fiusing meta mining support dm workflow planning optimization

 b 

 a 

x validation

x validation

dataprocessing
algorithm
fwalgorithm

classificationmodeling dataprocessing
algorithm
algorithm
fwalgorithm

highbiascma

multivariatefw
algorithm

classificationmodeling
algorithm
highvariancecma

univariatefw
algorithm

figure     top ranked workflow patterns according average absolute weights given
matrix v 

incomplete preference matrices  using available information  course clear
available information  whether form complete preference matrices
form extensive base level experiments large number datasets  better
quality learned meta mining model be  interesting explore
sensitivity heterogeneous metric learning method different levels completeness
preference matrix  however outside scope present paper 
quantify importance different workflow patterns
operators properties analyzing linear transformation workflow patterns
contained heterogeneous metric  precisely  establish learned importance
workflow pattern averaging absolute values weights assigned
different factors  rows  v linear transformation matrix eq       note
approach  establish importance patterns  whether
associated good bad predictive performance  figure     give two
important patterns determined basis averaged absolute
weights  describe relations workflow operators  first one
indicates multivariate feature weighting algorithm followed high bias
classification algorithm  second describes univariate feature weighting algorithm
followed high bias classification algorithm  systematic analysis learned model
could provide hints one focus ontology building effort  looking
important patterns well patterns used  addition 
reveal parts ontology might need refinement order distinguish
different workflows respect expected performance 

   conclusions future work
paper  presented is  best knowledge  first system
able plan data mining workflows  given task given input dataset 
expected optimize given performance measure  system relies tight
interaction hierarchical task network planner learned meta mining model
plan workflows  meta mining model  heterogeneous learned metric  associates
datasets characteristics workflow characteristics expected lead good
   

finguyen  hilario   kalousis

performance  workflow characteristics describe relations different components workflows  capturing global interactions various operators appear
within them  incorporate domain knowledge latter given data mining ontology  dmop   learn meta mining model collection past base level mining
experiments  data mining workflows applied different datasets  carefully evaluated
system task classification showed outperforms significant
manner number baselines default strategy plan operators
experimented base level experiments  performance
advantage less pronounced consider planning operators
experimented base level experiments  especially
properties operators present within operators
experimented base level experiments 
system directly applicable mining tasks e g  regression  clustering 
reasons focused classification mainly practical  extensive
annotation classification task related concepts data mining ontology 
large availability classification datasets  extensive relevant work meta learning
dataset characterization classification  main hurdle experimenting
different mining task annotation necessary operators dmop ontology
set base level collection mining experiments specific task  although
annotation new algorithms operators quite labor intensive task  many
concepts currently available dmop directly usable mining tasks  e g  cost
functions  optimization problems  feature properties etc  addition  small active
community  dmo foundry    maintaining augmenting collaboratively ontology
new tasks operators  significantly reducing deployment barrier new task 
dmo foundry web site  one find number tools templates facilitate
addition new concepts operators ontology well annotate
existing ones  said note use dmop sine qua non
system function  well perform workflow characterization task
mining ground operators  without using ontology  downside
would extracted patterns generalized contain operator
properties  instead defined ground operators  everything else remains
is 
number issues still need explore finer detail  would
gain deeper understanding better characterization reduced performance planning untested operators  example  conditions
relatively confident suitability untested operator within workflow  want
experiment strategy suggested parameter tuning  treat
parameters yet another property operators  order see whether gives better
results  expect will  want study detail level missing information
preference matrix affects performance system  well whether using
ranking based loss functions metric learning problem instead sum squares would
lead even better performance 

   http   www dmo foundry org 

   

fiusing meta mining support dm workflow planning optimization

ambitious level want bring ideas reinforcement learning  sutton
  barto         let system design workflows systematic way
applied collection available datasets order derive even better characterizations
workflow space relate dataset space  exploring example areas
meta mining model less confident 

acknowledgments
work partially supported european community  th framework program ict          grant number        e  lico  e laboratory interdisciplinary collaborative research data mining data intensive science  alexandros
kalousis partially supported rsco isnet nft project  basic htn planner result collaborative work within e lico project jorg uwe kietz 
floarea serban  simon fischer  would thank jun wang important contribution developing metric learning part paper  addition would
thank members ai lab  adam woznica  huyen do  jun wang 
significant effort placed providing content dmop  finally  would
thank reviewers suggestions helped improve paper 

   

finguyen  hilario   kalousis

appendix a  detailed results
k
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
total

p 
w l p value
 
 
 
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
wins    losses  

p 
w l p value
       
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
wins   losses  

metric
w l p value
 
        
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
wins   losses  

eucl
w l p value
 
        
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
wins   losses  

table    wins losses respective p values mcnemars test number
times kendal similarity method better kendal similarity
default  scenario    bold  winning p value lower      

   

fiusing meta mining support dm workflow planning optimization

k
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
total

p 
avg acc w l p value
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
 
 
     
     
           
     
 
 
 
wins    losses  

p 
avg acc w l p value
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
 
 
 
wins     losses

metric
avg acc w l p value
     
           
     
           
     
       
     
       
     
       
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
       
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
 
 
     
wins   losses  

ec
avg acc w l p value
     
           
     
       
     
       
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
       
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
 
 
 
wins    losses  

def
avg acc
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    average accuracy  wins losses  respective p values mcnemars test
number times average accuracy method better average
accuracy default  scenario    bold  winning p value lower
     

   

finguyen  hilario   kalousis

k
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
total

p 
w l p value
 
        
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
wins   losses  

p 
w l p value
 
        
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
wins   losses  

table    wins losses p values mcnemars test number times kendal
similarity method better kendal similarity default  scenario
   bold  winning p value lower      

   

fiusing meta mining support dm workflow planning optimization

k
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
total

p 
avg acc w l
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
wins   losses

p value
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
  

p 
avg acc w l p value
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
     
           
wins   losses  

def
avg acc
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    avg acc   wins losses  respective p values mcnemars test
number times average accuracy method better average
accuracy default  scenario    bold  winning p value lower
     

   

finguyen  hilario   kalousis

references
bernstein  a   provost  f     hill  s          toward intelligent assistance data mining
process  ontology based approach cost sensitive classification  knowledge
data engineering  ieee transactions on                 
bose  r  j  c     der aalst  w  m  v          abstractions process mining  taxonomy
patterns  proceedings  th international conference bussiness process
management 
brazdil  p   giraud carrier  c   soares  c     vilalta  r          metalearning  applications
data mining    edition   springer publishing company  incorporated 
bringmann  b          matching frequent tree discovery  proceedings fourth
ieee international conference data mining  icdm    pp         
chapman  p   clinton  j   kerber  r   khabaza  t   reinartz  t   shearer  c     wirth  r 
        crisp dm     step by step data mining guide  tech  rep   crisp dm
consortium 
fagin  r   kumar  r     sivakumar  d          comparing top k lists  proceedings
fourteenth annual acm siam symposium discrete algorithms  soda     pp 
      philadelphia  pa  usa  society industrial applied mathematics 
fayyad  u   piatetsky shapiro  g     smyth  p          data mining knowledge
discovery databases  ai magazine             
forbes  j     andre  d          practical reinforcement learning continuous domains 
tech  rep   berkeley  ca  usa 
gil  y   deelman  e   ellisman  m   fahringer  t   fox  g   gannon  d   goble  c   livny 
m   moreau  l     myers  j          examining challenges scientific workflows 
computer                
hall  m   frank  e   holmes  g   pfahringer  b   reutemann  p     witten  i  h         
weka data mining software  update  sigkdd explor  newsl                
hilario  m          model complexity algorithm selection classification  proceedings  th international conference discovery science  ds     pp         
london  uk  uk  springer verlag 
hilario  m     kalousis  a          fusion meta knowledge meta data casebased model selection  proceedings  th european conference principles
data mining knowledge discovery  pkdd     pp          london  uk  uk 
springer verlag 
hilario  m   kalousis  a   nguyen  p     woznica  a          data mining ontology
algorithm selection meta learning  proc ecml pkdd   workshop
third generation data mining  towards service oriented knowledge discovery 
hilario  m   nguyen  p   do  h   woznica  a     kalousis  a          ontology based metamining knowledge discovery workflows  jankowski  n   duch  w     grabczewski 
k   eds    meta learning computational intelligence  springer 
   

fiusing meta mining support dm workflow planning optimization

ho  t  k     basu  m          complexity measures supervised classification problems 
ieee trans  pattern anal  mach  intell                  
ho  t  k     basu  m          data complexity pattern recognition  springer 
hoffmann  j          ff  fast forward planning system  ai magazine             
kalousis  a          algorithm selection via metalearning  ph d  thesis  university
geneva 
kalousis  a   gama  j     hilario  m          data algorithms  understanding
inductive performance  machine learning                 
kalousis  a     theoharis  t          noemon  design  implementation performance
results intelligent assistant classifier selection  intell  data anal             
    
kietz  j  u   serban  f   bernstein  a     fischer  s          towards cooperative planning data mining workflows  proc ecml pkdd   workshop third
generation data mining  towards service oriented knowledge discovery  sokd     
kietz  j  u   serban  f   bernstein  a     fischer  s          designing kdd workflows via
htn planning intelligent discovery assistance   th planning learn
workshop ws   ecai       p     
king  r  d   feng  c     sutherland  a          statlog  comparison classification
algorithms large real world problems  applied artificial intelligence            
    
klinkenberg  r   mierswa  i     fischer  s          free data mining software  rapidminer
     formerly yale   http   www rapid i com  
kopf  c   taylor  c     keller  j          meta analysis  data characterisation
meta learning meta regression  proceedings pkdd    workshop data
mining  decision support meta learning ilp 
kramer  s   lavrac  n     flach  p          relational data mining   chap  propositionalization approaches relational data mining  pp          springer verlag new
york  inc   new york  ny  usa 
leite  r     brazdil  p          active testing strategy predict best classification algorithm via sampling metalearning  proceedings      conference ecai
        th european conference artificial intelligence  pp          amsterdam 
netherlands  netherlands  ios press 
mcdermott  d   ghallab  m   howe  a   knoblock  c   ram  a   veloso  m   weld  d    
wilkins  d          pddl the planning domain definition language  
michie  d   spiegelhalter  d  j   taylor  c  c     campbell  j          machine learning 
neural statistical classification  
nguyen  p   kalousis  a     hilario  m          meta mining infrastructure support kd
workflow optimization  proc ecml pkdd   workshop planning learn
service oriented knowledge discovery    
   

finguyen  hilario   kalousis

nguyen  p   kalousis  a     hilario  m       a   experimental evaluation e lico
meta miner   th planning learn workshop ws   ecai      
p     
nguyen  p   wang  j   hilario  m     kalousis  a       b   learning heterogeneous similarity
measures hybrid recommendations meta mining  ieee   th international
conference data mining  icdm   pp            
peng  y   flach  p  a   soares  c     brazdil  p          improved dataset characterisation
meta learning  discovery science  pp          springer 
pfahringer  b   bensusan  h     giraud carrier   c          meta learning landmarking various learning algorithms   proc    th international conference machine
learning         
r core team         r  language environment statistical computing  http 
  www r project org  
smart  w  d     kaelbling  l  p          practical reinforcement learning continuous
spaces  proceedings seventeenth international conference machine learning  icml     pp          san francisco  ca  usa  morgan kaufmann publishers
inc 
soares  c     brazdil  p          zoomed ranking  selection classification algorithms based
relevant performance information  proceedings  th european conference
principles data mining knowledge discovery  pkdd     pp         
london  uk  springer verlag 
srebro  n   rennie  j  d  m     jaakkola  t  s          maximum margin matrix factorization  saul  l  k   weiss  y     bottou  l   eds    advances neural information
processing systems     pp            mit press  cambridge  ma 
sutton  r     barto  a          reinforcement learning  introduction  neural networks 
ieee transactions on              
van der aalst  w  m     giinther  c          finding structure unstructured processes 
case process mining  application concurrency system design       
acsd       seventh international conference on  pp       ieee 
yang  q     wu  x             challenging problems data mining research  international
journal information technology   decision making                 
zaki  m  j          efficiently mining frequent trees forest  algorithms applications 
ieee transactions knowledge data engineering                    special
issue mining biological data 
zakova  m   kremen  p   zelezny  f     lavrac  n          automating knowledge discovery workflow composition ontology based planning  automation science
engineering  ieee transactions on                

   


